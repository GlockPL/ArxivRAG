{"title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs", "authors": ["Zhihan Liu", "Shenao Zhang", "Yongfei Liu", "Boyi Liu", "Yingxiang Yang", "Zhaoran Wang"], "abstract": "Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce Direct Preference Learning with Only Self-Generated Tests and Code (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.", "sections": [{"title": "Introduction", "content": "The recent advancement of large language models (LLMs) has significantly transformed numerous fields, including code generation. LLMs pre-trained on vast code corpora demonstrate substantial potential in generating code, and many studies have further enhanced these code models through supervised fine-tuning (SFT) to improve performance on coding tasks. Although SFT has shown promising results in improving code LMs, SFT focuses on replicating patterns within the dataset and often lacks the generalization needed to perform well on unseen instructions. This limitation has driven researchers to apply reinforcement learning (RL) to improve code LMs beyond the SFT stage by allowing them to adapt to more complex or variable inputs.\nHowever, one of the main challenges in applying RL to code LMs is the scarcity of high-quality preference datasets tailored specifically to code generation tasks. Unlike standard language tasks, where user or human annotator feedback is widely available, a preference dataset for the code generation has unique requirements, including correctness and functional alignment, which complicates data collection and is much more expensive. Recent works, such as Starcoder2-15b-instruct (Wei et al., 2024a), leverage execution feedback from the compiler to create SFT training datasets from self-generated tests and code snippets, aiming to simulate preference signals without human annotations. However, this reliance on self-generated tests introduces risks: inaccuracies in these synthetic tests may result in unreliable annotations, which can negatively impact the effectiveness of the RL training process. For instance, in our experiments using the self-generated oss-instruct-sd2-50k dataset created by Starcoder2-15b-instruct, only 48% of responses are correct when evaluated with an external model, Deepseek-coder-v2-instruct (with 236 billion parameters), highlighting a significant failure rate that suggests the need for a more reliable approach to leverage these synthetic tests effectively.\nIn addition, direct preference learning methods, such as Direct Policy Optimization (DPO, Rafailov et al. (2023)) and Kahneman-Tversky Optimization (KTO, Ethayarajh et al. (2024)), offer advantages in terms of computational efficiency and simplicity compared to more complex RL methods like PPO (Schulman et al., 2017) as they bypass the training of a reward model and a critic model. Therefore, we select the direct preference learning method as the backbone of the RL training framework and analyze the following problem in this paper:\nWith only self-generated code and tests, how can we generate reliable preference pairs to improve code generation accuracy for code LMs through direct preference learning?\nTo address this question, we introduce a novel approach called Direct Preference Learning with Only Self-Generated Tests and Code (DSTC), which leverages only self-generated code snippets and tests to construct effective preference pairs. DSTC, as depicted in Figure 1, comprises two main components: (1) a minimax selection mechanism and (2) a concatenation of code snippets and tests. This framework aims to achieve two goals: (a) to enhance the quality of the chosen code-test pairs while enlarging the quality gap between the chosen"}, {"title": "1.1 Contributions", "content": "We summarize our contributions in two-folds.\n\u2022 We propose a mechanism DSTC to generate the preference pair from the self-generated code and tests to perform direct preference learning algorithms like DPO and KTO. DSTC consists of a minimax selection procedure and concatenation of the code and test. As a result, DSTC constructs preference pairs with a reliable correct order and enlarges the quality gap between the chosen code and rejected code, which reduces the misleading effect of incorrect self-generated tests and aids the training of direct preference learning methods such as DPO and KTO.\n\u2022 Experimental results demonstrate that combining our proposed DSTC mechanism with DPO and KTO improves coding accuracy (pass@1 score) in code LMs across multiple benchmarks and model sizes (15 billion and 33 billion parameters). Ablation studies further validate the importance of each component within DSTC."}, {"title": "1.2 Related Works", "content": "Reinforcement Learning from Human Feedback Our work builds on a rich body of research on Reinforcement Learning from Human Feedback (RLHF), a framework that has become essential for training state-of-the-art large language models (LMs) such as ChatGPT (Achiam et al., 2023), Gemini (Team et al., 2023), and Claude (Anthropic, 2023). In the RLHF pipeline, LMs are fine-tuned to optimize a learned reward model using the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017). However, PPO is often criticized for its instability, sample inefficiency, and sensitivity to hyperparameter tuning (Engstrom et al., 2020), making it computationally expensive and less accessible for the open-source community.\nTo address these limitations, recent research in RLHF has explored alternatives to PPO-based methods, with direct preference learning algorithms emerging as a promising direction (Zhao et al., 2023; Rafailov et al., 2023). These approaches bypass the reward model training phase, aligning LMs directly with human preferences. Since the introduction of the original Direct Preference Optimization (DPO) algorithm (Rafailov et al., 2023), numerous variants of the direct preference learning framework have been proposed, each tackling specific challenges from different angles (Liu et al., 2023b; Azar et al., 2023; Xiong et al., 2023; Tang et al., 2024; Ji et al., 2024; Ye et al., 2024; Pal et al., 2024; Hong et al., 2024; Rosset et al., 2024; Liang et al., 2024; Zhang et al., 2024b,d,c; Xu et al., 2024; Tajwar et al., 2024; Wu et al., 2024; Liu et al., 2024b).\nOne notable example is Kahneman-Tversky Optimization (KTO), proposed by Ethayarajh et al. (2024), which leverages the Kahneman-Tversky model of human utility to minimize a human-aware loss, in contrast to the standard Bradley-Terry model. While these methods prioritize improving LM general alignment abilities with human preferences, our work focuses on a different and specific goal: enhancing pass@1 scores in code generation.\nLarge Language Models for Code Generation. Besides the great achievements in Natural Language Processing, LMs exhibit great potential in several complex reasoning tasks like math (Ahn et al., 2024) and coding (Jiang et al., 2024). In particular, pre-training on code datasets has resulted in impressive performance from models such as StarCoder (Lozhkov et al., 2024), DeepSeekCoder (Zhu et al., 2024). Other models, like CodeQwen (Hui et al., 2024) and CodeLlama (Roziere et al., 2023), have further enhanced their capabilities through ongoing pre-training with additional code data. To further improve the coding performance, there exists a rich line of literature that study how to fine-tune the current pre-trained LMs for coding. Among them, Wei et al. (2024b) propose a mechanism to"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Reinforcement Learning from Human Feedback and Direct Policy Optimization", "content": "Reinforcement Learning from Human Feedback (RLHF) utilizes human preferences to guide the training of language models (LMs). A common approach involves pairwise preference modeling, where feedback is provided by comparing two model-generated responses to the same prompt. Consider a language model represented as a policy \\(\\pi_{\\theta}(.\\|.)\\), parameterized by \\(\\theta\\). Given a prompt x from the state space X, the model generates a response a from the distribution \\(\\pi_{\\theta}(.\\|x)\\). For a specific prompt x, the model generates two candidate responses a\u00b9 and a\u00b2 from the action space A. Human evaluators, or a reward model approximating human preferences, provide feedback as a binary label y \u2208 {0, 1}, indicating whether a\u00b9 > a\u00b2 (y = 1) or a\u00b2 > a\u00b9 (y = 0). This preference is modeled probabilistically using the Bradley-Terry (BT) model (Bradley and Terry, 1952) as follows:\n\\(P_r(b = 1|x, a^1, a^2) = \\sigma(r(x, a^1) \u2013 r(x, a^2)),\\)\nwhere r(x, a) is the human-provided score reflecting the quality of response a for prompt x, and \\(\\sigma(z) = 1/(1 + exp(-z))\\) is the sigmoid function."}, {"title": "2.2 Kahneman-Tversky Optimization", "content": "Instead of using the BT preference model (2.1) as DPO, Ethayarajh et al. (2024) proposed another direct preference learning algorithm Kahneman-Tversky optimization (KTO) from the view of Kahneman & Tversky's prospect theory. In KTO, the preference dataset \u0414\u043a\u0442\u043e = {(xi, ai, bi)}=1 contains N pairs of a prompt x\u2081 and a response ai, accompanied by a binary variable bi that indicates whether a\u017c is desired. Given the preference dataset \u0414\u043a\u0442\u043e and the reference policy \u03c0ref, \u039a\u03a4\u039f loss is defined as\n\\( \\mathcal{L}_{KTO} (\\theta) = \\mathbb{E}_{(x,a,b) \\sim D_{KTO}} [\\lambda_b \u2013 v(x, a)], \\)\nwhere\n\\( v(x, a) = \\begin{cases} \\lambda_{\\mathcal{H}}(\\beta (r_{\\theta}(x, a) \u2013 z_0)), & \\text{if } b = 1 \\\\  \\lambda_{\\mathcal{H}}(\\beta (z_0 \u2013 r_{\\theta}(x, a))), & \\text{otherwise} \\end{cases} \\)\n\\(r_{\\theta}(x, a) = \\log \\frac{\\pi_{\\theta}(a|x)}{\\pi_{ref}(a|x)} \\)\n\\(z_0 = \\mathbb{E}_{(x',a',b') \\sim D_{KTO}} [KL(\\pi_{\\theta} (a' | x) || \\pi_{ref} (a' |x))].\\)\nCompared with DPO, we remark that KTO allows an imbalance between the number of chosen and rejected responses."}, {"title": "2.3 Large Language Models for Code Generation", "content": "We formulate the code generation problem in the context of RLHF, where state x \u2208 X refers to some natural language instruction and action a \u2208 A refers to the response provided by the code LM \\(\\pi_{\\theta}(.\\|x)\\) that contains a code snippet y \u2208 \u03a5. To check the correctness of a code snippet, one common approach is to feed the concatenation of the code snippet y and a test z to a compiler and output the binary execution feedback. Here, test z \u2208 Z can be either hand-crafted or generated by LMs, which can also be incomplete or even wrong. Since we can execute the concatenation of code and test in a compiler and receive the binary feedback (pass or fail), we define a binary reward function r(z, y) : Z \u00d7 Y \u2194 {0,1} for any code snippet y and test z. Assume that there exists a ground-truth test z for the instruction x such that z can perfectly check the correctness of any code snippet for the instruction x. Similar to the RLHF objective in (2.2), the objective of code generation task can be formulated as\n\\(\\max_{\\theta} \\mathbb{E}_{x \\sim D, a \\sim \\pi_{\\theta}(.\\|x)} [r(z, E_x(a)) - \\beta \\cdot KL(\\pi_{\\theta}(.\\|x) | \\pi_{ref}(.\\|x))],\\)\nwhere Ex function extracts the code snippet y from the response a. The similarity between (2.2) and (2.7) motivates us to leverage the direct preference learning method like DPO and KTO to solve the code generation task while bypassing the training of a reward function."}, {"title": "3 Method", "content": "Overview. In this section, we introduce a novel mechanism called Direct Preference Learning with Only Self-Generated Tests and Code (DSTC), detailed in Algorithm 1 and illustrated in Fig. 1. DSTC begins with prompting the model to generate multiple code snippets and tests for each given instruction. Given self-generated code snippets and tests, DSTC comprises two key components: minimax selection and code-test concatenation. Intuitively, DSTC extracts more reliable preference pairs from self-generated code snippets and tests while mitigating the misleading effects of incorrect tests or code. Using the preference dataset constructed by DSTC, direct preference learning algorithms such as DPO and K\u03a4\u039f can be effectively employed to train code LMs.\nPreprocess: Generating code snippets and tests. (Lines 2-6 in Algorithm 1) In the preprocessing phase of DSTC, we prompt the model to generate J pairs of code snippets {y^j}_{j=1}^{J} and tests {z^j}_{j=1}^{J} for each instruction x\u2081 in the instruction set. Here, we adapt the prompt template\u00b9 from the oss-instruct-sd2-50k dataset (Wei et al., 2024a) to generate high-quality code and tests. We show the details of the prompt in Appendix B. To mitigate errors caused by mismatched entry points (e.g., function or class names) during the evaluation from a compiler, such as when assessing the correctness of a code snippet y^j\u00b9 with a test z^j where j1 \u2260 j2, we include entry points in the prompts. These entry points ensure the consistency"}, {"title": "Component I: Minimax selection", "content": "(Lines 7-15 in Algorithm 1) For each self-generated code snippet y^j and test z^k, we concatenate them in pairs and record their binary execution feedback r_{ijk} \u2208 {0,1} from a compiler. This feedback determines which code snippets and tests are chosen or rejected based on the following criteria (if an optimization problem is infeasible, the solution defaults to None):\n\u2022 Selection of the chosen code snippet. (Line 11 in Algorithm 1) For the chosen code, we wish to select the code snippet that is the most likely to be correct. By measuring the correctness via the number of passing tests, we formulate the selection of the chosen code snippet y^{j'} as the following optimization\n\\(j' = \\text{arg max}_j \\sum_{k=1}^{J} r_{ijk}.\\)\n\u2022 Selection of the chosen test. (Line 12 in Algorithm 1) For the chosen test, we wish to select the test that is the most difficult to pass that can be evaluated by the number of passing code snippets. We formulate the selection of the chosen test z^{k'} as the following constraint optimization\n\\(k' = \\text{arg min}_k \\sum_{j=1}^{J} r_{ijk} \\text{ s.t. } r_{ij'k} = 1,\\)\nwhere the constraint ensures that the chosen code snippet passes the chosen test.\n\u2022 Selection of the rejected test. (Line 13 in Algorithm 1) For the rejected test, we wish to select the test that is the easiest to pass. In contrast to (3.2), we formulate the selection of the rejected test z^{k^\\ddagger} as the following constraint optimization\n\\(k^\\ddagger = \\text{arg max}_k \\sum_{j=1}^{J} r_{ijk} \\text{ s.t. } \\sum_{j=1}^{J} r_{ijk} < J,\\)\nwhere the constraint ensures that at least one code snippet fails the rejected test.\n\u2022 Selection of the rejected code snippet. (Line 14 in Algorithm 1) For the rejected code snippet, we wish to select the code snippet that is the least likely to be correct. In contrast to (3.1), we formulate the selection of the rejected code snippet y^{j^\\ddagger} as the following constraint optimization\n\\(j^\\ddagger = \\text{arg min}_j \\sum_{k=1}^{J} r_{ijk} \\text{ s.t. } r_{ij^{k^\\ddagger}} = 0,\\)\nwhere the constraint ensures that the failed code snippet fails the rejected test."}, {"title": "Component II: Concatenation between the code snippet and test", "content": "(Lines 16-31 in Algorithm 1) After the phase of the minimax selection in Algorithm 1, we concatenate the chosen (resp. rejected) code snippet and test as the chosen (resp. rejected) response with the following prompt template for concatenation:\n\nThe idea of this concatenation is to form a preference pair with a reliable order brought by the execution feedback. Depending on which specific preference learning algorithm we use, we consider different manners to construct the preference dataset. For DPO, the preference dataset involves chosen-rejected pairs, hence we filter out the data without both feasible chosen and rejected responses generated via the minimax selection. For KTO, we only filter out the data without feasible chosen responses, as KTO preference dataset permits an imbalance between the number of chosen and rejected responses. With the preference dataset generated by DSTC, we incorporate the corresponding direct preference learning algorithm to train the code LMs."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to show the effectiveness of our proposed DSTC in improving LMs' code generation ability."}, {"title": "4.1 Experimental Setup", "content": "We implement DSTC on two both open-resource fine-tuned models StarCoder2-15b-Instruct2 and Deepseek-coder-33b-instruct\u00b3, respectively. We use the instructions from the oss-instruct-sd2-50k dataset to form the instruction set, where all the instructions are generated and filtered by Starcoder2-15b given seed functions. Specifically, StarCoder2-15b-Instruct is supervised fine-tuned (SFT) on the oss-instruct-sd2-50k dataset without further RLHF training. We use the implementation of DPO and KTO in the OpenRLHF codebase (Hu et al., 2024) as our training framework. The detailed training configurations are provided in Appendix A.\nBenchmarks. We evaluate the LLM coding generation ability by five benchmarks: HumanEval Base (Chen et al., 2021), HumanEval Plus (Liu et al., 2024a), Mostly Basic Python Problems (MBPP) Base (Austin et al., 2021), MBPP Plus (Liu et al., 2024a), and BigCodeBench (BCB) (Zhuo et al., 2024). In particular, BCB consists of two splits: instruct split and complete split, where instruct split only involves NLP instructions and complete split uses structured docstring in prompts. For each split, hard subset in BCB contains the most challenging and user-centric fraction of the full set. During the evaluation of all these benchmarks, we use greedy decoding in the code generation and all results are reported as the pass@1 score. Here, the pass@1 score measures the accuracy of code LMs on their initial attempt at generating the correct code."}, {"title": "4.2 DSTC Enhances Code LMs Across Diverse Benchmarks", "content": "The results in Table 2 demonstrate that DSTC consistently enhances the performance of code LMs, surpassing baseline models by a stable margin across all benchmarks when combined with either DPO or KTO. Notably, DSTC achieves improvements in the pass@1 score without requiring additional resources, such as canonical solutions or synthetic tests generated by ChatGPT. This highlights its potential for developing automated systems to further advance code LMs effectively."}, {"title": "4.3 Ablation Study", "content": "DSTC can improve code LM with a larger size. Evaluation results in Table 3 showcase the effectiveness of DSTC when applied to the DeepseekCoder-33b-instruct, a large-scale code language model with 33 billion parameters. These findings underscore the scalability of DSTC, demonstrating its ability to improve performance even in substantially larger models."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel mechanism named DSTC to incorporate direct preference learning methods such as DPO and KTO to enhance the code generation capabilities of LMs using only self-generated code snippets and tests to create preference pairs, circumventing the need for costly annotated datasets. By employing a minimax selection process and concatenated test-code pairs, DSTC mitigates the impact of low-quality self-generated tests, enabling stable performance gains across multiple coding benchmarks. Applied with direct preference learning methods like DPO and KTO, DSTC consistently improves pass@1 scores,"}, {"title": "A Detailed Training Configurations", "content": "We train all the models with 8 NVIDIA A100 GPUs and the OpenRLHF codebase (Hu et al., 2024). We report the hyperparameters for the training in Table 5."}, {"title": "B Prompts for Generating Code Snippets and Tests", "content": "In this section, we give details of the prompts used to generate code snippets and tests. We show an example of the full prompt as follows."}]}