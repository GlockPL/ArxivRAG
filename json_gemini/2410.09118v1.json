{"title": "FSW-GNN: A BI-LIPSCHITZ WL-EQUIVALENT GRAPH NEURAL NETWORK", "authors": ["Yonatan Sverdlov", "Yair Davidson", "Nadav Dym", "Tal Amir"], "abstract": "Many of the most popular graph neural networks fall into the category of message-passing neural networks (MPNNs). Famously, MPNNs' ability to distinguish between graphs is limited to graphs separable by the Weisfeiler-Lemann (WL) graph isomorphism test, and the strongest MPNNs, in terms of separation power, are WL-equivalent.\nRecently, it was shown that the quality of separation provided by standard WL-equivalent MPNN can be very low, resulting in WL-separable graphs being mapped to very similar, hardly distinguishable features. This paper addresses this issue by seeking bi-Lipschitz continuity guarantees for MPNNs. We demonstrate that, in contrast with standard summation-based MPNNs, which lack bi-Lipschitz properties, our proposed model provides a bi-Lipschitz graph embedding with respect to two standard graph metrics. Empirically, we show that our MPNN is competitive with standard MPNNs for several graph learning tasks and is far more accurate in over-squashing long-range tasks.", "sections": [{"title": "INTRODUCTION", "content": "Graph neural networks are a central research topic in contemporary machine learning research.\nMany of the most popular models, such as GIN (Xu et al., 2019), GraphSage (Hamilton et al., 2017), GAT (Velickovic et al., 2018), and GCN (Kipf & Welling, 2017), can be seen as an instantiation of Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017).\nA well-known limitation of MPNNs is that they cannot differentiate between all distinct pairs of graphs. In fact, a pair of distinct graphs that cannot be separated by the Weisfeiler-Leman (WL) graph isomorphism test will not be separated by any MPNN (Xu et al., 2019). Accordingly, the most expressive MPNNs are those that are WL-equivalent, which means they can separate all pairs of graphs that are separable by WL. WL-equivalent MPNNs were proposed in the seminal works of Xu et al. (2019); Morris et al. (2019), and the complexity of these constructions was later improved in (Aamand et al., 2022; Amir et al., 2023).\nWhile separation should theoretically be achieved for WL-equivalent MPNNs, in some cases, their separation is so weak that it cannot be observed with 32-bit floating-point computer numbers (see Bravo et al. (2024b)). This observation motivates the development of quantitative estimates of MPNN separation by means of bi-Lipschitz stability guarantees. These guarantees would ensure that Euclidean distances in the MPNN feature space are neither much larger nor much smaller thandistances in the original graph space, which are defined by a suitable graph metric (defined up to WL equivalence).\nSome first steps towards addressing these challenges have already been made by Davidson & Dym (2024). Their work analyzes a weaker notion of Lipschitz and Holder guarantees in expectation, and shows that essentially all popular MPNN models are not lower-Lipschitz, but they are lower-Holder in expectation, with an exponent that grows worse as the MPNN depth increases. In contrast, they propose SortMPNN, a novel MPNN which is bi-Lipschitz (in expectation).\nHowever, SortMPNN has several limitations. First, it is only bi-Lipschitz in expectation-a relaxed notion of Lipschitzness that guarantees smoothness only in expectation over the model parameters and for fixed pairs of graphs rather than uniformly on all input graphs. Additionally, their method addresses neighborhoods of different sizes by augmenting them to a predetermined maximum size. This approach has significant limitations: it is both computationally expensive, as the model cannot exploit graph sparsity, and it necessitates prior knowledge of the maximal graph size for the learning task at hand.\nIn this paper we introduce a novel MPNN called FSW-GNN (Fourier Sliced-Wasserstein GNN), which overcomes the limitations of SortMPNN. We show that this model is Bi-Lipschitz in the standard sense rather than in expectation, with respect to both the DS metric of Grohe (2020) and the Tree Mover's Distance (TMD) metric of Chuang & Jegelka (2022). Furthermore, this model can handle sparsity well and thus is much more efficient than SortMPNN for sparse graphs.\nEmpirically, we show that FSW-GNN has comparable or better performance than MPNN on 'standard' graph learning tasks, but achieves superior performance when considering long-range tasks, which require a large number of message-passing iterations. We hypothesize this is because the Holder exponent of standard MPNNs deteriorates with depth, whereas our FSW-GNN is bi-Lipschitz for any finite number of iterations. This hypothesis provides an alternative, and perhaps complementary, explanation to the difficulty of training deep standard MPNNs, commonly attributed to over smoothing and over squashing"}, {"title": "1.1 RELATED WORKS", "content": "MPNNs with advanced pooling mechanisms In addition to the SortMPNN model discussed earlier, our approach is conceptually related to other MPNNs that replace basic max-, mean-, or sum-pooling with more advanced pooling methods, such as sorting (Balan et al., 2022; Zhang et al., 2018), standard deviation (Corso et al., 2020), or Wasserstein embeddings via reference distributions (Kolouri et al.). However, these methods lack the bi-Lipschitzness guarantees that our model provides.\nBi-Lipschitzness Bi-Lipschitzness guarantees arise naturally in many domains, including frames (Balan, 1997), phase retrieval (Bandeira et al., 2014; Cheng et al., 2021), group invariant learning (Cahill et al., 2020; 2024b) and multisets (Amir et al., 2023; Amir & Dym, 2024). In the context of MPNNs, a recent survey by Morris et al. (2024) identifies bi-Lipschitzness guarantees as a significant future challenge for theoretical GNN research. While most MPNNs are upper Lipschitz, as discussed in (Chuang & Jegelka, 2022; Levie, 2023; Davidson & Dym, 2024), achieving bi-Lipschitzness remains an open problem.\nWL-equivalent metrics Metrics with the separation power of WL include the DS metric (Grohe, 2020) (also called the tree metric), the TMD metric (Chuang & Jegelka, 2022), and the WL metric (Chen et al., 2022). In this paper, we prove that the graph embeddings computed by our FSW-GNN model are bi-Lipschitz with respect to the DS and TMD metrics. This analysis is for graphs with bounded cardinality and continuous, bounded features. Weaker notions of equivalence between these metrics, in the setting of graphs with unbounded cardinality and without node features, are discussed in (B\u00f6ker et al., 2024; B\u00f6ker, 2021)."}, {"title": "2 PROBLEM SETTING", "content": "In this section, we outline the problem setting, first providing the theoretical background of the problem and then stating our objectives."}, {"title": "Vertex-featured graphs", "content": "Our main objects of study are graphs with vertex features, represented as triplets $G = (V, E, X)$, where $V = \\{v_i\\}_{i=1}^n$ is the set of vertices, $E \\subset \\{\\{v_i, v_j\\} | i,j \\in [n]\\}$ is the set of undirected edges in G, and $X = [x_1,..., x_n]$ is a matrix containing the vertex feature vectors $x_i \\in \\Omega$, where the feature domain $\\Omega$ is a subset of $\\mathbb{R}^d$. We denote by $\\mathcal{G}_{\\leq N}(\\Omega)$ the set of all vertex-featured graphs with at most N vertices and corresponding features in $\\Omega$. Throughout the paper, we use $\\{\\}$ to denote multisets."}, {"title": "Weisfeiler-Lemann Graph Isomorphism test", "content": "Two graphs are isomorphic if they are identical up to relabeling of their nodes. Perhaps surprisingly, the problem of determining whether two given graphs are isomorphic is rather challenging. To date, no known algorithm can solve it in polynomial time (Babai, 2016). However, there exist various heuristics that provide an incomplete but often adequate method to test whether a given pair of graphs is isomorphic. The most notable example is the Weisfeiler-Leman (WL) graph isomorphism test.\nThe WL test can be described as assigning to each graph $G = (V, E, X)$ a feature vector $c^T$ according to the following recursive formula.\n$c^0_v := X_v, \\quad v \\in V$,\n$c^t_v := \\text{Combine}(c^{t-1}_v, \\text{Aggregate}(\\{c^{t-1}_u \\mid u \\in N_v\\})), \\quad 1 \\leq t \\leq T$,\n$c^T := \\text{Readout}(\\{c^T_v, \\dots, c^T_n\\})$,\nwhere Aggregate and Readout are functions that injectively map multisets of vectors in Euclidean space into another Euclidean space, Combine is an injective function from one Euclidean space to another, and $N_v$ denotes the neighborhood of the vertex $v$ in G."}, {"title": "Definition (WL graph equivalence).", "content": "Two vertex-featured graphs G and \u011e are said to be WL-equivalent, denoted by $G \\stackrel{\\text{WL}}{\\equiv} G'$ , if $c^T = \\bar{c}^T$ for all $T \\geq 0$. Otherwise, they are said to be WL-separable or WL-distinguishable.\nIt is a known fact (Grohe, 2021; Morris et al., 2023) that for $G,\\bar{G} \\in \\mathcal{G}_{\\leq n}(\\mathbb{R}^d)$, if the equality $c^N = \\bar{c}^N$ is satisfied for $T = N$, then it is satisfied for all $T > 0$, and thus $G \\stackrel{\\text{WL}}{\\equiv}G'$ .\nWhile the WL test can distinguish most pairs of non-isomorphic graphs, there exist examples of non-isomorphic graph pairs that WL cannot separate; see (Zopf, 2022). Note that there exist higher-order versions of this test called k-WL, k \u2265 2, but in this paper, we consider only the 1-WL test, denoted by WL for brevity."}, {"title": "Message passing neural networks", "content": "Message Passing Neural Networks (MPNNs) are the most common neural architectures designed to compute graph functions. They operate on a similar principle to the WL test, but with the purpose of performing predictions on graphs rather than determining if they are isomorphic. Their core mechanism is the message-passing procedure, which maintains a hidden feature for each vertex and iteratively updates it as a function of the neighbors' features. This process is outlined as follows:\n1. Initialization: The hidden feature $h^0_v$ of each node is initialized by its input feature $x_v$.\n2. Message aggregation: Each node $v \\in V$ aggregates messages from its neighbors by\n$m^{(t)}_v := \\text{Aggregate}(\\{h^{(t-1)}_u \\mid u \\in N_v\\})$\nWhere Aggregate is a multiset-to-vector function.\n3. Update step: Each node updates its own hidden feature according to its aggregated messages and its previous hidden feature, using a vector-to-vector update function:\n$h^{(t)}_v := \\text{Update} (m^{(t)}_v, h^{(t-1)}_v)$,\n4. Readout: After T iterations of steps 2-3, a global graph-level feature $h_G$ is computed from the multiset of hidden features $\\{h^{(T)}_v \\mid v \\in V\\}$ by a readout function:\n$h_G := \\text{Readout} (\\{h^{(T)}_v \\mid v \\in V\\}).$\nNumerous MPNNs were proposed in recent years, including GIN (Xu et al., 2019), GraphSage (Hamilton et al., 2017), GAT (Velickovic et al., 2018), and GCN (Kipf & Welling, 2017), the main differences between them being the specific choices of the aggregation, update, and readout functions. An MPNN computes an embedding $F(G) = h_G$, which maps the graphs in $\\mathcal{G}_{\\leq N}(\\Omega)$ to vectors in $\\mathbb{R}^m$. The obtained embedding is often further processed by standard machine-learning tools for vectors, such as multi-layer perceptrons (MLPs), to obtain a final graph prediction. The ability of such a model to approximate functions on graphs is closely related to the separation properties of F. If F can differentiate between any pair of non-isomorphic graphs, then a model of the form MLP \u25e6 F would be able to approximate any functions on graphs Chen et al. (2019).\nUnfortunately, MPNN cannot separate any pair of WL-equivalent graphs, even if they are not truly isomorphic Xu et al. (2019); Morris et al. (2019). Accordingly, the best we can hope for from an MPNN, in terms of separation, is WL equivalence: for every pair of graphs $G, G' \\in \\mathcal{G}_{\\leq N}(\\Omega)$, $F(G) = F(G')$ if and only if $G \\stackrel{\\text{WL}}{\\equiv}G'$. While MPNNs based on max- or mean-pooling cannot be WL-equivalent Xu et al. (2019), it is possible to construct WL-equivalent MPNNs based on sum-pooling, as discussed in Xu et al. (2019); Morris et al. (2019); Aamand et al. (2022); Amir et al. (2023); Bravo et al. (2024a). Theoretically, a properly tuned graph model based on a WL-equivalent MPNN should be capable of perfectly solving any binary classification task, provided that no two WL-equivalent graphs have different ground-truth labels. However, this separation does not always manifest in practice. One reason is that WL-equivalent functions may map two input graphs far apart in the input space to outputs that are numerically indistinguishable in the output Euclidean space. In fact, Davidson & Dym (2024) provides an example of graph pairs that are not WL-equivalent yet are mapped to near-identical outputs by standard sum-based MPNNs. Consequently, these MPNNS fail on binary classification tasks for such graphs.\nThis paper aims to address this limitation by devising an MPNN whose embeddings preserve distances in the original graph space in the bi-Lipschitz sense. To state our goal formally, we need to define appropriate notions of WL metrics on the input space and bi-Lipschitz graph embeddings."}, {"title": "WL-metric for graphs", "content": "WL-metrics quantify the extent to which two graphs are not WL-equivalent:\nDefinition (WL-metric). A WL-metric on $\\mathcal{G}_{\\leq N}(\\Omega)$ is a function $\\rho : \\mathcal{G}_{\\leq N}(\\Omega) \\times \\mathcal{G}_{\\leq N}(\\Omega) \\rightarrow \\mathbb{R}_{\\geq 0}$ that satisfies the following conditions for all $G_1, G_2, G_3 \\in \\mathcal{G}_{\\leq N}(\\Omega)$:\n$\\rho(G_1, G_2) = \\rho(G_2, G_1)$ \\text{Symmetry}\n$\\rho(G_1, G_3) \\leq \\rho(G_1, G_2) + \\rho(G_2, G_3)$ \\text{Triangle inequality}\n$\\rho(G_1, G_2) = 0 \\Leftrightarrow G_1 \\stackrel{\\text{WL}}{\\equiv} G_2$. \\text{WL equivalence}\nNote that strictly speaking, such $\\rho$ is a pseudo-metric on $\\mathcal{G}_{\\leq N}(\\mathbb{R}^d)$ rather than a metric, as it allows distinct graphs to have a distance of zero if they are WL-equivalent. However, we will use the term metric to denote a pseu-dometric for convenience.\nIn this paper, we consider two WL metrics: The first is the DS metric, proposed in (Grohe, 2021). Originally, this metric was defined only for featureless graphs of the same cardinality. In the next section, we will discuss extending it to the more general case of $\\mathcal{G}_{\\leq N}(\\mathbb{R}^d)$. The second WL metric we consider is the Tree Mover's distance (TMD). This metric was proposed and shown to be a WL-metric by Chuang & Jegelka (2022)."}, {"title": "Bi-Lipschitzness", "content": "Once a WL-metric is defined to measure distances between graphs, one can bound the distortion incurred by a graph embedding with respect to that metric, using the notion of bi-Lipschitzness:\nDefinition (Bi-Lipschitz embedding). Let $\\rho$ be a WL-metric on $\\mathcal{G}_{\\leq N}(\\Omega)$. An embedding $E: \\mathcal{G}_{\\leq N}(\\Omega) \\rightarrow \\mathbb{R}^m$ is said to be bi-Lipschitz with respect to $\\rho$ on $\\mathcal{G}_{\\leq N}(\\Omega)$ if there exist constants $0 < c < C < \\infty$ such that\n$c \\cdot \\rho(G_1, G_2) \\leq ||E(G_1) - E(G_2)||_2 \\leq C \\cdot \\rho(G_1, G_2), \\quad \\forall G_1, G_2 \\in \\mathcal{G}_{\\leq N}(\\Omega).$\nIf E satisfies just the left- or right-hand side of (5), it is said to be lower-Lipschitz or upper-Lipschitz, respectively.\nBi-Lipschitzness ensures that the embedding maps the original space $\\mathcal{G}_{\\leq N}(\\Omega)$ into the output Euclidean space with bounded distortion, with the ratio $\\frac{C}{c}$ acting as an upper bound on the distortion, akin to the condition number of a matrix. This enables the application of metric-based learning methods, such as clustering and nearest-neighbor search, to non-Euclidean input data. This is discussed, for example, in (Cahill et al., 2024a)."}, {"title": "Lipschitzness, Holder, and depth", "content": "In the context of graph neural networks, we conjecture that the advantage of bi-Lipschitz MPNNs over standard sum-based MPNN will be more apparent for 'deep' MPNNs, where the number T of message-passing iterations is large. Our reasoning for this conjecture can be explained via the related notion of lower-Holder MPNN.\nDefinition. A graph embedding is lower-Holder in expectation with constants $c > 0, \\alpha > 1$, if\n$c \\cdot \\rho(G_1, G_2)^\\alpha \\leq ||E(G_1) - E(G_2)||_2, \\quad \\forall G_1, G_2 \\in \\mathcal{G}_{\\leq N}(\\Omega)$.\nIn general, the larger \u03b1 is, the worse the distortion. The best case of \u03b1 = 1 coincides with lower-Lipschitzness. Davidson & Dym (2024) showed that standard sum-based MPNNs are lower-Holder, with an exponent \u03b1 the becomes worse as the number T of message-passing iterations increases. This means that the worst-case distance between sum-based graph embeddings can go to zero super-exponentially with T. In contrast, our bi-Lipschitz MPNN will remain bi-Lipschitz for any finite T (although the distortion C/c may depend on T), and hence will be more robust to increasing the number of message-passing iterations.\nThe challenge of training deep MPNNs is one of the core problems in graph neural networks (Morris et al., 2024). The difficulty in doing so is often attributed to oversmoothing (Rusch et al., 2023) or oversquashing (Alon & Yahav, 2020). Based on our results, we conjecture that distortion of the graph metric may be the root of this problem, and that, as a result, bi-Lipschitz MPNNs are a promising solution. We provide empirical evidence for this conjecture in Section 4, where we show that our bi-Lipschitz MPNN is far superior to standard MPNNs on long range tasks which require training a deep MPNN."}, {"title": "3 MAIN CONTRIBUTIONS", "content": "In this section, we discuss our main contributions. We begin by defining our generalized DS metric. We will then discuss our MPNN and FSW-GNN and show that it is bi-Lipschitz with respect to both DS and TMD."}, {"title": "The DS metric", "content": "The roots of the DS metric come from a relaxation of the graph isomorphism problem. Two graphs G and \u011e, each with n vertices, and corresponding adjacency matrices A A are isomorphic if and only if there exists a permutation matrix P such that AP = P\u00c3. Since checking whether graphs are isomorphic is intractable, an approximate solution can be sought by considering the equation AS = S\u00c3, where S is a matrix in the convex hull of the permutation matrices: the set of doubly stochastic matrices, denoted by $\\mathcal{D}_n$. These are n \u00d7 n matrices with non-negative entries whose rows and columns all sum to one. Remarkably, this equation admits a doubly stochastic solution if and only if the graphs are WL-equivalent (Scheinerman & Ullman, 2013). Accordingly, a WL-metric can be defined by the minimization problem.\n$\\rho_{\\text{DS}}(G,\\bar{G}) = \\min_S || AS - S\\bar{A}||_2$\nwhere $|| \\cdot ||_2$ denoted the entry-wise $l_2$ norm for matrices. The optimization problem in (7) can be solved by off-the-shelf convex optimization solvers and was considered as a method for finding the correspondence between two graphs in many papers, including Aflalo et al. (2015); Lyzinski et al. (2016); Dym (2018); Dym et al. (2017); Bernard et al. (2018).\nThe idea of using the DS metric for MPNN stability analysis was introduced in (Grohe, 2020) and further discussed by B\u00f6ker (2021). To apply this idea to our setting, we need to adapt this metric to vertex-featured graphs with varying numbers of vertices. We do this by augmenting it as follows:\n$\\rho_{\\text{DS}}(G,\\bar{G}) = |n - \\bar{n}| + \\min_{S \\in \\Pi(n,\\bar{n})} ||AS - S\\bar{A}|| + \\sum_{i \\in [n],j \\in [\\bar{n}]} S_{ij}|X_i - \\bar{X}_j||$"}, {"title": "Theorem 3.1.", "content": "[Proof in Appendix B.1] Let $\u03c1_{DS} : \\mathcal{G}_{\\leq N}(\\mathbb{R}^d) \u00d7 \\mathcal{G}_{\\leq N}(\\mathbb{R}^d) \u2192 \\mathbb{R}_{>0}$ be as in (8). Then $\u03c1_{DS}$ is a WL-equivalent metric on $\\mathcal{G}_{\\leq N}(\\mathbb{R}^d)$."}, {"title": "Bi-Lipschitz MPNN", "content": "We now present our main contribution: a novel MPNN that is not only WL-equivalent but also bi-Lipschitz, both with respect to the metric $\u03c1_{DS}$ and TMD.\nThe core innovation in our MPNN lies in its message aggregation method. To aggregate messages, we use the Fourier Sliced-Wasserstein (FSW) embedding-a method for embedding multisets into Euclidean space, proposed by Amir & Dym (2024), where it was shown to be bi-Lipschitz. Consequently, it seems plausible a priori that an MPNN based on FSW aggregations will be bi-Lipschitz for graphs. In the following, we prove that this is indeed the case. We begin by describing the FSW embedding and then introduce our FSW-GNN architecture.\nThe FSW embedding maps input multisets $\\{x_1,..., x_n\\}$, with $x_1,..., x_n \\in \\mathbb{R}^d$, to output vectors $z = (z_1,..., z_m) \\in \\mathbb{R}^m$. In addition to the input, it depends on parameters $v_i \\in S^{d-1}$ and $\\xi_i \\in \\mathbb{R}$, representing projection vectors and frequencies; see (Amir & Dym, 2024) for details. It is denoted by\n$E_{FSW}(\\{x_1,...,x_n\\}; \\{(v_i, \\xi_i)\\}_{i=1}^I) = z$.\nThe i-th coordinate of the output z is a scalar $z_i$, defined by the formula\n$\\begin{aligned}\ny_i &= \\text{sort}(v_i \\cdot x_1,..., v_i \\cdot x_n) \\\\\nQ_{y_i}(t) &= \\sum_{j=1}^n y_{i,j} \\mathbb{I}_{[j/n,1)}(t)\\\\\nz_i &= 2(1 + \\xi) \\int_0^1 Q_{y_i}(t) \\cos(2\\pi \\xi t)dt\n\\end{aligned}$\nwhere in this formula $x \\cdot y$ denotes the standard inner product of x and y, the function $\\mathbb{I}_{[a,b)}$ is the indicator function of the interval [a, b), and $y_{i,j}$ is the j-th entry of the vector $y_i$.\nThe FSW embedding is essentially computed in three steps: first, a direction vector $v_i$ is used to project each d dimensional vector to a scalar. We thus obtain a multiset of scalars which can then be sorted. This first step is the sort-type embedding used in SortMPNN, and it can be shown to be bi-Lipschitz on multisets of fixed cardinality Balan et al. (2022). However, the disadvantage of taking $y_i$ as the embedding is that it has the same cardinality as the multiset, and so this embedding is not readily applicable to multisets of different cardinalities. The next two steps of the FSW embedding can be seen as an attempt to fix this disadvantage.\nIn the second step, the vector $y_i$ is identified with a step function $Q_{y_i}$ (the quantile function, see interpretation in (Amir & Dym, 2024)). Then, in the third step, the cosine transform, a variant of the Fourier transform, is applied to $Q_{y_i}$ at the given frequency $\\xi_i$, to obtain the final value $z_i$. Note that the integral in equation 11 has a closed form solution, and the whole procedure can be computed with complexity linear in n, d. Moreover, the dimension of the parameters and output of the embedding does not depend on n, and thus this embedding is suitable for multisets of varying sizes.\nFSW-GNN The FSW-GNN model processes input graphs $G = (V, E, X)$ by T message-passing iterations according to the following recursive formula:\n$\\begin{aligned}\nh^{(0)}_v &:= x_v, \\\\\nq^{(t)}_v &:= E_{FSW}(\\{h^{(t-1)}_u \\mid u \\in N_v\\}), \\quad 1 \\leq t \\leq T \\\\\nh^{(t)}_v &:= \\Phi^{(t)}([h^{(t-1)}_v; q^{(t)}_v]),\n\\end{aligned}$\nwhere the functions $E_{FSW}$ are all instances of the FSW embedding, $\\Phi^{(t)}$ are MLPs, and $[x; y]$ denotes column-wise concatenation of column vectors x and y. Finally, a graph-level output is computed by:"}, {"title": "Formula", "content": "$\nh_G := E_{Glob}(\\{h^{(T)}_v \\mid v \\in V\\}),$\nwhere, again, $E_{Glob}$ is an FSW embedding, and \u03a8 is an MLP.\nThe following theorem shows that with the appropriate choice of MLP sizes and number of iterations T, our proposed architecture is WL equivalent:\nTheorem 3.2 (Informal). [Proof in Appendix B.2] Consider the FSW-GNN architecture for input graphs in $\\mathcal{G}_{\\leq N}(\\mathbb{R}^d)$, with $T = N$ iterations, where $\\Phi^{(t)}, \\Psi$ are just linear funtions, and all features (except for input features) are of dimension $m > 2Nd + 2$. Then for Lebesgue almost every choice of model parameters, the graph embedding defined by the architecture is WL equivalent."}, {"title": "From separation to bi-Lipschitzness", "content": "In general, WL-equivalence does not imply bi-Lipschitzness. As mentioned above, sum-based MPNN can be injective but are never bi-Lipschitz. In contrast, we will prove that for FSW-GNN, WL-equivalence does imply bi-Lipschitz-ness, under the additional assumption that the feature domain \u03a9 is compact:\nTheorem 3.3. [Proof in Appendix B.3] Let $\\Omega \\subset \\mathbb{R}^d$ be compact. Under the assumptions of Theorem 3.2, the FSW-GNN is bi-Lipschitz with respect to $\u03c1_{DS}$ on $\\mathcal{G}_{\\leq N}(\\Omega)$. If, additionally, \u03a9 is a compact polygon that does not contain 0, then the FSW-GNN is bi-Lipschitz with respect to TMD on$\\mathcal{G}_{\\leq N}(\\Omega)$.\nWe now give a high-level explanation of the proof idea. The full proof is in the appendix. To prove Theorem 3.3, we rely on the following facts: (1) the output of FSW-GNN for an input graph $G = (V, E, X)$ is piecewise-linear with respect to the vertex-feature matrix X. This follows from properties of the FSW embedding functions used in (12) and (13). (2) both metrics $\u03c1_{DS}$ and TMD can be transformed, with bounded distortion, into piecewise-linear metrics by choosing all the vector norms they employ to be the $l_1$ norm. The claim them follows from these observations and the following lemma:\nLemma 3.4. [Proof in Appendix B.3] Let $f,g: M \\rightarrow \\mathbb{R}_{\\geq 0}$ be non-negative piece-wise-linear functions defined on a compact polygon $M \\subset \\mathbb{R}^d$. Suppose that for all $x \\in M$, $f(x) = 0$ if and only if $g(x) = 0$. Then there exist real constants $c, C > 0$ such that\n$c \\cdot g(x) \\leq f(x) \\leq C \\cdot g(x), \\quad \\forall x \\in M.$"}, {"title": "4 NUMERICAL EXPERIMENTS", "content": "We compare the performance of FSW-GNN with standard MPNN on both real world benchmarks and synthetic long range tasks. While our main baseline is MPNN models", "networks": "Geisler et al.", "graph transfer": "asks from Di Giovanni et al. (2023). In this problem", "source": "ode is propagated to a 'target' node", "blank": "ode features.\nWe consider this problem for the three different graph topologies suggested by Di Giovanni et al. (2023): clique, ring, and crossring, and with a problem radius r varying from 2 to 15.\nAs shown in Figure 2, FSW-GNN is the only method attaining 100% accuracy across"}]}