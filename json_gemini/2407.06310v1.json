{"title": "Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly Speaker Adaptation", "authors": ["Mengzhe Geng", "Xurong Xie", "Jiajun Deng", "Zengrui Jin", "Guinan Li", "Tianzi Wang", "Shujie Hu", "Zhaoqing Li", "Helen Meng", "Xunying Liu"], "abstract": "The application of data-intensive automatic speech recognition (ASR) technologies to dysarthric and elderly adult speech is confronted by their mismatch against healthy and non-aged voices, data scarcity and large speaker-level variability. To this end, this paper proposes two novel data-efficient methods to learn homogeneous dysarthric and elderly speaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN and Conformer ASR models. These include: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that exploit a special regularization term to enforce homogeneity of speaker features in adaptation; and 2) feature-based learning hidden unit contributions (f-LHUC) transforms that are conditioned on VR-SBE features. Experiments are conducted on four tasks across two languages: the English UASpeech and TORGO dysarthric speech datasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora. The proposed on-the-fly speaker adaptation techniques consistently outperform baseline iVector and xVector adaptation by statistically significant word or character error rate reductions up to 5.32% absolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24% absolute (9.20% relative), while operating with real-time factors speeding up to 33.6 times against xVectors during adaptation. The efficacy of the proposed adaptation techniques is demonstrated in a comparison against current ASR technologies including SSL pre-trained systems on UASpeech, where our best system produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features and f-LHUC transforms are insensitive to speaker-level data quantity in test-time adaptation. T-SNE visualization reveals they have stronger speaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC transforms.", "sections": [{"title": "I. INTRODUCTION", "content": "In spite of the remarkable advancement on automatic speech recognition (ASR) techniques for normal speech, accurate recognition of disordered speech, for example, voice recorded from speakers with dysarthria, remains a highly challenging task to date [1]\u2013[11]. Millions of people world-wide are clinically diagnosed with speech disorders [12]. In addition, neurocognitive disorders, such as Alzheimer's disease (AD), are often found among older adults and manifest themselves in speech impairments [13], [14]. Given that large-scale pathological assessment of speech disorders among all older adults is practically difficult due to the limited availability of professional speech pathologists, the actual number of people affected by speech disorders is much larger. As aging presents enormous challenges to health care worldwide, there is a pressing need to develop suitable ASR-based assistive technologies customized for dysarthric and elderly speakers to improve their life quality and social inclusion [15]\u2013[44].\nDysarthric and elderly speech bring challenges on three fronts to current ASR technologies predominantly targeting normal speech recorded from healthy, non-aged users. These challenges include: a) substantial mismatch against normal speech due to the underlying neuro-motor control conditions [45], [46] and aging, for example, imprecise articulation, hoarse voice and increased disfluency [47], [48]; b) data scarcity due to the difficulty in collecting large quantities of such data from dysarthric and elderly speakers with mobility limitations; and c) large speaker-level diversity among dysarthric and elderly talkers, when sources of variability commonly found in normal speech, e.g. accent or gender, are further aggregated with speech pathology severity and aging.\nA key task for all ASR systems is to model the speech variability attributed to speaker-level characteristics. To this end, speaker adaptation techniques provide a key role in customizing ASR systems for users' needs. For normal speech recognition tasks, three main categories of such techniques have been studied: 1) auxiliary embedding features that are speaker-dependent (SD) [49]\u2013[51]; 2) feature transformations that produce speaker-independent (SI) features to remove such variability at the front-ends [52]\u2013[54]; and 3) model-based adaptation using specifically designed SD DNN parameters [55]\u2013[58]. A detailed review of speaker adaptation techniques targeting normal speech is presented in Sec. II.\nModeling the large speaker-level heterogeneity in dysarthric and elderly speech requires powerful speaker adaptation techniques to be developed. However, only limited prior research in this direction has been conducted. Earlier works focused on deriving speaker adaptation methods for traditional HMM-based ASR systems with Gaussian mixture model (GMM) based hidden state densities, predominantly targeting dysarthric speech only. A combination of HMM state transition interpolation and maximum a posteriori (MAP) adaptation is utilized to account for dysarthric speaker diversity in [59]. The use of maximum likelihood linear regression (MLLR) and MAP adaptation was explored in [1], [60]\u2013[62]. Combined use of MLLR with MAP adaptation in speaker adaptative training (SAT) of GMM-HMM models was investigated in [2]. Feature-space MLLR (f-MLLR) based SAT training [63] and regularized speaker adaptation to dysarthric speakers using Kullback-Leibler (KL) divergence [64] were also developed for GMM-HMM systems. MLLR adaptation of GMM-HMM models to elderly speakers using cross-speaker statistics pooling was studied in [60].\nMore recent dysarthric and elderly speaker adaptation approaches applied to state-of-the-art hybrid and end-to-end (E2E) neural network based ASR systems include the following categories: 1) direct speaker-level parameter fine-tuning of hybrid LF-MMI trained TDNN [27], [28] and RNN-T [65] models; 2) the use of iVector speaker adaptation for dysarthric [66], [67] and elderly [34] speech recognition; 3) model-based speaker adaptation using, for example, learning hidden unit contributions (LHUC) for dysarthric [24], [29], [30], [38] and elderly [34], [38] speakers; 4) Bayesian domain [68] and speaker adaptation [29] methods that are more robust to dysarthric and elderly speech data scarcity; 5) dysarthric and elderly speaker-level averaged spectro-temporal basis embedding features [36], [38] for hybrid DNN/TDNN and E2E Conformer adaptation; 6) f-MLLR and xVector based dysarthric speaker adaptation of self-supervised learning (SSL) based Wav2vec 2.0 models [5]; and 7) dysarthric speaker adapter fusion of pre-trained E2E Transformer models [42].\nSuitable dysarthric and elderly speaker adaptation methods that can meet with above-mentioned challenges should satisfy the following requirements: 1) high data efficiency to model the very limited speaker-level data; 2) strong speaker homogeneity to ensure the distinct speaker-level characteristics are consistently represented in the adapted model; and 3) low processing latency to allow adaptation to be performed immediately on-the-fly from the onset of a new user's enrolment in the system to minimize their efforts and fatigue. In this regard, prior research only addressed some of the above. For example, the Bayesian model based adaptation using very limited speaker data [29], [34], [68] only addresses the aforementioned data scarcity issue, but the latency problem remains unvisited. Similarly, the spectro-temporal deep embedding features [36], [38] are averaged over all speaker-level data in an offline manner. This introduces considerable processing latency and is thus unsuitable for on-the-fly test-time adaptation.\nTo this end, one possible solution is to derive suitable rapid, on-the-fly feature-based dysarthric and elderly adaptation techniques. Such methods serve as multi-purpose solutions to handle not only the speaker-level data scarcity and diversity when representing speaker attributes, but also the processing latency incurred by model-based fine-tuning or adaptation to speaker data. In this paper, two novel forms of feature-based on-the-fly rapid speaker adaptation approaches are proposed. The first is based on speaker-level variance-regularized spectral basis embedding (VR-SBE) features. An extra variance regularization term is included when training spectral basis embedding neural networks [36], [38] to ensure speaker homogeneity of the embedding features. This in turn allows them to be applied on-the-fly during test-time adaptation to dysarthric or elderly speakers. The second approach utilizes on-the-fly feature-based LHUC (f-LHUC) transforms conditioned on VR-SBE features. Specially designed regression TDNN [69] predicting dysarthric or elderly speaker-level LHUC transforms is constructed to directly generate and apply such SD parameters on the fly during test-time adaptation.\nExperiments are conducted on four different tasks: 1) the English UASpeech [70] and TORGO [71] dysarthric speech datasets; 2) the English DementiaBank Pitt [72] and the Cantonese JCCOCC MoCA [73] elderly speech corpora. Among these, UASpeech is by far the largest available and most extensively used dysarthric speech corpus, while DementiaBank Pitt is the largest publicly available elderly speech database. The performance of the proposed two feature-based on-the-fly speaker adaptation approaches: VR-SBE and VR-SBE feature conditioned f-LHUC, are compared against those of baseline iVector [50] or xVector [74] based on-the-fly adaptation and LHUC [57] based model adaptation on three fronts:\n1) ASR performance in WER/CER: Our proposed on-the-fly speaker adapted hybrid DNN/TDNN and E2E Conformer systems consistently outperform the corresponding on-the-fly iVector/xVector adapted systems, with statistically significant\u00b9 word/character error rate (WER/CER) reductions up to 5.32% absolute (18.57% relative). Our on-the-fly adaptation methods also outperform the comparable offline batch-mode model-based LHUC adaptation by statistically significant WER/CER reductions up to 2.24% absolute (9.20% relative).\n2) Analysis on processing latency: Experiments on the benchmark UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest that statistically significant WER/CER reductions are consistently obtained over the corresponding on-the-fly iVector/xVector adapted systems, while the proposed VR-SBE speaker adaptation operates with a real-time factor speeding up ratio up to 33.6 times against xVector. It incurs a minimal processing latency by using an input acoustic feature window as short as 10 ms in duration.\n3) Analysis on speaker feature homogeneity: T-SNE visualization [76] of baseline iVectors/xVectors, offline model-based LHUC transforms, offline SBE [38] features, our proposed on-the-fly VR-SBE features and the associated f-LHUC transforms across varying amounts of adaptation data is conducted. This suggests more consistent, data quantity invariant dysarthric and elderly speaker characteristics can be learned via the proposed on-the-fly speaker adaptation approaches.\nThe main contributions of this paper are summarized below:\n1) This paper presents novel approaches to learn homogeneous speaker features tailored for rapid, on-the-fly dysarthric and elderly speaker adaptation. In contrast, prior studies either considered adaptation techniques using all speaker-level data and operating in batch-mode, offline manner [24], [68], [30], [29], [34], [38], [42], or used existing iVector/xVector features not tailored for dysarthric/elderly speech and produced mixed results on such data [66], [34], [5], [67]. In particular, model-based adaptation methods not only use all speaker-level data, but also introduce multiple decoding passes and explicit parameter estimation or fine-tuning stages during test-time adaptation [68], [30], [29], [34], [42].", "subsections": []}, {"title": "II. REVIEW OF SPEAKER ADAPTATION", "content": "This section reviews three major categories of speaker adaptation techniques traditionally designed for normal non-aged, healthy speakers, respectively based on: a) auxiliary features, b) feature transformations, and c) model-based adaptation.\nAmong these, auxiliary speaker embedding features encode speaker-dependent (SD) characteristics via compact representations. The resulting SD features are used as auxiliary inputs to facilitate speaker adaptation during the training and evaluation of ASR systems. Such SD features can be either estimated independently of the back-end systems, e.g. using universal background models (UBMs) based on Gaussian mixture models (GMMs) to learn iVectors [50], [51], or jointly learned with the back-end systems, e.g. alternate updating speaker representations and the remaining parameters when learning speaker codes [49]. Auxiliary speaker features can be flexibly incorporated into both GMM-HMM or hybrid systems and more recent end-to-end E2E systems [77], [78], [5].\nSpeaker adaptative feature transformations are applied to acoustic front-ends to produce speaker-invariant canonical input features, for example, feature-space maximum likelihood linear regression (f-MLLR) [52] estimated at speaker-level from GMM-HMM systems. Speaker-level physiological differences motivated adaptation based on vocal tract length normalization (VTLN) [79], [80] can be further adopted by applying piecewise linear frequency warping factors [80].\nModel-based speaker adaptation techniques apply compact forms of specially designed SD parameters in different network layers, for example, linear input networks (LIN) [81], [56], linear hidden networks (LHN) [55], learning hidden unit contributions (LHUC) [57], [82], linear output networks (LON) [55], parameterized activation functions (PAct) [58], factorized linear transformation [83], and SD neural beamforming, encoder, attention or decoder modules in E2E multi-channel systems [84]. Speaker adaptive training (SAT) [85], [86] can be further applied during training to enable a joint optimization of both SD and SI parameters and to produce a canonical model that can be better adapted to unseen speakers during test-time adaptation. To alleviate the risk of overfitting to limited speaker data during model adaptation, a series of regularized speaker adaptation strategies have been developed. These include the use of L2 [87], [88] or Kull-back-Leibler (KL) divergence regularization [89]\u2013[92], and maximum a posterior (MAP) [93], [94] or Bayesian inspired adaptation [82]. Data augmentation on the target speaker via text-to-speech (TTS) synthesis has also been explored in [95], [96]. Alternative objective functions have been investigated for speaker adaptation, for example, adversarial learning [97], [98], [91] and multitask learning [99]\u2013[101], [90], [91].", "subsections": []}, {"title": "III. ON-THE-FLY SPEAKER ADAPTATION VIA VARIANCE REGULARIZED SPECTRAL BASIS EMBEDDING FEATURES", "content": "On-the-fly feature-based speaker adaptation techniques provide practical solutions to handle both speaker-level data scarcity and processing latency incurred by model-based fine-tuning or adaptation to user data. A key task in designing such techniques is to ensure the homogeneity of speaker-level features and consistent representation of speaker characteristics. To this end, the proposed variance-regularized spectral basis embedding (VR-SBE) features are extracted in three phases (Fig. 1), including: a) conducting singular value decomposition (SVD) on utterance-level dysarthric/elderly speech spectra to produce initial time-invariant spectral bases (Fig. 1 left, in yellow); b) feeding the spectral bases through a first embedding module trained using speaker ID and speech intelligibility/age information, which extracts latent embeddings more consistent and relevant to dysarthric/elderly speaker attributes (Fig. 1 top right, in light blue); and c) feeding the spectral bases through a second embedding module, with an additional output variance regularization cost using the averaged speaker embeddings obtained in stage b) as targets (Fig. 1 bottom right, in pink). Such a process ensures the maximum speaker homogeneity of the final VR-SBE features for on-the-fly adaptation.", "subsections": []}, {"title": "A. Speech Spectrum Subspace Decomposition", "content": "SVD-based spectrum decomposition [36], [38] provides an intuitive approach to decouple the latent time-invariant spectral and time-variant temporal features in speech signals. In phase-1 of VR-SBE feature extraction (Fig. 1 left, in yellow), SVD is applied to the mel-filterbank log amplitude spectrum $O_{C \\times T_r}$ of utterance r with Tr frames and C filterbank channels:\n$O_{C \\times T_r} = U \\Sigma V^T$ (1)\nwhere the set of column vectors of the $C \\times C$ dimensional U matrix (the left-singular vectors) and the set of row vectors of the $T_r \\times T_r$ dimensional $V^T$ matrix (the right-singular", "subsections": []}, {"title": "B. Spectral Basis Deep Embedding", "content": "The above spectral bases are obtained in an unsupervised manner during SVD. To further extract latent information more consistent and relevant to speech impairment severity or age information, supervised feature learning is performed in phase-2 (Fig. 1 top right, in light blue) via developing a 4-block speech intelligibility or age embedding module with the selected top d spectral bases as inputs. The first three blocks each contain 2000 hidden nodes, while the fourth has a 25-dim bottleneck for embedding extraction. Each block comprises the following internal structures in sequence: affine transformation (in green), rectified linear unit (ReLU) activation (in yellow), and batch normalization (in orange). Additionally, we apply linear bottleneck projection (in light green), dropout operation (in grey) and softmax activation (in dark green) respectively to the intermediate two blocks' inputs, the first three blocks' outputs and the output block. The first block is connected to the third via a skipping connection. Following [38], the training targets comprise speech intelligibility groups + speaker IDs for dysarthric speech and binary aged vs. non-aged annotations for elderly speech. We then extract 25-dim spectral basis embedding (SBE) features from the trained bottleneck block.", "subsections": []}, {"title": "C. Variance-Regularized Deep Embedding", "content": "Feature-based adaptation techniques require speaker homogeneity to be consistently encoded in the spectral basis embed-ding features obtained above in Sec. III-B. As such features are computed over individual utterances or portions of them, additional smoothing is required to ensure their homogeneity, e.g. an overall reduction of speech volume among dysarthric or elderly speakers. Prior research applies an offline speaker-level averaging [38] of all utterance-level produced features, which conflicts with the low processing latency and streaming objective of on-the-fly adaptation.\nTo this end, an alternative form of speaker-level spectral basis embedding features smoothing based on variance regularization is proposed. This requires a second embedding module to be constructed in phase-3 (Fig. 1 bottom right, in pink). During its training, an additional regression task is adopted using the speaker-level averaged spectral basis embeddings obtained from phase-2 as targets (Fig. 1 blue bold line), which minimizes the output features' variance and thus further maximizes speaker homogeneity in the final embedding features for on-the-fly adaptation. The overall multitask learning cost interpolates: 1) the cross-entropy (CE) loss over speech intelligibility/age groups, optionally plus that over speaker IDs, and 2) the mean squared error (MSE) between the bottleneck hidden features and the averaged speaker representations obtained from phase-2, given as2:\n$\\mathcal{L}_{MTL} = \\beta_1 \\cdot \\mathcal{L}_{MSE} + \\beta_2 \\cdot \\mathcal{L}_{CE_{group}} + \\beta_3 \\cdot \\mathcal{L}_{CE_{ID}}$ (2)\nHere $\\mathcal{L}_{MSE} = \\frac{1}{R} \\sum_{r=1}^{R} (y_r^s - \\hat{y}_r)^2$, where $y_r^s$ and $\\hat{y}_r$, respectively denote the rth bottleneck hidden feature and averaged representation of speaker s from phase-2.\nThe 25-dim variance-regularized spectral basis embedding (VR-SBE) features are then extracted from the bottleneck of the second embedding module (Fig. 1 bottom right, red bold line) and concatenated with acoustic features before being fed into hybrid DNN/TDNN (Fig. 2(a)) or E2E Conformer (Fig. 2(b)) ASR systems to facilitate on-the-fly speaker adaptation during both model training and test-time adaptation.", "subsections": []}, {"title": "IV. ON-THE-FLY SPEAKER ADAPTATION VIA VR-SBE FEATURE DRIVEN F-LHUC TRANSFORMS", "content": "Model-based speaker adaptation techniques offer powerful, fine-grained parameterization of speaker-level attributes when personalizing ASR systems. However, their application to", "subsections": []}, {"title": "A. Batch-Mode Model-Based LHUC Adaptation", "content": "The standard approach of applying LHUC speaker adaptation involves training speaker-dependent (SD) linear scaling transforms to adjust activation amplitudes on DNN nodes [102], [57]. Let As denote such transform in a specific layer for speaker s. To reduce the risk of overfitting to limited speaker data, As can be further constrained as a diagonal matrix, equivalent to applying vector vs to modify amplitudes of activations. The adapted outputs can be expressed as:\n$h^s = \\xi(v^s) \\odot h$ (3)\nwhere h and $\\odot$ denote the activated hidden vector and the Hadamard product operation. $\\xi(\\cdot)$ denotes the activation function, e.g. the element-wise 2 \u00d7 sigmoid(\u00b7) function [57] ranging from 0 to 2. In this case, the SD LHUC parameters vs are typically initialized as 0. Such parameters can be estimated by finding the minimum cross entropy (CE) estimator $\\mathcal{V}_{CE}^s$:\n$\\mathcal{V}_{CE}^s = \\arg \\min_{v^s}\\{ -\\log P(H^s|D^s, v^s) \\}$ (4)\nwhere Ds and Hs denote the adaptation data recorded from speaker s and the corresponding supervision label, e.g. HMM states in hybrid systems or output tokens in E2E systems. Without loss of generality, we omit the SI model parameters.\nDuring unsupervised test-time adaptation, the label Hs is commonly produced by initially decoding the adaptation data Ds. Such batch-mode adaptation process can be iteratively performed to refine both the quality of hypothesis supervision and resulting re-estimated SD parameters. The adapted model with the final estimated SD parameters undergoes another decoding pass to infer hypotheses $\\hat{H}^s$ for test data $\\hat{D}^s$.\nWhen using LHUC-based speaker adaptive training (LHUC-SAT) [85], the SD parameters tied to training speakers are joint-optimized with the shared SI parameters. The resulting SI parameters provide a canonical model that more effectively learns speaker-invariant speech properties and can be better adapted to unseen speakers during test-time adaptation.", "subsections": []}, {"title": "B. Feature-Based LHUC Transforms", "content": "All batch-mode speaker adaptation techniques, including those based on LHUC of Sec. IV-A, operate with multiple decoding and SD parameter estimation passes. This normally requires all speaker-level data to be used to ensure the robustness and fine-granularity of SD parameters. In order to improve data efficiency, reduce processing latency and minimize the impact of supervision errors, an alternative approach is to directly learn a homogeneous mapping between speaker data and associated SD parameters, bypassing the multi-pass decoding followed by unsupervised adaptation procedure entirely. This allows speaker-level feature-based LHUC (f-LHUC) transforms to be directly predicted on the fly during evaluation using the speech data arriving in a streaming mode. To ensure the homogeneity of such regression mapping, the VR-SBE features proposed in Sec. III are used as the regression inputs together with mel-scale filterbank (FBK).\nAs shown in Fig. 3, our TDNN LHUC regression network [69] contains four 500-dim context-splicing layers (in blue) and two 500-dim feed-forward layers comprising affine transformation (in green) and sigmoid activation (in red). Three 300-dim linear bottleneck projections (in light green) are inserted between the context-splicing layers to reduce the number of parameters, while a specifically designed online averaging layer (in light blue) is integrated before the sigmoid activation of the second feed-forward layer (Fig. 3 upper right). Such a layer stores the preceding speech utterances in memory via an accumulated history vector and a frame counter for each speaker. The output of this online averaging layer is given by:\n$\\text{m}_r^s = \\frac{ \\sum_{t=1}^{T_r} h_t^s + \\alpha \\times G^{r-1} } { T_r + \\alpha \\times N^{r-1} }$ (5)\nwhere $G^{r-1}$ and $N^{r-1}$ refer to the accumulated history vector and the frame counter until the (r \u2013 1)th utt. of speaker s. Tr", "subsections": []}, {"title": "V. IMPLEMENTATION DETAILS", "content": "This section discusses several implementation details that affect the performance of our VR-SBE features and f-LHUC transforms, including: 1) the number of principal spectral bases as inputs to VR-SBE embedding network; 2-4) the inputs to f-LHUC regression network, its PCA compressed regression targets' dimensionality and its history interpolation factor; and 5) baseline speaker adaptation of E2E Conformer. Ablation studies are conducted on the benchmark UASpeech [70] dysarthric and DementiaBank Pitt [72] elderly speech datasets\u00b3. 40-dim mel-filterbank (FBK) log amplitude spectra serve as inputs to SVD. The history interpolation factor", "subsections": []}, {"title": "VI. EXPERIMENTS AND RESULTS", "content": "In this section, we investigate the performance of our feature-based on-the-fly adaptation approaches on four tasks: the English UASpeech [70] and TORGO [71] dysarthric speech datasets, and the English DementiaBank Pitt [72] and Cantonese JCCOCC MoCA [73] elderly speech corpora. All the settings determined in the ablation studies of Sec. V are adopted. SI and SD speed perturbations [24], [34] based data augmentation are applied to all tasks. The extraction of 100-dim iVector and 25-dim xVector follow the Kaldi recipes using the same training data as for ASR systems.", "subsections": []}, {"title": "A. Experiments on Dysarthric Speech", "content": "1) The UASpeech Dataset: As the largest publicly available dysarthric speech dataset, UASpeech [70] is an isolated word recognition task in English with 103 hours of speech from 16 dysarthric and 13 control speakers recorded using a 7-channel microphone array. The block based training-evaluation data partitioning protocol [70], [1]\u2013[3], [17] is adopted. For each speaker, the data is split into B1, B2 and B3, each with the same 155 common words and a different 100 uncommon words. The training set combines B1 and B3 data of all 29 speakers, while the evaluation set contains the B2 data from the 16 dysarthric speakers. Silence stripping [29] leads to a 30.6h training set (99195 utt.) and a 9h evaluation set (26520 utt.). Further data augmentation [24], [29] produces a 130.1h training set (399110 utt.). As E2E systems are sensitive to training data coverage, B2 of the control speech and its speed-perturbed versions are also used for Conformer system training, creating a 173h training set (538292 utt.).\n2) The TORGO Dataset: The English TORGO [71] dysarthric dataset contains 13.5h of speech from 8 dysarthric and 7 control speakers based on short sentences and single words. We adopt a 3-block based training-evaluation data partition similar to UASpeech. The control speech and two-thirds of the speech of each impaired speaker are used for training, while the remaining one-third is for evaluation. Silence stripping produces a 6.5h training set (14541 utt.) and a 1h evaluation set (1892 utt.). Further speed perturbation [24], [104] produces a 34.1h augmented training set (61813 utt.).\n3) Experimental Setup for UASpeech: Following [24], [29], the hybrid DNN systems with six 2000-dim and one 100-dim layers are implemented using extended Kaldi [105]. Each hidden layer contains linear bottleneck projection, affine", "subsections": []}, {"title": "B. Experiments and Results on Elderly Speech", "content": "1) The DementiaBank Pitt Dataset: The English Dementia-Bank Pitt [72] dataset contains 33h of cognitive impairment assessment interviews between 292 elderly participants and clinical investigators. The training set includes 688 speakers (244 elderly and 444 investigators), while the development and evaluation sets14 respectively contain 119 (43 elderly and 76 investigators) and 95 speakers (48 elderly and 47 investigators). There is no overlap speaker between the training and the development or evaluation sets. Silence stripping [34] leads to a 15.7h training set (29682 utt.), a 2.5h development set (5103 utt.) and a 0.6h evaluation set (928 utt.), while data augmentation [34] produces a 58.9h training set (112830 utt.).\n2) The JCCOCC MoCA Dataset: The Cantonese JCCOCC MoCA dataset comprises cognitive impairment assessment interviews between 256 elderly participants and clinical investigators [73]. The training set contains 369 speakers (158 elderly and 211 investigators), while the development and evaluation sets each contain speech from 49 elderly other than those in the training set. Silence stripping leads to a 32.1h training set (95448 utt.), a 3.5h development set (13675 utt.) and a 3.4h evaluation set (13414 utt.). Further data augmentation [36] produces a 156.9h training set (389049 utt.).\n3) Experiment Setup for the DementiaBank Pitt Corpus: Following the Kaldi [105] chain setup, the hybrid factorized TDNN systems contain 14 context-slicing layers with a 3-frame context. The setup of E2E graphemic Conformers follows that for UASpeech. 40-dim FBK features are used as inputs. A word-level 4-gram LM with Kneser-Ney (KN) smoothing is trained [34], with a 3.8k vocabulary covering all words in DementiaBank Pitt adopted during evaluation.\n4) Experiment Setup for the JCCOCC MoCA Corpus: The setup of the hybrid TDNN and E2E character Conformer systems are the same as those for DementiaBank Pitt. 40-dim FBK features are adopted as inputs. A word-level 4-gram LM with KN smoothing is trained using the transcription of JCCOCC MoCA (610k words), with a 5.2k recognition vocabulary covering all words in JCCOCC MOCA adopted.", "subsections": []}, {"title": "C. Further Analyses on Feature Homogeneity", "content": "We further analyze the homogeneity of the proposed on-the-fly adaptation features. T-distributed stochastic neighbor embedding (t-SNE) [76] visualization is conducted on the proposed VR-SBE features against baseline iVectors and x-Vectors in 2-D plots (Fig. 5(a)-(c)), and also on VR-SBE or FBK+VR-SBE feature driven f-LHUC transforms against batch-mode spectral basis embedding (SBE) features and LHUC transforms in 3-D plots (Fig. 5(d)-(i)). Three UASpeech16 dysarthric speakers of mixed genders are selected from the very low (F03), low (F02) and mid (M11) speech intelligibility groups. For each speaker, 101 distinct speaker-level adaptation data quantity operating points are used for computing the above speaker features or LHUC transforms. These points correspond to using only one utterance or 1%, 2%,...,99%, and up to 100% of the speaker-level data. Fig. 5 shows that the proposed on-the-fly VR-SBE features and their associated f-LHUC transforms consistently exhibit stronger speaker homogeneity measured in covariance determinants after applying t-SNE projection than baseline iVectors, x-Vectors and batch-mode SBE features and LHUC transforms.\nWe further compare the performance of the proposed on-the-fly VR-SBE adaptation with batch-mode LHUC and SBE adaptation when using limited amounts of speaker-level adaptation data. As depicted in Fig. 6(a) and Fig. 6(b), VR-SBE adaptation is more robust to changes in the amount of speaker-level data than batch-mode LHUC and SBE adaptation. Specifically, VR-SBE adaptation outperforms SBE adaptation when using less than 20% of the speaker data and surpasses LHUC adaptation when using less than 40% of speaker data.", "subsections": []}, {"title": "VII. DISCUSSION AND CONCLUSION", "content": "This paper presents two novel methods to learn homogeneous dysarthric and elderly speaker features for on-the-fly test-time adaptation of TDNN and Conformer models, including: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that ensure speaker-level feature consistency via specially designed regularization; and feature-based LHUC (f-LHUC) transforms driven by VR-SBE. Experiments conducted on four dysarthric and elderly speech corpora across English and Cantonese suggest our proposed approaches achieve statistically significant WER or CER reductions of up to 5.32% absolute (18.57% relative) over baseline iVector/xVector adaptation, and 2.24% absolute (9.20% relative) over batch-mode offline LHUC adaptation. Further processing latency analyses and t-SNE visualization show that our VR-SBE features and f-LHUC transforms are robust to speaker-level data quantity in test-time adaptation, while exhibiting stronger speaker-level homogeneity than iVectors, xVectors and batch-mode LHUC transforms. Future research will focus on rapid adaptation of pre-trained ASR systems.", "subsections": []}]}