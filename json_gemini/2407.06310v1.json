{"title": "Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly Speaker Adaptation", "authors": ["Mengzhe Geng", "Xurong Xie", "Jiajun Deng", "Zengrui Jin", "Guinan Li", "Tianzi Wang", "Shujie Hu", "Zhaoqing Li", "Helen Meng", "Xunying Liu"], "abstract": "The application of data-intensive automatic speech recognition (ASR) technologies to dysarthric and elderly adult speech is confronted by their mismatch against healthy and non-aged voices, data scarcity and large speaker-level variability. To this end, this paper proposes two novel data-efficient methods to learn homogeneous dysarthric and elderly speaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN and Conformer ASR models. These include: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that exploit a special regularization term to enforce homogeneity of speaker features in adaptation; and 2) feature-based learning hidden unit contributions (f-LHUC) transforms that are conditioned on VR-SBE features. Experiments are conducted on four tasks across two languages: the English UASpeech and TORGO dysarthric speech datasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora. The proposed on-the-fly speaker adaptation techniques consistently outperform baseline iVector and xVector adaptation by statistically significant word or character error rate reductions up to 5.32% absolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24% absolute (9.20% relative), while operating with real-time factors speeding up to 33.6 times against xVectors during adaptation. The efficacy of the proposed adaptation techniques is demonstrated in a comparison against current ASR technologies including SSL pre-trained systems on UASpeech, where our best system produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features and f-LHUC transforms are insensitive to speaker-level data quantity in test-time adaptation. T-SNE visualization reveals they have stronger speaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC transforms.", "sections": [{"title": "I. INTRODUCTION", "content": "In spite of the remarkable advancement on automatic speech recognition (ASR) techniques for normal speech, accurate recognition of disordered speech, for example, voice recorded from speakers with dysarthria, remains a highly challenging task to date [1]\u2013[11]. Millions of people world-wide are clinically diagnosed with speech disorders [12]. In addition, neurocognitive disorders, such as Alzheimer's disease (AD), are often found among older adults and manifest themselves in speech impairments [13], [14]. Given that large-scale pathological assessment of speech disorders among all older adults is practically difficult due to the limited availability of professional speech pathologists, the actual number of people affected by speech disorders is much larger. As aging presents enormous challenges to health care worldwide, there is a pressing need to develop suitable ASR-based assistive technologies customized for dysarthric and elderly speakers to improve their life quality and social inclusion [15]\u2013[44].\nDysarthric and elderly speech bring challenges on three fronts to current ASR technologies predominantly targeting normal speech recorded from healthy, non-aged users. These challenges include: a) substantial mismatch against normal speech due to the underlying neuro-motor control conditions [45], [46] and aging, for example, imprecise articulation, hoarse voice and increased disfluency [47], [48]; b) data scarcity due to the difficulty in collecting large quantities of such data from dysarthric and elderly speakers with mobility limitations; and c) large speaker-level diversity among dysarthric and elderly talkers, when sources of variability commonly found in normal speech, e.g. accent or gender, are further aggregated with speech pathology severity and aging.\nA key task for all ASR systems is to model the speech variability attributed to speaker-level characteristics. To this end, speaker adaptation techniques provide a key role in customizing ASR systems for users' needs. For normal speech recognition tasks, three main categories of such techniques have been studied: 1) auxiliary embedding features that are speaker-dependent (SD) [49]\u2013[51]; 2) feature transformations that produce speaker-independent (SI) features to remove such variability at the front-ends [52]\u2013[54]; and 3) model-based adaptation using specifically designed SD DNN parameters [55]\u2013[58]. A detailed review of speaker adaptation techniques targeting normal speech is presented in Sec. II.\nModeling the large speaker-level heterogeneity in dysarthric and elderly speech requires powerful speaker adaptation techniques to be developed. However, only limited prior research in this direction has been conducted. Earlier works focused on deriving speaker adaptation methods for traditional HMM-based ASR systems with Gaussian mixture model (GMM) based hidden state densities, predominantly targeting dysarthric speech only. A combination of HMM state transition interpolation and maximum a posteriori (MAP) adaptation is utilized to account for dysarthric speaker diversity in [59]. The use of maximum likelihood linear regression (MLLR) and MAP adaptation was explored in [1], [60]\u2013[62]. Combined use of MLLR with MAP adaptation in speaker adaptative training (SAT) of GMM-HMM models was investigated in [2]. Feature-space MLLR (f-MLLR) based SAT training [63] and regularized speaker adaptation to dysarthric speakers using Kullback-Leibler (KL) divergence [64] were also developed for GMM-HMM systems. MLLR adaptation of GMM-HMM models to elderly speakers using cross-speaker statistics pooling was studied in [60].\nMore recent dysarthric and elderly speaker adaptation approaches applied to state-of-the-art hybrid and end-to-end (E2E) neural network based ASR systems include the following categories: 1) direct speaker-level parameter fine-tuning of hybrid LF-MMI trained TDNN [27], [28] and RNN-T [65] models; 2) the use of iVector speaker adaptation for dysarthric [66], [67] and elderly [34] speech recognition; 3) model-based speaker adaptation using, for example, learning hidden unit contributions (LHUC) for dysarthric [24], [29], [30], [38] and elderly [34], [38] speakers; 4) Bayesian domain [68] and speaker adaptation [29] methods that are more robust to dysarthric and elderly speech data scarcity; 5) dysarthric and elderly speaker-level averaged spectro-temporal basis embedding features [36], [38] for hybrid DNN/TDNN and E2E Conformer adaptation; 6) f-MLLR and xVector based dysarthric speaker adaptation of self-supervised learning (SSL) based Wav2vec 2.0 models [5]; and 7) dysarthric speaker adapter fusion of pre-trained E2E Transformer models [42].\nSuitable dysarthric and elderly speaker adaptation methods that can meet with above-mentioned challenges should satisfy the following requirements: 1) high data efficiency to model the very limited speaker-level data; 2) strong speaker homogeneity to ensure the distinct speaker-level characteristics are consistently represented in the adapted model; and 3) low processing latency to allow adaptation to be performed immediately on-the-fly from the onset of a new user's enrolment in the system to minimize their efforts and fatigue. In this regard, prior research only addressed some of the above. For example, the Bayesian model based adaptation using very limited speaker data [29], [34], [68] only addresses the aforementioned data scarcity issue, but the latency problem remains unvisited. Similarly, the spectro-temporal deep embedding features [36], [38] are averaged over all speaker-level data in an offline manner. This introduces considerable processing latency and is thus unsuitable for on-the-fly test-time adaptation.\nTo this end, one possible solution is to derive suitable rapid, on-the-fly feature-based dysarthric and elderly adaptation techniques. Such methods serve as multi-purpose solutions to handle not only the speaker-level data scarcity and diversity when representing speaker attributes, but also the processing latency incurred by model-based fine-tuning or adaptation to speaker data. In this paper, two novel forms of feature-based on-the-fly rapid speaker adaptation approaches are proposed. The first is based on speaker-level variance-regularized spectral basis embedding (VR-SBE) features. An extra variance regularization term is included when training spectral basis embedding neural networks [36], [38] to ensure speaker homogeneity of the embedding features. This in turn allows them to be applied on-the-fly during test-time adaptation to dysarthric or elderly speakers. The second approach utilizes on-the-fly feature-based LHUC (f-LHUC) transforms conditioned on VR-SBE features. Specially designed regression TDNN [69] predicting dysarthric or elderly speaker-level LHUC transforms is constructed to directly generate and apply such SD parameters on the fly during test-time adaptation.\nExperiments are conducted on four different tasks: 1) the English UASpeech [70] and TORGO [71] dysarthric speech datasets; 2) the English DementiaBank Pitt [72] and the Cantonese JCCOCC MoCA [73] elderly speech corpora. Among these, UASpeech is by far the largest available and most extensively used dysarthric speech corpus, while DementiaBank Pitt is the largest publicly available elderly speech database. The performance of the proposed two feature-based on-the-fly speaker adaptation approaches: VR-SBE and VR-SBE feature conditioned f-LHUC, are compared against those of baseline iVector [50] or xVector [74] based on-the-fly adaptation and LHUC [57] based model adaptation on three fronts:\n1) ASR performance in WER/CER: Our proposed on-the-fly speaker adapted hybrid DNN/TDNN and E2E Conformer systems consistently outperform the corresponding on-the-fly iVector/xVector adapted systems, with statistically significant\u00b9 word/character error rate (WER/CER) reductions up to 5.32% absolute (18.57% relative). Our on-the-fly adaptation methods also outperform the comparable offline batch-mode model-based LHUC adaptation by statistically significant WER/CER reductions up to 2.24% absolute (9.20% relative).\n2) Analysis on processing latency: Experiments on the benchmark UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest that statistically significant WER/CER reductions are consistently obtained over the corresponding on-the-fly iVector/xVector adapted systems, while the proposed VR-SBE speaker adaptation operates with a real-time factor speeding up ratio up to 33.6 times against xVector. It incurs a minimal processing latency by using an input acoustic feature window as short as 10 ms in duration.\n3) Analysis on speaker feature homogeneity: T-SNE visualization [76] of baseline iVectors/xVectors, offline model-based LHUC transforms, offline SBE [38] features, our proposed on-the-fly VR-SBE features and the associated f-LHUC transforms across varying amounts of adaptation data is conducted. This suggests more consistent, data quantity invariant dysarthric and elderly speaker characteristics can be learned via the proposed on-the-fly speaker adaptation approaches.\nThe main contributions of this paper are summarized below:\n1) This paper presents novel approaches to learn homogeneous speaker features tailored for rapid, on-the-fly dysarthric and elderly speaker adaptation. In contrast, prior studies either considered adaptation techniques using all speaker-level data and operating in batch-mode, offline manner [24], [68], [30], [29], [34], [38], [42], or used existing iVector/xVector features not tailored for dysarthric/elderly speech and produced mixed results on such data [66], [34], [5], [67]. In particular, model-based adaptation methods not only use all speaker-level data, but also introduce multiple decoding passes and explicit parameter estimation or fine-tuning stages during test-time adaptation [68], [30], [29], [34], [42].", "latex": []}, {"title": "II. REVIEW OF SPEAKER ADAPTATION", "content": "This section reviews three major categories of speaker adaptation techniques traditionally designed for normal non-aged, healthy speakers, respectively based on: a) auxiliary features, b) feature transformations, and c) model-based adaptation.\nAmong these, auxiliary speaker embedding features encode speaker-dependent (SD) characteristics via compact representations. The resulting SD features are used as auxiliary inputs to facilitate speaker adaptation during the training and evaluation of ASR systems. Such SD features can be either estimated independently of the back-end systems, e.g. using universal background models (UBMs) based on Gaussian mixture models (GMMs) to learn iVectors [50], [51], or jointly learned with the back-end systems, e.g. alternate updating speaker representations and the remaining parameters when learning speaker codes [49]. Auxiliary speaker features can be flexibly incorporated into both GMM-HMM or hybrid systems and more recent end-to-end E2E systems [77], [78], [5].\nSpeaker adaptative feature transformations are applied to acoustic front-ends to produce speaker-invariant canonical input features, for example, feature-space maximum likelihood linear regression (f-MLLR) [52] estimated at speaker-level from GMM-HMM systems. Speaker-level physiological differences motivated adaptation based on vocal tract length normalization (VTLN) [79], [80] can be further adopted by applying piecewise linear frequency warping factors [80].\nModel-based speaker adaptation techniques apply compact forms of specially designed SD parameters in different network layers, for example, linear input networks (LIN) [81], [56], linear hidden networks (LHN) [55], learning hidden unit contributions (LHUC) [57], [82], linear output networks (LON) [55], parameterized activation functions (PAct) [58], factorized linear transformation [83], and SD neural beam-forming, encoder, attention or decoder modules in E2E multichannel systems [84]. Speaker adaptive training (SAT) [85], [86] can be further applied during training to enable a joint optimization of both SD and SI parameters and to produce a canonical model that can be better adapted to unseen speakers during test-time adaptation. To alleviate the risk of overfitting to limited speaker data during model adaptation, a series of regularized speaker adaptation strategies have been developed. These include the use of L2 [87], [88] or Kull-back-Leibler (KL) divergence regularization [89]\u2013[92], and maximum a posterior (MAP) [93], [94] or Bayesian inspired adaptation [82]. Data augmentation on the target speaker via text-to-speech (TTS) synthesis has also been explored in [95], [96]. Alternative objective functions have been investigated for speaker adaptation, for example, adversarial learning [97], [98], [91] and multitask learning [99]\u2013[101], [90], [91].", "latex": []}, {"title": "III. ON-THE-FLY SPEAKER ADAPTATION VIA VARIANCE REGULARIZED SPECTRAL BASIS EMBEDDING FEATURES", "content": "On-the-fly feature-based speaker adaptation techniques provide practical solutions to handle both speaker-level data scarcity and processing latency incurred by model-based fine-tuning or adaptation to user data. A key task in designing such techniques is to ensure the homogeneity of speaker-level features and consistent representation of speaker characteristics. To this end, the proposed variance-regularized spectral basis embedding (VR-SBE) features are extracted in three phases (Fig. 1), including: a) conducting singular value decomposition (SVD) on utterance-level dysarthric/elderly speech spectra to produce initial time-invariant spectral bases (Fig. 1 left, in yellow); b) feeding the spectral bases through a first embedding module trained using speaker ID and speech intelligibility/age information, which extracts latent embeddings more consistent and relevant to dysarthric/elderly speaker attributes (Fig. 1 top right, in light blue); and c) feeding the spectral bases through a second embedding module, with an additional output variance regularization cost using the averaged speaker embeddings obtained in stage b) as targets (Fig. 1 bottom right, in pink). Such a process ensures the maximum speaker homogeneity of the final VR-SBE features for on-the-fly adaptation.\nA. Speech Spectrum Subspace Decomposition\nSVD-based spectrum decomposition [36], [38] provides an intuitive approach to decouple the latent time-invariant spectral and time-variant temporal features in speech signals. In phase-1 of VR-SBE feature extraction (Fig. 1 left, in yellow), SVD is applied to the mel-filterbank log amplitude spectrum \\(O_{C \\times T_r}\\) of utterance r with \\(T_r\\) frames and C filterbank channels:\n\\(O_{C \\times T_r} = U \\Sigma V^T\\) where the set of column vectors of the C\u00d7 C dimensional U matrix (the left-singular vectors) and the set of row vectors of the \\(T_r \u00d7 T_r\\) dimensional V matrix (the right-singular", "latex": ["O_{C \\times T_r} = U \\Sigma V^T"]}, {"title": "IV. ON-THE-FLY SPEAKER ADAPTATION VIA VR-SBE FEATURE DRIVEN F-LHUC TRANSFORMS", "content": "Model-based speaker adaptation techniques offer powerful, fine-grained parameterization of speaker-level attributes when personalizing ASR systems. However, their application to", "latex": []}, {"title": "V. IMPLEMENTATION DETAILS", "content": "This section discusses several implementation details that affect the performance of our VR-SBE features and f-LHUC transforms, including: 1) the number of principal spectral bases as inputs to VR-SBE embedding network; 2-4) the inputs to f-LHUC regression network, its PCA compressed regression targets' dimensionality and its history interpolation factor; and 5) baseline speaker adaptation of E2E Conformer. Ablation studies are conducted on the benchmark UASpeech [70] dysarthric and DementiaBank Pitt [72] elderly speech datasets\u00b3.\nB. Inputs to F-LHUC Regression Network\nWhen performing f-LHUC online speaker adaptation of Sec. IV-B, the results in Tables III and IV suggest that on both the UASpeech and DementiaBank Pitt, the combined use of both FBK and VR-SBE features as the inputs to the f-LHUC regression network (Fig. 3, left) generally outperforms using either feature alone, especially when the most powerful configuration combining VR-SBE and f-LHUC is used. Hence, both FBK and VR-SBE features are used as the inputs to the f-LHUC regression network for all the following f-LHUC adaptation experiments.\nD. History Interpolation Factor of F-LHUC Regression\nTables VII and VIII suggest that setting the f-LHUC adaptation history interpolation factor (Eqn. (5), Sec. IV-B) as a = 0.9 produces the best performance (Sys.7 vs. others in", "latex": []}, {"title": "VI. EXPERIMENTS AND RESULTS", "content": "In this section, we investigate the performance of our feature-based on-the-fly adaptation approaches on four tasks: the English UASpeech [70] and TORGO [71] dysarthric speech datasets, and the English DementiaBank Pitt [72] and Cantonese JCCOCC MoCA [73] elderly speech corpora. All the settings determined in the ablation studies of Sec. V are adopted. SI and SD speed perturbations [24], [34] based data augmentation are applied to all tasks. The extraction of 100-dim iVector and 25-dim xVector follow the Kaldi recipes using the same training data as for ASR systems.\nA. Experiments on Dysarthric Speech\n1) The UASpeech Dataset: As the largest publicly available dysarthric speech dataset, UASpeech [70] is an isolated word recognition task in English with 103 hours of speech from 16 dysarthric and 13 control speakers recorded using a 7-channel microphone array. The block based training-evaluation data partitioning protocol [70], [1]\u2013[3], [17] is adopted. For each speaker, the data is split into B1, B2 and B3, each with the same 155 common words and a different 100 uncommon words. The training set combines B1 and B3 data of all 29 speakers, while the evaluation set contains the B2 data from the 16 dysarthric speakers. Silence stripping [29] leads to a 30.6h training set and a 9h evaluation set (26520 utt.). Further data augmentation [24], [29] produces a 130.1h training set. As E2E systems are sensitive to training data coverage, B2 of the control speech and its speed-perturbed versions are also used for Conformer system training, creating a 173h training set.\n2) The TORGO Dataset: The English TORGO [71] dysarthric dataset contains 13.5h of speech from 8 dysarthric and 7 control speakers based on short sentences and single words. We adopt a 3-block based training-evaluation data partition similar to UASpeech. The control speech and two-thirds of the speech of each impaired speaker are used for training, while the remaining one-third is for evaluation. Silence stripping produces a 6.5h training set (14541 utt.) and a 1h evaluation set (1892 utt.). Further speed perturbation [24], [104] produces a 34.1h augmented training set.\n3) Experimental Setup for UASpeech: Following [24], [29], the hybrid DNN systems with six 2000-dim and one 100-dim layers are implemented using extended Kaldi [105]. Each hidden layer contains linear bottleneck projection, affine\nB. Experiments and Results on Elderly Speech\n1) The DementiaBank Pitt Dataset: The English Dementia-Bank Pitt [72] dataset contains 33h of cognitive impairment assessment interviews between 292 elderly participants and clinical investigators. The training set includes 688 speakers (244 elderly and 444 investigators), while the development and evaluation sets\u00b9 respectively contain 119 (43 elderly and 76 investigators) and 95 speakers (48 elderly and 47 investigators). There is no overlap speaker between the training and the development or evaluation sets. Silence stripping [34] leads to a 15.7h training set and a 2.5h development set and a 0.6h evaluation set while data augmentation [34] produces a 58.9h training set.\n2) The JCCOCC MoCA Dataset: The Cantonese JCCOCC MoCA dataset comprises cognitive impairment assessment interviews between 256 elderly participants and clinical investigators [73]. The training set contains 369 speakers (158 elderly and 211 investigators), while the development and evaluation sets each contain speech from 49 elderly other than those in the training set. Silence stripping leads to a 32.1h training set (95448 utt.), a 3.5h development set and a 3.4h evaluation set\n3) Experiment Setup for the DementiaBank Pitt Corpus: Following the Kaldi [105] chain setup, the hybrid factorized TDNN systems contain 14 context-slicing layers with a 3-frame context. The setup of E2E graphemic Conformers follows that for UASpeech. 40-dim FBK features are used as inputs. A word-level 4-gram LM with Kneser-Ney (KN) smoothing is trained [34], with a 3.8k vocabulary covering all words in DementiaBank Pitt adopted during evaluation.\n4) Experiment Setup for the JCCOCC MoCA Corpus: The setup of the hybrid TDNN and E2E character Conformer systems are the same as those for DementiaBank Pitt. 40-dim FBK features are adopted as inputs. A word-level 4-gram LM with KN smoothing is trained using the transcription of JCCOCC MoCA (610k words), with a 5.2k recognition vocabulary covering all words in JCCOCC MOCA adopted.\nC. Further Analyses on Feature Homogeneity", "latex": []}, {"title": "VII. DISCUSSION AND CONCLUSION", "content": "This paper presents two novel methods to learn homogeneous dysarthric and elderly speaker features for on-the-fly test-time adaptation of TDNN and Conformer models, including: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that ensure speaker-level feature consistency via specially designed regularization; and feature-based LHUC (f-LHUC) transforms driven by VR-SBE.\nExperiments conducted on four dysarthric and elderly speech corpora across English and Cantonese suggest our proposed approaches achieve statistically significant WER or CER reductions of up to 5.32% absolute (18.57% relative) over baseline iVector/xVector adaptation, and 2.24% absolute (9.20% relative) over batch-mode offline LHUC adaptation. Further processing latency analyses and t-SNE visualization show that our VR-SBE features and f-LHUC transforms are robust to speaker-level data quantity in test-time adaptation, while exhibiting stronger speaker-level homogeneity than iVectors, xVectors and batch-mode LHUC transforms. Future research will focus on rapid adaptation of pre-trained ASR systems.", "latex": []}]}