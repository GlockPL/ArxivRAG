{"title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription", "authors": ["Hounsu Kim", "Taegyun Kwon", "Juhan Nam"], "abstract": "Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available on the accompanying website 1.", "sections": [{"title": "I. INTRODUCTION", "content": "Piano transcription involves converting the acoustic signals of piano performances into individual notes. These notes are usually represented in a MIDI format, containing pitch information and the start and end timings (onset and offset) of each note. While recent approaches have achieved near-perfect performance [1], [2], most previous work have relied on feed-forward models. This means the final prediction for each note is made independently without considering the final predictions of other notes. In other words, the model does not effectively consider other neighboring notes when predicting each individual note.\nRecent advancements in deep generative models mainly utilize diffusion models, which progressively denoise data initially sampled from a stationary distribution. Continuous domain diffusion is used for continuous data, such as in image and audio generation tasks where it operates in the pixel and sample domain respectively. Discrete domain diffusion, on the other hand, handles discrete data, often in the form of tokens. Discrete diffusion models have recently proved their effectiveness in generative tasks utilizing VQ-VAEs [3]. In addition to their contributions to generative tasks, diffusion models have shown competitive performance in discriminative tasks such as image segmentation [4], [5]. Building on these foundings, we explore the potential of utilizing discrete diffusion models for piano transcription tasks for the first time. We hypothesize that the discrete denoising module not only predicts the states of each pixel in the piano roll gradually, but also refines the previously predicted states, which indicates that the final prediction of each note take into account the final predictions of other notes more effectively than feedforward approaches.\nPrevious research explored the use of continuous domain diffusion for transcription [6], but it failed to achieve performance comparable to current state-of-the-art (SOTA) methods. We address the short-comings by utilizing a discrete diffusion model, which includes a conditioning encoder, based on a pretrained acoustic model, and a denoising decoder composed primarily of Neighborhood Attention (NA) layers [7]. Additionally, we introduce a novel strategy involving separate transition states for the training and inference stages of discrete diffusion models. Comparing to the baseline, which is the acoustic model trained with a Focal loss, we demonstrate that utilizing the diffusion-based approach is effective for piano transcription."}, {"title": "II. RELATED WORKS", "content": "Most piano transcription models have utilized feed-forward deep neural networks to estimate notes from audio. Typically, the automatic music transcription (AMT) frameworks are optimized using either frame-wise loss [1], [8]\u2013[11] or a seq2seq framework [12]. In addition, [2] recently achieved state-of-the-art results using a Semi-CRF framework. In terms of model architecture, CNN-RNN-based models have been the most common choice for many years [8]\u2013[11], [13], while transformer-based models [1], [2], [12] have also been explored more recently. Another important factor is the representation of the output. Typically, onsets, frames, and offsets are represented as separate binary arrays [8]\u2013[10], [13]. In contrast, we employ a multi-label note states representation, where the state of each frame and pitch is expressed with a single discrete multi-label array. We followed the multi-state label approach used by [14], where the states are onset, sustain, re-onset, offset, and off."}, {"title": "B. Previous Diffusion-based Approach", "content": "To our knowledge, DiffRoll is currently the only approach that directly utilizes diffusion models for automatic music transcription [6]. DiffRoll is a continuous diffusion model based on the DiffWave [15] model, where the WaveNet [16] architecture is employed as the denoising decoder, with diffusion timestep and mel spectrogram conditioned through fully-connected layers and 1D convolutional layers respectively. The piano roll is represented with binary states, on and off, which are casted to the continuous domain to be input to the diffusion model. One limitation of their approach is the discrepancy between the discrete nature of the note space and the continuous representation. In our work, we address this by representing note states as a multi-label discrete variable and applying a discrete diffusion process. Furthermore, the audio feature encoder used in the DiffRoll was quite simple, consisting of only a 1D convolutional layer. We reinforced the acoustic encoder of the model by adopting a more structured, existing acoustic encoder."}, {"title": "C. Discrete Diffusion Models", "content": "The diffusion model in the discrete domain was first introduced by [17] in the form of binomial diffusion, and has been further expanded to multinomial diffusion models by [18], [19]. A transition matrix characterized by the uniform probability of random replacement of the current state with another state, alongside a probability for conversion to a specific [MASK] state, has proven to be particularly effective, as demonstrated in [20], [21]. As mentioned in [21], this choice of transition matrix can be regarded as refining the predicted results during the diffusion process. This insight motivated us to adopt this approach for piano transcription tasks, where the note states undergo refinement."}, {"title": "D. Neighborhood Attention", "content": "Neighborhood Attention (NA) [7] is a sliding window attention mechanism first introduced in the image domain, where each pixel of the image attends to its k-nearest neighbor pixels. By hierachically stacking NA blocks, the attention mechanism can be efficiently applied to high-resolution images while providing locality as an inductive bias. In our work, we employed 2D Neighborhood Attention in the denoising module of the discrete diffusion model, efficiently managing the high resolution of piano rolls. The inductive bias allows each pixel of the piano roll to consider neighboring notes during the denoising process. This can be seen as iterative refinement when the model is trained to allow the state of each pixels to transit to another state during a single denoising step. For parallel computation of Neighborhood Attention, we utilized the NATTEN python package\u00b2."}, {"title": "III. METHOD", "content": "The denoising decoder takes the roll of a discrete diffusion model, gradually predicting the multi-states of the piano roll and refining the predicted tokens during each diffusion timestep. It is composed of a pitchwise bidirectional LSTM layer followed by stack of NA 2D self-attention blocks, with AdaLN layers in between to condition the diffusion timesteps. The pitchwise bidirectional LSTM operates the LSTM separately for each of the 88 pitches, focusing on the temporal transitions within a single pitch. This layer was demonstrated to be effective in [10], [11]."}, {"title": "B. Acoustic Encoder", "content": "The acoustic encoder receives the audio waveform and provides appropriate conditioning features to the denoising decoder. We adapted the HPPNet [10] with a few modifications as our baseline. The HPPNet takes a Constant-Q Transform (CQT) as input, which is processed by a stack of 2D convolution and harmonic-dilated convolution (HDC) layers. The output of this convolution stack is then passed through FG-LSTM, which is a pitchwise LSTM layer. In our modified model, we used a multi-scale mel-like spectrogram, where each frequency bin is mapped to logarithmically spaced bins aligned with MIDI frequencies. We also increased the number of HDC layers from 1 to 3, employed a single CNN-HDC stack, and adapted a multi-state representation [14]. This model, trained with the Focal loss and without the diffusion process, served as our baseline.\nThe penultimate output feature of this model has the same height and width dimensions as the piano roll and is sought to contain key features that has local coherence with the expected piano roll output. Therefore, for our diffusion model, we cross-attend these penultimate features of the pretrained baseline model with the denoising decoder using NA, which we call NA 2d cross. The overall model architecture is illustrated in Figure 1. All encoder parameters are finetuned during training."}, {"title": "C. Discrete Diffusion for Piano Transcription", "content": "The target label is represented as $y \\in \\mathbb{R}^{T \\times 88 \\times 6}$, where the last dimension is a one-hot vector containing K = 5 multi-label note states plus a [MASK] state. During the forward process, the label is corrupted by a transition matrix $[Q_{\\tau}]_{ij} = q(y_{t+1} = j | y_t = i) \\epsilon \\mathbb{R}^{6 \\times 6}$ structured as follows:\n$Q_T = \\begin{bmatrix} \\alpha_{\\tau} + \\beta_{\\tau} & \\beta_{\\tau} & \\beta_{\\tau} & \\beta_{\\tau} & 0 & 0 \\\\  &  \\\\ \\beta_{\\tau} & \\alpha_{\\tau} + \\beta_{\\tau} & \\beta_{\\tau} & \\beta_{\\tau} & 0 & 0 \\\\  &  \\\\ \\beta_{\\tau} & \\beta_{\\tau} & \\alpha_{\\tau} + \\beta_{\\tau} & \\beta_{\\tau} & 0 & 0 \\\\  &  \\\\ \\beta_{\\tau} & \\beta_{\\tau} & \\beta_{\\tau} & \\alpha_{\\tau} + \\beta_{\\tau} & 0 & 0 \\\\  &  \\\\ 0 & 0 & 0 & 0 & \\gamma_{\\tau} & 1 - \\gamma_{\\tau} \\\\  &  \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\\\  \\end{bmatrix}$        (1)\nwhere $y_t$ denotes the state of a label y at position t \u2208 {0, 1, ..., T \u2013 1}, pitch p \u2208 {0,1, ..., 87} and diffusion timestep \u03c4 \u2208 {0,1, ..., T\u2212 1}."}, {"title": "D. Absorbing State Sampling for Effective Refinement", "content": "As shown in (3) and illustrated in Figure 2, to obtain $p_{\\theta}(y_{t-1}|y_t, x)$, Our model first predicts the clean sample $\\hat{y_0}$, and then applies the transition $q(y_{t-1}|y_t, \\hat{y_0})$. This method is optimal for the training stage, as the model learns to refine the tokens with probability \u03b2. However, during the sampling stage, the transition replaces the tokens with another token again, which potentially disrupts the refinement process.\nTo address this, we first simply assume that the model identifies diffusion manifolds as samples with the same total portion of cor-rupted tokens. Then, instead of corrupting tokens by both replacing and masking them with probabilities \u03b2, and respectively, we can mask the tokens with a total probability of $\\gamma_\\tau + K\\beta_\\tau$, only during the sampling stage ($\\tilde{q}(y_{t-1}|y_t, y_0)$ in Figure 2). This posterior process is identical to the absorbing state mentioned in [19] and places the sample on the manifold of the desired diffusion timestep. This simple adjustment to the posterior process during sampling leads to improved results. Henceforth, we refer to our proposed model as D3RM, an abbreviation for Discrete Denoising Diffusion Refinement Model."}, {"title": "IV. EXPERIMENTS", "content": "We trained and evaluated our models with the MAESTRO (V3) dataset [22], utilizing the provided {train/valid/test} splits. To mea-sure the onset and offset accuracy, we employed standard transcrip-tion metrics, including a 50ms onset threshold and either a 50ms or 20% of note length threshold for the offset. We utilized the mir_eval package [23] for this purpose."}, {"title": "B. Model Configuration", "content": "Each piano roll state is represented as an embedding vector of size 4. The window size for each of the 8 NA layers is uniformly set to 3, with dilation factors of [1,2,4,8,1,2,4,8]. The hidden dimension of the NA layers is set to 48. Diffusion timesteps are configured with T = 100, where for the training stage \u03b1 decreases linearly from 1 to 0, and increases linearly from $ \\gamma_{\\tau} $ = 0 to varying values of $ \\gamma_{\\tau} $ = {0.4, 0.6, 0.8, 0.9, 1.0}. Note that when $ \\gamma_{\\tau} $ = 1.0, $ \\beta_{\\tau}$ remains 0.0 throughout the diffusion process, meaning no substitution occurs, which is equal to the absorbing state. The model does not learn to refine the previous predicted states in this case. The auxiliary loss weight is set to 0.0005, following [20].\nThe model is optimized using the AdamW optimizer with \u03b2s = [0.9, 0.96]. The learning rate begins at 0.00045 after a warmup period of 1000 steps, and is reduced by a factor of 0.8 when the diffusion loss does not improve for 25k steps, following [20]. The acoustic encoder has total 1.5M trainable parameters, and the denoising decoder has 477K parameters. Note that during the inference stage, to reduce overall sampling time, we save the output feature from the encoder at the first diffusion timestep. This saved feature are then fed into the decoder during the remaining timesteps, requiring only a single initial forward pass through the encoder. This significantly accelerates inference, as the lightweight denoising decoder processes data quickly on its own. Model is trained for 400K steps with batch size of 8. We train the model on a single RTX 4090 GPU for 7 days. Inference for the test set took around 3.5 hours."}, {"title": "V. RESULTS", "content": "As depicted in Table I, our D3RM outperforms the baseline in terms of note F1 score, both with and without offsets. Compared to DiffRoll, D3RM utilizes much fewer parameters and shows a significant improvement. D3RM also outperforms most previous piano transcription models, except for the recent model [2]."}, {"title": "B. Ablation Studies", "content": "We conducted additional experiments on various settings, as shown in Table II. The top half of the table examines the effects of using the absorbing state sampling (AS sampling), pretrained encoder versus jointly training the encoder from scratch (Enc. Init.), conditioning encoder features via cross-attention or in-context conditioning. The bottom half investigates the impact of linearly increasing $ \\gamma_{\\tau}$ from 0 to varying values of $ \\gamma_{\\tau}$. In-context conditioning concatenates the output feature of the acoustic encoder with the noisy piano roll along the channel-wise direction before feeding it into the denoising decoder.\nWhen using the absorbing state ($ \\gamma_{\\tau}$ = 1.0) for the training stage, it demonstrated poorer performance (F1 97.55 \u2192 96.82). This suggests that allowing the model to learn transitions between states ($ \\gamma_{\\tau}$ < 1.0) enables an effective refinement of the piano roll. Our proposed refinement method (AS sampling) also improved the performance (F1 97.21 \u2192 97.55) which shows the effectiveness of selecting absorbing state transition matrix for the sampling stage. For the rest of the settings, including training the encoder parameter from scratch, in-context conditioning, and varying $ \\gamma_{\\tau}$ = {0.4, 0.6, 0.8}, the results showed only subtle and unremarkable differences."}, {"title": "C. Empirical Analysis", "content": "To investigate the difference between the models with and without refinement capability ($ \\gamma_{\\tau}$ = 0.9 vs. $ \\gamma_{\\tau}$ = 1.0), we visualized the errors of each model and identified the differences. We hypothesized that in the case of the model that has no refinement capability, false note frames predicted during the denoising process may not be corrected. Indeed, we observed errors in the predictions of the model $ \\gamma_{\\tau}$ = 1.0, which we suspect is a result of this issue. In Figure 3, the predictions for $ \\gamma_{\\tau}$ = 1.0 show several false positive errors, where onsets are located near other note onsets (illustrated with red lines in the last row). These results suggest that the refinement capability is beneficial for correcting false predictions during the diffusion process."}, {"title": "VI. CONCLUSION", "content": "We demonstrate the potential of discrete denoising diffusion prob-abilistic models for discriminative tasks by adding a lightweight NA-based denoising module on top of a pre-existing acoustic model. Additionally, we propose a novel inference strategy which enhances the refinement capability of discrete diffusion models. The results in piano transcription validate the effectiveness of this approach, which could potentially be extended to other domains or even generative tasks. Future work will focus on developing a robust objective metric that better captures the model's refinement capability. For the discrete token transition, alternative representations of tokens, such as note-level representations, may also be considered."}]}