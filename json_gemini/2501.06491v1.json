{"title": "Improving Requirements Classification with SMOTE-Tomek Preprocessing", "authors": ["Barak Or"], "abstract": "This study emphasizes the domain of requirements engineering by applying the SMOTE-Tomek preprocessing technique, combined with stratified K-fold cross-validation, to address class imbalance in the PROMISE dataset. This dataset comprises 969 categorized requirements, classified into functional and non-functional types. The proposed approach enhances the representation of minority classes while maintaining the integrity of validation folds, leading to a notable improvement in classification accuracy. Logistic regression achieved 76.16% \u00b1 2.58%, significantly surpassing the baseline of 58.31% \u00b1 2.05%. These results highlight the applicability and efficiency of machine learning models as scalable and interpretable solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "Requirements engineering (RE) is a cornerstone of the software development lifecycle, translating stakeholder needs into actionable system specifications [1], [2]. Accurate and efficient requirement classification is essential to ensure project clarity, prioritization, and goal alignment. Requirements are typically categorized into functional, non-functional, and subtypes [3]\u2013[6]. A survey revealed that over 60% of failed projects neglected non-functional requirements, underscoring the critical importance of effective classification [7]. Despite its significance, requirement classification remains a largely manual process, prone to inconsistencies, inefficiencies, and scalability challenges [8].\nOver the years, researchers have explored various approaches to automate requirement classification. Early rule-based systems were among the first attempts, offering interpretable but inflexible solutions that were labor-intensive to maintain [9]. The advent of machine learning (ML) brought more adaptive models, demonstrating moderate success, particularly in structured datasets [3], [6], [10]. However, these methods often face challenges with class imbalance, a prevalent issue in real-world datasets, including the widely used PROMISE dataset for requirements engineering [11]. This imbalance skews model predictions toward majority classes, undermining performance on minority classes.\nOver the past decade, deep learning (DL) has achieved transformative breakthroughs across multiple domains, including natural language processing (NLP), computer vision, and time series analysis. These advancements are primarily driven by the unprecedented representational power of deep neural networks, which leverage millions, and in some cases billions, of trainable parameters to extract and model intricate, high-dimensional patterns from large-scale datasets [12].\nEqually significant are the advances in time series analysis, where DL models have demonstrated remarkable capabilities in tasks such as motion sensing classification and physical quantity estimation [13]\u2013[16]. These innovations highlight DL's ability to capture temporal dependencies and complex properties in sequential data.\nIn computer vision, architectures such as ResNet [17] and Vision Transformers (ViT) [18] have revolutionized image classification, object detection, and semantic segmentation, achieving state-of-the-art accuracy while enabling applications that were previously unattainable.\nIn NLP, DL models such as BERT [19] and GPT-3 [20] have set new standards in tasks like language translation, text summarization, and question answering, surpassing traditional methods and enabling more nuanced understanding and generation of human language [21]\u2013[25]. These models excel in NLP tasks due to their ability to capture contextual relationships within text using self-attention mechanisms. They are particularly effective at modeling semantic intricacies in textual data, enabling strong performance in complex classification tasks. However, these methods require not only substantial computational resources for fine-tuning and inference but also access to massive amounts of high-quality data to achieve their full potential. This reliance on extensive datasets and powerful hardware often makes them less practical for small-scale or resource-constrained projects, where data availability and computational capacity may be limited.\nTo address these challenges, this study integrates the Synthetic Minority Oversampling Technique (SMOTE) [26] with Tomek Links (SMOTE-Tomek) [27], providing a robust preprocessing solution for imbalanced datasets. SMOTE oversamples minority classes by generating synthetic samples through interpolation, effectively enhancing the diversity of underrepresented classes while mitigating overfitting, a common issue with random oversampling methods. Complementing this, Tomek Links identifies and removes noisy or borderline samples, improving class separability by reducing overlapping data points between classes. This dual-action preprocessing strategy results in cleaner and more balanced training datasets, enabling ML models to better learn decision boundaries for minority classes.\nThe proposed approach is particularly suited to text-based datasets like the PROMISE dataset, where class imbalance and noise frequently undermine classification performance. By addressing these issues, SMOTE-Tomek creates an optimal foundation for training modern ML models, which can achieve"}, {"title": "II. LEARNING METHOD", "content": "This section provides an overview of the methodology employed in this study, detailing the dataset, class imbalance challenges, and the preprocessing techniques applied to address them. It outlines the evaluated ML models, the implementation of the SMOTE-Tomek approach for data balancing, and the use of K-fold cross-validation to ensure robust and unbiased performance assessment."}, {"title": "A. Dataset", "content": "The study utilizes the expanded PROMISE dataset, a diverse collection of 969 software requirements extracted from Software Requirements Specification (SRS) documents using the Google search engine [11]. The dataset contains 444 functional requirements (45.8%) and 525 non-functional requirements (54.2%), distributed across 12 distinct categories. These categories represent a spectrum of requirements in software engineering, such as Security (SE), Usability (US), Portability (PO), and Performance (PE). Despite its comprehensiveness, the dataset exhibits notable class imbalance. For example, the Portability category is severely underrepresented, comprising only 12 requirements (1.24%), while the most prevalent non-functional class, Security, includes 125 requirements (12.9%).\nThis disparity is further amplified in the broader classification between functional and non-functional requirements, which affects the performance and generalizability of ML models."}, {"title": "B. Pre-Processing", "content": "Transforming unstructured text into a numerical representation suitable for ML algorithms is a fundamental preprocessing step in many NLP tasks. The PROMISE dataset comprises individual sentences, rather than full documents. The common Term Frequency-Inverse Document Frequency (TF-IDF) remains a highly effective vectorization technique [29], [30]. TF-IDF quantifies the importance of a term within a sentence while considering its prevalence across the entire dataset of sentences. By emphasizing terms that are frequent within a specific sentence but rare across the dataset, TF-IDF captures features that are contextually meaningful and relevant to the classification task. Despite the brevity of the textual units, TF-IDF preserves the semantic significance of terms."}, {"title": "C. Class Imbalance Challenge", "content": "Class imbalance poses a critical challenge in training ML models, as they tend to prioritize majority classes, resulting in poor generalization and suboptimal performance on minority classes [31]. Such biases can significantly impact the usability and reliability of classification models in practical applications, where correctly identifying minority classes is often essential. Balancing the training set ensures that the model learns to give equal attention to all classes, preventing it from being overly biased toward majority classes."}, {"title": "D. SMOTE-Tomek", "content": "Among various methods to address data imbalance, SMOTE-Tomek is a hybrid technique that combines the benefits of oversampling and data cleaning. The SMOTE addresses the imbalance by generating synthetic samples for minority classes, ensuring the training set has a more equitable class distribution. However, SMOTE alone can introduce noise by oversampling borderline or overlapping samples. Tomek Links complement SMOTE by identifying and removing borderline or ambiguous samples that may hinder model training. Together, SMOTE-Tomek enhances data quality while balancing the class distribution in the training phase. Unlike random oversampling, which duplicates existing minority class samples, SMOTE creates new synthetic samples based on linear interpolation between existing samples.\nIn practical applications of NLP tasks, where textual data is complex and often imbalanced, SMOTE-Tomek emerges as a robust and effective solution. The synthetic examples generated by SMOTE are not direct textual entities but instead constitute feature vectors that represent the underlying characteristics of the minority class, as presented in Table II. These vectors serve as abstract representations, enabling the model to internalize the structural properties of underrepresented classes without requiring additional annotated textual data.\nThe SMOTE-Tomek process is described in the following steps:\n1) Generate Synthetic Samples for Minority Classes: For each sample $x_i$ belonging to the minority class, synthetic samples are generated using linear interpolation:\n$x_{synthetic} = x_i + \\lambda (x_{nn} - x_i)$,\nwhere $x_{nn}$ is a randomly selected nearest neighbor of $x_i$ from the same minority class, and $\\lambda \\in [0, 1]$ is a random scalar.\n2) Identify Tomek Links: A Tomek Link is defined as a pair of samples $x_i$ and $x_j$ thatare that are mutual nearest neighbors, meaning $x_i$ is the closest sample to $x_j$ and vice versa. Also, $x_i$ and $x_j$ are belonging to different classes, with one from the minority class and the other from the majority class. The equation for identifying Tomek links is give by:\n$d(x_i, x_j) = min\\{d(x_i, x_k) | x_k\\}$,\nwhere $d(x_i, x_j)$ is the distance (commonly Euclidean) between $x_i$ and $x_j$ and $x_k \\in$ different class from $x_i$.\n3) Remove Majority Class Samples: For each identified Tomek Link, remove the sample belonging to the majority class to enhance class separation and reduce noise."}, {"title": "E. Stratified K-fold Cross-Validation Method", "content": "Stratified K-fold cross-validation provides significant advantages over traditional train/test splits, particularly in handling class imbalance. By dividing the dataset into K equal folds and ensuring that each fold is used for validation exactly once, this method maximizes the use of available data. Through stratified sampling, it maintains the proportional representation of all"}, {"title": "F. Trainig Algorithm", "content": "The proposed method, as outlined in Algorithm 1, emphasizes the integration of stratified K-fold cross-validation to ensure class proportions are preserved across folds. Importantly, SMOTE-Tomek is applied solely to the training folds to prevent data leakage, thereby preserving the integrity of"}, {"title": "G. Classical ML Models", "content": "The following ML models were evaluated in this study to capture diverse classification perspectives.\n\u2022 Decision Tree (DT): A tree-based algorithm that splits the data into subsets based on feature values, creating a hierarchy of decisions to classify data efficiently.\n\u2022 Random Forest: An ensemble method combining multiple DTs, where each tree votes, and the majority decision is taken for robust and accurate predictions.\n\u2022 Support Vector Machine (SVM) with Linear Kernel: A linear classifier that finds the hyperplane maximizing the margin between classes, optimized for linearly separable data.\n\u2022 SVM with RBF Kernel: A nonlinear classifier that maps data into a higher-dimensional space using the radial basis function (RBF) kernel for better class separation.\n\u2022 Naive Bayes: A probabilistic model based on Bayes' theorem, assuming feature independence, and commonly used for text classification tasks.\n\u2022 Logistic Regression: A statistical model that predicts the probability of an outcome using a logistic function.\n\u2022 K-Nearest Neighbors (KNN): A non-parametric algorithm that classifies samples based on the majority class of their k-nearest neighbors in the feature space."}, {"title": "III. RESULTS AND DISCUSSION", "content": "This section evaluates the performance of various ML models for requirement type classification, with a focus on the impact of SMOTE-Tomek preprocessing. It outlines the error metrics used, compares the baseline performance of the models to their performance with SMOTE-Tomek integration, highlights the interpretability of the logistic regression model, and discusses the achieved improvements and their implications."}, {"title": "A. Error Metrics", "content": "Let TP, FP, TN, and FN denote the true positives, false positives, true negatives, and false negatives, respectively. These metrics serve as the foundational components for evaluating the performance of classification models.\nPrecision quantifies the proportion of correctly predicted positive cases out of all predicted positive cases:\n$Precision = \\frac{TP}{TP + FP}$,\nproviding an indication of the model's reliability in identifying true positive instances among its positive predictions.\nRecall, also referred to as sensitivity, evaluates the proportion of actual positive cases that the model correctly identifies:\n$Recall = \\frac{TP}{TP+FN}$,\nmeasuring the model's effectiveness in capturing all relevant positive instances.\nF1-Score integrates precision and recall into a single performance metric by calculating their harmonic mean:\n$F1-Score = \\frac{2\\cdot TP}{2\\cdot TP+FP + FN}$,\noffering a balanced evaluation of the model's precision and recall.\nAccuracy assesses the overall correctness of the model by accounting for both positive and negative classifications:\n$Accuracy = \\frac{TP+TN}{TP+TN + FP + FN}$,\nproviding a general measure of the model's performance across all classes.\nMatthews Correlation Coefficient (MCC) offers a comprehensive evaluation of binary classification quality, particularly under imbalanced data conditions. It accounts for all elements of the confusion matrix (TP, TN, FP, FN) and is defined as:\n$MCC = \\frac{(TP \\cdot TN) \u2013 (FP \\cdot FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$,"}, {"title": "B. Baseline Performance Without SMOTE-Tomek", "content": "The experimental results, detailed in Table IV and illustrated in Figure 3, reveal that the Linear SVM model consistently surpassed all other classifiers across the evaluated metrics. It achieved the highest performance, with a mean accuracy of 71.10%, precision of 68.25%, recall of 71.10%, F1-score of 66.47%, and an MCC of 0.5977.\nAmong the KNN models, configurations with k=3 and k=5 exhibited robust performance, with mean accuracies of 67.69% and 68.42%, respectively, and MCC values closely aligned with the Linear SVM, indicating their effectiveness on this dataset. The Gradient Boosting classifier also demonstrated competitive results, with a mean accuracy of 66.98% and an MCC of 0.5344, indicating its ability to model complex decision boundaries. Conversely, ensemble methods such as AdaBoost and Random Forest exhibited suboptimal performance, with AdaBoost showing particularly low precision of 28.21% and MCC of 0.1963, reflecting limitations in handling class imbalances. Traditional classifiers such as LR and DTS achieved moderate performance levels but were consistently outperformed by more advanced approaches like SVM and KNN."}, {"title": "C. Enhanced Performance Using SMOTE-Tomek", "content": "Following the application of the SMOTE-Tomek preprocessing technique, the LR model demonstrated significant performance improvements, as shown in Figure 4 and Table V. The model achieved a mean accuracy of 76.16% \u00b1 2.58%, a mean precision of 75.31% \u00b1 4.30%, a mean recall of 76.16% \u00b1 2.58%, a mean F1-score of 74.34% \u00b1 2.93%, and a MCC of 0.67 \u00b1 0.03. Performance was further enhanced, with accuracy improving by approximately 1%, through grid search optimization using C=10, L2 penalty, and saga solver. Here, C controls the trade-off between model complexity and training accuracy, with C=10 favoring a closer fit to the data. The L2 penalty regularizes model coefficients to prevent overfitting, while the saga solver is an efficient optimization algorithm designed for large datasets [32].\nIn comparison, Naive Bayes and Gradient Boosting also performed robustly, with mean accuracies exceeding 70%"}, {"title": "D. Impact of SMOTE-Tomek on Logistic Regression Interpretability", "content": "Understanding feature importance is critical in requirements classification tasks, as it provides actionable insights into the linguistic patterns that define requirement categories. Logistic regression (LR), as an interpretable model, offers a transparent mechanism for analyzing these relationships through its coefficients.\nThe coefficients presented in Table VI (LR with SMOTE-Tomek) and Table VII (LR without SMOTE-Tomek) highlight the influence of key features (words) in predicting requirement types. These coefficients quantify the strength and direction of association between words and their respective types. Positive coefficients indicate strong alignment with a particular requirement type, while the magnitude reflects the importance of the word in driving classification decisions. As an iterpretable model, LR benefits considerably from preprocessing techniques that address noise and class imbalance. SMOTE-Tomek enhances model performance, stabilizes, and refines coefficients.\nThe results reveal critical insights into the predictive influence of words and the effect of preprocessing on model behavior. For example, while the word \"seconds\" in the \"Functional\" type exhibited an extreme negative coefficient (-16.59) without preprocessing, SMOTE-Tomek balanced the dataset and reduced this coefficient to 6.55, ensuring a more consistent representation. Similarly, new influential words such as \"interface\" in the \"Look-and-Feel\" type emerged after preprocessing, indicating that SMOTE-Tomek enhances the visibility of class-distinguishing features.\nThe coefficients illustrate the inherent relationship between words and requirement types. High coefficients for words such as \"24x7\" in \"Availability\" and \"encrypted\" in \"Security\" highlight their criticality in these types. These insights can guide the development of keyword-based heuristics and inform domain-specific feature engineering."}, {"title": "E. Discussion", "content": "The results achieved in this study, with the highest accuracy reaching 76.16% using LR after applying SMOTE-Tomek, highlight both the potential and the limitations of modern ML models for requirements classification. While it demonstrates improvement over the baseline, it reveals the challenges imposed by data scarcity and inherent class imbalance in the PROMISE dataset. These challenges persist despite preprocessing techniques, emphasizing the need for a larger, more representative dataset to better capture the diversity and nuances of requirement types."}, {"title": "IV. CONCLUSIONS", "content": "We demonstrated the potential of integrating SMOTE-Tomek preprocessing with modern ML models to address class imbalance in requirements classification. LR, in particular, exhibited the most substantial improvement, achieving 76.16% \u00b1 2.58% accuracy and a 7% increase in the MCC compared to the baseline. The interpretability of LR allowed for an analysis of feature importance, while SMOTE-Tomek preprocessing stabilized coefficients, reduced noise, and improved class representation, thus enabling more robust and meaningful predictions.\nThese findings emphasize the practicality and scalability of modern ML models for imbalanced text datasets, particularly in resource-constrained environments.\nThe methodology is applicable to other domains, such as legal or healthcare document analysis, where similar challenges exist. Future work will explore larger datasets, advanced feature representations, and hybrid approaches combining lightweight models with deep learning techniques to further enhance performance and generalizability."}]}