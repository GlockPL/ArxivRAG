{"title": "Non-Myopic Multi-Objective Bayesian Optimization", "authors": ["Syrine Belakaria", "Alaleh Ahmadianshalchi", "Stefano Ermon", "Barbara Engelhardt", "Janardhan Rao Doppa"], "abstract": "We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO.", "sections": [{"title": "1 Introduction", "content": "Many engineering design and scientific applications involve performing a sequence of experiments under a small-resource budget to optimize multiple expensive-to-evaluate objective functions. Consider the example of searching nanoporous materials optimized for hydrogen-powered vehicles. In this real-world problem, we need to perform physical lab experiments to make a candidate material and evaluate its"}, {"title": "2 Problem Setup and Background", "content": "This section formally defines the problem setting and provides the background necessary for our solution approach.\nMulti-objective optimization (MOO). Let $X \\subset \\mathbb{R}^d$ be the input space of $d$ design variables, where each candidate input $x \\in X$ is a $d$-dimensional vector. Let ${f_1,\\dots, f_k}$ with $K > 2$ represent the black-box objective functions defined over the input space $X$, where $f_1(x),\\dots, f_k(x) : X \\rightarrow \\mathbb{R}$. We denote the function evaluation at $x$ as $y = [y_1,\\dots, y_K]$, where $y_i = f_i(x)$ for all $i \\in {1,\\dots, K}$. For example, to optimize nanoporous materials for hydrogen-powered vehicles, $x$ represents a candidate material, $f_1(x)$ and $f_2(x)$ corresponds to a physical lab experiment to evaluate the volumetric and gravimetric hydrogen uptake of the material. WLOG, we assume maximization for all $K$ objective functions.\nAn input $x$ Pareto-dominates another input $x'$ if and only if $\\forall i : f_i(x) \\geq f_i(x')$ and $\\exists j : f_j(x) > f_j(x')$. The optimal solution of the MOO problem is a set of inputs $X^* \\subset X$ such that no input $x \\in X \\backslash X^*$ Pareto-dominates another input $x' \\in X^*$. The set of input solutions $X^*$ is called the optimal Pareto set, and the corresponding set of function values $Y^*$ is called the optimal Pareto front. In SED, we select one input for evaluation in each iteration, and our goal is to uncover a high-quality Pareto front while minimizing the total number of expensive function evaluations. A typical measure to evaluate the quality of a Pareto front is the Pareto hypervolume (PHV) indicator Zitzler and Thiele [1999] of the Pareto front with respect to a reference point $r$.\nNon-myopic MOO problem. Our goal is to approximate the optimal Pareto front for a MOO problem within a maximum experimental budget $T$, which is typically small (i.e., finite-horizon SED). This problem can be formulated as:\n$\\max_{X \\in P(X)} \\max_{x \\in X} f_1(x) \\dots f_k(x), \\quad s.t. |X| = T,$"}, {"title": "3 Related Work", "content": "Most of the prior work on acquisition functions in BO falls under the myopic category. There is less work on non-myopic acquisition functions with a focus on single-objective BO.\nMyopic single-objective BO. Recent advances in single-objective BO have predominantly focused on refining and extending traditional acquisition functions. The most notable techniques Garnett [2023], Wang and Jegelka [2017], Hern\u00e1ndez-Lobato et al. [2014], Ament et al. [2024] use Gaussian processes as surrogate models.\nNon-myopic single-objective BO. Non-myopic approaches in single-objective BO Jiang et al. [2020a], Lam and Willcox [2016], Gonz\u00e1lez et al. [2016], Ginsbourger and Le Riche [2009], Osborne and Osborne [2010], Osborne et al. [2009] represent an emerging direction aimed at overcoming the limitations of myopic acquisition functions, which typically optimize for immediate gains. These lookahead methods consider the future impact of current decisions Wu and Frazier [2019], Lee et al. [2021], thereby enhancing long-term optimization outcomes when the experimental resource budget is small.\nMulti-objective BO. When compared to single-objective BO, multi-objective BO (MOBO) has received relatively less attention. The predominant methods for designing MOBO strategies include information-theoretic methods Tu et al. [2022], Hern\u00e1ndez-Lobato et al. [2016], Belakaria et al. [2019], Suzuki et al. [2020], Garrido-Merch\u00e1n and Hern\u00e1ndez-Lobato [2021], uncertainty based methods Belakaria et al. [2020b], hypervolume-based methods Konakovic Lukovic et al. [2020], Daulton et al. [2020], Ahmadianshalchi et al. [2024a], and scalarization-based methods Daulton et al. [2022], Knowles [2006]. The joint entropy search for MOBO (JESMO) algorithm Tu et al. [2022] aims to maximize information gain about the optimal Pareto front by evaluating inputs that maximally reduce its uncertainty. The expected hypervolume improvement (EHVI) method Daulton et al. [2020] extends the concept of EI from a single-objective to a MOO problem by assessing the potential improvement in hypervolume from new input evaluations. Several efforts to extend these approaches to handle constraints and preferences have resulted in more general frameworks that accommodate complex real-world decision-making scenarios Garrido-Merch\u00e1n and Hern\u00e1ndez-Lobato [2019], Belakaria et al. [2020a], Ahmadianshalchi et al. [2024b].\nNon-myopic MOBO. Despite advances in myopic strategies for MOBO, a notable knowledge gap remains. There is no prior work on non-myopic acquisition functions for MOBO problems with a finite horizon. A recent approach proposed for settings with decoupled function evaluation Daulton et al. [2023] considers a one-step lookahead only and thus lacks strategic foresight. Our goal is to precisely fill this critical gap in the current state of knowledge."}, {"title": "4 Non-Myopic Multi-Objective BO", "content": "In this section, we provide details of our proposed approach for non-myopic MOBO. We first discuss the challenge of maintaining the validity of the Bellman optimality principle and, by consequence, the ability to use the Bellman equation and its lower-bound to solve the SED problem in the multi-objective setting. We then discuss design choices for the marginal utility function that enables the effective use of the Bellman equation in our setting. We finally provide the details of the non-myopic acquisition functions we propose, their computational challenges, and practical algorithms."}, {"title": "4.1 Challenges of Non-Myopic MOBO", "content": "For the Bellman optimality principle to hold and for the optimal policy to be computable by solving the Bellman equation, the reward function has to satisfy several criteria. In our problem setting, we refer to the reward and the marginal gain in utility defined in equation 2 interchangeably. We state below some of the reward function requirements that are not straightforward to satisfy when dealing with a multi-objective problem:\n\u2022 Scalarization: the reward has to be defined as a single scalar value and not a vector of values Boutilier et al. [1999], Roijers et al. [2013]. In single-objective optimization, the reward is defined as the improvement in the best-found function value Lam et al. [2016], Jiang et al. [2020a]. However, in MOBO problems, every input evaluation leads to a vector of values representing a trade-off where some of the objectives may improve while others degrade.\n\u2022 Monotonicity: the reward has to be a monotonically increasing function with respect to the quality of the actions (i.e., better actions lead to higher rewards). This monotonic property ensures that the optimization trajectory is aligned with improving performance Roijers et al. [2013].\n\u2022 Additivity: the reward must satisfy the additivity condition, i.e, the total reward is a sum of the rewards obtained at each step of the decision-making process Boutilier et al. [1999], Roijers et al. [2013], Van Moffaert and Now\u00e9 [2014]. Additivity is essential for the recursive decomposition of the sequential decision process.\nIt is important to note that the choice of the scalarization approach plays a key role in preserving the monotonicity and additivity conditions. In MOBO problems, the challenge lies in combining both the improvement and degradation incurred in several objectives into a single, quantifiable metric that accurately reflects the quality of an input in terms of Pareto dominance regardless of trade- offs in individual objectives. In broader applications of the Bellman equation (e.g., multi-objective reinforcement learning), linear scalarization is the most common technique known to preserve the additivity of the reward, where each objective is multiplied by a predetermined nonnegative weight"}, {"title": "4.2 Our Proposed Non-Myopic MOBO Method", "content": "In this section, we provide the details of the design choices of our utility function that enable the application of the Bellman equation in the MO setting. We then provide the details of the different acquisition functions we propose for non-myopic MOBO."}, {"title": "4.2.1 Utility for Multi-Objective Non-Myopic Setting", "content": "The hypervolume (HV) quality indicator provides a scalar measure of a particular Pareto front $Y$. HV is the volume between a reference point and the Pareto front. This measure has been of particular interest in MOBO problems because it is known to be strictly monotonically increasing with regard to Pareto dominance. However, the HV measure does not satisfy the additivity condition necessary for Bellman's optimality principle Van Moffaert et al. [2013]. In this work, we propose to use the hypervolume improvement (HVI) to scalarize our multiple single-objective rewards defined as the improvement of each of the functions. This approach quantifies the quality of a new point $y$ by the volume of the objective space that is newly encompassed by extending the Pareto front to include the new point.\n$HVI(y|Y) = HV(Y \\cup y) \u2013 HV(Y)$"}, {"title": "4.2.2 Proposed Acquisition Functions", "content": "Given our choice of reward as the HVI, we can define our SED problem using the Bellman equation. We formally define the marginal gain in the utility of our multi-objective setting as follows:\n$u(y|x, D_t) = HVI(y|Y).$\nSimilar to the single-objective case, computing the optimal policy by fully solving the Bellman equation to optimality is intractable even for a moderately large horizon $T \u2013 t$ Jiang et al. [2020b]. An effective approach to navigating this complexity is to approximate the optimal policy by optimizing the lower- bound on the Bellman equation (Equation 8). In what follows, we denote by $X$ the full remaining horizon at iteration $t$ with $|X| = T - t$, $x$ the next input to evaluate and the first input in the horizon $X$, and $X'$ the lookahead horizon with $|X'| = T - t - 1$ and $X = {x, X'}$. The selection of the next input to evaluate while accounting for the lookahead horizon (the remaining sequence of inputs in the horizon) can be achieved by maximizing the acquisition function $a(x|D_t)$ (Equation 9). By defining the marginal gain in utility as the HVI, we rewrite the acquisition function as:\n$a(x|D_t) = EHVI(x|D_t) + \\max_{X':|X'|=T-t-1} Ey[BEHVI(X'|D_t \\cup (x, y))]$"}, {"title": "5.1 Results and Discussion", "content": "Figure 1 shows the hypervolume versus BO iterations for NMMO-Joint with lookahead horizon $H \\in {4,8}$ and BINOM with lookahead horizon $H = 4$ against existing state-of-the-art baselines.\nNon-myopic methods perform better than myopic ones. The non-myopic methods outperform the state-of-the-art myopic MOBO algorithms on all six real-world MOO problems (Figure 1). These results highlight the capability of non-myopic methods in maximizing the hypervolume when the experimental budget is small (i.e., 65 BO iterations).\nNon-myopic methods with different horizons. From our results, we often observe a slight decrease in performance with increasing horizon values in non-myopic MOO. This trend can be primarily attributed to the increased complexity and model uncertainty as the decision horizon extends. A longer horizon necessitates the consideration of a larger set of future outcomes and their interactions, which not only complicates the optimization task but also introduces greater uncertainty and potential for errors in prediction. Consequently, these accumulated errors can degrade the optimization strategy, making it challenging for the algorithm to effectively balance short-term gains against long-term goals, and negatively impacting performance. This finding highlights the challenges in implementing long-term strategic planning in complex optimization environments. These challenges can be seen in the NMMO-Joint and BINOM methods run on each benchmark with increasing lookahead horizon $H \\in {2,4,6,8}$ (Appendix Figure 2 and Figure 3).\nComputational runtime comparison. In our comparative analysis of myopic and non-myopic MOO methods, there are computational complexity and optimization performance trade-offs. Myopic MOO methods, due to their focus on immediate gains, have lower computational complexity, which translates into faster runtimes (Table 1). However, this computational efficiency comes at the cost of performance. Our results indicate that, while myopic methods are faster, they underperform in comparison to their non-myopic counterparts in limited-budget settings. Hence, while non-myopic approaches require more computational resources, their better optimization results justify the additional complexity.\nAblation study of information-gain within BINOM. In addition to the EHVI acquisition function, we incorporated information-gain acquisition functions, specifically MESMO and JESMO, into our proposed BINOM approach. Even though information gain based scalarization does not always satisfy the monotonicity condition as discussed in section 7.3, These instantiations of our framework provide a complementary approach to the hypervolume-based strategies. The results, provided in the Appendix demonstrate that the non-myopic methods maintain strong performance across different instantiations of BINOM, highlighting the versatility and effectiveness of our framework. The results also show that the HVI instantiation performs better in most cases."}, {"title": "6 Summary", "content": "This paper introduced a novel set of non-myopic methods for multi-objective Bayesian optimization (MOBO), to specifically address the problem of finite-horizon sequential experimental design (SED) for MOO problems. By addressing the difficulties related to scalarization, monotonicity, and additivity of the reward function typically encountered in MOO scenarios, we successfully adapted the Bellman optimality principle for MOBO by adopting a new approach to scalarization using hypervolume improvement (HVI). Our proposed methods, including the Nested, Joint, and BINOM acquisition functions, represent the first non-myopic acquisition functions tailored for MOBO problems. Our experiments on multiple real-world problems, show that our methods show significant improvements by consistently outperforming the state-of-the-art myopic approaches in limited-budget settings."}]}