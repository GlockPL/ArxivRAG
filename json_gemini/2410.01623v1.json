{"title": "FIRA: CAN WE ACHIEVE FULL-RANK TRAINING OF LLMS UNDER LOW-RANK CONSTRAINT?", "authors": ["Xi Chen", "Kaituo Feng", "Changsheng Li", "Xunhao Lai", "Xiangyu Yue", "Ye Yuan", "Guoren Wang"], "abstract": "Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. This raises a question: whether it is possible to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes? In this paper, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to achieve this goal. First, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. Based on this observation, we propose a norm-based scaling method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to enable full-rank training. In this way, we can preserve the low-rank constraint in the optimizer while achieving full-rank training for better performance. Moreover, we find that there are sudden gradient rises during the optimization process, potentially causing loss spikes. To address this, we further put forward a norm-growth limiter to smooth the gradient via regulating the relative increase of gradient norms. Extensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore, achieving performance that is comparable to or even better than full-rank training. For instance, our Fira can reduce the memory usage of optimizer states by 61.1%, while achieving improved performance for pre-training on the LLaMA 1B architecture. Notably, for pre-training on the LLaMA 7B architecture, our method uses an 8\u00d7 smaller rank than GaLore, yet outperforms it by a large margin.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable advancements in various domains (Achiam et al., 2023; Sima et al., 2023; Feng et al., 2024). While the substantial increase in model size contributes significantly to these advancements, it also introduces considerable memory bottlenecks, especially for optimizer states (Zhao et al., 2024a). For instance, pre-training a LLaMA-7B model from scratch \u00b9 requires at least 58 GB memory, allocated as follows: 14GB for loading"}, {"title": "2 RELATED WORK", "content": "Low-rank Adaptation. Low-Rank Adaptation (LoRA) has been introduced by Hu et al. (2022) as an efficient fine-tuning method for LLMs. The core idea of LoRA is to freeze the pre-trained weights and introduce trainable low-rank matrices as decomposed representations of the pre-trained weights. In this way, the memory usage of training LLMs could be saved. Recently, a variety of methods by extending LoRA have been proposed to further improve the performance (Zhang et al., 2023c; Wen & Chaudhuri, 2023; Xia et al., 2024; Zhang et al., 2023b; Dettmers et al., 2024). For instance, ReLoRA (Lialin et al., 2024) is proposed to extend the application of LoRA from fine-tuning to pre-training. However, it still requires full-rank warm-up training before low-rank training, which prevents achieving memory efficiency. It is worth noting that while LoRA based methods reduce memory usage by limiting training to a low-rank parameter subspace, they inevitably reduce representation capacity (Xia et al., 2024).\nGradient Projection. Recent works (Zhang et al., 2023b; Xia et al., 2024; Valipour et al., 2022) have indicated that LoRA may yield sub-optimal performance since its low-rank constraints in pa- rameters. Inspired by traditional projected gradient descent methods (Chen & Wainwright, 2015; Chen et al., 2019), GaLore (Zhao et al., 2024a) has been proposed recently to mitigate this prob- lem. It enables full-parameter learning under low-rank constraints by projecting the gradient into a low-rank subspace, reducing memory usage for optimizer states. However, while GaLore allows memory-efficient full-parameter training, it confines the gradient to a low-rank subspace, discarding the portion outside the subspace and resulting in significant information loss.\nSystem-Based Memory-Efficient Techniques. Many system-based techniques have been devel- oped to reduce memory usage in LLM training (Chen et al., 2016; Ren et al., 2021). However, most of these methods achieve memory efficiency by compromising either time or precision. Gradient checkpointing (Chen et al., 2016) is proposed to reduce memory usage by trading increased com- putational time for the re-computation of activations. Quantization (Dettmers et al., 2024) reduces memory consumption by using lower-bit data types, but at the cost of model precision. Memory offloading (Zhang et al., 2023a; Ren et al., 2021) reduces GPU memory usage by using non-GPU memory (e.g., CPU) as an extension. However, it introduces additional communication overhead, such as CPU-GPU transfer time. It's important to note that our proposed method is complementary to these approaches and can potentially be combined with them to further reduce memory usage."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 REGULAR FULL-RANK TRAINING", "content": "At time step t, we denote the full-rank weight matrix as $W_t \\in \\mathbb{R}^{m\\times n}$. The full-rank gradient can be represented as $G_t = \\nabla_w f_t(W_t) \\in \\mathbb{R}^{m\\times n}$, where f is the objective function. Then the regular full-rank training can be expressed as follows:\n$W_{t+1} = W_t - \\eta\\psi_t(G_t),$ (1)\nwhere \u03b7 is the learning rate, and $V_t$ is the gradient correction function of the optimizer (for vanilla SGD, $V_t(G_t) = G_t$). Instead of vanilla SGD, adaptive optimizers (e.g., Adam (Kingma & Ba, 2014), AdamW (Loshchilov & Hutter, 2019)) are usually employed to correct the raw gradient for improving the training performance. However, this typically requires additional memory for storing optimizer states used in gradient correction. For instance, Adam (Kingma & Ba, 2014) requires storing the optimizer states M and V, which consume 2mn of memory. The gradient correction process is as follows:\n$M_t = \\beta_1M_{t-1}+ (1 - \\beta_1)G_t,$ (2)\n$V_t = \\beta_2V_{t-1} + (1 - \\beta_2)G_t^2,$ (3)\n$\\psi_t(G_t) = \\frac{\\sqrt{1-\\beta_2^t}M_t}{\\sqrt{1-\\beta_1^t}\\sqrt{V_t}+ \\epsilon},$ (4)\nwhere all matrix operations are element-wise. \u03b2\u2081 and \u03b22 are Adam's hyper-parameters, and $e$ is a small constant (e.g., 1 \u00d7 10\u22128) used for numerical stability. Since this regular full-rank training typically consumes a large amount of memory for training LLMs, many representative low-rank training methods, e.g., LoRA (Hu et al., 2022) and Galore (Zhao et al., 2024a), have been proposed to reduce memory usage in recent years."}, {"title": "3.2 Low-RANK ADAPTATION", "content": "The basic idea behind LoRA (Hu et al., 2022) is to use low-rank matrices as decomposed repre- sentations of the pre-trained weights during training, in order to reduce memory usage. Formally, LORA freezes the full-rank weight matrix $W_o \\in \\mathbb{R}^{m\\times n}$ and incorporates two low-rank matrices At and Bt for training as:\n$W_t = W_o + B_tA_t,$ (5)\nwhere $B_t \\in \\mathbb{R}^{m\\times r}$, $A_t \\in \\mathbb{R}^{r\\times n}$, and the rank r < min(m, n). While LoRA reduces memory usage by limiting training to a low-rank subspace of the weight, it inevitably diminishes the representation capacity of the weight matrix Wt."}, {"title": "3.3 GRADIENT LOW-RANK PROJECTION", "content": "In contrast to LoRA, GaLore (Zhao et al., 2024a) utilizes a projection matrix $P_t \\in \\mathbb{R}^{m\\times r}$ to project the full-rank gradient $G_t \\in \\mathbb{R}^{m\\times n}$ to a low-rank gradient $R_t = P_t^TG_t \\in \\mathbb{R}^{r\\times n}$ (m < n)\u00b3. By doing so, the memory usage of optimizer states could be reduced. The parameter update in GaLore can be formulated as:\n$W_{t+1} = W_t - \\eta P_t\\psi_t(R_t),$ (6)\nwhere the projection matrix Pt can be obtained through singular value decomposition (SVD) of Gt and can be updated every T steps:\n$G_t = UV^T = \\sum_{i=1}^{r}\\sigma_iu_iv_i^T, P_t = [u_1, u_2, ..., u_r],$ (7)\nwhere ui is the i-th column vector of the left singular matrix U. By selecting the first r columns of matrix U that correspond to the largest singular values, the projection matrix Pt effectively cap- tures the most significant directions in the gradient space, leading to faster convergence (Zhao et al., 2024a). The optimal switching frequency T is usually set to be between 50 to 1000, and the addi- tional computational overhead introduced by SVD is negligible (< 10%), as stated in (Zhao et al., 2024a). Since Galore restricts the gradient in the low-rank subspace, the gradient information out- side this subspace is lost, leading to inferior performance."}, {"title": "4 PROPOSED METHOD", "content": "To achieve full-rank training under low-rank constraints, our framework, named Fira, consists of two important components: (i) a norm-based scaling method, enabling full-rank training by leveraging the scaling effects of adaptive optimizers; (ii) a norm-growth limiter, which restricts the growth of the gradient norm to prevent spikes in training loss. Next, we will elaborate on these two components."}, {"title": "4.1 NORM-BASED SCALING", "content": "The low-rank constraint makes it challenging to record complete optimizer states for correcting raw gradients in full-rank training. Fortunately, we find an interesting phenomenon in LLM training: the scaling factor at the matrix level remains similar from low-rank training to full-rank training. Based on this observation, we propose a norm-based scaling strategy that approximately corrects the raw gradient, similar to adaptive optimizers, thereby enabling full-rank training.\nChallenge Analysis. Given the difficulty of incor- porating trainable low-rank weights into LoRA to achieve full-rank weight training (Zhao et al., 2024a), we focus on investigating how to achieve full-rank gradient training by extending the gradient projection method, Galore, in this paper. In GaLore, the projec- tion matrix $P_t \\in \\mathbb{R}^{m\\times r}$ projects the full-rank gradient $G_t \\in \\mathbb{R}^{m\\times n}$ of the full-rank weight $W_t\\in \\mathbb{R}^{m\\times n}$, to the low-rank subspace gradient $R_t = P_t^TG_t \\in \\mathbb{R}^{r\\times n}$. The gradient outside this subspace can be represented as: $(I - P_tP_t^T)G_t = G_t - P_tP_t^TR_t$. In other words, the full-rank gradient Gt can be divided into two terms: $P_tR_t$ and $(G_t - P_tP_t^TR_t)$.\nIn GaLore, the optimizer states only store the infor- mation of Rt instead of Gt to realize the low-rank constraint. The term of $(G_t - P_tP_t^TR_t)$ is directly dis- carded in Galore due to the lack of corresponding op- timizer states of Gt for correction in optimizers. This would lead to significant information loss especially when r < dmodel, where dmodel = min(m, n) is the full-rank dimension of models. Intuitively, to capture the information of $(G_t - P_tP_t^TR_t)$, we can directly add it based on Eq. (6) as follows:\n$W_{t+1} = W_t - \\eta P_t\\psi_t(R_t) - \\eta(G_t - P_tP_t^TR_t).$ (8)\nWe denote the update strategy in Eq. (8) as GaLore-add. However, as illustrated in Figure 2, GaLore- add exhibits almost no improvement compared to updates using Eq. (6) in GaLore. This phe- nomenon primarily arises because the term of $(G_t - P_tP_t^TR_t)$ doesn't have corresponding optimizer states for gradient correction. As a result, the optimization of $(G_t - P_t P_t^TR_t)$ uses vanilla SGD, yielding sub-optimal outputs. Besides, in GaLore-add, $P_t\\psi_t(R_t)$ employs the Adam optimizer for training while $(G_t - P_t P_t^TR_t)$ employs vanilla SGD. This gradient misalignment may also account for the lack of noticeable improvement.\nSimilarity of Scaling Factor. To tackle this challenge, we propose the concept of the scaling factor, which is defined as follows:\n$\\Phi_t(R_t) = \\frac{||\\psi_t(R_t)||}{|| R_t ||},$ (9)\nwhere the scaling factor $t represents the magnitude of the correction applied by the adaptive opti- mizer to the gradient norm. Based on the scaling factor $t, we observe an interesting phenomenon during LLM training: the scaling factors at the matrix level exhibit a high degree of similarity between low-rank and full-rank training. As shown in Figure 3, sorting weight matrices by their"}, {"title": "Norm-based Scaling.", "content": "Inspired by this, we propose a norm-based scaling method that utilizes the scaling factor of a weight matrix in low-rank training as a substitute for the corresponding factor in full-rank training:\n$W_{t+1} = W_t - \\eta P_t\\psi_t(R_t) - \\eta\\Phi_t(R_t)(G_t - P_tP_t^TR_t).$ (10)\nBy Eq. (10), we can approximately correct $(G_t - P_tP_t^TR_t)$ as adaptive optimizers do, so as to achieve full-rank training under low-rank constraints.\nFurthermore, we can use a more fine-grained average of the scaling factor in Eq.(10), by considering each column in the weight matrix:\n$\\Phi_t(R_t)_i = \\frac{||\\psi_t(R_{t,:,i})||}{||R_{t,:,i} ||}, i = 1, 2, ..., n,$ (11)\nwhere $R_{t,:,i}$ is the i-th column of Rt, and $t(R_t)i$ is the i-th scaling factor."}, {"title": "4.2 NORM-GROWTH LIMITER", "content": "We find that there are suddenly sharp increases of the gradient during training, which could intro- duce loss spikes. As shown in Figure 4, Fira-w.o.-limiter (our method without using the proposed norm-growth limiter) experiences spikes in both gradient norm and training loss. In this section, we analyze the reasons for this issue and propose a norm-growth limiter which transforms abrupt gradient spikes into gradual, smooth increases.\nLoss Spike Analysis. There are two main reasons for the spikes: (i) Switching the projection matrix Pt in gradient projection methods would cause instability during training. As illustrated in Fig- ure 2, both GaLore and GaLore-add exhibit significant training loss spikes at integer multiples of T (i.e., the frequency of switching the projection matrix Pt). This instability occurs because, when switching projection matrices Pt, the optimizer retains states linked to the previous matrix, while the current input gradient uses a new projection matrix, leading to significant misalignment. Further- more, as shown in Figure 2, GaLore-add also exhibits training spikes, reinforcing our earlier claim that directly incorporating $(G_t - P_tP_t^TR_t)$ may introduce instability and hinder training; (ii) Maintain- ing the original direction of the raw gradient $(G_t - P_t^TR_t)$ may be insufficient for handling the sharp"}, {"title": "Addressing Loss Spikes.", "content": "To address this issue, a straightforward solution is to use gradient clipping techniques (Pascanu et al., 2013) to avoid loss spikes. However, clipping based on the absolute norm of gradient matrices fails to account for significant differences between them, leading to sub-optimal results. This point can be also verified in Figure 4 and Table 4. To this end, we propose a norm- growth limiter method that constrains the ratio of the current gradient norm to the previous step's norm to a fixed ratio y when the gradient norm increases:\nif $\\frac{||S_t||}{||S_{t-1}||} > \\gamma$ then $S_t \\leftarrow \\frac{||S_{t-1}||}{||S_{t}||}S_t,$ (12)\nwhere \u03b3 is a threshold ensuring that the rate of gradient growth does not exceed this value. $S_t =  _t(R_t)(G_t - P_tP_t^TR_t)$ is the corrected gradient by applying our norm-based scaling. This approach limits the magnitude of gradient norm increases, converting sudden spikes into gradual rises and thus preventing loss spikes. Moreover, by constraining the relative increase of each gradient matrix's norm, our method is more flexible than the absolute norm clipping. As illustrated in Figure 2 and Figure 4, Fira with our proposed limiter improves the optimization performance without significant spikes."}, {"title": "4.3 OVERALL ALGORITHM", "content": "We present the overall algorithm of Fira with Adam in Algorithm 1. Our main components, the norm-based scaling method and the norm-growth limiter, are straightforward to im- plement, requiring only 3 additional lines of code. Moreover, Fira is a plug-and-play framework which can be easily integrated into the train- ing process without requiring signif- icant modifications. The plug-and- play Pytorch-like pseudo-code of Fira is provided in Appendix A.4.\nIt's worth noting that Fira only introduces one parameter ||St-1|| for each weight matrix in the optimizer state, which is negligible, as shown in Table 1. Besides, in addition to the original hyper- parameters of optimizers and gradient projection methods, Fira only adds one hyper-parameter \u03b3 in the norm-growth limiter. The hyper-parameter y is set to 1.01 across all experiments, which consistently yields satisfactory results."}, {"title": "5 EXPERIMENTS", "content": "In this section, we validate the effectiveness of Fira in pre-training and fine-tuning tasks of LLMs. In our experiments, we denote our method using the strategy of Eq. (10) as Fira-matrix, and denote our method additionally using the column-wise strategy of Eq. (11) as Fira."}, {"title": "5.1 \u039c\u0395\u039cORY-EFFICIENT PRE-TRAINING", "content": "Experimental Setup. We follow the settings in Galore (Zhao et al., 2024a) to conduct the pre- training experiments. We compare Fira with GaLore (Zhao et al., 2024a), LoRA (Hu et al., 2022), ReLORA (Lialin et al., 2024), and full-rank training baselines with Adam optimizer (without gradi- ent projection) on the C4 dataset in the BF16 format. The settings of these baselines can be found in Zhao et al. (2024a). The dataset C4 is a colossal, cleaned version of Common Crawl's web crawl cor- pus, which is widely used in LLM pre-training (Raffel et al., 2020). Following Zhao et al. (2024a), we utilize LLaMA-based architectures equipped with RMSNorm and SwiGLU activations (Zhang & Sennrich, 2019; Shazeer, 2020; Touvron et al., 2023). As in Zhao et al. (2024a), our training pro- tocol excludes data repetition and spans a sufficiently large dataset, encompassing a diverse array of model sizes (60M, 130M, 350M, 1B). To guarantee a fair comparison, we employ the same learning rate 0.01 as used in GaLore and maintain the same rank r for each model size. The detailed settings of pre-training are provided in Appendix A.2. We use 8 A100 80G GPUs to conduct pre-training experiments.\nResult Analysis. As shown in Table 2, Fira consistently outperforms low-rank training baselines by a large margin under the same rank constraint, and even surpasses full-rank training. Following Zhao et al. (2024a), we estimate the memory reduction of the optimizer states via the same memory estimation method introduced in Zhao et al. (2024a). From Table 2, our Fira saves 61.1% memory usage of the optimizer states when pre-training the LLaMA 1B architecture compared to full-rank training, while Fira achieving better results. Compared to full-rank training, Fira's superior perfor- mance may be attributed to the following reason: the gradient direction in the norm-based scaling method is determined by the current state, rather than by historical gradients in Adam. Therefore,"}, {"title": "5.2 SCALING UP TO LLAMA 7B PRE-TRAINING.", "content": "To validate the scalability of our method, we scale up by pre-training the LLaMA 7B model with the full- rank dimension dmodel = 4096. We compare Fira with the GaLore baseline, which generally achieves the best performance among low-rank training base- lines, as shown in Table 2. As illustrated in Figure 5, our method demonstrates a significant improvement over GaLore for pre-training LLaMA 7B, while using an 8\u00d7 smaller rank. This highlights Fira's effective- ness, suggesting it could be a viable solution for large- scale LLM pre-training."}, {"title": "5.3 \u039c\u0395\u039cORY-EFFICIENT FINE-TUNING", "content": "Experimental Setup. Following Hu et al. (2023), we perform the fine-tuning task to compare Fira with LoRA, GaLore, and other baseline methods, including Prefix-tuning (Prefix) (Li & Liang, 2021), Series Adapter (Series) (Houlsby et al., 2019), and Parallel Adapter (Parallel) (He et al., 2021), on the LLaMA-7B model for commonsense reasoning tasks. This task consists of eight sub-tasks, each with its own designated training and testing sets. Following the approach of Hu et al. (2023), we combine the training datasets from all eight sub-tasks into a unified training set, while evaluating each sub-task individually using its respective testing dataset. In the fine-tuning task, the rank r is set to 32 and the learning rate is set to le-4. The detailed settings of fine-tuning are provided in Appendix A.3. We adopt RTX 4090 GPUs for fine-tuning experiments."}, {"title": "Result Analysis.", "content": "As shown in Table 3, our Fira achieves the highest performance on 4 out of 8 datasets, demonstrating better or comparable performance compared to the baseline methods. No- tably, GaLore struggles to adapt to the HellaSwag and WinoGrande datasets, resulting in a significant decline in scores. In contrast, our Fira adapts to these tasks well and achieves the highest scores on WinoGrande. In terms of memory efficiency, our method uses comparable or even less memory than the low-rank training methods LoRA and GaLore. These results illustrate the effectiveness of our method for the fine-tuning of LLMs."}, {"title": "5.4 ABLATION STUDY", "content": "In this section, we conduct an ablation study to assess the effectiveness of each component in our method. We adopt the same settings in Section 5.1 for pre-training the LLaMA 60M model. We design four variants of our method for the ablation study: (1) Fira-w.o.-scaling: our Fira without using the scaling factor to correct the gradient (i.e., setting $t(Rt) to a fixed value of 1). (2) Fira- matrix: our Fira using the scaling factor at the matrix level instead of at the column level. (3) Fira- w.o.-limiter: our Fira without using norm-growth limiter to avoid training loss spikes. (4) Fira- gradient-clipping: our Fira using gradient clipping to avoid loss spikes instead of our proposed norm-growth limiter."}, {"title": "5.5 PERFORMANCE UNDER VARYING RANKS", "content": "In this section, we illustrate the advantages of our Fira over Galore under a lower rank. We adjust various rank configurations within the set {4, 16, 64, 128} and dmodel = 256, and then assess the performance of pre- training the LLaMA 60M model on the C4 dataset as outlined in Section 5.1. The validation perplexity of Fira and GaLore after 10K steps across different ranks is depicted in Figure 6. From Figure 6, we can ob- serve that Fira consistently surpasses GaLore across all rank configurations. Notably, even when the ranks are set very low (4 and 16), Fira still achieves perfor- mance comparable to full-rank training. In contrast, the performance of GaLore significantly declines in these cases. These results highlight the superiority of our proposed Fira at lower ranks and its effectiveness in reducing memory usage."}, {"title": "6 CONCLUSION", "content": "In this paper, we present a plug-and-play memory-efficient training framework for LLMs, called Fira, as the first attempt to facilitate full-rank training consistently under low-rank constraints. First, we find a notable phenomenon in LLM training: the scaling effect of adaptive optimizers on the gra- dient norm remains similar between low-rank and full-rank training. Building on this observation, we propose a norm-based scaling method that applies the scaling effect of low-rank optimizers in"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 THEORETICAL ANALYSIS", "content": "Variance of Scaling Factors. The variance of adaptive learning rates is significantly elevated dur- ing the early stage of training, often necessitating a warm-up to mitigate this variance and stabilize training (Liu et al., 2019). As illustrated in Figure 7, the scaling factor in Fira exhibits a similar pattern, characterized by substantial variance during the early stage of training, which also neces- sitates a warm-up. However, the addition of an extra warm-up hyper-parameter for Fira would be inefficient. Therefore, it is crucial to investigate whether the original warm-up would have mitigated the variance in Fira efficiently. In the subsequent theoretical analysis, we show that, during the early training phase, the variance of the scaling factor of Fira is less than or equal to that of the adaptive learning rate. This finding suggests that the existing warm-up strategy is sufficient to mitigate the variance of Fira, thereby eliminating the need for an additional warm-up hyper-parameter.\nConsider independent random vectors ${g_j^{(i)}}_{j=1}^{t}$, where each $g^{(i)} = (g_1^{(i)}, g_2^{(i)},..., g_t^{(i)})$. Here, the superscript i indicates the index of the weight matrix to which the vector belongs, while the subscript j (where j ranges from 1 to t) denotes training iterations with each parameter. Following (Liu et al., 2019), we assume the adaptive learning rate of Adam $\\psi(.) =  (1- \\beta_2^t) \\Sigma_{j=1}^t g_j^{(i)}^2$ and $g^{(i)} \\sim N(0,\\sigma^2)$ for all i and j in the early stage. Additionally, approximate the distri- bution of the exponential moving average as the distribution of the simple average, $p(\\psi(.)) =p( \\sqrt{\\frac{1-\\beta_2^t}{(1-\\beta_2)} \\Sigma_{j=1}^t g_j^{(i)^2}})$ (Nau, 2014), and then $\\psi^2(.) \\sim Scale-inv-X^2(p,\\frac{2}{\\sigma^2})$.\nTheorem 1. (Variance of Scaling Factors) In the early stages of training, if $\\psi^2(.) \\sim Scale-inv-X^2(p,\\frac{2}{\\sigma^2})$, and $g_j^{(i)^2} \\sim N(0,\\sigma^2)$ for all i, j, then for all p > 4, the scaling factor $\\phi^2 = \\frac{\\Sigma_{j=1}^t (g_j^{(i)})^2}{\\Sigma_{j=1}^t g_j^{(i)^2}}$ satisfies $Var[\\phi^2] < Var[\\psi^2]$. If we approximate $\\sqrt{\\psi^2}$ and $\\sqrt{\\phi^2}$ to the first order, we have Var[$\\phi$] \\leq Var[$\\psi$].\nProof. We express \u03c6\u00b2 as a weighted sum:\n$\\varphi^2 = \\sum_{i=1}^{n} w_i\\psi_i,$ (13)\nwhere the weights are defined as:\n$w_i = \\frac{(\\Sigma_{j=1}^t (g_j^{(i)})^2)}{\\Sigma_{j=1}^t g_j^{(i)^2}},$ (14)\nEach $w_i$ is a non-negative random variable satisfying $\\sum_{i=1}^{n} w_i = 1$.\nIn the context of adaptive optimization algorithms like Adam, the squared gradients $  accumulate information from past iterations to adapt the learning rate for each parameter. With \u03b22 = 0.999, the moving average of the squared gradients places significant weight on historical data, making $  dependent mainly on past gradients, yielding:\n$ \\varphi  \\approx  (g_{t-1}^{(i)}, ..., g_1^{(i)}).$ (15)\nSince $  primarily depend on past gradients $g_j^{(i)^2}$, ..., $g_1^{(i)^2}$, and $w_i$ depend solely on the current gradients $g_t^{(i)}$, we can consider $  and $w_i$ to be independent random variables.\nConsequently, we can express the variance of \u03c6\u00b2 as:\n$Var[\\varphi^2] = Var[\\sum_{i=1}^{n} w_i\\psi_i^2],$ (16)"}, {"title": "A.2 DETAILED PRE-TRAINING SETTING", "content": "This section provides an overview of the LLaMA architectures and the hyper-parameters employed during pre-training. To ensure a fair comparison, we adopt the same settings as Zhao et al. (2024a). Table 5 presents the hyper-parameters of the LLaMA architectures across various sizes. For all architectures, we utilize a maximum sequence length of 256 and a batch size of 131K tokens. Fur- thermore, we implement a learning rate warm-up during the initial 10% of training steps and employ cosine annealing for the learning rate schedule, which decreases to 10% of the initial learning rate."}, {"title": "A.3 DETAILED FINE-TUNING SETTING", "content": "We fine-tune the pre-trained LLaMA-7B model for commonsense reasoning tasks benchmark de- signed for LLM fine-tuning, which include eight sub-tasks (Hu et al., 2023). Table 6 shows the hyper-parameter configurations."}, {"title": "A.4 PLUG-AND-PLAY FRAMEWORK FOR FIRA", "content": ""}, {"title": "A.5 QUANTITATIVE ANALYSIS OF SIMILARITIES", "content": "In this section, we analyze the similarities among the three ranking (i.e., order) sequences $R_1$, $R_2$, and $R_3$ depicted in Figure 3, which highlight the discrepancies in scaling factors across different weight matrices. Each ranking comprises ten distinct items. We will employ Kendall's Tau cor- relation coefficient (Abdi, 2007) and Spearman's rank correlation coefficient (Sedgwick, 2014) to evaluate their concordance and divergence.\nThe three ranking sequences are defined as follows:\n$R_1 = (7,6, 1, 2, 4, 8, 5, 10, 9, 3)$ (32)\n$R_2 = (7, 8, 2, 1, 5, 4, 6, 10, 9, 3)$ (33)\n$R_3 = (6, 8, 2, 1, 5, 4, 7, 10, 9, 3)$ (34)"}, {"title": "A.5.1 KENDALL'S TAU CORRELATION COEFFICIENT", "content": "Kendall's Tau, a non-parametric statistic, measures the ordinal association between two rankings. It quantifies the degree to which the presence of one ranking implies a similar ranking in another. The formula for"}]}