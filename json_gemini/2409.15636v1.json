{"title": "Personalized Federated Learning via Backbone Self-Distillation", "authors": ["Pengju Wang", "Bochao Liu", "Dan Zeng", "Chenggang Yan", "Shiming Ge"], "abstract": "In practical scenarios, federated learning frequently necessitates training personalized models for each client using heterogeneous data. This paper proposes a backbone self-distillation approach to facilitate personalized federated learning. In this approach, each client trains its local model and only sends the backbone weights to the server. These weights are then aggregated to create a global backbone, which is returned to each client for updating. However, the client's local backbone lacks personalization because of the common representation. To solve this problem, each client further performs backbone self-distillation by using the global backbone as a teacher and transferring knowledge to update the local backbone. This process involves learning two components: the shared backbone for common representation and the private head for local personalization, which enables effective global knowledge transfer. Extensive experiments and comparisons with 12 state-of-the-art approaches demonstrate the effectiveness of our approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning models have achieved remarkable performance in numerous significant tasks [46], primarily due to the availability of large-scale data. Nevertheless, in many practical scenarios like multimedia analysis [65], medical diagnosis [22], and financial ana- lytics [4], access to multimedia data is limited for model training because of privacy issues [31, 47]. Federated learning [39] has re- cently been proposed as a practical and feasible solution for col- laborative training of models on distributed data from multiple clients while preserving privacy. The core concept revolves around sharing knowledge, such as weights, as opposed to sharing data among multiple clients through a centralized server. However, an important challenge facing current federated learning is the data heterogeneity problem [21]. It is mainly manifested as different data distributions due to different local tasks of clients. Traditional federated learning only learns a common global model, but non- independent and identically distributed data would lead to poor performance for each client, i.e., client drift [23]. Researchers have addressed this issue by focusing on model per- sonalization [32], catastrophic forgetting [28], and related aspects. Recently, researchers have shown a preference for personalized fed- erated learning [50], which can be implemented by adjusting model structures and model parameters. In terms of model structures, [1] proposes a personalization approach that divides the model into the same base layers and unique personalization layers. [8] aims"}, {"title": "2 RELATED WORK", "content": "Personalized federated learning. Federated learning is a collab- orative training algorithm without exposing local private data [39]. However, it encounters challenges with data heterogeneity, lead- ing to issues such as client drift. Consequently, researchers have focused on addressing the challenges posed by heterogeneous data in federated learning [14, 17, 23, 29, 61]. Personalized federated learning has emerged as a viable solution to address this issue and has been extensively investigated in recent research [50]. Broadly, these approaches can be categorized into local fine-tuning [9, 48, 59], meta-learning [12, 24, 57], multi-task learning [6, 58, 62], and mix- ture of global and local models [10, 15, 38]. These approaches per- sonalize the parameters around the entire model. From the aspect of model architecture, [36] shares only one part of the model while personalizing the rest of the model. [1] introduces FedPer to learn the base layers and the personalization layers. [8] proposes FedRep to learn the global representation and the unique head. As these approaches do not share all parameters, they only personalize one part of the model, while the other part is less personalized.\nFederated learning with knowledge distillation. Knowledge distillation is an algorithm proposed by [16] that aims to enhance the performance of a small student model by leveraging a large teacher model. To extract essential information, researchers have started experimenting with integrating knowledge distillation into federated learning [40]. This integration has the potential to reduce communication overhead and overcome challenges such as data heterogeneity. Federated distillation [19, 30, 52] reduces the com- munication cost by transmitting the model logits with the data's label information instead of the model parameters. Alternatively, through distilling the local knowledge of the large model into the small model [55], only the parameters of the small model need to be communicated, thus reducing the communication overhead. Apart from the client-side knowledge distillation discussed earlier, some researchers also leverage multiple models on the server-side to carry out integrated knowledge distillation [7, 37, 49]. Knowledge distillation can be a practical solution to address the issue of data heterogeneity in federated learning [37, 53, 54]. It can also be used to augment data for clients through data-free knowledge distilla- tion [5, 60, 64] and to facilitate knowledge transfer between global and local models [7, 42, 51]. Furthermore, it is an effective approach in federated learning for achieving personalization [11, 20, 44]."}, {"title": "3 APPROACH", "content": "Our backbone self-distillation approach aims to enhance the shared backbone and personalize the private head. As shown in Fig. 2, each client model is divided into a shared backbone and a private head, with only the backbone weights uploaded to the server for aggrega- tion. This ensures that each client's head is unique and personalized. Purely personalizing the head for personalized federated learning has limitations. Therefore, we employ self-distillation to enhance the client model by using the global backbone as the teacher and the local backbone as the student. This procedure improves the performance of the local backbone."}, {"title": "3.1 Problem Formulation", "content": "Assume that there are n clients, with local data $D_k = \\{x_k, y_k\\}$, $k \\in [1,..., n]$, where $x_k$ is the sample space, $y_k$ is the label space, and k denotes the k-th client. Each client usually has its local model $\\Phi_k(x_k; w_k)$ and the corresponding loss function $l_k$, where $w_k$ is the model weights of the k-th client. To utilize the data information from all clients, federated learning ensures privacy-preserving joint modeling by uploading the model weights or gradients. In the case of traditional federated learning, the format is as follows,\n$\\min \\frac{1}{n} \\sum_{k=1}^n l_k(\\Phi_k(x_k; w_k), y_k).$ \n(1)"}, {"title": "3.2 Common Representation Learning", "content": "In traditional federated learning approaches, there is typically a re- quirement for exchanging all model parameters between the client and the server. Considering that each client is engaged in a clas- sification task, we divide the client task into two parts: common representation learning and personalized classifier learning. Ac- cordingly, for each client model $\\Phi_k(x_k;w_k)$, we divide it as follows,\n$\\Phi_k(x_k;w_k) \\rightarrow \\phi_k(x_k;w_{k,b},w_{k,h}),$\n(2)\nwhere $w_{k,b}$ and $w_{k,h}$ correspond to the weights of the backbone and the head of the k-th client model, respectively. We only exchange $W_{k.b}$ between the client and the server. Then, the server aggregates the shared backbone weights to obtain the global backbone,\n$W_g = \\frac{1}{n} \\sum_{k=1}^n Ew_{k.b}$\n(3)\nAfter the server completes the aggregation of the shared back- bone, the weights are returned to each client to replace the previous backbone, i.e., $w_{k,b} = w_g$.\nIn our approach, the server aggregates the shared backbone to obtain a common representation. Meanwhile, each client utilizes local data to learn its personalized head. Upon reviewing the entire"}, {"title": "3.3 Backbone Self-distillation", "content": "The common representation mitigates to some extent the harm caused by data heterogeneity in federated learning, but it is still essentially a general federated averaging approach. Clearly, for the k-th client with local data $D_k$, local model $\\Phi_k(x_k;w_k)$ does not achieve the best performance. Therefore, we consider the optimiza- tion of the shared backbone on the client based on the previous section in order to attain improved personalized models.\nKnowledge distillation is widely used in transfer learning to ex- tract model knowledge effectively. Generally, it first trains a teacher model $\\Phi_t (x;w_t)$, and then allows the student model $\\Phi_s (x;w_s)$ to learn the probability distribution of output from the teacher model to improve the generalization.\n$L = l(\\phi_t(x;w_t), \\phi_s(x;w_s)),$\n(4)\nwhere $l()$ denotes the distillation loss.\nKnowledge distillation includes two losses: the Cross-Entropy (CE) loss $L_{CE}$ and the Kullback-Leibler (KL) divergence loss $L_{KL}$.\n$L = L_{CE} (O_s, y) + \\lambda L_{KL} (P_t, P_s),$\n(5)\nwhere $P_t$ and $P_s$ are the prediction of the teacher and student models, $O_s$ is the output of the student model, y is the true label and $\\lambda$ is the weight factor to balance the two losses.\nFor multi-class classification tasks with m classes, The KL loss function is defined as follows,\n$L_{KL} (P_t, P_s) = - \\sum_{i=1}^m P_{ti}(t) log(\\frac{P_{si}(t)}{P_{ti}(t)}),$\n(6)"}, {"title": "4 EXPERIMENTS", "content": "To validate the effectiveness of our approach, we conduct experi- ments on simulated and real-world datasets and compare with 12 state-of-the-art approaches."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We consider the image classification task and utilize three datasets for approach validation, i.e., CIFAR10 [25], CIFAR100 [25], and FEMNIST [3]. To simulate a heterogeneous federated scenario, we randomly assign different numbers of classes S to N clients in a dataset. For CIFAR datasets, CIFAR10 has three settings (N = 100, S = 2), (N = 100, S = 5) and (N = 1000, S = 2), CIFAR100 has"}, {"title": "4.2 Experimental Results", "content": "Simulated experiments. We present the experimental results of FedBSD across three datasets and six experimental settings, as shown in Tab. 1. The experimental results demonstrate that our approach FedBSD not only outperforms existing federated learning approaches but also significantly surpasses the accuracy in the Local only case, where each client trains its model locally. In the CIFAR10 experiment with setup (N = 100, S = 2), FedBSD achieves a performance that is less than 0.90% away from the Local only results. These results affirm that FedBSD, with the utilization of backbone self-distillation, effectively addresses the issue of client drift in federated learning and achieves impressive performance. Notably, when using the MLP architecture on the FEMNIST dataset, FedBSD surpasses the performance of FedRep by 4.75%. This finding suggests that FedBSD performs exceptionally well with simple MLP architecture when dealing with limited datasets, making it highly suitable for practical applications. Additionally, the poor performance of PFedSD and FedPAC further highlights the negative impact of the private head, thereby confirming the validity of our self-distillation approach for the backbone only.\nReal-world experiments. The use of local batch normalization (BN) before averaging the model helps alleviate feature bias in the heterogeneous federated scenario [35]. To ensure a fair comparison, we utilize AlexNet in the DomainNet dataset and six convolutional layers in the Digits dataset as the base models. Furthermore, a BN layer is added after each convolutional layer or fully-connected layer. Methodologically, we introduce FedBN [35], which excludes"}, {"title": "4.3 Ablation Studies", "content": "This section aims to conduct ablation studies on the CIFAR10 and CIFAR100 datasets to investigate the effects of data heterogeneity, training epochs, and communication rounds. In the experimental setup, unless otherwise stated, the CIFAR10 dataset is set to (N = 100, S = 2), and the CIFAR100 dataset is set to (N = 100, S = 5).\nEffect of data heterogeneity. The total classes S on each client serve as an indicator of the data heterogeneity, we compare the performance across different S values. For the CIFAR10 dataset, the parameter S is tuned among {2, 4, 6, 8, 10}. For the CIFAR100 dataset, the parameter S is tuned among {10, 20, 30, 40, 50}. When S = 10, CIFAR10 is evenly distributed, and each client contains"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a backbone self-distillation approach called FedBSD for personalized federated learning. FedBSD divides each client model into a shared backbone and a private head, where the backbone is communicated with the server for knowledge shar- ing among clients. Each client conducts self-distillation to transfer the global backbone knowledge, resulting in a personalized pri- vate head when enhancing accuracy. Importantly, the proposed approach does not rely on external data and can be readily ex- tended to other federated learning approaches. In future work, we aim to combine backbone self-distillation with model compression to minimize communication costs."}]}