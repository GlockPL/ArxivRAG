{"title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction", "authors": ["Xiaoping Wu", "Jie Hu", "Xiaoming Wei"], "abstract": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.", "sections": [{"title": "1. Introduction", "content": "The advent and evolution of diffusion models [15-17, 31] have significantly advanced the generation of high-fidelity and high-resolution images. To further enhance performance, DiT [29] introduced a pure transformer architecture to replace the traditional UNet structure, facilitating the scalability of subsequent models such as SD3.0 [13] and FLUX \u00b9. Concurrently, efficient sampling methods [26, 28, 35, 36] have been proposed to reduce the number of diffusion steps required.\nTo achieve structural compatibility with large language models (LLMs) [20, 39], autoregressive (AR) methods [12, 40, 43, 45] employ vector quantization (VQ) [30, 41] during"}, {"title": "2. Related Work", "content": "Image synthesis [16, 35] has become a pivotal area of research in the pursuit of general artificial intelligence, aiming to generate high-quality and realistic visual content. This section reviews three primary approaches in image generation: diffusion, autoregression, and mask-based mechanisms."}, {"title": "2.1. Diffusion-based Image Synthesis", "content": "Diffusion models operate in two phases: a forward process where noise is incrementally added to an image, simulating diffusion, and a reverse process that restores image details from noise through denoising. Early models like GANS [2, 4, 14, 21, 33] and VAEs [22] directly learned data distributions. In contrast, DDPMs [16] utilize parameterized Markov chains for data generation, achieving high image quality while avoiding training instability and mode collapse. However, DDPMs lag in log-likelihood performance compared to likelihood-based models.\nSubsequent improvements [28] in inverse diffusion variance, hybrid learning objectives, and noise schedules enhanced DDPM's coverage of target distributions. DDIM [35] further accelerated sampling by generalizing DDPM to a non-Markov chain diffusion scheme, optimizing variance, and reducing the need for randomness, thus enabling deterministic generation with fewer denoising steps. The Consistency Model [37] and Latent Consistency Model (LCM) [27] introduced methods to reduce sampling steps without sacrificing quality, utilizing classifier-free guidance and LoRA [18] for efficient training.\nIn conditional image synthesis, Dhariwal and Nichol [11] employed classifiers to guide diffusion models, while CFG [15] proposed end-to-end training for conditional guidance. Traditional models like U-Net [32] faced scalability challenges, leading to the adoption of transformer architectures [29] in models such as SD3 [13], Pixart [8], and Kolor. Diffusion models excel in high-resolution image generation, balancing quality and efficiency."}, {"title": "2.2. AutoRegression-based Image Synthesis", "content": "Autoregressive (AR) models, successful in LLMs [1, 5, 39], are increasingly applied to image synthesis. VQ-VAE [41] introduced encoding images into discrete tokens using a codebook for vector quantization. VQ-VAE2 [30] refined this with hierarchical reconstruction, while VQ-GAN [12] integrated perceptual loss and adversarial training. Yu et al. [43] further enhanced this by employing vision transformers and improving loss functions.\nDespite their strengths, AR models face challenges with information loss during quantization, addressed by RQ-VAE [23] through residual quantization. AR models are adept at capturing dependencies in sequence data and integrating with LLMs for multi-modal models, but face challenges with parallelization and embedding conditions affecting efficiency and quality."}, {"title": "2.3. Mask-based Image Synthesis", "content": "Mask-based generative models, like MaskGit [6], represent a novel approach in image synthesis, predicting image positions in parallel but with varying validity. During training, random masking allows the model to infer unknown regions from known patches. Inference begins with reliable patches, gradually completing the image. This approach is utilized in models like MAGVIT [44] and Muse [7]. Li et al. [24] demonstrated that joint training of generative and representation learning models enhances performance.\nWhile mask-based models improve efficiency and resource utilization, they lack the iterative refinement of diffusion models and the long dependency capture of AR models. Inspired by these methods, our proposed RDPM innovatively models the diffusion process into a recurrent token prediction framework, requiring only 10 iterations. Through parallel forward passes, RDPM progressively converges from pure noise to the data distribution."}, {"title": "3. Methodology", "content": "Our image generation approach comprises two key components: a diffusion-based image tokenization module that encodes images into discrete tokens, and a recurrent token prediction module that serves as a diffusion mechanism for image generation."}, {"title": "3.1. Diffusion-based Image Tokenization", "content": "Building on existing research [16, 35], our method performs image generation in the latent space rather than the pixel space, significantly reducing data dimensionality and computational resource consumption. The initial step involves training a VAE for image encoding and decoding.\nImage Encoding: The encoder E compresses natural images into feature representations, mapping an input image"}, {"title": "Diffusion-based Quantization Process", "content": "This subsection aims to divide the original single-step vector quantization operation into a T-step diffusion process. Instead of adding noise directly to the original image, we apply the diffusion paradigm within the quantization process. This strategy minimally increases the parameter volume and computational cost of the original VAE model. Specifically, we mix Gaussian noise into the latent variable v in T steps.\nAt each step t, let vt and v't denote the noisy vector and its corresponding quantized vector, respectively:\n\n$v_t = A_t z_t + B_t e_t,  v'_t = Q(v_t).$\n\nHere, $z_t = z_{t-1} - v'_{t-1}$ performs a residual operation similar to RQVAE [23], with $z_1 = z$. The quantization operation Q() searches for the nearest neighbor in the codebook C (containing K embeddings) based on the Euclidean distance, resulting in a code $C_t$ of size h \u00d7 w \u00d7 1. Additionally, $e_t ~ N(0, 1)$ is noise sampled from a standard normal distribution. $a_t$ is a weight controlling the image proportion, which increases from small to large values, and when t = T, $a_t = 1$, with $\\beta^2_t + a_t = 1$. That is, when t is small, the main component of the noisy vector is Gaussian noise, and as it approaches T, the image content increases. During the T steps of repetition, the model continuously accumulates operations on $v_t$, i.e., $z_t = \\sum_{i=1}^{t} v'_{i}$. This represents the current step's image quantization representation. Similar to RQVAE [23], the purpose of the residual and accumulation operations on the latent vector is to minimize the loss of information during the quantization process. Algorithm 1 meticulously delineates the quantization process of an input image, subjected to T diffusion steps, culminating in a quantized representation.\nImage Decoding: At the final step, i.e., the T-th step, we obtain T quantized vectors {v'1, ..., v'T} and their index codes {$C_1$, ..., $C_T$}, as well as $z'_T = \\sum_{i=1}^{T} v'_{i}$. The quantized vector representation $z'_T$ is then passed through the decoder D to recover the original image x'. Similar to encoder E, the decoder D primarily consists of residual blocks; however, the difference lies in the upsampling of the quantization vector by a multiple of H/h."}, {"title": "Loss Function", "content": "The loss function is consistent with that used in VQGAN [43], incorporating several components. The reconstruction loss is defined as:\n\n$L_{recon} = ||x - x'||^2$.\n\nwhich minimizes the pixel-wise difference between the input image and the reconstructed image. The quantization loss is expressed as:\n\n$L_{quant} = \\sum_{t=1}^{T} \\gamma_t (v_t - v'_t)^2,$\n\nrepresenting the weighted average across the T steps to mitigate information loss during vector quantization. In practice, $\\gamma_t = a_t$, as the image's proportion in the noise vector directly influences the model's learning difficulty. The subsequent experimental section further validates the advantageous impact of this weight parameter on image reconstruction performance. Additionally, the loss calculation for the VAE includes perceptual loss $L_{pept}$ and adversarial loss $L_{gan}$:\n\n$L_{vae} = L_{recon} + \\delta L_{quant} + L_{pept} + \\eta L_{gan}.$\n\nThe perceptual loss aligns the high-level semantics of the input and reconstructed images, typically using features extracted from VGG16 [34], to enhance visual fidelity. Concurrently, the adversarial loss promotes realism through the interaction of a generator and discriminator. The weights $\\delta$ and $\\eta$ for each loss component are set following VQGAN's configuration, at 0.25 and 0.75, respectively."}, {"title": "3.2. Recurrent Token Prediction for Generation", "content": "The goal of a generative model is to learn how to transition from random noise to a target data distribution within the latent space over T iterative steps."}, {"title": "Training Process", "content": "The input image x is encoded to yield T quantized vectors of size h \u00d7 w \u00d7 d', denoted as {v'1, ..., v'T}, and corresponding codes of size h \u00d7 w, {$C_1$, ..., $C_T$}, alongside the accumulated noise {e1, ..., eT} from the diffusion process. At each step t, all known factors are input into a generative model f. These factors include the noise et, a condition (i.e., category label y), the time step t, and the accumulated quantized vectors from step 1 to t-1, $z_{t-1}$, to predict v't (with $z'_{t-1} = 0$ when t = 1):\n\n$v'_t = f(e_t, y, t, z'_{t-1}).$\n\nThe objective is to minimize the discrepancy between the predicted v't and the ground truth v't. In practice, noise and quantized vectors are concatenated to form a variable of size h \u00d7 w \u00d7 2d, which is then projected back to h \u00d7 w \u00d7 d. After unfolding into h \u00d7 w sequence tokens and processing through a transformer model, a classification layer predicts the quantization code index $C_t$ at step t, using cross-entropy loss:\n\n$L_{gen} = - \\frac{1}{hxw} \\sum_{i=1}^{hxw} \\sum_{j=1}^{K} C_{t,i,j} log\\frac{e^{v'_{t,i,j}}}{\\sum_{k=1}^{K} e^{v'_{t,i,k}}}.$\n\nHere, i indexes the token position, while j and k index the embedding positions in the codebook. Algorithm 2 succinctly encapsulates the predictive procedure, beginning with stochastic noise and sequentially predicting the subsequent data distribution over T iterations in a recurrent manner. The aggregated distribution is then input into the decoder, culminating in the generation of the desired image."}, {"title": "Testing Phase", "content": "During the testing phase, the prediction of v't is conducted based on known conditions, following the formula $z'_t = z'_{t-1} + f(e_t, y, t, z'_{t-1})$. Here, when t = 0, $z'_{-1} = 0$. This recurrent forward propagation over T steps yields the data distribution $z'_T$. Subsequently, this distribution is processed through the decoder to generate the final image. The incorporation of random noise at each step of the T-step prediction process ensures the diversity of the generated outputs."}, {"title": "4. Experiment", "content": "In this section, we provide a comprehensive overview of our experimental setup, detailing the datasets, evaluation metrics, and parameter configurations employed in our study. We perform ablation studies to scrutinize the impact of hyperparameter selection. Furthermore, we conduct a thorough comparison of our method against SOTA models."}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "Our experiments involve training image autoencoder and generative models using the ImageNet-1k training dataset [10], which consists of 1.28 million images.\nTo assess the quality of the generated images, we primarily employ the Fr\u00e9chet Inception Distance (FID). This"}, {"title": "4.2. Experimental Setting", "content": "In the training phase of the image tokenization module, input images with dimensions of 256 \u00d7 256 are downscaled by a factor of 16, resulting in 16 \u00d7 16 tokens. The codebook utilized consists of 4096 vectors, each with an embedding dimension of 32. We employ the Adam optimizer with betas [0.5,0.9], setting the weight decay to 0. The learning rate is maintained at 4e-5, with a warmup period spanning 0.5 epochs. The batch size is configured to 128, and training is executed over 10 epochs using the ImageNet-1k dataset. Compared to VQ-GAN [12], our model incorporates a minimal increase in parameters, including the addition of a Layer Normalization [3] layer and a convolutional layer. Specifically, the encoder's latent vector undergoes layer normalization to harmonize its scale with noise prior to diffusion-based quantization. Furthermore, a shared convolutional layer is introduced during the quantization phase, serving as a bias for the quantized vectors, which enhances image reconstruction as substantiated by subsequent experimental results.\nFor the generative model training, we follow the LLama [39] framework, setting the transformer's hidden size to depth \u00d7 64, with the number of self-attention heads equaling the depth. The probability of randomly nullifying the category condition is 10%. The time-step t and category condition y are integrated using AdaLN-Zero [29]. To expedite training, mixed-precision techniques are employed, and flash-attention v2 [9] is utilized to enhance the efficiency of attention operations and reduce memory usage. The batch size is set to 1024, with an initial learning rate of 4e-4, and a warmup period of 1000 steps. Training extends over 200 to 250 epochs, with gradient clipping set to 1. The AdamW optimizer is applied with betas [0.9, 0.95] and a weight decay of 0.03. Model parameters are updated using an exponential moving average (EMA) with a decay rate of 0.9999."}, {"title": "4.3. Ablation Study", "content": "This subsection presents an extensive experiments aimed at validating the selection of hyperparameters in training processes of both image autoencoder and generative models.\nAs depicted in Table 2, Exp.1-6, we investigate the impact of varying the timestep T on an image tokenization model based on the diffusion process. When T is less than 10, the reconstruction performance significantly deteriorates. Conversely, increasing T beyond 10 results in only marginal improvements across various metrics. Consequently, we have fixed T at 10. Upon further analysis, we observe that latent quantization loss increases as the timestep decreases and the noise mixing ratio rises. Additionally, images reconstructed in the initial timesteps are heavily infused with noise. Given the codebook's limited capacity (i.e., 4096), the extent of random noise mixing directly influences information loss during quantization and the model's learning difficulty. To address this, as shown in Eq. 4, we introduce a weight $\\gamma_t$ at each timestep, proportional to the image latent vector $a_t$. This adjustment reduces the rFID to 2.04, as indicated Exp.7 in Table 2. Moreover, incorporating an additional timestep-shared convolutional layer to process quantized vectors and using these vectors as bias can further mitigate quantization loss at smaller timesteps, achieving an rFID of 2.06 (Exp. 8). By combining both strategies, the rFID decreases to 1.52, with improvements observed in SSIM and PSNR metrics.\nDuring the inference phase of our image synthesis model, as illustrated in Table 3, we assess the benefits of various sampling strategies. Initially, applying CFG [15] results in substantial accuracy improvements for generated images, albeit at the cost of reduced recall. This suggests"}, {"title": "4.4. Comparison with SOTA methods", "content": "We next evaluate the performance of various generative models for class-conditional image generation at the 256 \u00d7 256 ImageNet scale. Table 5 summarizes the results of all state-of-the-art models.\nFrom the table, it is clear that GAN-based methods, such as StyleGAN-XL [33], achieve outstanding FID scores, underscoring their effectiveness. The one-step sampling process of these models further highlights their efficiency. However, they encounter challenges related to training stability, which may hinder their scalability to more complex tasks, such as text-to-image generation.\nDiffusion models simplify the generation process by decomposing the data distribution transformation into 1000 steps, thereby significantly reducing the complexity of image generation. Coupled with a robust VAE from StableDiffusion [31], DiT-XL/2 [29] achieves a FID of 2.27, with balanced Precision and Recall indicating a closer alignment with the reference image distribution. While diffusion-based methods are hindered by the number of sampling steps (e.g., 250), acceleration strategies like iDDPM [28] and DPM-solver [26] can reduce this to tens of steps, albeit at the cost of image quality.\nDiscrete autoregressive methods, such as VQGAN [12] and ViT-VQGAN [43], rely on predicting the raster scan order and often exhibit inefficiency during the sampling process, typically requiring numerous iterations to achieve satisfactory results. However, their compatibility with large language models (LLMs) offers substantial scaling potential. For example, RQTran. [23] scales up to 3.8B parameters while reducing sampling steps to 68. Additionally, it is well-known that VQ-VAE [30] utilizes discrete representations, leading to information loss during quantization, which results in inferior performance compared to other methods. Mask-based image synthesis strategies achieve moderate FID scores but excel in sampling efficiency, requiring just eight steps. In this domain, VARVAR [38] distinctively employs a coarse-to-fine autoregressive (AR) generation strategy to attain superior performance.\nOur proposed paradigm strikes a superior balance between sampling efficiency and image quality. Specifically, RDPM delivers high-quality image generation with only 10 timesteps and fewer parameters than autoregressive models. With 133M parameters, RDPM surpasses all autoregressive models without the need for rejection sampling [6, 12, 30]. Even at the same parameter scale, RDPM matches diffusion-based methods in quality, with a notable"}, {"title": "4.5. Discussion", "content": "Recent studies, such as MAR [25], have skillfully combined masked token prediction techniques with diffusion methods, effectively modeling the per-token probability distribution through a diffusion process. Concurrently, other research has explored various fusion strategies for integrating text autoregressive models with image diffusion techniques, thereby unifying text and visual generation tasks, as exemplified by ShowO [42] and Transfusion [46].\nIn contrast to these methodologies, our work introduces a novel perspective by implementing a diffusion process through multi-step discrete token prediction, aligning with the next token prediction strategy commonly employed in large language models (LLMs). This approach not only maintains the optimization consistency inherent in LLMs\u2019 next token prediction but also addresses the limitations associated with image quality loss that arise from single-step prediction mechanisms. Consequently, this method fills a critical gap in the field of Discrete Diffusion models, as illustrated in Fig 1, and paves the way for the integration of discrete and continuous signal generation models."}, {"title": "5. Conclusion", "content": "In conclusion, we introduce a novel paradigm for image generation, termed RDPM, which gradually denoises and ultimately generates natural images from a standard Gaussian distribution by recursively predicting the discrete codes for subsequent timesteps. This work addresses a significant gap in the field of Discrete Diffusion within image generation. Within RDPM, we propose a diffusion-based image tokenization method that effectively converts images into a sequence of timestep-based discrete codes, a technique that is also applicable to other continuous signal domains. Our approach has demonstrated impressive results on the ImageNet dataset, achieving state-of-the-art performance among methods employing discrete tokenizers. We anticipate that this method will inspire new directions in the development of unified multimodal models."}]}