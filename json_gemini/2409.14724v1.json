{"title": "EDSNET: EFFICIENT-DSNET FOR VIDEO SUMMARIZATION", "authors": ["Ashish Prasad", "Pranav Jeevan", "Amit Sethi"], "abstract": "Current video summarization methods largely rely on transformer-based architectures, which, due to their quadratic complexity, require substantial computational resources. In this work, we address these inefficiencies by enhancing the Direct-to-Summarize Network (DSNet) with more resource-efficient token mixing mechanisms. We show that replacing traditional attention with alternatives like Fourier, Wavelet transforms, and Nystr\u00f6mformer improves efficiency and performance. Furthermore, we explore various pooling strategies within the Regional Proposal Network, including ROI pooling, Fast Fourier Transform pooling, and flat pooling. Our experimental results on TVSum and SumMe datasets demonstrate that these modifications significantly reduce computational costs while maintaining competitive summarization performance. Thus, our work offers a more scalable solution for video summarization tasks.", "sections": [{"title": "Introduction", "content": "As of June 2022, more than 500 hours of video are uploaded to YouTube every minute, marking a 40% increase from 2014 [1]. This vast and largely unannotated video data underscores the increasing importance of video summarization. Video summarization involves extracting the most crucial information from a video. This technique has several applications, including managing information overload, content indexing, enhancing searchability [2], social media monitoring and analysis [3], surveillance and security [4, 5], and personalized content recommendations.\nA significant portion of research in supervised video summarization uses transformer encoder blocks [6], which struggle with the O(n\u00b2) complexity of self-attention, making it difficult to handle long sequences. While feasible for small-scale applications, this becomes impractical for the massive data volumes on social media, surveillance footage, and streaming platforms. To tackle this, we incorporate Nystr\u00f6mformer [7] and FNet blocks [8], which reduce complexity, enabling more efficient handling of large-scale video data.\nCurrent research in video summarization uses a frame-wise classification approach, labeling each frame as relevant or irrelevant. However, this does not reflect how humans process videos we first understand the global context before focusing on specific moments. Our approach mimics this by using efficient token-mixers to grasp the overall plot, followed by a temporal region proposal network to identify key segments for summarization. This method involves binary classification for segment selection and offset refinement through regression, capturing global context with token-mixers and refining finer details with the regression block for accurate summarization."}, {"title": "Related Work", "content": "Supervised video summarization approaches focus on training models with annotated datasets to generate summaries close to human-created ones. The Fully Convolutional Sequence Network (FCSN) [9] was an early deep learning method that used convolutional layers to encode temporal dependencies, predicting frame-level importance scores. To improve temporal modeling, the Visual-Temporal Attention-based Network (VASNet) [10] introduced a soft attention mechanism, capturing both local and global dependencies and achieving state-of-the-art performance by effectively learning contextual frame importance. More recent approaches have incorporated advanced attention mechanisms, to enhance video summarization quality. The Deep Reinforcement Learning-based Deep Summarization Network (DR-DSN) [11] used a reinforcement learning framework to capture long-term dependencies and contextual information. The Memory Augmented Video Summarizer (MAVS) [12] introduced an external memory network to store visual information from the entire video, improving the model's ability to generate comprehensive summaries.\nEfficient transformers have been developed to reduce the quadratic complexity of traditional self-attention, especially for long-sequence tasks. The Nystr\u00f6mformer [7] approximates self-attention using the Nystr\u00f6m method, enabling linear complexity for longer sequences. Linformer reduces costs through low-rank factorization [13], while Performer [14] uses kernel-based approximations for linear time complexity. The Longformer [15] combines global and local sparse attentions to handle lengthy texts efficiently. Hybrid models incorporating these efficient mechanisms with convolutional layers perform well in resource-constrained vision tasks [16].\nTemporal segment localization focuses on identifying the start and end times of actions in videos. Early methods, such as sliding window-based approaches [17, 18, 19], used fixed-length windows to sample frames, capturing temporal dependencies but suffered from high computational costs. Recent methods leverage deep learning for more efficient localization. The convolutional-deconvolutional network [20] enhances boundary accuracy through temporal upsampling and spatial downsampling, while the Segment-Tube detectorr [21] refines localization with per-frame masks. Multi-Stage CNNs [22] generate proposals more efficiently, and approaches like super-voxels [23] and actionness scores [24] focus on generating action tubelets. Deep Action Proposals (DAPs) [25], utilizing LSTM networks, highlight the significance of temporal context for precise localization."}, {"title": "Approach", "content": "We take the Detect-to-Summarize Network (DSNet) [26] architecture and modify the feature extraction and region proposal networks to enhance its efficiency and performance. We employed different token-mixing modules for temporal modeling and compared them on accuracy (F1 score), GPU usage, and model size."}, {"title": "Feature Extraction", "content": "We used GoogLeNet [27] for spatial feature extraction from video frames similar to DSNet [26]. Given a video with N frames, the extracted features are vi, where i \u2208 {1, 2, ..., N}. To efficiently extract temporally relevant spatial information, we replace softmax self-attention [6] with other token mixers.\nFourier transform: The fourier transform replaces the self-attention mechanism with two 1-D Discrete Fourier Transform (DFT) along the sequence and embedding dimensions as used in FNet[8]. The DFT decomposes sequences into their frequency components, efficiently mixing tokens without learnable parameters. The DFT operation makes the computation faster than softmax attention for longer sequences.\nNystr\u00f6mformer [7]: Nystr\u00f6mformer approximates the standard self-attention mechanism using the Nystr\u00f6m method-based low-rank matrix approximation. By decomposing the attention matrix into smaller matrices, Nystr\u00f6mformer reduces the complexity to O(N). This method preserves global context while reducing memory usage and computational overhead, making it suitable for longer sequences.\nDiscrete wavelet transform [28]: Similar to WaveMix in computer vision, we employ a 1-dimensional discrete wavelet transform (1D-DWT) for token-mixing in video summarization tasks, effectively capturing both temporal and frequency domain information. The DWT token-mixing module uses a specified wavelet (Haar) to decompose the input sequence into approximation and detail coefficients as shown in Fig. 3. The approximation coefficients are then passed through fully connected layers with a GELU non-linearity. The output is then combined with detail coefficients components and is normalized using layer norm to stabilize training and improve convergence. A 1-D transposed convolutional layer is employed to restore sequence length after downsampling by 1-DWT, refining the temporal resolution. The DWT-based approach offers computational efficiency while capturing essential features without introducing any trainable parameters."}, {"title": "Region Proposal Network", "content": "Similar to DSNet [26], we employ an anchor-based method for region proposals in video frames. We propose segments of lengths lk at each frame, where k \u2208 1,2,..., K. At temporal location t, \u2208 1, 2, ..., N, K interest proposals are appointed within the range [t-\\frac{l_k}{2},t +\\frac{l_k}{2}), where lk represents the duration of the k-th interest proposal. Thus, a total of N \u00d7 K interest proposals are generated for a video sequence with N frames."}, {"title": "Feature Extraction for Segment Proposals", "content": "We replace the temporal averaged pooling layer of DSNet [26] with three different methods of pooling to extract features from segment proposals.\nRegion of interest pooling: Region of Interest (ROI) pooling is used to manage variable-length segments by converting them into fixed-size representations suitable for fully connected layers. In our implementation, ROI pooling is applied along the temporal dimension, using average pooling for each anchor scale. However, ROI pooling's reliance on averaging can result in a loss of fine-grained details, which may not significantly impact segment classification but is crucial for accurate segment localization.\nFast Fourier transform pooling: Fast Fourier transform (FFT) pooling uses FFT to retain fine-grained details that may be lost in average pooling. The Fourier transform is only applied along the temporal dimension of each segment.\nFlat pooling: Flat pooling is a simpler approach where each segment is flattened directly. This method involves concatenating all segments into a single representation without any transformation."}, {"title": "Classification and Localization", "content": "Similar to DSNet [26], the pooled features are fed into the classification and regression module. The module is composed of a shared fully connected layer followed by ReLU non-linearity, layer-normalization, and two sibling output branches. The first branch outputs importance scores of proposals using coarse features (except for ROI pooling-based method), and the second branch outputs the associated center and segment length offsets using fine features (except for ROI pooling-based method).\nDuring testing, predicted offsets refine segment proposals, with non-maximum suppression (NMS) used to remove low-confidence and overlapping segments. To generate video summaries, we follow previous work [11, 32] where videos are first segmented into shots using Kernel Temporal Segmentation (KTS) [33], and shot-level importance scores are calculated by averaging frame-level scores. To ensure fair comparison, shot selection is constrained to 15% of the video length, solved as a 0/1 knapsack problem via dynamic programming to maximize the summary's importance."}, {"title": "Experiments", "content": "The datasets used in our experiments are TVSum [34] and SumMe [35], two well-established benchmarks for video summarization evaluation. TVSum includes 50 videos across genres like news, how-to, documentaries, vlogs, and egocentric content, with 1,000 shot-level importance scores crowd sourced (20 per video). SumMe consists of 25 videos, each with at least 15 human-generated summaries, totaling 390 annotated summaries.\nAs in previous studies, we downsampled the videos to 2 frames per second (fps). Downsampling reduces computational complexity and speeds up processing while retaining sufficient visual information for effective summarization. We employed 5-fold cross-validation with an 8:2 ratio for training and testing. The F1 score was used as the evaluation metric due to its balance between precision and recall."}, {"title": "Implementation Details", "content": "From the down-sampled Video frames, 1024-dimensional spatial image features (feature dimension size) are extracted using GoogLeNet [27] pre-trained on ImageNet [36]. We use the attention mechanism to extract global attention features, which are then compressed to a 128-dimensional (num_hidden) vector using a fully connected layer and ReLU Activation. A dropout of 0.5 is used. We use the same multi-task loss used by [26] with the same settings of hyperparameters, and the non-maximum suppression threshold was set to 0.5. Our anchor-based model was trained for 300 epochs using the Adam optimizer, with an initial learning rate of 5 \u00d7 10-5 and a weight decay of 10-5. The experiments were conducted on the Nvidia P100 GPU available on Kaggle. GPU memory consumption is reported for a batch size of 1.\nTo compare the performance, the fully connected (FC) depth was set to 1, and the F1 score was compared for various token mixers with different pooling operations and segment lengths (SL) of 4, 8, and 12.\nThe nomenclature for our EDSNet models is EDSNet (token-mixer, pooling) with the name of the token-mixer in the feature extractor and pooling method used in the segmentation feature extractor."}, {"title": "Results and Discussions", "content": "Table 1 presents a comprehensive comparison of various state-of-the-art (SOTA) models for video summarization, focusing on parameters, performance metrics, and GPU memory usage. The proposed models, EDSNet-(Nystr\u00f6m, ROI) and EDSNet-(FT, FFT), outperform several state-of-the-art (SOTA) models in terms of efficiency and accuracy. EDSNet-(Nystrom, ROI) achieves the highest accuracy on SumMe (57.07%), surpassing PGL-SUM [30] and DSNet [26]. Similarly, EDSNet-(FT, FFT) and EDSNet-(softmax, FFT) deliver competitive results on TVSum (62.88% and 62.97%, respectively). Notably, EDSNet-(FT, FFT) has the lowest parameter count (1.42M) compared to all models. Furthermore, EDSNet-(Nystrom, ROI) demonstrates the most efficient GPU memory consumption on SumMe (405 MB), outperforming DSNet and PGL-SUM, which consume over 500 MB. Overall, our models maintain high accuracy while offering substantial improvements in resource efficiency, making them suitable for memory-constrained environments. The results of the comparison of EDSNet with different token-mixers, pooling mechanisms, and segment lengths are shown in Table 2.\nFor SumMe, FFT pooling shows stable performance across different token-mixers, with Nystr\u00f6mformer achieving the highest F1 scores, peaking at 51.18% for a segment length of 8, suggesting FFT pooling effectively captures temporal features for this model. In contrast, Fourier token-mixing struggles, with a best score of 48.87% at a segment length of 12. For TVSum, FFT pooling performs well, with softmax attention and Fourier token-mixing achieving competitive scores of 62.4% and 62.88%, respectively, indicating its effectiveness in handling temporal variations. ROI pooling generally boosts performance, particularly for Nystr\u00f6mformer, which reaches 57.07% and 59.6% for SumMe and TVSum at segment length 12. Softmax attention also benefits from ROI pooling but to a lesser extent, showing it is compatible with models like Nystr\u00f6mformer that rely on capturing fine-grained features. Flat pooling performs inconsistently, often yielding lower results compared to FFT and ROI, as it fails to adequately capture temporal dependencies."}, {"title": "Ablation Studies", "content": ""}, {"title": "Segment Length", "content": "At a segment length of 4, the DWT model performs well, particularly with FFT pooling, indicating that shorter segments favor models that capture both temporal and frequency domain information effectively. Nystr\u00f6mformer also performs reasonably well with ROI pooling, benefiting from fine-grained temporal dependencies, while Fourier token-mixing underperforms across most settings. At a segment length of 8, Nystr\u00f6mformer and Softmax attention improve, especially with ROI and FFT pooling, reaching 54.32% and 51.37% accuracy on SumMe, suggesting that this intermediate length balances temporal dynamics and contextual information. DWT's advantage diminishes at this stage. At a segment length of 12, Nystr\u00f6mformer excels, particularly with ROI pooling, benefiting from longer segments, while Fourier token-mixing and Softmax attention with flat pooling continue to show lower performance, indicating they struggle with longer sequences."}, {"title": "Fully connected layer depth analysis", "content": "To compensate for the reduced number of parameters in the Fourier, DWT, Nystr\u00f6mformer token-mixing mechanisms, we increase the depth of the fully connected (FC) layer after the feature extraction step in 2, using default ROI poolings and segment length = [4, 8, 16, 32]. The experimental results of varying FC layer depths on both SumMe and TVSum datasets are shown in Figure 5.\nSoftmax attention and Nystr\u00f6mformer attention show the most stable performance across FC depths on both datasets, suggesting robustness and reliability in varying configurations. Fourier and DWT token-mixing demonstrate greater sensitivity to FC depth changes, particularly on the SumMe dataset. This analysis indicates the importance of selecting appropriate attention mechanisms and FC depths to optimize model performance for specific datasets."}, {"title": "Conclusion", "content": "Traditional approaches for video summarization using transformer-based models often face computational challenges, especially with long video sequences. To overcome these limitations, we propose enhancement in DSNet by employing efficient token-mixing mechanisms such as Fourier, DWT, Nystr\u00f6mformer, optimized through anchor-based region proposals and varying pooling methods. Our experiments, conducted on the TVSum and SumMe datasets, show that our models achieve competitive F1 scores while significantly reducing GPU memory usage and parameter counts. The results highlight the stability and robustness of Nystr\u00f6mformer across varying Fully Connected (FC) layer depths, while Fourier and DWT token-mixing demonstrate sensitivity to these changes. We also see that while ROI pooling performs well on SumMe, FFT pooling consistently achieves the best results for TVSum, highlighting the importance of selecting the appropriate pooling method based on dataset characteristics for video summarization. Through comprehensive comparisons with existing state-of-the-art, we demonstrate that our approach offers a more computationally efficient alternative without compromising summarization accuracy."}]}