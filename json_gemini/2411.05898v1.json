{"title": "Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent", "authors": ["Linfeng He", "Yiming Sun", "Sihao Wu", "Jiaxu Liu", "Xiaowei Huang"], "abstract": "In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, crucial for comprehensive environmental awareness. Experiments on the DriveLM visual question answering challenge demonstrate significant improvements over baseline models, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr metrics, indicating closeness of model answer to ground truth. Our method represents a promising step towards more capable and interpretable autonomous driving systems. Possible safety enhancement enabled by detection modality is also discussed.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in autonomous driving systems have led to an increased focus on developing end-to-end models capable of handling complex driving scenarios. Despite significant progress, current approaches still face challenges in generalisation, especially when faced with rare or unseen situations. Moreover, the ability to interact with human users and provide explanations for the model's decisions is crucial for building trust and acceptance of autonomous vehicles. To address these challenges, the recently introduced DriveLM challenge Sima et al. (2023) aims to leverage the power of vision-language models (VLMs) and large language models (LLMs) in the context of autonomous driving. By combining the visual understanding capabilities of VLMs with the reasoning and natural language processing abilities of LLMs, DriveLM seeks to improve generalisation and enable interactive communication between autonomous vehicles and human users. Inspired by the DriveLM framework Sima et al. (2023), this paper presents a novel approach that integrates additional modalities into the LLMs to enhance its perception and reasoning capabilities for autonomous driving tasks.\nOur method builds upon the Llama-Adapter Zhang et al. (2023a), a parameter-efficient fine-tuning approach that allows for the incorporation of task-specific knowledge into the pre-trained LLM. The common practice for image perception capability in Llama-Adapter is to incorporate a pre-trained image embedder, specifically the CLIP Radford et al. (2021a) model with trainable vision transformers (ViT) Dosovitskiy et al. (2020) to generate adaptation queries. The queries are then"}, {"title": "2 Related work", "content": "Fu et al. (2024) and Wen et al. (2023) explore the potential of leveraging large language models (LLMs) to imbue autonomous driving systems with human-like reasoning, interpretation, and memorization capabilities. They argue that such knowledge-driven approaches can address the limitations of traditional optimization-based and modular systems when dealing with complex scenarios and long-tail corner cases. The DiLu framework proposed by Wen et al. Wen et al. (2023) combines Reasoning and Reflection modules to enable decision-making based on common-sense knowledge and continuous evolution, demonstrating strong performance and generalization in experiments. DriveGPT4 Xu et al. (2023) presents an interpretable end-to-end autonomous driving system that utilizes large language models. They develop a new visual instruction tuning dataset for interpretable autonomous driving with the assistance of ChatGPT, and mix-finetune DriveGPT4 on this dataset. Mao et al. Mao et al. (2023) introduce GPT-Driver, which reformulates motion planning as a language modeling problem and utilizes the GPT-3.5 model as a motion planner. Their prompting-reasoning-finetuning strategy exhibits superior planning performance, generalization, and interpretability compared to existing methods on the nuScenes dataset. These works highlight the potential of LLMs in enabling more human-like reasoning and decision-making in autonomous driving systems, addressing the challenges faced by traditional approaches."}, {"title": "2.2 VLM-based autonomous driving", "content": "Recent works explore leveraging visual information to enhance autonomous driving systems. Sima et al. (2023) introduces Graph VQA, a task modeling graph-structured reasoning in autonomous driving, and propose DriveLM-Data and DriveLM-Agent, a VLM-based baseline. Their experiments demonstrate the potential of integrating VLMs into driving systems to enhance generalization and interactivity. Chen et al. (2023) proposes a novel object-level multimodal LLM architecture that fuses"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Model Architecture", "content": "The model architecture is based on traditional decoder block language model architecture. Let the model input $x \\in R^{N\\times d_{seq}}$, where N is the length of current sequence and $d_{seq}$ denote the initial token dim. Let the embedding dim be $d_{emb}$, the decoder-only transformer without token prediction head is formulated by a function $f_{trans}: R^{N\\times d_{seq}} \\rightarrow R^{N\\times d_{emb}}$. Assume we have an L-layer transformer, within each layer, the decoder block $f_{block}: \\mathbb{R}^{N\\times d_{emb}} \\rightarrow R^{N\\times d_{emb}}$ is defined by\n$f_{block} (z) := W_{o}(normalize(W_{q}(z)W_{k}(z)^{\\overline{}})W_{v}(z)) \\in R^{N\\times d_{emb}}$,\nwhere $W_{q}, W_{k}, W_{v}, W_{o} : R^{N\\times d_{emb}} \\rightarrow R^{N\\times d_{emb}}$ are linear transformations within attention blocks. With an initial linear transformation $f_{init} : R^{N\\times d_{seq}} \\rightarrow R^{N\\times d_{emb}}$, the transformer $f_{trans}$ is defined by\n$f_{trans}(x) := f_{block}(f_{block}(f_{block}(f_{init}(x))))) \\in R^{N\\times d_{emb}}$,\nand the final embedding is obtained via\n$z_{final}^{(L)} = f_{trans}(x)$.\nThe next token is predicted by feeding $z_{N-1,:}^{(L)} \\in R^{1\\times d_{emb}}$ into prediction head."}, {"title": "3.2 Integration of Textual and Visual Data", "content": "To incorporate visual data, our model interfaces the Transformer Blocks with a Perception Network and a Detection Network. The Perception Network processes raw images to generate visual embedding, while the Detection Network focuses on identifying specific features within the images. The integration process can be formalised as:\n$Q_{percept} \\in R^{M\\times d_{emb}}, Q_{detect} \\in R^{M\\times d_{emb}} = f_{percept} (X_{imgs}), f_{detect} (X_{imgs}),$\nwhere $f_{percept} (X_{imgs}), f_{detect} (X_{imgs)$ are respectively the Perception and Detection Network (as in Fig. 2). $Q_{percept}$ and $Q_{detect}$ are respectively the Perceptual and Detector Query as shown in Fig. 1. with the queries, we transform them according to the following procedure\n$Tok_{*} = W_{o}(normalize(W_{q}(z)W_{k}(Q_{*})^{T})W_{v}(Q_{*})) \\in R^{N\\times d_{emb}}.$\nReplacing \u2217 in Eq. (5) by {percept, detect} we obtain the $Tok_{percept} \\in R^{N\\times d_{emb}}$ and $Tok_{detect} \\in \\mathbb{R}^{N\\times d_{emb}}$, which have exactly the same shape as textual tokens $f_{block}(z)$ in Eq. (1). These tokens"}, {"title": "3.3 Detection Network with Camera-Specific Adaptation", "content": "A distinctive aspect of our model is the Detection Network's adaptation to multiple camera inputs. Each camera input is associated with a unique set of ID-separator Tokens to distinguish between different sources. Same as in Fig. 2, the process on obtaining Detector Query $Q_{detect}$ is described as\n$Q_{detect} = f_{proj}(\\parallel_{i=1}^{K_{ID}} Tok_{icam_{i}}^{yolos} ) \\in R^{M\\times d_{emb}}$ ,where $\\parallel$ denotes concatenation, $Tok_{icam_{i}}$ denotes ID-seperator Token (a trainable tensor) corresponding to the ith camera, $Tok_{i}^{yolos}$ denotes YOLOS (Fang et al. (2021b))' output token given input image of the ith camera. Here, we took the output of the last layer of YOLOS' encoder blocks, and $d_{yolos}$ denotes dimension of this output. N is the length of the outputted tokens, which is fixed and corresponds to the number of detection tokens configured in YOLOS2 Feeding $Q_{detect}$ to Eq. (5) gives the adapted token $Tok_{detect}$. Finally $Tok_{detect}$ is passed through a projection layer, which is a transformation $f_{proj}: R^{d_{emb}} \\rightarrow R^{d_{emb}}$. M in this case is equally 6 + 6N. In such as way, the Detector Query $Q_{detect}$, enriched with camera-specific visual information, is integrated into the Transformer's processing pipeline.\nOur use of trainable ID-separator tokens to separate groups of tokens in concatenation could be seen as an extension to trainable adapter prompt technique used in Zhang et al. (2023a). In Zhang et al. (2023a), llama-adapter used trainable adapter prompt to store guidance to language model learnt through fine-tuning. We used a similar structure differently to encode the token group information, prefixed each token sequence with unique \"trainable prompt\" which we call ID-separator, separating object from different images."}, {"title": "3.4 Next Word Prediction from Merged Token Embeddings", "content": "According to our framework Fig. 1, we merge layer-wise Perception/Detection tokens with contextual tokens as\n$Tok_{merged}^{(l)} = f_{block}(z^{(l)}) + g_{percept} Tok_{percept}^{(l)} + g_{detect} Tok_{detect}^{(l)},$\nwhere $Toks$ are generated via Eq. (5). $g+s$ are trainable zero gates as introduced in Zhang et al. (2023a). The output $Tok^{(l)} \\in R^{N\\times d_{emb}}$ from the final Transformer Block is passed to the Prediction Head, which generates the next word in the sequence based on both the textual and visual contexts. The predicted next word\n$NextWord = Sample(Softmax(FFN((Tok_{merged}^{(L-1,:)}))),$\nwhere FFN : $R^{d_{emb}} \\rightarrow R^{d_{vocab}}$ here is a feed forward network transforming the vector in model dimension $R^{d_{emb}}$ to the vocabulary size $R^{d_{vocab}}$ (typically 32000). The softmax is required after FFN for inspecting the sampling-probability of each word within the vocabulary dictionary, and finally a sampler is applied for generating the next word. The framework repeat this procedure for continued word generation until the [eos] token is generated."}, {"title": "4 Experiments and discussions", "content": ""}, {"title": "4.1 Experiment details", "content": "We used NuScenes(Caesar et al. (2019)) for finetuning and experiment. The dataset is structured into training set, test and validation set. The training set consist of 2896 QA pairs, each corresponds to a scene with 6 camera images. We tested on test dataset of length 66 and 2987, as well as on our validation set of length 15480."}, {"title": "4.1.1 Experiments", "content": "We list the experiments as follows\nGround Truth Using ground truth in place of model output; used as reference of best scores a model can get.\nGround Truth (only Tag 0 correct) Using ground truth in place of model output only for questions in NuScenes dataset with tag 03\nDriveLM-Agent Using DriveLM-Agent (Sima et al. (2023)) to answer the questions, and compare output with ground truth.\nOur Method (Llama-Adapter) Using llama-adapter-v2-multimodal-7b (Zhang et al. (2023b)) to answer questions and compare with ground truth.\nOur Method (Yolos) Using our architecture integrating Yolos-based detection network to answer the questions, and compare output with ground truth."}, {"title": "4.1.2 Metrics", "content": "The metrics employed in the experiments are as follows\nAccuracy 1 if the output string is exactly the same as the ground truth answer, 0 otherwise. Average is taken on all (output, ground truth) pairs.\nChatGPT Score given by ChatGPT, with prompt \"rate the following answer based on the correct answer\", providing it ground truth and output.\nMatch percentage of points in groundtruth (e.g., \"[1.,2.]\") that are \"close\" to any point in question (with $L^{1}$ distance less than 16)\nBLEU_{1,2,3,4} BLEU score for text similarity evaluation with {1,2,3,4} n-gram precisions.\nROGUE_L The ROGUE-L(Lin and Och (2004)) longest-common-sequence-based score for text similarity evaluation.\nCIDEr The CIDEr (Vedantam et al. (2014)) score for image description evaluation.\nFinal_Score The final score is a weighted sum of the previous scores, defined as:\n$Final_{Score} = 0.4 \\times \\frac{ChatGPT}{100} + 0.2 \\times (\\frac{match}{100} + 0.2 \\times accuracy + 0.2 \\times \\sum_{i=1}^{4} \\frac{BLEU_{i}}{3} + \\frac{ROGUE_{L} + \\frac{CIDEr}{10}}{3})$"}, {"title": "4.2 Analysis of Experiment Results", "content": "From Tab. 1-3, we conclude the followings\nGood Performance with Smaller Sized Test Dataset. The main difference caused by size of test dataset is distribution of question types and variety of scenario/objects. As test datasets are not used in training, it's likely that over-fitting is not the cause; rather, this indicates that the addition of object detection modality could be enhancing answering of different types of questions differently. A next step in this direction is to evaluate the model's performance on questions with different tags seperately, as well as visualizing characteristics of question it got wrong/right. Apart from shift of question category distribution, larger dataset could also be challenging when more types of objects are getting involved, whereas YOLOS can only label 92 classes of objects.\nGenerally Better Than Llama-Adapter. In all experiments our model performs better than its predecessor (Llama-Adapter with only CLIP visual network), indicating that object detection modality, if not generally useful in visual question answering, enhance more answers than those it corrupts.\nHigh Match Score. Our model's Match score in all experiments are particularly high as compared to other metrics, as well as other models. Match score specifically measures how accurate points in answers are compared to ground truth, such as current or future coordinates of objects. High Match score indicates that addition of object detection modality enhanced precise object position detection and/or subsequent prediction/reasoning.\nLow CIDEr Score on Larger Dataset. Our model have especially lower CIDEr score on larger datasets than llama-adapter, while higher CIDEr score on smaller datasets. Since CIDEr score measures accuracy of image captioning, it indicates that addition of YOLOS object detection modality corrupts the model's image captioning ability on larger datasets. Our guess is while CLIP is pre-trained on a very large WebImageText dataset (Radford et al. (2021b)), YOLOS is pre-trained on COCO (Lin et al. (2015)), which is much smaller and have only 92 classes of objects. Therefore, it's lacking captioning capability could be corrupting CLIP's work, such as by classifying detected object wrong or way too simply(e.g., car instead of black Nissan), while the test dataset gets larger and more types of objects start appearing in the dataset."}, {"title": "4.3 Using additional modalities to defend against backdoor attack", "content": "As shown by Perez et al. (2022); Liu et al. (2024); Ni et al. (2024), LLM/VLM could be subject to various visual attacks, exploiting vulnerabilities from the visual modality, such as modality red-teaming, backdoor attack with trigger objects, adding to risks in autonomous driving setting.\nWith detection modality, and perhaps more modalities to add, we can put up defense against such attacks by (1) Reaping more robust understanding with the driving scene backed and double checked with multiple modalities and (2) Distributing visual modality's potentially pivoting influence to model's output over to detection modalities and other potential modalities.\nWith object detection modality as example:\nHigher-precision Scene Understanding In Ni et al. (2024), one example of such backdoor attack is by showing a photo of a red balloon, to trigger the model to output sudden high acceleration. The attack is done with poisonous training example that associates red balloon with sudden acceleration. With addition of object detection modality, the scene will no longer be percieved as only \"scene with"}, {"title": "5 Conclusion", "content": "This paper introduces a novel framework enhancing visual comprehension in autonomous driving systems by integrating large language models. We combine a YOLOS-based detection network with a CLIP perception network in the Llama-Adapter framework, improving object recognition and scene understanding. Our approach, featuring innovative camera ID-separators for multi-view processing, shows significant performance gains on the DriveLM challenge. This work advances the development of more capable and interpretable autonomous driving systems that effectively merge language models with visual perception. Our experimental results show competitive performance across various metrics, including ChatGPT scores and BLEU scores. While there's room for improvement in accuracy, our approach shows promise in combining LLMs with specialised visual modules for autonomous driving. The addition of detection modalities can also potentially enhance driving agent's safety. Future work should focus on further integrating detection and perception networks, enhancing multi-view processing, explore the enhancement of safety against various attacks and expanding the dataset for more comprehensive evaluations. This research represents a step towards more capable and interpretable autonomous driving systems that leverage the strengths of both language models and visual perception modules."}, {"title": "6 Limitations", "content": "Dependency on High-Quality Dataset. Experiment is carried out only on a portion of Nuscenes dataset. The current evaluation is confined to specific datasets and scenarios. The model's performance in diverse, real-world environments remains untested. The model's performance heavily relies on the quality and variety of the data used for training. Any biases or deficiencies in the dataset could adversely affect the system's reliability and decision-making.\nModel Complexity and Computation. The framework have good scalability in term of finetuning, inherited from llama-adapter. The large base models(Llama, CLIP, YOLOS) are frozen, only adapter networks are trained. Inference-wise, increase of computation cost is linear to number of adapted networks a fixed number more step will be computed in decoder layers, and a fixed number more steps will be computed in processing. This might limit the deployment of such systems in real-world scenarios where computational resources are constrained.\nScalability Issues. The scalability of the proposed approach to larger, more diverse datasets and across different geographic regions has not been fully explored. This includes challenges related to adapting the system to various driving conditions and legal requirements. Autonomous driving systems must adhere to evolving regulations and standards.\nInterpretability and Error Analysis. While the model incorporates LLMs for better interpretability, the complex interactions between textual and visual data within the model make it challenging to pinpoint the source of errors and understand decision-making processes in depth."}]}