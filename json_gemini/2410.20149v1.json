{"title": "AdaNeg: Adaptive Negative Proxy Guided\nOOD Detection with Vision-Language Models", "authors": ["Yabin Zhang", "Lei Zhang"], "abstract": "Recent research has shown that pre-trained vision-language models are effective at\nidentifying out-of-distribution (OOD) samples by using negative labels as guidance.\nHowever, employing consistent negative labels across different OOD datasets often\nresults in semantic misalignments, as these text labels may not accurately reflect\nthe actual space of OOD images. To overcome this issue, we introduce adaptive\nnegative proxies, which are dynamically generated during testing by exploring\nactual OOD images, to align more closely with the underlying OOD label space\nand enhance the efficacy of negative proxy guidance. Specifically, our approach\nutilizes a feature memory bank to selectively cache discriminative features from\ntest images, representing the targeted OOD distribution. This facilitates the creation\nof proxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset, the\nsample-adaptive proxies weight features based on their similarity to individual test\nsamples, exploring detailed sample-level nuances. The final score for identifying\nOOD samples integrates static negative labels with our proposed adaptive proxies,\neffectively combining textual and visual knowledge for enhanced performance.\nOur method is training-free and annotation-free, and it maintains fast testing speed.\nExtensive experiments across various benchmarks demonstrate the effectiveness\nof our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet\nbenchmark, our AdaNeg significantly outperforms existing methods, with a 2.45%\nincrease in AUROC and a 6.48% reduction in FPR95. Codes are available at\nhttps://github.com/YBZh/OpenOOD-VLM.", "sections": [{"title": "1 Introduction", "content": "In real applications, artificial intelligence (AI) systems often encounter test samples of unknown\nclasses, termed out-of-distribution (OOD) data. These OOD data often result in overly confident\nerrors [52, 44], posing security threats. Therefore, accurately identifying OOD data is essential for\nensuring the reliability and security of AI systems in open-world environments.\nTraditional OOD detection methods in image domain primarily rely on vision-only models [20, 33, 36].\nRecent advancements in vision-language models (VLMs) have demonstrated remarkable OOD\ndetection performance by leveraging multi-modal knowledge [17, 15, 39]. Recently, NegLabel [27]\nexplores negative labels by identifying text labels that are semantically distant from the in-distribution\n(ID) labels. This method achieves state-of-the-art performance by detecting test images closer to\nnegative labels as OOD. In other words, NegLabel regards these negative labels as proxies of OOD\ndata. However, employing consistent negative labels across different OOD datasets often leads to\nsemantic misalignment, where these text labels may not accurately reflect the actual label space\nof OOD images, as shown in Fig. 1. This misalignment between the proxies and targeted OOD\ndistribution leads to sub-optimal performance.\nTo promote the alignment between the negative proxies and target OOD distribution, we introduce the\nAdaptive Negative proxies (AdaNeg), which are dynamically generated during testing by exploring\nactual OOD images. Specifically, we start by initializing an empty category-split memory bank for\neach OOD dataset and selectively cache features of discriminative OOD images during testing. The\nOOD discrimination is assessed using mined negative labels, as detailed in [27]. With this feature\nmemory, we develop task-adaptive proxies by simply averaging cached features within each category.\nThese proxies, derived from actual OOD images, reflect the distinct characteristics of the target OOD\ndataset and align more closely with the underlying OOD label space.\nThe task-adaptive proxies mentioned previously provide unique proxies for different OOD datasets\nwhile maintaining consistency across various test samples within the same dataset. To delve into the\nfine-grained nuances at the sample level, we introduce the sample-adaptive proxies by weighting\ncached features based on their similarity to a particular test sample. This is achieved with an attention\nmechanism, where the feature memory serves as both keys and values, and the test feature acts as the\nquery. The final score for detecting OOD samples integrates static negative labels with our adaptive\nproxies, effectively combining textual and visual knowledge for enhanced performance.\nWe conduct extensive experiments on standard benchmarks to validate the effectiveness of AdaNeg,\nwhere our proposed adaptive proxies outperform the negative-label-based one, enhancing performance\nwith complementary multi-modal knowledge. Particularly, on the large-scale ImageNet dataset, our\nAdaNeg method outperforms existing methods by 2.45% AUROC and 6.48% FPR95. Notably, our\nmethod is training-free and annotation-free, and it maintains fast testing speed, as analyzed in Tab.\n4. The ability to dynamically adjust to new OOD datasets without affecting testing speed or labor-\nintensive annotation/training makes our approach particularly valuable for real-world applications\nwhere adaptability and efficiency are crucial. We summarize our contribution as follows:\n\u2022 We first identify the label space misalignment between existing negative-label-based proxies\nand the target OOD distributions. In response, we introduce adaptive negative proxies that\nare dynamically generated during testing by exploring actual OOD images, resulting in a\nmore effective alignment with the OOD label space.\n\u2022 Our adaptive negative proxies are constructed with a feature memory bank that selectively\ncaches discriminative image features during testing. We instantiate this concept by devel-\noping task-adaptive proxies to reflect the unique characteristics of each OOD dataset and"}, {"title": "2 Related Work", "content": "OOD Detection focuses on identifying OOD test samples with semantic shifts, thus distinguishing\nit from generalization studies which typically focus on covariate shifts [3\u20135, 75]. A variety of\nOOD detection techniques have been developed, which can be roughly categorized into score-based\n[20, 33, 36, 37, 66, 26, 64, 56], distance-based [58, 59, 57, 12, 41, 53], and generative-based [50, 29]\nmethods. Among them, score-based methods are particularly notable by employing a variety of\nscoring functions to differentiate between ID and OOD samples. These functions include confidence-\nbased [20, 36, 56, 64], discriminator-based [29], energy-based [37, 66], and gradient-based [25]\nscores. In contrast, distance-based methods determine OOD samples by evaluating the distance in the\nfeature space between test data and the closest ID samples [58] or ID prototypes [59], using metrics\nsuch as KNN [57, 12, 41] or Mahalanobis distance [33, 53].\nDespite their achievements, traditional OOD detection methods generally rely on manually annotated\nID images and often overlook the integration of textual information. To leverage the textual knowl-\nedge, recent advancements have focused on employing VLMs [39, 40, 27, 77, 76, 15, 42, 65, 45].\nSpecifically, ZOC [15] applies VLMs to discern OOD instances by training a captioner that generates\npotential OOD labels. Nevertheless, this captioner often fails to produce effective OOD labels,\nparticularly for ID datasets containing many classes. LoCoOp [42] adopts a novel approach by\nlearning ID prompts from few-shot ID samples, and further enhances the robustness of these prompts\nby incorporating OOD features mined from the backgrounds of images. CLIPN [65], LSN [45]\nand LAPT [76] explore learning text prompts for expressing negative concepts. In specific, CLIPN\ninitializes text prompts with the word 'no' combined with ID labels and refines them with large-scale\nmulti-modal data; LSN starts with manually collected ID samples to learn negative prompts, offering\na different approach to leveraging textual information in OOD detection; LAPT conducts automated\nprompt tuning with automatically collected training samples, boosting OOD detection without any\nmanual effort. MCM [39] utilizes ID class names to facilitate effective zero-shot OOD detection. It is\nfurther refined by NegLabel [27], which incorporates additional negative class names mined from\navailable data sources as negative proxies. However, as illustrated in Fig. 1, there is a mismatch be-\ntween the negative-label-based proxies and the target OOD distribution, underscoring the limitations\nof this strategy. This observation has inspired us to construct adaptive proxies by exploring potential\nOOD test images during testing. This leads to an efficient method that aligns better with the target\nOOD distribution, resulting in enhanced OOD detection performance.\nFurthermore, we clarify the relationship between our method and existing approaches on OOD\nexposure [17, 21, 73]. Most OOD exposure methods introduce manually collected negative images\nduring training, where manual labor is necessary to ensure that the labels of negative images are\ndifferent from ID ones. Moreover, involving negative images in training typically introduces additional\ncomputational overhead, impeding its practical deployment. Unlike these methods, NegLabel [27] is\nexposed to negative labels during the test phase in a training-free manner. However, given a fixed ID\ndataset, the exposed negative texts remain consistent for different OOD datasets, inevitably resulting\nin label misalignment, as shown in Fig. 1. To address this, we expose the VLMs to adaptive negative\nproxies, which explore actual OOD samples during testing and align more effectively with OOD\ndistribution. Our method does not require manual annotations and works in a training-free manner,\nmaking it an appealing solution for real applications.\nTest-time Adaptation. We adopt an online update of the negative proxies during testing, resembling\ntest-time adaptation (TTA) methods [35, 63, 54]. Existing TTA methods primarily address covariate\nshifts between training and testing domains. In contrast, our approach mitigates the label shift\nbetween negative proxies and the target OOD distribution by exploring online test samples. Recently,\nTTA strategies have been considered in the field of OOD detection. However, these methods typically\nrequire test-time optimization [18, 72, 16], slowing down the testing process. In contrast, our method"}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nOOD Detection Setup. Consider X as the image domain and Y = {y1, ..., yc } as the space of ID\nclass labels, where y comprises text elements such as y = {cat, dog, ..., bird}, and C represents\nthe total number of classes. Let \u00e6in and \u00e6ood be random variables representing ID and OOD samples\nfrom X, respectively. We define P\u00e6in and P\u00e6ood as the marginal distributions for ID and OOD,\nrespectively. In conventional classification scenarios, it is assumed that the test image \u00e6 originates\nfrom the ID and is associated with a specific ID label, specifically, x \u2208 P\u00e6in and y \u2208 y, with\ny being the label of x. However, in real applications, AI systems often face data from unknown\nclasses, denoted by x \u2208 Pxood and y \u2209 Y. Such occurrences can make AI models incorrectly\ncategorize these instances into familiar ID categories with substantial certainty [52, 44], resulting in\nsecurity concerns. To address these challenges, OOD detection is proposed to accurately categorize\nID samples into their respective classes and reject OOD samples as non-ID. Recognition within the\nID categories is performed using a C-way classifier, following standard classification approaches\n[31, 19]. Concurrently, OOD detection typically employs a scoring mechanism S [33, 36, 37] to\ndifferentiate between ID and OOD inputs:\n$G_\\gamma(x) = \\text{ID}, \\text{ if } S(x) \\geq \\gamma; \\text{ otherwise, } G_\\gamma(x) = \\text{OOD},$  (1)\nwhere $G_\\gamma$ represents the OOD detector set at a threshold $\\gamma \\in \\mathbb{R}$. The test sample \u00e6 is identified as an\nID sample if and only if $S(x) \\geq \\gamma$.\nCLIP and NegLabel. For an ID test image x within the label space Y, we derive the image feature\nvector v = fimg(x) \u2208 RD and the text feature matrix Cid = ftxt(p(V)) \u2208 RC\u00d7D using pre-trained\nCLIP encoders, where D represents the feature dimension. The functions fimg(\u00b7) and ftxt(\u00b7) are the\nencoders for images and text, respectively. The function p(\u00b7) is the text prompt mechanism, typically\ndefined as 'a photo of a <label>.', where label is the actual class name, for example, 'cat' or 'dog'.\nBoth v and Cid undergo L2 normalization across the dimension D. Then, zero-shot classification\nprobabilities are computed utilizing C as the classifier:\n$p^{id} = \\text{Softmax}(vC/\\tau) \\in \\mathbb{R}^C,$  (2)"}, {"title": "3.2 AdaNeg", "content": "where T > 0 is the scaling temperature.\nThe vanilla CLIP is proposed for zero-shot ID recognition and has recently been extended to\nOOD detection. Specifically, the NegLabel approach [27] introduces negative class names Y\u00af =\n{YC+1,...,\u0423\u0421+M} sourced from broad text corpora, where M is the length of negative classes and\n\u05be\u05e2 \u2229 Y = 0. Then, we can obtain the full text feature matrix C = ft\u00e6t (p(YUY\u00af)) \u2208 R(C+M)\u00d7D\nwith the pre-trained CLIP text encoder, leading to the classification probability across C + M classes:\n$p = \\text{Softmax}(vC^T /\\tau) \\in \\mathbb{R}^{C+M}.$  (3)\nAssuming that ID samples exhibit greater similarity to ID labels and lesser similarity to negative\nlabels compared to OOD samples, NegLabel introduces the following score for OOD detection:\n$S_{nl}(v) = \\sum_{i=1}^{C}p_i$  (4)\nwhere p\u2081 is the i-th entry of p, indicating the classification probability of the i-th class. Intuitively,\nthe NegLabel method uses negative labels as proxies of the OOD distribution, detecting OOD images\nbased on the similarity to these negative labels.\nAlthough NegLabel has successfully employed negative labels as the OOD proxies, there is a semantic\nmisalignment between such OOD proxies and actual OOD labels, as illustrated in Fig. 1. We aim\nto obtain OOD proxies that align better to the targeted OOD distribution. However, acquiring such\nOOD proxies is challenging, as the OOD information is unknown prior to actual testing.\nFrom another perspective, we can access real OOD information during testing, motivating us to refine\nOOD proxies in the testing stage. We can identify discriminative negative images during testing and\nthen adjust the OOD proxies toward detected images. This is achieved by selectively caching features\nof test images into a task-aware memory bank, followed by memory reading operations to produce\nadaptive proxies. We detail our implementation as follows.\nTask-aware Memory. We construct a task-aware memory as a category-split tensor M\u2208\nR(C+M)\u00d7L\u00d7D, where L is the memory length for each category. M is initialized with zero values\nand gradually filled with features of selected images during testing. Specifically, for a test image with\nfeature v, we first calculate its score Snl(v) with Eq. (4). If Sni(v) < \u03b3, we detect this test point\nas a negative image, and otherwise, it is identified as a positive sample. For negative and positive\nimages, we respectively identify their closest labels as:\n$\\text{Negative} : y = \\text{arg max } p^{ood}_i + C,$\n(5)\n$\\text{Positive}: y = \\text{arg max } p^{id}_i ,$\n(6)\nwhere pood = p[C : C + M] and pid = p[0 : C'] are the classification probabilities corresponding to\nnegative and ID class names, respectively. Then, we cache this feature v into the task-aware memory:\n$M_{y,l} = v,$ (7)\nwhere l indicates an empty slot of My \u2208 RL\u00d7D. Once My is filled, we drop the image feature with\nthe highest prediction entropy, as detailed in Appendix A.2. In one word, we keep confident image\nfeatures with low prediction entropy in the memory.\nThe aforementioned strategy attempts to cache all test images, including those with high confusion\nbetween ID and OOD. However, we found that selectively retaining only those image features that\nexhibit strong ID/OOD distinguishing capabilities can effectively reduce this confusion. Specifically,\nwe modify the selection criterion for memorization as follows:\n$\\text{Negative}: S_{nl}(v) < \\gamma \\rightarrow S_{nl}(v) < \\gamma - g\\gamma,$\n$\\text{Positive} : S_{nl}(v) \\geq \\gamma \\rightarrow S_{nl}(v) \\geq \\gamma + g(1 - \\gamma),$\n(8)\nwhere g \u2208 [0, 1] is a hyperparameter that introduces a gap in the score space. Consequently, image\nfeatures falling within the gap y \u2212 gy \u2264 Sni(v) < \u03b3 + g(1 \u2212 \u03b3) are considered to have low\ndistinguishing confidence and are not cached."}, {"title": "Task-adaptive Proxies", "content": "Given the updated M, we can easily get the task-adaptive proxy by averaging\nalong the length dimension L:\n$C^{ta} = \\mathcal{L}_2(\\frac{1}{L+1} \\sum_{l=1}^{L+1} M_{:,l,:}) \\in \\mathbb{R}^{(C+M)\\times D},$  (9)\nwhere $\\mathcal{L}_2(\\cdot)$ indicates the $\\mathcal{L}_2$ normalization along feature dimension $D$, and $\\tilde{M} = [M, C] \\in\n]$\\mathbb{R}^{(C+M)\\times (L+1) \\times D}$ is the extended memory with vanilla text proxies $C$. Such a memory extension\nis necessary since $M$ is initially empty and uninformative. Initially, this extension initializes the\nadaptive proxies $C^{ta}$ with the basic text proxies $C$. However, there are two key distinctions between\nthe adaptive $C^{ta}$ and the vanilla $C$. First, unlike the NegLabel approach, which employs a fixed\nproxy $C$ across various OOD datasets, our $C^{ta}$ dynamically adjusts to the targeted OOD domain as\nthe memory accumulates data, thereby providing dataset-specific adaptive proxies. Second, the $C^{ta}$\nprimarily incorporates image features, offering modal knowledge that complements the text-based\nproxies $C$. The benefits of this approach are further analyzed in Tab. 3.\nThe score function for OOD detection with the task-adaptive proxy $C^{ta}$ is derived as:\n$S_{ta}(v) = \\frac{\\sum_{i=1}^{C} e^{cos(v, c^{ta}_i)/\\tau}}{\\sum_{i=1}^{C} e^{cos(v, c^{ta}_i)/\\tau} + \\sum_{j=C+1}^{C+M} e^{cos(v, c^{ta}_j)/\\tau}},$  (10)\nwhere $cos(\\cdot, \\cdot)$ measures the cosine similarity, and $c^{ta}_i$ is the $i$-th entry of $C^{ta}$."}, {"title": "Sample-adaptive Proxies", "content": "As evidenced in Table 3, our task-adaptive proxies significantly out-\nperform the fixed text proxies by effectively adapting to the characteristics of target OOD dataset.\nBuilding on this success, we further refine our approach by leveraging finer-grained, sample-level\nnuances to introduce even more effective sample-adaptive proxies. Specifically, given the extended\nmemory M and the test image feature v, we introduce the sample-adaptive proxy $C^{sa} \\in \\mathbb{R}^{(C+M)\\times D}$\nvia the following cross-attention operation:\n$c^{sa}_i = \\mathcal{L}_2 (\\psi(v(M_i)^T) M_i) \\in \\mathbb{R}^D,$\n(11)\nwhere $v(M_i) \\in \\mathbb{R}^{1\\times (L+1)}$ measures the cosine similarities between normalized features of v and\n$M_i$, and $\\psi(x) = exp(-\\beta (1-x))$ modulates the sharpness of $x$ with hyper-parameter B. $c^{sa}_i$ and\n$M_i$ are the i-th entry of $C^{sa}$ and $M$, respectively.\nBoth the task-adaptive and the sample-adaptive proxies are derived from the memorized image\nfeatures stored in M. The primary distinction between them lies in their respective weighting\nstrategies. For $C^{ta}$, each feature $M_{:,l,:}$ in the memory is assigned a uniform weighting coefficient\nof $\\frac{1}{L+1}$. Conversely, in constructing $C^{sa}$, the weighting coefficient for each cached feature is\ndynamically determined based on its cosine similarity to the test image feature, denoted as $v(M_i)^T$.\nConsequently, while $C^{ta}$ remains constant across different test samples, $C^{sa}$ adapts to each individual\ntest sample. This adaptability allows $C^{sa}$ to tailor its response based on the specific characteristics"}, {"title": "Multi-modal Score", "content": "As previously discussed, the score function Snl (v) relies primarily on text\nfeatures, whereas the sample-adaptive score function Ssa (v) utilizes cached image features. Given the\ncomplementary nature of text and image modalities, we derive the final score function by integrating\nknowledge from both modalities:\n$S_{all}(v) = S_{nl}(v) + \\lambda S_{sa}(v),$ (13)\nwhere x > 0 is the hyperparameter balancing the two modalities. The overall pipeline of our method\nis illustrated in Fig. 2 and summarized in Algorithm 1."}, {"title": "4 Experiments", "content": "4.1 Setup\nDatasets. We conduct extensive experiments with the large-scale ImageNet-1k [9] as ID data.\nFollowing prior practice [26, 39, 27], four OOD datasets of iNaturalist [60], SUN [68], Places [78],\nand Textures [7] are evaluated. We also validate our method on the OpenOOD benchmark [74, 71],\nwhere OOD datasets are grouped into near-OOD (e.g., SSB-hard [62], NINCO [2]) and far-OOD\n(e.g., iNaturalist [60], Textures [7], OpenImage-O [64]) according to their similarity to ImageNet\ndataset. Besides ImageNet, we also evaluate our method on smaller-sized CIFAR10/100 datasets [30]\nwith the OpenOOD setup. Specifically, with the ID dataset of CIFAR10/100, we adopt near-OOD\ndatasets of CIFAR100/10 and TIN [32], and far-ood datasets of MNIST [10], SVHN [43], Texture [7],\nand Plances365 [78]. These experiments with various ID and OOD datasets enable a comprehensive\nevaluation on various OOD settings.\nImplementation Details. We adopt the visual encoder of VITB/16 pretrained by CLIP [48] and\nanalyze more backbone architectures in Tab. A11. For hyper-parameters, we adopt the memory\nlength L=10, threshold y=0.5 with the gap g=0.5 in Eq. 8, \u03b2=5.5 in Eq. 11, and \u5165=0.1 in Eq. 13 in\nall experiments. These hyper-parameters are carefully analyzed in Sec. 4.3. Following NegLabel, we\nadopt the text prompt of 'The nice <label>.', set temperature T=0.01, and define the number M of\nnegative labels as 10,000 for the ImageNet dataset. For the CIFAR datasets, we set the number M as\n70, 000 since we find that a larger M leads to better results for CIFAR. Following common practice\n[26, 39, 27], we employ the evaluation metrics of FPR95, AUROC, and ID ACC, which are detailed\nin Appendix A.3. All experiments are conducted with a single Tesla V100 GPU."}, {"title": "5 Conclusion and Limitations", "content": "We proposed adaptive negative proxies that aligned more effectively with OOD distributions, thereby\nproviding more effective guidance for OOD detection. These proxies were constructed using a\ntask-aware feature memory that selectively cached discriminative image features during testing.\nOur approach facilitated the generation of both task-adaptive and sample-adaptive proxies through\ncarefully designed memory reading operations. Notably, our method was training-free and annotation-\nfree, and it maintained fast testing speed and achieved state-of-the-art results on various benchmarks.\nThese results validated the effectiveness of the proposed adaptive proxies.\nOne minor limitation of our method is the introduction of an external memory requirement. For\nexample, this memory occupies a storage space of 214.75MB when using the ImageNet dataset as\nthe ID, which may pose challenges for storage-constrained applications."}, {"title": "A.1 ID-Similarity to OOD Ratio with ground truth ID and OOD labels", "content": "We introduce the ID-Similarity to OOD Ratio (ISOR) to quantitatively measure the relative alignment\nof negative proxies with ground truth OOD labels, compared to their alignment with ID labels. In\nimplementation, we adapt the score function of NegLabel (i.e., Eq. 4), which originally measures the\nsimilarity of a test image to ID labels over negative labels. We modify this function by replacing the\nnegative labels with ground truth OOD labels and changing the input from test images to negative\nproxies (e.g., negative texts), while keeping other aspects consistent with Eq. 4. This modified\nscore function allows us to assess the degree of similarity between the inputs and ground truth\nID/OOD labels, thereby quantifying the relative alignment between negative proxies and OOD labels.\nSpecifically, lower ISOR indicates a higher similarity to OOD labels and a reduced similarity to ID\nlabels."}, {"title": "A.2 Entropy-based caching strategy for full memory", "content": "The memory we construct is of finite length; hence, as the number of cached images increases, it\nmay become fully occupied. To address this, we have devised a simple strategy to retain only those\nimages with low entropy, e.g., high confidence. Specifically, when storing an image in memory, we\nalso record its entropy pertinent to OOD detection:\n$\\text{Entropy}(v) = -S_{nl}(v) \\text{log}(S_{nl}(v)) - (1 - S_{nl}(v)) \\text{log}(1 - S_{nl}(v)),$  (A.1)\nwhere Snl (v) represents the probability of belonging to the ID, as shown in Eq. (4). Given the entropy\nof the current test image and a full memory My, we replace the item with the maximum entropy in\nMy with the current image feature if the current test image exhibits lower entropy. Otherwise, we do\nnot cache the current test sample."}, {"title": "A.6 More analyses", "content": "Detailed Implementation of AdaGap Module. We have implemented an adaptive gap (AdaGap)\nmodule to adjust the memorization selection criteria dynamically. This strategy builds on the\nobservation that as the score Sni increases/decreases, the probability that a sample is ID/OOD also\nincreases accordingly. By enforcing a stringent selection criterion, we can effectively minimize the\ninclusion of misclassified samples in our memory. Specifically, we first online estimate the ratio\nof ID to OOD samples in the test data using a First-In-First-Out queue, which caches the ID/OOD\nestimation (cf. Eq. 8) of the most recent N samples:\n$MR = \\frac{\\text{Estimated ID Number}}{\\text{Estimated ID Number + Estimated OOD number}},$ (A.2)\nwhere the ID and OOD numbers are acquired within the queue.\nLeveraging the estimated mix ratio (MR), we can dynamically adjust the gap g in memory caching\nto avoid a majority of misclassified samples within the memory. For instance, if we find that ID"}]}