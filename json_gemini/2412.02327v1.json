{"title": "Switchable deep beamformer for high-quality and real-time passive acoustic mapping", "authors": ["Yi Zenga", "Jinwei Lib", "Hui Zhu\u00aa", "Shukuan Lud,*", "Jianfeng Lib,c,*", "Xiran Caia,e,f,*"], "abstract": "Passive acoustic mapping (PAM) is a promising tool for monitoring acoustic cavitation activities in the applications of ultrasound therapy. Data-adaptive beamformers for PAM have better image quality compared to the time exposure acoustics (TEA) algorithms. However, the computational cost of data-adaptive beamformers is considerably expensive. In this work, we develop a deep beamformer based on a generative adversarial network, which can switch between different transducer arrays and reconstruct high-quality PAM images directly from radio frequency ultrasound signals with low computational cost. The deep beamformer was trained on the dataset consisting of simulated and experimental cavitation signals of single and multiple microbubble clouds measured by different (linear and phased) arrays covering 1-15 MHz. We compared the performance of the deep beamformer to TEA and three different data-adaptive beamformers using the simulated and experimental test dataset. Compared with TEA, the deep beamformer reduced the energy spread area by 18.9%-65.0% and improved the image signal-to-noise ratio by 9.3-22.9 dB in average for the different arrays in our data. Compared to the data-adaptive beamformers, the deep beamformer reduced the computational cost by three orders of magnitude achieving 10.5 ms image reconstruction speed in our data, while the image quality was as good as that of the data-adaptive beamformers. These results demonstrated the potential of the deep beamformer for high-resolution monitoring of microbubble cavitation activities for ultrasound therapy.", "sections": [{"title": "1. Introduction", "content": "Passive acoustic mapping (PAM) is a promising tool for localizing acoustic cavitation sources to monitor and guide ultrasound therapies. This imaging mode uses passively received acoustic emissions originated from a region of interest using an ultrasonic array, and allows continuous monitoring of cavitation activities without interrupting the therapeutic pulses [1]. PAM has demonstrated its efficacy in monitoring acoustic cavitation activities during thermal ablation treatments [2, 3], histotripsy [4] and microbubble (MB) mediated ultrasound treatments for drug delivery [5], and blood-brain barrier opening in both animal models and human subjects [6]."}, {"title": "2. Related work", "content": "The goal of PAM is to reconstruct a map representing the spatial distribution of the cavitation energy. The map can be formed based on the DAS operation. Considering an array with N elements, the jth delayed element signal is given as:\ns_j(r,t) = d_j(r)p_j(t + \\tau_j (r))\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n(1)\n\\end{document}\nwhere p\u2c7c(t) is the passively received RF signal by the jth element, \u03c4\u2c7c(r) and dj(r) are the traveling time and distance of the sound wave from location r to the jth element, respectively. The delayed element signals are firstly weighted and summed across the receiving channels coherently. Then, the signal energy of the coherently summed signal over a period of time is calculated and scaled by a factor of 4\u03c0 to represent the cavitation source energy at r:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\Psi(r) = \\frac{4\\pi}{p_0} \\int_{T_0}^{T_0 + \\Delta T} (w(r) \\sum_j s_j(r,t))^2 dt\n\\end{document}\n(2)\nwhere \u03c1\u2080 is the density of the medium, T\u2080 is the onset of the interested time interval [T\u2080,T\u2080 + \u25b3T] for the RF signal, and w(r) is the channel specific weights for location r.\nEstimation of cavitation source energy depends on the choice of w. TEA [23] assumes uniform channel weights, whereas data-adaptive beamformers calculate the weights by minimizing the output power of interfering signals. Here, we briefly summarize the weight calculation for the data-adaptive beamfomers (Table 1).\n1) EISRCB [10]: Considering steering vector uncertainty, RCB utilizes the covariance matrix of the delayed signals (R(r) = [\u222bTo+Ts(r,t)s(r,t)dt) and minimizes the output power of interfering signals (Eq. (4)) to suppress artifacts [9]. To further improve the capability of artifacts suppression, EISRCB calculates the weights by projecting the RCB weights (wRCB) onto the signal subspace of R by Eq. (5).\n2) DAX-RCB[12]: DAX-RCB combines dual apodization with cross-correlation (DAX-TEA) [24] that uses two complementary apodization functions to suppress unwanted sidelobes, clutter signals, and the artifacts in RCB in the region where the coherence is weak. The received signals are divided by a pair of complementary apodization functions which become two groups of signals that are used to calculate the minimum variance weights by Eq. (4) and then beamformed to obtained s\u2081(r,t) = wkcbs(r,t) and s\u2082(r,t) = wRCBs(r, t). The weights in DAX-RCB are then calculated according to Eq. (6), which utilizes the normalized cross-correlation coefficient to evaluate the similarity between s\u2081(r, t) and s\u2082(r,t).\n3) RLPB[11]: The aforementioned data-adaptive beamformers assume that the signals of interest follow Gaussian distribution while it was argued that broadband bubble emissions may deviate from Gaussian distribution. RLPB thus utilizes higher order statistics of signals to optimize the weighting vector. The weights are obtained by minimizing the l\u221e-norm of the beamformer's output according to Eq. (7).\nIn summary, TEA, a non-data-adaptive beamformer, uses equal weighting factors for all the channels and has low image resolution and high-level artifacts. Data-adaptive beamformers including EISRCB, DAX-RCB and RLPB were proposed to improve image quality for PAM. These methods adopt various strategies to calculate the weights for image quality enhancement while some are sensitive to the required user-defined parameters. Also, their computational costs are different. TEA is the most computationally efficient method and RLPB is the most computationally expensive method. During the implementation of the data-adaptive beamformers for PAM, we found that EISRCB is less sensitive to the user-defined parameters and is less computationally intensive among the data-adaptive beamformers, while the reconstructed image quality is among the best. Therefore, we chose EISRCB to reconstruct PAM images for training the network."}, {"title": "3. Methods", "content": "3.1. Simulated Data\nA simulated dataset of MB cavitation signals emitted from a single bubble cloud and two bubble clouds was built. The cavitation signals located at typical targeting locations in focused ultrasound (FUS) experiments acquired by three different transducer arrays (ATL P4-1, L7-4, CL15-7) were simulated (Table 2). These arrays include linear array and phased array and their working bandwidth ranges low (1-4MHz), middle (4-7MHz) and high frequency (7-15MHz). The length of the recorded cavitation signals was 2048 time samples, which corresponds to a duration of 102 \u03bcs, 98 \u03bcs and 58 \u03bcs for the three arrays, respectively. We assumed that the bubbles in the cloud (bubble count varied between 20-100 randomly) are randomly distributed in an elliptical area defined by the -3 dB area of the point spread function of a passive acoustic imaging system using a linear array [25, 26]. Thus, the size of the elliptical area depends on the imaging depth, and its major and minor axis ranged 0.6\u201312 mm and 0.1\u20130.8 mm, respectively. For single bubble cloud case, the center of the bubble cloud varied randomly (uniform distribution) within the full aperture range of the imaging array in the lateral direction and between 12.8-57.6 mm in the axial direction (F-number = 0.6-2.0). For two bubble clouds case, the center of the clouds were at both ends of"}, {"title": "3.2. Experimental Data", "content": "In the experimental dataset, three different transducer arrays (P4-1, L7-4, CL15-7) were successively interfaced to a Vantage 256 system (Verasonics, Kirkland, WA, USA) to receive the MB cavitation signals. To initiate the MB cavitation signals, two FUS transducers of different center frequencies (Doppler, Guangzhou, China) were used for in vitro and in vivo experiments, respectively. The experiments were performed in deionized and degased water. The recorded cavitation signals have 2048 time samples, which corresponds to a duration of 102 \u03bcs, 98 \u03bcs and 58 \u03bcs for the three arrays, respectively."}, {"title": "3.3. Image Reconstruction", "content": "The PAM images in the dataset were reconstructed by the EISRCB which were used as the ground-truth images. The parameters used in the EISRCB were chosen as \u03b4 = 0.5, \u03b5 = 20 and 30 for single and multiple sources, respectively, after trials [10]. The reconstruction parameters for different transducer models are listed in Table 4. All the PAM images have the same size (512x512) to facilitate network training. The pixel size in the lateral direction was 0.25 times of the pitch of each transducer model. In the axial direction, it was adjusted to be less than a half of the wavelength of the array's center frequency. The center of the arrays' aperture was placed at (0, 0) mm in both directions. The lateral image range covered the full aperture of the respective transducer models array. The axial image range was 0-100 mm for P4-1, 15-65 mm for L7-4, 10-50 mm for CL15-7 to balance the image size and resolution. All image reconstructions"}, {"title": "3.4. Input Data", "content": "To unify the dimension of the input RF signals received by different arrays, all signals were firstly padded with zeros to a size of 2048\u00d7128. The unified data was then resized to Y \u2208 R256\u00d7128. To introduce a priori information about the transducer models, a one-hot code mask vector m \u2208 R1\u00d73 [30] was created for each transducer: [1,0,0] for P4-1, [0,1,0] for L7-4, [0,0,1] for CL15-7. This mask vector m was expanded to a three-dimensional matrix M\u2208 R256\u00d7128\u00d73. Then, M was stacked with Y to generate stacked data Z\u2208 R256\u00d7128\u00d74 that served as the input of the network."}, {"title": "3.5. Network Architecture", "content": "The proposed switchable deep beamformer is based on the pix2pixGAN [31] and our previously proposed neural network [22], which consists of a generator G, a discriminator D and a classifier K.\nThe generator G implemented with an encoder-decoder structure (Fig. 2(a)) was trained to reconstruct high-quality PAM images from RF signals for different transducer arrays (P4-1, L7-4, CL15-7). Different from a traditional encoder-decoder structure, we designed the generator G with an asymmetric structure which allows asymmetric input and output size. In the encoder, there are three levels of convolutional layers with instance normalization and leaky rectified linear unit (ReLU) activation. The encoder transforms the input data from 4 channels to the feature maps with 128 channels. There is a bottleneck structure formed by cascading 6 residual blocks between the encoder and the decoder. Each residual block is composed of 3 convolutional layers with leaky ReLU activation and instance normalization. In the decoder, the first four levels consist of convolutional layers with instance normalization and leaky ReLU activation, transforming the feature maps from 128 channels to 16 channels. The last level of the decoder, which is a convolutional layer with hyperbolic tangent (Tanh) activation finally merges all the feature maps into one image. To improve the performance of the network, two attention blocks, local aware attention [32] and pixel attention [33], were added. Local aware attention, which enhances the ability of extracting high-frequency features, is performed after each residual block in the bottleneck. Pixel attention is performed on each convolutional layer in the decoder to improve the global attention of the generator G.\nThe discriminator D and the classifier K share the same network comprised of 6 levels of convolutional layers with leaky ReLU activation and a separate convolutional layer without activation (Fig. 2(b)). The discriminator D was trained to differentiate between the true images reconstructed by EISRCB and the fake images reconstructed by the generator G. The output of the discriminator D is an 8\u00d78 boolean matrix in which the value (1 or 0) of the matrix element represents whether the corresponding 64 patches assembling the image is real or fake. This approach has been found to enhance the high-frequency components of the output images. The classifier K outputs probability of the type of transducer model, i.e. to which transducer array an image corresponds. This helps the generator G learn better the prior information carried by the mask vector m. For training stability, PAM images and the corresponding RF signals were stacked and used as the inputs to the discriminator D and the classifier K."}, {"title": "3.6. Network Training", "content": "Traditional GAN adopts the cross-entropy loss to minimize the distance between original images and target images. However, the cross-entropy loss was found unsuitable for the scenario of directly mapping RF signals to images. Therefore, we propose an improved loss scheme based on the training scheme of Wasserstein GAN [34]. The new loss L consists of adversarial loss Ladv(G,D), reconstruction loss Lrec(G), dice loss Ldic(G), gradient penalty Lgp(D) and classification loss Lcls(K):\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL = \\lambda_{adv} L_{adv}(G, D) + \\lambda_{rec} L_{rec}(G) + \\lambda_{dic} L_{dic}(G) + \\lambda_{cls} L_{cls}(K) + \\lambda_{gp} L_{gp}(D)\n\\end{document}\n(8)\nwhere \u03bbadv, \u03bbrec, \u03bbdic, \u03bbcls and \u03bbgp are the weighting parameters.\n1) Adversarial Loss: It measures the statistical distance quantifying the similarity between the distribution of the deep beamformer yielded images and the distribution of the ground truth images, which is defined as:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL_{adv}(G, D) = E[D(G(x))] - E[D(y)]\n\\end{document}\n(9)\nwhere x is the RF signals and y is the ground truth image. E is the mathematical expectation function.\n2) Reconstruction Loss: To speed up the training, mean squared error between the network generated image and the ground truth image is calculated as:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL_{rec}(G) = E[||y - G(x)||^2]\n\\end{document}\n(10)\n3) Dice Loss: Under the reconstruction loss constraint, the generated image starts with low-energy and smooth characteristics and gradually approaches the ground truth during training. However, due to the smaller number of high-energy pixels compared to low-energy pixels in PAM images, the discrepancy between the high-energy regions in the generated images and the ground truth can only result in a sufficiently small reconstruction loss. This makes the network difficult to generate PAM images with the maximum values of 1. To address this issue, dice similarity coefficient [35], which encourages the network to output values closer to 1 in regions with non-zero values, was utilized:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL_{dic}(G) = 1 - \\frac{2 \\sum_{i=1}^{N} y_i G(x)_i}{\\sum_{i=1}^{N} y_i^2 + \\sum_{i=1}^{N} G(x)_i^2}\n\\end{document}\n(11)\nwhere i indexes the ith pixel, and N is the number of image pixels.\n4) Classification Loss: The classifier K was trained to predict the same discrete probability distribution as the ones in the ground truth. Kullback-Leiber divergence was selected to represent the loss:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL_{cls}(K) = E[-log K(y)]\n\\end{document}\n(12)\n5) Gradient Penalty: Imposing the condition of 1-Lipschitz [34] on the discriminator D improves the training process without compromising the stability of the training:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nL_{gp}(D) = -E[max(0, ||\\nabla_{\\hat{y}} D(y)|| - 1)^2]\n\\end{document}\n(13)\nwhere y = xy + (1 \u2212 \u03b1)G(x) and \u03b1 is a random value following uniform distribution between [0,1].\nThe proposed network was trained using both simulated and experimental data with a training-to-test set ratio of 7:1. During the training, white noise was added to the input RF signals for every acquisition to randomly vary the signal-to-noise ratio (SNR) within 0-15 dB (uniform distribution), covering the reported SNR range in previous works [26, 36]. The network was optimized with the Adam [37], starting with a learning rate of 5e-5 and linearly decreases to 5e-6. The total epoch was 128, and the batch size was 64. The weights of the loss function, \u03bbadv, \u03bbrec, \u03bbdic, \u03bbcls and \u03bbgp were set to 1, 100, 1, 0.1 and 30, respectively. The network was implemented on the Python with PyTorch and trained with the RTX 4090 GPU."}, {"title": "3.7. Evaluation Metrics", "content": "The proposed deep beamformer was compared with the EISRCB, RLPB, DAX-RCB and TEA using the simulated and experimental data in terms of the energy spread area, artifacts suppression capability and computational complexity. The energy spread area was evaluate as the A-3dB area where the pixel values were greater than half of the maximum value in the image [24, 12]. Image SNR (ISNR) was used to evaluate the artifacts suppression capability, defined as:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nISNR = 10log_{10}(\\frac{\\Psi_{Inside}}{\\Psi_{Outside}})\n\\end{document}\n(14)\nwhere \u03a8Inside is the mean pixel value inside A-3dB area, \u03a8Outside is the mean pixel value outside A-3dB area but inside A-20dB area, the union of -20 dB area of the images reconstructed by different methods. In addition, floating-point operations (FLOPs) and running time required to reconstruct an image was used to evaluate the computational complexity of the deep beamformer and the other methods. The FLOPs for the deep beamformer was estimated using the PyTorch library THOP [38], while for other beamformers, the FLOPs was estimated using the Matlab function FLOPS [39].\nTo evaluate the capability of localizing cavitation sources with the deep beamformer, we evaluated source position deviation between the images reconstructed by the deep beamformer and EISRCB using the test dataset, with the source position defined as the centroid rc in the A-3dB area:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nr_c = \\frac{\\sum_{i=1}^{N} \\Psi(r_i) r_i}{\\sum_{i=1}^{N} \\Psi(r_i)} \\qquad s.t. r_i \\in A_{-3dB}\n\\end{document}\n(15)\nand the deviation defined as Euclidean distance between the centroids. To visualize the difference, the cumulative probability distribution of the deviation by wavelength (defined at the center frequency of the corresponding transducer array) was also evaluated.\nTo compare different data-adaptive beamformers, we used randomly selected 120 test data for each transducer model to calculate the evaluation metrics, due to the limitation that they require too much time, particularly the RLPB method, to generate an image for the settings used in this work (see Sec. 4.2.4). To evaluate the deep beamformer, all test data were used to calculate the evaluation metrics."}, {"title": "4. Results", "content": "4.1. Comparison of the data adaptive beamformers\nIn our simulated and experimental data of imaging single bubble cloud, the RLPB method achieved the optimal energy spread area A-3dB, showing 55.6-75.0% improvements in average compared with the TEA method (Fig. 3, Table. 5). In terms of the artifacts suppression capability, the EISRCB method was the best which improved the ISNR by 14.8-24.1 dB in average compared to the TEA method. The difference in the average value of A-3dB between the EISRCB and RLPB images were less than 0.3 mm\u00b2, except for the experimental data measured by P4-1 (4.7 mm\u00b2) (Table. 5), indicating that the size of energy beam provided by EISRCB was similar to that by RLPB. Moreover, the proportion of the Euclidean distance below 4 wavelengths between the center of the bubble cloud and the position localized by EISRCB was 100.0% for all the transducer arrays, which was similar to the results of other beamformers (Fig. 3(c-f)). However, the computational time of EISRCB was about one order of magnitude faster than that of RLPB (see Sec. 4.2.4)."}, {"title": "4.2. Evaluation of the Deep Beamformer", "content": "We used the simulated dataset to quantitatively evaluate the performance of the deep beamformer.\n4.2.1. Image quality\nThe high-intensity region was well localized in the images reconstructed by the deep beamformer, which showed good consistency with the ones by EISRCB for all the transducer arrays (Fig. 4(a-f)). For single cavitation sources, the difference in the average value of A-3dB and ISNR was 0.0-0.2 mm\u00b2 and 0.0-1.2 dB in all the arrays (Table 5). Compared with TEA, the average A-3dB area of the PAM images reconstructed by the deep beamformer was reduced by about 55.7%, 44.4% and 62.5% for P4-1, L7-4 and CL15-7, respectively (Table 5). For the average ISNR of the PAM images, it was improved by 20.0 dB, 22.3 dB and 22.9 dB by the deep beamformer for P4-1, L7-4 and CL15-7, respectively, compared with TEA images (Table 5). In the case of multiple cavitation sources, the difference in the average value of A-3dB area and ISNR was 0.2-0.7 mm\u00b2 and 1.4-6.0 dB in all the arrays (Table 6). Compared to TEA, the average A-3dB area of the PAM images reconstructed by the deep beamformer was reduced by about 18.9%, 29.4% and 30.0% for P4-1, L7-4 and CL15-7, respectively (Table 6). For the average ISNR of the PAM images, it was improved by 17.5 dB, 9.3 dB and 12.2 dB by the deep beamformer for P4-1, L7-4 and CL15-7, respectively, compared with TEA images (Table 6). These results demonstrated that the deep beamformer can reduce the size of energy beam and have strong artifacts suppression for both single and multiple cavitation sources.\n4.2.2. Source localization accuracy\nCompared to the EISRCB images, we observed negligible (< 4 wavelengths) deviation of the localized source position in the deep beamformer images (Fig. 4(g-j)). For single cavitation source, the proportion"}, {"title": "4.2.3. Different noise levels", "content": "We also explored the anti-noise performance of the deep beamformer, with the SNR ranging from 0 to 15 dB in the cavitation signals (Fig. 5). The added noise was white noise, as commonly used in other studies to simulate the experimental conditions in cavitation imaging [9, 12, 26]. In this range, the average ISNR of the deep beamformer and EISRCB images was 30.8-35.5 dB (31.5-36.4 dB) and 22.9-32.2 dB (28.2-36.1 dB), respectively, for all arrays. In terms of source localization accuracy, the proportion of the deviation below 1 wavelength in the lateral direction negligibly decreased from 98.7%-99.8% (15 dB) to 98.2%-99.5% (0 dB), while the proportion of the deviation below 4 wavelengths decreased from 89.3%-99.3% (15 dB) to 88.1%-98.3% (0 dB) in the axial direction. In the investigated noise levels (Fig. 5), the image quality and source localization accuracy of the deep beamformer was negligibly affected demonstrating its robust anti-noise capability against Gaussian noise.\n4.2.4. Computational Complexity\nThe FLOPs required by the deep beamformer, EISRCB, RLPB, DAX-RCB and TEA to reconstruct one image of 262144 pixels was 1.1 \u00d7 1010, 2.3 \u00d7 1013, 4.2 \u00d7 1014, 1.0 \u00d7 1013, and 4.1 \u00d7 1011, respectively. The actual execution time of the beamformers depends on the implementation and the computation platform. For a fair comparison, we estimated the computation time for different beamformers on a single CPU thread. The average computational time required by the deep beamformer, EISRCB, RLPB, DAX-RCB and TEA to reconstruct one image of 262144 pixels was 0.24 s, 3112 s, 28835 s, 2627 s, 1246 s. We also compared"}, {"title": "4.3. Validation of the Deep Beamformer", "content": "4.3.1. Phantom experiments\nIn the phantom experiments, the location of high-intensity region was also consistent in the reconstructed images using the deep beamformer and EISRCB (Fig. 6(a-f)). For single cavitation source, the images reconstructed by the deep beamformer presented no difference in the average A-3dB compared to the ones reconstructed by EISRCB (Table 5). Compared with TEA, the average A-3dB area and ISNR of the deep beamformer images was improved by 35.6%-65.0% and 17.1\u201319.0 dB, respectively, for all the transducers (Table 5). For multiple cavitation sources, the maximum difference in the average value of the A-3dB area and ISNR was less than 0.5 mm\u00b2 and 3.1 dB. Compared with TEA, the average A-3dB area and ISNR of the deep beamformer images was improved by 21.6%-63.1% and 15.6-20.3 dB, respectively, for all the transducers (Table 6).\nThe deep beamformer images showed small source localization deviation compared to the EISRCB im-ages. For single cavitation source, the proportion of the position deviation below 1 wavelength was 99.6%, 93.6%, and 95.6% for P4-1, L7-4 and CL15-7, respectively, in the lateral direction, while the proportion of the position deviation below 4 wavelengths was 98.7%, 99.3%, and 98.8% for P4-1, L7-4 and CL15-7, re-spectively, in the axial direction (Fig. 6 (g-h)). For multiple cavitation sources, the proportion of the lateral position deviation below 1 wavelength was 99.5%, 95.1%, and 96.6% for P4-1, L7-4 and CL15-7 respectively,"}, {"title": "4.3.2. Mice experiments", "content": "In the mice experiments, the deep beamformer also presented equivalent acoustic source localization capability as the EISRCB (Fig. 6 (k)). The centroid of the cavitation source presented by the deep beamformer was at rc = (2.3 mm, 24.6 mm), while it was at (2.4 mm, 24.8 mm) by EISRCB, indicating negligible source localization deviation between the two methods. The average ISNR and A-3dB of the deep beamformer images and the EISRCB images were similar (26.7 dB and 0.7 mm\u00b2 vs 30.1 dB and 0.7 mm\u00b2). Compared with TEA, the ISNR and A-3dB were improved by 19.6 dB and 58.8% by the deep beamformer, respectively. The deep beamformer achieved 10.5 ms image reconstruction speed on the GPU. Thus, real-time and high-quality localization of MB cavitation activities in the mouse tumors was possible (Supplementary Video 1)."}, {"title": "5. Discussion", "content": "In this work, a deep beamformer for PAM has been presented. The deep beamformer was trained in a supervised learning approach with the EISRCB images used as the ground truth. This beamformer is switchable between different arrays (P4-1, L7-4, CL15-7) and directly maps the acoustic cavitation signals to PAM images with the same neural network.\nWe introduced both simulated and experimental (phantom and in vivo mouse) data to train the neural network, which sums up to a total of 68400 acquisitions of the acoustic cavitation signals. For the simulation, we used the Marmottant and the Vokurka models to generate the acoustic emissions from MB clouds under stable and inertial cavitation, respectively. We further added the acoustic cavitation signals acquired in the experiments to include the signal characteristics originated from bubble-bubble interaction, which is currently difficult to simulate with a numerical model in a timely manner [9, 10]. During data preparation, we considered various settings that are commonly used in MB-mediated FUS experiments [26]. The excitation frequency varied between 0.5-1.5 MHz, excitation duration varied between 58-98 \u03bcs, randomly distributed bubbles in the bubble clouds (single and multiple clouds) whose count varied between 20-100, and the center of the bubble cloud randomly fell in the full aperture range of the imaging arrays in the lateral direction and their respective nominal imaging depth (12.8-57.6 mm in the axial direction). We also varied the SoS of the medium between 1480-1600 m/s to help the beamformer adaptively reconstruct the images for various SoS values. In addition, we used the MBs from three different manufactures (Vevo, Sonovue, and Sonazoid) with varied acoustic pressures (PNP: 0.5-1.5 MPa) in the experiments. To improve the anti-noise capability of the network, the SNR of input RF signals was adjusted to 0-15 dB for each training epoch. All these efforts were made to provide a comprehensive dataset for training the network.\nWhile the other data-adaptive beamformers, i.e., DAX-RCB and RLPB, can produce high-quality PAM images as well, we found that EISRCB balances the best between the image quality, the sensitivity of the tuning parameters to the outcomes and the computational cost with our implementation. Compared to DAX-RCB, EISRCB had tight energy spread and less artifacts in the PAM images. Compared to RLPB,"}, {"title": "6. Conclusion", "content": "In this work, we developed a deep beamformer that can reconstruct PAM images of equivalent quality as the ones produced by the EISRCB algorithm with much less computational cost, for localizing one cavitation source and multiple cavitation sources. Results also showed that by introducing mask vectors, high-quality PAM images can be reconstructed directly from the RF signals for different transducer arrays using the same neural network. Using deep learning based beamformers for PAM is promising for real-time monitoring and accurate localization of MB cavitation activities."}, {"title": "Appendix", "content": "We also evaluated the performance of the deep beamformer for the cavitation signals, received by the P4-1, generated under the conditions not included in the training dataset. These conditions include using short excitation pulses (10 cycles) to generate the cavitation signals, higher excitation frequency (2 MHz), and long (3072 samples) and short (1024 samples) duration. Evaluation metrics were calculated using 100 simulated data for each condition.\n1) Short pulses (Fig. 7(a)): The difference of the averaged ISNR and A-3dB between the deep beamformer and EISRCB was 4.9 dB (5.4 dB) and 0.9 mm\u00b2 (0.9 mm\u00b2) for single (double) source(s), respectively. The percent of source position deviation below 1 wavelength was 98.0% in the lateral direction, while the proportion of the deviation below 4 wavelengths was 90.5% in the axial direction.\n2) 2 MHz excitation frequency (Fig. 7(b)): The difference of the averaged ISNR and A-3dB between the deep beamformer and EISRCB was 3.4 dB (2.0 dB) and 0.9 mm\u00b2 (2.1 mm\u00b2) for single (bubble) source(s), respectively. The percent of source position deviation below 1 wavelength was 95.0% in the lateral direction, while the proportion of the deviation below 4 wavelengths was 74.5% in the axial direction.\n3) Long and short acquisition duration (Fig. 7(c)): For long (3072 samples) and short duration (1024 samples), the signals were trimmed down and zero-padded to 2048 samples to fit to the input dimension of the deep beamformer. The difference of the averaged ISNR and A-3dB between the deep beamformer and EISRCB was 2.2 dB (2.7 dB) and 0.3 mm\u00b2 (0.2 mm\u00b2) for single (double) bubble source(s), respectively. The percent of source position deviation below 1 wavelength was 99.6% in the lateral direction, while in the axial direction, it was 99.0% below 4 wavelengths."}]}