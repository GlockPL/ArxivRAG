{"title": "DIFFNMR3: ADVANCING NMR RESOLUTION BEYOND INSTRUMENTAL LIMITS", "authors": ["Sen Yan", "Etienne Goffinet", "Fabrizio Gabellieri", "Ryan Young", "Lydia Gkoura", "Laurence Jennings", "Filippo Castiglione", "Thomas Launey"], "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a crucial analytical technique used for molecular structure elucidation, with applications spanning chemistry, biology, materials science, and medicine. However, the frequency resolution of NMR spectra is limited by the \"field strength\" of the instrument. High-field NMR instruments provide high-resolution spectra but are prohibitively expensive, whereas lower-field instruments offer more accessible, but lower-resolution, results. This paper introduces an AI-driven approach that not only enhances the frequency resolution of NMR spectra through super-resolution techniques but also provides multi-scale functionality. By leveraging a diffusion model, our method can reconstruct high-field spectra from low-field NMR data, offering flexibility in generating spectra at varying magnetic field strengths. These reconstructions are comparable to those obtained from high-field instruments, enabling finer spectral details and improving molecular characterization. To date, our approach is one of the first to overcome the limitations of instrument field strength, achieving NMR super-resolution through AI. This cost-effective solution makes high-resolution analysis accessible to more researchers and industries, without the need for multimillion-dollar equipment.", "sections": [{"title": "1 Introduction", "content": "Nuclear Magnetic Resonance (NMR) spectroscopy is a powerful and widely utilized analytical technique for elucidating molecular structures [1, 2]. By exploiting the magnetic properties of atomic nuclei, NMR provides rich information about molecular dynamics, chemical environments, and atomic connectivity. As a non-destructive technique, NMR has broad applications across various fields including chemistry, biology, materials science, and medicine. In chemistry, NMR is commonly used for identifying molecular structures, analyzing purity, and studying chemical reactions [2, 3]. In biology, it plays a critical role in the study of proteins, nucleic acids, and other biomolecules, often aiding in understanding complex processes [4]. NMR's applications also extend to material science, where it helps characterize polymers, solid-state materials, and surfaces [5]. Furthermore, in medicine, NMR principles are foundational for Magnetic Resonance Imaging (MRI), a key diagnostic tool in clinical settings [6].\nDespite its versatility, the performance of NMR spectroscopy, especially in terms of frequency resolution, is closely tied to the field strength of the instrument used [7]. Frequency resolution refers to the ability of the instrument to resolve closely spaced resonance peaks, which is crucial for accurately identifying and characterizing molecules. Higher field strengths provide better resolution, allowing researchers to distinguish between subtle variations in chemical environments and obtain finer details about molecular structures. For example, high-field NMR instruments offer exceptional spectral clarity, resolving complex overlapping signals. However, such instruments come at a significant cost, often ranging from millions to tens of millions of dollars, making them inaccessible to many research institutions and industries 2."}, {"title": "2 Related work", "content": "The explosion of interest in generative models came in the 2010s, as deep learning techniques became more mature. Generative models learn the underlying patterns and structures of the data, enabling them to produce novel instances. These models can generate a wide range of data types, such as images [14, 15, 16], text [17, 18], and audio [19, 20], and are capable of tasks like data augmentation [21, 22], content creation [18], and style transfer [23]. This period saw the development of key generative models such as Generative Adversarial Networks (GANs) [24], Variational Autoencoders (VAEs) [25], and more recently Diffusion models [11].\nDiffusion models are probabilistic models designed to learn a data distribution [11]. They are widely used in computer science, majorly in computer vision and visual-language models [26]. The model operates by gradually adding noise (generally, Gaussian noise) to data in the forward process (called diffusion process) and then learning to reverse this noise through a backward process (called denoising process) to recover the original data distribution. From 2020, the diffusion model family has gained attention due to its effectiveness in generating high-quality samples, initially in the context of image generation such as Repaint [27], then audio generation such as Diffwave [20], and more recently image-text generation such as Stable diffusion [28], GLIDE[29], and Dall-E 2 [30].\nThe backward process, which is the core of the diffusion model, aims to denoise the corrupted data in an iterative process. This reverse diffusion process in diffusion models generally relies on deep neural networks. As shown in Fig. 1, UNet, a well-established architecture originally designed for biomedical image segmentation is widely used in diffusion models due to its multi-scale nature that facilitates the integration of fine and coarse features [12]. The UNet or its variants [12, 13] is regarded as the backbone of the diffusion model.\nDiffusion models are in principle capable of modeling conditional distributions [28, 29, 30]. This can be implemented with a conditional UNet and paves the way to control the denoising process through auxiliary information (i.e., conditioning inputs) such as class labels [31], low-resolution data [32] and texts [29, 30]. By conditioning on this information, the model can generate outputs that adhere to desired constraints or specifications.\nIn the context of super-resolution, the conditioning input typically consists of a low-resolution version of the target data. The diffusion model learns to enhance the spatial resolution by iteratively refining the details while preserving the overall structure and content of the input. This can be achieved by concatenating or integrating the low-resolution input"}, {"title": "2.1 Diffusion model for the super-resolution task.", "content": "The explosion of interest in generative models came in the 2010s, as deep learning techniques became more mature. Generative models learn the underlying patterns and structures of the data, enabling them to produce novel instances. These models can generate a wide range of data types, such as images [14, 15, 16], text [17, 18], and audio [19, 20], and are capable of tasks like data augmentation [21, 22], content creation [18], and style transfer [23]. This period saw the development of key generative models such as Generative Adversarial Networks (GANs) [24], Variational Autoencoders (VAEs) [25], and more recently Diffusion models [11].\nDiffusion models are probabilistic models designed to learn a data distribution [11]. They are widely used in computer science, majorly in computer vision and visual-language models [26]. The model operates by gradually adding noise (generally, Gaussian noise) to data in the forward process (called diffusion process) and then learning to reverse this noise through a backward process (called denoising process) to recover the original data distribution. From 2020, the diffusion model family has gained attention due to its effectiveness in generating high-quality samples, initially in the context of image generation such as Repaint [27], then audio generation such as Diffwave [20], and more recently image-text generation such as Stable diffusion [28], GLIDE[29], and Dall-E 2 [30].\nThe backward process, which is the core of the diffusion model, aims to denoise the corrupted data in an iterative process. This reverse diffusion process in diffusion models generally relies on deep neural networks. As shown in Fig. 1, UNet, a well-established architecture originally designed for biomedical image segmentation is widely used in diffusion models due to its multi-scale nature that facilitates the integration of fine and coarse features [12]. The UNet or its variants [12, 13] is regarded as the backbone of the diffusion model.\nDiffusion models are in principle capable of modeling conditional distributions [28, 29, 30]. This can be implemented with a conditional UNet and paves the way to control the denoising process through auxiliary information (i.e., conditioning inputs) such as class labels [31], low-resolution data [32] and texts [29, 30]. By conditioning on this information, the model can generate outputs that adhere to desired constraints or specifications.\nIn the context of super-resolution, the conditioning input typically consists of a low-resolution version of the target data. The diffusion model learns to enhance the spatial resolution by iteratively refining the details while preserving the overall structure and content of the input. This can be achieved by concatenating or integrating the low-resolution input\nwith the noise-corrupted data at each step of the reverse diffusion process, or by integrating the auxiliary information into the model through additional channels or attention mechanisms [33]. For more information, please refer to the UNet with attention modules [13]. In the super-resolution task, the diffusion model usually takes the following inputs during the denoising process:\n\u2022 A low-resolution data, which serves as the conditioning input. This is often a downsampled version of the target high-resolution data.\n\u2022 The current time step t provides the information to the model on how much noise has been added to the original spectrum.\n\u2022 Optional auxiliary conditions, such as a class label or other features describing specific output characteristics. These conditions help guide the model to generate outputs aligned with the desired specifications.\nMost super-resolution techniques involve processing image data or multimodal data that includes images [34, 35]. SR3 [21] and SRdiff [22] have achieved high-quality super-resolution by manipulating the pixel domain. However, unlike previous works that process the pixel-wise input, in this paper, we apply the conditional diffusion model in the frequency-based domain. Furthermore, our approach offers multi-scale functionality (see Section 3.3)."}, {"title": "2.2 NMR super-resolution", "content": "The application of AI models for NMR super-resolution tasks is still in its early stages. To the best of our knowledge, prior work in this area has primarily focused on improving resolution through data acquisition (i.e., experimental optimization) rather than post-acquisition processing.\nMulleti et al. [36] applied finite-rate-of-innovation sampling (FRI) [37] to achieve super-resolution in NMR spectroscopy. By reconstructing signals with fewer measurements, their approach accurately resolves overlapping or broadened peaks, enabling precise estimation of chemical shifts and enhancing the analysis of complex systems.\nWenchel et al. [38] focused on optimizing the experimental process itself. Their method dynamically increases the number of scans over time to counteract signal decay, reducing peak linewidth and enhancing resolution. However, this approach is limited to improving resolution during the data acquisition process and does not address the enhancement of pre-existing low-resolution spectra.\nUnlike the aforementioned works [36, 38], we propose a novel AI-driven approach called the Multi-Scale Super-Resolution model (MSSR) shown in Fig. 1 and Fig. 2. MSSR directly enhances the resolution of NMR spectra without requiring any improvements in the data acquisition process. By leveraging a conditional diffusion model, our method achieves super-resolution in post-acquisition processing, enabling the generation of high-resolution spectra from low-resolution spectra. This eliminates the reliance on costly high-field NMR instruments, providing an accessible and efficient alternative for improving spectral resolution."}, {"title": "3 MSSR: Multi-Scale Super-Resolution pipeline", "content": "As illustrated in Fig.2, there are two processes in our MSSR pipeline: diffusion process and denoising process. In the diffusion process, the input is the original NMR (high-resolution) spectrum $x_0$. After adding noise to the input step by step, the final output is the noisy (high-resolution) spectrum $x_T$. In the denoising process, the noisy spectrum $x_T$ is fed to the conditional UNet (detailed in Fig. 1) with three conditioning inputs: the upscaling factor $f$, the time step $t$, and the low-resolution spectrum $x_{LR}$. The conditional UNet gradually denoises the noise-corrupted spectrum. The high-resolution spectrum denoted by $\\hat{x_0}$ is reconstructed as the output."}, {"title": "3.1 Diffusion process", "content": "The diffusion process (i.e., the forward process) of the diffusion model is illustrated in the upper part of Fig.2. The goal is to corrupt the spectrum by adding Gaussian noise step by step. A total number of time steps $T$ is defined. In each time step t, where $t \\in \\{1, . . ., T\\})$, Gaussian noise is gradually added to the spectrum. The diffusion process $q$ can be described mathematically as:"}, {"title": "3.2 Denoising process", "content": "The denoising process $p$ gradually removes noise to reconstruct the spectrum $\\hat{x_0}$ (illustrated in the lower part of Fig.2). It is modeled as a Markov process with learnable parameters:"}, {"title": "3.3 Condition mechanism", "content": "As shown in Fig. 1, the conditioning inputs are not directly fed to the conditional UNet. The projections are required to transfer the time step t and the upscaling factor f to a conditional embedding $z_{t,f}$. The low-resolution spectrum is concatenated to the input of the UNet (i.e., the noisy spectrum $x_t$ or the intermediate denoised spectrum $\\hat{x_t}$ with $t \\in \\{1, . . ., T \u2013 1\\}$).", "sections": [{"title": "Time embedding.", "content": "According to the literature [11, 33], each time step $t$ is encoded using a sinusoidal positional encoding [33] to create a high-dimensional time embedding denoted by $m_t$, with $t \\rightarrow m_t \\in \\mathbb{R}^d$, where d is the dimensionality of the embedding (pow of 2)."}, {"title": "Class condition.", "content": "We discretize the upscaling factor f by directly mapping each of the n distinct upscaling factors to an integer index: $f \\rightarrow c \\in \\{0,1, . . ., n \u2212 1\\}$, $\\forall i \\in [0, n \u2212 1]$ where c is the class label. For a given class labels c, we employ one-hot encoding $h_c \\in \\mathbb{R}^n$, where:"}, {"title": "Low-Resolution Spectrum.", "content": "The original spectrum (high-resolution) $x_0 \\in \\mathbb{R}^{h\u00d7w}$ undergoes downscaling by expanding the peaks' widths based on a given upscaling factor $f$. Note that h and w represent the dimension of the spectrum. Here, the input spectrum is convolved with the Gaussian window g. Then the noise is added to simulate the low-resolution spectrum."}]}, {"title": "3.4 Training", "content": "Firstly in the diffusion process, the noisy spectrum at time step t (denoted by $x_t$) is generated by adding t times of Gaussian noises from the original spectrum $x_0$, with t uniformly sampled from $\\{1, . . ., T\\}$, where T is the total number of time steps. Then in the denoising process, the UNet is trained to denoise the noisy spectrum $x_t$ step by step (from time step t to 0). The training objective is to minimize the Mean Reconstruction Error (MSE) loss $\\mathcal{L}$ between the noise added to the original spectrum and the predicted noise."}, {"title": "3.5 Inference", "content": "For the inference, given a upscaling factor f and a low-resolution spectrum $x_{LR}$, the model should reconstruct the spectrum $x_0$ from random noise sampled from the normal distribution. That is to say, at time step T, the input of the conditional UNet is $x_T \\sim \\mathcal{N}(0, 1)$. The three conditioning inputs of the conditional UNet are unchanged. The output of the UNet is controlled by the conditioning inputs. The final output is $\\hat{x_0}$ at time step t = 0, with $\\hat{x_0} \u2248 x_0$."}, {"title": "4 Experiments and Results", "content": null}, {"title": "4.1 Dataset and experiment configuration", "content": "Dataset. Massive datasets containing hundreds of millions of images have been used in recent advances in diffusion models for image generation [28, 39]. In contrast, the availability of large-scale datasets for NMR protein spectra is extremely limited, with only a few public repositories accessible. This paper concentrates on enhancing the resolution of 2D NMR spectra. The dataset used in this study is the 100-protein NMR spectra dataset (ARTINA) [40], comprising 1329 spectra in 2D, 3D, and 4D formats. These spectra, derived from 100 proteins from real life, were sampled using NMR machines operating at frequencies ranging from 600 to 950 MHz. To expand the dataset, we included additional 2D spectra by projecting multi-dimensional (3D/4D) spectra into 2D representations. This technique, commonly used in NMR workflows for visualization purposes, increases the total number of samples to over 3500. The process maintains the integrity of the original signals while significantly enriching the dataset for training.\nWe train our MSSR model on the ARTINA dataset with a train-validation-test ratio of 0.8, 0.1, and 0.1. This split ensures the test set's independence, minimizing the data leakage risk and guaranteeing robust performance evaluation. All the data in ARTINA are proteins whose entries are listed in the Protein Data Bank [41]. The original spectra $x_0$ are normalized to [-1,1] and resized to $x_0 \\in \\mathbb{R}^{256\u00d7256}$.\nUpscaling factors. Table 1 provides upscaling factors for frequencies ranging from 400 MHz to 900 MHz, commonly used in NMR spectroscopy. Note that the upscaling factor for 600 MHz/400 MHz and the upscaling factor for 900 MHz/400 MHz are the same. Thus there are totally n = 14 upscaling factors (n is defined in Section 3.3).\nLow-resolution spectrum. Considering that the standard practice in NMR spectroscopy has been to train models on simulated spectra [42, 43, 44, 45], we employ the simulated spectrum mentioned in Section 3.3 as the low-resolution spectrum. Validated by the lab expert, we set $\\sigma_{LR} = 0.01$."}, {"title": "4.2 Baseline", "content": "As illustrated in Fig. 1 and Fig. 2, our MSSR approach is a unified model with multi-scale functionality. Indeed, this model can realize super-resolution from different low frequencies to different high frequencies, controlled by the 14 upscaling factors listed in Table 1.\nTo create a comprehensive baseline for comparison, we trained 14 different models, each tailored to one of the 14 upscaling factors. The difference between the 14 baseline models and our MSSR model is the conditioning input. For each baseline model, we eliminated the class embedding, so there is only time embedding $m_t$ as the conditional embedding (see Section 3.3). For the low-resolution spectrum, each model only creates only one group of the low-resolution spectra $x_{LR}$ by a fixed Gaussian kernel with standard deviation $\\sigma_g = \\frac{1}{f}$, where $f$ is the given upscaling factor. Since the upscaling factor is no longer the conditioning input, each model works for one super-resolution task (corresponding to an assigned upscaling factor). Furthermore, the upscaling factor $f$ is removed from the loss function,"}, {"title": "4.3 Results", "content": "We illustrate in Figure 3, the original spectrum of the compound 2KZV from the ARTINA dataset (2KZV, experiment: HCCHCOSY@ALI, nucleus: C, HC), the low-resolution spectrum, the reconstruction from MSSR and the reconstruction from the baseline. Notably, both the baseline and our MSSR approach achieve impressive reconstruction quality; however, our approach excels in capturing finer details compared to the baseline. To facilitate a more nuanced evaluation of these methods, we have developed various metrics for comparison."}, {"title": "4.4 Metrics", "content": "The evaluation approach balances both a global perspective, assessing the overall agreement between the original and reconstructed spectra, and a local perspective, focusing on the accuracy of individual spectral peaks, which are essential for compound characterization. Together, we select nine metrics to provide a comprehensive assessment of our model's performance."}, {"title": "4.4.1 Global Metrics", "content": "Global metrics measure the overall alignment and similarity between the reconstructed and original spectra. We select Mean Squared Error (MSE) and Coefficient of Determination (R2). MSE (ranging from 0 to infinity) quantifies the magnitude of prediction errors, ensuring accuracy in capturing critical spectral features like peak intensities and positions. A lower MSE indicates higher accuracy in the reconstruction, as it shows that the overall deviation between the two spectra is minimized. R2 (ranging from 0 to 1) complements this by assessing how well the model explains the variance in the data, providing a normalized measure of fit. An R2 value close to 1 indicates that the reconstruction is highly representative of the original data, demonstrating a good fit. Together, these metrics ensure a robust evaluation of the method's precision and reliability."}, {"title": "4.4.2 Local Metrics", "content": "Local metrics focus on evaluating the accuracy of specific spectral peaks, which are critical for identifying compound features. These metrics assess how well the AI model reproduces the precise details of the spectrum, such as the positions and intensities of the peaks, which are necessary for accurate interpretation.\nWe investigate the following peak-focused metrics: the hallucination ratio and the missed peak ratio, the peak MSE, peak R2, the peak intensity difference, and the peak coordinate shift difference.\nThe hallucination ratio (ranging from 0 to 1) measures the proportion of peaks detected in the predicted spectrum that do not correspond to peaks in the original spectrum (analogous to the False Detection Rate). These peaks are errors introduced by the model and can distort the interpretation of the spectrum. Minimizing this ratio is important to avoid introducing false features in the reconstructed data.\nDiffering from the hallucination ratio, the missed peak ratio (ranging from 0 to 1) measures the proportion of peaks in the original spectrum that do not appear in the reconstructed spectrum (analogous to the False Negative Rate). Reducing this ratio ensures that critical peaks are retained in the reconstruction. Both the hallucination ratio and the missed peak ratio are calculated based on peaks identified by a consistent expert system using the same parameters for all spectra, ensuring fair and unbiased comparisons."}, {"title": "4.5 Discussion", "content": "The global metrics are illustrated in Figure 4. MSSR consistently outperforms the Baseline with significantly lower MSE values. This indicates a smaller overall reconstruction error. MSSR achieves higher R2 values compared to the Baseline. As the upscaling factor increases, MSSR's R2 approaches 1, whereas the Baseline shows slower improvement. This highlights MSSR's advantage in globally fitting the original spectra.\nLocal metrics are presented in Figure 5. For the hallucination ratio, both our MSSR and the Baseline maintain a relatively low ratio (less than 3%). The MSSR curve is consistently lower than the Baseline, indicating that MSSR produces fewer artifacts. This is crucial for ensuring the authenticity of the generated spectra. For the missed peak ratio, the Baseline is unstable when the upscaling factor is smaller than 0.57. The difference between our MSSR and the Baseline becomes smaller as the upscaling factor increases. This suggests that MSSR captures true peaks more effectively across different scales. For the peak MSE, MSSR achieves lower Peak MSE across all upscaling factors, particularly in the upscaling factors ranging from 0.57 to 0.89. This indicates MSSR reconstructs peak intensities with higher accuracy. About R2, MSSR consistently exhibits higher R2 values compared to the Baseline. This demonstrates that MSSR excels in modeling the correlation between reconstructed and original peaks. For the peak intensity difference, MSSR shows a significantly lower peak intensity difference compared to the Baseline. For the peak coordinate shift difference, the two curves are close. However, MSSR performs slightly better when the upscaling factors increase.\nMSSR demonstrates superiority in reducing artifacts, capturing true peaks, minimizing errors, and improving correlation modeling. Its adaptability to multi-scale spectral characteristics is particularly evident under low upscaling factor conditions. Globally, MSSR better preserves the overall structure and features of the original signal. Overall, MSSR outperforms the Baseline across global and local metrics."}, {"title": "5 Conclusion", "content": "In this paper, we have introduced a novel AI-driven approach, called the Multi-Scale Super-Resolution model (MSSR), for enhancing the frequency resolution of NMR spectra using a conditional diffusion model to achieve multi-scale super-resolution. This method addresses one of the key limitations of NMR spectroscopy, i.e., frequency resolution, by reconstructing high-resolution spectra from low-resolution spectrum. Our approach enables researchers to obtain spectral detail comparable to that of high-field instruments without the associated cost, making high-quality NMR analysis more accessible to a broader audience.\nThe diffusion model used in this study provides flexible super-resolution capabilities, allowing the reconstruction of spectra at varying field strengths. This multi-scale functionality offers significant advantages in tailoring the resolution to specific experimental needs, making the method adaptable across different research and industrial contexts.\nThe implications of this work can be far-reaching. By democratizing access to high-quality NMR data, this method has the potential to transform the way NMR is utilized in both academic and industrial settings. Researchers working with limited budgets can now achieve high-resolution results using affordable, lower-field NMR instruments. By reducing reliance on expensive high-field NMR machines, this approach makes NMR spectroscopy more accessible, thus broadening the scope and potential of molecular analysis across scientific disciplines.\nFurther improvements to the model's accuracy and scalability could extend the range of applications, including more complex molecular systems or higher-dimensional or multimodal NMR data. Additionally, integrating our MSSR method with other techniques such as Non-Uniform Sampling (NUS) [43, 42] may further optimize NMR analysis, opening up new possibilities for real-time, high-resolution molecular insights."}]}