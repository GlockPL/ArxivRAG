{"title": "Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings", "authors": ["Guangyao Zheng", "Michael A. Jacobs", "Vladimir Braverman", "Vishwa S. Parekh"], "abstract": "Self-supervised learning has revolutionized medical imaging by enabling efficient and generalizable feature extraction from large-scale unlabeled datasets. Recently, self-supervised foundation models have been extended to three-dimensional (3D) computed tomography (CT) data, generating compact, information-rich embeddings with 1408 features that achieve state-of-the-art performance on downstream tasks such as intracranial hemorrhage detection and lung cancer risk forecasting. However, these embeddings have been shown to encode demographic information, such as age, sex, and race, which poses a significant risk to the fairness of clinical applications.\nIn this work, we propose a Variation Autoencoder (VAE) based adversarial debiasing framework to transform these embeddings into a new latent space where demographic information is no longer encoded, while maintaining the performance of critical downstream tasks. We validated our approach on the NLST lung cancer screening dataset, demonstrating that the debiased embeddings effectively eliminate multiple encoded demographic information and improve fairness without compromising predictive accuracy for lung cancer risk at 1-year and 2-year intervals. Additionally, our approach ensures the embeddings are robust against adversarial bias attacks. These results highlight the potential of adversarial debiasing techniques to ensure fairness and equity in clinical applications of self-supervised 3D CT embeddings, paving the way for their broader adoption in unbiased medical decision-making.", "sections": [{"title": "1. Introduction", "content": "The rapid advancements in self-supervised learning have revolutionized medical imaging, enabling the extraction of compact, information-rich embeddings from large-scale unlabeled datasets. These embeddings have demonstrated exceptional performance across various clinical applications, including detecting intracranial hemorrhages and forecasting lung cancer risk (Yang et al., 2024; Health, 2024). However, recent studies have highlighted a critical limitation: these embeddings often encode sensitive demographic information, such as age, sex, and race (Zheng et al., 2024). This unintended encoding introduces risks of bias in downstream clinical tasks, potentially compromising fairness and equity in medical decision-making (Gichoya et al., 2022). Such biases can lead to disparities in patient outcomes, perpetuating systemic inequities in healthcare. Addressing these challenges is imperative to ensure that AI models used in medical imaging support equitable healthcare delivery.\nTo mitigate these risks, we propose an adversarial debiasing framework designed to transform 3D CT embeddings into a new latent space that excludes sensitive demographic information while preserving their utility for downstream clinical tasks. Adversarial debiasing is a technique used in machine learning to mitigate bias in models, especially when dealing with sensitive attributes like gender, race, or age. It leverages adversarial learning to encourage the model to make predictions that are not influenced by these sensitive attributes while maintaining overall performance on the main task (Zhang et al., 2018). In this work, we extend this approach using a variational autoencoder (VAE)-based architecture to transform demographic-encoding embeddings to demographic-free embeddings without affecting the performance of downstream models, as shown in Figure 1.\nAdversarial debiasing has been widely studied in the literature as a key technique for mitigating bias in machine learning models, particularly in medical applications (Correa et al., 2024, 2021; Jin et al., 2024; Agarwal et al., 2024; Yang et al., 2023). However, existing approaches often fail to meet three key constraints: independence from downstream tasks, the ability to debias multiple sensitive attributes simultaneously, and compatibility with black-box foundation models. For instance, Debias-CLR (Agarwal et al., 2024) employs contrastive learning but requires separate models for each sensitive attribute, while DNE (Jin et al., 2024) masks sensitive information in images using adversarial noise without explicitly evaluating its impact on downstream performance. Other frameworks, such as those by Yang et al. (Yang et al., 2023) and Correa et al. (Correa et al., 2021, 2024), integrate adversarial debiasing directly into task-specific models, limiting generalizability and necessitating access to internal model representations. In contrast, our proposed VAE Adversarial Debiasing framework ensures downstream task independence, debiases multiple attributes simultaneously, and operates effectively on embeddings from black-box self-supervised models, making it a more adaptable and clinically practical solution.\nWe evaluate the proposed framework on the NLST lung cancer screening dataset, focusing on two critical metrics: fairness and predictive accuracy (NLST, 2011). First, we assess whether the transformed embeddings successfully eliminate demographic signals, ensuring that downstream models cannot infer sensitive attributes such as age or sex. Second, we measure the impact of debiasing on the predictive accuracy of lung cancer risk forecasting at 1-year and 2-year intervals."}, {"title": "2. Materials and Methods", "content": ""}, {"title": "2.1. Dataset and 3D CT Foundation Model Embeddings", "content": "This retrospective study utilized the publicly available National Lung Screening Trial (NLST) dataset, which contains 3D CT images of the lungs from patients aged 55-74 years, along with associated demographic information such as age, sex, and race (NLST, 2011). The dataset was specifically curated for lung cancer screening and represents a diverse patient population."}, {"title": "2.2. Demographic Prediction", "content": "To evaluate whether the embeddings produced by the CT Foundation model encode demographic information, we conducted a series of experiments using linear classifiers, which are simple yet effective for detecting the presence of demographic information without the"}, {"title": "2.3. Adversarial Debiasing", "content": "To remove demographic information from the 3D CT embeddings while maintaining their effectiveness for clinical prediction tasks, we employed an adversarial debiasing framework based on a Variational Autoencoder (VAE) architecture. The VAE framework is particularly effective for learning continuous, disentangled representations, which is crucial for both debiasing and preserving task-relevant information.\nThe VAE architecture consists of two main components: the encoder and the decoder. The encoder uses a linear layer to map the input 3D CT embeddings into a latent space of mean and log variance parameters. These parameters are reparameterized to allow back-propagation during training. The decoder comprises a linear layer that reconstructs the embeddings from the latent space. During training, the VAE loss is computed as the sum of two components: (1) the reconstruction loss (mean squared error, MSELoss), which ensures the model generates embeddings that are close to the original input, and (2) the KL divergence loss, which regularizes the latent space to follow a unit Gaussian distribution.\nTo introduce adversarial debiasing, we added an adversarial network to predict demographic information from the latent representations. The adversary consists of multiple branches, each corresponding to a specific demographic attribute. In the NLST dataset case, age and sex. For binary classification tasks (sex), the adversary applies a sigmoid activation to the output of a branch, followed by binary cross-entropy loss (BCELoss). For continuous variables (age), the adversary directly computes mean squared error loss (MSELoss). The total adversarial loss is the sum of the individual losses from the two branches, and the encoder is trained to minimize this loss, ensuring that the demographic attributes cannot be predicted from the embeddings."}, {"title": "3. Experiments and Results", "content": ""}, {"title": "3.1. Experiment 1: Evaluation of baseline adversarial debiasing performance", "content": "Experimental Setup: To comprehensively evaluate our debiasing framework, we compared the original embeddings with adversarially debiased embeddings across three key metrics: (1) the predictive performance of downstream tasks, specifically lung cancer prediction at 1 and 2 years; (2) the extent to which the embeddings encode demographic information, such as age and sex, assessed by training linear models to predict these attributes from both original and debiased embeddings; and (3) the level of bias in age and sex, quantified using"}, {"title": "Equal Opportunity Difference (EOD)", "content": "Equal Opportunity Difference (EOD) (Obermeyer et al., 2019). EOD measures disparities in model performance across demographic groups by computing the difference in true positive rates (TPRs) between groups, where a higher EOD indicates greater unfairness, and an EOD closer to zero signifies fairer model performance. By evaluating EOD on lung cancer prediction at 1 and 2 years using both the original and debiased embeddings, we assessed whether our framework successfully mitigated bias while preserving predictive accuracy.\nThe VAE was trained for 100 epochs with a learning rate of 0.0005 for the encoder and decoder and 0.002 for the adversary, respectively, using a batch size of 32. The dimensionality of the latent space was empirically determined to be 500 (Appendix B), balancing the trade-off between representational capacity and demographic disentanglement.\nResults: Demographic Prediction: In the sex prediction task, the original embeddings exhibited a strong encoding of demographic information, achieving an accuracy of 0.994 and an AUC of 0.999. Following debiasing, these metrics declined substantially to 0.647 and 0.669, respectively, indicating a significant reduction in the model's ability to infer sex from the embeddings. This trend is further corroborated by the ROC curve in Figure 2(a), which illustrates the marked decrease in sex predictability, underscoring the efficacy of our framework in mitigating demographic bias. Similarly, for age prediction, the original embeddings yielded a mean absolute error (MAE) of 2.734, reflecting a strong correlation between age and the learned representations. After debiasing, the MAE increased to 4.169, signifying a considerable attenuation of age-related information. The scatterplot in Figure 2(b) provides additional empirical support, demonstrating that while the original embeddings closely adhered to the red dotted line\u2014indicating a strong linear correlation between predicted and actual age\u2014post-debiasing, the predicted age exhibited increased variability, with values distributed within the 55 to 75 range, as represented by the rectangular region in the plot.\nLung Cancer Prediction: Despite the reduction in demographic information, the embeddings retained strong predictive performance for lung cancer risk assessment. For the 1-year"}, {"title": "Downstream Task Fairness", "content": "Downstream Task Fairness: As shown in Table 1, our VAE-based debiasing framework effectively enhances fairness for both sex and age, as evidenced by the reduced EOD compared to the original embeddings.\nThese findings demonstrate that our adversarial debiasing framework effectively removes multiple sensitive demographic attributes from the embeddings while maintaining their utility for clinical prediction tasks. Furthermore, this procedure operates independently of the downstream task, ensuring that debiasing does not negatively impact lung cancer risk prediction."}, {"title": "3.2. Experiment 2: Robustness to Data Poisoning", "content": "Experimental Setup: Demographic predictability can expose models to adversarial data poisoning, where an attacker deliberately perturbs data to degrade model performance for specific demographic groups (Kulkarni et al., 2024). To simulate such an attack, we selectively \"target\" a particular demographic group (e.g., males) by randomly flipping their lung cancer prediction labels. In this experiment, we varied the proportion of label flipping across 0%, 25%, 50%, 75%, and100% for each demographic group separately, while ensuring that the labels for the other demographic group remained undisturbed. Similar to experiment 2, we evaluated if our framework improved the robustness of downstream models to demographically targeted adversarial attacks using EOD for sex and on downstream tasks cancer in 1 year and cancer in 2 years using the original embedding and the VAE debiased embedding.\nResults: As shown in Figure 4, as the percentage of male patients' cancer in 1-year labels become corrupted, the EOD for the model using the original embeddings without debiasing increases significantly, reaching close to 1, signifying a dramatic increase in unfairness in its predictions. In contrast, the EOD for the model using the debiased embeddings remains relatively stable, with a maximum EOD of 0.046. The same trend is observed for female data poisoning as well. We observe a similar trend for cancer prediction in 2 years, as shown in Figure 5, where the EOD for the model using the original embeddings increases as the percentage of corrupted labels rises, while the EOD for the model with the debiased embeddings remains consistently low."}, {"title": "4. Discussion", "content": "The results of this study are both promising and significant, demonstrating the potential of adversarial debiasing to enhance fairness in medical AI systems. The ability to remove multiple sensitive demographic information, such as sex and age, from 3D CT embeddings while preserving predictive accuracy for crucial clinical tasks, like lung cancer risk prediction, is a major advancement. This finding is especially important for deploying AI in real-world healthcare, where fairness and equity are paramount. Our framework shows that it is feasible to eliminate demographic biases without compromising the performance of models, which has critical implications for making AI systems more transparent, equitable, and trustworthy in clinical settings.\nThe encoding of demographic information, including sex and age, within medical imaging embeddings raises concerns about both fairness and security. If AI models learn to encode these attributes, they may inadvertently reinforce biases, leading to discriminatory outcomes for specific patient populations. From a security standpoint, malicious actors can exploit this demographic encoding and may target a specific demographic group to undermine the model's performance.\nThe VAE-based debiasing framework enhances both fairness and security by removing demographic information from embeddings, reducing the risk of demographic-based attacks. This ensures that sensitive attributes do not unfairly influence predictions while preserving performance on downstream clinical tasks. By mitigating bias and security vulnerabilities, this approach fosters more transparent and trustworthy AI systems for clinical applications.\nDespite the promising results, this work has several limitations. While the debiasing process effectively removed sex and age information, it was not evaluated for other sensitive attributes like race or location. Additionally, the framework was tested only on lung cancer risk prediction from 3D CT images. The effectiveness of debiasing may vary based on image complexity and task-specific requirements, necessitating further investigation into the trade-off between debiasing and downstream performance across different medical imaging domains.\nIn conclusion, our study demonstrates that VAE adversarial debiasing can effectively remove demographic biases from medical image embeddings without compromising predictive performance. This approach has important implications for creating fairer and more secure healthcare AI systems, helping mitigate both ethical and security risks associated with biased models. Moving forward, future work should explore the debiasing of additional sensitive attributes, further validate this method across diverse datasets, and evaluate its impact on other clinical tasks to ensure the robustness and generalizability of this framework."}, {"title": "Appendix B. Latent Space Optimization", "content": "Experimental Setup: The latent space dimension is a critical factor influencing the performance of VAEs. To identify the optimal latent space size, we conducted experiments on the NLST dataset, testing nine configurations: 10, 50, 100, 200, 300, 400, 500, 600, and 700. For each configuration, we trained the VAE on the train split to debias both sex and age and evaluated its performance on the tune split by comparing demographic prediction and lung cancer prediction before and after debiasing. The VAE was trained for 100 epochs with a learning rate of 0.0005 for the encoder and decoder and 0.002 for the adversary, using a batch size of 32.\nResults: As shown in Table B.1 and Figure B.1, increasing the latent dimension reduces the AUC difference between the linear classifier predictions for sex, 1-year cancer risk, and 2-year cancer risk before and after debiasing. Larger latent space preserves more information from the original embedding, which benefits downstream task performance but also retains more demographic information. Therefore, identifying an optimal latent dimension is crucial. Based on Table B.1, we found that at a latent dimension of 500, the reconstructed embeddings maintained or improved performance on lung cancer prediction while significantly reducing performance on sex prediction. Thus, we empirically determined that 500 is the most appropriate choice for balancing predictive performance and demographic debiasing."}]}