{"title": "Phantom Wiki: On-Demand Datasets for Reasoning and Retrieval Evaluation", "authors": ["Albert Gong", "Kamil\u0117 Stankevi\u010di\u016bt\u0117", "Chao Wan", "Anmol Kabra", "Raphael Thesmar", "Johann Lee", "Julius Klenke", "Carla P. Gomes", "Kilian Q. Weinberger"], "abstract": "High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities.", "sections": [{"title": "1. Introduction", "content": "Designing agents that can perform complex reasoning while interfacing with a large-scale, dynamic corpus\u2014like Wikipedia\u2014is a long-standing goal in the field of natural language processing (Feldman & El-Yaniv, 2019; Min et al., 2019). Such a goal may be within reach given the impressive capabilities of recent language models, which are all trained on internet-scale data. For example, the ability of LLMs to solve math problems on GSM8K (Cobbe et al., 2021) and mathematical olympiads (AlphaProof & AlphaGeometry, 2024) could bode well for agents to answer highly quantitative questions. On benchmarks like DROP (Dua et al., 2019) and MMLU (Hendrycks et al., 2020), these LLMs demonstrate advanced reading comprehension and general reasoning capabilities, both necessary for intelligent agents. When augmented with retrievers (Muennighoff et al., 2022) and tools (Patil et al., 2023), LLMs seem to already possess a strong ability for accessing external datastores and knowledge bases.\nHowever, it is unclear to what extent these models rely on their internal knowledge, which can easily become outdated, versus their reasoning and retrieval abilities. Consider the example, \"What is the date of birth of Wolfgang Amadeus Mozart?\". Since this fact is contained within LLMs' pre-training data, asking LLMs this question cannot provide reliable insight on whether the answer was deduced, retrieved or recalled. At the same time, existing approaches that perturb Wikipedia facts (Cohen et al., 2024; Meng et al., 2022; Elazar et al., 2021) to construct new question-answer pairs face challenges of ensuring factual consistency across articles. For example, changing Mozart's date of birth to 2025 also requires modifying Beethoven's article to erase the fact that Beethoven might have met Mozart in 1787!\nOne could hope to isolate reasoning from factual knowledge using mathematical or logical reasoning benchmarks. Unfortunately, such benchmarks are not entirely reliable as indicators of reasoning performance either. On GSM8K, a dataset of grade school math problems, Mirzadeh et al. (2024) report that frontier models perform significantly worse with minor or even meaningless alterations to the test data\u2014indicating these models are vulnerable to overfitting at best and exact memorization at worst. To ensure fair comparison, LLMs need to be evaluated in a way that does not depend on any particular dataset instance.\nFollowing this philosophy, we develop PhantomWiki. At the click of a button, PhantomWiki generates a fictional universe of characters along with a set of facts about them. We reflect these facts in a large-scale corpus, mimicking the style of fan-wiki websites. Then we generate question-answer pairs about the universe, encapsulating the types of multi-hop questions commonly considered in the question-answering (QA) literature.\nWe design PhantomWiki to decouple the testing of LLM reasoning and retrieval capabilities in a range of settings. In the first setting, the universe is small enough so that all relevant"}, {"title": "2. Related Work", "content": "Agent benchmarks, such as T-bench (Yao et al., 2024), Tool-Woz (Lattimer et al., 2024), Alfworld (Shridhar et al., 2020) and WebArena (Zhou et al., 2023), focus on tasks where the agent is given a binary reward for successful completion of a task (e.g., booking a flight, making a purchase). In this work, we focuses more on tasks where the agent is rewarded for responding to a question with a factually correct answer. (In Section 3, we concretize what we mean by a \"fact\" and \"correctness\".) Zhou et al. (2023, Section 3.2) include a category of information-seeking tasks, however these tasks often require navigation across multiple pages or focus on user-centric content. Yao et al. (2024, Appendix. A) measure task difficulty based on the average success rate of frontier models (e.g., GPT-4). Our work defines a model-agnostic measure of difficulty, which we show provides more meaningful insight into the reasoning and retrieval aspects of LLMs.\nIn the QA domain, existing benchmarks are designed to test whether LLMs are able to reason and use tools. Closer to our work in the space of question-answering agents is the ToolQA benchmark of Zhuang et al. (2023). They introduce a framework to construct question-answer pairs from databases and documents by first generating question templates using LLMs, then filtering for high-quality templates, and finally deriving ground-truth answers by writing corresponding Python programs for each question template. Zhuang et al. (2023, Tab. 1) construct two pure-text datasets: SciREX with 438 documents and 5 question templates, and Agenda with 10k event entries and 10 question templates. In constrast, PhantomWiki generates instances at much larger scale with 50 question templates and 1 million documents.\nFor generating factually-grounded answers, retrieval augmented generation (RAG) has emerged as the predominant paradigm (Lewis et al., 2020; Karpukhin et al., 2020; Guu et al., 2020). However, evaluating RAG systems is notoriously difficult, leading to a flourishing of retrieval benchmarks (Petroni et al., 2020; Saad-Falcon et al., 2023; Jin et al., 2024; Hsia et al., 2024; Mao et al., 2024; Rau et al., 2024). A key pain-point of RAG is handling questions that involve multi-hop reasoning. This motivated Tang & Yang (2024) to design the MultiHop-RAG dataset with synthetically generated questions and Su et al. (2024) to curate a dataset of question-answer pairs that requires intensive"}, {"title": "3. PhantomWiki Construction", "content": "PhantomWiki is at its core an on-demand random generator of fictional worlds. Similarly to the wiki hosting services popular in film, video games, and literature\u00b9, we represent these fictional worlds through Wikipedia-like biographical entries about their characters. We then test the model's retrieval skills and its understanding of the fictional world through an accompanying set of automatically generated question-answer pairs.\n3.1. Generating a Phantom Wiki Universe\nThe first stage of the PhantomWiki pipeline generates a random universe of n characters as well as the document corpus describing it, as illustrated in Figure 2, (1-2).\nGenerating Characters. Each character in a PhantomWiki universe is described through its social relationships and personal facts (Figure 2, (1)). For the social relationships, we first generate family trees, following the family tree generator of Hohenecker & Lukasiewicz (2020). We iteratively pick a person and generate their parent or child based on various constraints\u00b2, until the user-specified universe size of n people is reached. The user can also specify other hyperparameters like the number of trees, their maximal depth, and the maximal number of offspring for each person. In addition to the family trees, we generate a friendship graph using the Erd\u0151s\u2013R\u00e9nyi model (making two people friends with some fixed probability, typically controlled by the desired average number of friendships.)\nGenerating Facts. Next, we generate personal facts for each person in the PhantomWiki universe. Names are assigned during the family generation procedure, with the first name sampled based on the character's gender and the sur-"}, {"title": "3.2. Generating Question-Answer Pairs", "content": "In the second half of the PhantomWiki pipeline, we generate a set of questions with verifiable answers, as shown in Figure 2, (3-4).\nGenerating Questions. We implement automatic question generation through a context-free grammar (CFG, Hopcroft et al. 2001) of question templates, which we then use to sample complete questions. For example, the question template \"Who is the <relation> of <name>?\" can be used to sample the question \"Who is the friend of David?\" (see Figure 2, (3)). The main advantage of using a CFG is that it efficiently and systematically obtains all possible compositions of questions for some recursion depth d. For instance, the following subset of our context-free grammar:\nS\u2192 Who is R?\nR \u2192 the <relation> of R'\nR' \u2192 <name>\ncan lead to questions ranging from \u201cWho is the friend of David?\u201d to \"Who is the nephew of the friend of the brother of David?\u201d as d increases. In addition to these nested compositions, our CFG also supports questions about personal attributes (e.g. \"Who is the person whose hobby is bird-watching?\"), aggregation questions (\u201cHow many brothers does David have?\u201d), and combinations of all three (\"How many friends does the brother of the person whose hobby is birdwatching have?\") (For the full CFG see Appendix B.)\nGenerating Answers. To ensure that the answers to"}, {"title": "3.3. Phantom Wiki Complexity", "content": "The goal of PhantomWiki is to generate memorization-resistant evaluation datasets that are challenging in both reasoning and retrieval aspects. In this section, we discuss our conceptual and practical design choices that help us achieve this goal.\nUniverse Space Complexity. To ensure that our evaluation with PhantomWiki is memorization and data leakage-resistant,\nwe first show that the space of possible universes is sufficiently large to generate enough unique instances. Observe that the number of possible friendship assignments grows at the rate of  (Flajolet & Sedgewick, 2009, Ex. II.5) as the number of individuals n in the universe increases. Similarly, assuming each individual is assigned one fact from each category (job, hobby, etc.), the number of possible fact assignments grows at the rate  , where c is the total number of choices across the categories. PhantomWiki thus samples a corpus from , which leads to diverse datasets optimal for data leakage-resistant evaluation. We note that as future work PhantomWiki could be extended to increase this diversity, e.g. by adding a temporal dimension of events.\nReasoning Complexity. The CFG enables us to recursively compose templates that lead to complex reasoning questions. Observe that our CFG in Appendix B produces  question templates as the recursion depth d increases. Moreover, we can increase the difficulty of each template by"}, {"title": "4. Experimental Validation", "content": "We evaluate reasoning and retrieval capabilities of several frontier LLMs using PhantomWiki, by decomposing their performance over questions of varying difficulty and universes of varying sizes.\n4.1. Evaluation Setup\nWe generate PhantomWiki instances with n ranging from 50 to 10K\u2014a universe size for which the total length of articles exceed the LLM context length. For the evaluation, only the articles (not the Prolog database or the generated graphs) will be provided to the LLMs. To ensure that our findings are not tied to any specific PhantomWiki instance, we use 3 random dataset seeds for each configuration. Creating PhantomWiki instances with different random seeds leads to entirely different combinations of names, relations, and personal facts. In each instance, we generate question templates with maximum recursion depth d = 20, for a total of 50 templates. We sample 10 questions for each template, yielding a total of 500 questions per PhantomWiki instance. As shown in Figures 5 and 6 (Appendix B), these questions have varying difficulty and number of answers. Accordingly, we prompt the LLMs to predict all answers as a comma-separated list and measure correctness with the answer-level F1 score."}, {"title": "4.2. Models and Prompting Techniques", "content": "We test both open- and closed-source LLMs, namely OpenAI's GPT-40 (Hurst et al., 2024), Google's Gemini-1.5-Flash (Gemini Team, Google, 2024), and the instruction-tuned version of Meta's Llama-3.3-70B model (Dubey et al., 2024). We also evaluate DeepSeekAI's DeepSeek-R1-32B (Guo et al., 2025) (distilled with Qwen-2.5-32B (Yang et al., 2024)), which is an open-weights LLM trained on reasoning trace datasets. We prompt each LLM with the following techniques, broadly grouped in three ways:\nIn-Context Prompting. This technique includes the whole document corpus as part of the prompt. We use this type of prompting in conjunction with two strategies: ZEROSHOT-where the document corpus is immediately followed by the question-and Chain-of-Thought (COT) prompting (Wei et al., 2022), where we additionally include some examples on how step-by-step reasoning could lead to the correct answer. We include these prompts in Appendix C.2, as well as modifications to the ZEROSHOT strategy for DeepSeek-R1-32B.\nRAG Prompting. This setting augments generation with a pre-trained neural retriever (Lewis et al., 2020). We implement this by first searching for the 4 most relevant documents to the posed question based on UAE-LARGE-V1 embeddings. Next, we incorporate these retrieved documents into the model's prompt. Finally, we add in the same ZEROSHOT and COT prompts as In-Context Prompting. We document details on our retrieval algorithm in Appendix C.3.\nAgentic Prompting. REACT (Yao et al., 2022) is a prompting technique that enables LLMs to interleave reasoning steps with tool interactions, to solve complex tasks. For PhantomWiki QA task, the LLMs are provided with keyword-based tools RetrieveArticle and Search to retrieve relevant documents (see Appendix C.6 for tool"}, {"title": "4.3. Discussion", "content": "In Table 2, we report the mean F1 score across various universe sizes, LLMs, and prompting techniques. We first average F1 scores over all questions in a PhantomWiki instance, then compute the mean and standard error across the dataset generation seeds.\nWe first consider the small-universe setting (n = 50) in Table 2, which corresponds to roughly 16K tokens for the LLMs we test. In-Context prompting techniques outperform other techniques: COT with GPT-40 attains the highest performance, followed by ZEROSHOT with DeepSeek-R1-32B. Next, we consider the setting of medium universes (n = 500). Here the full document corpus can still be included in all LLMs' contexts, but we find that ZEROSHOT performs poorly for all LLMs, and DeepSeek-R1-32B especially struggles. F1 scores of COT for all LLMs degrade as well compared to n = 50, but not worse than REACT. Finally, in the setting of large universes (n = 5000), none of the LLMs we evaluate can accommodate the full document corpus. In-context techniques are no longer viable, and we must rely on RAG prompting and agentic prompting. RAG prompting attain poor F1 scores because the retriever fails to retrieve documents relevant for answering complex questions. On the other hand, agentic prompting technique shines in comparison to other techniques, indicating that LLMs are better suited to dynamically retrieve documents while reasoning on a question. We attribute the poor performance of DeepSeek-R1-32B in agentic prompting technique to its inferior tool-calling abilities compared to the other LLMs."}, {"title": "5. Evaluating Reasoning", "content": "To isolate LLM reasoning capabilities with PhantomWiki, we investigate model performance on small universes (n = 50) in Figure 3. Note that contexts of all LLMs can fully include small universe document corpora. Each PhantomWiki dataset contains questions covering a wide range of difficulty. We evaluate three approaches: in-context prompting, RAG prompting, and agentic prompting. For each we plot the F1 scores as a function of question difficulty, as measured by the number of reasoning steps necessary to answer the question. As mentioned in Section 3.3, this is determined by the type of question templates and the sampled relationships. For all LLMs and prompting techniques, we verify empirically that questions with larger reasoning steps are indeed more challenging to answer. By allowing question difficulty to be adjusted, PhantomWiki serves as a foundational benchmark for evaluating reasoning capabilities in language models.\nZEROSHOT performance declines sharply as the number of reasoning steps increases for all LLMs, except for DeepSeek-R1-32B, which deteriorates more gradually. LLMs perform better with COT than with ZEROSHOT, but each additional reasoning step remains increasingly challenging. This suggests that even in the absence of retrieval constraints, LLMs struggle to navigate logical reasoning sequences.\nRAG prompting techniques (ZEROSHOT-RAG and COT-RAG) stunt reasoning performance across the board\u2014F1 scores are near zero on questions with 5 or more reasoning steps as opposed to 15 steps for in-context prompting. We attribute this to a core problem with RAG prompting: retrieving documents in the initial prompt before starting to answer the question, as opposed to reasoning through the question and retrieving documents dynamically.\nWe find that RAG prompting techniques can only answer questions that require a single reasoning step, like Who is the friend of David?. On the other hand, answering questions that require information from multiple reasoning steps is extremely challenging for ZEROSHOT-RAG and COT-RAG. To illustrate, consider the question Who is the nephew of the friend of David? Answering this question requires retrieving David's document first and then retrieving their friend's document to find the nephew. Since RAG prompting techniques retrieve documents only once by matching vector embeddings of questions and documents, they are unlikely to retrieve all necessary documents required to answer such questions.\nFinally, the agentic prompting technique REACT allows LLMs to avoid the steep performance drop as seen in RAG prompting. On given a question, REACT prompting requires LLMs to retrieve documents dynamically in a conversation and justify why they are relevant. Concretely, before using a tool (RetrieveArticle or Search) in a conversation turn, the LLM is asked to describe how the tool will help using a \"Thought\" step (Yao et al., 2022), analogous to the COT prompting approach. This approach shows promise in answering questions correctly. Even so, REACT struggles as the question difficulty increases.\nFigure 3 thus decomposes LLM performance along the lines of reasoning capabilities. It reveals that all in-context prompting and agentic prompting achieve near-perfect F1 scores on low-difficulty questions. Therefore, the stratification between them in Table 2 can be attributed to varying performance on high difficulty questions. To further isolate the impact of question difficulty, in Figure 7 we plot F1 scores as a function of reasoning steps for questions with only one solution."}, {"title": "6. Evaluating Retrieval", "content": "Next, to evaluate LLM retrieval capabilities, we use PhantomWiki to contrast two settings: (1) small universes where the document corpus can comfortably fit in LLM context, and (2) large universes where the full corpus exceeds context lengths. To this end, we increase the universe size up to n = 10K, which corresponds to document corpora well beyond the context lengths of state-of-the-art LLMs, and display the results in Figure 4.\nFor very small universes, COT usually outperforms ZE-"}, {"title": "7. Conclusion and Future Work", "content": "We introduce PhantomWiki\u2014a benchmarking framework to evaluate reasoning and retrieval capabilities of language models. As we increase the question complexity and universe size, we observe that current state-of-the-art LLMs struggle in both aspects. PhantomWiki is scalable and memorization-resistant, hence well-suited to evaluate future generations of language models.\nOur work brings forth several research directions. Noting how we generate document corpora and questions, PhantomWiki is resistant to data contamination. We leave to future work to empirically test this claim, and to develop theory to formally prove that our benchmark is memorization-resistant. In this work we focus on question-answering over text corpora. We hope to extend PhantomWiki to other knowledge bases and modalities such as vision and audio, enabling analogous test suites for multimodal models."}, {"title": "A. Background", "content": "A.1. Context-Free Grammars\nContext-free grammar (CFG) is a type of formal grammar where the productions rules govern how to generate text from non-terminals and terminals. A context-free grammar is defined by G = (V, \u03a3, R, S) where V and \u2211 denotes nonterminal and terminal respectively. R is a finite relation in V \u00d7 (VUE)* which specifies the production rules of the grammar. S \u2208 V is the start symbol. A production rule in R has the form\n\u03b1 - \u03b2 (1)\nwhere a \u2208 V, \u03b2\u2208 (VU \u03a3)*. It is conventional to list all rules with the same left-hand side on the same line and separate the right-hand side with \u201c|\u201d like a \u2192 \u03b21 | \u03b22."}, {"title": "B. Question template generation", "content": "B.1. Context-Free Grammar\nWe use the following CFG to generate question templates:\nS -> Who is R? | What is A ? | How many RN_p does R_c have ?\nR -> the RN of R_c | the person whose AN is AV\nR_c -> RN\nA -> the AN of R\nRN -> <relation>\nRN_p -> <relation_plural>\nAN -> <attribute_name>\nAV -> <attribute_value>\nN-> <name>\nB.2. CFG-generated question templates\nOur CFG produces the following 50 question templates at recursion depth d = 20. Note how the recursive production rule R_c -> R | N leads to chained productions.\n1. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of the <relation>_15 of\nthe <relation>_17 of the person whose <attribute_name>_19 is <\nattribute_value>_19?\n2. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of the <relation>_15 of\nthe <relation>_17 of <name>_18?\n3. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of the <relation>_15 of\nthe person whose <attribute_name>_17 is <attribute_value>_17?\n4. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of the <relation>_15 of\n<name>_16?\n5. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of the person whose <\nattribute_name>_15 is <attribute_value>_15?\n6. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the <relation>_13 of <name>_14?\n7. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of the person whose <attribute_name>_13 is <\nattribute_value>_13?\n8. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the <relation>_11 of <name>_12?\n9. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of the person whose <attribute_name>_11 is <attribute_value>_11?\n10. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the <\nrelation>_9 of <name>_10?\n11. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of the person\nwhose <attribute_name>_9 is <attribute_value>_9?\n12. Who is the <relation>_3 of the <relation>_5 of the <relation>_7 of <name>_8?\n13. Who is the <relation>_3 of the <relation>_5 of the person whose <\nattribute_name>_7 is <attribute_value>_7?\n14. Who is the <relation>_3 of the <relation>_5 of <name>_6?\n15. Who is the <relation>_3 of the person whose <attribute_name>_5 is <\nattribute_value>_5?\n16. Who is the <relation>_3 of <name>_4?\n17. Who is the person whose <attribute_name>_3 is <attribute_value>_3?\n18. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of the <relation>_18 of <name>_19?\n19. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of the person whose <attribute_name>_18 is <\nattribute_value>_18?\n20. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of <name>_17?\n21. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the person whose <attribute_name>_16 is <attribute_value>_16?\n22. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof <name>_15?\n23. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the person whose <\nattribute_name>_14 is <attribute_value>_14?\n24. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of <name>_13?\n25. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the person whose <attribute_name>_12 is\n<attribute_value>_12?\n26. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of <name>_11?\n27. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the person whose <attribute_name>_10 is <attribute_value>_10\n?\n28. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\n<relation>_8 of <name>_9?\n29. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of the\nperson whose <attribute_name>_8 is <attribute_value>_8?\n30. What is the <attribute_name>_3 of the <relation>_4 of the <relation>_6 of <\nname>_7?\n31. What is the <attribute_name>_3 of the <relation>_4 of the person whose <\nattribute_name>_6 is <attribute_value>_6?\n32. What is the <attribute_name>_3 of the <relation>_4 of <name>_5?\n33. What is the <attribute_name>_3 of the person whose <attribute_name>_4 is <\nattribute_value>_4?\n34. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of the <relation>_18 of <name>_19 have?\n35. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of the person whose <attribute_name>_18 is <\nattribute_value>_18 have?\n36. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the <relation>_16 of <name>_17 have?\n37. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof the person whose <attribute_name>_16 is <attribute_value>_16 have?\n38. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the <relation>_14\nof <name>_15 have?\n39. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of the person whose <\nattribute_name>_14 is <attribute_value>_14 have?\n40. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the <relation>_12 of <name>_13 have?\n41. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of the person whose <attribute_name>_12 is\n<attribute_value>_12 have?\n42. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the <relation>_10 of <name>_11 have?\n43. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of the person whose <attribute_name>_10 is <attribute_value>_10\nhave?\n44. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\n<relation>_8 of <name>_9?\n45. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of the\nperson whose <attribute_name>_8 is <attribute_value>_8 have?\n46. How many <relation_plural>_2 does the <relation>_4 of the <relation>_6 of <\nname>_7 have?\n47. How many <relation_plural>_2 does the <relation>_4 of the person whose <\nattribute_name>_6 is <attribute_value>_6 have?\n48. How many <relation_plural>_2 does the <relation>_4 of <name>_5 have?\n49. How many <relation_plural>_2 does the person whose <attribute_name>_4 is <\nattribute_value>_4 have?\n50. How many <relation_plural>_2 does <name>_3 have?\nB.3. Question-Answer Characteristics\nC. Baseline Details\nC.1. LLM SAMPLING HYPERPARAMETERS\nWe used the above default hyperparameters values for all models, but DeepSeek-R1-32B, where we used temperature = 0.6 and top-p = 0.95."}, {"title": "D. Additional Results", "content": null}]}