{"title": "ENHANCING HUMAN-LIKE RESPONSES IN LARGE LANGUAGE MODELS", "authors": ["Ethem Ya\u011f\u0131z \u00c7al\u0131k", "Talha R\u00fczgar Akku\u015f"], "abstract": "This paper explores the advancements in making large language models (LLMs) more human-like.\nWe focus on techniques that enhance natural language understanding, conversational coherence, and\nemotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning\nwith diverse datasets, incorporating psychological principles, and designing models that better mimic\nhuman reasoning patterns. Our findings demonstrate that these enhancements not only improve user\ninteractions but also open new possibilities for AI applications across different domains. Future work\nwill address the ethical implications and potential biases introduced by these human-like attributes.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown remarkable progress in understanding and generating natural language,\nthanks to their training on vast and diverse datasets. Base models such as Llama [7], Qwen [5], and Mistral Nemo\n[3] are pre-trained on extensive corpora, enabling them to grasp language structure and semantics. However, despite\nthis progress, LLMs often produce responses that are formal and impersonal, falling short of the natural human-like\nconversations many users expect.\nOur research addresses this shortcoming by focusing on improving the \"human-likeness\" of LLM responses. Specifically,\nwe aim to make AI interactions feel more conversational, relatable, and emotionally attuned, without sacrificing accuracy\nin more formal or structured tasks. To achieve this, we developed synthetic datasets tailored for fine-tuning models\nusing the Direct Preference Optimization (DPO) technique [17]. These datasets allow the models to balance casual,\nconversational language with structured, topic-based dialogue, resulting in more natural and human-like interactions.\nOur findings demonstrate that these techniques significantly enhance both conversational fluency and user engagement,\nbringing AI closer to mimicking real human communication."}, {"title": "Related Works", "content": "A variety of research has been dedicated to enhancing the human-like qualities of large language model (LLM) responses.\nTechniques such as Reinforcement Learning from Human Feedback (RLHF) have significantly refined model outputs\nby aligning them with user preferences and expectations [13].\nOne prominent model, DialoGPT, leverages extensive Reddit data to produce responses that closely resemble human\nconversation [27]. Similarly, Meena, a multi-turn chatbot, has been optimized to achieve high dialogue coherence\nthrough metrics like Sensibleness and Specificity Average (SSA) [2]. The LLM Roleplay framework takes a unique\napproach by generating diverse dialogues through persona-based interactions, further simulating human-chatbot\nexchanges [22].\nOur methodology builds upon these inspring techniques by integrating psychological insights and utilizing a range of\nsystem prompts aimed at eliciting both casual and formal responses. By implementing Direct Preference Optimization\n(DPO), we emphasize user engagement while maintaining linguistic accuracy [17]. Additionally, we address the ethical\nimplications surrounding human-like AI responses, aligning our efforts with existing research that explores biases\nin model outputs and the potential consequences of emotional mimicry in AI [26]. Our contributions include the\ndevelopment of specialized datasets that enhance conversational coherence and mitigate the ethical risks associated\nwith AI models that closely replicate human emotions."}, {"title": "Data Preparation", "content": "To enhance the conversational abilities of Large Language Models (LLMs) and generate responses that better mimic\nhuman communication, we utilized the Llama 3 70B and 405B models to create a synthetic dataset, following a\nmethodology similar to the Self-Instruct [24] approach. The dataset generation involved Llama 3 405B for question\ngeneration and Llama 3 70B for answer generation. We employed custom system prompts designed to elicit both\nhuman-like and formal, impersonal responses. This strategy enabled us to categorize the responses into two distinct\ngroups: those that closely resemble natural human dialogue (chosen) and those that are more formal and impersonal\n(rejected)."}, {"title": "System Prompts", "content": "The data generation process centered around the use of carefully crafted system prompts that guided the Llama 3\n70B model to produce more conversational, human-like responses in the answer generation stage. These prompts\nwere designed to ensure that while the Llama 3 70B model became better at casual dialogue, it retained its strong\ngeneral knowledge and performance on topic-specific benchmarks. This balance allowed the model to excel in natural\nconversation without sacrificing its competence in handling diverse, subject-matter-specific queries.\n1. Conversational Questions: These prompts generated questions that mimic natural, human-like conversations,\nfocusing on personal experiences, preferences, and hypothetical scenarios.\n2. General Knowledge Questions: These prompts produced questions that address broader topics and require a\nmore informed response. The focus was on generating content that would challenge the model's ability to\nhandle complex, real-world issues.\nThese questions were then used as input for the Llama 3 70B to produce two distinct types of responses:\n1. Human-like Responses: Each question was presented to the LLM with a system prompt designed to\nelicit responses that are natural, conversational, and engaging, closely mimicking the way a person would\ncommunicate.\n2. Formal, Impersonal Responses: The same question was then provided to the LLM with a different system\nprompt, crafted to generate responses that are more formal and impersonal. This prompt encouraged the model\nto produce content that is structured, clear, and precise, but lacks the warmth and spontaneity of natural human\nconversation.\nThis approach allowed us to create a dataset that clearly differentiates between human-like and formal, impersonal\nresponses. By using these distinct prompts, we implemented a reward mechanism during training with Direct Preference\nOptimization (DPO) [17], guiding the model to prioritize more natural, engaging communication styles.\nFor generating questions, human-like responses, and formal responses, we used the system prompts referenced in\nAppendix A."}, {"title": "Data Generation Process", "content": "The data generation process involved configuring the Llama 3 70B model with specific parameters to control the\nvariability and creativity of the responses. We chose a temperature value of 1 and a top-p value of 1 to encourage the\nmodel to produce more creative and diverse responses [16]. These settings allowed the model to explore a broader\nrange of possible outputs, which was essential for generating different data points and achieving the variety needed to\ndistinguish between human-like and formal, impersonal responses."}, {"title": "Dataset Overview and Visualization", "content": "The resulting embeddings were visualized using the Atlas Nomic Map [4], as illustrated in Figure 2. This map provides\nan interactive exploration of the dataset, helping to analyze the structure and topic distribution effectively. We observed\nthat topics were naturally clustered into categories such as Traveling, Sports, Fitness, Music, Technology, Nature,\nHealth, Science, Family, Culture, Daily Life and Language.\nUsing the map allows us to better understand the composition of our dataset, identify clusters of related topics, and\ndetect potential imbalances. Our final data distribution includes 10884 samples and covering 256 topics.\nTo explore the dataset in greater detail, an interactive map is available here. Furthermore, notable examples from\nspecific clusters are presented in Appendix B to illustrate the dataset's breadth and depth. A representative sample row\nis provided in Table 1 for reference."}, {"title": "Model Training", "content": "We conducted extensive training on a variety of models, employing techniques such as LoRA (Low-Rank Adaptation)\nand DPO (Direct Preference Optimization) to significantly enhance their performance and capabilities. Our efforts\nfocused on leveraging the model's established strengths, such as its ability to understand context and generate coherent\nresponses, while refining its performance to facilitate more natural, human-like interactions."}, {"title": "Training Techniques", "content": "We employed the Low-Rank Adaptation (LoRA) [12] technique for fine-tuning the models, which addresses the\nchallenge of catastrophic forgetting by preserving the model's general knowledge while adapting to specific tasks [19].\nTo further enhance the models performance, we optimized the trained parameters using Direct Preference Optimization\n(DPO) [17]. Direct Preference Optimization (DPO) [17] technique was chosen to implement a reward mechanism that\nguides the model towards more human-like behavior during training. A detailed explanation of the technical aspects is\nalso provided in Appendix E."}, {"title": "Training Phase", "content": "We conducted our models training using the Axolotl [14] framework and used Weights and Biases [6] for tracking\nour experiments. The models were trained using a variety of hyperparameters, and their performance was assessed\nthrough targeted testing. The models used for training included Llama3-8B-Instruct [7], Qwen-2.5-7B-Instruct [5] and\nMistral-Nemo-Instruct-2407 [3]."}, {"title": "Hyperparameters", "content": "The following Table 2 and 3 summarizes the hyperparameters (model training settings) used during the training of\nofficial instruction models from various sources."}, {"title": "Training Overview and Performance Analysis", "content": "Each model was fine-tuned using 2xNVIDIA A100 SXM (80 GB) GPU [15]. The total training time and parameter size\nfor each model are detailed in Table 4, offering an overview of the computational resources allocated for training.\nThe training time of each model reflects the computational demands necessary to achieve the desired performance\nmetrics. Notably, the training durations for the Human-Like-Llama-3-8B-Instruct and Human-Like-Qwen-2.5-\nInstruct models were nearly identical, while the training duration for Human-Like-Mistral-Nemo-Instruct-2407 was\nlonger due to its larger parameter size."}, {"title": "Evaluation", "content": "To assess which models generated the most \"human-like\" responses, we implemented an anonymous voting system\nusing the Gradio library [1], hosted on Hugging Face Spaces [8]. This setup allowed participants to compare responses\nfrom our fine-tuned models with those from the official instruction models and select the response they judged to be\nmore human-like."}, {"title": "Human-Likeness Evaluation", "content": "To assess which models generated the most \"human-like\" responses, we implemented an anonymous voting system\nusing the Gradio library [1], hosted on Hugging Face Spaces [8]. This setup allowed participants to compare responses\nfrom our fine-tuned models with those from the official instruction models and select the response they judged to be\nmore human-like.\nEach voting session presented participants with two anonymized responses one from a fine-tuned model and one\nfrom an official instruction model. To minimize bias and identifiable patterns, all emojis were removed from the\ndisplayed responses. The evaluation was conducted using a set of 500 questions generated through the methodology\ndescribed in Section 3. In total, the study collected 2000 votes across three model pairs.\nThe annotators were a diverse pool, consisting mostly of high school students, who were generally non-native English\nspeakers, and adults, who included both native and non-native English speakers. The link to the annotation space\nwas broadly shared online across student communities to encourage participation. While this diversity offers a broad\nperspective on the perceived human-likeness of model responses, the language proficiency of the annotators and the\npredominance of high school students are noted as potential limitations, which are discussed in Section 6.1.\nThe results, summarized in Table 5, demonstrate that our fine-tuned models consistently outperformed the official\ninstruction models. The Human-Like-Llama-3-8B-Instruct and Human-Like-Qwen-2.5-7B-Instruct models were selected\nnearly 90% of the time. For the Mistral-Nemo-Instruct pair, the fine-tuned model was also preferred significantly more\noften, though by a narrower margin.\nThese results highlight the effectiveness of our fine-tuning approach in producing more human-like responses. To\nprovide a clearer understanding, Appendix D presents a detailed example where participants clearly favored responses\nfrom the fine-tuned models due to their superior conversational flow and context adherence. The example underscores\nthe models' ability to produce language that is natural, coherent, and engaging.\nA notable shortcoming of the official instruction models was their tendency to include self-referential disclaimers\nlike \"I am just a language model...\" or \"As a digital assistant, I cannot answer...,\" which disrupted the conversational\nexperience. In contrast, our fine-tuned models avoided such mechanical phrasing, delivering responses that were more\ndirect and contextually appropriate, thereby enhancing their perceived human-likeness.\nIn summary, our findings demonstrate that fine-tuning effectively reduces mechanical phrasing and enhances conver-\nsational coherence. This improvement brings AI interactions closer to natural human communication, making these\nmodels more suitable for real-world conversational applications."}, {"title": "Open LLM leaderboard Evaluation", "content": "We anticipated a minor performance change in benchmarks due to adjustments in the model's weights. To minimize\nthis impact while maintaining a human-like and conversational style, we specifically chose a lower value of r = 8 (as\noutlined in Section 4.2.1), which controls the number of trainable parameters of the model. This choice helps avoid\nsignificantly altering the model's weights and preserves its general capabilities across various benchmarks [19]. We\nevaluated our models using Open LLM Leaderboard [10], where their performance was assessed across IFEval [28],\nBBH [21], Hendrycks Math Level 5 [11], GPQA [18], MUSR [20], and MMLU-Pro [25] benchmarks.\nAs expected, there was a slight average performance change in our fine-tuned models. The comparison between our\nfine-tuned models and the official instruction models is summarized in Table 6."}, {"title": "Discussion", "content": "This research encountered several notable limitations. A primary issue was the lack of high-quality, human-generated\ndatasets, which are crucial for creating realistic, diverse training data. To address this, we generated synthetic datasets\ntailored to elicit human-like responses. While this approach improved the conversational quality of the model, the\ninherent limitations of synthetic data meant that it lacked the richness and variability found in real user interactions,\nthereby restricting the model's ability to generalize effectively across a wide range of topics.\nAnother challenge was the computational intensity of using the Llama 3 70B and 405B models. Their resource-heavy\nnature constrained both the volume of data we could generate and the number of experiments we could conduct within\na feasible timeframe. To compensate, we focused on optimizing the data generation process, ensuring that each sample\nwas of high quality and contributed meaningfully to training. Despite these efforts, the limited dataset size reduced the\nmodel's exposure to diverse contexts, which could have further enhanced its human-like response capabilities across a\nbroader array of scenarios.\nAdditionally, our human-likeness evaluation process was influenced by the composition of the annotator pool. The\nmajority of annotators were high school students, primarily non-native English speakers, with varying levels of language"}, {"title": "Limitations", "content": "This research encountered several notable limitations. A primary issue was the lack of high-quality, human-generated\ndatasets, which are crucial for creating realistic, diverse training data. To address this, we generated synthetic datasets\ntailored to elicit human-like responses. While this approach improved the conversational quality of the model, the\ninherent limitations of synthetic data meant that it lacked the richness and variability found in real user interactions,\nthereby restricting the model's ability to generalize effectively across a wide range of topics.\nAnother challenge was the computational intensity of using the Llama 3 70B and 405B models. Their resource-heavy\nnature constrained both the volume of data we could generate and the number of experiments we could conduct within\na feasible timeframe. To compensate, we focused on optimizing the data generation process, ensuring that each sample\nwas of high quality and contributed meaningfully to training. Despite these efforts, the limited dataset size reduced the\nmodel's exposure to diverse contexts, which could have further enhanced its human-like response capabilities across a\nbroader array of scenarios.\nAdditionally, our human-likeness evaluation process was influenced by the composition of the annotator pool. The\nmajority of annotators were high school students, primarily non-native English speakers, with varying levels of language\nproficiency. While some adult annotators participated - including both native and non-native English speakers - the\npredominance of younger, non-native speakers may have introduced bias in the perception of human-likeness. This\nvariation in age, language proficiency, and familiarity with AI systems represents a potential limitation in assessing the\ngeneralizability of our results.\nThese limitations underscore the trade-offs between data quality, computational resources, annotator demographics,\nand model performance in achieving human-like conversational abilities. Balancing these factors is critical for future\nadvancements. While high-quality datasets and computational power are essential for enhancing model capabilities,\nchallenges related to resource constraints and annotator variability can impact the overall effectiveness. Addressing\nthese issues through improved dataset diversity, efficient computation, and more representative evaluation processes\nwill be key to developing models that exhibit richer and more consistent human-like responses."}, {"title": "Ethical considerations", "content": "As large language models (LLMs) become increasingly human-like in their responses, several ethical concerns need to\nbe addressed. One significant challenge is the potential for users to mistake AI-driven interactions for human ones,\nespecially as these systems become more integrated into everyday life. If these systems, for example, are combined\nwith voice agents, it could become difficult for users to distinguish between a human and an AI, raising concerns about\ntransparency and trust. To mitigate this, AI developers should ensure that systems clearly disclose their machine nature,\nsuch as through verbal or visual cues, and maintain transparency in all interactions. This aligns with the EU AI Act\n[23], which emphasizes the need to avoid manipulative or subliminal techniques that could distort user behavior or\nimpair decision-making.\nMoreover, the human-like attributes of these models can inadvertently introduce or amplify biases present in the training\ndata, leading to unfair or discriminatory outcomes. To address this, rigorous bias detection and mitigation techniques\nmust be incorporated during both the training and deployment phases. Regular audits and updates of the model can\nfurther ensure ethical standards are maintained, particularly in sensitive domains such as healthcare, law, or customer\nservice. Compliance with the EU AI Act also requires that these models do not exploit vulnerabilities based on age,\ndisability, or socioeconomic status, underscoring the importance of ethical safeguards.\nAdditionally, the psychological impact of interacting with highly realistic AI systems must be carefully managed.\nUsers may form emotional attachments or misunderstand the limitations of these models, leading to unrealistic\nexpectations. To prevent this, developers should incorporate clear communication about the AI's capabilities and\nlimitations, perhaps through user education initiatives or built-in explanations. Furthermore, the EU AI Act explicitly\nprohibits emotion inference in sensitive contexts like workplaces and educational institutions. By adhering to these\nregulatory frameworks and establishing robust ethical guidelines, developers can ensure that advancements in LLMs are\nimplemented responsibly and transparently."}, {"title": "Conclusion", "content": "This study presents several contributions that advance the development of more natural and human-like interactions\nin large language models (LLMs). We demonstrate that open-source models can be fine-tuned to produce responses\nthat are more conversational and closely resemble human communication, addressing the common issue of formal and\nimpersonal output found in many existing LLMs. Importantly, our approach maintains the performance of these models\nacross various benchmarks, with no noticeable loss in accuracy or efficiency despite the enhancements in naturalness.\nAdditionally, we introduce a novel approach to dataset creation by developing synthetic datasets specifically designed\nto enhance the human-like qualities of LLMs. This work not only improves the conversational abilities of the models\nbut also contributes valuable resources that can be used in future research aimed at making AI systems more engaging\nand effective in real-world applications."}, {"title": "Summary of Contributions", "content": "This study presents several contributions that advance the development of more natural and human-like interactions\nin large language models (LLMs). We demonstrate that open-source models can be fine-tuned to produce responses\nthat are more conversational and closely resemble human communication, addressing the common issue of formal and\nimpersonal output found in many existing LLMs. Importantly, our approach maintains the performance of these models\nacross various benchmarks, with no noticeable loss in accuracy or efficiency despite the enhancements in naturalness.\nAdditionally, we introduce a novel approach to dataset creation by developing synthetic datasets specifically designed\nto enhance the human-like qualities of LLMs. This work not only improves the conversational abilities of the models\nbut also contributes valuable resources that can be used in future research aimed at making AI systems more engaging\nand effective in real-world applications."}, {"title": "Future work", "content": "Future research can advance this study through several key strategies. Expanding and diversifying the dataset could sig-\nnificantly enhance model performance and generalization across various scenarios. Investigating advanced optimization\ntechniques, such as Low-Rank Adaptation (LoRA) [12] and Direct Preference Optimization (DPO) [17], and contrasting\ntheir effectiveness with other training methods may uncover new insights and potential performance gains. Integrating\nuser-generated data could provide crucial feedback on the model's applicability in real-world contexts, offering practical\ninsights that could guide further refinement. Additionally, evaluating the model using a broader range of metrics\nand conditions would yield a more nuanced understanding of its strengths and limitations, facilitating more precise\nadjustments. Exploring advancements in model architectures and training methodologies could further the development\nand refinement of this research. Training larger models, when feasible, might result in improved performance and more\naccurate outcomes. These enhancements could lead to greater model scalability and effectiveness, paving the way for\nmore ambitious and impactful applications in the future."}, {"title": "System Prompts", "content": null}, {"title": "System prompt for generating human-like responses", "content": null}, {"title": "System prompt for generating formal, impersonal responses", "content": null}, {"title": "System prompts for generating questions", "content": null}, {"title": "Data Examples", "content": null}, {"title": "Generation Examples", "content": null}, {"title": "Generated Pair Example", "content": null}, {"title": "Training Techniques in Detail", "content": null}, {"title": "LORA (Low-Rank Adaptation)", "content": "LORA [12] utilizes low-rank approximations for fine-tuning large-scale models, which reduces computational and\nmemory overhead while preserving the model's structure and mitigating overfitting."}, {"title": "Low-Rank Approximation", "content": "A matrix $W \\in \\mathbb{R}^{d\\times k}$ with rank $r$ can be approximated by $A \\in \\mathbb{R}^{r\\times k}$ and\n$B \\in \\mathbb{R}^{d\\times r}$:\n$W \\approx A \\times B$"}, {"title": "Model Fine-Tuning", "content": "LORA introduces a low-rank update AW:\n$W' = W + \\Delta W$ with $\\Delta W = A \\times B$\nThe output for input x is:\n$W\\cdot x + \\Delta W \\cdot x = W\\cdot x + B A x$\nDuring fine-tuning, only A and B are updated."}, {"title": "DPO (Direct Preference Optimization)", "content": "Direct Preference Optimization (DPO) [17] is a method aimed at optimizing a model's preferences directly based on\npreferences. It utilizes the preferences among alternative outputs in a specific situation.\n\u2022 Reference Policy $\\pi_{ref}(y|x)$: Represents the probability distribution of the output $y$ given the state $x$.\n\u2022 Reward Function $r(x, y)$: Measures the level of reward for a given state $x$ and the output $y$. It is defined as:\n$Z(x) = \\sum_{y} \\pi_{ref}(y|x) \\exp(\\frac{1}{\\beta}r(x,y))$\n$r(x, y) = \\beta \\log \\frac{\\pi_{r}(y|x)}{\\pi_{ref}(y|x)} + Blog (Z(x))$\nwhere $\u03b2$ is the temperature parameter that adjusts the impact of the reward function on the outputs.\n\u2022 Optimization Objective: The goal is to optimize the following equation to align the model's outputs with\nhuman preferences:\n$\\pi_{\\gamma}(y|x) = \\frac{\\pi_{ref}(y|x) \\exp (\\frac{1}{\\beta}r(x,y))}{Z(x)}$\nThis equation aims to ensure that the learned model provides outputs that align with human preferences. The\nmodel optimizes the objective by minimizing the corresponding loss function, thereby improving performance\nin a manner consistent with human judgment."}]}