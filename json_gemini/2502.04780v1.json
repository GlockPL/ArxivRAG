{"title": "Sirius: Self-improving Multi-agent Systems via Bootstrapped Reasoning", "authors": ["Wanjia Zhao", "Mert Yuksekgonul", "Shirley Wu", "James Zou"], "abstract": "Multi-agent AI systems powered by large language models (LLMs) are increasingly applied to solve complex tasks. However, these systems often rely on fragile, manually designed prompts and heuristics, making optimization difficult. A key challenge in optimizing multi-agent systems is acquiring suitable training data for specialized agents. We introduce SIRIUS, a self-improving, reasoning-driven optimization framework for multi-agent systems. Central to our approach is the construction of an experience library: a repository of high-quality reasoning trajectories. The library is built by retaining reasoning steps that lead to successful outcomes, providing a robust training set for optimizing multi-agent system. Additionally, we introduce a library augmentation procedure that refines unsuccessful trajectories, further enriching the library. SIRIUS boosts performance by 2.86% to 21.88% on reasoning and biomedical QA and enhances agent negotiation in competitive settings. Our results show that SIRIUS enhances multi-agent performance while generating reusable data for self-correction and self-play enhancement in the future.", "sections": [{"title": "1. Introduction", "content": "Multi-agent AI systems powered by large language models (LLMs), where specialized agents collaborate to solve complex tasks, are becoming increasingly successful in real-world applications. Recent work has demonstrated their effectiveness in complex reasoning (Wang et al., 2024a; Smit et al., 2024), coding (Wu et al., 2023), drug discovery (Swanson et al., 2024) and ensuring safety via debate (Chern et al., 2024; Irving et al., 2018). These successes arise from specialized agents integrating their distinct capabilities through structured interactions, enabling more effective problem-solving than single agents. Moreover, multi-agent scrutiny acts as a built-in self-correction mechanism, where agents refine and verify each other's outputs. This often outperforms single agent setting, particularly on tasks demanding rigorous reasoning or factual validation.\nDespite these successes, optimizing multi-agent systems remains a fundamental challenge due to (1) the difficulty of acquiring appropriate training signals for each agent and (2) the sensitivity to multiple moving parts that influence overall performance (Smit et al., 2024). While task-level reward feedback is available, credit assignment across agents remains ambiguous-it is unclear how to attribute success or failure to specific intermediate decisions and reasoning steps made by each LLM agent. This challenge parallels the multi-agent credit assignment problem in reinforcement learning (Foerster et al., 2018). However, in language-based systems, reasoning unfolds through complex and unstructured interactions, making attribution far more difficult than in traditional RL settings with well-defined action spaces.\nWe present SIRIUS, a framework for learning effective multi-agent behaviors from outcome rewards. Our key insight is that when multiple agents successfully solve a task together, their entire interaction trajectory likely contains useful patterns even if we cannot pinpoint exactly which steps or decisions were crucial for success. Drawing inspiration from recent advances in bootstrapping reasoning capabilities (Zelikman et al., 2022), we collect and learn from successful agent interactions across many tasks, allowing the system to iteratively discover effective collaboration strategies from self-generated data. This approach sidesteps the need for direct supervision of intermediate steps, instead letting agents learn which interaction patterns tend to lead to successful outcomes. For trajectories that result in failed attempts, we perform trajectory augmentation by resampling original attempts with feedback from an additional agent grounded in the ground truth.\nOur experiments demonstrate that SIRIUS significantly enhances multi-agent performance across multiple domains. It improves reasoning and biomedical QA accuracy by 2.86% to 21.88%, while also strengthening agent negotiation in competitive scenarios. Beyond these gains, our approach offers a scalable mechanism for self-improvement, enabling agents to iteratively refine their reasoning and collaboration strategies. More broadly, SIRIUS provides a general framework for optimizing multi-agent systems via self-generated synthetic data, offering a principled way to enhance performance without requiring fine-grained human supervision."}, {"title": "2. Method", "content": "2.1. Multi-agent systems with LLMs\nWe define a multi-agent system by a tuple (S,A,T,R,N,G). Here, N = {A(1), A(2), ..., A(\u00d1)} is the set of N agents, each agent A(i) uses a policy \u03c0i parameterized by )(i). s \u2208 S is the state of the environment, a \u2208 A is the joint actions, and A is the joint action space. T:S\u00d7A\u2192 S is the transition function where T(s, a) yields the next state of the environment given the current state and joint actions a. The environment feedback is modeled via a payoff function Ri : S \u00d7 A \u2192 RN, which provides rewards for each agent k based on the state-action pairs.\nThe communication structure between agents is modeled as a directed graph G = (V, E, P), where V represents agents, and E defines interaction order.\nFor each edge (i, j) \u2208 E, agent A(i) receives an input derived from the state-action pair (s, a) and the output of agent A(i). This input determines agent A(i)'s subsequent action. For each agent A(i) in a topological graph G, its predecessors are the set of agents that influence its output: Pre(A(i)) = {A(j) | (A(j), A(i)) \u2208 G}. Here, (A(i), A(i)) denotes a directed edge in the graph, indicating that the output of agent A(i) directly influences the input of agent A(i).\nThroughout this paper, the collection of our agents will be based on language models and the primary environment that we use will be natural language. In particular:\n$a_i \\sim \\pi_i(\\cdot|s_t, \\{a_j\\}_{A(i)\\in Pre(A(i))}) \\forall A(i) \\in N$\n$a_t = (a_1, ..., a_N)$\n$s_{t+1} = T(s_t, a_t) = Concat(s_t, a_t)$", "latex": ["a_i \\sim \\pi_i(\\cdot|s_t, \\{a_j\\}_{A(i)\\in Pre(A(i))}) \\forall A(i) \\in N", "a_t = (a_1, ..., a_N)", "s_{t+1} = T(s_t, a_t) = Concat(s_t, a_t)"]}, {"title": "2.2. SIRIUS", "content": "The training pipeline of the proposed framework, denoted as SIRIUS, is illustrated in Figure 1. SIRIUS adopts a fine-tuning strategy to iteratively improve the policy parameters ((n) of each agent A(n) over T iterations. The process is initialized with a dataset D = {(xi, yi)}P=1, where each pair (xi, Yi) represents a problem and its solution. The core training procedure is outlined in Algorithm 1.\nAt each fine-tuning iteration t:\n\u2022 Action Sampling: For each agent A(n), an action an) is sampled from its policy,\n$a^{(n)}_t = P_{\\theta^{(n)}}(x_i, a_{pre(A(n))}),$", "latex": ["a^{(n)}_t = P_{\\theta^{(n)}}(x_i, a_{pre(A(n))}),"]}, {"title": "3. Multi-agent Settings", "content": "In this section, we explore several settings where agents with distinct expertise interact to solve challenging tasks. As shown in Table 1, we systematically analyze different agent configurations.\n3.1. Problem Solving Settings\nAgents with Specific Expertise. In this setting, each agent is assigned a domain-specific role to facilitate a structured and efficient problem-solving process. For instance, in the physics and chemistry domains, the problem-solving pipeline begins with a domain expert (e.g., a physicist or chemist) who analyzes the domain-specific problem, followed by a mathematician who formalizes the reasoning with quantitative models, and finally, a summarizer who consolidates the insights into a clear and comprehensive answer. This sequential collaboration ensures that the expertise of each agent is leveraged effectively while maintaining clarity in the solution process.\nThe sequential dependency between the agents can be described as follows:\n$a_{phy} \\sim \\pi_{Phy}(q),$\n$a_{Math} \\sim \\pi_{Math}(q, a_{Phy}),$\n$a_{sum} \\sim \\pi_{Sum}(q, a_{Phy}, a_{Math}),$", "latex": ["a_{phy} \\sim \\pi_{Phy}(q),", "a_{Math} \\sim \\pi_{Math}(q, a_{Phy}),", "a_{sum} \\sim \\pi_{Sum}(q, a_{Phy}, a_{Math}),"]}, {"title": "3.2. Actor-Critic Setting", "content": "The popular Actor-Critic framework facilitates iterative agent improvement through a feedback loop: the Actor Agent generates solutions while the critic evaluates and refines them, enhancing both the Actor Agent's reasoning and the Critic Agent's error correction capabilities. In practice, we separate judgment and feedback tasks by introducing a Judgment Agent alongside the Critic Agent, where the Judgment Agent classifies the Actor Agent's solutions as correct or incorrect, and for incorrect solutions, the critic provides feedback to guide the Actor Agent in regenerating improved solutions. Reward mechanisms are designed as: the Actor Agent receives rewards for correct solutions, the Judgment Agent for accurate classifications, and the critic for providing actionable feedback that leads to correct regenerations."}, {"title": "3.3. Competitive Settings", "content": "Competitive scenarios (Bianchi et al., 2024) examine multi-agent interactions under opposing objectives, where agents must balance cooperation and competition to achieve their goals. In this framework, two agent roles are defined: Player 1 and Player 2. Each player is initialized with a specific amount of resources, which evolve over the course of the game based on their interactions. The game progresses as a sequence of moves, resulting in a trajectory of states:\nPlayer 1 Trajectory: $x_0^{player1}, x_1^{player1}, x_1^{player2}, ..., x_T^{player1}$\nPlayer 2 Trajectory: $x_0^{player2}, x_1^{player1}, x_1^{player2}, ..., x_T^{player2}$\nThe sequence captures the evolution of game states as players compete at each timestept = 0,1,..., T, ultimately determining a winner and a loser. Our goal is to optimize each player's policy to maximize its own expected reward based on trajectory data and role-specific context. This can be formulated as:\n$max \\sum_{i=1}^{T} P(x_i^{player1} | x_i^{player1}, x_i^{player2})$\nwhere Player 1 optimizes its policy based on the historical trajectory of both itself and Player 2, and similarly for Player 2.\nWe explore three distinct competitive settings, all of which unfold over multiple rounds:\nResource Exchange Scenario. In this scenario, agents engage in a simulated environment where they exchange resources to maximize their individual utility.\nSeller and Buyer Scenario. This setting models economic interactions where one agent assumes the role of a seller and another the role of a buyer. The agents negotiate prices and terms to complete transactions, testing their ability to strategize under asymmetric setting.\nMulti-Turn Ultimatum Game. The Multi-Turn Ultimatum Game explores scenarios of fairness, cooperation, and negotiation over multiple rounds. One agent proposes a division of a resource, and the other agent decides whether to accept or reject it.", "latex": ["x_0^{player1}, x_1^{player1}, x_1^{player2}, ..., x_T^{player1}", "x_0^{player2}, x_1^{player1}, x_1^{player2}, ..., x_T^{player2}", "max \\sum_{i=1}^{T} P(x_i^{player1} | x_i^{player1}, x_i^{player2})"]}, {"title": "4. Experiments", "content": "4.1. Baseline\nWe compare our SIRIUS against the following baselines:\nSingle-Agent utilizes a single language model to process input and generate responses.\nSTaR (Zelikman et al., 2022), the Self-Taught Reasoner, focuses on enhancing the reasoning capabilities of a single agent by iteratively training it to improve its step-by-step reasoning through self-supervised fine-tuning.\nPrompt Multi-Agent System (CoMM) (Chen et al., 2024a) introduces a training-free, multi-agent collaborative framework where agents interact and share information to solve tasks collectively.\nTextGrad (Yuksekgonul et al., 2024) optimizes prompts for each agent in a multi-agent system by backpropagating natural language feedback through each interaction.\n4.2. Setup and Datasets\nBackbone Model. For a fair comparison, we use gpt-3.5-turbo-0125 and gpt-40-mini-2024-07-18 as the backbone model, and set the temperature to 0 in all our experiments. We use OpenAI's Fine-tuning API for supervised fine-tuning.\nCollege Physics/Chemistry. These two datasets are constructed by combining questions from Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), Graduate-Level Google-Proof Q&A (GPQA) (Rein et al., 2023), and Theorem-Driven Question Answering (TheoremQA) (Chen et al., 2023). It focuses on college-level physics problems, which remain difficult and demonstrate room for improvement in performance with large language models. We split the dataset into training and test sets, with the detailed data distribution provided in Appendix C.\nPubMedQA. This is a biomedical question-answering dataset comprising 1000 open-domain questions (Jin et al., 2019), each paired with context from PubMed abstracts and corresponding answers. It focuses on research-driven queries, requiring domain-specific understanding and reasoning over scientific texts. We follow the original split of the dataset for training (500) and testing (500) sets.\n4.3. Experimental Result of Problem Solving Setting\n4.3.1. MAIN RESULT\nTable 3 presents a performance comparison of various models and methods under the Problem Solving Setting. We observe that the prompted Multi-Agent System (COMM) generally improves performance, as agent collaboration enhances the ability to solve complex problems. STaR outperforms the base Single-Agent, indicating that fine-tuning contributes to improved performance. For smaller and weaker models, and in scenarios with long context lengths such as PubMedQA, TextGrad faces significant challenges in instruction-following during optimization. TextGrad (GPT-3.5-turbo) could not be applied to PubMedQA as its optimizer failed to parse instructions due to the model's limited capability and the excessive context length of the problem. Similarly, TextGrad (GPT-40-mini) struggles to generate answers in the required format, requiring manual extraction of answers. Our proposed method, SIRIUS, consistently outperforms across all tasks. By decomposing tasks into manageable sub-tasks assigned to agents and, crucially, fine-tuning each agent to specialize in its designated task, SIRIUS maximizes the effectiveness of collaboration, ensuring a more coordinated and efficient overall performance.\n4.3.2. ABLATION EXPERIMENTS\nTo evaluate the contributions of various components in SIRIUS, we conducted a series of ablation experiments. Each experiment was designed to answer a key question about the effectiveness of the multi-agent system. All ablations were performed on representative tasks within the Problem Solving Setting (PubMedQA) to ensure consistency in evaluation as shown in Table 4.\nDoes mixing SIRIUS with a base agent degrade performance? To understand the benefits of a jointly optimizing a collaborative multi-agent system, we first train all the agents together using SIRIUS. Then we replaced one SIRIUS agent with the original base agent-either SIRIUS Analyst + base Solver or base Analyst + SIRIUS Solver. This substitution hurts performance, demonstrating benefits from joint multi-agent optimization compared to optimizing a single agent.\nShould we fine-tune different LLMs for different roles, or optimize one LLM for all roles? We explored whether a single LLM fine-tuned on the combined training data of multiple roles could match the performance of separate role-specific models. The results showed a notable performance decline, highlighting that different roles require specialized adaptation and that a shared model struggles to effectively generalize across distinct agent functions.\nHow useful is experience augmentation? To assess the impact of experience augmentation, we removed the augmentation module while keeping the rest of the pipeline unchanged. Data augmentation introduces more diverse and challenging experiences as training data, enhancing the model's capability; therefore, omitting the augmentation module could negatively impact performance.\nDoes additional fine-tuning improve performance? We investigated whether increasing the number of fine-tuning iterations leads to further performance gains. Each iteration follows the full optimization pipeline illustrated in Figure 1, the previously fine-tuned SIRIUS is used to generate a new experience library, which is then used to further fine-tune the base model. As expected, an additional iteration yielded marginal performance gains, suggesting that the model can benefit from extended training."}, {"title": "4.4. Experimental Result of Actor-Critic Setting", "content": "Table 5 presents a performance comparison of various models, methods, and ablations under the Actor-Critic Setting on PubMedQA. As mentioned in Section 3.2, the Actor Agent first generates a solution, which is then evaluated by the Judgment Agent to determine its correctness. For solutions deemed incorrect by the Judgment Agent, the Critic Agent analyzes the original solution and provides feedback without access to the correct answer. The Actor Agent then regenerates the solution based on this feedback.\nA key challenge in this setting is the Judgment Agent's limited ability to differentiate between correct and incorrect solutions leading to two potential issues: (1) correct solutions may be mistakenly judged as incorrect and potentially modified into incorrect ones during the feedback and regeneration stages; (2) incorrect solutions may be judged as correct, failing to receive the necessary corrections. We report TP (True Positive) Accuracy as the ratio of solutions both correctly generated by the Actor and accurately validated by the Judgment Agent, while Overall Accuracy measures the total correct solutions after regeneration, accounting for the combined contributions of all agents.\nWe evaluate our method against two representative baselines: (1) Self-Correct, where Actor-generated solutions are refined through direct feedback-guided regeneration, and (2) Prompt, which exclusively employs prompting strategies to coordinate Actor-Judgment-Critic interactions without optimization mechanisms. A critical limitation observed in the Self-Correct framework is its significantly lower TP accuracy. This issue arises from its feedback mechanism, which modifies all generated responses with high probability, potentially leading to erroneous modifications of the initially correct solution. This is a common issue with using out-of-the-box LLMs for self-correction with no specialized training (Kumar et al., 2024).\nComparing GPT-3.5-Turbo and GPT-40-mini, we also find that GPT-3.5-Turbo struggles more with misjudging correct answers as incorrect, leading to a severe drop in TP Accuracy. Our method, SIRIUS, achieves a notable improvement in TP Accuracy, highlighting the Judgment Agent's enhanced ability to assess whether a response requires modification. The overall higher accuracy underscores the effectiveness of SIRIUS's framework, where fine-tuning enhances each agent's task-specific capabilities, and the collaboration of Judgment, Critic, and Actor Agents ensures appropriate revision of incorrect responses while minimizing unnecessary changes to correct answers.\nThe ablation study further underscores the contribution of each agent in SIRIUS. Fine-tuning only a single base LLM leads to a performance drop, highlighting the necessity of specialized agent roles and joint optimization. Notably,"}, {"title": "4.5. Experimental Result of Competitive Settings", "content": "To analyze the effect of training in the competitive setting, we study the performance of agents in scenarios where one player initially had a higher probability of winning, referred to as the \"winning player,\" while the other player was at a disadvantage, called the \"losing player.\" In general, when SIRIUS took on the role of the winning player competing against a base agent, it demonstrated an increased win rate and payoff. Additionally, when SIRIUS played the role of the losing player, it experienced fewer losses. Similarly, for both GPT-3.5 and GPT-40-mini when they compete with each other, SIRIUS-GPT-3.5 and SIRIUS-GPT-40-mini both demonstrate improved performance."}, {"title": "5. Related Work", "content": "Enhancing Reasoning in Single-Agent Systems. Building on the reasoning capabilities of state-of-the-art foundation models (Schulman et al., 2022; OpenAI, 2023; Liu et al., 2024), recent research explores approaches beyond scaling model parameters. Chain-of-Thought (Wei et al., 2022) enhances reasoning through step-by-step inference, while Tree of Thoughts (Yao et al., 2024), Graph of Thought (Besta et al., 2024), and Program of Thoughts (Chen et al., 2022) structure reasoning as tree searches with backtracking. Reasoning with Planning (RAP) (Hao et al., 2023) incorporates explicit planning, and Reflexion (Shinn et al., 2024) enables self-evaluation and refinement. (Wu et al.) introduce contrastive reasoning for instruction generation, while TextGrad (Yuksekgonul et al., 2024) applies gradient-based optimization to refine outputs. These methods enhance reasoning through structured decomposition, search, and planning.\nSelf-improvement. Self-improving models (Huang et al., 2022; Yu et al., 2023; Yuan et al., 2024; Zhang et al., 2024; Welleck et al., 2022; Peng et al., 2024) have garnered increasing attention for their potential to enhance reasoning capabilities through iterative feedback and refinement. Several studies (Zelikman et al., 2022; Li et al., 2024a; Pang et al., 2024; Lee et al., 2024)employ bootstrapping strategies by leveraging self-generated rationales, while others (Yuan et al., 2024; Chen et al., 2024c; Ramji et al., 2024; Guo et al., 2025) introduce a self-refinement mechanism through reinforcement learning.\nMulti-Agent Systems with LLMs. Multi-Agent Systems with LLMs. Recent advancements in multi-agent systems (Smit et al., 2024; de Zarz\u00e0 et al., 2023; Guo et al., 2024; Li et al., 2024b; Han et al., 2024; Wang et al., 2024b; Sun et al., 2024) highlight the potential of large language models in tackling complex tasks. Society of Minds (Du et al., 2023) enables agents to exchange answers, fostering collaboration. Mixture-of-Agents (Wang et al., 2024a) employs a layered architecture where agents refine responses based on prior outputs. CoMM (Chen et al., 2024a) enhances problem-solving through structured communication and role division. Multi-Persona (Liang et al., 2023) encourages diverse agent behaviors by assigning distinct personas. ChatEval (Chan et al., 2023) explores different multi-agent debate strategies for interaction and response management. DMAS (Chen et al., 2024b) explores token-efficient multi-agent planning frameworks to improve coordination and task success.Building on advances in multi-agent systems, recent work has explored fine-tuning with independently specialized agents that interact to generate diverse reasoning chains (Subramaniam et al., 2025). Unlike these approaches, our method prioritizes collaborative optimization through a shared experience library, enabling agents to collectively learn from and refine successful reasoning trajectories."}, {"title": "6. Conclusions", "content": "We introduced SIRIUS, a framework for optimizing multi-agent LLM systems by learning from successful interactions and augmenting failed trajectories with feedback. Our approach enables agents to refine collaboration strategies without explicit supervision. Experiments show that SIRIUS significantly improves performance across college-level reasoning, biomedical QA, and negotiation tasks. More broadly, our work provides a scalable mechanism for multi-agent self-improvement, offering a principled approach to optimizing collaborative AI systems."}, {"title": "A. Detailed Pipeline", "content": "Given the wrong answer problem set W = {(xi, Yi)}=1,In each iteration, we first select the agent to be optimized. For instance, as shown in the diagram, the selected agent is the physicist (A). The external agent provides feedback fi = P(ext) (xi, \u00e2i, Yi) based on the question xi, the original response \u00e2i, and the correct answer yi.\nThe physicist then regenerates the solution by incorporating the feedback: a = P(A) (\u00b7|xi, Yi, fi).\nTo ensure clarity and coherence, the regenerated response \u00e2 is subsequently rephrased to produce \u00ee\u011dinal, making it appear as if derived directly through problem-solving without mentioning any modifications or feedback. This updated response is then used in subsequent collaborations with other agents to refine the overall solution further."}, {"title": "B. Detailed Competitive Settings", "content": "We follow the settings of NEGOTIATIONARENA Platform (Bianchi et al., 2024).\nB.1. Resource Exchange Scenario\nIn this game, each agent has access to a set of resources and a goal. For example, an agent has access to resources 25 Xs and 5 Ys. The agent might have the goal of maximizing its total resources. Since this goal is very general, it could bring the models to employ different strategies (e.g., a model might want to diversify the resources it has or maximize only an"}, {"title": "B.2. Multi-Turn Ultimatum Game", "content": "The Ultimatum game (Sanfey et al., 2003) is a classical game used in economics to study aspects of human behavior, such as fairness and rationality. It involves two agents agreeing on a split of resources (often money). One agent is given all the game's resources and proposes a split of the resources. The second agent can either accept or reject the proposal, which means both agents lose all resources. In the classical Ultimatum game the rational actions correspond to (1) the first agent offering to give 1 unit of resource (i.e., the bare minimum) and (2) the second agent accepting any proposal that is greater than 0 units. The classical Ultimatum game has one round of negotiation (i.e. agent 2 can only decide whether or not to accept agent 1's first offer). In our version of the game, the game can go on for more turns (e.g. agents can make multiple counteroffers) and both players can accept the opponent's offer."}, {"title": "B.3. Seller and Buyer Scenario", "content": "We introduce a seller and buyer game involving two agents, one looking to sell a set of resources and one looking to buy them, similar to other approaches in the literature (e.g., (He et al., 2018)). We imbue agents with some beliefs about the object being sold, but unlike the ultimatum game, the seller and buyer game is an incomplete information game, i.e., players do not have complete information about other players (e.g., their beliefs). Only the seller is aware of the production cost of the object, and only the buyer is assigned and is aware of their willingness to pay for the object. Given these beliefs, the seller and the buyer are prompted to sell and buy the object, respectively. The seller starts first: reproducing a scenario in which the object is already on sale."}, {"title": "C. Dataset Details", "content": "C.1. Dataset Split Statistics\nIn this work, we use three datasets for evaluating the performance of our model: Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), Graduate-Level Google-Proof Q&A (GPQA) (Rein et al., 2023), and Theorem-Driven Question Answering (TheoremQA) (Chen et al., 2023). These datasets contain a variety of question types, with a focus on college-level physics and chemistry problems that remain difficult and present room for improvement in performance with large language models.\nThe dataset was split into training and test sets with a 2:1 ratio, and the data distribution for each dataset is shown in Table 6.\nC.2. Finetuning Dataset Statistics\nFor each experiment, we specify the Trajectories Augmentation Ratio and whether ground truth answers are used during the training process. We summarize the setup for each experiment in Table 7."}, {"title": "D. Additional Experiment Result", "content": "In this section, we present additional experiments conducted in a competitive setting to assess the generalization of SIRIUS. These results demonstrate the adaptability of SIRIUS across various configurations."}, {"title": "E. Agent Prompts", "content": "E.1. Problem Solving Setting\nPrompts for College-Physics Task\nSystem_prompt = \"\"You are part of a team with multiple experts from different disciplines. Your team aims to solve a given cross-discipline problem collectively.\nThe team is composed of three experts:\n1. The Physicist\nRole Definition: You are a physicist with a specialization in the field of college-level physics. Your vast knowledge covers multiple aspects of physics including classical mechanics, thermodynamics, electromagnetism, quantum mechanics, and statistical physics. You understand these topics in depth and have the ability to explain them in a way that is easily comprehensible to those less familiar with them.\nResponsibility: Focus on contributing physics-specific insights and collaborate with the mathematician to help develop and validate mathematical models.**Do not perform calculations or solve the entire problem**. Your goal is to provide a clear explanation of the physics, leaving calculations to the mathematician.\nPrinciples: Emphasize empirical, systematic, and data-driven approaches while fostering curiosity, innovation, and ethical scientific practices.\n2. The Mathematician\nRole Definition: You are a mathematician, specializing in the broad and complex field of mathematics at the college level. Your expertise ranges from pure mathematical theory, including algebra, calculus, geometry, number theory, and statistics, to applied mathematics such as optimization and probability theory. You have an innate ability to abstract and generalize problems, solving them with elegance and precision. You excel at creating mathematical models that represent real-world situations and can interpret the implications of those models. You are not only well-versed in complex equations and proofs, but also experienced in conveying these concepts to others through teaching.\nResponsibilities: Apply mathematical reasoning to analyze and address complex, cross-disciplinary problems; Collaborate with the physicist to refine mathematical models and validate their conclusions; Convey mathematical insights in a clear manner to facilitate team decision making.\nPrinciples: Foster a culture of analytical thinking and evidence-based decisions; Encourage an atmosphere of curiosity, innovation, and continuous learning; Maintain high mathematical integrity and respect for varying perspectives.\n3. The Final Answer Synthesizer\nRole Definition: You are the Final Answer Synthesizer, an integrative role in the team responsible for coalescing the insights provided by the experts. With a clear understanding of the different disciplines, you effectively distill the responses from the physicist and the mathematician into a coherent, final solution. Your role involves keenly interpreting expert input, synthesizing various problem-solving approaches, and presenting a clear, well-rounded answer that incorporates the collective wisdom of the team.\nResponsibility: summarize the solutions; give a final answer.\nPrinciples: make sure to give a specific answer to the given task.\u201d\nPhysicist_prompt = \"\"Your role is the physicist. Here is the given problem: \"question\" Your task is **only to explain** the relevant physics concepts and principles that apply to this problem. \u201d\nMathematician_prompt = \"\"Your role is the mathematician. Here is the given problem: \"question\" Here is the response from the physicist: \"{agent_1_response}\" Please give your opinion on how to solve the problem in consideration of the response from the physicist.\"\nSummarizer_prompt = \"\"Your role is the Final Answer Synthesizer. Here is the given problem: \"question\" Here is the response from the physicist: \"{agent_1_response}\" Here is the response from the mathematician: \"{agent_2_response}\"\nPlease provide a final answer to the given problem. {format_prompt}\"\""}, {"title": "Prompts for College-Chemistry Task", "content": "System_prompt = \"\"You are part of a team with multiple experts from different disciplines. Your team aims to solve a given cross-discipline problem collectively.\nThe team is composed of three experts:\n1. The Chemist\nRole Definition: You are a chemist with a specialization in the field of college-level chemistry. Your vast knowledge covers multiple aspects of chemistry including organic, inorganic, physical, analytical, and biochemistry. You understand these topics in depth and have the ability to explain them in a way that is easily comprehensible to those less familiar with them.\nResponsibility: Focus on contributing chemistry-specific insights and collaborate with the mathematician to help develop and validate mathematical models.**Do not perform calculations or solve the entire problem**. Your goal is to provide a clear explanation of the chemistry concepts, leaving calculations to the mathematician.\nPrinciples: Emphasize empirical, systematic, and data-driven approaches while fostering curiosity, innovation, and ethical scientific practices.\n2. The Mathematician\nRole Definition: You are a mathematician, specializing in the broad and complex field of mathematics at the college level. Your expertise ranges from pure mathematical theory, including algebra, calculus, geometry, number theory, and statistics, to applied mathematics such as optimization and probability theory. You have an innate ability to abstract and generalize problems, solving them with elegance and precision. You excel at creating mathematical models that represent real-world situations and can interpret the implications of those models. You are not only well-versed in complex equations and proofs, but also experienced in conveying these concepts to others through teaching.\nResponsibilities: Apply mathematical reasoning to analyze and address complex, cross-disciplinary problems; Collaborate with the chemist to refine mathematical models and validate their conclusions; Convey mathematical insights in a clear manner to facilitate team decision making.\nPrinciples: Foster a culture of analytical thinking and evidence-based decisions; Encourage an atmosphere of curiosity, innovation, and continuous learning; Maintain high mathematical integrity and respect for varying perspectives.\n3. The Final Answer Synthesizer\nRole Definition: You are the Final Answer Synthesizer, an integrative role in the team responsible for coalescing the insights provided by the experts. With a clear understanding of the different disciplines, you effectively distill the responses from the chemist and the mathematician into a coherent, final solution. Your role involves keenly interpreting expert input, synthesizing various problem-solving approaches, and presenting a clear, well-rounded answer that incorporates the collective wisdom of the team.\nResponsibility: Summarize the solutions; give a final answer.\nPrinciples: Make sure to give a specific answer to the given task.\u201d\nChemist_prompt = \"\"Your role is the chemist. Here is the given problem: \"question\" Your task is **only to explain** the relevant chemistry concepts and principles that apply to this problem. **Do not** perform any calculations or try to find the final solution. Your role is to explain the chemical reasoning, such as reactions or principles, but refrain from solving the equations or completing the solution. Leave the mathematical work to the mathematician.\"\nMathematician_prompt = \"\"Your role is the mathematician. Here is the given problem: \"question\" Here is the response from the physicist: \"{agent_1_response}\" Please give your opinion on how to solve the problem in consideration of the response from the physicist.\u201d\"\nSummarizer_prompt = \"Your role is the Final Answer Synthesizer. Here is the given problem: \"question\" Here is the response from the physicist: \"{agent_1_response}\" Here is the response from the mathematician: \"{agent_2_response}\"\nPlease provide a final answer to the given problem. {format_prompt}\u201d\""}, {}]}