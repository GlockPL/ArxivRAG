{"title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan", "David Skarbrevik", "Viren Bajaj", "Zeya Ahmad"], "abstract": "Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce langfair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.", "sections": [{"title": "1. Introduction", "content": "Traditional machine learning (ML) fairness toolkits like AIF360 (Bellamy et al., 2018), Fairlearn (Weerts et al., 2023), Aequitas (Saleiro et al., 2018) and others (Vasudevan and Kenthapadi, 2020; Wexler et al., 2019; Tensorflow, 2020) have laid crucial groundwork. These toolkits offer various metrics and algorithms that focus on assessing and mitigating bias and fairness through different stages of the ML lifecycle. While the fairness assessments offered by these toolkits include a wide variety of generic fairness metrics, which can also apply to certain LLM use cases, they are not tailored to the generative and context-dependent nature of LLMs.\nLLMs are used in systems that solve tasks such as recommendation, classification, text generation, and summarization. In practice, these systems try to restrict the responses of the LLM to the task at hand, often by including task-specific instructions in system or user prompts. When the LLM is evaluated without taking the set of task-specific prompts into account, the evaluation metrics are not representative of the system's true performance. Representing the system's actual performance is especially important when evaluating its"}, {"title": "2. Generation of Evaluation Datasets", "content": "The langfair.generator module offers two classes, ResponseGenerator and CounterfactualGenerator, which aim to enable user-friendly construction of evaluation datasets for text generation use cases.\nResponseGenerator class. To streamline generation of evaluation datasets, the ResponseGenerator class wraps an instance of a langchain LLM and leverages asynchronous generation with asyncio. To implement, users simply pass a list of prompts (strings) to the ResponseGenerator.generate_responses method, which returns a dictionary containing prompts, responses, and applicable metadata.\nCounterfactualGenerator class. In the context of LLMs, counterfactual fairness can be assessed by constructing counterfactual input pairs (Gallegos et al., 2024; Bouchard, 2024),"}, {"title": "3. Bias and Fairness Evaluations for Focused Use Cases", "content": "Following Bouchard (2024), evaluation metrics are categorized according to the risks they assess (toxicity, stereotypes, counterfactual unfairness, and allocational harms), as well as the use case task (text generation, classification, and recommendation). Table 1 maps the classes contained in the langfair.metrics module to these risks. These classes are discussed in detail below.\nToxicity Metrics The ToxicityMetrics class facilitates simple computation of toxicity metrics from a user-provided list of LLM responses. These metrics leverage a pre-trained toxicity classifier that maps a text input to a toxicity score ranging from 0 to 1 (Gehman et al., 2020; Liang et al., 2023). For off-the-shelf toxicity classifiers, the ToxicityMetrics class provides four options: two classifiers from the detoxify package, roberta-hate-speech-dynabench-r4-target from the evaluate package, and toxigen available on HuggingFace. For additional flexibility, users can specify an ensemble of the off-the-shelf classifiers offered or provide a custom toxicity classifier object.\nStereotype Metrics To measure stereotypes in LLM responses, the StereotypeMetrics class offers two categories of metrics: metrics based on word cooccurrences and metrics that leverage a pre-trained stereotype classifier. Metrics based on word cooccurrences aim to assess relative cooccurrence of stereotypical words with certain protected attribute words."}, {"title": "4. Semi-Automated Evaluation", "content": "AutoEval class. To streamline assessments for text generation use cases, the AutoEval class conducts a multi-step process (each step is described in detail above) for a comprehensive fairness assessment. Specifically, these steps include metric selection (based on whether FTU is satsified), evaluation dataset generation from user-provided prompts with a user-provided LLM, and computation of applicable fairness metrics. To implement, the user is required to supply a list of prompts and an instance of langchain LLM. Below we provide a basic example demonstrating the execution of AutoEval.evaluate with a gemini-pro instance."}, {"title": "5. Conclusions", "content": "In this paper, we introduced langfair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To aid practitioners, we provide numerous supporting resources, including API documentation, tutorial notebooks, and a technical companion paper (Bouchard, 2024)."}, {"title": "Author Contributions", "content": "Dylan Bouchard was the principal developer and researcher of the LangFair project, responsible for conceptualization, methodology, and software development of the langfair library. Mohit Singh Chauhan was the architect behind the structural design of the langfair library and helped lead the software development efforts. David Skarbrevik was the primary author of LangFair's documentation, helped implement software engineering best practices, and contributed to software development. Viren Bajaj wrote unit tests, contributed to the software development, and helped implement software engineering best practices. Zeya Ahmad contributed to the software development."}]}