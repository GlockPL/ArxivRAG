{"title": "SPIN: SELF-SUPERVISED PROMPT INJECTION", "authors": ["Leon Zhou", "Junfeng Yang", "Chengzhi Mao"], "abstract": "Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. Just by injecting an adaptive defense prompt at inference-time, our method is simple, effective, and compatible with existing safety-aligned models. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.", "sections": [{"title": "INTRODUCTION", "content": "Large-Language Models (LLMs) have achieved great success in code generation (Rozi\u00e8re et al., 2023), question answering (Touvron et al., 2023), and task planning (Sur\u00eds et al., 2023). However, they can be jailbroken by adversarial attacks and prompts (Zou et al., 2023; Zeng et al., 2024; Yuan et al., 2023), leading them to exhibit behaviors of manipulation (Carroll et al., 2023; Ji et al., 2024), deception (Park et al., 2023; Hazell, 2023), and explicit content (Kang et al., 2023). This poses an inherent risk as LLMs are widely deployed in many critical applications, like programming.\nTo align LLMs for safety, prior work focuses on training to make the model more robust. For example, Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) trains reward models based on human ratings, and is used to judge LLM outputs. DPO (Rafailov et al., 2023) trains model on direct preference optimization. To make train-time defenses robust against attacks, it is often required to train the model on the generated attacks, which can take significant resources to gather responses (Casper et al., 2023). In addition, training-time defenses have fundamental difficulties defending against unforeseen attacks and jailbroken prompt injection, due to a mismatched generalization between the smaller safety training data and the broader pre-training data (Wei et al., 2023).\nIn this paper we introduce a method to defend against these attacks by self-supervised prompt injection, dubbed SPIN allowing us to achieve a defense that adapts to each attack online. Just as an attacker finds the right prompt injection to break the LLM, our approach will find the right prompt injection to detect and repair the input. \nOur key insight is that the prompts which override the natural guidelines of the LLM also degrade other capabilities of the model. By constructing various self-supervised tasks, we can detect attacks without needing to know the true labels of the input requests or examples. Our constructed self-supervised tasks also allow us to repair attacked user prompts that slip through detection. This is achieved by adding tokens to the start of the input and taking gradient steps to reduce the perplexity of the resulting string. By using self-supervised prompt injection at test time, we can defend against unseen attacks and even adaptive attacks given the online adaptive property. Figure 2 shows the results of the different tasks rejecting malicious attacks while letting benign requests through.\nOur method is model agnostic and can be compatible with existing defense systems. Our self-supervised criteria do not depend on malicious or benign labels, thus can be used online at inference time on demand.\nVisualizations, experiments, and qualitative results demonstrate that our self-supervised prompt injection significantly improves the LLM safety upon existing alignment defenses. On Advbench with Universal Adversarial Triggers (Zou et al., 2023), we are able to reduce Attack Success Rates (ASR) to 12.11% and 0% on Vicuna and Llama-2 respectively. Our method also can detect natural language based jailbreak attacks (Wei et al., 2023), adversarial instructions (Wei et al., 2023), and role-play"}, {"title": "RELATED WORK", "content": "Large-Language Models: LLMs are pre-trained models on a huge corpus of textual data. A mix of these will have open source weights (Touvron et al., 2023), while others are closed off and are only accessible through APIs (Liu et al., 2023b). Studies have shown that various capabilities are learned at the pre-training stage (Zhou et al., 2023). Performance of these models is correlated with parameter size and train time (Hoffmann et al., 2022; Kaplan et al., 2020), however, larger models also exhibit more problematic behavior (Perez et al., 2022; Elazar et al., 2023) and even the fine-tuning of these large models on benign datasets can compromise their safety (Qi et al., 2023).\nAlignment: Alignment is an inbuilt system in recent LLMs. Originally used for text summarization (Stiennon et al., 2022), the main motivation recently has been to align the goals of AI models with human values and morals (Leike et al., 2018; Hendrycks et al., 2022). It can be done through reinforcement learning through human feedback (RLHF) or even through other LLMs as judges (Ziegler et al., 2020; Lee et al., 2023). However, as long as alignment does not remove the harmful behavior entirely, there will exist prompts that can elicit that behavior (Wolf et al., 2023). Wei et al. (2023) showed that two main issues of alignment come from conflicting objectives, and mismatched generalization.\nAdversarial Attacks: Specific modifications to inputs can cause models to output incorrect or malicious responses. This first gained prominence with Szegedy et al. (2014)'s paper attacking image classification models. For vision tasks, these attacks can also be repaired through natural supervision as well (Mao et al., 2021). For Large-language models, research has been done compiling jailbreak prompts that can cause harmful responses (Gehman et al., 2020; Liu et al., 2023a). These refer to user-created prompts, and usually involve a scenario that encourages the LLM to break away from the preset safety guidelines (Wei et al., 2023). Recent developments such as AutoDAN (Liu et al., 2024) and CodeChameleon (Lv et al., 2024) incorporate LLMs to generalize and automatically generate these malicious prompts for a variety of requests.\nUniversal Adversarial Triggers (UATs) (Wallace et al., 2021) has been one method to attack these models which embeds a sequence of tokens into the user prompt that will cause it to bypass alignment and safety guidelines. In particular, gradient-guided search (Guo et al., 2021) has proven remarkably effective in adversarially misleading the model. Zou et al. (2023) show that these attacks are also transferable across models, allowing harmful generation even in closed-box models."}, {"title": "METHOD", "content": "We begin by describing a series of defenses that use self-supervision to defend against both hand-crafted jailbreak attacks and optimization-based adversarial trigger attacks (Zou et al., 2023). We provide two layers of defense, detection and reversal, that complement each other. Both methods rely on self-supervised signals that we construct for language tasks. These signals are shown in Figure 3 being able to accurately distinguish between benign and malicious prompts. Lastly, we consider the scenarios where the attacker can adapt their attack in response to the defense, and empirically show that our methods are still robust."}, {"title": "JAILBROKEN LANGUAGE MODELS", "content": "In a given user interaction, the user inputs the request x, and the chatbot responds with response y. The LLM generates text outputs by predicting the next token, sampling from a probability distribution conditioned on the input such that y ~ F(x).\nFor attacks, the attacker wants to minimize the difference between the response y and their desired goal t. In this case, this would be malicious instructions prefaced by an agreement such as \"Sure, here's how\". To do this, they can add a set of tokens a which causes the model to respond with their desired output. This results in an attack optimization goal of:\na = \\underset{a}{\\text{min }} L_{attack} (F(x + a), t)\nNote here that '+' denotes the concatenation of two strings, and that the attack is appended to the end of the user request. For GCG attack (Zou et al., 2023), this optimization is through gradient descent. For natural language jailbreaks (Wei et al., 2023), this optimization is through human construction."}, {"title": "DETECT JAILBREAKS VIA SELF-SUPERVISION", "content": "We find jailbreaks prompt to bypass the LLM alignment contains different structure than benign input. We find jailbreak input often leave a trace, such as degrades other capabilities of the LLM in order to achieve a successful attack. Motivated by this, we propose to construct tasks that we already know the groundtruth or the expected behavior, so that we can detect jailbreak if we observe a violation of those tasks. While there are many self-supervised task in natural language we can construct, here we show self-supervised tasks as defense measures.\nRepeat: One simple request of LLMs is to ask it to repeat back the input. For most requests, there is no issue at all, and it is only when the input is harmful or it is under attack that the LLM will have trouble doing so. We use Levenshtein Distance \u201clev(a1:i, b1:j)\u201d between the original user request and the newly generated sentence. This distance is then weighted by the average length of the two strings, where s(x) denotes the size of the string, to compensate for differences in request length.\nL_{repeater}(x') = \\frac{2 \\text{ lev}(x', F(x'))}{s(x') + s(F(x'))}\nInterjection: At the end of the user request, another method is to interject with a separate question where we know the correct answer. This utilizes the internal knowledge set of the model, testing if it can answer fluently. We hypothesis that if the LLM has been jailbroken, they also degrade their capability in ansering those questions, like \"What is the capital of France?\" By calculating softmax for the logits for 'Paris' as the next token, it provides the following loss function:\nL_{interject}(x', y) = \\frac{e^{P(y | x')}}{\\Sigma_{v \\in V} e^{P(v | x')}}\nEach of these loss functions then cluster the inputs that are harmful and benign. Figure 4 shows the different loss functions for the repeater and interjection tasks. By placing a threshold T on each of these self-supervised tasks, we are able to detect when a model has its capabilities attacked."}, {"title": "REVERSALS", "content": "As a second layer for defending against adversarial attacks, we can even reverse attacks that slip through detection. To do so, we append additional tokens in front of the request x', and aim to find a series of tokens that will restore the natural alignment of the LLM. This would allow it to work with existing defenses, and further strengthens the models against attacks. Our proposed method, the introduction of defense tokens into the prefix of the user prompt also leaves the core content of the request unchanged. One observation we made with the GCG adversarial triggers (Zou et al., 2023) was that their inclusion would drastically increase the total perplexity of the input. By defining our Lautoreg as the perplexity of the entire user input, we get the following objective:\nd = \\underset{d}{\\text{arg min}} L_{autoreg} (d+ x')\nAfter every p stages of optimization, we run the completion of the new input request \u0177 = F(d+x') and record if the response began with any of the common alignment refusals. To pass through, it must run through all n steps without triggering any of the common denial prefixes. Once the optimal d is found, the final output of the model is \u0177 = F(d+ x')."}, {"title": "TOTAL SYSTEM", "content": "For an attack to be successful, it must pass through all the detection stages, and the generated response will have the defense tokens as the prefix. The multi-layered approach allows each defense system to plug up holes in the others, and adds to the variables that an attack must consider. Due to the compositionality of our self-supervised defense, users can easily remove some tasks to speed up the system."}, {"title": "ADAPTIVE ATTACKER", "content": "Another key advantage of our defense is that it is also robust to adaptive attackers. If the attack ignores our defense strategy, then we will defend it. If the attackers consider our defense strategy and then attack, then they need to optimize to also respect the self-supervised tasks, therefore make their optimization harder due to the additional constraints. Formally, an adaptive attacker will alternate between attacking and defending the same prompt until it converges:\na = \\underset{a}{\\text{arg min}} L_{attack}(F(x+a), t)"}, {"title": null, "content": "d = \\underset{d}{\\text{arg min}} L_{autoreg} (d+ x + a)\nThis iterative process is computationally intensive, and has the equivalent result as a constrained optimization problem. We use the Lagrangian penalty method by adding a new penalty to the score function. For each new defense system, we can include it as a new multi-target optimization the attack must consider. With all the detection layers and reversal implemented, the adaptive attack would have a multi-target optimization problem as follows:\nL_{adapt}(x', t, \\lambda_r, \\lambda_i, \\lambda_s, \\lambda_p) = L_{attack}(x', t) + \\lambda_rL_{repeater}(x') + \\lambda_iL_{interject}(x') + \\lambda_pL_{autoreg}(x')\nBy allocating more optimization budget to satisfying these constraints, the resulting attack is also less efficient. There is a trade-off that the attacker must consider: by optimizing to bypass the detection layers, it also increases the coherence of the input and constraint the input the be benign. If the attacker does not optimize to bypass the detection layers, our defense algorithm will be activated due to the violation of our self-supervised constraints. Thus, adding our self-supervised constraint will result in a lose-lose situation for the attacker, and we will straightly increase the safety."}, {"title": "EXPERIMENTS", "content": null}, {"title": "DATASETS", "content": "Advbench 'harmful behaviors' contains 520 malicious requests, which we then added adversarial triggers by following Zou et al. (2023)'s method of multi-prompt attack.\nTriviaQA (Joshi et al., 2017) contains a series of trivia questions crafted by humans along with an evidence dataset that contains the answer. We evaluate our performance on the wikipedia validation set with one-shot learning and closed-book performance after reversal."}, {"title": "MODELS", "content": "Llama 2-chat is a chatbot released by Meta, finetuned for dialogue and aligned through human feedback (Touvron et al., 2023).\nVicuna-7b is an open-source chatbot released by LMSYS based on finetuning Llama 2 with conversations from ShareGPT (Zheng et al., 2023)."}, {"title": "ATTACKS", "content": "Universal Adversarial Trigger. We implement Greedy Coordinate Gradient-based (GCG) search as done in (Zou et al., 2023). Each malicious request gets appended a suffix, and the suffix gets optimized for up to 500 iterations or until alignment is broken. Then the next request begins with the previous optimized suffix appended. In the case of Vicuna, the suffix injection is able to achieve an Attack Success Rate (ASR) of 100%.\nNatural Language Jailbreak. Jailbreak Chat is a website for users to share handcrafted prompts that can bypass AI alignment. We take the top 5 upvoted jailbreak prompts and pair them each with 30 randomly selected Advbench requests for a total of 150 prompts. These prompts tend to be older attacks, and were published before Llama2 released.\nAdversarial Instructions. These attacks bypass alignment by circumventing the priorities of the LLM as mentioned in Wei et al. (2023). We test the strongest attack combination presented in their paper including prefix injection and refusal suppression, and pair it with 150 Advbench samples.\nMultiple Role-Play. By simulating role playing in conversation, attackers instruct the LLM to act out a role which is able to bypass safety guardrails. This type of attack is similar to 'Do Anyhing Now' (DAN) attacks in (Shen et al., 2024). We pair the 'dev mode v2' (Wei et al., 2023) attack with the same 150 malicious requests from Advbench.\nAutomated Jailbreaks. Code Chameleon (Lv et al., 2024), AutoDAN (Liu et al., 2024), and ICA (Wei et al., 2024), are all automated ways of generating natural language jailbreaks. The attacks commonly have a starting attack template that is gradually modified with the attack query, similar to GCG-search, to bypass defense systems. We pair each attack with 150 malicious requests from Advbench."}, {"title": "RESULTS", "content": "Detection. For our Repeater layer, we inject the string \"Repeat only the following sentences exactly without changes: \" into the model with the entire input surrounded by quotation marks. We use the best threshold of 0.89 to filter out inputs that failed to correctly repeat.\nFor our Interjection layer, we chose the question to be \"But first, answer only what is the capital of France?\". This gets appended to the end of the model, which leaves the system prompt unchanged. On benign cases, the model was able to answer correctly more than 99% of the time, making it a reasonable question to ask. It passes the layer if the generation returns the answer within the first 10 tokens, and in practice, this gave similar results to using the loss threshold.\nThe ROC curves in figure 4 show that there is a clear delineation in loss between malicious and benign requests when asked to repeat back the entire string. With only the malicious requests, the LLM will identify the harmful content and refuse to repeat the sentence. When the adversarial tokens do appear, the repeated string will usually not include the suffix tokens, leading to a higher loss. This is because when predicting the next word, the actual token that follows is too random to be reliably chosen.\nAs for the interjection task, it makes sense that the benign and malicious prompts without the attack have similar losses. This is because the model still has all of its capabilities, and alignment will still work. Only when an attack disturbs the coherency of the input does the loss go up."}, {"title": "DEFENSE AWARE ADAPTIVE ATTACK", "content": "We present the adaptive attack success rates in Figure 6. The same defense is run, but depending on the A penalty chosen, the attacker can optimize the attack for certain layers. However, this usually comes with a lower ASR in other layers. This is because the attacker has less of a budget to tackle the original goal of breaking alignment. Our defense system still reduces the attack success rate by up to 76%, even under an adaptive attacker, proving that the method is resilient."}, {"title": "ABLATION STUDY", "content": "Computational Overhead With the detection layers, on benign inputs the model goes through two iterations of token generation. As shown in Table 1, the interjection task and the repeat task causes inference to run up to 1.2x and 1.3x longer, respectively. Reversal is 4.5x longer than inference when doing 25 steps of optimization, and using less steps can significantly speed up inference, however it is important that attacks do not succeed for the sake of a faster response time. Compared to the cost of running the GCG attack, our reversal is still only 1% of the cost."}, {"title": "CONCLUSION", "content": "In this paper, we show that self-supervised metrics are sufficient in detecting adversarial attacks, and that the attacks are also repairable by targeting perplexity. We introduce an inference time defense system to detect and repair input generation through prompt injection. Our defense works with existing models and alignment, while not requiring any additional training or fine-tuning. This ensures that our defense system can react to new types of attacks that were not present in the alignment training set. The creation of these self-supervised defenses allow for multiple variations that can detect attacks. The layered nature of the method also shows resilience against adaptive attackers."}]}