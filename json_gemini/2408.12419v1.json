{"title": "4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment", "authors": ["Kaihui Cheng", "Ce Liu", "Qingkun Su", "Jun Wang", "Liwei Zhang", "Yining Tang", "Yao Yao", "Siyu Zhu", "Yuan Qi"], "abstract": "Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.", "sections": [{"title": "1 Introduction", "content": "The observation and prediction of protein structures are pivotal in elucidating the complex relationship between protein conformation and function. This understanding drives significant advancements in biological research and pharmaceutical development, while also providing essential guidance for related experimental endeavors and design strategies. The 3D architecture of a protein is intricately encoded within its linear 1D amino acid sequence, which fundamentally dictates the protein's biological functionality. Deciphering the process of protein folding has long posed a formidable challenge within the domain of computational biophysics. Key challenges in protein structure prediction include the accurate identification of suitable templates for protein structures, particularly for sequences lacking closely related templates; the refinement of these templates to closely resemble the native state; the enhancement of force field precision and conformational exploration; as well as the effective management of computational costs associated with predicting protein structures. This is especially pertinent in scenarios involving free modeling, where structures must be generated de novo.\nRecent advancements in deep learning techniques, coupled with the exponential growth of experimental protein structures within the Protein Data Bank (PDB) (Bank 1971) have markedly propelled learning-based structural studies. AlphaFold2 (Jumper et al. 2021) has introduced a groundbreaking approach to predicting 3D protein structures, achieving accuracy comparable to experimental methods. In tandem, RoseTTAFold (Baek et al. 2021) has enhanced predictive capabilities by incorporating a three-track network architecture, resulting in superior accuracy. Concurrently, ESMFold (Rives et al. 2019) and OmegaFold (Wu et al. 2022) capitalize on high-capacity transformer language models trained on evolutionary data to derive unsupervised representations of protein sequences. Moreover, the accessibility of large-scale data repositories has substantially advanced research in protein conformation sampling, which seeks to generate diverse structural conformations. For instance, Distributional Graphformer (DiG) facilitates the prediction of equilibrium distributions in molecular systems, enabling efficient generation of diverse conformations and the estimation of state densities (Zheng et al. 2024). EigenFold (Jing et al. 2023) approaches protein structures as systems of harmonic oscillators, fostering a cascading-resolution generative process along the system's eigenmodes. AlphaFlow (Jing, Berger, and Jaakkola 2023) optimizes single-state predictors through a custom flow matching framework to develop sequence-conditioned generative models of protein architectures. Building on its predecessor, AlphaFold3 (Abramson et al. 2024) utilizes a diffusion network and updated algorithmic architecture to incorporate joint structures across proteins, nucleic acids, small molecules, ions, and modified residues. Furthermore, Str2Str (Lu et al. 2024) introduces an innovative framework for structure-to-structure translation, capable of zero-shot conformation sampling while maintaining roto-translation equivariance. Despite these significant advancements in structural and conformational predictions, the exploration of dynamic protein structures remains underdeveloped. This study aims to address this gap, focusing on the dynamic aspects of protein structures.\nMolecular dynamics (MD) simulations serve as crucial tools in the fields of computational biology, biophysics, and chemistry, providing a comprehensive and dynamic perspective of molecular systems. These simulations generate substantial high-quality data, which can be exploited for data-driven, learning-based methodologies. Nevertheless, the computational expense associated with MD simulations typically scales cubically with the number of electronic degrees of freedom. Moreover, critical biomolecular processes, such as conformational changes, often occur on timescales that surpass the capabilities of classical all-atom MD simulations. In response, deep learning techniques have been employed to address these limitations. Despite these advancements, existing methods are predominantly applicable to proteins with significantly fewer atoms than typical proteins, necessitating the adoption of coarse-grained atomic representations for larger systems. This study aims to leverage extensive, high-quality MD data to generate dynamic structures of proteins comprising up to hundreds of amino acids, including complex structures with complete side-chain representations. Our approach seeks to extend the applicability of MD simulations to larger and more intricate protein systems, thereby enhancing our understanding of their dynamic behaviors.\nThis paper presents an innovative approach to modeling dynamic protein structures utilizing a 4D diffusion model. Our research is concentrated on three primary areas: Firstly, we propose a unified diffusion model designed to predict protein structures that encompass both backbone and side-chain components. By organizing atoms within each residue into rigid groups to minimize the degrees of freedom, our framework efficiently simulates protein motion for structures with hundreds of residues. The amino acid sequence is represented by node and edge features derived from structure prediction models, which guide the diffusion model for precise protein generation. Unlike methods constrained to de novo structure prediction, we incorporate side-chain dihedral angle predictions and introduce an amino acid atomic model to accurately recover individual atomic coordinates based on dihedral angles. Secondly, the initial 3D protein structure is integrated as a condition and encoded through a reference network for latent embedding, thereby incorporating relevant features into the denoising diffusion network. The reference network is instrumental in maintaining structural consistency of proteins during motion. Thirdly, we propose a motion alignment module within the score-based diffusion network, which includes temporal attention layers to aggregate kinetic information from adjacent frames within the diffusion model. This enhancement improves the coherence of motion in generated dynamic proteins, mitigating abrupt transitions during motion. Thus, our diffusion model effectively generates dynamic protein structures across multiple time steps simultaneously, enhancing efficiency and ensuring the prediction of consistent sequences of protein structures within a temporal framework. In summary, our approach enhances the efficacy of dynamic protein structure generation while ensuring the prediction of coherent and temporally consistent sequences.\nIn this investigation, we conducted a comprehensive qualitative and quantitative analysis utilizing widely recognized benchmark datasets, including ATLAS (Vander Meersche et al. 2024) and Fast-Folding (Lindorff-Larsen et al. 2011) protein datasets. Our study successfully achieved dynamic protein structure predictions for sequences of up to 256 amino acids across 32 time steps. This capability enabled us to model dynamic protein conformations sampled at various temporal intervals, demonstrating notable accuracy in capturing both subtle intra-conformational motions and significant inter-conformational changes. The findings of this research represent a significant advancement in the field of dynamic protein structure prediction, contributing valuable insights for future developments in this domain."}, {"title": "2 Related Work", "content": "De Novo Protein Design. The task of de novo protein design (Trippe et al. 2023; Luo et al. 2022; Anand and Achim 2022) involves generating novel proteins based on physical principles, with specified structural and/or functional properties. FoldFlow (Bose et al. 2023) introduces a simulation-free approach for learning deterministic continuous-time dynamics and matching invariant target distributions on SE(3). VFN-Diff (Mao et al. 2024) presents the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thereby enhancing the capability for modeling frames. In recent years, with the rapid development of diffusion-based generative models (Ho, Jain, and Abbeel 2020; Song et al. 2020; Zhu et al. 2024; Xu et al. 2024), these technologies have also been applied to de novo protein design. RFDiffusion (Watson et al. 2023) fine-tunes the RoseTTAFold (Baek et al. 2021) structure prediction network on protein structure denoising tasks, resulting in a generative model of protein backbones based on the diffusion model in the formulation of DDPM (Ho, Jain, and Abbeel 2020; Nichol and Dhariwal 2021). SE(3)-Diff (Yim et al. 2023) establishes the theoretical foundations of SE(3) invariant diffusion models across multiple frames, facilitating the learning of SE(3) equivariant scores over multiple frames using a score-based diffusion model (Song et al. 2020; Song and Ermon 2020). In this paper, we follow the score-based diffusion model (Song et al. 2020; Song and Ermon 2020), extending it not only to protein structure prediction but also to the dynamic motion within the temporal domain.\nPrediction of 3D Protein Structure. Predicting the 3D structure of proteins from their amino acid sequences has long been a significant challenge in biology. Various approaches, including thermodynamic and kinetic simulations and bioinformatics analyses, have been proposed. This paper focuses on deep learning-based methods. An early deep learning effort, Raptor-X (Xu 2019), utilizes a dilated ResNet to predict atom pair distances. Subsequently, tr-Rosetta (Yang et al. 2020) enhances accuracy by predicting inter-residue geometries. AlphaFold2 (Jumper et al. 2021) marks a milestone with its novel attention mechanisms and training procedures, leveraging evolutionary, physical, and geometric constraints to significantly improve accuracy. RoseTTAFold (Baek et al. 2021) further refines network architectures with a three-track network, achieving superior accuracy. Additionally, ESMFold (Rives et al. 2019) and OmegaFold (Wu et al. 2022) employ high-capacity transformer language models trained on evolutionary data in an unsupervised manner to learn protein sequence representations. Recently, AlphaFold3 (Abramson et al. 2024) extends protein structure prediction using a diffusion network and an updated algorithmic architecture, encompassing joint structures of proteins, nucleic acids, small molecules, ions, and modified residues. However, the aforementioned works primarily focus on static structure prediction using diffusion generative models. In contrast, this paper addresses the prediction of dynamic structures over temporal sequences.\nProtein Conformation Sampling. Proteins are dynamic macromolecules, where conformational changes play critical roles in biological processes. To obtain a diverse set of conformations, classical approaches such as MSA subsampling have been employed, which subsample the Multiple Sequence Alignment (MSA) input to AlphaFold2. Recently, diffusion models have emerged for protein conformation generation. Distributional Graphformer (DiG) predicts the equilibrium distribution of molecular systems, enabling efficient generation of diverse conformations and estimation of state densities (Zheng et al. 2024). EigenFold (Jing et al. 2023) models the structure as a system of harmonic oscillators, naturally inducing a cascading-resolution generative process along the eigenmodes of the system. AlphaFlow (Jing, Berger, and Jaakkola 2023) fine-tunes single-state predictors under a custom flow matching framework to obtain sequence-conditioned generative models of protein structure Str2Str (Lu et al. 2024) adopts a novel structure-to-structure translation framework capable of zero-shot conformation sampling with roto-translation equivariant properties. It is important to note that protein conformation sampling predicts the distribution of structures rather than structures within the temporal domain.\nLearning Based Molecular Dynamics. Deep learning has significantly impacted complex atomic systems by reducing the need for time-consuming calculations (No\u00e9 et al. 2020; Merchant et al. 2023; Kearnes et al. 2016; Pfau et al. 2020). Applications include estimating free energy surfaces (Behler and Parrinello 2007), constructing Markov state models of molecular kinetics (Mardt et al. 2017), and generating samples from equilibrium distributions (Jing, Berger, and Jaakkola 2023). Here, we briefly review research on learning kinetics models. VAMPNet (Mardt et al. 2017) introduces a variational approach for Markov processes (VAMP) to develop a deep learning framework for molecular kinetics. DiffMD (Wu and Li 2023) employs a diffusion model to estimate the gradient of the log density of molecular conformations. DFF (Arts et al. 2023) leverages connections between score-based generative models, force fields, and molecular dynamics to learn a coarse-grained force field without requiring force inputs during training. However, these approximations are designed for general purposes and make limited use of prior knowledge of proteins. Consequently, learning atomic interactions incurs high computational costs, restricting their application to large molecules. In this paper, the objective is to generate dynamic 3D structures of proteins encompassing hundreds of amino acids across numerous time steps."}, {"title": "3 Preliminaries", "content": "Protein Parameterization. We adopt the frame-based representation of protein structure used in AlphaFold2 and extend it to incorporate a temporal dimension accounting for structural changes over time. A static protein comprises a sequence of amino acid residues, each parameterized by a backbone frame, consisting of atoms [N, Ca, C] with Ca positioned at the origin (0,0,0). We hence define a dynamic protein composed of N amino acid residues, each parameterized by a backbone frame that undergoes transformations across S time steps. Those frames are transformed by special Euclidean transformations that preserve orientations from the local frames to a global reference frame, represented by $T_{s,i} = [R_{s,i}, X_{s,i}] \\in SE(3)$, where $s \\in \\{1, ..., S\\}$, $i \\in \\{1, ..., N\\}$, $R_{s,i} \\in SO(3)$ is a 3 \u00d7 3 rotation matrix, and $X_{s.i}\\in \\mathbb{R}^3$ is the translation vector. All additional atoms coordinates in a residue are organized into rigid groups based on their dependency on torsion angles, such that all atoms within a rigid group maintain constant relative positions and orientations to preserve the chemical integrity of the structure. This setup allows each residue to be parameterized by torsion angles $a_{s,i} \\in \\mathbb{R}^7$ that model the rotations required to align atom groups relative to the backbone. The angles facilitate the precise adjustment of atom positions within each frame, and the transformation parameters allow the model to reconstruct all atom positions from idealized, experimentally determined coordinates over time.\nScore-based Modeling on $SE(3)^{S\\times N}$. The score-based model functions by diffusing a data distribution towards a noise distribution through a stochastic differential equation (SDE) and then learning to reverse this diffusion to generate samples. This process entails systematically reducing the structure in the data by introducing noise until the original signal is almost entirely removed. In our study, we diffuse the frames $T = [T_{s,i}] \\in SE(3)^{S\\times N}$ following the prior work (Yim et al. 2023). More specifically, we construct two independent forward processes for $R = [R_{s,i}] \\in SO(3)^{S\\times N}$ and $X = [X_{s,i}] \\in \\mathbb{R}^{S\\times N \\times 3}$ respectively:\n$dT(t) = [dR(t), dX(t)]$\n$= [\\frac{1}{2} \\sigma^2_t dW_{SO(3)^{S\\times N}},  \\sigma_t dW_{\\mathbb{R}^{S\\times N \\times 3}}],$\n(1)\nwhere $B(t)_{SO(3)^{S\\times N}}$ and $B_{\\mathbb{R}^{S\\times N \\times 3}}$ are the Brownian motion on $SO(3)^{S\\times N}$ and $\\mathbb{R}^{S\\times N \\times 3}$ respectively, and $t \\in [0, 1]$ denotes the diffusion time variable. Superscripts in parentheses are used to represent specific time step. Lowercase letters denote deterministic variables, and uppercase letters denote random variables.\nAccordingly, the associated backward process is given by the equation $dT(t) = [d\\bar{R}(t), d\\bar{X}(t)]$, where\n$d\\bar{R}(t) = \\sigma^2_t \\nabla log p_{1-t}(R(t))dt + d\\bar{B}_{SO(3)^{S\\times N}},$\n(2)\n$d\\bar{X}(t) = (\\bar{X}(t) + \\sigma^2_t \\nabla log p_{1-t}(X(t)))dt + d\\bar{B}_{\\mathbb{R}^{S\\times N \\times 3}}.$\n(3)\nThen, we can learn the score\n$\\nabla log p_t (T(t)) = [\\nabla log p_t (R(t)), \\nabla log p_t(X(t))]$ (4)\nwith neural networks $s_{\\theta}(t,T(t))$ trained by minimizing the denoising score matching loss:\n$\\mathcal{L}(\\theta) = E_{t, \\epsilon}[ \\lambda_t || \\nabla log P_{t|0} (T(t) | T(0)) - s_{\\theta}(t, T(t)) ||^2]$, (5)\nwhere $\\lambda_t \\in \\mathbb{R}^+$ is a weight, the expectation is taken over $t \\sim U[0, 1]$. and\n$\\nabla log P_{t|0} (T(t) | T(0)) = [\\nabla log P_{t|0} (R(t) | R(0)), \\nabla log P_{t|0}(X(t) | X(0))]$. (6)"}, {"title": "4 Methodology", "content": "The proposed methodology requires as input a sequence of amino acid residues, the reference 3D structure of a protein at a specific time step, and, the 3D structures of additional proteins from preceding time steps; and the output is the predicted protein trajectories for subsequent time steps. The paper commences with an overview of the generative network in Section 4.1. In Section 4.3, we present the proposed reference network and the motion alignment approach for learning temporal dynamic structures. Furthermore, Section 4.4 discusses the loss function employed, while Section A.2 provides detailed information regarding the training and inference processes.\n4.  1 Network Overview\nThe architecture of our model is depicted in Figure 2. To capture the dynamic behavior of a protein composed of N residues across S time steps, we utilize node features $V^l = [V^l_{s,i}] \\in \\mathbb{R}^{S\\times N \\times D_v}$ and edge features $Z^l = [Z^l_{s,(i,j)}] \\in \\mathbb{R}^{S\\times N \\times N \\times D_z}$. $V^l_{s,i}$ denotes the feature of the residue i at the time step s in layer l, and $Z^l_{s,(i,j)}$ encodes the relationship between residues i and j at the time step s in layer l. Positional attributes of atoms are represented by frames $T_{s,i}$ and torsion angles $a_{s,i}$. The reference structure is defined by $V^{ref}, Z^{ref}, and T^{ref}$. Motion structures are characterized by $V^{mot}, Z^{mot}, and T^{mot}$ which describe multi-order motion information, including velocity, acceleration, and other related parameters across M time steps.\nFeature Embedding of Amino Acid Sequence. For a given amino acid sequence, we initially extract node and edge features using the GeoFormer protein prediction method (Wu et al. 2022). These features are further enriched by encoding the diffusion time step, resulting in initial features $V^0_{s,i} \\in \\mathbb{R}^{N \\times D_v}$ and $Z^0_{s, (i,j)} \\in \\mathbb{R}^{N \\times \\tilde{N} \\times D_z}$ for each residue i and each pair of residues (i, j). The noisy 3D structures $T^0$ are sampled from the Isotropic Gaussian on SO(3) and the Gaussian distribution for capturing rotation and translation.\nInvariant Point Attention. We apply IPA mechanism in our networks in each layer l, it utilizes node features $V^l_{s,i}$, edge features $Z^l_{s, (i,j)}$, and frames $T^l_{s,i}$ as inputs. Each node feature $V^l_{s,i}$ generates query, key, and value points, which are subsequently transformed using the frame $T^l_{s,i}$. A self-attention mechanism aggregates these points based on attention scores at each time step s, integrating edge feature information and producing updated node features through fully-connected layers. To preserve reference coordinates, we introduce features without implementing the mapping back operation in the output points of the IPA, as elaborated in Appendix A.1.\n4.  2 Iterative Update\nThe iterative update process occurs across each network layer l, where node features are updated, followed by edge features and frames. Specifically, for each layer l, we concatenate the updated node features $V^{l+1}_{s,i}$ and $V^{l+1}_{s,j}$. These concatenated features undergo transformation through fully-connected layers to produce new edge features $Z^{l+1}_{s,(i,j)}$ for each time step s and residue pair (i, j). Simultaneously, a frame update $\\Delta T^l_{s,i}$ is computed based on the new nodes for each residue i via fully-connected layers and applied to the current frame to obtain the updated frame $T^{l+1}_{s,i}$. This iterative procedure of updating node features, edge features, and frames is repeated throughout the network, facilitating continuous propagation of updates.\n4.  3 Reference Guided Motion Alignment\nReference Network. The reference network is integral in encoding the structural features of the reference 3D protein structure. Its primary function is to ensure that the dynamic sequence generation of 3D structures retains these structural characteristics. Initially, we integrate the residue relationships $Z^l$ and positions $T^l$ into the node features $V^l$ for both the reference and noisy structures using the Invariant Point Attention (IPA) module. As illustrated in Figure 3(a), we calculate the interaction between the reference node $V^{ref}_{s,i}$ and the noisy node $V^l_{s,i}$ by implementing a spatial module on the concatenated features $[V^{ref}_{s,i}, V^l_{s,i}] \\in \\mathbb{R}^{S \\times N \\times 2D}$. For each time step s, the node feature is updated as follows:\n$[A^{ref}_{s,i}, A^l_{s,i}] = SelfAttention([V^{ref}_{s,i}, V^l_{s,i}]) $ \n$V^l_{s,i} = A^{ref}_{s,i} W^o + V^l_{s,i}$\n(7)\nwhere [] represents the collection of hidden features across dimension D. Here, $V^l_{s,i} \\in \\mathbb{R}^{N \\times D}$ represents the output node features for the time step s, and $W^o \\in \\mathbb{R}^{D \\times D}$ is a linear projection matrix.\nMotion Alignment. To accurately capture and reflect the protein's dynamic behavior, we introduce a motion alignment module. This component subjects the structural features of the protein to temporal self-attention within a diffusion-based generative process framework. Specifically, the module incorporates the 3D structures of the protein over several motion time steps preceding the reference time step, thereby embedding dynamic protein characteristics and enhancing the model's ability to capture protein kinetics. We compile the node features across all time steps into a comprehensive sequence, denoted as $[V^l_{s,i}]^{S}_{1, \\hat{S}}$, where $\\hat{S}$ represents the total of motion $S_{mot}$, reference $S_{ref}$ and noise time steps S. Sinusoidal positional time embeddings are then added to $[V^l_{s,i}] \\in \\mathbb{R}^{N \\times \\hat{S} \\times D}$. For each residue i, the module operates as follows:\n$[A^{mot}_{s,i}, A^{ref}_{s,i}, A^l_{s,i}] = SelfAttention([V^l_{s,i}])$ \n$V^l_{s,i} = A^{W} + V^l_{s,i}$\n(8)\nwhere [] the collection of hidden features across the time steps S. Here, $V^l_{s,i} \\in \\mathbb{R}^{S \\times D}$ denotes the output node features for time steps $\\{1, ..., S\\}$ for residue i, and $W^o \\in \\mathbb{R}^{D \\times D}$ is a linear projection matrix.\n4.  4 Loss Function\nWe define the overall loss function comprising the Denoising Score Matching (DSM) loss and several auxiliary losses.\nDenoising Score Matching Loss. The neural network is trained to learn rotation and translation scores by minimizing Equation 5. Specifically, we apply the weighting schedule for the rotation component as\n$\\lambda_t^R = 1/E[|\\nabla log P_{t|0}(R(t)|R(0))||_{SO(3)}]$.\n(9)\nFor the translation component, we use\n$\\lambda_t^X = (1 - exp{\\frac{t}{\\tau}}) / exp{\\frac{t}{\\tau}},$\n(10)\nto prevent instability in loss values at low t. The DSM loss is defined as follows:\n$\\mathcal{L}_{dsm} = \\mathcal{L}_{dsm}^R + \\mathcal{L}_{dsm}^X$\n(11)\nTorsion Angle Loss. We employ a Multi-Layer Perceptron (MLP) (Jumper et al. 2021) to predict the side chain and backbone torsion angles $a_{s,i}$, represented as points on the unit circle $||a_{s,i}|| \\in \\mathbb{R}^{7 \\times 2}$ with sine and cosine values. Due to the 180\u00b0 rotational symmetry of some side chains, the model is allowed to predict either the torsion angles or an alternative set of angles:\n$\\mathcal{L}_{torsion} = \\frac{1}{N} \\sum_{i=1}^N (min(||a_{i,s}^{gt} - a_{i,s}^{p}||^2, ||a_{i,s}^{gt} - a_{alt,i,s}^{gt}||^2))$\n(12)\nwhere $a_{i,s}^{p}, a_{i,s}^{gt}$ and $a_{alt,i,s}^{gt}$ represent predicted, ground truth, and alternative ground truth torsion angles, respectively, for each residue i.\nAuxiliary loss. To mitigate chain breaks or steric clashes, penalties are imposed on atomic errors. Define $\\Omega = \\{N, C, C_{\\alpha}, O\\}$. The first auxiliary loss is the mean squared error on the positions of selected atoms in $\\Omega$:\n$\\mathcal{L}_a = \\frac{1}{4N} \\sum_{i=1}^N \\sum_{a \\in \\Omega} ||a^{(0)}_i - \\bar{a}^{(0)}_i||^2$\n(13)\nwhere $a^{(0)}_i$ and $\\bar{a}^{(0)}_i$ are the ground truth and predicted atom positions for atom a in residue i. The second auxiliary loss penalizes pairwise atomic distance errors:\n$\\mathcal{L}_{2D} = \\frac{1}{C} \\sum_{i,j=1}^N \\sum_{a,b \\in \\Omega} \\mathbb{1}\\{d_{ab}^{(0)} < 0.6\\} ||a^{(0)}_i - \\bar{b}^{(0)}_j||^2$\n(14)\nwhere $C = \\sum_{i,j=1}^N \\sum_{a,b \\in \\Omega} \\mathbb{1}\\{d_{ab}^{(0)} < 0.6\\} - N$, $d_{ab}^{(0)} = ||a^{(0)}_i - \\bar{b}^{(0)}_j||$, and $\\mathbb{1}\\{d_{ab}^{(0)} < 0.6\\}$ is an indicator variable to penalize only atoms that within 0.6 nm. These auxiliary losses are applied only when t < 1.\nTotal Loss. The comprehensive training loss is thus formulated as:\n$\\mathcal{L} = \\mathcal{L}_{dsm} + w_1\\cdot\\mathbb{1}\\{t < 1\\}(\\mathcal{L}_a+\\mathcal{L}_{2D})+w_2\\cdot\\mathcal{L}_{torsion},$\n(15)\nwhere $w_1$ and $w_2$ are the weights for the auxiliary and torsion losses, respectively. In our experiments, we set $w_1 = 0.25$ and $w_2 = 1."}, {"title": "5 Experiments", "content": "Dataset. We conducted experiments and statistical comparisons against prior work utilizing datasets such as ATLAS (Vander Meersche et al. 2024) and fast-folding proteins (Lindorff-Larsen et al. 2011). (a) ATLAS: This dataset consists of 1,390 protein chains sourced from the Protein Data Bank (PDB), selected for their structural diversity as classified by the ECOD (Schaeffer et al. 2016) domain classification. To enhance the model's ability to capture structured elements like alpha-helices and beta-sheets, we used the DSSP (Kabsch and Sander 1983) algorithm to calculate the random coil content of each protein and excluded those with over 50%. We also applied a polynomial regression model to filter out proteins exceeding the maximum allowable radius of gyration for their sequence length. This approach effectively removed outliers and structurally anomalous proteins, resulting in the selection of 758 proteins from the ATLAS dataset for further analysis. (b) Fast-folding Proteins: This dataset encompasses folding and unfolding events, rendering their simulated trajectories particularly complex. We selected six proteins, which are Chignolin, Trp_cage, BBA, Villin, BBL, and protein_B, for our experimental investigations.\nImplementation Details. Our framework is implemented using PyTorch 1.13.1 and Python 3.9, utilizing CUDA 11.4 for acceleration. All experiments and statistical analyses presented in this paper were conducted on a computing machine equipped with an NVIDIA A100 GPU with 80 GB of memory. We trained the parameters of our network using a batch size of 4, with an initial learning rate set to 0.0001, which subsequently decreases according to a cosine annealing schedule. The training procedure consists of a total of 550 epochs, with reference time steps $S_{ref} = 1$ and motion time steps $S_{mot} = 2.\n5.  1 Quantitative Results\nTask. Following the methodology established in DiffMD (Wu and Li 2023), we empirically evaluate our approaches on two tasks: (a) Short-term-to-long-term (S2L) Trajectory Generation. In this task, models are trained on short-term trajectories and are subsequently required to generate long-term trajectories for the same protein, given a specified starting conformation. The training process utilizes the first 90% of the frames, while validation and testing are conducted on the remaining 10% of the frames. This time-based extrapolation is designed to evaluate the model's ability to generalize across the temporal view. (b) One-to-others (O2O) Trajectory Generation. In this task, models are trained on the trajectories of a subset of proteins and evaluated on the trajectories of different proteins. This assessment aims to evaluate the model's ability to generalize to the conformations of distinct proteins, thereby measuring its performance across various protein types.\nMetric. We adopt the Root Mean Square Error (RMSE), expressed in angstroms \u00c5, as the evaluation metric for all snapshots over a specified time period comprising S time steps, denoted as $\\{s\\}_{s=1}^S$. We define $R_s$ as the average RMSE calculated over the first s time steps. The term $C_{\\alpha}$-RMSE refers to the RMSE computed between carbon alpha atoms. To derive our results, we sampled 500 snapshots and calculated the average RMSE across these samples.\nComparison to SOTA. In this section, we compare our framework with S2L model DFF (Arts et al. 2023) and Flow-Matching (Kohler et al. 2023), utilizing the ATLAS and fast-folding protein datasets. The results are summarized in Tables 1 and 2, where our framework demonstrates superior accuracy across both datasets. Notably, our approach excels in long-term predictions, as evidenced by a reduction in the $R_{32}$ error from 4.60 to 2.12 on the ATLAS dataset, and from 5.48 to 4.39 on the Fast-Folding protein dataset on the S2L task. Additionally, our model shows strong performance on the O2O task, comparable to that of S2L, underscoring its impressive generalization capability. The inclusion of proteins with longer simulation times entails greater kinetic variations at each trajectory step, further highlighting the efficacy of our method."}, {"title": "5.  2 Qualitative Results", "content": "We visualize the distribution of dynamic proteins across the first two TIC generated by our model and compare it with the ground truth, as depicted in Figure 4. We can see that our model effectively predicts the kinetics of proteins, aligning closely with the ground truth distribution. The error between the predicted values (blue) and the actual MD simulation results (red) is presented in Figure 5. The predictions maintain a $C_{\\alpha}$-RMSE within 2 \u00c5 of the simulation results, demonstrating that our model accurately captures the MD simulation trajectory, particularly in light of the fact that the diameter of a carbon atom is approximately 1.4 \u00c5. Figure 6 illustrates the reverse diffusion process of our model at selected time steps, highlighting how the protein structure progressively attains greater coherence throughout the denoising process. The qualitative results of different time steps are shown in Figure 7. We can see that the proposed method effectively captures protein kinetics and generates plausible trajectories."}, {"title": "5.  3 Ablation Studies", "content": "Effect of Motion Alignment. We conducted a series of detailed ablation studies on the ATLAS dataset to assess the effectiveness of each model component. As indicated in Table 3, the incorporation of motion alignment results in a reduction of the $R_2$ error from 1.40 to 1.30 for $C_{\\alpha}$-RMSE. Our findings highlight that motion alignment is critical for the 4D dynamic protein prediction task, as it introduces essential kinetic characteristics that enhance model performance.\nTraining Protein Number. We investigate the impact of increasing the number of training proteins on model performance, as illustrated in Figure 8(a). The results reveal that, when the protein trajectory length is held constant, an identifiable scaling law emerges that significantly enhances model performance with the increasing number of training proteins. Training Protein Trajectory Length. Similarly, as depicted in Figure 8(b), the results demonstrate that when the number of proteins is fixed and the trajectory length is increased, a corresponding scaling law emerges that effectively improves model performance.\nEfficiency Analysis. As illustrated in Table 4, the error of our models remains steady as the predicted trajectory length increases. In contrast, the error of the iterative method rapidly increase with longer trajectory lengths.\nIterative vs. Simultaneous Prediction. Our framework generates dynamic structures for S time steps simultaneously. In contrast to previous approaches (Arts et al. 2023; Wu and Li 2023), which predict each step iteratively, we demonstrate that our design achieves superior accuracy. To facilitate a comparison, we also evaluate our model using an iterative approach. Specifically, we train our model to predict the 3D structure solely for the next time step, subsequently generating the entire trajectory through iterative predictions"}]}