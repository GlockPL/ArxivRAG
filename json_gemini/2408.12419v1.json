{"title": "4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided\nMotion Alignment", "authors": ["Kaihui Cheng", "Ce Liu", "Qingkun Su", "Jun Wang", "Liwei Zhang", "Yining Tang", "Yao Yao", "Siyu Zhu", "Yuan Qi"], "abstract": "Protein structure prediction is pivotal for understanding the\nstructure-function relationship of proteins, advancing biolog-\nical research, and facilitating pharmaceutical development\nand experimental design. While deep learning methods and\nthe expanded availability of experimental 3D protein struc-\ntures have accelerated structure prediction, the dynamic na-\nture of protein structures has received limited attention. This\nstudy introduces an innovative 4D diffusion model incorpo-\nrating molecular dynamics (MD) simulation data to learn dy-\nnamic protein structures. Our approach is distinguished by\nthe following components: (1) a unified diffusion model capa-\nble of generating dynamic protein structures, including both\nthe backbone and side chains, utilizing atomic grouping and\nside-chain dihedral angle predictions; (2) a reference network\nthat enhances structural consistency by integrating the latent\nembeddings of the initial 3D protein structures; and (3) a mo-\ntion alignment module aimed at improving temporal struc-\ntural coherence across multiple time steps. To our knowl-\nedge, this is the first diffusion-based model aimed at pre-\ndicting protein trajectories across multiple time steps simulta-\nneously. Validation on benchmark datasets demonstrates that\nour model exhibits high accuracy in predicting dynamic 3D\nstructures of proteins containing up to 256 amino acids over\n32 time steps, effectively capturing both local flexibility in\nstable states and significant conformational changes.", "sections": [{"title": "1\nIntroduction", "content": "The observation and prediction of protein structures are piv-\notal in elucidating the complex relationship between protein\nconformation and function. This understanding drives sig-\nnificant advancements in biological research and pharma-\nceutical development, while also providing essential guid-\nance for related experimental endeavors and design strate-\ngies. The 3D architecture of a protein is intricately en-\ncoded within its linear 1D amino acid sequence, which\nfundamentally dictates the protein's biological functional-\nity. Deciphering the process of protein folding has long\nposed a formidable challenge within the domain of compu-\ntational biophysics. Key challenges in protein structure pre-\ndiction include the accurate identification of suitable tem-\nplates for protein structures, particularly for sequences lack-\ning closely related templates; the refinement of these tem-\nplates to closely resemble the native state; the enhancement\nof force field precision and conformational exploration; as\nwell as the effective management of computational costs as-\nsociated with predicting protein structures. This is especially\npertinent in scenarios involving free modeling, where struc-\ntures must be generated de novo.\nRecent advancements in deep learning techniques, cou-\npled with the exponential growth of experimental pro-\ntein structures within the Protein Data Bank (PDB) have markedly propelled learning-based structural\nstudies. AlphaFold2 has introduced\na groundbreaking approach to predicting 3D protein struc-\ntures, achieving accuracy comparable to experimental meth-\nods. In tandem, RoseTTAFold has en-\nhanced predictive capabilities by incorporating a three-track\nnetwork architecture, resulting in superior accuracy. Concur-\nrently, ESMFold and OmegaFold  capitalize on high-capacity transformer lan-\nguage models trained on evolutionary data to derive un-\nsupervised representations of protein sequences. Moreover,\nthe accessibility of large-scale data repositories has sub-\nstantially advanced research in protein conformation sam-\npling, which seeks to generate diverse structural conforma-\ntions. For instance, Distributional Graphformer (DiG) facil-\nitates the prediction of equilibrium distributions in molec-\nular systems, enabling efficient generation of diverse con-\nformations and the estimation of state densities. EigenFold approaches pro-\ntein structures as systems of harmonic oscillators, foster-\ning a cascading-resolution generative process along the sys-\ntem's eigenmodes. AlphaFlow optimizes single-state predictors through a custom\nflow matching framework to develop sequence-conditioned\ngenerative models of protein architectures. Building on\nits predecessor, AlphaFold3 utilizes a diffusion network and updated algorithmic architec-\nture to incorporate joint structures across proteins, nucleic\nacids, small molecules, ions, and modified residues. Fur-\nthermore, Str2Str introduces an innovative\nframework for structure-to-structure translation, capable of\nzero-shot conformation sampling while maintaining roto-\ntranslation equivariance. Despite these significant advance-\nments in structural and conformational predictions, the ex-\nploration of dynamic protein structures remains underdevel-\noped. This study aims to address this gap, focusing on the\ndynamic aspects of protein structures.\nMolecular dynamics (MD) simulations serve as crucial\ntools in the fields of computational biology, biophysics, and\nchemistry, providing a comprehensive and dynamic per-\nspective of molecular systems. These simulations gener-\nate substantial high-quality data, which can be exploited\nfor data-driven, learning-based methodologies. Neverthe-\nless, the computational expense associated with MD sim-\nulations typically scales cubically with the number of elec-\ntronic degrees of freedom. Moreover, critical biomolecular\nprocesses, such as conformational changes, often occur on\ntimescales that surpass the capabilities of classical all-atom\nMD simulations. In response, deep learning techniques have\nbeen employed to address these limitations. Despite these\nadvancements, existing methods are predominantly appli-\ncable to proteins with significantly fewer atoms than typ-\nical proteins, necessitating the adoption of coarse-grained\natomic representations for larger systems. This study aims\nto leverage extensive, high-quality MD data to generate dy-\nnamic structures of proteins comprising up to hundreds of\namino acids, including complex structures with complete\nside-chain representations. Our approach seeks to extend the\napplicability of MD simulations to larger and more intri-\ncate protein systems, thereby enhancing our understanding\nof their dynamic behaviors.\nThis paper presents an innovative approach to modeling\ndynamic protein structures utilizing a 4D diffusion model.\nOur research is concentrated on three primary areas: Firstly,\nwe propose a unified diffusion model designed to predict\nprotein structures that encompass both backbone and side-\nchain components. By organizing atoms within each residue\ninto rigid groups to minimize the degrees of freedom, our\nframework efficiently simulates protein motion for struc-\ntures with hundreds of residues. The amino acid sequence\nis represented by node and edge features derived from struc-\nture prediction models, which guide the diffusion model for\nprecise protein generation. Unlike methods constrained to\nde novo structure prediction, we incorporate side-chain di-\nhedral angle predictions and introduce an amino acid atomic\nmodel to accurately recover individual atomic coordinates\nbased on dihedral angles. Secondly, the initial 3D protein\nstructure is integrated as a condition and encoded through\na reference network for latent embedding, thereby incorpo-\nrating relevant features into the denoising diffusion network.\nThe reference network is instrumental in maintaining struc-\ntural consistency of proteins during motion. Thirdly, we pro-\npose a motion alignment module within the score-based dif-\nfusion network, which includes temporal attention layers to\naggregate kinetic information from adjacent frames within\nthe diffusion model. This enhancement improves the coher-\nence of motion in generated dynamic proteins, mitigating\nabrupt transitions during motion. Thus, our diffusion model\neffectively generates dynamic protein structures across mul-\ntiple time steps simultaneously, enhancing efficiency and\nensuring the prediction of consistent sequences of protein\nstructures within a temporal framework. In summary, our\napproach enhances the efficacy of dynamic protein struc-\nture generation while ensuring the prediction of coherent\nand temporally consistent sequences.\nIn this investigation, we conducted a comprehensive qual-\nitative and quantitative analysis utilizing widely recognized\nbenchmark datasets, including ATLAS and Fast-Folding protein datasets. Our study successfully achieved dynamic\nprotein structure predictions for sequences of up to 256\namino acids across 32 time steps. This capability enabled\nus to model dynamic protein conformations sampled at var-\nious temporal intervals, demonstrating notable accuracy in\ncapturing both subtle intra-conformational motions and sig-\nnificant inter-conformational changes. The findings of this\nresearch represent a significant advancement in the field of\ndynamic protein structure prediction, contributing valuable\ninsights for future developments in this domain."}, {"title": "2 Related Work", "content": "De Novo Protein Design. The task of de novo protein de-\nsign involves generating novel proteins based on physical\nprinciples, with specified structural and/or functional prop-\nerties. FoldFlow introduces a simulation-\nfree approach for learning deterministic continuous-time dy-\nnamics and matching invariant target distributions on SE(3).\nVFN-Diff presents the Vector Field Net-\nwork (VFN), which enables network layers to perform learn-\nable vector computations between coordinates of frame-\nanchored virtual atoms, thereby enhancing the capability for\nmodeling frames. In recent years, with the rapid develop-\nment of diffusion-based generative models, these technologies have also been applied to de novo\nprotein design. RFDiffusion fine-tunes\nthe RoseTTAFold structure prediction\nnetwork on protein structure denoising tasks, resulting in\na generative model of protein backbones based on the dif-\nfusion model in the formulation of DDPM."}, {"title": "Prediction of 3D Protein Structure.", "content": "Predicting the 3D\nstructure of proteins from their amino acid sequences has\nlong been a significant challenge in biology. Various ap-\nproaches, including thermodynamic and kinetic simulations\nand bioinformatics analyses, have been proposed. This pa-\nper focuses on deep learning-based methods. An early\ndeep learning effort, Raptor-X utilizes a dilated\nResNet to predict atom pair distances. Subsequently, tr-\nRosetta enhances accuracy by predicting\ninter-residue geometries. AlphaFold2 marks a milestone with its novel attention mechanisms\nand training procedures, leveraging evolutionary, physical,\nand geometric constraints to significantly improve accuracy.\nRoseTTAFold further refines network ar-\nchitectures with a three-track network, achieving superior\naccuracy. Additionally, ESMFold and\nOmegaFold employ high-capacity trans-\nformer language models trained on evolutionary data in an\nunsupervised manner to learn protein sequence representa-\ntions. Recently, AlphaFold3 extends\nprotein structure prediction using a diffusion network and an\nupdated algorithmic architecture, encompassing joint struc-\ntures of proteins, nucleic acids, small molecules, ions, and\nmodified residues. However, the aforementioned works pri-\nmarily focus on static structure prediction using diffusion\ngenerative models. In contrast, this paper addresses the pre-\ndiction of dynamic structures over temporal sequences."}, {"title": "Protein Conformation Sampling.", "content": "Proteins are dynamic\nmacromolecules, where conformational changes play crit-\nical roles in biological processes. To obtain a diverse set\nof conformations, classical approaches such as MSA sub-\nsampling have been employed, which subsample the Mul-\ntiple Sequence Alignment (MSA) input to AlphaFold2. Re-\ncently, diffusion models have emerged for protein conforma-\ntion generation. Distributional Graphformer (DiG) predicts\nthe equilibrium distribution of molecular systems, enabling\nefficient generation of diverse conformations and estima-\ntion of state densities. EigenFold models the structure as a system of harmonic\noscillators, naturally inducing a cascading-resolution gen-\nerative process along the eigenmodes of the system. Al-\nphaFlow fine-tunes single-state predictors under a custom flow matching framework to\nobtain sequence-conditioned generative models of protein\nstructure Str2Str adopts a novel structure-\nto-structure translation framework capable of zero-shot con-\nformation sampling with roto-translation equivariant prop-\nerties. It is important to note that protein conformation sam-\npling predicts the distribution of structures rather than struc-\ntures within the temporal domain."}, {"title": "Learning Based Molecular Dynamics.", "content": "Deep learning\nhas significantly impacted complex atomic systems by re-\nducing the need for time-consuming calculations. Applications include estimating free energy\nsurfaces , constructing Markov\nstate models of molecular kinetics , and generating samples from equilibrium distributions. Here, we briefly review re-\nsearch on learning kinetics models. VAMPNet introduces a variational approach for Markov pro-\ncesses (VAMP) to develop a deep learning framework for\nmolecular kinetics. DiffMD employs a\ndiffusion model to estimate the gradient of the log den-\nsity of molecular conformations. DFF leverages connections between score-based generative mod-\nels, force fields, and molecular dynamics to learn a coarse-\ngrained force field without requiring force inputs during\ntraining. However, these approximations are designed for\ngeneral purposes and make limited use of prior knowledge\nof proteins. Consequently, learning atomic interactions in-\ncurs high computational costs, restricting their application\nto large molecules. In this paper, the objective is to generate\ndynamic 3D structures of proteins encompassing hundreds\nof amino acids across numerous time steps."}, {"title": "3 Preliminaries", "content": "Protein Parameterization. We adopt the frame-based\nrepresentation of protein structure used in AlphaFold2 and\nextend it to incorporate a temporal dimension accounting\nfor structural changes over time. A static protein comprises\na sequence of amino acid residues, each parameterized by a\nbackbone frame, consisting of atoms [N, Ca, C] with Ca po-\nsitioned at the origin (0,0,0). We hence define a dynamic\nprotein composed of N amino acid residues, each param-\neterized by a backbone frame that undergoes transforma-\ntions across S time steps. Those frames are transformed by\nspecial Euclidean transformations that preserve orientations\nfrom the local frames to a global reference frame, repre-\nsented by \\(T_{s,i} = [R_{s,i}, X_{s,i}] \\in SE(3)\\), where \\(s \\in \\{1, ..., S\\}\\), \\(i \\in \\{1, ..., N\\}\\), \\(R_{s,i} \\in SO(3)\\) is a 3 \u00d7 3 rotation matrix, and\n\\(X_{s.i}\\in \\mathbb{R}^3\\) is the translation vector. All additional atoms co-\nordinates in a residue are organized into rigid groups based\non their dependency on torsion angles, such that all atoms\nwithin a rigid group maintain constant relative positions and\norientations to preserve the chemical integrity of the struc-\nture. This setup allows each residue to be parameterized by\ntorsion angles \\(a_{s,i} \\in \\mathbb{R}^7\\) that model the rotations required to\nalign atom groups relative to the backbone. The angles fa-\ncilitate the precise adjustment of atom positions within each\nframe, and the transformation parameters allow the model\nto reconstruct all atom positions from idealized, experimen-\ntally determined coordinates over time.\nScore-based Modeling on SE(3)S\u00d7N. The score-based\nmodel functions by diffusing a data distribution towards a\nnoise distribution through a stochastic differential equation\n(SDE) and then learning to reverse this diffusion to gener-\nate samples. This process entails systematically reducing the\nstructure in the data by introducing noise until the original\nsignal is almost entirely removed. In our study, we diffuse\nthe frames \\(T = [T_{s,i}] \\in SE(3)^{S \\times N}\\) following the prior\nwork. More specifically, we construct two\nindependent forward processes for \\(R = [R_{s,i}] \\in SO(3)^{S \\times N}\\)\nand \\(X = [X_{s,i}] \\in \\mathbb{R}^{S \\times N \\times 3}\\) respectively:\n\\[dT(t) = [dR(t), dX(t)]\n= \\left[\\frac{1}{2} \\sigma_R^2 (t) J J^T dt + \\sigma_R(t) dB^{SO(3)^{S \\times N}}, \\frac{1}{2} \\sigma_X^2 (t) dt + \\sigma_X(t) dB^{\\mathbb{R}^{S \\times N \\times 3}}\\right],\\qquad(1)\\]\nwhere \\(B^{SO(3)^{S \\times N}}\\) and \\(B^{\\mathbb{R}^{S \\times N \\times 3}}\\) are the Brownian motion\non \\(SO(3)^{S \\times N}\\) and \\(\\mathbb{R}^{S \\times N \\times 3}\\) respectively, and \\(t \\in [0, 1]\\) de-\nnotes the diffusion time variable. Superscripts in parentheses\nare used to represent specific time step. Lowercase letters\ndenote deterministic variables, and uppercase letters denote\nrandom variables.\nAccordingly, the associated backward process is given by\nthe equation \\(dT(t) = [dR(t), dX(t)]\\), where\n\\[d\\tilde{R}(t) = [-\\frac{1}{2} \\sigma_R^2(t) J J^T dt + \\sigma_R(t) dB^{SO(3)^{S \\times N}}],\\qquad(2)\\]\n\\[d\\tilde{X}(t) = (-\\sigma_X^2(t) \\nabla_{X(t)} \\log p_{1-t}(X(t)) + \\sigma_X(t) dB^{\\mathbb{R}^{S \\times N \\times 3}}).\\]\nThen, we can learn the score\n\\[\\nabla \\log p_t (T(t)) = [\\nabla \\log p_t (R(t)), \\nabla \\log p_t(X(t))]\\qquad(4)\\]\nwith neural networks \\(s_\\theta(t,T(t))\\) trained by minimizing the\ndenoising score matching loss:\n\\[L(\\theta) = E[\\lambda_t ||\\nabla \\log p_{t|0} (T(t)|T(0)) - s_\\theta(t, T(t))||^2],\\qquad(5)\\]\nwhere \\(\\lambda_t \\in \\mathbb{R}^+\\) is a weight, the expectation is taken over\n\\(t \\sim U[0, 1]\\). and\n\\[\\nabla \\log p_{t|0} (T(t)|T(0)) = [\\nabla \\log p_{t|0} (R(t)|R(0)),\\qquad(6)\n\\nabla \\log p_{t|0}(X(t)|X(0))].\\]"}, {"title": "4 Methodology", "content": "The proposed methodology requires as input a sequence of\namino acid residues, the reference 3D structure of a protein\nat a specific time step, and, the 3D structures of additional\nproteins from preceding time steps; and the output is the pre-\ndicted protein trajectories for subsequent time steps. The pa-\nper commences with an overview of the generative network\nin Section 4.1. In Section 4.3, we present the proposed refer-\nence network and the motion alignment approach for learn-\ning temporal dynamic structures. Furthermore, Section 4.4\ndiscusses the loss function employed, while Section A.2 pro-\nvides detailed information regarding the training and infer-\nence processes.\n4.1 Network Overview\nThe architecture of our model is depicted in Figure 2. To\ncapture the dynamic behavior of a protein composed of N\nresidues across S time steps, we utilize node features \\(V^l =\n[V^l_{s,i}] \\in \\mathbb{R}^{S \\times N \\times D_v}\\) and edge features \\(Z^l =\n[Z^l_{s,(i,j)}] \\in \\mathbb{R}^{S \\times N \\times N \\times D_z}\\). \\(V^l_{s,i}\\) denotes the feature of the residue i at\nthe time step s in layer l, and \\(Z^l_{s,(i,j)}\\) encodes the relation-\nship between residues i and j at the time step s in layer l."}, {"title": "4.2 Iterative Update.", "content": "The iterative update process occurs across each network\nlayer l, where node features are updated, followed by edge\nfeatures and frames. Specifically, for each layer l, we con-\ncatenate the updated node features \\(V^{l+1}_{s,i}\\) and \\(V^{l+1}_{s,j}\\). These\nconcatenated features undergo transformation through fully-\nconnected layers to produce new edge features \\(Z^{l+1}_{s,(i,j)}\\) for\neach time step s and residue pair (i, j). Simultaneously, a\nframe update \\(\\Delta T^l\\) is computed based on the new nodes for\neach residue i via fully-connected layers and applied to the\ncurrent frame to obtain the updated frame \\(T^{l+1}_{s,i}\\). This itera-\ntive procedure of updating node features, edge features, and\nframes is repeated throughout the network, facilitating con-\ntinuous propagation of updates."}, {"title": "4.3 Reference Guided Motion Alignment", "content": "Reference Network. The reference network is integral in\nencoding the structural features of the reference 3D protein\nstructure. Its primary function is to ensure that the dynamic\nsequence generation of 3D structures retains these struc-\ntural characteristics. Initially, we integrate the residue rela-\ntionships \\(Z^l\\) and positions \\(T^l\\) into the node features \\(V^l\\) for\nboth the reference and noisy structures using the Invariant\nPoint Attention (IPA) module. As illustrated in Figure 3(a),\nwe calculate the interaction between the reference node \\(V^{ref}\\)\nand the noisy node \\(V^l_i\\) by implementing a spatial module on\nthe concatenated features \\([V^{ref}, V^l_i] \\in \\mathbb{R}^{S \\times N \\times 2D}\\). For each\ntime step s, the node feature is updated as follows:\n\\[[A^{ref}_s, A^l_s] = SelfAttention([V^{ref}, V^l_s])\\]\n\\[V^l_s = A^{ref}_s W^{ref} + A^l_s W^l,\\]\nwhere [] represents the collection of hidden features across\ndimension D. Here, \\(V^l_s \\in \\mathbb{R}^{N \\times D}\\) represents the output node\nfeatures for the time step s, and \\(W^{ref} \\in \\mathbb{R}^{D \\times D}\\) is a linear\nprojection matrix.\nMotion Alignment. To accurately capture and reflect the\nprotein's dynamic behavior, we introduce a motion align-\nment module. This component subjects the structural fea-\ntures of the protein to temporal self-attention within a\ndiffusion-based generative process framework. Specifically,\nthe module incorporates the 3D structures of the protein over\nseveral motion time steps preceding the reference time step,\nthereby embedding dynamic protein characteristics and en-\nhancing the model's ability to capture protein kinetics. We\ncompile the node features across all time steps into a com-\nprehensive sequence, denoted as \\([V_i]^\\hat{S}_{\\text{mot},1}. ^\\hat{S}\\), where \\(\\hat{S}\\)\nrepresents the total of motion \\(S_{\\text{mot}}\\), reference \\(S_{\\text{ref}}\\)\nand noise time steps S. Sinusoidal positional time embeddings are\nthen added to \\([V_i] \\in \\mathbb{R}^{N \\times \\hat{S} \\times D}\\). For each residue i, the mod-"}, {"title": "4.4 Loss Function", "content": "We define the overall loss function comprising the Denois-\ning Score Matching (DSM) loss and several auxiliary losses.\nDenoising Score Matching Loss. The neural network is\ntrained to learn rotation and translation scores by minimiz-\ning Equation 5. Specifically, we apply the weighting sched-\nule for the rotation component as\n\\[\\lambda_R = 1/E[||\\nabla \\log p_{t|0}(R(t)|R(0))||_{SO(3)}^2].\\qquad(9)\\]\nFor the translation component, we use\n\\[\\lambda_X = (1 - \\exp^{-t}) / \\exp^{-t}.\\qquad(10)\\]\nto prevent instability in loss values at low t. The DSM loss\nis defined as follows:\n\\[L_{dsm} = L^R_{dsm} + L^X_{dsm}.\\qquad(11)\\]\nTorsion Angle Loss. We employ a Multi-Layer Percep-\ntron (MLP) to predict the side chain\nand backbone torsion angles \\(a_{s,i}\\), represented as points on\nthe unit circle \\(||a_{s,i}|| \\in \\mathbb{R}^{7 \\times 2}\\) with sine and cosine values.\nDue to the 180\u00b0 rotational symmetry of some side chains,\nthe model is allowed to predict either the torsion angles or\nan alternative set of angles:\n\\[L_{torsion} = \\frac{1}{N} \\sum_{i=1}^N (\\min(||a^{gt}_{i} - a^{pred}_i||^2, ||a^{gt}_{i} - a^{alt, gt}_i||^2))\\qquad(12)\\]\nwhere \\(a^{pred}_{i}, a^{gt}_{i}\\) and \\(a^{alt, gt}_i\\) represent predicted, ground truth,\nand alternative ground truth torsion angles, respectively, for\neach residue i.\nAuxiliary loss. To mitigate chain breaks or steric clashes,\npenalties are imposed on atomic errors. Define \\(\\Omega =\\n\\{N, C, C_\\alpha, O\\}\\). The first auxiliary loss is the mean squared er-\nror on the positions of selected atoms in \\(\\Omega\\):\n\\[L_a = \\frac{1}{4N} \\sum_{i=1}^N \\sum_{a \\in \\Omega} ||a^{(0)}_i - \\tilde{a}^{(0)}_i||^2\\qquad(13)\\]\nwhere \\(a^{(0)}_i\\) and \\(\\tilde{a}^{(0)}_i\\) are the ground truth and predicted\natom positions for atom a in residue i. The second auxiliary\nloss penalizes pairwise atomic distance errors:\n\\[L_{2D} = \\frac{1}{C} \\sum_{i,j=1}^N \\sum_{a,b \\in \\Omega} \\mathbf{1}\\{d^{(0)}_{ab} < 0.6\\} ||a^{(0)}_i - b^{(0)}_j||^2 \\qquad(14)\\]\nwhere \\(C = \\sum_{i,j=1}^N \\sum_{a,b \\in \\Omega} \\mathbf{1}\\{d^{(0)}_{ab} < 0.6\\} - N\\), \\(d^{(0)}_{ab} =\n||a^{(0)}_i - b^{(0)}_j||\\), and \\(\\mathbf{1}\\{d^{(0)}_{ab} < 0.6\\}\\) is an indicator variable\nto penalize only atoms that within 0.6 nm. These auxiliary\nlosses are applied only when t < 1.\nTotal Loss. The comprehensive training loss is thus for-\nmulated as:\n\\[L = L_{dsm} + w_1 \\cdot \\mathbf{1}\\{t < 1\\} \\cdot (L_a + L_{2D}) + w_2 \\cdot L_{torsion},\\qquad(15)\\]\nwhere \\(w_1\\) and \\(w_2\\) are the weights for the auxiliary and tor-\nsion losses, respectively. In our experiments, we set \\(w_1 =\n0.25\\) and \\(w_2 = 1\\)."}, {"title": "5 Experiments", "content": "Dataset. We conducted experiments and statistical com-\nparisons against prior work utilizing datasets such as AT-\nLAS and fast-folding pro-\nteins. (a) ATLAS: This dataset\nconsists of 1,390 protein chains sourced from the Protein\nData Bank (PDB), selected for their structural diversity as\nclassified by the ECOD domain clas-\nsification. To enhance the model's ability to capture struc-\ntured elements like alpha-helices and beta-sheets, we used\nthe DSSP algorithm to calcu-\nlate the random coil content of each protein and excluded\nthose with over 50%. We also applied a polynomial re-\ngression model to filter out proteins exceeding the maxi-\nmum allowable radius of gyration for their sequence length.\nThis approach effectively removed outliers and structurally\nanomalous proteins, resulting in the selection of 758 pro-\nteins from the ATLAS dataset for further analysis. (b) Fast-\nfolding Proteins: This dataset encompasses folding and un-\nfolding events, rendering their simulated trajectories partic-\nularly complex. We selected six proteins, which are Chig-\nnolin, Trp_cage, BBA, Villin, BBL, and protein_B, for our\nexperimental investigations.\nImplementation Details. Our framework is implemented\nusing PyTorch 1.13.1 and Python 3.9, utilizing CUDA 11.4\nfor acceleration. All experiments and statistical analyses\npresented in this paper were conducted on a computing ma-\nchine equipped with an NVIDIA A100 GPU with 80 GB of\nmemory. We trained the parameters of our network using a\nbatch size of 4, with an initial learning rate set to 0.0001,\nwhich subsequently decreases according to a cosine anneal-\ning schedule. The training procedure consists of a total of\n550 epochs, with reference time steps \\(S_{\\text{ref}} = 1\\) and motion\ntime steps \\(S_{\\text{mot}} = 2\\)."}, {"title": "5.1 Quantitative Results", "content": "Task. Following the methodology established in DiffMD, we empirically evaluate our approaches\non two tasks: (a) Short-term-to-long-term (S2L) Trajec-\ntory Generation. In this task, models are trained on short-\nterm trajectories and are subsequently required to generate\nlong-term trajectories for the same protein, given a spec-\nified starting conformation. The training process utilizes\nthe first 90% of the frames, while validation and testing\nare conducted on the remaining 10% of the frames. This\ntime-based extrapolation is designed to evaluate the model's\nability to generalize across the temporal view. (b) One-to-\nothers (O2O) Trajectory Generation. In this task, mod-\nels are trained on the trajectories of a subset of proteins and\nevaluated on the trajectories of different proteins. This as-\nsessment aims to evaluate the model's ability to generalize\nto the conformations of distinct proteins, thereby measuring\nits performance across various protein types.\nMetric. We adopt the Root Mean Square Error (RMSE),\nexpressed in angstroms \u00c5, as the evaluation metric for all\nsnapshots over a specified time period comprising S time\nsteps, denoted as \\({\\delta_s\\}_{s=1}^S\\). We define \\(R_s\\) as the average\nRMSE calculated over the first s time steps. The term \\(C_\\alpha\\)-\nRMSE refers to the RMSE computed between carbon alpha\natoms. To derive our results, we sampled 500 snapshots and\ncalculated the average RMSE across these samples.\nComparison to SOTA. In this section, we compare our\nframework with S2L model DFF and Flow-\nMatching , utilizing the ATLAS and fast-\nfolding protein datasets. The results are summarized in Ta-\nbles 1 and 2, where our framework demonstrates superior\naccuracy across both datasets. Notably, our approach ex-\ncels in long-term predictions, as evidenced by a reduction\nin the \\(R_{32}\\) error from 4.60 to 2.12 on the ATLAS dataset,\nand from 5.48 to 4.39 on the Fast-Folding protein dataset on\nthe S2L task. Additionally, our model shows strong perfor-\nmance on the O2O task, comparable to that of S2L, under-\nscoring its impressive generalization capability. The inclu-\nsion of proteins with longer simulation times entails greater\nkinetic variations at each trajectory step, further highlighting\nthe efficacy of our method."}, {"title": "5.2 Qualitative Results", "content": "We visualize the distribution of dynamic proteins across the\nfirst two TIC generated by our model and compare it with\nthe ground truth, as depicted in Figure 4. We can see that\nour model effectively predicts the kinetics of proteins, align-\ning closely with the ground truth distribution. The error be-\ntween the predicted values (blue) and the actual MD simu-\nlation results (red) is presented in Figure 5. The predictions\nmaintain a \\(C_\\alpha\\)-RMSE within 2 \u00c5 of the simulation results,\ndemonstrating that our model accurately captures the MD\nsimulation trajectory, particularly in light of the fact that the\ndiameter of a carbon atom is approximately 1.4 \u00c5. Figure 6\nillustrates the reverse diffusion process of our model at se-\nlected time steps, highlighting how the protein structure pro-\ngressively attains greater coherence throughout the denois-\ning process. The qualitative results of different time steps\nare shown in Figure 7. We can see that the proposed method\neffectively captures protein kinetics and generates plausible\ntrajectories."}, {"title": "5.3 Ablation Studies", "content": "Effect of Motion Alignment. We conducted a series of\ndetailed ablation studies on the ATLAS dataset to assess the\neffectiveness of each model component. As indicated in Ta-\nble 3, the incorporation of motion alignment results in a re-\nduction of the \\(R_2\\) error from 1.40 to 1.30 for \\(C_\\alpha\\)-RMSE. Our\nfindings highlight that motion alignment is critical for the\n4D dynamic protein prediction task, as it introduces essen-\ntial kinetic characteristics that enhance model performance.\nTraining Protein Number. We investigate the impact of\nincreasing the number of training proteins on model per-\nformance, as illustrated in Figure 8(a). The results reveal\nthat, when the protein trajectory length is held constant, an\nidentifiable scaling law emerges that significantly enhances\nmodel performance with the increasing number of training\nproteins."}, {"title": "5.4\nLimitations and Future Works", "content": "Our current model effectively addresses both local move-\nments in relatively stable states and conformational changes\nin proteins containing up to 256 amino acids over 32 time\nsteps. Additionally, it exhibits a degree of extrapolative ca-\npability, facilitating the generation of long-term molecular\ndynamics processes. Looking ahead, we intend to focus on\nthree key areas for improvement. (a) Longer Temporal Pre-\ndictions: To enhance the accuracy of long-term predictions,\nwe propose incorporating dynamic energy or force con-\nstraints into diffusion-based generative models. This integra-\ntion will ensure that predictions remain stable and consistent\nwith molecular dynamics principles. (b) Improving Predic-\ntions of Large Conformational Changes: We aim to diver-"}, {"title": "A.1 Modules", "content": "EdgeUpdate and BackboneUpdate Here we provide the\ndetail of IPA presented"}]}