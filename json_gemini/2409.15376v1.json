{"title": "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "authors": ["Nuo Chen", "Ning Wu", "Jianhui Chang", "Jia Li"], "abstract": "Utilizing large language models (LLMs) for data augmentation has yielded encouraging results in mathematical reasoning. However, these approaches face constraints in problem diversity, potentially restricting them to in-domain/distribution data generation. To this end, we propose ControlMath, an iterative method involving an equation-generator module and two LLM-based agents. The module creates diverse equations, which the Problem-Crafter agent then transforms into math word problems. The Reverse-Agent filters and selects high-quality data, adhering to the \"less is more\" principle, achieving better results with fewer data points. This approach enables the generation of diverse math problems, not limited to specific domains or distributions. As a result, we collect ControlMathQA, which involves 190k math word problems. Extensive results prove that combining our dataset with in-domain datasets like GSM8K can help improve the model's mathematical ability to generalize, leading to improved performances both within and beyond specific domains.", "sections": [{"title": "1 Introduction", "content": "Currently, mathematical reasoning (Cobbe et al., 2021; Chen et al., 2023d; Zhou et al., 2022; Weng et al., 2022) is regarded as the one of the most challenging areas for current Large Language Models (LLMs). Typically, prompting-based approaches (Wei et al., 2022b,a; Wang et al., 2022) are common ways to improve the mathematical abilities of closed-source LLMs. These methods design different prompts for these LLMs to solve multi-step and complicated math problems, setting a high benchmark and demonstrating the potential of LLMs in tackling sophisticated mathematical problems.\nRecently, the focus has shifted towards improving the capabilities of smaller, open-source LLMs through instruction-tuning (Yu et al., 2023; Chen et al., 2024a; Ouyang et al., 2022; Peng et al., 2023;"}, {"title": "2 Related Works", "content": "Mathematical Reasoning with LLMs. In recent times, large language models (LLMs) (Brown et al., 2020; Hu et al., 2021; OpenAI, 2023; Touvron et al., 2023a; Huang et al., 2024; Chen et al., 2023c) have demonstrated remarkable abilities in handling complex mathematical reasoning tasks (MR) (Scao et al., 2022; Cobbe et al., 2021; Zhou et al., 2022; Weng et al., 2022; Chen et al., 2023e; Imani et al., 2023a). Two primary approaches are used: (1) Prompting-based methods (Imani et al., 2023b; Wang et al., 2022; Gao et al., 2023; Shi et al., 2022; Chen et al., 2023e), which employ diverse prompts to aid LLMs in solving mathematical problems. A notable example is Chain-of-Thought (CoT) (Wei et al., 2022b), which guides LLMs through step-by-step prompting. (2) Finetuning-based approaches (Li et al., 2023b; Yuan et al., 2023), which unlock the potential of open-source LLMs for mathematical reasoning through instruction-tuning, relying on effective downstream training data.\nData Augmentation with LLMs. In the era of LLMs, data augmentation is regarded as a useful"}, {"title": "3 ControlMath", "content": "The primary motivation behind ControlMath is to address the challenges of overfitting and inefficiency in training LLMs for mathematical reasoning tasks. To achieve this, ControlMath employs a three-step approach involving two specialized agents and one Python function module. In this section, we present the details of ControlMath."}, {"title": "3.1 Controllable Equation Generation", "content": "Our first step is the controllable generation of mathematical equations, driven by the need to ensure diversity in the following generated math word problems. Specifically, we employ an equation generation module to allow precise control over the characteristics of the equations, addressing the issue of homogeneous training samples that limit the model's ability to generalize.\nIn our implementation, the equation generation module is a Python function designed to produce equations based on specified parameters. We focus on generating three main types of equations: 1) Multi-step Calculation: These include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more complex functions like trigonometric functions, roots, and exponents. For generating such equations, we specify the input parameters, including steps, operators and numerical ranges, to create diverse equations; 2) Polynomials: These equations involve terms with variables raised to various powers, often requiring multiple steps to simplify or solve. By generating polynomials of varying degree of the polynomial and the coefficients, we ensure that the model encounters a wide range of polynomial-related problems; 3) Probability: These equations involve calculating probabilities, which often require understanding combinations, permutations, and probability rules. We follow (Hendrycks et al., 2021) and set predefined topics: a set of letters or sequence as templates. These templates serve as the basis for generating diverse probability problems, ensuring the LLMs develop a strong foundation in statistical reasoning.\nBy customizing different inputs, we generate a wide variety of formulas that cover a broad spectrum of mathematical concepts and operations. This controlled generation ensures the foundation for creating diverse mathematical word problems."}, {"title": "3.2 Efficient Data Selection: More data is not always helpful", "content": "Problem-Rewriter Agent. The third step focuses on selecting effective training data and eliminating redundant or ineffective samples. The motivation for this step is to enhance training efficiency by ensuring that only useful data is retained, thereby improving model performance without overfitting. The Problem-Rewriter agent is crucial for this step. After generating an initial problem by the Problem-Crafter, this agent rewrites the problems to create variations. Note that the rewriter agent is constrained to maintain the original numerical values of the problems, changing only the phrasing or topics, as shown in Appendix A.\nEfficient Data Selection. Then, both the original and rewritten math word problems are presented to the smaller open-sourced LLM; if the LLM can solve both correctly, it means such problem does not contribute to further learning and can be discarded because model may have mastered this level of problems. However, if the model answers incorrectly to one of them, it indicates a genuine gap in understanding, making these problems valuable for further training. Such problem will be collected in our ControlMathQA. This process mimics human cognitive learning, where exposure to redundant information does not enhance understanding. By focusing on non-redundant, effective samples, we improve the overall quality and efficiency of the training data. To reduce variance, the temperature is set to 0 when sampling LLM's outputs.\nValidation. In practice, we observe that sometimes the new problems generated by the Problem-Crafter Agent might not correctly match the corresponding formulas or the generated reference solutions might be inaccurate. To address these issues, we introduce a validation step using GPT-4 (gpt-4-turbo). For validation, we input the formula, the generated problem, and the corresponding solution into GPT-4. If GPT-4 identifies any mismatches-either the problem does not align with the formula, or the solution does not correctly solve the problem-we discard the problematic entry. This validation step ensures that only accurate and relevant problems are included in our dataset, further enhancing the quality of the training data. The prompts are presented in Appendix A.\nIterative Process. In theory, our approach can be iteratively applied indefinitely to continually generate needed corpus. However, due to resource constraints, we limit experimentation to three iterations. In the first iteration, we use Mistral-7B which has not undergone any fine-tuning. For the second and third iterations, we fine-tune the model using the data generated from the previous iterations. This iterative process allows us to progressively enhance the model's performance by continually refining the training data and focusing on the model's weaknesses identified in each round.\nWhy we need to rewrite the problem? The purpose of rewriting the problem is to ensure the robustness of the model's understanding. Several works (Chen et al., 2024c; Yu et al., 2023) have proved that simply rephrasing or changing the order of the question could confuse LLMs. Thus, simply testing the model on the original problem might not fully capture its comprehension, as the model could be overfitting to specific phrasings or patterns in the training data. By presenting both the original and the rewritten problem, we can more accurately assess whether the model truly understands the underlying concepts. We have also complie a confusion matrix showing the proportions"}, {"title": "3.3 Statistics", "content": "In each iteration, we generate 160k multi-step calculation formulas, 10k polynomials and 10k probability problems (See details in following Section 4.1). After data selection and GPT-4 validation process, we collect 110k math word problems in total. Inspired by (Yuan et al., 2023), response augmentation also contributes to the benefit of math reasoning LLMs, we also include this type augmentation in our ContronlMathQA, where we utilize GPT-3.5-Turbo-0613 to generate answers for each question three times and remain the correct ones. As a result, we collect about 190k question-answer pairs, where 155k for multi-calculation, 15k for polynomials and 20k for probability problems. Note, we decide to keep only original problems as we find including rewritten ones contributes limited improvements (Results in Appendix, Figure 7)."}, {"title": "4 Experiments", "content": "In this section, we first present the details of our implementation, including data generation, training settings. Then we show the main results of different backbones with ControlMathQA."}, {"title": "4.1 Implementation", "content": "In equation generation (Section 3.1), we introduce several constraints to ensure the generated equations are both high in quality and diverse: (1) Unique equations: To secure robust samples, it is essential to keep each generated equation distinct; (2) Diverse Distributions: a) For multi-step calculation formulas, we set the range for steps from 2 to 9, covering most multi-step reasoning scenarios. The numerical range is divided into four tiers: 1-100, 100-k, 1k-10k, and 10k-1million. During each iteration, these ranges are randomly combined to produce 5,000 unique equation examples for each combing group, with operators including basic arithmetic (addition, subtraction, multiplication, division), square roots, and exponents. b) For polynomials, we set the degree range from 1 to 3 and coefficients from -100 to 100, producing polynomials of different complexities, ensuring the LLMs encounter a broad range of polynomial-related problems. Each iteration randomly samples 5,000 polynomial equations. c) Following Saxton et al. (2018), we consider two sampling settings: with or without replacement to generate probability problems. The sample space templates consist of a set of characters and a sequential sequence, where the number of unique characters does not exceed"}, {"title": "4.1.2 Experimental Settings", "content": "Training Settings. The question-answer pairs in ControlMathQA are formatted in Alpaca-format (Taori et al., 2023). Experimentally, we select the LLaMA 2-7B,13B and Mistral-7B (Jiang et al., 2023) as backbone models. We use a batch size of 128, 512 max token length and train on the ControlMathQA dataset for 3 epochs using a learning rate of 2e-5 on NVIDIA A100 GPUs. To better illustrate the generalization of our ControlMathQA, we separately combine it with GSM8K and MetaMath in-domain training datasets to validate its effectiveness in in-domain and out-of-domain results.\nEvaluation Settings. We evaluate different models in the following datasets: (1) SVAMP (Patel et al., 2021), an elementary-level math dataset with 1,000 test examples. (2) SVAMP-Hard (Chen et al., 2023b), which replaces numbers in SVAMP questions with values between 100k and 10M. (3) GSM8K (Cobbe et al., 2021), a dataset of 1,391 linguistically diverse grade school MWPs crafted by human writers. (4) GSM8K-Hard (Gao et al., 2023), an advanced version of GSM8K with larger numerical values to test LLM generalization. (5) The Polynomials and Probability subsets from Mathematics Datasets (Saxton et al., 2018), each containing 2,000 samples. (6) MATH (Hendrycks et al., 2021), which provides competition-level challenges across different mathematical domains."}, {"title": "4.2 Results", "content": "We evaluate ControlMathQA to answer the following questions: Q1: Can our dataset promote math generalist LLMs across various domains? Q2: Does our adaptively efficient data selection strategy help model achieve better results with less data?\nRQ1: ControlMathQA promotes Math Generalist Models. Table 3 presents three backbones' performances with combining ControlMathQA and other in-domain training datasets, separately. Obviously, ours could help different LLMs exhibit exceptional performances across different domains. A notable example is that when training LLaMA 2-7B with ours and GSM8K, the resulting model attains impressive improvements of 15.8% and 68.3% on SVAMP and Probability datasets. Interestingly, though MetaMath could significantly boost the model performances in-domain benchmarks, it downgrades the baseline results in Prob-"}, {"title": "4.3 Discussion 1: Tailor ControlMath for specific datasets", "content": "Our method can focus on both generalization and adaptation to a targeted dataset. A trivial version of our method involves adaptively generating data based on poor performance on a targeted dataset. Here, we provide an example of how to augment the GSM8K dataset using our method:\nFirst, we train a base SFT model (LLaMA 2-7B) using the GSM8K training set. Then, we test this model on the test set, recording the error rates for"}, {"title": "4.4 Discussion 2: Cost-Effective ControlMath", "content": "Using GPT-4's API for data generation and validation is costly, resulting in more than 10k dollars. Intuitively, can we use smaller LLMS for data generation to reduce these costs while maintaining high quality? To explore this, we collect related 20k training samples from GPT-4 to train Mistral-7B as the two Problem-Crafter and Problem-Rewriter Agents. We also apply rejection sampling to conduct answer augmentation. With the trained Mistral-7B agents, we conduct data augmentation over nearly two months on 8xA100 GPUs, resulting in the collection of approximately 1 million training samples. We term the collected dataset as ControlMathQA-Open.\nInterestingly, as shown in Figure 5, although ControlMathQA-Open consists of more than five times as much data as the base version, it performs"}, {"title": "4.5 Discussion 3: Diversity Gains", "content": "we evaluate the diversity of our generated math word problems using the diversity gain (Bilmes, 2022) metric. This metric measures how much a new dataset contributes to the diversity of a base dataset.\nConcretely, given a base dataset $D_{base}$: Comprises (N) samples, each represented as $(q_i, r_i)$"}, {"title": "5 Conclusion", "content": "In this paper, we propose a new data augmentation approach for mathematical reasoning, called ControlMath. The core of this approach lies into two lines: 1) It first utilizes an equation generation module to control the distributions of the generated equations. Then a Probelm-Crafter Agent generates the math word problems based on these equations. 2) It introduces a Problem-Rewriter Agent to help ensure the effectiveness of the generated samples, filtering out redundant ones. Our approach could generate diverse problems and focus on quality over quantity to enable stronger math generalist models with smaller data. The resulting dataset, ControlMathQA, could help LLMs obtain improvements in both in-domain and out-of-domain datasets. Future works include expanding the size of ControlMathQA and exploring other strategies to select more efficient data."}, {"title": "Limitation", "content": "In this paper, we focus on using large language models to generate training samples for mathematical reasoning, Some issues remain to be explored: In practice, we observe that gpt-4-turbo may generate unrelated questions from the given equations, especially for complex ones involving 7 or 8 steps. We found that the error rate for such cases can be as high as 25-35%, leading to significant cost inefficiencies. A more efficient solution needs to be explored to address this issue.\nAnother limitation is that we have not yet conducted experiments on larger LLMs like LLaMA 2-30B or 70B models, primarily due to training resource constraints.\nAdditionally, due to the specificity of our method, we can generate large amounts of data tailored to specific datasets, potentially causing the model to overfit within that domain and thus achieve better performance. This could lead to unfair advantages in leaderboard rankings, though it ultimately depends on the practitioners' professional ethics."}, {"title": "Examples B.1: Several cases of our models solving Probability problems", "content": "Question: Three letters picked without replacement from y: 3, a: 1, p: 3, v: 2, c: 2, t: 2. Give prob of sequence cca.\nAnswer: To find the probability of picking the sequence \u201ccca\u201d without replacement from the set {y: 3, a : 1, p: 3, v : 2, c : 2, t : 2}, we calculate the probability step by step: The total number of letters is 3 + 1 + 3 + 2 + 2 + 2 = 13. The probability of picking \u201cc\u201d first is $\\frac{2}{13}$ since there are 2 \"c\"s out of 13 letters. After picking the first \"c\", there are now 12 letters left, and the probability of picking another \"c\" is $\\frac{1}{12}$. After picking the second \u201cc\u201d, there are now 11 letters left, and the probability of picking \"a\" is $\\frac{1}{11}$. The probability of the sequence \u201ccca\u201d is the product of these probabilities:\n$\\frac{2}{13} \\times \\frac{1}{12} \\times \\frac{1}{11} = \\frac{1}{858}$\nTherefore, the probability of picking the sequence \u201ccca\u201d without replacement is $\\frac{1}{858}$. Therefore, the answer is 1/858. ### 1/858.\nQuestion: Two letters picked without replacement from wdwwwwwrdwrrww. Give prob of sequence dr.\nAnswer: To find the probability of picking the sequence \"dr\" without replacement from the string \"wdwwwwwrdwrrww\", we calculate as follows: The total number of letters is 14. There are 2 \"d\"s and 3 \"r\"s in the string. The probability of picking \"d\" first is $\\frac{2}{14}$ since there are 2 \u201cd\u201ds out of 14 letters. After picking \u201cd\u201d, there are now 13 letters left, and the probability of then picking \u201cr\u201d is $\\frac{3}{13}$, as there are 3 \"r\"s remaining. The probability of the sequence \u201cdr\u201d is the product of these probabilities:\n$\\frac{2}{14} \\times \\frac{3}{13} = \\frac{3}{91}$\nTherefore, the probability of picking the sequence \u201cdr\u201d without replacement is $\\frac{3}{91}$. Therefore, the answer is 3/91. ### 3/91."}, {"title": "Examples B.2: Several cases of our models solving Polynomial problems", "content": "Question: Express -37 - f**3 + 70 + 26 as b + w*f**2 + x*f + y*f**3 and give b.\nAnswer: Given equation: -37 \u2013 $f^3$ + 70+ 26 Simplify the constants: -37+ 70 + 26 = 59 Therefore, the equation can be rewritten as: 59 \u2013 $f^3$ In the form b + wf2 + x f + y $f^3$, we have:\nb = 59\nw = 0 (since there is no $f^2$ term)\nx = 0 (since there is no $f$ term)\ny = -1 (coefficient of $f^3$)\nTherefore, the answer is 59. ### 59.\nQuestion: Frankie, a young scientist, is experimenting with temperature effects on a certain chemical compound. She measures the change in energy, denoted as E, when a sample is cooled. The energy change depends on the temperature in Fahrenheit, f, according to the equation:\nE = \u221237 \u2013 $f^3$ + 70 + 26\nFrankie records a temperature of -2 Fahrenheit during one of her experiments. Calculate the energy change for this temperature using the simplified equation."}, {"title": "Examples B.3: Several cases of our models solving GSM8K/GSM8K-Hard problems", "content": "Question: Susan made 100 cookies for Christmas and was going to equally divide them between her 6 nephews. Before Susan could package them, her husband snuck 4 cookies for himself. How many cookies will each of Susan's nephews get?\nAnswer: Susan made 100 cookies in total. Her husband snuck 4 cookies for himself, so there are \u00ab100 - 4 = 96\u00bb cookies left.Susan wants to divide these cookies equally between her 6 nephews. To find out how many cookies each nephew will get, we divide the total number of cookies by the number of nephews: <<96 / 6 = 16\u00bb. Each of Susan's nephews will get 16 cookies. Therefore, the answer is 16. ### 16.\nQuestion: Siobhan has 2 fewer jewels than Aaron. Aaron has 5 more jewels than half of Raymond's jewels. If Raymond has 973054 jewels, how many jewels does Siobhan have?\nAnswer: We know that Raymond has 973054 jewels.Half of Raymond's jewels is \u00ab973054/2 = 486527\u00bb jewels.Aaron has 5 more jewels than half of Raymond's jewels, so Aaron has \u00ab486527 + 5 = 486532\u00bb jewels.Siobhan has 2 fewer jewels than Aaron, so Siobhan has \u00ab486532 - 2 = 486530\u00bb jewels.Therefore, Siobhan has 486530 jewels. Therefore, the answer is 486530. ###486530."}]}