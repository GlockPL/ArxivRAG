{"title": "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation", "authors": ["Sang-Hoon Lee", "Ha-Yeong Choi", "Seong-Whan Lee"], "abstract": "Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios. Although GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech. Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks. Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals. In this paper, we propose PeriodWave, a novel universal waveform generation model. First, we introduce a period-aware flow matching estimator that can capture the periodic features of the waveform signal when estimating the vector fields. Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals. Although increasing the number of periods can improve the performance significantly, this requires more computational costs. To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference. Additionally, we utilize discrete wavelet transform to losslessly disentangle the frequency information of waveform signals for high-frequency modeling, and introduce FreeU to reduce the high-frequency noise for waveform generation. The experimental results demonstrated that our model outperforms the previous models both in Mel-spectrogram reconstruction and text-to-speech tasks. All source code will be available at https://github.com/sh-lee-prml/PeriodWave.", "sections": [{"title": "1 Introduction", "content": "Deep generative models have achieved significant success in high-fidelity waveform generation. In general, the neural waveform generation model which is called \"Neural Vocoder\" transforms a low-resolution acoustic representation such as Mel-spectrogram or linguistic representations into a high-resolution waveform signal for regeneration learning [Tan et al., 2024]. Conventional neural vocoder models have been investigated for text-to-speech [Oord et al., 2016; Shen et al., 2018; Ren et al., 2019; Kim et al., 2020; Jiang et al., 2024] and voice conversion [Lee et al., 2021; Choi et al., 2021]. Furthermore, recent universal waveform generation models called \"Universal Vocoder\" are getting more attention due to their various applicability in neural audio codec [Zeghidour et al., 2021; D\u00e9fossez et al., 2023; Kumar et al., 2024; Ju et al., 2024], audio generation [Kreuk et al., 2023; Roman"}, {"title": "2 Related Works", "content": "Neural Vocoder WaveNet [Oord et al., 2016] has successfully paved the way for high-quality neural waveform generation tasks. However, these auto-regressive (AR) models suffer from a slow inference speed. To address this limitation, teacher-student distillation-based inverse AR flow methods [Oord et al., 2018; Ping et al., 2019] have been investigated for parallel waveform generation. Flow-based models [Kim et al., 2019; Prenger et al., 2019; Lee et al., 2020] have also been utilized, which can be trained by simply maximizing the likelihood of the data using invertible transformation.\nGAN-based Neural Vocoder MelGAN [Kumar et al., 2019] successfully incorporated generative adversarial networks (GAN) into the neural vocoder by introducing a multi-scale discriminator to reflect different features from the different scales of waveform signal and feature matching loss for stable training. Parallel WaveGAN [Yamamoto et al., 2020] introduces multi-resolution STFT losses that can improve the perceptual quality and robustness of adversarial training. GAN-TTS [Bi\u0144kowski et al., 2020] utilized an ensemble of random window discriminators that operate on random segments of waveform signal. GED [Gritsenko et al., 2020] proposed a spectral energy distance with unconditional GAN for stable and consistent training. HiFi-GAN [Kong et al., 2020] introduced a novel discriminator, a multi-period discriminator (MPD) that can capture different periodic features of waveform signal. UnivNet [Jang et al., 2021] employed adversarial feedback on the multi-resolution spectrogram to capture the spectral representations at different resolutions. BigVGAN [Lee et al., 2023] adopted periodic activation function and anti-aliased representation into the generator for generalization on out-of-distribution samples. Vocos [Siuzdak, 2024] proposed an efficient waveform generation framework using ConvNeXt blocks and iSTFT head without any temporal domain upsampling. Meanwhile, neural codec models [Zeghidour et al., 2021; D\u00e9fossez et al., 2023; Kumar et al., 2024] and applications [Wang et al., 2023; Yang et al., 2023a] such as TTS and audio generation have been investigated together with the development of neural vocoder.\nDiffusion-based Neural Vocoder DiffWave [Kong et al., 2021] and WaveGrad [Chen et al., 2021] introduced a Mel-conditional diffusion-based neural vocoder that can estimate the gradients of the data density. PriorGrad [Lee et al., 2022b] improves the efficiency of the conditional diffusion model by adopting a data-dependent prior distribution for diffusion models instead of a standard Gaussian distribution. FastDiff [Huang et al., 2022a] proposed a fast conditional diffusion model by adopting an efficient generator structure and noise schedule predictor. Multi-band Diffusion [Roman et al., 2023] incorporated multi-band waveform modeling into diffusion models and it significantly improved the performance by band-wise modeling because previous diffusion methods could not model high-frequency information, which only generated the low-frequency representations. This model also focused on raw waveform generation from discrete tokens of neural codec model for various audio generation applications including speech, music, and environmental sound."}, {"title": "3 PeriodWave", "content": "The flow matching model [Lipman et al., 2022; Tong et al., 2023] has emerged as an effective strategy for the swift and simulation-free training of continuous normalizing flows (CNFs), producing optimal transport (OT) trajectories that are readily incorporable. We are interested in the use of flow matching models for waveform generation to understand their capability to manage complex transformations across waveform distributions. Hence, we begin with the essential notation to analyze flow matching with optimal transport, followed by a detailed introduction to the proposed method."}, {"title": "3.1 Preliminary: Flow Matching with Optimal Transport Path", "content": "In the data space $R^d$, consider an observation $x \\in R^d$ sampled from an unknown distribution q(x).\nCNFs transform a simple prior $p_0$ into a target distribution $p_1 \\approx q$ using a time-dependent vector field\n$v_t$. The flow $\\varPhi_t$ is defined by the ordinary differential equation:\n$\\frac{d}{dt}\\Phi_t(x) = v_t(\\Phi_t(x); \\theta), \\quad \\Phi_0(x) = x, \\quad x \\sim p_0, $\n(1)\nwhere $\\Phi_t(x)$ denotes the state of the system at time t, driven by the vector field $v_t(\\cdot; \\theta)$. The flow matching objective, as introduced by [Lipman et al., 2022], aims to match the vector field\n$v_t(x)$ to an ideal vector field $u_t(x)$ that would generate the desired probability path $p_t$. The flow matching training objective involves minimizing the loss function $L_{FM}(\\theta)$, which is defined by\nregressing the model\u2019s vector field $v_{\\theta}(t, x)$ to a target vector field $u_t(x)$ as follows:\n$L_{FM}(\\theta) = \\mathbb{E}_{t \\sim [0,1], x \\sim p_t(x)} \\|v_{\\theta}(t, x) - u_t(x)\\|^2.$\n(2)"}, {"title": "3.2 Period-aware Flow Matching Estimator", "content": "In this work, we propose a period-aware flow matching estimator, which can reflect the different periodic features when estimating the vector field for high-quality waveform generation as illustrated in Figure 1. First, we utilize a time-conditional UNet-based structure for time-specific vector field estimation. Unlike previous UNet-based decoders, PeriodWave utilizes a mixture of reshaped input signals with different periods as illustrated in Figure 2. Similar to [Kong et al., 2020], we reshape the 1D data sampled from $p_t(x)$ of length T into 2D data of height T/p and width p. We will refer to this process as Periodify. Then, we condition the period embedding to indicate the specific period of each reshaped sample for period-aware feature extraction in a single estimator. We utilize different periods of [1,2,3,5,7] that avoid overlaps to capture different periodic features from the input signal. We utilize 2D convolution of down/upsampling layer and ResNet Blocks with a kernel size of 3 and dilation of 1, 2 for each UNet block. Specifically, we downsample each signal by [4,4,4] so the representation of the middle block has height T/(p \u00d7 64) and width p. After extracting the representation for each period, we reshape the 2D representation into the original shape of the 1D signal for each period path. We sum all representations from all period paths. The final block estimates the vector fields from a mixture of period representations.\nFor Mel-spectrogram conditional generation, we only add the conditional representation extracted from Mel-spectrogram to the middle layer representation of UNet for each period path. We utilize"}, {"title": "3.3 Flow Matching for Waveform Generation", "content": "To the best of our knowledge, this is the first work to utilize flow matching for waveform generation.\nIn this subsection, we describe the problems we encountered and how to reduce these issues. First, we found that the it is crucial to set the proper noise scale for $x_0$. In general, waveform signal is ranged\nwith -1 to 1, so standard normal distribution $N(0, 1)$ would be large for optimal path. This results in high-frequency information distortion, causing the generated sample to contain only low-frequency information. To reduce this issue, we scale down the $x_0$ by multiplying a small value $\\alpha$. Although we successfully generate the waveform signal by small $\\alpha$, we observed that the generated sample sometimes contains a small white noise. We simply solve it by additionally multiplying temperature T on the $x_0$ as analyzed in Table 4. Furthermore, we adopt data-dependent prior [Lee et al., 2022b] to flow matching-based generative models. Specifically, we utilize an energy-based prior which can be simply extracted by averaging the Mel-spectrogram along the frequency axis. We set N(0, \u03a3) for the distribution of $p_0(x)$, and multiply \u03a3 by a small value of 0.5. All of them significantly improve the sample quality and boost the training speed."}, {"title": "3.4 High-frequency Information Modeling for Flow Matching", "content": "Similar to the findings demonstrated by [Roman et al., 2023], we also observed that flow matching-based waveform generation models could not provide the high-frequency information well. To address this limitation, we adopt three approaches including multi-band modeling and FreeU [Si et al., 2024]\nMulti-band Flow Matching with Discrete Wavelet Transform Previously, MBD [Roman et al., 2023] demonstrated that diffusion-based models are vulnerable to high-frequency noise so they introduce the multi-band diffusion models by disentangling the frequency bands and introducing specialized denoisers for each band. Additionally, they proposed frequency equalizer (EQ) processor to reduce the white noise by regularizing the noise energy scale for each band.\nUnlike MBD, we introduce a discrete wavelet Transform based multi-band modeling method which can disentangle the signal and reproduce the original signal without losing information\u00b2. PeriodWave-MB consists of multiple vector field estimators for each band [0-3, 3-6, 6-9, 9-12 kHz]. Additionally, we first generate a lower band, and then concatenate the generated lower bands to the $x_0$ to generate higher bands. We found that this significantly improve the quality even with small sampling steps. During training, we utilize a ground-truth discrete wavelet Transform components for a conditional information. Additionally, we also utilize a band-wise data-dependent prior by averaging Mel-spectrogram according to the frequency axis including overlapped frequency bands [0-61, 60-81, 80-93, 91-100 bins]. Moreover, we downsample each signal by [1,4,4] by replacing the first down/upsampling with DWT/iDWT, and this also significantly reduce the computational cost by reducing time resolution.\nFlow Matching with FreeU FreeU [Si et al., 2024] demonstrated that the features from the skip connection contain high-frequency information in UNet-based diffusion models, and this could ignore the backbone semantics during image generation. We revisited this issue in high-resolution waveform generation task. We also found that the skip features of our model contain a large ratio of"}, {"title": "4 Experiment and Result", "content": "Dataset We train the models using LJSpeech [Ito and Johnson, 2017] and LibriTTS [Zen et al.,\n2019] datasets. LJSpeech is a high-quality single-speaker dataset with a sampling rate of 22,050 Hz.\nLibriTTS is a multi-speaker dataset with a sampling rate of 24,000 Hz. Following [Lee et al., 2023], we adopt the same configuration for Mel-spectrogram transformation. For the LJSpeech, we use the Mel-spectrogram of 80 bins. For the LibriTTS, we utilize the Mel-spectrogram of 100 bins.\nTraining For reproducibility, we will release all source code, checkpoints, and generated samples at\nhttps://periodwave.github.io/demo/. For the LibriTTS dataset, we train PeriodWave using the AdamW optimizer with a learning rate of $5 \\times 10^{-4}$, batch size of 128 for 1M steps on four NVIDIA A100 GPUs. Each band of PeriodWave-MB is trained using the AdamW optimizer with a learning rate of $2 \\times 10^{-4}$, batch size of 64 for 1M steps on two NVIDIA A100 GPUs. It only takes three days to train the model while GAN-based models take over three weeks. We do not apply any learning rate schedule. For the ablation study, we train the model with a batch size of 128 for 0.5M steps on four NVIDIA A100 GPUs. For the LJSpeech dataset, we only train the multi-band model for 0.5M steps.\nSampling For the ODE sampling, we utilize Midpoint methods with sampling steps of 164. Additionally, we compared the ODE methods including Euler, Midpoint, and RK4 methods according to different sampling steps in Appendix D. The experimental details are described in Appendix H and I."}, {"title": "4.1 LJSpeech: High-quality Single Speaker Dataset with 22,050 Hz", "content": "We conducted an objective evaluation to compare the performance of the single-speaker dataset. We utilized the official implementation and checkpoints of HiFi-GAN, PriorGrad, and FreGrad, which have the same Mel-spectrogram configuration. Table 1 shows that our model achieved a significantly improved performance in all objective metrics without M-STFT. It is worth noting that our model can achieve a better performance than diffusion baselines even with unprecedented small training steps of 0.05M while other models should be trained over 1M steps. Additionally, GAN-based models take much more time to train the model due to the discriminators. Furthermore, our proposed methods require smaller sampling steps than diffusion-based models. We observed that diffusion-based model and flow matching-based models could not model the high-frequency information because their objective function does not guarantee the high-frequency information while GAN-based models utilize Mel-spectrogram loss and M-STFT-based discriminators. To reduce this issue, we utilize multi-band modeling and FreeU operation, and the results also show improved performance in most metrics."}, {"title": "4.2 LibriTTS: Multi-speaker Dataset with 24,000 Hz", "content": "We conducted objective and subjective evaluations to compare the performance of the multi-speaker dataset. We utilized the publicly available checkpoints of UnivNet, BigVGAN, and Vocos, which are trained with the LibriTTS dataset. Table 2 shows our model significantly improved performance in all metrics but the M-STFT metric. Although other GAN-based models utilize Mel-spectrogram distance loss and multi-resolution spectrogram discriminators which can minimize the distance on the spectral domain, we only trained the model by minimizing the distance of the vector field on the waveform. However, our model achieved better performance in subjective evaluation. Specifically, our models have better performance on the periodicity metrics, and this means that our period-aware structure could improve the performance in terms of pitch and periodicity by significantly reducing the jitter sound. Both PeriodWave-MB and PeriodWave demonstrated significantly lower pitch error distances compared to BigVGAN. Specifically, PeriodWave-MB and PeriodWave (FreeU) achieved a pitch error distance of 16.829 and 18.730 (17.398), respectively, while BigVGAN's pitch error distance was 25.651. Table 3 also demonstrated the fast training speed of PeriodWave. The model trained for 0.15M steps could achieve comparable performance compared to baseline models which are trained over 1M steps."}, {"title": "4.3 Sampling Robustness, Diversity, and Controllability", "content": "We utilize a flow matching model for PeriodWave, allowing it to generate diverse samples with different Gaussian noise. However, our goal is a conditional generation using the Mel-spectrogram. We need to decrease the diversity to improve the robustness of the model. To achieve this, we can multiply the small scale of temperature \u03c4 to the Gaussian noise during inference. Table 4 shows that using a \u03c4 of 0.667 could improve the performance. We also observed that samples generated with a \u03c4 of 1.0 contain a small amount of white noise, which decreases perceptual quality despite having the lowest lowest M-STFT metrics. Furthermore, we could control the energy for each band by using different scales of \u03c4. This approach could be utilized for a neural EQ that can generate the signal by reflecting the conditioned energy, not merely manipulating the energy of the generated samples."}, {"title": "4.4 MUSDB18-HQ: Multi-track Music Audio Dataset for Out-Of-Distribution Robustness", "content": "To evaluate the robustness on the out-of-distribution samples, we measure performance on the\nMUSDB18-HQ dataset that consists of multi-track music audio including vocals, drums, bass, others,\nand a mixture. We utilize all test samples including 50 songs with 5 tracks, and randomly sample\nthe 10-second segments for each sample. Table 5 shows our model has better performance on all metrics without M-STFT. Table 6 shows that PeriodWave-MB outperformed the baseline models by\nimproving the out-of-distribution robustness. Specifically, we significantly improve the performance\nof bass, the frequency range of which is known between 40 to 400 Hz. Additionally, we observed\nthat our model significantly reduces the jitter sound in the out-of-distribution samples."}, {"title": "4.5 Analysis on Adaptive Sampling Steps for Multi-Band Models", "content": "We proposed an adaptive sampling for multi-band models. We can efficiently reduce the sampling\nsteps for high-frequency bands due to the hierarchical band modeling conditioned on the previously\ngenerated DWT components. Table 7 shows that it is important to model the first DWT components.\nAfter sampling the first band, we can significantly reduce the sampling steps for the remaining\nbands, maintaining the performance with only a small decrease. The results from the sampling steps\nof [4,4,8,16] demonstrated that it is important to model the first band for high-fidelity waveform\ngeneration and accurate high-frequency modeling could improve the M-STFT metrics."}, {"title": "4.6 Ablation Study", "content": "Different Periods We conduct ablation study for different periods at the same structure. Table 8\nshows that the model with a period of 1 shows the lowest performance. Increasing the number of\nperiods could improve the entire performance in terms of most metrics, consistently. However, this"}, {"title": "4.7 Single Speaker Text-to-Speech", "content": "We conduct two-stage TTS experiments\nto evaluate the robustness of the proposed\nmodels compared to previous GAN-based\nand diffusion-based models. We utilized\nthe official implementation of Glow-TTS\nwhich is trained with the LJSpeech dataset.\nTable 9 demonstrated that our model has a\nhigher performance on the two-stage TTS\nin terms of MOS and UTMOS. Although\nHiFi-GAN shows a lower performance in\nreconstruction metrics, we observed that\nHiFi-GAN shows a high perceptual perfor"}, {"title": "4.8 Multi Speaker Text-to-Speech", "content": "We additionally conduct two-stage multi-speaker TTS experiments to further demonstrate the robustness of the proposed models compared to previous large-scale GAN-based models including BigVGAN and BigVSAN [Shibuya et al., 2024]. Note that BigVGAN and BigVSAN were trained for 5M and 10M steps, respectively. We utilize ARDIT-TTS [Liu et al., 2024] as zero-shot TTS model which was trained with LibriTTS dataset. We convert 500 samples of generated Mel-spectrogram into waveform signal by each model. The Table 10 shows that our model has better performance on the objective and subjective metrics in terms of UTMOS and MOS. Furthermore, Our model with FreeU has much better performance than others. We can discuss that FreeU could reduce the high-frequency noise resulting in better perceptual quality."}, {"title": "5 Broader Impact and Limitation", "content": "Practical Application We first introduce a high-fidelity waveform generation model using flow matching. We demonstrated the out-of-distribution robustness of our model, and this means that the conventional neural vocoder can be replaced with our model. We see that our models can be utilized for text-to-speech, voice conversion, audio generation, and speech language models for high-quality waveform decoding. For future work, we will train and release Codec-based PeriodWave for audio generation and speech language models.\nSocial Negative Impact Recently, speech AI technology has shown its practical applicability by synthesizing much more realistic audio. Unfortunately, this also increases the risk of the potential social negative impact including malicious use and ethical issues by deceiving people. It is important to discuss a countermeasure that can address these potential negative impacts such as fake audio detection, anti-spoofing techniques, and audio watermark generation.\nLimitation Although our models could generate the waveform with small sampling steps, Table\nE shows that our models have a slow synthesis speed compared to GAN-based neural vocoder.\nTo overcome this issue, we will explore distillation methods or adversarial training to reduce the\nsampling steps for much more fast inference by using our period-aware structure. Additionally, our\nmodels still show a lack of robustness in terms of high-frequency information because we only train\nthe model by estimating the vector fields on the waveform resolution. Although we utilize multi-band\nmodeling to reduce this issue, we have a plan to add a modified spectral objective function and blocks\nthat can reflect the spectral representations when estimating vector fields by utilizing short-time\nFourier convolution proposed in [Han and Lee, 2022] for audio super-resolution. Moreover, we see\nthat classifier-free guidance could be adapted to our model to improve the audio quality."}, {"title": "6 Conclusion", "content": "In this work, we proposed PeriodWave, a novel universal waveform generation model with conditional flow matching. Motivated by the multiple periodic characteristics of high-resolution waveform signals, we introduce the period-aware flow matching estimators which can reflect different implicit periodic representations when estimating vector fields. Furthermore, we observed that increasing the number of periods can improve the performance, and we introduce a period-conditional universal estimator for efficient structure. By adopting this, we also implement a period-wise batch inference for efficient inference. The experimental results demonstrate the superiority of our model in high-quality waveform generation and OOD robustness. GAN-based models still hold great potential and have shown strong performance but require multiple loss functions, resulting in complex training and long training times. On the other hand, we introduced a new flow matching based approach using a single loss function, which offers a notable advantage. Furthermore, we see that the pre-trained flow matching generator could be utilized as a teacher model for distillation or fine-tuning. We hope that our approach will facilitate the study of waveform generation by reducing training time, so we will release all source code and checkpoints."}, {"title": "G Train-inference Mismatch Problem", "content": "Current two-stage Text-to-Speech (TTS) models consists of acoustic models and neural vocoder. Due\nto noisy Mel-spectrogram, these two-stage TTS models suffer from train-inference mismatch problem.\nAlthough one-step GAN-based neural vocoder could generate high-quality Mel-spectrogram, these\nmodels might generate the samples with a noisy sound due to train-inference mismatch problem.\nTo reduce this issue, HiFi-GAN [Kong et al., 2020] proposed the fine-tuning methods with the\ngenerated Mel-spectrogrm by teacher-forcing mode. [Lee et al., 2022a; Jang et al., 2021; Kaneko\net al., 2022; Kim et al., 2020; \u0141a\u0144cucki, 2021] followed this fine-tuning method to improve the\nperceptual quality of two-stage TTS model.\nMeanwhile, end-to-end TTS models [Kim et al., 2021; Lim et al., 2022] outperformed the performance\ncompared to two-stage models in terms of audio quality. They have a limitation of model architecture\nrestriction to align high-resolution waveform signal and text, and they require more training times.\nAdditionally, recent end-to-end TTS models showed lower zero-shot TTS performance than recent\ntwo-stage TTS models including VoiceBox [Le et al., 2024], P-Flow [Kim et al., 2024], E2-TTS\n[Eskimez et al., 2024], ARDIT-TTS [Liu et al., 2024], and DiTTO-TTS [Lee et al., 2024].\nAlthough recent TTS models have shown their powerful performance on zero-shot TTS, there are\nstill train-inference mismatch problem which contains some noise on the generated Mel-spectrogram\nresulting noisy sound.\nTo address this issue, we shift our focus from one-step generation to the iterative sampling based\nwaveform generation. Following diffusion-based neural vocoder [Koizumi et al., 2023; Jang et al.,\n2023; Huang et al., 2022b; Koizumi et al., 2022; Roman et al., 2023], waveform generation with\niterative sampling could refine the waveform signal when the conditioning is flawed or imperfect.\nWe also adopt the iterative sampling methods by optimizing flow matching objective to reduce the\nsampling steps. The results also show that our models have shown better performance even with small\nsampling steps. Furthermore, our model has shown the best performance on two-stage text-to-speech\nscenarios by iterative sampling."}]}