{"title": "Self-supervised Preference Optimization:\nEnhance Your Language Model with Preference Degree Awareness", "authors": ["Jian Li", "Haojing Huang", "Yujia Zhang", "Pengfei Xu", "Xi Chen", "Rui Song", "Lida Shi", "Jingwen Wang", "Hao Xu"], "abstract": "Recently, there has been significant interest\nin replacing the reward model in Reinforce-\nment Learning with Human Feedback (RLHF)\nmethods for Large Language Models (LLMs),\nsuch as Direct Preference Optimization (DPO)\nand its variants. These approaches commonly\nuse a binary cross-entropy mechanism on pair-\nwise samples, i.e., minimizing and maximiz-\ning the loss based on preferred or dis-preferred\nresponses, respectively. However, while this\ntraining strategy omits the reward model, it\nalso overlooks the varying preference degrees\nwithin different responses. We hypothesize\nthat this is a key factor hindering LLMs from\nsufficiently understanding human preferences.\nTo address this problem, we propose a novel\nSelf-supervised Preference Optimization (SPO)\nframework, which constructs a self-supervised\npreference degree loss combined with the align-\nment loss, thereby helping LLMs improve their\nability to understand the degree of preference.\nExtensive experiments are conducted on two\nwidely used datasets of different tasks. The\nresults demonstrate that SPO can be seam-\nlessly integrated with existing preference opti-\nmization methods and significantly boost their\nperformance to achieve state-of-the-art perfor-\nmance. We also conduct detailed analyses to\noffer comprehensive insights into SPO, which\nverifies its effectiveness.", "sections": [{"title": "1 Introduction", "content": "The alignment of Large Language Models (LLMs)\nwith human preferences is paramount, as it ensures\nthat the outputs of LLMs are congruent with human\nvalues and ethical standards (B\u00f6hm et al., 2019;\nPerez et al., 2019; Ziegler et al., 2019). Through\nmeticulous tuning and ongoing learning of human\npreferences, LLMs can more accurately meet user\nneeds while avoiding the generation of harmful or\nbiased content (Stiennon et al., 2020b; Lee et al.,\n2024). Effective preference alignment not only en-\nhances the applicability and safety of LLMs but\nalso constitutes a critical step towards the responsi-\nble utilization of artificial intelligence.\nTo achieve human preference alignment of\nLLMs, a variety of methods have been developed.\nOne prominent approach is Reinforcement Learn-\ning from Human Feedback (RLHF) (Stiennon et al.,\n2020b; Bai et al., 2022), such as Proximal Pol-\nicy Optimization (PPO) (Schulman et al., 2017),\nREINFORCE (Williams, 1992) and their variants\n(Ramamurthy et al., 2023), which involve train-\ning reward models to optimize for objectives that\nare iteratively refined based on human feedback.\nHowever, these methods introduce increased com-\nplexity into the training process, involving training\nmultiple models and sampling from the LLM in\nthe loop of training (Ethayarajh et al., 2024; Yuan\net al., 2024). To streamline this process, recent\nworks have proposed alternative solutions to rein-\nforcement learning (Liu et al., 2023a; Zhao et al.,\n2023; Ethayarajh et al., 2024; Azar et al., 2023).\nDPO (Rafailov et al., 2023) and its variants (Wang\net al., 2023; Song et al., 2024; Ethayarajh et al.,\n2024; Azar et al., 2024; Amini et al., 2024; Meng\net al., 2024; Yu et al., 2024) directly leverage pair-\nwise responses to imbue the model with preference\nknowledge without a reward function. These meth-\nods achieve preference alignment by minimizing or\nmaximizing the loss between each token in the lan-\nguage model's output and the tokens that are either\npreferred or not preferred. However, this train-\ning strategy overlooks a crucial aspect of a reward\nmodel: its ability to differentiate between varying\ndegrees of human preferences in responses. We\nhypothesize that this is a key factor that prevents\nLLMs from fully understanding human preferences\nin those RLHF methods without a reward model.\nTo address this issue, we propose a novel Self-"}, {"title": "2 Method", "content": "In this section, we initially examine the pipeline of\nmethods alternative to RLHF, with a primary focus\non pairwise approaches that do not incorporate a\nreward model. Subsequently, we present the Self-\nsupervised Preference Optimization (SPO), aimed\nat assisting LLMs in learning preference degrees at\na fine-grained level."}, {"title": "2.1 Preliminaries", "content": "Methods alternative to RLHF generally avoid the\nprocess of learning rewards and consist of two\nstages: supervised fine-tuning (SFT) and prefer-\nence optimization. These stages have seen exten-\nsive application in later research (Zhao et al., 2023;\nEthayarajh et al., 2024).\nSFT phase: To tap into the capabilities of LLMs\nfor particular tasks (e.g., summarization and dia-\nlogue), it is common practice to fine-tune a gener-\nically pre-trained LLM using supervised learning\non a carefully curated dataset.\nPreference optimization phase: The RLHF\nmethods without a reward model typically start by\ngathering a pair of preferred $y_w$ and dispreferred\n$y_l$ responses for each prompt $x$. In the optimiza-\ntion process, these methods aim to make the LLM\n$\\pi_\\theta$ (initialized from the SFT model) produce a re-\nsponse that aligns more closely with $y_w$ and less\nso with $y_l$. To achieve this, the prompt $x$ is con-\ncatenated with both $y_w$ and $y_l$ separately as inputs,\nwhich are then fed into $\\pi_\\theta$ to generate predictions.\nThese predictions are subsequently assessed by cal-\nculating the loss between them and $y_w$ as well as\n$y_l$. This loss is typically measured using the cross-\nentropy between each predicted token and its corre-\nsponding target token in the responses, as follows:\n$\\pi_{\\theta}(y_{\\epsilon}|x) = \\frac{1}{K_{\\epsilon}}\\sum_{i=1}^{K_{\\epsilon}} -log P_{\\theta}(y_{\\epsilon}^{(i)}|x, y_{\\epsilon}^{<i})$ (1)\nwhere $\\epsilon \\in {w,l}$ and $K_{\\epsilon}$ denotes the number of\ntokens in $y_{\\epsilon}$, and $P_{\\theta}(y_{\\epsilon}^{(i)}|x, y_{\\epsilon}^{<i})$ signifies the pre-\ndicted probability of the $i^{th}$ target token in $y_{\\epsilon}$. The\nRLHF approach without a reward model primarily\nfocuses on decreasing and increasing $\\pi_{\\theta}(y_{w}|x)$ and\n$\\pi_{\\theta}(y_{l}|x)$, respectively. Additionally, these methods\nemploy a reference model $\\pi_{ref}$ (e.g., a frozen SFT\nmodel) to mitigate deviation throughout the op-\ntimization process. Here, inputs are concurrently\nprovided to $\\pi_{ref}$ to calculate the corresponding loss\n$\\pi_{ref}(y_{\\epsilon}|x)$. Based on these losses, such methods\nachieve their goal by the following loss function:\n$L_{DPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x,y_w,y_l)~D} [log\\sigma(\\beta(log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - log\\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}))]$ (2)\nwhere $\\sigma(\\cdot)$ denotes a logistic function, such as the\nsigmoid function. The parameter $\\beta$ regulates the\nextent of deviation from $\\pi_{ref}$. While the specific\noperations employed by these methods vary, their\ncore focus uniformly centers on $\\pi_{\\theta}(y_{\\epsilon}|x)$ (Etha-\nyarajh et al., 2024; Azar et al., 2023). A more\ncomprehensive discussion on alternative methods\nto RLHF is presented in Appendix A."}, {"title": "2.2 Self-supervised Preference Optimization", "content": "To grasp the degree of preference, we propose a\nstraightforward Self-supervised Preference Opti-\nmization (SPO) method, which consists of prefer-\nence extraction and self-supervised classification."}, {"title": "2.2.1 Preference Extraction and Removing", "content": "To facilitate the learning of preference degrees by\nLLMs, it is essential to provide them with a series\nof responses with different preference levels. To\nachieve this objective, existing methods commonly\nrely on generating multiple responses through one\nor more LLMs, subsequently employing manual ef-\nforts to annotate or rank these responses according\nto their preference levels (Stiennon et al., 2020a;\nZhao et al., 2023). This process undeniably leads\nto an increase in both human labour and training\nexpenses. To this end, we propose a novel and\nsimple method for constructing preference data by\nextracting and removing key content from predic-\ntions of LLMs. From a semantic perspective, a sen-\ntence commonly contains key and additional con-\ntent, where the former primarily dictates whether\nthe sentence meets human preferences. Meanwhile,\nour experiments (described in Subsection 4.2) re-\nveal a close correlation between key content and\npreference information, indicating that adjusting\nthe key content effectively modulates the degree of\npreference. Consequently, we try to extract the key\ncontent and gradually remove them to construct\ndifferent responses. Specifically, during training,\nwe decode all tokens predicted by LLMs into the\ncorresponding text and then employ the Rapid Au-\ntomatic Keyword Extraction (RAKE) (Rose et al.,\n2010a) to pinpoint the key content within the text.\nRAKE is an efficient, unsupervised method for the\nextraction of keywords from individual documents.\nIt operates on a simple premise: keywords are typ-\nically content-bearing phrases that exclude com-\nmon stop words and punctuation. The algorithm\nsegments the document into candidate keywords k\nand computes a score $S_k$ for each as follows:\n$S_k = \\sum_{w \\in k} \\frac{deg(w)}{freq(w)}$ (3)\nwhere $deg(w)$ is the degree of the word, represent-\ning its co-occurrence with other words within the\ncandidate keyword, and $freq(w)$ is the frequency\nof the word in the document. The candidate key-\nwords with the highest scores are selected as the\nfinal keywords, providing a compact representation"}, {"title": "2.2.2 Self-Supervised Classification Modules", "content": "To enhance LLMs' understanding of preference de-\ngrees, we introduce an innovative self-supervised\npreference classification module that improves pref-\nerence awareness without incurring any additional\nlabeling costs. Specifically, we first construct sam-\nples (using both preferred and dispreferred ground\ntruth responses) with different preference degrees\nusing our method in 2.2.1. The constructed samples\nare then fed into the self-supervised preference clas-\nsification module to compute the preference clas-\nsification loss, which is backpropagated together\nwith the original DPO loss. The detailed architec-\nture and operational processes of these modules are\noutlined below.\nAfter extracting and removing key content from\nthe predictions, we identify the corresponding to-\nkens and hidden states of the remaining content.\nTo help self-supervised classifier understand pref-\nerence better, we propose to augment these hidden\nstates $H = {h_1, h_2, ..., h_r}$ from the last layer of\nLLMs with positional encoding before being fed\ninto a Multilayer Perceptrons (MLP) (LeCun et al.,\n2015), which can be defined as follows:\n$H_{pos} = H + P$ (4)\nwhere $H_{pos}$ is the positionally encoded hidden\nstates. Following (Devlin et al., 2019), the posi-\ntional encoding $P$ can be computed as follows:\n$P_{(pos,2i)} = sin(\\frac{pos}{10000^{2i/d}})$ (5)\n$P_{(pos,2i+1)} = cos(\\frac{pos}{10000^{2i/d}})$ where pos denotes the position of a token (hidden\nstate) in the sequence, $i$ for the dimension within\nthe positional encoding, and d as the size of the\nencoding vector. Subsequently, the hidden states\n$H_{pos}$ are fed into a projection layer following the\ndesign of (Chen et al., 2020a; He et al., 2020; Grill\net al., 2020) which outputs prediction probabilities\np for N classes. The classification loss can be\ncomputed as follows:\n$loss = - \\sum_{i=1}^{N} y_i log P_i$ (6)\nwhere y represents the predefined self-supervised\nlabel based on one-hot encoding. Considering the\nimplementation of two self-supervised modules,\ntwo classification losses are derived and then inte-\ngrated with the main loss (e.g., $L_{DPO}$) as follows:\n$Loss = L_{DPO} + \\gamma * (loss_{pref} + loss_{dispref})$ (7)\nwhere $\\gamma$ is a hyperparamter for scaling the classifi-\ncation losses $loss_{pref}$ and $loss_{dispref}$ from prefer-\nence and dispreference modules, respectively."}, {"title": "3 Experiment", "content": "3.1 Settings\nDatasets. In our experiments, two datasets de-\nsigned for summarization and dialogue tasks are\nintroduced, and LLMs are optimized using vari-\nous alignment methods on the preference dataset\n$D = \\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\}_{i=1}^N$. For the summarization\ntask, the input $x$ denotes a forum post from Red-\ndit\u00b9, and the LLMs are tasked with generating a\nsuccinct summary $y$ that captures the essence of\nthe post. Following prior works (Rafailov et al.,\n2023), the Reddit TL;DR dataset (V\u00f6lske et al.,\n2017) along with human preferences gathered by\nStiennon et al. (2020a) is employed. In the dialogue\ntask, $x$ represents a human query, and LLMs need\nto produce an engaging and informative response $y$.\nThe Antropic HH dataset (Bai et al., 2022) is uti-\nlized, containing 170k dialogues between humans\nand automated assistants.\nCompared Methods. To evaluate the efficacy\nof SPO in enhancing preference alignment, we\nextensively apply SPO to diverse existing meth-\nods (i.e., DPO (Rafailov et al., 2023), IPO (Azar\net al., 2023), KTO (Ethayarajh et al., 2024)), as\nwell as across different models, including Mistral-\n7B, LLaMA-7/13B and LLaMA3-8B. Furthermore,"}, {"title": "3.2 Main Result", "content": "The results of the proposed SPO applied to exist-\ning alignment methods are shown in Table 1. The\nresults clearly demonstrate that SPO successfully\nimproves the performance of all methods across\nboth datasets. On TL;DR summarization dataset,"}, {"title": "4 Analysis", "content": "4.1 Constructing Self-supervised Responses\nOur objective is to inject preference degrees into\nLLMs in a simple and efficient manner during the\nalignment process. To this end, the removal of spe-\ncific content from predictions is proposed to effec-\ntively convey preference information. We hypothe-\nsize that different clauses or sub-words within the\npredictions contribute to preference degrees. By se-\nlectively removing certain elements, the preference\nlevels can be altered accordingly. To validate this\nhypothesis, two strategies are explored: random\nremoval and removal of key content. The results,\npresented in Table 3, demonstrate that both strate-\ngies yield performance improvements, suggesting\nthat the model has successfully learned to represent\npreference levels. Notably, the key content extrac-\ntion method outperforms random deletion in iden-\ntifying content that significantly influences pref-\nerence levels, thereby facilitating the construction\nof self-supervised responses with greater prefer-"}, {"title": "4.2 Analysis of Adjusting Key Content", "content": "In this work, we extract key content from LLMs'\npredictions and then incrementally remove them\nto construct responses with varying preference de-\ngrees. To demonstrate its rationality, we first train\ntwo reward models initialized by LLaMA-7B on\nAntropic HH and TL;DR datasets, respectively, and\nfurther randomly sample 1,000 instances from each\nof these datasets. Following this, we extract their\nkey content and sequentially remove 1-3 key ele-\nments from them to create four subsets with dif-\nferent preference intensities. The reward model is\nthen employed to compute the average scores for\nthese sets. The average score and length of each\nset are shown in Figure 3. The experimental re-\nsults indicate that as the number of key elements\nremoved increases, the length of preference pairs\ngradually decreases. More importantly, the scores\nof preferred responses progressively decline, sug-\ngesting the preference information is being system-\natically eliminated. Conversely, the scores of dis-\npreferred responses exhibit an upward trend, as the\ndis-preferred information is being removed. These\nfindings demonstrate the extracted key content ac-\ncurately contains preference information and pro-\ngressively removing these elements can construct\nresponses with different preference intensities."}, {"title": "4.3 Analysis of Extracting Methods", "content": "To identify an appropriate method for key content\nextraction, we investigate various extraction tech-\nniques (i.e., YAKE (Campos et al., 2020), RAKE\n(Rose et al., 2010b) and PositionRANK (Florescu\nand Caragea, 2017)) with DPO+SPO. The experi-"}, {"title": "4.4 Self-supervised Classification Number", "content": "The classification number N serves as a crucial\nhyperparameter within the self-supervised module.\nThis study evaluates the impact of different N on\nthe performance of LLaMA-7B and 13B on the\nTL;DR dataset. As illustrated in Figure 4, employ-\ning various values of N consistently outperforms\nthe baseline (i.e., LLaMA-7/13B with DPO), un-\nderscoring our method's efficacy. Specifically, the\nLLaMA-13B exhibits optimal performance with\nN of 5, whereas further increasing the value of N\nnegatively affects performance. This trend suggests\nthat a bigger N complicates the classification task,\nthereby hindering effective learning. Similarly, the\nLLaMA-7B achieves its peak performance with N\nof 6. These findings suggest choosing the number\nN around 5 is a favourable option for alignment."}, {"title": "4.5 The Weight of Self-supervised Loss", "content": "This study investigates the impact of weights $\\gamma$\nas defined in Equation 7 on the performance of\nLLaMA-7/13B using the TL;DR dataset. The find-"}, {"title": "4.6 Analysis of Two Self-supervised Modules", "content": "In this work, we introduce two separate modules\nfor preferred and dis-preferred predictions, respec-\ntively. To validate the combined efficacy of the\nmodules, we additionally assess the impact of uti-\nlizing a single module for either preferred or dis-\npreferred prediction. As shown in Table 4, the"}, {"title": "4.7 Accuracy of Self-supervised Classification", "content": "To assess whether the self-supervised modules\nfunction as intended, we evaluate their classifica-\ntion accuracy with KTO+SPO for Mistral-7B on\nthe TL;DR dataset, as shown in Figure 6. Within\nthe first 1,000 steps, a significant upward trend\nin accuracy is observed, demonstrating that self-\nsupervised modules can learn information related\nto preference intensity, thereby achieving precise\nclassification. Subsequently, the accuracy of both\nmodules stabilizes at over 90%. This consistently\nhigh performance highlights the modules' ability"}, {"title": "5 Related Work", "content": "5.1 Aligning LLMs with Human Preferences\nPreference alignment commonly begins with train-\ning a reward model on a preference dataset and\nfurther fine-tunes LLMs to maximize the identified\nreward by reinforcement learning, such as Proxi-\nmal Policy Optimization (PPO) (Schulman et al.,\n2017), REINFORCE (Williams, 1992) and their\nvariants (Ramamurthy et al., 2023). Although these\nmethods effectively incorporate preference infor-\nmation into LLMs, they significantly complicate\nthe training process in view of training multiple\nmodels and sampling from the LLM within the\ntraining loop (Ethayarajh et al., 2024; Yuan et al.,\n2024). Following this, various methods have been\nproposed to streamline this process. For exam-\nple, DPO (Rafailov et al., 2023) bypasses the re-\nward function to optimize LLMs by maximizing\nthe difference between preferred and dispreferred\nresponses. KTO (Ethayarajh et al., 2024) stream-\nlines the creation of preference pairs by optimizing\nthe loss computation, eliminating the need for strict\npairing between prompts and their preferred and\ndispreferred sequences. RSO (Liu et al., 2023a)\nsuggests obtaining preference data from the esti-\nmated target optimal policy through rejection sam-\npling in an offline manner. SimPO (Meng et al.,\n2024) utilizes the average log probability of a se-\nquence as an implicit reward and eliminates the\nneed for a reference model, making it more com-\npute and memory efficient.\nWhile these methods show impressive perfor-\nmance, they overlook the degree of preference un-\nder a binary cross-entropy mechanism, which limits\nLLMs' ability to fully understand human prefer-\nences. In this work, we introduce a novel SPO\nframework to enhance LLMs' ability to learn hu-\nman preference degrees in direct preference opti-\nmization methods, thereby improving their under-\nstanding capabilities of LLMs."}, {"title": "5.2 Self-Supervised Learning", "content": "Self-Supervised Learning (SSL) has emerged as a\npowerful paradigm for leveraging unlabeled data to\nlearn useful representations without explicit super-\nvision (Liu et al., 2023b; Liang et al., 2023; Yuan\net al., 2023; Zhang et al., 2022). The foundational\nwork of self-supervised learning can be traced back\nto the idea of using auxiliary tasks for which data it-\nself provides supervision. Dosovitskiy et al. (2014)\nintroduces a novel approach where neural networks\nwere trained to predict parts of the data given other\nparts, effectively learning representations without\nlabelled data. This concept is further explored by\nNoroozi and Favaro (2016), who demonstrate that\nsolving jigsaw puzzles as a pretext task could sig-\nnificantly improve feature learning. Following this\nline of thought, important self-supervised methods\nhave emerged like mushrooms after rain and have\nhad a profound impact on the field of deep learning\nresearch (van den Oord et al., 2018; Chen et al.,\n2020b, 2021; Grill et al., 2020; Khosla et al., 2020;\nHe et al., 2022).\nWe integrate SSL into RLHF by leveraging self-\nsupervised auxiliary tasks for the first time to en-\nhance the comprehension abilities of LLMs."}, {"title": "6 Conclusion", "content": "In this work, we first identify a gap in alterna-\ntive methods to RLHF, which overlooks the learn-\ning of preference degrees. To this end, we intro-\nduce a novel self-supervised preference optimiza-\ntion framework that integrates fine-grained human\npreference information into large language mod-\nels (LLMs), thereby enhancing the understanding\nof human preferences. This approach does not re-\nquire additional manual annotation and inference\noverhead. The proposed SPO can extract key con-\ntent from the prediction of LLMs and selectively\nremove the content to construct responses with\nvarying preference intensity. Subsequently, these\nresponses are classified by the self-supervised mod-\nules and their losses are integrated with the align-\nment loss to jointly optimize LLMs. Extensive\nexperiments and analyses fully demonstrate the ef-\nfectiveness of our SPO."}, {"title": "Limitations", "content": "It would exist two limitations in this work. Firstly,\nthe proposed SPO involves two hyperparameters $\\gamma$\nand N, for which the optimal settings vary across\ndifferent methods and datasets, thereby undermin-\ning the convenience of SPO. In future work, we\nwill explore adaptive hyperparameter tunning to\ntackle this issue. Furthermore, this work constructs\nresponses with varying preference degrees by re-\nmoving key content from predictions, which may\ncompromise their semantic coherence. Although"}, {"title": "Ethics Statement", "content": "While conducting our research on Self-supervised\nPreference Optimization (SPO), we are keenly\naware of our ethical duties, including the preven-\ntion of misinformation and the protection of data\nprivacy. The datasets in our experiments are all de-\nrived from publicly available information and we\nguarantee that we strictly adhere to the data usage\npolicies outlined in the public datasets. In terms of\nself-supervised data construction, we ensure that\nno personal data is introduced, no manual labelling\nis involved, and we strictly adhere to privacy and\ndata protection standards. In the experiments, we\nfollowed the evaluation methods in (Rafailov et al.,\n2023; Ethayarajh et al., 2024), using OpenAI APIs\nand strictly adhering to OpenAI's ethical and pri-\nvacy protection guidelines."}, {"title": "A Alternative Methods to RLHF", "content": "A.1 Direct Preference Optimization\nThe Direct Preference Optimization (DPO) method\ncomputes the losses associated with preferred (or\ndispreferred) responses by summing up the cross-\nentropy of each token in the preference answers\nalongside the matching token produced by LLMs,\nas described below:\n$L_{DPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x,y_w,y_l)~D} [log\\sigma(\\beta(log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - log\\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}))]$ (8)\nA.2 Sequence-Likelihood Calibration\nThe Sequence-Likelihood Calibration (SLiC) em-\nploys a margin to regulate the difference in loss\nbetween preferred and dispreferred responses, as\ndescribed below:\n$L_{cal}(\\pi_{\\theta}) = E_{x,y_w,y_l~D} [max(0, \\beta - log\\pi_{\\theta}(y_w|x) + log\\pi_{\\theta}(y_l|x)]$ (9)\nwhere $\\beta$denotes the margin ensuring that the log\nprobability of the preferred response surpass that\nof the dispreferred response by at least $\\beta$. Further-\nmore, SLiC includes a cross-entropy component\nfor responses generated by the reference model,\nwith the goal of minimizing substantial divergence\nfrom the reference model, as outlined below:\n$L_{SLiC}(\\pi_{\\theta}, \\pi_{ref}) = L_{cal}(\\pi_{\\theta}) + \\lambda E_{x~D,y~\\pi_{ref}(x)} [-log \\pi_{\\theta}(y|x)]$ (10)\nA.3 Kahneman-Tversky Optimization\nThe Kahneman-Tversky Optimization (KTO)\nmethod posits that pairs of preferences might be\nunnecessary and advocates for the direct maximiza-\ntion of utility derived from LLMs outputs, rather\nthan focusing on maximizing the log-likelihood of\npreferences, as described below:\n$L_{KTO} = E_{(x,y)~D}[W(y)(1 - h(x, y; \\beta))]$ (11)"}, {"title": "B Gpt-4 prompts for Evaluating", "content": "In our work, GPT-4 serves as a surrogate for as-\nsessing the responses from LLMs in comparison\nto the baselines from the Antropic-HH dialogue\nand TL;DR summarization datasets. This section\noutlines the prompts utilized to derive win rates for\nour experimental analysis."}, {"title": "B.1 Prompt for Summarization", "content": "Following (Rafailov et al., 2023), we use the fol-\nlowing prompt to evaluate the answers of LLMs:\nWhich of the following summaries does a better\njob of summarizing the most import points in the\ngiven forum post, without including unimportant or\nirrelevant details? A good summary is both precise\nand concise.\nPost: <post>\nSummary A: <Summary A>\nSummary B: <Summary B>\nFIRST provides a one-sentence comparison of\nthe two summaries, explaining which you prefer\nand why. SECOND, on a new line, state only \"A\" or\n\"B\" to indicate your choice. Your response should\nuse the format: Comparison: <one-sentence com-"}, {"title": "B.2 Prompt for Dialogue", "content": "Here is a conversation between Human and Assis-\ntant.\nConversation: <Conversation>\nAssistant can now choose from different re-\nsponses.\nResponse 1: <Response 1>\nResponse 2: <Response 2>\nBetween these two responses, which response is\nmore helpful, harmless, and concise? Please only\nanswer RESPONSE 1 or RESPONSE 2."}, {"title": "B.3 Impact of Data Volume and Update Steps", "content": "To address the potential impact of preference data\ndifferences and update steps on performance, we\nconducted additional experiments. We explored the\neffects of varying data volumes and update steps\non model performance using LLaMA-7B on the\nTLDR dataset."}, {"title": "B.4 Impact of Different Module of\nSelf-Supervised Classcification Module", "content": "In our self-supervised training, we utilized a classi-\nfication model with a two-layer MLP and positional\nencoding. This design is based on two hypotheses:\n\u2022 Compared to directly inputting embeddings\ninto the classification head, using a two-layer\nMLP helps mitigate the negative impact of\nself-supervised loss on the embedding distri-\nbution, thereby improving the effectiveness of\nthe self-supervised embeddings.\n\u2022 We hypothesized that the method of keyword\ndeletion might lead to semantic discontinuity,\ncausing the model to struggle with learning\npreferences effectively. Therefore, we added\nthe original positional encoding to the latent\nembeddings, hoping that the model could bet-\nter learn preferences."}, {"title": "C Cases of different extracting method", "content": "We employ various extraction techniques ((i.e.,\nYAKE (Campos et al., 2020), RAKE (Rose et al.,\n2010b) and PositionRANK (Florescu and Caragea,\n2017))) to identify key content on HH dataset, with\nillustrative examples provided below.\n\u2022 Raw response: \"I'm sorry, this doesn't seem\nlike the kind of thing I'm built to handle. Can\nyou explain to me more what you mean? Is it\nreally that loud?\"\n\u2022 RAKE: Key content: [\"Can you explain to\nme more what you mean\", \"doesn't seem like\nthe kind of thing I'm built\", \"Is it really that\nloud\"]\n\u2022 YAKE: Key content: [\"kind of thing I built to\nhandle\", \"built to handle\", \"kind of thing\"]\n\u2022 PositionRank: Key content: [\"kind\", \"thing\",\n\"handle\"]\nBased on the above samples, we can see that\nRAKE tends to extract more continuous key con-\ntent while YAKE and PositionRANK generate\nsparse key contents."}]}