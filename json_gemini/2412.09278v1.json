{"title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine", "authors": ["Xiaoshuang Huang", "Lingdong Shen", "Jia Liu", "Fangxin Shang", "Hongxiang Li", "Haifeng Huang", "Yehui Yang"], "abstract": "In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at https://github.com/ShawnHuang497/MedPLIB.", "sections": [{"title": "Introduction", "content": "Owing to their impressive capabilities in image understanding and text generation, models such as GPT-4V and LLaVA (Liu et al. 2024) within the realm of Multimodal Large Language Models (MLLMs) have garnered widespread research interest from both academic and industrial sectors (Chen et al. 2023; You et al. 2023). Numerous researchers have dedicated efforts to explore the potential applications of MLLMs in the biomedical field (Wu et al. 2023; Zhang et al. 2023; Moor et al. 2023), including LLaVA-Med (Li et al. 2024) and Med-PaLM M (Tu et al. 2024). MLLMs not only generate high-quality responses but also analyze biomedical imagery, demonstrating significant potential to transform traditional medical paradigms (Li et al. 2024; He et al. 2024). For doctors, such chatbots could significantly alleviate their heavy workloads and enhance efficiency. For patients, it provides more convenient access to professional medical knowledge and advice (Liu et al. 2023a). Additionally, this could also help alleviate the uneven distribution of medical resources, particularly in regions where they are scarce.\nUnlike the image-level VQA of MLLMs in the natural world, the medical domain requires a finer-grained pixel-level understanding to ensure accuracy and answer interpretability. However, existing medical MLLMs (Li et al. 2024; Wu et al. 2023; Moor et al. 2023; Tu et al. 2024) capacity remains restricted to image-level understanding, falling short of pixel-level perceptions. Pixel-level MLLMS offer several advantages over image-level perception: Firstly, they are capable of recognizing and processing detailed information by focusing on each pixel within an image, such as small lesions or subtle changes in tissue structures. Secondly, they facilitate improved structural recognition and pixel grounding. Pixel-level perception provides more precise grounding outcomes, aiding physicians in better understanding pathological images and formulating treatment plans. Thirdly, pixel-level analysis enables models to understand contextual information on a finer scale. Overall, pixel-level perception is particularly vital in multimodal large models within the medical field, especially in applications requiring high accuracy and detailed recognition.\nPixel-level MLLMs in biomedicine currently face significant challenges due to these two issues: (1) Data scarcity: Owing to privacy regulations and the high cost of labeling, there is a severe scarcity of pixel-level and complex VQA data. Openly available VQA datasets are typically designed for image-level multiple-choice questions or simple question-and-answer formats, lacking potential for pixel"}, {"title": "Method", "content": "The overall framework of MedPLIB comprises three structural layers: the encoder, the MoE LLM, and the decoder, as illustrated in Figure 2.\nEncoder The encoding module aims to encode all types of inputs (image, text, visual prompt) into a unified feature space.\nVision Tower and Pixel Encoder. Given the input image $v \\in \\mathbb{R}^{H\\times W\\times 3}$, we utilize the pre-trained CLIP visual encoder CLIP-ViT-L/336 (Radford et al. 2021) with a vision-projector as vision tower to extract the feature $V \\in \\mathbb{R}^{C_{llm}\\times N_v}$, where $N = \\frac{H}{14} \\times \\frac{W}{14}$ and C is the hidden size of vision tower. Then project it as $V \\in \\mathbb{R}^{C_{llm}\\times N_v}$, where $C_{llm}$ is the hidden size in the MoE LLM. Similarly, we employ a pre-trained Visual Transformer (ViT) (Dosovitskiy et al. 2020) with medical adapter layers (Cheng et al. 2023) as pixel encoder to get the pixel features $V, \\in \\mathbb{R}^{C_p\\times N_v}$, where $C_p$ denotes the hidden size in the pixel decoder.\nVision Prompt Encoder. This block aims to appropriately prompt the LLM with user-specified areas of interest (boxes, points, free shapes) as inputs. Inspired by SEEM (Zou et al. 2024), we define a vision sampler to convert all types of non-textual queries into visual prompts that reside within the same visual embedding space. Assuming the area of interest input is $R$ and m is the sampling pixel number, the visual prompt features $V_{vp}$ can be formatted as $V_{vp} = MLP(\\phi(V,m))$, where $\\phi$ and MLP is the random sampling function and linear function.\nText Prompt Embedding. Inspire by LLaVA (Liu et al. 2024), we expand the tokenizer's vocabulary with \u201c<region>\" and \"</region>\u201d tokens to better integrate vi-"}, {"title": "Multi-stage Training", "content": "As shown in Figure 3, we present the multi-stage training strategy.\nStage I-Alignment: Following LLaVA-Med (Li et al. 2024) and LLaVA (Liu et al. 2024), we consider only the cross-entropy loss $L_{reg}$ for text responses during this stage.\nStage II-Recognition: We train the visual-language MLLM as a base model to create an MLLM proficient in medical knowledge and medical imagery understanding. In this stage, we tackle complex visual-language tasks, such as visual knowledge multiple-choice questions, intricate medical Q&A, and region-based visual question answering. Specifically, we use vast question-and-answer pairs to fine-tune all modules except the vision tower. Thus, we have obtained a MLLM enriched with extensive medical imaging knowledge, where the FFN can be designated as $E_{vl}$. Similar to stage I, the training objective is to minimize the loss $L_{reg}$.\nStage III-Grounding: To enhance the pixel grounding capability of the model, we focus on training the grounding expert in this stage. We use the model obtained from stage II as the initial model. We then train using the MeCoVQA-G dataset specifically targeting the FFN layer, pixel decoder, and T-projector. Ultimately, we achieve an MLLM equipped with pixel grounding knowledge, where the FFN is designated as $E_{ground}$. In this stage, we use binary cross-entropy $L_{bce}$ and dice loss $L_{dice}$ for pixel grounding losses, and $L_{reg}$ for the text responses associated with the \u201c<SEG>\" token.\nStage IV-Adaption: After completing stage II and stage III, we obtain the parameters for $E_{vl}$, $E_{ground}$, and other modules. We then mix all available data and unfreeze all parameters, employing LoRA for fine-tuning through expert mixing. Using the router $G(\u00b7)$, tokens are distributed to different experts for collaborative processing. This approach not only maintains minimal computational expenditure but also preserves the distinct prior knowledge of each expert. During mixed training, the optimization objective can be for-\""}, {"title": "MeCoVQA Dataset", "content": "Large models are increasingly used to generate high-quality data, addressing data scarcity. However, in medical imaging, open-source datasets for detailed question-and-answer interactions remain limited. These are vital for intelligent biomedical assistants who need to perform detailed medical analyses and interact with patients. We suggest a new strategy for creating such detailed interactive data. MeCoVQA was generated through the collaborative efforts of humans and an AI assistant, derived from large-scale biomedical image segmentation datasets. As shown in Figure 4, the generation process can be divided into three steps:\nI. Manually generating instance-level meta information for each image based on its mask. We randomly sampled 100k biomedical images with instance masks from the SAM-Med2D-20M (Ye et al. 2023a). Then we enrich the images with additional details to compile the meta information, which includes modality, scanned region, orientation, and object instances.\nII. We use an AI assistant to get global descriptions for images, adjusting prompts to produce 500 data points per modality, which are manually reviewed for quality. We finalize the prompts only when all points meet quality standards.\nIII. Utilizing the AI assistant to craft pixel-level conversations based on the meta information and global descriptions obtained in step II. At this step, we used complex instructions to generate diverse data, manually refining prompts multiple times, as in stage II, to ensure quality."}, {"title": "Experiments", "content": "Experimental Setup\nModel Settings. This project was conducted on Pytorch. We employ SAM-Med2D (Cheng et al. 2023) as the pixel encoder and mask decoder. We use LLaMA-7B (Touvron et al. 2023) as a base LLM. Following LLaVA 1.5 (Liu et al. 2024), we utilize CLIP-Large (Radford et al. 2021) as the vision tower and the MLP consists of two linear layers with GELU activation function (Hendrycks and Gimpel 2016). The parameters of the model with 2 experts are 12 Billion. We adopt 4 NVIDIA 40G A100 GPUs for training and the training scripts are based on the deepspeed engine. The training durations for stages I to IV are 9, 17, 15, and 77 hours, respectively. We provide additional model details in the Appendix.\nDatasets. For the training data in stage I, we employ LLaVA-Med-alignment (Li et al. 2024). We utilize the union of MeCoVQA-R, MeCoVQA-C, SLAKE (Liu et al. 2021), PathVQA (He 2021), PMC-VQA (Zhang et al. 2023), ImageClef2021 (Ben Abacha et al. 2021), ImageClef2019 (Abacha et al. 2019), and VQA-RAD (Lau et al."}, {"title": "Performance Evaluation", "content": "Performance on VQA Benchmark. OmniMedVQA (Hu et al. 2024) is a large medical VQA benchmark that utilizes single-choice questions. We present the evaluation results of the open-source portion of the OmniMedVQA benchmark in Table 1. Across seven modalities, MedPLIB leads the second-best model, BLIP-2 (Li et al. 2023a), by an advantage of 7.84 points in the mean performance. Additionally, our MedPLIB significantly outperforms other biomedical MLLMs. For more analysis of this table, please refer to the Appendix.\nComplex VQA Evaluation. Compared to test sets like OmniMedVQA (Hu et al. 2024), MeCoVQA-C features longer open-ended questions. As indicated in the third and fourth columns at the end of Table 1, our MedPLIB achieves better precision compared to LISA (Lai et al. 2024), but slightly lower recall. We believe this is due to MedPLIB balancing its pixel grounding capabilities, which slightly compromises its ability to handle long-text VQA tasks.\nRegion-level VQA Evaluation. Region-level VQA demands pixel-level image understanding. Current models lack support for region-specific prompts. To enable this, we integrate coordinates into the prompts for models like LLaVA (Liu et al. 2024), LLaVA-Med (Li et al. 2024), and LISA (Lai et al. 2024). As demonstrated in the last two columns of Table 1, our MedPLIB significantly outperforms these models.\nPixel Grounding Evaluation. Since there has not yet been a biomedical MLLM with pixel grounding capabilities, we compare our MedPLIB with small models that possess pixel grounding capabilities and with the influential LISA (Lai et al. 2024) from the general domain. As shown in the second column of Table 2, our MedPLIB surpasses LISA (Lai et al. 2024) by 5.37 points on the mDice met-"}, {"title": "Results and Discussion", "content": "Overall, the average performance of MoE across four datasets is 2.62 points higher than that of FFN, demonstrating MoE's adaptability to our tasks.\nEffect of Multi-stage Training. In Table 3, we conduct five variant experiments to demonstrate the rationale of our multi-stage tuning. Following LLaVA-Med (Li et al. 2024), we samely use stage I to align visual features to text embedding space. Variants (b) and (c) indicate that having alignment is more beneficial for fine-tuning in stage IV. Variants (c) and (d) demonstrate that using the $E_{vl}$ from stage II as the initial weights for stage IV helps the model focus more on VL tasks. Similarly, variants (c) and (e) show that using the $E_{ground}$ from stage III as the initial weights for stage IV help the model focus more on VL tasks. Lastly, the tuning strategy in stage IV as per variant (f), compared to variant (b), allows the model to better balance image-level and pixel-level tasks.\nEffect of Top-k. We explored the impact of using top-1 and top-2 routing on model performance. Utilizing top-2 in our experiments implies equivalence to a dense model (where each expert processes all tokens) since we are using only two experts. The last two rows of Table 4 indicate that a dense model is less effective than a sparse activated model.\nEffect of Capability Factor. We examined the impact of the Capacity Factor (CF). At CF=1, each expert handles up to half the tokens, risking information loss due to their prior knowledge. At CF=2, experts can process all tokens, leading to noise and redundancy. Empirical evidence suggests CF=1.5 is optimal, balancing the reduction of information loss and noise in token distribution."}, {"title": "Conclusion", "content": "In this paper, we present MedPLIB, a multimodal large language model with pixel-level insight for biomedicine. MedPLIB features flexible inputs and outputs, thereby supporting multiple tasks and creating a more versatile and patient-friendly MLLM. To achieve the mentioned targets, we have made efforts on both the model and data levels. On the model level, we introduce a three-layer architecture and a novel MoE training strategy within MLLMs that incorporates expert prior knowledge. On the data level, we introduce the MeCoVQA, which comprises an array of 8 modalities for answering complex medical imaging questions, understanding image regions, and pixel grounding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes on the OmniMedVQA benchmark and MeCoVQA test sets. Moreover, MedPLIB has demonstrated encouraging performance in its zero-shot ability for pixel-level grounding."}]}