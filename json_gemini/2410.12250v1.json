{"title": "Dual Action Policy for Robust Sim-to-Real Reinforcement Learning", "authors": ["Ng Wen Zheng Terence", "Chen Jianda"], "abstract": "This paper presents Dual Action Policy (DAP), a novel approach to address the dynamics mismatch inherent in the sim-to-real gap of reinforcement learning. DAP uses a single policy to predict two sets of actions: one for maximizing task rewards in simulation and another specifically for domain adaptation via reward adjustments. This decoupling makes it easier to maximize the overall reward in the source domain during training. Additionally, DAP incorporates uncertainty-based exploration during training to enhance agent robustness. Experimental results demonstrate DAP's effectiveness in bridging the sim-to-real gap, outperforming baselines on challenging tasks in simulation, and further improvement is achieved by incorporating uncertainty estimation.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) has revolutionized the way we train intelligent agents. Conventional RL involves real-world interactions, which is expensive, time-consuming, and even dangerous. The use of simulators is emerging as a powerful alternative, offering numerous benefits for RL training [10, 19]. Simulators provide a safe, controlled, and cost-effective environment for agents to learn. Unlike the real world, where mistakes can have significant consequences, simulations allow agents to explore freely and experiment without fear of failures. This shift from the real world to simulated environments opens the door to training intelligent agents for various applications, paving the way for advancements in fields like robotics [10, 19], healthcare [22], and finance [3].\nWhile simulators offer numerous advantages for training RL agents, they also introduce challenges such as sim-to-real gap [4, 26]. Despite efforts to create realistic simulators, discrepancies between simulated and real-world environments can lead to performance degradation when deploying agents trained solely in simulation to the physical world. Factors such as inaccuracies in physics modeling, sensor noise, and environmental variations can all contribute to this mismatch. This can lead to sub-optimal performance or failure, emphasizing the need to bridge the sim-to-real gap for effective and safe RL agents.\nSeveral approaches have been explored to tackle the sim-to-real gap, and this work specifically focuses on the challenge of dynamics mismatch. One common approach is"}, {"title": "Related Work", "content": "Several methods have been proposed to bridge the sim-to-real gap in RL, particularly the challenge of dynamics mismatch between training and deployment environments. These methods fall into three main categories: system identification, domain randomization, and domain adaptation. System identification, the earliest and most established approach, uses offline data to calibrate the simulator, essentially parameterizing the simulated environment to better match the real world [4]. A closely related solution is online system identification, which directly utilizes inferred system parameters to update a meta-policy [29]. Both methods are often data and computationally expensive, especially for complex, high-dimensional environments. Domain randomization (DR) offers a contrasting perspective. Here, RL policies are trained across a diverse range of"}, {"title": "Uncertainty in Deep Reinforcement Learning", "content": "Deep Neural Networks (DNNs) have been instrumental in the success of deep reinforcement learning (DRL) due to their ability to represent complex functions [23], including smooth dynamics often present in robotics [21]. However, when data is limited, DNNs are susceptible to over-fitting, which can lead to significant uncertainty about the model's true capabilities and degrade the performance of deep RL frameworks. A systematic approach to address this issue is parametric Bayesian inference [2]. This method leverages a predefined probability distribution to represent the uncertainty in a model's parameters. However, its performance heavily relies on choosing an informative prior, and calculating the posterior distribution in complex models with many parameters, which can still be computationally expensive, especially for high-dimensional data.\nAn effective alternative is to use non-parametric bootstrapping. This technique leverages an ensemble of models with random initializations. Bootstrapping ensembles have found applications in various areas of DRLs. For example, they are used for learning an accurate dynamics model in model-based RL [6, 14]. They have also been used in policy and value functions within model-free RL [17, 25] and offline RL [1, 9]. Furthermore, model uncertainty estimation plays a crucial role in designing exploration rewards [20, 25]. In our work, we leverage bootstrapped models to estimate uncertainty in the dynamics modelling, a factor we demonstrate to be crucial for domain adaptation."}, {"title": "Background", "content": "In RL [27], an environment is characterized by a Markov Decision Process (MDP) denoted as M = (S,A,P,R, \u03b3, do). Here, S and A represent the state and action spaces, respectively. The transition dynamics are defined by P: S\u00d7A\u00d7S \u2192 [0, 1], and the reward"}, {"title": "Domain Adaptation with Rewards from Classifier (DARC)", "content": "To mitigate the dynamics mismatch, Eysenbach et al. [8] proposed a domain adaptation method DARC, which learns a policy whose behavior receives high reward in the source domain and has high likelihood under the target domain dynamics. Specifically, they minimized the reverse KL divergence between the desired distribution over trajectories in the target domain p(t) and the agent's distribution over trajectories in the source domain q(t) as follows:\n\nmin DKL(9|1P) = -Epsource \u2211r($t, a\u2081) + \u0397\u03c0 [\u03b1\u03b9 | s\u2081] + Ar($t,A1, $t+1)\n\nwhere\n\n\u2206r (St, at, St+1) log p (St+1 | St, at) \u2013 logq (St+1 | St, At).\n\n\u0397\u03c0 is the entropy of the policy and t is the time-step. The reward adjustment Ar, penalises the agent for taking transitions more likely in the source domain than in the target domain and vice versa. The reward adjustment Ar requires an explicit model of the dynamics which may be inaccurate in continuous control tasks with a high dimensional state-action space. Subsequently, they estimated Ar using Bayes rules with two domain classifiers p(\u00b7|st, at, St+1) and p(\u00b7|st, at) as follows:\n\nAr (St, At, St+1) = log p (target | St, At, St+1) \u2013 log p (target | St, at)\n\n- log p (source | St, At, St+1)+logp (source | St, at)\n\nThe domain classifiers are binary classifiers trained separately used to distinguish between source and target domain transitions."}, {"title": "Methodology", "content": "The key to DARC for domain adaptation lies in the introduced reward adjustment. This adjustment incentivizes agents to choose actions that lead to transitions resembling those in the target domain. However, this approach has two limitations:\n1. Action Set Constraint: During training, the action set used to compute the reward\nadjustments (Equation 1) coincides with the action set used for updating the maxi-\nmum entropy RL method through simulation sampling. Restricting both actions to\nthe same set hinders the search for an optimal solution that maximises both rewards.\n2. Classifier Errors: Due to the epistemic errors in the DARC classifiers, the dynam-\nics reward adjustments might be inaccurate. These errors are particularly problem-\natic if they are overly optimistic. In such cases, the policy might be led to sample\nstate-action pairs that fall outside the target distribution during rollouts in the target\nenvironment. Consequently, the agent lacks the capability to self-correct and return\nto the desired distribution, causing planning in the target environment to diverge.\nIn the following, we propose novel solutions to address these limitations."}, {"title": "Dual Action Policy (DAP)", "content": "The first limitation of DARC lies in the action set constraint. To address this issue, we propose a relaxation strategy using DAP. The core idea of DAP is to utilize a single policy to simultaneously predict two distinct sets of actions a = [asrc, atgt]. The first set asrc, is used for sampling within the simulation environment which aligns with the standard behavior of maximum entropy RL methods. The second set introduces a novel concept: predicting an additional set of actions asrc. The decoupling of the actions into two sets would make it easier for atgt to address the dynamics mismatch via reward adjustments, while asrc focuses on maximizing the task reward. During deployment, we utilize algt to sample the target environment.\nFormally, following Equation 1, the modified objective function can be expressed as:\n\nmin\n\u03c0([arc,ag]|s)\nDKL (9||P) = - Epsource (s\u0131, arc) + Hz (asre | s\u2081) + Ar (st, ast, St+1)\n\ntgt\nand the modified reward adjustment is:\n\nAr St, art, St+1\n\nlogp (S1+1 | St, at) - logq st+1 | St, ast\n\nThis objective is optimized under a new MDP, Mdual = (S, Adual, P, R, y, do), where Adual = 2 \u00d7 A, and A is the original action-space. Similar to DARC, following Equation 2, we use a pair of domain classifiers to estimate Ar:"}, {"title": "Regularization", "content": "From our DAP formulation in Equation 3, the source and target policies play distinct but interconnected roles. The source policy faces a dual optimization challenge: it must maximize returns while generating state sequences indistinguishable from the target tasks by the classifiers. In contrast, the target policy's primary influence is on the reward shaping term. This design creates an interesting dynamic. To this end, we introduce a regularization term, || asrc-ar \u2013 a\u00ba\u00ba||2, to prevent the generation of infeasible actions by atgt, controlled by a hyperparameter \u03bb as follows:\n\nStar\n\ntgt\nAt (s\u0131,a, St+1) = Ar (s\u0131,at, St+1, \n\\arc \u2212 a\u00a3^||2"}, {"title": "Uncertainty-based Robust Action Resampling", "content": "Next, we address the problem of epistemic errors in the DARC's domain classifiers, which can lead to overly optimistic strategies during deployment. Our solution tackles this issue within the training framework by first measuring these uncertainties. Thereafter, we modify the predicted action to a more robust choice based on the severity of the uncertainty. This is achieved by randomly perturbing the action with a magnitude proportional to the uncertainty level. This approach allows actions with low uncertainty to remain unchanged, while forcing uncertain actions to explore a wider range of states. By encouraging exploration in uncertain areas, the agent gains the capability to self-correct and return to state-action distributions with greater certainty.\nFormally, to quantify the dynamics uncertainty due to epistemic errors, we follow a simple but effective method in [18] which utilizes a deep ensemble. Specifically, we train an ensemble of N domain classifiers (Equation 4), denoted by pi(target St, At, St+1)\nand pi(source|st, at, St+1), respectively, for i = 1,..., N, with randomly initialized weights.\nThe intuition behind this approach is that a high standard deviation in the log probabilities across the ensemble indicates significant disagreement about the predicted state transitions, suggesting higher uncertainty in the dynamics model. During training, for each sampled action a = [asrc, atgt], we resample and replace asrc with a robust action, denoted as are, from a normal distribution:\n\n\u00e2rc ~ N(arc, ko\u2081),\n\nwhere\n\nStd logpi (target | St-1,481, 51) - log Pi target | St-1,\n\n+ log pi (source | St-1,\n\nHere, k represents a scaling hyper-parameter, \u03c3\u03b5 represents an uncertainty measure and Std() is the standard deviation function. The modified action encourages exploration where the agent may behave erroneously in the target environment, enhancing its robustness. During deployment, the agent directly relies on the sampled action from the policy, bypassing the robust action."}, {"title": "Experiments", "content": "To evaluate our method's ability to bridge the sim-to-real gap, we conducted experiments in the MuJoCo physics simulator [28]. We used a diverse set of four challenging settings created by modifying the physical properties of simulated robots in these environments: Ant, Half-Cheetah, Hopper, and Walker2d. We collected an offline dataset consisting of M = 20000 samples using a source behavioral policy sampled in the target"}, {"title": "Main Results", "content": "We evaluated baseline policies across four target tasks. Each evaluation comprised 100 episodes across 3 random seeds per checkpoint. The evaluation curves are presented in Figure 2, with the title specifying details about the tasks and environment settings. We"}, {"title": "Ablation Experiments", "content": "Our ablation studies on A as seen in Figure 3a revealed a clear trade-off: as approaches 0, performance on the target task significantly deteriorates due to the target policy generating \"interesting\" actions without optimizing returns. Conversely, large \u03bb values cause DAP to converge towards DARC-like behavior, as the regularization term dominates the shaping. We found an optimal range for a that preserves DAP's unique benefits while ensuring sufficient closeness between the policies. This balance is critical: it allows the target policy to explore potentially beneficial actions while still leveraging the source policy's optimized behavior. The careful tuning of \u03bb thus enables DAP to outperform both unconstrained exploration and strict imitation of the source policy.\nOur next experiment investigates the importance of the scaling parameter, k, for uncertainty-based action resampling (Equation 5). We evaluate a range of k values on the Walker2D environment and present the results in Figure 3b. We begin with DAP without any resampling (k = 0) as the baseline. When we slightly increase k to 0.01, we observe slight improvement over the baseline at certain points during training. However, this performance gain doesn't persist throughout the training process. Further increasing k to 0.05 and 0.10 leads to significant improvement"}, {"title": "Conclusion", "content": "This paper introduces Dual Action Policy (DAP), a reinforcement learning method addressing the reality gap. DAP utilizes a single policy to predict two action sets: one maximizing task rewards, the other addressing dynamics mismatch. This decoupling makes it easier to maximise the overall reward in the source domain during training. Experiments demonstrate DAP's superior performance compared to strong baselines, with further improvements observed when incorporating uncertainty estimation."}]}