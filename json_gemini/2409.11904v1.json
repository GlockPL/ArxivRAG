{"title": "FINDING THE SUBJECTIVE TRUTH", "authors": ["Dimitrios Christodoulou", "Mads A. Kuhlmann-J\u00f8rgensen"], "abstract": "Efficiently evaluating the performance of text-to-image models is difficult as it inherently requires subjective judgment and human preference, making it hard to compare different models and quantify the state of the art. Leveraging Rapidata's technology, we present an efficient annotation framework that sources human feedback from a diverse, global pool of annotators. Our study collected over 2 million annotations across 4,512 images, evaluating four prominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style preference, coherence, and text-to-image alignment. We demonstrate that our approach makes it feasible to comprehensively rank image generation models based on a vast pool of annotators and show that the diverse annotator demographics reflect the world population, significantly decreasing the risk of biases.", "sections": [{"title": "1 Introduction", "content": "Generative models are a groundbreaking class of artificial intelligence systems designed to produce new data instances based on extensive training data. Among these, text-to-image models such as DALL-E 3 [1], Stable Diffusion 3 [2], MidJourney, and the newest, Flux.1, have gained immense popularity for their ability to create detailed images from textual descriptions. These models use deep learning to generate images from text. Each new model iteration aims to outperform the predecessor in generating high quality images. However, while the quality in a few cases can be quantified objectively, e.g. number of certain objects in the generated image, the image quality must generally be judged subjectively. This makes it non-trivial to benchmark new models against each other and ensure continuous improvement of the state of the art.\nMeaningful benchmarks have played an important role in maturing other areas of deep learning research such as computer vision where the community has settled on a few standardized benchmarks such as ImageNet for image classification and COCO for object detection. A similar standardized benchmark does not exist for text-to-image generative models. Instead, authors often propose their own evaluation setup when publishing the results of their newest models [1]. This makes it difficult to gain an ob-"}, {"title": "2 Methodology", "content": "The benchmark is designed to evaluate the quality of text-to-image outputs over a range of meaningful settings based on three criteria common in literature [7, 1] - Style, Coherence, and Text-to-Image alignment. Concretely, this is done by repeatedly presenting annotators with two generated images and asking them to select the best option based on one of the following questions respectively:\n1. Which image do you prefer?\n2. Which image is more plausible to exist and has fewer odd or impossible-looking things?\n3. Which image better reflects the caption above them?"}, {"title": "2.1 Annotation Framework", "content": "To ensure quick and efficient data collection, we have designed the annotation interface to be intuitive and accessible to users with little or no introduction. Inspired by existing works, we decide to always have annotators make a choice between two presented images, as this is deemed more intuitive than e.g. rating individual images. Thus, the base design consists of a criteria question at the top, with the two image options displayed below. In addition, some variations were needed to e.g. display the prompt for text-to-image alignment and ensure higher data quality."}, {"title": "2.1.1 Interface Design", "content": "1. Base Design: used for the Preference and Coherence criteria: Participants are shown pairs of images generated by different models and asked to select the one that better fits the criteria. They are not provided with the prompt used for generating the image as this is not relevant for style preference and coherence.\n2. Display Prompt: used for the Text-to-Image Alignment criteria: In this task, users are given a prompt and must choose which image best matches the given description. The core structure remains the same, but with an additional feature: an animated prompt displayed word-by-word at the top of the screen. This dynamic presentation of the prompt engages users and ensures that they fully comprehend the description before making a selection between the two images.\n3. Validation Tasks: These tasks are embedded to test the attentiveness and engagement of users to ensure higher quality responses. They are crafted to be simple, where the correct choice is obvious, and the annotator will be punished for a wrong answer, ensuring they are actively participating in the task."}, {"title": "2.2 Distribution", "content": "Our annotation process leverages crowdsourcing techniques to gather detailed assessments of text-to-image model performance from a broad and diverse user base. By distributing tasks through mobile apps, we ensure accessibility and ease of participation, allowing us to tap into a global pool of annotators across different demographics, geographic regions, and cultural backgrounds. This approach not only broadens the diversity of perspectives in the evaluations but also increases scalability, enabling us to handle large volumes of data in a cost-effective manner."}, {"title": "2.3 Quality Assurance", "content": "To enhance the reliability of the data collected and mitigate the risk of contributions from adversarial users, we implement several quality control measures.\nTime-Based Controls\nA minimum expected time is set for completing each task. If a participant submits a response faster than this threshold, a mandatory five-second penalty is enforced before allowing them to proceed. This ensures that users dedicate adequate time to consider each image pair or prompt, discouraging quick and thoughtless responses.\nValidation Questions\nTo confirm that users are genuinely engaged, we incorporate validation tasks that are simple and intentionally straightforward, with an obvious correct answer. Annotators who do not successfully complete these tasks may be"}, {"title": "2.4 Prompts", "content": "The benchmark is organized into distinct categories, each tailored to evaluate critical attributes for assessing generative AI models. We base the input prompts on the existing benchmark, DrawBench [8], however, to ensure a comprehensive evaluation, we enhanced it by integrating prompts from various complementary benchmarks. This allowed for a more comprehensive and diverse assessment of model capabilities across a wide range of attributes. While DrawBench covers key aspects such as color accuracy, handling of descriptions, misspellings, counting, rare words, and conflicting scenarios, it lacks coverage for certain nuanced attributes. To address this, we incorporated additional datasets, as guided by ImagenHub [9], ensuring a broader evaluation of model capabilities:\nDiffusionDB [10]: Introduces a variety of general descriptions, enhancing model testing across a wide range of scenarios. Example: \"average man mugshot photo.\"\nABC-6K [11]: Provides hard prompts with complex details, challenging the model's ability to manage intricate descriptions. Example: \u201cMale tennis player with white shirt and blue shorts swinging a black tennis racket.\u201d\nHRS-Bench[12]: Focuses on evaluating fidelity, emotion, bias, size, creativity, and counting, crucial for assessing more subjective and contextual aspects of image generation. Example: \u201cA real scene about a laptop, which makes us feel anger.\u201d\nT2I-CompBench [13]: Tests key structural elements such as color, actions, shapes, texture, and spatial/non-spatial relationships, challenging models with both abstract and realistic prompts. Example: \u201cThe fluffy pillow was on top of the hard chair.\u201d\nDALLE3-EVAL [14]: Focuses on complex and varied descriptions of celebrities. Example: \"Adele, wearing a chef's apron, experiments with exotic ingredients in a modern kitchen. The vibrant colors and textures of the food contrast with her focused expression as she creates a culinary masterpiece.\""}, {"title": "2.5 Image Generation", "content": "For image generation we applied the same text prompts across all models. We used APIs for DALL-E 3, MidJourney, and Flux.1. For Stable Diffusion we used the model through Hugging Face. This standardized method allowed for direct comparison of each model's ability to interpret and visually render the prompts.\n\u2022 Flux.1: Used via the Replicate API with the 'flux-pro' model, standard image quality, and dimensions set to 1024x1024 pixels.\n\u2022 DALL-E 3: Accessed via OpenAI's API with 'dall-e-3', standard image quality, and 1024x1024 pixel dimensions. We generated multiple images per prompt through parallel requests.\n\u2022 Mid Journey: Employed through the ImaginePro API with 'MidJourney Version 5.2', an aspect ratio of 1:1, minimal chaos, quality set to 1, and stylization at 100. Four upscaled images were selected for comparison.\n\u2022 Stable Diffusion: Used via Stability AI on Hugging Face with 'stable-diffusion-3-medium-diffusers' and a guidance scale of 7, producing 1024x1024 pixel images, dropping the T5 Text Encoder during Inference."}, {"title": "3 Results", "content": "To provide a comprehensive comparison, we generated four images for each model and prompt. For each prompt, each image was paired against the 12 images from the other models, resulting in 96 pairwise comparisons per prompt for each criteria. Across the entire dataset of 282 prompts, this led to 27,072 total comparisons per criteria.\nFor each comparison, we collected 26 votes, leading to 2,496 votes per prompt. With 282 prompts in total, we accumulated over 700,000 votes per criteria, resulting in more than 2 million votes across the three criterias. Our research involved a vast and diverse group of 144,292 participants from 145 different countries, reflecting a truly global representation."}, {"title": "3.1 Data", "content": "To derive a ranking for the models based on the collected votes, we employed the Iterative Bradley-Terry ranking algorithm [15]. This algorithm is well-suited for scenarios where data is collected in full before determining the ranking, whereas e.g. Elo is a typical choice for online use cases. Using the Bradley-Terry model we assign probabilistic scores to each image generation model. Based on these scores, the probability that model i beats model j in a pairwise comparison can be found as $P(i > j) = \\frac{P_i}{P_i+P_j}$, where $p_i$ and $p_j$ are the scores assigned to model i and j respectively. There is no known closed form solution for calculating the score from a set of pairwise comparisons, however, the problem can be solved by iteration. At each"}, {"title": "3.2 Ranking Algorithm", "content": "iteration, the score for each model is updated according to eq. (1)\n$P_i = \\frac{\\sum_{j}w_{ji}\\frac{p_j}{p_i+p_j}}{\\sum_{j}w_{ji}}$\n(1)\nwhere $w_{ij}$ is the number of comparisons where model i beat model j. Since the scores are arbitrary up to a common factor, we decide to normalize the scores to sum to 100."}, {"title": "3.3 Ranking", "content": "Using the Bradley-Terry ranking method, we derived the final rankings of these models. The summary of the results is presented in table 1 and fig. 3, highlighting the relative performance of each model across the three criteria.\nPreference: The newest model, Flux.1, significantly outperformed the other three in terms of style preference, achieving the highest ranking with a score of 29.86. The Stable Diffusion model ranked considerably lower than the others at a score of 21.99, whereas DALL-E 3 and MidJourney placed very close. While style preference is obviously subjective, these results are also reflected particularly in figs. 1 and 4\nCoherence: In terms of image coherence, the Flux.1 model again outperformed the others with a score of 29.61. Interestingly however, Stable Diffusion scored higher than both DALL-E 3 and MidJourney for this criteria at 24.09, indicating that while performing the worst in the other two criterias, Stable Diffusion may generally be less likely to generate incoherent and implausible artifacts than DALL-E 3 and MidJourney. Figure 5 illustrates the relationship, with Flux.1 generating a very realistic and plausible image - DALL-E 3 and MidJourney somewhat less so. Stable Diffusion completely failed to adhere to the prompt, but generated a very realistic and plausible image nonetheless, which is a behavior that has been observed a few times.\nText-to-Image Alignment: The Flux.1 model comes out on top again in text-to-image alignment, with a score of 27.36. However, the gap is not as large as for the other criterias with DALL-E 3 closely following with a score of 26.76. MidJourney and Stable Diffusion lagged behind with scores of 24.48 and 21.4, respectively. Figures 4 and 6, illustrates what could be part of the reason for DALL-E 3 to bridge the gap with Flux.1 in text-to-image alignment. In cases of typos and odd sentences, from our point of view it appears that DALL-E 3 in these cases interprets the prompts more 'intuitively'. In fig. 6 DALL-E 3 interprets the misspelled prompt as 'Tennis racket' whereas Flux.1 seemingly focuses on 'packet' part of 'Tcennis rpacket'. In fig. 4 most models ignore the fact that the pizza is supposed to be the subject doing the cooking, however DALL-E 3 captures that aspect. It appears as if the other models instead interpreted the prompt as 'A pizza cooking in an oven' instead of interpreting it literally."}, {"title": "3.4 Demographic Statistics", "content": "With our proposed benchmark and annotation framework, we aim to improve global representation to reduce biases. In this section we therefore present and analyze demographic data of the participating annotators. We present three statistics; geographic location, age group, and gender. Location is directly provided through the distribution method, whereas gender and age are self-reported by the annotators.\nLocation: For simplicity and visualization, in fig. 7 we break down location by continent. We recognize that particularly in vast and diverse continents like Asia, this may leave out some nuances, but it provides comprehensible insights. By comparing our distribution to the world population distribution, we see that the general trend is captured, however we have an overrepresentation of African annotators and a slight underrepresentation of annotators from Asia. This is something that can be considered and adjusted in the future. We also see an underrepresentation of North America, however one could argue that this region is culturally very close to Europe, which is slightly overrepresented. Overall, the distribution represents a reasonable estimate to world population and underlines the argument that our proposed framework reaches a diverse set of annotators to reduce biases. Additionally, for future work, we can adjust this, as our tool allows us to target specific countries or even languages, offering flexibility in tailoring tasks to particular regions or linguistic groups.\nAge: The graph provides a breakdown of the percentage of users across various age groups who submitted votes in our evaluation. We observe a higher representation of younger annotators, which is probably to be expected from the crowd-sourcing method. Overall however, the distribution is quite even.\nGender: The graph illustrates the gender distribution of users who participated in the evaluation. We observe a very even distribution between men and women. However, we also observe a large portion that answered another category or did not provide an answer. This can be taken into consideration to structure and phrase this question better in the future."}, {"title": "4 Conclusion", "content": "We have shown that our annotation framework and proposed benchmark provides an efficient, representative, and scalable way to comprehensively evaluate image generation models. This is only possible through the novel crowd-sourcing approach made accessible through Rapidata's technology. In the scope of just a few days, we cost-effectively collected more than 2 million responses \u2013 to the best of our knowledge, the largest image generation model evaluation by a significant margin. The scale and efficiency illustrated in this work also opens the door for incorporating human feedback directly into the training process. Reinforcement learning from human feedback (RLHF) has been a significant driver in bringing LLMs to the level we see today [16]. Rapidata's technology provides easy access to diverse, efficient, and scalable feedback to include humans in the training loop."}]}