{"title": "Diffusion Suction Grasping with Large-Scale Parcel Dataset", "authors": ["Ding-Tao Huang", "Xinyi He", "Debei Hua", "Dongfang Yu", "En-Te Lin", "Long Zeng"], "abstract": "While recent advances in object suction grasping have shown remarkable progress, significant challenges persist particularly in cluttered and complex parcel handling scenarios. Two fundamental limitations hinder current approaches: (1) the lack of a comprehensive suction grasp dataset tailored for parcel manipulation tasks, and (2) insufficient adaptability to diverse object characteristics including size variations, geometric complexity, and textural diversity. To address these challenges, we present Parcel-Suction-Dataset, a large-scale synthetic dataset containing 25 thousand cluttered scenes with 410 million precision-annotated suction grasp poses. This dataset is generated through our novel geometric sampling algorithm that enables efficient generation of optimal suction grasps incorporating both physical constraints and material properties. We further propose Diffusion-Suction, an innovative framework that reformulates suction grasp prediction as a conditional generation task through denoising diffusion probabilistic models. Our method iteratively refines random noise into suction grasp score maps through visual-conditioned guidance from point cloud observations, effectively learning spatial point-wise affordances from our synthetic dataset. Extensive experiments demonstrate that the simple yet efficient Diffusion-Suction achieves new state-of-the-art performance compared to previous models on both Parcel-Suction-Dataset and the public SuctionNet-1Billion benchmark.", "sections": [{"title": "I. INTRODUCTION", "content": "With the growing popularity of the e-commerce industry, logistics companies are facing a heavy burden in processing and shipping the rapidly increasing volume of parcels [1]. At present, traditional manual processing can no longer cope with such increasing parcels [2]. However, due to the disorderly stacked and diverse types of express packages, processing and shipping parcels face technical challenges. In particular, accurate and quick object grasping has become a fundamental issue in the effective implementation of vision-based intelligent parcels processing system.\nDue to the simplicity and reliability, suction grasping is widely used in real-world tasks in many industrial and daily scenes, among the different grasping techniques [3]. Despite the success of recent object suction grasping methods, challenges still exist and need to be addressed especially in the context of the cluttered and complex parcel scenes. Two major challenges persist in obtaining robust and accurate suction grasping for parcel object. The first challenge lies in the absence of a universal and efficient suction grasping datasets for parcel scenes. Zeng et al. [4] solely relied on manual experience to annotate the suctionable area and previous works [5] proposed a synthetic suction dataset collection pipeline for cluttered environment. These dataset lack a general focus on the characteristics of parcel objects and a substantial amount of flat object data features. Consequently, models trained on such datasets cannot directly generalize to complex parcel scenes. Moreover, most grasp datasets samples cannot adequately account for the characteristics of large-area and severe-obstruction of objects, resulting in failed suction grasping in real-world scene.\nSecondly, suction grasping prediction is a challenging task because it involves finding feasible suction grasping poses given the wide range of object sizes, shapes and textures. Current methods directly formulate this problem as a regression problem. Many object 6D pose estimation models [6], [7], [8], [9] project the pre-defined suction configuration onto the scene. However, these approaches lack generalization for new objects. Alternatively, discriminative approaches [4], [10], [5] directly generate suction grasping poses from point clouds or RGB-D images. Nonetheless, these approaches are frequently limited by specialized network architectures and diverse training strategies to generate reliable predictions.\nIn this study, we introduce the Parcel-Suction-Dataset and Diffusion-Suction for suction grasping prediction in cluttered scenes, as shown in Fig. 1. We propose a novel Self-Parcel-Suction-Labeling framework which can automatically generate general and diverse dataset information in parcel stacked scenes. To the best of our knowledge, this comprehensive dataset is the first large-scale synthetic suction grasping dataset in parcel stacked scenes, called Parcel-Suction-Dataset. Specifically, we propose an image to 3D parcel asset pipeline which can comprehensively capture both geometry and appearance information to obtain 3D high-quality assets. Moreover, we use Bullet and Blender platform to create random unstructured parcel scenes and generate realistic synthetic rendering information, such as RGB images, segmentation images and depth maps. We introduce a suction grasping candidate evaluation algorithm that calculate suction cup gripper seal score, wrench score, collision score and visibility score without requiring labor-intensive processes.\nMoreover, we propose a novel Diffusion-Suction framework to predict feasible and accurate suction grasping poses by reformulating the problem as an iterative diffusion-denoising refinement process with 3D visual-condition guided. Diffusion-Suction is decoupled into the point clouds encoder and suction grasping score decoder. It uses advanced point clouds feature extraction network PointNet++ to extract global information as condition guidance. Subsequently, we propose a novel and light-weighted Pointcloud Conditioned Denoising Block which conduct Attention on the hierarchical channel and spatial information to emphasize the significant features. The point clouds encoder is executed in a single time and the diffusion process is performed using a light-weighted decoder head. With this simple and efficient structure, Diffusion-Suction significantly reduces the inference computational overhead. During training stage, gaussian noise controlled by a variance schedule [11] is iteratively added to ground truth to obtain noisy maps. At the inference stage, Diffusion-Suction generates reliable suction grasping score by reversing the learned diffusion process. To the best of our knowledge, this is the first work introducing the diffusion model into suction grasping prediction task.\nWe evaluate our proposed method using the Parcel-Suction-Dataset and public SuctionNet-1Billion [3] benchmarks. Our experimental results demonstrate that Diffusion-Suction significantly outperforms state-of-the-art methods. We present extensive ablation study to further reveal the effectiveness and properties of Diffusion-Suction on Parcel-Suction-Dataset. In summary, the main contributions of our work are:\n\u2022\tWe propose a novel Self-Parcel-Suction-Labeling framework to generate general and diverse large-scale Parcel-Suction-Dataset in parcel stacked scenes.\n\u2022\tWe formulate suction grasping prediction task as a generative denoising process, which is the first study to apply the diffusion model to this task.\n\u2022\tWe propose a novel Diffusion-Suction framework to predict feasible and accurate suction grasping pose with 3D visual-condition guided."}, {"title": "II. RELATED WORK", "content": "A. Suction Dataset\nZeng et al. [4] conducted a manual annotated real-world dataset that contains cluttered scenes. Its disadvantage is that it relies solely on manual experience annotation for the suctionable and non-suctionable area, making the annotation process highly time-consuming and expensive. In addition, manual annotation introduces significant potential errors. Jiang et al. [12] proposed a reachability evaluation metric to avoid problems when a high grasp quality configuration is unable to reach the target. Shao et al. [13] conducted a self-supervised learning robotic bin-picking system to generate training data and completed training within a few hours. But it has only been validated on cylindrical objects, limiting the application of this method. Suctionnet [3] conducted a real word dataset which utilizes a new physical model to analytically evaluate seal and wrench formation. The annotation error depends on the accuracy of pose estimation in real scenes. CoAS-Net. [5] proposed a suction dataset collection synthetic pipeline that contains collision-free annotation and uses the newest simulator Issac Sim to reduce the domain gap. Sim-Suction [14] proposed a large-scale synthetic dataset for cluttered environments, which uses object-aware suction grasp sampling and considers dynamic interactions in unstructured environments. However, there is currently a lack of readily available data in parcel fields to explore and improve suction grasping. We propose Self-Parcel-Suction-Labeling framework to build large-scale parcel suction dataset which includes a variety of parcel objects acquired in the real world. In addition, our dataset is specifically designed for parcel scenes, which is severely stacked and contains a substantial amount of flat object data features.\nB. Suction-based Grasping\nHernandez et al. [6] estimated the 6D pose of the object [7], [8], [9] and projected the pre-defined suction configuration onto the objects in the scene. However, such methods lack generalization for new objects. Zeng et al. [4] design a single FCN architecture trained on a human-labeled dataset, learning pixel-wise suction affordances. Suctionnet [3] proposed pixel-wise suction scores prediction network which predicts seal score heatmap and center score heatmap separately. Sim-Suction [14] proposed a object-aware affordance network which directly inputs text prompts to obtain identify regions of interest grasp object and outputs point-wise suction probability. Zhang et al. [15] propose the suction reliability matrix and suction region prediction model. However, it has not been tested for effectiveness in multi object scenes. CoAS-Net. [5] utilized context aware suction network trained with a synthetic dataset. Rui Cao et al. [10] proposed an uncertainty-aware and multi-stage framework which exploit both aleatoric and epistemic uncertainties. These methods essentially belong to discriminative approaches and generative approaches have not yet been explored in terms of suction grasping task. Diffusion models [16] demonstrate remarkable performance in text-to-image generation task and surpass previous generative models, such as Generative Adversarial Networks (GAN) [17] or VAE [18]. We propose to utilize the diffusion model to denoise random noise into suction grasping score map with the guidance of input point clouds visual-condition, instead of adopting it as a normal regression head. To the best of our knowledge, this is the first work introducing the diffusion model into suction grasping prediction task."}, {"title": "III. LARGE-SCALE PARCEL SUCTION DATASET", "content": "As mentioned earlier, parcel scene has not been widely covered in existing datasets. Therefore, we propose a Self-Parcel-Suction-Labeling (SPSL) framework which contains diverse information and constructs Parcel-Suction-Dataset as shown in Fig. 2. This section will provide more detailed information about the SPSL.\nA. Randomized Scene Generation\nAcquiring 3D high-quality assets is the first step in generating scenes for dataset construction. However, there is currently a lack of available 3D parcel assets. The 3D parcel assets have the characteristics of privacy, complexity, and diversity. Using traditional laser scanning instruments to obtain 3D assets through scanning cannot effectively solve the above problems and is time-consuming [19]. To address this issue, we propose a pipeline that converts images to 3D parcel assets. Specifically, we collect multiview images in real parcel stacked scenes as prompts for the latest 3D generation method TRELLIS [20] which can comprehensively capture both structural (geometry) and textural (appearance) information. We have generated a total of 113 high-quality parcel assets and clustered them in three categories according to the Chamfer Distance and Normal Consistency metric: rectangular, planar, and cylindrical.\nWe create random unstructured parcel scenes using the previously generated assets. In the real world, parcels are stacked randomly into a pile. To simulate this real state, parcels are randomly dropped into a bin with arbitrary poses on the Bullet simulator platform [21] which can simulate the real dynamical collision. Moreover, we use Blender platform [22] to generate realistic and accurate synthetic rendering information, such as RGB images, segmentation images and depth maps. To reduce the domain gap between the synthetic and real domains, we randomize several aspects of the simulation.\n\u2022\tRandomly select objects from the parcel dataset.\n\u2022\tRandomly sample the number of objects in the scene and the sampling range is [1,50].\n\u2022\tRandomly sample the pose of objects to ensure it can fall into the bin.\n\u2022\tRandomly sample the coulomb friction coefficient to model tangential forces between contact surfaces.\n\u2022\tWe apply gaussian noise augmentations with random intensity to the rendering RGB images.\nB. Suction Pose Annotation\nAt this stage, we will describe how to compute the suction grasping score, meaning that the higher the score, the easier it is to suction grasp in real-world scene. We assign them four continuous scores, namely seal score, wrench score, collision score and visibility score. We define the final metric as a product of the sub-evaluation score $S = S_{seal} \\times S_{wrench} \\times S_{collision} \\times S_{visibility}$.\nSeal Score Evaluation. The seal score is used to determine whether the suction cup can maintain a vacuum state when suction grasping is performed in a specific pose. The higher the surface flatness of the object, the higher the seal score. Similar to Cao et al [3], we leverage the compliant suction contact model [4] to evaluate this process, which models the suction cup as a quasi-static spring system. Specifically, we select a suction cup gripper with a 10 mm radius suction cup and sample dense vertex at the suction cup {V1, V2, ..., vn}. Then we transform the suction cup model to the object surface along a specific approach vector to obtain object projection surface vertex {\\bar{v_1}, \\bar{v_2}, ..., \\bar{v_n}}. We use the distance variation between sampling points to evaluate the seal score, which can be defined as:\n$S_{seal} = 1-max{r_1, r_2,..., r_n}$\t(1)\nwhere the i-th sampling point distance variation $r_i = min(1, \\frac{l_i}{\\bar{l_i}})$ and the i-th distance between adjacent sampling point $l_i = v_i - v_{i+1}$. The weight of objects is not considered during seal score evaluation process. Note that we control the sampling points based on the size changes of the object,"}, {"title": "in order to achieve an optimal balance between accuracy and efficiency.", "content": "Wrench Score Evaluation. The wrench score is used to determine whether the suction cup fails to resist the wrench caused by gravity in a specific pose. DexNet 3.0 [4] has 5 different basic wrenches criteria, which can be defined as:\nFriction : $\\sqrt{3}|f_x| \\leq \\mu f_n $\\sqrt{3}|f_y| \\leq \\mu f_n $\\sqrt{3}|t_z| \\leq r\\mu f_n$,\nMaterial : $\\sqrt{2}|t_x| \\leq \\pi rk $\\sqrt{2}|t_y| \\leq \\pi rk$,\nSuction: $f_z >-V_f$\t(2)\nwhere u is the friction constant, Vf is suction grasping force, ff = (fx, fy) is frictional force, \\tau_z is torsional friction, \\tau_e = (\\tau_x, \\tau_y) is elastic restoring torque, r is the radius suction cup and k is a material-related constant. Similar to Cao et al [3], we have simplified these conditions by reserving only the material condition to evaluate this process. In the real world, suction grasp objects from the side often leads to failure, and humans are accustomed to suction grasp objects from the top. However, for planar category objects in parcel dataset as mentioned earlier, due to the short distance from the suction grasping point to the center of mass of the object when suction grasping from the side, this evaluation algorithm obtains a high wrench score. Therefore, we propose a correction strategy that uses the angle between the suction grasping normal vector and gravity to correct. Based on previous assumption, the wrench score can be defined as:\n$S_{wrench} = (1 - min(1,|t_{\\theta}|/T_{thr}))(1-\\alpha/\\pi)$\t(3)\nwhere the torque constant threshold $T_{thr} = rkn$ and $\\alpha$ is the angle between the suction grasping normal vector and gravity.\nCollision Score Evaluation. The collision score is used to determine whether the suction cup collides with objects in a specific pose in the scene. We check whether any points in the scene appear inside the mesh of the suction cup gripper. If there are points in the gripper, Scollision is 0, and vice versa. To balance between computational cost and accuracy, we set up a workspace around the suction cup gripper for collision detection.\nVisibility Score Evaluation. The visibility score is used to reflect the obstruction of the object in the scene. In highly occluded parcel scenes, some parcels are severely occluded and we are not interested in these parcel instances which cannot be captured at the bottom. Moreover, when these parcels are grabbed, it will seriously affect other objects in the scene, causing grabbing failure. Specifically, given an occluded scene image where parcels are stacked randomly into a pile, we calculate the pixel area size of each parcel $\\bar{p_o}$. Then, we iteratively remove other objects in the scene and calculate the unobstructed pixel area size of each object $p_o$. Visibility score can be defined as $S_{visibility} = p_o/\\bar{p_o}$\nC. Dataset Details\nTo the best of our knowledge, this comprehensive dataset is the first large-scale synthetic suction grasping dataset in parcel stacked scenes, called Parcel-Suction-Dataset. Specifically, it contains 500 cycles and each cycle contains 50 scenes. There are a total of 25 thousand scenes included. To cover the entire scene, 16384 extracted annotations are sampled from each scene using farthest point sampling. Each scene includes RGB images, segmentation images, depth maps, scene point clouds, 6D object poses, 6D object bounding boxes and camera matrices. In addition, the dataset is divided into a training set and a testing set. The training set contains 480 cycles, and the test set contains 20 cycles. The training set contains 100 objects, and the test set contains 13 other objects that does not appear in the training set. Notably, this Self-Parcel-Suction-Labeling framework is intended to serve as a synthetic benchmark for parcel suction grasping."}, {"title": "IV. DIFFUSION-SUCTION MODEL", "content": "A. Task Reformulation\nPreliminaries. Diffusion models [16], [23] are a class of generative models parametrized by a Markov chain, which can gradually transform a simple distribution to a structured complex distribution. These models involve two processes. The first is diffusion process q, which can be defined as:\nq(xt|xo) = N(xt|\\sqrt{\\bar{a}_t}xo, (1-\\bar{a}_t)I)\t(4)\nwhich iteratively transforms desired distribution xo to a latent noisy sample Xt for t\u2208 {0,1,...,T} steps. The constant $\\bar{a}_t = \\Pi_{s=0}^t{a_s} = \\Pi_{s=0}^t(1 - \\beta_s)$. \u03b2s is a constant that defines the added noise schedule [16].\nIn the denoising process that reconstructs data from pure noise, the reverse process model \u03bc\u04e9 (x,t) is trained to interactively predict xt-1 from x\u2081, which can be defined as:\n$P_{\\theta}(X_{t-1}|X_t) = N(X_{t-1};\\mu_{\\theta} (x,t), \\beta_t 1)$\t(5)\nwhere $\\bar{B_t} =\\frac{\\beta}{1-\\bar{a_t}}$ denotes the transition variance.\nDenoising as Suction Grasping. Given a stacked scene where parcels are stacked randomly into a pile, we are interested in detecting the suction grasping pose. The pose is defined as:\nV = [t,n]\t(6)\nwhere t represents suction cup position on the object surface, and n represents the orientation of the suction cup. We define suction grasping prediction task as, given the point clouds of the parcel stacked scenes P, the algorithm predicts a feasible suction grasping poses V. The orientation of the suction grasping n usually follows the direction of the normal vector on the object surface, and the normal vector can be calculated directly. Therefore, the suction grasping task is actually transformed into finding a feasible suction grasping pose V with a high confidence score x in the scene. Subsequently, the diffusion process will occur in the confidence score space. In this paper, we reformulate the suction grasping prediction as a 3D visual-condition guided denoising process, which can be defined as:\n$P_{\\theta}(X_{t-1}|X_t,P) = N(X_{t-1};\\mu_{\\theta} (x_t,t,P), \\beta_t 1)$\t(7)\nwhere the reverse process model \u03bc\u04e9 (xt,t, P) is trained to refine suction grasping score from latent x\u2081 to x\u2081-1 conditioned on the scene point cloud P.\nB. Architecture\nThe diffusion model generates data samples progressively, requiring the model \u03bc\u04e9(xt,t,P) runs multiple times during the inference stage. However, directly applying the model \u03bc\u03b8 (xt,t,P) on the raw scene point clouds at each iterative step significantly increases the computational overhead. To resolve this problem, we propose to separate the entire model into two parts: point clouds encoder and suction grasping decoder, as shown in Fig. 3. The point clouds encoder runs only once to extract the deep feature representation from the input scene point clouds P. Then the suction grasping decoder takes this deep feature as visual-condition guided, instead of the raw point clouds P, to progressively refine the suction grasping prediction score from the noisy x\u2081. With this simple and efficient structure, DiffusionSuction reduces the inference computational overhead.\nPoint cloud encoder. It takes the original point clouds of the parcel stacked scenes as input and applies a feedforward network for feature extraction. For this purpose, we employ PointNet++[24] serving as the backbone and there are other alternative backbones, e.g. DGCNN [25], Pointnet [26] and MinkowskiEngine [27]. The extracted point-wise guidance Fp has a size of Np \u00d7 Nf. Calculating object surface normals can effectively extract the main 3D structure information of the object and provide effective prior information without the training process. Therefore, we use Open3D [28] to directly estimate normals and then fuse 3D normals prior feature with the point clouds to obtain accurate prediction.\nSuction grasping decoder. Neural network model \u03bc\u03b8 (xt,t,P) takes available 3D visual information feature from parcel stacked scene and iteratively refines suction grasping noisy map x\u2081. We introduce light-weighted Point-cloud Conditioned Denoising Block (PCDB) to achieve this process, which efficiently reuses the shared parameters during the multi-step reverse diffusion process. Specifically, as shown in Fig. 3, suction grasping noisy map x\u2081, time steps embedding t and visual-condition guided Fp are sent to different MPLs for powerful feature representation. Afterwards, the three feature maps are combined via element-wise addition. To emphasize the significant features, we conduct attention on the channel and spatial information with the convolutional block attention module (CBAM) [29]. Inspired by the success of residual module [30], we conduct residual connection between the fused attention feature maps x\u2081 to obtain x\u2081-1. Our PCDB balances effectiveness and efficiency, refining suction grasping with few parameters.\nC. Training and Inference\nWe add Gaussian noise to the ground-truth suction grasping score, according to Equation (5). The noise scale is controlled by at, which adopts the monotonically cosine schedule for at in different time steps. Trainable parameters are mainly the PCDB and point cloud feature extractors. The model is trained by minimizing the loss between diffusion suction grasping prediction and ground-truth, which is provided in Algorithm 1. The iteration number 1 \u2264 t \u2264 T _is sampled from a uniform distribution, and the epsilon \u03b5 is sampled from a standard distribution. Notably, the scaling factor scale controls the signal-to-noise ratio (SNR) [31], which has a significant effect on the performance of the model. Therefore, we normalize and scale the range of ground-truth.\nThe inference procedure of Diffusion-Suction is a multi-step denoising process from a Gaussian distribution. Given a raw scene point clouds as the condition input, the model progressively refines its predictions and starts with a noise map, as shown in Algorithm 2. Moreover, we apply the DDIM [11] inversion process for the improving inference process. Fig. 4 illustrates the denoising reverse process with 20 inference steps of Diffusion-Suction."}, {"title": "V. EXPERIMENTS", "content": "A. Datasets and evaluation metrics\nTo comprehensively evaluate the suction grasping performance of our method in stacked parcel scenes, we select the Parcel-Suction-Dataset. There are a total of 25 thousand annotated scenes included. Moreover, we also choose to conduct testing on the public datasets SuctionNet-1Billion [3] to demonstrate the effectiveness of our framework. This dataset contains 190 scenes and we collect 256 perspectives for each scene. The dataset is divided into three subsets to test performance in different scenes: seen, similar, and novel. We evaluate the method's performance by calculating the Average Precision metric. Specifically, we strictly follow the evaluation protocol in proposed method [3] and calculate the seal score and wrench score for each predicted result online. If the product of the seal score and the wrench score exceeds the threshold of 0.4 or 0.8, the predicted suction pose is considered correct. To evaluate the model's performance in real-world cluttered scenes, we also select the prediction grasping pose with the highest confidence for evaluation. When evaluating the Parcel-Suction-Dataset, we calculate visibility scores for online evaluation.\nB. Evaluation on Parcel-Suction-Dataset\nTo validate the performance of our proposed approach on the Parcel-Suction-Dataset, we compare Diffusion-Suction against several state-of-the-art approaches, as shown in Table II. As we can observe, our proposed approach achieves state-of-the-art results. It outperforms other methods wtih an improvement of +32% on average precision the in the top-50 metric. Moreover, our Diffusion-Suction achieves great improvement in the Top-1 metric, indicating its ability to generate the best suction grasping candidate in cluttered scenes, which is crucial for real-world suction grasping applications. Notably, in terms of the AP0.8 metric, our method significantly outperforms other approaches, showing the most substantial improvement. This indicates that most of suction grasping poses predicted by Diffusion-Suction have high scores. Fig. 5 illustrates the qualitative results of Diffusion-Suction on the cluttered and complex parcel scenes, showing the Top-50 suction grasps.\nC. Evaluation on SuctionNet-1Billion\nTable I summarizes the comparison results between our proposed approach and current suction grasping methods on public benchmark. Overall, our proposed approach outperforms three state-of-the-art approaches by a significant margin across both RealSense and Kinect collection source. Specifically, it's important to note that our proposed approach demonstrates consistently better performance in all the top-50 benchmark compared wtih other methods. A crucial contributing factor is that our diffusion head with advanced 3D visual-condition feature extraction can better capture global information, leading to highly accurate prediction overall performance. In addition, our proposed approach outperforms other models in all the seen scenes and exhibits strong learning capability for seen objects. It shows that utilizing diffusion model on this task can achieve robustness and efficiency prediction in terms of the seen scenes. Our proposed approach also demonstrates competitive performance on the similar and novel scenes.\nD. Ablation study\nWe present extensive ablation study to further reveal the properties of Diffusion-Suction and the ablation is conducted on the the Parcel-Suction-Dataset.\nFramework settings. We report the compatibility of the proposed model, as shown in Table III. The Diffusion-Suction (w/o Normal) and Diffusion-Suction (w/o Visibility) results demonstrate the importance of visibility score decoder and 3D normal prior feature fusion. Moreover, this indicates that the proposed framework with PointNet++ [24] for feed-forward feature extraction can achieve optimal performance compared to PointNet [26] and DGCNN [25] model.\nSignal scale. We study the effect of different scaling factor, which controls the signal-to-noise ratio of the diffusion process, as shown in Table IV. Diffusion-Suction with the signal scaling factor of 0.5 demonstrates optimal performance. We explain that this is because Diffusion-Suction with a larger scaling factor results in more easier cases being preserved with the same time step.\nDenoising step. Training and inference step not only affect inference speed, but also require high GPU memory consumption. To explore the balance between accuracy and efficiency, we change the training and inference step within the range of 5 to 20, as shown in Table V. Changing the inference step directly can result in a significant decrease in performance. However, keeping the training and inference step the same and reducing the number of step slightly decrease the performance, but it can accelerate the model speed."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we introduce the Parcel-Suction-Dataset and Diffusion-Suction for suction grasping prediction in parcel cluttered scenes. The Parcel-Suction-Dataset comprises 25 thousand cluttered scenes with 410 million annotated suction grasping poses. Moreover, Diffusion-Suction denoise random prior noise into suction grasping score map with the guidance of input point clouds visual-condition. Experiments show that Diffusion-Suction improvements average precision compared to well-established models on Parcel-Suction-Dataset and public SuctionNet-1Billion benchmarks. We will release the code and dataset soon (https://github.com/TAO-TAO-TAO-TAO-TAO/Diffusion_Suction)."}]}