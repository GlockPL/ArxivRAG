{"title": "A Resource Efficient Fusion Network for Object Detection in Bird's-Eye View using Camera and Raw Radar Data", "authors": ["Kavin Chandrasekaran", "Sorin Grigorescu", "Gijs Dubbelman", "Pavol Jancura"], "abstract": "Cameras can be used to perceive the environment around the vehicle, while affordable radar sensors are popular in autonomous driving systems as they can withstand adverse weather conditions unlike cameras. However, radar point clouds are sparser with low azimuth and elevation resolution that lack semantic and structural information of the scenes, resulting in generally lower radar detection performance. In this work, we directly use the raw range-Doppler (RD) spectrum of radar data, thus avoiding radar signal processing. We independently process camera images within the proposed comprehensive image processing pipeline. Specifically, first, we transform the camera images to Bird's-Eye View (BEV) Polar domain and extract the corresponding features with our camera encoder-decoder architecture. The resultant feature maps are fused with Range-Azimuth (RA) features, recovered from the RD spectrum input from the radar decoder to perform object detection. We evaluate our fusion strategy with other existing methods not only in terms of accuracy but also on computational complexity metrics on RADIal dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Driving Systems (ADS) often rely on different types of sensors to achieve accurate perception. Most of the self-driving vehicles are equipped with cameras, radars, and LiDARs [1]. Camera data provide rich visual information about the environment. However, they are susceptible to bad weather conditions and lack depth perception capabilities. On the contrary, the expensive LiDAR sensor provides denser point clouds with precise depth and spatial information, delivering a higher resolution detail for objects in 3D space compared to camera images or sparser radar points. Both the camera and the LiDAR suffer in adverse weather conditions like fog, smog, and snowstorms [2].\nOn the other hand, the radar point cloud data processed typically from constant false alarm rate (CFAR [3]) algorithm suffers low angular resolution and severe sparsity as contex-tual information of radar returns are lost [4]. In contrast, leveraging raw radar data is considered to hold significant potential for perception tasks. Thankfully, radar and camera technologies complement each other significantly, making their fusion a promising solution to common perception tasks, specifically object detection. Nevertheless, effectively employing the raw radar data, particularly for fusion with other sensors, remains a challenge."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Camera-Radar dataset", "content": "To enable effective sensor fusion, it is essential that the sensor data streams are synchronized both temporally and spatially. Additionally, precise calibration parameters for each sensor involved in the fusion process must be accurately determined and known. Recently, there has been increasing attention to leveraging radar data beyond point cloud representations for fusion with camera images for enhancing object detection. As a result, some attempts have been made in providing radar data in the form of range-azimuth-Doppler (RAD) tensor [17], [18], range-azimuth (RA) maps [19], range-Doppler (RD) spectrum [20] or even raw Analog Digital Converter (ADC) data [20], [21]. All the above forms can be derived from ADC data using Fast Fourier Transform (FFT).\nOur motivation is to leverage the potential of raw radar data due to its comprehensive representation instead of sparser point clouds. Recent datasets that offer such a representation are RADIal [16], Radatron [2], RADDet [18], \u039a-Radar [22]. The RADIal benchmark [16] has been chosen for this research, as it is the only dataset providing an analog-to-digital converter (ADC) signal, Range-Angle-Doppler (RAD) tensor, Range-Angle (RA) view, Range-Doppler (RD) view, point cloud (PC) representation of HD radar data, combined with camera, LiDAR, and odometry. This implies that there are greater opportunities to explore various fusion settings. However, the scope of detection is limited to vehicle class, since the majority of road users in this dataset are several moving vehicles. But it is also possible to extend our work to other road users given a suitable dataset."}, {"title": "B. Camera-Radar fusion methods for object detection", "content": "Methods utilizing radar point cloud face challenges due to sparsity and low angular resolution. On the other hand, storage and computation pose a concern when using RAD tensors. As a result, RADDet [18] takes the Doppler di-mension as channels and [23] utilizes RAD tensors and projects it to multiple 2D views. RAD tensors are divided into small cubes in RTCNet [24] and to reduce computational efforts, 3D CNNs are applied. Apart from that, the networks of [16], [25] intakes complex RD spectrum to extract spatial information. But RODNet [26] operates using RA maps for detection, preventing false alarms caused by extended Doppler profile. Recently, utilization of ADC data has gained attention [27], [28] with minimal success.\nThe aforementioned methods operate on raw radar data as standalone inputs. Fusing various sensor data yields com-plementary cues, thereby enhancing performance robustness. The approaches of [29], [30] fuses camera images and projected radar point cloud data in a perspective space at input level. [31], [32] target to fuse at the Region of Interest (RoI) level, while [33], [34] combine RoIs generated independently by different sensors. Architectures like [35], [36] perform feature level fusion by integrating the feature maps from different modalities, while [37], [38] use RoIs to crop and unify features across modalities.\nThe recent architectures that use RADIal [16] dataset to perform fusion are Cross Modal Supervision (CMS) [39], ROFusion [40] and EchoFusion [41]. CMS [39] uses the RD spectrum and takes support from pseudo-labels generated from camera images, while ROFusion [40] associates the RD and image feature maps by additionally using radar points that are located within the bounding box labels, proposing a hybrid point-wise fusion strategy. EchoFusion [41] on the other hand, uses Polar Aligned Attention technique that fuses the features from Range-Time (RT) radar maps and images in a unified Bird's-Eye View (BEV) perspective, which also demonstrated the potential of radar as a low-cost alternative to LiDAR.\nIn contrast to the aforementioned methods, we preprocess the camera data prior to feeding them into our network. Our image processing technique involves the transformation of the front-view camera image to a BEV radar-like represent-ation in the polar domain as described in Section III-B, and the detailed fusion setup is described in Section III-C with experimental setup in Section IV.\nWhen employing raw radar data, it is important to take into account computational complexity metrics. However, previous studies often lack detailed reporting on the resource demands of their models. In this work, we not only compare the performance of our approach with other models in terms of accuracy, but also on resource consumption, as pointed out in Section V."}, {"title": "III. METHOD", "content": ""}, {"title": "A. Problem statement", "content": "The problem statement revolves around the need to present an architecture that solves the proposed perception task resource efficiently by introducing an independent image processing pipeline."}, {"title": "B. Image processing pipeline", "content": "The camera images are typically recorded in perspective view, while radar data can be transformed from raw ADC signal to Range-Angle-Doppler (RAD) tensor, Range-Angle (RA) view, Range-Doppler (RD) view, or Point Cloud (PC) representation. Thus, it is crucial to identify a shared repres-entation such that the sensor data fusion can be achieved to perform the intended task.\nProcessing High Definition (HD) radar data, renowned for its enhanced resolution and complexity, places a considerable demand on computational resources. Consequently, in this approach, we transform the camera image to an RA like representation that entails less intensive computational re-quirements. This transformation involves two steps, as shown in Fig. 2. We emphasize that a taxonomy of algorithms are presented in a recent survey [42] that includes our inspiration PolarFormer [43] which performs object detection in BEV Polar coordinate.\nStep 1: We are given a set of $N$ camera input images with training sample index $n$, ${Img_n \\in R^{H \\times W \\times 3}}_{n=1}^N$. As a first step, the camera image ($Img$) is converted to BEV Cartesian domain image [44]. Given the camera intrinsic parameters ($\\Pi \\in R^{3 \\times 3}$), height at which the camera is mounted ($h$) and pitch value ($p$), the image formation is, $\\epsilon$.\n$\\epsilon = f(\\Pi, h, p)$ (1)\nGiven $\\epsilon$, a BEV Cartesian object is created which is used to transform the given front-view camera image:\n$BEV_{Cartesian} = f_{Cartesian}^{Image}(\\epsilon, \\eta, O, I_{img})$ (2)\nwhere, $\\eta = [5, 50, -22, 22]$ corresponds to the output space as $[x_{min}, x_{max}, y_{min}, y_{max}]$ in vehicle coordinate system. The X-axis oriented forward from the vehicle and Y-axis oriented towards the left. $O \\in R^{rows \\times cols}$ is specified as output image size $[nrows, ncols]$ in pixels.\nStep 2: The obtained BEV Cartesian image is further converted to the Polar domain $f_{Polar}^{Cartesian}[BEV_{Cartesian}]$. Firstly, all Cartesian pixel indices $(x, y)$ are transformed to Polar indices $(\\theta, r)$ using the following equations:\n$\\theta = arctan2(y, x)$ (3)\n$r = \\sqrt{x^2 + y^2}$ (4)\nThe obtained Polar indices $(\\theta, r)$ are projected back to pixel coordinates $(\\theta_{pixel}, r_{pixel})$ for an an image like repres-entation as follows:\n$\\theta_{pixel} = r \\cdot sin(\\theta)$ (5)\n$r_{pixel} = r \\cdot cos(\\theta)$ (6)\nUsing these pixel coordinate values, each channel of the camera image array is projected individually to the BEV Polar image using spline interpolation [45] and restacked. This approach consumes less memory than projecting the three-dimensional image array in one step.\nThis whole independent process of pre-aligning camera images in BEV Polar space offers multiple advantages. Mainly, it eliminates the necessity for feature transformation within the network, potentially enhancing computational efficiency during the training process."}, {"title": "C. Architecture design", "content": "Fig. 3 shows the architecture with radar (bottom block) and camera (top block) network.\nRadar feature extractor: The computational cost involved in processing the raw range-azimuth Doppler (RAD) 3D tensor is higher. Therefore, the use of a denser Range Doppler (RD) map is an alternative consideration, especially when there is a possibility to recover the angle information from RD maps [16].\nThe radar sensor used comprises 12 transmitting antennas and 16 receiving antennas, resulting in a total of 16 channels within the input tensor. This means the signature of any object, say a vehicle in front, will be visible 12 times for each receiving antenna. In particular, it will be measured at range-Doppler positions ($R, (D + k\\Delta)_{[D_{max}]})_{k=1,...,12}$ where $\\Delta$ denotes the Doppler shift that is caused by the phase shift $\\Delta$ in the transmitted signal. $D_{max}$ is the maximum measurable Doppler value. If the measured Doppler value $(D + k\\Delta)$ exceeds $D_{max}$, it will be truncated to fit within $D_{max}$, ensuring that all measured Doppler values fall within this allowable range.\nThe Range-Doppler (RD) tensor hence is organized as complex numbers (R+iD) representing Range and Doppler values. Rearranging and concatenating the real and imaginary parts of this tensor results in 32 channels for the input with 512 and 256 range and Doppler bins, respectively. To this end, we adapt a Multiple Input Multiple Output (MIMO) pre-encoder [46] that reorganizes this RD tensor into a meaningful representation for the resnet-50 [47] like encoder blocks with 3, 6, 6 and 3 residual layers respectively from FFTRadNet [16].\nMore specifically, the extracted feature tensors can be viewed as azimuth, range, Doppler, respectively. Since the objective is to acquire the angle information, channel swap-ping strategy is employed by swapping the Doppler and azimuth axes before upscaling the feature maps. This is depicted with a rhombus highlighted in purple in Fig. 3. As a result, we seek to learn a dense feature embedding of RA maps, thus recognizing their relevance to the subsequent object detection task.\nCamera feature extractor: As explained in Section III-B, the camera images are processed (refer Fig. 2) to obtain a Bird's-Eye View RA Polar representation. This representa-tion is the input to our camera only CNN model. We have chosen this representation as it directly relates to the decoded features of the radar only model, which in turn supplements the radar features upon fusion, as shown by a thick black fusion circle in Fig. 3.\nThe camera only model starts with a pre-encoder block that performs an initial feature extraction with a standard kernel size of 3. Our Feature Pyramid Network (FPN) encoder is composed of 4 blocks with 3, 6, 6, and 3 residual layers, respectively [47]. Each encoder block here performs a 2\u00d72 downsampling which leads to a reduction of tensor size by a factor of 16 in height and width. This downsampling is to prevent losing the signature of small objects that are few pixels in BEV Polar image.\nChannel swapping strategy is not required within the decoder of camera only network, since the extrac-ted features already take the form of Range-Azimuth (RA) like representation. It is important to understand that the channels are still swapped twice for a dif-ferent purpose. After the second basic block (BB in Fig. 3 represents Basic Block), the channels are swapped $(128x128x64 swap 64x128x128)$ to increase dimension of the azimuth axis $(64x128x128 \\xrightarrow{conv2d} 224x128x128)$ using a convolutional layer. Further, it is again swapped back so that the RA tensor is regained. This strategy allows to view the decoded feature map in a dimension $(128x128x224)$ that helps the network in effectively fusing with the radar features during training by reducing computational overhead. Please note that our camera only architecture backbone module could be replaced by heavier models depending on computational budgets (refer to Section VI).\nFused inputs to the detection head: The RA latent"}, {"title": "IV. EXPERIMENTAL SETUP", "content": ""}, {"title": "A. Dataset", "content": "The RADIal dataset is a compilation of two hours of raw data from synchronized automotive-grade sensors (camera, laser, and High Definition radar) in a variety of settings (city street, highway, country road) that includes GPS data. The three sensors are synchronized for about 25,000 frames, of which 8,252 are labeled with a total of 9,550 vehicles. Recent radar-camera fusion surveys [48], [49], [50] compare the other relevant publicly available dataset."}, {"title": "B. Training details", "content": "The network was trained on a workstation equipped with an Intel Core i9-10940X CPU, a Nvidia RTX A6000 GPU and 52 GB RAM. The dataset is randomly split so that 70% corresponds to the training data and the remaining is the validation and test set (approx. 15% each). Training is carried out for 100 epochs using Adam optimizer [51] with mini-batches of size 4. The initial learning rate of le-4 has been set with a decay of 0.9 for every 10 epochs.\nSince a large proportion of the scene belongs to back-ground, we use focal loss [52] on the classification output, so that the training process is stabilized thus avoiding the class imbalance issue. The smooth L1 loss is used on the regression output specifically for positive detections.\n$L_{det} = Focal(y_{cls}, \\hat{y}_{cls}) + \\alpha Smooth-L1(Y_{reg}, \\hat{y}_{reg})$ (7)\nwhere $y_{cls}$ and $\\hat{y}_{cls}$ represent the ground truth and predicted values for classification, respectively. $Y_{reg}$ and $\\hat{y}_{reg}$ represent the ground truth and the predicted values for regression part, respectively. $\\alpha$ is a hyperparameter that balances the contributions of the two loss components. Please refer [53] for details."}, {"title": "C. Evaluation metric", "content": "We use Average Precision (AP) and Average Recall (AR) with an Intersection-Over-Union (IoU) threshold of 50% as our accuracy metrics in all experiments. F1-Score is computed from AP and AR directly: $F1 = \\frac{2 \\times AP \\times AR}{AP+AR}$ We also present the absolute Range (in meters) and Angle Error (in degrees) as follows:\n$RE = \\frac{\\sum_{n=0}^{N} \\sum_{m=0}^{M} (r_m - \\hat{r}_m)}{M_{total}} ; AE = \\frac{\\sum_{n=0}^{N} \\sum_{m=0}^{M} (\\theta_m - \\hat{\\theta}_m|)}{M_{total}}$ (8)\nwhere, $r_m, \\hat{r}_m$ are the ground truth and the predicted range values of an object in meters. $\\theta_m, \\hat{\\theta}_m$ are the ground truth and the predicted azimuth values of an object in degrees. $M$ denotes the number of objects in a particular frame. $N$ denotes the total number of frames in the test data. $M_{total}$ is the total number of objects in the test data. The AP, AR, Range Error (RE) and Angle Error (AE) are computed for different classification confidence score thresholds from 0.1 to 0.9 with a step of 0.1 and averaged by following the official implementation [16].\nAdditionally, we evaluate the model's complexity by comparing the total number of trainable parameters (#: in millions), the Average Frames Per Second (FPS); for a given model, an FPS value is computed for each frame in the test set and averaged. We also calculate the standard deviation (\u03c3) from the FPS values, where a lower \u03c3 indicates a more consistent performance across frames. Furthermore, we consider the size of the model in megabytes (MB) and GPU memory cost in gigabytes (GB) as shown in Table I."}, {"title": "D. Baselines", "content": "Most of the methods presented in Sections I and II rely on low-resolution traditional radars and can encounter difficulty in accommodating HD radar data due to memory constraints. As HD radar is used in this work, we consider as baselines the state-of-the-art models that have been trained on the RADIal dataset. As discussed in Section II, FFTRadNet [16], TFFTRadNet [27], ADCNet [28] are some of the works that fall under this category. Since we focus on camera and raw radar data fusion in Bird's-Eye View for object detection, the Cross-Modal Supervision (CMS) [39], ROFusion [40] and EchoFusion [41] are closely related."}, {"title": "V. RESULTS", "content": "This section conducts a comprehensive evaluation of our model, featuring quantitative and qualitative results analysis with visuals of predictions."}, {"title": "A. Quantitative Evaluation", "content": "We compare our model performance not only on accuracy metrics, but also on the computational parameters presented in Section IV-C. The GPU cost is influenced by various hyperparameters, with batch size being one of the most significant factors. We used the same training parameters as other models for a fair comparison as presented in Section IV-B.\nAccuracy: We outperform the existing fusion detection frameworks in Range and Angle Error which indicates that the detected objects are accurately localized in the scene. Achieving second best F1-Score and marginal difference"}, {"title": "VI. ABLATION STUDY", "content": "To deepen the performance, complexity analysis and val-idate the choice of our network architecture and its input components, we conduct an ablation study with different backbones, presented in Table III. Specifically, the ablations are carried out by replacing our camera backbone and not the radar, due to the unique complex alignment of the raw radar data, as explained in Section III-C. We compare the original ResNet-50 [47], EfficientNet-B2 [54] and ResNet-18 backbone-based model with a transformer decoder called R18-UNetFormer [55] with our model results from Table I.\nIt is evident that our lightweight model is resource and storage efficient, performing at a high-speed frame rate of 58.91 FPS."}, {"title": "VII. LIMITATIONS", "content": "The height of the camera mounted above the ground and the pitch of the camera toward the ground are important parameters when transforming the front-facing camera data to a BEV object. Furthermore, as stated in Section III-B, the area in front of the camera (0 to 50 meters) as well as to either side of the camera (22 meters) are defined to the best of our knowledge as RADIal camera sensor suite does not have range specification information.\nAlso theoretically, the green ground truth point in the BEV image in Fig. 5 is affected by an offset. There could be one or several reasons for this discrepancy, such as a transformation error from camera to BEV Polar space due to vehicle pitch variations, imprecise intrinsic or extrinsic camera calibration, labelling method employed and poor synchronization between the camera and radar. Nevertheless, our predictions followed the ground-truth as expected.\nOur method could be deployable within autonomous driv-ing systems. However, improper understanding or usage may lead to performance degradation, thereby increasing security risks."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this work, upon proposing a fusion strategy in BEV space, we analysed how the performance affects the com-putational metrics in various aspects. Our approach demon-strates proficient performance while upholding a comparat-ively lower level of computational complexity which align with our research motivation and results shown on RADIal dataset. Nevertheless, obtaining high-quality time synchron-ized multi-modal data with precise annotations require con-siderable effort. Hence a further potential direction is to build a diversely recorded large-scale high-quality dataset to accelerate further research."}]}