{"title": "ASSISTIVE AI FOR AUGMENTING HUMAN DECISION-MAKING", "authors": ["Natabara M\u00e1t\u00e9 Gy\u00f6ngy\u00f6ssy", "Bern\u00e1t T\u00f6r\u00f6k", "Csilla Farkas", "Laura Lucaj", "Attila Menyh\u00e1rd", "Krisztina Menyh\u00e1rd-Bal\u00e1zs", "Andr\u00e1s Simonyi", "Patrick van der Smagt", "Zsolt Z\u0151di", "Andr\u00e1s L\u0151rincz"], "abstract": "Regulatory frameworks for the use of AI are emerging. However, they trail behind the fast-evolving malicious AI technologies that can quickly cause lasting societal damage. In response, we introduce a pioneering Assistive AI framework designed to enhance human decision-making capabilities. This framework aims to establish a trust network across various fields, especially within legal contexts, serving as a proactive complement to ongoing regulatory efforts. Central to our framework are the principles of privacy, accountability, and credibility. In our methodology, the foundation of reliability of information and information sources is built upon the ability to uphold accountability, enhance security, and protect privacy. This approach supports, filters, and potentially guides communication, thereby empowering individuals and communities to make well-informed decisions based on cutting-edge advancements in AI. Our framework uses the concept of Boards as proxies to collectively ensure that AI-assisted decisions are reliable, accountable, and in alignment with societal values and legal standards. Through a detailed exploration of our framework, including its main components, operations, and sample use cases, the paper shows how AI can assist in the complex process of decision-making while maintaining human oversight. The proposed framework not only extends regulatory landscapes but also highlights the synergy between AI technology and human judgement, underscoring the potential of AI to serve as a vital instrument in discerning reality from fiction and thus enhancing the decision-making process. Furthermore, we provide domain-specific use cases to highlight the applicability of our framework.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Motivation and AI", "content": "Advances in Artificial intelligence (AI) reached the level that they approach or even surpass human performance and can be used in a variety of applications. These advances are driven by research and development on foundation models [18], such as GPT-4 coupled with DALL-E, BERT, and related specific tools, such as Stable Diffusion, neural radiance fields, CodeLlama. We are in a transition phase, where the integration of AI into society profoundly shapes the future and well-being of humanity. The consequences of current and evolving AI-driven advancements in social, economic, and international realms are challenging to fully comprehend; including the intricacies of and rapid pace of the transformation unfolding during this integration.\nIn the long term, the emergence of superhuman intelligence will influence this transformation. Reaching this level may be close, but neither that achievement nor its consequences are within the scope of this paper. We are concerned about the misuse of AI. It is already present and can create substantial pitfalls for society. Powerful hacker tools and personalizable text generation software have already emerged, and they don't require advanced technical expertise. The hacker tools contain established attack tools and are capable of automated attacks. Text generation tools can generate persuasive fake phishing texts and email accounts. In addition, social media platforms spread fake information very quickly [24]. How can we get reliable information to make informed decisions that are in line with our goals and values, and are congruent with the law?\nThe debate on AI seems to focus primarily on whether and to what extent the technology can replace human resources in different professions or activities [162, 108, 106], how to manage AI risks [79], and how to regulate AI, see, e.g., [111], and [154] and the references to the regulations in different countries therein. Meanwhile, there is less emphasis on how AI can help us to make decisions on a sound basis. The reality is that, both professional services and most platform services, AI is already helping people to make decisions [136], while the reliability of the information technologies on which those decisions are based is questionable and falsification of information is possible.\nSome decisions can be made by AI itself. For example, consider the decisions on acceleration and slowing down for self-driving cars. Such AI decisions can be achieved given appropriate testing and a social consensus on the associated"}, {"title": "1.2 Why Assistive AI?", "content": "In this paper, we aim to introduce a concrete model for exploiting the positive application opportunities inherent in AI to those individuals and their communities that are looking for reliable guides in the increasingly the complex digital reality that surrounds us. The concept that we present here complements recent developments related to the regulation of AI (see Figure 1). However, we are offering a different approach by supporting safe, reliable, and accountable AI applications. The initiatives of the European Union \u2013 the AI Act and the AI Liability Directive (AILD) can be considered as particularly remarkable regulatory developments worth supplementing with application models in a complementary approach. In addition to the AI Act (which focuses on the common risks associated with the use of AI systems), and the AILD (which settles the liability of potential damage caused by AI), the present concept outlines a possible AI model that can serve as a concrete practical solution to some challenging issues.\nRegulations, especially certain requirements of the future AI Act, are important starting points for our model, but our focus is on moving forward and supplement the regulatory approach as follows:\n\u2022 While the regulation focuses on the common negative aspects of AI systems (their risks or damages) and their management, we outline a special application model of AI, with which AI itself can become one of our important supports in clarifying the reality around us.\n\u2022 While in the regulation, AI is the object of control (that is, we bring AI under control), in our model, AI is the means of control (which helps us in bringing other processes under control).\n\u2022 While for the regulation, AI is a product whose development and application must be guided by requirements, for us, Al is a special instrument to examine the enforcement of requirements on other products, outputs, etc. (in our terminology below: communications)."}, {"title": "1.3 Central concepts", "content": "There are a few critical concepts and components that we consider necessary for the efficient and correct functioning of Assistive AI. A brief overview of these concepts is given below."}, {"title": "1.3.1 Anonymity and accountability", "content": "In an era rife with misinformation, we believe it is crucial to have systems that ensure authors are accountable for their content to foster trust. However, scenarios exist where authors might need to stay anonymous, such as when revealing information ethically necessary but contractually restricted. For example, if a breach of the law is observed but a contract prevents the disclosure of this information. Conversely, anonymous disclosures can inadvertently cause harm or spread falsehoods, raising liability issues. A solution involves algorithms that preserve anonymity until a legitimate authority deems it necessary to reveal the author's identity for accountability. We shall elaborate on this critical issue in Section 3.4."}, {"title": "1.3.2 Credit and credibility", "content": "Similarly to anonymity and accountability, the reputation or credit of an author could convey the significance of the author's communications. There are numerous approaches to grasp the real meaning behind credibility. Hovland, et al. define it from the communication science perspective, where credibility refers to the property of a source based on the recipient's acceptance of the source's message [72]. Other definitions include credibility as a perceived value by the recipient or the relevance and truth value of the conveyed information [130]. Numerous studies point out that in modern media, human-computer interactions, and online communication credibility is closely related to relevance, communication quality, truthfulness, and eventually the trust of the recipient [83, 130, 164, 169].\nIn this work, we identify two possible sources of communication and interaction: a) Authors and a Board. Authors of communications belong to a given community. A Board oversees a given community by designing and controlling"}, {"title": "1.3.3 Role of Boards", "content": "Through the supervision of legal compliance, the State ensures that the concerned entities affected by the legal act in the course of exercising their rights and obligations, or their operations shall comply with the provisions of the legislation in force.\nOur concept, illustrated by Figure 3, is based on the fact that in communication data must be authentic, up-to-date, real, and legitimate, i.e., not only the operation of an organization or a community must be legitimate, but also the legitimacy of the data/d-credit must be ensured in the process, on the reason that the two are closely in line and interact back and forth.\nThe authority in charge of supervision of legal compliance, in our case the Board, has the right to indicate or authorize a correction based on findings.\nIf the organization or community fails to remedy a situation that is in breach of (law), to commence the procedure may be requested for as long as the situation persists and shall continue until the lawful situation or lawful operation is restored.\nThe Board shall take its action depending on the circumstances and severity of those circumstances."}, {"title": "1.4 Main contributions", "content": "Our main contributions are as follows:\n\u2022 We put forth a novel Assistive Artificial Intelligence framework for communities. The basic unit of the community is the Board which must be accountable by law. Boards are partially ordered and hierarchically extendable.\n\u2022 We show how cutting-edge Assistive Artificial Intelligence tools may complement and extend regulatory environments. Our proposed Assistive AI assesses the reliability of communications, serving as a proactive tool that can also evaluate the credibility of information sources. It is also fast and able to keep up with the revolutionary AI developments, supporting decision-making, and securing community interests.\n\u2022 Assistive Artificial Intelligence framework combines anonymity, accountability, reliability, and legal compli-ance, decreasing the risk in Al governance.\n\u2022 At the heart of the Assistive Artificial Intelligence framework is the collaboration between humans and AI, with automation capabilities but under human control.\n\u2022 Assistive Artificial Intelligence is a high-level concept at present involving cutting-edge AI and cybersecurity methods. Its physical realization is feasible as the components are available. Thus, incremental, step-by-step construction and implementation of the Assistive Artificial Intelligence system is feasible. We review the testing methods required by Assistive Artificial Intelligence for the AI tools to be included.\n\u2022 We provide 4 examples, such as recommendation on advertisements of drugs or health services, opinion about the credibility of communication on academic subjects, financial product recommendation, and financial credit scoring."}, {"title": "1.5 Structure of the paper", "content": "The paper is organized as follows. We start with a section on related works 2. This section briefly reviews, e.g., AI-based recommendations, and community and public sector decision-making. We list the critical components that we need for our work and provide the references. Section 3 is about components, such as the Board concept, questions related to security and privacy, anonymity and accountability, and ethical issues. The Assistive Artificial Intelligence Framework is treated in the next section (Sect. 4). Sample use cases can be found in Sect. 5. We discuss our approach in Sect. 6. Conclusions are drawn in Sect. 7. The appendix (Sect. A) contains additional AI-related details about the financial use case (Sect. A.3), detailed consideration of recommendation and advertisements for drugs and health services (Sect. A.1), and our opinion on the credibility of communication of academic subjects (Sect. A.2)."}, {"title": "2 Related work", "content": "Our focus lies in using Assistive Artificial Intelligence to enhance decision-making processes, which can apply to indi-vidual choices or decision-making within groups and public entities. On an individual level, AI-driven personalization, like the kind seen in services offered by Netflix, Amazon, Google, and numerous other online platforms, provides tailored recommendations that aid users in decision-making. Similarly, AI technologies may offer valuable support to both patients and healthcare professionals by delivering personalized healthcare recommendations. Additionally, this customized approach is increasingly adopted in the financial sector, guiding individuals in managing their finances and making investment choices more effectively, but also raising ethical challenges, and risks due to the lack of transparency and the possibility of generated fake information (see, e.g., Khan and Umer [89] and the references therein).\nCommunity and public sector decision-making processes are closer to our theme. In this case, resource allocation, urban planning, and public safety could serve smart cities [78]. Applications may arise in managing public health crises, including pandemic response and vaccine distribution strategies [149]. Environmental management is another field. Here, AI may provide advice in environmental protection and sustainability efforts, including climate modelling and conservation strategies, see, e.g., [150, 141]. ELSI (Ethical, Legal, and Social Implications) can also take advantage of Assisitive AI, e.g., in discussions on (a) biases in AI algorithms and their impacts on decision-making for diverse populations, (b) privacy and security issues, (c) regulation and governance, and (d) specific instances where AI has been successfully implemented. New AI technologies and methods will significantly improve decision-making capabilities in different sectors. Tools that enable the combination of artificial and human intelligence to optimize decision-making processes are flourishing. AI can assist by gathering, summarizing, and collating information across data landscapes and the trend is propelled by advances in Natural Language Processing (NLP) and Understanding (NLU). Beyond the different views that frequently appear on websites [114], papers in scientific journals started to study AI approach to ethical decision making, e.g., in human resource management [131], decision support in patient care [50], and also considering the effect of superhuman intelligence on human decision making [138], to list only a few."}, {"title": "3 Background", "content": "Before presenting the concept of Boards and discussing questions related to security and privacy, the method of accountable anonymity, and ethical issues, we examine the basic concepts of law and how artificial intelligence can be used in this field."}, {"title": "3.1 Possible paths for artificial intelligence in the field of law", "content": "Law transforms plural values of the society into the binary code of right and wrong [63]: The law involves social evaluation, reflected in the court's judgement in each individual case [172].\nDecisions in social conflicts take into account the specific contexts and are influenced by a careful balancing of all the interests involved. The leading guideline is equal treatment of equal situations, the desired model of judicial adjudication.\nJudgements frequently involve prioritizing competing values, such as freedom of speech versus human dignity. Con-sequently, any algorithm designed to support the enforcement of law must, to some extent, be capable of performing social evaluations.\nRegulation, contrasted with law, is supposed to prescribe clear-cut rules for social coordination and does not necessarily reflect values. Instead, it implements the direct policy of the regulatory power. Algorithmic legal decision-making by"}, {"title": "3.2 The concept of Boards and related entities", "content": "Boards, as mentioned in the Introduction, have functions that include leadership, decision-making, and oversight responsibilities, among others. Note that the word 'Board' can be used in different senses. To disambiguate what we mean, we provide two examples. We also describe the concepts of Consultants and Auditors (sketched in Fig. 2) through the example of the Editorial Board of the journal and the other example, the relation between the Food and Drug Administration and the Internal Review Boards of clinics."}, {"title": "3.2.1 Example 1: The editorial Board of a journal", "content": "Reviewers of the Board are much like consultants. They offer their expertise and knowledge to assess the quality, accuracy, and relevance of the manuscript including the relevance for the intended readership. They provide constructive feedback, may suggest improvements, and provide recommendations on whether a manuscript should be accepted, revised, or rejected. Their role is advisory and supportive, aimed at enhancing the quality of the published work and thus, the venue.\nJust as consultants are often hired for their specialized knowledge in a particular field, reviewers are selected based on their expertise in the subject matter of the manuscript. They apply their understanding and evaluate the contribution of the paper to the field."}, {"title": "3.2.2 Example 2: Food and Drug Administration and the Internal Review Board of a clinic", "content": "The Food and Drug Administration (FDA), the Internal Review Boards (IRBs) and researchers of a clinic make another example of our multilevel structure.\nThe IRB is a board that has the authority to approve, require modifications in, or disapprove research studies based on their assessment of risks and benefits to participants, the informed consent process, and the protection of participants' rights and privacy.\nThe FDA, on the other hand, is another large unit and IRB and FDA are two critical components of a broader framework that governs clinical research in the USA: (a) FDA focuses on regulatory oversight for the safety and efficacy of products entering the market and (b) IRBs focuses on the ethical conduct of research and protection of human subjects. Now, the FDA has several advisory committees, including the FDA Science Board (which is not a board in our nomenclature as it does not make decisions) and members of these committees can be seen as consultants who offer guidance, recommendations and professional opinions for the FDA to make informed decisions. The Office of the Commissioner of the FDA, on the other hand, is similar to the Editorial Board of a journal even though there are fundamental differences in their roles, responsibilities, and the contexts in which they operate. However, there are aspects of their functions that can be seen as somewhat analogous, particularly in terms of leadership, decision-making, and oversight responsibilities."}, {"title": "3.3 Relevant questions related to security and privacy", "content": ""}, {"title": "3.3.1 AI testing safety and monitoring", "content": "Currently, many issues have been exposed by academia and investigative journalists around the absence of proper oversight mechanisms internally to test AI systems before deployment, as well as external mechanisms, such as regulations, certification, and third-party auditing to enforce and mandate better practices for the companies developing such systems [107, 31, 171, 128]. One of the most pressing issues seem to remain in the monitoring and testing of AI systems, which carries enormous costs as well as implementation issues [16].\nMany AI systems could cause significant harm. This includes foundation models, which have been put on the market and subsequently have been found to need better mitigation strategies to keep up with their rapidly increasing performance that can be used for malicious intent [140, 57]. Currently AI developers seem to have issues with scaling safety operations, which results in many cases of bypassing the safety filters of systems such as ChatGPT or Stability AI [140]. To prevent such harm from occurring, dedicated and strong forms of transparency from foundation model developers are necessary [17].\nTesting of AI has not reached the maturity of testing in software engineering, where testing is seen as a fundamental practice to develop well-functioning software [21]. It is key to learn from the software engineering field. It can guide how to navigate the technical challenges that arise as organizations have to develop large-scale solutions, whilst keeping the systems robust and secure by enabling the adoption of quality assurance practices [8]. The maintenance practices"}, {"title": "3.3.2 Potential practices", "content": "Red team exercises have been deployed in the software engineering domain to address the potential misuse of the systems as well as unveil its potential vulnerabilities [12]. Audit trails are another important practice that provides an overview of the important processes that determined the development of a system as well as keeping track of the incidents and failures emerging during the development phase [139, 12, 21].\nBias and safety bounties are an established practice consisting of the financial rewarding of security experts who need to expose potential vulnerabilities of the system before deploying it in the market phase [12, 21].\nNonetheless, the development of AI systems poses new challenges for practitioners as opposed to software application domains. For instance, the data management practices are much more complex; model customisation and reuse require different skills from the software domain; and the AI components are more complex to handle as distinct phases as they all intertwine with another much more than is customary in traditional software approaches [8, 73]."}, {"title": "3.3.3 Privacy preservation", "content": "Protecting the individual's private information has been studied extensively in statistical databases (see Denning and Schlorer 35, Farkas and Jajodia 49 for an overview). Statistical inference control aims to protect private data while allowing the computation of useful statistics. Inference control techniques are based on limiting the users' statistical queries, anonymizing and suppressing data, and introducing noise in the data in a way that the modified data still lead to useful statistics, but the individuals' records are not real. These techniques are still used to provide privacy protection in large datasets used for statistical purposes, such as the United States Census Bureau.\nWith the advent of large-scale data analytics to support data mining applications, the need to provide privacy protection that is suitable for these applications surfaced. In 1998, the concept of k-anonymity was proposed by [134]. The promise of k-anonymity is that an individual's personal data cannot be distinguished from at least k-1 other individuals' data. During the early 2000s, researchers proposed methods to strengthen the k-anonymity model by addressing attacks based on the homogeneity of the data and the attackers' background knowledge (see Sweeney 148, Machanavajjhala et al. 110, Kabir et al. 84 for representative examples). While there are well-documented limitations of the k-anonymity techniques, they represent a practical approach to express and enforce privacy requirements [13]. These works also formed the foundation of further studies on privacy-preserving data mining in a variety of context [159, 160, 87, 142, 11, 4, 166]. A fundamental aspect of all proposed approaches is the trade-off between the accuracy of the data mining application and the level of privacy protection. There are two main approaches to introducing noise in the data mining application: 1) introduce noise in the data set before the mining application and 2) introduce noise in the learned model. In each case, the tradeoff is to achieve a useful result while guaranteeing a privacy protection level.\nDifferential privacy [52, 119, 41] provides a formal measurement of the level of privacy. Intuitively, complete privacy is achieved if the learned model from the original dataset is the same as the model learned by removing any of the individual's data. That is, the dataset does not reveal any specific information about an individual. The amount of noise introduced in the dataset is dependent on the privacy sensitivity of the individual's personal data and the usage of the data in statistical analysis. Local differential privacy is achieved by the individuals adding noise to the dataset containing their personal data. The advantage of this approach is that each individual modifies his/her data, therefore no one else will know the real personal data. However, this approach may result in too much noise and may reduce the accuracy of the application. Global differential privacy is achieved by adding noise to the output of the system. The advantage of this approach is a high level of accuracy at the cost of trust in the database curator."}, {"title": "3.4 Anonymity with accountability", "content": "Accountability is the ability to hold active entities responsible for their actions. A system may provide accountability at multiple levels. For example, accountability may be enforced on a group instead of a particular individual or enforced within a virtual environment without addressing consequences in the real world. Enforcing accountability requires some form of identity management. In digital systems, strong authentication enables linking actions to user identities. Depending on the level of accountability, these identities may be virtual identities, such as usernames and/or links between multiple virtual identities, or identities of the human users, systems, or applications running on behalf of the users.\nA seemingly conflicting requirement for accountability is privacy protection. Users may not be willing to share their identities with other users or computer systems. Similarly, user actions may not be linked to real users. For example, in oppressive regimes, human lives may be at risk if someone carries out an action that is against that regime. The problem is further complicated in distributed and autonomous systems without a trusted identity management capability.\nIn earlier works [49, 186] it was shown that one can achieve both privacy protection and accountability even in the absence of an untrusted identity management server. A community-based identity management and accountability framework was proposed. This system only allows the users' virtual or real identities to be revealed if a quorum of the community members agree to it. The basic assumption is that members of a community trust the community to make the right decision. The method provides a proven cryptographic approach to identity management. However, potential privacy breaches due to data analytics and behavioural modelling of the community members were not addressed.\n[58] aims to define accountable universal composability, focusing on the modular analysis of the accountability framework. However, the privacy impact of the rapidly evolving Artificial Intelligence (AI) systems has not been sufficiently studied.\nTo address accountability in Al systems, information about data quality and provenance must be available and trustworthy. This requirement is difficult to achieve when dealing with data collected from open sources or untrusted (unverified) depositories. Moreover, federated computing and secure multiparty computing may not support data provenance recording. During the last decade, several data provenance approaches have been proposed to support a variety of applications, from classical databases [22] to tracing misinformation [5], from scientific workflows [3] explainable AI [85]. The provenance data must be protected from unauthorized and unethical modifications. Technologies, such as blockchain [156] are proven to provide reliable and secure integrity protection of data and metadata."}, {"title": "3.5 Ethical issues", "content": "Research has shown the unintended negative consequences emerging from over-reliance on AI systems in sensitive decision-making contexts, without a clear understanding of the limitations and safe extent of the use of the various models [116]. In the criminal justice system, AI has been used for recidivism prediction. Al helps to assess the likelihood of a convict re-offending, thereby informing parole decisions and rehabilitation efforts. Predictive policing uses AI algorithms to analyse historical data and predict potential future crimes. This may help to allocate police resources effectively [48]. However, these systems have been producing disparate results and are prone to discriminating against vulnerable minorities. For example, cases of scoring African-American defendants with a false positive score of higher likelihood to recommit a crime than white defendants have been documented [168, 98]. Such disparate results have been caused by the problematic data quality selection for the algorithm, which scored based on the neighbourhood of provenance and family history in crime, which are both outside of the individual responsibility [168, 98]. Therefore, such systems could reproduce harmful biases when used in these sensitive decision-making contexts.\nIn the domain of human resources, AI assists in screening resumes, predicting candidate success, and even monitoring employee productivity, and has been deployed to attempt more data-driven hiring and management decisions [93]. Nonetheless, such systems have been perpetuating biases in terms of gender, ethnicity, sexual orientation, or other characteristics when selecting candidates [91]."}, {"title": "4 Assistive Artificial Intelligence framework", "content": "In this section, we describe an Assistive AI framework that we envision. For the sake of simplicity, oftentimes we refer to this system as if it was already realized. It should be noted that this Assistive Artificial Intelligence framework is a high-level abstract concept. Some of the building blocks of this Assistive Artificial Intelligence framework are already present in state-of-the-art methods and tools that we partially overview in the Appendix A). Implementation (at least in part) can be the subject of future work."}, {"title": "4.1 Preliminaries", "content": "Communication is defined as information which is not private anymore, or, a product becoming available to customers. If the communication is available to everybody, then it is public. We propose a system of appropriate procedures for (i) credible and accurate estimation of product characteristics and quality, and (ii) rules limiting and controlling product use, through legal redress for the maker of the product, i.e., accountability, without infringing freedom of expression and the protection of personal rights.\nThe principles are illustrated through examples. Modern computing and AI methods complement these illustrations, and we give examples in a separate section. They present the potential uses of Assistive AI today and show some of the current shortcomings highlighting future research and development challenges."}, {"title": "4.2 Main components of the system", "content": ""}, {"title": "4.2.1 Knowledge base", "content": "The Knowledge Base component holds the governing rules as well as facts of the community that utilizes Assistive Artificial Intelligence. Instances of this component are distinguished based on the type of knowledge they hold as well as the Board and its level of authority they belong to.\nThe two types of knowledge we would like to represent are factual knowledge and rules.\nFactual information serves as a cornerstone in decision-making processes. This category is diverse, encompassing a wide range of facts that are instrumental in guiding judgements and conclusions. Specifically, these facts can be drawn from legal precedents, including both statutory provisions and individual case law, which provide a rich repository of legal reasoning and findings to inform future deliberations. Additionally, this category includes what might be termed as accepted or common-sense evidence. This spans historical facts, which offer context and background for understanding current events and decisions, and scientific statements, which are underpinned by empirical research and evidence, lending credibility and reliability to the conclusions drawn from them.\nThe selection of relevant or necessary facts is specific to the particular domain or community in question. Different fields or areas of study may prioritize certain types of facts over others based on their relevance, applicability, and the norms that govern that sphere of knowledge. For instance, in the legal domain, the emphasis might be on case law and statutory texts, while in scientific communities, the focus would be on data derived from rigorous experimentation and research.\nMoreover, these facts are dynamic: they are influenced by new discoveries, changing social norms and changes in the legal framework. This calls for a flexible and adaptive approach to knowledge representation and inference so that the knowledge base system can adapt to changes and check the consistency of the knowledge space occupied by modified and new facts.\nThe term \"rule\" may carry different interpretations, which merit further exploration within the context of the law. In our framework, a rule is the description of the required social conduct or its result. Rules shall be derived a) directly from normative legal instruments (written or unwritten constitutions, laws, [acts] and other regulatory documents) produced by legislators or b) from judgements of courts and other decision-making instances (like different regulatory agencies). The latter could be (i) judicial or official interpretation of legal norms and standards [109] or (ii) a result of a"}, {"title": "Legal Rules and their relations to Assistive Artificial Intelligence", "content": "When developing the knowledge base of Assistive AI, we conceptualize the law as a hierarchical system of behavioural rules [88] in which the rules described in the legal system's normative documents (made specifically for regulatory purposes) are given concrete forms by the decisions of judicial forums and other bodies with decision-making powers. We distinguish three main levels of rules in the system: the level of fundamental rights (constitutional rights), rules of conduct, and regulatory norms (the distinction between rules and regulations, see above) contained in other legislation and individual decisions.\nFundamental rights are at the top level in the hierarchy of rules. In most cases, written constitutions contain them. Their role is twofold: on the one hand, if the rules of the legal system come into conflict with each other, they help resolve the conflict. On the other hand, fundamental rights serve as a yardstick in the interpretation of generally formulated rules [44, 43]. Ethical rules [46] concerning AI are not included directly in our framework, but rules, especially rules of fundamental rights incorporate ethics in an indirect way.\nLegal rules and regulations are closely interrelated part of the system. Since Assistive AI is a general concept, these rules can be diverse, depending on the particular domain the Assistive Artificial Intelligence is supporting. We have to distinguish between two types of normative structures, law and regulation. Although there is no sharp line between the two norm types, the law is more based on social evaluation and consideration of interests, while regulation more functions like a rule-based system (or an algorithm, in certain cases), which prescribes clear-cut procedures for social coordination, does not necessarily reflect values, and implements direct policy of the regulatory power.\nThe use cases of Section 5 contain a mixture of legal and regulatory tools. The Assistive AI should be able to handle both normative structures to generate recommendations in a particular situation.\nThe judicial decisions and decisions of other conflict resolution instances (hereinafter both will be called judicial decisions) provide examples of rules, but sometimes they act as rules themselves, especially in case law (common-law) systems [71, 39, 145, 6]. In different legal systems, this rule-character of judicial decisions prevails to a different extent. Cases play an important role in the so-called civil law (continental) systems and in these systems, their interpretative (explanatory) character is stronger [67, 152].\nOne may hope that a high-level Assistive Artificial Intelligence system will be capable of suggesting the formulation of rules from a set of facts if it is desired. We outline this in Section 4.3.5.\nOne may also differentiate between knowledge that can be modified by the Board of a community, the \"Internal Knowledge Base\" and knowledge that is external to the community forming the \"External Knowledge Base\". The \"Policy of the Board\" refers to the rules contained by the Internal Knowledge Base. The External Knowledge Base includes \"Law\" and \"General Facts\" as those rules and facts are superior to the community and cannot be affected by the Board of that community. Such knowledge bases involve the external legal ecosystem of the community as well as common sense facts and judicial decisions that cannot be changed by the community."}, {"title": "4.2.2 Author", "content": "The author is an entity that communicates (e.g., produces products), and may have a historical d-credit score based on previous communications and the related d-credit scores. The d-credit score, according to our definition, is domain dependent. Overlapping domains are possible. The entity that authored a communication can be:\n\u2022 \"fully anonymous\" if nobody can provide evidence that a communication belongs to a given entity unless the entity voluntarily chooses to reveal its identity;\n\u2022 \"anonymous, but accountable\", if the identity of the entity can be uncovered according to the rules of the community; and\n\u2022 \"known\", if the identity of the communicating entity is available within a community or to everybody.\nNote that anonymous author accountability involves proving that a given person belongs to a given code. Care must be taken in this regard, as this task is becoming increasingly difficult due to rapidly evolving faking technologies."}, {"title": "4.2.3 Communication", "content": "Communications are at the centre of the Assistive Artificial Intelligence model. In contrast to AI regulations, our system is not about controlling AI, but using AI to evaluate, filter, or possibly control communications so that individuals and"}, {"title": "4.2.4 Derived value-related information", "content": "Information collected and processed from historical facts and other resources (including the Knowledge Base) and used for the estimation of values are called derived information. Typical examples include\n\u2022 the reliability score,\n\u2022 the credibility score,\n\u2022 the network of communicated information, their cross-references and the belonging values,\n\u2022 quality tags of communication.\nThe Assistive AI System may be able to discover hidden (contextual) variables and explanatory variables that may influence court decisions."}, {"title": "4.2.5 Board", "content": "We are considering communities that have Boards. The Board is the decision-making body of the community to catalyze and regulate the working of the community according to its prescribed role if there is any. The Board is the human component in decision-making as regards the reliability and compliance of communicated information. The Board could be anonymous but must be accountable, and must have a credit score that is related to the reliability of their decisions and internal regulations (focusing on e.g., withdrawing anonymity, advertising, giving salaries, and prizes, organizing activities, and so on). Going back to the example of retracted publications, such an event may influence both the reviewers of the publication and the Editorial Board, too. Board decisions can be overruled by higher-order Boards (see below) or by the court. The Board's decisions can be assisted by the Assistive AI System's recommendations.\nThe Board"}]}