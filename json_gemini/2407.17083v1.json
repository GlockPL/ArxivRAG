{"title": "When Text and Images Don't Mix: Bias-Correcting Language-Image Similarity Scores for Anomaly Detection", "authors": ["Adam Goodge", "Bryan Hooi", "Wee Siong Ng"], "abstract": "Contrastive Language-Image Pre-training (CLIP) achieves remarkable performance in various downstream tasks through the alignment of image and text input embeddings and holds great promise for anomaly detection. However, our empirical experiments show that the embeddings of text inputs unexpectedly tightly cluster together, far away from image embeddings, contrary to the model's contrastive training objective to align image-text input pairs. We show that this phenomenon induces a \u2018similarity bias' - in which false negative and false positive errors occur due to bias in the similarities between images and the normal label text embeddings. To address this bias, we propose a novel methodology called BLISS which directly accounts for this similarity bias through the use of an auxiliary, external set of text inputs. BLISS is simple, it does not require strong inductive biases about anomalous behaviour nor an expensive training process, and it significantly outperforms baseline methods on benchmark image datasets, even when access to normal data is extremely limited.", "sections": [{"title": "Introduction", "content": "Anomaly detection (AD) is an important task in many vision-related applications, such as medical diagnosis and industrial defect detection. Neural networks are trained to embed in-put images into a latent space where anomalies are more easily detected. Vision-language models (VLM), which embed both image and textual inputs, have surged in popularity re-cently due to their flexibility and strong performance in various downstream tasks. CLIP [26] is particularly noteworthy; it is contrastively trained to maximise the cosine similarity between the latent embeddings of image-text caption pairs and minimise it between non-pairs. A query image should have high similarity to text inputs of its class label, and low similarity to unrelated text inputs, which is exploited for predicting class membership.\nIn this work, we conduct empirical experiments to examine the latent space learnt by CLIP. We find that, contrary to its contrastive training objective, all text inputs are highly"}, {"title": "Related Work", "content": "There are two types of tasks commonly referred to as anomaly detection. In the first, normal data is characterized by its belonging to a normal class while anomalies are any samples that do not belong to a normal class. In this setting, existing methods exploit properties such as the distance to normal samples [3, 11] or a normal hyper-sphere [29] in the latent space to detect anomalies. Some methods train models to perform auxiliary tasks, such as reconstruc-tion [2, 5, 36], generation [1, 30, 31, 35] or classification [10, 12, 32] instead, expecting the model to generalise to other normal samples in the test set but not to anomalies. In this work, we refer to this task as 'semantic anomaly detection', and it is highly related to the similar 'out-of-distribution detection' task. On the other hand, the second type of anomaly detec-tion task characterizes anomalies as small aberrations from otherwise highly uniform normal data. This setting is most often seen in industrial defect detection [6, 16, 25, 28], for which several methods exploiting CLIP's multi-modal capabilities have emerged in recent years [13, 18]. However, these methods typically require some level of prior knowledge about anomalous behaviour, which limits their applicability in settings where anomalies can result from unknown and unpredictable sources. Indeed, even the strongest methods in industrial defect detection tend to perform poorly in semantic AD [20].\nIn this work, we focus on the semantic AD task, making no assumptions about the na-ture of anomalies in the test set. Existing zero-shot CLIP-based methods [9, 21, 23] do not exploit information from labelled normal images. However, in practical settings, it is widely acknowledged that prior examples of normal data are often accessible. These examples are"}, {"title": "Motivation", "content": "In this section, we examine the latent space of CLIP and find what we call the \u2018text clustering effect'. We show the impact of this phenomenon on anomaly scoring, namely that it induces a 'similarity bias' which causes prediction errors. We begin by defining the semantic anomaly detection task of interest in this work."}, {"title": "Problem Statement", "content": "We have labelled normal samples $X^{(train)} = \\{(x_1^{(train)}, y_1^{(train)}),..., (x_{it}^{(train)}, y_{it}^{(train)})\\}$, where $\\forall i y_i^{(train)}$ is one of the normal classes with text labels $C = \\{C_1,...,C_N\\}$. We also have a set of unlabelled test images $X = \\{x_1,...,x_m\\}$, each of which may be normal (belonging to a class in C) or anomalous. Our goal is to define a function $s : X \\rightarrow \\mathbb{R}$ which maps an input image x to an anomaly score s(x) which is low for normal images and high for anomalies."}, {"title": "Text Clustering Effect", "content": "Based on its contrastive training objective, a reasonable expectation of the latent space learnt by CLIP is illustrated in Figure 1(a). Using CIFAR-10 [14] for demonstration, the class labels {'dog', 'cat' and 'bird' } are distanced from each other (exaggerated for clarity) as they are semantically distinct concepts, while their associated images tightly cluster around their paired labels. In Figure 1(b), we show that this expectation is not met in reality. We plot the t-SNE projections [33] of the real embeddings of CIFAR-10 class labels (crosses) and images (dots). Following previous work, class labels are converted into text prompts with the template: \"This is a photo of a {class label}\". All embeddings are normalized to the unit hyper-sphere.\nWe immediately see that the text and image embeddings occupy highly separated regions of the latent space. The images embeddings are broadly clustered by class, and these clusters are dispersed throughout the latent space. On the other hand, the text embeddings are tightly clustered together (they all visually overlap at approximately (-45,-25) in the plot). This is despite the fact that most labels are not at all related, for example the \u201cairplane\" and \"cat\" class labels. We call this phenomenon the \u2018text clustering effect'.\nFigure 2 supports this observation. The orange line shows the distribution of cosine similarities between each CIFAR-10 image and its correct class label, which we see peaks around 0.25. On the other hand, the blue line shows the average similarity between the same class labels and a set of general purpose text inputs (we use ImageNet [7] class labels), which we refer to as the 'dictionary'. As the dictionary entries covers a very large and broad range of concepts, the vast majority of entries should be unrelated to any given class label, and we may therefore expect the average similarity of each class label to the dictionary to be relatively low. However, we see that it is actually vastly higher than it is for the image inputs, peaking at around 0.75. This means that the class label text inputs are significantly more similar to other text inputs regardless of their content than they are to the images they are supposed to describe. This is despite the fact that CLIP was directly trained to maximise"}, {"title": "Similarity Bias in Anomaly Scoring", "content": "A simple approach to anomaly scoring would be to measure the similarity of a query image to the normal class label(s). A higher similarity suggests the sample is normal, while a lower similarity indicates an anomaly. However, the text clustering effect identified above means that normal class labels are tightly clustered with unrelated text labels, potentially including labels of anomalies. We now investigate the impact of this finding on AD performance.\nTo this end, we measure the average similarity of CIFAR-10 images to the dictionary mentioned earlier. As the dictionary is large and conceptually diverse, this average similar-ity can be interpreted as a measure of the similarity of an image to general text embeddings, rather than to their specific class label. In theory, any given image should be meaningfully related to only a few entries and unrelated to the vast majority of them, meaning the average dictionary similarity should be not be a very informative measure of an image's anomaly sta-tus. However, in Figure 3, we show that the distribution of errors from this anomaly scoring"}, {"title": "Methodology", "content": "BLISS consists of two components; an internal class score and external text score. We rely on the fixed, pre-trained CLIP backbone model with image encoder $I$ and text encoder $T$. We firstly obtain all of the embeddings of labelled normal images using the image encoder and store them in a memory bank:\n$Z^{(train)} = \\{z_1^{(train)},..., z_n^{(train)}\\}$, where $z_i^{(train)} = I(x_i^{(train)})$,                                         (1)\nNote that the encoders are fixed and we do not fine-tune any model parameters. For a given unlabelled test image, x, we similarly find its own embedding:\nz = I(x).                                                       (2)\nWe now define the two components, starting with the internal class score."}, {"title": "Internal Class Score", "content": "The internal class score measures the normality of the test image based on its similarity to the normal class label(s). We obtain all of the text embeddings from the normal class labels with the fixed CLIP text encoder:\n$C = \\{C_1, .., C_N\\}$, where $C_i = T(C_i)$.                                      (3)"}, {"title": "External Text Score", "content": "The external text score is designed to address the similarity bias. It measures the similarity of images to the dictionary representing general text inputs. We embed each entry in the dictionary $D = \\{D_1,..., D_\\}$ with the CLIP text encoder:\nD := \\{D1, .., D\\}, where $D_i = T(D_i)$.                                         (6)\nFor the given test image x, with embedding z, we denote its top K closest matches with the highest similarity in D as D*:\nD* = topK\\{sim(z, Di) : Di \u2208 D\\}                                             (7)\nNote that the elements of $D^*$ are specific to the given test sample. As before, we find the mean and standard deviation statistics from the labelled images from each normal class to each $d^* \\in D^* $:\n$\\mu_i(d^*) = \\underset{y_i^{(train)}=C_i}{mean} sim(z_i^{(train)}, d^*),  \\sigma_i(d^*) = \\underset{y_i^{(train)}=C_i}{std} sim(z_i^{(train)}, d^*)$.                                                 (8)\nThe external text score is then similarity computed as the mean of sim(z, d*) over all $d^* \\in D^* $, normalized by these statistics:\n$ET (x, C_i) = \\underset{d^*\\in D^*}{mean}  \\frac{sim(z, d^*) - \\mu_i (d^*)}{ \\sigma_i (d^*) + \\epsilon}$,                                     (9)\nIntuitively, $\\mu_i(d^*)$ and $\\sigma_i(d^*)$ capture the learned notion of normality using statistics from each normal class. After correcting for these statistics, test samples with high external text scores are those with high similarity to the dictionary words. From our observation of similarity bias, such samples tend to appear normal in terms of internal class score, and thus have a higher chance of being false negatives. The external text score corrects this by 'subtracting away' this bias, leaving behind a more meaningful view of the similarity between an image and the normal class labels. As our method only requires forward passes through the fixed backbone model, it is light-weight and efficient to compute even with a very large dictionary size."}, {"title": "BLISS Score", "content": "We take a linear combination of the internal class and external text scores, regulated by the hyper-parameter $\\lambda$:\ns(x, Ci) = IC(x, Ci) + \u03bbET (x,Ci).                                           (10)\nIn the case of multiple normal classes, i.e. |C| > 1, we repeat scoring over every $C_i \\in C$ and take the minimum as the final anomaly score:\ns(x) = min s(x, Ci).                                            (11)\nIn summary, we jointly consider similarity not only to the normal class labels (internal) but also to general text embeddings (external), both standardized to account for the learned distribution of normal data."}, {"title": "Experiments", "content": "We now perform experiments to answer the following questions about our methodology:\nRQ1 (Performance): Does BLISS outperform baseline methods in anomaly detection on benchmark datasets?\nRQ2: (Ablation Study) How do the components of BLISS perform in different experimen-tal settings?\nDatasets We use the most popular datasets in semantic AD: CIFAR-10, CIFAR-100 [14] and TinyImageNet [15]. For CIFAR-10, we partition the 10 classes into 1/6/9 normal classes and 9/4/1 anomaly classes in different experiments. For 1 normal class and 9 normal classes, we run 10 trials one for each permutation of normal and anomaly class splits. For 6 normal classes, and for CIFAR-100 and TinyImageNet experiments, we run 5 trials with the same class splits as [9].\nModel Setup We use the publicly available pre-trained ViT-B/16 [8] CLIP model and do not fine-tune any model parameters. Images are resized, center-cropped and normalized. We use ImageNet class labels as the external dictionary. Classes containing multiple labels, e.g. \"goldfish, Carassius auratus\" are treated as separate entries \u201cgoldfish\" and \"Carassius auratus\", resulting in 1850 dictionary entries. The text prompt \u201cThis is a photo of a {class label}\" is formulated for each class label and dictionary entry. We try different prompt formulations in the supplementary material and find little effect on performance. For both modalities, we normalized their 512-dimensional embeddings to the unit hyper-sphere. We set K = 10 for the external text score and $\\lambda$ = 0.5 to approximately match the range of the two scores and we do not conduct hyper-parameter search for optimal performance.\nBaselines We copy the results from [9] of their method ZOC, CSI [32] and CAC [22], for which some experiments are missing (marked by -). We also implement our own baseline methods as follows. To measure the role of multi-modal learning, we use features from the image-only ViT-B/16 model pre-trained on ImageNet-21k from HuggingFace [34] with AD methods: DN2 (K = 5) [4], LUNAR [11] and Gaussian mixture model (GMM). DINO-FT"}, {"title": "RQ1: (Performance)", "content": "Table 1 shows the mean and standard deviation in AUROC scores. We use the AUROC metric to avoid choosing an anomaly score threshold. We see that BLISS outperforms all baselines in all datasets and often by a significant margin, a result of accounting for the similarity bias as well as exploiting normality statistics from labelled normal samples.\nAs TinyImageNet is a subset of ImageNet, all of its class labels are also present in the dictionary. This could be seen as a violation of the assumption of no prior knowledge, therefore we also measured performance after removing all TinyImageNet labels from the"}, {"title": "RQ2: (Ablation Study)", "content": "Weight hyper-parameter Table 2 shows the performance of BLISS as $\\lambda$ in (10) is varied. A higher value gives more weighting to the external text score. As the internal class score is generally smaller in magnitude than the external text score, we heuristically set $\\lambda$ = 0.5 to approximately equalise their weight. However, we see that $\\lambda$ = 0.75 is better for performance across all datasets. Overall, performance is robust within a reasonable range of $\\lambda$."}, {"title": "Conclusion", "content": "In this paper, we identified a peculiarity of the CLIP latent space that all text embeddings are highly clustered together, away from their associated images. This hinders anomaly de-tection performance, and may also have important implications for other downstream tasks. We propose a novel anomaly scoring approach called BLISS which addresses this bias by measuring similarities against a large, external source of text inputs. BLISS is fast, anomaly-agnostic and achieves state-of-the-art performance on benchmark datasets. More broadly, our findings highlight the need for greater understanding of multi-modal learning and the need to address their unexpected characteristics; a goal of fundamental importance."}]}