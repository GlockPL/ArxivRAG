{"title": "Out-of-distribution generalization via composition: a lens through induction heads in Transformers", "authors": ["Jiajun Song", "Zhuoyan Xu", "Yiqiao Zhong"], "abstract": "Large language models (LLMs) such as GPT-4 sometimes appear to be creative, solving novel tasks\noften with a few demonstrations in the prompt. These tasks require the models to generalize on distribu-\ntions different from those from training data\u2014which is known as out-of-distribution (OOD) generaliza-\ntion. Despite the tremendous success of LLMs, how they approach OOD generalization remains an open\nand underexplored question. We examine OOD generalization in settings where instances are generated\naccording to hidden rules, including in-context learning with symbolic reasoning. Models are required\nto infer the hidden rules behind input prompts without any fine-tuning.\nWe empirically examined the training dynamics of Transformers on a synthetic example and con-\nducted extensive experiments on a variety of pretrained LLMs, focusing on a type of components known\nas induction heads. We found that OOD generalization and composition are tied together-models can\nlearn rules by composing two self-attention layers, thereby achieving OOD generalization. Furthermore,\na shared latent subspace in the embedding (or feature) space acts as a bridge for composition by aligning\nearly layers and later layers, which we refer to as the common bridge representation hypothesis.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are sometimes able to solve complex tasks that appear novel or require rea-\nsoning abilities. The appearance of creativity in task-solving has sparked recent discussions about artificial\ngeneral intelligence [17, 44, 22, 13].\nThe classical notion of statistical generalization does not seem to account for the progress observed in\nLLMs. Traditionally, both training instances and test instances are drawn from the same distribution, and\nit is generally not expected that a model will generalize well on a different test distribution without explicit\ndomain adaptation involving model updates.\nThe recent success of LLMs suggests a different story: if test data involve compositional structures,\nLLMs can generalize across different distributions with just a few demonstrations in the prompt (few-shot\nlearning) or even without any demonstrations (zero-shot learning) [16] without updating the model parame-\nters. Indeed, the apparent ability of models to infer rules from the context of a prompt\u2014known as in-context\nlearning (ICL)\u2014is a hallmark of LLMs [21]. Moreover, a growing body of literature on chain-of-thought\nprompting explicitly exploits the compositional structures of reasoning tasks. For example, phrases like"}, {"title": "1.1 An exemplar: copying", "content": "Copying is a simple yet nontrivial task that exemplifies OOD generalization. Briefly speaking, given a\nsequence that contains several consecutive tokens such as [A], [B], [C], a model predicts the next token as\n[C] upon receiving [A], [B]:\nnext-token prediction\n... [A], [B], [C] . . . [A], [B] ... [A], [B], [C] . . . [A], [B], [C]\nFormally, consider a sequence of tokens s = (81,..., sT) where each st is in a discrete vocabulary set\nA. Suppose that a segment of length L is repeated in this sequence: STo:(To+L\u22121) = S(T-L+1):T where\nL < (T \u2013 To)/2 + 1. Upon receiving the sequence 81:(T\u22121), copying requires a model to output\u00b9 token st.\nCopying represents a primitive form of abstract reasoning. While humans can code the copying rule\ninto a model, classical statistical models have difficulty learning this rule purely based on instances of\nsuch sequences. For instance, hidden Markov models and n-gram models [15] require estimating a large\ntransition matrix or high-order conditional probability, which scales exponentially in L.\nAs a simple experiment, we fix |A| = 64 and consider a power law distribution P on A. We sample each\nsequence s of length Tmax = 64 independently by planting repeated segments in a \u201cnoisy background\":\n1. Sample L uniformly from {10, 11, . . ., 19}, sample s# from P independently for t = 1,..., L, and\nform a segment s# = (s#,...,s#);\n2. Denote r1 = Tmax - 2L. Sample two integers uniformly from {1, 2, . . ., r1 } and denote the smaller/larger\nones by To, T1;\n3. Form a sequence (*, s#, *, s#, *) where * is filled by random tokens of lengths To, T1 - To, rL - T1\nrespectively drawn from P independently.\""}, {"title": "1.2 Compositional structure is integral to OOD generalization", "content": "In Figure 1, we observe that the 2-layer Transformer experiences two phases. (i) In the weak learning phase,\nthe model learns the marginal token distribution P and predicts the most probable token irrespective of the\nstructure of the entire sequence s. This results in a decrease of the ID error but not the OOD error. (ii) In the\nrule-learning phase, the model learns the copying rule and generalize reasonably well on both ID and OOD\ndata. In contrast, one-layer Transformer only achieves weak learning. Note that the OOD error is smaller\nafter training because the longer repetition segment means a larger signal strength.\nLearning the copying rule requires the model to generalize on OOD instances in two aspects.\n1. Generalize to a larger repetition length Lood (aka length generalization).\n2. Generalize to a different token distribution Pood.\nOur analysis of training dynamics later suggests that the two layers of the Transformer play complementary\nroles: one layer specializes in processing positional information and another in token information. Compos-\ning the two layers yields OOD generalization."}, {"title": "2 Dissecting sharp transition in synthetic example", "content": "As in Section 1.1, we train a minimally working Transformer for the copying task as a clean synthetic\nexample, though we find similar results on larger models. In particular, the model has 2 self-attention layers\nwith a single head and no MLPs. The architecture and training are standard, including residual connection,\nLayerNorm, RoPE, dropout, autoregressive training with the AdamW optimizer, and weight decay. See\nSection A for a list of notations and Section B for experimental details.\nModel. For analytical purposes, we introduce a basic form of Transformer from the circuits perspective\n[26]. Let X = [x1,...,XT] \u2208 RT\u00d7d be the initial embeddings or the hidden states representing a\nsequence of length T in d dimensions. A multihead self-attention (MSA) is a mapping MSA(X; W) :\nRT\u00d7d \u2192 RT\u00d7d given by\nMSA(X; W) := X + \\sum_{j=1}^H Softmax (XW_{QK,j}(W_{QK,j})^TX) XW_{OV,j} \\qquad (1)\nwhere WQK,j, Wov,j \u2208 Rdxd and W is a collection of all such matrices. An (attention-only) Transformer\nis a composition of mappings X \u2192 MSA(X; W) where each layer has different trainable parameters W.\nSection B.5 provides further details, interpretations, and comparison with practical model variants."}, {"title": "2.1 Progress measures", "content": "Under the circuits perspective in Eq. 1, each of the H attention head consists of a \u201creading\u201d QK circuit, a\n\"writing\" OV circuit, and the softmax operation. Since H = 1 in our experiment, our goal is to study how\nthe 1st-layer attention head interact with the 2nd-layer head. To this end, we define several measurements.\n1. Diagonal score. For 1st-layer Wov and 2nd-layer WQK, we calculate WQKOV = WQKWOV \u2208\nRd\u00d7d and define z = z(WQK, Wov) as\nz = \\frac{Ave((WQKOV)_{id}) - Ave((W_{QK}^\\top WOV)_{i \\neq j, i,j \\leq d})}{Std((W_{QK}^\\top WOV)_{i,j \\leq d})},\nwhere Ave, Std mean taking average and standard deviation respectively.\nThis score measures the strength of diagonal entries. Roughly speaking, z is interpreted as the signal-to-\nnoise ratio under the \u201cXI\u300f + noise\" assumption on WQKOV.\n2. Subspace matching score. Fix rank parameter r = 10. We calculate the top-r right singular vectors\nU \u2208 Rdxr of 2nd-layer WQK and top-r left singular vectors V \u2208 Rd\u00d7r of 1st-layer Woy. The col-\numn linear span PQK := span(U) (or similarly for V) represents the principal subspace for reading\n(or writing) information. We calculate a generalized cosine similarity between the two subspaces.\nsim(PQK, Pov) := \\sigma_{max}(U^\\top V), \\qquad (2)\nwhere omax denotes the largest singular value.\nThis score is equivalent to the regular cosine similarity between two optimally chosen unit vectors within\nthe two subspaces. Results are analogous under a similar average similarity between PQK, POV.\nIn Figure 2 (top row), we discovered simultaneous sharp transitions in generalization and also in the two\nscores. To investigate why the structural matching yields OOD generalization, we consider more measure-\nments. An attention matrix A = (At,t')t,t'<T is the output of the softmax in Eq. 1. The weight At,t' \u2208 [0,1]\nrepresents the coefficient (aka attention) from position t to t' in a sequence and satisfies \u2211t, At,t' = 1."}, {"title": "2.2 Results", "content": "OOD generalization is accompanied by abrupt emergence of subspace matching. The top row of\nFigure 2 shows that the sharp transition of generalization occurs at training steps around 2000. In the\nmeantime, the weight matrices in the two layers of the model exhibit a sudden increase of alignment: a large\ndiagonal component in WQKOV appears, and the two principal subspaces of WQK, Wov change from\nbeing relatively random (similar to random initialization) to highly synchronized.\nThe structure of WQKOV helps to match similar embeddings. Indeed, if we believe that WQKWOV\nhas the \u201cAI + noise\" structure, then the embedding \u00e6t at position t satisfies \u00e6 + WQKWovy \u2248 x+y.\nTo maximize this inner product, y of fixed length must be approximately aligned with \u00e6t, so embeddings\nsimilar to \u00e6t tend to receive large attentions in the 2nd layer. Further, subspace matching between QK and\nOV shows that aligning the embeddings depends on the low-dimensional principal subspaces.\nTwo layers have complementary specialties: position shifting and token matching. The bottom row\nof Figure 2 provides an explanation for OOD generalization. The 1st-layer attention head has a large PTH\nscore after training, even for sequences of completely random tokens. The high PTH score indicates that\nthe 1st-layer head specializes in position shifting. In fact, in the ideal case where At,t\u22121 = 1, the map\nX \u2192 AX is simply the shifting operator. Complementarily, the 2nd-layer QK matches the OV circuit\nand serves as token matching. So upon accepting the shifted tokens as inputs, the 2nd-layer head attends\nto the next position after the repeated token. Collectively, they yield an IH head, attending correctly to the\nto-be-copied token; see Figure 3 (right)."}, {"title": "3 Intervention experiments in LLMs", "content": "How is the synthetic example relevant to realistic reasoning tasks for LLMs? In this section we address this\nquestion by presenting two types of realistic scenarios: out-of-distribution (OOD) prompts (Section 3.1) and\nrealistic compositional structures (Section 3.2). Through these examples, we aim to demonstrate two key\npoints:\n1. Prompts (natural language inputs) planted with arbitrarily chosen symbols can be inferred by LLMs\nin certain tasks without fine-tuning. This reasoning abilities depend crucially on IHs.\n2. Subspace matching as the compositional mechanism takes a more general form in multilayer and\nmultihead models, where a shared latent subspace matches many PTHs and IHs simultaneously.\nPretrained LLMs. We consider a variety of LLMs in our experiments: (1) Llama2-7B [80], (2) Llama3-\n8B [23], (3) Mistral-7B [38], (4) Falcon-7B [6], (5) Falcon2-11B [50], (6) OlMo-7B [30], (7) Gemma-7B\n[78], (8) Gemma2-8B [79]. See Appendix C.1 for details about models and implementations.\nDefining induction heads and previous-token heads. We sample N = 100 test prompts with a simple\nrepetition pattern (s#,s#): a block of 25 uniformly random tokens followed by a replica of the block,\ntotaling T = 50 tokens. For any layer l and head j of a Transformer, we denote by A\u2208 RTXT the\nattention matrix defined in Eq. 1 on a test prompt i. By definition (Ai) = 1. This definition of IHs\nand PTHs only depend on the model, irrespective of downstream tasks.\nFor each model, we score all attention heads according to Eq. 3 and 4 based on the test prompts, yielding\nscorePTH and score. Then we rank the PTH scores and IH sco scores in descending order respectively. For a\npre-specified K, we define PTHs (or IHs) as the top-K attention heads according to scorePTH (or score). Section E.2 provides lists of PTHs and IHs in each model."}, {"title": "3.1 Symbolized language reasoning", "content": "In each task, we sample prompts based on a specified rule and use LLMs directly to predict next tokens.\nTokens in blue are the target outputs for prediction. See appendix C.2 for a set of complete examples.\n1. Fuzzy copying: [A], [B], [C] . . . [A'], [B'], [C']\nWe consider conversion from lower-cased words to upper-cased words. For example, the correct completion\nof \"bear snake fox poppy plate BEAR SNAKE FOX POPPY\u201d is \u201cPLATE\u201d.\nThe IOI and ICL tasks below were proposed by [84] and [69], and recently analyzed by [87, 62, 2, 33,\n73]. We extended the method in [69] to construct symbolized prompts.\n2. Indirect object identification (IOI): [Subject] . . . [Object] . . . [Subject] ... [Object]"}, {"title": "3.2 Common bridge representation hypothesis", "content": "How do compositions work in LLMs beyond the synthetic example? With a microscopic analysis focusing\non the copying task, we posit the following Common Bridge Representation (CBR) hypothesis.\nFor compositional tasks, a latent subspace stores intermediate representations from the outputs of relevant\nattention heads and then matches later heads.\nThis latent subspace can be viewed as a sophisticated example of feature superposition [25] for composi-\ntional tasks.\nWe experiment on a variety of LLMs and highlight key findings in Figure 5(b)\u2013(f), where we use GPT-2\nas a recurring example and summarize results of other models. Top-scoring PTHs and IHs are distributed\nacross different layers of LLMs, though more PTHs appear in early layers. We sample sequences of the for-3#,s#,s#) and calculate the average token-wise probability/accuracy for predicting the third segment\ns#. See experiment details in Section C.\nExperiments with two interventions. For both experiments, we apply screening to top PTHs and IHs\nbased on diagonal scores, resulting in an average of 9 effective PTHs and 7 effective IHs. The first inter-\nvention experiment involves shuffling heads. We randomly permute matrices WQK within the list of IHs,\nyielding an edited model (\u201cshuffle within\u201d). As comparison, we replace each WQK within the list wby a\nrandom WQK Outside the list, yielding another edited model (\u201cshuffle outside\u201d). In a parallel experiment,\nwe shuffle Wov circuits within PTHs similarly. We evaluate the original model and edited models by\ncalculating the average probability of predicting correct tokens.\nThe second intervention experiment involves projection of weight matrices. First, we stack the QK\nmatrices from IHs into a matrix [WK,..., WK] and extract the top right singular vectors V \u2208 Rd\u00d7r for\na pre-specified r. We call the column linear span of V as the bridge subspace. Then, we edit 25% of all\nattention heads by weight projection WQK \u2190 WQKVVT. After the edit ('keep'), the attention calculation\ncan only use the component of embeddings within the bridge subspace. In a parallel experiment, we make a\nprojection edit (\u2018remove') with an orthogonal complement WQK + WQK(Id \u2013 VVT) to force attention"}, {"title": "4 Related work", "content": "There are many recent papers on analyzing and interpreting Transformers [95, 12, 75, 35]; see [9] for a\ncomprehensive survey. We highlight a few key threads of research.\nMechanistic interpretability and induction heads. Mechanistic interpretability (MI) aims to provide\nmicroscopic interpretability of inner workings of Transformers [57]. In particular, [26, 58] proposed IHs as\ncrucial components of Transformers. In particular, they suggested that matching OV circuits and QK circuits\nis crucial for ICL. A line of research extends IHs in various aspects [84, 66, 52, 74, 27, 5, 29, 4]. Compared\nwith the existing literature, we conducted extensive experiments on both small Transformers and a variety of\nLLMs, demonstrating that IHs are crucial components for many reasoning tasks. Further, we propose that\nsubspace matching\u2014and more broadly common bridge representation hypothesis\u2014as the compositional\nmechanism.\nOOD generalization and compositions. Historically, studies of generalization on novel domains focus\non extrapolation, distribution shift, domain adaptation, etc. Since GPT-3 [16], recent studies of OOD\ngeneralization focus on arithmetic and algebraic tasks [43, 96], formal language and deductive reasoning\n[71, 67, 77, 83], learning boolean functions [1], etc. Our copying task is an example of length generalization\n[7, 99]. Compositional tasks are strongly related to reasoning abilities of LLMs, such as arithmetic tasks\n[24, 42], formal language [32], logical rules [14, 91], and so on. Despite recent insights [10, 24], a system-\natic analysis remains challenging. Our work is a first step toward understanding sophisticated compositional\ncapabilities of LLMs.\nLinear Representation Hypothesis. Linear Representation Hypothesis (LRH) states that monosemantic\n(meaning basic or atomic) concepts are represented by linear subspaces (or half-spaces) in embeddings\n[11, 25, 63]. This hypothesis is empirically verified in not only in LLMs but also in simpler statistical\nmodels [53, 64]. LRH treats subspaces as fundamental units for representing linguistic concepts, but it does\nexplain how models solve reasoning tasks or achieve OOD generalization. Our CBR hypothesis furthers\nthis view by linking intermediate outputs in compositions to interpretable subspaces."}, {"title": "5 Limitations and future work", "content": "First, our hypothesis is a general conjecture on the mechanism of compositions in Transformers, based on\nour analysis of IHs. While IHs are pervasive in LLMs, other components or mechanisms for compositions\nmay exist. Additionally, our interpretations are based on a simplified form of self-attention. It would be\ninteresting to explore alternative mechanisms for compositions, and examine variants or practical techniques\nin LLMs that may impact our hypothesis.\nSecond, we did not develop insights to explain the emergence of the bridge subspace during training.\nThe sharp transition in prediction accuracy is related to the emergent abilities of LLMs observed in broader"}]}