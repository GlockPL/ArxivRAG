{"title": "Out-of-distribution generalization via composition: a lens through induction heads in Transformers", "authors": ["Jiajun Song", "Zhuoyan Xu", "Yiqiao Zhong"], "abstract": "Large language models (LLMs) such as GPT-4 sometimes appear to be creative, solving novel tasks often with a few demonstrations in the prompt. These tasks require the models to generalize on distributions different from those from training data\u2014which is known as out-of-distribution (OOD) generalization. Despite the tremendous success of LLMs, how they approach OOD generalization remains an open and underexplored question. We examine OOD generalization in settings where instances are generated according to hidden rules, including in-context learning with symbolic reasoning. Models are required to infer the hidden rules behind input prompts without any fine-tuning.\nWe empirically examined the training dynamics of Transformers on a synthetic example and conducted extensive experiments on a variety of pretrained LLMs, focusing on a type of components known as induction heads. We found that OOD generalization and composition are tied together-models can learn rules by composing two self-attention layers, thereby achieving OOD generalization. Furthermore, a shared latent subspace in the embedding (or feature) space acts as a bridge for composition by aligning early layers and later layers, which we refer to as the common bridge representation hypothesis.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are sometimes able to solve complex tasks that appear novel or require reasoning abilities. The appearance of creativity in task-solving has sparked recent discussions about artificial general intelligence [17, 44, 22, 13].\nThe classical notion of statistical generalization does not seem to account for the progress observed in LLMs. Traditionally, both training instances and test instances are drawn from the same distribution, and it is generally not expected that a model will generalize well on a different test distribution without explicit domain adaptation involving model updates.\nThe recent success of LLMs suggests a different story: if test data involve compositional structures, LLMs can generalize across different distributions with just a few demonstrations in the prompt (few-shot learning) or even without any demonstrations (zero-shot learning) [16] without updating the model parameters. Indeed, the apparent ability of models to infer rules from the context of a prompt\u2014known as in-context learning (ICL)\u2014is a hallmark of LLMs [21]. Moreover, a growing body of literature on chain-of-thought prompting explicitly exploits the compositional structures of reasoning tasks. For example, phrases like"}, {"title": "1.1 An exemplar: copying", "content": "Copying is a simple yet nontrivial task that exemplifies OOD generalization. Briefly speaking, given a sequence that contains several consecutive tokens such as $[A], [B], [C]$, a model predicts the next token as\n[C] upon receiving [A], [B]:\n... [A], [B], [C] . . . [A], [B]\n... [A], [B], [C] . . . [A], [B], [C]\nFormally, consider a sequence of tokens $s = (s_1,..., s_T)$ where each $s_t$ is in a discrete vocabulary set A. Suppose that a segment of length L is repeated in this sequence: $s_{T_0:(T_0+L-1)} = s_{(T-L+1):T}$ where $L < (T \u2013 T_0)/2 + 1$. Upon receiving the sequence $s_{1:(T\u22121)}$, copying requires a model to output\u00b9 token $s_T$.\nCopying represents a primitive form of abstract reasoning. While humans can code the copying rule into a model, classical statistical models have difficulty learning this rule purely based on instances of such sequences. For instance, hidden Markov models and n-gram models [15] require estimating a large transition matrix or high-order conditional probability, which scales exponentially in L.\nAs a simple experiment, we fix $|A|$ = 64 and consider a power law distribution $P$ on $A$. We sample each sequence $s$ of length $T_{max}$ = 64 independently by planting repeated segments in a \u201cnoisy background\":\n1. Sample L uniformly from {10, 11, . . ., 19}, sample $s_t^{\\#}$ from $P$ independently for $t = 1,..., L$, and form a segment $s^{\\#} = (s_1^{\\#},...,s_L^{\\#})$;\n2. Denote $r_1 = T_{max}-2L$. Sample two integers uniformly from {1, 2, . . ., $r_1$} and denote the smaller/larger ones by $T_0$, $T_1$;\n3. Form a sequence (*, $s^{\\#}$, *, $s^{\\#}$, *) where * is filled by random tokens of lengths $T_0$, $T_1 - T_0$, $r_L - T_1$ respectively drawn from P independently.\nWe train a 2-layer attention-only Transformer on batches of fresh samples, namely each training step uses independently drawn sequences. Model architecture and training follow the standard practice; see Section B for details. We report both in-distribution (ID) test errors and OOD test errors by computing the average token-wise prediction accuracy based on the second segment. Here the OOD error is evaluated on sequences of a similar format but P is replaced by the uniform distribution $P_{ood}$, and L is replaced by $L_{ood}$ = 25. As a comparison, we train a 1-layer attention-only Transformer using the same data.\""}, {"title": "1.2 Compositional structure is integral to OOD generalization", "content": "In Figure 1, we observe that the 2-layer Transformer experiences two phases. (i) In the weak learning phase, the model learns the marginal token distribution P and predicts the most probable token irrespective of the structure of the entire sequence s. This results in a decrease of the ID error but not the OOD error. (ii) In the rule-learning phase, the model learns the copying rule and generalize reasonably well on both ID and OOD data. In contrast, one-layer Transformer only achieves weak learning. Note that the OOD error is smaller after training because the longer repetition segment means a larger signal strength.\nLearning the copying rule requires the model to generalize on OOD instances in two aspects.\n1. Generalize to a larger repetition length $L_{ood}$ (aka length generalization).\n2. Generalize to a different token distribution $P_{ood}$.\nOur analysis of training dynamics later suggests that the two layers of the Transformer play complementary roles: one layer specializes in processing positional information and another in token information. Composing the two layers yields OOD generalization."}, {"title": "2 Dissecting sharp transition in synthetic example", "content": "As in Section 1.1, we train a minimally working Transformer for the copying task as a clean synthetic example, though we find similar results on larger models. In particular, the model has 2 self-attention layers with a single head and no MLPs. The architecture and training are standard, including residual connection, LayerNorm, RoPE, dropout, autoregressive training with the AdamW optimizer, and weight decay. See Section A for a list of notations and Section B for experimental details.\nModel. For analytical purposes, we introduce a basic form of Transformer from the circuits perspective [26]. Let $X = [x_1,...,x_T] \\in \\mathbb{R}^{T \\times d}$ be the initial embeddings or the hidden states representing a sequence of length T in d dimensions. A multihead self-attention (MSA) is a mapping MSA(X; W) : $\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d}$ given by\n$MSA(X; W) := X + \\sum_{j=1}^{H} Softmax (XW_{QK,j}W^T) XV_{OV,j}$"}, {"title": "2.1 Progress measures", "content": "Under the circuits perspective in Eq. 1, each of the H attention head consists of a \u201creading\u201d QK circuit, a \"writing\" OV circuit, and the softmax operation. Since H = 1 in our experiment, our goal is to study how the 1st-layer attention head interact with the 2nd-layer head. To this end, we define several measurements.\n1. Diagonal score. For 1st-layer $W_{OV}$ and 2nd-layer $W_{QK}$, we calculate $W_{QKOV} = W_{QK}W_{OV} \\in \\mathbb{R}^{d \\times d}$ and define $z = z(W_{QK}, W_{OV})$ as\n$z = \\frac{Ave((W_{QKOV})_{ii})_{i \\leq d} - Ave((W_{QKOV})_{ij})_{i,j \\leq d}}{Std((W_{QKOV})_{ij})_{i,j \\leq d}}$\nwhere Ave, Std mean taking average and standard deviation respectively.\nThis score measures the strength of diagonal entries. Roughly speaking, $z$ is interpreted as the signal-to-noise ratio under the \u201c$XI_d$ + noise\" assumption on $W^{QKOV}$.\n2. Subspace matching score. Fix rank parameter r = 10. We calculate the top-r right singular vectors $U \\in \\mathbb{R}^{d \\times r}$ of 2nd-layer $W_{QK}$ and top-r left singular vectors $V \\in \\mathbb{R}^{d \\times r}$ of 1st-layer $W_{OV}$. The column linear span $P_{QK} := span(U)$ (or similarly for V) represents the principal subspace for reading (or writing) information. We calculate a generalized cosine similarity between the two subspaces.\n$sim(P_{QK}, P_{OV}) := \\sigma_{max}(U^TV)$\nwhere $\\sigma_{max}$ denotes the largest singular value.\nThis score is equivalent to the regular cosine similarity between two optimally chosen unit vectors within the two subspaces. Results are analogous under a similar average similarity between $P_{QK}, P_{OV}$.\nIn Figure 2 (top row), we discovered simultaneous sharp transitions in generalization and also in the two scores. To investigate why the structural matching yields OOD generalization, we consider more measurements. An attention matrix $A = (A_{t,t'})_{t,t'<T}$ is the output of the softmax in Eq. 1. The weight $A_{t,t'} \\in [0,1]$ represents the coefficient (aka attention) from position t to t' in a sequence and satisfies $\\sum_{t'} A_{t,t'} = 1$."}, {"title": "2.2 Results", "content": "OOD generalization is accompanied by abrupt emergence of subspace matching. The top row of Figure 2 shows that the sharp transition of generalization occurs at training steps around 2000. In the meantime, the weight matrices in the two layers of the model exhibit a sudden increase of alignment: a large diagonal component in $W^{QKOV}$ appears, and the two principal subspaces of $W_{QK}$, $W_{OV}$ change from being relatively random (similar to random initialization) to highly synchronized.\nThe structure of $W^{QKOV}$ helps to match similar embeddings. Indeed, if we believe that $W_{QK}W_{OV}$ has the \u201c$\\lambda I + noise$\u201d structure, then the embedding $x_t$ at position t satisfies $\\lambda x_t + W_{QK}W_{OV} x_t \\approx x_t + y$. To maximize this inner product, $y$ of fixed length must be approximately aligned with $x_t$, so embeddings similar to $x_t$ tend to receive large attentions in the 2nd layer. Further, subspace matching between QK and OV shows that aligning the embeddings depends on the low-dimensional principal subspaces.\nTwo layers have complementary specialties: position shifting and token matching. The bottom row of Figure 2 provides an explanation for OOD generalization. The 1st-layer attention head has a large PTH score after training, even for sequences of completely random tokens. The high PTH score indicates that the 1st-layer head specializes in position shifting. In fact, in the ideal case where $A_{t,t\u22121} = 1$, the map $X \u2192 AX$ is simply the shifting operator. Complementarily, the 2nd-layer QK matches the OV circuit and serves as token matching. So upon accepting the shifted tokens as inputs, the 2nd-layer head attends to the next position after the repeated token. Collectively, they yield an IH head, attending correctly to the to-be-copied token; see Figure 3 (right)."}, {"title": "3 Intervention experiments in LLMs", "content": "How is the synthetic example relevant to realistic reasoning tasks for LLMs? In this section we address this question by presenting two types of realistic scenarios: out-of-distribution (OOD) prompts and realistic compositional structures. Through these examples, we aim to demonstrate two key points:\n1. Prompts (natural language inputs) planted with arbitrarily chosen symbols can be inferred by LLMs in certain tasks without fine-tuning. This reasoning abilities depend crucially on IHs.\n2. Subspace matching as the compositional mechanism takes a more general form in multilayer and multihead models, where a shared latent subspace matches many PTHs and IHs simultaneously.\nPretrained LLMs. We consider a variety of LLMs in our experiments: (1) Llama2-7B [80], (2) Llama3-8B [23], (3) Mistral-7B [38], (4) Falcon-7B [6], (5) Falcon2-11B [50], (6) OlMo-7B [30], (7) Gemma-7B [78], (8) Gemma2-8B [79]. See Appendix C.1 for details about models and implementations.\nDefining induction heads and previous-token heads. We sample N = 100 test prompts with a simple repetition pattern (s#,s#): a block of 25 uniformly random tokens followed by a replica of the block, totaling T = 50 tokens. For any layer l and head j of a Transformer, we denote by $A_{l,j} \\in \\mathbb{R}^{T \\times T}$ the attention matrix defined in Eq. 1 on a test prompt i. By definition $\\sum_{j}(A_{i})_{ij} = 1$. This definition of IHs and PTHs only depend on the model, irrespective of downstream tasks.\nFor each model, we score all attention heads according to Eq. 3 and 4 based on the test prompts, yielding $score^{PTH}$ and $score^{IH}$. Then we rank the PTH scores and IH sco scores in descending order respectively. For a pre-specified K, we define PTHs (or IHs) as the top-K attention heads according to $score^{PTH}$ (or $score^{IH}$). Section E.2 provides lists of PTHs and IHs in each model."}, {"title": "3.1 Symbolized language reasoning", "content": "In each task, we sample prompts based on a specified rule and use LLMs directly to predict next tokens. Tokens in blue are the target outputs for prediction. See appendix C.2 for a set of complete examples.\n1. Fuzzy copying: [A], [B], [C] . . . [A'], [B'], [C']\nWe consider conversion from lower-cased words to upper-cased words. For example, the correct completion of \"bear snake fox poppy plate BEAR SNAKE FOX POPPY\u201d is \u201cPLATE\u201d.\nThe IOI and ICL tasks below were proposed by [84] and [69], and recently analyzed by [87, 62, 2, 33, 73]. We extended the method in [69] to construct symbolized prompts.\n2. Indirect object identification (IOI): [Subject] . . . [Object] . . . [Subject] ... [Object]"}, {"title": "3.2 Common bridge representation hypothesis", "content": "How do compositions work in LLMs beyond the synthetic example? With a microscopic analysis focusing on the copying task, we posit the following Common Bridge Representation (CBR) hypothesis.\nFor compositional tasks, a latent subspace stores intermediate representations from the outputs of relevant attention heads and then matches later heads.\nThis latent subspace can be viewed as a sophisticated example of feature superposition [25] for compositional tasks.\nWe experiment on a variety of LLMs and highlight key findings in Figure 5(b)\u2013(f), where we use GPT-2 as a recurring example and summarize results of other models. Top-scoring PTHs and IHs are distributed across different layers of LLMs, though more PTHs appear in early layers. We sample sequences of the format (*,s#,s#,s#) and calculate the average token-wise probability/accuracy for predicting the third segment s#. See experiment details in Section C.\nExperiments with two interventions. For both experiments, we apply screening to top PTHs and IHs based on diagonal scores, resulting in an average of 9 effective PTHs and 7 effective IHs. The first intervention experiment involves shuffling heads. We randomly permute matrices WQK within the list of IHs, yielding an edited model (\u201cshuffle within\u201d). As comparison, we replace each WQK within the list wby a random WQK Outside the list, yielding another edited model (\u201cshuffle outside\u201d). In a parallel experiment, we shuffle Wov circuits within PTHs similarly. We evaluate the original model and edited models by calculating the average probability of predicting correct tokens.\nThe second intervention experiment involves projection of weight matrices. First, we stack the QK matrices from IHs into a matrix $[W_{QK}^{1},..., W_{QK}^{K}]$ and extract the top right singular vectors $V \\in \\mathbb{R}^{d \\times r}$ for a pre-specified $r$. We call the column linear span of V as the bridge subspace. Then, we edit 25% of all attention heads by weight projection $W_{QK} \\leftarrow W_{QK}VV^T$. After the edit ('keep'), the attention calculation can only use the component of embeddings within the bridge subspace. In a parallel experiment, we make a projection edit (\u2018remove') with an orthogonal complement $W_{QK} \\leftarrow W_{QK}(I_d \u2013 VV^T)$ to force attention"}, {"title": "5 Limitations and future work", "content": "First, our hypothesis is a general conjecture on the mechanism of compositions in Transformers, based on our analysis of IHs. While IHs are pervasive in LLMs, other components or mechanisms for compositions may exist. Additionally, our interpretations are based on a simplified form of self-attention. It would be interesting to explore alternative mechanisms for compositions, and examine variants or practical techniques in LLMs that may impact our hypothesis.\nSecond, we did not develop insights to explain the emergence of the bridge subspace during training. The sharp transition in prediction accuracy is related to the emergent abilities of LLMs observed in broader"}]}