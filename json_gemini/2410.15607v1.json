{"title": "Reinforced Imitative Trajectory Planning for Urban Automated Driving", "authors": ["Di Zeng", "Ling Zheng", "Xiantong Yang", "Yinong Li"], "abstract": "Reinforcement learning (RL) faces challenges in trajectory planning for urban automated driving due to the poor convergence of RL and the difficulty in designing reward functions. The convergence problem is alleviated by combining RL with supervised learning. However, most existing approaches only reason one step ahead and lack the capability to plan for multiple future steps. Besides, although inverse reinforcement learning holds promise for solving the reward function design issue, existing methods for automated driving impose a linear structure assumption on reward functions, making them difficult to apply to urban automated driving. In light of these challenges, this paper proposes a novel RL-based trajectory planning method that integrates RL with imitation learning to enable multi-step planning. Furthermore, a transformer-based Bayesian reward function is developed, providing effective reward signals for RL in urban scenarios. Moreover, a hybrid-driven trajectory planning framework is proposed to enhance safety and interpretability. The proposed methods were validated on the large-scale real-world urban automated driving nuPlan dataset. The results demonstrated the significant superiority of the proposed methods over the baselines in terms of the closed-loop metrics.", "sections": [{"title": "1 Introduction", "content": "Urban automated driving (AD) presents significant challenges due to the complexity and diversity of traffic scenarios. To ease the difficulty, automated driving systems are usually divided into modules, such as perception, prediction, planning, and control. The planning module plays a crucial role in achieving a safe and comfortable driving experience since it determines the behavior of automated vehicles (AVs). Despite the interpretability, conventional approaches for planning heavily rely on hand-crafted rules, making it difficult to handle complex and diverse situations. In contrast, learning-based approaches learn planning policies from large-scale data, thus circumventing the need for meticulously designed rules.\nIn recent years, imitation learning (IL) and reinforcement learning (RL) have dominated the"}, {"title": "2 Reinforced Imitative Trajectory Planning", "content": "AD can be modeled as a Markov decision process (MDP) defined by the tuple (S, A, T, R, \u03b3), with states $s \\in S$, actions $a \\in A$, a transition function $T(s_{t+1}|s_t, a_t) : S \u00d7 S \u00d7 A \u2192 [0, 1]$, a reward function $R(s): S\u2192 R$, and a discount factor $\u03b3 \\in (0,1]$. At each time step t, the ego agent (i.e., the AV) takes an action $a_t$ given a state $s_t$ according to its policy $\u03c0(s_t)$, receives a scalar reward $r_{t+1}$, and visits a new state $s_{t+1}$, resulting in an MDP sequence $(s_0, a_0, r_1, s_1, a_1, . . . , r_{T_m}, s_{T_m}, a_{T_m})$, where $T_m$ is the time horizon. The return is defined as the discounted cumulative reward\n$G_t = \\sum_{k=t+1}^{T_m} \\gamma^{k-t-1}r_k$ \n$G_t = \\sum_{k=t+1}^{T_m} \\gamma^{k-t-1}R(s_{k-1},a_{k-1}).$ \nIn RL, the goal is to identify the optimal policy, parameterized by \u03c6, that maximizes the expected return $E_{s_k\u223c\u03c1_{\u03c0_\u03c6},a_k\u223c\u03c0_\u03c6(s_k)}[G_t]$, where $\u03c1_{\u03c0_\u03c6}$ is the distribution of $s_k$ under policy $\u03c0_\u03c6$. In actor-critic methods, the optimal policy (i.e., the actor) can be obtained by maximizing an approximated value function (i.e., the critic). The value function is defined as $Q^\u03c0(s, a) = E_{s_k\u223c\u03c1_\u03c0,a_k\u223c\u03c0(s_k)}[G_t | s_t = s, a_t = a]$, which can be learned with temporal difference learning based on the Bellman equation.\n$Q^\u03c0(s, a) = R(s, a) + \u03b3E_{s',a'}[Q^\u03c0(s', a')]$\nwhere (s', a') is the subsequent state-action pair of (s, a).\nFor high-dimensional problems, such as AD, the value function is usually approximated by a neural network $Q_\u03b8(s, a)$ parameterized by \u03b8. The deep Q-learning algorithm optimizes the approximator $Q_\u03b8(s, a)$ to match a target q output by a secondary frozen target network $Q_{\u03b8'}(s, a)$\n$q = R(s, a) + \u03b3Q_{\u03b8'} (s', a'),$\nwhere $a' \u223c \u03c0_{\u03c6'} (s')$, $\u03c0_{\u03c6'}$ is a secondary frozen target actor network with parameter $\u03c6'$. The parameters \u03b8' and \u03c6' are either periodically substituted by \u03b8 and \u03c6 or updated by a small proportion \u03be at each time step, i.e., $\u03b8' \u2190 \u03be\u03b8 + (1 \u2212 \u03be)\u03b8'$ and $\u03c6' \u2190 \u03be\u03c6 + (1 \u2212 \u03be)\u03c6'$. However, the value estimated by $Q_\u03b8(s,a)$ is an overestimation . Addressing this problem, the twin delayed deep deterministic policy gradient algorithm (TD3) uses two critic networks ($Q_{\u03b8_1}, Q_{\u03b8_2}$) with two target critic networks ($Q_{\u03b8'_1}, Q_{\u03b8'_2}$) and replaces the target in Eq. (3) by\n$q = R(s, a) + \u03b3\\min_{i=1,2} Q_{\u03b8'_i} (s', \u03c0_{\u03c6'} (s')) .$\nTo further reduce the estimation variance, TD3 adds a clipped normally distributed random noise \u03f5 to the target policy, resulting in a variant of Eq."}, {"title": "2.1 Preliminaries", "content": "AD can be modeled as a Markov decision process (MDP) defined by the tuple (S, A, T, R, \u03b3), with states s \u2208 S, actions a \u2208 A, a transition function T(St+1|St, at) : S \u00d7 S \u00d7 A \u2192 [0, 1], a reward function R(s): S\u2192 R, and a discount factor \u03b3 \u2208 (0,1].\n$G_t = \\sum_{k=t+1}^{T_m} \\gamma^{k-t-1}r_k$\n$G_t = \\sum_{k=t+1}^{T_m} \\gamma^{k-t-1}R(S_{k-1},a_{k-1}).$\n$Q^\u03c0(s, a) = R(s, a) + \u03b3E_{s',a'}[Q^\u03c0(s', a')]$\n$q = R(s, a) + \u03b3Q_{\u03b8'} (s', a'),$\n$q = R(s, a) + \u03b3\\min_{i=1,2} Q_{\u03b8'_i} (s', \u03c0_{\u03c6'} (s')) .$"}, {"title": "2.2 Framework", "content": "As mentioned in Section 1, training a deep neural network-based policy for urban trajectory planning using conventional RL methods is intractable. Therefore, we integrate IL into RL with an actor-critic setting, proposing the reinforced imitative trajectory planning method, as shown in Fig. 1. Specifically, at each time step t, the actor takes an action a\\u207f given a state st. Con-currently, the action sampler generates S actions, i.e., {$a_i^s$}$_{i=0}^S$, for the same state st. The actions are defined as trajectories {$a | a = {$Pi+j$}$_{j=1}^{T_p}$}$_{i=0}^S$, with spatial location $Pi+j = (x_{t+j}^i, Y_{t+j}^i)$ and planning time horizon $T_p$. Afterward, only the action a\\u207f is sent to the environment, which then out-puts the reward rt+1 and the subsequent state St+1. Next, observing the state st+1, the actor and action sampler output actions {$a_{t+1}^s$}$_{i=0}^S$, and the process repeats accordingly.\nWhile the actor and the action sampler interact with the environment, the experience (St, {$a_i^s$}$_{i=0}^S$, rt+1, St+1) is added to the replay buffer at each time step. Subsequently, a mini-batch of experiences is sampled from the replay buffer to update the critic network. Afterward, for each experience, the action with the highest value estimated by the critic network is selected from {$a_i^s$}$_{i=0}^S$ as the optimal action a\\u207f. Finally, the optimal state-action pair (st, a\\u207f) is used to update the actor via IL, and the updated actor is employed to interact with the environment at the next time step."}, {"title": "2.3 Environment", "content": "The environment can be built on a real-world urban automated driving dataset. During train-ing, the environment is in non-reactive log-replay mode, where the other traffic participants, such as vehicles, pedestrians, and cyclists, do not react to the behavior of the ego agent and simply act according to the logged data. Upon receiv-ing the action a\\u207f from the actor, the environment propagates the motion of the ego vehicle using an LQR-based trajectory tracker and the kinematic bicycle model."}, {"title": "2.4 State Representation", "content": "The query-centric paradigm is adopted to encode the traffic scene context. Consider a sce-nario with A agents (including the ego agent and the other traffic participants), M map polygons (either from the upstream perception module or the high-definition map), and T historical time steps. The i-th agent's information at historical time step t-g (g\u2208 {T \u2013 1,\u0422 \u2013 2, ...,0}) con-sists of the spatial position $p_{i}^{t-g} = (x_{i}^{t-g}, Y_{i}^{t-g})$, the heading $h_{i}^{t-g}$, the temporal position $t_{-g}$, the velocity $v_{i}^{t-g} = (v_{i,x}^{t-g}, v_{i,y}^{t-g})$, and the semantic attributes (e.g., agent type). Each map polygon (e.g., lanes, stop lines, and crosswalks) has P sam-ple points with spatial positions $p_{i}^g = (x_{i}^{g}, y_{i}^{g})$, ori-entations $h_{i}^g$, and semantic attributes (e.g., traffic light status of a lane).\nThe query-centric paradigm builds an indi-vidual local spacetime coordinate system for the i-th agent at time step t-g based on the position $p_{i}^{t-g}$ and the heading $h_{i}^{t-g}$ or for the i-th map polygon based on the position and orientation of the first sample point of the poly-gon. Then, the information of an agent at time step t-g is represented as $a_{i}^{t-g} = (||p_{i}^{t-g} - p_{i}^{t-g-1}||_2, <(p_{i}^{t-g}, h_{i}^{t-g}), ||v_{i}^{t-g}||_2, <(v_{i}^{t-g}, h_{i}^{t-g}))$, where heading vector $h_{i}^{t-g} = (cos h_{i}^{t-g},sinh_{i}^{t-g})$, and <(,) denotes the angle between two vectors. The information of a sample point of a map polygon is represented as $m_{i}^g = ||p_{i}^g - p_{i}^{g-1}||_2$.\nIn order to incorporate the relative infor-mation between two spatial-temporal scene ele-ments (e.g., an agent at some time step t-g or a lane), a relative spatial-temporal positional encoding is employed. In particu-lar, given a pair of spatial-temporal positions"}, {"title": "2.5 Actor Network: MotionFormer", "content": "Trajectory planning can be considered as a special case of trajectory prediction. Although trajec-tory prediction involves all agents while trajectory planning focuses solely on the ego agent, both tasks infer future trajectories based on traffic scene understanding. Therefore, a transformer-based actor network, referred to as MotionFormer, is constructed using QCNet architecture for saving effort in actor network design, as shown in Fig. 2.\nMotionFormer comprises an encoder that gen-erates scene encodings and a decoder that decodes K future trajectories over a horizon of Tp time steps for each agent along with the estimated like-lihoods utilizing the scene encodings. Each future trajectory is called a mode. During training, the trajectory with the highest likelihood is considered as the ego agent's action. Additionally, the pre-dicted trajectories for other agents are necessary for downstream post-optimization during testing, as explained in Section 4.\nIn the encoder, the Fourier embedding block first maps the state representation of the agents and map polygons, i.e., $a_{i}^{t-g}$ and $m_{i}^g$, to the Fourier features to enhance the learning of high-frequency signals. These Fourier features are then combined with the semantic attributes, such as the traffic light status of a map polygon and an"}, {"title": "2.6 Action Sampler", "content": "Ideally, the actor should imitate the optimal action $a*$ that maximizes the critic given a state s, i.e., $a*$ = arg $\\max_{a\u2208A}$ Q(s, a). However, finding the optimal action is our goal in the first place. It is intractable because the action space A for AD is continuous, high-dimensional, and with kinematic and dynamic constraints. Therefore, TRAVL samples spline-based actions that are physically feasible for the ego agent in the Frenet frame of the road . Inspired by TRAVL, we propose sampling actions based on piece-wise polynomials, allowing more flexible driving maneuvers.\nSpecifically, an action is decoupled into longi-tudinal and lateral trajectories\n$x_i(t) = P_{xi} [1, t, t^2, t^3, t^4] ^T$\n$y_i(t) = P_{yi} [1, t, t^2, t^3, t^4, t^5] ^T$\nwhere $x_i(t)$ and $y_i(t)$ denote the i-th piece longitu-dinal and lateral trajectories in the Frenet frame, respectively and $P_{xi} = (P_{xi,0}, P_{xi,1},..., P_{xi,4})$ and $P_{yi} = (P_{yi,0}, P_{yi,1},..., P_{yi,5})$ are vectors of coefficients, such that each piece of a trajectory is smoothly connected with its predecessors and successors at the knots. By sampling knots with different lateral positions for lane-keeping and lane-changing maneuvers, a set of trajectories (i.e., action samples) is obtained, as shown in Fig.\nIn practice, denser samples (up to 230,000 trajectories per state in our experiments) can be generated to minimize the risk of missing the optimal action by adjusting the time length of"}, {"title": "2.7 Critic Network: CriticFormer", "content": "In RL, the critic network evaluates how good an action is in a certain state. Aiming this, our critic network, namely CriticFormer, consists of a scene encoder, an action embedding block, and an action-scene attention block, as shown in Fig. 6. Essentially, CriticFormer embeds the action into a query to attend to traffic scene encod-ings and then generates the critic value. To be specific, considering the similarity of the actor and critic networks in traffic scene understanding, CriticFormer employs a scene encoder with the same architecture as the actor network's encoder to encode the scene context. Furthermore, the action embedding block transforms the action into Fourier features. The Fourier features are further embedded by a GRU, the final hidden state of which serves as the action embedding.\nIn the action-scene attention block, the action embedding is successively updated by three cross-attention layers (action-history cross-attention, action-map cross-attention, and action-agent cross-attention) to attend to the ego agent's encodings across T historical time steps, the map encodings, and the surrounding agents' encod-ings at the current time step, respectively. For the ego agent at the current spatial-temporal position ($p^{ego}, h_{i}^{ego,t}$) and the i-th agent or i-th map polygon, the positional encodings for the three cross-attention layers are given by $r_i^{t-g-t} r_i^{ego}$, and $r_i^{ego}$. Finally, the critic value is obtained by passing the updated query through an MLP."}, {"title": "2.8 Training Objectives", "content": "Following QCNet, the ego agent's action is parameterized as a mixture of Laplace distribu-tions:\n$f ({p_{t+j}}|_{j=1}^{T_p}) = \\sum_k P_k \\prod_{j=1}^{T_p}Laplace(p_{t+j} | \\mu_{t+j}^k, \\sigma_{t+j}^k)$\nwith the mixing coefficient pk, the location $\u03bc_{t+j}^k$, and the scale $\u03c3_{t+j}^k$. MotionFormer is trained by imitation learning with a total loss defined as the sum of a classification loss Lc, a regression loss for the proposed trajectory Lp, and a regression loss for the refined trajectory Lr:\n$L_{actor} = L_c + L_p + L_r,$\nwhere\n$L_c = -\\log{\\sum_k^{K}{P_kLaplace(p_{t+j}^{target} | \\mu_{t+j}^k, \\sigma_{t+j}^k)}}$ \n$L_p = \\frac{1}{T_p}\\sum_{i=1}^{T_p} \\log{Laplace(p_{t+j}^{target} | \\mu_{t+j}^k, \\sigma_{t+j}^k)}$\n$L_r = \\frac{1}{T_p}\\sum_{j=1}^{T_p} \\log{Laplace(p_{t+j}^{target} | \\mu_{t+j}^k, \\sigma_{t+j}^k)}$"}, {"title": "3 Bayesian RewardFormer", "content": "The reward function is critical in RL. How-ever, few studies concentrate on learning effective reward functions for urban AD. Besides, as intro-duced in Section 1, existing IRL methods for AD simplify the reward functions with the linear"}, {"title": "3.1 Model Structure", "content": "AVRL models a reward function as a deep Bayesian neural network (BNN) , which esti-mates not only the reward for a state but also the uncertainty of the reward (i.e., how uncon-fident the model is). With variational inference , the deep BNN can be approximated by a deep neural network with dropout layers . To encode the traffic scene context effectively, we also employ the query-centric state representation (see Section 2.4) and propose a transformer-based reward function, namely Bayesian RewardFormer.\nSpecifically, let Mi and bi be the weight"}, {"title": "3.2 Training Objectives", "content": "Defining w = {$W_i$}$_{i=1}^L$ as the set of weight matrices for all L layers, the loss with respect to {$M_i$}, {$b_i$}$_{i=1}^L$ for training Bayesian Reward Former is\n$\\sum_t \\log{R(s_t,a_t)} - \\sum_t R(s,w)$\n$\\sum_t \\log{ \\sum_\\xi(s,w)}$\n$\\sum_\\xi + \\sum_i \\frac{1}{2}\\mid\\mid M_i \\mid\\mid + b_i + (15)\nwith $w$ sampled from the distribution of w. In Eq. (15), at denotes the ego agent's action (trajectory) in state st in the dataset. {$S_{t+j}$}$_{i=1}^{Tp}$ denotes the set of the predicted states that the ego agent will visit after taking action at, assuming that the ego agent follows the kinematic bicycle model and the other agents are in a non-reactive log-replay mode. S' represents the set of the predicted states for another possible action sampled based on polynomials in the Frenet frame of the road .R() is defined as  R(s_{t+j}). The base b\nof the exponentiation is set to 1.1, which differs from the natural constant e employed in AVRL and improves the convergence when training the reward function."}, {"title": "3.3 Predicted Uncertainty-Penalized Reward", "content": "Neural networks make arbitrary predictions out-side the domain they were trained on, known as epistemic uncertainty . In the case of RL, a neural network-based reward function may output wrong rewards when the states differ from those in the dataset, thus leading to unexpected or even dangerous actions of the learned policy. Therefore, the reward at each RL time step is replaced with"}, {"title": "4 Post-Optimizer", "content": "The epistemic uncertainty is inevitable for neu-ral networks, including MotionFormer. Motion-Former may output dangerous actions in unfamil-iar situations, e.g., a trajectory leading to a colli-sion with a pedestrian. Therefore, a model-driven post-optimizer is developed to further optimize the action output by MotionFormer in the testing stage.\nThe post-optimizer consists of QP path plan-ning and QP speed planning similar to Ref. , both of which are performed in the Frenet frame of the road. In the post-optimization, an action is decoupled into a path profile y(x) and a speed profile x(t), which are smoothing spline functions. Here, x, y, and t represent the longitudinal coor-dinate, lateral coordinate, and time, respectively.\n$min J(y) = \\sum_{i=0}^{3} w_i J_i (y)$ \nsubject to linear constraints L(y(x)) \u2264 0, where \u03a9 = {$y : [X_o, X_n] -> R\\mid y,y1,..., y^{(m)}$ is absolutely continuous and fan($y^(i)^2< \u221e$ , where $Y(i)$ denotes\ny(x) == Yo+ Yi(x-x_i)\nSpecifically y(x) is defined as\ny(x) == y0(x - x0), x \u2208 [x1, x2)] x \u2208 [xn-1\nx \u2208 x0, x1))\nJ0 == fan(y(x)- y (x)) dx. J1 ==fan((y(1) (x))2dx.J2 ==fan((y(2) (x))2dx.J3 ==fan((y(3) (x))2dx."}, {"title": "5 Experiments", "content": "We conducted experiments on the large-scale real-world automated driving nuPlan dataset [21, 22] to validate the effectiveness of our approach.\nThe nuPlan dataset consists of 1282 hours of diverse urban driving scenarios from four cities\nOur experiments utilized the simulation and eval-uation framework provided by the nuPlan dataset which enables closed-loop simulation considering the interactions between traffic participants. The simulator receives the planned trajectory of 8\nTo evaluate the performance of a planner, the closed-loop non-reactive score and the closed-loop"}, {"title": "5.4 Comparison with Baselines", "content": "The results of the baselines and proposed approach on the testing set are summarized in Table 2. RITP involves training MotionFormer to predict the trajectories of all surrounding agents (referred to as multi-agent supervision hereafter), while RITP- focuses solely on optimizing the tra-jectory prediction for the ego agent. HRITP and HRITP-, where \"H\" stands for hybrid-driven, employ the post-optimizer to refine the output of MotionFormer.\nComparing the results of Urban Driver, QCMAE, and HRITP in Table 2, it can be concluded that our HRITP method substantially outperforms the baselines in terms of both the closed-loop non-reactive score (over 72% improve-ment) and the closed-loop reactive score (over 60% improvement). Furthermore, HRITP significantly reduces at-fault collisions and better controls vehi-cle spacing, improving the success rate by over 63% and 36% for the two modes, respectively. Fewer collisions happened in the closed-loop reac-tive mode than in the closed-loop non-reactive mode because the surrounding agents in the lat-ter mode automatically controlled the distance to the ego agent to avoid collisions.\nThe performance improvement of HRITP is attributed to several factors. Firstly, the superior performance of RITP over QCMAE demonstrates that RITP achieves better generalization through effective environment exploration based on the RL paradigm. Secondly, the comparison between RITP and HRITP highlights the effectiveness of the post-optimizer in refining MotionFormer's output, thus preventing erroneous plans. Further-more, the multi-agent supervision technique also contributes to performance improvement. This technique allows MotionFormer to fully utilize information from each scenario to model interac-tions between agents, thus improving the accuracy of trajectory prediction for the ego agent and sur-rounding agents. Note that predicted trajectories are crucial as they provide collision-avoidance con-straints for the post-optimizer. We also found that integrating IL into RL is necessary. Without IL, MotionFormer trained solely by RL failed to gen-erate a feasible trajectory and only produced a set of disorganized positions."}, {"title": "5.5 Qualitative Results", "content": "Some qualitative results on the testing set are pre-sented in this section. Fig. 8 depicts a scenario of traversing an intersection, where the expert turned right with a trajectory represented by the orange curves in Fig. 8(a), (b), and (c). The ego vehicle controlled by UrbanDriver, QCMAE, and HRITP produced different trajectories of the ego vehicle as depicted by the blue curves in Fig. 8(a), (b), and (c). The top and bottom halves of Fig. 8(a), (b), and (c) represent the first and last frames of simulation, respectively. The motion states are shown in Fig. 8(d). As shown in Fig. 8, at the beginning of the simulation, Urban-Driver and QCMAE were able to drive within the lane. However, they drove out of the drivable area later, which was caused by covariate shift. In con-trast, the ego vehicle controlled by HRITP entered the intersection following the preceding vehicle and properly stopped when the preceding vehicle stopped, probably due to pick-up or drop-off, just as the expert did.\nAnother scenario is shown in Fig. 9, where the ego vehicle was supposed to stop near the inter-section. However, the UrbanDriver quickly veered off, running a red light and even driving into the wrong lane, as depicted in Fig. 9(a). QC\u041c\u0410\u0415 performed much better during the first 7 seconds but gradually drove out of the drivable area over the next 8 seconds, as presented in Fig. 9(b). By comparison, HRITP successfully guided the ego vehicle to slow down and keep a safe distance from the preceding vehicle, as shown in Fig. 9(c)."}, {"title": "5.6 Discussion", "content": "The application of RL in trajectory planning for urban automated driving is limited by the poor convergence of RL and the difficulty of design-ing reward functions. The proposed RITP method integrates RL with IL to overcome the conver-gence problem. It trains CriticFormer to evaluate the closed-loop effect of an action, according to which supervisory signals are generated for IL. In contrast to RL, the gradients computed by IL are sufficient to optimize deep neural networks such as MotionFormer. Then, MotionFormer trained through IL explores the state and action spaces using the trajectory noise, thus resolving the suboptimality and the online computational com-plexity issues in TRAVL. Moreover, Bayesian RewardFormer provides effective reward signals for RL. As indicated by the results presented in the previous subsections, RITP significantly out-performs the baselines in terms of the closed-loop metrics. Another important reason for the effec-tiveness of RITP is that CriticFormer, Motion-Former, and Bayesian RewardFormer utilize the attention mechanism to extract information from the query-centric representation effectively. We also found that combining data-driven and model-driven methods can further improve the perfor-mance of RITP. Due to the inevitable epistemic uncertainty in neural networks, RITP might plan dangerous trajectories in unfamiliar situations. The post-optimizer refines the trajectories output by RITP to correct these wrong plans.\nIt is worth noting that common IL-based meth-ods can be incorporated into the proposed RITP framework by simply replacing MotionFormer and"}, {"title": "6 Conclusion", "content": "This paper proposes a novel RL-based trajec-tory planning method for urban automated driv-ing. The proposed method integrates IL into RL to improve the convergence, thus enabling planning for multiple future steps. Furthermore, a transformer-based Bayesian reward function is developed, which removes the linear struc-ture assumption imposed by most existing meth-ods and is applicable to RL in urban scenar-ios. Besides, a hybrid-driven trajectory planning framework is proposed to enhance the feasibility of the trajectories planned by the RL agent. The results of the experiments conducted on the large-scale real-world urban automated driving nuPlan dataset indicate that the proposed methods sig-nificantly outperform baselines in terms of the closed-loop metrics."}]}