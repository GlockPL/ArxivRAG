{"title": "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "authors": ["Eunseong Choi", "Sunkyung Lee", "Minjin Choi", "June Park", "Jongwuk Lee"], "abstract": "Large language models (LLMs) have achieved significant performance gains using advanced prompting techniques over various tasks. However, the increasing length of prompts leads to high computational costs and often obscures crucial information. Prompt compression has been proposed to alleviate these issues, but it faces challenges in (i) capturing the global context and (ii) training the compressor effectively. To tackle these challenges, we introduce a novel prompt compression method, namely Reading To Compressing (R2C), utilizing the Fusion-in-Decoder (FiD) architecture to identify the important information in the prompt. Specifically, the cross-attention scores of the FiD are used to discern essential chunks and sentences from the prompt. R2C effectively captures the global context without compromising semantic consistency while detouring the necessity of pseudo-labels for training the compressor. Empirical results show that R2C retains key contexts, enhancing the LLM performance by 6% in out-of-domain evaluations while reducing the prompt length by 80%.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have recently exhibited remarkable performance gains in various tasks owing to a wide variety of prompting, e.g., Retrieval-Augmented Generation (RAG) (Gao et al., 2023), Chain-of-Thought (CoT) (Wei et al., 2022), and In-Context Learning (ICL) (Dong et al., 2023). The use of rich prompts unlocks the abilities of LLMs, but prompts can be verbose to deliver sufficient information. The lengthy prompts not only increase computational costs but also make LLMs struggle to discern important information. Although the input length limit has recently been extended to a million tokens (Reid et al., 2024), the quadratic increase in computation over the input length is still a substantial burden."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Prompt Compression", "content": "As more complex prompting techniques are proposed to unlock the capabilities of LLMs, prompt compression has been actively studied to handle lengthy input. Prompt compression methods are broadly categorized into soft prompting and two token pruning methods which are abstractive and extractive compression."}, {"title": "Soft Prompting", "content": "Soft prompting methods compress texts in embedding space, using a limited number of learnable embeddings to imply input tokens (Chevalier et al., 2023; Mu et al., 2023; Qin and Durme, 2023; Cheng et al., 2024). They achieve high compression ratios with minimal loss of semantics. However, the embedding must be learned for each language model, and it is challenging to apply to API-based LLMs."}, {"title": "Abstractive Compression", "content": "Abstractive compression aims to generate the core information of a prompt using a generative model. Wang et al. (2023) introduced sentence-level pseudo-labels and trained the model to generate labeled sentences given the original prompt. Chuang et al. (2024) proposed to optimize the compressor using a reward function considering length constraints. Recently, Ali et al. (2024) proposed to construct a graph with LLMs and reconstruct the prompt using subgraphs within the graph. While abstractive compression effectively compresses prompts by reconstructing them, they suffer from the substantial cost of auto-regressive generation."}, {"title": "Extractive Compression", "content": "Extractive compression methods extract only core information from prompts. Representative works proposed by Li et al. (2023); Jiang et al. (2023a) remove redundancy based on the entropy-based metric without any training. Jiang et al. (2023b) additionally leverages question-aware perplexity. However, the entropy-based methods are hardly aligned with the objective of prompt compression, which is to retain only the essential information (Ali et al., 2024). Recently, there have been works on training compressors to extract the salient information from the prompt. Pan et al. (2024); Xu et al. (2024) created pseudo labels and Huang et al. (2023); Jung and Kim (2023) incorporated reinforcement learning for training a compressor. However, they still struggle to capture the global context across whole prompts since each segment of prompts is compressed independently."}, {"title": "2.2 Fusion-in-Decoder (FiD)", "content": "Fusion-in-Decoder (Izacard and Grave, 2021b) has been introduced for Open-Domain Question Answering (ODQA), effectively aggregating information from multi-documents to generate an answer. Recent studies have demonstrated the versatility of the FiD structure in various tasks thanks to its ability to handle lengthy input without information loss. Izacard and Grave (2021a) showed that the cross-attention score obtained from the FiD decoder can be utilized as a label for retrieval. In addition, Ye et al. (2023) incorporated the FiD structure for in-context learning with long input. This paper introduces a new method for compressing long prompts utilizing FiD and demonstrates its effectiveness."}, {"title": "3 Proposed Method", "content": "This section introduces R2C, a novel prompt compression method using the Fusion-in-Decoder (FiD) architecture. In contrast to the existing compression methods, which consider the local context within each chunk, R2C effectively seizes the global context that lies across chunks in the lengthy prompt.\nThe prompt $P_c$ consists of an instruction I, a context C, and a question Q, i.e., $P_c = (I; C; Q)$, where the context is usually the longest component. R2C compresses the context C to $\\hat{C}$ and reduces the overall prompt length from $|P_c|$ to $|\\hat{P_c}|$.\nFirst, we divide context C into multiple chunks and feed them into FiD to obtain the importance score for each token (Section 3.1). Next, token scores are aggregated in chunk- and sentence-level for multi-granularity compression (Section 3.2). Finally, we hierarchically compress the context in a coarse-to-fine manner, i.e., chunk-to-sentence order (Section 3.3). Note that the training process is omitted in this paper since R2C is not trained for prompt compression. Instead, R2C utilizes the trained FiD weights for the QA task."}, {"title": "3.1 Identifying Importance in Context", "content": "We calculate the token-level importance in the context using FiD (Izacard and Grave, 2021b). The context is divided into smaller chunks as input units, and each chunk is processed individually with multiple encoders. The outputs of multiple encoders are concatenated and utilized as a key-value matrix of cross-attention for decoding. Even if the overall length of a prompt exceeds the maximum input length of language models, FiD discerns the important parts considering the global context. Here, we leverage the attention score as the importance criterion.\nGiven a prompt $P_c$, we divide the context C into K chunks: $C = [C_1, C_2, ..., C_K]$. For each chunk $C_i$, we input it to the FiD-encoder and get the token embeddings $H_i \\in R^{M \\times h}$, where M is the maximum sequence length of the FiD-encoder, and h is the size of the hidden dimension. Similar to the original QA task, we prepend the question Q to $C_i$ if a question is given in the dataset, otherwise, we set Q as an empty string.\n$H_{i} = FiD\\text{-}encoder(Q + C_{i}) \\in R^{M \\times h}$.\nWe perform it for K chunks and concatenate all token embeddings as $H = (H_{1}; ...; H_{K}) \\in R^{(K \\times M) \\times h}$. Then, H is converted to the key matrix K through the projection layer $W_k \\in R^{h \\times h}$. The cross-attention score $A \\in R^{1 \\times (K \\times M)}$ is calculated by the matrix product using query embedding $q_{[BOS]} \\in R^{1 \\times h}$ and the key matrix K.\n$A = \\text{softmax}(\\frac{q_{[BOS]}K}{\\sqrt{h}}), \\text{ where } K = W_kH^{T}$.\nDue to the maximum sequence length, existing works (Jiang et al., 2023a,b) are limited to the local context within chunks. In contrast, R2C effectively captures the global context across all chunks by processing concatenated outputs. We define the token-level importance $t_{ij}$ of the j-th token in i-th chunk as the sum of the attention scores A over all layers and heads in the FiD-decoder.\n$t_{ij} = \\sum_{l=1}^{L} \\sum_{h=1}^{H} A_{i, j}^{(l, h)}$\nHere, L is the number of layers in the decoder, and H is the number of heads. $A_{i,j}^{(l, h)}$ denotes the attention score for the j-th token in the i-th chunk of h-th head in l-th layer."}, {"title": "3.2 Aggregating Unit Importance", "content": "We adopt two compression units with coarser granularity than tokens, i.e., chunks and sentences. A naive way of compression is to prune redundant tokens with low importance scores until the desired compression ratio is reached. However, token-level compression neglects the semantic integrity and the grammatical structure of the text (Jiang et al., 2023a; Ali et al., 2024). Otherwise, R2C adopts two"}, {"title": "3.3 Performing Hierarchical Compression", "content": "Given the multi-granularity unit importance, i.e., chunk and sentence, R2C hierarchically compresses the prompt. To preserve semantic integrity (Jiang et al., 2023b; Huang et al., 2023), it performs compression from chunks to sentence excluding token-level. Token-level compression can break the grammatical structure or struggle to generate answers exactly matched to the ground truth\u00b9.\nAlgorithm 1 describes the compression procedure of R2C. Given sorted chunks C as input, R2C first performs chunk-level compression and generates $\\hat{C}\\text{chunk}$ by retaining only the important chunks (line 1-8). Then, it further performs sentence-level compression and selects only the crucial sentences for each chunk to yield $\\hat{C}\\text{sent}$ (line 9\u201323). We use $\\hat{C}\\text{sent}$ for the final compressed context $\\hat{C}$ (line 24),"}, {"title": "4 Experiments Setup", "content": ""}, {"title": "4.1 Datasets", "content": "We validate the performance of R2C on two datasets. (i) In-domain: We utilize Natural Questions (NQ) (Kwiatkowski et al., 2019), which are widely adopted in Open-domain Question Answering (ODQA) tasks. We retrieve 20 candidate passages for each question using DPR (Karpukhin et al., 2020; Izacard and Grave, 2021a). (ii) Out-of-domain: To evaluate the generalizability of compressed prompts, we use the LongBench (Bai et al., 2023)\u00b2. We include five types of tasks: single-document QA (SingleDoc), multi-document QA (MultiDoc), summarization (Summ.), few-shot learning (FewShot), and code completion (Code). Note that we only evaluate the English datasets and omit the synthetic tasks to validate the model's ability in real-world scenarios."}, {"title": "4.2 Evaluation Metrics and Prompts", "content": "For Natural Questions, we use Span Exact Match (Span EM) and prompts following Liu et al. (2023) to evaluate whether the generated text contains the answer. For LongBench, we follow metrics and prompts of each dataset provided by the official benchmark"}, {"title": "4.3 Baselines", "content": "We compared R2C with the following models. (i) Two retrieval-based models: BM25 (Robertson and Walker, 1994) and DPR (Karpukhin et al., 2020), performing only chunk-level compression. DPR (Karpukhin et al., 2020) is trained with knowledge distillation on the NQ dataset (Izacard and Grave, 2021a). For the BM25 results in the LongBench dataset, we follow the experimental setup from LongLLMLingua (Jiang et al., 2023b). The key difference is that we apply BM25 at the chunk-level instead of the sentence-level. (ii) Five compression-based models: Selective-Context (Li et al., 2023), LLMLingua"}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Main Results", "content": "Question Answering Task. Table 1 reports the accuracy of FiD and two target LLMs, namely GPT-3.5 and LLaMA2-7B, with different compression methods on the Natural Questions test set. (i) The proposed method achieves the best performance compared to state-of-the-art compression methods. Specifically, it shows improvements of 5.6% and 11.1% over the most effective baseline, RECOMP (Xu et al., 2024), when GPT-3.5 and LLaMA2-7B are used as target LLMs, respectively. Notably, R2C outperforms the original prompt when used with LLaMA2, and with GPT-3.5, we achieve comparable performance with six times fewer tokens. (ii) DPR (Karpukhin et al., 2020), RECOMP (Xu et al., 2024), and R2C, which are trained on question-answer datasets, i.e., in-domain evaluation, significantly outperform other methods."}, {"title": "5.2 In-depth Analysis", "content": "Compression Efficiency. Figure 3-(a) presents the efficiency of R2C and baseline methods on LongBench. We measure the latency using a single NVIDIA A6000 GPU with a batch size of 1. The y-axis represents the average accuracy of the tasks included in LongBench, and the x-axis represents the average compression latency per prompt. Results are based on T = 2000 and GPT-3.5 as the target LLM. Notably, R2C dramatically improves efficiency while also enhancing accuracy, achieving a latency improvement of 1.6 times over the most efficient existing method, i.e., LLMLingua-2, and 14.5 times over the most effective method, i.e., LongLLMLingua. This improvement in efficiency is largely due to the smaller model size used for compression. Specifically, R2C is based on T5-base (Raffel et al., 2020), which has 223M parameters, while LLMLingua-2 adopts XLM-ROBERTa-large (Conneau et al., 2020) with 355M parameters. LongLLMLingua and LLMLingua use a large"}, {"title": "End-to-end Efficiency", "content": "Table 3 illustrates the end-to-end latency of R2C depending on the different number of target tokens. We use 3,350 samples in LongBench datasets and set the maximum decoding token of the API to 200 for all datasets following (Jiang et al., 2023b). Although token pruning can be used in API-based models and reduce API costs, the benefits can be offset if the compression process takes too long. Notably, the overall end-to-end inference time is accelerated with the proposed method. As shown in Figure 3-(a), the compression is efficiently performed by R2C. Considering the latency of both compression and the API, R2C can infer the answer with 74.0% latency compared to the original prompt."}, {"title": "Varying Target Length", "content": "Figure 3-(b) shows the average performance on LongBench according to the length of compressed prompts. We adjusted the compression ratio from 2x to 10x (i.e., 5K-"}, {"title": "5.3 Ablation Study", "content": "Comparison with variants of R2C. Table 4 analyzes the effectiveness of various strategies utilized in R2C. (i) R2C with token-level compression drastically degrades the performance of R2C. This indicates that since it focuses only on prominent tokens while neglecting semantic consistency, it easily confuses the target LLM, as shown in Section 3.3. (ii) R2C without the QA task training, i.e., T5-base (Raffel et al., 2020) initialization, shows a significant performance drop of 23.2%. It implies"}, {"title": "Effectiveness of Hierarchical Compression", "content": "Figure 4 illustrates the impact of hierarchical compression parameters (i.e., \u03c1 and \u03b3) on the NQ dev set. Figure 4-(a) illustrates that the balance of chunk and sentence-level compression by \u03c1 = 0.8 yields the best performance, indicating that it can preserve essential information more effectively by considering both coarse- and fine-grained levels. Figure 4-(b) depicts the effect of \u03b3 in eq (6). Higher \u03b3 removes more tokens from less important chunks. While \u03b3 = 0 treats all chunks equally, using chunk-level importance in sentence-level compression improves performance by tailoring compression to the importance of each chunk. However, high \u03b3 values reduce performance, suggesting that aggressive sentence-level compression on low-scored chunks harms overall compression quality."}, {"title": "6 Conclusion", "content": "In this work, we explored the capabilities of multi-document readers for prompt compression and successfully bridged two different lines of research. We propose a novel prompt compression method, namely Reading To Compressing (R2C), leveraging the Fusion-in-Decoder. R2C effectively captures global context and identifies salient information across multiple segments. Furthermore, by training the compressor using question-answering datasets, the most influential tokens are identified without using noisy pseudo-labels for compressed prompts. Experimental results demonstrate that R2C outperforms existing prompt compression methods by preserving semantic integrity and even surpasses uncompressed prompts by removing ambiguity."}, {"title": "7 Limitations", "content": "Task Generalization. While training QA captures important context across various benchmarks, it remains challenging to generalize the capability to all tasks. Prompt compression requires understanding both instructions and the entire prompt. Recent work (Ye et al., 2023) validates the FiD structure in in-context learning and suggests excluding redundant questions. Given FiD's efficiency in capturing global context, further research, including diverse training strategies, remains to be explored.\nDynamic Compression Ratios and Granularity. R2C sets a target number of tokens for prompt compression, providing intuitive usability. However, each prompt has an optimal compression ratio, and using fewer tokens can remove noise and enhance performance in some cases. It is necessary to adjust the length of prompts dynamically based on each prompt. Additionally, the appropriate compression granularity, e.g., chunk, sentence, phrase, or token, should be adjustable. Even when compressing the same number of tokens, determining the appropriate granularity for each prompt and compressing accordingly can ensure that the most relevant information is retained, potentially improving performance across various tasks."}, {"title": "Ethics Statement", "content": "This work fully respects ACL's ethical guidelines. We have utilized scientific resources available for research under liberal licenses, and our use of these tools is consistent with their intended applications."}]}