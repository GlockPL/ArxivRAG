{"title": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation", "authors": ["Yihong Guo", "Yixuan Wang", "Yuanyuan Shi", "Pan Xu", "Anqi Liu"], "abstract": "Training a policy in a source domain for deployment in the target domain under a dynamics shift can be challenging, often resulting in performance degradation. Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories. However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain. In this work, we propose to utilize imitation learning to transfer the policy learned from the reward modification to the target domain so that the new policy can generate the same trajectories in the target domain. Our approach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL), utilizes the reward modification for domain adaptation and follows the general framework of generative adversarial imitation learning from observation (GAIfO) by applying a reward augmented estimator for the policy optimization step. Theoretically, we present an error bound for our method under a mild assumption regarding the dynamics shift to justify the motivation of our method. Empirically, our method outperforms the pure modified reward method without imitation learning and also outperforms other baselines in benchmark off-dynamics environments.", "sections": [{"title": "1 Introduction", "content": "The objective of reinforcement learning (RL) is to learn an optimal policy that maximizes rewards through interaction and observation of environmental feedback. However, in domains such as medical treatment [1] and autonomous driving [2], we cannot interact with the environment freely as the errors are too costly or the amount of access to the environment is limited. Instead, we might have access to a simpler or similar source domain. This requires domain adaptation in reinforcement learning. In this paper, we study a specific problem of domain adaptation in reinforcement learning (RL), where only the dynamics (transition probability) are different in two domains. This is called off-dynamics RL [3\u20135]. Specifically, we focus on a problem setting in which we have limited access to rollout data from the target domain, but we do not have access to the target domain reward, following the previous off-dynamics work [3\u20135].\nPrevious work on off-dynamics RL, such as Domain Adaptation with Rewards from Classifiers (DARC) [3] and [6, 5], focuses on training the policy in the source domain with a modified reward function that compensates for the dynamics differences. The reward modification is derived so that the distribution of the learning policy's experience in the source domain matches that of the optimal trajectories in the target domain. As a result, their experience in the source domain will"}, {"title": "2 Backgrounds", "content": "Off-dynamics reinforcement learning We consider two Markov Decision Processes (MDPs): one is the source domain Msrc, defined by (S, A, R, Psrc, \u03b3), and the other one is the target domain Mtrg, defined by (S, A, R, Ptrg, Y). The difference between them is the dynamics p, also known as transition probability, i.e., Psrc \u2260 Ptrg or Psrc(St+1|St, at) \u2260 Ptrg (St+1|St, at). In our paper, we experiment with two types of dynamics shift: 1) broken environment [3], in which the 0-th index value is set to be 0 in action, and 2) modifying the gravity/density setting of the target environment [9]. The source and the target domain share the same reward function, i.e., r'src(st, at, St+1) = 'trg(St, at, St+1). All other settings, including state space S, action space A, and the discounting factor y, are the same. We will use y = 1 in the derivation and analysis in our paper.\nWe aim to learn a policy (as) using interaction from the source domain together with a small amount of data from the target domain (st, at, St+1)trg to maximize the expected discounted sum of reward \u0395\u03c2, Purg [\u03a3\u03c4ytr(st, at)] in the target domain. Note that we assume we only have limited access to the target domain transition, namely (st, at, St+1)trg, in the whole process and we do not utilize the target domain reward.\nPt\nImitation learning (from Observation) Imitation Learning (IL) trains a policy to mimic an expert policy \u03a0\u0395 with expert demonstration {(so, ao), (s1, a\u2081), ...} or {(80, 81), (S1, S2), ...}. Generative adversarial imitation learning (GAIL) [7] uses an objective similar to Generative adversarial networks (GANs) that minimizes the distribution generated by the policy and the expert demonstration. It alternatively trains a discriminator Dw and a policy \u03c0\u03b8 to solve the min-max problem:\nming maxD E(s,s')~\u03c0\u03b5 [log Dw(s, s')] + E(s,s')~\u03c0\u03bf [log(1 \u2013 Dw(s, s'))] \u2013 1\u0397(\u03c0\u03bf), (2.1)\nwhere s' is the next state and H(\u03c0\u03bf) is the entropy of the policy \u03c0\u03b8. Note that in our problem, we mimic the state-only expert demonstrations {(50,51), (81, 82), ...} instead of the expert's actions. This setting is also called imitation learning from observation [8]. We will further discuss why we use state observation instead of action in section 3.2. Dw is the classifier that discriminates whether the state pair is from the expert \u03a0\u0395 Or generated by the policy \u03c0\u03b8. Then, the policy is trained with the RL algorithm using reward estimation \u2013 log Dw (s, s') as the reward. The optimization of the Eq. (2.1) involves alternatively training the policy and the discriminator."}, {"title": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning", "content": "In this section, we present our algorithm, DARAIL, under the off-dynamics RL problem setting. First, we introduce DARC [3] in Section 3.1, which provides the distribution of target optimal trajectories in the source domain to mimic. Then, in Section 3.2, we introduce the imitation learning component through which we utilize the trajectories provided by DARC and transfer the DARC policy to the target domain. We aim to learn a policy that generates the same distribution of trajectories in the target domain as the DARC trajectories in the source domain."}, {"title": "3.1 Off-dynamics RL via Modified Reward", "content": "DARC is proposed to solve the off-dynamics RL through a modified reward that compensates for the dynamics shift [3]. Here, we first introduce DARC and its drawbacks. DARC seeks to match the policy's experiences in the source domain and optimal trajectories in the target domain. We define T = {(81, A1), (82, A2), ..., (st, at), ...} as a trajectory. We use Tre to represent the trajectories generated by \u03c0\u03b8 in the source domain. The policy's distribution over trajectories in the source domain is defined as:\nq(T) = P1(81) \u03a0t Psrc(St+1|St, at)\u03c0\u03c1(at|St). (3.1)\nLet \u03c0* = argmax \u0395\u03c0,pug [\u2211tr(st, at)] be the policy maximizing the cumulative reward in the target domain. We use T to represent the trajectories generated by \u03c0* in the target domain. Given the assumption that the optimal policy \u03c0* in the target domain is proportional to the exponential reward, i.e., \u03c0*(at|st) x exp(Str(st, at)), the desired distribution over trajectories in the target domain is defined as:\np(\u03c4) \u00d7 P1 (81) \u03a0t Ptrg (St+1|St, at) \u00d7 exp (Str(st, at)).\nDARC policy can be obtained by minimizing the reverse KL divergence of p(7) and q(7):\nmin DKL(q||P) = - min Epsrc \u2211tr(st, at) + \u2206r(st, at, St+1) + \u0397\u03c0\u03bf [at|St] + C, (3.2)\n(3.3)\nwhere Ar(st, at, St+1) := log Ptrg (St+1|St, at) - log Psrc(St+1|St, at) and c is a partition function of p(T), which is independent of the dynamics and policy. The Ar(st, at, St+1) can be calculated through the following procedure: i), train two classifiers p(trg|st, at) and p(trg|st, at, St+1) with\ncross-entropy loss LCE; ii), Use Bayes' rules to obtain the log ( Psrc (St+1 St,at)\n). Details are in Appendix C.1. Eq. (3.3) shows that \"DARC can be obtained via maximum entropy algorithm with a\nmodified reward\" rmodified\n= r(st, at) + \u2206r(st, at, St+1) at every step.\nHowever, DARC matches the distribution of 7 and ARC. As the dynamics shift exists, \u03c0\u03c1ARC will\nnot recover the optimal policy \u03c0*, and deploying the DARC in the target domain will usually suffer\nfrom performance degradation due to the dynamics shift, as shown in Figure 1(a) and Figure 9 in\nAppendix. However, in the source domain 7 DARC resembles those optimal trajectories in the target\ndomain. Given the property of TCARC, we propose to use imitation learning from observation with\nTIDARC as expert demonstrations to transfer DARC to the target domain. The new policy in the target\ndomain should behave similarly (generate similar trajectories) as DARC in the source domain."}, {"title": "3.2 Imitation Learning from Observation with Reward Augmentation", "content": "In this section, we present the Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) method, which mitigates the problem of DARC via imitation learning from observation. As described in Section 3.1, TSCARC resembles the target optimal trajectories, and we want to transfer DARC's behavior to the target domain. A natural way to tackle it is utilizing imitation learning to mimic the expert demonstration TSC. Following [7, 8], the objective can be formulated as:\nmint maxw {Epug, [\u03a3 log Dw(st, St+1)] + E(St, St+1)~DARC [Et log(1 \u2013 Dw(st, St+1))]}. (3.4)\nwhere Dw is the discriminator in the generative adversarial imitation learning and ( is the policy to be learned in the target domain. In the objective function Eq. (3.4), the (st, St+1) pairs are from the target domain, while we do not have much access to the target domain. Alternatively, we can use the (St, St+1) pairs from the source domain and re-weight the transition with the importance sampling method to account for the dynamics shift. The objective with data rolled out from the source domain, and the importance sampling is as follows:\nmint maxw {Epsre, [tp(St, St+1) log Dw(st, St+1)] + E(st,St+1)~T DARC [Et log(1 - Dw(st, St+1))]}, (3.5)\nwhere p(st, St+1) =\nPug (St+1 St,at)\nPsrc (St+1 St,at) is the importance weight. Note that we do the generative adversarial imitation learning from only state observations (GAILfo) with (St, St+1) [9\u201311] instead of (st, at).\nThis is because we aim to learn a policy ( to produce the same trajectory distributions in the target as the ones TDARC produces in the source domain, despite the dynamics shift, rather than mimicking the\npolicy. Mimicking the (st, at) pairs will recover the same policy as DARC, and deploying it to the target domain will not recover the expert trajectories due to the dynamics shift.\nThis objective Eq. (3.5) can be interpreted as training the discriminator Dw to discriminate whether the (st, St+1) generated by in the target domain matches the distribution of DARC trajectories in the source domain using data rolled out from the source domain with and importance weight. Then, after the discriminator is fitted, the policy can be trained with the reward estimator RAE with model-free RL. The objective is:\nmaxs Epsre, [\u03a3 RAE(St, St+1)], (3.6)\nwhere RAE is defined as follows:\nRAE(St, St+1) = \u2212 log Dw(st, St+1) + p(St, St+1)(r'src(St, at, St+1) + log Dw(st, St+1)). (3.7)\nHere the r'src (St, at, St+1) is the reward obtained from the source domain, which is the same as the reward from the source domain, i.e. 'trg(St, at, St+1). In imitation learning, the \u2013 log Dw(st, St+1) can be viewed as a local reward function for the policy optimization step and the objective is\nmaxs Epsre, [\u03a3t \u2013 log Dw(st, St+1)]. So Eq.(3.5) can be viewed as learning a reward function for the training of \u03da. However, as the dynamics shift exists, the estimation of the \u2013 log Dw(st, St+1) could be biased, which is similar to the case in off-policy evaluation (OPE) [12\u201316] when training\na reward estimation on biased data. As we have access to the source domain and can obtain the reward from the rollout, we are motivated to use both the reward estimation \u2013 log Dw(st, St+1) and\nthe ground truth reward in the source domain rsrc(St, at, St+1) so that we could have a better reward estimation than \u2013 log Dw(st, St+1) under dynamics shift. The RAE here can be viewed as using\nlog Dw(st, St+1) as a base estimator of the reward and use r'src (St, at, St+1) and importance weight p(st, St+1) to correct it. This correction idea is similar to the doubly robust estimator (DR) [12] in\nOPE. The DR estimator combines the reward estimation \u00ee and the importance-weighted difference between true reward r and \u00ee. Specifically, the DR method takes the reward estimation as a base estimator and applies the importance weighting to the difference between true reward r and r, which is p(r \u2013 r) term, to correct the bias of the r, where p is the importance weight."}, {"title": "4 Theoretical Analysis of DARAIL", "content": "Let \u03c0* = argmax\u201e \u0395\u03c0,pug [\u2211tr(st, at)] be the optimal policy maximizing the cumulative reward in the target domain and ( be the policy learned from DARAIL. Now, we provide an error bound for DARAIL. Details of the proof are deferred to Appendix B.\nTheorem 4.1. Let m be the number of the expert demonstration and R(m)=\nEo [SUPDED \u03a3=1&D(St, St+1)] be the empirical Rademacher complexity. Let B be the error bound of DARC in the source domain, i.e. Epsc, DARC [Str(st,at) + H[at|st]]\nEpsc, DARC [Str(st, at)] < B and W be the upper bound of the importance weight, i.e. p(st, St+1) \u2264\nW, (St, St+1). Let discriminator class D be a \u2206-bounded function, i.e. |Dw(St, St+1)| < \u25b3 given\nany (St, St+1). ||r||D measures the richness of the discriminator to represent the ground truth reward\nas defined in Appendix B.2. dp is a defined neural network distance between the (st, St+1) distribu-\ntions generated by the #DARC and \u03c0\u0109 defined in Appendix B.1. Given the empirical training error of\nthe imitation learning, i.e. dd (DARC) - inf, dd (ARC) < \u20ac, \u2200 \u03b4 \u2208 (0, 1), with probability\nat least 1 - 8, we have\nEpirg, \u03c0* [Str(st, at)] - E Epire [tr(stat)]\n< Epsre, DARC [Str(st, at) + H[at|st]] - Epsrc, Darc [Str(st, at)]\n+\n||r||D [\u00ea + inf dd(\n+\n(1) DARC Error Bound in Source\nsrc\nDARC)\n+]\n(2.1) Approximation Error\ninf dd(DARC) +\n(2.2) Estimation Error\n2R+2WR)+(6W + 1)\u2206\u221alog(4/5)/2m] .\n(2) Imitation Learning Error Bound\nRemark 4.2. Our error bound depends on (1) the DARC error bound in the source domain and (2) the imitation learning generalization error, where (2) is further decomposed into (2.1) approximation error and (2.2) estimation error. This bound demonstrates how the two important components in our proposed approach contribute to a good performance. Firstly, we would want a well-trained policy on the source to reduce (1), which can be achieved by a good policy learning algorithm and well-trained\nclassifiers for reward modification. Secondly, we utilize imitation learning from observation to transfer the experience to the source. (2.1) depends on the upper bound of the importance weight, which can be decreased with a richer policy class or when the dynamics shift becomes smaller. Additionally, a better imitation can be also achieved by increasing the complexity of the discriminator function class and the number of samples, which pushes (2.2) to be smaller."}, {"title": "4.1 Comparison with the Analysis of DARC", "content": "As we discussed in Section 3.1, the DARC algorithm [3] trains a policy DARC on the source domain via matching the distribution of trajectories generated by DARC in the source and the distribution of the optimal trajectory in the target domain. Consequently, the learned policy #DARC will be suboptimal if it is directly deployed in the target domain.\nIn the DARC analysis, it is assumed that the optimal policy for the target domain \u03c0* lies in the no exploit set defined as follows [3, Assumption 1].\nIno exploit\n{Ea~\u03c0(als) [\u03a3 DKL(Psrc(St+1|St, at)||Ptrg(St+1|St, at))] \u2264 \u20ac}. (4.1)\nHere, the no exploit set means that the experiences for any policy in this set are similar in the source and target domains. Consequently, any two policies in this no exploit set also receive similar expected rewards in the two domains, and thus the reward received by \u03c0* in the target domain is similar to"}, {"title": "5 Experiment", "content": "In this section, we conduct experiments on off-dynamics reinforcement learning settings on four OpenAI environments: HalfCheetah-v2, Ant-v2, Walker2d-v2, and Reacher-v2. We compare our method with seven baselines and demonstrate the superiority of the proposed DARAIL."}, {"title": "5.1 Experiments Setup", "content": "Dynamics Shifts: We examine our algorithm with two types of dynamics shift. 1) Broken environ- ment. Following previous work [3], we freeze the 0-index value to 0 in action: zero torque is applied to this joint, regardless of the commanded torque. Different from DARC [3], who only test their method in intact source and broken target environment, we further test our algorithm in the broken source and intact target environment, where the source has less support than the target domain. As discussed in Section 4.1, violating the \u03c0* \u2208 \u03a0no exploit assumption leads to significant performance degradation for DARC and similar methods. When the source domain is intact, this assumption is more likely to hold and DARC can achieve a near-optimal policy in the target domain. So, besides the setting in DARC, we focus on a harder problem for off-dynamics RL where DARC is prone to failure\ndue to the violation of the assumptions in Section 4.1. Further, for the Ant and Walker2d, the source environment is broken with pf = 0.8 probability, which means that with 0.8 probability, the 0-index will be set to be 0, and 0.2 probability remains the original value. More details about the broken\nenvironment will be introduced in the Appendix C.3. 2) Modify parameters of the environment. Besides the broken environment, we create dynamics shifts by modifying MuJoCo's configuration files for the target domain. Specifically, we modify one of the coefficients of {gravity, density} from\n1.0 to one of the value {0.5, 1.5}.\nBaselines: We first compare our method with DARC performance in the source and target domains. DARC Training and DARC Evaluation, defined as Epsrc, DARC [Str(st, at)] and Epug, DARC [Str(st, at)] respectively, represent DARC performance in the two domains. We compare DARAIL with DARC training performance as we mimic the DARC behavior in the source domain, which should receive a similar reward as the DARC training reward in the source domain. We compare with DARC Evaluation to show that our method mitigates the problem of DARC and outperforms DARC in the target domain. Further, we compare our method DARAIL with several baselines that we describe as follows. Importance Sampling for Reward (IS-R) re-weights the reward in the transition with Pug (St+1 St,at), and update the policy with reward Pug (St+1 stat) [18]. Importance Sam- Par(st, at) [18] re-weights the transitions in the SAC actor and Psrc(St+1|St,at)\npling for SAC Actor and Critic Loss (IS-ACL)\ncritic loss. DAIL is a reduction of DARAIL without reward augmentation. Model-based RL method\nMBPO [19] uses short model rollouts branched from real data to reduce the compounding errors of inaccurate models and decouple the model horizon from the task horizon. MATL [20] uses different modified rewards and is similar to our problem setting, except that they have access to rewards in"}, {"title": "6 Related Work", "content": "Off-dynamics RL Off-dynamics RL [3] is a specific domain adaptation [21, 22] and transfer learning problem in the RL domain [23] where the goal is to learn a policy from a source domain to adapt to a target domain where the dynamics are different. Similar to many works in off-policy evaluation (OPE) [12] in bandit and offline/off-policy RL [13, 24], an importance weight approach can be used to account for the difference between the transition dynamics with Pug (St+1 stat). However, this method can easily suffer from high variance due to the estimation bias of Psrc(St+1|St, at) [12]. Another line of method for the off-dynamics RL is through reward shaping [3, 5]. DARC [3] learns a policy from a modified reward function that accounts for the dynamics shifts through a trajectories distribution matching objective. [6] proposed an unsupervised domain adaptation method with KL regularized objective, which uses the same reward modification techniques trajectories distribution matching"}, {"title": "7 Conclusion", "content": "In this paper, we propose Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) for off-dynamics RL. We recognize the drawbacks of DARC and its following work with the same modified rewards function. We demonstrate that DARC or similar reward modification methods can only obtain a near-optimal policy in the target domain. We then propose to mimic the trajectory distribution generated by DARC in the source domain. Specifically, we propose a reward-augmented estimator for the policy optimization step in imitation learning from observation. Theoretically, we established the finite sample upper bounds of rewards for the proposed method, relaxing the restrictive assumption about the optimal policy in the previous work. Empirically, we conducted experiments on four Mujoco environments, demonstrating the superiority of our method. From the safety perspective, our method avoids directly training a policy in a high-risk environment. Our future work includes investigating off-dynamics reinforcement learning under safety constraints and more severe domain gaps in reinforcement learning."}, {"title": "A Analysis of DARC", "content": "Figure 4 shows the objective of DARC, which minimizes the reverse KL divergence of the trajectories generated by the TDARC in the source domain and \u03c0* in the target domain. Note that the optimal policy is assumed to be proportional to the exponential form of the reward, i.e. \u03c0* xexp(r(st, at)). Given this assumption, the reverse KL divergence can be re-formulated to Eq. (3.3) with modified reward. So, the DARC will not be optimal in the target domain but can generate trajectories in the source domain that resemble the optimal trajectories given the objective."}, {"title": "A.2 DARC Error Bound", "content": "Now, we show that without the assumption of \u03c0* \u2208 \u03a0no exploit in [3], the error of \"DARC cannot be trivially bounded.\nLemma A.1. If \u03c0* & Ino exploit, the error bound of the \"DARC is in the following form:\nPtrg, \u03c0* \u03a3r(st, at) +\ntEpirg, DARC \u03a3r(st, at) +\n<2Rmax 1 DKL (Prg, \u03c0* (7), Pave, \u03c0. (7)) +\n+2Rmax \u221ae/2.\nProof. In [3] Lemma B.2, they show that for any policy \u03c0\u2208 Ino exploit, the following inequality\nholds:\n\u03a3\n<2Rmax \u221a\u20ac/2, (A.1)\nWhere Rmax refers to the maximum entropy-regularized return of any trajectories. However, the\ninequality Eq. (A.1) only holds for \"DARC, not for \u03c0*. Now, we show that without the assumption\n\u03c0* \u2208 \u03a0no exploit, the error could not be bounded trivially.\nWe start with the same decomposition. Therefore, we have"}]}