{"title": "FLOWAGENT: Achieving Compliance and Flexibility for Workflow Agents", "authors": ["Yuchen Shi", "Siqi Cai", "Zihan Xu", "Yulei Qin", "Gang Li", "Hang Shao", "Jiawei Chen", "Deqing Yang", "Ke Li", "Xing Sun"], "abstract": "The integration of workflows with large language models (LLMs) enables LLM-based agents to execute predefined procedures, enhancing automation in real-world applications. Traditional rule-based methods tend to limit the inherent flexibility of LLMs, as their predefined execution paths restrict the models' action space, particularly when the unexpected, out-of-workflow (OOW) queries are encountered. Conversely, prompt-based methods allow LLMs to fully control the flow, which can lead to diminished enforcement of procedural compliance. To address these challenges, we introduce FLOWAGENT, a novel agent framework designed to maintain both compliance and flexibility. We propose the Procedure Description Language (PDL), which combines the adaptability of natural language with the precision of code to formulate workflows. Building on PDL, we develop a comprehensive framework that empowers LLMs to manage OOW queries effectively, while keeping the execution path under the supervision of a set of controllers. Additionally, we present a new evaluation methodology to rigorously assess an LLM agent's ability to handle OOW scenarios, going beyond routine flow compliance tested in existing benchmarks. Experiments on three datasets demonstrate that FLOWAGENT not only adheres to workflows but also effectively manages OOW queries, highlighting its dual strengths in compliance and flexibility.", "sections": [{"title": "1 Introduction", "content": "With the enhanced understanding and reasoning capabilities of large language models (LLMs), pretrained LLMs are increasingly being utilized in dialogue systems (He et al., 2022; Bang et al., 2023). Compared with traditional chatbots, LLMs can interact more flexibly with users to address diverse needs, leveraging the vast amount of commonsense knowledge stored in their parameters (Yi et al., 2024). However, in real-world applications, we often expect chatbots to follow specific rules and procedures to perform certain tasks (e.g., guiding users to make an appointment for appropriate hospitals, departments, and doctors (Mosig et al., 2020; He et al., 2022)). The procedures that must be followed through dialogues are known as workflows. LLMs, acting as workflow agents, assist users via conversations and invoke relevant tools to fulfill requests (Xiao et al., 2024).\nExisting research can be broadly classified into two categories: rule-based and prompt-based methods. Rule-based methods (Coze, 2024; Dify, 2024; Flowise, 2024) control the conversation between the agent and the user through deterministic programs, modeling the progress of dialogue as state transitions within a graph composed of nodes representing different dialogue states, as shown in the upper part of Fig. 1(a). In this approach, the LLM functions as a node within the graph and cannot control the entire conversation flow. As a result, this method provides high compliance but often at the expense of the LLM's inherent flexibility. As illustrated in the lower part of Fig. 1(a), introducing a new flexible feature within this system (e.g., allowing users to pause an appointment booking process to inquire about a condition before resuming) requires the addition of numerous transition egeds (dashed lines), significantly increasing complexity. In contrast, prompt-based methods leverage LLMs to autonomously manage dialogue by representing workflows textually (natural language, code or other structured data, Fig. 1(b)). While this method imparts soft control over LLM responses (workflow as part of prompt), LLMs' probabilistic nature often leads to compliance issues, like hallucinating incorrect information, which can have serious repercussions (e.g., notifying a user about a successful appointment booking when it hasn't occurred) (Zhang et al., 2023)."}, {"title": "2 Related Work", "content": "2.1 LLM-Driven Conversational Systems\nThe evolution of task-oriented dialogue (TOD) systems has transitioned from modular pipelines (Yi et al., 2024) to end-to-end LLM paradigms. While traditional systems suffered from error propagation across NLU, DST, and NLG modules (He et al., 2022; Su et al., 2021), modern approaches leverage LLMs for holistic dialogue management via workflow-guided interactions (Xiao et al., 2024; Wallace et al., 2024). This shift necessitates new evaluation metrics focusing on task success rates over modular accuracy (Arcadinho et al., 2024), motivating our framework's dual focus on procedural compliance and adaptive flexibility.\n2.2 Agentic Workflow Architectures\nThe progression of LLMs has led to the development of LLM-based agents across various domains (Park et al., 2023; Tang et al., 2023; Qian et al., 2023). LLM-based agents enhance task execution through tool usage and dynamic planning (Yao et al., 2022; Schick et al., 2023; Wang et al., 2023; Zhu et al., 2024). We distinguish two paradigms: 1) Workflow generation creates procedures via LLM reasoning (Li et al., 2024; Xu et al., 2024; Liu et al., 2023; Chen et al., 2023; Valmeekam et al., 2022), and 2) Workflow execution operates within predefined structures (Xiao et al., 2024; Qiao et al., 2024).\nOur research primarily focuses on the latter paradigm, treating workflows as predefined knowledge to build robust, user-centric agents. Within this context, two main approaches are adopted to integrate structured workflows with linear-textprocessing language models: 1) Rule-based Approach: This method involves hard-coding workflow transition rules as fixed logic, defining the current node and state transitions explicitly in the program. 2) Prompt-based Approach: Here, workflows are represented in flexible formats such as natural language, code (or pseudocode), or flowchart syntax (Xiao et al., 2024; Zhu et al., 2024). Each method presents unique challenges: rule-based systems often lack flexibility, while prompt-based methods might deviate from intended procedures. Our solution aims to strike a balance between process control and adaptability, ensuring workflows are both structured and responsive to dynamic interactions."}, {"title": "3 Preliminary and Background", "content": "3.1 Workflow\nA workflow defines a structured process designed to accomplish a specific task or goal within a particular scenario. For instance, in a hospital appointment booking scenario, a typical workflow involves steps such as querying the user for their preferred hospital, department, and time, retrieving available appointment slots using relevant tools, confirming the details with the user, and completing the booking. Formally, we can represent a workflow as a directed acyclic graph (DAG) denoted by $G(V, \u03b5)$ (Qiao et al., 2024; Zhang et al., 2024), where V represents the set of nodes, each corresponding to an atomic operation (e.g., querying the user, invoking an API, retrieving from a knowledge base), and E represents the directed edges that define the temporal and dependency relationships between these operations, effectively specifying the workflow's progression.\n3.2 Workflow Agent\nA workflow agent is designed to assist users in completing tasks by interacting with them and utilizing available tools. It can be conceptualized as an agent making sequential decisions within an environment composed of the user and the available tools. This interaction can be modeled as a Markov Decision Process (MDP), which provides a valuable framework for understanding the agent's decision-making process over time. In this framework, the agent transitions through a sequence of states (s), takes actions (a) based on the current state, and receives feedback (r) from the environment (user responses or tool-generated outputs). This process can be represented as {(so, ao, ro), (s1, a1, r1),..., (St\u22121, at-1, rt-1)}. Consequently, the decision-making process of the workflow agent can be expressed as:\n$a_t \u2190 A(H_{t-1}, G),$                                                                                                (1)\nwhere $H_{t-1}$ encompasses all actions and observations up to time t\u22121, and G serves as the guide for the agent's actions.\nBased on how the workflow is represented and integrated into the agent's decision-making process, workflow agents can be broadly classified into two categories: rule-based agents and prompt-based agents. Rule-based agents rely on explicitly programmed procedures to guide the workflow, while prompt-based agents utilize a single language model to autonomously manage the entire decision-making and dialogue process."}, {"title": "4 Methodology", "content": "In this work, we introduce a novel procedural description language (PDL) designed to represent workflows, alongside FLOWAGENT, an execution framework that enhances the agent's behavioral control.\n4.1 PDL Syntax\nPDL consists of three primary components: 1) Meta Information: Basic workflow details such as name and description. 2) Node Definitions: Resources accessible to the agent, which includeAPI nodes (for external tool calls) and ANSWER nodes (for user interaction). 2) Procedure Description: The procedural logic of the task, expressed in a mix of natural language and pseudocode.\nFor illustration, in the Hospital Appointment workflow, Fig.3 presents a segment of the node definitions \u00b9. Fig.4 illustrates a portion of the procedure description. Key features of PDL include: 1) Precondition Specification: Nodes include a preconditions attribute, defining dependencies between nodes. For example, check_department requires check_hospital as a prerequisite, ensuring hospital selection before department inquiry. 2) Hybrid Representation: The integration of natural language and code in the procedure description ensures a concise and yet flexible workflow representation, maintaining the clarity of NL with the accuracy of code.\n4.2 FLOWAGENT Architecture\nTo enhance the compliance of workflow agents, we introduce FLOWAGENT, an execution framework tightly integrated with PDL. FLOWAGENT enforces a set of controllers that govern the agent's decision-making process, thereby promoting reliable action execution without sacrificing the LLM's autonomy.Algorithm 1 outlines FLOWAGENT's overall execution. Each round begins with a user query (line 3), which the agent interprets to produce a response or a tool call (line 18), ultimately generating a user-facing response (line 21).\nTo ensure decision-making stability, FLOWAGENT incorporates two categories of controllers: pre-decision controllers ($C_{pre} = \\{c_{i}^{pre}\\}$) and post-decision controllers ($C_{post} = \\{c_{j}^{post}\\}$). Pre-decision controllers proactively guide the agent's actions by evaluating the current state and providing feedback to the LLM (e.g., identifying unreachable nodes based on the dependency graph $G^{(pdl)}$)."}, {"title": "5 Evaluation and Data", "content": "5.1 Compliance Evaluation\nWe follow previous studies (Xiao et al., 2024; Chen et al., 2023) to conduct both turn-level and session-level assessments. In turn-level evaluation, there is a reference session (considered as ground truth) (Dai et al., 2022). For each turn in the reference session, the evaluation system provides the prefix of the session $H_{t-1}$ to the bot for predicting the current $\u00e2_t$. The judge compares $\u00e2_t$ with $a_t$ to determine if the bot's response for that turn is correct, and the average result across all turns yields the Pass Rate. To assess the agent's tool usage capability, for turns involving tool callings, we evaluate the tool selection and parameter infilling performance of the agent in Precision, Recall, and F1-score.\nFor session-level evaluation, we simulate user interactions with the bot using an LLM, which serves to mimic real user behavior while minimizing human assessment costs. To ensure these simulated sessions accurately reflect real-world complexity, we define detailed user profiles comprising: (1) demographic information; (2) conversational style, capturing behavioral patterns; and (3) workflow-related user needs, detailing primary and secondary session objectives. An illustrative user profile is provided in App. A.2. For each generated session, we conduct a binary assessment to verify whether the user's primary workflow objectives are achieved, yielding the Success Rate. Additionally, by tracking the number of sub-tasks initiated and completed, we derive the Task Progress metric. Sessions are evaluated end-to-end using prompts consistent with those recommended by Xiao et al. (2024). Furthermore, we evaluate the LLM agent's performance in tool invocation with Precision, Recall, and Fl-score metrics.\n5.2 Flexibility Evaluation\nPrevious work (Zhong et al., 2018; Wu et al., 2019; Li et al., 2024) has primarily focused on evaluating whether bots can follow a specific procedure to complete a conversation, which partially emphasizes compliance while neglecting flexibility in handling user requests. Such incomprehensive evaluation may not reflect the capabilities of LLM agents under real-world scenarios, where an \u201cimperfect\u201d user might not adhere to the procedure and violates the sequential steps during multiple rounds of interactions. Consequently, to evaluate the performance of workflow agents in OOW scenarios, we have additionally developed a targeted evaluation method to assess flexibility.\nSpecifically, we categorize OOW scenarios into three types: (1) intent switching, where the user suddenly changes the original intent requests or requirements, including modification of API slots/-parameters and demand for cancellations; (2) procedure jumping, where the user does not follow the established workflow sequence to provide information and express confirmation, including skipping steps or jumping back; and (3) irrelevant answering, where the user deliberately avoids direct reply to questions raised by the agent, such as answers with topic shifts and rhetorical questions;\nBased on these classifications, flexibility can be evaluated by examining the agent's performance in OOW scenarios using the metrics introduced in Sec. 5.1. At the turn-level, we insert OOW user interventions to assess the agent's immediate adaptive responses in these specific interactions. At the session-level, we assess the agent's overall performance in sessions that include OOW queries to measure its long-term flexibility.\n5.3 Data\nWe constructed three test datasets based on existing datasets and business-related data: SGD (Rastogi et al., 2019), STAR (Mosig et al., 2020), and In-house. The data construction process is detailed in App. D.1. Statistics for these datasets are shown in Tab. 1, and differences from datasets used in other studies are highlighted in Tab. 2."}, {"title": "6 Experiments", "content": "We raise the following research questions:\nQ1: Compared with other models, does our proposed FLOWAGENT show improvements in compliance and flexibility?\nQ2: In which way the proposed controllers exert constraints on the model to facilitate workflows with both compliance and flexibility?\n6.1 Experimental Setup\nBaselines We selected ReAct (Yao et al., 2022) as a baseline method for comparison, which makes decisions in each round by utilizing a combination of thought and action, and treats the feedback from environment an observation. It belongs to the category of prompt-based methods introduced in Sec. 3.2. For representing the workflow, we chose three formats: natural language (NL), code, and FlowChart, denoted as ReActNL, ReActcode, and ReActFc, respectively. To ensure a fair comparison, we reused the prompts from FlowBench (Xiao et al., 2024) in our experiments.\nImplementation In session-level evaluation, GPT-4o-mini is used for user simulation. For the bot, we initially tested two representative model series, the GPT series (Achiam et al., 2023) and the Qwen series (Yang et al., 2024). Preliminary studies revealed that small models are not competent for complex workflow tasks. Therefore, in the present study, we choose GPT-4o and Qwen2-72B for demonstrations. During the evaluation process, we used GPT-4-Turbo for judgment. More implementation details can be seen in App. C.1.\n6.2 Session-level Experimental Results\nA1.1: FLOWAGENT outperforms the other three baselines in terms of task compliance. We first compare the session-level performance of different methods in Tab. 3. The results indicate that FLOWAGENT outperforms the other three baselines in terms of task completion metrics Success Rate, Task Progress, and tool usage metrics like Tool F1.\nA1.2: FLOWAGENT exhibits robustness towards OOW interventions with higher flexibility. Tab. 4 presents the performance of different methods under OOW scenarios. A general performance decline is observed across all models on the three datasets. However, FLOWAGENT exhibits only a slight decline, achieving the best results across all datasets. Fig. 5(a) visualizes the Task Progress metric under different settings, highlighting FLOWAGENT's advantage in OOW scenarios, demonstrating strong flexibility.\n6.3 Turn-level Experimental Results\nA1.3: FLOWAGENT maintains the superior compliance and flexibility across datasets in turn-level evaluation. We present the turn-level experimental results of Qwen2-72B in Tab. 6. The results show that the FLOWAGENT framework achieves the best performance in both IW and OOW settings. What's more, Fig. 5(b) compares the Success Rate across different models and settings.\n6.4 Ablation Studies\nA2: Controllers play an indispensable role in enforcing steady progress of workflows with OOW interventions. We conducted ablation experiments on FLOWAGENT in OOW settings, with the results shown in Tab. 5. In the table, \u201c-post\u201d indicates the removal of the post-decision controllers Cpost from the complete model, while \u201c-post-pre\u201d further removes the pre-decision controllers Cpre. According to the experimental results, it is evident that removing either controller negatively impacts model performance, validating that controllers in FLOWAGENT enhance the model's compliance."}, {"title": "7 Conclusion", "content": "In this paper, we reviewed existing LLM-based workflow methods and compared their strengths and weaknesses in terms of compliance and flexibility. Aiming to enhance the compliance capability of LLMs without significantly compromising their flexibility, we proposed the PDL syntax to express workflows and used the FLOWAGENT framework to control agent behavior. For evaluating compliance and flexibility capabilities, we constructed datasets based on existing data and designed specific evaluation methods. Experiments on three datasets demonstrated that FLOWAGENT not only possesses strong compliance capabilities but also exhibits robust flexibility when handling out-of-workflow queries."}, {"title": "8 Limitations", "content": "We acknowledges two primary limitations:\nWorkflow Generation Our current research emphasizes enhancing LLM performance within manually constructed workflows using the PDL syntax. Consequently, the evaluation is limited to these artificially defined settings, lacking exploration of automated workflow generation (Qiao et al., 2024; Zhang et al., 2024). Future work should investigate dynamic workflow synthesis to adapt to varying and complex user demands without manual intervention.\nDialogue Diversity and Evaluation While this study evaluates agent performance in OOW scenarios using simulated user interactions, the real-world applicability relies on testing across a broader spectrum of authentic user demands."}, {"title": "A Dataset Examples", "content": "A.1 PDL Example\nBelow is a PDL example in a real-world scenario.For formats of natural language, code and flowchat,see Xiao et al. (2024)."}]}