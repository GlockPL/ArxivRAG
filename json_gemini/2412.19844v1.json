{"title": "Review: Latent representation models in neuroimaging", "authors": ["C. V\u00e1zquez-Garc\u00eda", "F. J. Martinez-Murcia", "F. Segovia Rom\u00e1n", "Juan M. G\u00f3rriz"], "abstract": "Neuroimaging data, particularly from techniques like MRI or PET, offer rich but complex in-formation about brain structure and activity. To manage this complexity, latent representation models such as Autoencoders, Generative Adversarial Networks (GANs), and Latent Diffusion Models (LDMs)- are increasingly applied. These models are designed to reduce high-dimensional neuroimaging data to lower-dimensional latent spaces, where key patterns and variations related to brain function can be identified. By modelling these latent spaces, researchers hope to gain insights into the biology and function of the brain, including how its structure changes with age or disease, or how it encodes sensory information, predicts and adapts to new inputs. This review discusses how these models are used for clinical applications, like disease diagnosis and progression monitoring, but also for exploring fundamental brain mechanisms such as active inference and predictive coding. These approaches provide a powerful tool for both understanding and simulating the brain's com-plex computational tasks, potentially advancing our knowledge of cognition, perception, and neural disorders.", "sections": [{"title": "Introduction", "content": "Neurodegenerative diseases (NDDs), such as Alzheimer (AD) or Parkinson (PD) are amongst the most prevalent in the world, affecting over 29.8 million and 6.2 million people respectively. In the last years, the prevalence of these diseases has escalated drastically, with more than a twofold increase from 1990 to 2015. This increase has led to a decline in quality of life as the average life expectancy gets larger. In addition, there is a large spectrum of neurological disorders that affect millions of people all over the world across all ages, such as Schizophrenia (SZ), ADHD, Autism or Brain tumors, hindering their daily lives. These diseases do not only have a severe effect on the healthcare system, but they also have an impact on the economy. Therefore, the need for effective clinical therapies is becoming increasingly urgent.\nCurrently, most therapeutic approaches prioritize improving patients' quality of life and alleviating symptoms, rather than directly addressing the underlying causes of the disease. Although these treat-ments may offer temporary relief for certain individuals, their efficacy is often case-dependent and tends to diminish over time. Moreover, they are unable to halt or reverse disease progression, as clinical trials generally begin many years after the neurochemical processes driving the condition have already been set in motion.\nResearch in neuroscience has shown that neuropathological processes of NDDs such as AD or PD begin years, even decades, before the first symptoms appear. This presymptomatic stage of the diseases opens a window of opportunity to implement preventive therapies and treatments. In this context, Disease-Modifying Therapies (DMTs) have emerged as a basis for developing new therapeutic techniques. These therapies aim to modify the pathological processes underlying neurological diseases by targeting the mechanisms that drive their progression. Without such a comprehension, the identification of the right mechanisms to target may be misinterpreted. For instance, most pharmacological trials for AD have been developed to decrease the levels of Amyloid-beta (AB) aggregates and plaques by using drugs. Nevertheless, these trials have not been successful in retrieving the phenotypes of the disease, compelling many researchers and funding bodies to shift the focus to another potential causes. On the other hand, non-degenerative diseases, such as neurological disorders, even though they might not be as dramatic as NDDs, they also hinder severely the lives of the patients in most cases. Alike NDDs, the mechanisms underlying these disorders are still not well understood. Moreover, the vast heterogeneity of the spectrum of these disorders greatly difficult the progress in research."}, {"title": "An opportunity: latent representations", "content": "Beyond clinical practice, there is a growing interest in understanding the fundamental mechanisms of brain function. Research has revealed that the brain operates as a higly dynamic system capable of encoding, predicting, and adapting to environmental inputs. These processes, including sensory encoding, predictive coding, and active inference, reflect the brain's ability to generate internal representations that optimize perception and behavior. However, unraveling these mechanisms remains a challenge due to the high-dimensional nature of neuroimaging data, which captures both brain structure and function but is difficult to interpret effectively.\nIn this context, research has focused on designing techniques that allow to interpret and capture relevant insights from these non-invasive neuroimaging techniques. In the last years a variety of models and techniques have been developed to analyze clinical data, among them latente generative models have emerged as powerful tools for addressing these challenges. This review aims to provide a comprehensive overview of how latent generative models are applied in neuroimaging. We explore their role in clinical tasks, such as disease diagnosis, progression monitoring, and harmonization of multi-site data, as well as their contributions to fundamental neuroscience, including the study of brain networks, cognitive pro-cesses, and sensory representations. By bridging clinical and theoretical research, these models offer new opportunities to advance out understanding of brain function and dysfunction, ultimately paving the way for improved diagnosis, therapies, and insights into the brain's remarkable computational capabilities.\nMedical images are commonplace in hospitals and laboratories worldwide. In neuroimaging, these im-ages are extensively used for their non-invasive nature and the valuable insights they provide into the structure and functionality of the brain. In the context of neurodegenerative diseases (NDDs) and neu-rological disorders, these neuroimaging techniques play a critical role in monitoring disease progression and assisting clinical decisions. Despite their utility, neuroimaging data can be highly complex, requiring both technological expertise and an understanding of the diseases being studied. Moreover, the high dimensionality of these data, when combined with other biomarkers, presents significant challenges for modeling and interpretation.\nOne promising approach to tackling these challenges comes from the manifold hypothesis, which suggests that real high-dimensional data tend to lie on or near a lower-dimensional manifold. This means that even though the data may have a large number of dimensions, much of it is constrained to a more compact, lower-dimensional structure. This hypothesis, first explored in mathematics and later applied to various fields like psychology and neuroscience, proposes that the high-dimensional data we collect, including neuroimaging data, can be better understood by uncovering this underlying manifold structure."}, {"title": "Latent Generative Models", "content": "Deep learning models have proven highly effective for deriving latent representations of complex, high-dimensional, non-linear data in neuroimaging, as highlighted in the previous section. This is typically achieved through the use of latent generative models, which are designed to learn compact representations of input data while simultaneously generating new samples that adhere to the true underlying data distribution. These models excel at capturing the essential features and variations within the data, enabling a robust characterization of its distribution. In this section, we delve into the foundational principles underlying the most widely employed latent generative models, setting the stage for subsequent sections where their applications in neuroimaging will be examined in detail."}, {"title": "The (Variational) Autoencoder", "content": "An Autoencoder (AE) is a neural network that learns to encode input data into a lower-dimensional latent space and then reconstruct it back to its original form. It is commonly used for dimensionality reduction, feature extraction, and denoising. The AE consists of two main components: an encoder, which compresses the input data into the latent representation, and a decoder, which reconstructs the data from this compressed representation.\nWhile AEs are effective at capturing key features of the data, their latent representation is learned directly from the data without additional constraints. This means that the structure of the latent space may not be well-organized or interpretable, and it might not effectively capture the variability within the dataset. For example, in the MNIST dataset of handwritten digits, each digit has unique variations depending on the writer's style (e.g., different stroke thicknesses or curvatures for the digit '2'). An AE might learn a representation for each sample that captures its unique details, but these representations might not generalize well to unseen variations.\nSimilarly, when applied to datasets like magnetic resonance images (MRI) of healthy subjects and subjects with neurodegenerative diseases (NDDs), an AE might capture general brain patterns but fail to account for inter-subject variability. For instance, two patients with Alzheimer's disease (AD) might exhibit different patterns of brain degeneration, one might show severe hippocampal atrophy, while"}, {"title": "Latent Diffusion Models", "content": "Latent Diffusion Models (LDMs) are latent generative models that focus on transforming the input data into noise through a diffusion process and learn how to revert that process to generate high quality samples. The main idea of this process is to map the complex data into a latent representation, where the diffusion process is more efficient and easy to handle, in order to learn how to generate new synthetic data in this latent representation. The LDM is divided in three main stages: i) encoding of the input data into a latent representation, which is usually performed with an encoding model, such as a AE, ii) forward noisy diffusion, and iii) reverse process. During the forward diffusion, random gaussian noise is added to the input data. This is performed via a Markov process, meaning that each diffusion step depends only on the immediate previous step and not in the history of the process. At each step, a small amount of noise is added to the entire latent representation, such that the process is described by:\n$q(z_t|z_{t-1}) = N(z_t; \\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI).$\nThis distribution describes how the diffusion step $z_t$ is produced by the previous step $z_{t-1}$. This can be translated as, given $z_{t-1}$, $z_t$ is distributed as a normal distribution with mean $\\sqrt{1 - \\beta_t}z_{t-1}$ and variance $\\beta_tI$, where the term $\\sqrt{1 - \\beta_t}$ controls how much of $z_{t-1}$ is conserved and $\\beta_t$ controls how much noise is added. The entire diffusion process is simply the product of each diffusion step.\nOnce the data has turned into noise, the objective of the LDM is to train a generative model that learns how to reverse this process and retrieve the original representation $z_o$ from the final noisy step $z_t$. To do that, a UNet or transformer is commonly trained to estimate the noise $\\epsilon_\\theta(z_t, t)$. To do so, the goal is to minimize the difference between estimated noise and real noise added to the input:\n$L(\\theta) = E_{z_0,t} [||\\epsilon_\\theta(z_t, t) - \\epsilon(z_t)||^2],$\nwhere $\\epsilon(z_t)$ is the real noise at step $t$ and $\\epsilon_\\theta(z_t,t)$ is the prediction of the model. Once the denoising process is finished we can reconstruct the synthetized neuroimage by using the decoder of the model we used to encode the images into the latent representation as $\\hat{x} = Decoder(z_0)$. As we will see this model is commonly used in neuroimaging to produce high quality images from latent representations.\nAs we have discussed before, VAEs tend to reconstruct blurry images due to their nature, in order to obtain meaninful representations. If we combine a VAE model with a LDM we can exploit the rich representations learnt by the VAE and still generate high quality images. Notice that, unlike the VAE, LDMs do not learn how to capture relevant underlying information about neurological processes hidden in neuroimages or how to model their distributions. Instead, they need a previous step to obtain representations and then they make use of that information to generate new data. In that sense, LDMs can be thought as a tool to handle the rich information of the latent space, more than a method to extract them."}, {"title": "Generative Adversarial Networks", "content": "GANs were introduced by Goodfellow in 2014 [28] as DNN based on two counterparts: a generator G and a discriminator D. The GAN can be thought as a competition between a generative model that is learning how to generate synthetic data that resembles the input, and discriminator that is trying to guess whether the received image is real o generated. The generator G takes a noise vector z from a latent space and maps it into a data space of neuroimages, while the discriminator takes a sample x which can be real or generated, and computes the probability of x being real. Hence, it is formalised a a minmax problem, where the loss function is:\n$\\underset{G}{min} \\underset{D}{max} V (D, G) = E_{x~p_{data}(x)} [log D(x)] + E_{z p_z(x)} [log(1 - D(G(z)))].$\nThe goal is to learn the generator distribution $p_g$ over the data x. We can think of the model as a child who is trying to learn how to draw. The generator is the child and his task is to make the drawings look real. The discriminator is a teacher who is trying to guess whether the drawing is real or not (generated). The generator is trying to fool the discriminator and the discriminator is trying to improve in order not to be fooled. If we take a look at the loss function (4) what we see is that the generator wants to minimize V(D,G) and the discriminator is trying to maximize it. The first term of that equation assures that the discriminator classifies the real samples as real, while the second term assures that the generator produces better images."}, {"title": "State-of-the-art", "content": "In this section we will present the most relevant lines of research in obtainment, exploitation, and analysis of latent representations through latent generative models in neuroimaging. Here, we will explore how researches have been using the bayesian inference framework as a mean for understanding the internal cognitive processes and neural connections in the brain. Additionally, we will also explore how these latent representations are used to create models that aim to aid typical clinical practice problems such as the inter-center noise due to the different use of scans and methodologies of each institution, as well as the translation of low resolution neuroimages into higher resolutions, like 7T MRI. Moreover, we will see how these latent representations are able to capture longitudinal patterns of the brain, allowing us"}, {"title": "The Brain as an inference machine", "content": "As we have discussed, generative models are bioinspired by the encoding and decoding mechanisms of the brain. However, these models are not just powerful tools to analyze or generate data, but they also give us an opportunity to understand the most abstract and fundamental processes of the brain.\nTraditionally, it was thought that the brain receives stimuli from its surroundings and processes them to form perceptions. However, we now know from psychology that perception is not a passive process. Instead, the brain actively generates inferences about the environment based on prior knowledge and expectations. This active inference means that the brain not only perceives but also predicts what is going to happen next, using previous knowledge and past experiences to predict future outcomes. This actually resembles the usual Bayesian Inference that we are familiar with, where we could consider that the brain is producing constant beliefs about the world and updating those beliefs according to the sensory information following the Bayes' rule:\n$p(z|x) = \\frac{p(x|z)p(z)}{p(x)},$\nwhere p(z) is the belief and p(x|z) is the likelihood that the observation x fits the belief z. The brain is, without a doubt, a physical object, and thus there must exist a mathematical formalism to describe it. While this formalism may be highly complex, Bayesian inference, though not the ultimate answer, undoubtedly brings us a step closer to understanding how neural encoding, perception, and cognition operate. In this context, several researchers have leveraged the powerful Bayesian framework to gain deeper insights into how inference and representations are generated within the brain.\nFor instance, authors in [24] discuss the relationship between generative models, brain function and neuroimaging. They focus on how functional specialization of the brain depends on the integration between population of neurons. They use generative models to explain the paper of feedback connections among neurons allowing cortical regions to reconfigure dynamically depending on the context. The authors explore the idea that forward and backward connections in deep learning models are very similar to the bottom-up and top-down connections of neurons. Bottom-up connections refer to the flow of information from lower levels (sensory areas that receive stimuli) to higher levels (areas that process more complex information). This represents the data-driven pathway, where stimuli are progressively transformed into perceptions. In top-down the information flows from higher to lower areas, in which case, higher areas of the brain generate predictions about what they expect to receive and send these predictions to lower areas. These predictions act as a guide or context for a better interpretation of the received sensory information. If the prediction does not coincide with the input information the neuron population would produce an error that error is used to adjust the predictions in the higher areas until these predictions coincide with the perceived reality. The error disappears once there is a consensus between prediction and reality.\nThe main idea of the authors is that processing within the brain does not only take into account information coming from stimuli (bottom-up) but also from the predictions created by the active inference of the brain. Moreover, this means that what we perceive is not only a product of the stimuli but it is also influenced by the constant context and prediction generated by the brain based on the previous experiences and environment. The authors in this paper argue that the brain can be conceptualized as a system that is constantly generating predictions about the external world, adjusting these predictions according to the feedback provided by the external stimuli. These generative models allow the brain to construct an internal representation of reality that helps to optimize its prediction ability and reaction to stimuli.\nHere, they also propose that this theory can be experimentally validated using neuroimaging tech-niques. First, they consider a task consisting of saying \"yes\" when the subjects see a recognisable object, and \"yes\" when they see an unrecognisable non-object. The idea is to see how recognisable objects are perceived in the brain by specific areas related to visual object recognition, while abstract, unrecognisable"}, {"title": "Image-to-Image translation & Harmonization", "content": "One open problem in the field of neuroscience analysis is that every research center, institution or collaboration, aiming to gather subject data in many modalities (ranging from neuroimaging scans such as MRI-T1, MRI-T2, PET, CT, etc. to other clinical data such as A\u00df amyloid, tau or even genetics), use their own measures and techniques to register such information. This means that each project chooses which biomarkers are going to be measured and the specifics of the scans used to obtain images. Even amongst a certain project there are usually differences in the scans used. This produces a tremendous problem in the data analysis field, since it hinders the possibility of validation. Any model that aims to produce insights about a certain disease must be applicable to any database in order to validate its results. However, in the field of neuroimaging, there are certain patterns that differentiate images that are taken using different methodologies, which is translated as inter-center noise. For example, if we try to train a VAE using images from two different databases, this inter-center noise will severely affect the coding of our latent representation, which will be noisy too, as a consequence. In order to address this issue, researchers have proposed different methods and techniques using generative models to isolate this inter-center noise and eliminate it from the images in order to obtain a harmonized dataset from two or"}, {"title": "Visual reconstruction using fMRI", "content": "Image reconstruction is a topic that has gathered significant attention in recent years. In computer vision we receive an image (e.g. a natural image) and try to either reconstruct that image (for image synthesis) by learning its distribution, or identify different elements within the image (for segmentation or parcelling tasks). In neuroimaging, researchers haven been trying to unravel the neural encoding of visual information by characterizing how this information X is codified by the brain into latent representations Z. In general, the idea is to find the distribution p(X|Z) that leads from visual input to neural representations that are later used to generate cognitive processes and responses. To do so, researchers use the information from fMRI images to see how different parts of the brain react to visual input. These activation maps contain relevant information of the latent representations, which can be analysed to understand the encoding process.\nFor instance, authors in [63] use a GAN approach model to reconstruct natural images from fMRI. Here, they use a codifying model based on a feature-weighted receptive field (fwRF) model, described in previous work [62]. This model predicts brain activity in response to visual stimuli, generating an activity vector V from an image X, allowing prediction of fMRI responses to new images. An AE is applied to reduce dimensionality of V, generating a latent code C that captures the relevant features of the brain activity. A conditional GAN is trained using the latent representation. The GAN is trained to generate images that are consistent to such code C.\nThey found that the model accurately predicts brain activity in response to natural images, and recovers well-known patterns of the brain in visual processing. They also found that reconstructed images were not easily recognizable by eye, but relevant features such as dominant lines were preserved by the model.\nAuthors in [61] propose a method to reconstruct natural images from brain activity measured through fMRI scans using a GAN-based approach. During the experiments, subjects are presented with natural images from a dataset while their brain activity is recorded. A GAN is first trained on the dataset of natural images to learn a latent space capable of generating realistic reconstructions. Simultaneously, a predictive model is trained to map the recorded fMRI signals to the GAN's latent space. To achieve this, fMRI data are used as input to the predictive model, while the corresponding latent space coordinates of the GAN serve as the target labels.\nThis setup allows the predictive model to learn the relationship between brain responses and the latent space coordinates of the GAN. Once trained, the system is tested by presenting subjects with novel images not included in the original dataset. Their fMRI responses are processed by the predictive model to generate latent space coordinates, which are then fed to the GAN. The GAN subsequently reconstructs an approximation of the presented image based on the derived latent space representation. The main idea is that the fMRI responses are linearly related to the latent representations. To validate their model, authors use three different datasets: BRAINS, that contains hand-written numbers, vim-1 with gray-scale natural images, and Generic Object Decoding, with gray-scale images of objects. They found that general structured of the images was reconstructed by the model but the quality depended on the dataset. For BRAINS dataset the reconstructed images were recognised by the subjects in 54% of the cases, while in vim-1 and Generic Object Decoding was a 66.4% and 66.2% respectively.\nAuthors in [31] propose a very similar model but with a VAE approach to reconstruct natural images from fMRI responses from subjects passively watching natural videos. The VAE is used to extract latent"}, {"title": "Disease classification", "content": "Generative latent models, particularly Variational Autoencoders (VAEs) and their variants, have become pivotal in the field of neuroimaging for disease classification. These models are uniquely suited to extract structured, low-dimensional latent representations from high-dimensional neuroimaging data, capturing complex patterns while preserving critical information about disease-related variations.\nThe latent representations obtained from generative models offer several advantages: they provide a compact and interpretable feature space for classification tasks, enable the exploration of disease progression trajectories, and allow the integration of uncertainty into predictions. This section highlights recent advancements in applying generative latent models to neuroimaging for classifying neurological and psychiatric diseases, focusing on the extraction and utilization of latent spaces to enhance both predictive performance and interpretability. By leveraging these latent spaces, researchers not only improve classification metrics but also gain insights into the neurobiological underpinnings of disorders, paving the way for more personalized and precise diagnostics.\nAuthors in [4] propose a VAE+MLP model to predict the status (Normal or Alzheimer) of a patient 6 months after a visit, using MRI. The VAE extract relevant features of the MRI which they use to see which brain areas contribute the most to the model and quantify the distribution of possible trajectories of the subject. To predict the status label they use a generative classifier that models the joint distribution of features and labels p(X, Y).\nThe latent representations of the VAE are fed to the MLP to predict the status, using a cross-entropy loss along with the usual ELBO of the VAE. Using the VAE approach the authors obtain an accuracy of 74.40\u00b10.01 and a F1-score of 0.66. Additionally, they perform a risk analysis. For each MRI in the test set, they take 100 samples from the latent space and predict the future disease status. They find that 59% of the samples are not at risk, while the rest show varying degrees of risk. In contrast, the other models used for comparison (CNN and CNN-AE) predict that only 2.79% and 9.16% of the samples are not at risk, suggesting that the VAE approach provides a more robust prediction of risk compared to the other models. Qualitatively, using relevance maps, the authors found that the model focuses on anatomical specific areas for the prediction, such as the cerebellum, the neocortex and the brainstem, which are associated with the progression of Alzheimer's disease. They claim that their study predicts future progression, while other studies focus on correlating changes with current symptoms. However, they note that there are limitations in terms of which specific features the model is using within this regions, suggesting that further research is needed to understand the model's behaviour.\nOn the other hand, authors in [78] use a multi-modal fusion of different data modalities, such as MRI or PET, using ADNI scans, to classify dementia patients. They aim to create a model that is both useful in clinical setting and interpretable. To perform the multi-modal learning the authors use a Negative Matrix Factorization (NMF) which is a technique that decomposes a data matrix into simpler components, with the property of each matrix being non-negative. If we have a matrix X of dimensions dxn where n is the number of samples and d the number of features, the method targets to decompose X into two matrices B, containing the fundamental features and H that indicates how to reconstruct the data. The idea is that the product B\u00b7\u0397 resembles X. In the multi-modal case we have B(v) and \u0397(v), one pair of matrices for each independent modality. However, using the model\n$\\underset{V}{min} \\sum_{v=1} ||X^{(v)} - B^{(v)}H^{(v)}||_-,$\nthere would be no relation between different modalities. In order to address this issue, the authors use a common H matrix for all modalities. In order to obtain deep latent representations, they use the deep NMF model, which consists on decomposing each matrix into lower representations:\n$X^{(v)} \\approx B_1^{(v)} ... B_L^{(v)} H_L.$\nUsing these latent representations the authors classify Alzheimer patients in the ADNI dataset into three categories: Normal Control (CN), Mild Cognitive Impairment (MCI) and AD, utilizing MRI and PET scans. They compare their model to other state-of-the-art classification models, using accuracy, sensitivity, specifity and F-score as performance metrics. The found that the proposed model outperforms other methods such as SVM, MKL, shallow NMF, etc. in all metric values on all classification taks. They also found that all methods using multi-modality data outperforms all models using a single modality, demonstrating its robustness. In addition, they perform a pMCI vs SMCI classification task, showing that the model shows the highest accuracy values compared to other state-of-the-art methods."}, {"title": "Functional brain networks", "content": "Functional brain networks (FBNs) play a critical role in understanding neural connectivity and activity. Several recent studies have leveraged deep learning techniques to uncover these networks from fMRI data. For instance, the work in [55] introduces a deep variational autoencoder (DVAE) to derive latent representations from 4D fMRI data signals (3D volumes + temporal axis) associated with Functional Brain Networks (FBNs). Using the latent space, they apply Lasso regression between the original fMRI data (converted to a 2D matrix by concatenating time and voxel dimensions) and the latent variables. This generates a coefficient matrix that is mapped back to the 3D brain space. Each row of the matrix is transformed into a 3D image representing an FBN, effectively acting as a filter to identify the voxels where each latent variable is most relevant for functional activity. This approach links latent variables to specific FBNs.\nTemporal series are then extracted from regions of interest (ROIs) defined by these FBNs, and Pear-son's correlation coefficients between ROIs are computed to construct functional connectivity matrices for each subject. These matrices serve as input for supervised classifiers, including linear and radial SVMs, Random Forest, and a custom-designed deep neural network (DNN).\nTo validate the DVAE model, the authors compare it with Sparse Dictionary Learning (SDL) and a standard autoencoder (AE). The results show that DVAE outperforms AE in deriving meaningful FBNs, particularly in small datasets, as it avoids overfitting and captures more generalized features. Quantitative results demonstrate robustness across datasets and superior performance in three out of the five centers analyzed in the ADHD-200 dataset.\nThe study highlights the hierarchical organization of FBNs derived from different DVAE layers. Shal-low layers capture simple connectivity patterns, while deeper layers reveal complex, global interactions. This hierarchy is validated through the Inheritance Similarity Rate (ISR), which measures how FBNS from shallower layers overlap with and contribute to those in deeper layers. Results indicate that multi-ple partially overlapping FBNs in shallow layers combine to form more complete FBNs in deeper layers, aligning with findings in the literature.\nFinally, the DVAE pipeline demonstrates competitive performance for ADHD classification, achieving higher accuracy in some centers compared to other fMRI-based methods and offering robust results despite variability in dataset size and scanning parameters. The authors emphasize the DVAE's capacity to extract hierarchical and biologically meaningful features, making it a promising tool for fMRI data analysis.\nThe authors in [76] propose the Deep Multimodal Brain Network (DMBN) model, which integrates structural and functional brain networks, representing them as graphs. The structural graph acts as a scaffold, imposing constraints on functional activity, while functional activity gradually influences the structural anatomy over time [7]. The DMBN model employs an encoder-decoder architecture to translate structural networks into functional representations. Initially, node representations are extracted using a convolutional kernel on the graphs, capturing connectivity patterns among neighboring nodes in the structural network, which are then used to learn the functional network representation. To enhance the model's ability to dynamically adapt connection weights, an attention mechanism is applied, enabling the identification of nuanced relationships between nodes. The model captures the non-linear and indirect relationship between structural and functional connectivity, learning an effective translation from one modality to the other. The decoder reconstructs the functional network from the structural node representation, capturing both direct connections and complex, non-linear interactions.\nThe authors benchmarked the DMBN model against five state-of-the-art methods, including three machine learning models (tBNE [10], MK-SVM [19], mCCA+ICA [64]) and two deep learning models (BrainNetCNN [36], Brain-Cheby [40]). DMBN outperformed all the baselines in a gender prediction task, achieving an accuracy of 81.9% and a 10% improvement in the F1-score. Using saliency maps, they identified the top 10 brain regions most relevant to the prediction task. These regions included cortical areas such as the orbital gyrus, precentral gyrus, and insular gyrus, as well as subcortical areas like the basal ganglia, which are critical for cognitive regulation, motor and emotional control, and likely exhibit gender-related differences. Moverover, an ablation analysis shows that the use of multimodal networks is essential to obtain a representative latent space, along with the attention mechanism.\nFurthermore, the authors applied DMBN to a disease classification task using the PPMI database for Parkinson's Disease. The model demonstrated superior performance compared to baseline models, achieving a 5-9% improvement in accuracy. Additionally, DMBN identified 10 key regions associated with Parkinson's biomarkers, such as the bilateral hippocampus and basal ganglia, which are well-documented in neuroimaging studies of Parkinson's Disease.\nThe study by Dong et al. [17] employ a 3D residual autoencoder (ResAE) to model deep represen-tations of fMRI data. Their approach simplifies functional brain network (FBN) estimation by using a deep autoencoder (AE) to learn latent representations of functional activity. These latent representations are subsequently used for FBN estimation via lasso regression, enabling the construction of a functional"}, {"title": "Multimodality integration", "content": "Generative latent models have proven to be highly effective in integrating multimodal neuroimaging data by projecting diverse data sources into shared lower-dimensional latent spaces. These models enable the extraction of unified representations that capture complementary aspects of the underlying data while maintaining interpretability and scalability.\nFor instance, the Variational Autoencoder (VAE) proposed in [25] integrates functional and structural neuroimaging data into a shared latent space using a single encoder-decoder scheme. This approach facilitates the extraction of cross-modal patterns, allowing for interpolation and interpretability between modalities. By employing these shared latent representations, the model achieved superior performance in schizophrenia classification tasks compared to traditional fusion methods.\nTo evaluate the effectiveness of the representations, the authors conducted a schizophrenia classifi-cation task. The model achieved a receiver operating characteristic area under the curve (ROC-AUC) of 0.8609, outperforming both eatly and late fusion PCA methods. The VAE's ability to reconstruct differences in the latent space was also validated through visualization of group differences between schizophrenia patients and healthy controls. Key regions, such as the thalamus and cerebellum, emerged as important, consistent with prior findings.\nMoreover, the model demonstrated scalability with increasing latent dimensions, retaining meaningful information without overfitting, even in small datasets. Importantly, the latent space showed clusters corresponding to specific modalities, such as intrinsic functional networks (ICNs) and structural MRI (SMRI) supporting its robustness. The model's generative capability offers potential applications in data augmentation and further exploraation modality-specific interactions.\nOn the other hand, following the same multimodality approach, authors in [47] propose a joint VAE model that learns a shared latent representation of both 123I-ioflupane SPECT images and clinical data scores of Parkinson's Disease (PD). Unlike previous work, they use two VAEs, one for neuroimaging and another for clinical data, with shared latent spaces cross-related by an additional loss term that minimizes differences between shared latents while ensuring disentanglement of non-shared latents. Moreover, the authors employ Maximum Mean Discrepancy (MDD) instead of the usual Kullback-Leibler divergence to maximize mutual information between input data and latent space.\nUsing this shared latent representation, the model achieved an $R^2$ of up to 0.86 in same-modality tasks and 0.441 in cross-modality tasks for predicting motor symptomatology and clinical features such as UPDRS. The framework demonstrates the feasibility of bridging neuroimaging and clinical modalities, identifying latent features predictive of motor symptoms and cognitive deficits. Notably, their analy-sis revealed that neuroimaging-based latent features are predominantly specific to motor impairments, reflecting dopaminergic deficits characteristic of PD. The study underscores the importance of data stan-dardization and DSSIM for enhancing latent representations, making a significant contribution to the understanding of multimodal neurodegenerative patterns. Future directions include early-stage detection and extending the framework to other neuroimaging modalities.\nIn [27], the authors propose a model to integrate imaging and genetic data into a lower-dimensional manifold guided by clinical diagnosis. Each subject in the framework has three inputs: an fMRI activation map, genetic information, and a binary diagnosis (control or affected). The model employs two feature"}, {"title": "Image synthesis", "content": "The generation of synthetic medical images has emerged as a vital tool for addressing challenges such as data scarcity, privacy concerns, and the need for controlled experimentation. Among the most pop-ular frameworks for image synthesis are Generative Adversarial Networks (GANs) and Latent Diffusion Models (LDMs), which have proven particularly effective in medical imaging and neuroimaging applica-tions. These models are widely used to enhance image realism, diversity, and structural fidelity, playing a critical role in generating data for training algorithms and simulating complex anatomical variations.\nZhang et. al [75"}]}