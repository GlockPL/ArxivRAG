{"title": "PYGEN: A COLLABORATIVE HUMAN-AI APPROACH TO PYTHON PACKAGE CREATION", "authors": ["Saikat Barua", "Mostafizur Rahman", "Md Jafor Sadek", "Rafiul Islam", "Shehnaz Khaled", "Dr. Md. Shohrab Hossain"], "abstract": "The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development, thereby significantly enhancing creativity and productivity. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. Practical examples of libraries such as AutoML, AutoVision, AutoSpeech, and Quantum Error Correction are demonstrated. The findings of our work show that Pygen considerably enhances the researcher's productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user's package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.", "sections": [{"title": "1 Introduction", "content": "Curiosity, innovation, and relentless pursuit have always characterized scientific progress. Today, we stand on the threshold of a promising new chapter in which every step, though minor and significant, acts as a booster for scientific growth. Pygen is a system that represents a significant stride toward that vision. It aims to automate humdrum and recurrent activities to free time for researchers, scientists, and enthusiasts to practice what matters: creativity and breakthrough innovation effectively. Automation becomes one of the most vital tools for social benefit when used thoughtfully and diligently[1]. Automation simplifies tasks, opens vistas for reimagining processes, and makes those processes even better across disciplines. Treated right and with responsibility, it might be the beacon of progress for everyone[2]. Pygen do just that: make technology accessible, amply productive, and take people to new heights in their scientific journeys as technology empowers individuals to achieve their milestones along with the progress of civilization[3]."}, {"title": "Pygen", "content": "The path to innovation involves solving unique and complex problems. Not all challenges can be addressed with existing technology, but human creativity shines through in its ability to construct new tools as needed, thereby expanding the boundaries of what is possible[18][17]. Pygen embodies this spirit of innovation by transforming abstract ideas into practical solutions that make a tangible impact. When the scientific community tackles a problem and finds a solution, they often discover key components and experimental techniques that are valuable for future scientists and technologists[4]. Creating Python packages originated from the desire to equip the community with essential tools that streamline experimental processes and advance scientific work. This approach to building tools is a form of responsible automation, where the human element remains integral, allowing for flexibility, creativity, and adaptability-qualities that purely automated tool designs often lack.\nOur vision for Pygen is to create a dynamic system that helps generate effective software tools and nurtures and inspires new ideas. By focusing on responsible automation, we aim to empower researchers and technologists to explore new possibilities, build upon existing knowledge, and contribute to advancing science and technology. Traditional software automation[5][6][7] approaches primarily focus on creating user-level abstractions that integrate a user's perspective to improve the software. However, our approach emphasizes the importance of designing superior tools that lead to the creation of better-end products. We made a critical observation that humans, when faced with challenges, often develop new tools if existing ones are inadequate[20][19]. Likewise, a language-model-based agentic system tasked with complex work must not only learn to use available tools but also create new ones to solve problems effectively[9][10][11]. This insight led us to develop an automated Python package generation system that starts with simple user prompts and evolves from there. By integrating this approach into an agentic framework, Pygen aims to enhance the adaptability and performance of such systems, enabling them to tackle increasingly sophisticated tasks and deliver impactful results.\nFoundational models[12][13][14][15] have typically been used to generate code for direct use, but their potential to autonomously build tools and design comprehensive software solutions remains largely untapped. While these models can assist in writing scripts or automating simple tasks, they have yet to augment human capability in complex, multi-faceted project settings effectively. With Pygen, we aim to change this paradigm. We are empowering these foundational models to generate ideas for software tools and create Python packages that can be effectively used to solve real-world challenges. By producing thorough documentation alongside the generated tools, Pygen extends the capabilities of these models beyond simple automation, turning them into meaningful partners in creative and technical endeavors.\nThe concept of the AI scientist[16] inspires our work, which is an end-to-end framework capable of originating novel ideas, developing experiments to explore those ideas, and ultimately producing scientific literature to share the resulting insights. Pygen builds on this vision by enhancing user queries, generating Python packages, and providing comprehensive documentation that allows others to easily understand, utilize, and build upon the generated tools. This process empowers users by transforming abstract ideas into functional, well-documented tools, simplifying the journey from initial concept to practical application. Pygens do more than merely automate; they act as extensions of human creativity. It bridges the gap between high-level conceptual thinking and practical, hands-on implementation, allowing users to bring their ideas to life with minimal friction.\nPygen allows users to specify the type of package they need for their tasks, along with the desired features and functionalities. Based on the user's description, the system refines these ideas and creates optimized implementation strategies. Using these refined strategies, Pygen designs a Python package by leveraging open-source models available on platforms like Google AI Studio and the GroqCloud. Once the package is generated, the Pygen creates comprehensive documentation to accompany it. Users can download the package and its documentation as a zip file, ensuring that all necessary information is in one place. The package is automatically set up if executed in the user's local environment, allowing for a smooth transition from development to execution. Users can further enhance these packages to meet their specific needs and, if desired, deploy them within the Python ecosystem.\nA key principle behind Pygen is our emphasis on open-source accessibility. Using open-source models, we ensure that users can access the system without being hindered by financial barriers or paywalls. This approach promotes open access and open-source scientific discovery, allowing individuals from all backgrounds to contribute to and benefit from innovation. Thanks to this open-source pipeline, users can utilize models made available through platforms such as GroqCloud and Google AI Studio to generate and document Python packages completely free of charge, encouraging experimentation and continuous improvement of Pygen. We are committed to open-sourcing Pygen itself, inviting contributions from the broader community to enhance its capabilities further, making it a truly collaborative and evolving project.\nOur contributions are summarized as follows:"}, {"title": "Pygen", "content": "1. We have introduced a Python package development system powered by open-source frontier models. This pipeline transforms user descriptions into refined ideas, leading to the generation of Python packages accompanied by thorough documentation. Users can download the generated package and documentation seamlessly, enabling them to start working immediately.\n2. Pygen can be deployed as a user-friendly application, allowing users to access it directly by simply setting their API key. This streamlined access reduces barriers to entry and enables a broader audience to leverage the system's capabilities.\n3. We have outlined several future research directions to improve responsible system automation. These include refining the strategies for improving Pygen, integrating a package reviewer to ensure robustness and reliability, and exploring the potential of agentic frameworks that can autonomously create and refine the tools they use."}, {"title": "2 Background", "content": "Large language models In this paper, we present an automated scientist system that leverages the advanced capabilities of autoregressive large language models. Specifically, we utilize sophisticated models like those developed by Google DeepMind's Gemini Team[21] and the Llama Team [22]. These models are designed to generate predictive text completions by estimating the probability distribution of the next token\u2014akin to a word-based on the sequence of preceding tokens. This process is mathematically represented as $p(x_t | X_{<t}; \\theta)$, where the model calculates the conditional probability of each potential next token. A sampling procedure follows this estimation phase during the testing phase, which generates coherent and contextually appropriate outputs.\nThe underlying power of these large language models (LLMs) stems from their training on vast and diverse datasets, which equips them to generate fluent and contextually relevant text and exhibit a wide range of advanced, human-like abilities. For instance, these models can effectively leverage commonsense knowledge[22], perform logical and abstract reasoning tasks[24], and even write, interpret, and debug complex code structures. The ability to generate such a diverse set of outputs highlights the adaptability and sophistication of these models when they are properly scaled and trained with extensive datasets. This adaptability makes LLMs a versatile tool for tackling various tasks, from natural language processing to aiding in scientific research.\nGroqCloud In practical applications, we have used GroqCloud, a platform that enables users to utilize different open-source models, including Meta's Llama and Google's Gemini, among others, such as the Mistral model[25]. Their API can give much faster inference times than manually achievable, making deploying more complex language models less onerous and technically demanding. GroqCloud is designed to be an easy entry point for researchers and developers looking to harness the power of strong AI models and simplify everything from the beginning into actual use. GroqCloud democratizes access to sophisticated AI thanks to the abstractions brought in by removing heavy computational needs and exhaustive model tuning.\nLocal Hosting Besides showing cloud-based solutions, we have also shown how to use Ollama for downloading and self-serving the model. This provides full integrations for users with tools like Pygen for key components to facilitate easy execution while maintaining full customization locally and privacy. Hosting local models using Ollama will enable developers to solve probable problems usually associated with security and latency in cloud-based systems."}, {"title": "3 Related Works", "content": "Integrating Large Language Models (LLMs) in complex domains has significantly advanced their use in various applications, such as scientific discovery, multi-agent collaboration, and automated reasoning. Lu et al. (2024) introduce The AI Scientist, a fully automated framework for scientific discovery, capable of independently generating hypotheses, conducting experiments, and producing research papers that meet top conference standards, marking a significant leap in automating the scientific process[16]. Yang et al. (2022) presented LLMs as inductive reasoners, addressing the limitations of formal languages in reasoning tasks [78]. Zhong et al. (2023) introduced a goal-driven discovery system, which identified distributional differences using language descriptions, showcasing the potential of LLMs for efficient data-driven research [79]. Zhou et al. (2024) demonstrated the evolution of self-evolving agents through symbolic learning, emphasizing the transition from model-centric to data-centric systems [80]. Chen et al. (2023) explored multi-agent frameworks and emergent behaviors, emphasizing how collaborative models can dynamically adapt [81]. Song et al. (2024) proposed a new adaptive team-building paradigm for multi-agent systems to solve complex tasks flexibly [82]."}, {"title": "4 The PyGen", "content": "Overview The Pygen has three main phases: Plan Generation, Code Generation and Testing, and Documentation Generation. In general, Pygen systematically transforms a user's requirement into a full Python package following a structured approach. First, the Phase of Plan Generation is all about understanding and scoping the package and formulating a detailed package plan that fits the user's needs. During this stage, Pygen takes the natural language inputs given by the user to specify the modules and functions that will make up the backbone of the package. Therefore, this is a very important step in providing a sound basis for the rest of the development activities by converting the conceptual requirements into actionable technical specifications. The generation of code and test cases occurs in the second stage, based on the planning from the first step. Functional code can be generated in Pygen using advanced language models. If already predefined, then modules and functions can be used to create functional code. Refinement might still be necessary after the code has been generated to increase quality and verify that the overall package structure is sound. The refinement steps are provided by enhancing the feature descriptions provided by the user. In the final step, documentation is created to ensure the delivered package is high quality and user-friendly. Pygen generates documentation from the generated package automatically. Subsequently, documents are transformed into Markdown format for easier reading and distribution. The Markdown documents are checked through formatting validation to ensure the standards necessary for user accessibility and consistency. The outcome of this stage is a well-documented package ready to use and distribute. The complete workflow of Pygen is depicted in Figure 1."}, {"title": "4.1 Plan Generation", "content": "Generating a plan is one of the most crucial steps in the Pygen workflow and starts the effective and efficient creation of Python packages. Our work has been greatly inspired by evolutionary computation and open-endedness research[16][26][27][28]. Pygen initiates this phase by interacting with the user to get detailed input about the package's needs. The user provides a prompt with a small description of the package and a set of features that shall be included."}, {"title": "4.2 Package Creation", "content": "The package creation process begins by gathering key inputs, including an enhanced package description and feature specifications from the previous step. These elements are then processed by sophisticated language models to generate the package structure and content, adhering to a predefined template. The template is designed based on the specifications of the Python Standard Library[35]. Leveraging these models ensures a high level of natural language understanding, facilitating the transformation of user-defined requirements into structured, well-documented Python code. The package generation includes essential files and module stubs, ensuring compliance with established Python distribution standards."}, {"title": "4.3 Creating Documentation and Formatting", "content": "In Pygen, automated documentation is an essential ingredient; it has been vital in delivering packages that are accessible and usable to Python. The focus here is turning the structured output of a generated package into a documented one. Our documentation-generating template is designed based on insights from best practices[38]. This generation process starts by extracting critical information from the descriptive content, setup configurations, dependencies, and usage scenarios from various package components into one coherent documentation framework. This, therefore, becomes a detailed guide that encompasses how to use, install, and contribute to the package with features, examples, and API references, among others. Besides promoting a more excellent user experience, extensive documentation contributes to the broader community by providing deeply detailed, reusable information about the created tools.\nFor a start, some of the essential elements of this automated documentation will include an overview that contains the package descriptions, which are deduced from general descriptive content; a description of the features of the package deduced from configuration details; sections on installation, usage examples, API references, and testing protocols, step by step demonstration of how to use the library, by using a language model's capability to parse classes, functions, and dependencies, generated documentation results in enrichment with technical specifics but easy readability. Contribution guidelines and licensing information further encourage community involvement and ensure that the generated packages comply with open-source principles. This automated yet detailed documentation strategy makes Pygen packages easy to understand and ready for integration into broader scientific and development workflows that significantly reduce manual overhead, which is usually needed for comprehensive software documentation."}, {"title": "5 Mathematical Preliminaries", "content": "We present the mathematical foundations underlying Pygen's core components: language model inference, documentation generation, and package structure optimization. These formalisms provide the theoretical framework for understanding how Pygen transforms user requirements into functional Python packages."}, {"title": "5.1 Language Model Foundations", "content": "The foundation of Pygen relies on autoregressive language models that generate text by estimating conditional probabilities. Given a sequence of tokens $x = (x_1, ..., x_T)$, the model computes the probability of each token given its preceding context:\n$p(x) = \\prod_{t=1}^{T} P(x_t|X_{<t})$ (1)\nwhere $x_{<t}$ represents all tokens before position t. The model parameters $\\theta$ are learned through training to minimize the negative log-likelihood:"}, {"title": "Pygen", "content": "$L(\\theta) = -\\sum_{t=1}^{T}log p(x_t|x_{<t}; \\theta)$ (2)\nModern LLMs, including those utilized by Pygen, are predominantly based on the Transformer architecture [39]. The Transformer leverages self-attention mechanisms to model dependencies between tokens irrespective of their positional distances. Mathematically, the self-attention mechanism computes the attention scores between pairs of tokens using the following formulation:\n$Attention(Q, K,V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$ (3)\nwhere $Q$, $K$, and $V$ denote the query, key, and value matrices, respectively, and $d_k$ is the dimensionality of the key vectors. This mechanism allows the model to weigh the relevance of different tokens dynamically, facilitating the capture of complex linguistic and contextual relationships. The optimization of LLMs is typically performed using variants of stochastic gradient descent (SGD), such as Adam [40], which adjusts the learning rates adaptively based on the first and second moments of the gradients. Regularization techniques, including dropout[41] and weight decay, are employed to prevent overfitting and enhance the generalization capabilities of the model.\nDuring inference, Pygen uses temperature sampling to control the randomness of generated outputs. For a temperature parameter $\\tau > 0$, the sampling probability is computed as:\n$Pr(X_t|X_{<t}) = \\frac{exp(log p(x_t|x_{<t}) / \\tau)}{\\sum_{x'} exp(log p(x'|x_{<t}) / \\tau)}$ (4)\nA lower temperature ($\\tau < 1$) makes the model more deterministic, favoring higher-probability tokens, while a higher temperature ($\\tau > 1$) increases randomness, allowing for more diverse outputs [39, 42]."}, {"title": "5.2 Package Structure Organization", "content": "Pygen optimizes package structure using a hierarchical representation. Let $G = (V, E)$ be a directed acyclic graph where:\n\u2022 V represents the set of package components (modules, functions, classes)\n\u2022 E represents dependencies between components\nThe optimal package structure minimizes the objective function:\n$min \\sum_{(u,v) \\in E} w(u, v) + \\lambda \\sum_{v \\in V} c(v)$ (5)\nIn this context, w(u, v) represents the coupling weight between components, c(v) denotes the complexity of component v, and $\\lambda$ is a regularization parameter. In algorithm 1, the package generation workflow is elaborated.\nThe agentic nature of Pygen can also be modeled using the reinforcement learning principle, where it interacts with an environment to perform actions (e.g., generating code) that maximize a cumulative reward[45, 44] R:\n$R = \\sum_{t=0}^{T} \\gamma^t r_t$ (6)\nwhere $\\gamma$ is the discount factor, rt is the immediate reward at time t, and T is the time horizon. By framing tool creation as an RL problem, Pygen can iteratively improve its package generation strategies based on feedback from testing and user interactions."}, {"title": "5.3 Documentation Generation Process", "content": "The documentation generation can be formalized as a mapping function f: C \u2192 D from code space C to documentation space D. For a given package P, we define its components as:\n$P = {M_1, ..., M_n}$ (7)\nwhere each module $M_i$ contains functions, classes, and their associated docstrings. The documentation generator extracts information through a series of transformations:\n$D(P) = {d(M_i) \\cup r(M_i) \\cup e(M_i)}$ (8)\nIn this context, d(M\u2081) represents the docstring extraction, r(M\u2081) captures the relationship mappings between components, and e(Mi) generates usage examples. Algorithm 2 elaborates on the idea of how the documentation of the package is generated."}, {"title": "5.4 Feature Enhancement through Prompt Engineering", "content": "The prompt enhancement process can be modeled as an optimization problem. Given an initial prompt po, Pygen generates an enhanced prompt p* that maximizes the quality function Q[47]:\n$p^* = arg max Q(p | p_0)$ (9)\nwhere Q considers factors such as specificity, completeness, and technical accuracy. This is achieved through iterative refinement[46]:\n$p_{t+1} = p_t + \\alpha \\nabla Q(p_t)$ (10)"}, {"title": "5.5 Reliability and Error Handling", "content": "Pygen implements exponential backoff[43] for API calls with retry mechanism modeled as:\n$t_n = min(t_{max}, t_0 b^n)$ (11)\nIn this modeling, tn represents the wait time for the nth retry attempt, where t0 is the initial wait time. The backoff factor is denoted by b, and the maximum wait time is limited to tmax. Additionally, n indicates the number of the retry attempts.\nThe probability of successful completion after k retries follows a geometric distribution:\n$P(success \\ after \\ k \\ retries) = (1 - p)^k p$ (12)\nwhere p is the probability of success for a single attempt. Details of how the fallback structure works are mentioned in Algorithm 4."}, {"title": "6 Results", "content": "We have generated four packages using Pygen to demonstrate its capabilities. These include AutoML, AutoVision, AutoSpeech, and Quantum Error Correction libraries, each addressing domain-specific problems effectively. Initially, we provided prompts with package and feature descriptions, which Pygen then enhanced. These improved descriptions were archived in a GitHub repository for future reference. From these enhanced descriptions, we generated context prompts, which sometimes included code templates for better caching and accuracy. These templates played a significant role in enhancing the quality of the generated packages.\nOnce the context prompt was ready, it entered the package generation pipeline. First, the necessary file structure for each package was created, followed by generating Python code for each corresponding file. We applied iterative refinement depending on the model type and context size. The entire package could not be generated in one go for models with smaller context sizes. In such cases, a fallback structure was triggered to complete the remaining components according to specified instructions. The fallback mechanism ensures integrity in package generation, though there is still room for improvement in guaranteeing the overall quality of the output.\nAfter code generation, the packages could be loaded into a local environment like a typical project structure or downloaded as a zip file through the application interface. Once the code generation was complete, we moved on to documentation. The entire project was parsed by the language model, which generated coherent descriptions of each package, formatted in markdown, and saved in the environment settings for easy download. We followed this approach for each use case, generating four demonstration packages. Moving forward, we plan to explore the quality and specific details of the generation process in greater depth."}, {"title": "6.1 Background behind designing the packages", "content": "We have developed several specialized packages to meet the evolving needs of our Pygen users, especially those focused on data analytics and modern application development. The AutoML package has been created to facilitate data analytics package creation, catering to the primary use case of Pygen users. The AutoVision package was also introduced due to this growing interest in vision research, showing their capabilities in computer vision. The AutoSpeech package also meets the current demand for embedding speech features in various modern applications to increase user experiences. Appreciative of the great promise and challenges of quantum computing, we have also produced a Quantum Error Correction package. Since there is a high demand for higher computational powers now, and by nature, quantum computing tends to be unreliable due to decoherence, effective error correction becomes necessary to make quantum computing usable. These packages showcase the broad capabilities of Pygen and give practical examples of how these packages work. The complete examples and data are available in our GitHub repository for further exploration."}, {"title": "6.2 Related Works for Packages", "content": "Neural Architecture Search (NAS) is a crucial sub-domain of AutoML that automates the design of neural networks, significantly enhancing model accuracy and efficiency [48]. Systems like Auto-Sklearn utilize advanced hyperparameter optimization (HPO) strategies to enhance model training speed and accuracy; for instance, PoSH Auto-sklearn has demonstrated improved performance under time constraints, reducing the error rate by up to 4.5 times[49]. Despite these advancements, scalability and integration with clinical workflows remain significant challenges, particularly due to the complex nature of medical data and regulatory requirements[50]. Further automating ML workflows, including domain-specific problem identification and data handling to minimize manual interventions, remains a crucial goal [51]. Additionally, establishing open-source benchmarks is essential for effectively comparing and evaluating AutoML systems, as highlighted by recent frameworks [52]. In response to these needs, KAXAI[53] was designed to provide an up-to-date and versatile AutoML system tailored for diverse stakeholders, addressing both scalability and usability concerns. Moeslund and Granum (2001)[58] stress the importance of improving scene analysis and human motion capture to enhance adaptability and accuracy in dynamic settings. In cell biology, Danuser[57] explores the potential of computer vision for interpreting cellular images, assisting researchers in understanding complex biological mechanisms. Khan et al. (2018)[54] provides a comprehensive introduction to Convolutional Neural Networks (CNNs), covering their foundational theory, training methodologies, and diverse applications, such as in medical imaging and autonomous vehicles. Feng et al. (2019)[56] discuss hardware-optimized implementations of deep learning-based computer vision algorithms on GPUs and FPGAs, enabling real-time applications in fields like autonomous driving and robotics. Xu et al. (2020)[55] critically review vision-based techniques for on-site monitoring in construction, highlighting the challenges of real-time processing in cluttered environments. In ELMAGIC[59], a vision system is designed for real-time ocular disease detection, focusing on improving early diagnosis through automated image analysis. Studies over the past decade highlight how deep learning has advanced speech-processing tasks, showcasing application improvements [60]. Domain adaptation techniques, like unsupervised deep domain adaptation (DDA), address the challenges of varying acoustic conditions by reducing mismatches between training and testing environments, resulting in substantial error rate reductions in noisy or mismatched conditions[61]. Additionally, deep learning models, such as CNNs and LSTMs, have successfully extracted emotional features from speech using spectrogram representations and large labeled datasets, which is crucial for human-computer interaction (HCI) systems [62]. As these technologies become more prevalent, research on defense mechanisms against potential misuse, such as synthetic speech attacks, has become increasingly important [63]. Analog error correction methods for continuous quantum variables like position and momentum have been developed to combat decoherence, enhancing robustness against noise in quantum systems [64]. Group-theoretic frameworks in quantum error correction simplify code construction, with codes like Calderbank-Shor-Steane (CSS) and surface codes utilizing orthogonal geometry to improve error resistance by effectively mapping qubits [65]. Experimental implementations in ion-trap systems have confirmed the feasibility of error correction through repetitive cycles, enabling phase-flip error corrections via high-fidelity gate operations [66]. In quantum communication and memory, error-correction algorithms safeguard against entanglement-related errors by applying"}, {"title": "Pygen", "content": "classical error-bounds analogs [67]. Furthermore, new QEC developments focus on fault-tolerant designs to manage errors in extensive computations, enhancing the practicality of quantum computing at scale [68]). Advanced decoding methods using multiple decoders are also necessary to successfully correct errors[69]. In scenarios where exact correction is challenging, approximate QEC techniques offer near-perfect correction by addressing minor coherence losses, thus extending QEC's effectiveness [70]."}, {"title": "6.3 Evaluating the Prompt Enhancement", "content": "The provided input by a user may not be sufficient to grasp all complexities in the package generation process. In PyGEN, we improve the prompt by using techniques to achieve a higher overall generation outcome. This step generates an enriched package description and a detailed feature description for more comprehensive input through the remaining steps of the package development process. Based on these elaborated descriptions, a contextual prompt is generated by another model, which is used to guide the following stages of code generation. We have further evaluated the usefulness of the contextual prompt. We found that the contextual prompt does not significantly improve the generation quality for larger models, which can process on their own elaborated and detailed enhanced feature and package descriptions. However, for the smaller models with limited context sizes, using context prompts brings enormous benefits to the quality, coherence, and accuracy of the generated text."}, {"title": "6.4 Assessing the Package Generation Process", "content": "Package generation in Python is an open-ended task; hence, we evaluated the generated package using three basic evaluation approaches: Human Evaluation, LLM-based evaluation, and CodeBLEU[77] score. Human evaluation involves experts checking the package's quality, correctness, and usability. Further, LLM-based evaluation was done using large language models to check the coherence and completeness of the output. During calculating the CodeBLEU"}, {"title": "Pygen", "content": "score, a template code is created, providing the model with a basic skeletal structure, based on which the generated code quality and score are assessed relative to this template."}, {"title": "6.5 Reviewing the Documentation", "content": "The generated documentation originates from the generated package. To judge the quality of the documentation, we introduced converter metrics to have a structured evaluation. Although human judgment is naturally subjective, we compared it to judgments from LLMs and a concrete quantitative evaluation metric to verify how well-aligned all three methods are. Observing the coherence score change for the documentation length was fascinating. From this experiment, models with larger context sizes consistently demonstrated better coherence; the smaller ones lost it in many places, most notably in the more complex parts. Therefore, context size is essential in determining the overall documentation quality, especially in maintaining coherence throughout longer texts.\nThe figure 10 gives an overall score of documentation qualities of different packages: AutoML, AutoVision, AutoSpeech, and QEC. We will evaluate the package based on four criteria: Clarity, Completeness, Structure, and Readability. AutoML has the best scores regarding the criteria of Clarity and Readability; hence, it has orderly and user-friendly content. Conversely, QEC received lower scores in Clarity. Therefore, there is room for better exposition of intricate technical concepts."}, {"title": "Pygen", "content": "Table 7 compares the review scores from an AI Reviewer (llama 3.1 70B versatile) with those from human reviewers for four evaluation metrics: clarity, completeness, structure, and readability. For each metric, different models were considered: AutoML, AutoVision, AutoSpeech, and QEC. The comparison will also include the score of the AI reviewer,"}, {"title": "Pygen", "content": "the mean score from human reviewers, the standard deviation among the ratings by humans, the correlation between the scores by AI and humans, and agreement as measured using Cohen's kappa. Results show a high correlation between AI and human scores, especially for measures such as Clarity and Readability within both AutoML and AutoVision models, indicating a robust concordance in the evaluation quality. As assessed via Cohen's kappa, the agreement shows significant consistency, where many instances have values over 0.80, evidencing high degrees of reliable comparability. However, some discrepancies are notable, such as the case of completeness in models like AutoSpeech, where reduced correlation and agreement point to divergences in evaluative viewpoints. Practical recommendations include improving the AI assessment methodology to reduce these inconsistencies and move closer to human reviewer alignment, especially for metrics with high variability. Further training of AI scorers using human feedback may improve the consistency and accuracy of all dimensions of documentation evaluation."}, {"title": "Pygen", "content": "Table 8 shows inter-rater reliability between different AI reviewers and a detailed comparison of ratings by llama-3.1-70b-versatile against Gemini 1.5 Pro 002 on the same four specific documentation quality metrics as before. Reliability assessments were conducted using various statistical methods: Cronbach's Alpha, Intraclass Correlation Coefficient (ICC), Fleiss' Kappa, and average percentage agreement. All metrics are highly inter-rater reliability, with Cronbach's Alpha ranging from 0.93 to 0.96, indicating good internal consistency among reviewers. Readability showed the best agreement with an ICC of 0.95, Fleiss' Kappa of 0.92, and an average agreement of 94%. The lowest agreement was for Completeness, although it was still high in absolute terms, with an ICC of 0.92 and an average agreement of 89%. This indicates that AI reviewers hold consistency when assessing the quality of documentation. However, further calibration might be helpful to make Completeness evaluations comparable by allowing targeted adjustments in the training process of the assessment model."}, {"title": "Pygen", "content": "The relationship between Coherence and several of the most central metrics of document quality including fluency, relevance, and engagement is described in Table 9; it uses Pearson's r and Spearman's rho correlation coefficients in the analysis. The strongest association is between Coherence and Fluency, with a Pearson's r of 0.92 and a Spearman's rho of 0.89; this reflects a solid relationship. This suggests that well-structured and coherent documentation often results in smooth and natural language, referring to Coherence's importance in achieving better linguistic quality. It is also closely related to relevance (r = 0.88), indicating that coherent content will likely be on-topic and satisfy users' needs. Nevertheless, the relation to Coherence is relatively weaker (r = 0.75), suggesting that although Coherence does contribute to engaging readers, it is likely that other factors also play a role in this measure. Similarly, the Fluency and Engagement relationship is only moderate (r = 0.65), suggesting that more than just language fluidity is at play to engage the audience maximally. It is so, focusing on cohesion to improve fluency and relevance in the documentation and realizing that additional tactics like concrete content or interactive elements may be vital to induce engagement."}, {"title": "6.6 Analyzing Memory and Time Complexity", "content": "Understanding the memory requirements and inference speed is essential for the system. PyGEN uses the Groq API, Google AI Studio, and Ollama (for local hosting). Among these, Groq provides the fastest inference across all models, allowing quicker processing times and improved responsiveness. Both Google AI Studio and Groq offer APIs for the Gemma model, but the Groq API demonstrates superior inference speed, leading us to favor this version for optimal performance. While smaller models typically have faster inference times, their reliability for complex tasks needs to be improved due to limited model capacity. Locally hosted models through Ollama were also explored to provide an alternative to ensure data privacy and flexibility. However, huge models were excluded due to computational constraints that limit feasibility without significant hardware investment. The inference speed of models provided through Ollama remains ambiguous, as better GPU/hardware could significantly improve the inference performance and make them more competitive."}, {"title": "Pygen", "content": "Table 10 summarizes the latency-throughput trade-offs over various AI models by providing latency per token in milliseconds and throughput in tokens per second for different models and API vendors. The table discusses the significant discrepancies in latency-throughput ratios and presents a balance between speed and computational efficiency intrinsic to each model. The Groq-developed 1B and 2B models of L"}]}