{"title": "EFFICIENT VISION-LANGUAGE MODELS BY SUMMARIZING VISUAL TOKENS INTO COMPACT REGISTERS", "authors": ["Yuxin Wen", "Qingqing Cao", "Qichen Fu", "Sachin Mehta", "Mahyar Najibi"], "abstract": "Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers\u2014about 1% of the original tokens\u2014Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3\u00d7.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-language models (VLMs) have attracted considerable attention for their capability to process visual and textual information, enabling various real-world applications, such as image caption- ing, visual question answering, and multimodal reasoning (OpenAI, 2023; Liu et al., 2024c). For example, GPT-4V (OpenAI, 2023) demonstrates the potential of these models in helping visually impaired individuals to \u201csee\u201d the world through cell phone cameras.\nRecent transformer-based vision-language models, such as LLaVA (Liu et al., 2024c), employ a pre- trained vision encoder as the model's \"eye\u201d to extract visual features and use a pre-trained language model as the \"brain\" to perform reasoning and text generation. This simple architecture is highly effective and requires only a small instruction-based fine-tuning dataset for achieving state-of-the- art results on standard benchmarks. The visual tower in VLMs decomposes high-resolution images into a large number of visual tokens, which are then concatenated with prompt tokens as an input to the language tower. This process significantly increases the computational cost due to the quadratic attention cost with respect to tokens. As an example, LLaVA-NeXT (Liu et al., 2024b) uses 2,880 tokens to represent a single image, which can be overly redundant in many scenarios. In contrast, the average text instruction length across all benchmarks used in LLaVA-NeXT has fewer than 70 tokens, as shown in Appendix A.1. Therefore, to improve the efficiency of VLMs, reducing the number of visual tokens is essential.\nThe recent state-of-the-art method, FastV (Chen et al., 2024), achieves this by dropping unimpor- tant visual tokens. This approach is highly effective when reducing the number of tokens by up to half. However, the model's performance drops significantly when more than half of the tokens are removed. Moreover, FastV requires the retrieval of attention scores from the self-attention block."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 VISION-LANGUAGE MODELS", "content": "Modern vision-language models typically combine a pre-trained image encoder with a large lan- guage model to handle multimodal data (Li et al., 2023b). One popular approach, often referred to as the autoregressive or LLaVA-style model (Li et al., 2023b; Liu et al., 2024c), directly projects visual features into the input embedding space of the language model, treating these features as part of the input tokens. However, in this design, the number of visual tokens is large and of- ten exceeds the number of textual tokens, leading to inefficiencies. Another common approach is cross-attention-based fusion (Alayrac et al., 2022), where added cross-attention blocks inside of the language transformer layers allow textual tokens to attend to visual tokens. More recently, early- fusion models like Fuyu (Bavishi et al., 2023), MoMA (Lin et al., 2024), and Chameleon (Team, 2024) use a unified transformer that processes raw textual tokens and visual patches simultaneously. Additionally, a key component of modern vision-language models recipe is instruction fine-tuning (Dai et al., 2023; Zhu et al., 2023; Liu et al., 2024a;c; Singla et al., 2024), which enables the model to function as a typical chatbot while also processing images, even with a small synthetic fine-tuning dataset. In this paper, we focus on LLaVA-style models."}, {"title": "2.2 VISUAL TOKEN REDUCTION", "content": "To improve the efficiency of vision or vision-language models, pruning or distilling visual tokens has been widely studied. Rao et al. (2021) introduce DynamicViT, which uses a small module to predict the importance of each visual token, dropping less important ones to enhance efficiency. Similarly, EViT (Liang et al., 2022) retains important tokens and fuses the less important ones within the model, using attention scores from the class token to the visual tokens. Further, PuMer (Cao et al., 2023) reduces the number of both textual and visual tokens by progressively pruning and merging them throughout the cross-modal encoder. Another interesting approach by Saifullah et al. (2023) involves discretizing visual features into textual tokens to reduce dimensionality. For more recent vision-language models, Perceiver Resampler (Jaegle et al., 2021; Alayrac et al., 2022; Bai et al., 2023) and Q-Former (Li et al., 2023b) are commonly used to pool visual tokens into a smaller set of queries using a transformer-based model. Additionally, in the FastV paper (Chen et al., 2024), the authors observe that in LLaVA-style models, the attention from textual tokens to visual tokens significantly diminishes after the first few layers, with the attentiveness declining close to zero after 10% of the transformer layers. Intuitively, their proposed state-of-the-art method drops the unimportant visual tokens accordingly after the first few layers."}, {"title": "2.3 VISUAL REGISTERS", "content": "Burtsev et al. (2020) first introduce memory tokens, which function similarly to registers. These tokens are used to store global information, enabling the model to effectively handle long-context tasks. Darcet et al. (2023) apply the idea of registers to ViTs. In their work, the authors observe that vision transformer models implicitly use low-information tokens to store global information for internal computations. However, this leads to abnormally high norms for these tokens, making it difficult to interpret the attention maps. Therefore, to address this, they introduce additional reg- ister tokens at the end of the sequence to handle this task. This approach not only improves the interpretability of attention maps but also boosts model performance. In our work, we show that these register tokens also enhance the performance of vision-language models, as demonstrated in Section 5.6. However, our primary focus in this paper is on using these registers for information dis- tillation, enabling the model to condense visual information into the registers to improve efficiency."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 MOTIVATION", "content": "We start with a key observation: many visual tokens exhibit significant redundancy. To demon- strate this, we calculated the cosine similarities between visual tokens generated by vision towers in VLMs. As shown in Figure 3, these similarities tend to cluster around 1, indicating a high de- gree of similarity among the visual tokens. This suggests that compressing the visual tokens into a smaller set would result in a minimal information loss. To achieve this, we append a set of learnable register tokens to the visual tokens and leverage the language model to summarize the visual infor- mation into these registers. Rather than using a separate model, such as the Perceiver Resampler, we utilize the more powerful language model for this task, as it inherently understands which visual tokens are important and how to organize the information. Consequently, as demonstrated in Fig- ure 3, our compact visual registers show reduced redundancy compared to the baseline visual tokens.\nFurthermore, based on observations from FastV\n(Chen et al., 2024), textual tokens in the lan- guage model primarily attend to visual tokens in the early transformer layers, with attention scores to visual tokens dropping to nearly zero after these layers. This suggests the language model requires only a few layers to process the visual information. Therefore, instead of using the entire model, we employ only the first few transformer layers to summarize the visual to- kens into the register tokens. After summariza- tion, the visual tokens are discarded, improving model efficiency. An overview of our method is provided in Figure 21."}, {"title": "3.2 VICTOR", "content": "We now formally introduce our method: Victor (Visual Compact Token Registers). A LLaVA- style vision-language model consists of three main components: (1) the image tower $I$, which is a pre-trained vision model, such as the CLIP image encoder (Radford et al., 2021); (2) the language tower $T$, a pre-trained LLM, such as LLaMA (Touvron et al., 2023); and (3) a projector $P$ that bridges the two, mapping the image features into the input embedding space of the language model. Given an image $X_{img}$, we first extract its features using the image tower $I$ and produce a set of projected visual tokens $x_V = \\{x_V^0, x_V^1, ..., x_V^{N-1}\\}$ from the projector $P$.\nAs described in Algorithm 1, for Victor, we additionally introduce a set of learnable visual reg- isters $X_R = \\{x_R^0, x_R^1, ..., x_R^{M-1}\\}$, where $M$ is a hyperparameter controlling the number of visual registers. A smaller $M$ results in a more efficient model, and usually $M \\ll N$. We then concatenate the projected visual tokens, visual registers, and textual tokens to form the input: $x = [x_V;X_R;X_T]$. This input is processed through the language tower for the first $k$ layers. At the start of layer $k$, all visual tokens are discarded, and the model continues with the truncated hidden states for the remaining layers.\nDuring training, we do not explicitly force the language model to summarize the visual informa- tion into the visual registers, but we empirically observe that it does so implicitly. In Section 5.7, we provide an empirical analysis showing that the visual registers effectively summarize important information from the visual tokens. Moreover, because Victor leverages the language model it- self for this summarization, rather than relying on an external model, and the language model is both powerful and knowing at identifying the most useful image features, our method experiences minimal performance drop compared to approaches like Perceiver Resampler while requiring much fewer additional model parameters.\nIn practice, we typically set $k$ to 3, which is approximately 10% of the language tower. This means the language tower processes the full-length hidden states for only the first 10% of its layers. For the remaining 90% layers, it operates on a significantly shorter context, thereby improving model effi- ciency. FastV (Chen et al., 2024), a state-of-the-art method, follows a similar idea and drops unim- portant tokens in the early layers and achieves a comparable theoretical FLOPs reduction. However, we find that since FastV relies on attention scores to determine which tokens to drop, it cannot utilize efficient attention implementations like FlashAttention (Dao et al., 2022; Dao, 2023) or Py- Torch High-Performance Scaled Dot Product Attention (SDPA). Consequently, FastV delivers less improvement in throughput than Victor when applying the same token-drop ratio in practice, and also empirically experiences a greater performance degradation in high token-drop ratio regimes."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "In this paper, we primarily follow the setting of the open-sourced LLaVA-v1.5 (Liu et al., 2024a). The training consists of two main stages: pre-training and instruction fine-tuning.\nPre-trained Models. For the image tower, we use the pre-trained OpenAI CLIP ViT-Large model (Radford et al., 2021), and for the text tower, we use the Vicuna-7B-v1.5 model (Zheng et al.,"}, {"title": "4.2 EVALUATION", "content": "We use LMMs-Eval (Li et al., 2024) and run evaluations over the 11 tasks reported in the LLaVA- v1.5 (Liu et al., 2024a) (i.e. VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), Sci- enceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), VizWiz-VQA (Gurari et al., 2018), POPE (Li et al., 2023c), MME (Yin et al., 2023), MMBench (Liu et al., 2023), SEED-Bench (Li et al., 2023a), LLaVA-Bench-in-the-Wild (Liu et al., 2024c), MM-Vet (Yu et al., 2023)), supplemented by the MMMU task (Yue et al., 2024). These benchmarks provide a comprehensive assessment of models' multi-modal reasoning capabilities, encompassing academic-task-oriented and instruction- following tasks. For simplicity, we primarily report the average of normalized benchmark scores. Specifically, for MME, the score is divided by 2,000, which represents the full score, as its metric is calculated by summing the accuracies of individual subtasks.\nWe evaluate efficiency by measuring the increase in throughput with the KV-cache enabled (Pope et al., 2023). After gathering statistics from all 12 benchmarks, presented in Appendix A.1, we evaluate two settings: 1) 2-token generation and 2) 128-token generation. The 2-token generation simulates scenarios where questions expect a single word, as in GQA (Hudson & Manning, 2019) and TextVQA (Singh et al., 2019). In contrast, the 128-token generation represents open-ended question scenarios, such as in LLaVA-Bench-in-the-Wild (Liu et al., 2024c) and MM-Vet (Yu et al., 2023). In both settings, we use a text prompt length of 64 and a batch size of 16. We choose a batch size of 16 because it is the largest batch size that fits in memory. All training is conducted on 8 NVIDIA A100 GPUs, with efficiency profiling performed on a single NVIDIA A100. By default, we set $k$ to 3 and vary the number of final visual tokens to 256, 128, 64, 32, 16, and 8 for all methods to thoroughly assess the trade-off between efficiency and performance, where the number of visual tokens for the original LLaVA-v1.5 model is 576.\nWe compare our method against two baselines: FastV (Chen et al., 2024) and Perceiver Resam- pler (Jaegle et al., 2021). FastV is the state-of-the-art token reduction method that filters out less important vision tokens based on attention scores, while the Perceiver Resampler is a compact, transformer-based model designed to condense input tokens into a smaller query set."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 THROUGHPUT INCREASE", "content": "We present the efficiency and performance trade-offs for both generation settings in Figure 4, and the performance on individual benchmarks is provided in Appendix A.2. As shown, our method has a better Pareto frontier than FastV and the Perceiver Resampler in both scenarios."}, {"title": "5.2 FLOPS REDUCTION", "content": "We also report the theoretical FLOPs reduction of the methods, calculated using the FLOPs formula from Chen et al. (2024). As demonstrated in Figure 5, while our method shows a slightly smaller FLOPs reduction due to the presence of additional register tokens at the start of the language tower, the overall reduction is comparable under the significantly higher token reduction rate. Although the Perceiver Resampler achieves a notable increase in throughput, its FLOPs reduction is substantially lower than that of FastV and Victor, primarily due to the additional transformer layers it employs."}, {"title": "5.3 TRAINING-TIME REDUCTION", "content": "Victor not only reduces inference costs but also lowers training costs. As indicated in Fig- ure 6, both Perceiver Resampler and Victor significantly reduce training time in both pre- training and fine-tuning stages, with the re- duction being especially notable during pre- training due to the shorter text tokens. Victor achieves a greater overall time reduction. In contrast, training with FastV only reduces pre-training time and does not improve fine-tuning efficiency. This is because fine-tuning typically involves a large number of text tokens (often exceeding a thousand), and the use of a naive at- tention implementation in this phase introduces significant overhead, reducing training efficiency. Additionally, we observe that training with FastV does not match the performance of inference-time FastV. However, it exhibits slower benchmark performance decay as the number of visual tokens decreases and outperforms inference-time FastV when the number of visual tokens drops below 32."}, {"title": "5.4 DIFFERENT LANGUAGE TOWERS", "content": "We extensively evaluate the effectiveness of our method with different language towers. As shown in Figure 7, replacing the original Vicuna-7B-v1.5 language model with Vicuna-13B-v1.5 (Zheng et al., 2024), Meta-Llama-3-8B-Instruct (Dubey et al., 2024), and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Victor remains highly effective and significantly outperforms the two baseline meth- ods. For both Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, Victor demonstrates min- imal performance drop and a slow decay in performance as the number of visual tokens decreases. Notably, for these two models, when the number of visual tokens is reduced by half, the method shows no performance degradation at all.\nWe further demonstrate the performance of our method on a different vision-language model design: LLaVA-NeXT (LLaVA-v1.6) (Liu et al., 2024a). LLaVA-NeXT follows a similar architecture to LLaVA-v1.5 but increases the number of visual tokens from 576 to 2,880 by incorporating different aspect ratios, enhancing the model's capabilities. Additionally, LLaVA-NeXT utilizes Qwen2-7B- Instruct (Yang et al., 2024) as its language tower, benefiting from its extended context length. In our experiments, we reduce the number of visual tokens to 512, 256, 128, 64, 32, and 16. As indi- cated in Figure 7d, our method remains highly effective in the LLaVA-NeXT setting, consistently outperforming both FastV and the Perceiver Resampler."}, {"title": "5.5 DIFFERENT LAYERS TO DROP VISUAL TOKENS", "content": "We show the results of the ablation study on which layer to drop the visual tokens (hyper- parameter $k$) in Figure 8. In terms of through- put improvement, it is clear that the earlier we drop the visual tokens, the more efficient the model becomes. For lower-layer numbers, such as $k$ = 1 or $k$ = 2, the model's efficiency sig- nificantly increases, with throughput reaching nearly a 4\u00d7 improvement. However, this comes with a substantial performance drop, suggest- ing that one or two layers are likely insuffi- cient for the summarization process. In con- trast, when $k\u2265$ 3, the performance degradation is minimal, staying within a 5% performance score loss. Notably, when $k$ = 5, with half of the visual tokens dropped, the model experiences no performance loss."}, {"title": "5.6 EFFECT OF VISUAL REGISTERS ON REGULAR VLMS", "content": "Figure 9 presents the results of not dropping the visual tokens and instead using visual registers as a means for the model to store useful information, similar to those proposed by Darcet et al. (2023)."}, {"title": "5.7 ANALYSIS", "content": "In Section 3.1, we empirically demonstrate that the visual registers are more compact than the orig- inal visual tokens. In this subsection, we perform a simple analysis to examine whether and how visual registers summarize visual information. The attention map from visual registers to visual tokens is shown in Figure 10. Although the model is not explicitly trained to summarize visual information into the visual registers, they implicitly encode the visual tokens, as indicated by the significant attention scores between visual registers and visual tokens. Interestingly, the visual reg- isters exhibit low attention to visual tokens in the first two layers, and the summarization primarily occurs in the third layer, just before the visual tokens are removed. This may be because the first two layers focus on processing the visual tokens or aligning the visual tokens and registers into a shared space to facilitate communication in later layers. This observation aligns with the ablation results discussed in Section 5.5, where dropping visual tokens in the first or second layer causes a significant performance drop. This suggests that it is more effective for the summarization process to occur in the later layers."}, {"title": "6 LIMITATION AND FUTURE WORK", "content": "While Victor is simple and effective, we identified some limitations and directions for future improvements. Currently, Victor is not a training-free method, and it must be incorporated at the training stage of the vision-language modeling. Developing a version of Victor that could be applied post-training would be a valuable advancement. However, this might be challenging, as the language tower may need to be specifically trained to learn to effectively utilize the visual registers. Another limitation is the inflexibility of the number of visual registers. As discussed in"}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce Victor, a novel visual token summarization method that significantly enhances the efficiency-performance trade-off in vision-language models. Without explicit enforce- ment, the language tower utilizes register tokens to summarize visual information within the first 10% of the layers. After summarization, Victor eliminates the need for visual tokens in the fol- lowing layers. Our approach offers a superior balance in efficiency compared to state-of-the-art methods. Moreover, with just up to 0.03% additional parameters, Victor is compatible with vari- ous attention mechanisms, providing a user-friendly and efficient solution across different hardware environments for future applications."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 LENGTH STATISTICS FOR INDIVIDUAL BENCHMARKS", "content": "We show the length statistics of benchmarks in Table 2. Based on the representative lengths of these benchmarks, there are two main categories: 1) short-generation, represented by the 2-token gener- ation scenario in our experiments, and 2) long-generation, represented by the 128-token generation scenario."}, {"title": "A.2 PERFORMANCE ON INDIVIDUAL BENCHMARKS", "content": "We show the performance on individual benchmarks of Section 5.1 in Figure 11."}, {"title": "A.3 TOTAL TRAINING-TIME REDUCTION", "content": "The total training-time reduction is shown in Figure 12."}, {"title": "A.5 ABLATION ON ADJUSTING THE NUMBER OF VISUAL REGISTERS AT INFERENCE", "content": "In our main experiments, we retrain the model whenever a different number of visual registers is required. In this subsection, we explore two strategies for dynamically adjusting the number of visual registers at inference time. Given a Victor model with $M$ visual registers, if we want to use $M' < M$ registers, we either select the first $M'$ registers (referred to as \"head\") or the last $M'$ registers (referred to as \"tail\"). As shown in Figure 14, the performance of these adjustments is not as effective as retraining the model from scratch. However, we believe adding certain auxiliary losses during training can make our method more flexible, and we leave this for future work."}, {"title": "A.6 IMPORTANCE OF VISUAL REGISTERS FOR SUMMARIZATION", "content": "In this subsection, we conduct an ablation study to demonstrate the necessity of using visual registers for summarization. Specifically, we compare our approach to an alternative method where instead of prepending additional tokens to the visual tokens, we retain the last $M$ visual tokens at layer 3. This requires the model to summarize all visual information into these last existing $M$ visual tokens. As shown in Figure 15, while the ablated method results in a slight improvement in throughput, overall the performance drops significantly. This highlights the importance of incorporating visual registers for effective summarization."}, {"title": "A.7 DIFFERENT VISUAL REGISTERS", "content": "We also experiment with various types of visual registers. In addition to using learnable tokens, we test three alternative methods for visual registers: 1) Pooled Image Feature: utilizing average- pooled visual tokens as the register tokens, 2) Zeros: initializing with all zeros, and 3) \u201cImage\" Token: using the embedding of the word \"Image.\u201d The results are presented in Figure 16. The \"Image\" Token method is effective for the visual registers, especially when the number of visual tokens is reduced to 256 and 128, as there is no performance drop. However, all alternative methods showed relatively worse performance compared to learnable queries in the low visual token regime. Therefore, we adopt learnable queries for Victor as they offer better overall performance."}]}