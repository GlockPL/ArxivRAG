{"title": "PSCon: Toward Conversational Product Search", "authors": ["Jie Zou", "Mohammad Aliannejadi", "Evangelos Kanoulas", "Shuxi Han", "Heli Ma", "Zheng Wang", "Yang Yang", "Heng Tao Shen"], "abstract": "Conversational Product Search (CPS) is confined to simulated con- versations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.", "sections": [{"title": "1 Introduction", "content": "In traditional product search systems, users formulate queries and then browse the products to locate their target items (i.e., products). This is, however, inefficient and may be mismatched by product search systems because of the semantic gap between queries and products [43, 61]. Recently, researchers have augmented search functionality by allowing search systems to interact with users by natural language and collect explicit feedback from users as a step towards better understanding users' information needs [7, 52, 61]. As for the existing research on Conversational Product Search (CPS), Zhang et al. [52] presented the first CPS model by simulat- ing conversations over item \"aspects\u201d extracted from user reviews. Based on that, Bi et al. [7] proposed a Conversational Product Search (CPS) model based on the negative feedback and simulated user feedback on the item \"aspects.\" Similarly, Zou et al. [60] simu- lated conversations based on item \"aspects\" and proposed a CPS model via representation learning. On the other hand, Zou and Kanoulas [61] proposed a question-based Bayesian product search model by simulating conversations based on extracted informative terms instead of item \"aspects.\u201d The existing work on CPS mainly simulates user conversations while interacting with the product search system. That is, there is a lack of a real CPS dataset driven by human-like language, and thus a public dataset for product search with human-like conversations is of great importance and significance.\nMoreover, most conversational domain-specific datasets are cre- ated for a particular market or a particular language. However, (i) e-commerce companies often operate across markets. For example, the Amazon platform has 20 markets around the world\u00b9 for global selling. Typically, some e-commerce companies operate in multiple countries and they can benefit from the experience and data gath- ered across several markets. Therefore, it is significant to construct a cross-market dataset for product search [8]; (ii) different users from different countries use different languages and each language has its own grammar and syntactic rules. It is important to create a non-single language CPS dataset to facilitate the development of CPS models that can generalize well to different languages. Yet re- sources of conversational datasets in languages other than English are lacking.\nTo this end, in this paper, we create PSCon, a new dataset for product search with human-like conversations that supports dual markets and two languages. Specifically, we build the dataset by formulating the task of CPS. We define a pipeline for CPS, which includes the following six sub-tasks: (T1) user intent detection, (T2) keyword extraction, (T3) system action prediction, (T4) question se- lection, (T5) item ranking, and (T6) response generation. First, user"}, {"title": "2 Related Work", "content": "Recently, there has been a growing interest regarding CPS in the information retrieval community [18, 24, 30, 33, 50]. In the early stage, Zhang et al. [52] proposed a first CPS model by introducing a unified framework for conversational search and recommenda- tion. This model engages users by posing questions about various aspects derived from user reviews and collects their feedback on these aspect values. Similarly, Bi et al. [7] simulated conversations and solicited explicit user responses to aspect-value pairs mined from product reviews, with a particular emphasis on the impor- tance of negative feedback in the context of CPS. In contrast to this approach, Zou and Kanoulas [61] focused on querying users about informative terms, typically entities, extracted from item-related descriptions and reviews. They introduced a sequential Bayesian method that employs cross-user duet training for enhancing CPS. Based on that, Zou et al. [62] conducted an empirical study to measure user willingness and the accuracy of answers provided in question-based product search systems, shedding light on the effectiveness of existing question-based product search approaches. More recently, Zou et al. [60] asked clarifying questions [45] on item aspects and proposed a CPS model via representation learning.\nThe aforementioned studies on CPS all simulate user conver- sations when interacting with the product search system. That is, their conversations are based on template-based questions and sim- ulated user answers. Although they demonstrate CPS is a promising research direction, their deployment of simulated conversations is suboptimal as this is not a human-like setting [60]. This is mainly due to the lack of real conversations for product search. In this work, we fill this gap by proposing a dataset for CPS with real human-like conversations."}, {"title": "2.2 Conversational Datasets", "content": "There are several conversational datasets available for different tasks, such as conversational recommendation [31, 39, 49], and conversational search [38]. The majority of conversational recom- mendation datasets focus on the movie domain, which involves a collection of annotated dialogs where a seeker requests movie suggestions from the recommender [23, 27, 56]. In contrast, conver- sational search datasets [3, 4, 9, 10, 25, 29, 32, 36, 38, 42, 51] focus on conversations to assist information seeking in the search scenar- ios. For instance, Dalton et al. [10] and Thomas et al. [41] released the CAST and MISC datasets, which are created by volunteers or experts with 80 and 110 conversations, respectively. Ren et al. [38] proposed a conversational information-seeking dataset with the Wizard-of-Oz (WoZ) setup. In this paper, we use a human-human data collection protocol based on participants recruited from uni- versities, which is more reliable [6]. Compared with conversational search for locating relevant documents, product search focuses on locating the potential products for purchase [2, 47], which is a subset of relevant products and thus more challenging. Moreover, user queries in product search are often short and thus preference elicitation plays an important role in CPS due to its goal of locating the purchased products.\nAs for product search and e-commerce, there are only a few conversational datasets available. Fu et al. [11] built a conversa- tional dataset for e-commerce constructed from Amazon reviews. However, they use simulated users and conversations. Jia et al. [19] introduced a conversational dataset from the E-commerce domain with user profiles, but they are in Chinese only and involve a single market only. Bernard and Balog [6] collected a dataset of multi-goal conversations for e-commerce, which is the closest dataset to ours. However, they are in a sole language, i.e., English, and involve a single market only. Also, they only contain less than 20 conversa- tions per category. Different from the above publications, we build a CPS dataset through real human-human conversations, which is on a suitable scale. Moreover, it can support dual markets and two languages, along with a knowledge graph."}, {"title": "2.3 Conversational Recommender System for e-Commerce", "content": "Conversational recommender systems for e-commerce are also associated with product search. For instance, Zhang et al. [52] pro- pose a unified paradigm for product search and recommendation. Conversational recommender systems utilize human-like natural language to deliver personalized and engaging recommendations through conversational interfaces like chatbots and intelligence as- sistants [12, 17]. In general, the existing research on conversational"}, {"title": "3 Data Collection & Analysis", "content": "We have developed an online Web system to collect the data. Fol- lowing Figure 1, the human-human data collection protocol used in this study is as follows:\n(1) Participants are presented with detailed instructions regarding this study. They are also trained to be familiar with the system through detailed videos.\n(2) Participants are asked to complete a demographic survey, e.g., their gender, age, and career field.\n(3) The study uses a human-human data collection protocol, where some participants mimic system roles to help users locate their target products, while others play user roles to seek products. They chat in the chat room.\n(4) The conversation starts from the user role. From the beginning, we ask the user role to imagine a target product that she/he wants to buy in mind. Then, the user role initiates the conver- sation by revealing the product needs and chatting with the system to get the target product.\n(5) The user role is asked to pick a search intent (e.g., uttering a greeting or sending a request (Section 4)) and then start the conversation by writing a message to send to the system.\n(6) Whenever the system role receives the message from the user role, the system role needs to extract keywords (Section 4) from the conversation history that are used to understand the natural language conversation. After that, the system role has to select an action label that reflects the action to take (Section 4), e.g., asking a clarifying question (Section 4) or recommending products (Section 4). At last, the system role needs to generate a natural language response to send to the user based on the selected action accordingly (Section 4).\n(7) For each round of conversation, the system roles can input a query (i.e., keywords) to search for related results for prod- ucts. They can also search for products by using search filters provided in the system when necessary.\n(8) When the system roles think they need more information about the user's need, they can scan corresponding attributes of prod- ucts provided by the Amazon product search API, and ask a clarifying question about any attributes (or others) to the user.\n(9) When the system roles think that there are products meeting the user's need, they can select corresponding products from the search result panel (based on the Amazon product search API) to recommend to the user.\n(10) The conversation repeats until the user ends the chat. For each turn, both the user and system role can send multiple messages at once. For messages, they can send text, product images, or URLs.\n(11) After the conversation is done, we ask the user role to mention if the system managed to find their target products or not. The user is also asked to evaluate the relevance of the products recommended by the system in the conversations.\n(12) At the end, participants are asked to complete a post-task ques- tionnaire, on their experience and satisfaction.\nWe value participants' privacy. All participants are informed that their data is securely encrypted and any identifiable information is removed. We did not collect any data to breach their privacy. We also ask the participants to report any adult content and/or offensive content in the conversations that might raise ethical issues. Our study was approved by the ethics committee of the institute."}, {"title": "3.2 Data Collection System", "content": "The online Web system for data collection mainly includes admin- istration pages and the chat room.\nWhen entering the system, the new participant needs to register first. When registration, the new par- ticipant is assigned a role (i.e., the user role or the system role) randomly, following the random assignment setup of related litera- ture [14, 20, 58]. After logging in with the assigned role, participants are directed to the instruction page. The instruction page shows detailed instructions regarding the study and what is expected con- cerning conversational goals. The instructions are designed for each role. That is, the participants of user roles only see their own instruc- tions for customers. In contrast, the participants of system roles only see their own instructions for the system role as a shopping assistant. Following the instructions, some demographic question- naires are shown to the participants, such as their gender, age, and career field. After that, the participants are redirected to the chat room (see Section 3.2.2 for details). After the conversation in the chat room is done, the participants are redirected to a post-task page. The post-task page shows some post-task questionnaires regarding the experience and satisfaction of participants. In addition, there is a relevance judgment page before the post-task questionnaires. On the relevance judgment page, the user role needs to evaluate the relevance of the products (using like/dislike labels to indicate their assessment of the recommended items) recommended by the system in the conversations."}, {"title": "3.2.2 Chat Room.", "content": "The interface for the chat room differs depend- ing on the role of the participant. The interface for the system role is shown in Figure 2, while the interface for the user role is omitted due to its simplicity and limited space. Both the interface for the user role and the system role are divided into two vertical blocks. The left block displays the ongoing conversation while the right block shows the control panel. They both contain an instruction in the top right corner. The interface for the control panel differs between the user role and the system role. For the user roles, the control panel allows them to select an intent and send messages. They can also finish the conversation anytime by clicking the \"finish the conversation\" button on the bottom. For the system roles, the control panel also allows them to select a system action and send messages. In addition, the system interface includes a search en- gine. The search engine produces results of products and attributes by keywords. The system's role is allowed to submit keywords to search results from the search engine. The search engine we used in this paper is based on Amazon product search API. We use Amazon China\u00b2 when collecting the Chinese dataset, while using Amazon USA\u00b3 when collecting the English dataset. During the conversation, the system allows them to select products from the entire Amazon product pool to send to the user. The selected product list is comprised of curated products with descriptions and pictures. Both the user and the system can urge their chat partner to speed up, which is shown on the upper right of the interface."}, {"title": "3.3 Quality Control", "content": "To ensure data quality, we perform quality control measures to avoid including low-quality conversations. Specifically, we deploy five quality checks: (1) we perform an on-site training on the study goal and how to use the system for all participants before the partic- ipants start the study; (2) we provide detailed videos demonstrating the process and detailed documentation on the study, e.g., the in- struction of using the interfaces, the steps in creating the data, the explanation of the labels, and the positive and negative examples; (3) we evaluate the time participants spent reading the textual descrip- tions and ask participants questions about the study descriptions to ensure that they have read and understood the instructions; (4) we remove the data with incomplete conversations (i.e., not ended normally or conversations without any products recommended); (5) The collected data are further checked manually to ensure that re- moved participants and conversations are not filtered out wrongly. We also manually go over the data and correct issues such as typos, grammatical errors, and mislabels.4"}, {"title": "3.4 Participants", "content": "Participants in this study were 465 volunteers recruited through email invitations (students and staff of two universities, one in Europe and one in Asia). Of those participants, 63 were in the pilot study and 402 were in the actual study. The pilot study is deployed to make sure the data collection runs smoothly as expected before the actual data collection. Participants were paid around 2.5 pounds on average for each conversation. In the actual study, 196 participants took part in the Chinese dataset collection and 206 participants took part in the English dataset collection. Their demographic data is briefed as follows: (1) Gender: 163 females, 228 males, 11 non- binary; (2) Age: 232 participants were 18-24 years old, 146 were 25-34, and 24 were older than 35; (3) Career field: 145 in computer science, and 257 in others.5"}, {"title": "3.5 Data Statistics", "content": "The datasets contain an English and a Chinese dataset, which in- clude a total of 1,730 conversations and 10,887 utterances recom- mending 5,212 products. The data statistics for the datasets are reported in Table 1. The Chinese dataset includes a total of 904 conversations and 5,254 utterances recommending 2,548 products. Each conversation contains 5 to 16 utterances, 3 to 7 turns, and 1 to 30 products. 98.01% of the conversations have 3-4 turns. 91.82% of the conversations recommend 1-4 products. The English dataset in- cludes a total of 826 conversations and 5,633 utterances recommend- ing 2,664 products. Each conversation contains 2 to 17 utterances, 1 to 8 turns, and 1 to 14 products. 96.58% of the conversations have 2-6 turns. 85.98% of the conversations recommend 1-6 products. This is in line with that users expect the system to recommend high-quality products with fewer rounds in real applications, and thus they prefer to converse with short interactions by following such a real-world setting. For the Chinese dataset, the average number of utterances per conversation is 5.81, The average number of turns per conversation is 3.37, and the average number of items per conversation is 2.82. On average, utterances have 8.17 \u00b1 4.43 words. For the English dataset, the average number of utterances per conversation is 6.82, The average number of turns per conver- sation is 3.86, and the average number of items per conversation is 3.23. On average, utterances have 6.57 \u00b1 5.12 words.\nBesides the conversation, we also collect the information related to the conversation. We collect the conversation annotations in- cluding the user intent, keywords, system action, attributes for clarifying questions, and selected products recommended in the conversation to support our subtasks. The example of a collected conversation is shown in Figure 3. Also, the user preference for prod- ucts (like/dislike) recommended by the system in the conversation is collected. For each search behavior, the search results for products (top 100 products) and related attributes are also collected. In addi- tion, we also collect information on each product including the URL, product image, product title, product description, product reviews, and product metadata such as category, brand, features, product ratings, and related products (also bought, also viewed, bought together, compared, and sponsored products). Given that many existing studies on product search incorporate external knowledge graphs to capture user preference, we construct a knowledge graph"}, {"title": "3.6 Data Analysis", "content": "We analyze the PSCon dataset with a focus on the distribution of user intents and system actions, which is shown in Figure 4. We observe that, in the Chinese dataset, the top intents selected by users are \"Interpret\" and \"Reveal,\" while the top system actions are \"Recommend\" and \"Clarify.\" This indicates that systems ask clarifying questions or recommend products in most cases, whereas users reveal their product needs and provide feedback to the system in most cases when interacting with the CPS system. We omit the figures of the English dataset, due to a similar trend and limited space.\nMoreover, we explore how the number of liked products found by the system is affected by the number of conversation turns, as shown in Figure 5. We obverse that a higher number of turns leads to a significantly higher number of liked products found. The one- way ANOVA test shows significant differences among different number of turns (p < 0.05). This might be because the number of recommended products increases with the conversation going on. Also, the system captures more information and is closer to the true user preferences, and thus more likely to recommend the products the user likes, with the conversation going on. This is in line with"}, {"title": "4 Task Design", "content": "User Intent Detection (T1) We define a new user intent taxonomy based on previous conversational systems [5, 38]. The user intent taxonomy includes five intents: (1) Reveal: reveal a new intent, i.e., issue a new query; (2) Revise: revise an intent, i.e., reformulate the query; (3) Interpret: interpret or refine an intent by answering a clarifying question or responding to product recommendations from the system; (4) Inquire: inquire about the response from the system, e.g., ask the system to rephrase its response if it is unclear; (5) Chitchat: do not fit other labels, e.g., greetings or other utter- ances that are not related to the user's product need. This user intent detection aims to detect the user intent $i_u$ for the current user utterance, given conversation history C and the current user utterance U, which is defined as learning a mapping: $C, U \\rightarrow i_u$.\nKeyword Extraction (T2) Conversations usually contain noisy data and more content than necessary. A user utterance may also not be semantically complete due to ellipsis and anaphora [10]. It is challenging to feed the user utterances and conversations to the product search model directly. Therefore, it is important to extract keywords from the user utterance for a language understanding task. Given the conversation history C and the current user ut- terance U, this subtask aims to select a sequence of keywords $T_u$, that can best describe the user utterance. Formally, it is defined as learning a mapping: $C, U \\rightarrow T_u$.\nSystem Action Prediction (T3) The action stage helps the sys- tem to take appropriate actions at the right time, e.g., when to ask a clarifying question to query new information and when to recommend products. Action prediction has been studied in dia- logue systems [35] and conversational recommender [22]. However, action prediction has not yet been studied in CPS. Considering previous work on conversational search and conversational recom- mender [22, 38] while making a connection to product search, we define the system actions: (1) Clarify: ask questions to clarify the user's product need for preference elicitation; (2) Recommend: give a suggestion, which can be a certain product or a list of products; (3) Inquire: inquire about the response from the user, e.g., ask the user to rephrase its response if it is unclear; (4) No-answer: cannot find related products to meet user's need; (5) Chitchat: do not fit other labels, e.g., greetings or other utterances unrelated to the user's product need. This subtask aims to select an optimal system action a, that can help the users locate their target items efficiently, given the context (e.g., conversation history C, the current user utterance"}, {"title": "5 Conversational Product Search", "content": "In this section, we propose a model to replace the conversational agent for product search based on our collected data and then evaluate the model on our collected dataset."}, {"title": "5.1 Base Model", "content": "We adopt the Transformer framework [44] as our base model, con- sisting of the embedding and self-attention layers.\nGiven a sequence $S = [s_1,..., s_k, ..., s_K]$, the embedding of each element of the input sequence is the concentration of positional embedding s and sequence embedding p.\n$h_k = s_k + p_k$.\nAll elemental embeddings $h_k$ form a matrix H, which will then be passed through a self-attention layer. Suppose we are at the n-th transformer layer, we will update H in the following manner:\n$H^{n+1} = MultiHead(PFFN(H^n))$,\nwhere MultiHead is a multi-head self-attention sub-layer, and PFFN is a Position-wise Feed-Forward Network constructed by the Feed- Forward Network (FFN) with ReLU activation [1]. Afterward, we deploy a residual connection around each of the two sub-layers (the sub-layer is MultiHead or PFFN in Equation 2), followed by a dropout and layer normalization.\nGiven a task sequence S, we utilize the final hidden representa- tions at the N-layer of Transformer $h^N$ to generate the output of each subtask. Specifically, we apply a softmax function through a linear layer with ReLU activation in between to produce an output of probability:\n$P = Softmax(ReLU(W \\times h^N + b))$,\nwhere W is a learnable transformation matrix, and b denotes the trainable bias matrix."}, {"title": "5.2 Task-Specific Modules", "content": "For the task of user intent detec- tion, the sequence is defined as $S_{T1} = [[T1], C, U]$, where C is the conversation history, U is the current utterance, and [T1] is an inserted classification token at the beginning of the input sequence. The sequence is passed to the Transformer base model to get the hidden representation of [T1]. Finally, the model computes the probability of user intent according to Equation 3."}, {"title": "5.2.2 Keyword Extraction (T2).", "content": "Similar to T1, the sequence of key- word extraction is defined as $S_{T2} = [[T2], C, U]$. After inputting the sequence into the Transformer base model, the hidden repre- sentation of [T2] is obtained. A linear classifier with a sigmoid function is utilized to predict whether each token T in U belongs to a keyword or not, i.e., replace softmax with sigmoid in Equation 3."}, {"title": "5.2.3 System Action Prediction (T3).", "content": "For system action prediction, the sequence is defined as $S_{T3} = [[T3], C, U, D]$, where D denotes the candidate products. When the system takes action, the candidate products should be considered since the quality of the candidate products affects the action. In this paper, we use product titles and product descriptions as the textual information of D. The candidate products are products returned by the search results queried on the keywords in T2 (keyword extraction). The hidden representa- tion of [T3] is obtained from the transformer base model and then Equation 3 is applied to predict the system actions."}, {"title": "5.2.4 Question Selection (T4).", "content": "For question selection, the sequence is defined as $S_{T4} = [[T4], C, U, Q]$, where Q is the question. Also, we obtain the hidden representation of [T4] based on the transformer base model. After that, we apply a softmax function through a feedforward network with ReLU activation in between (Equation 3) to calculate the distribution probability over candidate questions. In this paper, we model it as a ranking problem based on the output distribution over candidate questions, one can also model it as a"}, {"title": "5.2.5 Item Ranking (T5).", "content": "Similar to system action prediction and question selection, the sequence of item ranking is defined as $S_{T5} = [[T5], C, U, D]$. We first obtain the hidden representation of [T5] from the transformer base model. Then, similar to (T4), we apply a softmax function through a feedforward network with ReLU activation in between (Equation 3) to calculate the distribution probability over candidate items. Again, we model it as a ranking problem, one can also model it as a binary classification problem to predict whether each candidate item is selected or not."}, {"title": "5.2.6 Response Generation (T6).", "content": "We use a standard Transformer decoder to conduct response generation. The transformer decoder outputs a representation at each decoding time step. For response generation, we put the predicted system action (T3) at the beginning to indicate the type of response. We use the representations of T1 (user intent detection), T4 (question selection), and T5 (item ranking) as the representations of input sequences and fuse them with max pooling to get the final hidden representation. Then a linear classifier with softmax is applied on top to predict the probability of the token at each time step. In addition, we deploy the copy mechanism [46], which is commonly used and helpful for generating informative utterances [13], to copy tokens in the conversation history C, and product-related documents D."}, {"title": "5.2.7 Training.", "content": "We use binary cross-entropy loss for keyword ex- traction (T2) while using cross-entropy loss for user intent detection (T1), system action prediction (T3), question selection (T4), item ranking (T5), and response generation (T6). The model is designed to handle the six subtasks (T1-T6) in an integrated manner. During training, different subtasks are jointly trained with shared inputs, generating a unified loss for backpropagation. To train a complex or deep model, a large number of data is needed. To this end, we first use other conversational datasets to pre-train our model and then fine-tune the model on our PSCon dataset. In our implementation, we use two conversational datasets, DuConv [48] and KdConv [55], which are for knowledge-grounded conversations, for pre-training."}, {"title": "6 Experiments", "content": "We use binary cross-entropy loss for keyword ex- traction (T2) while using cross-entropy loss for user intent detection (T1), system action prediction (T3), question selection (T4), item ranking (T5), and response generation (T6). The model is designed to handle the six subtasks (T1-T6) in an integrated manner. During training, different subtasks are jointly trained with shared inputs, generating a unified loss for backpropagation. To train a complex or deep model, a large number of data is needed. To this end, we first use other conversational datasets to pre-train our model and then fine-tune the model on our PSCon dataset. In our implementation, we use two conversational datasets, DuConv [48] and KdConv [55], which are for knowledge-grounded conversations, for pre-training."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we introduced a pipeline of CPS, including six sub- tasks: user intent detection, keyword extraction, system action pre- diction, question selection, item ranking, and response generation. We collected and released a new CPS dataset, called PSCon, based on a human-human data collection protocol, to support comprehen- sive and in-depth research on the six subtasks and all aspects of CPS. The collected data is available for dual markets and two languages, and can support the development of knowledge graph-based CPS models. We further proposed a benchmark model to model the six subtasks of CPS. Extensive experiments on the PSCon dataset show that the model is effective.\nTo collect the data, two workers need to be paired as participants to generate one conversation. Each conversation usually involves multiple searches and scanning for products which costs much ef- fort of the participants. This makes the data collection challenging and expensive, leading to generating a limited volume of data like existing work [6, 10, 41, 42]. We understand that, although the data contains more than one thousand conversations, the data is still limited to training a large model in an end-to-end manner. To this end, we utilize other conversational datasets to pre-train our model to relax this limitation and the experimental results of our proposed model demonstrate its effectiveness. Nevertheless, we plan to main- tain the dataset by expanding it with more training data from a large number of users in the future. As we present a system and data"}]}