{"title": "CSP-AIT-Net: A contrastive learning-enhanced spatiotemporal graph attention framework for short-term metro OD flow prediction with asynchronous inflow tracking", "authors": ["Yichen Wang", "Chengcheng Yu"], "abstract": "Accurate origin-destination (OD) passenger flow prediction is crucial for enhancing metro system efficiency, optimizing scheduling, and improving passenger experiences. However, current models often fail to effectively capture the asynchronous departure characteristics of OD flows and underutilize the inflow and outflow data, which limits their prediction accuracy. To address these issues, we propose CSP-AIT-Net, a novel spatiotemporal graph attention framework designed to enhance OD flow prediction by incorporating asynchronous inflow tracking and advanced station semantics representation. Our framework restructures the OD flow prediction paradigm by first predicting outflows and then decomposing OD flows using a spatiotemporal graph attention mechanism. To enhance computational efficiency, we introduce a masking mechanism and propose asynchronous passenger flow graphs that integrate inflow and OD flow with conservation constraints. Furthermore, we employ contrastive learning to extract high-dimensional land use semantics of metro stations, enriching the contextual understanding of passenger mobility patterns. Validation of the Shanghai metro system demonstrates improvement in short-term OD flow prediction accuracy over state-of-the-art methods. This work contributes to enhancing metro operational efficiency, scheduling precision, and overall system safety.", "sections": [{"title": "1. Introduction", "content": "Metro systems provide efficient, high-capacity transit solutions that reduce surface congestion and promote sustainable urban mobility (Lin et al., 2022; C. Yang et al., 2023). However, managing these systems presents significant challenges, particularly in balancing fluctuating passenger demand with service capacity, reducing station"}, {"title": "2. Related works", "content": ""}, {"title": "2.1 Station-level prediction methods", "content": "The prediction of metro passenger flow has evolved significantly, beginning with statistical approaches during the early stages of research (Yan et al., 2018). Statistical methods, such as the autoregressive integrated moving average (ARIMA), Kalman filter, and generalized autoregressive conditional heteroskedasticity (GARCH) models, utilized historical ridership data to forecast demand by capturing autocorrelations and temporal volatility (Chen et al., 2020; Jiao et al., 2016). While these models effectively predicted short-term trends, they were limited in their ability to capture the nonlinear dynamics inherent in metro systems, and their computational efficiency suffered with the introduction of larger datasets (Fang et al., 2022).\nMachine learning approaches were adopted to overcome these limitations, offering the capability to model complex, nonlinear relationships without predefined assumptions (Toqu\u00e9 et al., 2017). Techniques such as artificial neural networks"}, {"title": "2.2 OD-level prediction methods", "content": "Predicting origin-destination (OD) passenger flows in metro systems presents unique challenges compared to station-level flow forecasting (Huang et al., 2023; Wei and Chen, 2012). The summary of methods for OD-level metro passenger flow prediction is shown in Table 1. Unlike station-level models that focus on forecasting ridership at individual stations, OD flow prediction aims to estimate passenger movement between specific station pairs, typically within short time intervals (Su et al., 2024; Y. Yang et al., 2023). This requires modeling interactions across the entire network, influenced by both temporal patterns and spatial factors, which significantly increases the complexity (Li et al., 2019; Yang et al., 2022).\nOne of the primary challenges is modeling the dynamic relationships between OD pairs, where interactions depend on both network structure and temporal variations (Lu et al., 2023). Unlike localized station-level traffic patterns, OD-level models must consider the complete network structure and the temporal dependencies that arise from daily and weekly travel patterns (Zhan et al., 2024). The complexity of OD prediction is compounded by the incomplete or unavailable nature of OD matrices in real-time, particularly in large transit systems (Liu et al., 2019).\nRecent OD-level prediction methods have focused on incorporating advanced spatiotemporal models to address these challenges. The multi-view passenger flow model introduced by Zheng et al. (2023) separately learns station-level and cross-station dynamics, utilizing GATs and GRUs. By integrating real-time data such as inflows and outflows, it has demonstrated a 2.5% improvement in WMAPE compared to traditional methods, highlighting the importance of inter-station interactions.\nBeyond spatiotemporal modeling, integrating diverse data sources has been another focus area. Su et al. (2024) proposed the high-dimensional feature fusion deep learning (HDF-DL) model, which incorporates external data such as weather conditions and land use features. By clustering OD pairs, the model reduces prediction complexity, particularly for systems with large numbers of OD pairs.\nDespite significant advancements, several challenges remain in OD-level prediction methods as metro systems expand. Scalability is a persistent issue, especially given the increasing complexity of urban transit networks. Lu et al. (2021) addressed this by proposing a dual-attention graph neural network (DAGNN), which effectively captured both inbound and outbound relationships between stations, enhancing inter-station modeling capabilities. While DAGNN improved prediction accuracy, its computational requirements still pose challenges for high-dimensional OD matrices. As networks grow, ensuring these methods maintain efficiency under larger, more dynamic conditions remains an area needing further exploration.\nData sparsity is another key challenge in OD-level prediction, often resulting from incomplete OD matrices due to asynchronous passenger movement records. Jiang et al. (2022) tackled this issue with a deep learning framework using LSTM networks to reconstruct missing data and capture temporal dependencies, which significantly improved prediction accuracy. However, irregularities in OD data continue to present difficulties, particularly for real-time and large-scale applications. Addressing data gaps effectively in evolving transit environments thus remains a critical focus.\nIncorporating flow conservation principles further complicates OD prediction. Accurate modeling requires that system entries match exits, a complexity not fully addressed by traditional models. Liu et al. (2023) made progress in this area with a dual information transformer, which captured mutual causality between OD and DO flows, enhancing consistency and accuracy. However, explicitly incorporating flow conservation across large-scale transit networks remains challenging. This paper seeks"}, {"title": "3. Preliminaries", "content": "This section defines the key concepts and notations employed in the study, formally articulates the metro OD passenger flow prediction task and provides an overview of the background algorithms that underpin the proposed methodology."}, {"title": "3.1 Notions and definitions", "content": "Definition 1: Time interval. The time interval represents a specific segment of the day used for temporal discretization. The entire day is divided evenly into $T$ time intervals, each of duration $\\Delta T$, where $\\Delta T \\in \\{10,20,30,60\\}$ minutes; $t\\in T = \\{1,2,...,T\\}$ denote the index for each interval, where $t$ represents a distinct period within the day.\nDefinition 2: Metro network. The metro network is modeled as an undirected graph $G = (V,E)$, where $V = \\{V_1, V_2,..., V_N\\} \\in \\mathbb{R}^N$ is the set of nodes, each representing a station, and $N$ is the total number of stations. $E = \\{e_{ij}\\} \\in \\mathbb{R}^{N\\times N}$ is the set of edges,"}, {"title": "Definition 3: Origin-Destination (OD) Trip", "content": "The OD trip o describes the complete journey of a passenger through the metro system, including all relevant attributes:\n\u2022 $v^o \\in V$ : The origin station from which the passenger begins their journey.\n\u2022 $v^\\tau \\in T$ : The interval when the passenger enters the origin station.\n\u2022 $v^d \\in V$ : The destination station at which the passenger leaves the train.\n\u2022 $v^{d'} \\in T$ : The interval when the passenger exits the destination station.\nThe trips are recorded by the Automatic Fare Collection (AFC) system, which logs the entry and exit data for passengers."}, {"title": "Definition 4: Station-level inflow and outflow", "content": "The station-level inflow and outflow describe the movement of passengers entering or leaving a particular station during a given time interval.\n\u2022 Inflow $\\phi^{(i)}_t$ for station $v_i \\in V$ at time interval $t\\in T$ is defined as the total number of passengers entering station $v_i$ during interval $t$:\n$\\phi^{(i)}_t = \\sum_{o} \\Theta(v^\\tau = t \\land v^o = v_i), \\forall v_i \\in V$   (1)\nwhere $\\Theta(\\cdot)$ is an indicator function equal to 1 if the condition is met, and 0 otherwise. The inflows across all stations during time interval $t$ can be represented as a vector:\n$\\Phi_t = [\\phi^{(1)}_t, \\phi^{(2)}_t,..., \\phi^{(N)}_t] \\in \\mathbb{R}^N, \\forall t \\in T$  (2)\n\u2022 Outflow $\\psi^{(i)}_t$ for station $v_i \\in V$ at time interval $t\\in T$ is defined as the number of passengers leaving station $v_i$ during the interval $t$:"}, {"title": "Definition 5: OD passenger flow matrix", "content": "The OD passenger flow matrix $P \\in \\mathbb{R}^{N\\times N}$ at time interval $t\\in T$ represents the number of passengers traveling between origin-destination (OD) station pairs. Specifically, $p^{(ij)}_t$ represents the number of passengers departing from station $v_i$ and arriving at station $v_j$ during time interval $t$:\n$p^{(ij)}_t = \\sum_{o} \\Theta(v^\\tau = t \\land v^o = v_i \\land v^d = v_j ), \\forall t \\in T, \\forall v_i,v_j \\in V, v_i\\neq v_j$ (5)\nwhere the OD passenger flow matrix $P$ encapsulates the spatial distribution of passenger flows for every station pair during time interval $t$."}, {"title": "Definition 6: Asynchronous OD inflow and outflow", "content": "Due to differing travel times between origin and destination stations, asynchronous OD inflow represents the temporal discrepancy in the inflow and outflow observations for different OD pairs. Define $\\tau_{ij}$ as the average travel time between origin station $v_i \\in V$ and destination station $v_j \\in V$. The asynchronous inflow tensor is represented by:\n$A^{(ij)}_t = \\sum^{\\tau = t - \\tau_{ij} + \\Delta t}_{ \\tau = t - \\tau_{ij} - \\Delta t} \\phi^{(\\tau)}, \\forall t\\in T, \\forall v_i, v_j \\in V \\land v_i \\neq v_j $ (6)\nThe relationship indicates that inflows recorded at an earlier time interval $t - \\tau_{ij}$ (with a buffer time interval $\\Delta \\tau$ ) result in outflows at a later time interval $t$, reflecting the average travel time between stations."}, {"title": "Definition 7: Land use semantics", "content": "The land use semantics vector $s_i$ for station $v_i \\in V$ captures important land-use features in the surrounding area which may impact passenger flow. The POI intensity is defined as the number of Points of Interest (POI) within a radius of the station:\n\u2022 $\\gamma_{res}^{(i)}$: Number of residential-related POIs.\n\u2022 $\\gamma_{work}^{(i)}$: Number of work-related POIs.\n\u2022 $\\gamma_{com}^{(i)}$: Number of commercial and entertainment-related POIs.\n\u2022 $\\gamma_{edu}^{(i)}$: Number of educational-related POIs.\n\u2022 $\\gamma_{med}^{(i)}$: Number of medical-related POIs.\nOther indicators are dummy variables which indicate the specific land use type:\n\u2022 $\\eta_{CBD}^{(i)}$: Whether the metro station is proximity to the Central Business District (CBD).\n\u2022 $\\eta_{Uni}^{(i)}$: Whether the metro station is in proximity to the university.\n\u2022 $\\eta_{Hub}^{(i)}$: Whether the metro station is in proximity to the transportation hub.\n\u2022 $\\eta_{Cen}^{(i)}$: Whether the metro station is located within the city center.\nThe station's land use semantics are expressed as:\n$S_i = [\\gamma_{res}^{(i)}, \\gamma_{work}^{(i)}, \\gamma_{com}^{(i)}, \\gamma_{edu}^{(i)}, \\gamma_{med}^{(i)}, \\eta_{CBD}^{(i)}, \\eta_{Uni}^{(i)}, \\eta_{Hub}^{(i)}, \\eta_{Cen}^{(i)}] \\in \\mathbb{R}^{N\\times d_1}, \\forall v_i \\in V$ (7)\nwhere $d_1$ represents the feature dimension."}, {"title": "3.2 Problem statement", "content": "Given the metro network $G = (V,E)$, the land use semantics $s_i$ for each station $v_i \\in V$, and the historical OD passenger flow $p_t \\in \\mathbb{R}^{N\\times N}$ for past time intervals $t\\in T$, our goal is to predict the OD-level passenger flows $P_{t+1}\\in \\mathbb{R}^{N\\times N}$ for a future target interval $t+1$.\nThe prediction task must account for two key constraints:\n1. Asynchronous passenger flow constraint: Due to varying travel times $\\tau_{ij}$ between different origin-destination pairs, the OD flow predictions must consider the temporal misalignment between inflows and corresponding outflows.\n2. Inflow consistency constraint: The aggregated OD passenger flow must be consistent with the originating inflows, ensuring passenger conservation across the network.\nFormally, the metro OD passenger flow prediction task can be expressed as learning a deep learning function $F$ such that:\n$P^{(ij)}_{t+1} = F(G, \\{s_i\\}^N_{i=1}, \\{P_\\tau\\}_{\\tau\\in [1,t]}), \\forall v_i,v_j \\in V \\land v_i \\neq v_j$ (8)\nwhere $F$ () utilizes the graph structure $G$, the land use attributes $s_i$, and historical OD data to predict the OD passenger flows for the future time interval $t+1$, while considering asynchronous and inflow consistency constraints."}, {"title": "3.3 Background algorithm knowledge", "content": "This section provides an overview of two algorithms that support the station-level outflow prediction: Graph Attention Networks (GAT) for capturing spatial relationships between stations and Long Short-Term Memory (LSTM) networks for learning temporal dependencies of passenger flows."}, {"title": "3.3.1 GAT for station-level spatial dependency", "content": "GAT are utilized to learn the spatial dependencies among metro stations by utilizing an attention mechanism that assigns varying importance to neighboring"}, {"title": "3.3.2 LSTM for temporal dependency", "content": "LSTMs can learn both short-term and long-term temporal patterns in sequences of passenger flows. It maintains hidden state $h_t$ and a cell state $c_t$ to retain memory. Specifically, the forget gate $f_t$ determines the extent to which the previous cell state $c_{t-1}$ is retained:\n$f_t = \\sigma (W_f x_t + U_f h_{t-1} + b_f)$ (12)"}, {"title": "4. Methodology", "content": "The proposed CSP-AIT-Net framework consists of four components: contrastive learning-enhanced station semantic extraction, outflow prediction and asynchronous inflow tracking, spatiotemporal graph attention mechanism for outflow decomposition, and joint loss of OD flow prediction accuracy and inflow constraints. The workflow of CSP-AIT-Net framework is shown in Figure 2."}, {"title": "4.1 Contrastive learning-enhanced station semantic extraction", "content": "Contrastive learning is utilized to generate high-dimensional semantic embeddings for each metro station, deriving a robust feature vector for each station.\nFor a metro network $G = (V,E)$ consisting of N metro stations denoted by the set $V = \\{V_1,V_2,...V_N\\}$, the semantic attribute vector $s_i\\in \\mathbb{R}$ is defined, including POI characteristics (work, residential, commercial, educational, medical-related POI intensity) and location dummy variables (whether the metro station is near to central business district, university, transportation hub or located in the suburban areas). The feature matrix representing all stations is expressed as:\n$S = [s_1,s_2,...,s_N] \\in \\mathbb{R}^{\\Otimes d_1}$ (18)\nThe contrastive learning is then utilized to project the semantic attributes into a higher-dimensional embedding space $z_i \\in \\mathbb{R}^{d_2}$ where $d_2>d_1$. To generate the embedding vector $z_i$,the positive station pair $(v_i,v_j)$ and negative station pair $(v_i,v_j)$ are firstly identified based on the daily outflows sequence, denoted as:\n$\\Psi_i = [\\psi^{(i)}_1,\\psi^{(i)}_2,...,\\psi^{(i)}_t] \\in \\mathbb{R}^T, \\forall v_i \\in V$ (19)"}, {"title": "4.2 Outflow prediction and asynchronous inflow tracking", "content": "The outflow prediction task combined GAT and LSTM to obtain station-level outflow for future time intervals. GAT is constructed on the metro graph $G = (N,V)$ and updates each station's embedding vectors $z_i\\in \\mathbb{R}$ by aggregating from neighboring nodes. As described in section 3.3, the attention coefficient $a_{ij}$ between station $v_i$ and $V_j$ is computed as:\n$\\alpha_{ij} = \\frac{exp(LeakyReLU (a^T [W_{GAT} z_i || W_{GAT} z_j]))}{\\sum_{k \\in \\mathbb{M}_i} exp (LeakyReLU (a^T \\times [W_{GAT} z_i || W_{GAT} z_k]))}$ (25)\nwhere $W_{GAT} \\in \\mathbb{R}^{&\\times d_3}$ represents the learnable weight matrix, $a \\in \\mathbb{R}^{& d_3}$ is an attention vector, $\\mathbb{M}_i$ represents the set of neighbors of station $v_i$. The updated embedding for station $v_i$ is then given by:\n$z'_i = \\sigma(\\sum_{j \\in \\mathbb{M}_i} a_{ij} W_{GAT}z_j)$  (26)\nwhere $\\sigma(.)$ is a non-linear activation function such as ReLU. The temporal dynamics of the outflow at each station are captured with LSTM network, where historical outflow sequence is utilized to produce latent representation $H_t$ at interval $t$:\n$H_t = LSTM (\\psi_t), \\forall t \\in T$ (27)\nThe GAT-updated embedding $Z'\\in \\mathbb{R}^{N \\times d_2}$ and the LSTM-generated hidden state"}, {"title": "4.3 Spatiotemporal graph attention mechanism for outflow decomposition", "content": "The spatiotemporal graph attention mechanism is proposed to effectively decompose predicted outflows into detailed origin-destination (OD) flows. The input includes a historical OD flow tensor $P \\in \\mathbb{R}^{N\\times N \\times T}$ Additionally, the predicted outflows at each station for a future time interval $t+1$ are denoted as $\\psi_{i,t+1} \\in \\mathbb{R}^N$. The adaptive mask $M\\in \\mathbb{R}^{N\\times N \\times T}$ regulates the attention contribution of origin-destination pairs."}, {"title": "4.4 Joint loss of OD flow prediction accuracy and inflow constraints", "content": "A joint loss function is introduced that balances both the OD passenger flow prediction accuracy and consistency with actual inflow. The asynchronous inflow tracking process utilizes the predicted OD flows for time t+1 to calculate the corresponding inflows at each origin station. Given that different OD pairs have varying average travel times, denoted by $\\tau_{ij}$ for an OD pair from station $v_i$ to station $V_j$ , the relevant inflow can be determined by:\n$\\hat{A}^{(ij)}_t = \\sum_{t=t-T_{ij}-\\Delta t}^{t-T_{ij}+\\Delta t} \\sum_{j=1}^{N_{t+1}} \\hat{P}_{t+1}, \\forall v_i \\in V, t\\in T $ (39)\nThe inflow constraint loss is expressed as:\n$L_{Inflow} = \\frac{1}{N} \\sum_{t=1}^{T}\\frac{1}{N}\\sum_{i=1}^{N}  (\\hat{A}^{(i)}_{t+1} - A^{(i)}_{t+1})^2$ (40)\nThe OD flow prediction loss is defined as:\n$L_{OD} = \\frac{1}{N} \\frac{1}{T}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\sum_{t=1}^{T}(\\hat{P}^{(t+1)}_{ij} - P^{(t+1)}_{ij})^2$ (41)\nThe overall joint loss function combines these two components to ensure both prediction accuracy and inflow consistency:\n$L_{Joint} = \\lambda_{OD}L_{OD} +  \\lambda_{Inflow}L_{Inflow}$ (42)"}, {"title": "5. Experiment", "content": ""}, {"title": "5.1 Dataset preparation", "content": "The dataset used in this study consists of OD-level passenger flow data from the Shanghai metro system, collected over a three-week period from September 1 to September 21, 2023. Data were recorded at 10-minute intervals during standard metro"}, {"title": "5.2 Parameters and experimental settings", "content": "The experimental setup for the proposed CSP-AIT-Net model was designed to optimize model performance while maintaining computational efficiency. A systematic selection of hyperparameters was undertaken, incorporating empirical tuning and best practices from the literature.\nThe Adam optimizer was employed with a learning rate of 1e-3. This value was chosen for its balance between stable convergence and efficient training, as demonstrated in similar prediction tasks. Preliminary tests indicated that lower learning rates, such as 1e-4, led to slower convergence, while higher rates, like le-2, caused instability in model training.\nA batch size of 128 was selected to strike an optimal balance between training efficiency and resource usage. Larger batch sizes were avoided due to their significant memory demands, which could hinder scalability, while smaller batches introduced excessive variance in the gradient updates. The chosen batch size is consistent with similar large-scale transportation prediction studies, where it provides sufficient statistical representation without compromising model efficiency.\nThe embedding dimension for station semantics was set to 128, following insights from prior research that suggest this dimensionality is adequate for representing key station features while avoiding excessive model complexity. This choice allows the model to capture spatial dependencies effectively, including the relationships between stations within proximity, while maintaining manageable computational demands.\nThe masking mechanism employed 80 flow backtracking instances, reflecting the Pareto principle where a small subset of OD pairs accounts for the majority of passenger flows. It was chosen after testing several options between 50 and 100, with 80 found to provide the best balance between computational efficiency and prediction accuracy.\nFor the loss function, OD flow prediction loss was weighted at 0.9, while inflow consistency loss was assigned a weight of 0.1. This weighting scheme was based on empirical observations, which showed that emphasizing OD flow prediction led to more impactful operational improvements, while still preserving flow conservation as a secondary constraint.\nThe model was trained for 100 epochs, with early stopping applied if the validation loss did not improve for 10 consecutive epochs, thus preventing overfitting and ensuring robust generalization. A dropout rate of 0.3 was used as an additional regularization technique to enhance model robustness.\nThe experiments were conducted using Python 3.8.17 and PyTorch 2.0.0+cu117, running on an NVIDIA GeForce RTX 3050 GPU with 12 GB of memory. This computational setup was selected for its ability to efficiently manage the model's"}, {"title": "5.3 Comparable methods", "content": "Autoregressive Integrated Moving Average (ARIMA) is a widely used method for time series forecasting that models the relationship between a variable and its lagged values, as well as past forecast errors. It combines autoregressive (AR) terms, moving average (MA) terms, and a differencing operation to make the time series stationary.\nRandom Forest (RF) is an ensemble machine learning method that constructs multiple decision trees during training and outputs the mode of the predictions for classification or the mean prediction for regression. By aggregating predictions from numerous trees, RF improves model accuracy and reduces overfitting.\nGradient Boosting Decision Trees (GBDT) is another ensemble learning method that builds a series of decision trees sequentially, where each tree attempts to correct the errors of the previous one. GBDT has been widely applied to metro passenger flow prediction, offering flexibility in modeling complex non-linear relationships and interactions between various features.\nLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to model sequential data with long-range dependencies. LSTM addresses the vanishing gradient problem of traditional RNNs by introducing memory cells that store information over long sequences.\nGated Recurrent Unit (GRU) is a simpler variant of LSTM, designed to improve computational efficiency while maintaining the ability to capture long-term temporal dependencies. GRU achieves this by using a simplified gating mechanism,"}, {"title": "6. Results analysis", "content": ""}, {"title": "6.1 Model performance", "content": "The analysis of metro passenger flow prediction models in Tables 2 and 3, using the Mean Absolute Percentage Error (MAPE) metric across prediction intervals of 10, 20, 30, and 60 minutes, provides a clear assessment of model robustness and scalability. As prediction intervals lengthen, MAPE values generally increase. Notably, CSP-AIT-Net consistently maintains lower MAPE values compared to other models, with MAPE rising from 36.019% at 10 minutes to 38.483% at 30 minutes, and then to 43.460% at 60 minutes. This performance demonstrates CSP-AIT-Net's robustness, particularly in adapting to long time intervals, where other models exhibit larger errors.\nFor example, at the 10-minute prediction interval, models can be categorized into traditional (ARIMA), machine learning (RF, GBDT), and deep learning (LSTM, GRU, HGNN, STAN, SARGCN, AFFN, CSP-AIT-Net). ARIMA, with the highest MAPE (69.554%), is clearly limited in capturing complex passenger flow dynamics. Machine learning models, such as RF and GBDT, show moderate improvement with MAPE values of 52.001% and 51.590%, respectively, yet they still struggle with the"}, {"title": "6.2 Performance on the spatial and temporal domain", "content": "The analysis of the prediction accuracy of Origin-Destination (OD) passenger flow across different spatial regions and temporal periods reveals distinct trends. As shown in Fig 5., for metro stations near universities and work-related areas, passenger flow patterns exhibit greater regularity due to the consistent nature of activities. University regions, for instance, follow a predictable schedule of classes and activities, resulting in stable flow patterns that are easier to model. Similarly, the underlying regularity in commuting behavior enhances the predictability of stations in work-related areas.\nConversely, areas characterized by mixed land use, such as Central Business Districts (CBDs) and transportation hubs, present more complexity in passenger flow dynamics. The diversity of activities in CBDs\u2014ranging from business, and retail, to leisure-leads to high variability in flow patterns, which makes prediction"}, {"title": "6.3 Ablation analysis", "content": "The ablation study in Tables 4, 5, and 6 highlights the necessity of three key components in CSP-AIT-Net: graph contrastive learning, asynchronous inflow, and the graph attention mechanism. These components significantly enhance model accuracy and robustness. Graph contrastive learning effectively enhances station semantic extraction. Without semantic attributes, the MAPE at the 10-minute interval is 47.245%, decreasing to 38.758% with basic encoding and 36.019% with graph contrastive learning. The consistent reduction across intervals demonstrates its role in capturing complex spatial features, leading to lower prediction errors."}, {"title": "7. Conclusion", "content": "This study proposed CSP-AIT-Net, a contrastive learning-enhanced spatiotemporal graph attention framework for short-term metro OD passenger flow prediction with asynchronous inflow tracking. The proposed framework significantly improved OD flow prediction accuracy by capturing the asynchronous characteristics of metro flows and integrating inflow and outflow data into a unified prediction model. The contrastive learning allows the model to better capture complex contextual patterns related to land use and passenger mobility. Experimental results on real-world metro datasets demonstrated that CSP-AIT-Net consistently outperformed state-of-the-art methods, achieving lower prediction errors across various time intervals and reducing the MAPE by over 4% compared to the best-performing existing models.\nThe innovation of CSP-AIT-Net lies in its integration of asynchronous inflow tracking, spatiotemporal graph attention, and contrastive learning for enhanced semantic representation of metro stations. Unlike traditional models that neglect temporal misalignment between inflows and outflows, our framework directly"}]}