{"title": "The Best Defense is a Good Offense: Countering LLM-Powered Cyberattacks", "authors": ["Daniel Ayzenshteyn", "Roy Weiss", "Yisroel Mirsky*", "Ben-Gurion University of the Negev, Israel"], "abstract": "As large language models (LLMs) continue to evolve, their potential use in automating cyberattacks becomes increasingly likely. With capabilities such as reconnaissance, exploitation, and command execution, LLMs could soon become integral to autonomous cyber agents, capable of launching highly sophisticated attacks. In this paper, we introduce novel defense strategies that exploit the inherent vulnerabilities of attacking LLMs. By targeting weaknesses such as biases, trust in input, memory limitations, and their tunnel-vision approach to problem-solving, we develop techniques to mislead, delay, or neutralize these autonomous agents. We evaluate our defenses under black-box conditions, starting with single prompt-response scenarios and progressing to real-world tests using custom-built CTF machines. Our results show defense success rates of up to 90%, demonstrating the effectiveness of turning LLM vulnerabilities into defensive strategies against LLM-driven cyber threats.", "sections": [{"title": "1 Introduction", "content": "Rapid advancements in artificial intelligence (AI) and large language models (LLMs) have drastically reshaped multiple sectors, enabling more efficient automation and decision-making processes. LLMs, in particular, have showcased remarkable capabilities in natural language understanding, content generation, and complex problem-solving, achieving previously unattainable results. As these models continue to evolve, their applied usage has expanded to touch on more critical areas like cybersecurity. Leveraging the reasoning and automation capabilities of LLMs, security researchers, and practitioners are beginning to explore how these models can be employed both defensively and offensively in the cybersecurity landscape [38].\nOne emerging application of LLMs is penetration testing, where they can simulate cyberattacks to find vulnerabilities in systems. Traditionally, this process requires skilled professionals, but with the advent of LLMs, much of the work can now be automated, even allowing unskilled personnel to perform tests [7]. In some cases, these models can even run tests without any human involvement, handling tasks like reconnaissance and exploitation on their own. This automation speeds up the process and allows for more frequent and scalable security assessments.\nWhile these advancements offer significant benefits for legitimate penetration testing, they also raise concerns about the potential for LLMs to be exploited by malicious actors [11,27]. As these models become more powerful and accessible, they could be used to automate cyberattacks, making it easier for adversaries to conduct sophisticated operations with minimal effort. The ability to execute complex attack strategies without human intervention\u2014such as exploiting vulnerabilities and escalating privileges could enable threat actors to launch large-scale attacks at unprecedented speed and scale [19]. This potential misuse of LLMs in offensive cyber operations poses a significant challenge for the cybersecurity community, as it lowers the barriers to launching attacks and increases the difficulty of defending against them.\nCurrently, no existing work specifically addresses defenses against the emerging threat of LLM-powered cyberattacks. While there are numerous studies focused on defending against traditional threat actors, these approaches do not target the unique vulnerabilities of LLMs. As a result, the current landscape of defensive strategies is not equipped to handle the rapid advancements in LLM technology, leaving them ill-prepared for future threats posed by autonomous, AI-driven attacks. This lack of future-proof solutions underscores the urgent need for defenses that specifically counter LLM-based threat actors.\nTo address this challenge, we propose a novel set of defenses against the emerging threat of LLM-driven cyberattacks. Our approach focuses on targeting and exploiting known vulnerabilities in LLMs to disrupt the attacking models. We introduce various strategies and techniques designed to delay, prevent, and detect attacks. Additionally, we out-"}, {"title": "2 Background & Related Work", "content": "In this section, we provide essential background and review relevant existing works to contextualize the defenses we present. First, we explore studies that utilize Large Language Models (LLMs) to automate penetration testing. Following this, we examine the known vulnerabilities of LLMs and discuss how these weaknesses can be leveraged by defenders to counter LLM-powered cyberattacks."}, {"title": "2.1 Automated Pentesting", "content": "In this section, we explore the current landscape of automated penetration testing. Although there are several works and industry solutions that claim to automate the process, a fully autonomous penetration testing system remains elusive [2, 23, 24,30]. This gap persists due to the need for deep vulnerability understanding and the ability to devise a strategic plan of action [7].\nLLM-powered penetration testing has the potential to bridge this gap by leveraging the advanced reasoning and decision-making capabilities of large language models. Unlike traditional automated tools, LLMs can autonomously analyze complex systems and adapt their attack strategies in real-time, offering a more dynamic and comprehensive approach. In this section, we will focus on how LLM-powered pentesting can address these challenges and reshape the future of cybersecurity.\nA study by [7] introduced PentestGPT, a framework that leverages LLMs to automate much of the penetration testing process. While human oversight is required to execute commands, PentestGPT automates key tasks such as parsing inputs, building testing strategies, and generating commands for tasks like brute-forcing SSH services. This semi-automated approach makes the tool accessible to users with less technical expertise, reducing the need for advanced knowledge.\nHappe et al. introduced HackingBuddyGPT [12], a fully automated LLM-powered tool for Linux privilege escalation, capable of achieving root access in under three seconds in some cases. However, the tool's primary limitation is the absence of consecutive reasoning, as it tracks only commands and outputs without capturing the LLM's thought process. As discussed in Section 6.1, this lack of reasoning hampers the tool's effectiveness in handling more complex tasks.\nXu et al. [35] and Huang et al. [14] introduced two advanced LLM-powered tools, AutoAttacker and PenHeal, which significantly improve automated penetration testing. Both tools offer end-to-end automation of the penetration testing lifecycle, building on earlier frameworks like PentestGPT and HackingBuddyGPT. By automating command execution and integrating reasoning capabilities, they enable fully autonomous attacks, covering all stages from reconnaissance to exploitation without human intervention."}, {"title": "2.2 LLMs & Their Vulnerabilities", "content": "Large language models (LLMs), while powerful, are not without their vulnerabilities. These weaknesses, such as inherent trust in input [10], biases [18], and memory limitations [36], can be exploited to manipulate or mislead the model. In this section, we explore these vulnerabilities and the various ways they can be exploited, including prompt injections [16], controlled code execution [6], and more novel techniques like luring, which we introduce as part of our defense strategies. In our research we notice the following vulnerabilities in LLMs that could be exploited by attackers but as we will see later also by defenders in a cyber attack environments.\nV1: Bias LLMs are prone to biases, often reflecting correlations present in their training data [36]. We introduce the novel concept of exploiting these biases during a cyber attack by deliberately steering the attacking LLM towards actions aligned with its biases. For instance, PentestGPT [7] demonstrated that LLMs tend to perform unnecessary brute-force operations, largely due to biases in the training data. Defenders could exploit this by providing the attacker with numerous seemingly promising-but ultimately futile-brute-force opportunities on the target machine.\nV2: Trust in User Input LLMs often place undue trust in the input they receive, especially when it appears to come from a credible source [10]. This vulnerability can be exploited in various ways, ranging from traditional prompt injection techniques [10, 15, 16] to luring the LLM into traps or even making it execute maliciously crafted code-effectively turning the attack against the attacker. We will delve deeper into these novel concepts and techniques in the following chapters.\nV3: Memory Limitations LLMs are constrained by limited memory, often leading to the omission of crucial details or even generating hallucinations when dealing with complex tasks involving multiple dimensions [4, 36]. Blue team defenders can leverage this weakness by introducing carefully designed challenges that complicate the LLM's task within the target environment. These obstacles do not enhance the security of the system itself but are intended to exploit the model's memory limitations, causing it to lose track of important context or misinterpret information.\nV4: Tunneled Search Large Language Models (LLMs), when guided appropriately, tend to address problem-solving tasks sequentially, focusing on one component thoroughly before moving on to the next, which can resemble a depth-first search (DFS) approach in reasoning [7, 33]. Defenders can exploit this by feeding the LLM misleading but enticing information, causing it to become stuck in a prolonged search chasing false leads."}, {"title": "3 Threat Model", "content": "In this section, we describe the attack model considered in this paper, which focuses on defending against LLM-powered agents. Several recent works have introduced fully automated frameworks for LLM-powered agents that operate without human intervention [1,9,12,14,22,25,35,37]. These frameworks demonstrate agents capable of autonomously performing every stage of an attack, from reconnaissance and planning to execution. With multiple integrated components, these agents are designed to run commands and carry out full-scale attacks entirely on their own, showcasing the potential threat posed by fully automated LLM-driven cyberattacks.\nWe assume an attack model in which a fully autonomous LLM-powered agent is capable of executing every stage of the attack lifecycle. Operating without human oversight, this agent independently carries out all phases of the attack: Reconnaissance, Scanning, Vulnerability Assessment, Exploitation, and Post-Exploitation [34]."}, {"title": "4 Defence Model", "content": "In order to effectively counter LLM-powered autonomous cyber agents, we have categorized our defensive strategies into three primary goals: Prevention, Detection, and Delay. Each goal represents a different phase of defense, with strategies aimed at disrupting or manipulating the behavior of the"}, {"title": "4.1 Prevention", "content": "The goal of Prevention is to stop the LLM from proceeding with its intended attack or to misdirect its objectives. We explore several strategies to achieve this:\nS1: Run Code on Threat Actor As previously mentioned, one vulnerability of LLM-powered cyber attack agents is their ability to execute arbitrary code. Defenders can exploit this by convincing the LLM to execute malicious code against the attacker (such as initiating a reverse shell) or connect the agent to a honeypot, which can also aid in detecting the presence of the attack and the LLM itself.\nAdditionally, this strategy can be leveraged by enticing the attacker with a seemingly promising script that ultimately wastes time, further delaying the attack.\nS2: Confuse By exploiting Memory Limitation 2.2 vulnerability, a defender can, by injecting misleading or conflicting information, cause the LLM to hallucinate [4], thereby corrupting its attack.\nS3: Blind Spot Certain injections can create blind spots in the LLM's processing, preventing it from recognizing or interacting with specific assets we aim to protect. This can be achieved by convincing the LLM that no vulnerabilities exist in the targeted asset. Another method involves using special characters to disrupt the LLM's ability to process the protected asset, causing it to overlook or misread the information. For example, this technique has been shown to corrupt translation tasks in NLP models [5]. We extend this concept to demonstrate that it can also disrupt an LLM's perception of input, leading to missing or incomplete output. By deploying these defenses, the LLM is more likely to either bypass the protected asset entirely or fail to process critical information correctly.\nS4: Trigger Safeguards Sensitive content strategically embedded within files and other assets can activate the LLM's internal safety mechanisms. When the LLM encounters such content, it may refuse to continue the at-"}, {"title": "S5: Change Objective", "content": "By injecting targeted content, we can shift the LLM's focus and prompt it to engage in role-playing, distracting it from its original attack goal. Studies on prompt injections, such as [10, 16], have explored this approach. We demonstrate that this strategy can be implemented both with and without performing prompt injection."}, {"title": "S6: Convince to STOP", "content": "By issuing carefully crafted suggestions or demands, we can cause the LLM to halt its attack entirely, effectively creating a denial-of-service (DoS) condition for the LLM-powered agent. This is achieved by exploiting the LLM's inherent compliance with specific instructions or ethical constraints."}, {"title": "4.2 Detection", "content": "The goal of Detection is not only to identify the attack itself but also to detect the presence of an LLM-powered attacker within the environment. This can be accomplished using carefully crafted strategies and techniques that exploit known vulnerabilities in LLMs. These methods are designed to differentiate human users from LLM-driven agents by leveraging tactics such as embedding special hidden characters, concealing payloads in overlooked assets (e.g., HTML pages), or making honeypots blatantly obvious-techniques that a human would recognize but an LLM may not.\nS7: Adversarial Building on the work by [39], we discovered that it is possible to prefix or postfix invisible characters to passwords, links, and other honeytokens, causing the LLM to use a different asset than human would. A human user, when copying text by double-clicking, selecting, or manually copying, will only interact with the visible characters. In contrast, the LLM cannot ignore these invisible characters and tends to convert them into visible characters, inadvertently appending them to the adjacent word. This results in the LLM using a modified honeytoken that differs from the one a human would use.\nS10: Run Code on Victim The LLM's ability to execute code can be extended to running scripts on the victim's machine. Defenders can leverage this by placing an obviously suspicious (to humans) script, luring the LLM to execute it. This allows for the detection of an attacker, particularly an LLM-powered agent, within the environment.\nAdditionally, this can be exploited to delay the attacker by having the LLM run time-wasting scripts or engage in brute force attempts, further slowing down its progress.\nS11: Lead to Honeypot By exploiting known biases in LLMs, particularly their tendency to trust provided information, we can lure them toward honeypot assets (such"}, {"title": "4.3 Delay", "content": "The goal of Delay is to slow down the LLM's progress, buying time for additional defensive actions. Many strategies overlap with those in the Detection phase, but their primary focus is on increasing the time taken by the LLM to complete its task:\nS7: Adversarial In addition to detection strategies, adversarial examples can be used to delay the LLM's attack. One notable example is the use of Sponge Examples [29], which are designed to consume the model's resources, causing delays and slowing the attack. Another example involves strategically placed word inaccuracies that preserve the overall semantic meaning but lead the model to fail [32].\nS8: Cause a Loop Search By embedding cyclic references within multiple seemingly valuable assets, we can cause the LLM agent to enter a continuous search loop. While the agent may recognize the cycle, it tends to persist in the loop due to our ability to exploit its biases and inherent trust in the information it processes.\nS9: Overwhelm Exploiting the Tunneled Search vulnerability in LLMs 2.2, Defenders, by overwhelming the agent with numerous promising attack vectors\u2014such as potential CVEs, credentials, files, or open ports-we can significantly slow down its progress and decision-making process."}, {"title": "5 Evaluation Setup", "content": "In this section, we examine the performance of our proposed defenses under black-box assumptions. We begin by evaluating them in single prompt-response scenarios."}, {"title": "5.1 Single Prompt Setup", "content": "For the single prompt black-box evaluation, we utilized PurpleLlama [31], a benchmarking tool specifically designed to measure the impact of prompt injections and content manipulation on large language models (LLMs). PurpleLlama provides a standardized framework for assessing how injected content influences LLM outputs. Each prompt was paired with a specific question for a judge LLM, which was used to determine whether the injection was successful. For this role, we used GPT-40, as it is the most robust model for evaluating prompt effectiveness and assessing the outcomes.\nAs discussed earlier in Section 4, we proposed 11 defensive strategies, each comprising several techniques. The complete relationships between these strategies and techniques are illustrated in Figure 2, while the payloads used for testing can be found in Appendix 4.\nOur evaluation targeted four distinct LLM powered penetration testing tools: PentestGPT [7], HackingBuddyGPT [12],"}, {"title": "6 Black box Approach", "content": "In this section, we examine the performance of our proposed defenses under black-box assumptions. We begin by evaluating them in single prompt-response scenarios."}, {"title": "6.1 System Prompt Analysis", "content": "In this phase, we evaluate each technique across different assets, language models, attack tools, and exploit methods (prompt injection and Luring). To facilitate this analysis, we developed a dataset of over 10,000 prompts, covering all possible combinations of techniques, assets, and exploit methods. This dataset allows for a comprehensive evaluation of the effectiveness and versatility of each approach.\nAs outlined in the Setup Section 5.1, we evaluated four distinct penetration testing tools. These tools are distinguished by their method of utilizing LLMs within their agents, specifically the difference between a summarizer and a nonsummarizer approach. In the summarizer approach, the system prompts instruct the LLM to summarize the output and decide the next action or command to execute. This method is used to pass essential information to subsequent steps of the attack and is employed by PentestGPT, AutoAttacker, and PenHeal [14]. In contrast, the non-summarizer approach directs the LLM to simply generate the next command to run without providing a summary. This approach is utilized by HackingBuddyGPT. We differentiate between these two methods because they result in fundamentally different outputs: one produces summarized text, while the other outputs only the command.\nWe present our Defence Success Rate of each strategy in compression to the injected asset when comparing the two exploit methods, while differentiating between different LLMs. These figures correspond to tools that use the summarizer-based approach, while This section provides a detailed evaluation of our attack by examining each contributing factor individually.\nModel. When using Prompt Injection as the exploit method, GPT-40 seems to be the most robust model against our proposed defenses. However, there are still combinations of techniques and assets that work phenomenally (for instance, using the Decoder strategy S5ii with multiple file assets).\nUsing prompt injections, GPT-40 and other models such as Sonnet and Gemini exhibit strong built-in safeguards. However, they are still prone to following user instructions, particularly when leveraging strategies like Changing Objective (S5) or Convincing to Stop (S6). Both strategies are effective due to an inherent vulnerability in LLMs\u2014Trust in User Input 2.2. Prompt injections are particularly powerful in altering the model's goals, proving more effective than the Lure method. By deploying sophisticated and well-crafted payloads, we can influence the model's objectives more successfully than by simply attempting to convince it through standard Lure techniques.\nAsset. Upon analyzing the results, we observe that while the choice of assets does impact the effectiveness of the defenses, the influence is relatively minor, particularly in the worst-case performance across all models. This effect becomes more evident when Lure is used as the exploit method. Interestingly, we found that simpler assets tend to yield better results. Assets with minimal content, such as empty web pages or empty text files, generally perform more successfully compared to those containing additional information, like login or upload pages. This suggests that when there are fewer distractions or unrelated data in the asset, the defenses are more effective at influencing the LLM's behavior.\nTechniques. The results indicate that the most effective"}, {"title": "6.1.1 Best Techniques and Assets Analysis", "content": "In the previous section, we evaluated numerous techniques and assets, testing each combination across multiple LLM models and attack tools. To identify the most effective defense strategies, we analyzed the worst-case performance for each pair of assets and techniques, selecting the lowest result across all models and tools. This allowed us to create a \"worst-case\" heat map, as from which we derived the top-performing techniques and assets.\nWe then organized these results into groups: 1x1 (the best individual technique and asset), 3x3 (the average performance of the top 3 techniques and assets combined), 5x5, and so on. the performance of these \"best\" techniques and assets across different LLM models (GPT-40, Sonnet 3.5, Gemini 1.5 Pro, and Llama-3.1 70B).\nThe goal of this table is to demonstrate that defenders who implement our recommended top techniques and assets can consistently outperform attackers, regardless of the LLM model being used in the attack. The table shows strong performance across various models, with high success rates for the best technique-asset combinations. For example, we can see that the 1x1 pair performs at over 90% effectiveness in all models. Even as we expand to 3x3 and 5x5 combinations, the average performance remains robust (above 80%), indicating that these defenses maintain effectiveness across"}, {"title": "7 Discussion", "content": "Our novel defenses are highly effective across a range of LLM models and attack scenarios, consistently leveraging vulnerabilities such as bias, trust in inputs, and memory limitations. Simpler assets, like empty files and web pages, were particularly successful, suggesting that reducing complexity in target environments enhances defense effectiveness. These strategies offer long-term protection by exploiting fundamental vulnerabilities in LLMs that are not easily patched, ensuring they remain adaptable as LLM technologies continue to evolve.\nThe taxonomy of strategies and techniques introduced in this paper offers a structured framework for defenders. By categorizing defenses under prevention, detection, and delay, we provide a systematic approach to countering LLM-based threats. This structure also highlights the scalability of the defenses, allowing them to be adapted for different attack scenarios and environments.\nHowever, while layering multiple defenses shows great promise, it also introduces potential limitations. As the number of defenses increases, so does the complexity of managing them. Although the exponential reduction in attack success rates is beneficial, this must be weighed against the added resource and management overhead required to maintain these defenses in real-world environments. Ensuring that the bene-"}, {"title": "7.1 Longevity: Robust Adversary", "content": "As adversaries become more familiar with defensive techniques like prompt injections, they are likely to implement countermeasures to bypass these strategies. In this section, we examine potential defenses that a sophisticated attacker might use and discuss the limitations of these approaches when facing advanced LLM-powered defenses."}, {"title": "7.1.1 Defenses Against Prompt Injections", "content": "One widely explored approach is the use of prompt classifiers or naive LLM-based detection systems. Tools like Meta's Prompt Guard [31] and similar LLM-based systems aim to detect and block malicious or injected prompts by analyzing their intent. However, despite their potential, studies [16] show that they suffer from a high rate of false positives, misclassifying legitimate inputs as malicious. This is especially problematic for adversaries using automated attack frameworks, as false positives can halt the attack process, leading to inefficiencies and delays.\nAnother method adversaries might employ is paraphrasing prompts to avoid detection. While paraphrasing can sometimes help evade prompt injection defenses, it often leads to a loss of critical context, resulting in incomplete or less effective responses from the LLM. This limits the adversary's ability to execute precise and impactful attacks."}, {"title": "7.1.2 Challenges for Robust Adversaries", "content": "Although these defensive measures could be implemented by attackers to avoid prompt injections, they come with inherent limitations. False positives, in particular, pose a significant challenge for adversaries relying on automated LLM systems, as they can unintentionally disrupt the flow of their attacks. Furthermore, these techniques do not address more sophisticated methods like Luring, where defenses are embedded within assets the LLM interacts with over time, making detection and avoidance much more complex.\nUltimately, while adversaries may develop robust defenses against prompt injections\u2014using classifiers, detection models, or paraphrasing techniques\u2014these strategies are far from foolproof. The dynamic nature of LLMs, coupled with evolving defensive strategies, ensures that attackers will continue to face significant challenges. For now, comprehensive solutions that completely neutralize advanced defensive techniques, such as luring, remain out of reach for even the most sophisticated adversaries, likely necessitating human intervention for success in the near future."}, {"title": "8 Conclusion", "content": "As the capabilities of large language models (LLMs) continue to advance, so too does their potential for malicious use in cyberattacks. This paper has introduced novel defense strategies that not only counteract these LLM-driven threats but also leverage the inherent weaknesses of such models to disrupt and neutralize attacks. By exploiting vulnerabilities such as bias, input trust, memory limitations, and a tunneled approach to problem-solving, we demonstrated that effective countermeasures can be implemented, achieving defense success rates of up to 90%.\nOur proposed defense taxonomy offers a structured framework for prevention, detection, and delay tactics, ensuring comprehensive protection against LLM-powered agents. Through extensive evaluation, the effectiveness of these strategies was confirmed across various models and assets.\nIn conclusion, this research underscores the importance of proactive defenses against the rising threat of autonomous LLM-driven cyberattacks. By turning LLM weaknesses into defensive advantages, we can mitigate the risks posed by these advanced technologies and stay one step ahead of malicious actors in the evolving cybersecurity landscape."}]}