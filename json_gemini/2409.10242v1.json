{"title": "HEDGING IS NOT ALL YOU NEED: A SIMPLE BASELINE FOR\nONLINE LEARNING UNDER HAPHAZARD INPUTS", "authors": ["Himanshu Buckchash", "Momojit Biswas", "Rohit Agarwal", "Dilip K. Prasad"], "abstract": "Handling haphazard streaming data, such as data from edge devices, presents a challenging problem.\nOver time, the incoming data becomes inconsistent, with missing, faulty, or new inputs reappearing.\nTherefore, it requires models that are reliable. Recent methods to solve this problem depend on a\nhedging-based solution and require specialized elements like auxiliary dropouts, forked architectures,\nand intricate network design. We observed that hedging can be reduced to a special case of weighted\nresidual connection; this motivated us to approximate it with plain self-attention. In this work, we\npropose HapNet, a simple baseline that is scalable, does not require online backpropagation, and is\nadaptable to varying input types. All present methods are restricted to scaling with a fixed window;\nhowever, we introduce a more complex problem of scaling with a variable window where the data\nbecomes positionally uncorrelated, and cannot be addressed by present methods. We demonstrate\nthat a variant of the proposed approach can work even for this complex scenario. We extensively\nevaluated the proposed approach on five benchmarks and found competitive performance.", "sections": [{"title": "1 Introduction", "content": "Sensors are heavily used in a wide number of time-series, industrial and measurement problems, such as for energy\nmanagement, industrial IoT, smart cities, agriculture, healthcare, smart homes, fraud detection, autonomous systems\netc. However, the reliability of the models (based on these sensor inputs) or the predictions of these models is only as\ngood as the reliability of data from the sensors. Many a times, due to faults, the sensors do not transmit data. In such\nscenarios, the input features to a model may vary, leading to the problem of variation in input size. Similarly, in data\ncollection under a multi-entity environment, the failure of some actors may directly influence the model's input feature\nspace. These challenging scenarios where the input space does not stay fixed, can be categorized under a single term\ncalled haphazard inputs [1]. This category of problems have not been typically studied until recent past, where Agarwal\net al. have tried to formalize the problem domain, identifying important shared characteristics among problems [1, 2].\nMaking better models to address haphazard inputs is an important area of research and bears significant economic value\nas well.\n\nThe typical machine learning models assume that the dimensions of the input space are fixed and do not change\nduring training or inference. However, haphazard inputs, challenge this assumption and force us to rethink about\ntypical machine learning models. This is a very new field and a major contribution has been made by Agarwal et al.\n[1, 2]. Their idea works by making per input models that interact with each other based on the hedging algorithm\n[3]. Hedging regulates the weight/contribution of each input towards model's final prediction. However, we have"}, {"title": "2 Proposed Approach", "content": "We use a self-attention based mechanism to capture the correlations among the input features. During training, each\ninput feature at time t is randomly masked and used for training. The proposed approach is straight forward as shown in\nFig. 1a. We first formally explain the haphazard input problem. Then the role of hedging is discussed, followed by the\nproposed approach.\n\nTask formulation. Assuming a faulty data source or a sensor, generating the input data stream D, the objective of\nonline learning with haphazard inputs is to correctly label the sample ft \u2208 D arriving at time step t, and simultaneously\nupdate the model parameters with the gradient of prediction loss at t. We assume that since the data source is faulty,\na subset of the sample ft is called auxiliary, denoted by f\u0165, and contains reliable information with a probability of\navailability of data, p. When p = 1, all information in for is available and fully reliable. The difference or base subset of\nft, denoted by fo, is mutually exclusive to fe and is calculated as the difference between ft and fro. Unlike f\u00e5, the data\nin fe is always available and reliable. The online model works in a predict-then-update manner where the prediction for\nft+1 cannot be made before the online model is updated with prediction for ft. In other words, the time-series data\noffers a feature to be learned. Each such feature consists of reliable (base) and unreliable (auxiliary) features. The\nmodel is tasked with online learning with this time-series data.\n\nHedging as weighted residuals. Hedging algorithm proposed by [3] has severed as the primary driving force of many\nhaphazard data processing methods. The main idea is to create an ensemble of classifiers where the outcome of each\nclassifier is weighted by a scalar value, as shown below:\n\n$\\hat{y}_{t} = \\sum \\alpha_{i} \\hat{y}_{ti}$ (1)\n$\\hat{y}_{ti} = \\sigma (\\Theta_{i} \\sigma (W_{i} h_{i-1}))$ (2)"}, {"title": "3 Experiments", "content": "This section presents the experimental protocol and details, the dataset used, results, and analysis. We use state-of-the-art\nmodels for processing haphazard inputs [1, 2] and other deep learning methods [3, 14] for comparison to the proposed\nmethod. Average error i.e. number of incorrect predictions out of total samples tested is used as the evaluation metric.\nAlong with that, the cross entropy (CE) loss is used to train and test the models. Rarely, macro and micro F1 scores and\naccuracy have been used as alternative metrics. The encoder consists of a transformer with 6 blocks, a dropout of .15\nis used with layer normalization and two liner layers on the outputs of self-attention. If not reported otherwise, the\nlearning rate was 0.0001 with Adam optimizer. All experiments were repeated 20 times and their mean and standard\ndeviations have been reported.\n\nDatasets Five datasets have been used in our experiments. These are (a) Italy power demand dataset [15] (b) german,\nsvmguide3, magic04, a8a [16]. All these datasets contain time-series data from different domains and applications.\nMore details can be found in Agarwal et al. [1]."}, {"title": "3.1 Conclusion", "content": "This work propose a simple yet effective algorithm for haphazard input based problems. It shows that the more\nwidely used hedging based algorithm can be well estimated by a more generic version \u2013 self-attention, leading to\nbetter generalization, scalability, ease of use and adaptability to different modalities. The proposed methods achieved\ncompetitive performance to other state-of-the-art methods. We further introduced a more challenging case of haphazard\ninputs where the features values are positionally non-correlated. We showed that a feed-back based modification of\nthe proposed HapNet may address this challenging case as well. Extensive ablations on the five datasets revealed the\neffectiveness of the proposed HapNet approach. However there are some limitations to the proposed work. Limitation.\nAlthough the HapNet approach shows competitive performance, however, in some cases the other methods outperform\nit. It shows that a more optimized variant of HapNet can be developed. In place of LSTMs in HapNetPU, some other\nnetwork like GRU or transformers may be used to further improve its performance. Future scope. Our approach can\nbe extended to images and videos as the transformer architecture is omnipresent and the data can be easily divided.\nDifferent techniques may be used to replace the feed-back module in HapNetPU to further improve its performance."}]}