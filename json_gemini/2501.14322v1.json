{"title": "Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination", "authors": ["Eric Nyiri", "Olivier Gibaru"], "abstract": "Machine learning methods are solving very successfully a plethora of tasks, but they have the disadvantage of not providing any information about their decision. Consequently, estimating the reasoning of the system provides additional information. For this, Layer-Wise Relevance Propagation (LRP) is one of the methods in eXplainable Machine Learning (XML). Its purpose is to provide contributions of any neural network's output in the domain of its input. The main drawback of current methods is mainly due to division by small values. To overcome this problem, we provide a new definition called Relative LRP where the classical conservation law is satisfied up to a multiplicative factor but without divisions by small values except for Resnet skip connection. In this article, we will focus on image classification. This allows us to visualize the contributions of a pixel to the predictions of a multi-layer neural network. Pixel contributions provide a focus to further analysis on regions of potential interest. R-LRP can be applied for any dense, CNN or residual neural networks. Moreover, R-LRP doesn't need any hyperparameters to tune contrary to other LRP methods. We then compare the R-LRP method on different datasets with simple CNN, VGG16, VGG19 and Resnet50 networks.", "sections": [{"title": "1 LRP Introduction", "content": "XML methods are especially useful in safety-critical domains like finance, vision control, robotics [10] [9] [25] [27] [8] or gesture assistance [28] [26] [29] where engineers (resp. practitioners for medicine) must know exactly what the network is paying attention to. Other use-cases are the diagnosis of network behavior/misbehavior and improving the network architecture. The umbrella term eXplainable Machine Learning or explainable AI can be used for many different techniques [13].\nLIME [23] and RISE [22] methods consider networks as black-boxes. They produce relevant regions in saliency maps in order to explain the decision of a network. As explained in [4], these techniques estimate importance by probing the model with randomly masked versions of the input image. They then obtained the corresponding outputs, which are used to produce the map. The advantage is that they don't need to know the structure of"}, {"title": "1.1 DAGs for feed-forward networks", "content": "The typical setting for supervised learning is to find an approximation of an unknown function f, given a training data set of its point values: {x(i), f(x(i))}i=1,...,m where x(i) \u2208 Rd and f(x(i)) \u2208 RP. For example, in classification problems for images, d is the number of pixels \u00d7 the number of channels in the images, typically somewhere in the range of 103 to 106, and for videos it is even higher."}, {"title": "Definition 1 (Feed-forward neural network)", "content": "A feed-forward neural network Nis defined by a directed acyclic graph G = (V,E) where\n1. V is a finite set of vertices and E is a finite set of directed edges\n2. I is the subset of 'source-input' vertices of V which do not have any incoming edges from E\n3. O is the subset of 'sink-output' vertices of V which do not have any outgoing edges from E\n4. for all directed edge e \u2208 E, we associate a weight we \u2208 R\n5. for all vertex v \u2208 V\n(a) v must belong to at least one edge e \u2208 E\n(b) we associate to v a neuron output\n\u03a7\u03c5:= \n{   \u03c3\u03c5 \u03b4\u03c5 +\n\u03a3\n\u03bd' \u2208Prev(v)\nWe.Xv'\nthe given input value for vertex v\nif v \u2208 V \\ I\nelse\nwith\n\u2022 \u03c3\u03c5: R \u2192 R being an activation function and bv \u2208 R being a bias,\n\u2022 Prev(v) being the subset of all the previous vertices of any edge in E terminating at v\nThe vertices in H := V \\ {I, O} are called hidden.\nAs we can see, each vertex is associated to one neuron. The DAGs admit topological sorting and no cycles [35]. This allows us to deduce relationships in a logical order. Consequently, for any given feed-forward neural network, we can compute the list of all paths from one input vertex of I to any output vertex of O and conversely. Moreover, for any e \u2208 E and \u03bd\u2208 V \\ I, the parameters {we, bu} are then trainable.\nIt's a very general definition of feed-forward neural networks. We will see that it encompasses virtually all network architecture used in practice: fully connected, convolutional networks, residual networks, Unet, U2net, Transformers. In order to respect the Definition notations for the Softmax function implementation, we have to add intermediate layers of neurons with the appropriate activation functions, weights and biases (see figure 1)."}, {"title": "Proposition 2 (Feed-forward neural network output)", "content": "Let us given a feed-forward neural network N (see Definition 1) and x \u2208 Rd with d = card(I) then\nSN(x) := {xv}v\u2208O\nis a function mapping Rd into RP where p = card(O), called the output of N.\nThe performance of these architectures is evaluated on new draws of data in the sense of probability or expectation of accuracy on said draws. Note that in this setting, there is no model class assumption on the function f giving rise to the data, and so there can be no provable bound for the error. What is done in practice is to give an empirical bound based on checking performance from a lot of new random draws which are referred to as test data. We can now consider fully-connected feed-forward neural networks."}, {"title": "Proposition 3 (Fully-connected Neural networks)", "content": "Let a feed-forward neural network N (see Definition 1) where the vertices are organized into layers. Each vertex of one layer is connected via outgoing edges to all vertices from the next layer and to no other vertices from any other layer. If X(1) \u2208 RN(1) is the vector of neuron outputs corresponding to nodes v in layer 1 with N(1) = card({v \u2208 V ; v belonging to layer 1}), l = 0, ..., L then the output of N is given by\nSF(X(0)) = \u03c3(L) \u03bf\u03b1(L) \u03bf\u03c3(L\u22121) \u03bf\u03b1(L\u22121) \u03bf ... \u03bf\u03c3(0) \u03bf\u03b1(0) (X(0))\nwhere X(0) is made from all the vertices belonging to I, a(\u00b9)(y) = W(1)y+b(1) being the linear activation vector of layer l and W(1) \u2208 RN(1)\u00d7N(1-1) (resp. b(1) \u2208 RN(t)) is made of each weight values (resp. bias values) issued from edges terminating in each vertex of layer 1. The \u03c3(1) activation function is defined to act on any vector coordinatewise.\nWhen all the activation functions are the ReLU function, we then obtain Continuous Piecewise Linear (CPwL) functions [5] in the outputs."}, {"title": "Proposition 4 (Convolutional Neural networks part)", "content": "Let a feed-forward neural sub-network N (see Definition 1) where the vertices are organized in K\u0131 grids of size I\u012b into 2L layers. Therefore, each vertex v(i,k) has a coordinates (i, k) in a layer l, i \u2208 I\u00a1 being the position of the vertex in a grid, k \u2208 K\u0131 being the grid number (figure 2). Each vertex i of one grid of one even layer 2l + 2 is connected via incoming edges to each vertex i in all grid from the previous odd layer 2l + 1 and to no other vertices from any other previous layer. Each vertex (i,k) of one grid of one odd layer 21+1 is connected via incoming edges to each vertex (i + m + P\u2081,k), \u2200m \u2208 NI\u0131/m < 2M\u2081 with M\u2081 \u2208 NI\u0131 and P\u2081 \u2208 NI\u0131, from the previous even layer 21 and to no other vertices from any other previous layer. If X(1) \u2208 RI\u0131 K\u0131 is the tensor of neuron outputs corresponding to nodes v(i,k) in a layer 1, l = 0, ..., 2L then the output of N is given by\nS(X(0)) = (2L\u22121) \u25e6a(2L\u22121) \u25cba(2L\u22122) \u03bf... \u03bf \u03c3(1) \u03bf\u03b1(1) \u03bf\u03b1(0) (X(0))\nwhere X(0) \u2208 RIo\u00d7K is made from all the vertices belonging to I. a(21) (y) = W(21) y + b(21) is the activation of layer 21 and W(21) \u2208 RI21\u00d7K21\u00d7K21-1 (resp. b(21) \u2208 RI21\u00d7K21) is made of each weight values (resp. bias values) issued from edges terminating in each vertex of layer 21. a(2l+1)(y) = W(2l+1)y+b(21+1) is the activation of layer 21+1 and W(21+1) \u2208 []R(1+2M21+1)\u00d7K21+1 (resp. b(21+1) \u2208 RK21+1 ) which defines the weight values (resp. bias values) of edges terminating in each vertex of layer 21 + 1. The relationships between the layers are:I(21) = I(21\u22121),\nI(21+1) = I(21) - M(21+1) and K(21+1) = K(21)\nIn practice, a convolutional layer brings together one even layer followed by one odd layer. Note that Average pooling layers and Max pooling layers are special cases of convolutional layers."}, {"title": "Proposition 5 (Sequential Convolutional Neural networks)", "content": "A Sequential Convolutional Neural networks N such as VGG16 or VGG19, is defined by a Convolutional Neural networks part, followed by a Flatten layer if necessary, and after by a Fully-connected Neural networks. The Flatten layer allows to convert the tensor from the Convolutional part to a vector which can be used as input in the Fully-connected network. The output of N is then given by\nSF (X(0)) = S \u2022 Flatten\u25cbS(X(0))\nwhere X(0) \u2208 RI0\u00d7K is made from all the vertices belonging to I and Flatten is a Flatten layer."}, {"title": "Proposition 6 (ResNet-Block type Neural networks)", "content": "In [11], authors used residual learning blocks (see figure 3) in order to avoid optimization problem when training. They defined a building block by y = F(x) + x where x is the block input and y the output. If x and y have the same size, a simple sum is done at the end of the block, otherwise a linear projection W is performed onto x before summing with y.\nAs F is a Convolutional Neural Networks Part, for any input vector X(0), the output of a ResNet Neural network can be defined by\nSR(X(0)) = S \u2022 Flatten \u25e6 SW(g-1) \u25cb \u03c3(q-2) \uff61G(q\u22122) \uff61 ...\u03c3\u00b9 \uff61G(1) \u2022 SW (0) (X(0))\nwhere G(1) (x) = S(1)(x) + W(1)x is a ResNet-Block"}, {"title": "1.2 LRP background", "content": "The LRP methods decompose the output of a nonlinear decision function in terms of input variables, forming a vector of input features scores (i.e. contributions) that constitutes the 'explanation'. For the decomposition process, LRP methods usually use back propagation (from one network output to its inputs) in order to calculate a contribution score of each input. There are different back propagation rules for LRP. For LRP0 method, the contribution is defined in the following form:"}, {"title": "Definition 7 (LRP0)", "content": "According to the definition in [2], the contribution of neuron i of layer l to the activation/output xlj+1) of neuron j from layer l + 1 is given by\nDij = WiXli\u03a3kWlXlk(1)\nResulting scores can be visualized as a heatmap of the same dimensions as the input. Initially, LRP is a conservative technique, meaning the magnitude of any output is conserved through the backpropagation process and is equal to the sum of the relevance map of the input layer; this is call conservation law [2]. The property holds for any consecutive layers, and by transitivity for the input and output layer."}, {"title": "Definition 8 (Conservation Law)", "content": "According to the definition in [2], the contribution law states that\n\u03a3N(l)zil) = xlq+1) for all l \u2208 {0,q}\nwhere q is the depth of the neural network, zil) = (zi,j)i,jel,l+1 of length N(1) is a vectorization of z(l) and x(q+1) is one selected output value."}, {"title": "1.3 LRP0 drawback", "content": "The problem of the LRP0 method is mainly due to the fact that the calculation of the contribution zi,j (see above) is numerically very sensitive to the value of \u03a3kWk,jxlk, as we can see in figure 4, for some pixels of layer I which do not contribute a lot to the final classification. The LRP0 formula (1) may admit small denominators \u03a3kWk,jxlk \u2248 0. Consequently, the surrounding pixel contributions in this layer I can be amplified positively and negatively. Hence, their contributions become non-representative [2].\nTo illustrate this effect, we have trained a simple network composed of three dense layers with respectively 256, 128 and 10 neurons using a modified MNIST dataset with 50x50 images where the background is made of non-uniform gray values. After 50 epochs, the network reached an accuracy of 0.925 for the training dataset and 0.91 for the validation dataset. The LRP0 is then applied to it to evaluate the effectiveness of the method. As we can see in figure 4, the contributions obtained are not so relevant.\nWith deeper CNN networks, for example a Keras-VGG16 network whose coefficients are pre-trained on ImageNet and RGB images, the results without any post-processing step is quite disappointing (see figure 5). Consequently, direct LRPO calculations for a given network do not provide a representative image of contributions."}, {"title": "1.4 LRP modified formulas", "content": "In order to avoid this problem, some LRP-methods use modified formulas to finally produce post processed heatmaps. Usually, these methods are combined to get the best results. For images, the rule of choice has been introduced in [20]. For lower-layers, the explanation has to be more smooth and less noisy since these layers are already very close to the relevance map we humans will see and have to make sense of. For this purpose, LRP-y (see equation (3)) rule that disproportionately favors positive evidence over negative evidence is used:\nZi(l)j = (Wk,jxk(l))+(\u03a3k(Wk,jxk(l))+\u03b3x(l+1)j\nwhere x+ denotes the function max(0,x).\nFor higher-layers, either LRP-y or LRP-e rule are used. They remove some of the noise from the relevance map. In particular, LRP-\u20ac (see equation (4)) rule aims to solve the problem of gradient noise by introducing a small positive term, e, to the denominator in"}, {"title": "R-LRP XML", "content": "order to absorb some relevance when contributions are weak or contradictory.\nZi(l)j = Wi,jx(l)i(\u03a3k(Wk,jxk(l))j+eX(l+1)j\nIn [2], LRP-\u03b1\u03b2 rule is used to treat positive and negative contributions asymmetrically. This alternate propagation method also allows controlling manually the importance of positive and negative evidence, by choosing different factors a and \u1e9e with a + \u03b2 = 1:\nz(l)i,j = \u03b1\u03a3kWk,jxk(l)+x(l+1)j\u03a3kWk,jxk(l)++\u03b2\u03a3kWk,jxk(l)\u2212x(l+1)j\u03a3kWk,jxk(l)\u2212\nwhere x\u00af denotes min(0,x).\nIn [15], the authors introduced LRP-z+ or LRP-w\u00b2 which give similar results than LRP-a\u00df and LRP-y.\nConservation law (see Definition 8) for the global relevance in between layers can only be guaranteed with LRP-y and LRP-\u03b1\u03b2 when a + \u03b2 = 1. In [2], authors used \u03b2 = \u22121 and so a = 2 in order to get usable heatmaps as they assume that \"for rectified linear neurons this is a reasonable assumption because they fire only when their input is positive\". The main issue of these modified methods is that we have to adjust hyperparameters (\u03b3, \u03b5, \u03b1, \u03b2, ...) and/or to mix these methods depending on the dataset."}, {"title": "2 Relative LRP or R-LRP method", "content": "2.1 Introduction\nIn this section, we introduce new relative input/pixel contributions. We call this method R-LRP where \"R\" stands for Relative. The objective is to have no hyperparameter to tune and no need to mix LRP methods.\nAs mention above, equation (1) doesn't give a representative relative magnitude of the contributions when \u03a3kWk,jxlk \u2248 0. Indeed, in these cases, input contributions zij may have huge values whereas outputs x(l+1)j are close to zero. To avoid this phenomenon, we propose this new backward contribution rule definition."}, {"title": "Definition 9 (R-LRP)", "content": "According to Definition 1, let us give the output k of the final layer then the contribution of neuron i of layer l to the output xl+1)j of neuron j from layer l + 1 is defined by\nz(l)i,j,k = 1M(l+1)Wlij,k(xl+1)jKi\u03a3(l)i,j,k\nwhere 1(l+1) is the input number of neurons connected to neuron j. We then define the relative contribution of neuron i from layer l on any neuron output k of the final layer by\nzi,k(l) = Card(J)N(l+1)\u03a3(l)i,j,k"}, {"title": "R-LRP XML", "content": "where N(l+1) is the number of neurons in layer 1+1 and J_ being the set of all neuron indexes from layer 1+1 connected to neuron i from layer l. We set z(q+1)k,k = Xk x(q+1)k for any neuron k in the final layer. Consequently, z(0)ik is the contribution of the input/pixel i on the output neuron k of the final layer.\nThe main advantage of this formula is that there is no division by small values, nor any hyperparameter to set or tune. The division by the number of neurons in the current layer avoids some overflow problems but does not affect relative contributions.\nThe total contribution of an input to an output, up to a factor that is the same for all inputs, is calculated independently of other inputs. For networks which can be associated with a DAG, the sum of the inputs' contributions is equal to the selected output (up to a factor for our case). Therefore, up to a real factor we respect the classical LRP-methods conservation law (see Equation 2) because an input contribution is relative to an output value and thus relative to other inputs' contributions. In this sense, the contributions are relative. Moreover, calculations are lighter and, therefore, faster."}, {"title": "Proposition 10 (R-LRP contribution and activation)", "content": "According to the previous definition, R-LRP only keeps input which do not inactivate neuron(s) in the network. The activation threshold of a neuron 1l+1) is specified by the bias term, thus, equation (6) makes it possible to take into account the contribution of neuron although \u03a3kwk,ixlk = 0"}, {"title": "2.2 Effective R-LRP calculations from CNN layer components", "content": "In practice, efficient calculations of the contributions takes into account the layer component in order to simplify/rearrange equation 7"}, {"title": "Proposition 11 (R-LRP Dense)", "content": "In the case of Dense layers, the contribution of a neuron i for the output k can be calculated by\nZi,k(l) = 1N(l+1)\u2211j\u2208JWi,jX(l+1)j\u03a3j,k\nwhere Card(J) = N(l+1) and N(1) = M(l+1)j"}, {"title": "Proposition 12 (R-LRP Convolution)", "content": "A convolution layer with pool-size (P1, P2), filter mask W = {W(p1,p2) \\ P1 \u2208 P1,P2 \u2208 P2} and strides (s1,s2) acts like a grid of neurons where each neuron (j1, j2) have P1xP2 entries (see Definition 4) and the output are\nX(l+1)(j1,j2) = \u2211p1p2W(p1,p2).Xl(j1.s1+p1),(j2.s2+p2)\nwhere the values Xl(j1.s1+P1),(j2.s2+p2) are neuron/pixel contribution from layer 1.\nThus, from equation (6), the contribution of a neuron/pixel (i1,i2) of layer l to the output xl+1)(j1,j2) of the neuron (11,12) for the final output k is defined by\nZ(l)(i1,i2),(j1,j2),k = 1P1.P2\u2211p1.p2W(p1,p2)Z(l+1)j,k"}, {"title": "R-LRP XML", "content": "where i\u2081 = j1.81 + P1_and i2 = j2.82 + p2_The total contribution of a neuron/pixel (i1, i2) of layer l for the final output k is obtained from equation (7):\nZ(l)(i1,i2),k = Card(J)N(l+1)1P1.P2Xl(i1,i2)\u2211i,j,k(l+1)\nSince, 1P1.P2 is a constant and Xl(i1,i2) is independent of J, we can rewrite equation (11) as\nZ(l)(i1,i2),k = Card(J)N(l+1)1P1.P2Xl(i1,i2)\u2211W(p1,p2)Z(l+1)j,k\nwhere . \u2211W(p1,p2)Z(l+1)j,k\nis calculated with a transposed convolution [7]. This definition is also available for convolutions with padding."}, {"title": "Proposition 13 (R-LRP Average and Max Pooling)", "content": "An Average Pooling2D layer with pool-size (P1, P2) is a convolution layer where the filter mask is defined by\nW = {W(p1,p2) = 1P1.P2with p1 \u2208 P1,P2 \u2208 P2}.\nThus in case of pooling layers, we can use equation (12) with the corresponding filter mask. A Max Pooling2D layer can also be considered as a convolution layer with a filter mask\nW = {W(p1,p2) = 1if (k1,k2) \u2208 argmaxx(i1.s1+p1,i2.s2+p2) otherwise 0}.\nAs the filter mask depends on (i1,i2), the calculation of\n\u2211Z(l+1)j,k\nin equation (12) cannot be obtained by a simple transposed convolution but by an adapted one. The argmax function returns all the indices of the maximum values in a list or in an array."}, {"title": "2.3 R-LRP calculations with ResNet", "content": "In order to get the contributions at the input of a block, one must calculate the contributions of the residual part (Sf(x)), the contributions of the skip connection part (Wsx) and then sum the two contributions. About ResNet, in [11], authors wrote that:\n1. if the input and output have the same size, a simple sum is done at the end of the block. So for the skip connexion part, the contributions of the input are the same as the output.\n2. if the output and input sizes are different, they perform a linear projection Ws by the shortcut connection to match the dimensions : y = Sf(x) + Wx. Practically, the projection is done by a 1x1 convolution with a downsampling stride of 2. For the skip connexion part, the contributions of the input of the block are obtained by oversampling the output contributions using eq. 12"}, {"title": "E. NYIRI AND O. GIBARU", "content": "For the residual part, as Sf(x) is a Convolutional part, R-LRP calculation is used. While summing the contributions, we must take care of the conservation law (definition 8) because if residual's and skip connection's contributions don't have the same magnitude, artifacts occur in the input contributions result (see Figure 6). We can also see this problem in [16, figure7] where the artifacts are mostly due to the oversampling in the skip part."}, {"title": "E. NYIRI AND O. GIBARU", "content": "R-LRP contributions are relatives, thus we have to \"normalize\" the residual and the skip connection contributions before summing them. From the conservation law (equation 2), we can state that,\n\u2211N(l)(l)p = \u2211N(m)z2m),(l, m) \u2208 {0, ..., q} \u00d7 {0,..., q}\nThus, the normalization consists in having the same value for sums at the end and the beginning of each part of the residual building block. This normalization ensures that no artifact occurs due to separate evaluation of each part of the residual block."}, {"title": "Proposition 14 (R-LRP Residual Learning Block)", "content": "A learning block is defined by\ny = S(x) + Wsx\nwhere S(x) defines a residual part that uses CNN layer components and W\u025bx is a skip connection with a convolution Ws. Based on the conservation law property, we then define the contribution of the inputs of a learning block by\nZ1i,k(l) = 2zi,k(l) + z1im)\u2211N(1)z1zi,k(l)1im)\nwhere (l)(resp. z1m)) is the contribution at the beginning (resp. at the end) of the block using the skip connection part path (Wsx) and z1i,k(l) (resp. z2m) ) is the contribution at the beginning (resp. at the end) of the block using the residual part path (S(x)) (see Figure 7)."}, {"title": "3 Evaluations with MNIST-like dataset", "content": "In order to evaluate the LRP methods, we have designed a network (using Python and the Keras library) composed of two convolution layers (32 filters each), a flatten one and a 128-dense layer before the output dense layer. After 5 epochs training iterations with batch"}, {"title": "R-LRP XML", "content": "size of 128, we obtained an accuracy of 98%. For each image in the modified MNIST test dataset, we have created images corresponding to, respectively, 1%, 5%, 10%, 15%, 20%, 25%, 40%, 50%, 60%, 75%, 80%, 85%, 90%, 95%, 99% of the most relevant pixels for each LRP method (see figure 8 for example). In this article, we only show results for test image dataset which are identified in their effective category because for train dataset images, we obtain similar results and for misidentified images the results are symmetrical."}, {"title": "E. NYIRI AND O. GIBARU", "content": "The evaluation gives the accuracy of the test set based on the percentage of the most relevant pixels in the images for each LRP methods with commonly used parameter values (see figure 9). In all graphs, rlrp stands for R-LRP, lrp0 stands for LRP-0 method, lrp_eps01 (resp. lrp_eps001) stands for LRP-e method with parameter e=0.01 (resp. \u20ac=0.001), lrp_ab0505 (resp. lrp_ab21) stands for LRP-a\u00df method with parameters a=0.5 and \u1e9e=0.5 (resp."}, {"title": "R-LRP XML", "content": "a=2 and \u03b2=-1) and lrp_gamma25 stands for LRP-y with parameter y=0.25. Accuracy is calculated using the trained simple CNN network according to the founded category and the pixels of the images, whose contributions are less than the percentage, in black. We prefer this method because masking relevant pixels in images can induce a bias due to the shape of the mask itself and guide the choice of the network."}, {"title": "E. NYIRI AND O. GIBARU", "content": "Although all these methods achieve quickly more than 95% of success, we naturally want the contributions of pixels to focus on the object in the input image and therefore, will have a higher value on the corresponding input unlike the other pixels which must have lower contributions. With modified LRP formulas, this seems to not be completely the case (see 8). Moreover, these methods need to adjust some hyperparameters (\u03b3, \u03b5, \u03b1, \u03b2, ...) which are image sets dependent and require human experts in order to be understandable [21]."}, {"title": "3.1 Using positive and negative contributions", "content": "In figures 9, we can see that in between about 20% and 80% of most relevant pixels, all the accuracies decrease. This seems to show that pixels with negative contributions are also significant.\nThus, we have evaluated our R-LRP methods using absolute value of the contributions. This means that the most relevant pixels in an image (see figure 10) are defined by the absolute value of the contributions of each pixel calculated with equation (6).\nIn figure 11, we can see that with 5% of the most relevant pixels in absolute values, the accuracy reaches over than 98% for each category. Although the scores are worse with small percentage of most relevant pixels by using absolute value contributions than using direct contributions, it seems that pixels with negatives contributions are closest to the digit in the image (figure 10).\nThis definitely shows the relevance of our R-LRP method to define the contribution of the pixels in the decision process with our simple \"MNIST-network\" (which contains dense and convolution layers) for MNIST dataset."}, {"title": "4 R-LRP validation with cat vs dog dataset", "content": "In order to validate our new formula 6, we decided to use a more complex CNN than the one used for MNIST dataset and a more complex dataset. To do so, we have defined a new CNN architecture using VGG16 convolutional part pre-trained on ImageNet and a custom fully connected part (Dense(128,\u2019relu') connected to output Dense(2, 'softmax')) trained with RGB images."}, {"title": "4.1 Training process", "content": "To train the new CNN, we have downloaded the 25000 RGB images \"Charm Myae Zaw and all\" cat-dog dataset from Kaggle. The dataset contains 20000 images for training and 5000 images for testing, each image size is 224x224x3. The training program was written in Python using the Keras library and ImageDataGenerator(width_shift_range=0.1,"}, {"title": "E. NYIRI AND O. GIBARU", "content": "height_shift_range=0.1, horizontal_flip=True) for augmentation data. Due to hardware limitations, training only has 50 epochs but reaches an accuracy of 98% over test data."}, {"title": "4.2 Contributions using R-LRP formula vs other LPR methods", "content": "In a first step, we have calculated mask images (images of contributions) for all images from the test dataset, each mask image contains a percentage (1,5,10,15,20,25,40,50,60,75,80,85,90,95,99) of the highest contribution values over the three channels. Then, like for previous tests, we create, for each percentage, a dataset where images only contain the most relevant inputs, other inputs are set to 0."}, {"title": "4.3 Evaluations", "content": "As we can see in figure 12, R-LRP avoids having non-relevant inputs in high contribution values. Therefore, we have calculated the accuracy of the test dataset at each percentage"}, {"title": "E. NYIRI AND O. GIBARU", "content": "using masked images in order to evaluate the LRP methods (figure 13).\nR-LRP outperforms all the other methods with an accuracy of 0.81 with only 15% of the inputs of each image, whereas the over best score with the same percentage of inputs is 0.62."}, {"title": "4.4 Evaluations using absolute value contributions", "content": "In figure 13, we can note that, like for MNIST dataset, inputs with negative contribution values seem to be significant since after 80% the scores increase sharply. So, we have"}, {"title": "E. NYIRI AND O. GIBARU", "content": "4.5 Evaluations with pixels\nIn the previous tests, we have used input"}]}