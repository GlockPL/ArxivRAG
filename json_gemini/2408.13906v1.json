{"title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models", "authors": ["Yeji Park", "Deokyeong Lee", "Junsuk Choe", "Buru Chang"], "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated responses fail to accurately reflect the given image pose a significant challenge to their reliability. To address this, we introduce ConVis, a novel training-free contrastive decoding method. ConVis leverages a text-to-image (T2I) generation model to semantically reconstruct the given image from hallucinated captions. By comparing the contrasting probability distributions produced by the original and reconstructed images, ConVis enables MLLMs to capture visual contrastive signals that penalize hallucination generation. Notably, this method operates purely within the decoding process, eliminating the need for additional data or model updates. Our extensive experiments on five popular benchmarks demonstrate that ConVis effectively reduces hallucinations across various MLLMs, highlighting its potential to enhance model reliability. Source code is available at https://github.com/yejipark-m/ConVis", "sections": [{"title": "Introduction", "content": "Multimodal Large Language Models (MLLMs) are advanced language models capable of understanding both images and text, such as image captioning and visual question answering (VQA). While MLLMs have achieved significant success that utilize both visual and textual information, the issue of hallucination, where the models generate responses that do not align with the given image, has greatly undermined their reliability. This problem poses a significant obstacle to adopting MLLMs in critical fields where reliability is crucial. For instance, in medical applications, it could lead to incorrect diagnoses, while in MLLM-based autonomous systems, it might result in erroneous interpretations. Recent research has been actively conducted to address this. WoodPecker and LURE reduce hallucinations by post-processing the generated responses. Datasets such as LRV-Instruction and RLHF-V have been proposed to mitigate hallucinations through instruction tuning of MLLMs. However, these studies often rely on external APIs like GPT-3.5, require costly human feedback collection, and necessitate additional training of MLLMs.\nIn contrast, this paper focuses on decoding strategies that reduce hallucinations by intervening solely in the decoding process, without the need for additional data or model training. The following studies fall into this category: OPERA imposes penalties on token generation that does not reference visual tokens. VCD creates contrasting distributions using distorted images to reduce the model's reliance on statistical biases and priors that lead to hallucinations. HALC corrects hallucinations by leveraging cues provided by visual information from various fields of view.\nIn this study, we propose a contrastive decoding method called ConVis (Contrastive Decoding with Hallucination Visualization), which can be applied to any existing MLLM without additional training. Inspired by the previous work, ConVis leverages text-to-image (T2I) generation models, specifically Hyper-SDXL, to capture visual contrast signals. The process begins with the MLLM generating a caption for the input image, after which the T2I model reconstructs an image based on this caption. As shown in Figure 1, if the generated caption contains hallucinations (e.g., a book'), there will be vi-"}, {"title": "Related Work", "content": "Multimodal Large Language Models\nThe emergence of LLMs has revolutionized the paradigm of Natural Language Processing (NLP). The significant success of LLMs in the NLP field has led to research on leveraging LLMs in the visual domain. Consequently, MLLMs that can simultaneously handle visual and textual data have recently been proposed. Specifically, to process visual information, LLaVA uses a CLIP vision encoder and a linear layer to project images into the LLM's input embedding space. MiniGPT-4 employs a Q-Former and a linear layer to project images into the LLM's input embedding space. Additionally, mPLUG-Owl2 introduces a modality-adaptive module that preserves modality-specific features, allowing the model to excel in both multimodal and NLP tasks.\nHowever, despite these efforts, misalignment between modalities can still occur for various reasons, leading to generated responses that do not correspond to the visual information. This phenomenon, known as hallucination, undermines the reliability of MLLMs and poses a significant challenge to their application in real-world scenarios.\nHallucination Mitigation\nTo address the hallucination problem in MLLMs, several studies have been proposed recently. Lure and Woodpecker employ post-processing methods to revise generated responses, either by training a revisor or using GPT-3.5-turbo. Fine-tuning approaches mitigate hallucinations through instruction tuning with additional data, but they require significant data collection and training resources. Given the large number of parameters in MLLMs, this is computationally inefficient.\nTherefore, methods for improving the decoding process have recently received great attention due to the advantage that they do not require additional training. Specifically, OPERA explores aggregation patterns that cause hallucinations. OPERA utilizes this insight to suppress the generation of tokens that exhibit these patterns. VCD leverages the characteristic that the model tends to prioritize prior knowledge over visual information when responding to distorted images. As a result, the responses to the distorted image and the original image show significant differences in hallucinated tokens, and VCD contrasts these to mitigate the hallucinations. HALC observes that when images with varying fields of view are input into the MLLM, the probability changes for ground truth tokens are much greater than for hallucinated tokens. This observation helps identify visual context candidates that clearly depict objects, and by contrasting these candidates, HALC reduces hallucinations.\nUnlike existing techniques, we propose a new decoding method that utilizes a T2I model. Specifically, our approach visualizes hallucinations in the initially generated caption using a T2I model, then contrasts the responses generated from the reconstructed image with those from the original image. Through this process, we contrast distributions of the hallucinated tokens and effectively mitigate hallucinations."}, {"title": "Methodology", "content": "Preliminaries\nResponse Generation. The MLLM generates a response y corresponding to a given input image v and instruction text x. The input image is projected into visual tokens through an image encoder, and these tokens, along with the tokens corresponding to the instruction text, are fed into the LLM. The response is generated through autoregressive decoding according to the following equation:\n$y_{t} \\sim P_{\\theta}(\\cdot | v, x, y_{<t}) \\propto \\exp \\left(f_{\\theta}(\\cdot | v, x, y_{<t})\\right)$, (1)"}, {"title": "Hallucination Visualization", "content": "where $\\theta$ denotes the parameters of the MLLM, $y_t$ represents the t-th token of response, and $y_{<t}$ is the sequence of tokens generated up to time t. $f_\\theta$ denotes the logit distribution generated by the MLLM. Hallucination refers to the phenomenon where the output y generated by the MLLM does not correspond to the input image v. This study focuses on mitigating hallucinations while maintaining the overall performance of the MLLM as a language model.\nText-to-Image Generation. The core component of ConVis is the T2I model that generates images based on a given query. The goal of the T2I model is to create an image that accurately depicts the query. Among the recently proposed T2I models, we utilize Hyper-SDXL, an enhanced version of Stable Diffusion, which has demonstrated excellent T2I performance. The diffusion-based Hyper-SDXL model begins with a pure noise and progressively reconstructs it through an iterative reverse diffusion process which ultimately results in the generated image $v'$.\nHallucination Visualization\nWe hypothesize that the T2I model can help mitigate hallucinations by providing visual contrast signals during the decoding process. If the T2I model receives a caption generated by the MLLM that contains hallucinations, it will faithfully visualize those hallucinations in the generated image. We refer to this process as hallucination visualization.\nTo implement this, ConVis first generates an initial caption c for the original image v using a simple instruction text that directs the MLLM to describe the image. This process is illustrated in Figure 2. The T2I model then takes the caption c as a query and generates an image $v'$ based on it. If the caption contains hallucinations, these will be faithfully visualized in the generated image $v'$. Conversely, if the initial caption is accurate and free of hallucinations, the generated image will be semantically similar to the original image.\nDiversity of Generated Images. Given that the current T2I model may not generate images that fully align with the captions, we address this limitation by increasing the diversity of the generated images using the following approaches: (1) We first generate a diverse set of n captions using Nucleus Decoding instead of Greedy Decod-"}, {"title": "Contrastive Decoding", "content": "ing. (2) Then, the T2I model uses these n captions to generate n corresponding images. This approach increases coverage of the various potential hallucinations that the MLLM might generate by diversifying the captions. Additionally, by using multiple images instead of a single one, we enhance the robustness of our method against the T2I model's potential misalignment between the caption and the generated image due to its imperfect performance.\nWe have found these approaches to be effective, with detailed results available in the experiment section.\nContrastive Decoding\nHallucinations in captions cause visual differences between the original image v and the generated image $v'$. We mitigate these hallucinations by capturing the visual contrast signals from these differences. To achieve this, during the decoding process, we utilize both the original image v and the n generated images to produce the logit distribution for each image. The final contrastive logit distribution $\\hat{f}_\\theta$ is derived by averaging the contrastive logit distributions between the original image and each generated image as follows:\n$\\hat{f}_{\\theta}=\\frac{1}{n} \\sum_{i=1}^{n}\\left((1+\\alpha) f_{\\theta}(\\cdot | v, x, y_{<t})-\\alpha f_{\\theta}\\left(\\cdot | v_{i}^{\\prime}, x, y_{<t}\\right)\\right)$, (2)\nwhere $\\alpha$ is a hyperparameter that controls the strength of the difference between the logit distributions from the original and generated images. The contrastive logit distribution $\\hat{f}_\\theta$ is used to generate the response y. For tokens associated with hallucinations, the contrastive logit distribution is significantly amplified compared to other tokens, allowing us to penalize these tokens and reduce the hallucinations.\nNote that, Equation 2 is similar to the contrastive decoding methods used in VCD and HALC . However, our method is distinguished from existing approaches by directly capturing visual contrastive signals from the hallucinations visualized by the T2I generative model."}, {"title": "Experiments", "content": "Benchmarks. To evaluate the performance of our method, we conduct experiments on three benchmarks to evaluate the mitigation of hallucinations and two general-purpose benchmarks to assess the general performance of the MLLM:\n\u2022 Hallucination: CHAIR, HallusionBench, and Polling-based Object Probing Evaluation (POPE)\n\u2022 General-purpose: MLLM Evaluation (MME) and LLaVA-Bench\nDetailed information on these benchmarks can be found in the Appendix.\nBackbones. To evaluate our method, we utilize three well-known MLLMs with publicly available checkpoint weights: LLaVA-1.5, mPLUG-Owl2, and MiniGPT-4.\nCompared Methods. Our method is designed to replace existing decoding methods used in the LLM component, and therefore, we compare it against baselines such as Greedy Search, Nucleus Sampling , and Beam Search (beam=5). We also evaluate our method's effectiveness against other decoding methods in hallucination mitigation, including OPERA, VCD and HALC . We use the same hyperparameters borrowed from the original papers of the compared methods to ensure a fair comparison.\nImplementation Details. We utilize the Hyper-SDXL T2I model for image generation. Specifically, in all experiments, unless otherwise noted, we use the Step 1 generation results of Hyper-SDXL model. The maximum length of text queries that the T2I model could accept is 77 tokens, which is too short to process the captions generated by MLLM. To address this, we leverage Compel, which allows for processing more than 77 tokens. We set the maximum token count for the caption generation to 256 and use Nucleus sampling with a temperature of 0.7 and a top-p of 0.9 to generate the images. The query used in this process is \"Please describe this image in detail.\" We set the number of generated images, n, to 4, producing four images based on distinct captions generated using different random seeds. For contrastive decoding, we follow using adaptive plausibility constraint to contrast only meaningful tokens. The plausibility constraint hyperparameter $\\lambda$ is set to 0.1. We also set $\\alpha$, which controls the degree of contrastive emphasis, to 1 for captioning-based metrics such as CHAIR and LLaVA-Bench, and to 0.1 for VQA metrics, including POPE, HallusionBench, and MME. To generate responses, we use a greedy decoding approach for all methods. For CHAIR, we sample three different sets of images using different random seeds and assess the performance using the mean and standard deviation of these results."}, {"title": "Experimental Results", "content": "Results on CHAIR. We report our evaluation results on the CHAIR benchmark in Table 1. Our assessment includes basic decoding strategies--Greedy search, Nucleus sampling, and Beam search-along with three state-of-the-art approaches-VCD, OPERA, and HALC. Our method achieves the best performance on the CHAIRS metric across all three backbone models (LLaVA-1.5, mPLUG-Owl2, and MiniGPT-4). Remarkably, it significantly improves the CHAIRS score compared to both the basic decoding strategies and the state-of-the-art methods, highlighting its superior ability to mitigate hallucinations. In terms of the CHAIRI metric, our method consistently ranks either first or second across all backbone models. These results demonstrate that our method both excels in reducing the total number of hallucinations throughout entire sentences and minimizes the number of hallucinated objects across all evaluated image sets.\nResults on HallusionBench. In Table 2, we present the evaluation results for the visual dependent category of the HallusionBench benchmark. HallusionBench is evaluated with the assistance of GPT-4V, which incurs significant costs; therefore, we conduct experiments using only the LLaVA-1.5 backbone. Our method demonstrates superior performance in Figure Accuracy (fAcc), outperforming all baseline decoding strategies (Greedy Search, Nucleus Sampling, Beam Search) as well as state-of-the-art techniques (VCD, OPERA, HALC). This indicates that our model effectively interprets the visual details of images when responding to visually dependent questions,"}, {"title": "Results on POPE.", "content": "indicating its ability to mitigate hallucinations by providing responses that closely align with the given visual content. Furthermore, our method achieves the highest performance on the All Accuracy (aAcc) metric, which measures overall accuracy across all questions within the visual dependent category, demonstrating its effectiveness in handling a wide range of visually dependent queries.\nResults on POPE. Table 3 reports the evaluation results on the POPE benchmark using the MSCOCO dataset (val2014 split). We present the average F1-scores across the three POPE question splits-Random, Popular, and Adversarial for three different backbone models. Detailed performances on each POPE question split are in the Appendix.\nOur method achieve a new SOTA performance on MiniGPT-4, and demonstrate performance comparable to existing techniques on LLaVA-1.5 and mPLUG-Owl2. In terms of average performance across all backbones, our method outperforms previous techniques. This indicates that our approach consistently delivers strong performance across various backbones.\nWhile we achieves overall strong performance on this benchmark, the performance improvements across different backbone models are relatively modest. This might be because the POPE question split does not fully align with the types of hallucinations that T2I models generate. POPE questions, which ask, \u201cIs this [object] in this image?\u201d sample objects randomly, popularly, or adversarially. Meanwhile, our method visualizes hallucinations in captions generated by prompts like \u201cPlease describe this image in detail.\u201d As a result, T2I model may visualize the objects unrelated to the actual POPE questions which limits our method's effectiveness. This limitation will be explored further through a qualitative analysis of POPE samples later in this section.\nResults on MME. In Table 4, we present the evaluation re-"}, {"title": "Results on LLaVA-Bench.", "content": "sults on the MME benchmark using the LLaVA-1.5 backbone. Due to space limitations, we focus on the performance in the two main categories of the MME benchmark: Perception and Cognition. Scores for the subcategories are provided in the Appendix. Our method outperforms all others in the Perception category, demonstrating its effectiveness in accurately interpreting and processing visual information across various tasks. This strong performance indicates that our model is particularly well-suited for visual tasks, making it highly effective for applications that require precise visual understanding. In the Cognition category, our method demonstrates competitive performance, comparable to OPERA and superior to HALC, further underscoring the versatility and robustness of our approach. While VCD excels in cognitive tasks, our method achieves stronger overall performance when both the Perception and Cognition categories are considered together. This suggests that our model provides a more comprehensive and effective solution across diverse tasks. Its balanced and reliable performance in both visual and cognitive challenges makes it an adaptable solution for a wide range of applications.\nResults on LLaVA-Bench. Table 5 shows the experimental results on the LLaVA-Bench, which verify whether the language model capabilities are preserved. For this evaluation, we uses the LLaVA-1.5 backbone. Our method outperforms existing techniques across all categories: complex reasoning, conversation, and detailed description. These results demonstrate that our method effectively mitigates hallucinations while also enhancing the performance of the MLLM."}, {"title": "Analysis and Discussion", "content": "Diversity of Generated Captions and Images. Although T2I models have made significant advancements, they still struggle to generate images that perfectly align with the given captions. To address these limitations, we increase the coverage of hallucination visualization by generating diverse images. Specifically, we use Nucleus sampling, which is known for producing more varied responses than Greedy search, to generate multiple captions. These captions are then utilized to generate images.\nTo evaluate the effectiveness of this strategy, we analyze how caption diversity impacts hallucination reduction. First, we compare the CHAIR scores of the final responses when using Greedy search and Nucleus sampling during the image generation stage. In this experiment, we limit the number of generated images to one and compare which decoding strategy performs better. As shown in Table 7, Nucleus sampling outperforms Greedy search, demonstrating its po-"}, {"title": "Impacts of Image Generation Quality.", "content": "tential to generate more diverse captions. Furthermore, in Figure 4, we investigate how the number of generated images from different captions using Nucleus sampling affects CHAIR scores. We observe that as the number of images n increases, both CHAIRS and CHAIRI scores improve, confirming that using multiple reconstructed images, rather than a single image, is more effective for improving performance. These findings validate our design choice of utilizing Nucleus sampling and multiple captions for image generation.\nImpacts of Image Generation Quality. To investigate the impact of generated image quality on hallucination mitigation, we evaluate the performance of our method using various text-to-image (T2I) models. Table 6 presents the generation quality (CLIPScore) of the T2I models alongside their corresponding CHAIR scores. We compare three T2I models: Hyper-SD1.5, SDXL-Turbo, and Hyper-SDXL, with the inference step fixed at 1.\nThe results indicate a clear trend: as the CLIPScore improves, so does the CHAIR score. Notably, SDXL-Turbo consistently outperforms Hyper-SD1.5 across all backbones, except for mPLUG-Owl2. Moreover, Hyper-SDXL significantly outperforms Hyper-SD1.5 in all cases. These findings suggest that using higher-quality T2I models, which are better aligned with the original captions, can more effectively mitigate hallucination issues. Consequently, we believe that as more advanced T2I models are developed, the performance of our method will continue to improve.\nQualitative Analysis. Figure 5 shows the KL divergence between output distributions at each decoding step when the"}, {"title": "Limitations.", "content": "is and .  For tions about  generated captions  may not the for (b),  is not econstructed e technique xed a  to  the s an . adaptive Conclusion In , we ConVis, a  designed  By a a we .  In  for the al all to , all the method effectiveness type"}, {"title": "Conclusion", "content": "images and caption from Figure 6 (a) are provided to the MLLM. We observe that the KL divergence is high for the hallucinated token car, while non-hallucinated tokens exhibit lower KL divergence. This indicates that the generated image can produce visual contrastive signals for hallucinated tokens when compared to the original image. This supports our argument that the differences between the original and generated images are primarily influenced by the hallucinated tokens.\nTo more clearly demonstrate how our method mitigates multimodal hallucinations, we present an example in Figure 6 (a), illustrating the process from the initial hallucinated caption to the generated image, followed by the contrastive decoding result. Specifically, for an image of a dog jumping into a pool, the MLLM incorrectly describes the scene as \u201ca dog jumping over a car in a parking lot.\" Using this caption, the T2I model generates a reconstructed image that faithfully visualized the hallucinated content. By contrasting the distributions of the reconstructed and original images during decoding, our method effectively reduces hallucinations.\nLimitations. One of the key limitations of our approach is its strong dependence on T2I generation models. This reliance may hinder effectiveness in tasks like VQA, where the generated captions can sometimes contain hallucinations that deviate significantly from the specific question. This limitation is particularly evident in our experiments with the POPE benchmark, where the performance gain is not as significant as expected. Regarding questions about the presence of specific objects, if the object in question is not related to the hallucinations generated by the caption, visualizing with a T2I model may not sufficiently reflect the information needed for the VQA task. In Figure 6 , a question about the presence of a bed in an original image where people are looking at fruits might not be well served by the reconstructed image. This indicates the effectiveness of our method may decrease for certain type of questions.\nCurrently, our technique employs a fixed prompt for image captioning. However, we believe that adapting the prompt to respond more specifically to the given question could mitigate this issue. We plan to explore this adaptive approach in future work.\nConclusion\nIn this paper, we presented ConVis, a novel contrastive decoding method designed to mitigate hallucinations in MLLMs. By utilizing a T2I generation model, our approach effectively visualizes hallucinations and contrasts probability distributions between the original and reconstructed images. This process allows for the penalization of hallucinated content during the decoding phase, all without the need for additional data or model retraining.\nOur extensive experiments across five benchmarks,"}, {"title": "Appendix", "content": "In this appendix, we provide additional details into the benchmarks referenced in the main paper. To evaluate hallucinations, we employ the following five benchmarks:\nCHAIR evaluates how well the generated captions align with the content of the given image. CHAIR consists of two versions: CHAIR_S, which measures the inaccuracies at the sentence level, and CHAIR_I, which evaluates at the object level within the sentence by comparing the number of false objects to the total number of objects. For evaluation, we use the val2014 split of the MSCOCO dataset, which includes annotations for 80 object categories. We randomly select 500 images from the entire dataset and used the prompt \"Please describe this image in detail.\" for the MLLM.\nHallusionBench is a hallucination evaluation benchmark designed to assess whether a model ignores visual context and relies solely on language priors (Language Hallucination) or exhibits the opposite phenomenon (Visual Illusion). The questions in HallusionBench are divided into two main categories, one of which is the Visual Dependent (VD) category. In this category, pairs of similar but different images are presented, and the same question is asked for each pair. The questions are presented in a VQA format with binary ground truth (GT) answers. Accuracy is calculated using GPT-4V by determining whether the model's responses are similar to, different from, or difficult to compare with the answers generated by GPT-4V. Since this paper focuses on preventing MLLMs from generating hallucinated information based on a given image, we specifically conduct experiments on the Visual Dependent category.\nPolling based Object Probing Evaluation (POPE) is a VQA-based metric proposed to assess hallucinations in MLLMs. This metric evaluates the MLLM's response to the prompt \"Is [object] is in this image?\" To emphasize that this is a binary VQA task, we appended the prompt with \"Please answer yes or no.\" To select objects referenced in the question prompt, we followed three different sampling options: random, popular, and adversarial. We evaluated performance across all sampling options.\nMLLM Evaluation (MME) evaluates the capabilities of MLLMs, dividing the evaluation into two major categories: perception and cognition. The perception category includes fine-grained tasks such as existence, count, location, rough color, poster, celebrity, scene, landmark, artwork identification, and OCR. The cognition category includes tasks like commonsense reasoning, numerical calculations, text translation, and code reasoning. All questions in this benchmark are structured to be answered with a simple yes or no.\nUsing the LLaVA-Bench we further demonstrated how well our proposed method maintains the language model performance. This benchmark involves posing various situational questions, such as dialogue, detailed descriptions, and complex reasoning, to randomly selected images from the MSCOCO val2014 dataset. A total of 60"}]}