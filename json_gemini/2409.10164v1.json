{"title": "QUANTILE REGRESSION FOR DISTRIBUTIONAL\nREWARD MODELS IN RLHF", "authors": ["Nicolai Dorka"], "abstract": "Reinforcement learning from human feedback (RLHF) has become a key method\nfor aligning large language models (LLMs) with human preferences through the\nuse of reward models. However, traditional reward models typically generate\npoint estimates, which oversimplify the diversity and complexity of human values\nand preferences. In this paper, we introduce Quantile Reward Models (QRMs),\na novel approach to reward modeling that learns a distribution over rewards in-\nstead of a single scalar value. Our method uses quantile regression to estimate a\nfull, potentially multimodal distribution over preferences, providing a more pow-\nerful and nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and accom-\nmodates conflicting preferences by modeling them as distinct modes in the distri-\nbution. Our experimental results show that QRM outperforms comparable tradi-\ntional point-estimate models on RewardBench. Furthermore, we demonstrate that\nthe additional information provided by the distributional estimates can be utilized\nin downstream applications, such as risk-aware reinforcement learning, resulting\nin LLM policies that generate fewer extremely negative responses. Our code and\nmodel are released at https://github.com/Nicolinho/QRM.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating\nremarkable capabilities across a wide range of tasks (Anthropic, 2023; Team et al., 2023; OpenAI,\n2023). However, the sheer scale and breadth of their training data present both opportunities and\nchallenges. While LLMs can process and generate human-like text with unprecedented fluency, their\noutputs may not always align with human preferences, ethics, or real-world applicability. To bridge\nthis gap and ensure that these powerful tools truly benefit humanity, it has been recognized that fine-\ntuning techniques are necessary that can align LLMs with human values and intentions (Christiano\net al., 2017; Ziegler et al., 2019; Bai et al., 2022a). This process of refinement is essential to harness\nthe full potential of LLMs while mitigating potential risks associated with their deployment in real-\nworld scenarios.\nReinforcement learning from human feedback (RLHF) has emerged as a prominent and effective\nmethod to align LLMs with human preferences. RLHF uses reinforcement learning (RL) to fine-\ntune language models by maximizing rewards derived from a trained reward model. This reward\nmodel, is itself learned from human preferences. By quantifying human judgments on responses for\na prompt, the reward model provides a crucial bridge between human values and the optimization\nobjective of the language model. As a result, accurate reward models are very important in order\nto finetune LLMs and it has been shown that improvements in reward model quality translates to\nimprovements in the quality of the finetuned LLM (Touvron et al., 2023).\nHowever, current reward models are designed to output a single scalar value for a given query-\nresponse pair, an approach that fails to capture the inherent complexity and diversity of human\nvalues. This oversimplification can lead to problematic outcomes. For instance, in scenarios where\nhuman opinions diverge significantly - with some individuals finding a response appropriate while\nothers deem it inappropriate - the reward model may resort to outputting an average value to mini-\nmize penalties across these disparate groups during training. This compromise fails to represent the"}, {"title": "2 RELATED WORK", "content": "RLHF involves using RL to align language models with human preferences or feedback (Christiano\net al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). The process generally involves training a re-\nward model on preference data collected from crowdworkers (Bai et al., 2022a; Ouyang et al., 2022)\nor model-selected responses (Bai et al., 2022b). Once a reward model is developed, RL algorithms\ncan be used to train policies. Another method is to directly optimize a policy by comparing selected\nand rejected responses using DPO (Rafailov et al., 2023) and followup works (Tang et al., 2024).\nWhile this method bypasses the need for an explicit reward model, our focus is on approaches that\nexplicitly train such a model.\nEstimating distributions rather than point estimates in regression has a long history and has proven\nvaluable across many fields (Kneib et al., 2023). Quantile regression (Koenker, 2005), in particular,\nis a powerful tool for approximating distributions. In RL, distributional regression has also demon-\nstrated its effectiveness (Bellemare et al., 2017), with quantile regression achieving especially strong\nresults (Dabney et al., 2018; Kuznetsov et al., 2020; Dorka et al., 2022).\nA key distinction exists between epistemic and aleatoric uncertainty (Fox & \u00dclk\u00fcmen, 2011;\nH\u00fcllermeier & Waegeman, 2021). Epistemic uncertainty arises from incomplete knowledge and\ncan be reduced by improving data or models. In contrast, aleatoric uncertainty stems from inher-\nent randomness in the system and cannot be reduced. This paper focuses on modeling aleatoric\nuncertainty.\nSeveral previous works have modeled preferences using distributions. Distributional Preference\nLearning (DPL) (Siththaranjan et al., 2023) suggests that hidden contextual factors influence pref-\nerences, arising from the diverse values of annotators. This variability is captured by estimating\na distribution over rewards. Similarly, the Distributional Preference Reward Model (DPRM) (Li\net al., 2024) learns a distribution over rewards, utilizing an optimal transport loss to train the model\non preference data. Both DPL and DPRM, like our approach, model a distribution over rewards.\nHowever, unlike our method, which incorporates attribute regression, these models rely solely on\npreference data and optimize the likelihood that the preferred response is ranked higher."}, {"title": "3 DISTRIBUTIONAL REWARD MODELS", "content": "In this section, we first introduce the reinforcement learning from human feedback (RLHF) frame-\nwork, then introduce multi-attribute regression reward models, followed by an explanation of quan-\ntile regression. Finally, we propose our approach using quantile regression to obtain distributional\nreward models and explain how it can be used for risk-aware RLHF."}, {"title": "3.1 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "content": "The typical RLHF process using an explicit reward model consists of three stages:"}, {"title": "3.1.1 SUPERVISED FINE-TUNING", "content": "In the first stage, the pre-trained language model is instruction-tuned by supervised learning on a\ndataset consisting of a prompt and a high-quality response. Similarly to the pre-training phase the\nmodel is trained with a cross-entropy loss over tokens but only on the tokens of the response. The\nresulting model is often used as the initialization for the reward model and the final policy trained\nwith reinforcement learning."}, {"title": "3.1.2 REWARD MODEL TRAINING", "content": "After supervised fine-tuning, the next step is to train a reward model $r_\\phi(x, y)$, which evaluates the\nquality of responses $y$ relative to the prompt $x$ based on human preferences. Typically, reward\nmodels are trained on a preference dataset $D_{pref} = \\{x, y^-, y^+\\}$ consisting of a prompt $x$ and a\npreferred $y^+$ and not-preferred $y^-$ response. The goal is that the reward model learns to assign\nhigher rewards to outputs that better align with human preferences, such that it can later be used as\nan informative proxy signal in guiding the optimization of the language model."}, {"title": "3.1.3 REINFORCEMENT LEARNING", "content": "Once the reward model is trained, it can be used to optimize the language model further through\nreinforcement learning. In this stage, the language model generates outputs for a dataset of prompts,\nand the reward model assigns rewards to these outputs based on their estimated quality. The language\nmodel is then trained to maximize these rewards. This iterative process allows the model to improve\nits performance by continuously refining its behavior according to the feedback provided by the\nreward model, making it more capable of generating high-quality, human-aligned responses.\nMore formally, the goal is to finetune the language model $\\pi_\\theta$ by maximizing the expected reward. To\nprevent reward hacking, which results in gibberish outputs, the model is penalized for deviating too\nmuch a reference policy. This is achieved with a KL divergence penalty between the current policy\nand the reference policy. Often the initial policy $\\pi^{sft}$ is used as reference policy. The complete RL\nobjective can be expressed as:\n$L(\\theta) = E_{x \\sim D, y \\sim \\pi_\\theta(y|x)} [r_\\phi(x, y) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref})]$,\nwhere $\\beta$ is a hyperparameter that controls the strength of the KL penalty, balancing the trade-off\nbetween maximizing the reward and staying close to the original model distribution. In principle\nany RL algorithm can be used to optimize the policy. A common choice is to use PPO (Schulman\net al., 2017) and more recently more simple REINFORCE based algorithms (Ahmadian et al., 2024)."}, {"title": "3.2 MULTI-ATTRIBUTE REWARD MODELS", "content": "Most existing reward models are trained using the Bradley-Terry loss on binary preference data,\nwhere one response is labeled as preferred to another. This approach essentially frames the problem\nas binary classification. This method, however, fails to account for whether a response was clearly\nbetter or only marginally superior to the other. As a result, training a reward model on such data\ncan lead to problems, as it may penalize good responses that are only slightly less favorable than an\neven better one in the same way it penalizes a very bad response.\nRecent datasets are increasingly generated by first collecting absolute labels rather than relative ones\n(Cui et al., 2023; Wang et al., 2023). Responses are rated across various dimensions such as helpful-\nness, truthfulness, and harmlessness, with a fine-grained score assigned to each dimension. These\nindividual scores are then aggregated to produce a final score. Preferences are subsequently deter-\nmined by labeling the response with the higher aggregated score as the preferred one. Alongside\nthese datasets, new approaches have emerged that use regression to directly estimate the fine-grained\nscores (Wang et al., 2023; 2024a). In this framework, the reward network outputs a single scalar\nvalue for each objective and is trained to minimize the mean squared error between its predictions\nand the fine-grained scores."}, {"title": "3.3 QUANTILE REGRESSION", "content": "Quantile regression (Koenker, 2005) is a versatile statistical technique that generalizes the con-\nventional linear regression model by estimating the conditional quantiles of the response variable.\nFormally, while ordinary least squares (OLS) regression focuses on minimizing the sum of squared\nresiduals to estimate the conditional mean, quantile regression minimizes an asymmetric loss func-\ntion to estimate specific quantiles belonging to the distribution of the response variable. The quan-\ntile function $Q(\\tau)$ (where 0 < $\\tau$ < 1) of a probability distribution is the inverse of its cumulative\ndistribution function (CDF) and hence $Q(\\tau)$ denotes the value x such that the probability of the\ncorresponding random variable X being lower than x is equal to $\\tau$, i.e. $P(X <= x) = \\tau$. For a\ngiven quantile $\\tau$, linear quantile regression solves the following optimization problem:\n$\\min_w \\sum_{i:y_i \\geq x_i w} \\tau |y_i - x_i w| + \\sum_{i:y_i < x_i w} (1 - \\tau) |y_i - x_i w|.$\nHere, $y_i \\in \\mathbb{R}$ represents the labels, $x_i \\in \\mathbb{R}^d$ the d-dimensional inputs, and $w \\in \\mathbb{R}^d$ is the vector of\ncoefficients to be estimated.\nOne of the key strengths of quantile regression lies in its ability to model the conditional distribu-\ntion of the response variable at different quantiles, offering a more detailed view of the relationship\nbetween the dependent and independent variables. This is particularly advantageous in situations\nwhere the effects of the input variables are not uniform across the distribution of the outcome vari-\nable, such as in the presence of skewed or multimodal distributions, heteroscedasticity, or outliers.\nUnlike OLS, which provides a single estimate of the central tendency, quantile regression allows to\napproximately represent a distribution over the response variable, offering more information about\nthe data. Hence, quantile regression provides a comprehensive framework for understanding the dis-\ntributional effects of input variables on the response variable, making it a critical tool in applications\nwhere understanding the full conditional distribution is essential."}, {"title": "3.4 DISRTIBUTIONAL REWARD MODELS VIA QUANTILE REGRESSION", "content": "Our objective is to develop a reward model that outputs a distribution over rewards. To achieve this,\nwe propose a two-step approach called Quantile Reward Models (QRM). This method involves:\n(1) estimating distributions over attribute scores (e.g., helpfulness and harmlessness) using quantile\nregression, and (2) training a gating network to aggregate these individual attribute distributions into\na final reward distribution. An illustration of our approach is depicted in Figure 2."}, {"title": "3.4.1 STEP 1: ATTRIBUTE DISTRIBUTION ESTIMATION", "content": "In the first step, we estimate the distributions for each of the M attributes using quantile regression.\nSpecifically, we perform regression on $K \\in \\mathbb{N}$ evenly spaced quantiles $\\tau_k \\in (0, 1)$. For each\nattribute, we train K linear quantile regression layers, with each layer predicting the value at a\nspecific quantile."}, {"title": "3.4.2 STEP 2: GATING NETWORK FOR DISTRIBUTION AGGREGATION", "content": "In the second step, we train a gating network that predicts the mixing weights for combining the in-\ndividual attribute distributions into a final reward distribution. The gating network outputs weights\n$g_m$ for each attribute $m = 1, ..., M$, ensuring that they sum to one: $\\sum_m g_m = 1$. These weights\ndetermine the contribution of each attribute's distribution to the final distribution. The mixed dis-\ntribution is then computed as the weighted sum of the M quantile values for each attribute, given\nby:\n$Q_{mix} (\\tau_i) = \\sum_{m=1}^M g_m \\cdot Q_m (\\tau_i)$.\nThis calculation is performed for all quantiles $\\tau_i$. The input to the gating network is a feature\nvector from the LLM backbone that encodes only the prompt. The gating network is trained using\npreference data and the Bradley-Terry loss following the approach in (Wang et al., 2024a). To adapt\nthis approach to our scenario, we compute the expected value of the final distribution, which serves\nas the reward for optimizing the Bradley-Terry loss. Importantly, only the gating network is trained\nduring this stage, while the quantile regression layers remain fixed.\nIn the end, we obtain both a distribution over attributes and a final distribution over rewards. Notably,\nour approach can also accommodate fixed attribute weightings, as has been done in works such as\nHelpSteer (Wang et al., 2023; 2024b)."}, {"title": "3.5 RISK-AWARE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "content": "We believe that distributional reward models offer significant potential by leveraging the additional\ninformation contained in the reward distribution for downstream reinforcement learning (RL) tasks.\nIn this section, we demonstrate one possible application of this concept.\nWhen deploying a chat model, a critical concern is avoiding particularly poor responses, as low-\nquality outputs can negatively impact user experience across various dimensions, such as overall\nquality and safety. Even occasional bad responses can lead to user dissatisfaction. In these cases, it\ncan be beneficial to optimize a metric that emphasizes penalizing low-quality responses as perceived\nby users.\nA point estimate reward model provides only a single value to assess a response. For example,\nif a response is perceived as excellent by some users and poor by others, a point estimate model\nmay produce an average score that can lead the LLM to generate such mixed-quality responses. In\ncontrast, a distributional reward model allows us to penalize responses with significant disagreement."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 IMPLEMENTATION", "content": "Our LLM backbone is based on LLaMA-3 with 8 billion parameters (Meta, 2024), initialized with\nweights from a reward model trained using the Bradley-Terry loss (Dong et al., 2024). The backbone\nremains frozen during both stages of training. This allows for significant computational efficiency,\nas we can precompute the backbone features once and reuse them throughout the subsequent stages,\nthereby significantly reducing the required computation resources. For multi-attribute regression,\nwe follow prior work and use 19 attributes from Wang et al. (2024a), sourced from eight datasets\n(details provided in the appendix). To limit the computational requirements, we limit the number of\ndata points per attribute to 60,000 when training the linear quantile regression layers. The quantile\nregression is implemented using Scikit-learn Pedregosa et al. (2011), with L1 weight regularization\nset to 0.003. For each attribute, we train 19 quantile regression models corresponding to the evenly\nspaced quantiles: 0.05, 0.10, ..., 0.90, 0.95. Additionally, as in Wang et al. (2024a), we mitigate the\ncorrelation between the verbosity attribute and other attributes by applying their penalty scores, and\nadjusting the quantile estimates accordingly.\nThe gating network used for aggregation is a multi-layer perceptron with three hidden layers and\na softmax activation at the output layer. We train the network for 3 epochs using the AdamW\n(Loshchilov & Hutter, 2019) optimizer with a learning rate of 0.0003, batch size of 1024, and a\ncosine decay learning rate scheduler. The weight decay is set to 0.001. The training is performed on\ndata from 10 datasets, with further details outlined in the appendix."}, {"title": "4.2 REWARDBENCH RESULTS", "content": "To examine the general capabilities of our distributional reward model we evaluate it on Reward-\nBench Lambert et al. (2024) which is a benchmark for evaluating reward models. It features diverse\ntasks across 4 categories: Chat, Chat Hard, Safety, and Reasoning. Each category includes datasets"}, {"title": "4.3 RISK-AWARE RLHF EXPERIMENTS", "content": "To evaluate our approach, we train two RL policies. For comparability, both use our distributional\nreward model, but in one case we apply the risk-aware utility function to compute the rewards,\nwhile in the other, we simulate a point-estimate reward by using the expected value of the reward\ndistribution.\nWe use the LLaMA-3 model with 8 billion parameters as the base model, training on the Anthropic\nHH-RLHF dataset (Bai et al., 2022a). Both supervised fine-tuning (SFT) and RL are performed\nusing LoRA, with a rank value of 32 and a = 64. For SFT, we train on the responses from the\ndataset selected as chosen, filtering for examples with at most 1024 tokens. The SFT model is trained\nfor one epoch with a learning rate of 0.0003, batch size of 1024, using the AdamW optimizer and a\ncosine learning rate schedule.\nAfter completing the SFT stage, we merge the LORA weights into the model's base weights. For\nRL, we optimize the LLM policy using the RLOO algorithm (Ahmadian et al., 2024), initializing\nboth the policy and reference policy with the SFT model. We train with a value of k = 2 for RLOO,"}, {"title": "5 CONCLUSION", "content": "In this work, we proposed QRM, a novel approach for training a distributional reward model in the\ncontext of RLHF. Traditionally, reward models are treated as point estimators, which face challenges\nin accommodating diverse human preferences and handling the resulting label noise during training.\nIn contrast, estimating a distribution that can be multimodal is more powerful, capturing distinct\nvalues and preferences more effectively. QRM uses quantile regression to estimate distributions\nover attribute scores such as helpfulness and harmlessness. A gating network produces weights,\naggregating these individual distributions into a final reward distribution.\nOur experimental results demonstrate that QRM outperforms previous approaches with comparable\nbase architectures. Notably, even when reduced to point estimates by calculating the expectation\nof the distribution, our model showed improvements. Moreover, the distributional reward model\nprovides additional information that can be utilized for risk-aware RLHF. The final policy generated\nby our approach produces responses with higher quantile estimates for the left tail of the distribution,\nreducing the occurrence of extremely poor responses.\nOur work opens several promising avenues for future research. One exciting direction is to di-\nrectly incorporate conflicting labels into training. Unlike traditional preference learning datasets\nthat rely on majority-vote labels, our model could account for raw, conflicting annotations for the\nsame prompt-response pair. Additionally, it would be interesting to explore integrating our distri-\nbutional reward model with distributional RL algorithms that estimate a distribution for the value\nfunction. Another promising avenue is to leverage the extra information provided by our model\nwithin search algorithms during decoding. More broadly, the extra information provided by our\ndistributional reward model allows for more interpretability as well as more creative ways to steer\nLLMs during downstream fine-tuning."}, {"title": "A EXPERIMENTAL DETAILS", "content": "Attribute Training Datasets To train the quantile regression layers, we follow Wang et al. (2024a)\nand use the following training datasets with corresponding reward objectives.\n\u2022 HelpSteer (Wang et al., 2023) (35k data): helpsteer-helpfulness, helpsteer-correctness,\nhelpsteer-coherence, helpsteer-complexity, helpsteer-verbosity\n\u2022 UltraFeedback (Cui et al., 2023) (240k data): ultrafeedback-overall-score, ultrafeedback-\ninstruction-following, ultrafeedback-truthfulness, ultrafeedback-honesty, ultrafeedback-\nhelpfulness\n\u2022 BeaverTails-30k (Ji et al., 2023) (30k data): beavertails-is-safe\n\u2022 CodeUltraFeedback (Weyssow et al., 2024) (50k data): code-complexity, code-style,\ncode-explanation, code-instruction-following, code-readability\n\u2022 Prometheus (Kim et al., 2024a) (200k data): prometheus-score\n\u2022 Argilla-Capybara\u00b9 (Daniele & Suphavadeeprasit, 2023) (15k data): argilla-overall-quality\n\u2022 Argilla-OpenOrca\u00b2 (13k data): argilla-judge-lm\n\u2022 Argilla-Math-Preference\u00b3 (2.4k data): This dataset shares the objective ultrafeedback-\ninstruction-following with UltraFeedback\nFor each objective we normalize the attribute values in the range [0, 1]. Further, for each objective\nwe limit the number of data points to 60, 000 for computational efficiency.\nTraining Data for the Gating Network To train the gating network we again follow the Wang\net al. (2024a) and train on the following datasets:\n\u2022 HelpSteer (Wang et al., 2023) (37k pairs)\n\u2022 UltraFeedback (Cui et al., 2023) (340k pairs)\n\u2022 SHP (Ethayarajh et al., 2022) (93k pairs)\n\u2022 HH-RLHF (Bai et al., 2022a; Ganguli et al., 2022) (157k pairs)\n\u2022 PKU-SafeRLHF-30K (Ji et al., 2023)\n\u2022 Argilla-Capybara (15k pairs)\n\u2022 Argilla-Math-Preferences (2.4k pairs)\n\u2022 CodeUltraFeedback (Weyssow et al., 2024) (50k pairs)\n\u2022 PRM-Phase-2 (Lightman et al., 2023) (80k pairs)\n\u2022 Prometheus2-Preference-Collection (Kim et al., 2024b) (200k pairs)"}]}