{"title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "authors": ["Kayo Yin", "Chinmay Singh", "Fyodor O Minakov", "Vanessa Milan", "Hal Daum\u00e9 III", "Cyril Zhang", "Alex X. Lu", "Danielle Bragg"], "abstract": "Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of Al resources for STEM education in ASL. We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled words which can later be used to query for appropriate ASL signs to suggest to interpreters.", "sections": [{"title": "1 Introduction", "content": "American Sign Language (ASL) is the primary and the most accessible language for many deaf children in the U.S. Despite the crucial importance of accessible education in ASL, deaf and hard of hearing (DHH) students often face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education (Pagliaro and Kritzer, 2013; Traxler, 2000). Few general educational resources exist in ASL, and even fewer ASL resources exist for STEM content. The scarcity of STEM resources in ASL compounds challenges experienced by DHH students with limited English literacy (Lang and Steely, 2003).\nIn particular, the lack of standardization of STEM terminology in ASL creates obstacles for deaf education (Lang et al., 2007). However, despite the existence of ASL signs for STEM con-"}, {"title": "2 Background & related work", "content": "STEM communication and education in American Sign Language. ASL is a relatively young language and is primarily used by deaf individuals who have historically been underrepresented in science. This in turn limits the growth of ASL in STEM fields (Cavender et al., 2010; Lualdi et al., 2023). As a result, many STEM concepts do not have an agreed upon ASL sign (Lang et al., 2007)."}, {"title": "3 ASL STEM Wiki Dataset", "content": "We now describe how we collected ASL STEM Wiki. Our dataset is published under a license that permits use for research purposes. We also include the datasheet of ASL STEM Wiki in Appendix A with further details."}, {"title": "3.1 Text curation", "content": "We first selected a set of STEM-related Wikipedia articles to interpret. From a public Wikipedia download accessed July 2020, we filtered to popular or important articles, and selected STEM-related articles using topic modeling and manual validation.\nThe final set contains 254 articles, including 113 articles in Science, 50 in Geography, 47 in Technology, 26 in Mathematics, and 18 in Medicine.\nWe also selected a subset of five \"control\" articles, chosen to cover diverse topics and to be short.\nWe use these control articles to verify interpreter quality, as all interpreters were asked to record these articles. The articles selected as control are: Acid catalysis, EDGE species, Hal Anger (person), Relativistic electromagnetism, and Standard score."}, {"title": "3.2 Video collection", "content": "With IRB and ethics review approval, we recruited 37 professional ASL interpreters to provide interpretations of the STEM-related Wikipedia articles described above. Videos were collected through a website, and on their first visit, contributors engaged in a consent process and were asked about basic demographics.\nOf the 37 interpreters we recruited, 59% report their gender as female, 30% as male, 3% other (and 8% undisclosed), with ages in the 28\u201359 range (\u03bc = 42, \u03c3 = 8.6). Eight identified as d/Deaf, 27"}, {"title": "3.3 Dataset annotation", "content": "To help train fingerspelling detection and alignment models described in \u00a74, and to evaluate their performance, we collected annotations for video segments that contain fingerspelling and the English words being fingerspelled. Annotators for this task used our bilingual interface (Figure 2) to view English text and ASL recordings in parallel. Annotators were asked to view the requested content, and mark the start and stop times of each occurrence of fingerspelling in a spreadsheet, and label each with the corresponding English word. They were\nasked to include only strict fingerspelling (e.g. not including numbers or loan signs), and to annotate each word in a contiguous fingerspelled sequence separately. All annotators were professional ASL interpreters, distinct from the set of interpreters we used for data collection, and were compensated at a standard hourly remote interpreter rate.\nTo establish inter-annotator agreement, we first recruited a set of 5 annotators to all annotate the same 15 sentences, randomly selected to span 15 different articles recorded by 15 different interpreters. On this set, we obtain mean pairwise intersection over union (IOU; see \u00a75.4) of 0.7 across frame spans containing fingerspelling, indicating a significant inter-annotator agreement (as a random baseline, we computed IOU scores between annotators after randomly shuffling the frame spans for its sentence, and obtain a mean IOU score of 0.15).\nAfter establishing suitable inter-annotator agreement, additional interpreters annotated a larger set of contents consisting of 15 complete articles: three separate interpretations for each of the five control articles. We selected interpretations at random such that each of the 15 recordings contains different interpreters. We divide the annotations between 2 annotators who participated in the smaller collection, such that one annotator was assigned all recordings for 2 of the control articles, and the other to all recordings of the other 3 control articles, to reduce the time required to familiarize themselves with new content. In total, we collected fingerspelling annotations for 15 articles and 507 sentences."}, {"title": "3.4 Dataset statistics", "content": "The final dataset contains 64,266 videos, each corresponding to an English sentence or section title from a STEM-related Wikipedia article. Because the videos were recorded by professional ASL interpreters, they include plain backgrounds, and the interpreters typically wear plain-colored clothes to help with clarity of the signed content. The total video content is 315.84 hours, with individual"}, {"title": "3.5 Use cases", "content": "Given the novel domain of ASL STEM Wiki, our dataset can be used for various new studies and challenges. We propose a series of appropriate use cases for our dataset with applications in human-centered natural language processing (NLP).\nAutomatic sign suggestion. Our dataset reflects an increased usage of fingerspelling in interpretations of STEM documents. We suggest developing systems to detect when fingerspelling is used and suggest appropriate ASL signs to use instead. Suggestions would be dependent on the domain and context (e.g. \"protein\u201d in the context of nutrition, structural biology, or protein engineering may have distinct ASL signs), as well as on the audience (e.g. the sign to use for an elementary school class may be different from the sign to use with a college audience). Fingerspelling may be appropriate in some cases as well, for example when introducing a new sign that is not well-known."}, {"title": "4 Automatic sign suggestion", "content": "As an initial step towards exploring our dataset for modeling and downstream applications, we focus on automatic sign suggestion. This task can be broken into two separate subtasks: identifying fingerspelled words, and retrieving videos of those words being signed in a dictionary. We focus on modeling the first aspect\u2014given an English sentence and its ASL interpretation, we detect all instances of fingerspelling (fingerspelling detection) and align each detected fingerspelling segment to the word in the English sentence that it\nspells out (fingerspelling alignment)\u2014and leave the retrieval task to future work."}, {"title": "4.1 Formal task specification", "content": "Fingerspelling detection. Given an input ASL video x, represented as a sequence of frames {x\u2081,x\u2082,...,x\u2099}, the goal is to output a set of frame spans F that correspond to instances of fingerspelling: F = {[s\u2081, e\u2081], [s\u2082, e\u2082],..., [s\u2096, e\u2096]}, where frame ranges xs\u1d62 through xe\u1d62 correspond to fingerspelling for all i.\nFingerspelling alignment. Given an input ASL video x, a corresponding English sentence w = {w\u2081,w\u2082,..., w\u2098 }, and a set of frame spans F, the goal is to associate with each span [si, ei] \u2208 F with an index ji \u2208 {1,...,m} such that the word fingerspelled in [xs\u1d62, xs\u1d62] corresponds to the English word w\u2c7c\u1d62 in the sentence."}, {"title": "5 Models for fingerspelling detection and alignment", "content": "We now propose baseline systems for fingerspelling detection and alignment: a neural model on ASL STEM Wiki for fingerspelling detection and a heuristic model based on word frequency for fingerspelling alignment."}, {"title": "5.1 Preprocessing", "content": "We use 2D human pose representations extracted from raw ASL video using MediaPipe (Zhang et al., 2020) with model complexity 1 and minimum detection confidence 0.5. We select 75 keypoints of the two hands and body, and discard face mesh keypoints to reduce dimensionality. We also discard\nthe z-coordinate of each keypoint because depth estimation in MediaPipe may not be accurate."}, {"title": "5.2 Fingerspelling detection", "content": "First, we pre-train models with contrastive learning to learn representations of English-ASL data on\nunlabeled data. Then, we train models on finger-spelling annotations collected in \u00a73.3 to perform fingerspelling detection."}, {"title": "5.2.1 Model architecture", "content": "Our model consists of a video and a text component (Figure 4). We use a Pytorch (Paszke et al., 2017) implementation of the temporal graph con-volutional network from Li et al. (2020) for the\nvideo component of the model. Our network stacks 6 graph convolution blocks with hidden size 64."}, {"title": "5.2.2 Self-supervised pretraining", "content": "Our model is first pre-trained in a self-supervised manner for representation learning. This leverages\nthe English-ASL data in ASL STEM Wiki without requiring explicit labels for fingerspelling. We define two contrastive learning objectives aimed to\nlearn representations of ASL videos, and associations between ASL video and English text:\n\u25b7 Temporal contrastive learning objective (Hu et al., 2021): Given two randomly sampled ASL video clips x\u2081,x\u2082 each of length\n200 frames, the model predicts whether the two clips come from the same video, and if so,\nwhether x\u2081 happens earlier or later than x\u2082 in\nthe video. In examples where x\u2081 and x\u2082 are\nsampled from the same video, we ensure that\nthe original video is longer than 400 frames\nand that there is no overlap between the two\nclips. We balance the number of samples in\neach class and we optimize for cross-entropy\nloss on the trinary prediction.\n\u25b7 Sentential contrastive learning objective:\nWe define a new contrastive learning objective to learn representations between ASL\nvideos and text. Given an ASL video x of\nlength 2000 frames and two English sentences w\u00b9, w\u00b2 each of length 300 characters, the\nmodel predicts which English sentence the ASL video x is an interpretation of. Since we\ntake a shorter sentence length, some sentences\nare truncated, so the model also learns to map\nwhether a subset of frames in x matches a sen-\ntence. We balance the number of samples in\neach class and we optimize for binary cross-\nentropy loss."}, {"title": "5.2.3 Supervised training", "content": "On top of the initialized weights from pretraining, we fine-tune our model for fingerspelling detec-\ntion using the subset of the dataset annotated with\nfingerspelling labels. We represent labels as a se-\nquence y = y\u2081, y\u2082, ..., y\u2099 where n is the number of frames in the input video, y\u1d62 = 1 if the i-th frame\nis part of a fingerspelling segment, and y\u1d62 = 0 oth-erwise. We use weighted binary cross-entropy loss to balance the 0 and 1 labels.\nWe pretrain for 50 epochs and fine-tune for 20\nepochs. To measure the contribution of pretraining, we also train a model from random initialization\nfor 40 epochs. For both pretraining and supervised\ntraining, we use Adam optimizer (Kingma and Ba,\n2014) (lr=0.001) and batch size 32. Each model\nwas trained on one RTX A6000 GPU for 45 hours."}, {"title": "5.3 Fingerspelling alignment", "content": "For fingerspelling alignment, we use a heuristic ap-proach based on the property that fingerspelling is often used to spell infrequent English words\n(Battison, 1978). We first estimate the frequency of English words using 10,000 randomly sampled\nWikipedia articles in English from all domains (not\nnecessarily STEM). Then, given a video where n fingerspelling segments are detected, we predict\nthe n least frequent words in the English sentence as the words fingerspelled in the video segments.\nThe first fingerspelling segment is aligned to the"}, {"title": "5.4 Evaluation", "content": "Given the limited annotated data for fine-tuning and evaluation, we use cross-validation to evaluate\nmodels. The subset with fingerspelling annotations contains 5 articles, we use 4 articles to fine-tune\nand 1 article to evaluate the model, and repeat this process 5 times changing the article used for evaluation each time. We report the average performance\nover samples in all 5 articles. We also evaluate detection and alignment independently to better understand how models perform on each step.\nTo measure detection and alignment accuracy, we compute the intersection over union (IOU).\nGiven a ground truth set of frame spans F and corresponding alignment indices j, and a predicted set of frame spans F' and alignment indices j', we compute $\\text{IOU}(F, F') = \\frac{|F \\cap F'|}{|F \\cup F'|}$, where F = {(fk, w\u2c7c\u2096) : [si, ei] \u2208 F,k \u2208 [si, ei]} is the set of frames (and paired English words) from F,\nand F' is computed equivalently for F'. For evaluating detection alone, we use the same score but ignore the aligned English words."}, {"title": "6 Results & discussion", "content": "6.1 Results\nWe report IOU scores for fingerspelling detection and alignment in Table 4. As a random baseline\nfor fingerspelling detection, we compute the percentage of frames P containing fingerspelling on\na held-out dataset. Then, for each frame in the input video, we predict that it is part of a finger-spelling span with probability P. Similarly for\nfingerspelling alignment, we estimate the percentage of letters P' that is fingerspelled in the video,\nand predict each letter in the input sample that it is fingerspelled with probability P'.\nWe find that both baseline models with and without pre-training have a higher IOU than random. In"}, {"title": "6.2 Error analysis & discussion", "content": "We conduct an error analysis of our fingerspelling detection and alignment models by manually in-specting failure cases to better understand the per-formance limitations and potential areas for im-provement.\nFor fingerspelling detection, our model fre-quently exhibits false positives on instances of one-handed ASL signs with rapid hand movements.\nOn the other hand, the model struggles with false\nnegatives in instances involving very short finger-spelling words (e.g. two-letter abbreviations), or in dynamic contexts where there is a seamless transi-tion between ASL signing and fingerspelling, espe-cially in fast-paced signing, so the model struggles to isolate the fingerspelling segments.\nFor fingerspelling alignment, the main source of error comes from a difference in the order in which different fingerspelled words appear in the ASL video and the English sentence. Our heuristic\nmethod makes the assumption that the fingerspelled words in ASL and English appear in the same order, which is not always true due to differences in syntax\nbetween the two languages. Therefore, a heuristic that takes into account the syntax of English and ASL, or a model trained to perform fingerspelling alignment, could improve performance. Another common error is when a fingerspelling clip contains a compound word or multiple words, but only one\nword from the English sentence is mapped to the clip.\nWe also believe that lack of data with finger-spelling annotations is a bottleneck of this chal-lenge, for both fingerspelling detection and align-ment. While our presented dataset provides a large amount of video data, it does not include annota-tions over all videos. To improve model perfor-mance, future work can expand dataset size by ei-ther collecting and annotating more data, perform-ing data augmentation, or using synthetic data, es-pecially targeting the type of data where our model has often made errors as described above. We also\nfind evidence of contrastive learning improving fin-gerspelling detection, therefore we also suggest"}, {"title": "7 Conclusion", "content": "We present ASL STEM Wiki: a large dataset of Wikipedia articles on STEM topics professionally interpreted from English to ASL. The first continuous sign language STEM dataset, ASL STEM\nWiki captures unique characteristics of interpreted STEM content and serves as a valuable resource for developing AI tools that enhance the accessi-bility of STEM education for deaf students. We also present baseline models working towards au-tomatic sign suggestion, which highlight the chal-lenge of this new task while also suggesting that\ncontrastive learning may be a promising approach. We invite others to use ASL STEM Wiki to fur-ther advance automatic sign suggestion and address other new challenges in human-centered NLP."}, {"title": "Limitations", "content": "We discussed how interpreters may resort to sub-optimal strategies when interpreting STEM docu-ments, such as frequent fingerspelling in \u00a72. As a re-sult, videos in ASL STEM Wiki may not be suitable for training a generative ASL model since it is not\nun-prompted natural signing and may have notice-able translationese effects from English. We, there-fore, discuss appropriate use cases of our dataset with these properties in mind in \u00a73.5, and also leverage these features to study how AI systems\ncan assist humans on difficult interpretations to overcome these limitations.\nFor dataset collection, we recruited interpreters\nfrom an accredited interpreting services provider. We initially recruited certified deaf interpreters\n(CDI), however, the volume of interpretations be-came too large for the CDI pool. We therefore also included certified hearing ASL interpreters. Al-though all interpreters are RID and DBE certified,\nthe skill and fluency of signing may vary between interpreters included in our dataset.\nWe provide baseline models of fingerspelling de-tection and alignment that can be later used to query for ASL signs from a lexicon, with the motivation\nto use this pipeline for automatic sign suggestion. However, as discussed in \u00a73.5, a full, working sys-tem for automatic sign suggestion would require\nfine-grained understanding of the domain and con-text to suggest signs that are appropriate to the document and the audience, which exceeds cur-"}]}