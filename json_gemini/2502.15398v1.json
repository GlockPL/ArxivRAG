{"title": "Enhancing Vehicle Make and Model Recognition with 3D Attention Modules", "authors": ["Narges Semiromizadeh", "Omid Nejati Manzari", "Shahriar B. Shokouhi", "Sattar Mirzakuchaki"], "abstract": "Vehicle make and model recognition (VMMR) is a crucial component of the Intelligent Transport System, garnering significant attention in recent years. VMMR has been widely utilized for detecting suspicious vehicles, monitoring urban traffic, and autonomous driving systems. The complexity of VMMR arises from the subtle visual distinctions among vehicle models and the wide variety of classes produced by manufacturers. Convolutional Neural Networks (CNNs), a prominent type of deep learning model, have been extensively employed in various computer vision tasks, including VMMR, yielding remarkable results. As VMMR is a fine-grained classification problem, it primarily faces inter-class similarity and intra-class variation challenges. In this study, we implement an attention module to address these challenges and enhance the model's focus on critical areas containing distinguishing features. This module, which does not increase the parameters of the original model, generates three-dimensional (3-D) attention weights to refine the feature map. Our proposed model integrates the attention module into two different locations within the middle section of a convolutional model, where the feature maps from these sections offer sufficient information about the input frames without being overly detailed or overly coarse. The performance of our proposed model, along with state-of-the-art (SOTA) convolutional and transformer-based models, was evaluated using the Stanford Cars dataset. Our proposed model achieved the highest accuracy, 90.69%, among the compared models.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of Intelligent Transportation Systems (ITS), a novel topic pertaining to vehicle analysis is VMMR. In the automotive industry, the term \"make\" denotes the producer of a specific vehicle (such as Hyundai, Ford, and Toyota), whereas \"model\" specifies a certain kind of vehicle created by these producers (such as Azera, Corolla, and Focus). Traffic control, surveillance, traffic statistics, law enforcement, self-driving vehicles, and the detection of suspicious vehicles are just a few of the fields in which VMMR systems are extensively utilized [1].\nVMMR can be regarded as a kind of fine-grained classification, which aims to identify and distinguish between categories with very similar features in a classification problem. The enormous existing categories, low inter-class and high intra-class variance make VMMR a highly challenging problem. The appearance of subtle visual disparities in specific cases can make recognition extremely difficult, even for humans [2].\nNowadays, deep learning networks are frequently used in varying computer vision tasks, one of which is image recognition. CNNs, as one of the deep learning models, have remarkably enhanced the performing of vision-related tasks due to their rich representational power [3-6]. Many of the proposed methods for VMMR have utilized CNN-based models and have achieved remarkable results. Transformer is also a kind of deep neural network that was first used in the field of natural language processing (NLP). Motivated by the significant achievement of the transformer in the field of NLP, transformer based network is used in many computer vision tasks including image recognition [7]. Since the transformer is based on the self-attention mechanism, it is suitable for extracting long-range dependencies and is more capable of extracting global features [8]. In contrast, CNNs are more capable of extracting local features.\nIn human perception, attention is crucial. One key aspect of the human eye anatomy is that individuals do not seek to process an overall scene simultaneously. Rather, people utilize a subset of partial glimpses and selectively concentrate on prominent elements to more effectively grasp visual structure. This concept inspired the creation of the attention mechanism in deep learning [9]. Since in computer vision tasks, including image recognition, both local and global features are significantly important, the attention mechanism can be employed to enhance CNNs' ability to extract global information.\nIn this paper, we present an enhanced network for VMMR that incorporates a simple and parameter-free attention module (SimAM). This module, which we demonstrate to be more efficient than other existing SOTA attention modules, is combined with a suitable convolutional model in an effective manner. The result is a significant increase in the model's representational power. The attention mechanism guides the model to focus on important section of an image, extracting the most information while ignoring less important areas. This reduction in the negative impact of intra-class variation and inter-class similarity instills confidence in the effectiveness of our approach. The rest of this study is organized as bellows:\nWe investigate the related works in Section II. Section III explains the structure of our proposed model. In Section IV, we demonstrate the results of evaluating the performance of our proposed network and other SOTA architectures and analyze our results. Finally, we bring to a conclusion our study in Section V."}, {"title": "II. RELATED WORKS", "content": "In this section, several notable works on image recognition models and attention modules are briefly reviewed."}, {"title": "A. Deep Learning Models", "content": "CNNs are commonly found in a variety of computer vision tasks, including VMMR. Several studies concentrate on de- signing model architectures to enhance representational power of CNNs. For instance, in ResNet [3], shortcut connections are utilized, allowing the CNN to expand to hundreds of layers. Their findings reveal that raising the architecture depth can considerably strengthen the representation power of a CNN. DenseNet [5] employs dense connections across layers, allow- ing each layer to access the feature maps of all previous layers as inputs. This design provides several key advantages: it helps alleviate the vanishing-gradient issue, facilitates feature reuse, improves feature propagation, and notably reduces the total pa- rameter count. Based on the depthwise separable convolutions [10], proposes MobileNetV2, which is specifically designed for limited resources and mobile environments, which uses a new inverted residual structure as the building block. Using neural architecture search, [11] designs a new baseline archi- tecture and scales it up using a new compound scaling method to derive a family of models known as EfficientNets that outperform earlier CNNs in terms of efficiency and accuracy.\nTransformer performance in the field of NLP drew the attention of the computer vision society to it, and with the in- troduction of ViT, Transformer was seriously incorporated into computer vision tasks. Following the ViT, other transformer- based models were proposed to improve performance in com- puter vision tasks. For example, Swin Transformer [12], which functions effectively as a versatile backbone for computer vision, generates a hierarchical structure design and has linear computational cost in relation to the size of the input image. Shifted window-based self-attention, a fundamental compo- nent of Swin Transformer, is demonstrated to be successful and efficient in addressing vision challenges. CrossViT [13] is a"}, {"title": "B. Attention Modules", "content": "Various works have introduced attention modules that can be applied to many CNNs [16, 17]. These attention modules modify feature maps, empowering the CNN to target on im- portant areas of the image or specific and informative features and make decisions based on them. A representative work is Squeeze-and-Excitation (SE) [4], which proposes channel- wise attention and models the interdependencies between the channels. This is achieved via a three-step process: squeeze (global average pooling), excitation (learning channel depen- dencies with fully connected layers), and scale (reweighting feature maps). Later works, CBAM [9] combines spatial and channel-wise attention mechanisms. SRM [18] improves a CNN's representational strength by embedding styles into the feature maps. GC [19] effectively captures long-range dependencies to enhance semantic comprehension. ECA [20] adopts a local cross-channel interaction method without di- mensionality reduction, implemented efficiently through 1D convolution. CA [21] presents a new lightweight attention method, termed Coordinate Attention, designed for mobile net- works. This mechanism combines the advantages of channel attention in modeling inter-channel relationships with the abil- ity to capture long-range dependencies and maintain accurate positional information. To compute the 3-D weights, SimAM [22] designs an energy function using some established neuro- science theories.Using simple yet effective operations, such as average pooling to capture long-range dependencies and the dot product to represent cross-dimensional interactions, SIAM [23] generates 3-D attention maps with minimal computational overhead."}, {"title": "A. Revisiting attention modules", "content": "Attention modules are generally embedded within each block to enhance the outputs of preceding layers. This re- finement usually occurs along either the channel or spatial dimension, producing 1-D or 2-D weights and treating neurons similarly within each channel or spatial position, which may limit the capacity to learn more distinct features. Consequently, generating 3-D attention weights and assigning a unique weight to each neuron for refining the feature map within a layer is considered more effective.\nDirect estimation of 3-D weights is challenging. In [24], the use of an encoder-decoder framework for learning 3-D weights is proposed. However, this method introduces various sub-networks from lower layers to higher layers of a ResNet [3], making it difficult to integrate into other modularized networks. CBAM [9] is another example, applying channel and spatial attention modules in sequence without directly generating 3-D weights. This two-step approach in CBAM requires substantial computation time, suggesting that 3-D weight computation should be streamlined to keep the module efficient and lightweight.\nThe method of weight production is another crucial aspect of attention modules. Designing the structure requires signifi- cant engineering, and attention weights are calculated using a few groundless heuristics in the majority of studies that currently exist. For instance, compared to SE [4], CBAM [9] uses global max pooling (GMP) in addition to global average pooling (GAP) to create finer attention. As shown in Table I, attention modules are based on many common operators such as FC, Conv2D, BN, etc., and some special operators such as channel-based convolution (C1D).\nSimAM can produce 3-D attention weights for a layer's feature map, assigning a distinct weight to each neuron. This module is inspired by the concept of spatial suppression in mammalian brains, utilizing an energy function to calculate these weights. Furthermore, SimAM [22] speeds up weight computation by employing a fast closed-form solution for the energy function."}, {"title": "III. METHODOLOGY", "content": "In this section, we present our proposed model. To provide a clearer understanding of its architecture, we first review the characteristics of some well-known attention modules. We then outline the formulation of SimAM [22] before detailing our model's structure."}, {"title": "B. SimAM attention module", "content": "In SimAM [22], each neuron's importance is estimated to produce 3-D attention weights. In visual neuroscience, neurons that provide the most critical information often display unique patterns compared to their neighbors. Additionally, the spatial suppression phenomenon indicates that an active neuron can suppress the activity of its neighboring neurons. Neurons exhibiting strong spatial suppression effects tend to be more significant in visual processing. To identify such neurons, we can measure the linear separability between a target neuron and its surrounding neurons. This understanding leads to defining each neuron's energy function as follows:\n$e_{n}(w_{n},b_{n}, P, q_{i}) = (p_{n} \u2013 w_{n}n \u2013 b_{n})^{2} + \\frac{1}{M-1}\\sum_{i=1}^{M-1} (p_{0}\u2013 w_{n}q_{i} \u2013 b_{n})^{2}$ (1)\nHere, the linear transformations of $n$ and $q_{i}$ are $\\hat{n}= w_{n}n + b_{n}$ and $\\hat{q_{i}} = w_{n}q_{i} + b_{n}$, where $n$ and $q_{i}$ represent the target neuron and other neurons within a single channel of the input feature $Z \\in R^{D \\times H \\times W}$, i indexes across the spatial dimension, and $M = H \\times W$ represents the number of neurons in that channel. $w_{n}$ and $b_{n}$ are the weight and bias of the transformation. When $\\hat{n}$ matches $p_{n}$ and all other $\\hat{q_{i}}$ are $p_{0}$ (where $p_{n}$ and $p_{0}$ are distinct values), Eqn (1) achieves its minimum value. Minimizing Eqn (1) equates to identifying the linear separability between the target neuron n and all other neurons within the same channel. By assigning binary labels (e.g., 1 and -1) to $p_{n}$ and $p_{0}$ and introducing a regularizer, the final energy function becomes:\n$e_{n}(w_{n}, b_{n}, P, Q_{i}) =  \\frac{1}{M-1}\\sum_{i=1}^{M-1} (-1- (w_{n}q_{i} + b_{n}))^{2}$  + $(1 \u2212 (w_{n}n + b_{n}))^{2} + \u03bbw_{n}^{2}$ (2)\nThe closed-form solution for Eqn (2) is computed as follows:\n$w_{n} = \\frac{2 (n - a_{n})}{(n \u2212 a_{n})^{2} + 2\\beta^{2} + 2\u03bb }$ (3)\n$b_{n} = - \\frac{1}{M-1} (\\beta_{a} -a_{n}) w_{n}$ (4)\n$\u03b2^{2} =  \\frac{1}{M-1} \\sum (q_{i} - a_{n})^{2}$ and $a_{n} = \\frac{1}{M-1} \\sum q_{i}$ are the variance and mean calculated over all neurons except n in that channel. Assuming that all pixels of a single channel follow the same distribution, the mean and variance are calculated once and reused across all neurons in that channel, significantly reducing computational costs. Thus, the minimum energy is computed as follows:\n$e_{n}^{*} = \\frac{4 (\u03b2^{2} +\u03bb)}{(n \u2212 a_{n})^{2} + 2\\beta^{2} + 2\u03bb }$ (5)\nwhere $a = \\sum_{i=1}^{M} q_{i}$ and $\u03b2^{2} = \\sum_{i=1}^{M} (q_{i} - a)^{2}$. Eqn (5) suggests that a lower energy $e_{n}^{*}$ indicates that the neuron n is more distinct from its neighbors, making it more important for visual processing. Consequently, each neuron's importance can be represented by 1/$e_{n}^{*}$. The final refinement phase of the module is expressed as:\n$\\tilde{Z} = sigmoid (\\frac{1}{F} ) \\bigodot Z$ (6)\nwhere Faggregates all $e_{n}^{*}$ across channel and spatial dimensions. The sigmoid function is included to constrain overly large values in $F$ [22]."}, {"title": "C. Using SimAM within EfficientNet", "content": "SimAM is compatible with most standard CNN architec- tures. We select a strong convolutional model, EfficientNet- B4 [11], as the baseline model. By deploying SimAM to this baseline model, we obtain a new model called SimAM- EfficientNetB4, as shown in Figure 1. EfficientNet's core building block is the mobile inverted bottleneck, MBConv, which is the same inverted residual block used in the structure of MobileNetV2 [10]. However, it not only uses 3\u00d73 convo- lutions but also 5\u00d75 convolutions and includes the squeeze- and-excitation optimization [4]. According to Table II, which details the architecture of EfficientNet-B4, it consists of nine stages. The fourth stage has four MBConv blocks and the fifth stage has six MBConv blocks. The attention module is incorporated into a convolutional neural network at a layer where the feature map extracted holds an optimal level of information from the input frames, balancing between detail and abstraction. To this purpose, the SimAM is added to EfficientNet-B4 in two parts: 1) In the first MBConv of the fourth stage, after the 5\u00d75 depthwise convolution (DW Conv) and before BN. The attention module receives a feature map of size 28\u00d728\u00d7192 and outputs a modified feature map of the same size, 28\u00d728\u00d7192. 2) In the first MBConv of the fifth stage, after the 3x3 depthwise convolution and before BN. The attention module receives a feature map of size 14\u00d714\u00d7336 and outputs a modified feature map of the same size, 14\u00d714\u00d7336."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we begin by presenting the dataset utilized to train our proposed model, followed by a description of the experimental settings. Next, we perform an ablation study to demonstrate the impact of the coefficient value within the attention module of our model. Lastly, we compare our model with state-of-the-art (SOTA) models, evaluating accu- racy, model parameters, and floating-point operations (FLOPs) on the target dataset."}, {"title": "A. Dataset and Implementation details", "content": "We conduct experiments and evaluate the effectiveness of our proposed model on a vehicle dataset. The Stanford Cars [25] dataset consists of 16,185 images of cars across 196 classes, with roughly 50 by 50 divisions between training and testing data. The training set includes 8,144 images, while the testing set comprises 8,041 images, each representing one of the 196 car classes. The images are taken from different angles and provide different views of the vehicles.\nTo conduct our experiments, we used Google Colab as our coding environment, which offered a Tesla T4 GPU and 15 gigabytes of RAM. We used the PyTorch framework to imple- ment our experiments. We trained and tested our proposed and several SOTA models on the Stanford Cars dataset, including convolutional and transformer-based models. We used the Adam optimizer to train our proposed model and all convolu- tional models. Additionally, a CosineAnnealingLR scheduler with a T-max of 21000 and eta-min of le-7 was employed to decrease the learning rate during the training process. All transformer-based models were trained using the Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 and a weight decay of le-4. A StepLR scheduler was utilized to decrease the learning rate during the training process, such that the learning rate was divided by 10 after every 20 epochs. All models were trained for 80 epochs. The initial learning rate was set to le-3. We set the batch size for both testing and training to 32 and used the Cross-Entropy loss function. The images were resized to 224 x 224. We employed several data augmentation techniques, such as random horizontal flip, random rotation up to 15, random grayscale, and random posterization during training."}, {"title": "B. RESULTS", "content": "Table III presents a performance evaluation of various SOTA models alongside the proposed model using the Stanford Cars dataset. The comparison is based on three metrics: Accuracy, Parameters in millions, and FLOPs in billions. The proposed model achieves the highest accuracy at 90.69%, outperforming all other models while maintaining a relatively low parameter count and FLOPs. This indicates that our model is not only more accurate but also more efficient in terms of computational resources compared to many SOTA methods.\nTable IV examines the performance metrics of the proposed model at various values of the hyperparameter $\\lambda$. The pro- posed model exhibits its best performance with an accuracy of 90.69% at $\\lambda$ = 7 \u00d7 $10^{-4}$. The parameters and FLOPs remain unchanged across different $\\lambda$ values, indicating that the computational efficiency and model complexity are unaffected by the variations in $\\lambda$. Therefore, adjusting $\\lambda$ primarily impacts the accuracy, with $\\lambda$ = 7 \u00d7 $10^{-4}$ being the optimal value for the highest accuracy."}, {"title": "V. CONCLUSION", "content": "In this study, we addressed the challenges of VMMR by in- tegrating an attention module into a convolutional model. The attention module, designed to focus on critical areas containing distinguishing features, was incorporated parameter-free, en- hancing the model's ability to handle inter-class similarity and intra-class variation without increasing computational com- plexity. Using the Stanford Cars dataset, our proposed model was evaluated alongside CNN and transformer-based methods. The results demonstrated that our model outperformed all other compared models. The analysis also showed that our model maintained a relatively low parameter count and FLOPs, indicating its efficiency in computational resources. Future work could explore the application of this attention mechanism to other fine-grained classification problems and investigate its integration with more advanced neural network architectures."}]}