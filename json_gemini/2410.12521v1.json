{"title": "Spectrum Sharing using Deep Reinforcement Learning in Vehicular Networks", "authors": ["Riya Deshpande", "Faheem A. Khan", "Qasim Zeeshan Ahmed"], "abstract": "As the number of devices getting connected to the vehicular network grows exponentially, addressing the numerous challenges of effectively allocating spectrum in dynamic vehicular environment becomes increasingly difficult. Traditional methods may not suffice to tackle this issue. In vehicular networks safety critical messages are involved and it is important to implement an efficient spectrum allocation paradigm for hassle free communication as well as manage the congestion in the network. To tackle this, a Deep Q Network (DQN) model is proposed as a solution, leveraging its ability to learn optimal strategies over time and make decisions. The paper presents a few results and analyses, demonstrating the efficacy of the DQN model in enhancing spectrum sharing efficiency. Deep Reinforcement Learning methods for sharing spectrum in vehicular networks have shown promising outcomes, demonstrating the system's ability to adjust to dynamic communication environments. Both SARL and MARL models have exhibited successful rates of V2V communication, with the cumulative reward of the RL model reaching its maximum as training progresses.", "sections": [{"title": "I. INTRODUCTION", "content": "Beyond 5G encompasses not only wireless communications but also a myriad of applications, including Teleportation, Augmented Reality (AR)/ Virtual Reality (VR), Vehicular Communications, Robotics, and many more [1], [2]. This progressive shift is driving innovations and advancements in diverse fields, ushering in a new era of connectivity and technological possibilities [3], [4]. Vehicular communication refers to a network of multiple vehicles communicating with each other or the infrastructure. In other words vehicular communication is also known as Vehicle to Everything (V2X) communication [5]. V2X network refers to the communication between wireless communication links such as Vehicle-to-infrastructure (V2I) and Vehicle-to-vehicle (V2V). In traditional V2X networks, managing spectrum efficiently poses a significant challenge [4]. This difficulty arises due to the dynamic nature of the environment, where vehicles are in constant motion and communication necessities fluctuates [6]. The V2X network has limited bandwidth and it is necessary to manage the limited spectrum efficiently.\nRecognizing the complexities involved, the implementation of Artificial Intelligence (AI) has emerged as a promising solution to address these challenges and automate the spectrum management process. One specific AI approach that has shown notable progress in resolving the inefficiencies associated with spectrum sharing in V2X networks is Reinforcement Learning (RL). RL which is a Machine Learning algorithm introduced was aimed to address the challenge of inefficient spectrum sharing, and it has demonstrated significant progress over time. Its ability to learn from interactions and adapt to changing conditions makes it well-suited for optimizing spectrum allocation in real-time scenarios [3], [4], [7].\nBy leveraging RL algorithms, V2X networks can enhance their adaptability of the dynamic vehicular environment and responsiveness to the requirement of frequency spectrum, ensuring more efficient utilization of available spectrum resources. The interference between channels was gradually reduced after the implementation of the algorithm. This not only addresses the challenges posed by the dynamic movement of vehicles but also contributes to the overall improvement of communication reliability and performance in V2X systems over time. The vehicular networks are based on decentralised decision making where the agents interacts, observes and make decisions on its own unlike other wireless networks where the decisions are taken by the network or infrastructure. In this paper a Deep Reinforcement Learning (DRL) model for the Cellular Vehicle-to-Everything (C-V2X) system is implemented. The system involves vehicles acting as agents that interact with the vehicular environment, generating rewards based on these interactions."}, {"title": "II. RELATED WORK", "content": "The progress toward beyond 5G communications is strongly driven by the need to support Vehicular Communication, encouraged by the influence of the 5G Automotive Association (5GAA). Researchers have conducted surveys to examine how the telecommunication industry can contribute to advancing Vehicle-to-Everything (V2X) applications [5]. In a related study [8], investigations were undertaken to explore radio communication technologies like millimeter wave (mm-wave), Cellular V2X (C-V2X), and beyond 5G technologies for autonomous vehicles. The study encompassed discussions on challenges associated with these technologies and proposed solutions, specifically addressing the compatibility of wireless communication with vehicular network architecture. Machine Learning (ML) techniques were introduced in the field of vehicular communication. Authors in [9] explored various opportunities and challenges associated with the application of AI and ML techniques in V2X for different applications. Additionally, they provided a description of the vehicular network architecture.\nThe utilization of RL techniques is expanding over time. The combination of RL with Neural Networks has proven successful in addressing numerous problems. RL is based on the concept of trial-and-error where the agent interacts with the environment and observes the state and receives reward or penalties in terms of feedback. Neural Networks integrated with RL termed as Deep Reinforcement Learning (DRL) has been successful in automating the spectrum sharing process and improved the system efficiency and made the system scalable. A DRL model for spectrum sharing for Device to Device (D2D) communication was implemented by authors in [10] a Multi-Agent Reinforcement Learning (MARL) was designed and simulated.\nIn V2X communication, there exist multiple heterogeneous networks, and the requirements continually evolve based on the available network size. It is imperative for the system to adapt to these changes and ensure the fulfillment of Quality of Service (QoS) requirements. The authors in [11] successfully applied the Multi-agent Deep Reinforcement Learning (MADRL) technique to ensure QoS in vehicular networks, yielding promising results. Additionally, in [12], a comparison between two RL models, SARL and MARL, utilizing Neural Networks for spectrum sharing in V2X networks was conducted. The study concluded that RL models achieved a high success probability for V2V transmission. This paper focuses on implementing the RL technique using Neural Network, specifically the DQN model, with a detailed description of the design provided in the next section."}, {"title": "III. SYSTEM MODEL", "content": "Reinforcement Learning (RL) has made significant advancements in the realm of vehicular communication, and various algorithms have been developed for its implementation. This paper focuses on employing a DQN model, which integrates both RL and Deep Learning (DL), to address spectrum sharing challenges in vehicular networks. This section delves into the system's methodology, offering a comprehensive overview.\nIn the context of RL, the dynamic between the agent and the vehicular environment is mathematically represented by a Markov Decision Process (MDP), as illustrated in Fig 1. The agent observes the current state of the environment, learns from it, and subsequently makes relevant decisions. In simpler terms, the agent acts as the learner, while the environment encompasses all elements with which the agent interacts. [13] The main aim of MDP is to maximize the cumulative reward through the agent and environment interaction by trial-and-error method.\nAt each discrete time step t, the agent is provided with a representation of the environment, indicated as $s_t \\in S$, commonly known as the state. Based on this information, the agent takes an action $a_t \\in A$, resulting in a numerical reward denoted as $r_{t+1}$, and leading the environment to transition to a new state $S_{t+1}$. The Markov property asserts that the current state encapsulates all relevant information about both previous and future states, with future states being independent of past states. In the realm of Reinforcement Learning (RL), the primary objective for an agent is to maximize future rewards, even though a high reward at the present state does not necessarily guarantee superior rewards in the future. In this paper, we consider a spectrum sharing scenario illustrated in Fig. 2 involving Y V2I links and X V2V links, where Y = {1, 2, .., y} and X = {1,2, .., x}. The V2I links are high frequency communication links and are connected directly to the base station for data services such as live streaming or surfing internet. It is assumed that V2I links Y are preoccupied and reused by V2V links X.\nThe channel power gain for x V2V links over y V2I link is denoted in (1).\n$g_{x[y]} = a_x l_x[y]$", "latex": ["s_t \\in S", "a_t \\in A", "g_{x[y]} = a_x l_x[y]"]}, {"title": "A. Multi-Agent Reinforcement Learning for Spectrum sharing", "content": "RL encompasses two noteworthy models: Single Agent Reinforcement Learning (SARL) and Multi-Agent Reinforcement Learning (MARL). SARL addresses scenarios where a single agent interacts with the environment, generating rewards. Conversely, MARL involves multiple agents interacting simultaneously with the environment. The primary objective of these models is to enhance spectrum efficiency and ensure scalability, allowing a maximum number of devices to communicate efficiently with minimized interference. In these complex and uncertain situations RL algorithms have provided promising results. Within the spectrum sharing context, numerous V2V links endeavor to access the constrained V2I spectrum that is already in use. In this scenario, V2V links function as agents interacting with the vehicular environment, making observations that are subsequently utilized in policy design. The V2V agents refine strategies for spectrum sharing and power control based on their experiences derived from the current state of the environment. The MARL model, as applied in this paper, is segregated into training and testing phases, with a detailed description provided for the fundamental processes involved in MARL-based spectrum sharing.\n1) State and Observation: In RL, the agent explores the vehicular environment for discrete time t, and make some observations $Z_t^{(x)}$ is received by agent of current state $S_t$. The state space is mathematically described in MDP. Here observation $Z_t^{(x)} = O(S_t, x)$ for individual agent x. The Observation space includes own channel information, information of local channel, and interference links from all transmitters in the network.\n2) Action: The process of spectrum sharing involves the selection of a spectrum band and the control of transmission power. In the simulation, the power of X V2V links has been restricted to four levels, namely [23, 10, 5, -100] dBm. Specifically, a power level of -100 dBm signifies zero transmit power. The actions in this context are represented by pairs of spectrum and power.\n3) Reward Design: The primary objective of the model is to optimize the success probability of V2V payload transmission and enhance the V2I transmission capacity. The RL algorithm aims to maximize the cumulative reward, consequently increasing the likelihood of achieving maximum data transfer for V2V links. The calculation of the cumulative reward is carried out during the training episodes\n4) Training: The training process is segmented into 3000 episodes, each with a V2V payload generation time of T = 100ms and varying sizes P = [1, 2, ...] * 1060 bytes. During each episode, the agent observes the environment and, based on these observations, selects suitable actions along with an exploration rate. The DQN for each V2V agent comprises hidden fully connected layers with 500, 250, and 120 neurons, respectively. The neural network illustrated in Fig 3 is employed to learn the policy, mapping states to actions and generating state-action pairs. A learning rate of 0.001 is utilized, and the mini-batches are divided into 2. The algorithm for MARL was implemented for training of the agents. The training and testing models were simulated separately. Relevant actions are selected after the observation process and the outcomes are stored into replay memory buffer. The channel fading is updated to reduce the train loss and interference."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section results for the model implemented are discussed, these results are generated in python using TensorFlow and NumPy frameworks. The channel model for V2I and V2V links is described in 3GPP documentation as well as the cellular scenario used to implement the environment simulator is mentioned in [16]. Fig 4 illustrates the V2V success probability across test episodes, offering insights into the system's performance. The plot indicates the likelihood of establishing a successful V2V link. While the SARL model exhibits the highest probability, the MARL model demonstrates a more consistent and stable payload transmission. The cumulative reward, as shown in Fig 5, is computed with a constant payload size of P = 2,120 Bytes during training the model. The channel model specifications are outlined in [14]. In ideal scenario it is assumed that RL has to maximize the cumulative reward so that maximum data is transferred. The cumulative reward is calculated while training the agents and it is the sum of all rewards received in one episode. The ROC converges after 2500 episodes. Fluctuations in the plot represents that results are influenced by channel fading. The path loss and channel fading parameters are mentioned in [14].\nThe findings presented in Table 1 showcase the Average V2I transmission rate, measured in Mega bits per second (Mbps), and the V2V success probability. The conclusion drawn is that the RL model exhibits a higher probability of successfully establishing V2V links. The results indicate that the DQN model adeptly adjusts to the dynamic and uncertain vehicular environment. This adaptability contributes to the overall efficiency of the system, as successful communication links are established across the network. The transmission rate is notably influenced by the optimization of decisions made by DQN agents."}, {"title": "V. CONCLUSION", "content": "The work was focused on the implementation of spectrum sharing model in vehicular networks using deep reinforcement learning algorithm. The study involves research on RL for Vehicular Network and the implementation of a DQN model. The results shows the increase in successful transmission of V2V links and the cummulative reward is maximized while the training is progressed. It is concluded that the average transmission rate for RL model is comparatively higher than other algorithm."}]}