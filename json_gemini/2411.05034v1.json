{"title": "Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion", "authors": ["Tiantian Liu", "Hongwei Yao", "Tong Wu", "Zhan Qin", "Feng Lin", "Kui Ren", "Chun Chen"], "abstract": "Embeddings have become a cornerstone in the functionality of large language models (LLMs) due to their ability to transform text data into rich, dense numerical representations that capture semantic and syntactic properties. These embedding vector databases serve as the long-term memory of LLMs, enabling efficient handling of a wide range of natural language processing tasks. However, the surge in popularity of embedding vector databases in LLMs has been accompanied by significant concerns about privacy leakage. Embedding vector databases are particularly vulnerable to embedding inversion attacks, where adversaries can exploit the embeddings to reverse-engineer and extract sensitive information from the original text data. Existing defense mechanisms have shown limitations, often struggling to balance security with the performance of downstream tasks. To address these challenges, we introduce Eguard, a novel defense mechanism designed to mitigate embedding inversion attacks. Eguard employs a transformer-based projection network and text mutual information optimization to safeguard embeddings while preserving the utility of LLMs. Our approach significantly reduces privacy risks, protecting over 95% of tokens from inversion while maintaining high performance across downstream tasks consistent with original embeddings.", "sections": [{"title": "Introduction", "content": "In recent months, large language models (LLMs) such as ChatGPT [8], Claude [5], and ChatGLM [16] have garnered significant popularity among the general public. These models have demonstrated human-level accuracy and proficiency in a wide range of downstream tasks, including writing long stories, answering professional questions, and translating. In practical applications, online LLMs typically integrate several crucial components and techniques, such as vector databases, planning units, action execution units and prompt engineering. These integrated components enhance the models' capabilities in information retrieval, interaction, and logical reasoning, thereby overcoming limitations related to memory and timeliness.\nAmong aforementioned components, embedding vector databases [40, 54] play a critical role, functioning as a long-term memory system that alleviates the inherent memory constraints of LLMs. By integrating embedding databases with LLMs through a retrieval mechanism, retrieval-augmented generation (RAG) [20,65] has emerged as a powerful tool for developers in AI assistant APIs. Upon receiving a query (e.g., a partial sentence), the system first retrieves the top-k most relevant passages from the vector database to serve as prompts, thereby enhancing the quality and factual accuracy of the generated text. Recently, OpenAI's release of the embeddings API 1 has further empowered LLMs' capability by providing access to the embedding vectors of submitted queries, facilitating tasks such as similar query searches, clustering, and recommendations.\nDespite the extensive advantages offered by embedding vector databases, their use also raises significant concerns regarding privacy leakage. Embedding vectors store rich, dense representations of text data, capturing both semantic and syntactic properties. This wealth of information, if not properly secured, poses a risk of sensitive or personal data exposure. Recent pioneering studies have revealed that embedding vectors are vulnerable to well-designed attacks targeting privacy leakage. These attacks can be broadly categorized into three main types: embedding inversion attacks [24,27,49], membership inference attacks [46, 49], and attribute inference attacks [33,58]. Embedding inversion attacks exploit vulnerabilities in embedding vectors to extract and disclose sensitive information from the original input. If an adversary obtains an embedding vector, they may be able to reverse-engineer the raw input query. These risks highlight the pressing need for developing effective strategies to mitigate privacy leakage from embedding vectors.\nIn response to the threat of privacy leakage, researchers have proposed several general defense mechanisms against inversion attacks. These defenses can be categorized into noise superposition-based defenses [60,64], perturbation and rounding-based defenses [19], and Differential Privacy (DP)-based defenses [29,41]. However, existing defense approaches exhibit certain limitations that restrict their practical effectiveness against embedding inversion attacks. Defending against these attacks presents unique challenges. Firstly, embeddings encapsulate sensitive input features that are difficult to disentangle directly. Secondly, modifying embedding layers inherently compromises the accuracy of LLMs on downstream tasks. Thirdly, the flexibility of LLMs in performing various downstream tasks, such as sentiment analysis, natural language inference, and text summarization, poses a significant challenge for defense methods to preserve the utility of LLMs. This raises a pivotal question: how can sensitive information in the embedding vector be effectively detached while maintaining its utility for downstream tasks?\nIn order to address this question, we conduct a systematic investigation of the potential privacy vulnerabilities during the inference phase of LLMs. Our research reveals that the optimization process of the embedding model fosters a strong correlation between the original inputs and their respective embedding vectors. As a means to mitigate the threat posed by embedding inversion attacks, the most straightforward and effective approach is to disrupt this correlation. To solve this problem, we consider to project the original embedding space into a secured embedding space, wherein the sensitive features are detached.\nIn this paper, we propose Embedding Guard (Eguard), a novel defense mechanism designed to mitigate embedding inversion attacks by projecting embeddings through a transformer-based network, optimized using text mutual information. Our approach aims to reduce the correlation between the text and its corresponding embeddings, while ensuring that the transformed embeddings remain within the feature space required for downstream task performance. A primary challenge lies in the discrete nature of text and the continuous nature of embeddings, which complicates the direct computation of mutual information between the two. To handle the discrepancy between discrete text values and continuous embeddings, we utilize an autoencoder to encode and decode text, enabling the calculation of mutual information between the latent representation and the embeddings based on information entropy theory. This approach allows us to estimate the mutual information between text and embeddings, providing a more accurate measure of their association. To achieve an optimal balance between defense and functionality, we introduce a multi-task optimization"}, {"title": "Preliminaries", "content": ""}, {"title": "Embedding Model", "content": "An embedding model is a mathematical transformation that projects high-dimensional raw input into a lower-dimensional continuous vector space. Let V denote the discrete input space (i.e., vocabulary set), and let Rd represent the r-dimensional continuous vector space. For an input sequence of l words, x = [W1, W2, ..., wr], the embedding model \u03c6 first encodes x into a sequence of vectors v = [v1, v2, ..., vl] using a tokenizer W : V \u2192 Rr:\n$v_i = W(w_i)$ for i = 1, 2, ..., l.  (1)\nNext, the embedding model feeds the sequence of word vectors v to a Transformer (e.g., BERT [13], T5 [44], LLaMA [55]), obtaining a sequential hidden representation h = [h1,h2,..., h\u012b] for each word in x:\nh = Transformer(v).  (2)\nFinally, the embedding model f reduces the sequential hidden representation to a single vector representation e through a pooling operation, where e belongs to a d-dimensional vector space Rd (i.e. e \u2208 Rd). Typical techniques for pooling operation include mean pooling, max pooling, or using the hidden state of a special token (e.g., the [CLS] token in BERT):\ne = Pooling(h).  (3)\nThus, the embedding model can be formally defined as a function \u03c6 : V \u2192 Rd, which maps an input sequence x to its corresponding embedding vector e (i.e., \u03c6(x) = e).\nThese embedding vectors encapsulate the semantic essence of the raw input, enabling various downstream tasks such as nearest neighbor search, retrieval, and classification. Beyond retrieval, they are essential for applications in ChatGPT, the Assistants API, and a variety of advanced AI tools, including natural language understanding (NLU) systems, sentiment analysis, text generation, and personalized recommendation systems."}, {"title": "Embedding Vector Database", "content": "The use of embeddings as feature representations has led to the development of embedding-based feature databases, which are specialized storage and retrieval systems optimized for handling dense vectors. Examples of such databases include Pinecone, Qdrant, Vdaas, Weaviate, and LangChain. These systems allow users to index, search, and manage embeddings efficiently. Embeddings are indexed into the server database, whose indexes are constructed by the hierarchical navigable small world graphs (HNSWG). At searching phase, a query embedding q is submitted to the vector database. The database returns the correlated embeddings that is closest to the query based on similarity metrics like cosine similarity q.e and Euclidean distance d(q,e) = ||q \u2212 e||2. The server or third-party software is restricted to only accessing and storing text embeddings, rather than the raw text from clients, thereby safeguarding user privacy. Nonetheless, embedding inversion attacks pose a significant threat to the privacy and security of the stored embeddings.\nEmbedding-based feature databases are integral to many modern applications, enabling efficient and scalable retrieval of relevant information from large datasets. They support various use cases such as recommendation systems, search engines, and personalized content delivery by leveraging the power of embeddings to capture complex patterns in data. As the reliance on embedding-based systems continues to grow, so does the importance of safeguarding these embeddings against potential attacks. Understanding the intricacies of embeddings produced by PLMs and the architecture of embedding-based databases provides a foundation for developing effective defense strategies against embedding inversion attacks."}, {"title": "Threat Model", "content": "In this paper, we consider the embedding vector database attack scenario, which involves two parties in the threat model: embedding inversion adversary and defender.\nMotivation. Embedding vector databases are extensively utilized in LLM platforms, such as ChatGPT and Claude. These vectors are derived from user inputs, which often contain sensitive information. Although existing frameworks assert that embedding vectors do not leak sensitive user information, recent research indicates that embedding vector inversion poses a significant threat to LLM platforms [28, 49, 63]. This attack vector enables adversaries to potentially reconstruct original textual inputs from embedding vectors, thereby exposing sensitive user information if the vector database server is compromised. Thus, safeguarding embedding vectors against inversion attacks has become an urgent and critical necessity to ensure user privacy and data security.\nAdversary's capability. Embedding inversion attacks aim at reverse mapping embedding vectors to texts. Given the queried embedding \u03c6(x) \u2208 Rd of an input text x encoded by an encoding model pre-trained on extensive corpora, an attacker builds an inverse model \u03c8(e) : e \u2192 x with the intent of reconstructing the original text x. We consider a strong adversary adept at acquiring all users' embeddings stored within the vector database server, a feat possibly accomplished through man-in-the-middle or SQL injection. The capabilities of adversary can be summarized as:\n\u2022 Auxiliary dataset Daux: The adversary has access to an auxiliary text dataset for querying,"}, {"title": "Embedding Inversion Attacks", "content": "In the operational workflow of producing text embeddings, the process involves inputting original text into an encoder that has been pre-trained to map the text to a continuous vector space, thereby generating an embedding vector that represents the semantic and syntactic features of the original text. However, a critical concern arises in scenarios where the target encoder on the server is subject to repeated queries aimed at computing embeddings for an auxiliary text dataset. In the scenario of an embedding inversion attack, the attacker possesses the capacity to execute a limitless series of queries against the target encoder, with the intent of generating embeddings for an auxiliary text dataset Daux. The goal of the attacker is to search for a candidate text x\u0303 that exhibits an embedding distribution similar to that of the original text. This is formulated as an optimization problem that minimizes the Euclidean distance between the embedding of the candidate text \u03c6(x\u0303) and the embedding of the original text \u03c6(x):\n$x\u0303 = arg min ||\u03c6(x) \u2212 \u03c6(x\u0303)||_2$. (4)\nThe brute-force approach to enumerating all possible combinations of words for the purpose of computing the embedding inversion as expressed in Eq. 4 is rendered computationally intractable due to the exponential growth in permutations with sequence length. Furthermore, the application of multi-label classification techniques is inherently constrained in their ability to predict ordered word sequences, as they are typically designed to handle classification tasks where the order of labels is inconsequential. In light of these limitations, the attacker adopts a decoder-based transformer model \u03c8, a strategy that has been previously explored by Morris et al. [34] and expanded upon by Li et al. [27] This method involves the utilization of a state-of-the-art generative pre-trained transformer, such as GPT-2, to serve as the inversion decoder. The transformer is tasked with inverting the embedding produced by the encoder through a maximum likelihood estima-"}, {"title": "Defense Approach", "content": "In this section, we introduce Eguard, a multi-task optimization-based mechanism aimed at mitigating embedding inversion attacks. To achieve this goal, Eguard projects the embedding vector from its original space to a secured space, with two primary objectives: detaching sensitive information from the embedding vector and preserving its functionality. We propose the three criteria (i.e., effectiveness, harmlessness and robustness) to evaluate the effectiveness of our method:\n\u2022 Effectiveness: This criterion pertains to the efficacy of the modified embedding e' in obliterating sensitive feature information, thereby thwarting potential inversion attacks, while concurrently ensuring that the LLM maintains its utility in task performance.\n\u2022 Harmlessness: Crucially, we ascertain that the"}, {"title": "Overview", "content": "Eguard transforms the original embedding vector e into a secured embedding vector e' through a projection network:\ne' = gp(e),  (7)\nwhere gp denotes the projection network. Embeddings e and e' are vectors with same dimension. The Eguard contains two main modules: sensitive feature detachment and functionality preservation. The detachment module employs an auxiliary network ga to enhance the mutual information between the secured embedding and the raw input, effectively mitigating embedding inversion attacks. Simultaneously, the functionality preservation module assesses the performance of the secured embedding on downstream tasks to maintain its utility. Overall, the optimization process can be conceptualized as:\ngp = arg min \u2212L1(x; \u03c6, ga, gp) + \u03b1L2(x; f, \u03c8, gp),  (8)\nwhere, \u03b1 is a hyper-parameter. L1 and L2 denote the losses of sensitive feature detachment module and functionality preservation module, respectively. Figure 3 illustrates the architecture of proposed Eguard."}, {"title": "Sensitive Feature Detachment", "content": "In this subsection, we introduce the sensitive feature detachment module and explain its design principles.\nEmbedding models are typically trained using unsupervised learning methods on extensive, unlabeled corpora. During the optimization process, embedding models are trained to reveal subtle differences among input data. Through large-scale optimization, the models establish strong correlation between raw inputs and their corresponding embedding vectors. Adversaries exploit this correlation to invert embedding vectors back to their original inputs. To mitigate this risk, the sensitive feature detachment module aims to detach the correlation between raw inputs and their embedding vectors, thereby disrupting the adversary's ability to reconstruct raw inputs from embedding vectors.\nTo achieve this, we project the original embedding vector to a secured embedding vector, and employ the mutual information as a metric to assess the correlation between raw input x and secured embedding vector e'. For a given raw input sample (x, y) with its corresponding secured embedding vector e', the mutual information between the raw input x and the embedding vector e', as well as the ground-truth label of downstream tasks y, is denoted as I(e'; x) and I(e'; y), respectively. Overall, the optimization objective is:\nmax I(x; e') \u2212 \u03b1I(e'; y),  (9)\nwhere \u03b1 is the hyper-parameter, e denotes embedding of input x, and e' is the secured version of e. Equation 9 defines the objective of optimization. Without loss of generality, the mutual information I(x; e') can be calculated as:\nI(x; e') = H(x) \u2212 H(x | e'), (10)\nwhere H(x) is the differential entropy of x, and H(x | e'), is the conditional differential entropy of x given e'. However, since x and e', as well as e and y, are variables with different dimensions. Consequently, the mutual information I(x; e') and I(e'; y) cannot be directly computed.\nMutual Information Estimation. Recent studies have explored the estimation of mutual information through the use of neural networks [7]. Building on these advancements, we introduce a pre-trained Au-"}, {"title": "Functionality Preservation", "content": "In this subsection, we discuss the functionality preservation module. Projecting the original embedding vector to a new embedding space inevitably diminishes the performance of LLMs on downstream tasks. To address this challenge, we propose optimizing the projection network specifically for these downstream tasks. Consequently, the optimization of projection network is designed to maintain the utility of LLMs in these tasks. Formally, the objective of functionality preservation module is:\ngp = arg min L2(x; f, \u03c8, gp), (13)\nwhere L2 represents the loss function that measures the performance of the modified embedding vector on the downstream tasks.\nTo extend our method to a variety of downstream tasks, we now discuss the adaptation of Equation 13 for different applications. For sentiment analysis and natural language inference tasks, we input the optimized embedding vectors into a multi-layer perceptron (MLP) to predict the corresponding labels, utilizing cross-entropy loss as our loss function, denoted as LCE(MLP(e), y). For the question retrieval"}, {"title": "Evaluation", "content": ""}, {"title": "Experimental Setup", "content": "Datasets and language tasks. We utilize several text datasets, each corresponding to different language understanding tasks: sentiment analysis (SST), natural language inference (NLI), question retrieval (QR), and text summarization (TS). These datasets are encoded by the embedding models to evaluate their performance on tasks such as classify-ing sentiment, determining logical relationships be-tween sentence pairs, retrieving relevant questions, and generating concise summaries from longer texts. Detailed descriptions of each dataset and task can be found in the Appendix E.\nEmbeddings models. We employ five state-of-the-art embedding models to conduct embedding inversion attacks and evaluate our defense mechanisms: T5, RoBERTa, MPNet, LLaMA, and Gemma. Each model is used with frozen parameters, utilizing their pre-trained weights. The models vary in architecture and dimensionality, with T5 and MPNet producing 768-dimensional embeddings, RoBERTa generating"}, {"title": "Overall Performance", "content": "Defense against embedding inversion attacks. In this section, we evaluate the effectiveness of our defense across multiple LLM-based embedding mod-"}, {"title": "Evaluation on Robustness", "content": "Robustness to embedding perturbations. Given that embedding vectors are typically stored on cloud servers and exchanged between clients, they are susceptible to interference and data compression. To evaluate the robustness of our defense strategy, we introduce five types of interferences into the target embeddings: 1) white noise, 2) Gaussian noise, 3) truncation, where we apply the ReLU function to obtain sparse embedding vectors, 4) PCA (Principal"}, {"title": "OpenAI embeddings", "content": "Embeddings facilitate the comprehension of relationships between content by machine learning models and other algorithms, enabling them to perform tasks such as clustering and retrieval. These representations underpin various applications, including knowledge retrieval in both ChatGPT and the Assistants API, as well as numerous retrieval-augmented generation developer tools. We have also assessed the efficacy of embedding inversion attacks and defense techniques on OpenAI embeddings."}, {"title": "Interpretability", "content": "To elucidate the interpretability of the designed defense mechanism for text embeddings, we have designed a comparison study that contrasts the distributions and feature distributions of three types of text embeddings. The interpretability analysis involves a comparative evaluation across three different types of text embeddings:1) the original embeddings, 2) the embeddings after defense, and 3) the embeddings with alternative pruning. The gpt3 embeddings, i.e., text-embedding-ada-002 and text-embedding-3-small, are the target in this interpretability experiment. This comparative approach aims to provide a comprehensive understanding of how the defense mechanism impacts the embeddings and preserves their semantic integrity. We visualize the embeddings using t-SNE, to provide a comprehensive understanding of the embeddings' structure, as shown in Figure 8.\nThe t-SNE plots of the original embeddings serve as a baseline, showcasing how the text data is initially distributed in the high-dimensional embedding space. These visualizations demonstrate a clear and coherent clustering of semantically similar texts, reflecting the embeddings' effectiveness in capturing the intrinsic relationships within the data. The embeddings after defense exhibit significant changes in their spatial distribution. The t-SNE plots indicate that while the clusters maintain a degree of consistency, the defense mechanism introduces shifts in the original feature space. These shifts suggest that the embedding transformation aims to thwart inversion attacks while also attempting to preserve the fundamental downstream performance. The slight expansion and reorganization of clusters imply that our defense mechanism effectively disrupts the embedding space, thereby ob-"}, {"title": "Discussion", "content": "In this paper, we introduce Eguard, a defense framework against embedding inversion attacks. The objective is to utilize textual mutual information to establish a projection space with low correlation to the original semantic space, yet fulfill the requirements for effective performance in downstream tasks. The approach demonstrates superior performance in both protecting critical information and maintaining high performance for downstream tasks. On the one hand, compared with the differential privacy mechanism, our defense approach prevents the model accuracy degradation caused by noise. On the other hand, compared with adversarial training, the strategy without modifying the training objectives ensures controllable computational complexity and training time, making it a better choice for large-scale applications.\nWhereas our approach successfully limits the impact of embedding inversion attacks in LLMs, we concentrate mainly on the security of the text-generated LLMs. With the advancement of artificial intelligence technology, researchers have shifted their focus to multimodal LLMs (e.g., text-to-image LLMs [42,43], audio-text LLMs [3,10], and video-text LLMs [9,51]). Both the attacks and defenses against these models need to take into account the collaborative relationship between different modalities, which will undoubtedly pose a greater challenge. Future research should incorporate insights from this study to implement defenses on various modalities of embeddings, exploring individual privacy issues beyond the textual modality.\nMoreover, we wish to build a common defense architecture with balanced defense performance across multiple heterogeneous models. Inspired by relevant research in the field of LLMs, we could publish the pre-trained defense models, combining with few-shot fine-tuning technology to facilitate the construction of more vertical application defenses. This will greatly increase the flexibility and effectiveness of our solutions against future attacks and models.\nOverall, our approach achieves privacy protection without affecting model performance, demonstrating excellent defense capability and wide applicability."}, {"title": "Related work", "content": ""}, {"title": "Text embeddings are universal", "content": "Text embeddings are low-dimension representations of words, phrases, or documents in a continuous vector space, encapsulating semantic information and relationships among texts. Owing to their semantic understanding and low complexities, text embeddings have been widely applied to various natural language processing tasks and feature database storage [6, 17, 56]. For instance, Sentence-BERT [45],"}, {"title": "Text Embeddings Security", "content": "A large body of work has explored the vulnerability of text embeddings facing adversarial attacks, membership inference attacks, and reconstruction attacks. i) Adversarial attacks on text embeddings involve the manipulation of textual inputs, resulting in a model to misclassify or produce an unintended output. ii) Membership inference attacks on text embeddings involve determining whether a particular sample is part of the training dataset. iii) Reconstruction attacks on text embeddings are purposeful attempts to reconstruct sensitive or private information straight from the embeddings [2, 23, 27, 34,49]. These attacks exploit the inadvertent leakage of information embedded within the representations, which may inadvertently disclose confidential details about the original text input.\nIn response, two primary strategies have emerged to guarantee the privacy-preserving nature of text embeddings. One approach utilizes differential privacy mechanisms, which add controlled noise to the embeddings to protect sensitive information. The other approach leverages adversarial training, where models are trained to produce embeddings that are robust to adversarial attacks, thereby preserving privacy. i) Differential privacy: A classical attempt is differentially private stochastic gradient descent (DP-SGD) [1], which add noise into gradients computed during each step of the stochastic gradient descent optimization process. Feyisetan et al. adopt d-privacy mechanism [18], wherein the indistinguishability of output distributions is scaled according to the distance between corresponding inputs. DP-Forward [15] implements an analytic matrix Gaussian mechanism, utilizing noise drawn from a possibly non-i.i.d. matrix Gaussian distribution to perturb the forward-pass embeddings. ii) Adversarial training: Most researches modify the training objectives of the model to maximize the loss of adversarial optimization [30, 57], leading to an improved privacy-accuracy trade-off. Dai et al. introduce interpretable adversarial training method by adding perturbations in the embedding vector to improve the robustness and generalization ability [12]. Additionally, to counteract the wrong prediction perturbed by the word-level adversarial attacks, Yang et al. propose fast triplet metric learning to generate unbiased embeddings by drawing words closer to their synonyms while pushing them away from their nonsynonyms within the embedding space [62].\nDespite these advancements, balancing the trade-off between model performance and privacy remains a significant issue. Methods like DP-SGD can lead to substantial degradation in model accuracy due to the noise introduced, making it crucial to develop more efficient techniques that minimize this trade-off. Additionally, while adversarial training has shown promise, it often increases the computational complexity and training time, which can be a barrier for large-scale applications."}, {"title": "Conclusion", "content": "In this paper, we addressed the critical issue of embedding inversion attacks in large language models and presented Eguard, a novel defense mechanism. By leveraging a transformer-based projection network and optimizing for text mutual information, our approach effectively reduces the risk of privacy breaches while ensuring the preservation of LLM utility on downstream tasks. Through extensive evaluation, we demonstrated the efficacy of Eguard in mitigating embedding inversion attacks, striking a balance between security and performance. Our work represents a significant step forward in safeguarding user data privacy and security in LLMs, paving the way for the development of more robust and secure natural language processing systems."}]}