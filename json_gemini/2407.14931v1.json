{"title": "POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation", "authors": ["Alexey Skrynnik", "Anton Andreychuk", "Anatolii Borzilov", "Alexander Chernyavskiy", "Konstantin Yakovlev", "Aleksandr Panov"], "abstract": "Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments with, mostly, few agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot navigation and obstacle avoidance, that have been conventionally approached with the classical non-learnable methods (e.g., heuristic search) is currently suggested to be solved by the learning-based or hybrid methods. Still, in this domain, it is hard, not to say impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To this end, we introduce POGEMA, a set of comprehensive tools that includes a fast environment for learning, a generator of problem instances, the collection of pre-defined ones, a visualization toolkit, and a benchmarking tool that allows automated evaluation. We introduce and specify an evaluation protocol defining a range of domain-related metrics computed on the basics of the primary evaluation indicators (such as success rate and path length), allowing a fair multi-fold comparison. The results of such a comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) has gained an increasing attention recently and significant progress in this field has been achieved [6, 32, 62]. MARL methods have been demonstrated to generate well-performing agents' policies in strategic games [2, 63], sport simulators [49, 66], multi-component robot control [59], city traffic control [20], and autonomous driving [70]. Currently, several ways to formulate and solve MARL problems exist, based on what information is available to the agents and what type of communication is allowed in the environment [67]. Due to the increased interest in robotic applications, decentralized cooperative learning with minimizing communication between agents has recently attracted a specific attention [45, 68]. Decentralized learning naturally suits the partial observability of the environment in which the robots usually operate. Reducing the information transmitted through the communication channels between the agents increases their degree of autonomy.\nThe main challenges in solving MARL problems are the non-stationarity of the multi-agent environment, the need to explicitly predict the behavior of the other agents to implement cooperative behavior, high dimensionality of the action space, which grows exponentially with the number of agents, and the sample inefficiency of existing approaches. The existing MARL including model-based and hybrid learnable methods [12, 26] exhibit faster and more stable learning in SMAC-type environments [13] with vector observations and full observability. Currently, the best results are shown by the discrete explicit world models, that use Monte Carlo tree search for planning with various heuristics to reduce the search space [18, 26].\nHowever, in numerous practically inspired applications, like in mobile robot navigation, agents' observations are typically high-dimensional (e.g. stacked occupancy grid matrices or image-based observations as compared to 32-dim vectors in SMAC [13]) and only partially describe the state of the environment, including the other agents [17, 14]. This makes the problem specifically challenging, especially in the environments where a large number of agents are involved. For example, it is not uncommon in robotics to consider settings where up to hundreds of agents are acting (moving) simultaneously in the shared workspace as opposed to 2-10 agents in conventional MARL environments such as SMAC [13] or Google Research Football [21]. Learning to act in such crowded, observation-rich and partially-observable environments is a notable challenge to existing MARL methods.\nConventionally, the problem of multi-robot cooperative navigation (which is very important due to its applications in modern automated warehouses and fulfillment centers [11]) is framed as a search problem over a discretized search space, composed of robots-locations tuples. All robots are assumed to be confined to a graph, typically \u2013 a 4-connected grid [38], and at each time step a robot can either move following a graph's edge or stay at the current vertex. This problem setting is known as (Classical) Multi-agent Pathfinding problem [50]. Even in such simplified setting (discretized space, discretized time, uniform-duration actions etc.) obtaining a set of individual plans (one for each robot) that are mutually-conflict-free (i.e. no vertex or edge is occupied by disctinct agents at the same time step) and minimize a common objective such as, for example, the arrival time of the last agent (known as the makespan in the literature) is NP-Hard [65]. Moreover if the underlying graph is directed even obtaining a valid solution is HP-Hard as well [31].\nTo this end the focus of the multi-agent pathfinding community is recently being shifted towards exploring of how state-of-the-art machine learning techniques, especially reinforcement learning and imitation learning, can be leveraged to increase the efficiency of traditional solvers. Methods like [48, 46, 47, 29, 61, 42, 58, 27, 10] are all hybrid solvers that rely on both widespread search-based techniques and learnable components as well. They all are developed using different frameworks, environments and datasets and are evaluated accordingly, i.e. in the absence of the unifying evaluation framework, consisting of the (automated) evaluation tool, protocol (that defines common performance indicators) and the dataset of the problem instances. Moreover, currently most of the pure MARL methods, i.e. the ones that do not involve search-based modules, such as QMIX [37], MAMBA [12], MAPPO [64] etc., are mostly not included in comparison. The main reason is that to train MARL policies a fast environment is needed, which is suited to cooperative multi-agent navigation.\nTo close the mentioned gaps we introduce POGEMA, a comprehensive set of tools that includes:\n\u2022 a fast and flexible environment for learning and planning supporting several variants of the multi-robot navigation problem,"}, {"title": "2 Related Work", "content": "Currently, a huge variety of MARL environments exists that are inspired by various practical applications and encompass a broad spectrum of nuances in problem formulations. Notably, they include a diverse array of computer games [41, 13, 39, 7, 51, 19, 5, 3, 21]. Additionally, they address complex social dilemmas [1] including public goods games, resource allocation problems [35], and multi-agent coordination challenges. Some are practically inspired, showcasing tasks such as competitive object tracking [34], infrastructure management and planning [23], and automated scheduling of trains [30]. Beyond these, the environments simulate intricate, interactive systems such as traffic management and autonomous vehicle coordination [57], multi-agent control tasks [39, 36], and warehouse management [16]. Each scenario is designed to challenge and analyze the collaborative and competitive dynamics that emerge among agents in varied and complex contexts. We summarize the most wide-spread MARL environments in Table 1. A detailed description of each column is presented below.\nNavigation Navigation tasks arise in almost all multi-agent environments (e.g. unit navigation in SMAC or robotic warehouse management in RWARE), however only a handful of environments specifically focus on challenging navigation problems: Flatland, Nocturne, RWARE, and POGEMA.\nPartially observable Partial observability is an intrinsic feature of a generic multi-agent problem, meaning that an individual agent does not have access to the full state of the environment but rather is able to observe it only locally (e.g. an agent is able to determine the locations of the other agents and/or static obstacles only in its vicinity). Most of the considered environments are partially observable, with the exception of Overcooked.\nPython based Most of the current multi-agent environments are implemented in Python, however, some of them include bindings of the other program languages or external dependencies that might complicate their usage. Pure Python implementations generally ensure ease of modification and customization, allowing researchers to readily adapt and extend the environments.\nHardware-agnostic setup means that the environment doesn't require setup for any specific type of training or inferencing hardware.\nPerformance >10K Steps/s Training and evaluating multi-agent reinforcement learning agents often requires making billions of steps (transitions) in the environment. Thus, it is crucial that each transition is computed efficiently. In general, performing more than 10K steps per second is a good indicator of the environment's efficiency. While XLA versions can provide high performance by vectorizing the environment on GPU, they require modern hardware setups, which can be a barrier for some researchers. In contrast, fast environments like POGEMA or RWARE can achieve high performance without such stringent hardware requirements, making them more accessible and easier to integrate into a variety of research projects."}, {"title": "3 POGE\u039c\u0391", "content": "POGEMA, which comes from Partially-Observable Grid Environment for Multible Agents, is an umbrella name for a collection of versatile and flexible tools aimed at developing, debugging and evaluating different methods and policies tailored to solve several types of multi-agent navigation tasks."}, {"title": "3.1 POGEMA Environment", "content": "POGEMA5 environment is a core of POGEMA suite. It implements the basic mechanics of agents' interaction with the world. The environment can be installed using the Python Package Index (PyPI). The environemnt is open-sourced and available at github under MIT license. POGEMA provides integration with existing RL frameworks: PettingZoo [54], PyMARL [40], and Gymnasium [56].\nBasic mechanics The workspace where the agents navigate is represented as a grid composed of blocked and free cells. Only the free cells are available for navigation. At each timestep each agent individually and independently (in accordance with a policy) picks an action and then these actions are performed simultaneously. POGEMA implements collision shielding mechanism, i.e. if an agent picks an action that leads to an obstacle (or out-of-the-map) than it stays put, the same applies for two or more agents that wish to occupy the same cell. POGEMA also has an option when one of the agents deciding to move to the common cell does it, while the others stay where they were. The episode ends when the predefined timestep, episode length, is reached. The episode can also end before this timestep if certain conditions are met, i.e. all agents reach their goal locations if MAPF problem (see below) is considered.\nProblem settings POGEMA supports two generic types of multi-agent navigation problems. In the first variant, dubbed MAPF (from Multi-agent Pathfinding), each agent is provided with the unique goal location and has to reach it avoiding collisions with the other agents and static obstacles. For MAPF problem setting POGEMA supports both stay-at-target behavior (when the episode successfully ends only if all the agents are at their targets) and disappear-at-target (when the agent is removed from the environment after it first reaches its goal). The second variant is a lifelong version of multi-agent navigation and is dubbed accordingly \u2013 LMAPF. Here each agent upon reaching a goal is immediately assigned another one (not known to the agent beforehand). Thus the agents are constantly moving trough in the environment until episode ends.\nObservation At each timestep each agent in POGEMA receives an individual ego-centric observation represented as a tensor \u2013 see Fig. 1. The latter is composed of the following (2R+1) \u00d7 (2R+1) binary matrices, where R is the observation radius set by the user:\n1. Static Obstacles \u2013 0 means the free cell, 1 - static obstacle\n2. Other Agents \u2013 0 means no agent in the cell, 1 \u2013 the other agent occupies the cell\n3. Targets - projection of the (current) goal location of the agent to the boundary of its field-of-view\nThe suggested observation, which is, indeed, minimalist and simplistic, can be modified by the user using wrapper mechanisms. For example, it is not uncommon in the MAPF literature to augment the observation with additional matrices encoding the agent's path-to-goal (constructed by some global pathfinding routine) [46] or other variants of global guidance [29].\nReward POGEMA features the most intuitive and basic reward structure for learning. I.e. an agent is rewarded with +1 if it reaches the goal and receives 0 otherwise. For MARL policies that leverage centralized training a shared reward is supported, i.e. \\(r_t = goals/agents\\) where \\(goals\\) is the number of goals reached by the agents at timestep t and agents is the number of agents. Indeed, the user can specify its own reward using wrappers.\nPerformance indicators The following performance indicators are considered basic and are tracked in each episode. For MAPF they are: Sum-of-costs (SoC) and makespan. The former is the sum of time steps (across all agents) consumed by the agents to reach their respective goals, the latter is the maximum over those times. The lower those indicators are the more effectively the agents are solving MAPF tasks. For LMAPF the primary tracked indicator is the throughput which is the ratio of the number of the accomplished goals (by all agents) to the episode length. The higher \u2013 the better."}, {"title": "3.2 POGEMA Toolbox", "content": "The POGEMA Toolbox is a comprehensive framework designed to facilitate the testing of learning-based approaches within the POGEMA environment. This toolbox offers a unified interface that enables the seamless execution of any learnable MAPF algorithm in POGEMA. Firstly, the toolbox provides robust management tools for custom maps, allowing users to register and utilize these maps effectively within POGEMA. Secondly, it enables the concurrent execution of multiple testing instances across various algorithms in a distributed manner, leveraging Dask7 for scalable processing. The results from these instances are then aggregated for analysis. Lastly, the toolbox includes visualization capabilities, offering a convenient method to graphically represent aggregated results through detailed plots. This functionality enhances the interpretability of outcomes, facilitating a deeper understanding of algorithm performance.\nPOGEMA Toolbox offers a dedicated tool for map generation, allowing the creation of three distinct types of maps: random, mazes and warehouse maps. All generators facilitates map creation using adjustable parameters such as width, height, and obstacle density. Additionally, maze generator includes specific parameters for mazes such as the number of wall components and the length of walls. The maze generator was implemented based on the generator provided in [10]. POGEMA Toolbox can be installed using PyPI, and licenced under Apache License 2.0."}, {"title": "3.3 Baselines", "content": "POGEMA integrates a variety of MARL, hybrid and planning-based algorithms with the environment. These algorithms, recently presented, demonstrate state-of-the-art performance in their respective fields. Table 2 highlights the differences between these approaches. Some, such as LaCAM and RHCR, are centralized search-based planners. Other approaches, such as SCRIMP and DCC, while decentralized, still require communication between agents to resolve potential collisions. Learnable modern approaches for LifeLong MAPF that do not utilize communication include Follower [46], MATS-LP [47], and Switchers [48] (Assistant Switcher, Learnable Switcher). All these approaches utilize independent PPO [43] as the training method.\nThe following modern MARL algorithms are included as baselines: MAMBA [12], QPLEX [60], IQL [53], VDN [52], and QMIX [37]. For environment preprocessing, we used the preprocessing scheme provided in the Follower approach, enhancing it with the anonymous targets of other agents' local observations. We utilized the official implementation of MAMBA, as provided by its authors, and employed PyMARL2 framework10 for establishing MARL baselines."}, {"title": "4 Evaluation Protocol", "content": "We include the maps of the following types in our evaluation dataset (with the intuition that different maps topologies are necessary for proper assessment):\n\u2022 Mazes maps that encouter prolonged corridors with 1-cell width that require high level of cooperation between the agent to accomplish the mission. These maps are procedurally generated.\n\u2022 Random one of the most commonly used type of maps, as they are easy to generate and allow to avoid overfitting to some special structure of the map. POGEMA ontains an integrated random maps generator, that allows to control the density of the obstacles.\n\u2022 Warehouses \u2013 this type of maps are usually used in the papers related to LifeLong MAPF. While there is no narrow passages, high density of the agents might significantly reduce the overall throughput, especially when agents are badly distributed along the map. These maps are also can be procedurally generated."}, {"title": "4.2 Metrics", "content": "The existing works related to solving MAPF problems evaluates the performance by two major criteria success rate and the primary performance indicators mentioned above: sum-of-costs, makespan, throughput. These are directly obtainable from POGEMA. While these metrics allow to evaluate the algorithms at some particular instance, it's might be difficult to get a high-level conclusion about the performance of the algorithms. Thus, we want to introduce several high-level metrics that covers multiple different aspects:\nPerformance \u2013 how well the algorithm works compared to other approaches. To compute this metric we run the approaches on a set of maps similar to the ones, used during training, and compare the obtained results with the best ones.\nPerformance MAPF = \n\\begin{cases}\nSoCbest/SoC \\\\\n0 \\\\\n\\end{cases}\n \\text{if solved} \\\\ \\text{if not solved} (1)\nPerformance LMAPF = throughput/throughputbest (2)\nOut-of-Distribution - how well the algorithm works on out-of-distribution maps. This metric is computed in the same way as Performance, with the only difference that the approaches are evaluated on a set of maps, that were not used during training phase and have different structure of obstacles. For this purpose we utilize maps from MovingAI-tiles set of maps.\nOut_of_DistributionMAPF = \n\\begin{cases}\nSoCbest/SoC \\\\\n0 \\\\\n\\end{cases}\n \\text{if solved} \\\\ \\text{if not solved} (3)\nOut_of_DistributionnLMAPF = throughput/throughputbest (4)\nScalability \u2013 how well the algorithm scales to large number of agents. To evaluate how well the algorithm scales to large number of agents, we run it on a large warehouse map with increasing number of agents and compute the ratio between runtimes with various number of agents.\nScalability = \\( \\dfrac{runtime(agents_1)/runtime(agents_2)}{|agents_1|/|agents_2|}\\) (5)\nCooperation \u2013 how well the algorithm is able to resolve complex situations. To evaluate this metric we run the algorithm on Puzzles set of maps and compare the obtained results with best solutions that were obtained by classical MAPF/LMAPF solvers.\nCooperation MAPF = \n\\begin{cases}\nSoCbest/SOC \\\\\n0 \\\\\n\\end{cases}\n \\text{if solved} \\\\ \\text{if not solved} (6)\nCooperation LMAPF = throughput/throughputbest (7)\nCongestion - how well the algorithm distributes the agents along the map and reduces redundant waits, collisions, etc. To evaluate this metric we compute the average density of the agents presented in the observations of each agent and compare it to the overall density of the agents on the map.\nCongestion = \\(\\dfrac{\\sum_{i \\in agents} agents\\_density(obs_i)/agents\\_density(map)}{|agents|}\\) (8)\nPathfinding \u2013 how well the algorithm works in case of presence of a single agent on a large map. This metric is tailored to determine the ability of the approach to effectively lead agents to their goal locations. For this purpose we run the approaches on large city maps from MovingAI benchmark sets. The obtained solution cost (in fact - length of the path) should be optimal.\nPathfinding = \n\\begin{cases}\n1 \\\\\n0 \\\\\n\\end{cases}\n \\text{if path is optimal} \\\\ \\text{otherwise} (9)"}, {"title": "5 Conclusion and Limitations", "content": "This paper presents POGEMA \u2013 a powerful suite of tools tailored for creating, assessing, and comparing methods and policies in multi-agent navigation problems. POGEMA encompasses a fast learning environment and a comprehensive evaluation toolbox suitable for pure MARL, hybrid, and search-based solvers. It includes a wide array of methods as baselines. The evaluation protocol described, along with a rich set of metrics, assists in assessing the generalization and scalability of all approaches. Visualization tools enable qualitative examination of algorithm performance. Integration with the well-known MARL API and map sets facilitates the benchmark's expansion. Existing limitations are two-fold. First, a conceptual limitation is that communication between the agents is not currently disentangled in POGEMA environment. Second, the technical limitations include the lack of JAX support and integration with other well-known GPU parallelization tools."}]}