{"title": "POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation", "authors": ["Alexey Skrynnik", "Anton Andreychuk", "Anatolii Borzilov", "Alexander Chernyavskiy", "Konstantin Yakovlev", "Aleksandr Panov"], "abstract": "Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments with, mostly, few agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot navigation and obstacle avoidance, that have been conventionally approached with the classical non-learnable methods (e.g., heuristic search) is currently suggested to be solved by the learning-based or hybrid methods. Still, in this domain, it is hard, not to say impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To this end, we introduce POGEMA, a set of comprehensive tools that includes a fast environment for learning, a generator of problem instances, the collection of pre-defined ones, a visualization toolkit, and a benchmarking tool that allows automated evaluation. We introduce and specify an evaluation protocol defining a range of domain-related metrics computed on the basics of the primary evaluation indicators (such as success rate and path length), allowing a fair multi-fold comparison. The results of such a comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.", "sections": [{"title": "Introduction", "content": "Multi-agent reinforcement learning (MARL) has gained an increasing attention recently and significant progress in this field has been achieved [6, 32, 62]. MARL methods have been demonstrated to generate well-performing agents' policies in strategic games [2, 63], sport simulators [49, 66], multi-component robot control [59], city traffic control [20], and autonomous driving [70]. Currently, several ways to formulate and solve MARL problems exist, based on what information is available to the agents and what type of communication is allowed in the environment [67]. Due to the increased interest in robotic applications, decentralized cooperative learning with minimizing communication between agents has recently attracted a specific attention [45, 68]. Decentralized learning naturally suits the partial observability of the environment in which the robots usually operate. Reducing the information transmitted through the communication channels between the agents increases their degree of autonomy.\nThe main challenges in solving MARL problems are the non-stationarity of the multi-agent environment, the need to explicitly predict the behavior of the other agents to implement cooperative behavior, high dimensionality of the action space, which grows exponentially with the number of agents, and the sample inefficiency of existing approaches. The existing MARL including model-based and hybrid learnable methods [12, 26] exhibit faster and more stable learning in SMAC-type environments [13] with vector observations and full observability. Currently, the best results are shown by the discrete explicit world models, that use Monte Carlo tree search for planning with various heuristics to reduce the search space [18, 26].\nHowever, in numerous practically inspired applications, like in mobile robot navigation, agents' observations are typically high-dimensional (e.g. stacked occupancy grid matrices or image-based observations as compared to 32-dim vectors in SMAC [13]) and only partially describe the state of the environment, including the other agents [17, 14]. This makes the problem specifically challenging, especially in the environments where a large number of agents are involved. For example, it is not uncommon in robotics to consider settings where up to hundreds of agents are acting (moving) simultaneously in the shared workspace as opposed to 2-10 agents in conventional MARL environments such as SMAC [13] or Google Research Football [21]. Learning to act in such crowded, observation-rich and partially-observable environments is a notable challenge to existing MARL methods.\nConventionally, the problem of multi-robot cooperative navigation (which is very important due to its applications in modern automated warehouses and fulfillment centers [11]) is framed as a search problem over a discretized search space, composed of robots-locations tuples. All robots are assumed to be confined to a graph, typically \u2013 a 4-connected grid [38], and at each time step a robot can either move following a graph's edge or stay at the current vertex. This problem setting is known as (Classical) Multi-agent Pathfinding problem [50]. Even in such simplified setting (discretized space, discretized time, uniform-duration actions etc.) obtaining a set of individual plans (one for each robot) that are mutually-conflict-free (i.e. no vertex or edge is occupied by disctinct agents at the same time step) and minimize a common objective such as, for example, the arrival time of the last agent (known as the makespan in the literature) is NP-Hard [65]. Moreover if the underlying graph is directed even obtaining a valid solution is HP-Hard as well [31].\nTo this end the focus of the multi-agent pathfinding community is recently being shifted towards exploring of how state-of-the-art machine learning techniques, especially reinforcement learning and imitation learning, can be leveraged to increase the efficiency of traditional solvers. Methods like [48, 46, 47, 29, 61, 42, 58, 27, 10] are all hybrid solvers that rely on both widespread search-based techniques and learnable components as well. They all are developed using different frameworks, environments and datasets and are evaluated accordingly, i.e. in the absence of the unifying evaluation framework, consisting of the (automated) evaluation tool, protocol (that defines common performance indicators) and the dataset of the problem instances. Moreover, currently most of the pure MARL methods, i.e. the ones that do not involve search-based modules, such as QMIX [37], MAMBA [12], MAPPO [64] etc., are mostly not included in comparison. The main reason is that to train MARL policies a fast environment is needed, which is suited to cooperative multi-agent navigation.\nTo close the mentioned gaps we introduce POGEMA, a comprehensive set of tools that includes:\n\u2022 a fast and flexible environment for learning and planning supporting several variants of the multi-robot navigation problem,"}, {"title": "POGEMA", "content": "POGEMA, which comes from Partially-Observable Grid Environment for Multible Agents, is an umbrella name for a collection of versatile and flexible tools aimed at developing, debugging and evaluating different methods and policies tailored to solve several types of multi-agent navigation tasks."}, {"title": "POGEMA Environment", "content": "POGEMA5 environment is a core of POGEMA suite. It implements the basic mechanics of agents' interaction with the world. The environment can be installed using the Python Package Index (PyPI). The environemnt is open-sourced and available at github under MIT license. POGEMA provides integration with existing RL frameworks: PettingZoo [54], PyMARL [40], and Gymnasium [56].\nBasic mechanics The workspace where the agents navigate is represented as a grid composed of blocked and free cells. Only the free cells are available for navigation. At each timestep each agent individually and independently (in accordance with a policy) picks an action and then these actions are performed simultaneously. POGEMA implements collision shielding mechanism, i.e. if an agent picks an action that leads to an obstacle (or out-of-the-map) than it stays put, the same applies for two or more agents that wish to occupy the same cell. POGEMA also has an option when one of the agents deciding to move to the common cell does it, while the others stay where they were. The episode ends when the predefined timestep, episode length, is reached. The episode can also end before this timestep if certain conditions are met, i.e. all agents reach their goal locations if MAPF problem (see below) is considered.\nProblem settings POGEMA supports two generic types of multi-agent navigation problems. In the first variant, dubbed MAPF (from Multi-agent Pathfinding), each agent is provided with the unique goal location and has to reach it avoiding collisions with the other agents and static obstacles. For MAPF problem setting POGEMA supports both stay-at-target behavior (when the episode successfully ends only if all the agents are at their targets) and disappear-at-target (when the agent is removed from the environment after it first reaches its goal). The second variant is a lifelong version of multi-agent navigation and is dubbed accordingly \u2013 LMAPF. Here each agent upon reaching a goal is immediately assigned another one (not known to the agent beforehand). Thus the agents are constantly moving trough in the environment until episode ends.\nObservation At each timestep each agent in POGEMA receives an individual ego-centric observation represented as a tensor \u2013 see Fig. 1. The latter is composed of the following (2R+1) \u00d7 (2R+1) binary matrices, where R is the observation radius set by the user:\n1. Static Obstacles \u2013 0 means the free cell, 1 - static obstacle\n2. Other Agents \u2013 0 means no agent in the cell, 1 \u2013 the other agent occupies the cell\n3. Targets - projection of the (current) goal location of the agent to the boundary of its field-of-view\nThe suggested observation, which is, indeed, minimalist and simplistic, can be modified by the user using wrapper mechanisms. For example, it is not uncommon in the MAPF literature to augment the observation with additional matrices encoding the agent's path-to-goal (constructed by some global pathfinding routine) [46] or other variants of global guidance [29].\nReward POGEMA features the most intuitive and basic reward structure for learning. I.e. an agent is rewarded with +1 if it reaches the goal and receives 0 otherwise. For MARL policies that leverage centralized training a shared reward is supported, i.e. $r_t = \\frac{goals}{agents}$ where goals is the number of goals reached by the agents at timestep t and agents is the number of agents. Indeed, the user can specify its own reward using wrappers.\nPerformance indicators The following performance indicators are considered basic and are tracked in each episode. For MAPF they are: Sum-of-costs (SoC) and makespan. The former is the sum of time steps (across all agents) consumed by the agents to reach their respective goals, the latter is the maximum over those times. The lower those indicators are the more effectively the agents are solving MAPF tasks. For LMAPF the primary tracked indicator is the throughput which is the ratio of the number of the accomplished goals (by all agents) to the episode length. The higher \u2013 the better."}, {"title": "POGEMA Toolbox", "content": "The POGEMA Toolbox is a comprehensive framework designed to facilitate the testing of learning-based approaches within the POGEMA environment. This toolbox offers a unified interface that enables the seamless execution of any learnable MAPF algorithm in POGEMA. Firstly, the toolbox provides robust management tools for custom maps, allowing users to register and utilize these maps effectively within POGEMA. Secondly, it enables the concurrent execution of multiple testing instances across various algorithms in a distributed manner, leveraging Dask7 for scalable processing. The results from these instances are then aggregated for analysis. Lastly, the toolbox includes visualization capabilities, offering a convenient method to graphically represent aggregated results through detailed plots. This functionality enhances the interpretability of outcomes, facilitating a deeper understanding of algorithm performance.\nPOGEMA Toolbox offers a dedicated tool for map generation, allowing the creation of three distinct types of maps: random, mazes and warehouse maps. All generators facilitates map creation using adjustable parameters such as width, height, and obstacle density. Additionally, maze generator includes specific parameters for mazes such as the number of wall components and the length of walls. The maze generator was implemented based on the generator provided in [10]. POGEMA Toolbox can be installed using PyPI, and licenced under Apache License 2.0."}, {"title": "Baselines", "content": "POGEMA integrates a variety of MARL, hybrid and planning-based algorithms with the environment. These algorithms, recently presented, demonstrate state-of-the-art performance in their respective fields. Some, such as LaCAM and RHCR, are centralized search-based planners. Other approaches, such as SCRIMP and DCC, while decentralized, still require communication between agents to resolve potential collisions. Learnable modern approaches for LifeLong MAPF that do not utilize communication include Follower [46], MATS-LP [47], and Switchers [48] (Assistant Switcher, Learnable Switcher). All these approaches utilize independent PPO [43] as the training method.\nThe following modern MARL algorithms are included as baselines: MAMBA [12], QPLEX [60], IQL [53], VDN [52], and QMIX [37]. For environment preprocessing, we used the preprocessing scheme provided in the Follower approach, enhancing it with the anonymous targets of other agents' local observations. We utilized the official implementation of MAMBA, as provided by its authors, and employed PyMARL2 framework10 for establishing MARL baselines."}, {"title": "Evaluation Protocol", "content": "We include the maps of the following types in our evaluation dataset (with the intuition that different maps topologies are necessary for proper assessment):\n\u2022 Mazes maps that encouter prolonged corridors with 1-cell width that require high level of cooperation between the agent to accomplish the mission. These maps are procedurally generated.\n\u2022 Random one of the most commonly used type of maps, as they are easy to generate and allow to avoid overfitting to some special structure of the map. POGEMA ontains an integrated random maps generator, that allows to control the density of the obstacles.\n\u2022 Warehouses \u2013 this type of maps are usually used in the papers related to LifeLong MAPF. While there is no narrow passages, high density of the agents might significantly reduce the overall throughput, especially when agents are badly distributed along the map. These maps are also can be procedurally generated."}, {"title": "Metrics", "content": "The existing works related to solving MAPF problems evaluates the performance by two major criteria success rate and the primary performance indicators mentioned above: sum-of-costs, makespan, throughput. These are directly obtainable from POGEMA. While these metrics allow to evaluate the algorithms at some particular instance, it's might be difficult to get a high-level conclusion about the performance of the algorithms. Thus, we want to introduce several high-level metrics that covers multiple different aspects:\nPerformance \u2013 how well the algorithm works compared to other approaches. To compute this metric we run the approaches on a set of maps similar to the ones, used during training, and compare the obtained results with the best ones.\n$Performance_{MAPF} = \\begin{cases} \\frac{SoC_{best}}{SoC} & \\text{if solved} \\\\ 0 & \\text{if not solved} \\end{cases}$\n$Performance_{LMAPF} = throughput/throughput_{best}$\nOut-of-Distribution - how well the algorithm works on out-of-distribution maps. This metric is computed in the same way as Performance, with the only difference that the approaches are evaluated on a set of maps, that were not used during training phase and have different structure of obstacles. For this purpose we utilize maps from MovingAI-tiles set of maps.\n$Out\\_of\\_Distribution_{MAPF} = \\begin{cases} \\frac{SoC_{best}}{SoC} & \\text{if solved} \\\\ 0 & \\text{if not solved} \\end{cases}$\n$Out\\_of\\_Distribution_{LMAPF} = throughput/throughput_{best}$\nScalability \u2013 how well the algorithm scales to large number of agents. To evaluate how well the algorithm scales to large number of agents, we run it on a large warehouse map with increasing number of agents and compute the ratio between runtimes with various number of agents.\n$Scalability = \\frac{runtime(agents_1)/runtime(agents_2)}{|agents_1|/|agents_2|}$\nCooperation \u2013 how well the algorithm is able to resolve complex situations. To evaluate this metric we run the algorithm on Puzzles set of maps and compare the obtained results with best solutions that were obtained by classical MAPF/LMAPF solvers.\n$Cooperation_{MAPF} = \\begin{cases} \\frac{SoC_{best}}{SOC} & \\text{if solved} \\\\ 0 & \\text{if not solved} \\end{cases}$\n$Cooperation_{LMAPF} = throughput/throughput_{best}$\nCongestion - how well the algorithm distributes the agents along the map and reduces redundant waits, collisions, etc. To evaluate this metric we compute the average density of the agents presented in the observations of each agent and compare it to the overall density of the agents on the map.\n$Congestion = \\frac{\\sum_{i \\in agents} agents\\_density(obs_i)/agents\\_density(map)}{|agents|}$\nPathfinding \u2013 how well the algorithm works in case of presence of a single agent on a large map. This metric is tailored to determine the ability of the approach to effectively lead agents to their goal locations. For this purpose we run the approaches on large city maps from MovingAI benchmark sets. The obtained solution cost (in fact - length of the path) should be optimal.\n$Pathfinding = \\begin{cases} 1 & \\text{if path is optimal} \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "Experimental Results", "content": "We have evaluated all the supported baseline algorithms (12 in total) on both MAPF and LMAPF setups on all 6 datasets. In both setups, i.e. MAPF and LMAPF, the best results in terms of cooperation, out-of-distribution and performance metrics were obtained by centralized planners, i.e. LaCAM and RHCR respectively.\nFor MAPF tasks, LaCAM outperforms all other approaches on all metrics except congestion. It is hypothesized that in this approach, the even distribution of agents across the environment is not crucial due to its centralized nature, which efficiently resolves complex conflicts. Specialized learnable MAPF approaches, i.e., DCC and SCRIMP, take second place, showing close performance but with different specifics. DCC shows better results on out-of-distribution tasks and pathfinding tasks than SCRIMP, which is better at managing congestion. Surprisingly, the results of SCRIMP are inferior on pathfinding tasks, suggesting a problem with this approach in single-agent tasks that do not require communication, which can be an out-of-distribution setup for this algorithm. MARL algorithms such as QPLEX, VDN, and QMIX underperform in comparison with other approaches, exhibiting a significant gap in the results, which can be attributed to the absence of additional techniques used in hybrid approaches, despite incorporating preprocessing techniques from the Follower approach. This could suggest that the MARL community lacks large-scale approaches and benchmarks for them. Predictably, IQL shows the poorest performance, highlighting the importance of centralized training for multi-agent pathfinding (MAPF) tasks that require high levels of cooperation. The best results out of all MARL approaches are shown by MAMBA, which has non-zero results on the performance metric and better results in terms of the cooperation metric. Its results are still much worse than those obtained by hybrid methods, and it is not able to solve any instances in the out-of-distribution dataset.\nFor LMAPF tasks, the centralized approach, RHCR, dominates in cooperation, out-of-distribution, performance, and pathfinding metrics. However, it significantly lags behind other approaches in terms of congestion and scalability metrics. Out of the non-centralized approaches, the best results, depending on the metric, are shown by either MATS-LP or Follower. While MATS-LP is better in terms of cooperation and pathfinding, the latter is better in terms of congestion and scalability metrics. It's also worth noting that MATS-LP requires much more runtime than other approaches as it runs MCTS for each of the agents at each step, which takes time. There are also two more hybrid approaches \u2013 ASwitcher and LSwitcher \u2013 which differ in the way they switch between planning-based and learning-based parts. One of the reasons for their mediocre results is a total lack of global information, i.e., the Switcher approach assumes that the agents have no information about the global map; thus, each of them needs to reconstruct the map based on their local observations. Again, MARL approaches underperform in these scenarios, with QMIX and QPLEX showing comparable results. QMIX performs better in cooperation and out-of-distribution metrics, while QPLEX excels in performance. As in the MAPF setup, among all the MARL approaches, the best results are demonstrated by MAMBA, which is slightly better than others in terms of cooperation and out-of-distribution metrics and much better in terms of the performance metric."}, {"title": "Conclusion and Limitations", "content": "This paper presents POGEMA \u2013 a powerful suite of tools tailored for creating, assessing, and comparing methods and policies in multi-agent navigation problems. POGEMA encompasses a fast learning environment and a comprehensive evaluation toolbox suitable for pure MARL, hybrid, and search-based solvers. It includes a wide array of methods as baselines. The evaluation protocol described, along with a rich set of metrics, assists in assessing the generalization and scalability of all approaches. Visualization tools enable qualitative examination of algorithm performance. Integration with the well-known MARL API and map sets facilitates the benchmark's expansion. Existing limitations are two-fold. First, a conceptual limitation is that communication between the agents is not currently disentangled in POGEMA environment. Second, the technical limitations include the lack of JAX support and integration with other well-known GPU parallelization tools."}, {"title": "Code examples for POGEMA", "content": "from pogema import pogema_v0, GridConfig, AnimationMonitor\ngrid =\n\"\"\"\n.....#.....\n#. ....#.\n....#....\n.#. #.####...\n....###.##\n....#.....\n....#..   .\n.#.\n\"\"\"\n# Define new configuration with 6 randomly placed agents\ngrid_config = GridConfig(map=grid, num_agents=6)\n# Create custom Pogema environment with AnimationMonitor\nenv = AnimationMonitor(pogema_v0(grid_config=grid_config))\nenv.reset()\n# Saving SVG animation\nenv.save_animation ('four-rooms.svg')\nListing 1: Setting up a POGEMA instance with a custom map and generating an animation.\nPOGEMA is an environment that provides a simple scheme for creating MAPF scenarios, specifying the parameters of GridConfig. The main parameters are: on_target (the behavior of an agent on the target, e.g., restart for LifeLong MAPF and nothing for classical MAPF), seed \u2013 to preserve the same generation of the map; agent; and their targets for scenario, size \u2013 used for cases without custom maps to specify the size of the map, density - the density of obstacles, num_agents \u2013 the"}, {"title": "POGEMA Toolbox", "content": "from pogema import BatchAStarAgent\n# Registring A* algorithm\nToolboxRegistry.register_algorithm('A', BatchAStarAgent)\n# Creating algorithm\nalgo = ToolboxRegistry.create_algorithm(\"A*\")\nListing 2: Example of registering the A* algorithm as an approach in the POGEMA Toolbox.\nfrom pogema_toolbox.registry import ToolboxRegistry\n# Creating cusom_map\ncustom_map =\n\"\"\"\n..#.\n...#...#.\n.#.###.#.\n\"\"\"\n# Registring custom_map\nToolboxRegistry.register_maps({\"custom_map\": custom_map})\nListing 3: Example of registering a custom map to the Pogema Toolbox.\nSecond, it provides a unified way of conducting distributed testing using Dask 11 and defined configurations. An example of such a configuration is provided in Listing 4. The configuration is split into three main sections; the first one details the parameters of the POGEMA environment used for"}, {"title": "Extended Related Work", "content": "StarCraft Multi-Agent Challenge - The StarCraft Multi-Agent Challenge (SMAC) is a highly used benchmark in the MARL community. Most MARL papers that propose new algorithms provide evaluations in this environment. The environment offers a large set of possible tasks where a group of units tries to defeat another group of units controlled by a bot (a predefined programmed policy). Such tasks are partially observable and often require simple navigation. However, the benchmark has several drawbacks, such as the need to use the slow simulator of the StarCraft II engine, deterministic tasks, and the lack of an evaluation protocol.\nNevertheless, some of these drawbacks have already been addressed. SMAX [39] provides a hardware-accelerated JAX version of the environment, but it cannot guarantee full compatibility since the StarCraft II engine is proprietary software. SMAC v2 [13] solves the problem of determinism, highlighting this issue in the original SMAC environments. Moreover, an evaluation protocol for the SMAC environment is proposed in [15]. Despite these efforts, it's hard to say that these tasks require the generalization ability of the agent, since the training and evaluation are conducted on the same scenario.\nMulti-agent MuJoCo \u2014 In MAMuJoCo, the standard tasks involve agents controlling different sets of joints (or a single joint) within a simulated robot. This set of environments is a natural adaptation of the environment presented in the well-known MuJoCo physics engine [55]. These tasks don't require high generalization abilities or navigation. In the newer version, MuJoCo provides a hardware-accelerated version, forming the basis for Multi-agent BRAX [39], which enhances performance and efficiency.\nGoogle Research Football - Google Research Football [21] is a multi-agent football simulator that provides a framework for cooperative or competitive multi-agent tasks. Despite the large number of possible scenarios in the football academy and the requirement for simple navigation, the tasks are highly specific to the studied domain. Additionally, the number of possible agents is limited. Moreover, the framework offers low scalability, requiring a heavy engine.\nMulti-robot warehouse The multi-robot warehouse environment RWARE [35] simulates a warehouse with robots delivering requested goods. The environment is highly specific to delivery tasks; however, it doesn't support procedurally generated scenarios, thus not requiring generalization abilities or an evaluation protocol. The best-performing solution [8] in this environment is trained on only 4 agents. The benchmark is highly related to multi-agent pathfinding tasks; however, it doesn't provide centralized solution integration, which could serve both as an upper bound for learnable decentralized methods and as a source of expert demonstrations.\nLevel-Based Foraging \u2014 Multi-agent environment LBF [35] simulates food collection by several autonomously navigating agants in a grid world. Each agent is assigned a level. Food is also randomly scattered, each having a level on its own. The collection of food is successful only if the sum of the levels of the agents involved in loading is equal to or higher than the level of the food. The agents are getting rewarded by level of food they collected normalized by their level and overall food level of the episode. The game requires cooperation but also the agents can emerge competitve behavior. The environment is very efficiently designed and very simple to set up; however, it doesn't support procedurally generated scenarios, thus not requiring generalization abilities or an evaluation protocol.\nFlatland The Flatland environment [30] is designed to address the specific problem of fast, conflict-free train scheduling on a fixed railway map. This environment was created for the Flatland Competition [22]. The overall task is centralized with full observation; however, there is an adaptation to partial observability for RL agents. Unfortunately, during several competitions, despite the presence of stochastic events, centralized solutions [24] from operations research field have outperformed RL solutions by a large margin in both quality and speed. The environment is procedurally generated, which requires high generalization abilities, and the benchmark provides an evaluation protocol. A significant drawback is the extremely slow speed of the environment, which highly restricts large-scale learning.\nOvercooked The Overcooked is a benchmark environment for fully cooperative human-AI task performance, based on the widely popular video game [7]. In the game, agents control chefs tasked to cook some dishes. Due to possible complexity of the cooking process, involving multistep decision-making, it requires emergence of cooperative behaviour between the agents.\nGriddly This is a grid-based game engine [4], allowing to make various and diverse grid-world scenarios. The environment is very performance efficient, being able to make thousands step per second. Moreover, there is test coverage and continuous integration support, allowing open-source development. The engine provides support for different observation setups and maintains state history, making it useful for search based methods.\nMulti-player game simulators Despite the popularity of multi-player games, it's a challenging problem to develop simulators of the games that could be used for research purposes. One of the most popular adaptations are MineCraft MALMO [19] that allows to utilise MineCraft as a configurable research platform for multi-agent research and model various agents' interactions. In spite of the game's flexible functionality, it depends on external runtime, so might be very hard to set up or extremely slow to iterate with. That's why there are several alternatives that prioritise fast iteration over the environment complexity, like Neural MMO [51] that models a simple MMO RPG with agents with a shared resource pool. On top of that, there are even faster implementation, targeting coordination or cooperation, like Hide-and-Seek [3], modelling competition, or GoBigger [69], focusing on competition between cooperating populations.\nMulti-agent Driving Simulators - Autonomous driving is one of the important practical appli-cations of MARL, and Nocturne [57] is a 2D simulator, written in C++, that focuses on different scenarios of interactions e.g. intersections, roundabouts etc. The simulator is based on trajectories collected in real life, so allows modelling practical scenarios. This environment has evaluation protocols and supports open-source development with continuous integration and covered by tests. There are also environments, focusing on particular details of driving, for example, Multi-car Racing [44] that represents racing from bird's eye view.\nSuits of multi-agent environments These multi-agent environments are designed to be very simple benchmarks for specific tasks. Jumanji [5] is a set of environments for different multi-agent scenarios connected to combinatorial optimization and control, for example, routing or packing problems. With the purpose for each environment to be focused on the particular task, the overall suit doesn't test generalization or enable procedural generation. Multi Particle Environments (MPE) [28] is a communication oriented set of partially observable environments where particle agents are able to interact with fixed landmarks and each other, communicating with each other. SISL [16] is a set of three dense reward environments was developed to have a simple benchmark for various cooperative scenarios. For environment suits, testing generalization, MeltingPot [1] comes into place. This set of the environments contains a diverse set of cooperative and general-sum partially observable games and maintains two populations of agents: focal (learning) and visiting (unknown to the environment) to benchmark generalization abilities of MARL algorithms. The set in based on the own game engine and might be extended quite easily.\nReal-world Engineering in Practice Real-world engineering tasks can often be addressed by sophisticated MARL solutions. IMP-MARL [23] provides a platform for evaluating the scalability of cooperative MARL methods responsible for planning inspections and repairs for specific system components, with the goal of minimizing maintenance costs. At the same time, agents must cooperate to minimize the overall risk of system failure. MATE [34] addresses target coverage control challenges in real-world scenarios. It presents an asymmetric cooperative-competitive game featuring two groups of learning agents, cameras and targets, each with opposing goals."}, {"title": "Examples of Used Maps", "content": "The examples of used maps are presented in Figure 9, showing a diverse list of maps. The map types used in the POGEMA Benchmark include: Maze, with prolonged 1-cell width corridors requiring high-level cooperation; Random, easily generated maps to avoid overfitting with controllable obstacle density; MovingAI-tiles, smaller modified slices of MovingAI maps; Puzzle, small hand-crafted maps with challenging patterns necessitating agent cooperation; Warehouse, used in LifeLong MAPF research, featuring high agent density and throughput challenges; and MovingAI, benchmark maps with varying sizes and structures for single-agent pathfinding."}, {"title": "MARL Training Setup", "content": "For training MARL approaches, such as MAMBA, QMIX, QPLEX, and VDN, we used the default hyperparameters provided in the corresponding repositories12, and employed the PyMARL2 framework\u00b9\u00b3 to establish MARL baselines. As input, we apply preprocessing from the Follower approach, which is the current state-of-the-art for decentralized LifeLong MAPF. We attempted to add a ResNet encoder, as used in the Follower approach; however, this addition worsened the results, thus we opted for vectorized observation and default MLP architectures. For centralized methods that work with the state of the environment (e.g., QMIX or QPLEX), we utilized the MARL integration of POGEMA, which provides agent positions, targets, and obstacle positions in a format similar to the SMAC environment (providing their coordinates).\nOur initial experiments on training this approach with a large number of agents, similar to the Follower model, showed very low results. We adjusted the training maps to be approximately 16 \u00d7 16, which proved to be more effective and populated them with 8 agents. This setup shows better results. We continued training the approaches until they reached a plateau, which for most algorithms is under 1 million steps."}, {"title": "Resources and Statistics", "content": "To evaluate all the presented approaches integrated with POGEMA we have used two workstations with equal configuration, that includes 2 NVidia Titan V GPU, AMD Ryzen Threadripper 3970X CPU and 256 GB RAM. The required computation time is heavily depends on the approach by itself.\nThe statistics regarding the spent time on solving MAPF and LMAPF instances are presented in Table 11 and Table 12 respectively. Please note, that all these approaches were run in parallel in multiple threads utilizing dask, that significantly reduces the factual spent time.\nWe used pretrained models for all the hybrid methods, such as Follower, Switcher, MATS-LP, SCRIMP, and DCC, thus, no resources were spent on their training. RHCR and LaCAM are pure search-based planners and do not require any training. MARL methods, such as MAMBA, QPLEX, QMIX, IQL, and VDN, were trained by us. MAMBA was trained for 20 hours on the MAPF instances, resulting in 200K environment steps, and for 6 days on LifeLong MAPF instances, resulting in 50K environment steps, which corresponds to the same amount of GPU hours. For MARL approaches, we trained them for 1 million environment steps, which corresponds to an average of 5 GPU hours for each algorithm."}, {"title": "Community Engagement and Framework Enhancements", "content": "Our team is committed to maintaining an open and accountable POGEMA framework. Since 2021, we have continuously improved POGEMA, including the addition of the POGEMA Toolbox and the recent introduction of POGEMA Benchmark. We ensure transparency in our operations and encourage the broader AI community to participate. Our framework includes a fast learning environment, problem instance generator, visualization toolkit, and automated benchmarking tools, all guided by a clear evaluation protocol. We have also implemented and evaluated multiple strong baselines that simplify further comparison. We practice rigorous software testing and conduct regular code reviews. We promptly address issues reported on GitHub and welcome any feedback and contributions through GitHub."}]}