{"title": "LiPCoT: Linear Predictive Coding based Tokenizer\nfor Self-supervised Learning of Time Series Data via\nLanguage Models", "authors": ["Md Fahim Anjum"], "abstract": "Language models have achieved remarkable success in various natural language\nprocessing tasks. However, their application to time series data, a crucial com-\nponent in many domains, remains limited. This paper proposes LiPCoT (Linear\nPredictive Coding based Tokenizer for time series), a novel tokenizer that encodes\ntime series data into a sequence of tokens, enabling self-supervised learning of\ntime series using existing Language model architectures such as BERT. Unlike\ntraditional time series tokenizers that rely heavily on CNN encoder for time series\nfeature generation, LiPCoT employs stochastic modeling through linear predictive\ncoding to create a latent space for time series providing a compact yet rich rep-\nresentation of the inherent stochastic nature of the data. Furthermore, LiPCoT is\ncomputationally efficient and can effectively handle time series data with varying\nsampling rates and lengths, overcoming common limitations of existing time series\ntokenizers. In this proof-of-concept work, we present the effectiveness of LiPCOT\nin classifying Parkinson's disease (PD) using an EEG dataset from 46 participants.\nIn particular, we utilize LiPCoT to encode EEG data into a small vocabulary of\ntokens and then use BERT for self-supervised learning and the downstream task\nof PD classification. We benchmark our approach against several state-of-the-art\nCNN-based deep learning architectures for PD detection. Our results reveal that\nBERT models utilizing self-supervised learning outperformed the best-performing\nexisting method by 7.1% in precision, 2.3% in recall, 5.5% in accuracy, 4% in\nAUC, and 5% in F1-score highlighting the potential for self-supervised learning\neven on small datasets. Our work will inform future foundational models for time\nseries, particularly for self-supervised learning.", "sections": [{"title": "1 Introduction", "content": "Time series data, representing sequences of values over time, plays a vital role in diverse fields like\nfinance, healthcare, weather, and sensor networks. However, analyzing and extracting insights from\nsuch data often requires specialized techniques. Traditional time series analysis methods heavily\nrely on domain-specific knowledge and feature engineering. Recent works explored recurrent neural\nnetworks (RNN) and convolution neural networks (CNN) for time series tasks, achieving promising\nresults. However, these require significant computational resources, can struggle with capturing\nlong-term dependencies, and aren't inherently suitable for self-supervised learning. On the other\nhand, transformer-based language models have recently shown outstanding performance in capturing\nlong-term dependency, self-supervised learning, and computational efficiency. Thus, there is a need\nto integrate language models for time series analysis via self-supervised learning.\nSelf-supervised representation can offer unique benefits over supervised learning. First, supervised\nlearning needs annotated data and is limited by the labeled data size and the quality of the label-\ning. Second, supervised models force a narrow learning of features for a single downstream task\nwhereas self-supervised features can achieve better generalization for many downstream applications.\nHowever, there are some unique challenges in self-supervised learning for the time series domain\ncompared to the natural language processing (NLP) of texts and similar transformer-based image\nmodels. This is mainly due to the fundamental difference in the nature of time series data, which are\ncontinuous-valued sequences, and text/images, which take discrete values from a finite vocabulary.\nTherefore, unlike NLP applications where word or sub-word tokens are used, there is no lexicon of\ndiscrete time series units, making it challenging to apply predictive losses in self-supervised learning.\nThis work proposes Linear Predictive Coding based Tokenizer for time series (LiPCoT), a novel\ntokenizer specifically designed to tokenize time series data for enabling self-supervised learning\nvia language models. In particular, LiPCoT transforms time series data into a sequence of tokens,\nallowing existing language models like BERT to be leveraged for self-supervised training leading to\ndownstream tasks like anomaly detection, forecasting, and classification.\nInstead of using CNN encoders which utilize temporal features, LiPCoT considers time series as a\nrealization from an underlying stationary stochastic random process and creates a latent space of\ntime series data using the parameters of the underlying random processes. This provides a stochastic\nrepresentation of time series data from which discrete tokens are constructed. By utilizing the\nstochastic representation of time series, LiPCoT offers some unique benefits over other methods. For\nexample, LiPCoT does not depend on the sampling frequency or length of the time series data which\nare crucial for other methods that utilize CNN encoders.\nIn this paper, we present a proof-of-concept study where we propose LiPCoT and demonstrate the\nefficacy of LiPCoT in classifying Parkinson's disease (PD) using EEG data from 46 participants. We\nutilize LiPCoT for tokenizing EEG signals which are then leveraged by BERT for self-supervised\nlearning and subsequent PD classification. We benchmark our approach against four state-of-the-art\ndeep learning architectures, and our findings show that BERT models utilizing self-supervised learning\non LiPCoT tokens outperform existing methods across all evaluated metrics for PD classification.\nThe rest of the paper is organized as follows. Section 2 discusses prior time series tokenization\napproaches in the literature. Section 3 provides a detailed theory and methodology of LiPCoT. Section\n4 details our experiments for evaluating LiPCoT performance. The outcomes of our results are given\nin Section 5 and the ablation study is provided in Section 6. Finally, Section7 discusses the limitations\nand future direction of our work, and Section 8 concludes the paper."}, {"title": "2 Related Works", "content": "So far, there have been limited attempts to convert time series data into discrete tokens using a discrete\ncodebook. One of the fundamental approaches to this end is the quantization of time series data which\nconverts a sequence of continuous numerical data into discrete representations. There are mainly\ntwo widely known approaches for this. The first one is Vector Quantized Variational AutoEncoder\n(VQ-VAE) which utilizes traditional encoder-decoder VAE architecture based on CNN to learn a\ndiscrete codebook via vector quantization technique. This approach has been utilized for time series\nencoding in various time series analyses and architectures including TOTEM, DeWave, Auto-TTE,\nUniAudio, and VioLA [30, 9, 7, 33].\nAnother approach is to utilize a CNN encoder for extracting features from time series data which\nare then fed to a transformer-based architecture for generating a latent space via masked prediction.\nFinally, clustering techniques like k-means are utilized for creating a discrete codebook for time\nseries data. This approach has been utilized in many transformer-based time series architectures such\nas HuBERT and Wav2Vec [13, 4].\nBoth of these approaches utilize CNN to reduce the length of the input time series sequences which\nare then fed to VAE or transformer-based encoder to generate a discrete codebook. Yet another\napproach to quantizing time series is by utilizing the frequency domain information. For example,"}, {"title": "3 LiPCoT: Tokenization of time series data", "content": null}, {"title": "3.1 Objective", "content": "The primary objective of our approach is to provide a tokenization method of time series data such\nthat it is readily compatible with the existing NLP language models for self-supervised learning. To\nthis end, we propose a novel tokenization approach that can take time series data and convert them\ninto discrete tokens that can be leveraged by language models such as BERT via pre-training and\nfine-tuning."}, {"title": "3.2 Overview", "content": "Fundamentally, we assume that the time series data is piece-wise stationary and divide time series\ndata into segments. Then, assuming each segment as a realization from a stationary stochastic random\nprocess, we estimate the underlying random process and create a latent space of time series data\nusing the parameters of the random processes. This gives us a representation of time series data from\nwhich we create tokens by clustering the aforementioned latent space. Finally, we feed these tokens\nto a language model for pre-training and fine-tuning tasks."}, {"title": "3.3 Stochastic modeling of time series", "content": null}, {"title": "3.3.1 Linear Predictive Coding", "content": "Our proposed approach utilizes Linear Predictive Coding (LPC), a widely used technique in signal\nprocessing and speech analysis for estimating the stochastic random process of time series. One of\nthe key advantages of LPC is its ability to compactly represent the spectral characteristics of a signal\nusing a small number of coefficients. This compression of signals via stochastic modeling makes\nit a powerful tool for predicting the behavior of distinguishing time series [1]. LPC is one of the\ndominant analyzing techniques in speech processing, enhancement, and coding [3, 21, 22, 27, 28]. It\nhas also been used in EEG coding [16], economics [24], control theory [10], filtering [14], and a host\nof other applications."}, {"title": "At its core, LPC fits an autoregressive (AR) model to a time series. Specifically, suppose one has the\ntime series sequence:", "content": "At its core, LPC fits an autoregressive (AR) model to a time series. Specifically, suppose one has the\ntime series sequence:\nx0, x1, ..., xN\u22121\nwith subscripts representing the sample indices. The Lth order LPC model of such a Wide Sense\nStationary (WSS) time series provides an autoregressive (AR) approximation of the data. In particular,\nwith \u03b7n as a white driving sequence that comes from a white WSS process \u03b7 with variance \u03c3\u00b2, LPC\napproximates xn as the output of a predictor that uses a linear combination of its past samples.\n\\hat{x}_n = -\\sum_{k=1}^{L} a_k x_{n-k} + \\eta_n\nThe coefficients a = (a1, a2,..., aL) are the parameters of LPC model known as LPC coefficients\nand are calculated by minimizing the prediction error, E[(xn - \\hat{x}_n)^2].\nUsing the z-domain, one can see that LPC approximates xn using \u03b7n and a predictor with transfer\nfunction\nH(z) = \\frac{1}{1 + \\sum_{k=1}^{L} a_k z^{-k}}\nwhere z\u00af\u00b9 denotes unit delay shift operation. Note that, many algorithms exist for the calculation\nof LPC coefficients ak. Two alternative characterizations are possible for describing the predictor\ntransfer function H(z). The first uses the LPC coefficients, a = (a1, a2,..., aL) as shown in (3).\nThe second uses p = (p1, p2, ..., pL), the poles of the transfer function:\nH(z) = \\prod_{k=1}^{L} \\frac{1}{(1-p_kz^{-1})}\nIt can be seen that the poles are roots of the polynomial 1 + \\sum_{k=1}^{L} a_k z^{-k} = 0.\nOne of the advantages of LPC is its ability to efficiently approximate the spectral density from time\nseries. Specifically, one can obtain the estimate of spectral density by.\nP(f) = \\sigma^2 |H(e^{j2\\pi f/F_s})|^2.\nwhere Fs is the sampling frequency. It is well known, [20], that as long as the L-th order autocorre-\nlation matrix of xn is positive definite, H(z) has all poles in the open unit unit disk |pk| \u2264 1. For\nreal-valued time series xn, the poles pk come as conjugate pairs and their phase provides an estimate\nof the dominant frequency component characterizing the time series. Specifically, if the time series\nhas a dominant spectral component (peak in the power spectrum) at frequency foc Hz, then the LPC\npoles will be of the form,\np_k = Ae^{\\pm j2\\pi f_{oc}/F_s}"}, {"title": "3.3.2 Frequency-warped Linear Predictive Coding", "content": "The spectral density captured by LPC uniformly covers all frequencies within the sampling frequency\nrange of a time series. However, in practice, useful information often is not uniformly distributed\nacross all frequencies but rather localized in higher or lower frequencies. A good example of this\nphenomenon is the brain activity signals where we observe 1/f characteristics in the frequency domain\nwhere most informative activities occur in low-frequency ranges.\nFrequency-warped linear predictive coding is a variation of LPC that estimates spectral powers in a\nnon-uniform resolution [12, 11, 26]. It includes a warping coefficient \u03bb \u2208 [-1,1] which enables it to\nprovide higher resolution at low-frequency powers for \u5165 > 0 or at high-frequency powers for > < 0.\nAt x = 0 it becomes the traditional LPC with a uniform resolution for all frequencies (see details in\nAppendix A.2)."}, {"title": "3.4 Latent space for time series", "content": "Here, We describe how univariate time series data can be represented in a latent space based on\nstochastic modeling of the time series via Frequency-warped LPC. In particular, we use Burg's\nmethod for calculating the LPC coefficients[26]. We augmented the algorithm proposed in [26]"}, {"title": "in two ways (Algorithm 1). First, we added the estimation of the power of prediction error \u03c32\nand implemented a fast version of the traditional Burg's algorithm proposed in [32]. Note that the\nconventional implementation of Burg's method has the complexity of O(NL) which can be reduced\nto O(NlogN + L\u00b2) using Fast Fourier Transform[32].", "content": "in two ways (Algorithm 1). First, we added the estimation of the power of prediction error \u03c32\nand implemented a fast version of the traditional Burg's algorithm proposed in [32]. Note that the\nconventional implementation of Burg's method has the complexity of O(NL) which can be reduced\nto O(NlogN + L\u00b2) using Fast Fourier Transform[32]."}, {"title": "Next, we extract features from the frequency-warped LPC model to construct our Latent space. One\ndesired property we seek is to fully recover the LPC models from the feature space. There are a few\nways this can be achieved:", "content": "Next, we extract features from the frequency-warped LPC model to construct our Latent space. One\ndesired property we seek is to fully recover the LPC models from the feature space. There are a few\nways this can be achieved:"}, {"title": "3.4.1 LPC coefficients", "content": "We can use the weighted LPC coefficients where the weights are w1, w2,..., wL to create a L-\ndimensional latent space. However, this formulation is agnostic to total signal power. Hence, We\npropose an extended L + 1-dimensional space by adding the power of prediction error. In particular,\na feature vector for x is,\nF_1(a) := (w_1 a_1, w_2 a_2, ..., w_L a_L, \\log \\sigma^2).\nThe distance metric between two LPC models a and a' is,\nd_{COEF}(x, x') := \\sqrt{(\\log \\sigma^2 - \\log {\\sigma'}^2)^2 + \\sum_{i=1}^{L} w_i^2(a_i - a'_i)^2}.\nThe rationale for using weights is that not all a's has the same impact on the AR model. However,\ndefining a good set of weights is a hard problem [23]. In this study, we assume wi = 1; \u2200i. However,\nwe note that the weights can be optimized using any appropriate cost function in a self-supervised\narchitecture. This will be investigated in future studies."}, {"title": "3.4.2 Cepstrum coefficients", "content": "The cepstrum of a stochastic random process is defined by the inverse Fourier transform of the log\nof the power spectrum of the process [5]. For a stochastic model such as LPC, the cepstrum of the"}, {"title": "output process (c0, c1, ..., cn, ...) can be calculated from model parameters[5, 15]. We create our\nlatent space by taking the first M weighted cepstrum coefficients (Appendix A.3),", "content": "output process (c0, c1, ..., cn, ...) can be calculated from model parameters[5, 15]. We create our\nlatent space by taking the first M weighted cepstrum coefficients (Appendix A.3),\nF_2(a) := (c_0, c_1, \\sqrt{2}c_2, ..., \\sqrt{M}c_M)"}, {"title": "3.4.3 Dominant spectral components", "content": "With the limitations of the aforementioned approaches, we propose yet another latent space with\n2L + 1 dimensions termed dominant spectral components. The salient point of the latent space is to\ncreate a space based on the dominant spectral components as determined by the poles pk in (6) where\nthe dominant frequency is fo. However, the poles pk have no particular order. Therefore, first, we\norder the poles based on the frequencies of the dominant spectral components and then construct the\nlatent space with the angle and radius of the poles as well as the prediction error \u03c3\u00b2 from Algorithm\n1,\nF_3(a) := (\\nu_1, \\nu_2, ..., \\nu_L, \\upsilon_1, \\upsilon_2, ..., \\upsilon_L, \\log \\sigma^2)\nwhere,\n\\nu_k = \\frac{\\angle p_k}{2\\pi},\n\\upsilon_k = -2\\log(1-\\mid p_k \\mid)\nwith the ordering scheme,\n\\nu_i \\leq \\nu_j if i < j.\nThe first L components in this latent space are the dominant frequencies and the next L elements\nare analogous to the power in the respective dominant frequencies (see Appendix A.1). Finally, the\nlast component is the power of the white noise process. This definition of the latent space in (10)\nis inspired by how we naturally interpret power spectrum plots which makes the latent space more\ninterpretative.\nFew things to note here. First, for zero mean time series data, all poles are complex and come in\nconjugate pairs. Hence, there are duplicates of values within \u03bdi in (10) due to the conjugate pair\nof poles which can be discarded for an equivalent but smaller L + 1 dimensional latent space for\nlow-pass filtered data. Second, the latent space is agnostic of the sampling frequency Fs and signal\nlength, which is a desired quality for a general time series tokenization model."}, {"title": "3.5 Tokenization of time series", "content": null}, {"title": "3.5.1 Data Segmentation", "content": "As time series data can have a variable length, first we divide the time series into segments of a\nfixed window and we fit a LPC model for each segment. These segments can be overlapping or\nnon-overlapping."}, {"title": "3.5.2 Token generation", "content": "During training, we calculate LPC model a for each time series segment x using Algorithm 1 and\ngenerate a latent space based on (7),(9) or (10) such that each LPC model a is projected into the\nlatent space Fi(a). Next, we cluster the space for the quantization of the LPC models such that\nFi(a) \u2208 Fk; \u2200a for some k \u2208 {1, 2, . . ., K } and assign a unique token (and a particular word) to each\ncluster Fk resulting in a vocabulary {F1, F2, . . ., \u00caK }. For this, first, we normalize each dimension\nof the latent space during training and utilize an unsupervised k-means clustering algorithm."}, {"title": "3.5.3 Encoding", "content": "The encoding step is similar to the token generation where for each time series segment we calculate\nthe LPC model and obtain a representation Fi(a) in the latent space. However, the corresponding\ncluster is estimated by using the previously trained k-means clusters. This provides a unique token\nfor the time series segment. Similarly, tokens are generated for all segments of the time series."}, {"title": "3.5.4 Decoding", "content": "As we use a stochastic model of the time series for tokenization, recovering the exact time series is\nnot possible. However, we can obtain a realization of the stochastic source of the time series. To\nestimate a time series segment from a token or word, we use white noise as the primary source which\nis then filtered appropriately to match the stochastic nature of the desired time series segment. First,\nwe find the corresponding cluster center Fk from the given token which gives an estimation of the\nLPC model \u00e2 and the noise power \u00f4\u00b2 for the time series segment. Now, we construct the estimated\ntransfer function H\u0124(z) and a WSS process \u03b7 with variance \u00f4\u00b2. Finally, the time series estimation \u00ee is\nobtained by filtering a WSS realization sequence \u03b7n with \u0124(z)."}, {"title": "3.6 Integration with Language Models", "content": "As LiPCoT converts time series into tokens with a corresponding vocabulary, integrating into an\nNLP-based language model is relatively simple and can be done in two ways. First, we can pre-train a\nlanguage model such as BERT [8] from scratch with the given vocabulary and training data. This can\nlead to a language model capable of analyzing time series data. Another way of integrating LiPCoT is\nto take a pre-trained language model and add the new tokens from LiPCoT to its existing vocabulary.\nIn this case, the embedding space has to be resized and the model needs to get further pre-training to\ngenerate embedding for the newly added timeseries related words. In this work, we focus on the first\nmethod."}, {"title": "3.6.1 Self-supervised learning", "content": "During the pre-training stage, we utilized the traditional Masked Language Modeling (MLM) to train\nour BERT model via self-supervised learning (Figure 3.2). In particular, MLM was implemented by\nmasking 15% of the words randomly where 80% of the words with \"[MASK]\" token, 10% with some\nother random words, and the rest 10% were unchanged."}, {"title": "3.6.2 Fine-tuning task: binary classification", "content": "In this study, we focused on binary classification of Parkinson's disease (PD) from EEG data as\na downstream task. To achieve this, we fine-tuned the model by first obtaining the final hidden\nembedding layer for the \"[CLS]\" class token from the pre-trained BERT model and then adding a\nfully-connected layer to this with a sigmoid function (Figure 3.2). Finally, we normalized the outputs\nof the sigmoid function to obtain the probability of binary classification."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Dataset", "content": "For our experiments, we used an EEG dataset of 54 participants from a study at the University of\nNew Mexico (UNM; Albuquerque, New Mexico) where 27 had PD and the rest of the participants\nwere healthy which was previously described in [2]. Upon manual inspection, we utilized EEG data\nfrom 46 participants (22 PD and 24 healthy subjects). EEG data were recorded with a sampling rate\nof 500 Hz on a 64-channel Brain Vision system. PD patients were in OFF medication state."}, {"title": "4.2 Preprocessing of data", "content": "In this work, We utilize EEG data from 59 channels out of 63 based on average channel data quality.\nThe data from each channel were high-pass filtered at 1 Hz to remove noise. No other pre-processing\nwas implemented. Only the first one minute of EEG data from each participant were utilized which\ncorresponds to eyes closed resting state EEG.\nThe multi-channel data (5n seconds) for each subject (R59\u00d75nFs) were converted into 5-second\nsegments (Rn\u00d759\u00d75Fs). For each 5-second segment, LiPCoT was utilized to convert the time series\ndata for each channel into one token resulting in a single sequence of 59 tokens where the location of\neach channel in the sequence was fixed. This approach of constructing sequences (Nn\u00d759\u00d71) was"}, {"title": "4.3 Experiment setup", "content": "First, we randomly shuffled data at the subject level and split the dataset into training (60%), validation\n(20%), and test (20%) datasets. We utilized the training data without classification labels for self-\nsupervised training via BERT using MLP. The validation data without labels were used for evaluating\nthe model's performance against overfitting and the best-performing model on the validation set\nwas selected. For the PD classification task, we used the validation data with labels for training. To\nevaluate the model's performance, we utilized the training data with classification labels and selected\nthe best-performing model. To measure the classification performance, we utilized five metrics:\nprecision, recall, accuracy, F1-score, and AUC. The classification performance was evaluated on the\ntest dataset."}, {"title": "4.4 Comparison with state-of-the-art supervised learning", "content": "We investigated whether fine-tuning a self-supervised BERT model through LiPCoT tokens can\noutperform traditional state-of-the-art CNN-based models that are trained on raw time series data\nvia supervised learning for the downstream PD classification task. To achieve this, we utilized\nfour CNN architectures that have been shown to perform well in PD classification using EEG data:\n13-layer Deep CNN [25], ShallowConvNet [31], DeepConvNet [31] and EEGNet [18]. We chose\nthese methods as they were shown to be very effective neural network architectures tailored for\nEEG-based PD classification in the literature. Unlike BERT which was trained on tokenized data,\nthese CNN-based models were trained on continuous-valued time series data without any tokenization.\nThe input to these state-of-the-art models were 5-second time series data segments from 59 channels\n(Rn\u00d759\u00d75Fs) with corresponding labels. We used the validation data with labels to train these models."}, {"title": "4.5 Model Parameters", "content": "For training LiPCoT, we utilized the training dataset without labels. LPC models were 16th order\n(L = 16). The total vocabulary length was set to 64. Additionally, the warping coefficient (1) was\n0.2, and the Latent space was generated using LPC coefficients. We initialized the BERT model\nwith 6 hidden layers each with 256 neurons. We utilized relative position for increased robustness\nand to eliminate the limitation of token length. The total parameter size was 11,356,485 (Appendix\nA.4). During self-supervised learning, BERT was trained with 256 epochs and a batch size of 2. We\nutilized the Bayesian optimization method to determine the optimal learning rate and batch size for\nthe downstream classification task. The optimal batch size was 4 and the learning rate was 1.9 \u00d7 10-5.\nThe training was conducted for 64 epochs."}, {"title": "5 Results", "content": "Figure 5 depicts an example of time series tokenization via LiPCoT. The spectral density of the\ntokenized data segments showed variations in the spectral profile of the tokens (Figure 5). This\nresulted in the effective capturing of temporal changes in time series data by LiPCoT tokens (Figure\n5)."}, {"title": "6 Ablation study", "content": null}, {"title": "6.1 Optimal tokenization approach for LiPCoT", "content": "In Section 3.4, we have provided three approaches to construct a latent space via LPC for LiPCOT\ntokenization. We investigated the effectiveness of these approaches for extracting features from\ntime series. In other words, we aimed to find the best approach among the three for time series\ntokenization suitable for self-supervised learning and the PD classification task. For this, We utilized\nthese approaches for the tokenization step before self-supervised training of BERT and compared\nthe performance in PD classification. Our results show that among the aforementioned methods for\nconstructing latent space for LiPCoT, LPC coefficients (detailed in Section 3.4.1) were the most\neffective latent features for LiPCoT in the downstream PD classification task. In particular, they\noutperformed the other methods by up to 10.9% in accuracy, 11% in AUC, and 19% in F1-score (Table\n2). The cepstrum coefficients provided similar performance to LPC coefficients with higher precision\nbut a lower recall rate. Dominant spectral coefficients showed significantly lower performance than\nthe rest of the methods indicating that the latent space for this method is not inherently Euclidean and\nstandard k-means is not a suitable method for clustering the space."}, {"title": "6.2 Effectiveness of self-supervised learning on tokenized data", "content": "We also investigated whether self-supervised learning provides any significant advantage for the\nsupervised classification task on the LiPCoT tokenized time series data. For this, we utilized an\nuntrained BERT model initialized with random weights and biases as a baseline for the pre-trained"}, {"title": "BERT model trained via MLM self-supervised learning. Both were utilized in the fine-tuning stage for\nthe PD classification task and their performances were compared. This paved the way to measure how\nmuch information a self-supervised BERT model can add to a downstream supervised classification\nmodel when deployed on time series data tokenized by LiPCOT.", "content": "BERT model trained via MLM self-supervised learning. Both were utilized in the fine-tuning stage for\nthe PD classification task and their performances were compared. This paved the way to measure how\nmuch information a self-supervised BERT model can add to a downstream supervised classification\nmodel when deployed on time series data tokenized by LiPCOT.\nWe found that self-supervised learning via BERT significantly boosted the performance in supervised\nclassification tasks for PD detection. In particular, when compared to BERT models initialized with\nrandom seeds, models with self-supervised training showed performance enhancement up to 9.8%\nin accuracy, 10.8% in precision, 9.1% in recall, 10% in F1-score and 11% in AUC (Table 2). The\nhighest performance boost resulted from the LPC coefficients. Recall that the supervised training\nwas conducted on a validation dataset which was only 3 times smaller than the training dataset of\nself-supervised learning. Therefore, these results demonstrate that even on a small scale, LiPCoT\nhas the potential to effectively tokenize time series data that can boost performance for supervised\nclassification via self-supervised learning in unlabeled data."}, {"title": "6.3 Fourier Transform vs. LPC", "content": "Stochastic modeling via LPC can provide an envelope of the power spectrum of the time series\n(Appendix A.1). One advantage of the LPC-based power spectrum over traditional Discrete Fourier\ntransform (DFT) is its dynamic frequency resolution. Unlike DFT, where frequency resolution is\nuniform across 0 Hz to Fs/2 Hz and depends on the number of data points used, LPC-based power\nspectrum allows evaluation at any frequency without being affected by the sequence length resulting\nin more accurate detection of major oscillations compared to DFT. Furthermore, frequency-warped\nLPC enables us to further emphasize higher or lower frequency (depending on the warping coefficient\n\u03bb; Appendix A.2). Another advantage of stochastic modeling is its ability to compress a time series\ninto a few LPC parameters by estimating the underlying random process. This means that it is\ninvariant of time shifts and can be made agnostic to scaling of time series when necessary."}, {"title": "6.4 LiPCoT vs. CNN", "content": "While many of the existing architecture utilizes CNN for generating features from time series data,\nLiPCoT uses LPC which encodes data via stochastic modeling. Hence, the input data for LiPCoT\ncan be of variable lengths with different sampling frequencies which is not possible for a CNN-based\nfeature generation. This is especially important for creating foundation models with large datasets.\nFurthermore, the latent space of LiPCoT is shift invariant and interpretative. Finally, the tokenization\nof LiPCoT is computationally efficient making it suitable for large-scale deployments."}, {"title": "7 Limitations and Future Directions", "content": "One major limitation of our work is the lack of a bigger and more diverse dataset which could\nhighlight the implications of our approach for a more generalized time series classification. However,\ndue to the limited computational power and scarcity of similar datasets, in this proof-of-concept work,\nwe focused on the feasibility of our approach and measured the key advantages of our architecture\nin the downstream task performance. To this end, even if our dataset was limited, we were able\nto observe a significant benefit of self-supervised learning of time series data enabled by LiPCoT\ntokenization and superior performance in the downstream task of PD classification compared to\ntraditional approaches. Our results indicate that with a sufficiently large dataset for the pre-training,\nthe performance can go even higher.\nAnother limitation of our proposed LiPCoT tokenizer is its inability to fully recover the original time\nseries after tokenization. The tokens in LiPCoT capture the underlying stochastic random process.\nThis can possibly limit its effectiveness for short-term forecasting tasks. However, such stochastic\nrepresentation of time series data can be beneficial for long-term forecasting.\nIt should be noted that the process of forecasting and generation of time series through LiPCoT is\nvery similar to the diffusion model as both of them generate outputs from white noise. Conceptually,\none can generate many 'candidate' time series predictions using the LPC models embedded in the\nlatent space of LiPCoT and choose the best candidate via optimization of prediction error. These\naspects will be further investigated in a future study."}, {"title": "8 Conclusion", "content": "In this work, we propose LiPCoT, a tokenizer for time series data that converts time series signals into\ndiscrete tokens via stochastic modeling. We measured LiPCoT's performance by utilizing BERT for\nself-supervised learning and the downstream PD classification task on tokenized time series data. We\ncompared the performance of our downstream"}]}