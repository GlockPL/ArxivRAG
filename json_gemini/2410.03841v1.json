{"title": "EXPLAINING THE (NOT SO) OBVIOUS: SIMPLE AND FAST\nEXPLANATION OF STAN, A NEXT POINT OF INTEREST\nRECOMMENDATION SYSTEM", "authors": ["Fajrian Yunus", "Talel Abdessalem"], "abstract": "A lot of effort in recent years have been expended to explain machine learning systems. However,\nsome machine learning methods are inherently explainable, and thus are not completely black\nbox. This enables the developers to make sense of the output without a developing a complex\nand expensive explainability technique. Besides that, explainability should be tailored to suit the\ncontext of the problem. In a recommendation system which relies on collaborative filtering, the\nrecommendation is based on the behaviors of similar users, therefore the explanation should tell\nwhich other users are similar to the current user. Similarly, if the recommendation system is based on\nsequence prediction, the explanation should also tell which input timesteps are the most influential.\nWe demonstrate this philosophy/paradigm in STAN (Spatio-Temporal Attention Network for Next\nLocation Recommendation), a next Point of Interest recommendation system based on collaborative\nfiltering and sequence prediction. We also show that the explanation helps to \"debug\" the output.", "sections": [{"title": "1 Introduction", "content": "Machine learning has seen widespread adoption for the past many years. However, unlike the classic linear regression,\nit is often hard to understand the internal working of modern machine learning techniques like deep learning. This\nproblem led to the emergence of explainable machine learning as a research area. There are techniques such as Shapley's\nvalues [1] or LIME [2] which explain a black box machine learning method. However, such techniques can be expensive.\nFor example, the calculation of Shapley's values often involve sampling. Besides that, the explanation should to take\ninto account the context of the problem itself. In collaborative filtering methods, the information about which other\nusers are similar to the current user is important. Similarly, if the problem is modeled as a sequence prediction problem,\nthe information about which input timesteps are influential is an important information.\nSpatio-Temporal Attention Network for Next Location Recommendation (STAN) [3] is a next Point of Interest (POI)\nrecommendation system based on deep learning. STAN gives a personalized recommendation of the next POI based\non the user's past visited POIs and the visit timestamps. The number of the past visits varies between users. STAN\ninternally uses attention mechanism [4]. Attention mechanism has a side effect that it \u201cexplains\u201d which input timesteps\nare the most important for the output. STAN also has an embedding module whose output represents the \u201cbehavior\" of\nthe user. This creates a notion of similarity between the different users, and thus implements collaborative filtering\nwhere the behaviors of similar users is taken into account to decide the recommendation.\nWe perform a local explanation based on the relevant other users and the relevant timesteps of the current user's past\ntrajectory (i.e., \u201cby relevant user or item\u201d according to [5]'s classification). We use the standard dataset of STAN. This\ndataset contains only the POIs'latitude and longitude and the users's trajectory. The trajectory is modeled as a sequence\nof POI ID and timestamp. Therefore, the explanations are which other users are similar to the current user, and which"}, {"title": "2 Background", "content": "STAN is a next Point of Interest (POI) recommendation system. Its output is the recommended POI'ID while the inputs\nare the user's ID and the user's past trajectory. The trajectory is a variable length sequence where each element is the\nvisited POI's ID and the visit's timestamp. The timestamp's resolution is one hour and the timestamps are cycled per one\nweek, so two check-ins which are 24*7=168 hours apart will have the same timestamps. The only learned information\nabout the POIs is their latitudes and longitudes. These geographic coordinates enable the calculation of distance between\ntwo POIs, and the calculation is done by using haversine formula. STAN does not learn any information about the users\nother than their past trajectories.\nWe use the dataset which is used in STAN's paper: the New York subset of Gowalla full dataset. The dataset has 1083\nusers and 5135 Points of Interest. There is only one trajectory per user."}, {"title": "3 Related Works", "content": "There have been works on techniques to explain the feature importance in machine learning. Shapley's values [1] assign\na scalar value to quantify the feature importance. It works by permutating certain features and observing the output\ndifference. Intuitively, if a feature is important, replacing the values of this feature will induce a significant output change.\nHowever, these permutation and observation operations are expensive, so in practice, approximation and sampling\nare necessary. Different extensions of Shapley's values do the approximation and sampling differently. Some of the\nextensions of Shapley's values are KernelSHAP [6, 7], FastSHAP [8], and TreeSHAP [9]. Shapley's values can do both\nglobal and local explanation. LIME [2] is another method to explain a black box machine learning model. Fundamentally,\nLIME works by using a simpler but interpretable model (e.g., linear regression) to do local approximation. LIME\ncan do local explanation only. However, when there are several features which correlate one another, there will\nbe many correct approximations (i.e, different learned models which are all correct), so there will be many correct\nexplanations too. For example, if f (x1, x2, x3) = 3x1 + 5x2 + 7x3 + \u0454 while x1 = 2x3 + \u20ac, all these approximations\n(or learned models) are correct: f(x1,x2, x3) = x1 + 5x2 + 11x3 + \u20ac, or f(x1, x2, x3) = 5x1 + 5x2 + 3x3 + \u20ac, or\nf(x1, x2, x3) = 5x2 + 13x3 + \u20ac. This problem of non-unique explanation is beyond our concern.\nThere are also techniques which are not meant for explainability, but happen to be naturally explainable. This is the\ncase for the attention mechanism [4]. The attention mechanism was initially introduced for the text translation problem.\nBoth the input and the output data are expressed as variable length sequence of words. The attention mechanism is\nessentially a weight matrix which channels the input elements to the output elements. Because it is a weight matrix, it\nalso explains the relative importance of the input's ith timestep on the output's jth timestep.\nThere are also works to calculate the importance of the training data points. Data Shapley [10] and TracIn [11] solve\nthis problem by using techniques from feature importance problem. Intuitively, if we imagine a 2D matrix where one\naxis contains the features and the other axis contains the data points, and then we run a feature importance algorithm,\nwe will get the importance of each feature. But if we transpose the matrix before running the the feature importance\nalgorithm, we will instead get the importance of each data point. In the context of collaborative filtering, data valuation\nis important because the recommendation explicitly takes into account the behavior of similar users, which are basically\nother data points."}, {"title": "4 Methods", "content": "STAN's output is a vector of length L where L is the number of Points of Interest (POIs). The final output (i.e., the\nrecommended POI) is the argmax of that vector. This is the standard way multi-class neural network classifiers represent\nthe output. Before those L candidates, there are T intermediate values where T is the maximum length of the input\ntimesteps. The \u201cAttention Matching\" block in Figure 1 is a weight matrix which channels those T intermediate values\nto L candidate POIs. This is shown in Equation 1 where W is a weight matrix of size L \u00d7 T, v is an intermediate vector\nof the size T \u00d7 1, and lrecommended is the recommended POI's ID. Thus, the most important timestep is the one whose\nvalue is the highest in Wit (see Equation 2).\n\n\nIrecommended = arg max Wiv\nl\u2208L\n\ntimportant = arg max Wiv\nt\u2208T\n\nThe user behavior embedding is shown in Figure 1 in the \"Multimodal Embedding\" module as eM. The embeddding size\nis configurable, but default setting is 50 dimensions per input timestep. Because the original user embedding is for each\ninput timestep, we create a small neural network to \"compress\" all of them into a vector of 16 dimensions per user. The\nnetwork has hidden layers of 512 and 16 neurons and use ReLU activation function. The network learns by outputting\nthe max value in the slot of the correct user (i.e., the argmax), as is usual in multi-class neural network classifiers.\nWe show the network schema in Figure 2. Onward we will refer to this network as compressor network. Having a\nrepresentation learning network to represent a user enables us compare different users easily. Because this embedding\nis learned from the users's past visits, similar users should have their corresponding 16-dimensional embedding vector\nclose to each other, and similar users should also be within the arg-top-k of the output. This allows us to know which\nother users whose data greatly influences the recommendation for the current user, which is the essence of collaborative\nfiltering."}, {"title": "5 Experiment", "content": "We use the standard STAN dataset (as is described in Section 2). We also use STAN's standard code and standard\nconfiguration to train the model.\nIn (Experiment 1), we try to find the two most important timesteps of each trajectory. Note that there is only one\ntrajectory per user. We use the output of the \u201cAttention Matching\" block in Figure 1. We follow the method which\nwe describe in Section 4, Formulae 1 and 2. In order to verify that those two timesteps are indeed important, for\neach of those timesteps, we replace both the POI ID and the timestamp with random values (Experiment 1/1 and\nExperiment 1/2). Due to the sequential nature of trajectory, the random timestamp is constrained to be between the\nprevious and the next timesteps'timestamps. We do this randomization 10 times and we count how many times the\noutput (i.e., the recommended POI) changes. Then, we do a similar experiment, but we randomly choose the timestep\nto be randomized (Experiment 1/3). Intuitively, if the allegedly important timestep is indeed important, the output\nshould change more often than when we randomly choose the timestep. We redo this experiment one more time as a\nsanity check (Experiment 1/4). We compare the frequencies of output change by using one-way ANOVA test with the\np-value threshold of 0.05. The results are shown in Tables 1 and 2.\nIn the results of Experiment 1, as is shown in Table 1, we find that randomizing the POI ID and the timestamp at a the\nmost important timestep (Experiment 1/1) induces more frequent output (i.e., recommended POI) change than when the"}, {"title": "6 Conclusion", "content": "In Experiment 1, as is shown in Table 1, we find that randomizing the POI ID and the timestamp at a carefully chosen\ntimestep (i.e., Experiments 1/1 and 1/2) changes the output / recommendation more frequently than when the timestep"}]}