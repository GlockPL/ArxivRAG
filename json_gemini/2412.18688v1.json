{"title": "Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation", "authors": ["FARAZ WASEEM", "MUHAMMAD SHAHZAD"], "abstract": "An image may convey a thousand words, but a video-composed of hundreds or thousands of image frames-tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora [110], the current state-of-the-art system, is still limited to producing videos of up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions-essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative Al with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.", "sections": [{"title": "1 Introduction", "content": "The year 2022 marked a significant milestone in the field of the generative AI era with the release of ChatGPT [108]. ChatGPT is an advanced language model that produces human-like text from user input, supporting tasks like answering questions, creative writing, and conversation. This technology uses complex deep neural networks based on large language models trained on extensive text data to capture intricate linguistic patterns and contextual nuances for precise text generation and understanding. Since then, major tech companies have introduced their large language models (LLMS), such as Facebook's LLama series [103], Google's Gemini [102], and a few other notable models, including Claude [101] and Mistral [107]. A transformative breakthrough in image generation accompanied the success of LLMs. DALL-E 2 [79] represented a significant leap beyond traditional Generative Adversarial Network (GAN) and Variational Autoencoder (VAE) models, thanks to its ability to interpret natural language and render a wide range of concepts and styles. It excels at creating photo-realistic images from both realistic and imaginative prompts. Other leading generative Al systems, such as Stable Diffusion 3 [19] and MidJourney [106], also showcase remarkable capabilities in producing photo-realistic visuals based on real and imagined ideas."}, {"title": "1.1 Survey Contributions - Need for Summarizing Long Video Generation Methods", "content": "Long video generation presents numerous challenges beyond crafting and maintaining consistent storylines across scenes. These include the lack of large-scale video datasets with detailed captions and the requirement of significant computational resources. Despite these hurdles, long video generation has emerged as a new frontier in generative Al, offering potential applications in entertainment, education, medicine, marketing, and gaming. This has attracted significant research interest, leading to a rapid increase in studies. As shown in Fig. 4, most papers on long video generation have been published within the last two years. Owing to these interests and opportunities, the time is right to summarize the state-of-the-art in long video generation and discuss the associated challenges, progress, and future directions to support the advancement of the field. To our knowledge, there are only two related long video-generation surveys [50] and [154]. The former [50] delves into the latest trends in long video generation, emphasizing the divide-and-conquer and autoregressive approaches as two primary themes. It also examines photo-realism trends and generative paradigms, such as VAE, GAN, and diffusion-based models. However, while it highlights the divide-and-conquer approach, which simplifies the complexity"}, {"title": "1.2 Survey Focus - Techniques, Challenges & Key Questions in Long Video Generation", "content": "Video generation utilizes various techniques, such as sampling from latent space [57], creating small video segments or images, generating intermediate frames (\"divide and conquer\") [3.2], employing autoregressive methods [3.1] to predict future frames based on initial ones, and enhancing latent state representations for longer videos. Training video generation models presents challenges due to the higher computational requirements and more extensive memory needs for video datasets. Many video generation models build on pre-trained image models[ [37], [62], [5]], enhancing attention mechanisms to ensure consistency between adjacent frames, as video is essentially a sequence of frames. Some long video models are developed by extending short video generation models[ [147], [11], [124]] and improving control mechanisms for longer content. Another significant aspect of video generation is input guidance [[4.1], [4.3],[4.2] ]. Unlike image or short video generation, guidance is crucial for long video generation and is often based on text embedding like CLIP [76]. LLMs [3.2.1] are crucial in guiding the video generation process by understanding world dynamics, object trajectories, and action groupings, using their extensive knowledge to inform the creation of videos. When evaluating the quality of the generated videos[[6.1], [6.2],[6.3],[6.4]], it is important to assess the quality of individual frames, the fluidity of motion, and the overall aesthetic appeal. It is essential to ensure that the generated videos align with the input text and maintain the consistency of entities throughout the video, such as cars and actors. This has sparked interest in investigating innovative research directions in the field and addressing the following relevant research questions:\n(1) How can we generate long videos with multiple semantic segments with different actors, actions, and objects?\n(2) How can we ensure semantic consistency across long video segments, such as maintaining consistent models of objects like cars?\n(3) How can we manage resource requirements for training and inference with long videos?\n(4) How can we handle control signals (conditioning) for extended video sequences?\nOur survey article centers around these critical questions, providing insights to guide researchers and practitioners in addressing these challenges."}, {"title": "1.3 Survey Methodology", "content": "For this survey, we conducted searches across several conferences, including but not limited to ICCV, ECCV, CVPR, ICLR, ICML, IEEE, TPAMI, IEEE Neural Networks, AAAI, WACV, ICIP, NeurIPS, KDD, ACCV, IJCV, AI Open, and CVIU. We used keywords such as \"video generation,\" \"long video generation,\" and \"LLM-guided video generation.\" Additionally, we searched academic databases, including Google Scholar, IEEE Xplore, ACM Transactions, and Scopus, focusing on the term \"long video generation\" for our survey.\nOur survey covers papers published between 2021 and 2024 (till August end), with a focus on \"video generation\" and \"generative AI\". We gathered 100+ articles through snowball sampling, using keywords like text-to-video, generative AI, visual interpretation, and extended video generation."}, {"title": "1.4 Survey Organization", "content": "We will begin by discussing the foundational framework for video generation, including embedding and LLMs, to set the stage for more advanced topics. The goal is to familiarize readers with these fundamental components, enabling them to explore these building blocks according to their level of expertise. Next, we will explore backbone mechanisms for video generation, like divide and conquer autoregressive and global enhancements. We will explore input guidance mechanisms, including strategies like LLM guidance, and categorize them into different levels based on the depth of control the LLM exerts. We will also address the necessary image and video diffusion model modifications to facilitate such control. We will also discuss the post-processing pipelines required to achieve high temporal and spatial quality in videos generated by diffusion models. We will then discuss datasets used to train video generation models. We will then talk about metrics used to measure generated video quality. Finally, we will talk about future trends and open challenges."}, {"title": "2 Long Video Generation: Backbone Architectures and Methods", "content": "Progress in long video generation builds on advancements in many foundational building blocks. These include GANs-based architecture 2.1, autoencoders 2.2, Transformers-based models 2.3, LLMs and language understanding 2.4 and image and video diffusion models 2.5."}, {"title": "2.1 GAN Based Video Generation", "content": "Generative Adversarial Networks (GANs) were considered the state-of-the-art (SOTA) for generative tasks from around 2014 to the early 2020s, but recent advancements such as diffusion models and transformer-based methods have surpassed them in both performance and versatility, particularly in applications like image synthesis and video generation. This section explores the formulation of GANs and highlights the pivotal papers that have shaped advance-ments in the field. GANs [22] are inspired by game theory and consist of two core components: the Generator and the Discriminator. The Generator's purpose is to convert random noise, typically sampled from a simple, uniform distribution, into data samples, which can range from images to videos. Simultaneously, the Discriminator assesses these samples, classifying them as real (originating from the training dataset) or fake (created by the Generator). This GAN formulation is the basis of image and video generation, which further sections explain."}, {"title": "2.1.1 GAN Based Image Generation", "content": null}, {"title": "2.1.2 Video/Multi Frame Generation.", "content": "Early Attempts: Early research in video synthesis primarily concentrated on video prediction like in [65], which involves generating future frames based on a sequence of previously observed ones. [122] extended the convolutional model to include two distinct convolution networks. It decomposes video into a static background and a moving foreground, generating them with 2D and 3D convolutional networks. VGAN can generate 32 frames of realistic videos of golf courses, beaches, train stations, etc. VGAN is an unconditional video generator, i.e., without incorporating any auxiliary input such as text. 'Multi-stage Dynamic Generative Adversarial Networks' [136] predicts future video frames based on the first frame. It improved existing models by generating 32 frames with 128 x 128 resolution in a time-lapse pattern. The first stage generates a time-lapse video with realistic content in its two-stage model. In contrast, the second stage optimizes the results from the first stage, primarily by incorporating dynamic motion information to enhance realism. The models mentioned above are not prompt-based, and prompt-based models will be explored in the following section."}, {"title": "2.2 Autoencoder Based Video Generation", "content": "Autoencoders [114], variational autoencoders [47], and masked autoencoders [25] belong to the family of models that compress information into a compact latent space and serve as building blocks for image and video generation pipelines. Masked autoencoders can generate videos from this learned latent space. We will discuss the foundations of autoencoders and build up the video generation process via masked autoencoders."}, {"title": "2.2.1 Autoencoder Formulation", "content": "Autoencoder is an unsupervised neural network that compresses its input $x$ to a compact latent layer and then learns to reproduce its input $\\hat{x}$ via backpropagation. The autoencoder is trained to minimize the reconstruction loss between the input $x$ and the reconstructed output $\\hat{x}$. The most common application of autoencoder for video and long video generation is the construction of compressed latent space. For example, in [84], authors use an encoder and decoder to project images from pixel to latent space to decrease the computational complexity of learning image distribution."}, {"title": "2.2.2 Masked Autoencoders", "content": "Masked autoencoder [25] are scalable self-supervised learners for computer vision and have been used as the backbone for video generation by masking random patches of the input image and reconstructing the missing pixels. Video mask autoencoder is based on image mask autoencoder. During pre-training, a sizeable random portion of image patches are masked. The encoder processes only the remaining visible patches. After encoding, mask tokens are introduced, and the combined set of encoded patches and mask tokens is passed through a small decoder to reconstruct the original image at the pixel level. An extension of this approach to the video domain but using convolution instead of transformers is VideoMAC [73]. It employs Masked Encoders and resource-friendly convNet architecture. VideoMAC is"}, {"title": "2.3 Transformer Based Video Generation", "content": "GANs have limitations like mode collapse [32], training instability, which needs fine-tuning of parameters, and a lot of training time and resources. Transformers, introduced in 2017 [118], made inroads into image and generation via autoregressive and masked encoding. Some of the key concepts to understand are Vision Transformers and Video Transformers."}, {"title": "2.3.1 Transformer Based Image Generation.", "content": "Vision Transformers (ViT) [18] represent a significant shift in the landscape of computer vision. The ViT architecture operates similarly to transformers used in natural language processing by dividing an image into smaller patches, like breaking text into tokens. DALL-E [81] trained autoregressive transformer using 250 million image text pairs. They used 'Discrete VAE' [83] to compress 256*256 images to 32 \u00d7 32 grid of image tokens. They concat these tokens with corresponding BPE text tokens and autoregressively learn the joint distribution of text and image tokens using a GPT transformer. CogView [15] based on GPT transformer architecture outperforms DALL-E using FID distance, but DALL-E has better rendering capabilities of complex prompts. Autoregressive approaches like DALL-E and CogView suffer from slow generation because of their unidirectional token-by-token generation nature. CogView2 [17] improved the limitation of previous methods by training the Cross-Modal General Language Model by learning the joint distribution of image and text tokens using masking."}, {"title": "2.3.2 Autoregressive Based Video Generation", "content": "Video Transformers (VViT) [3] represent a significant shift in the landscape of computer video generation. The VViT architecture functions like vision transformers [18], but instead of tokenizing image patches, it tokenizes video patches. Phenaki [121] can generate arbitrary long videos conditioned on a sequence of prompts. Phenaki uses a pre-trained language model t5x for text embeddings, [82]. It uses a variation of ViViT [3], C-ViViT, which extracts and compresses video tokens to a minimum. It masked some tokens and learned to predict masked video tokens by the bidirectional transformer. During inference, it can predict the next token using an autoregressive transformer. The compression mechanism allows us to train and predict very long video sequences. CogVideo [35] extend CogView2 [17]. It uses hierarchical training with multiple frame rates to enhance the alignment of text-clip pairs, significantly improving generation accuracy, especially for movements with complex semantics. In its two-stage architecture for video generation, it first generates keyframes auto-regressive, and then another transformer interpolates between these frames."}, {"title": "2.4 Language Understanding In Video Generation", "content": null}, {"title": "2.4.1 Text To Image Feature Representation", "content": "The core principle behind text-based visual generation tasks is effectively pairing text with the visual content. Many visual generation pipelines leverage pre-existing image-text pair models like CLIP (Contrastive Language-Image"}, {"title": "2.4.2 LLMs Based Video Guidance", "content": "Many visual generation models like LLM Director [156] leveraged standalone large language models (LLM) [6] to enhance their performance. By integrating LLM, visual generation models can benefit from advanced natural language processing capabilities, allowing them to interpret and generate more nuanced and contextually relevant descriptions of captions in a single or multiple prompt with detailed scenes. One example of this design is LLM grounded VDM [52]. When paired with visual inputs, LLMs can transform simple image descriptions into more elaborate storytelling, adding layers of meaning and context that enhance the viewer's experience. LLM can also act as director of the whole video generation process and create coherent script shown by Vlogger [158]. The details on how the recent long video generation methods leverage LLMs are explained in 3.2.1."}, {"title": "2.5 Diffusion Models", "content": "Diffusion models are now state-of-the-art in image and video diffusion. Diffusion-based image and video pipelines use many building blocks, such as variational autoencoders, transformers, and language understanding, and have introduced new baselines. We will briefly cover the history and progress of diffusion-based architecture. The foundational work [92] established a crucial framework for this approach, applying principles from nonequilibrium thermodynamics to unsupervised learning. This insight paved the way for subsequent advancements in generative modeling. A probabilistic diffusion model [30] is a parameterized Markov chain trained via variational inference to generate samples that closely resemble the data over a finite duration. These ideas are applied to image and video generation, as discussed in the next section."}, {"title": "2.5.1 Image Diffusion .", "content": "Image diffusion models generate images by denoising noisy inputs through noise prediction. [94] introduced a novel perspective by estimating the data distribution's gradients, further enriching the understanding of generative processes. However, the significant breakthrough in this area came in with [30] influential paper on \"Denoising Diffusion Probabilistic Models.\" do forward and reverse diffusion processes in latent space. The general mechanism for conditioning from single modality like text is to have a conditional denoising decoder using a cross attention layer of transformer in UNET backbone trained with embedding from text encoder like CLIP [76]. In addition to CLIP, text encoders like BERT [13] or T5 Encoder [78] can be used. In their paper [84], the authors implement this paradigm by conditioning image generation through modifications to the attention layer of the U-Net. They achieve this by concatenating embeddings from different modalities, such as text or others [118]."}, {"title": "2.5.2 Video Generation From Diffusion models.", "content": "Video diffusion models are built on the same diffusion architecture as image diffusion models, but video diffusion models also need to ensure consistency between frames. Image diffusion model can generate videos by either extending it into the temporal domain, as done in the Video Diffusion Model [31], or by generating keyframes using an image"}, {"title": "3 Long Video: Generation Paradigm", "content": "We summarized various video generation approaches into three core paradigms.\n\u2022 Auto-regressive Paradigm: Videos are generated sequentially, with each frame conditioned on the frames generated before.\n\u2022 Divide and Conquer Approach: Videos are produced by creating keyframes or short video segments guided by storyline prompts, often with the aid of a large language model (LLM).\n\u2022 Implicit Video Generation: Videos are generated implicitly from the model without needing explicit extrapolation (autoregressive approach) or explicit interpolation (divide-and-conquer) by designing latent space to represent variable-size videos.\nThese approaches will be explained in the following sections."}, {"title": "3.1 Auto Regressive Approaches", "content": "The Autoregressive generation paradigm works by future frame prediction. It trains and performs inference of the model on previous video frames. Each frame serves as the input for the future frames. This prediction process ensures the generated videos are consistent and coherently structured. It generates a context from anchor frame (generated via text prompt), text-to-image embedding, image, or multimodal input [[14], [120], [[113]]. It uses context and previous N frames and generates future N frames optionally based on additional prompts and prior frames. It ensures robust consistency between frames due to the sequential nature of frame transition. This is illustarted in Fig. 5. Nevertheless, it needs to improve performance bottlenecks when generating long videos, as it has to create frames sequentially and can not leverage parallelization.\nCogVideo [35] is one of the first open-source autoregressive transformers for long video generation. CogVideo builds on the pre-trained text-to-image model CogView2 [16] to inherit the knowledge gained from text-to-image pretraining. CogVideo has limitations on the length of the input sequence and the spatial resolution of frames (default 160 x 160 can be upsampled to 480 x 480). NUWA-Infinity [54] improved spatial resolution by adopting a hierarchical video generation model. One limitation of CogVideo and NUWA-Infinity is that they consume a single prompt as input. Phenaki [120] can handle multiple progressive text prompts. The model manages to preserve the temporal coherence of the video while adapting to the new prompt. It uses a single-level autoregressive model instead of the two-level"}, {"title": "3.2 Divide And Conquer Paradigms", "content": "The basic theme of divide and conquer is to generate keyframes or short clips based on single or multiple prompts and then interpolate between these frames or clips. The system often uses an anchor image as a reference frame for generating all subsequent frames. The system generates each key frame independently, allowing for parallel processing. Some challenges with the divide and conquer approach are keeping semantic consistency, smooth motion transformation between interpolated frames, and frame quality. One key theme of the Divide and Conquer approach is separating the planning and video generation stages, differentiating it from the autoregressive approach as illustrated in Fig. 8. Divide And Conquer Paradigms can be divided into three sub-paradigms: the Large language model as director, the Intermediate transition model, and the Agent-based framework. Some milestone papers with timelines are illustrated in Fig. 7"}, {"title": "3.2.1 Large Language Model As Director.", "content": "The Large Language Model as Director [55] [125] [86], [86] [38] paradigm starts by identifying key frames that define the core narrative and then creates the intermediate frames to smoothly connect these critical frames into a cohesive, extended video. This approach differentiates between essential storyline keyframes and the additional frames required for continuity. It consists of two main components: the LLM Planner, which produces the storyline blueprint, and the Video Generator Backbone, which follows the blueprint provided by the LLM to generate the video as illustrated in Fig. 9. Initially, the model develops a script detailing each frame and optional metadata, such as entities, layouts, and actions. The model can then create images and interpolate between them using the semantic descriptions and entities from earlier frames to guide the denoising process. LLMs are crucial in this approach, as they generate detailed storylines from text prompts, specifying scenes, characters, and actions [55]. This paradigm works with both training-based and training-free (zero-shot) approaches. Free-Bloom [37] took a zero-shot approach. It has a three-stage architecture starting with LLM to generate a sequence of text-based prompts that decompose the original prompt into a semantically coherent sequence of prompts using world knowledge of LLM. It proposes novel techniques like joint noise sampling, step-aware attention, and dual-path interpolation to ensure the temporal coherence of the generated frames."}, {"title": "3.2.2 Multi stage/Agent Based Divide And Conquer Approach .", "content": "An Agent-Based LLM Framework [146] is a system where multiple models function as specialized agents, each designed to perform a distinct task or interact within a defined environment. In this system, the LLM serves as the \"brain,\" responsible for overseeing complex operations, while simpler models function as tools, executing more specific, supportive tasks. A multi-agent framework for video generation represents a multi-layered approach to text-to-video generation. It produces high-quality long videos like those generated by Sora [58] by dividing the video creation challenge into multiple systems, each specializing in some aspect of the video generation pipeline as illustrated in Fig. 10. The framework facilitates efficient and dynamic video production by coordinating these agents flexibly and iteratively. Systems like Mora [145] and Vlogger [158] illustrate how integrating and coordinating multiple specialized agents can enhance and streamline the video creation. In Mora [145] Multiple models collaborate through an agent framework for script creation, image generation, image enhancement, video production, and video patching. VLogger [158] is a comprehensive system designed to produce cohesive vlogs by integrating various modules. It begins with the LLM Director translating a user's story into a detailed script outlining scenes and duration. The process continues with the Actor module, where the LLM Director assesses the script to identify characters and collaborates with a character designer like SD-XL [74] to generate appropriate reference images for the actors. It has a show maker module to act as a videographer and a voicer module that synchronizes the script with voice-over, utilizing a model like Bark [111] to align subtitles with the video. This setup exemplifies a modular agent-based system for high-quality video production."}, {"title": "3.2.3 Divide and Conquer Compositional/Transition Approach.", "content": "This method uses or generates short video segments using any standard video generation technique, such as divide-and-conquer strategies or autoregressive models. It then integrates these segments by employing a transition model to create smooth transitions between them. The challenge with transition videos is to ensure coherence and visual quality and smoothly transition from one set of entities to another. SEINE [11] adopts a predictive approach for creating transitions between short videos produced by any public text-to-video model. It uses the last frame of one segment and the first frame of the next as input, applying a random masked diffusion model to handle the transitions. This approach involves jointly denoising overlapping short videos in the temporal domain. It starts by generating these short videos with a standard diffusion model, incorporating temporal overlaps based on given prompts. The model then stitches these segments together into a longer video. The process includes transforming the video into a reduced-noise domain through noise inversion and additional denoising with the GenL model. MEVG [68] ensures temporal coherence between independently generated video clips based on multiple event descriptions. It first creates a video for each event description using a public single-prompt video generation model. Subsequent clips are then generated by conditioning on the last frame of the previous clip and the provided text, maintaining continuity. MAVIN [147](Multi-Action Video INfilling model) is designed to generate transition videos that seamlessly connect two given videos, forming a cohesive integrated sequence. It learns video infilling by dividing a training video into three segments, corrupting the intermediate segment, and then learning to predict the noise of the missing segment. ENCODER-EMPOWERED GAN [140] generates long videos using an encoder-based GAN to connect short generated video clips into longer sequences of hundreds of frames. It models temporal relationships between consecutive video clips. The process involves generating short video clips with EncGAN3 and then training to enforce temporal connectivity between these clips through a recall mechanism."}, {"title": "3.3 Implicit Video Generation", "content": "Implicit video generation covers a family of models that generate complete video using compact latent space representation, enhanced attention mechanisms to incorporate long-term dependencies, or denoising strategies to include short-term and global noise in the generated video. What differentiates implicit video generation from divide-and-conquer and the autoregressive approach is that it generates all frames simultaneously instead of sequentially or parallelly. For example, Sora [57] compresses raw video into a latent spacetime representation, extracting patches to capture visual appearance and motion dynamics over short intervals. As a diffusion transformer, Sora consists of three components: (1) a time-space compressor that maps the original video into latent space, (2) a Vision Transformer (ViT) that processes this latent representation to produce a denoised version, and (3) a CLIP-like conditioning mechanism that uses LLM-augmented instructions and visual prompts to guide video generation. This is illustrated in 11. FreeNoise [75] presents a tuning-free and time-efficient paradigm that enhances pre-trained video diffusion models' generative capabilities while maintaining semantic consistency. GLOBER [97] is a video auto-encoder that includes a video encoder to compress videos into global features and a video decoder based on a diffusion model to reconstruct video frames from these features in a non-autoregressive manner. Despite progress in these areas, motion consistency, semantic alignment, and parallel processing remain key obstacles to achieving scalable, high-quality long video generation. An example of this limitation can be seen in state-of-the-art video generation models like SORA [110]. For instance, SORA generates videos that are only up to one minute long, far shorter than the typical duration of entertainment or educational videos. Issues such as unrealistic"}, {"title": "4 Long Video: Input Control", "content": "Input conditioning involves diffusion models, GANs, or autoencoders using signals from user text prompts, entity layouts, bounding boxes, and images to condition video generation. Although long video generation utilizes many of the same techniques for input control as image and video generation models, it also faces the additional challenge of preserving long-term dependencies. Video generation models utilize innovative strategies like use of LLM to create progressive prompts from single input prompt [37], [33], [52], [128], [55] and enhancement in generation mechanism to create semantic consistency between frames [37], [62], [33]. Poplar mechanisms for input conditioning of long videos are User Textual Prompt, User Textual Prompt With Scene Layout, and Image Input With Textual Prompt And Scene Layout, which will be discussed next."}, {"title": "4.1 User Textual Prompt", "content": "User textual prompts are the most common way of conditioning the video generation models. Video generation can utilize single or multiple text prompts. Dall-E [80] is one of the first transformer-based text-to-image generation models. Dall-E concatenated BPE-encoded text tokens with the image tokens and trained an autoregressive transformer to model the joint distribution over the text and image tokens. CogVideo [34] takes a similar approach and trains autoregressive transformers with text and image tokens together on joint distribution. CogVideo has conditioning limitations to a single prompt. Phenaki [121] improved this paradigm by incorporating t5x [82] pre-trained embedding and conditioned"}, {"title": "4.2 User Textual Prompt with Scenes layout", "content": "Multi-modal input control mechanisms can work with multi-modalities like text, images, edge maps, bounding boxes, music, etc. We can enhance text-only prompts by adding metadata about bounding boxes, entity descriptions, trajectories, and background context, as illustrated in Fig. 8. Stable Diffusion did some groundwork for multi-modal input control [84]. It conditions diffusion-based general conditioning mechanisms based on cross-attention by augmenting their underlying UNet backbone as illustarted in Fig. 13. ControlNet [150] lays out a groundbreaking approach to"}, {"title": "4.3 Image Input With Textual Prompt And Scenes Layout", "content": "Images provide high-quality aesthetic context for video generation. image prompts provide spatial information as well as details with different granularities like the car's color, the carpet's texture, and the relative position and sizes of entities, which are difficult to specify in the scene description capability of LLM. We can generate or provide key entity images like actor images as a condition for the video generation pipeline. NUWA-Infinity [54] can process both"}, {"title": "5 Existing Datasets", "content": "The existing datasets for long video generation can be categorized as classification datasets 5.1 and captions datasets 5.2"}, {"title": "5.1 Classification Datasets", "content": "Classification datasets contain videos labeled with different categories, such as actions, objects, scenes, or events. UCF-101 [95] is one of the earlier popular long video classification datasets with single class labels. It includes 101 action classes with 13,320 clips from 2,500 distinct videos divided into five categories: Human-Object Interaction, Body Motion Only, Human-Human Interaction, Playing Musical Instruments, and Sports. UCF101 has limitations due to the scale and number of classes. The Kinetic dataset [9] developed by DeepMind Google improves the scale of video datasets to 306,245 videos. DeepMind released four versions of the Kinetics dataset: Kinetics-400, Kinetics-600, Kinetics-700, and Kinetics-700-2020, with the numbers representing the number of classes. Kinetics-700-2020 was the latest version, released in 2020. The Kinetics dataset contains 306,245 distinct videos, a vast improvement compared to 2500 videos from the UCF101 dataset, and is also a single class per video. Youtube-8M [1] increased the scale of videos further by curating 8 million YouTube videos with 350k+ hours of video as a multi-label dataset as compared to single label UCF101 and Kinetics. Compared to UCF and Kinetics datasets, 8M YouTube datasets contain videos on diverse topics, including gaming, food, entertainment, health, and finance. HowTo100M [66] scaled it further by 136 million video clips with multi labels per video sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. The granularity of video labeling can be enhanced by moving from class labels to sentences that describe scenes in the video captions dataset. The video captions datasets pair clips with sentences like in MSR-VTT [137]; researchers sampled four frames from each video and annotated them with human-labeled sentences. This evolution of video datasets represents a general theme from single-label limited video datasets to large-scale video datasets with caption-based annotation."}, {"title": "5.2 Captions Datasets", "content": "MSR-VTT (MSRVideo to Text) is one of the first datasets to feature natural language video descriptions. It has 41.2 hours of videos and 200K clip-sentence pairs, covering the most comprehensive categories in earlier days of video research. MSR-VTT is illustrated in Fig. 14. WebVid-2M [4] improved the scale of the dataset by compiling two million videos and captions generated through automatic image captioning, similar to Google's conceptual captions [87]. InternVid [130] took a similar approach of using automatic captioning of images and enhanced the volume of videos further by creating a dataset of 234M video clips accompanied by detailed descriptions of a total of 4.1B words. A critical aspect of video generation is using spatial and temporal clues to create more detailed descriptions of scenes and events. InternVid and WebVid-2M lack high-quality captions with spatial and temporal context due to the algorithmic nature of caption generation and, in many cases, are often short(12-30 words). The latest dataset, like VideoInstruct-100K [64], addresses the quality of caption needed for video generation. VideoInstruct-100K took a hybrid approach to improve the quality of captions and used human-assisted and semi-automatic annotation techniques. Human workers took a subset of ActivityNet [27] and enriched it with detailed information about spatial and temporal aspects, object relationships, reasoning, scene descriptions, and the chronological sequence of events enriches the captions. Another recent dataset focused on improving the quality of video captions is MiraData [42]. They sampled video frames and used GPT4-V to create a \"dense caption,\" which covers the main subject, movements, style, backgrounds, and cameras using structured tags."}, {"title": "6 Performance Measures", "content": "Video generation metrics can mainly be categorized into four categories, including Image Quality Metrics 6.1, Video Quality Metrics 6.2, Semantics Quality Metrics 6.3 and Composite Metrics 6.4."}, {"title": "6.1 Image Quality Metrics", "content": "Image quality metrics are essential for evaluating the quality of individual image frames produced by generative models. Among the various metrics, the Inception Score (IS) [85"}]}