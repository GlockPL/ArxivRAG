{"title": "Boosting Audio Visual Question Answering via Key Semantic-Aware Cues", "authors": ["Guangyao Li", "Henghui Du", "Di Hu"], "abstract": "The Audio Visual Question Answering (AVQA) task aims to answer questions related to various visual objects, sounds, and their interactions in videos. Such naturally multimodal videos contain rich and complex dynamic audio-visual components, with only a portion of them closely related to the given questions. Hence, effectively perceiving audio-visual cues relevant to the given questions is crucial for correctly answering them. In this paper, we propose a Temporal-Spatial Perception Model (TSPM), which aims to empower the model to perceive key visual and auditory cues related to the questions. Specifically, considering the challenge of aligning non-declarative questions and visual representations into the same semantic space using visual-language pretrained models, we construct declarative sentence prompts derived from the question template, to assist the temporal perception module in better identifying critical segments relevant to the questions. Subsequently, a spatial perception module is designed to merge visual tokens from selected segments to highlight key latent targets, followed by cross-modal interaction with audio to perceive potential sound-aware areas. Finally, the significant temporal-spatial cues from these modules are integrated to answer the question. Extensive experiments on multiple AVQA benchmarks demonstrate that our framework excels not only in understanding audio-visual scenes but also in answering complex questions effectively. Code is available at https://github.com/GeWu-Lab/TSPM.", "sections": [{"title": "1 Introduction", "content": "Audio and visual cues abundantly contribute to conveying information in our daily lives, and both modalities jointly improve our ability in scene perception and understanding [34]. For instance, imagining that we are driving along a winding mountain road, honking the horn ahead of time is often safer than relying solely on observing the road ahead with our eyes. In recent years, we have seen significant progress in sound source localization [12, 13, 29] and separation [8, 39], event localization [31, 41], video parsing [30, 40], segmentation [33, 42], question answering [19, 23, 35], etc., towards audio-visual scene understanding. Particularly, the Audio-Visual Question Answering (AVQA) task, involving the fine-grained spatio-temporal perception and reasoning of complex audio-visual scenes, has emerged as valuable and challenging focus of research interest."}, {"title": "2 Related works", "content": "Inspired by the multisensory perception of humans, the community has paid more and more attention to audio-visual scene understanding in recent years [34]. Compared to other modalities, visual and auditory modalities possess unique characteristics such as cognitive foundation, semantic consistency, spatial consistency, temporal consistency, and rich support from real-world data. It includes various interesting tasks such as sound source localization [12, 13, 29], action recognition [9], event localization [31, 41], video parsing [3, 11, 30, 40], segmentation [20, 32, 42], etc. These studies integrate rich audiovisual information within multimodal scenes to overcome limitations in perception inherent to single modalities, thereby utilizing both auditory and visual modalities to explore finer-grained scene comprehension.\nApart from the above methods that facilitate scene understanding by excavating and analyzing different modalities, a unified multimodal model should also be able to reason their spatiotemporal correlation. Therefore, we focus on the audio-visual question answering task [5, 19, 23, 35, 37] and explore spatiotemporal perception and reasoning in the audio-visual context."}, {"title": "2.1 Audio Visual Scene Understanding", "content": "Audio-visual question answering, which exploits the natural multimodal medium of video, is attracting increasing attention from researchers [2, 24]. It requires a comprehensive understanding and integration of diverse modalities, leading to precise responses to distinct questions. To explore the above AVQA task, Yun et al. [37] proposed the Pano-AVQA, which includes 360-degree videos and their corresponding question-answer pairs, aimed at exploring understanding of panoramic scenes. Li et al. [19] presented that the MUSIC-AVQA has become a strong benchmark for promoting spatio-temporal reasoning research in dynamic and long-term audio-visual scenes. Considering that real-life scenarios contain a greater variety of audio-visual daily activities, AVQA benchmark is proposed in [35], which further expands the audio visual scene coverage of AVQA task. Recently, LAVISH [24] has dedicated to exploring improvements in audio-visual association and enhancing training efficiency, resulting in satisfactory outcomes.\nAbove research extract audio and visual features globally, without considering the importance of local feature representation. Chen et al. [4] consider the importance of the given question, which guides the feature extraction of both audio and visual signals. And then the PSTP-Net [18] is proposed to explore critical temporal segments and sound-aware regions among the complex audiovisual scenarios progressively. However, aligning questions with video semantics is challenging due to its non-declarative nature, making it hard to identify key relevant segments. Our work focuses on empowering the model to gradually perceive essential visual and auditory cues for audio-visual scene understanding."}, {"title": "3 Method", "content": "To solve the AVQA challenges, we propose an effective Temporal-Spatial Perception Model (TSPM) to achieve fine-grained audio-visual scene understanding, thus answering questions accurately. An overview of the proposed framework is illustrated in Fig. 2."}, {"title": "3.1 Input Representation", "content": "Given an input audio-visual video sequence, we first divide it into T non-overlapping audio and visual segment pairs {$a_t, v_t$}$_{t=1}^T$, where each segment is 1s long. Subsequently, we partition each visual frame into M patches and append a special [CLS] token to the beginning of the first patch. The question sentence Q is tokenized into N individual words {$q_n$}$_{n=1}^N$.\nAudio Representation. For each audio segment $a_t$, we use the pre-trained VGGish [10] model to extract the audio feature as $f_a \\in R^D$, where D is the feature dimension. The pretrained VGGish model is a VGG-like 2-D CNN network that trained on the large-scale AudioSet [10] dataset, employing over transformed audio spectrograms. Then the features at the audio spectrogram second-level can be interpreted as $F_a = {f_a^1, f_a^2, ..., f_a^n }$.\nVisual Representation. A fixed number of frames are sampled from each visual segment $v_t$. Then we apply pre-trained CLIP [27], with frozen parameters, extract both frame-level and token-level features as $f_v^f$ and $f_v^p$ on video frames, respectively, where $f_v^f \\in R^D$, $f_v^p \\in R^{M \times D}$ and M are token numbers of one frame. Finally, the visual frame-level and token-level features can be denoted as $F_v = {f_v^1, f_v^2, ..., f_v^n}$, $F_p = {f_p^1, f_p^2, ..., f_p^n}$, respectively.\nText Representation. Given an asked question Q, we represent each word $q_n$ in a fixed length vector with word embeddings, and then feed it into the pre-trained CLIP[27] model to get the question feature $F_Q$, where $F_Q \\in R^D$. Note that the first token pooling is used for extracting question features."}, {"title": "3.2 Temporal Perception Module", "content": "To highlight the $Topk$ crucial temporal segments that are relevant to the question, we propose a Temporal Perception Module (TPM) with a carefully designed text prompt. While previous works [4, 18] have considered identifying key segments through the semantic similarity between question and temporal visual segments, aligning questions with visual frame semantics poses a significant challenge due to the non-declarative sentence of the questions. Therefore, the key of TPM lies in constructing a declarative sentence, aligning it effectively with the semantic content of the video, and facilitating the identification of critical segments.\nTo achieve this, we devised a Text Prompt Constructor (TPC) with the goal of generating declarative statements based on input questions. This helps semantic alignment between the generated statements and the visual frame, facilitating the identify key temporal segments to enhance the model's temporal perception ability. Specifically, the TPC process is as follows: 1) Construction Guidelines: Since the input question does not contain answers, directly transforming them into declarative statements poses difficulties. Hence, considering the design of statements that exclude irrelevant segments, guiding the model's attention toward temporal content relevant to the questions. 2) Construction Process: Based on the question templates, we manually constructed corresponding declarative sentence templates following the guidelines. These templates were refined and optimized through multiple discussions with several contributors to ensure their validity. 3) Construction Results: Illustrated by the example in Fig. 2, for input question \"Where is the first sounding instrument?\", the objective is to identify the moment when the first instrument starts playing. Considering that instruments in the video do not play simultaneously but follow a sequential order, we direct the model's attention to segments in the video where instruments do not play simultaneously. This directs the model to focus on segments where there are changes in the order of instrument sounds, identifying crucial segments. Leveraging the TPC, we manually transform the question into a declarative sentence \"The instruments in the video do not sound at the same time.\", denoted as $TPrompt$, aligning its feature representation well with the semantic content of the video. This allows us to locate segments related to $TPrompt$ and subsequently locate temporal segments relevant to the question.\nFor a given declarative sentence $TPrompt$, its feature embedding $F_{TPrompt}$ using the same encoder as the given question. Concretely, we first use one linear projection layer $Key(.)$ to transform indexing visual features $F_v$ to indexing keys $k$. Then we get an attention score for each indexing key in the video temporal sequence. A Softmax layer normalizes the attention scores and generates an attention weight vector W by:\n$W = Softmax(-\\frac{F_{TPrompt} \\cdot [k_1,k_2, ..., k_T]^T}{\\sqrt{d}}),\\quad (1)$\nwhere $k_j = Key(F_v^j), j \\in {1, 2, ..., T}$, and d is the dimensionality of the key vector. Considering that the higher weight indicates a stronger correlation between the video content and $TPrompt$, we conduct $Topk$ feature selection over T segments. To be specific, we employ a temporal selection operation algorithm, denoted as $\\Psi$, which is implemented by the sorted algorithm for ranking and sorting to pick out the crucial relevant segments with the highest attention weights and their corresponding indices:\n$F_a', F_v', \\Omega_{TPM} = \\Psi(F_a, F_v, W, Topk), \\quad (2)$\nwhere $\\Psi$ is a selection operation, $\\Omega_{TPM}$ is the index position corresponding to the $Topk$ highest weights, $\\Omega_{TPM} \\in {0, 1, ..., k-1}^{Topk}$, $F_a', F_v'$ is selected temporal feature, $F_a' \\in R^{Topk \\times D}, F_v' \\in R^{Topk \\times D}$. Note that the $Topk$ temporal audio segments are corresponding to positions on the $Topk$ visual segments relevant to the question."}, {"title": "3.3 Spatial Perception Module", "content": "To identify visual regions that are pertinent to the key instrument, the Spatial Perception Module (SPM) is designed to merge visual tokens in selected temporal segments based on similarity, preserving their semantics, and subsequently engages in cross-modal interaction with audio to enhance audio-visual association. Given previous works [18] attempt to identify crucial regions by leveraging the semantics similarity between questions and visual tokens, the lack of semantic about objects within these tokens presents a challenge in establishing effective correlations with sound.\nTo address this, we enhanced the preservation of semantic information in visual tokens along selected key temporal sequences. We achieve this by merging similar tokens within each visual frame, resulting in merged tokens that carry richer semantic information about objects. Especially, given the visual token-level embedding $F_p$ and $Topk$ curious temporal segment index, we obtain the temporal visual token-level features as follows:\n$F_\\nu = \\Phi(F_p, \\Omega_{TPM}), \\quad (3)$\nwhere $F_\\nu' \\in R^{Topk \\times M \\times D}$, and $\\Phi$ represents an operation aimed at selecting relevant visual token-level features $F_p$ based on the $Topk$ indices $\\Omega_{TPM}$, to serve as the visual input for the SPM. Inspired by ToMe [1], for the given selected visual token-level feature $F_\\nu'$, we employ a token-merging strategy to enhance the semantic features between the attention and MLP branches of each transformer block. Then, similar tokens are merged in each transformer block per layer, and the merged visual token-level feature $F_p^'$ as:\n$F_p' = Merge(F_\\nu'), \\quad (4)$\nwhere $f_p^\\prime = {f_{p_1}, f_{p_2},...,f_{p_s}}$, $F_p' \\in R^{A \\times S \\times D}$ and $\\alpha$ is selected temporal segments' moment, S is merged tokens number. Specifically, as shown in Fig. 3, evenly divide the M tokens in $F_\\nu'$, into two subsets A and B of roughly equal size in Step 1. Then, for one subset A, calculate the similarity between each token and every token in the other subset B, drawing an edge for each calculated similarity. Subsequently, apply mean fusion to the tokens connected by the similar edges. And in Step 2, concatenating the two subsets to generate a merged visual token-level feature $F_p'$. It's worth noting that between each transformer block's attention branch and MLP branch, Step 1 through Step 3 of the visual token merging process is executed, resulting in the creation of multiple merged tokens sequences with semantic representations. During the merge process, all tokens are divided into two sets A and B based on their odd and even positions. Given this way, the position embedding merely serves as an odd-even indicator, having a negligible impact on the final merging outcome.\nThen, considering that the sound and the location of its visual source usually reflect the spatial association between audio and visual modality, we leverage the powerful cross-modal perception ability to interact between selected visual merged token-level features and audio embeddings $F_p', F_a'$. This enables concrete audio-visual correlation which performs attention-based patch-level merged tokens sound source perception. Denote $Attn(.)$ to be the scaled dot-product conducted on the query, keys, and values, the aggregated feature can be obtained by:\n$F_O = f_v^' + Attn(f_v', F_p', F_p') + Attn(f_a', f_p', \\hat{F_p'}),\\quad (5)$\nwhere $F_p' \\in R^{Topk \\times S \\times D}$. Thus far, we have progressively identified the key temporal segments that are most relevant to the input question, and its potential sound-aware areas."}, {"title": "3.4 Multimodal Fusion and Answer Prediction", "content": "To achieve the AVQA task, we concatenate the updated visual features $F_\\nu', F_v'$, and the audio features $F_a'$ obtained from TPM and SPM, respectively. Then the visual fusion feature $F_{av}$ is obtained by a linear layer. To verify the audio-visual fusion of our proposed effective Temporal-Spatial Perception Model, we employ a simple element-wise multiplication operation to integrate the question feature $F_Q$ and the previously obtained audio-visual fusion embedding $F_{av}$. And it can be formulated as:\n$F_{av} = FC(Concat [F_\\nu', F_v', F_O]),\\quad (6)$\n$e = F_Q \\odot F_{av} \\quad (7)$\nwhere $\\odot$ is element-wise multiplication operation, $\\delta$ and FC represent Tanh activation function and linear layer, respectively. Then a Softmax function is used to output probabilities $p_c \\in R^C$ for candidate answers, where C is the size of the pre-defined candidate answer vocabulary pool. With the predicted probability vector p and the corresponding groundtruth label y, we use a cross-entropy loss: $L_{qa} = \\sum_{c=1}^C y_c log(p_c)$. During testing, we can select the predicted answer by $\\hat{c} = arg \\max_c (p)$."}, {"title": "4 Experiments", "content": "MUSIC-AVQA [19], it contains 9,288 videos covering 22 different musical instruments, with a total duration of over 150 hours and 45,867 question-answering pairs. The questions are designed under multi-modal scenes containing 33 question templates covering nine types, i.e., the audio-visual, separate visual, and separate audio, depending on which modalities are used to discover question-related clues for answer prediction. The diversity question-answering pairs which occupy a large portion of the entire dataset, and there are five audio-visual question types referring to existential, counting, location, comparative, and temporal. The large-scale MUSIC-AVQA dataset is well suited for studying temporal-spatial perception for dynamic and long-term audio-visual scenes.\nAVQA [35], is designed for audio-visual question answering in general real-life scenario videos. It contains 57,015 videos from daily audio-visual activities, along with 57,335 question-answering pairs designed relying on clues from both modalities, where information from a single modality is insufficient or ambiguous.\nFor both datasets, we adopt the official split of the two benchmarks into training, evaluation, and test sets."}, {"title": "4.1 Datasets", "content": "For the visual stream, we divide the video into 1-second segments and sample frames at a rate of 1fps. We utilize the CLIP-ViT-L/14 [27] model pre-trained on ImageNet to extract 512-D feature representations for each visual segment, where [CLS] token denotes visual frame-level features. For the audio signal, it is sampled at 16kHz, which is a standard sampling rate for audio. we use the VGGish network pre-trained on AudioSet to extract 128-D features. For each input question sentence, we extract its feature same as visual frame-level encoder to obtain 512-D feature vector. In all experiments, we use Adam optimizer with an initial learning rate of 1e-4, and will drop by multiplying 0.1 every 10 epochs. The batch size and number of epochs are set to 64 and 30, respectively. We use the thop library in PyTorch to calculate the model's parameters and FLOPs. Our proposed model is trained on NVIDIA GeForce RTX 3090 and implemented in PyTorch."}, {"title": "4.2 Implementation Details", "content": "To verify the effectiveness of the proposed TSPM, we compare it with multiple existing methods: AVSD [28], Pano-AVQA [37], AVST [19], LAVISH [24], COCA [16], PSTP-Net [18], etc. Tab. 1 indicates that the TSPM outperforms all comparison methods. Specially, the TSPM method shows significant improvements in the subtask types of Audio-visual, including Localization, and Temporal. Specifically, compared to the recent PSTP-Net [18], the model achieves remarkable improvements of 0.92% (73.51% and 71.26%) in the above-mentioned complex audio-visual question types. It is worth noting that the model shows a performance boost of 5.14% (82.29% and 77.15%) and 7.54% (84.90% and 77.36%) in the Counting and Localization subtasks of the visual modality, respectively, when compared to PSTP-Net [18]. The significant performance improvements indicate that our TSPM effectively identifies crucial temporal segments and spatial tokens in videos. Moreover, in comparison to LAVISH [24], which fine-tunes large pretrained models, our model demonstrates superior efficiency without the need for fine-tuning. We conducted tests with an equal number of epochs under the same hardware configuration, and it was observed that LAVISH incurred a cost of 14x higher than our model. Additionally, we observed limitations in the performance of Comparative type questions, and we consider this may be attributed to the challenges of separating multiple sounds in complex audio-visual scenes. This motivates us to explore strategies (such as dynamic fusion) in future work that can achieve better performance on both single-modality and multi-modality aspects.\nTo further validate the capabilities of the proposed TSPM, in contrast to splitting by Question ID, we also partitioned the MUSIC-AVQA dataset based on Video ID, apportioning it into training, validation, and test sets in a ratio of 7:1:2. As shown in Tab. 2, it can be seen that our proposed TSPM achieves the best overall performance compared to the latest AVQA methods. Particularly, in the three subtask types of audio, visual, and audio-visual, our TSPM outperforms others significantly, showcasing the excellent generalization capability and performance of the proposed TSPM.\nIn summary, the TSPM offers significant improvements over existing approaches and provides a novel insight into question-oriented audio-visual scene understanding."}, {"title": "4.3 Quantitative Results and Analysis", "content": "In this subsection, we delve into examining the impact of various modules within the TSPM on the performance of the MUSIC-AVQA. To verify the effectiveness of the proposed components, i.e., TPM, SPM, TPC, Tokens merge, etc., we remove them from the primary model and re-evaluate the new model's performance. Tab. 3 shows that after removing a single component, the overall model's performance decreases, and different modules have different performance effects. The specific analysis is as follows:\nTSPM w/o. all. When we remove all designed modules or components within the framework, retaining only the simple fusion operation of input audio, video, and question features, a significant decline (76.79% and 73.35%) in model performance can be clearly observed from Tab. 3. This pronounced deterioration serves as compelling evidence that the multiple components intricately designed within the proposed TSPM play a pivotal role in bolstering the model's overall effectiveness.\nTSPM w/o. TPM. The motivation behind designing the TPM is to enable the model to select temporal segments most relevant to the given question. To validate the necessity of the TPM, we removed the TPM from the TSPM and assessed the performance of the new model. As shown in Tab. 3, when the TPM was removed, the new model's performance decreased to 75.82%, representing a 0.97% decrease compared to when TPM was utilized. Furthermore, noticeable performance declines were observed across the audio, visual, and audio-visual subtask types. These experimental results underscore the importance of TPM, which effectively enables the model to perceive crucial temporal segments, thereby enhancing temporal perception performance.\nTSPM w/o. SPM. The purpose of the SPM is to identify key objects and potential sound-aware areas within the selected visual frame. To demonstrate the significance of the SPM, we conducted an experiment where it is removed. As shown in Tab. 3, compared with TSPM, the result decreased to by 1.11% (from 76.79% to 75.68%), indicating the importance of spatial perception in improving performance.\nTSPM w/o. TPC. The designed TPC primarily generates declarative sentence text based on the given question, aligning it with the semantics of video frames to better identify temporal segments relevant to the question. Removing this module implies that all video frames (T=60) will be selected, potentially leading to temporal redundancy. As observed in Tab. 3, the utilization of TPC effectively enhances model performance, resulting in a 1.12% improvement (from 75.87% to 76.79%), thereby strengthening temporal perception capability.\nTSPM w/o. QPrompt. To validate whether transforming the given question into declarative statement indeed leads to better selection of key temporal segments, thereby effectively improving model performance, we replaced the constructed statements with input questions. As shown in Tab. 3, in this scenario, the model's performance is significantly lower compared to when using declarative statements (76.79% and 75.16%). The experimental results demonstrate the necessity of using declarative statements and indirectly highlight the importance of TPC.\nTSPM w/o. Tokens merge. Removing the Tokens merge operation from TSPM allows us to investigate whether it can preserve the semantic information of visual frame tokens. When this operation is removed, direct cross-modal interactions are conducted between all visual tokens and their corresponding temporal audio features. Tab. 3 shows that when Tokens merge is removed, there is a decrease in model performance, highlighting the importance of the Tokens merge strategy.\nIn general, each module contributes to better performance. When all modules are present, the TSPM achieves the best result on the MUSIC-AVQA dataset. Similarly, we explored the impact of key parameter configurations on model performance. As shown in Tab. 4, when the topk value is large, it may introduce temporal redundancy. When there are too many tokens, semantic merging on the token is not thorough enough; conversely, an excessive merging may result in semantic loss. The model achieves optimal performance when $topk = 10$ and $tokens = 14$, respectively. Note that there are subtle differences between the TSPM and PSTP-Net [18], with the former employing CLIP-ViT-L/14 and the latter utilizing CLIP-ViT-B/32. As shown in Tab 5: 1) The TSPM outperforms PSTP-Net regardless of the feature extractor used; 2) The model achieves better performance when equipped with a superior feature extractor. This underscores the effectiveness of the proposed TSPM."}, {"title": "4.4 Ablation Studies", "content": "Tab. 6 illustrates the computational costs of TSPM compared with ST-AVQA [19], PSTP-Net [18] and LAVISH [24]. It can be observed that TSPM has fewer training parameters, lower FLOPs, and higher accuracy compared to ST-AVQA. Although PSTP-Net boasts lower computational costs, our TSPM achieves superior results at extremely low computational costs. LAVISH achieves a well accuracy, but its parameters are more than three times those of TSPM. This is because LAVISH fine-tunes large pretrained models, whereas TSPM achieves comparable results without fine-tuning. In summary, our proposed TSPM achieves high performance at a relatively low cost, fully demonstrating the effectiveness and efficiency of the model."}, {"title": "4.5 Computational costs", "content": "To verify the generalization capability of the proposed TSPM, we compared it with multiple existing AVQA-based methods, including ACRTransformer [38], HCRN [17], PSTP-Net [18], etc., on the AVQA dataset. As illustrated in Tab. 7, the TSPM exhibits remarkable performance compared to recent methods. Specifically, our approach outperforms PSTP-Net [18] by 0.6% (90.8% and 90.6%), demonstrating notable superiority over earlier methods such as ACRTransformer [38]. Furthermore, while the performance improvement of TSPM on the AVQA dataset seems limited compared to its performance on the MUSIC-AVQA dataset, we attribute this primarily to the AVQA dataset's shorter duration (10s vs. 60s) and simpler audio-visual components.\nDespite these differences, our TSPM maintains its effectiveness even in this scenario. Notably, in Tab. 7, the PSTP-Net [18] achieved a 1.2% improvement over the HCRN [17], while our TSPM exhibited a more substantial 1.8% enhancement, indicating the significant effectiveness of TSPM's performance boost. Additionally, ablation studies further confirm the effectiveness of both TPM and SPM components. Moreover, for the experimental settings on the AVQA dataset, we selected the $Top-k = 8$ temporal segments relevant to the given question, with a merged token count of $tokens = 14$. It's worth noting that HAVF [35] in Tab. 7, serving as the baseline method for the AVQA dataset, includes three fusion modalities and integrates their outputs using an averaging strategy to generate answers. In summary, the proposed TSPM effectively demonstrates both its effectiveness and generalization."}, {"title": "4.6 Experiments on AVQA dataset", "content": "To showcase the temporal and spatial perception capabilities of the proposed TSPM, we provide two examples contrasting with the recent AVQA-related method PSTP-Net in Fig. 4. In Example 01, when presented with the question \"Where is the first sounding instrument?\", the TPM first identifies the temporal indices relevant to the question. Subsequently, the SPM sequentially locates potential sound-aware areas, with the heatmap indicating these regions. In this example, it becomes apparent that initially, only the \"flute\" on the right side is playing, but as time progresses, the violin on the left side also begins playing. The heatmap effectively illustrates the variation in multiple instruments playing within this dynamic and complex audio-visual scene. Consequently, it can be inferred that the correct answer to the question is the instrument on the \"right\" side. Similarly, in Example 02, the progressive temporal-spatial perception process is aptly demonstrated, resulting in the correct answer. These visualizations indicate that the proposed TSPM can effectively perceive the temporal segments relevant to the question and the spatial areas associated with sound, showcasing its efficacy in audio visual question answering task."}, {"title": "4.7 Visualization Results", "content": "In this work, we propose an effective Temporal-Spatial Perception Model framework for addressing complex question-answering tasks in dynamic audio-visual scenarios. It includes a temporal perception module with a declarative sentence text prompt and a spatial perception module incorporating token merging. These modules are employed to locate temporal segments relevant to the question and enhance spatial audio-visual associations, thereby facilitating fine-grained audio-visual scene understanding. Extensive experiments demonstrate that the proposed framework achieves precise temporal-spatial perception on multiple benchmarks, effectively showcasing the reasoning process involved in answering questions. We believe that our work will serve as inspiration for researchers in the field of audio-visual scene understanding."}, {"title": "5 Conclusion", "content": ""}]}