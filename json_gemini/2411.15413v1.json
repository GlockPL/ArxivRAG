{"title": "FG-CXR: A Radiologist-Aligned Gaze Dataset\nfor Enhancing Interpretability in Chest X-Ray\nReport Generation", "authors": ["Trong Thang Pham", "Ngoc-Vuong Ho", "Nhat-Tan Bui", "Thinh Phan", "Patel Brijesh", "Donald Adjeroh", "Gianfranco Doretto", "Anh Nguyen", "Carol C. Wu", "Hien Nguyen", "Ngan Le"], "abstract": "Developing an interpretable system for generating reports in\nchest X-ray (CXR) analysis is becoming increasingly crucial in Computer-\naided Diagnosis (CAD) systems, enabling radiologists to comprehend the\ndecisions made by these systems. Despite the growth of diverse datasets\nand methods focusing on report generation, there remains a notable gap\nin how closely these models's generated reports align with the interpreta-\ntions of real radiologists. In this study, we tackle this challenge by initially\nintroducing Fine-Grained CXR (FG-CXR) dataset, which provides fine-\ngrained paired information between the captions generated by radiolo-\ngists and the corresponding gaze attention heatmaps for each anatomy.\nUnlike existing datasets that include a raw sequence of gaze alongside a\nreport, with significant misalignment between gaze location and report\ncontent, our FG-CXR dataset offers a more grained alignment between\ngaze attention and diagnosis transcript. Furthermore, our analysis reveals\nthat simply applying black-box image captioning methods to generate\nreports cannot adequately explain which information in CXR is utilized\nand how long needs to attend to accurately generate reports. Conse-\nquently, we propose a novel explainable radiologist's attention generator\nnetwork (Gen-XAI) that mimics the diagnosis process of radiologists,\nexplicitly constraining its output to closely align with both radiologist's\ngaze attention and transcript. Finally, we perform extensive experiments\nto illustrate the effectiveness of our method. Our datasets and checkpoint\nis available at https://github.com/UARK-AICV/FG-CXR.", "sections": [{"title": "1 Introduction", "content": "Chest X-rays (CXRs) are commonly used for both screening and diagnostic\npurposes, resulting in a substantial daily workload. Additionally, the current\nshortage of trained radiologists in many healthcare systems highlights the need\nfor automated radiology report generation to help reduce radiologists' work-\nloads [51]. The success of Deep Learning [5, 21, 22, 30-32, 35, 37, 46, 48] has led\npeople to pursue its application in the medical domain [1,29]. However, most\nexisting methods lack explainability, which is a major reason for their limited\nadoption. In the safety-critical medical field, a highly accurate but opaque report\ngeneration system may not be adopted if the reasoning behind the generated re-\nport is not transparent and explainable [11,12,27]. Therefore, creating and using\nan interpretable system should be prefer to black-box system [37].\nIn the examination process, radiologists carefully examine every anatomy\nof CXRs and report their findings. Inspired by this process, we hypothesize\nthat understanding pixel importance and gaze patterns can improve AI model\nexplainability and accuracy in CXR diagnosis. However, the use of radiologist\ngaze-derived heatmaps in generating descriptive reports during CXR diagnosis\nremains underexplored. Recently, Tanida et al. [43] address this challenge by\nintroducing an interpretable system that uses bounding boxes, which lack detail.\nIn contrast, Pham et al. [34] propose a diagnosis system directly supervised by\ngaze attention. However, this system is limited as it can only predict whether an\nanatomical region is abnormal, requiring users to identify the specific findings\nthemselves, which can be impractical.\nTo address the aforementioned weaknesses, we introduce Gen-XAI pipeline,\nshown in Figure 1. Gen-XAI mimics how radiologists perceive images by de-\ncoding radiologist's gaze attention with the Gaze Attention Predictor and then\nexplaining its observations through the Report Generator. The Gaze Attention\nPredictor focuses on learning the regions of interest based on radiologists' gaze\nattentions, ensuring that the system captures the critical areas that a radiolo-\ngist would typically examine. The Report Generator then uses this information\nto produce an accurate radiology report, which is visually grounded with the\nanatomical gaze attention, enhancing the transparency and explainability of the\ndiagnostic process.\nExisting gaze datasets [1,17] provide raw gaze sequences along with reports\nfor each patient. However, radiologists typically observe before diagnosing, lead-\ning to a misalignment between the gaze location and the report at the same\ntimestamp, as illustrated in Figure 2. Therefore, a cleaner dataset is needed\nto evaluate this pipeline effectively. To address this, we curate a new dataset\nthat provides gaze sequences aligned with anatomical attention heatmaps. By\naligning gaze sequences with attention heatmaps, we ensure that the generated\nreports are not only accurate but also provide insights into the reasoning process\nbehind each diagnosis. Our main contributions are summarized as follows:\nWe introduce FG-CXR, a curated dataset that provides anatomical segmen-\ntation, gaze attention heatmaps annotated by radiologists, and radiology re-\nports that are aligned with the gaze attention heatmaps.\nWe propose a novel interpretable baseline Gen-XAI to efficiently generate\nradiology reports with meaningful attention heatmap."}, {"title": "2.1 Interpretable Deep Learning.", "content": "In high-stakes medical settings, understanding the decision-making process is\ncrucial [38]. A direction to enhance interpretability is to design an architecture\nthat can learn concepts [20, 34]. In our paper, we follow the interpretable ap-\nproach [38] by learning radiologists' intentions (gaze attention) across anatomi-\ncal parts. However, unlike previous works [28, 34, 40] focusing on classification,\nwe address the less explored task: the model must explain observations via report\ngeneration based on inferred intentions."}, {"title": "2.2 Interpretable-oriented Datasets.", "content": "Creating datasets with annotated abnormality localization traditionally involves\nmanual curation, but this is labor-intensive and often yields limited coverage,\ntypically 1-2 labels [10,42]. Recent efforts address this by providing datasets\nwith anatomy labels in reports. For instance, the Chest ImaGenome dataset\n[51] is a dataset containing localized annotations (bounding box), with corre-\nsponding reports for the associated CXR images. However, existing datasets\nlack the granularity needed, e.g. gaze, to develop models that mimic real radi-\nologist diagnoses. In contrast, our dataset enhances detail by mapping reports\nto 7 anatomical locations using radiologist attention heatmaps, providing deeper\ninsights into the diagnostic process."}, {"title": "2.3 Radiology Report Generation.", "content": "Early approaches [15,24] in report generation leveraged CNN-RNN architectures\nor transformer inspired by general image captioning. However, medical report\ngeneration differs from image captioning [43] due to varying lengths, complex-\nities, and biases in normal samples. To address these challenges, some models\nalign visual features with disease tags [53], while others incorporate medical\nknowledge graphs [25]. Notably, RGRG [43] tackles interpretability by outlining\nabnormal regions and generating captions about them, but it lacks precision in\nspecifying abnormality areas within bounding boxes. In contrast, our method\nsimulates radiologists' focus on important regions and generates insights based\non them.\nOur FG-CXR dataset closely simulates radiologists' real-life diagnostic pro-\ncess by providing detailed annotations, including anatomical localization, gaze\nattention, and corresponding medical reports. A comparison between our FG-\nCXR with the existing CXR datasets is given in Table 1, while a visualization\nof the comparison of gaze-based annotations is shown in Fig. 2."}, {"title": "3 Dataset: Fine-Grained CXR (FG-CXR)", "content": null}, {"title": "3.1 Anatomy Localization.", "content": "According to radiologists, their focus can be divided into seven key areas of\nCXR: heart, left, right, upper left, upper right, lower left, and lower right lungs.\nTherefore, we create anatomical masks for seven regions. Leveraging CXRS and\ngaze sequences from EGD [17] and REFLACX [1], we apply techniques from [34]\nto generate detailed masks for the heart and lungs, segmented into upper and\nlower regions. Finally, images with extreme brightness are filtered out."}, {"title": "3.2 Anatomical-aware Gaze Attention.", "content": "Given the gaze coordinates $G = {g_1,g_2, ...,g_{|G|}} \\in \\mathbb{N}^{|G|\\times2}$ of a CXR, our\nfiltering process as the follows: For a report $T = {S_1, S_2, ...S_{|T|}}$, we identify all\n$s_i$ that include keywords pertinent to the area of interest, select the latest end\ntime and remove all gaze points after that timestamp. The list of keywords is\ndescribed in Table 2. If transcript lacks keywords indicating anatomy, we use\nthe entire gaze sequence. Finally, we filter out any gaze points that fall outside\nthe segmentation masks corresponding to the anatomical areas of interest. The\nfinal gaze sequence of a $i$th sample is represented in two forms: gaze sequence in\na temporal order $G^i \\in \\mathbb{N}^{|G|\\times2} \\subseteq G$, and gaze attention heatmap $A \\in \\mathbb{R}^{H\\timesW}$,\nwhich is created by creating gaze frequency map and applying Gaussian blurring\nas in [17]."}, {"title": "3.3 Anatomical-aware Report", "content": "For every anatomical region, we associate it with a brief report. For instance,\nwe link \"the heart is normal\" with the heart area. However, a report from RE-\nFLACX [1] or EGD [17] might only include certain anatomical regions. To ad-\ndress this, we use a template to generate reports for any missing anatomy. If\na diagnosis for any region is absent after keyword filtering (Section 3.2), we\ncreate a default sentence based on MIMIC-CXR annotations: \"the {area} is\npossibly normal\u201d for no findings, or \"the patient is possibly suffering\nfrom {findings} in the {area}\u201d for specific findings. For example, if a pa-\ntient's current report lacks information for the left lung area, we refer to MIMIC-\nCXR and find that the label for this patient is \"no finding\". We then generate\nthe sentence \"the left lung is possibly normal\" for this patient's left lung."}, {"title": "3.4 Dataset Statistics", "content": "Our dataset contains 2,951 CXRs in total, with 20,657 pairs of {attention heatmap,\nreport}. Figure 3 provides deeper insights into our dataset.\nReports are mostly concise. In Figure 3a, we observe that the majority of\nreports are concise, particularly those describing the heart, with most reports\ncomprising fewer than 10 words. Reports on other anatomical regions tend to be\nlonger, though a significant number still fall within the 5 to 10 words range. This\nindicates a prevalent practice among radiologists of using succinct sentences.\nRadiologist's attention versus segmentation mask. Figure 3b reveals that\nmost gaze heatmaps typically cover a smaller area than the full anatomical seg-\nmentation mask. Notably, many heatmaps in the lower left and right lungs en-\ncompass a larger area (resulting in a ratio greater than 1). This can be attributed\nto the presence of dense gaze sequences that extensively cover a particular re-\ngion, and this is further amplified with the Gaussian filtering process. Such a\nphenomenon is particularly evident in the lower right and lower left regions,\nlikely due to radiologists' meticulous examination of the diaphragm, which ex-\ntends beyond the lower masks. Furthermore, the base of the left lung mask is\ntypically smaller than that on the right side, attributed to occlusion by the heart.\nRadiologist's attention versus bounding box. In Figure 3c, most heatmaps\nare confined to a portion of their bounding box, which is computed by taking\nthe top left and bottom right corners of the non-zero heatmap values. A notable\nobservation is that many gaze attention heatmaps utilize less than 20% of the\nbounding box area, especially in the left and right lungs.\nRadiologist's attention versus the whole image. In Figure 3d, most heatmaps\nuse little information from the whole image. This is true even for the left and right\nlungs, where one might expect a higher coverage area; however, most heatmaps\noccupy only about 10% of the image's total area.\nFor benchmarking in Section 5, we randomly split our dataset into 70% for\ntraining, 10% for validation, and 20% for testing. The number of samples is\nshown in Table 3."}, {"title": "3.5 How will our FG-CXR benefit the community?", "content": "We anticipate that the release of our FG-CXR will drive advancements in these\npromising areas:\nGaze-Interpretable Report Generation: While report generation is a growing\nresearch topic, enhancing and evaluating report generation with explainabil-\nity using radiologist gaze data remains relatively unexplored.\nGeneral Medical Tasks: The FG-CXR dataset, enriched with segmentation\nmasks for critical anatomical areas, serves as a valuable resource for devel-\noping and benchmarking Anatomical Segmentation algorithms [23,47]. The\nreports of FG-CXR dataset, validated by experts, is also richer and more\ninformative than the original REFLACX [1] and EGD [17] datasets, making\nit a potential benchmark for Radiology Report Generation [25,55]."}, {"title": "4 Methodology", "content": "In this section, we introduce a novel framework for Gaze-interpretable Report\nGeneration. Given a CXR image $I$, our goal is to produce gaze-based heatmaps $A$\nand generate a report $R$ of the attended region $A$. To ensure interpretability, the\ngaze attention $A$ has to be closely aligned with expert observation and the report\n$R$ has to be consistent with what gaze attention $A$. Our Gen-XAI architecture,\ndetailed in Figure 4, comprises three key modules: (i) Gaze Attention Predictor\n(GAP) to predict seven gaze-based heatmaps $A$; (ii) Spatial-aware Attended\nEncoder (SAP) to generate attended features $V$; and (iii) Report Decoder to\ngenerate diagnosis reports for seven anatomies."}, {"title": "4.1 Gaze Attention Predictor (GAP)", "content": "Given a CXR image $I$ and Anatomical Intention Token $T_{int.}$, this module pre-\ndicts the radiologist-like attention heatmap $A \\in [0,1]^{7\\times H/16\\times W/16\\times1}$. Inspired\nby [34], we adopt the idea of training a deep adapter on a pretrained CLIP [36]\nto predict heatmaps. Initially, we extract four intermediate features from the\nmiddle layers (i.e. {0,3,6,9} according to [34]) of the CLIP's visual encoder\nfrom the $I$. Subsequently, $I$ is split into $H/16 * W/16$ patches with size 16 \u00d7 16\nand projected into an embedding space $I_e$ to prepare for the fusion stage via\na Linear Projection layer. In the fusion stage, we fuse $I_e$ with the extracted\nCLIP's visual features $V$ and $T_{int.}$ Finally, a Predictor module, consisting of\nan MLP followed by a sigmoid activation, predicts seven gaze heatmaps for the\nseven parts of the lung. To create $T_{int.}$, we use CLIP's text encoder to extract\nseven textual features from: \u2018heart\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cupper left\u201d, \u201cupper\nright", "lower left": "and \"lower right"}, {"title": "4.2 Spatial-aware Attended Encoder (SAP)", "content": "This module produces an attended feature $V$ that contains information from its\npatch and neighbors, crucial for focusing on relevant areas while retaining essen-\ntial spatial context. Given CXR's grayscale nature, distinguishing areas like the\nlung from the background requires understanding their spatial relation to adja-\ncent patches. This spatial information only exists in the encoded feature [21,50].\nUnlike previous works [18,34] that apply attention to pixel level that may re-\nmove vital details, our approach applies attention to the latent visual features\n$V'$. The effectiveness is empirically proven and included in Section 5.3. Specifi-\ncally, we first extract the patches' visual feature $V' \\in \\mathbb{R}^{H/16\\times W/16\\times D}$ by using\na Visual Encoder (i.e. CvT [50]). Then, we create the spatial-aware attended\nfeature by performing element-wise multiplication between $A$ and $V'$ to create\nthe reweighted feature $V$. To further guide the model, we also concatenate the\ntoken of the current area of interest into the feature, for example concatenat-\ning the intention token of looking at the heart to the feature that is masked\nby the heart heatmap. Mathematically, we compute $V\\in \\mathbb{R}^{7\\times (H/16*W/16+1)\\times D}$\nwith $V(i) = [V' \\odot A(i), T_{int. (i)}]$, $\\forall i \\in [0, 6]$, where $i$ indicates $i^{th}$ region, $[.]$ is\nconcatenation and $\\odot$ is the Hadamard product."}, {"title": "4.3 Report Decoder", "content": "We utilize GPT2 [36], an auto-regressive network for text generation, as our tex-\ntual report decoder architecture. The token embedding is the embedding of pre-\nvious tokens, for example, \u201c[BOS],the,patient,is, possibly, suffer,from\u201d\nto predict the next word \"lung\", where [BOS] is the beginning of sentence token.\nFor every area $i$, we use $V(i)$ as key (K) and value(V), and the output feature\nfrom self attention of token embedding as query (Q) of the cross-attention module.\nAfter predicting all sentences, we concatenate them to create the final report."}, {"title": "4.4 Learning Objective", "content": "We train our model with the training loss $L = (1 + \\lambda_c)L_c + (1 + \\lambda_h)L_h$, where\n$\\lambda_c$, $L_c$, $\\lambda_h$, $L_h$ are the report penalty, cross-entropy loss for the generated report,\nheatmap penalty, and L2 loss for predicted heatmap, respectively. To enhance\nthe model's focus on predicting correct anatomy, we introduce two dynamic\ncoefficients as penalties: gaze attention prediction $(\\lambda_h)$ and report generation\n$(\\lambda_c)$. During the heatmap prediction, we use Intersection over Union (IoU) with a\nthreshold of 0.5 to identify instances where the model inaccurately focuses. Each\nincorrect prediction increases $\\lambda_h$ by 1. $A_{gt}$ is the ground truth gaze attention\nmap.\n$\\lambda_h = \\sum 1_{0.5} (IOU (A(i), A_{gt}(i))), where 1_{0.5} =\\begin{cases}1 & \\text{if } x > 0.5\\\\0 & \\text{otherwise}\\end{cases}$         (1)\nAt the report generation, we want the model the model explicitly predict\ndirections while minimizing incorrect directional words. Thus, if the model fails\nto predict anatomical keywords or mentions the wrong direction, $\\lambda_c$ increases\nby 1. For every CXR, both $\\lambda_h$ and $\\lambda_c$ are initialized to 0, indicating that they\nare not accumulated across all samples in an epoch. The ablation study on the\neffect of penalty terms is included in Section 5.3."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Implementation details", "content": "Architecture details. GAP comprises an FCN layer for an input patch size\nof 16 \u00d7 16 as the Linear Projection, 4 fusion layers, each with the Add Fusion\nblock [34], and the Self-attention block with a hidden dimension of 240 and\n6 attention heads. A BiomedCLIP [54] is used as CLIP and an MLP with 3\nhidden layers of 256 neurons each as the Predictor. The Report Decoder is a\nGPT2 [36] initialized with DistillGPT2 [39] that has 12 heads, 6 layers, and a\nhidden dimension of 768. The SAP's Visual Encoder is a CVT [50] initialized\nwith ImageNet [9] and $|V| \\in \\mathbb{R}^{768}$. We train Gen-XAI with a learning rate of\n5e - 5, batch size of 32, 6,000 iterations, and AdamW optimizer [26]."}, {"title": "5.2 Experimental results", "content": "Quantitative results. Tables 4 to 6 demonstrates the effectiveness of our in-\nterpretable approach, which outperforms other methods across all criteria. For\nexample, Gen-XAI has a high advantage in attention prediction and outshines\nthe runner-up [6] by +20.47 in fwIoU. The improvement in attention similarity\nis understandable as our method is explicitly constrained by the heatmap loss\nwhile other methods are not designed for Gaze-Interpretable Report Genera-\ntion. However, Gen-XAI also outperforms other black box methods, designed\nspecifically for radiology report generation. For example, in NLG, Gen-XAI sig-\nnificantly surpasses other models by a large margin, i.e. +0.671 on CIDEr metric\ncompared to the runner-up [3], and +0.182 on Div@2 metric compared to the\nrunner-up [6]. On example-based CE metrics, our model also achieves a higher\nscore of 0.497, +0.057 on Flex respectively compared to the runner-up [6]. One\nof possible reason is that previous works are not supervised by radiologist's gaze\nattention, and thus they fail to learn and use incorrect visual information. This\nwill be further confirmed in Figure 5, where looking at the wrong location causes\nthe model to fail in producing reliable diagnosis, and in Section 5.3, where our\nmodel also encounters the same issue when trained with a traditional attention\nmechanism.\nQualitative results. Figure 5 compares our Gen-XAI with leading methods,\nshowcasing superior performance in generating precise attention heatmaps and\ndiagnosis reports. CvT2DistilGPT2 produces unreliable and mostly incorrect\nreports due to inaccurate focus, despite occasional recognition of pleural effusion.\nOn the other hand, the M\u00b2 Transformer often misidentifies lungs as normal,\nalthough accurately diagnosing the heart. This highlights the effectiveness of\nour approach and the crucial role of precise anatomical focus in addressing the\nGaze-Interpretable Report Generation challenge."}, {"title": "5.3 Ablation Study", "content": "Applying gaze attention on pixel vs. features. In Section 4, we apply the\npredicted gaze attention on the features based on the intuition that the encoder\nmay extract important context information to represent a patch feature besides\nthe patch pixels. For example, the shape or spatial information can be in the\nfeature. This ablation alters only the attention choice, keeping penalty terms\nand other components as initially proposed. Therefore, we design an ablation\nstudy: Instead of applying the gaze attention on the feature, we apply it to the\nimage input, then feed the masked image into the encoder again. Every other\nsetting is kept the same. The results are shown in Table 7. Indeed, the findings\nindicate that incorporating gaze attention directly into the input detracts from\nmodel performance. As mentioned in Section 4, this approach can obscure spatial\ndetails, leading to confusion. For instance, a heatmap centered on the left lung\nmay not clarify enough whether it targets the left or right side due to uniform\ncoloration. This issue is amplified when training the model without an intention\ntoken (w/o. IT), as shown in our table's first row. On the other hand, attending\nto the latent feature improves the performance, and applying the intention token\ncan slightly boost the performance.\nAnatomical gaze attention vs. anatomical segmentation vs. traditional\nattention. Interpretable model is often mistakenly thought to be harmful to\nthe performance [37]. To demonstrate that this is not the case for our proposed\nmodel. We design an ablation study with two more settings:\nTraditional attention. We remove the Gaze Attention Predictor (GAP) mod-\nule and instead use a simple self-attention module to flexibly weigh the impor-\ntance of every patch feature. In other words, we let the model automatically\ndecide the importance of every patch.\nAnatomical segmentation. Instead of supervising our model on gaze atten-\ntion ground truth, we supervise the GAP with our anatomical segmentation\nmasks. Then we mask out patches that are not in the predicted mask. In\nother words, we let the GAP module be an anatomical mask predictor, and a\npatch is important, i.e. its weight is 1.0 if it is inside the anatomy of interest.\nTable 9 shows that our proposed anatomical gaze attention supervision is effec-\ntive and outperforms other settings. For the traditional attention setting, the\nblack-box and unconditional training pipeline causes the model to not know\nwhere to look, i.e. low scores on Attention criteria, and hence, it fails to give sat-\nisfaction diagnosis, i.e. low NLG and CE scores. One of the possible reasons for\nthis is because self self-attention mechanism is well-known for its data-hungry\nnature [19]. On the other hand, training the GAP module with segmentation\nmasks slightly decreases the performance. One possible reason is that we let the\nmodel use too much information, which can confuse the model in some cases.\nThe gain from correctly weighing important patches further confirms our hy-\npothesis in Section 1. Moreover, this suggests that our framework can also be\nused for segmentation prediction.\nEffectiveness of penalty terms. The intuition behind the penalty terms is\nsimple, yet effective. We design an ablation study: we train the model without\npenalty terms to demonstrate the effect of every penalty. As a result, we find that\nour penalties based on the idea of looking at the correct anatomy are beneficial\nto the model, as shown in Table 8."}, {"title": "6 Conclusion", "content": "In this work, we have introduced FG-CXR, a curated dataset for gaze inter-\npretable radiology report generation. Our dataset contains CXR images with\naligned gaze sequence, gaze attention heatmap, and the reports associated with\nseven anatomical parts of the lung. We then presented a novel method for gen-\nerating descriptive reports of chest X-ray images, using heatmaps based on ra-\ndiologist annotations to focus the model's attention and reduce the likelihood of\nmisinterpreting irrelevant regions. Our main contribution is the successful appli-\ncation of a radiologist-informed attention mechanism that guides a generative\nmodel, thereby enhancing the accuracy, reliability, and interpretability of au-\ntomated CXR report generation. We hope that the release of our dataset will\nadvance more research on interpretable report generation."}]}