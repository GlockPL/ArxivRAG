{"title": "K-ON: Stacking Knowledge On the Head Layer of Large Language Model", "authors": ["Lingbing Guo", "Yichi Zhang", "Zhongpu Bo", "Zhuo Chen", "Mengshu Sun", "Zhiqiang Zhang", "Wen Zhang", "Huajun Chen"], "abstract": "Recent advancements in large language models (LLMs) have significantly improved various natural language processing (NLP) tasks. Typically, LLMs are trained to predict the next token, aligning well with many NLP tasks. However, in knowledge graph (KG) scenarios, entities are the fundamental units and identifying an entity requires at least several tokens. This leads to a granularity mismatch between KGs and natural languages. To address this issue, we propose K-ON, which integrates KG knowledge into the LLM by employing multiple head layers for next k-step prediction. K-ON can not only generate entity-level results in one step, but also enables contrastive loss against entities, which is the most powerful tool in KG representation learning. Experimental results show that K-ON outperforms state-of-the-art methods that incorporate text and even the other modalities.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are trained on vast amounts of corpora and store world knowledge within billions of neurons (Achiam et al. 2023; Touvron et al. 2023). Despite of the prosperity in LLM-based applications, unleashing its power for knowledge graph (KG) tasks remains challenging.\nTokens are the basic elements for language models, but it needs to take at least several tokens to describe and identify different entities in a KG. Creating new token identifiers for each entity and learning them during fine-tuning is an alternative choice, however, which is extremely time-consuming and may negatively affect the native performance of LLMs.\nIn this paper, we explore how to effectively and efficiently use multiple tokens to describe entities in a given KG. Evidently, directly optimizing the sequence prediction objective will results in the out-of-KG problem as LLM lacks awareness of the KG's entities, while enumerating all entities in the input instruction is unrealistic. Take Figure 1a as an example, the task is to predict the target entity Matt Damon given the incomplete triplet (The Bourne Identity (2002 film), starring, ?). The vanilla learning schema is inefficient because generating a single entity requires multiple steps and cannot be parallelized across entities."}, {"title": "", "content": "Most existing methods compromise on this dilemma and apply LLMs only to simple KG tasks (Yang et al. 2023; Guan et al. 2024; Pan et al. 2023, 2024), such as verifying the correctness of a triplet (Zhang et al. 2023) or predicting the target from a limited number of candidates (Yao et al. 2023). In contrast, we propose K-ON to employ K head layers for predicting entities at one shot and stack knowledge on these heads by entity-level contrastive learning.\nAs shown in Figure 1b, K-ON adapts K different head layers from the original LLM head, where the k-th head is responsible for predicting the k-th token for all entities. For example, an entity Matt Damon is tokenized into K input token IDs $t_{0:K-1}$, with $t_0$ representing Matt and $t_{k-1}$ being the last token Damon or padding token. We then extract the the probability of the first token from the first head layer, and so forth. For the other entities that may serve as negative examples in contrastive learning, we can reuse the K-step probability estimations to extract their scores.\nThe risk underlying the prediction of the next K tokens is over-optimization. The model may over-optimize for predicting just the next K tokens, dropping the fact that these tokens form the target entity. For instance, while minimizing cross-entropy loss for the first token, with Matt as the positive label, many negative entries (tokens) are not constituent elements of an entity. Moreover, increasing the probability of Matt does not always equate to maximizing the probability of Matt Damon. To tackle this issue, K-ON employs an entity-level contrastive loss, which treats the K-step predictions as an integrity and estimates the joint probabilities.\nAnother risk in next-K-token prediction is distribution corruption. In the original schema, the probability of the second token \"Damon\" is conditioned on the first token \"Matt\". However, this conditioning is absent in the next-K-tokens schema. Such discrepancy may degrade the performance as well as inference ability of the original LLM.\nTo address this issue, we propose head trajectory tuning (HTT) to align the distribution trajectories between the original LLM's prediction and the next-K-token predictions. We first leverage a conditional attention layer to process the hidden states from K head layers to reconstruct the sequential dependencies between different steps. Then, we compute a standard sequence prediction loss with the original LLM head layer as target. Finally, we can align the output probability sequence of our next-k-token predictions with"}, {"title": "Related Works", "content": "Knowledge Graph Completion Knowledge Graph (KG) completion is one of the most important tasks in the KG area. Conventional methods leverage triplet information as training data but often ignore the rich contextual information embedded within the text (Bordes et al. 2013; Dettmers et al. 2018; Guo, Sun, and Hu 2019; Vashishth et al. 2020; Guo et al. 2020; Chen et al. 2021; Guo, Zhang, and Chen 2022; Guo et al. 2022). In other words, they assume that the entities carry no self-feature (including their names, e.g.,\nMatt Damon), and the relational connections are the only informative source. Recently, methods leveraging text and image information have proposed and achieved state-of-the-art performance on many benchmarks (Xie et al. 2017; Wang et al. 2019; Yao, Mao, and Luo 2019; Youn and Tagkopoulos 2022; Lin et al. 2023; Zhang, Chen, and Zhang 2023; Guo et al. 2024b). They can be divided into two groups: one focuses on the integration of more modalities (Lu et al. 2022; Lee et al. 2023; Zhang et al. 2024c); and the other concentrates on the fine-tuning of language models to better encoding text information (Yao, Mao, and Luo 2019; Youn and Tagkopoulos 2022; Lin et al. 2023).\nLLM-based Knowledge Graph Completion Entities possess rich features often represented in text forms such as descriptions, tables, and attributes. Many (mostly multi-modal) methods propose leveraging language models to encode text information and use the resulting representations for prediction (Yao, Mao, and Luo 2019; Wang et al. 2021; Lin et al. 2023; Youn and Tagkopoulos 2022; Chen et al. 2022; Zhang et al. 2024b,a; Guo et al. 2024a). In the recent advances, most LLM-based methods can be classified into this category. For example, LLMKGC (Zhu et al. 2023) directly feeds the textual triplets to ChatGPT (Achiam et al. 2023) for KG completion, although the results are not very promising. KGLlama (Yao et al. 2023) and KoPA (Zhang et al. 2023) fine-tune LLMs on triplet verification, i.e., estimating the correctness of a given triplet. These LLM-based methods employ in-context learning or LoRA-based fine-tuning (Dong et al. 2022; Wies, Levine, and Shashua 2024).\nTo our knowledge, no prior work has explored integrating KGs into the head layer of LLMs. The current LLM-based methods leverage additional text information but are directly compared against conventional methods (Yao et al. 2023; Zhang et al. 2023; Wei et al. 2024), which may lead to unfair evaluations. Therefore, in this paper, we consider multi-modal datasets as benchmarks (Liu et al. 2019; Xu et al. 2022; Chen et al. 2024).\nMulti-Head LLMs There are several works employing multiple head layers in LLMs. Medusa (Cai et al. 2024) proposes a tree attention mechanism for multi-step training and inference. The K different head layers are initialized with the original weights and then fine-tuned independently. MultiToken (Gloeckle et al. 2024) discovers that training LLMs and multiple head layers from scratch can outperform the single-head version, with this advantage being more significant in larger models. Unlike these methods, the K head layers in our approach are not only used for generating multiple future tokens in one step but also confine the output space to KGs and enable entity-level contrastive learning. Our work explores a new direction for integrating LLMs with KGs."}, {"title": "Methodology", "content": "In this section, we present the details of K-ON. We begin with a preliminary overview of knowledge graphs and large language models, and then illustrate the architecture and implementation of K-ON. Finally, we introduce head trajectory tuning as a self-supervised optimization method for K-ON."}, {"title": "Preliminaries", "content": "Knowledge Graphs We describe a knowledge graph by $G = {E,R,T}$, where $E, R, T$ denote the entity, relation, and triplet sets, respectively. As one of the most important tasks in KG area, KG completion aims to predict the missing entity given an incomplete triplet (Bordes et al. 2013), i.e., predicting a tail entity $e_2$ given $(e_1,r_1,?)$ or predicting the head entity $e_1$ given $(?, r_1, e_2)$.\nLarge Language Models Generally, a large language model comprises the following components: a tokenizer, which splits the input text into a sequence of N tokens $t_{0:N-1} = {t_n|t_n \u2208 V}_{n=0}^{N-1}$ with $|V| > N$ being the vocabulary; a Transformer-based model M, which processes the token sequence and generates a corresponding sequence of hidden states for prediction;\n$h_{0:N-1} = M(t_{0:N\u22121});$ (1)\nand a head layer H, which maps each hidden state to a probability distribution $p_n \u2208 R^{|V|}$:\n$p_n = H(h_m).$ (2)\nIn this paper, we focus primarily on the head layer of the LLM, and demonstrate that integrating knowledge graphs into the LLM at only this stage is sufficient to achieve state-of-the-art performance in KG completion."}, {"title": "K-ON", "content": "Entities typically require multiple tokens to be identifiable by name, which introduces a discrepancy between the entity distribution and the token probability distribution during prediction. Specifically, in a standard fine-tuning schema, the prediction objective is optimized at the token level rather than at the entity level. Consequently, the LLM is unaware of which entities are present in the given KG, except those provided in the context.\nGiven the vast number of possible entities, it is impractical to include all candidate entities in the input text. An alternative approach worth exploring is manipulating the output probability distributions. If we can obtain the probability sequence of the constituent tokens for each entity, we can construct an entity-level entropy-based loss for the LLM. For example, suppose that we have constructed the input query containing the information of $e_1, r_1, ?$, we wish that the LLM precisely generates the output, $e_2$, which comprises maximally K tokens. Then, we can extract the probability of each token and combine them as the joint probability for $e_2$. However, achieving this is challenging, primarily due to computational costs. Iteratively feeding every negative example into the LLM for back-propagation is computationally intensive. As shown in Figure 1a, it is not parallelizable across entities.\nTo address this problem, we propose K-ON. Figure 2 illustrates the overall architecture of K-ON. In addition to the original LLM's input and Transformer layers, K-ON introduces five new modules to support K-step token prediction.\nHead MLPs We first employ multiple head MLPs to process the output hidden states of the LLM into inputs for different steps. Specifically, each MLP consists of three components: a fully-connected layer $W\u2208 R^{d\u00d7d}$, an activation function $\u03c3$, and a normalization layer $L_h$:\n$h_{0:K-1} = {L_h(\u03c3(Wh_m))},$ (3)\nwhere $h_{0:K-1}$ are the K output hidden states of the head MLPs. It is worth noting that the LLM uses a decoder-only architecture, and the input hidden state $h_m$ for K-ON is the last output hidden state of the query text. The task is to follow the query text and generate K subsequent tokens as pre-"}, {"title": "", "content": "dictions of entities. Similar to Llama 2 (Touvron et al. 2023), we use SiLU (Elfwing, Uchibe, and Doya 2018) and LlamaRMSNorm (Zhang and Sennrich 2019) as the activation function $\u03c3$ and normalization layer $L_h$, respectively. The bias vector is not used.\nConditional Attention While the head MLPs in K-ON are independent of each other, the subsequent outputs in the original LLM are conditioned on the previous inputs. Therefore, we leverage a small Transformer $M_s$ to mimic this process by incorporating a causal mask $M \u2208 R^{K\u00d7 K}$.\n$M_{ij} = \\begin{cases} 1 & i \u2265 j, \\\\ 0 & i < j, \\end{cases}$ (4)\nwhere $M_{ij}$ indicates the value at the i-th row and j-th column of M. When processing the k-th step, only the outputs of the previous k 1 steps from the head MLPs can be observed for the Transformer.\nAnother noteworthy aspect of the conditional attention is the residual connection layer. Specifically, we add the attention output to the initial output of the LLM to produce the final output:\n$h_k = M_s(h_{0:k}, M) + h_m,$ (5)\nwhere $M_s, M$ are aforementioned small Transformer and causual mask, respectively. With the residual connection, we can initialize the head MLPs with zeros, allowing them to gradually learn the adaptation for K-step prediction from the initial LLM hidden $h_m$.\nLORA Score Layer We use different score layers to estimate the probability distribution for each step. Unlike (Gloeckle et al. 2024) which trains each score layer from scratch, we propose to use a low-rank adaptation (LoRA) (Hu et al. 2021) layer for each step. This can be expressed as:\n$W_h^S = W^S + A_kB_k$ (6)\n$p_k = W_h^Sh_k$ (7)\nwhere $W^S \u2208 R^{|V|\u00d7d}$ is the original score layer of the LLM. $|V|$ denotes the vocabulary size (number of tokens) of the LLM. $A_k \u2208 R^{d\u00d7r}$ and $B_k \u2208 R^{r\u00d7d}$ are the down-scaling and upper-scaling matrices in LoRA. The hyper-parameter $r < d$ is the reduced dimensionality. Before fine-tuning, $A_k$ is initialized randomly while $B_k$ is initialized to zero. This ensures that the output of the adaptation layers is identical to the original head layer.\nK-step Gathering After obtaining the K-step predictions $p_0, p_1, ..., p_{K-1}$, we need to efficiently extract the elements relevant to each entity. To achieve this, we first convert each entity to an identifiable sequence of tokens:\n$t_{0:K-1} = P(\u03c4(l_e), K)$ (8)\nwhere $l_e$ is the textual label of the entity e and \u03c4 is the tokenizer. We perform padding and truncation $P$ to ensure that all entity token sequences have the same length K."}, {"title": "", "content": "Next, we stack the K-step predictions $p_0, p_1, ..., p_{K-1}$ into a probability matrix P:\n$P = \\begin{pmatrix} p_0 \\\\ p_1 \\\\ ... \\\\ p_{K-1} \\end{pmatrix}$, (9)\nsuch that the each token probability $p_t^e$ can be extracted from P with t as indices:\n$p^e = p_{0,t_0}, p_{1,t_1}, ..., p_{K-1,t_{K-1}}$ (10)\nHere, $p^e$ is the token probability sequence of entity e, with a strict length of K.\nContrastive Loss To incorporate the entity-level contrastive loss, we first estimate the scalar probability $p^e$ from its token probability sequence $p^e$. Our experimental results indicates that a weighted sum achieves the best performance:\n$p^e = \u03a3 \u03b1_kp_{k} \\\\ p_k \u2208 p^e$ (11)\nwhere $\u03b1_k$ is a learnable weight for k-th step shared across different entities.\nBy using the above equation to gather the scalar estimates for both positive and negative entities, we can construct an effective contrastive loss. Specifically, we randomly sample entities from the entity set E to form negative examples N ={ej|ej \u2260 e, ej \u2208 E}:\n$L_{NCE}(e) = -log p^e + \\frac{1}{|N|}\u03a3log p^{e_j},$ (12)\nej \u2208 N\nwhere $p^e$ and $p^{e_j}$ denote the joint probabilities for the positive entity e and negative entity $e_j$, respectively.\nHead Trajectory Tuning\nEntity-level contrastive learning is optimized against entities, which may inadvertently disrupt the token-level predictions of the original LLM, affecting performance on both common and training corpora. To mitigate this issue, we propose head trajectory tuning (HTT) to align the sequence estimations between single-step and K-step predictions."}, {"title": "", "content": "Supervised Fine-Tuning HTT consists of two objectives. The first is tuning the LLM on the training corpus, also known as supervised fine-tuning (SFT). For this, we apply LoRA to the LLM model and optimize the single-step estimations against the ground truth:\n$L_{sft}(e) = \\frac{1}{K}\u03a3(-log p_k^{e_j} + \\sum_{e_j\u2208 V}logp^{e_j})$, (13)\nk=0\nwhere $L_{sft}(e)$ is the supervised fine-tuning loss, with $p_k$ denoting the target probability at the k-th step and V representing the token vocabulary.\nToken Distribution Tuning Then, we propose token distribution tuning to align the probability estimations of KON with those of the original LLM. Specifically, we minimize the KL-divergence (Kullback and Leibler 1951) between each pair of estimations along the output trajectories.\n$L_{ldt}(e) = \\sum_{k=0}^{K-1} D_{KL}(p^{k-on}, p^{k}_{ellm}),$ (14)\nwhere $p^{k-on}, p^{k}_{ellm}$ denote the k-th token probability distributions of K-ON and the original LLM head, respectively.\nImplementation\nWe present Algorithm 1 to illustrate the implementation of K-ON step by step. We first construct the input text for each triplet in the training set, and then use the tokenizer to convert the query into token IDs. We feed the input into the LLM to obtain the hidden states, which will be estimated by K-ON and the original head layer, respectively. Lastly, we compute the losses introduced before, and jointly minimize them until convergence."}, {"title": "Experiment", "content": "In this section, we conduct experiments to verify the effectiveness of the proposed K-ON.\nSetting\nWe employ Llama-2-chat-7B (Touvron et al. 2023) as the base LLM model and train K-ON with 8 A100 GPUs. The learning rate is set to 1e-4 in all experiments, and we use AdamW (Kingma and Ba 2015) as the optimizer. The batch-size per device is set to 12 and the gradient accumulation is set to 8 to obtaining a larger batch-size. We follow (Bordes et al. 2013; Lu et al. 2022; Zhang et al. 2024c) to report the MRR and Hits@K results with filtered ranks.\nWe consider various KG completion methods as baselines: the conventional structure-only methods, such as TransE (Bordes et al. 2013) and RotatE (Sun et al. 2019); the methods leveraging image information, such as IKRL (Xie"}, {"title": "", "content": "et al. 2017) and TransAE (Wang et al. 2019); the methods leveraging text information, such as KG-Bert (Yao, Mao, and Luo 2019) and FLT-LM (Lin et al. 2023); the methods leveraging both text and image information, such as MMKRL (Lu et al. 2022) and MANS (Zhang, Chen, and Zhang 2023); and the LLM-based methods KG-Llama-7b (Yao et al. 2023) and GPT 3.5 (Zhu et al. 2023);\nDatasets\nWe consider DB15K and MKGW as benchmark, which are widely used in many recent works (Xie et al. 2017; Xu et al. 2022; Lee et al. 2023; Zhang, Chen, and Zhang 2023; Zhang et al. 2024c). The two datasets include not only the structural triplet data, but also the rich information of text and others. Thereby, we believe conducting experiments on them can gain a more comprehensive understanding on different methods and ensure a fairer comparison. The statistics of these two datasets are shown in Table 1.\nResults\nThe main experimental results are shown in Table 2. We can find that the methods considering additional information generally perform better than the structure-only methods, which verifies the effectiveness of leveraging external informative sources. Among these methods, the proposed K-ON achieves the best results, significantly surpassing all baseline methods.\nHowever, we also observe that the performance gap between conventional methods and multi-modal approaches narrows on the MKGW dataset. For instance, FLT-LM (Lin et al. 2023) and MANS (Zhang, Chen, and Zhang 2023) perform worse than RotatE on the MKGW dataset. This suggests that while additional information can be beneficial, it may also introduce noise, which does not always aid in entity prediction. Thanks to advancements in language models, K-ON leverages LLM to process text information and is optimized against an entity-level contrastive loss. Consequently, it still significantly outperforms all baselines across all metrics on the MKGW dataset.\nThe existing LLM-based methods, as mentioned in previous sections, are optimized against tokens rather than entities. Thus, their performance on the standard KG completion task is generally unsatisfactory, falling significantly short of many non-LLM methods. This phenomenon has also been examined by (Zhu et al. 2023). In contrast, our K-ON introduces entity-level contrastive loss, enabling the LLM to more effectively explore the KG structure.\nAblation Studies\nWe conduct ablation studies to verify the effectiveness of each module in K-ON. We remove or replace the core modules in K-ON to conduct experiments. Specifically, w/o Ltdt, w/o Lsft, w/o Lnce refer to the methods that exclude the token distribution tuning loss, the token supervised finetuning loss, and the entity-level negative contrastive loss, respectively. w/o Conditional Attention is the method without the attention module. Shared Head MLP and Shared Score"}, {"title": "", "content": "Layer refer to the methods where we replaced the K different LoRA layers for each step with a single shared LORA layer for all steps.\nThe results are presented in Table 3. w/o Lnce performs worst among all alternative methods, but it still shows some effectiveness, likely due to the presence of the token-level supervised fine-tuning loss. The substantial gap between KON and w/o Lnce highlights the importance of entity-level contrastive learning. Ltdt and Lsft are also crucial for KON, and removing either one results in a significant performance drop in MRR, Hits@1, and Hits@10. Interestingly, we observe that the Hits@10 results are relatively insensitive to our ablation settings, showing only minimal differences across multiple methods.\nRemoving the conditional attention module also leads to a significant performance decline, particularly in Hits@1. We believe this module is essential for accurately identifying"}, {"title": "Analysis on the K-ON Heads", "content": "The number of K-ON heads (denoted as K) is a crucial hyper-parameter, determining the maximum token length available for representing each entity. While a larger K can prevent the truncation of entity names and potentially enhance model performance, it also increases computational demands. To examine this trade-off, we conducted experiments with varying values of K.\nThe results are illustrated in Figure 3, where we present four subgraphs depicting MRR, the number of trainable parameters, per-step time, and overall training time. Notably, performance appears to saturate when K > 8, likely because most entity names consist of fewer than 8 tokens. Nonetheless, the computational cost increases linearly with K. As observed, both step time and training time show slight increases with larger K, while the number of trainable parameters significantly rises. Therefore, we choose K = 8 for the main experiments, as it strikes an optimal balance between performance and computational efficiency."}, {"title": "Analysis on the Entity-level Contrastive Loss", "content": "One of the key features of K-ON is the entity-level contrastive loss, defined by one positive example and |N| negative examples. It is interesting to analyze how varying |N| impacts both the performance and computational cost of KON. As illustrated in Figure 4, increasing the number of negative examples does not necessarily lead to improved performance. Our observations reveal that the performance curves across all three metrics exhibit a similar trend: initially, there is a rapid increase, followed by a steady decline, and ultimately, a plateau phase.\nIn contrast to the effects of varying the number of head layers K, we find that the computational cost remains relatively stable even as |N| increases. This stability arises because the negative examples are extracted from the K head output probability distributions of K-ON. Thus, adding more negative examples does not involve additional neural layers. Based on these findings, we have selected |N| = 128 as the optimal setting for our main experiments."}, {"title": "Analysis on the Joint Probability Function", "content": "The entity-level contrastive loss requires the entity probability as input, which is derived from its named tokens. As such, selecting an appropriate method to combine these token probabilities into a joint probability for the entity is crucial. We investigate four different methods for estimating this joint probability. The operations of addition and multiplication are denoted by + and *, respectively. The term learnable refers to the version where we utilize a weight vector to aggregate the probabilities of the tokens at different positions, which is shared across entities. Conversely, constant indicates an unweighted aggregation.\nTable 4 illustrates the results on the DB15K dataset. Notably, the learnable + method achieves the highest performance across all metrics. When we remove the learnable weights, there is a slight degradation in performance. In contrast, although the * operator appears to be intuitively more effective, it significantly underperforms compared to + across all metrics. We suspect that the lackluster performance of the multiplication method may be attributed to the issue of vanishing gradients. Specifically, multiplying multiple (8 in our experiments) probabilities may result in exceedingly small scalar values, although conceptually valid as a joint probability, might hinder the learning process."}, {"title": "Conclusion and Limitation", "content": "In this paper, we propose K-ON to stack the knowledge on the head layer of LLM. We introduce an entity-level contrastive loss that significantly reduces computational costs and develope HTT to align the output distributions with those of the original LLM head. Extensive experiments demonstrate the superior performance of K-ON compared to state-of-the-art baselines. K-ON still has limitations. First, its flexibility is constrained, as it does not support arbitrarily large values of K. To address this, we plan to explore a sliding window mechanism for processing K-step prediction in future; Second, K-ON currently lacks support for multimodal input. We also intend to incorporate large visionlanguage models into K-ON as part of our future work."}]}