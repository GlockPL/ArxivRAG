{"title": "Information Geometry and Beta Link for Optimizing Sparse Variational Student-t Processes", "authors": ["Jian Xu", "Delu Zeng", "John Paisley"], "abstract": "Recently, a sparse version of Student-t Processes, termed sparse variational Student-t Processes, has been proposed to enhance computational efficiency and flexibility for real-world datasets using stochastic gradient descent. However, traditional gradient descent methods like Adam may not fully exploit the parameter space geometry, potentially leading to slower convergence and suboptimal performance. To mitigate these issues, we adopt natural gradient methods from information geometry for variational parameter optimization of Student-t Processes. This approach leverages the curvature and structure of the parameter space, utilizing tools such as the Fisher information matrix which is linked to the Beta function in our model. This method provides robust mathematical support for the natural gradient algorithm when using Student's t-distribution as the variational distribution. Additionally, we present a mini-batch algorithm for efficiently computing natural gradients. Experimental results across four benchmark datasets demonstrate that our method consistently accelerates convergence speed.", "sections": [{"title": "Introduction", "content": "Bayesian learning (Ch and MAP 1997) has advanced significantly, with models like Student-t Processes (TPs) (Shah, Wilson, and Ghahramani 2014; Solin and S\u00e4rkk\u00e4 2015) proving effective for heavy-tailed distributions and outlier-prone datasets. This robustness is crucial for real-world data that often deviates from Gaussian assumptions. However, the computational complexity of TPs limits their scalability. To overcome this, Sparse Variational Student-t Processes (SVTPs) (Xu and Zeng 2024) were developed. SVTPs reduce computational demands while preserving the flexibility and robustness of TPs, making them suitable for large and varied datasets. They use stochastic gradient descent (SGD) (Amari 1993) for optimization, enabling practical large-scale data analysis.\nWhile gradient descent and its variants like Adam (Kingma and Ba 2014) are effective and widely used, they are not always the most efficient methods for every problem. These approaches can be limited in their ability to fully exploit the underlying geometry of the parameter space, potentially leading to slower convergence and suboptimal performance. To address these limitations, in this paper, we turn to information geometry (Amari 2012), which enhancing the optimization of SVTPs by employing natural gradients"}, {"title": "Background and Notations", "content": "Student-t Processes\nA Student-t Process (TP) (Shah, Wilson, and Ghahramani 2014; Solin and S\u00e4rkk\u00e4 2015) extends the Gaussian Process (GP) (Seeger 2004) by using the multivariate Student-t distribution, which has heavier tails to better handle outliers and extreme values. The degree of freedom parameter, \u03bd, controls the tail weight, converging to a Gaussian distribution as \u03bd approaches infinity. By adjusting \u03bd, TPs can model various tail behaviors, providing greater flexibility than GPs. Mathematically, the multivariate Student-t distribution is defined as follows:\nDefinition 1. The multivariate Student-t distribution with \u03bd\u2208 R+/ [0,2] degrees of freedom, mean vector \u00b5 and correlation matrix R is defined as:\nST(y|\u03bc,R,\u03bd)\n\n\nwhere \u0393(\u00b7) is the gamma function, y \u2208 Rn, and |\u00b7 | denotes the determinant of a matrix.\nSince the multivariate Student-t distribution is consistent under marginalization, using Kolmogorov's consistency theorem, the Student-t process (TP) is defined as follows:\nDefinition 2. A function f is a Student-t process with parameters v > 2, mean function \u03a8 : X \u2192 R, and kernel function k: X \u00d7 X \u2192 R, if any finite collection of function values has a joint multivariate Student-t distribution, i.e. (f(x1), ..., f (xn))T ~ TPn(\u03bd,\u03c6, \u039a) where K \u2208 \u03a0\u03b7, Kij = k(xi, xj), $i = \u03a8(xi), and TPn(v, \u00a2, K) represents the multivariate Student-t distribution\nSparse variational Student-t Processes\nThe Sparse Student-t Process (SVTP) (Xu and Zeng 2024) introduces inducing points to summarize data efficiently, reducing computational complexity. The key steps are:\nInducing Points Setup M inducing points Z are defined with function values u following a multivariate Student-t process prior:\nu ~ TP\u043c(v, 0, Kz,z\u0131).\nTraining Data and Noise Model Given N training data points (X,y), where y is the output vector and X is the input matrix, the observed output is defined as yi = f(xi) + Ei where ei is noise.\nJoint Distribution The joint distribution of f (function values at training points) and u are defined as a multivariate Student-t distribution:\np(u,f) = ST"}, {"title": "The Steepest Descent", "content": "Next, we discuss how to choose the optimal At from the perspective of information geometry (Amari 2012). Assume \u03a9 is a Riemannian space (Gromov et al. 1999), where we can induce a Riemannian metric tensor G(0), a positive definite P \u00d7 P matrix defined by [G(0)]ij = gij (0), which generally depends on 0. For instance, if G(0) is the identity matrix, \u03a9 corresponds to a Euclidean space. With G(0), a distance\ncan be defined in this Riemannian space via the inner product (, )G,"}, {"title": "Natural Gradient Learning in SVTP", "content": "Fisher Information Matrix as Riemannian Metric Tensor\nTo implement the natural gradient descent algorithm, we revisit classic information geometry literature (Cencov 2000; Campbell 1985) and find that the Riemannian structure of the parameter space can be defined by the Fisher information (Rao 1992; Amari 2012), which serves as a Riemannian metric tensor on the parameter space. Mathematically, the Fisher information matrix F(0) is a matrix whose (i, j)-th element is given by:\n\n\nwhere p(x;0) is the probability density function of a random vector \u00e6 given 0. This metric measures the \"distance\" between different parameter values (Amari 1998), thus enabling the use of geometric methods to study statistical models. Then we use this matrix F(0) to replace G(0) in equation (9), i.e.,"}, {"title": "Representation of Fisher Information Matrix in SVTP", "content": "In SVTP model, first we follow (Xu and Zeng 2024) to parameterize qe (u) as a Student-t distribution, i.e. \u03b8 = {\u0169,m, S} \u2208\u03a9\u2286 RP and qo(u) = ST(\u0169, m, S), where m = (m1, ..., \u0442\u043c). Unlike the original SVTP model, we parameterize S using a diagonal approximation, a technique also seen in previous Gaussian process models (Tran et al. 2021), i.e.,\n\nwhere the diagonal elements are \u03c31,...,\u03c3\u03bb\u03b9, \u03c3\u03b5 \u2208 R+, M is the number of inducing points. We adopt this approximation to simplify the computation of F(0). Then, \u03b8 can be expressed in vector form as,\n0 = (m1,...,\u0442\u043c,\u1fe6,\u03c3\u2081,\u2026\u2026\u2026,\u03c3\u03bc) \u2208R2M+1.\nNext, we will calculate the Fisher information matrix for the variational model parameters 0. According to equation (1), we find that the log-likelihood function of the Student-t distribution can be expressed as follows:\n\nwhere u = (u1, ..., \u0438\u043c)T. Based on equation (10), we consider directly calculating the Fisher information matrix as defined, i.e.,\n\nFor i \u2208 {1,..., M}, we can calculate the derivatives of log q(u; 0) with respect to mi, \u1fe6, and \u03c3\u03af,\n\nwhere \u03a8 (\u00b7) is the digamma function (Bernardo et al. 1976), i.e. \u03a8 (t) = dlog (t) for t \u2208 R. Given that the Fisher information matrix F(0) is a (2M + 1) \u00d7 (2M + 1) matrix and contains three groups of parameters m, S, and \u0169, we partition the matrix F(0) into 9 blocks as follows:\nwhere Fm \u2208 RM\u00d7M, Fmi \u2208 R1\u00d7M. FmS \u2208 RM\u00d71, Fi \u2208 R, FS \u2208 RMX1, FSm \u2208"}, {"title": "Fisher Information Matrix Linked to the Beta Function", "content": "Next, we will calculate the integral part of the eqution (21),\n\u03a6\u03af, (\u03be, \u03b8) \u03b1\u03be1\u03b1\u03be2\u00b7\u00b7\u00b7 \u03b1\u03be\n\nWhen i \u2260 j, according to Fubini's theorem (Ong 2021),\n\nBecause\n\nis an odd function with respect to \u00a7i, its integral over R is 0. Thus, this multiple integral is 0. Consequently, when i \u2260 j,\n\nWhen i = j, since the integration intervals for \u00a71,..., \u00a7\u043c are all R, by global symmetry we have,\n\nConsider the generalized spherical coordinate transformation (Shelupsky 1962) for \u00a7 \u2208 RM,\n\nfor 0 < r < +\u221e,0 < i < \u03c0, \u03af = 1,..., M-2 and 0 < \u03b7\u039c-1 < 2\u03c0. According to the results of the classical multivariate statistical theory (Muirhead 2009), we have the Jacobian of the transformation from \u03be \u03c4\u03bf \u03b7',\n\nwhere det() represents the determinant of a matrix, \u03b7' = (r, 71, 72, ..., \u03b7\u03bc\u22121) \u2208 RM and \u00a2\u03a4\u03be = r\u00b2. The integral with respect to N1, N2, \u00b7 \u00b7 \u00b7, NM-1 in equation (27) can be calculated using Wallis' Formula (Kazarinoff 1956; Guo and Qi 2015),\nLemma 2. (Kazarinoff 1956) (Wallis' Formula) for p\u2208 N, A\u2208 R, we have,"}, {"title": "Then we have,", "content": "\nThen we have,\n\nCombining equations (25, 26, 27, 29), we obtain,\n\n\nwhere the well-known Beta function (Abramowitz and Stegun 1948) can be defined by the abnormal integral\nBeta(a, b) =\n\nConsequently, combining equations (21, 22, 25, 30), we obtain, when i = j,\n\nBy the relationship between the Beta function and the Gamma function (Abramowitz and Stegun 1948), i.e.,\n\nequation (32) can also be expressed in a concise form as,\n\nFinally, the expression for the matrix block Fm is given by,"}, {"title": "Algorithm 1: Stochastic Natural Gradient Descent for SVTP (SNGD-SVTP)", "content": "Input: training data X, y mini-batch size B, training data size N.\nInitialize variational parameters 0 = {v, u, S}, inducing point inputs Z, kernel hyperparameters n.\nrepeat\nSet F(0)-1 by equation (40)\nSample mini-batch indices I C {1, ..., N} with I =\nB\nCompute LB(0, \u0396, \u03b7) by equation (5)\nDo one step Adam optimization on \u0396, \u03b7\nDo one step natural gradient descent on @ by equation\n(41)\nuntil 0, Z, \u03b7 converge\nwhich is a diagonal matrix. Similarly, we can calculate F\nas follows:\n\nwhere \u03b1 (\u03bd) = ()-()*()* and FS as follow:\n\nas well as F\u016bs as follow:\n\nFor Fms and Fm\u1ef9, based on the fact that the integral of an odd function over R is zero, for i, j \u2208 {1,2,..., M}, we have,\n\nThus, we obtain the Fisher Information Matrix of the multivariate Student's t-distribution under diagonal variance as,"}, {"title": "Stochastic Natural Gradient Descent", "content": "Similarly to stochastic gradient descent (Hoffman et al. 2013), in natural gradient descent, we can approximate the entire dataset by leveraging the idea of stochastic gradient descent and using a mini-batch of data as follows:\n\nwhere LB(0) corresponds to the loss function of the minibatch dataset, with B denoting the batch size. In addition to the variational parameters 0, we optimize other model hyperparameters such as kernel hyperparameters jointly within the Evidence Lower Bound (ELBO) framework (Hoffman et al. 2013). Since direct application of natural gradients to these hyperparameters isn't feasible due to the absence of a probability distribution, we employ an alternating scheme: first, we use Adam optimization for a step on the hyperparameters, followed by a step of natural gradient descent on the variational parameters 0. Our algorithm is outlined in Algorithm 1. In this paper, we use a diagonal matrix approximation for S, the complexity is O(M\u00b3 + NM\u00b2). Given that the original complexity of the SVTP model is O(NM\u00b2), where N > M, we believe that the proposed stochastic natural gradient descent algorithm would be more advantageous for larger datasets since it leverages the advantage of the steepest descent, thus providing scalability to the model."}, {"title": "Experiments", "content": "Datasets and Baselines\nWe utilize four real-world datasets from UCI and Kaggle for our experiments: Energy, Elevator, Protein, and Taxi. These datasets range in size from a few hundred to several hundred thousand instances. We split each dataset into training and testing sets with a ratio of 0.8/0.2. The number of inducing points is set to one-fourth of the original dataset size, and we employ the squared exponential kernel. For large datasets such as Protein and Taxi, the number of inducing points is selected to be one-fourth of the batch size. We compare our proposed Stochastic Natural Gradient Descent (SNGD) algorithm against the original SVTP paper's implementations of SGD and Adam, as well as other Adam variants, including Adamgrad (Ward, Wu, and Bottou 2020), Adamax (Kingma and Ba 2014), and Nadam (Dozat 2016). The batch size is set to 1024, the learning rate to 0.01, and all data are standardized. All experiments are conducted using the PyTorch platform on a single RTX 4090D GPU.\nPerformance in Real-world Dataset Regression\nFirst, we validate the efficiency of the SNGD algorithm on the training set, and present its results in Figure 1, which shows the negative ELBO (training loss) curves for the four datasets, with our method highlighted in red. It can be observed that our method consistently achieves the low-est curve positions on three datasets: Energy, Protein, and Taxi, indicating the fastest convergence among the tested algorithms. Although on the Elevator dataset, our method is eventually surpassed by others like Adam after 4 seconds, it still remains competitive. Notably, all algorithms exhibit significant fluctuations in loss on the Taxi dataset, likely due to extreme outliers that perturb the loss function. To further validate the performance of our algorithm, including its generalization capability, we also test these algorithms under the same conditions on the testing set, as shown in Figure 2, using Mean Squared Error (MSE) as the metric. Our method consistently performs the best across all datasets in the testing phase, demonstrating superior generalization. This is attributed to the natural gradient's ability to effectively capture the geometry of the parameter space, making it well-suited for variational Bayesian methods and enabling the algorithm to achieve better posterior estimates and model generalization. Additionally, the original SGD algorithm shows slow convergence in both training and testing phases. While Adam is a widely-used and excellent algorithm for regression tasks, in the context of the specific SVTP model, we recommend the SNGD algorithm."}, {"title": "Conclusion", "content": "In summary, our study introduces natural gradient methods from information geometry to optimize sparse variational Student-t Processes. This approach harnesses the curvature of the parameter space, leveraging the Fisher information matrix linked to the Beta function in our model. Experimental results across benchmark datasets consistently show that our method accelerates convergence speed compared to traditional gradient descent methods like Adam, thereby enhancing computational efficiency and model flexibility for real-world datasets. We are optimistic about extending this method to more Student's t natural gradient algorithms."}, {"title": "Reproducibility Checklist", "content": "This paper:\nIncludes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\nClearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\nProvides well marked pedagogical references for lessfamiliare readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (yes)\nIf yes, please complete the list below.\nAll assumptions and restrictions are stated clearly and formally. (yes)\nAll novel claims are stated formally (e.g., in theorem statements). (yes)\nProofs of all novel claims are included. (yes)\nProof sketches or intuitions are given for complex and/or novel results. (yes)\nAppropriate citations to theoretical tools used are given. (yes)\nAll theoretical claims are demonstrated empirically to hold. (yes)\nAll experimental code used to eliminate or disprove claims is included. (yes)\nDoes this paper rely on one or more datasets? (yes)\nIf yes, please complete the list below.\nA motivation is given for why the experiments are conducted on the selected datasets (yes)\nAll novel datasets introduced in this paper are included in a data appendix. (NA)\nAll novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\nAll datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (NA)\nDoes this paper include computational experiments? (yes)\nIf yes, please complete the list below.\nAny code required for pre-processing data is included in the appendix. (no).\nAll source code required for conducting and analyzing the experiments is included in a code appendix. (no)\nAll source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\nAll source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes)\nIf an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\nThis paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes)\nThis paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)\nThis paper states the number of algorithm runs used to compute each reported result. (yes)\nAnalysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes)\nThe significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)\nThis paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (yes)\nThis paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes)"}]}