{"title": "Information Geometry and Beta Link for Optimizing Sparse Variational Student-t Processes", "authors": ["Jian Xu", "Delu Zeng", "John Paisley"], "abstract": "Recently, a sparse version of Student-t Processes, termed sparse variational Student-t Processes, has been proposed to enhance computational efficiency and flexibility for real-world datasets using stochastic gradient descent. However, traditional gradient descent methods like Adam may not fully exploit the parameter space geometry, potentially leading to slower convergence and suboptimal performance. To mitigate these issues, we adopt natural gradient methods from information geometry for variational parameter optimization of Student-t Processes. This approach leverages the curvature and structure of the parameter space, utilizing tools such as the Fisher information matrix which is linked to the Beta function in our model. This method provides robust mathematical support for the natural gradient algorithm when using Student's t-distribution as the variational distribution. Additionally, we present a mini-batch algorithm for efficiently computing natural gradients. Experimental results across four benchmark datasets demonstrate that our method consistently accelerates convergence speed.", "sections": [{"title": "Introduction", "content": "Bayesian learning (Ch and MAP 1997) has advanced significantly, with models like Student-t Processes (TPs) (Shah, Wilson, and Ghahramani 2014; Solin and S\u00e4rkk\u00e4 2015) proving effective for heavy-tailed distributions and outlier-prone datasets. This robustness is crucial for real-world data that often deviates from Gaussian assumptions. However, the computational complexity of TPs limits their scalability. To overcome this, Sparse Variational Student-t Processes (SVTPs) (Xu and Zeng 2024) were developed. SVTPs reduce computational demands while preserving the flexibility and robustness of TPs, making them suitable for large and varied datasets. They use stochastic gradient descent (SGD) (Amari 1993) for optimization, enabling practical large-scale data analysis.\nWhile gradient descent and its variants like Adam (Kingma and Ba 2014) are effective and widely used, they are not always the most efficient methods for every problem. These approaches can be limited in their ability to fully exploit the underlying geometry of the parameter space, potentially leading to slower convergence and suboptimal performance. To address these limitations, in this paper, we turn to information geometry (Amari 2012), which enhancing the optimization of SVTPs by employing natural gradients (Amari 1998; Hensman, Fusi, and Lawrence 2013; Salimbeni, Eleftheriadis, and Hensman 2018; Martens 2020). Unlike ordinary gradients, natural gradients consider the geometry of the parameter space defined by the variational distribution. Specifically, natural gradients scale the ordinary gradients by the inverse Fisher information matrix (Rao 1992; Amari 2012), leading to more efficient and effective optimization paths.\nWe demonstrate that natural gradients can be computed efficiently using the Fisher information matrix in SVTP models. Specifically, we first parameterize the variational distribution using multivariate Student-t distributions (Xu and Zeng 2024) with diagonal covariance matrices and then present a robust method for computing the Fisher information matrix using classical multivariate statistical theory (Muirhead 2009). This method establishes a connection between the Fisher Information Matrix of the multivariate Student's t-distribution and the Beta function (Abramowitz and Stegun 1948), which we term the 'Beta Link'. Additionally, we seamlessly integrate natural gradients with contemporary optimizers like Adam, thereby facilitating improved hyperparameter learning and enhancing overall model performance. Furthermore, we introduce a mini-batch algorithm. Through extensive experiments involving various likelihoods and benchmark datasets, we consistently show that the application of natural gradients enhances the performance of SVTPs, resulting in significant improvements in convergence during training. Our contributions in this paper are threefold:\n\u2022 We propose the application of natural gradients for optimizing sparse variational Student-t processes, alongside introducing a mini-batch algorithm.\n\u2022 We present a robust method for computing the Fisher information matrix of multivariate Student-t distributions with diagonal covariance matrices, using classical multivariate statistical theory to establish a connection between the Fisher Information Matrix of the multivariate Student's t-distribution and the Beta function. The method provides robust mathematical support for the natural gradient algorithm when using Student's t-distribution as the variational distribution.\n\u2022 We empirically validate our approach across various likelihoods and benchmark datasets, illustrating its superior"}, {"title": "Background and Notations", "content": "Student-t Processes\nA Student-t Process (TP) (Shah, Wilson, and Ghahramani 2014; Solin and S\u00e4rkk\u00e4 2015) extends the Gaussian Process (GP) (Seeger 2004) by using the multivariate Student-t distribution, which has heavier tails to better handle outliers and extreme values. The degree of freedom parameter, \u03bd, controls the tail weight, converging to a Gaussian distribution as \u03bd approaches infinity. By adjusting \u03bd, TPs can model various tail behaviors, providing greater flexibility than GPs. Mathematically, the multivariate Student-t distribution is defined as follows:\nDefinition 1. The multivariate Student-t distribution with \u03bd\u2208 R+/ [0,2] degrees of freedom, mean vector \u00b5 and correlation matrix R is defined as:\nST(y|\u03bc, R, \u03bd)\n\n\nwhere \u0393(\u00b7) is the gamma function, y \u2208 Rn, and |\u00b7 | denotes the determinant of a matrix.\nSince the multivariate Student-t distribution is consistent under marginalization, using Kolmogorov's consistency theorem, the Student-t process (TP) is defined as follows:\nDefinition 2. A function f is a Student-t process with parameters v > 2, mean function \u03a8 : X \u2192 R, and kernel function k: X \u00d7 X \u2192 R, if any finite collection of function values has a joint multivariate Student-t distribution, i.e. (f(x1), ..., f (xn))T ~ TPn(\u03bd,\u03c6, \u039a) where K \u2208 \u03a0\u03b7, Kij = k(xi, xj), $i = \u03a8(xi), and TPn(v, \u00a2, K) represents the multivariate Student-t distribution\nSparse variational Student-t Processes\nThe Sparse Student-t Process (SVTP) (Xu and Zeng 2024) introduces inducing points to summarize data efficiently, reducing computational complexity. The key steps are:\nInducing Points Setup M inducing points Z are defined with function values u following a multivariate Student-t process prior:\nu ~ TP\u043c(v, 0, Kz,z\u0131).\n(2)\nTraining Data and Noise Model Given N training data points (X,y), where y is the output vector and X is the input matrix, the observed output is defined as yi = f(xi) + Ei where ei is noise.\nJoint Distribution The joint distribution of f (function values at training points) and u are defined as a multivariate Student-t distribution:\np(u,f) = ST \n\n(3)\n, \nConditional Distribution The conditional distribution p(fu) is derived to introduce inducing points for efficient computation:\np(fu) = ST(v + \u039c, \u03bc, \n-1\n\u03bd + \u03b2-2\n\u03bd +\u039c-2\n\u03a3), (4)\nwhere \u03bc = Kx,zKzzu, \u03b2 = uTK\u012bzu, and \u2211 = Kx,x - Kx,zKzzKz,x.\nTo handle the intractability of the true marginal likelihood log p(y), variational inference is used to introduce variational distributions qe(u) to approximate the true posterior p(uf, y), where \u03b8\u2208 \u03a9 \u2286 RP is the parameter of the distribution q and \u03a9 is the parameter space. Using Jensen's inequality, a lower bound L(0) on the marginal likelihood is derived as:\nlogp(y) \u2265 L(0)\n= Ep(f|u)qe (u) [log p(y|f)] \u2013 KL(qe(u)||p(u)).\n(5)\nOptimization Monte Carlo sampling and gradient-based optimization (Amari 1967) are employed to estimate and maximize the variational lower bound L(0), i.e.,\n0t+1 = 0 \u2013 AA(0t)\u2207oL(0)|0=\u03b8\u03b5,\n(6)\nwhere At \u2208 R denotes the step size and A(0t) is a suitably chosen P \u00d7 P positive definite matrix. On large datasets, the above formula can also be applied using a mini-batch of the data for acceleration (Hoffman et al. 2013).\nThe original SVTP (Xu and Zeng 2024) paper employs the most straightforward method, which is gradient descent, where A(0t) is set to the identity matrix. The step size can be fixed, decayed, or found by a line search on each iteration. A more sophisticated approach called Adam (Kingma and Ba 2014) is to use a designed diagonal matrix for A(\u03b8t). While gradient descent and its variants like Adam are effective and widely used, they are not always the most efficient methods for every problem. These approaches can be limited in their ability to fully exploit the underlying geometry of the parameter space, potentially leading to slower convergence and suboptimal performance. To address these limitations, we can turn to information geometry, which provides a framework for developing more efficient optimization algorithms by taking into account the curvature and structure of the parameter space.\nThe Steepest Descent\nNext, we discuss how to choose the optimal At from the perspective of information geometry (Amari 2012). Assume \u03a9 is a Riemannian space (Gromov et al. 1999), where we can induce a Riemannian metric tensor G(0), a positive definite P \u00d7 P matrix defined by [G(0)]ij = gij (0), which generally depends on 0. For instance, if G(0) is the identity matrix, \u03a9 corresponds to a Euclidean space. With G(0), a distance ||G can be defined in this Riemannian space via the inner product (, )G,\n|d0|& = (do, do) = \u2211gij(0)d0d0j,\n(7)"}, {"title": "Natural Gradient Learning in SVTP", "content": "where 0 and 0; denote the i-th and j-th components of 0, respectively. The steepest descent (Goldstein 1965) direction of a function L(0) at 0 is defined by the vector de that minimizes L(0), where do has a fixed length, that is, under the constraint\n|d0|2 = 2\nfor a sufficiently small constant \u025b. We use the following lemma to prove that A(0t) in equation (6) being G(0+)-1 ensures that the loss function L(0) achieves the steepest descent in a Riemannian space.\nLemma 1. (Amari 1998) The steepest descent direction of L(0) in a Riemannian space is given by\n-L(0) = \u2212G(0)-1\u2207L(0),\n(8)\nwhere G\u22121 = (gii) is the inverse of the metric G = (gij(0)) and L is the conventional gradient,\n\u25bdL(0)\n= \nT\nWe define L(0) = G(0)-1\u2207L(0) as the natural gradient of L(0) in the Riemannian space. Thus, \u2013\u2207L(0) represents the steepest descent direction of L(0), which corresponds to the natural gradient descent algorithm in equation (6), i.e.,\n0t+1 = 0\u03c4 \u2013 \u03bb\u03b5G(0t)\u00af\u00b9VoL(0)|0=\u03b8\u03b5.\n(9)\nFisher Information Matrix as Riemannian Metric Tensor\nTo implement the natural gradient descent algorithm, we revisit classic information geometry literature (Cencov 2000; Campbell 1985) and find that the Riemannian structure of the parameter space can be defined by the Fisher information (Rao 1992; Amari 2012), which serves as a Riemannian metric tensor on the parameter space. Mathematically, the Fisher information matrix F(0) is a matrix whose (i, j)-th element is given by:\nFij (0)\n= \n[\n]\n(10)\nwhere p(x;0) is the probability density function of a random vector \u00e6 given 0. This metric measures the \"distance\" between different parameter values (Amari 1998), thus enabling the use of geometric methods to study statistical models. Then we use this matrix F(0) to replace G(0) in equation (9), i.e.,\n0t+1 = 0t \u2013 XtF(0t)\u00af\u00b9\u22070L(0)|0=\u03b8\u03b5.\nRepresentation of Fisher Information Matrix in SVTP\n(11)\nIn SVTP model, first we follow (Xu and Zeng 2024) to parameterize qe (u) as a Student-t distribution, i.e. \u03b8 = {\u0169,m, S} \u2208\u03a9\u2286 RP and qo(u) = ST(\u0169, m, S), where m = (m1, ..., \u0442\u043c). Unlike the original SVTP model, we"}, {"title": "Fisher Information Matrix Linked to the Beta Function", "content": "parameterize S using a diagonal approximation, a technique also seen in previous Gaussian process models (Tran et al. 2021), i.e.,\n\nS =\n\n(12)\nwhere the diagonal elements are \u03c3\u2081,...,\u03c3\u03bb\u03b9, \u03c3\u03b5 \u2208 R+, M is the number of inducing points. We adopt this approximation to simplify the computation of F(0). Then, \u03b8 can be expressed in vector form as,\n0 = (m1,...,\u0442\u043c,\u1fe6,\u03c3\u2081,\u2026\u2026\u2026,\u03c3\u03bc) \u2208R2M+1. (13)\nNext, we will calculate the Fisher information matrix for the variational model parameters 0. According to equation (1), we find that the log-likelihood function of the Student-t distribution can be expressed as follows:\nlog qo (u) = - log \u0393 + log \u0393\n\n\n(14)\nwhere u = (u1, ..., \u0438\u043c)T. Based on equation (10), we consider directly calculating the Fisher information matrix as defined, i.e.,\nFij (0) = Eq(u;0) \n\n\n(15)\nFor i \u2208 {1,..., M}, we can calculate the derivatives of log q(u; 0) with respect to mi, \u1fe6, and \u03c3\u03af,\n\n\n\n\n\n\n(16)\nwhere (\u00b7) is the digamma function (Bernardo et al. 1976), i.e. \u03a8 (t) = dlog (t) for t \u2208 R. Given that the Fisher information matrix F(0) is a (2M + 1) \u00d7 (2M + 1) matrix and contains three groups of parameters m, S, and \u0169, we partition the matrix F(0) into 9 blocks as follows:\n\n\nF(0) =\n\n(17)\nwhere Fm \u2208 RMXM Fmi\nIR1\u00d7M. FmS E\nRM\u00d7M, F\u00fcm \u2208 RM\u00d71, Fu \u2208 R, FiS \u2208 RMX1, FSm E"}, {"title": "", "content": "RM\u00d7M, FSi \u2208 R1\u00d7M, FS \u2208 RM\u00d7M, and the superscripts correspond to the set of parameters required for the derivative of log q(u; 0\u2081). For example, the (i, j)-th element of the matrix Fm is defined as:\nF(0) = Eq(u;0) \n\n(18)\nOther matrix blocks are defined similarly. Additionally, we note that Fm = (F\u016bm)T, FmS = (FSm)T, and Fis = (FS)T. Therefore, we only need to solve six of these matrix blocks. For the matrix block Fm, combining equations (16) and (18), we have,\n\n(19)\nwhere Ci,j(0)\nYi,j (u, 0)\n+\n(20)\nBy making the transformation\n\u03be\u03af =\nwe have:\n\u03c6\u03ad,j (u, 0) = \u03a6\u03ad,j (\u03be, \u03b8) = (\u03cb\u03bd \u2013 2) \u03c3\u03b5\u03c3\u03be\u03b5\u03be; (1 + \u03a3\u03b5\nand\nui = mi + \u0123i\u221a\u1fe6 \u2013 2\u03c3\u03af,\nwhere \u03be = (\u00a71, ..., \u00a7\u043c). Using this transformation, we can rewrite eqution (19), specifically,\nF(0) = Ci,j (0) i,j (u, 0)du\u2081du2dum\n\n\n(21)\nNext, we will calculate the integral part of the eqution (21),\n\n=\n\n(22)"}, {"title": "", "content": "When i \u2260 j, according to Fubini's theorem (Ong 2021),\n\n\n==\n\n(23)\nBecause\n\n\nis an odd function with respect to \u00a7i, its integral over R is 0.\nThus, this multiple integral is 0. Consequently, when i \u2260 j,\nF(0) = 0.\n(24)\nWhen i = j, since the integration intervals for \u00a71,..., \u00a7\u043c are all R, by global symmetry we have,\n\n\n\n\n\n(25)\nConsider the generalized spherical coordinate transformation (Shelupsky 1962) for \u00a7 \u2208 RM,\n\n\n(26)\nfor 0 < r < +\u221e,0 < i < \u03c0, \u03af = 1,..., M-2 and 0 < \u03b7\u039c-1 < 2\u03c0. According to the results of the classical multivariate statistical theory (Muirhead 2009), we have the Jacobian of the transformation from \u03be \u03c4\u03bf \u03b7',\n\n\n(27)\nwhere det() represents the determinant of a matrix, \u03b7' = (r, 71, 72, ..., \u03b7\u03bc\u22121) \u2208 RM and \u00a2\u03a4\u03be = r\u00b2. The integral with respect to N1, N2, \u00b7 \u00b7 \u00b7, NM-1 in equation (27) can be calculated using Wallis' Formula (Kazarinoff 1956; Guo and Qi 2015),\nLemma 2. (Kazarinoff 1956) (Wallis' Formula) for p\u2208 N, A\u2208 R, we have,\n\n(28)"}, {"title": "", "content": "Then we have,\n\n\n\n\n\n\n\n\n(29)\nCombining equations (25, 26, 27, 29), we obtain,\n\n\n\n(30)\nwhere the well-known Beta function (Abramowitz and Stegun 1948) can be defined by the abnormal integral\nBeta(a, b)\n\n\n(31)\nConsequently, combining equations (21, 22, 25, 30), we obtain, when i = j,\n\n\n(32)\nBy the relationship between the Beta function and the Gamma function (Abramowitz and Stegun 1948), i.e.,\nBeta(x, y) =\n\nequation (32) can also be expressed in a concise form as,\n\n\n(33)\nFinally, the expression for the matrix block Fm is given by,\n\n(34)"}, {"title": "Algorithm 1: Stochastic Natural Gradient Descent for SVTP (SNGD-SVTP)", "content": "Algorithm 1: Stochastic Natural Gradient Descent for SVTP (SNGD-SVTP)\nInput: training data X, y mini-batch size B, training data size N.\nInitialize variational parameters 0 = {v, u, S}, inducing point inputs Z, kernel hyperparameters n.\nrepeat\nSet F(0)-1 by equation (40)\nSample mini-batch indices I C {1, ..., N} with I = B\nCompute LB(0, \u0396, \u03b7) by equation (5)\nDo one step Adam optimization on \u0396, \u03b7\nDo one step natural gradient descent on @ by equation (41)\nuntil 0, Z, \u03b7 converge\nwhich is a diagonal matrix. Similarly, we can calculate F as follows:\n\n\n(35)\nwhere a (1)-(\ub4e4)+()-2012) and FS as\nfollow:\n\n\n\n\n\n(i = j)\n(i \u2260 j)\n(36)\nas well as F\u016bs as follow:\n\n(37)\nFor Fms and Fm\u1ef9, based on the fact that the integral of an odd function over R is zero, for i, j \u2208 {1,2,..., M}, we have,\nFms\nij = 0, Fm = 0.\n(38)\nThus, we obtain the Fisher Information Matrix of the multivariate Student's t-distribution under diagonal variance as,\n\n(39)"}, {"title": "Stochastic Natural Gradient Descent", "content": "where O represents the all-zero matrix block,\nFm, F, FS, F\u016bs are defined in equations (34, 35, 36 and 37), respectively. Notably, we established a connection between the Fisher Information Matrix of the multivariate Student's t-distribution and the Beta function. To the best of our knowledge, this is the first time these results have been derived in the literature, therefore, we refer to this trick as the 'Beta link'. When performing natural gradient descent, we need to invert F(0). Through calculations, we found that it is a block-diagonal matrix, and one of its diagonal blocks, Fm, is also a diagonal matrix. We can simplify this inversion using the Sherman-Morrison-Woodbury Formula (Akg\u00fcn, Garcelon, and Haftka 2001), i.e.,\n\n\n(40)\nwhere Ais is an (M + 1) \u00d7 (M + 1) matrix.\nStochastic Natural Gradient Descent\nSimilarly to stochastic gradient descent (Hoffman et al. 2013), in natural gradient descent, we can approximate the entire dataset by leveraging the idea of stochastic gradient descent and using a mini-batch of data as follows:\n\n(41)\nwhere LB(0) corresponds to the loss function of the mini-batch dataset, with B denoting the batch size. In addition to the variational parameters 0, we optimize other model hyperparameters such as kernel hyperparameters jointly within the Evidence Lower Bound (ELBO) framework (Hoffman et al. 2013). Since direct application of natural gradients to these hyperparameters isn't feasible due to the absence of a probability distribution, we employ an alternating scheme: first, we use Adam optimization for a step on the hyperparameters, followed by a step of natural gradient descent on the variational parameters 0. Our algorithm is outlined in Algorithm 1. In this paper, we use a diagonal matrix approximation for S, the complexity is O(M\u00b3 + NM\u00b2). Given that the original complexity of the SVTP model is O(NM\u00b2), where N > M, we believe that the proposed stochastic natural gradient descent algorithm would be more advantageous for larger datasets since it leverages the advantage of the steepest descent, thus providing scalability to the model."}, {"title": "Experiments", "content": "Datasets and Baselines\nWe utilize four real-world datasets from UCI and Kaggle for our experiments: Energy, Elevator, Protein, and Taxi. These datasets range in size from a few hundred to several hundred thousand instances. We split each dataset into training and testing sets with a ratio of 0.8/0.2. The number of inducing points is set to one-fourth of the original dataset size, and we employ the squared exponential kernel. For large datasets such as Protein and Taxi, the number of inducing points is selected to be one-fourth of the batch size. We compare our proposed Stochastic Natural Gradient Descent (SNGD) algorithm against the original SVTP paper's implementations of SGD and Adam, as well as other Adam variants, including Adamgrad (Ward, Wu, and Bottou 2020), Adamax (Kingma and Ba 2014), and Nadam (Dozat 2016). The batch size is set to 1024, the learning rate to 0.01, and all data are standardized. All experiments are conducted using the PyTorch platform on a single RTX 4090D GPU.\nPerformance in Real-world Dataset Regression\nFirst, we validate the efficiency of the SNGD algorithm on the training set, and present its results in Figure 1, which shows the negative ELBO (training loss) curves for the four datasets, with our method highlighted in red. It can be observed that our method consistently achieves the lowest curve positions on three datasets: Energy, Protein, and Taxi, indicating the fastest convergence among the tested algorithms. Although on the Elevator dataset, our method is eventually surpassed by others like Adam after 4 seconds, it still remains competitive. Notably, all algorithms exhibit significant fluctuations in loss on the Taxi dataset, likely due to extreme outliers that perturb the loss function. To further validate the performance of our algorithm, including its generalization capability, we also test these algorithms under the same conditions on the testing set, as shown in Figure 2, using Mean Squared Error (MSE) as the metric. Our method consistently performs the best across all datasets in the testing phase, demonstrating superior generalization. This is attributed to the natural gradient's ability to effectively capture the geometry of the parameter space, making it well-suited for variational Bayesian methods and enabling the algorithm to achieve better posterior estimates and model generalization. Additionally, the original SGD algorithm shows slow convergence in both training and testing phases. While Adam is a widely-used and excellent algorithm for regression tasks, in the context of the specific SVTP model, we recommend the SNGD algorithm."}, {"title": "Conclusion", "content": "In summary, our study introduces natural gradient methods from information geometry to optimize sparse variational"}]}