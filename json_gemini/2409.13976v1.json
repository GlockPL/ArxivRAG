{"title": "Detecting Inpainted Video with Frequency Domain Insights", "authors": ["Quanhui Tang", "Jingtao Cao"], "abstract": "Video inpainting enables seamless content removal and replacement within frames, posing ethical and legal risks when misused. To mitigate these risks, detecting manipulated regions in inpainted videos is critical. Previous detection methods often focus solely on the characteristics derived from spatial and temporal dimensions, which limits their effectiveness by overlooking the unique frequency characteristics of different inpainting algorithms. In this paper, we propose the Frequency Domain Insights Network (FDIN), which significantly enhances detection accuracy by incorporating insights from the frequency domain. Our network features an Adaptive Band Selective Response module to discern frequency characteristics specific to various inpainting techniques and a Fast Fourier Convolution-based Attention module for identifying periodic artifacts in inpainted regions. Utilizing 3D ResBlocks for spatiotemporal analysis, FDIN progressively refines detection precision from broad assessments to detailed localization. Experimental evaluations on public datasets demonstrate that FDIN achieves state-of-the-art performance, setting a new benchmark in video inpainting detection.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid growth of multimedia and social networks has created a strong demand for advanced video editing techniques. Video inpainting, a key method in video editing, allows for the removal of unwanted elements and the reconstruction of visually plausible content in missing regions [1], [2]. While video inpainting is often used for legitimate purposes, it can also be exploited for malicious tampering, such as erasing copyright watermarks, fabricating content, or removing incriminating evidence from footage. These unethical applications raise serious moral and security concerns. To address these issues, video inpainting detection, a technique designed to identify areas that have been altered using inpainting methods, has emerged as an important field in safeguarding the integrity of multimedia systems.\nIn recent years, great progress has been made in video in-painting detection, especially deep learning-based approaches. Zhou et al. [3] introduced the first CNN-based [4] model with LSTM [5] to ensure temporal continuity and applied error level analysis (ELA) [6] for processing video frames, enhancing model robustness through multimodal features. However, ELA's dependence on JPEG compression parameters restricts its use to primarily MJPEG videos. Wei et al. [7] used a high-pass filter [8] for recursive intra-frame filtering and optical flow [9] for frame alignment, focusing on inter-frame residuals to bolster temporal correlations in detection. Nevertheless, challenges in accurate optical flow estimation can affect motion continuity and texture details, diminishing the model's overall effectiveness. Yu et al. [10] developed a Transformer [11]-based model with a frequency domain filtering module targeting unnatural edges by analyzing high-frequency video features. Nonetheless, its approach of separating frequency bands artificially without considering complex frequency domain correlations leads to inconsistent outcomes.\nWhile CNN-based methods extract spatial features effectively, they often struggle with temporal dynamics in videos. Conversely, Transformer models address these temporal aspects but tend to incur high computational costs. To tackle these challenges, we introduce the Frequency Domain Inights Network (FDIN), which utilizes a 3D convolutional approach to efficiently capture both spatial and temporal information while simplifying model architecture. The Adaptive Band Selective Response (ABSR) module within FDIN automatically learns frequency domain features tailored to various inpainting techniques, overcoming the limitations of previous models that relied on static frequency filtering. By integrating 3D ResBlocks [12] with the ABSR's adaptive learning, FDIN identifies unique spectral distributions and periodic artifacts from inpainting [13] operations. Furthermore, a Fast Fourier Convolution-based Attention (FFCA) module facilitates the detection of these artifacts [14]. By fusing shallow and deep frequency domain information through jump connections, FDIN progressively refines detection from broad overviews to localized masks, resulting in a precise identification of inpainting areas. Comparative evaluations demonstrate that our method establishes a new standard in detection performance.\nThe contributions of our paper can be summarised as follows:\n1) We introduce FDIN, a novel 3D convolutional-based encoder-decoder network for video inpainting detection, designed to efficiently integrate multi-domain information."}, {"title": "II. PROPOSED METHOD", "content": "The proposed FDIN enhances video inpainting detection by integrating spatial, temporal, and frequency domain features within an encoder-decoder architecture. As shown in Fig. 1, FDIN consists of four main components: the ABSR module, the 3D ResBlock encoder, the FFCA module, and a Mask Refinement decoder. ABSR captures adaptive frequency features relevant to inpainting techniques, which are processed by the 3D ResBlock encoder to extract spatio-temporal information. FFCA identifies periodic artifacts by analyzing both local and global frequency features, and the Mask Refinement decoder progressively sharpens the detection mask for precise inpainting localization. This structured encoder-decoder design enables FDIN to detect inpainting efficiently by leveraging insights from multiple domains."}, {"title": "A. Adaptive Band Selective Response (ABSR)", "content": "The ABSR module is designed to enhance the extraction of critical frequency-domain features for detecting inpainting traces. By adaptively selecting relevant frequency bands, the ABSR module helps highlight frequency components indicative of inpainting, while suppressing irrelevant information.\nInitially, the input video frame \\( I \\) is transformed into the frequency domain using the Discrete Cosine Transform (DCT), yielding the frequency spectrum \\( \\hat{I} \\):\n\\[\\hat{I} = DCT(I).\\]\nA learnable matrix \\( L \\), initialized with values in the range [0, 1], is applied element-wise to \\( \\hat{I} \\), selectively emphasizing frequency components correlated with inpainting artifacts:\n\\[\\hat{I}' = \\hat{I} \\odot L,\\]\nwhere \\( \\odot \\) denotes the Hadamard Product. This adaptive filtering isolates the relevant frequencies, and the filtered spectrum \\( \\hat{I}' \\) is transformed back to the spatial domain using the Inverse DCT (IDCT):\n\\[I' = IDCT(\\hat{I}').\\]\nThe output \\( I' \\) now contains enhanced frequency-related features associated with inpainted regions, enriching the detection process by providing a focused set of frequency components for further spatiotemporal analysis.\nThe resulting frame is then integrated with the subsequent 3D ResBlock module, effectively combining frequency-domain and spatiotemporal features. This ensures a comprehensive analysis of both spatiotemporal and frequency-based characteristics, improving the overall capability of detecting inpainting traces across diverse content.\nBy utilizing ABSR, the model can dynamically adjust to the variation of frequency distribution in inpainted videos, offering improved robustness and detection accuracy."}, {"title": "B. 3D ResBlock Encoder", "content": "The 3D ResBlock Encoder extracts spatiotemporal features from video data by employing a stack of 3D convolutional blocks. This allows it to simultaneously process spatial details and temporal relationships, which are crucial for identifying inpainting traces.\nEach 3D ResBlock includes a 3D convolution, batch normalization, and ReLU activation. Formally, the operation is represented as:\n\\[Y = X + F(X; W),\\]\nwhere \\( X \\) is the input, \\( F \\) is the convolutional transformation, and \\( W \\) are the learnable parameters. The residual connection facilitates efficient gradient flow, helping the network learn deeper features effectively.\nThe stacked 3D ResBlocks ensure the encoder captures both short- and long-term temporal behaviors, as well as fine spatial details. These features are essential for robust inpainting detection, as they highlight subtle disruptions in natural video sequences.\nAfter processing, the features from the encoder are passed to the Fast Fourier Convolution-based Attention (FFCA) module, which further enhances frequency-related artifacts. This combined spatiotemporal and frequency-domain analysis streamlines the detection of altered regions in video content."}, {"title": "C. Fast Fourier Convolution-based Attention (FFCA)", "content": "The FFCA module enhances the detection of periodic patterns and frequency-based artifacts in inpainted regions. It splits the deep feature representation \\( Z \\in \\mathbb{R}^{T \\times C \\times H \\times W} \\), obtained from the 3D ResBlock, into two frequency components: Local Fourier Unit (LFU) and Global Fourier Unit (GFU).\nThe LFU processes local features through standard 3D convolution:\n\\[\\tilde{Z}_L = Conv3D(Z_L).\\]\nFor the GFU, a real Fast Fourier Transform (FFT) is applied to capture global frequency features:\n\\[\\tilde{Z}_G = RFFT(Z_G),\\]\nfollowed by non-linear transformations for component refinement and an Inverse FFT (IFFT) to return the feature to the temporal domain:\n\\[\\hat{Z}_G = IRFFT(\\tilde{Z}_G).\\]\nThe outputs of LFU and GFU are concatenated to form the combined feature:\n\\[\\hat{Z} = Concat(\\tilde{Z}_L, \\hat{Z}_G),\\]\nwhich is then fused with the original input to maintain comprehensive spatiotemporal and frequency details. The FFCA provides robust artifact detection by leveraging local and global frequency-domain information in tandem."}, {"title": "D. Mask Refinement Decoder", "content": "The Mask Refinement Decoder is designed to refine the initial detection of inpainted areas. Starting from the rough mask predicted by the encoder, it progressively enhances the accuracy of the mask boundaries using a series of 3D convolutional layers.\nAt each stage, features from the encoder are upsampled and combined with multi-level features to restore spatial resolution and refine the mask edges [15]. This process is controlled by 3D convolutions that capture finer details from shallow layers, improving the distinction between inpainted and natural regions.\nThe final output is a high-resolution mask accurately highlighting the inpainted areas, ensuring that the detection is both precise and spatially coherent."}, {"title": "III. EXPERIMENTAL RESULTS", "content": null}, {"title": "A. Experimental setup", "content": "To validate FDIN, we conduct experiments on two benchmark datasets: DAVIS 2016 [16] and FVI [17].\nDAVIS 2016 contains 50 high-quality video sequences, divided into 30 training and 20 validation videos. Each sequence has a 1080P resolution at 24 FPS, making it suitable for inpainting detection tasks.\nFVI includes 100 test videos with multi-object inpainting scenarios. It poses a challenging setting due to complex occlusions and background reconstructions.\nFor performance evaluation, we use mean Intersection over Union (mIoU) and F1-Score, which are common metrics for inpainting detection. mIoU calculates the overlap between the predicted and ground truth inpainted regions, while the F1 Score balances precision and recall.\nTraining setup: The model is implemented in PyTorch and trained on four NVIDIA TITAN RTX GPUs. We set the batch size to 32, with an input resolution of 240 \u00d7 427. We train FDIN for 20 epochs using the Adam optimizer [18] with a learning rate of \\( 1 \\times 10^{-4} \\), which is halved after 10 epochs. FDIN is pre-trained on Kinetics-400 [19] and YouTube-VOS [20] to generalize better across diverse content.\nData preprocessing involves resizing frames and applying data augmentation like random cropping and horizontal flipping. A sliding window of Tc = 8 consecutive frames is used as input to maintain temporal continuity.\nFinally, the experiments are run under fixed random seeds, ensuring reproducibility and fair comparisons across models."}, {"title": "B. Results on DAVIS 2016 Dataset", "content": "To validate FDIN, we conducted experiments on the DAVIS 2016 dataset [16], a benchmark for inpainting detection tasks.\nModel comparison: As shown in Table I, we compare FDIN with several state-of-the-art methods, such as NO\u0399 [21], CFA [22], COSNet [23], HPF [8], VIDNet [3], FAST [10], and STTL [7], across three inpainting methods: VI [25], OP [26], and CP [27]. If a dataset is used for both training and testing, it is marked with an asterisk (*); otherwise, it refers to testing only.\nFDIN consistently outperforms other approaches. On OP*, FDIN achieves an mIoU of 0.70 and F1 score of 0.81, surpassing STTL's 0.69/0.80. For CP*, FDIN achieves 0.85/0.91, demonstrating its effectiveness across different inpainting methods.\nGeneralization performance: FDIN also shows robustness when trained on two inpainting methods and tested on a third. For example, in the VI+OP training and CP testing scenario, FDIN reaches 0.49 mIoU, outperforming VIDNet's 0.39, confirming its ability to generalize across various inpainting techniques.\nQuantitative gains: FDIN achieves an average mIoU improvement of 14.4% and an F1 score increase of 11.6% over existing methods, underscoring its strong detection performance due to the ABSR and FFCA modules."}, {"title": "C. Results on FVI Dataset", "content": "We evaluate FDIN on the FVI dataset [17], which involves more complex multi-object inpainting scenarios.\nModel comparison: As shown in Table II, FDIN surpasses prior methods like NOI [21], CFA [22], HPF [8], GSR-Net [24], VIDNet [3], and FAST [10]. FDIN achieves the top mIoU (0.315) and F1 score (0.408), outperforming FAST by notable margins (mIoU 0.285, F1 0.359), demonstrating superior detection, even in cases with complex occlusions."}, {"title": "D. QF interference experiment", "content": "To evaluate FDIN's robustness under varying compression levels, we conducted a Quantization Factor (QF) interference experiment. By testing with QF parameters of 70 and 90, we introduced MJPEG compression artifacts, as depicted in Figure 2. FDIN consistently outperforms existing methods, demonstrating resilience even when video quality is degraded due to compression.\nWhile MJPEG compression significantly impacts spatial textures, its effect on frequency domain features, especially those highlighting inpainting traces, remains limited. FDIN's ABSR and FFCA modules are well-equipped to enhance critical frequency domain features, allowing the model to retain high detection accuracy even under compression. This ability to leverage frequency insights ensures that FDIN effectively mitigates the adverse effects of compression artifacts, offering robust performance in challenging, real-world scenarios."}, {"title": "E. Ablation Study", "content": "We conducted an ablation study to assess the impact of FDIN's key components, as outlined in Table III.\nBase: The core model with a 3D ResBlock encoder-decoder, pre-trained on Kinetics [19].\n+ FFCA: Adding the FFCA module enhances global frequency feature detection, improving mIoU by up to 5%.\n+ ABSR: Introducing ABSR provides adaptive frequency selection, offering up to 7% gains in mIoU.\n+ F&A: Combining FFCA and ABSR achieves further improvements by effectively integrating both shallow and deep frequency features.\nFinal: The complete model, further enhanced by YouTube-VOS [20] pre-training, achieves the highest mIoU and F1 scores, showing up to 13% improvement over the base model. This ablation study highlights the significant performance boost provided by the combined use of FFCA and ABSR, illustrating their importance in accurate video inpainting detection."}, {"title": "F. Qualitative Results", "content": "Figure 3 shows a qualitative comparison on the DAVIS 2016 dataset, illustrating FDIN's superior performance in detecting inpainted regions compared to other models. The masks generated by FDIN align closely with the ground truth, highlighting its ability to accurately localize inpainted areas, even in challenging scenarios. This effectiveness stems from FDIN's integration of frequency-domain insights, enabling precise and robust detection of manipulations."}, {"title": "IV. CONCLUSION", "content": "This paper presents the FDIN for video inpainting detection, which leverages frequency domain analysis to achieve state-of-the-art performance on benchmark datasets. Our approach integrates ABSR and FFCA modules, enabling accurate detection of inpainting artifacts. Experimental results demonstrate significant improvements in detection accuracy, highlighting FDIN's potential to enhance multimedia security and address ethical concerns surrounding video manipulation."}]}