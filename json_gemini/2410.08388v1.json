{"title": "GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes", "authors": ["Maximus Powers", "Umang Mavani", "Harshitha Reddy Jonala", "Ansh Tiwari", "Hua Wei"], "abstract": "The detection of bias in natural language processing (NLP) is a critical challenge, particularly with the increasing use of large language models (LLMs) in various domains. This paper introduces GUS-Net, an innovative approach to bias detection that focuses on three key types of biases: Generalizations, Unfairness, and Stereotypes. GUS-Net leverages generative AI and automated agents to create a comprehensive synthetic dataset, enabling robust multi-label token classification. Our methodology enhances traditional bias detection methods by incorporating the contextual embeddings of pre-trained models, resulting in improved accuracy and depth in identifying biased entities. Through extensive experiments, we demonstrate that GUS-Net outperforms state-of-the-art techniques, achieving superior performance in terms of accuracy, F1-score, and Hamming Loss. The findings highlight GUS-Net's effectiveness in capturing a wide range of biases across diverse contexts, making it a valuable tool for social bias detection in text. This study contributes to the ongoing efforts in NLP to address implicit bias, providing a pathway for future research and applications in various fields. The Jupyter notebooks used to create the dataset and model are available at https://github.com/Ethical-Spectacle/fairly/tree/main/resources.", "sections": [{"title": "1 Introduction", "content": "The detection of bias in natural language processing (NLP) [13] is an important task, particularly with the increasing use of large language models (LLMs) [33] in domains like education [18] and business [29]. Bias can significantly influence public perception and decision-making, often subtly reinforcing stereotypes or propagating discriminatory practices. While explicit bias, which refers to clearly expressed prejudice or favoritism, is easy to define, implicit bias involves more subtle and often unconscious associations or attitudes. Therefore, identifying and mitigating implicit bias in the text is challenging: what is perceived as biased can vary greatly depending on the context, including the perspectives of viewers and speakers. For example, consider the phrase \u201chard-working immigrants\". To some, this phrase may appear positive, acknowledging the effort and diligence of immigrants. However, from another perspective, it might be perceived as implicitly biased, suggesting that immigrants are expected to work harder than others to be valued or accepted. This subtle implication can be seen as reinforcing a stereotype that separates immigrants from native citizens, placing an undue burden of proof on their worthiness. This subjectivity underlines the complexity of implicit bias detection, making it a critical area of research within NLP [14, 28, 25]."}, {"title": "2 Related Works", "content": "The detection of bias in natural language processing (NLP) is a critical area of research, particularly given the increasing use of large language models (LLMs) across various domains [7, 9, 33, 18, 8, 29, 30]. Traditional techniques for bias detection often rely on human annotators to label datasets. While this approach has been instrumental in creating foundational resources, it is inherently limited by the annotators' subjective perspectives, which can introduce their own biases into the annotation process [25, 26]. This limitation often leads to a narrow understanding of bias, especially in regard to implicit biases that are subtle and context-dependent [13, 14, 28]."}, {"title": "2.1 Ethical Dataset Construction", "content": "The construction of ethical datasets for bias detection is essential for ensuring comprehensive and fair analyses. Existing datasets often suffer from limitations in scope, failing to encompass the broad spectrum of biases and perspectives necessary for effective bias detection. For example, the Dbias model [26] utilized the MBIC dataset, which consists of a relatively small number of sentences, restricting the model's ability to generalize across different domains and types of bias. Although the NBias framework [25] expanded the use of named-entity recognition (NER) by introducing the entity \"BIAS\" it still primarily addressed explicit biases and overlooked the structural elements of implicit bias, such as stereotypes and generalizations.\nMoreover, studies that emphasize robust annotations often rely on human judgment, which can lead to a lack of diversity in viewpoints necessary to capture the nuance of implicit bias [27]. This reliance on human annotators may also perpetuate the biases present in society, resulting in datasets that do not adequately represent the full range of perspectives. Thus, there is a pressing need for more diverse and comprehensive datasets that can capture implicit biases in language."}, {"title": "2.2 Bias Detection", "content": "Traditional methods typically focus on explicit bias, which is easier to define and identify, while neglecting the subtler forms of bias that may influence public perception and decision-making. Implicit bias can manifest through word choice, framing, and the omission of certain viewpoints, making it challenging to detect using conventional approaches [13, 14].\nExisting frameworks, such as Dbias and Nbias, have made strides in bias detection but still focus primarily on explicit biases, leaving a gap in the understanding of how implicit biases operate [26, 25]. Additionally, the datasets used for these frameworks often lack the necessary diversity of perspectives, limiting their effectiveness in identifying implicit biases. In contrast, our proposed approach leverages generative AI and automated agents to construct a more comprehensive dataset. By utilizing synthetic data generated by these agents, we enhance the training of the pre-trained model BERT for multi-label token classification. This innovative methodology not only improves the specificity and depth of bias detection but also addresses the limitations of existing datasets, paving the way for more accurate and nuanced understanding of biases in various texts."}, {"title": "3 Methodology", "content": "With modern synthetic training data labeling techniques, we can create a comprehensive dataset encapsulating our novel entities"}, {"title": "3.2 Proposed Model", "content": "To efficiently and accurately identify social biases in text, we propose a multi-label token classification model. As shown in Figure 4, we fine-tune the pre-trained model bert-base-uncased [10] for multi-label classification [23].\nRather than implementing a single entity to capture all definitions and nuances of \"bias,\" our model achieves more granular and accurate insights with entities chosen for their individual semantic clarity and collective comprehensiveness of social bias. By utilizing"}, {"title": "3.2.1 Model Architecture", "content": "The GUS-Net model is a multi-label token classification system designed to identify social bias across three categories: generalizations, unfairness, and stereotypes. It outputs seven labels, allowing the model to capture the nuanced structure of biased language. These labels facilitate the identification of individual bias categories, and provide flexibility for overlapping and nested biases."}, {"title": "Input Processing", "content": "We tokenize all sentences using the pre-trained BERT tokenizer, ensuring that token splits (such as sub-words) inherit the correct entity labels from the parent word. Each text sequence is padded to a maximum length of 128 tokens to ensure consistent input size. Since sentences are rarely longer than 128 tokens, we reduced the BERT input size from the default 512 tokens, representing a 16x reduction in self-attention elements to be processed. Correspondingly, the NER tags were converted into a (128, 7) dimensional vector, where each of the seven elements represents a binary label (0 or 1) for the respective entity type. These vectors were padded with -100 values up to the full sequence length of 128 tokens, with the -100 values being ignored during the loss calculation."}, {"title": "Model Fine-Tuning", "content": "We fine-tune the pre-trained transformer model, specifically bert-base-uncased, due to its ability to capture deep contextual relationships between words, which is crucial for identifying implicit biases [10]. BERT's bidirectional nature allows it to process the entire input sequence, ensuring that each token is evaluated in the context of its surrounding words. This feature is particularly valuable in detecting subtle and complex forms of social bias.\nThe model is implemented using the Hugging Face transformers library. Input text is tokenized with the pre-trained BERT tokenizer, which breaks down sentences into sub-word units while preserving word boundaries. Each token sequence is padded to a length of 128 tokens, and the corresponding labels are mapped to ensure proper alignment. By reducing the input size from the default 512 tokens to 128, we optimize the model's computational efficiency without sacrificing performance for typical sentence lengths in the dataset."}, {"title": "3.2.2 Loss Function", "content": "Given the significant class imbalance in our dataset, where certain entities like STEREO are underrepresented compared to frequent entities like O, we employed focal loss to address this challenge [22]. Standard binary cross-entropy (BCE) tends to focus on the majority class, leading to poor performance on rare classes. In this paper, we use focal loss calculated over all tokens, defined as:\n$FL(p_t) = -a_t(1 - p_t)^\u03b3 log(p_t)$\nwhere $p_t$ is the predicted probability for the true class; $(1 - p_t)^\u03b3$ reduces the impact of well-classified examples, helping the model prioritize rare or difficult examples; $a$ balances the contribution of positive and negative samples, ensuring underrepresented entities receive more focus; $\u03b3$ down-weights well-classified examples, allowing the model to concentrate on harder-to-predict instances."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Settings", "content": "In this paper, we performed a token-level multi-label classification task, where each sentence is annotated with one or more labels to facilitate the identification of biased entities across token sequences. Token-level classification is essential for bias detection because biases can often be nuanced and context-dependent. By focusing on individual tokens, we can capture subtle implications and associations that may be overlooked in a sentence-level analysis. Moreover, we opted for multi-label classification instead of multi-class classification to better reflect the complex nature of biases, that may fall into one or many entity classes. A single-class approach, like the one proposed for Nbias, would limit our ability to capture the diversity of biases present in the text, as a statement may contain multiple biases simultaneously."}, {"title": "Metrics", "content": "To evaluate the model's performance, we utilized a variety of metrics to assess its ability to accurately identify biased entities:"}, {"title": "Hamming Loss", "content": "This metric measures the fraction of incorrect labels over all tokens in the sequence, accounting for multi-label classification. It is defined as:\nHamming Loss = $\\frac{1}{L}\\sum_{i=1}^{L}1(y_i \\neq \\hat{y_i})$\nwhere L is the total number of tokens, $y_i$ is the true label for the i-th token, $\\hat{y_i}$ is the predicted label, and 1 is an indicator function that evaluates whether the true label differs from the prediction."}, {"title": "Precision, Recall, and F1-Score", "content": "These metrics were calculated at two levels: individually for each entity class and as a macro-average across all classes. They are defined as follows:\nPrecision = $\\frac{TP}{TP + FP}$, Recall = $\\frac{TP}{TP+FN}$\nF1-Score = 2 $\\frac{Precision \\cdot Recall}{Precision + Recall}$\nwhere TP denotes true positives, FP denotes false positives, and FN denotes false negatives.\nGiven the imbalanced class distribution in our dataset, we evaluated both the macro-average performance of the model and individual entity-type-level metrics. By treating B- and I- tags as a single entity (e.g., combining B-GEN and I-GEN predictions), we enhance our evaluation of the model's ability to detect the"}, {"title": "4.1.1 Hardware and Environment", "content": "All experiments were conducted on a single NVIDIA T4 GPU with 16GB of memory, hosted on Google Colab, utilizing under 10GB of RAM. The codebase was implemented using PyTorch and the Transformers library, and executed on Ubuntu 20.04 with Python 3.8. We employed pytorch-lightning to streamline the training loops and logging mechanisms."}, {"title": "4.1.2 Hyperparameters", "content": "We trained our BERT-based multi-label token classification model with seven output classes over 17 epochs. The training process utilized a batch size of 16 and an initial learning rate of 5\u00d710-5. The AdamW optimizer with weight decay was implemented, along with a linear learning rate scheduler featuring a warm-up ratio of 0.1. To handle class imbalance, we employed focal loss with a = 0.65 for the I-GEN label. The classification threshold for all labels was set at 0.5. The original dataset was partitioned into training (70%), validation (15%), and test (15%) splits, ensuring similar distributions of biased entity types across these splits."}, {"title": "4.2 Results", "content": null}, {"title": "4.2.1 Overall Performance on Multi-Label Classification for Token-level Bias", "content": "The primary goal of this experiment is to evaluate the performance of our model in accurately identifying biased entities at the token-level within the GUS dataset. By focusing on token-level classification, we aimed to capture occurrences of social bias at the level of individual words and phrases, rather than merely at the sentence level.\nDue to the absence of directly comparable existing token-level work as a baseline for our model, we opted to implement a variant of Nbias, which was the state-of-the-art (SOTA) method designed for token-level classification. We modified Nbias into a multi-label framework and fine-tuned it on the GUS dataset. This adaptation allowed us to examine how well the Nbias architecture could perform in at multi-label classification, even though it was designed for multi-class classification, while also addressing the challenges associated with imbalanced class distributions. In our implementation, we employed focal loss instead of binary cross-entropy to better manage class imbalance. This choice was critical, as our dataset exhibited significant disparities in the representation of biased entities, particularly with respect to the Unfairness class."}, {"title": "4.2.2 Comparison with Human Annoation", "content": "The BABE (Bias Annotations By Experts) dataset [27] is a well-established resource in bias detection, containing a diverse range of biased statements annotated by trained experts. This dataset is valuable as it provides insights into various forms of bias across different demographics and contexts, making it a relevant benchmark for evaluating our model's performance.\nIn this analysis, we aimed to compare the normalized number of biased words per sentence in the BABE dataset with the number of positive (non-\u2018O') label classifications made by our model (GUS-Net). The normalized number of biased words refers to the count of biased words adjusted for sentence length, allowing for a fair comparison across sentences of varying lengths.\nTo obtain the normalized number of biased words, we first filtered the training split of the BABE dataset to include only sentences classified as biased. Since our model labels multiple entity types (GEN, UNFAIR, and STEREO) and the BABE dataset does not distinguish between different forms of bias, we adjusted for imbalance by binning the results and using the minimum number of GUS entities found in each bin. The number of biased words from BABE was then normalized by dividing by the sentence length."}, {"title": "4.2.3 Ablation Study", "content": "We conducted an ablation study to evaluate the impact of different configurations on the model's performance. Table 4 presents the macro-average Precision, Recall, F1-score, and Hamming Loss for the following settings: (1) Our proposed GUS-Net model; (2) GUS-Net without GUS dataset: This configuration relies on an existing corpus, BABE [27]. Since there are no token-level annotations for BABE, we used the same annotation pipeline outlined in this paper. (3) GUS-Net without focal loss: In this configuration, we trained the model using the Binary Cross-Entropy (BCE) loss function."}, {"title": "4.3 Parameter Sensitivity Study", "content": "To identify the optimal focal loss parameters, a and y, we conducted a sensitivity study by testing various values for each parameter while holding the other constant. As shown in Table 5, we evaluated the performance of the model at different a values while keeping y fixed at 2. The results indicate that the best-performing value for a was 0.65, which resulted in improved F1-Scores across all entity types. Table 6 shows the influence of y parameter while maintaining a at 0.65. The results reveal that the macro-average F1-Score remained at 0.80, indicating that this combination of parameters effectively balances sensitivity and specificity across entity types. Overall, the sensitivity study highlights the importance of tuning the focal loss parameters to improve the model's performance in identifying various biases. The optimal values used in this paper (a = 0.65 and y = 2) demonstrate the model's ability to adapt to class imbalances and enhance its performance in detecting biased entities."}, {"title": "4.4 Case Study", "content": "To demonstrate our model's labeling capabilities and generalizability, we present a case study involving religious bias from the GUS dataset. In Figure 6(a), we provide an example of a statement that exhibits religious bias, along with the corresponding labels generated by GUS-Net. Figure 6(b) showcases GUS-Net's outputs for this case study, illustrating its ability to accurately identify and classify instances of religious bias. The outputs are represented visually, highlighting how the model distinguishes between different types of bias, including Generalizations, Unfairness, and Stereotypes. This example indicates the effectiveness of GUS-Net in generalizing across various forms of bias, reinforcing its potential as a robust tool for bias detection in diverse contexts."}, {"title": "5 Conclusion and Discussion", "content": "The proposed GUS-Net model addresses limitations in existing bias detection methods by focusing on the nuanced identification of social biases with semantic categories of generalizations, unfairness, and stereotypes. Moreover, GUS-Net uses a multi-label token classification architecture, based on bert-base-uncased, that allows entities to span multiple tokens and be nested within each other. GUS-Net approaches bias with three detailed entities, offering a more granular and precise detection of social biases. This enables better insights into the structural components of biased language. Our results demonstrate that GUS-Net performs well at classifying tokens as each of the entities, with a notable strength in detecting stereotypes. In sum, GUS-Net contributes the field of bias detection in NLP by incorporating a fine-grained and multi-faceted view of biased language."}]}