{"title": "Neural Decompiling of Tracr Transformers", "authors": ["Hannes Thurnherr", "Kaspar Riesen"], "abstract": "Recently, the transformer architecture has enabled substantial progress in many areas of pattern recognition and machine learning. However, as with other neural network models, there is currently no general method available to explain their inner workings. The present paper represents a first step towards this direction. We utilize Transformer Compiler for RASP (Tracr) to generate a large dataset of pairs of transformer weights and corresponding RASP programs. Based on this dataset, we then build and train a model, with the aim of recovering the RASP code from the compiled model. We demonstrate that the simple form of Tracr compiled transformer weights is interpretable for such a decompiler model. In an empirical evaluation, our model achieves exact reproductions on more than 30% of the test objects, while the remaining 70% can generally be reproduced with only few errors. Additionally, more than 70% of the programs, produced by our model, are functionally equivalent to the ground truth, and therefore a valid decompilation of the Tracr compiled transformer weights.", "sections": [{"title": "1 Introduction", "content": "The transformer architecture [1] is a type of neural network that was originally designed for machine translation but is now also successfully used for modelling many different pattern recognition tasks. This notably includes advanced language understanding [2] but also the modelling of non-sequential data such as images [3]. The transformer architecture differentiates itself from other architectures by using a mixture of self-attention and MLP-layers. It can be used as both a decoder-only variant or in an encoder-decoder configuration where the encoder augments the decoder forward pass using cross-attention.\nInterpretability is a subfield of machine learning that is concerned with the problem of understanding the internals of neural networks [4]. Especially the interpretability of transformer-based models has seen growing attention in the past few years. While some progress has been made, e.g. [5,6,7], transformer neural networks are still largely regarded as black boxes. That is, interpretation relies on manual work which makes it hard to scale (for instance, manually translating billions of model weights and activations into human-readable descriptions is hardly feasible even if one knew how to do so). The fact that current state-of-the-art systems are not interpretable, means that they could have unacceptable failure modes, which may only be discovered after deployment."}, {"title": "2 Pairs of RASP Code and Transformer Weights", "content": "This section describes how we generate random but functional RASP code and how these RASP programs are then filtered and translated into transformer weights. This development of a novel dataset consisting of pairs of RASP code and the corresponding transformer weights is crucial for training the proposed model.\nWe use Algorithm 1 to create random, compileable RASP programs. This algorithm processes the primitives rasp.tokens and rasp.indices using the RASP functions Select(), Aggregate(), SelectorWidth(), Map() and SequenceMap() in a way that accounts for the compatibility of functions and the different datatypes.\nWe achieve this by basing the algorithm around a pool of available inputs, which is, in turn, initialized containing only the two primitives rasp.tokens and rasp.indices (Line 2 of Algorithm 1)"}, {"title": "3 The Transformer Decompiler Method (TraDe)", "content": "The formal representation of the RASP code (of the five component functions of the RASP code), into which the Tracr transformer weights are translated, is a crucial step in our procedure. It might be possible to use standard text tokenization [12] for this vectorization task. However, this would require the model to learn to distinguish valid RASP code from a very large space of possible outputs. Furthermore, since it is necessary to reverse the vectorization process, the application of commonly used methods for the vectorization of graphs, such as message passing [13], is not directly applicable to the computational graphs of the RASP program.\nBased on these considerations, we employ a series of one-hot-encoded vectors (four per line of RASP code) to vectorize RASP code. Each series represents a specific part of the line, viz. the function and the three possible inputs to this function. Depending on the function, these numbers are interpreted differently. For instance, if the first vector of a line represents the function SequenceMap(), the next vector will be interpreted as the one-hot-encoded position of the lambda in the list of lambdas that produces one output from two inputs. In Fig. 4, the process of RASP vectorization is visualized\nIt is also necessary to bring the weights of the Tracr transformer into a form in which they can be fed into the decompiler. This task is not trivial as transformer weights are not organized sequentially. However, as demonstrated by vision transformers [3], the transformer architecture can also process sequences of tokens arranged sequentially, despite their original non-sequential configuration. To retain some of the structure of the transformer, we use a matrix-based tokenization4. That is, each token corresponds to a weight matrix in the compiled transformer. The matrix is flattened and concatenated with a small vector"}, {"title": "3.2 Decompiler Model", "content": "Our aim is to solve the problem of translating from the modality of transformer weights to the modality of RASP code. To this end, we adapt an existing system that also translates between modalities; namely the Whisper speech-to-text models by OpenAI [14]. Similar to Whisper, we employ an encoder-decoder transformer architecture. The encoder looks at the model input, in our case the tokenized transformer weights, and guides the decoder which produces the next RASP token based on all of the previously produced ones."}, {"title": "4 Experimental Evaluation", "content": "To evaluate our model, we generate an additional dataset of 1,000 programs, independent from the original 533K samples used for training. We optimize the hyperparameters of our model on one Nvidia GTX 3090. Note that the optimization of hyperparameters is based on the token-accuracy on a validation set of 28K samples. Token-accuracy refers to the fraction of output tokens that are correct relative to the total number of tokens. That is, every output token of our model is compared with the token at the corresponding position in the program that the transformer weights were compiled from. We are aware that the usefulness of this token-accuracy remains unclear for practical applications. However, it seems plausible that when a majority of the tokens are correct, this allows for the extraction of useful features from the RASP code, like for instance, the causal flow through the network.\nFirst, we evaluate the trained decompiler model on our independently generated test set in the non-autoregressive mode, in which it predicts the next token based on the ground-truth prefix."}, {"title": "5 Conclusion", "content": "To date, there are no general solutions available for the automatic interpretation of neural network models. In the present paper, we suggest that the Transformer Decompiler Method (TraDe) could serve as an approach to address this issue. In an empirical evaluation we show that TraDe enables a model to interpret the weights of other, smaller transformer neural networks and translate them into a more human-readable modality with useful accuracy. Our work thus represents a significant step towards an end-to-end framework for better interpretability.\nHowever, there are still many limitations standing in the way of any practical application of the proposed concept. For instance, the transformer weights resulting from the Tracr compilation process are very different to transformer weights resulting from an optimization using stochastic gradient descent. The former is very sparse (except for certain structured elements), while the latter is very unstructured and dense. Moreover, the best-performing variant of the decompiler model is about three orders of magnitude larger than the models it is capable of decompiling (in terms of parameters). If this ratio is not significantly reduced, the application to modern, large transformer models will remain impossible. Lastly, though the step from weight matrices to RASP code is a large improvement in terms of interpretability, it would be wrong to call the"}]}