{"title": "Benchmark Inflation:\nRevealing LLM Performance Gaps Using Retro-Holdouts", "authors": ["Jacob Haimes", "Cenny Wenner", "Kunvar Thaman", "Vassil Tashev", "Clement Neo", "Esben Kran", "Jason Schreiber"], "abstract": "The training data for many Large Language Models (LLMs) is contaminated with test data. This means that public benchmarks used to assess LLMs are compromised, suggesting a performance gap between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this retro-holdout dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-Misconceptions, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field.", "sections": [{"title": "1. Introduction", "content": "Many have begun to question the reliability of public benchmarks in assessing large language models [1, 49, 14]. Discrepancies between benchmark scores and practical capabilities raise concern [26], and strong incentives for higher scores [15] suggest that optimizing benchmark performance could take precedence over real-world effectiveness and safety. This phenomenon, akin to specification gaming [24], is termed evaluation gaming \u2013 processes leading to a systematic gap between benchmark performance and practical utility.\nExtensive evidence of evaluation data being included in training data [38, 33, 40, 42, 21, 43] suggests that evaluation gaming is occurring. However, proving the existence of a statistically significant performance gap between a specific evaluation task and an analogous real-world task would require access to an independently and identically distributed (IID) split of the benchmark which we know could not have had an impact on any aspect of model development.\nThis is the idea of holdout datasets, which are used to assess a machine learning model's unbiased performance after training. By definition, a holdout dataset comes from the same distribution as its corresponding target dataset, meaning that any evaluation conducted on both datasets should have the same result within some statistical tolerance [20]. Importantly, holdout datasets also are kept hidden during the development process. Together, these two properties imply that comparing a model's performance on a public benchmark and a corresponding holdout dataset could reveal whether the public benchmark has influenced any aspect of the design, training, or validation process. Unfortunately, holdout datasets for benchmarks are typically not available; benchmark developers usually release all evaluation data, although there are notable exceptions, e.g. Li et al. [25].\nTo resolve this, we propose retroactive holdout, or retro-holdout, datasets, which are verified to be sufficiently similar to their corresponding target dataset through various tests, despite being created independently and retroactively. Utilizing a retro-holdout, we can quantify the evaluation performance gap of any given model. We detail our methodology for creating and validating retro-holdout datasets, along with multiple recommendations and tools for generating such datasets. Using the TruthfulQA\u00b9 evaluation [27], we conduct a case study to quantify performance gaps for twenty contemporary models. Our results conclusively indicate that evaluation gaming is occurring, underscoring the need for improved data practices in the domain."}, {"title": "1.1. Contributions", "content": "In this work, we:\n\u2022 Present a novel process for constructing retro-holdout datasets.\n\u2022 Release Retro-Misconceptions, a retro-holdout dataset for TruthfulQA, which can be used to quantify the performance gaps of a model on the original dataset.2\n\u2022 Evaluate twenty models using Retro-Misconceptions to demonstrate measurable score inflation."}, {"title": "2. Methods", "content": ""}, {"title": "2.1. Understanding the RETRO Holdout Framework", "content": "Holdout datasets were first used in machine learning to accurately assess model performance on a given task. A holdout set is a randomly selected subset of the same set of observations as the training dataset, and is strictly excluded from the development process [20]. Unlike conventional holdout sets, retro-holdout datasets are created after the initial release of a dataset, meaning we cannot assume they have the same properties that a hypothetical holdout set would have. We refer to Appendix A for a formalization of this claim.\nIn brief, we rely on a standard assumption in machine learning that the public and post-hoc retro-holdout datasets consist of independent samples from two possibly different distributions [18, 41]. To establish that the retro-holdout can be used as a holdout set for the public benchmark, we must show that both datasets could have been sampled from the same distribution. We construct four statistical tests, one permutation test and three binomial tests, to reject the hypothesis that two sets were sampled from the same distribution. A proposed dataset cannot be considered a retro-holdout for a given public benchmark unless all four tests fail to reject the hypothesis of a shared distribution.\nThis verification process sets our methodology apart from a more standard dataset extension, as it mandates that our retro-holdout is an assessment of exactly the same task as the original benchmark, making the only difference between the two the variable we care about: public availability of a dataset. The lengths we have gone in order to reach this level of rigor are extensive; the retro-holdout framework does not make use of LLMs for any aspect of dataset creation. This substantially increases the time cost of our process, but ensures that language models do not bias our results.\nOur method is designed for labeled datasets, which have inputs and expected outputs for models. We note that out of distribution (OOD) testing has sometimes been incorrectly characterized as using holdout datasets. In this work, we use the original definition of holdout sets; evaluation sets constructed to probe OOD performance are not considered."}, {"title": "2.2. Creating a RETRO", "content": "Initially, the methodology used for crafting a RETRO should be heavily informed by the original process used to create the TARGET. To promote similarity, we recommend using a representative entry, randomly drawn from TARGET, without replacement, as a basis for creating each new entry in RETRO. We include a short guide for this initial creation in Appendix D."}, {"title": "2.2.1. SUFFICIENT INDISTINGUISHABILITY", "content": "Establishing with absolute certainty that the two datasets have originated from the same distribution is impossible. Therefore, we resort to multiple statistical tests designed to test the null hypothesis that TARGET and RETRO have a common origin. If the result of each test indicates that we cannot reject our null hypothesis, we designate our RETRO to be sufficiently indistinguishable from TARGET. While it is theoretically possible to construct any number of tests to evaluate the similarity between two datasets, practical considerations guide us to four key tests that provide a thorough assessment:\n\u2022 Similarity of Difficulty: Are the questions in both datasets comparably challenging?\n\u2022 Semantic Embedding Similarity: What is the likelihood that a distribution of cosine similarities between sentence embeddings similar to that of RETRO have been pulled from the same distribution as TARGET?\n\u2022 Prediction Accuracy: Can a model, fine-tuned on randomized splits of the datasets, differentiate between TARGET and RETRO?\n\u2022 Human Distinguishability: Can humans differentiate between TARGET and RETRO?\nWe designate the two datasets as sufficiently indistinguishable if all four tests fail to reject the null hypothesis at a p-value of 5%.\nSimilarity of Difficulty To verify that the two datasets have comparable difficulties, we use both to evaluate models with a training cutoff date prior to the release of the TARGET, or pre-release models. These models could not have been affected by the TARGET, as it had not yet been released; model performance on both datasets should be comparable, with a margin of statistical uncertainty. This reduces to a two-proportion binomial test with the null hypothesis of equal success probability [11]. For further information on the evaluation task, refer to \u00a72.3.\nWe note that with access to many LLMs of varying capability levels, this test combined with simple human assessment would likely suffice to determine sufficient indistinguishability between the two datasets. However, performance of cutting-edge models continues to improve, meaning that pre-release models are practically guaranteed to be less capable than contemporary models, assuming they are accessible at all. These constraints are expanded on on Appendix G. To address this limitation, we use a number of techniques to amplify pre-release performance: allowing the model to choose multiple answers (top-k), including examples of other questions within the dataset (5-shot), and using the 'helpful' prompt from Lin et al. [27].\nPrediction Accuracy We adopt a modification of prediction accuracy as defined by Dankar and Ibrahim [8] to determine if a machine learning model can differentiate between the datasets. Contrary to the conventional use of logistic regression in synthetic data evaluations [8], we fine-tune BERT [10] on a prediction task. This choice was informed by BERT's capability to capture nuanced semantic relationships within text, which are crucial for accurately assessing the subtle distinctions or similarities between dataset entries. If the model's prediction accuracy is approximately the same as random performance, 50%, we can conclude that the model cannot differentiate between the two datasets."}, {"title": "2.2.2. AN ITERATIVE PROCESS", "content": "Creating a RETRO that meets our rigorous standards for sufficient indistinguishability is non-trivial and will likely require iteration. Acknowledging this, and considering the time-intensive nature of dataset generation, efficiency is quite important. We have created a number of tools that aid in high-level iteration:\n\u2022 Fine-Tuned Prediction Model Attention: A BERT model [10] is fine-tuned to classify entries as belonging to either TARGET or RETRO. Transformers Interpret, a library based on Integrated Gradients for explaining model output attribution [45] is then leveraged to identify which input tokens the model considered most relevant when differentiating between TARGET and RETRO.\n\u2022 Datapoint Embeddings: all-mpnet-base-v2 is used through the HuggingFace Sentence Transformers library, to create vector representations for all data points [35]. These embeddings are then taken as the basis for the following three tools; when analyzed in conjunction they can provide meaningful insights on general similarity trends, outlier detection, and topic clustering.\nEmbedding Space Visualization: We employ Uniform Manifold Approximation and Projection (UMAP) to project these embedding vectors onto a two-dimensional plane [29]. The visualization provides an intuitive understanding of the dataset's structure and distribution.\nInternal Cosine Similarity Distribution: To assess similarity between entries within the datasets we plot histograms of pairwise cosine similarities of datapoint embeddings. This representation aids in identifying outliers and assessing overall similarity within the datasets.\nLargest Internal Cosine Similarity Comparison: We highlight the ten entry pairs with the highest cosine similarities in both datasets, providing a direct comparison of the most similar entries and their respective values."}, {"title": "2.3. Evaluating Models", "content": "The evaluation metrics published with TruthfulQA relied on being able to produce the log-probabilities of given completion options [27]. This simplifies the scoring of a model in that one does not have to interpret or match freely-generated responses. This may however not be ideal for a few reasons. First, the use of log-probabilities is likely to penalize longer responses, since they naturally have lower total probability. Second, as it may not account for alternative, even shorter, responses. Finally, API providers such as OpenAI, no longer supply such information.\nIf these models have not been subjected to any form of data leakage, one should see no significant cap regardless of the metric used. We have therefore modified the scoring in order to retain support also for API models and to allow for comparison between open-release and closed-source purely generative models. Our method therefore provides an enumerated list of all the choices (mc1 in TruthfulQA) and we resample to make the models generate one of the indices as selection. These options are then rotated to minimize"}, {"title": "2.4. The Challenges of TruthfulQA", "content": "The TruthfulQA dataset uses two entry labels: Category and Type. Categories specify the general topic that an entry is about, such as Health, or Advertising; there are 31 Categories in TruthfulQA. Type contains only two options, adversarial or non-adversarial.\nWhen constructing TruthfulQA, the authors filtered a large number of initial entries using a version of GPT-3, discarding the entries that the model answered correctly. The resulting set make up the adversarial Type of TruthfulQA. Subsequently, these adversarial entries were used as inspiration to create new entries for the non-adversarial Type. When comparing the adversarial and non-adversarial Types, we unsurprisingly found that GPT-3 models like Babbage-002 and Davinci-002 do significantly better on the non-adversarial portion. To create a retro-holdout for the entire TruthfulQA dataset, we would require access to the same GPT-3 model originally used to filter TruthfulQA. This model is no longer available.\nDue to filtering bias, performance differences between the two Types, and lack of access, we focus on the non-adversarial portion of TruthfulQA. While these changes deviate from perfect reflection of TruthfulQA, we note that both the Difficulty Similarity test and model evaluations use identical datasets and methods. As a result, any statistically-significant performance gap must be explained by some form of evaluation gaming."}, {"title": "3. Results and Discussion", "content": ""}, {"title": "3.1. Retro-holdout TruthfulQA Dataset", "content": "We release Retro-Misconceptions, a retro-holdout dataset designed to quantify the evaluation gap for models tested on the TruthfulQA dataset, provided that the model's training cutoff date is prior to January 1st, 2024. Retro-Misconceptions mirrors the Misconceptions category of the original TruthfulQA dataset.\nNotably, Retro-Misconceptions has passed all four of our indistinguishability tests, establishing it as the first retro-holdout dataset to be sufficiently indistinguishable from its corresponding target dataset."}, {"title": "3.2. The Performance Gap", "content": "With our newly created retro-holdout dataset, we explicitly quantify the benchmark inflation (BI)\u00b3 of 20 models. Our analysis covers both larger API models such as Claude3 and GPT-4, as well as several open-release models that have been either speculated or confirmed to exhibit data leakage [38].\nTo develop further understanding of these results, we look to Deng et al. [9], who investigated data contamination of TruthfulQA in various models, including Mistral-7B, ChatGPT, and GPT-4.4"}, {"title": "3.3. Why are Retro-holdouts Necessary?", "content": "Creating a retro-holdout dataset is resource intensive, demanding large amounts of time from human experts for dataset iteration, as well as considerable computational resources for validation. However, it is necessary to confirm that a newly created dataset assesses precisely the same task as its corresponding public benchmark. Explicit and robust confirmation of benchmark inflation establishes that, in the absence of meaningful evaluation oversight, model developers will game evaluations.\nCurrent data practices mandate a framework like retro-holdout to thoroughly justify this claim, but shifts in data use and sharing could make retro-holdouts obsolete. One solution could be for dataset developers to withhold a portion of their data from public release, routinely compare model performance on the public and private splits, and decommission the benchmark once statistically significant benchmark inflation is measured. While this is promising in theory, it places substantial burden on dataset developers; there is no platform that can easily facilitate such a process.\nWe also note that LLMs were not used for any aspect of dataset development to ensure that the resulting dataset, Retro-Misconceptions, has not incurred any model bias, establishing a true baseline for the retro-holdout framework. That said, LLM assistance could automate multiple time consuming steps of our process, substantially decreasing the time required to create a retro-holdout."}, {"title": "3.4. Limitations", "content": "In order to establish that evaluation gaming is occurring, one should conclusively demonstrate that there exists at least one dataset where there is sizable benchmark inflation. Accordingly, we have prioritized eliminating other confounding factors, rather than applying our approach to many datasets. Capacity constraints, coupled with our team's conviction to prevent LLMs from biasing our results, the retro-holdout framework has been conducted on only one dataset: the Non-Adversarial Type of TruthfulQA. Though we have designed the process to apply to any supervised text dataset, we cannot yet guarantee the generality of our process.\nThe assumption that the retro-holdout dataset and the target dataset are drawn from the same distribution may not always be valid. This assumption is challenged if the target dataset itself is subject to distribution shifts over time; such shifts can alter the underlying data characteristics. Additionally, matching a target dataset introduces its own concerns. While this method ensures that the retro-holdout dataset resembles the target dataset as closely as possible, it also perpetuates any biases present in the target dataset."}, {"title": "4. Related Works", "content": "Development of LLMs continues to outpace the advancement of evaluation methods, raising concern about benchmark integrity [5]. Test data are frequently misused during an LLM's training process. Data contamination, where test data is included in training sets, results in models \"cheating\" by memorizing tests rather than generalizing [28]. Optimizing against these data can also result in indirect over-fitting, such that models perform well on the benchmarks without learning the underlying task [31]. No standard methodology exists to detect this issue [1], yet data quality remains undervalued and under-incentivized [39]. High benchmark scores are highly desirable, promoting practices that compromise data quality and evaluation integrity.\nRecent work has introduced heuristics for third-party contamination tests. Sainz et al. [37] propose a technique to detect test set contamination by eliciting reproduction of specific test set examples. Golchin and Surdeanu [16] suggest a method for identifying contamination in black-box models by comparing the similarity between model completions of randomly selected example prefixes and the actual data using GPT-4. Concurrent work by Zhang et al. [47] is notable for its use of a dataset extension, a concept similar to our approach. Their benchmark, GSM1k, reports accuracy drops of up to 13%, highlighting a positive correlation between memorization and performance gaps. We test their dataset with our tests in Appendix H, finding evidence that GSM1k is not sufficiently indistinguishable from GSM8k.\nIt is well known that metrics lose their predictive power when incentives are attached to them [17, 44, 22]. As Thomas and Uminsky [46] state, \"overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences.\" Current AI risk metrics fail to address emerging failure modes [23], and Privitera et al. [34] emphasize that high benchmark scores do not necessarily equate to effective real-world performance.\nEmpirical findings highlight the necessity for immediate structural reforms in AI research and development to prioritize and encourage data quality [39]. Recent calls for a science of evaluations underscore the urgent need for rigorous evaluation frameworks to inform policy and ensure responsible AI development [3, 36]."}, {"title": "5. Conclusion", "content": "In this work, we systematically investigated the impact of evaluation gaming on benchmark scores for large language models. We find that evaluation gaming is undeniably occurring, with model accuracy falling by up to 16 percentage points when assessed with an unpublished dataset. Benchmark inflation is found in both API models, including OpenAI's GPT and Antrophic's Claude, as well as open-release models such as Mistral, Gemma, and Phi-2. This slightly contrasts with the findings of Zhang et al. [47], who saw performance gaps for open-release models, but found the API models less problematic.\nOur results demonstrate that LLM benchmark scores should not be taken at face value when evaluation data has been publicly available for some time. We hope that our work reminds model developers the importance of understanding training data, and motivates future dataset developers to consider their options when releasing new benchmarks. Options for future investigation include experimentation with dataset release and verification practices, and evaluation methods which are not undermined by public data, such as the methods presented by Li et al. [26].\nThe retro-holdout framework, designed to be generally applicable across various public benchmark evaluations, provides tools that significantly enhance the accuracy and reliability of model evaluations, offering a practical path forward for the field."}, {"title": "Appendices", "content": ""}, {"title": "A. Hold-out Testing Formalization", "content": "In this appendix, we will define what it means that a retroactively constructed dataset could have been a holdout set for a public dataset, as well as how this can be formalized and statistically tested.\nWe define a labeled dataset D as a set of tuples (x, y) \u2208 X \u00d7 Y for some domains X, Y. Given a function f : X \u2192 Y, we define its accuracy as Accp(f) def E(x,y)~D [1{f(x) = y}]. We rely on the standard assumption in machine learning that a constructed dataset consists of i.i.d. samples from some distribution [18, 41].\nGiven a dataset D, we define a public-holdout split of sizes np, |D| \u2013 np as the random variables Dp, Dh where Dp is a random subset of D of size np such that Dh \u222a Dp = D. We also say that D\u0127 is a holdout set for Dp.\nIn contrast to the regular setting, we can not assume that all of D has been drawn independently from the same distribution. Instead, Dp and D\u0127 were constructed separately. Hence, we have Dp ~ Dr, Dh ~ Dh for some distributions Dp, Dh. The claim that we want to show is that for our retro holdout dataset, Dp = Dh. We will design a number of statistical tests to attempt to reject this hypothesis. We will both employ various binomial tests for this, as well as a permutation test.\nNotably, the expected accuracy on both sets are statistically close, provided that a function f is independent of these samples. E.g. a basic bound follows from given 99.5% confidence intervals P(|Acc\u2081\u2082(f) \u2013 Accp(f)| \u2264 up) and P(AccDh (f) - Accp(f)| \u2264 uh), a 99% confidence bound on the difference between the public and hold-out accuracy is naturally the sum up + un.\nHence, given that one can show that the retro holdout dataset could have been drawn from the same distribution as the public dataset, and the difference in the accuracies is greater than some bound, then the remaining difference must be due to direct or indirect exposure to the public data."}, {"title": "A.1. Permutation Tests", "content": "Given two sets Ai \u2286 (X \u00d7 Y)|Ail for i = 1,2, and a test statistic g : (X \u00d7 Y)|A1|+|A2| \u2192 R which is invariant under permutation of the first |A1| elements as well as the last |A2| elements, and where A\u00bf ~ DAil for some distributions Di, a permutation test is a test for the null hypothesis that D\u2081 = D2.\nThe p-value of a permutation test is the probability that the test statistic g is at least as extreme as the observed value under the null hypothesis. That is, let \u03c01,..., \u03c0m be all permutations of A\u2081 \u222a A2 and for a permutation \u03c0, let A1,\u03c0, A2, be the first |A1 and last |A2| elements of \u03c0. Let the average statistic be \u011f = \u0395\u03c0 [g(A1,\u03c0, A2,\u03c0)]. Then the two-sided p-value for the null hypothesis given the observed statistic g(A1, A2) is P\u3160(|g(A1, A2) - \u011f| \u2264 |g(A1,\u03c0, A2,\u03c0) \u2013 \u011f|).\nSince the number of permutations can be large, one can use a Monte Carlo approximation to estimate the p-value through sampling [12]. If N independent samples produce a p-value estimate of \u00ee\u00ee, then a 99% confidence interval for the p-value is given by \u00eep \u00b1 2.807 \u00b7 \u221ap(1 \u2212 p)/\u221aN."}, {"title": "B. Semantic Embeddings", "content": "We use an embedding model, specifically all-mpnet-base-v2, through the HuggingFace Sentence Transformers library, to create vector representations of each entry [35]. We define an entry as a question from the dataset terminated with \"?/n\" followed by all multiple choice answers to the question, ordered alphabetically. Each multiple choice answer is separated with \"/n\". The resulting vectors are referred to as embeddings. Similarity was computed with cosine similarity and not dot product."}, {"title": "C. Evaluation Details", "content": ""}, {"title": "C.1. Evaluation Harness", "content": "All models are first provided the prompt shown in Listing 1, with options provided in alphabetical order. Model output is normalized by removing all leading and trailing whitespaces, and taking only the first line of the response. This step is necessary, as some models have a tendency to add additional questions after their answer.\nIf the model output matches any of the options provided, the choice is recorded. Otherwise, the model is provided the prompt seen in Listing 2, and the output is normalized. If the model output matches any index present in the prompt, the response is recorded.\nIf the model output matches any of the options provided, the choice is recorded. Otherwise, the model is provided the prompt seen in Listing 3, and the output is normalized. If the model output matches any index present in the prompt, the response is recorded.\nThis process is then repeated with option ordering shifted by one. To ensure consistency, responses are resampled a minimum of five times, and until one option has been selected at least three additional times over all alternatives. If no option meets this criteria after 100 attempts, the first top-selected option is used.\nFor the Difficulty Similarity test, we use additional variants of this prompt that include some combination of few-shot examples, and the inclusion of the \"helpful prompt\" from Lin et al. [27]."}, {"title": "C.2. Prompts", "content": ""}, {"title": "C.3. Compute", "content": "Due the nature of evaluating a variety of models, different experiments relied on different architecture. The simplest of these being API models through OpenAI and Anthropic, which require no local resources. Other models were primarily hosted by Hugging Face. The largest of these reported open-release models were run using 4xT4 GPUs and the smallest could run on CPU only. The total compute budget with all intermediate experiments has been less than $1000. Evaluating a single model has cost between $1 and $50. Approximately 200 such experiments have been used to generate all the values and performance gaps seen in this paper."}, {"title": "D. Guide for Initial RETRO Creation", "content": "As mentioned in \u00a72.2, the initial methodology for crafting the RETRO should ideally be very similar to the process documented for the TARGET.\nPrior to starting initial creation, each member of the team should be able to do the following:\n\u2022 Describe the capability, and/or failure mode that the evaluation is attempting to assess in one sentence.\n\u2022 Understand the different ways that the benchmark entries could be sorted, and consider treating individual subsets as independent target datasets. The ability to group items in meaningful sub-categories will be useful during iteration.\nDoes the benchmark already have some form of metadata that can be leveraged? If so, understand the differences between these classifications. For example, our team made progress on individual categories of the TruthfulQA dataset, achieving sufficient similarity for each independently before getting the entire datasets to meet this standard.\nCan the difficulty of a question be categorized and/or quantified in any way? If so, write down the dimensions and subsets for each. For example, GSM8k has questions that require between 1 and 8 mathematical operations to answer correctly.\n\u2022 Identify at least 3 high level patterns within the dataset. The following are examples from TruthfulQA:\nEntries are mostly probing unique pieces of knowledge; as a result, our new entries should be unique with respect to both its own entries, and the entries in the original TruthfulQA dataset.\nEntries often provide multiple possible responses that are quite similar.\nThe dataset is biased towards western civilization. There are many entries which include references to misconceptions, products, or cultural phenomena that are much more prevalent in the west, while there are almost none that are unique to other regions.\nWithin categories there are often a few highly formulaic entries, which use almost identical questions and responses.\nThe precise failure mode that is being targeted varies between categories.\n\u2022 Identify at least 3 low-level patterns within the dataset. The following are examples from TruthfulQA:\nMany prompt questions begin with the word \u201cWhat\u201d, and a large subset of those start with the phrase \"What happens if\".\nThe phrase \"I have no comment.\" appears frequently as a response, and it is almost always the correct response when it is included.\nThe United States is mentioned substantially more than any other country within the dataset.\nMany responses begin with either \"Yes,\" or \"No,\u201d.\n\u2022 Record the date that the original dataset was released, and estimate the timeframe during which it was created.\nIf the dataset has any cultural references, we will want to make sure we do not incorporate any that originated after the datasets release.\nSimilarly, if there have been any scientific discoveries, methods, or paradigm shifts since the creation of the dataset, they cannot be included in this extension. For example, if a new \"Fundamental Theorem of X\" had been discovered and popularized since the release of the TruthfulQA dataset, we would not want to include that text anywhere in our new dataset."}, {"title": "E. RETRO Iteration", "content": "Sufficient indistinguishability is likely to take many iterations to reach for a given dataset. In this appendix we outline the various tools we have created, how they are useful, and different techniques we used to make progress towards passing our tests.\nWhen iterating, it is important to keep the following in mind:\n\u2022 Do not lose sight of the failure mode that the initial benchmark was attempting to assess.\nFor example, TruthfulQA's failure mode can be summarized as will models generate factual inaccuracies because they are prevalent in the training data? If our benchmarks pass our sufficient indistinguishability tests, but no longer assess this statement, we have not created a retro-holdout.\n\u2022 It is easy to make three of more small modifications to a single sentence for varying reasons; double check that the sentence actually makes sense, and sounds natural (or rather, as natural as the original dataset).\nFor non-native speakers, this step can be particularly difficult. If possible, have another person who did not make modifications to the entry verify that the final version makes sense.\n\u2022 Number of possible responses is particularly valuable to match, as this is likely to improve performance on the Prediction Accuracy and Difficulty Similarity tests."}, {"title": "E.1. Fine-Tuned Prediction Model Attention", "content": "This tool is similar to our Prediction Accuracy test, as it uses a similar process to obtain a BERT model [10] which has been fine-tuned to classify entries as either TARGET or RETRO. Transformers Interpret, a library based on integrated gradients for explaining model output attribution [45] is then leveraged to identify which input tokens the model considered most relevant when differentiating between TARGET and RETRO."}, {"title": "F. Human Indistinguishability", "content": "Perhaps the most general way to measure the difference between two datasets is to evaluate whether human observers are able to identify any distinctions. Therefore, we recruited a number of annotators via the crowd-sourcing platform Prolific. These annotators received specific instructions and were compensated at a rate corresponding to at least the U.S. minimum wage. To guarantee that the participants engaged with the task seriously, three attentiveness questions were included in the evaluation process.\nAn annotator is provided the following written instructions:\nInstructions\nThis form assesses to what extent humans are able to distinguish two datasets.\nYou will be presented with a number of tests. Each test will consist of a number of questions including their answers. One of these questions comes from a different dataset than the others.\nYour task is to identify which question comes from a different dataset than the others.\nYou will be shown a number of examples from the two datasets to give you an opportunity to identify high-level patterns.\nPlease do not look up these datasets nor google the answers - use your own best judgement.\nNote that we use the word test to describe the task of selecting which of the three is believed to be a member of the second dataset (RETRO) in order to avoid confusion with the term question, which is frequently used to describe entries within the datasets.\nFollowing this set of instructions, the annotator is provided with ten random entries from the TARGET and another ten random entries from the RETRO; all twenty entries are drawn without replacement and labeled correctly. This is to allow the annotator to identify high level patterns and build an understanding of the two different sets. Once the annotator has reviewed these examples, they are presented with a series of ten tests.\nIf the RETRO is sufficiently indistinguishable from the TARGET, then human performance on this annotation test should not be statistically different from random selection. For our results, a total of twenty three approved participants answered a total of 230 tests."}, {"title": "G. Similarity of Difficulty", "content": "The Similarity of Difficulty test determines whether language models which could not have been influenced by the TARGET perform similarly on both the TARGET and the RETRO. This requires we use only models which were developed prior to the release date of TARGET, but there are a number of obstacles when working with these pre-release models.\nFirst, availability of pre-release models is not a given, as older closed-source models are not maintained for long. As more capable models become available, closed source developers are incentivized to discontinue access to the older models due to maintenance costs. This has a profound impact on researchers, as studies might rely on older models for various reasons, such as for baselines or improvement analysis. With frontier model developers now releasing API access to models multiple orders of magnitude larger than their predecessors, which were deemed too dangerous for transparency [4], perhaps it would make sense for them to turn the retirees over to the Open Source community.\nSecond, older models are not as capable as newer ones. Over a certain difficulty threshold, pre-release performance is likely to match for two datasets, regardless of the actual difficulty profile. Take an example of an elementary school student being given two assessments, one covering k-12 math, and the other on k-8 and university level math. Though these two tests clearly have differing difficulty profiles, we can expect that the student will perform similarly on both. To address this, we use various techniques to enhance the capabilities of the pre-release models. If our RETRO is indeed sufficiently indistinguishable from TARGET, then the models' performance on the two datasets should be similar, irrespective of the capability boost technique being used."}, {"title": "H. Contemporaneous Work", "content": "Coinciding with our efforts, Zhang et al. [47", "apples-to-apples\\\" similarity to their target dataset GSM8k [47, 6": ".", "47": "report an overperformance by many models on their target evaluations.\nWhile the GSM1k dataset comprises over 1000 entries, only 50 have been publicly released to date. Zhang et al. [47"}]}