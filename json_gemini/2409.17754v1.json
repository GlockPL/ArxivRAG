{"title": "Byzantine-Robust Aggregation for Securing Decentralized Federated Learning", "authors": ["Diego Cajaraville-Aboy", "Ana Fern\u00e1ndez-Vilas", "Rebeca P. D\u00edaz-Redondo", "Manuel Fern\u00e1ndez-Veiga"], "abstract": "Federated Learning (FL) emerges as a distributed machine learning approach that addresses privacy concerns by training AI models locally on devices. Decentralized Federated Learning (DFL) extends the FL paradigm by eliminating the central server, thereby enhancing scalability and robustness through the avoidance of a single point of failure. However, DFL faces significant challenges in optimizing security, as most Byzantine-robust algorithms proposed in the literature are designed for centralized scenarios. In this paper, we present a novel Byzantine-robust aggregation algorithm to enhance the security of Decentralized Federated Learning environments, coined WFAgg. This proposal handles the adverse conditions and strength robustness of dynamic decentralized topologies at the same time by employing multiple filters to identify and mitigate Byzantine attacks. Experimental results demonstrate the effectiveness of the proposed algorithm in maintaining model accuracy and convergence in the presence of various Byzantine attack scenarios, outperforming state-of-the-art centralized Byzantine-robust aggregation schemes (such as Multi-Krum or Clustering). These algorithms are evaluated on an IID image classification problem in both centralized and decentralized scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of Machine Learning (ML) into modern technology has significantly transformed various sectors, particularly with the proliferation of Internet of Things (IoT) devices [1]. This growth has led to an exponential increase in the generation and distribution of big data across devices, highlighting both new opportunities and challenges for advanced data processing techniques [2]. The classical approach to handling this amount of data has several disadvantages in terms of security, privacy, and scalability, as all this data (and the trained algorithm) is stored and processed in a large centralized datacenter [3], [4].\nIn response to these challenges, Federated Learning (FL) has been proposed as an alternative paradigm that allows machine learning algorithms to work without the centralization of data [5]. FL leverages the computational capabilities of end devices, enabling them to train their own ML models with their local data. A central server then merges all the locally trained models from the end devices to compute a global model [6]. This method enhances privacy by design, as it does not expose all data at a single location, while utilizing the processing capabilities of the devices. However, traditional federated learning still relies on a central server to manage the learning algorithm process, which can be a target for privacy and security attacks by malicious attackers [7], among other ongoing challenges.\nThe concept of Decentralized Federated Learning (DFL) extends FL principles by eliminating the central server, thereby mitigating the risks associated with a single point of failure and enhancing privacy by avoiding a centralized coordination of the learning process [8]. This decentralized approach promotes greater resilience, allowing the learning process to continue even if some nodes fail or go offline, thus improving the robustness and fault tolerance of the overall network [9].\nHowever, while the decentralization of these learning environments offers benefits such as improved scalability, it also introduces new challenges, including increased vulnerability to security threats. The absence of a central server complicates the implementation of uniform security-preserving algorithms and makes the system more susceptible to attacks such as Byzantine failures [10], [11]. Notably, Byzantine failures occur when a group of Byzantine clients attempts to disrupt the proper learning process, e.g., by altering their updates to degrade other local model's performance (known as Byzantine attacks [12]). Additionally, the heterogeneity among nodes, with varying computational capabilities or data quality, can lead to inconsistencies and reduce the effectiveness of the learned model [13].\nDespite the potential of the DFL paradigm, research on securing such environments is still limited, with most existing works focusing on centralized FL approaches [14]. Many of these approaches utilize Byzantine-robust aggregation algorithms designed to mitigate/filter malicious attacks from adversarial nodes in the central server [15], [16]. This raises the question: are these algorithms proposed for centralized scenarios equally efficient in decentralized environments?\nIn light of these challenges, we introduce an innovative Byzantine-robust aggregation algorithm designed to ensure the proper performance of Decentralized Federated Learning environments, coined WFAgg, something that, to the best of our knowledge, has not been proposed before. This algorithm is designed to filter and mitigate Byzantine attacks by using different"}, {"title": "II. BACKGROUND", "content": "Originally introduced by Google in 2016 [5], the classical version of federated learning involves each client maintaining a local model trained with its own training dataset, while a central aggregator (or parameter server) maintains a global model that is updated through the local models of the clients. In depth, centralized FL proceeds as follows [17] (in a specific round): (i) the server sends the current global model parameters to all client devices, (ii) each client updates its local model by performing one or several steps of an optimization algorithm (in the literature, the most common is Stochastic Gradient Descent) using the current global model parameters and their local training datasets, then, (iii) devices send their updated local models back to the server where (iv) the server aggregates these local models from the clients using a specific aggregation rule to compute a new global model for the next round. The most recognized aggregation rule in FL field is Federated Average FedAvg. This process is carried out in multiple communication rounds, an iterative process that continues until the global model reaches the desired level of accuracy.\nOf particular interest in this work is the novel paradigm of Decentralized Federated Learning which, despite not being the predominant approach in this field, introduces certain features that make it a promising alternative. In DFL, various nodes/clients collaborate and train their local models collectively without the necessity of a central server [18], [19], distributing the aggregation process among the nodes of the learning network. This eliminates the need to share a global model with all clients [20].\nOne of the key advantages of DFL is promoting scalable and resilience environments, allowing nodes to join or disconnect from the learning network based on the power resources of the devices. Unlike centralized approaches, which can become bottlenecks or single points of failure, DFL distributes the workload over the entire network, improving fault tolerance and making the system more robust against attacks [21]. Another significant advantage of DFL over centralized approaches is the reduction in communication resources and the decrease in high bandwidth usage due to the elimination of intermediate model/gradient transmissions between clients and the server [8]. This is particularly relevant in environments with a large number of interconnected devices.\nDespite the significant potential of DFL, research on this topic remains sparse, indicating it is an emerging field. This scenario must confront new challenges absent in centralized approaches due to the decentralized learning process and decision-making across numerous devices. For example, designing communication protocols that define how clients exchange their models in order to prevent excessive communication overheads. Gossip Learning is a notable decentralized communication protocol for asynchronous updates between nodes, leveraging peer-to-peer (P2P) schemes to achieve scalability and potential privacy preservation [22], [23]. Although effective, gossip learning has a slower aggregation evolution due to its fully distributed nature [8]. Hybrid protocols combining gossip and broadcast techniques offer greater adaptability to various decentralized scenarios, facilitating model sharing only with neighboring nodes [24]. Other works [21], [25] propose blockchain-based FL environments that provide trustworthy schemes by guaranteeing the immutability of exchanged models through a peer-to-peer consensus mechanism.\nMoreover, DFL approaches lead to high inconsistency among local models since there is no global model. Some proposals, such as increasing gossip steps in local communications, achieve better consensus [26]. Another approach involves a consensus-based DFL algorithm inspired by discrete-time weighted average consensus frameworks [27]. Additionally, techniques like"}, {"title": "III. RELATED WORK", "content": "This section provides an overview of related work in the state of the art, highlighting key areas of security in Federated Learning. It is divided into three subsections that review security vulnerabilities, existing Byzantine-robust aggregation schemes proposed for centralized scenarios, and related works on various Byzantine-robust aggregation scheme proposals."}, {"title": "A. Security Vulnerabilities", "content": "The FL process is vulnerable to attacks on collaborative training, which can negatively impact model performance and the overall robustness of the system. Model performance can be compromised through specific targeted attacks (including backdoor attacks that disrupt the model's ability to accurately classify a particular category) or through general untargeted attacks (such as Byzantine attacks, which focus on obstructing the collaborative training process by providing false updates) [30]. There are two main different ways of carrying out untargeted attacks: by poisoning the data with which the model is trained (data poisoning) or by manipulating the local model before sending it to the server (model poisoning). As previous research [31] has shown that model poisoning attacks have a greater impact compared to data poisoning attacks, this work will concentrate on the former.\nOne of the most well-known techniques to counter data/model poisoning attacks performed by malicious nodes, which can be considered Byzantine attacks, are Byzantine-robust techniques. To mitigate or eliminate these Byzantine faults that threaten the proper functioning of FL, various Byzantine-robust algorithms have emerged [32]. These include redundancy-based and trust-based schemes that seek to mitigate Byzantine attacks either by assigning each client redundant updates or by assuming the presence of reliable nodes for filtering attacks, respectively [33], [34]. Of significant importance are robust aggregation schemes that aim to optimize the aggregation process itself to mitigate or filter Byzantine attacks [15], [17], [35]\u2013[37]."}, {"title": "B. Main Centralized Byzantine-Robust Algorithms", "content": "Due to the importance they will have throughout this document, we are going to describe some of the most renowned Byzantine-robust aggregation schemes for CFL in the state of the art. Suppose a server receives a subset of models {$\\theta_k$}$_{K}$ from its $K$ clients.\nMean [5]: This is the most basic aggregation rule and often serves as a benchmark for comparisons. This algorithm aggregates client models by calculating the coordinate-wise arithmetic mean of the given set of models. It simplifies the FedAvg algorithm which uses a weighted average based on the number of samples trained by each client.\nMedian [35]: This algorithm is based on a coordinate-wise aggregation rule to compute the global model. To achieve this, the server calculates each i-th parameter of the model by sorting the K values from the received models and computing their median value.\nTrimmed-Mean [35]: This is a variant of the Mean aggregation algorithm and another coordinate-wise aggregation rule. For a given trim rate $\\beta \\in (0,1/2)$, the server sorts the K received values of each i-th parameter in the model and removes the smallest and largest $\\beta K$ values. So, the i-th parameter of the global model is computed as the mean of the remaining $(1-2\\beta)K$ values.\nKrum and Multi-Krum [15]: This algorithm filters the models received from clients using Euclidean distances. For each client, the server assigns a score by calculating the sum of Euclidean distances from the client's model $\\theta_k$ to the $K-M-2$ closest neighbours models, where M is the number of malicious models. The model with the lowest score is selected as the global model. The Multi-Krum variant selects the m models with the lowest scores and computes the global model as the mean of these m selected models.\nClustering [17], [36]: The aim of this algorithm is to separate the models received from clients into two clusters and aggregate as the global model those belonging to the larger cluster. This is achieved using an agglomerative clustering algorithm with average linkage, employing pairwise cosine similarities between models as a distance metric.\nThe majority of existing Byzantine-robust aggregation schemes (including those described previously) can be categorized based on the techniques employed by them [32]:\nStatistics: These schemes utilize the statistical characteristics of updates, such as the median, to block abnormal updates and achieve robust aggregation. They are suitable only when the number of malicious users is less than half of the total users.\nDistance: These schemes aim to detect and discard bad or malicious updates by comparing the distances between updates. An update that is significantly different from others is considered malicious. These are suitable only to resist attacks that produce noticeably abnormal updates.\nPerformance: In these schemes, each update is evaluated over a clean dataset provided by the server, and any update that performs poorly is given low weight or removed directly. These schemes are more reliable than other solutions but rely on a clean dataset for evaluations, which is not always possible."}, {"title": "C. FL Defense Schemes", "content": "Following the previous subsection, not all proposals in the state of the art fit into the aforementioned classification. This is due to the development of innovative techniques that offer different approaches compared to the more well-known aggregation algorithms.\nSIGNGUARD [38] is a novel approach which enhances FL system robustness against model poisoning attacks by utilizing the element-wise sign of gradient vectors. It proposes processing the received gradients to generate relevant magnitude, sign, and similarity statistics, which are then used collaboratively by multiple filters to eliminate malicious gradients before the final aggregation. The key idea is that the sign distribution of sign-gradient can provide valuable information in detecting advanced model poisoning attacks.\nFLTRUST [39] proposes an alternative method by implementing a trust-based approach. Unlike the previous proposal, this method uses an initial trust model, trained on a small maintained dataset, to evaluate and weight the contributions of clients in the global model aggregation process. Thus, client updates that align closely with the server model's direction are deemed trustworthy, while others are adjusted or discarded based on their deviation.\nThere are also other proposals that do not only focus on the geometric properties of models for mitigating Byzantine attacks. For example, [17] proposes a method to mitigate the impact of Byzantine clients in federated learning through a spatial-temporal analysis approach. Apart from leveraging clustering-based methods to detect and exclude incorrect updates based on their geometric properties in the parameter space, the temporal analysis provides a more effective defense mechanism, which is crucial in environments where attack strategies may evolve during the course of model training. In this case, time-varying behaviors are addressed through an adjustment of the learning rate parameter.\nAnother example is SIREN [40] which proposes a FL system to improve robustness through proactive alarming. Clients alert abnormal behaviors in real-time using a continuous monitoring scheme based on the evolution of the global model's accuracy over the rounds. When a potential threat is identified, the clients issue an alarm, and the central server adjusts the model aggregation to mitigate the impact of malicious contributions."}, {"title": "IV. METHODOLOGY", "content": "Despite the existence of multiple Byzantine-robust proposals in the state of the art, the majority have been oriented towards centralized FL scenarios. To our knowledge, there is no proposed Byzantine-robust aggregation algorithm specifically designed to enhance security in DFL environments. In this section, we elaborate on the system configuration employed in DFL and detail all technical specifications of the proposed algorithm WFAgg designed to ensure Byzantine-robustness within this framework."}, {"title": "A. System Setting", "content": "We consider a cross-device scenario where multiple IoT devices (or servers) carry out a distributed collaborative learning of a machine learning model through DFL. Figure 1 illustrates the proposed scenario for the DFL system. This figure showcases a set of clients/devices $C = \\{C_i\\}$, $i = 1, 2, . . . , N$ involved in the collaborative training of a ML model. Each client possesses a local dataset $D_i$, $i \\in [N]$, with $n_i$ data samples, which is used for training their own local model, $\\theta_i$. The objective of FL can be formulated as a non-convex minimization problem:\n$\\displaystyle\\min_{\\theta \\in R^d} L(\\theta, D) = \\min_{\\theta \\in R^d} \\{\\frac{\\sum_{i\\in[N]} n_iL_i(\\theta, D_i)}{\\sum_{i\\in[N]} n_i} \\}=\\min_{\\theta \\in R^d} \\{\\frac{1}{|D|}\\sum_{i\\in[N]} |D_i|L_i(\\theta, D_i)\\}$\nwhere $L(\\theta,D)$ is the empirical loss function of the model $\\theta$ over the overall dataset $D = \\bigcup_{i\\in[N]} D_i$ and $|D| = \\sum_{i\\in[N]} |D_i|$ is the total number of samples across all clients.\nThe topology of the DFL scenario is modeled as an undirected graph $G = (C,E)$ where $C$ represents the devices participating in the learning process as vertices, which are interconnected by the set of links $E \\subseteq C \\times C$. Although the topology is dynamic across rounds, the problem is simplified by assuming a static topology where each node consistently communicates with the same set of nodes.\nThe majority of the client set $C$ consists of benign clients, i.e., they follow the established protocol and send honest models to the corresponding nodes, while a small fraction are Byzantine clients, i.e., they act maliciously and send arbitrary models with the intent of disrupting the proper training of the model. Nevertheless, this fraction of malicious nodes is relative since each node communicates with a different subset of nodes.\nAfter the training process on its own dataset, each node shares its local model with other nodes in the network. The communication protocol proposed for this framework is based on a Gossip-inspired protocol. Instead of sharing its model with only one random node in the topology, each client forwards the trained model to all of its neighbors in the communication network. The set of neighboring clients of client $C_i$ in topology graph $G$ is defined as $N_i(G) = \\{C_j \\in C, C_j \\neq C_i : \\{C_i,C_j\\} \\in E\\}$.\nUpon receiving the trained models from neighboring nodes (assuming receipt of all models), each node applies a Byzantine-robust aggregation algorithm in order to update its local model. This process is repeated iteratively, beginning again with the local model training in the next round."}, {"title": "B. Proposed Byzantine-Robust Aggregation Scheme", "content": "This proposal suggests, similarly to other approaches, performing a preliminary filtering of the received models before aggregating them. The WFAgg algorithm defines both procedures, and this can be seen in Figure 2 that summarizes the workflow of the Byzantine-robust aggregation algorithm.\nIn the WFAgg algorithm, the set of received models $T$ by a client (from its neighboring nodes) is passed through multiple filters, each of them with a different purpose. The aim is to perform a distinct type of criteria in each filter using various well-known techniques in this field for mitigating or eliminating malicious models. This approach seeks to compensate for the inadequacies of some techniques in detecting specific attacks by leveraging other techniques. This fact is especially relevant in adversarial decentralized environments, as the number of received models can be low and the fraction of malicious nodes can be high (depending on the network topology).\nSpecifically, the algorithm WFAgg comprises three multipurpose filters, including a distance-based filter, a similarity-based filter, and a temporal-based filter. As reviewed in Section II, several Byzantine-robust aggregation algorithms align with the described characteristics and could be utilized (e.g., Multi-Krum or Clustering). However, others such as those based on centrality statistics do not perform a filtering of the models per se and therefore would not be suitable (e.g., Median). Despite this, new algorithms for each of these filters, WFAgg-D, WFAgg-C and WFAgg-T, will be presented and detailed in the following subsections.\nOnce each of these filters has selected a subset of the received models identified as benign ($T_1, T_2, T_3$, respectively for each filter), the model aggregation process will be performed through a weighting of the different filter results. Specifically, each model is scored based on which filters, if any, have identified it as benign. The weighting assigned by each filter is predefined based on the relative importance attributed to that technique in filtering malicious models ($\\tau_1, \\tau_2, \\tau_3$, respectively for each filter, so $\\sum_i \\tau_i = 1$).\nConsidering that the algorithm may be attacked by targeting specific characteristics of some filters, it is understandable that a model must be accepted by two or more filters to be considered benign. This design decision is also influenced by the temporal-based filter, as it, by considering only temporal statistics, might identify a constant attack by a malicious node as benign.\nSeveral examples are proposed for better understanding (suppose client $C_i$ is performing the algorithm). If a model $\\theta_j$, from one of its neighboring nodes, has been identified as benign by all three filters, it receives the highest weighting in the"}, {"title": "C. WFAgg-D: Distance-based Model Filtering", "content": "The WFAgg-D algorithm (which is described in detail in Algorithm 2) is proposed as a Byzantine-robust aggregation algorithm aimed at detecting and mitigating models that geometrically differ from the rest. Specifically, it seeks to discard models that are at a greater Euclidean distance from a reference model. This reference model is characterized by the central statistical characteristics of the received models, specifically the information provided by the median model of the set of models.\nFirst, the median (using the Median algorithm explained in Section II) of the set of models received from neighboring nodes is calculated. Then, all Euclidean distances between each of the neighbor models and the reference model are computed. From these metrics, the $K-f-1$ models with the smallest Euclidean distance to the reference model, $T_1$, are selected, where $K$ is the number of received models and $f$ is the number of malicious nodes.\nThe computational complexity of the algorithm is $O(dK\\log K)$ for the median calculation since it involves sorting the $K$ values of the received models, $O(Klog K)$, for each of the $d$ components of the model. The complexities of calculating Euclidean distances and selecting the best results correspond to $O(dK)$ and $O(K)$ (on average, using a selection algorithm like Quickselect), respectively, which are terms dominated by the median calculation (for large values of $K$, $d$)."}, {"title": "D. WFAgg-C: Similarity-based Model Filtering", "content": "The WFAgg-C algorithm (which is described in detail in Algorithm 3) is proposed as another Byzantine-robust aggregation algorithm with the same objective as the WFAgg-D algorithm but a different criteria. Unlike WFAgg-D, which explores the Euclidean distances between models, WFAgg-C seeks to discard models that represent a significant change in direction relative to a reference model. Similarly, this reference model is characterized by the median of the set of received models. To quantify these directional changes the cosine distance is employed, a metric that measures the similarity between two vectors in a multidimensional space, defined as\n$\\alpha_{a,b} := 1 - cos(\\angle (a,b)) = 1 - \\frac{(a,b)}{||a|| \\cdot ||b||}$       \nfor two vectors $a, b \\in \\mathbb{R}^d$. Remark that $\\alpha_{a,b} \\in [0,2]$.\nFirst, the median (using the Median algorithm explained in Section II) of the set of models received from neighboring nodes is calculated. Additionally, the median magnitude is calculated to perform magnitude clipping on the models (the magnitude is not a relevant parameter in this algorithm). Then, all cosine distances between each of the neighbor models and the reference model are computed. From these metrics, the $K-f-1$ models with the smallest cosine distance to the reference model, $T_2$, are selected, where $K$ is the number of received models and $f$ is the number of malicious nodes. The computational"}, {"title": "E. WFAgg-T: Temporal-based Model Filtering", "content": "The WFAgg-T algorithm (which is described in detail in Algorithm 4) is proposed as a model filtering algorithm aimed at detecting nodes that make abrupt behavioral changes in models between rounds. It seeks to identify if a node has conducted a Byzantine attack in a particular round or if the nature of such an attack is semi-random, causing sudden model changes between rounds. Therefore, this algorithm cannot be considered for a Byzantine-robust aggregation scheme because if a malicious node perpetrates a well-designed attack with little variability between rounds, will not be detected.\nThe comparison of a node's current model with its models sent in previous rounds (which should be non consecutive) is done using geometric metrics, specifically, Euclidean distance and cosine similarity. When analyzing the temporal changes between rounds for a node, a time window of the last $W$ rounds is considered to extract the necessary metrics. Each node only needs to store the history of the distance metrics and only the last model sent by each neighboring node. Both the mean and the variability within this time window are studied to compare with the latest model update.\nThe algorithm requires an initial transient period of $T_{th}$ rounds during which it does not perform any classification. This is due to two reasons: the first and most obvious is the lack of sufficient data to determine the time window, and the second is that, due to the random and uncoordinated initialization of model parameters at each node, variations are significantly abrupt and could be mistakenly identified as attacks.\nA time window of the last W models of the node is taken as a reference, specifically, the last $W$ metrics of Euclidean distance and cosine similarity between the node's model updates. From these metrics, both the mean and the standard deviation are extracted by applying an Exponentially Weighted Moving Average (EWMA), i.e., the most recent metrics in the time window are considered, which is useful since patterns change over time (non-stationary). Subsequently, new metrics are calculated with the model received in the current round, and it is observed if they fall within the thresholds calculated with the mean and standard deviation. If any of them do not meet the criteria, the model update is considered abrupt and malicious in that round."}, {"title": "F. WFAgg-E: Weighted Model Aggregation Algorithm", "content": "The WFAgg-E algorithm is proposed as a model aggregation algorithm similar to the FedAvg. Unlike Mean and other variants, it performs a weighted average, giving more or less relevance to the local model of the node or to the updates from neighboring nodes. This aggregation algorithm is designed for adversarial environments so that if a malicious model is selected for aggregation, its influence is mitigated.\nThe fundamental idea of WFAgg-E consists of a variation of the first-order exponential smoother [41] (in other fields it is called Exponential Moving Average). It started to become popular in the 1960s and is a technical indicator used in financial analysis to analyze and smooth time series data. It assigns more weight to the most recent data, making it more sensitive to recent changes."}, {"title": "V. EXPERIMENTAL SETUP", "content": "This section outlines the experimental setup, including the scenario, learning algorithms and dataset for image classification, primary evaluation metrics, which Byzantine-robust aggregation algorithms are evaluated and which model performance attacks are performed in the simulations."}, {"title": "A. Validation scenario", "content": "Topology: We have defined a FL setting with 20 clients, 10% of them are malicious nodes. The selected bounds were chosen to ensure a balance between system stability and responsiveness to malicious behavior. Performance is evaluated in both centralized and decentralized scenarios. In the centralized scenario, a server coordinates the clients' training process, while in the decentralized scenario, each client communicates with its neighbors. The topology for the latter is modeled as an 8-regular graph\u00b9, i.e., each node has exactly 8 neighbors, and each client has, at most, 25% malicious neighbors. This enables a broad testing scenario to evaluate the influence of malicious nodes.\nLearning algorithm and dataset: The nodes perform learning tasks based on image classification on the MNIST dataset [42]. The MNIST dataset consists of 70,000 handwritten digit images (0 to 9), each represented as a 28 \u00d7 28 pixel grayscale matrix. The dataset is independently and identically distributed (IID) across all nodes. Nodes train the well-known learning algorithm ResNet-18 [43], which is a convolutional neural network architecture that consists of 18 layers, including convolutional layers,"}, {"title": "B. Model Performance Attacks", "content": "In order to assess the robustness of the different Byzantine-robust schemes, various attacks have been implemented. The attacks considered for the tests, representing the most popular poisoning attack found in the literature are detailed below [37], [38] (assume that Byzantine node $C_i$ is going to perform a poisoning attack on its ML parameters model $\\theta_i$):\nNoise Attack: Byzantine nodes send modified models by injecting Gaussian noise into the benign models as $\\theta_i \\leftarrow \\theta_i + N(\\mu, \\sigma^2I)$, where $I \\in \\mathbb{M}^{d\\times d}$ is the identity matrix of dimension $d$. In our experiments, both the mean and variance of the noise of each parameter is set to 0.1, i.e., $\\mu = 0.1 \\cdot \\mathbb{1} \\in \\mathbb{R}^d$ and $\\sigma = 0.1$.\nSign-Flipping (SF): The Byzantine clients send reversed models without changing their modulus as $\\theta_i \\leftarrow -\\theta_i$.\nLabel-Flipping (LF): This is an example of a data poisoning attack, where Byzantine clients flip the local sample labels during the training process to generate malicious models. In particular, the label of each training sample in Byzantine clients is flipped from $l \\in \\{0,1,..., C - 1\\}$ to $C \u2013 1 \u2013 l$, where $C$ is the total classes of labels.\nA Little is Enough (ALIE): This is a more sophisticated version of the Noise Attack, where model's parameters are selected carefully in order to appear benign and harm the model performance. A Little is Enough attack assumes that the benign models' parameters are expressed by a normal distribution. For each model's coordinate $j \\in [d]$, the attacker computes the mean $\\mu_j$ and standard deviation $\\sigma_j$ over benign models and built the corresponding malicious model by taking values in range $(\\mu_j \u2013 z_{max}\\sigma_j, \\mu_j + z_{max}\\sigma_j)$, where $z_{max} \\in [0, 1]$ is typically obtained from the Cumulative Standard Normal Function. Here, the threshold parameter is set to $z_{max} = 0.5$.\nInner Product Manipulation (IPM): The Inner Product Manipulation attack seeks for the negative inner product between the true mean of the updates and the output of the aggregation schemes. Assume that there are a set of models $\\{\\theta_k\\}_{k=1}^{N}$ where the first $M$ are malicious updates. So, a way of performing the IPM attack is\n$\\displaystyle\\theta_1 = ... = \\theta_M = \\frac{\\sum_{k=1}^N \\theta_k}{N} - \\frac{\\varepsilon}{N-M}\\sum_{k=M+1}^{N} \\theta_k$\nwhere $\\varepsilon$ is a positive coefficient controlling the magnitude of malicious updates. Then the models' mean becomes\n$\\displaystyle\\frac{1}{N}\\sum_{k \\in [N]} \\theta_k = \\frac{N - M(1 + \\varepsilon)}{N(N-M)} \\sum_{k=M+1}^N \\theta_k$.\nThe key of this attack lies in choosing the value of $\\varepsilon$. If $\\varepsilon < N/M \u2013 1$, IPM does not change the direction of the mean but decreases its magnitude, in the other case, the sign is reversed. We examine the two different cases by letting $\\varepsilon = 0.5$ and $\\varepsilon = 100$, respectively."}, {"title": "C. Metrics", "content": "The tests will primarily use two metrics: accuracy and R-squared. The accuracy metric (its micro average approach), which is one of the most popular metrics for a multi-class classification problems, can be defined as\n$\\displaystyle Accuracy = \\frac{\\sum_{l=0}^{C-1} T P_l}{|D_{test}|}$       \nwhere $TP_l$ are the true positives over class/label $l \\in [C]$, $C$ is the number of classes, and $|D_{test}|$ is the number of samples in the testset. In this work, this metric will be shown with its percentage value (%).\nTo quantify and evaluate the convergence of local models in a DFL scenario, we propose an adapted version of the well-known R-squared metric (also known as $R^2$) as, originally, the metric is defined for univariate cases. Given a set of N multidimensional vectors $V_1,..., v_n \\in \\mathbb{R}^d$, we are going to define the R-squared metric as follows:\n$R^2 = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\sum_{i=1}^{N} ||v_i - \\overline{v}||^2}{\\sum_{i=1}^{N} ||v_i||^2}$ \nwhere:\nSSR is the total sum of squares of the differences between each vector and the mean vector (the term to be minimized),\nSST is the total sum of squares of each vector (the term that normalizes the result of SSR),\n$\\overline{v} = \\frac{1}{N} \\sum_{i=1}^{N} V_i$ is the mean vector.\nThis metric provides a measure of the proportion of the total variability of the vectors explained by the mean vector, i.e., how similar the vectors are to each other. Applied to the DFL scenario, the set of vectors would correspond to the local models of the benign nodes, to analyze how similar these models are to each other."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "The objective of the upcoming tests, described in the following subsections, is to better understand the effectiveness of these Byzantine-robust aggregation schemes under multiple adversarial attacks. These attacks aim to degrade the model performance of the learning process, affecting both the local models accuracy and the global convergence of the learning process. Therefore, this evaluation consists of three different tests that will be analysed on the selected algorithms. First, examine and analyse the robustness of the algorithms to Byzantine attacks (Subsection VI-A1). Then, we study the evolution of the accuracy convergence throughout the communication rounds (Subsection VI-A2). Finally, we analyse the global consistency among benign models in decentralized scenario (Subsection VI-A3). The same tests will be analysed for the proposed algorithms (Subsections VI-B1 and VI-B2)."}, {"title": "A. Evaluation of state-of-the-art Byzantine-Robust Schemes", "content": "Are the proposed Byzantine-robust aggregation algorithms for centralized scenarios equally effective when evaluated in decentralized scenarios? This section aims to answer that question by conducting various analyses on well-known Byzantine-robust aggregation algorithms from the literature for the previously mentioned metrics."}, {"title": "VII. CONCLUSIONS AND FUTURE WORK", "content": "This proposal presents an algorithmic solution to strengthen security against malicious attacks in Decentralized Federated Learning environments. It integrates techniques based on Byzantine-robust algorithms, which mitigate security vulnerabilities in Federated Learning, particularly when central servers are removed to achieve true decentralization. The integration of various techniques, such as security filters based on distance, similarity, and temporal changes, enables the identification and mitigation of malicious contributions from Byzantine nodes and adapts to the adverse conditions posed by the variability of decentralized scenarios.\nExperiments demonstrated that the proposed algorithm effectively maintains model accuracy and achieves satisfactory model convergence in the presence of all the considered Byzantine attacks, outperforming multiple centralized algorithm proposals evaluated in decentralized scenarios. The combination of multiple filters allowed for more precise detection of malicious contributions by weighting the results of these filters and considering different decisions for a potential Byzantine attack.\nTherefore, the integration of Byzantine-robust schemes in Decentralized Federated Learning scenarios establishes an efficient alternative to address the challenges faced by such environments.\nHowever, further studies are still needed in the field of Decentralized Federated Learning. Continuing with the proposed work, it is interesting to study in depth the optimization of communication networks to reduce overhead and latency, among other factors, to enhance overall learning process performance. In this regard, we are working on the implementation of technologies such as Software-Defined Networks (SDN) to not only improve network parameters as mentioned but also to propose scalable and dynamic DFL environments. This paradigm allows exploration of other alternatives, such as enhancing the security of the learning process from the perspective of communication between devices."}]}