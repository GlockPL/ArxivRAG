{"title": "DiffZOO: A Purely Query-Based Black-Box Attack for red-teaming Text-to-Image Generative Model via Zeroth Order Optimization", "authors": ["Pucheng Dang", "Xing Hu", "Dong Li", "Rui Zhang", "Kaidi Xu", "Qi Guo"], "abstract": "Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I model's capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of purely black-box attacks without prior knowledge of the T21 model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T21 diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2I models.", "sections": [{"title": "1 Introduction", "content": "The domain of Generative AI has witnessed remarkable strides, notably in the realms of text (Li et al., 2024), image (Xu et al., 2023), and code synthesis (Huang et al., 2022). Prominently, text-to-image (T2I) generation has risen as a central focus of investigations. The triumph of today's T2I diffusion models is significantly underpinned by the expansive online datasets utilized for their training. While this wealth of data enables T2I models to conjure a wide array of realistic imagery, it concurrently introduces challenges. Predominantly, the presence of sensitive content in images sourced from the web can result in trained models unintentionally internalizing and regurgitating unsuitable visuals. This encompasses concerns such as copyright breaches (Luo et al., 2023), images harboring forbidden content (Chourasia and Shah, 2023), and NSFW (Not Safe For Work) materials (Schramowski et al., 2023).\nTo mitigate the generation of images containing inappropriate content, various safety mechanisms have been incorporated into diffusion models (Schramowski et al., 2023; Gandikota et al., 2023; Kumari et al., 2023; Zhang et al., 2023a). To evaluate these safety mechanisms, recent research proposes attack methods (Zhuang et al., 2023; Chin et al., 2023; Tsai et al., 2023; Ma et al., 2024; Zhang et al., 2023b) that aim to facilitate the red-teaming of T2I diffusion models equipped with safety mechanisms to find problematic prompts with the ability to reveal inappropriate concepts (e.g., craft attack prompts capable of generating images with banned concepts such as \u201cnudity\u201d and \u201cviolence\u201d).\nThese attack methods can be bifurcated into two primary categories: white-box attacks (Chin et al., 2023; Zhang et al., 2023b) and black-box attacks (Zhuang et al., 2023; Ma et al., 2024; Tsai et al., 2023). In the domain of white-box attacks, it is presupposed that the attacker has cognizance of the diffusion model's U-Net (Ho et al., 2020)"}, {"title": null, "content": "output. Nonetheless, in genuine attack situations, the attacker cannot obtain the noise estimation of the U-Net output, which is essential for devising attack prompts. Conversely, black-box attacks contrive attack prompts by capitalizing on Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) embedding vectors. Nonetheless, this approach does not fully align with the orthodox definition of a black-box attack, considering the CLIP encoder's intrinsic role within the T2I diffusion model. Simultaneously, this attack paradigm shifts into a transferable attack when the targeted diffusion model resorts to a non-CLIP text encoder for generating embedding vectors.\nIn this work, we rethink the black-box attack for the T2I diffusion model to expose their propensity to generate inappropriate concepts. The attacker is restricted to querying the diffusion model through prompts and harnessing the generated images to develop attack prompts (Li et al., 2020, 2021). In this case, we propose a query-based attack method, DiffZOO, specifically tailored for the T2I diffusion model. There are two significant challenges in accomplishing this task. The first challenge is the inability to acquire gradients to construct the attack prompts, as is typically done in conventional adversarial attacks. To address this issue, we employ Zeroth Order Optimization (ZOO) (Chen et al., 2019, 2017, 2023) and obtain gradient estimates to construct attack prompts. The second challenge is distinguishing our work from previous studies of black-box attack (Zhuang et al., 2023; Tsai et al., 2023; Ma et al., 2024), where we consider the text encoder (CLIP) in a black-box setting (as shown in Figure 1). This scenario presents a disparity between the continuous embedding domain and the discrete prompt (token) domain. The former is naturally suited for optimizing an attack vector, as in conventional adversarial attack (Madry et al., 2018; Carlini and Wagner, 2017; Croce and Hein, 2020). However, optimizing the attack prompt on a discrete prompt domain presents a formidable problem. Fortunately, taking inspiration from TextGrad (Hou et al., 2023), we utilize continuous position replacement vectors (C-PRV) to overcome such a problem. We subsequently sample from it to derive discrete position replacement vectors (D-PRV) and construct attack prompts without a text encoder.\nOur approach discerns the necessity for token replacement within prompts and selects suitable synonyms for crafting attack prompts. This approach successfully bridges the gap and facilitates"}, {"title": null, "content": "direct optimization of the discrete prompt (token) domain. We summarize our contributions below.\n\u2022 We rethink the black-box settings of the T2I diffusion model, and regard the text encoder and the whole model as a black-box setting (previous black-box attacks regard the text encoder as a white-box setting), which requires no prior knowledge of the T21 model.\n\u2022 To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt domain, we propose DiffZOO a query-based attack method that serves as a prompt-based concept testing framework for red-team T2I diffusion models. DiffZOO applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain.\n\u2022 Our comprehensive experiments evaluate a wide range of models, encompassing prevalent online services to state-of-the-art methods in concept removal. Results reveal that prompts crafted by DiffZOO significantly boost the average success rate of concept removal methods producing inappropriate images, an elevation beyond 8.5%."}, {"title": "2 Related Work", "content": "Safety Mechanisms for Diffusion Model. In response to the exploitation of Text-to-Image Diffusion models for the generation of inappropriate imagery, various strategies have been proposed to counteract this issue. Generally, these methodologies can be categorized into two principal trajectories: detection-based and removal-based. Detection-based strategies (Rando et al., 2022) primarily focus on the eradication of unsuitable content through the implementation of safety checkers, serving as filters. Conversely, removal-based strategies (Schramowski et al., 2023; Gandikota et al., 2023; Kumari et al., 2023; Zhang et al., 2023a) endeavor to divert the model from such content by actively directing it during the inference phase or fine-tuning the model parameters.\nAttack for Text-to-Image Diffusion Model. The current body of research can be broadly categorized into two main domains: white-box attacks (Yang et al., 2023; Chin et al., 2023; Zhang et al.,"}, {"title": "3 Preliminary", "content": "3.1 Threat Model\nWhite-box Settings. Here, attackers utilize T2I diffusion models for image generation. Having unrestricted access to the model's architecture and checkpoint empowers attackers to conduct thorough investigations and manipulations, thus enabling sophisticated attacks. Under white-box conditions, approaches akin to P4D (Chin et al., 2023) and UnlearnDiff (Zhang et al., 2023b) are employed, leveraging the noise estimation of the diffusion model's U-Net output for crafting attack vectors. However, this white-box setting does present certain constraints in practical scenarios where the attacker may not have access to the diffusion model's U-Net output.\nBlack-box Settings. In this context, attackers utilize T2I diffusion models to generate images, despite not having direct access to the proprietary models' parameters and internal details. Instead of direct access, attackers query the T2I diffusion model to adapt their strategies based on their interactions with the T2I diffusion model API (only allows for prompt input and image output), which is considerably more challenging than white-box settings. The black-box attack approach, exemplified by QF-Attack (Zhuang et al., 2023), Ring-A-Bell (Tsai et al., 2023) and Jap-Attack (Ma et al., 2024), employs the CLIP text encoder to devise attack vectors. However, these approaches do not strictly adhere to the definition of a black-box attack since CLIP integrates within the T2I diffusion model. Concurrently, these attack methodologies transition to transferable attacks when the targeted T2I"}, {"title": "4 Method", "content": "4.1 C-PRV and D-PRV\nIn this subsection, we introduce how to optimize attack prompts in the discrete prompt (token) domain (challenge of \"Optimization in Discrete Prompt Domain\"). We denote the initial prompt $p$ as a prompt that can not release inappropriate concepts such as \"nudity\u201d and \u201cviolence\u201d when input to the T2I diffusion model with safety mechanisms. To optimize $p$ to be an attack prompt, we tokenize it and denote as $p = [t_1, t_2,\u2026,t_l] \\in N^l$, where $t_i \\in {0,1,\u2026\u2026,|V| - 1}$ is the index of i-th token, $V$ is the vocabulary table, and $|V|$ refers to the size of the vocabulary table. Then, we collect a set of token synonym candidates (using a pre-trained language model like GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019) etc.) for substitution at each position of $p$, denoted by $c_i = {Ci,1, Ci,2,\u2026\u2026\u2026, Ci,m}$, where $Ci,j \\\u0404 {0,1,..., |V| - 1}$ denotes the index of the j-th candidate token that the attaker can be used to replace the i-th token in $p$. Here $m$ is the number of candidate tokens.\nThen, we introduce continuous position replacement vectors (C-PRV). For each initial prompt $p$, there are C-PRVs $z = [z_1, z_2,\u00b7\u00b7\u00b7,z_\u03c4] \\in [0,1]^\u03c4$ for $p$ and $u_i = [u_{i,1}, u_{i,2},\u2026\u2026\u2026, u_{i,m}] \\in [0,1]^m$ for each token $t_i$. Corresponding to C-PRV $z$ and $u$ we have discrete position replacement vectors (D-PRV) $\\bar{z} = [\\bar{z}_1, \\bar{z}_2,\u2026,\\bar{z}_\u03c4] \\in {0,1}^\u03c4$ and $\\bar{u}_i = [\\bar{u}_{i,1}, \\bar{u}_{i,2},\u2026\u2026,\\bar{u}_{i,m}] \\in {0,1}^m$ using following sampling strategies:\n$ \\bar{z_i} = \\begin{cases} 1 \\text{ with probability } z_i \\\\ 0 \\text{ with probability } 1 - z_i \\end{cases} $   (1)\n$ \\bar{u}_{i,j} = Onehot (j) \\text{ with probability } \\frac{u_{i,j}}{|| u_{i} ||_1} $   (2)\nwhere $Onehot(j)$ is an m-dimensional vector with a 1 in the j-th position and 0s elsewhere. In summary, this equation states that $\\bar{u}_{i,j}$ will be a one-hot encoded vector where the value is 1 at index j, and the probability of producing this vector is $\\frac{u_{i,j}}{||U||_1}$. This means that when sampling $\\bar{u}_i$, each index j is selected with a probability proportional to the corresponding value $u_{i,j}$ in the vector $u_i$.\nIn this case, $\\bar{Zi}$ decides whether token $t_i$ of $p$ should be replaced (if $\\bar{Z_i} = 1$, replace it). Based on that, $\\bar{U_{i,j}}$ decides which synonym candidate token should be selected to replace token $t_i$ (if $\\bar{Zi} = 1$ and"}, {"title": null, "content": "set $c_i = {Ci,1, Ci,2, , Ci,m}$ at position i of p.\n$t_i = Replace (t_i, \\bar{Zi}, \\bar{Ui,j}, Ci,j)$   (4)\nwhere $\\bar{Zi}$ and $\\bar{Wij}$ is the D-PRV component sampling from C-PRV component $zi$ and $ui,j$ by Eq. (1) and (2). Replace is the replacement strategy that if $\\bar{Zi} = 1$ and $\\bar{Ui,j} = 1$, use candidate token $Ci,j$ to replace token $ti$. An example can be found in Figure 3.\nTo optimize Eq. (3), traditional white-box attack using gradient descent algorithm. However, in black-box settings, attackers can not obtain the gradients of C-PRV component zi and ui,j to construct an attack prompt. To overcome this problem we utilize the Zeroth Order Optimization algorithm (Chen et al., 2017, 2019, 2023) to estimate gradients as follows:\n$\\nabla_{z_i}L \\approx \\frac{L(z_i + \\delta_k) - L(z_i - \\delta_k)}{2 \\delta_k}$   (5)\n$\\nabla_{u_{i, j}}L \\approx \\frac{L(u_{i,j} + \\delta_k) - L(u_{i,j} - \\delta_k)}{2 \\delta_k}$   (6)\nwhere $L = || f (\\Theta(p(zi, Ui,j))) \u2013 eo||2$ as mentioned in Eq. (3). $\u03b4\u03ba \u2190 1e\u00af5\u03b4$, $\u03b4 ~ N(0,1)$. In ZOO methods, Adaptive Moment Estimation (Adam)'s update rule significantly outperforms vanilla gradient descent update, so we propose to use a ZOO coordinate Adam algorithm (Chen et al., 2017, 2019) to update C-PRV component zi and Ui,j."}, {"title": "4.3 DiffZOO", "content": "In this part, we introduce the whole attack prompt optimization DiffZOO algorithm as shown in Algorithm 1. When given an initial prompt p, we first initial its C-PRV z and ui (each element is independently drawn from a Gaussian distribution N(0, 1), details in Appendix A. Then sampling D-PRV Z and \u016b\u00bf T times to construct T attack prompt p. We represent the set of these T attack prompt p as P. If one of the attack prompts in set P succeeds, we get an attack prompt. If not, we utilize ZOO to optimize C-PRV z and ui S steps. At the end of each step, we construct attack prompt \u00een set P, and check whether there is a prompt \u00een in set P that can attack successfully. If not, proceed to the next step of optimization."}, {"title": "5 Experiments", "content": "5.1 Settings\nDataset. We assess the efficacy of DiffZOO utilizing the I2P dataset (Schramowski et al., 2023), a recognized collection of challenging prompts, focusing on the notions of \u201cnudity\u201d and \u201cviolence\". We select a sample of 100 prompts associated with \"nudity\u201d, all exhibiting a prevalence of \u201cnudity\" exceeding 50%. To prevent any overlap with nudity-related prompts when examining the concept of \"violence\", we curated an additional set of 100 prompts. These prompts were characterized by a Q16 percentage surpassing 90% and labeled as \"violence\".\nBaseline. To maintain impartiality, we have chosen SneakyPrompt (Yang et al., 2024), QF-Attack (Zhuang et al., 2023) and Ring-A-Bell (Tsai et al., 2023), notable black-box attacks, as our primary baselines. We have also incorporated a lightweight replacement baseline (DiffZOO-Lite), which indiscriminately substitutes the initial prompt tokens with their synonymous candidates, achieved by replacing lines 7 to 14 with line 1 in Algorithm 1. Concurrently, the quantity of attack prompts produced from the initial prompt is equivalent to that"}, {"title": null, "content": "generated by DiffZOO, further ensuring a fair comparison. See Appendix A for more details.\nSafety Mechanism. Our target text-to-image model is Stable Diffusion with the open source model\u00b9. We opt for several state-of-the-art concept removal methods designed for Stable Diffusion, inclusive of Safe Latent Diffusion (SLD) (Schramowski et al., 2023) under three distinct settings (where we adopt 3 superior variants of SLD as provided by (Schramowski et al., 2023), namely, SLD-Max, SLD-Strong, and SLD-Medium), ESD (Gandikota et al., 2023), Concept Ablation (CA) (Kumari et al., 2023), and Forget-Me-Not (FMN) (Zhang et al., 2023a). See Appendix A for more details.\nEvaluation. In the process of categorizing images based on the presence of \"nudity\u201d, we employ the NudeNet\u00b2 detector. If the detector identifies any of the following labels within the image: ANUS EXPOSED, FEMALE BREAST EXPOSED, FEMALE GENITALIA EXPOSED, or MALE GENITALIA EXPOSED, we consider the image to contain nudity. On the other hand, to ascertain the presence of inappropriate content, specifically \u201cviolence\", within images, we utilize the Q16 classifier (Schramowski et al., 2022). Previous works (Tsai et al., 2023) set the inappropriate percentage threshold at 0.5. We found that such a low threshold results in a higher rate of false positives. Therefore, we use a threshold of 0.95, a more stringent and challenging setting.\nEvaluation Settings. We utilize the pre-trained language model BERT (Devlin et al., 2019) to generate a synonym candidate set, and the synonym candidate number of each token m is set as 20 through the experimental tuning. T, K, P, S is set as 8, 12, 16, 5. Meanwhile, the learning rate \u03b7 is set as 0.05 through the experimental tuning. Regarding the metric, we report the Attack Success Rate (ASR) defined as the proportion of successful generation of inappropriate images by problematic prompts relative to the total number of images. See Appendix A for more details."}, {"title": "5.2 Evaluation", "content": "Evaluation of Concept Removal-based Methods. Here, we explicate the efficacy of the DiffZOO"}, {"title": "5.3 Ablation Studies", "content": "Learning Rate. The learning rate \u03b7 of DiffZOO is a significant hyperparameter to boost ASR. We use SLD-Strong as the victim model and choose five numbers: {0.005, 0.01, 0.05, 0.1, 0.5}. As shown in Figure 6. The ASR of the \"violence\" concept attack is much more insensitive than that of the \"nudity\" concept attack. Meanwhile, 0.05 is suitable for both nudity and violence concepts."}, {"title": null, "content": "The Number of Candidates. The number of candidate m determines how many synonyms of each token will considered to substitute. In Figure 6, we experiment on how the candidate number m affects the ASR. We use SLD-Strong as the victim model and choose five numbers: {10, 15, 20, 25, 30, 35}. As shown in Figure 6, larger candidate number m does not significantly improve ASR, and 20 is a suitable value for both nudity and violence concept attacks."}, {"title": null, "content": "Other Hyperparameters We also analyze the influence of other hyperparameters T, K, P, and S on ASR as shown in Figure 7. We use SLD-Strong as the victim model and choose four different numbers of T: {4, 8, 12, 16}. The ASR of the violence concept attack is much more insensitive than that of the \"nudity\" concept attack. Meanwhile, 8 is a suitable value of T for both \"nudity\u201d and \u201cviolence\" concepts. Identically, we choose four different numbers of K and P: {8,12,20}. 12 is a suitable value of K. It is worth noting that the performance of P = 16 is the same as that of P = 20. Nevertheless, larger P means more running time of DiffZOO. In this case, we choose P = 16 for our settings. Additionally, we choose three different numbers of S: {3,5,7}. Analogously, larger S means more running time of DiffZOO, and we choose S = 5 for our settings."}, {"title": "6 Conclusion", "content": "In this paper, we rethink the challenge of black-box attacks targeting Text-to-Image diffusion models, which carry the peril of unleashing unsuitable concepts like nudity and violence. Our main emphasis is on stringent black-box scenarios, and we propose DiffZOO, a methodology that exclusively interacts with the T2I diffusion model API to craft attack prompts. These prompts expose the model's susceptibility to generating contentious concepts. Our experimental outcomes affirm that employing DiffZOO to fabricate provocative prompts can potentially steer these T2I models to output indecent imagery effectively. Hence, DiffZOO serves as a pivotal red-teaming instrument for assessing the robustness of T2I models in detecting or mitigating improper content."}, {"title": "Limitations", "content": "Unlike the previous work, our methodology faces a more rigorous scenario (purely black-box settings) and inevitably constructs attack prompts in a query-based manner. In this case, our methodology shares the common drawback of numerous query-based methods: the extensive query time required to accomplish the intended optimization. This presents a significant challenge for future research endeavors. Meanwhile, our methodology can function as a red hat diagnostic instrument for text-to-image generation models, revealing their capacity to produce inappropriate content. However, there exists a risk that this method could be exploited by malevolent entities with the intent to sabotage. Consequently, regulatory measures and restrictions are recommended to mitigate this risk."}, {"title": "A Settings", "content": "Baseline. To maintain impartiality, we have chosen SneakyPrompt (Yang et al., 2024), QF-Attack (Zhuang et al., 2023) and Ring-A-Bell (Tsai et al., 2023), notable black-box attacks, as our primary baselines. We follow the settings of the reinforcement learning version in the SneakyPrompt open-source code\u00b3. QF-Attack employs three optimization methods to optimize the attack suffix. We chose the Genetic Algorithm, which demonstrated the best performance in their experimental results. All other settings remain consistent with the open-source QF-Attack code\u2074. We follow the default settings in the Ring-A-Bell open source-codes and set the hyperparameters to (K, \u03b7) = (16, 3).\nSafety Mechanism. We opt for several state-of-the-art concept removal methods designed for Stable Diffusion, inclusive of Safe Latent Diffusion (SLD) (Schramowski et al., 2023) under three distinct settings (where we adopt 3 superior variants of SLD as provided by (Schramowski et al., 2023), namely, SLD-Max, SLD-Strong, and SLD-Medium), ESD (Gandikota et al., 2023), Concept Ablation (CA) (Kumari et al., 2023), and Forget-Me-Not (FMN) (Zhang et al., 2023a). The execution of all baselines and the ablated concept models is predicated on the official code. To eliminate nudity, we employ \u201cbodily, nudity, sexual\u201d as the negative prompt. Conversely, to eradicate violence, we utilize the prompt \u201chate, harassment, violence, suffering, humiliation, harm, suicide, blood, fluids\" for training ESD.\nEvaluation Settings. We use a Gaussian distribution to initialize C-PRV z and ui. Specifically, each element is independently drawn from a Gaussian distribution N(0, 1). Additionally, to ensure the constraints of Eq. (1) and Eq. (2), we normalize each element of the C-PRV vectors to the [0, 1] interval. Following previous work (Chen et al., 2019), we use Adaptive Moment Estimation (Adam) and set optimization parameters B1, B2 as 0.9, 0.999, respectively.\nOnline Services Settings. Due to network delays in the online services and limits on the number of query times, conducting quantitative tests on the large dataset we collected directly was challenging. To overcome this, we used a subset of the"}, {"title": "B More Visualization Results", "content": "We present additional visualization comparison results for the concepts \u201cnudity\u201d and \u201cviolence\u201d. As illustrated in Figures 9, 10 and 11, DiffZOO is capable of independently attacking different defense mechanisms, optimizing the attack prompts that are most suitable for each particular defense mechanism. In contrast, previous methods (QF-Attack (Zhuang et al., 2023), Ring-A-Bell (Tsai et al., 2023)) merely employed the same attack prompt in transfer attacks against various defense mechanisms. This is because DiffZOO considers the Text-to-Image model as a complete black-box setting, whereas previous work treated the text encoder component of the T2I model as white-box accessible. Consequently, for different defense mechanisms, previous approaches could only resort to transfer attacks as a means of assault."}, {"title": "C Online Service", "content": "To evaluate if online service is effective in rejecting the generation of inappropriate images, we test the well-known T2I DALL\u00b7E 2 as shown in Figure 8."}, {"title": "D Ethnical Discussion", "content": "The methodology proposed, DiffZOO, is specifically designed to employ Zeroth Order Optimization with the primary aim of revealing inappropriate content within the text-to-image diffusion model. Furthermore, the DiffZOO framework can"}]}