{"title": "FAIRPIVARA: REDUCING AND ASSESSING BIASES IN CLIP-BASED MULTIMODAL MODELS", "authors": ["Diego A. B. Moreira", "Alef Iury Ferreira", "Gabriel Oliveira dos Santos", "Luiz Pereira", "Jo\u00e3o Medrado Gondim", "Gustavo Bonil", "Helena Maia", "N\u00e1dia da Silva", "Simone Tiemi Hashiguti", "Jefersson A. dos Santos", "Helio Pedrini", "Sandra Avila"], "abstract": "Despite significant advancements and pervasive use of vision-language models, a paucity of studies has addressed their ethical implications. These models typically require extensive training data, often from hastily reviewed text and image datasets, leading to highly imbalanced datasets and ethical concerns. Additionally, models initially trained in English are frequently fine-tuned for other languages, such as the CLIP model, which can be expanded with more data to enhance capabilities but can add new biases. The CAPIVARA, a CLIP-based model adapted to Portuguese, has shown strong performance in zero-shot tasks. In this paper, we evaluate four different types of discriminatory practices within visual-language models and introduce FairPIVARA, a method to reduce them by removing the most affected dimensions of feature embeddings. The application of FairPIVARA has led to a significant reduction of up to 98% in observed biases while promoting a more balanced word distribution within the model. Our model and code are available at: https://github.com/hiaac-nlp/FairPIVARA.", "sections": [{"title": "1 Introduction", "content": "The rise of computational intelligence presents challenges, particularly as these technologies advance and become widely adopted. The large-scale adoption and use of models by companies and the general public has shown that the models have several shortcomings, not only in accuracy but also in ethical concepts [15]. Once deployed in society, these models must uphold ethical standards across all represented groups without compromising human ethics.\nVarious factors can cause unethical model behavior, including improper data usage and a lack of concern for the development team. The assumption that more data leads to better outcomes can encourage excessive data collection, resulting in datasets with ethical problems, such as privacy violations and other serious concerns [3].\nTraining data quality is crucial for models to meet performance and ethical standards [22, 13]. High-quality data must be accurate, complete, consistent, timely, and accessible to ensure precision and adherence to ethical guidelines [2, 8]. Creating an ideal training dataset is challenging, as perceptions vary across cultural contexts. According to Achard [1], a word's meaning is shaped by its context and the reader's or listener's memory, allowing for reinterpretation. A dataset alone cannot define grammar or meaning but only sets a boundary for interpretation. Similarly, from a materialist discursive view of language [16], biases in data can be seen as the repetition and perpetuation of meanings crystallized in dominant and hegemonic discourses, when the combination of words and images ends up reinforcing, for example, stereotypes, inequality, social, and epistemic injustice."}, {"title": "2 Related Work", "content": "The consolidation, use, and expansion of deep learning models have increased focus on assessing biases in learning models. Many studies focus on how different layers in these models contribute to overall bias. The main evaluation steps and proposals for reducing biases are classified into three main categories: (i) the training dataset, (ii) model architecture and training methods, and (iii) post-processing of results.\nWang et al. [20] analyzed gender bias in search models to determine whether gender-neutral languages still contain bias. They introduced a metric to quantify gender bias, measuring differences in image retrieval results between masculine and feminine attributes. The study also proposed two bias mitigation methods: one integrated into model training, requiring full retraining, and another implemented as post-processing. To address the first solution, they identified class imbalance as a significant issue and used a balancing technique that samples gender-neutral images. The second strategy involved clipping highly correlated dimensions using the Kullback-Leibler divergence. Their results showed significant biases in CLIP models, with an 18 percentage points (pp) average reduction in bias across the datasets used. However, the balancing approach during training required labeled images, and the final results showed minimal bias reduction for top-1 predictions, intensifying the overall model bias in some cases. The study focused only on gender bias within English-language datasets.\nJanghorbani and De Melo [11] assessed bias in multimodal models, proposing a post-processing technique for various concepts based on the work of Caliskan et al. [4]. Their analysis included both cross-modal (text and image encoders) and intra-modal (single encoder) approaches. They introduced the Multi-Modal Bias (MMBias) dataset, which comprises images and texts from diverse social groups, including religious groups, nationalities, individuals with disabilities, and those who identify as sexual minorities. Their bias removal strategy reduced bias by 60.2 pp for the class cut. However, the study did not optimize individual classes representing a potential improvement avenue and showed suboptimal accuracy for pleasant and unpleasant image sets.\nAnother key study by Wang et al. [21] compared CLIP multilingual architectures using Vision Transformers [7] and ResNet-50 [9], focusing on gender, race, and age biases. They evaluated individual fairness (performance across languages within the same semantic field) and group fairness (consistent performance regardless of language). The study found high individual fairness but significant discrepancies in group fairness without proposing solutions for inherent biases and shortcomings in model fairness.\nUnlike traditional methods focusing on data or model bias removal, our approach minimizes discrepancies without retraining the entire model. FairPIVARA optimizes multiple class concepts individually and proposes a single embedding to encompass all. We report both English and Portuguese results, extend the dataset to include Portuguese, and suggest terms with reduced political bias."}, {"title": "3 Methodology", "content": "Large models require high-cost training to achieve impressive results and can have a significant environmental impact. For example, training the LLaMA-2-70B [19] model consumed around $2.5 \\times 10^{12}$ joules of energy, with a carbon footprint of up to 291 tonnes of $CO_2$-equivalent [19]. To optimize resources and reduce training costs, CAPIVARA [6] proposes strategies for fine-tuning a pre-trained CLIP model for non-English languages.\nThese models are often trained on hastily reviewed text and image datasets, which raises ethical concerns. In this work, we analyze bias on OpenCLIP [10] and CAPIVARA models. By assessing both pre-trained and language-specialized models, we aim to investigate the impact of specialization on bias. We also introduce the FairPIVARA, a post-processing algorithm to reduce bias without retraining the entire model."}, {"title": "3.1 General Pipeline", "content": "Figure 1 illustrates the general flow of the FairPIVARA application. For bias analysis (left), we use a multimodal bias dataset composed of class and good/bad concepts. Class concepts consist of texts or images representing a given class associated with a group, such as \u201cMuslim\u201d. Here, we opted for the visual representation. Classes are organized into concept groups such as \u201cReligion\". Good/bad concepts refer to positive or negative representations, either as an image or as a text. The definition of good and bad concepts is inherited from the MMBias dataset, which in turn is defined by Caliska et al. [4]. Thus, a text is considered biased if it contains harmful, derivative, or precedent information. We also consider this definition when proposing the new, less politically charged word sets. Here, we use textual descriptions for these concepts, such as \u201cPeace\u201d or \u201cTerror\u201d. Our main goal is to investigate how often a multimodal model associates positive/negative terms to specific groups by comparing images (class concepts) and texts (good/bad concepts).\nFollowing the standard flow of multimodal models, the distance between these modalities (d) can be calculated to identify the degree of disparity between these representations. Employing this distance in conjunction with the biased image/text embedding, the FairPIVARA algorithm (Figure 1, right) can be applied to mitigate biases, which generates new embeddings after dimension removal. Our methodology is further described in Section 3.3."}, {"title": "3.2 Dataset", "content": "Two main sets were used: the bias and target task sets. The bias set comprised a portion of the MMBias dataset, which contains 3,500 images (visual class concepts) categorized into five religious groups, four nationalities, two forms of disability, and sexual orientation, with 250 images available for each class. Additionally, 250 images representing Good/Bad concepts were included, as identified by Steed and Caliskan [18]. The dataset also provides 280 English phrases (textual class concepts) corresponding to each class, such as \u201cThis is a Christian person\u201d. Moreover, 60 texts considered good and 60 bad concepts were provided. The original work collected all images and texts via the Flickr API.\nWe use MMBias images for class concepts and texts for good/bad concepts, as shown in Figure 2. We chose this specific portion because (1) we believe textual terms are better than images to semantically describe good/bad concepts, and (2) the provided textual class concepts do not adequately represent the classes. For instance, class concepts for the \"Chinese\" class include \u201cqiang\u201d, \u201cwen\u201d, \u201ccheng\". We also noted that MMBias good/bad sets mostly portray politically charged concepts (e.g., \u201cterrorism\u201d, \u201cfanaticism\u201d). For this reason, we included 60 new words for each good and bad concept. We refer to this set as the less politically charged set. These new texts were included in English and Portuguese for CAPIVARA.\nIn addition to the data provided by MMBias, we added a new target task set of images for the CAPIVARA model, which was not originally included in the CLIP model. We introduced 250 images representing Brazilian nationality, collected using Google's search algorithm with keywords to capture a broad image range. A native human annotator selected images representing different parts of the country and intersections with existing concepts, such as \"This is a Christian Brazilian.\u201d All images were sourced under a Creative Commons license."}, {"title": "3.3 FairPIVARA", "content": "Our model reduces bias by comparing its generated representations to good or bad concepts. This process involves contrasting each image input with previously selected concepts considered good or bad (Figure 3). The model encodes"}, {"title": "4 Experiments and Results", "content": "In this section, we present two analyses that demonstrate bias mitigation using FairPIVARA: individual (Section 4.1) and relative bias (Section 4.2). These analyses allow us to examine biases associated with each concept individually (Equation 1) and biases that arise when comparing one concept to another, following MMBias analysis [11]. It is essential to highlight that the FairPIVARA application is only based on Equation 1. However, we use the relative score to analyze our method further. In addition, the bias analysis performed for mitigation in FairPIVARA only considers the less politically charged set, although, in examining the results, we also consider the MMBias set.\nFor the results shown here, we used $\\theta = 0.05$, removing N = 54 dimensions, roughly 10% of the total number of dimensions in the embedding space. This configuration provided the most effective bias mitigation. A detailed comparison of results using different configurations can be found in Appendices A.4 and A.5."}, {"title": "4.1 Individual Bias", "content": "Tables 1, 2, and 3 show the top-15 good/bad concepts most frequently attributed for each class by the OpenCLIP model and the CAPIVARA model with and without FairPIVARA. We use a color-coded bias spectrum for visual interpretation. Red indicates bad concepts, while green indicates good ones. A class with more negative than positive values is negatively biased. Ideally, the model should have a neutral bias, where equal numbers of positive and negative words are attributed to each class. The color intensity corresponds to the average degree of similarity between the good/bad concepts and the image set (Equation 1).\nTable 1 presents the baseline results, without applying FairPIVARA, for the less politically charged dataset, aiming for a more neutral baseline by reducing political bias. The OpenCLIP model results are shown at the top of the table, while the CAPIVARA model results at the bottom. Some concepts exhibit significant bias, either positive or negative. For example, in the context of religion, \u201cChristianity\u201d and \u201cBuddhism\u201d show a high positive bias, while \u201cJudaism\" and \"Islam\" display a strong negative bias. This behavior is observed in both the English model and CAPIVARA, where fine-tuning for language sometimes reinforces bias, possibly due to the linguistic bias inherent in the image captions used. We hypothesized that using other languages with broader representation of these religions could help mitigate the negative bias.\nTable 2 shows the CLIP model results after bias mitigation using FairPIVARA. Dimension removal was performed on our less politically charged set (upper part) and MMBias set (lower part). For the less politically charged set, the positive and negative biases highlighted by the light colors are remarkably reduced, indicating that more words are used to represent each concept. While FairPIVARA effectively reduces bias in these seen terms, the untreated terms (MMBias set) still display strong biases, possibly because they are affected by other dimensions. Through the colors, with a lower score, and also through the figure A5, we observe that after applying FairPIVARA, the model starts to have a better distribution, using different words. However, we can still observe that there are words that are more used or preferred to be assigned to certain classes. The repetition of the terms between the different lines shows this.\nTo demonstrate FairPIVARA's effectiveness in other languages, Table 3 shows results from the CAPIVARA model with bias mitigation comparable to those of the CLIP model. In the upper section, the same light-color behavior observed for OpenCLIP on the less politically charged set can be seen for CAPIVARA, indicating the variation in word usage before and after the mitigation. In the lower section, the second set of words translated from the"}, {"title": "4.2 Relative Bias", "content": "We conducted a second analysis to examine the interrelationship between pairs of classes. For that, we used the Caliskan cosine similarity metric [4] similar to MMBias algorithm, which measures the distance between sets of images, X and Y, and Good and Bad texts, denoted as d(X, Y, Good, Bad). This distance indicates the relationship between classes X and Y with the sets of good/bad concepts. A positive distance means class X is more frequently associated with good concepts than Y, while a negative value indicates that Y is more frequently associated with good terms. A higher absolute value suggests a larger discrepancy between the classes.\nTable 4 presents the relative bias results across four concept groups disability, nationality, religion, and sexual orientation - each with its corresponding classes. A color gradient highlights the values, with orange indicating a dominance of class X and yellow showing a greater weight for class Y. The first group on the left shows relative values from the base OpenCLIP model, which used no bias mitigation techniques. This model has a noticeable imbalance, with absolute values reaching 1.71, such as in the Christian and Jewish comparisons. This exemplifies a strong positive score between the two concepts, with highly positive texts linked to the first class's images and highly negative texts linked to the second. This suggests significant bias, likely inherited from data sourced mainly from countries with large Christian populations, potentially leading to prejudices against Jews or other groups.\nThe results for the same OpenCLIP-based model, but with bias mitigation algorithms, are presented in the center. We used two methods: MMBias [11] and FairPIVARA. Each method has two columns: one showing the new bias after applying the method and the other showing the percentage bias reduction. MMBias reduces bias by an average of 10.8%, with a maximum of 61.1% and a minimum of 0%. However, the average bias remains -0.57, similar to the base model (-0.64). FairPIVARA shows a more significant reduction, averaging 92.8%, with biases nearly eliminated to an average of 0.01.\nWe also applied FairPIVARA to the CAPIVARA model to evaluate whether these results hold in models trained in other languages. The overall bias reduction was 97.9%, with an average bias of 0.003, against -0.55 from the CAPIVARA"}, {"title": "4.3 Classification Performance", "content": "We also evaluated the models' final performance with and without bias mitigation for downstream tasks using ImageNet-1K [5] and the ELEVATER image classification toolkit [12]. ELEVATER is a benchmark of 20 datasets for image classification tasks across various domains, with a ready-to-use toolkit for evaluating pre-trained language-augmented visual models. We conducted evaluations in both English and Portuguese. For the Portuguese evaluation, we manually translated the labels for each dataset and the templates, following the methodology of dos Santos et al. [6].\nTable 5 presents the performance results. For ImageNet with the OpenCLIP model, comparing results with and without bias mitigation, top-1 accuracy dropped by 0.5 pp and top-5 accuracy by 0.3 pp. For the CAPIVARA model, top-1 accuracy decreased by 1.2 pp and top-5 by 1.1 pp. In the CIFAR-100 dataset, the OpenCLIP model showed a 0.7 pp drop in accuracy with bias mitigation, while the CAPIVARA model dropped by 0.9 pp. For the ELEVATER benchmark, we report the average results across all datasets. The OpenCLIP model's performance decreased by 0.8 pp, while the CAPIVARA model dropped by 1.0 pp.\nBias mitigation consistently led to a slight performance decline across all datasets and models. However, the drop never exceeded 1.5 pp. We hypothesize that this slight decrease is due to the loss of bias from removing certain feature dimensions. While improving model performance, these dimensions exploit biases in the data that can be quite harmful in a real-world setting. For example, racial biases can be used to maximize a probabilistic outcome in a particular society and context. However, they do not represent individuals in general [14]. We must also emphasize"}, {"title": "5 Conclusion", "content": "Deep learning models must not only achieve high performance but also provide reliable and fair services. Despite the push from industry and academia to develop large-scale models and datasets aimed at surpassing previous results, many of these models still suffer from significant bias and fairness issues. In this study, we examined two leading vision-language models, CLIP and CAPIVARA, and not surprisingly identified existing biases. We proposed FairPIVARA, a bias removal algorithm that balances classes and reduces overall bias across all concepts by up to 98%.\nThe next step in our research will involve expanding the investigation to include more concepts and a larger dataset. This will help create more equitable models and enhance the ability to remove bias, reducing the influence of the dataset and researchers themselves. We plan to apply FairPIVARA to other multimodal architectures and explore the bias removal process in these new frameworks. Optimizing the algorithm for time efficiency will be crucial, mainly through parallelizing dimension verification."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Limitations", "content": "FairPIVARA effectively reduces bias in multimodal and multilingual models but has limitations. As a dimension removal technique, its processing time depends on the model's embedding size and the number of dimensions removed. Larger embeddings and more dimensions increase the application time. In tests, removing 54 dimensions from a CLIP model on 12 GB of memory took 14 hours without optimization or parallelism. This technique may reduce the overall information content in the embedding, though model performance remains similar.\nMoreover, FairPIVARA must be reapplied to each biased model, targeting the final representation without needing model retraining. However, it requires reapplication if the model is retrained or other biased terms are addressed, with optimization needed for different languages and terms."}, {"title": "A.2 Ethics Statement", "content": "FairPIVARA is a bias removal method that balances classes and reduces overall bias across all concepts by up to 98% from CLIP and CAPIVARA models while promoting a more balanced label distribution within the model. For this purpose, FairPIVARA reduces the bias by removing the most affected dimensions of feature embeddings. The bias mitigation consistently led to a slight performance decline across all datasets and models. The minimal impact on accuracy suggests that FairPIVARA effectively reduces unwanted biases while maintaining the models' predictive power.\nThe datasets used contain data with cultural, political, and religious positioning. FairPIVARA reduces bias by comparing its generated representations to good or bad concepts. The definition of good and bad concepts can vary depending on the values, principles, and language of the person or society using the model. Thus, the datasets used and the proposed method do not represent the values of every group. This can lead to linguistic biases and a lack of representativeness for some groups.\nComputer science researchers carried out this research with the support of a linguistics professional to help map good and bad words. As such, this research carries biases from these knowledge domains, influencing how each word was interpreted and classified, how the results were analyzed, and how the paper was written. Further research is needed to create a more equitable model and enhance the ability to remove bias. For this, it is necessary to expand the investigation to include more concepts and a larger dataset and mitigate the researchers' bias."}, {"title": "A.3 Results on ELEVATER and ImageNet-1K", "content": "In our supplementary experiments on the ImageNet-1K dataset and the ELEVATER benchmark, we evaluated the OpenCLIP model, CAPIVARA, and CAPIVARA + Opt, a variant optimized for efficiency. To mitigate bias, we applied an algorithm that identifies dimensions for removal in both OpenCLIP and CAPIVARA models, each with its own subset of dimensions. CAPIVARA + Opt used the same dimensions as CAPIVARA.\nAs shown in Table A1, bias mitigation led to a slight performance decrease in most Portuguese-language datasets in ELEVATER: OpenCLIP decreased by 0.54%, CAPIVARA by 1.03%, and CAPIVARA + Opt by 1.02%. This is expected, as some removed dimensions likely contributed to better performance.\nInterestingly, bias mitigation improved performance in some datasets. For instance, OpenCLIP showed gains in Caltech-101, KITTI-Distance, MNIST, and Patch Camelyon; CAPIVARA in Caltech and MNIST; and CAPIVARA + Opt in KITTI-Distance. We hypothesize that dimension removal eliminated redundant information, boosting performance.\nIn the Portuguese ImageNet-1K dataset, all experiments decreased performance with bias mitigation, though the reduction was minimal. For the English version, only the OpenCLIP model was tested, showing a similar trend (Table A2). Bias mitigation led to improved performance in EuroSAT and KITTI-Distance (2.48 and 1.83 percentage points, respectively) and a minimal reduction of 0.49% in ImageNet-1K."}, {"title": "A.4 Result with Different Number of Dimensions Removed", "content": "The proposed technique removes dimensions with higher bias values to reduce overall model bias. To avoid a completely greedy approach, it is necessary to predefine the number of dimensions to be removed. We studied the optimal number of dimensions for removal, as shown in Table A3, where the same dimensions were removed for both text and image. Figure A1 demonstrates the average bias reduction achieved through these removals."}, {"title": "A.5 Theta Size", "content": "Figure A2 illustrates the impact of varying theta values on bias mitigation. The results show a gradual reduction in bias starting at a theta value of 0.01. Bias decreases as theta increases, with 0.05 being the most effective, achieving the optimal bias reduction within the analyzed subset. Beyond 0.05, the effectiveness plateaus, with no significant improvement observed up to 0.11. The \u201cw/o Theta\" case (without theta) shows that bias mitigation is less effective than the optimal theta value. The tested range of theta values was from 0.01 to 0.11, with increments of 0.01."}, {"title": "A.6 Qualitative Analysis", "content": "Qualitative analyses were performed to visually examine the biases introduced by the model. Figures A3 and A4 showcase the results of an image retrieval task based on specific terms. This task was chosen because it involves optimized image embeddings and is inherently visual.\nIn the first example, related to the term \"Fanaticism\", Figure A3 shows images retrieved by the unmodified OpenCLIP model in the upper section and those retrieved by OpenCLIP with FairPIVARA applied in the lower section. The unmodified model exhibited a nationality bias, with most images reflecting characteristics of Middle Eastern countries. In contrast, the FairPIVARA-modified model retrieved images representing a more diverse range of nationalities.\nIn the second example, for the term \"Indecent\" the images retrieved by OpenCLIP showed a potential gender bias, with several images of women and one of an LGBT couple. After applying FairPIVARA, the bias remained largely unchanged. However, the LGBT couple was no longer included in the group of indecent images. This indicates that while FairPIVARA reduces certain biases and broadens the representation of concepts, some biases still persist."}, {"title": "A.7 Distribution of Associations", "content": "Figure A5 shows the distribution of labels associated with their respective concepts. The frequency of each concept's occurrence is averaged for each set. Labels with negative bias are shown as negative values, while positive values represent those with positive bias. In the base CLIP model (orange), a few terms are used for each concept, leading to a concentration of bias in a few labels. As the graph shows the average across concepts, the model tends to use the same terms repeatedly, resulting in a high overall bias.\nThe blue bars, representing the CLIP model after applying FairPIVARA, show a broader distribution of terms. This leads to a more diverse use of terms, reducing bias and balancing the distribution of positive and negative terms. This approach avoids overusing specific terms, increasing variability in image captions."}]}