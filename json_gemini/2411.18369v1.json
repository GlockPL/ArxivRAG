{"title": "G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation", "authors": ["Tianxing Chen", "Yao Mu", "Zhixuan Liang", "Zanxin Chen", "Shijia Peng", "Qiangyu Chen", "Mingkun Xu", "Ruizhen Hu", "Hongyuan Zhang", "Xuelong Li", "Ping Luo"], "abstract": "Recent advances in imitation learning for 3D robotic \u0442\u0430- nipulation have shown promising results with diffusion- based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and se- mantic understanding. We present G3Flow, a novel frame- work that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foun- dation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation re- quirements. By incorporating semantic flow into diffusion policies, we demonstrate significant improvements in both terminal-constrained manipulation and cross-object gen- eralization. Extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% average suc- cess rates on terminal-constrained manipulation and cross- object generalization tasks respectively. Our results demon- strate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic manip- ulation policies.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed significant advances in imita- tion learning for robotic manipulation, leading to remark- able achievements across various tasks [1, 5, 15, 16, 40]. Image-based imitation learning methods often face chal- lenges in precise manipulation and sample efficiency due to their limited ability to capture geometric relationships. In parallel, researchers have developed 3D imitation learning methods utilizing point clouds [4, 10, 39] or voxel [6, 29] representations, which enhanced few-shot learning capabil- ities by capturing geometric information. Among these ap- proaches, 3D diffusion-based policies [10, 39] have shown promising results across multiple robotic tasks, owing to their superior ability to model multi-modal distributions. However, these geometry-centric methods, despite their ad- vantages, often lack the crucial semantic understanding nec- essary for sophisticated manipulation tasks. For instance, in pose-aware manipulation scenarios such as shoe place- ment or tool organization, purely geometric representations struggle to identify semantically meaningful parts (such as"}, {"title": "2. Related Work", "content": "Semantic fields have emerged as a promising direction for enhancing robotic manipulation by providing a rich seman- tic understanding of the environment [24, 28, 33, 34]. These approaches aim to bridge the gap between geometric per- ception and semantic comprehension, which is crucial for advanced manipulation capabilities.\nD\u00b3Fields[34] pioneered the integration of dynamic 3D descriptor fields for manipulation, while subsequent works further expanded the potential of semantic fields. OVMM[24] explored open-vocabulary mobile manipula- tion through vision-language models, GenDP[33] addressed category-level generalization in diffusion policies, and F3RM[28] enabled natural language specification through CLIP-based semantic distillation.\nHowever, fundamental challenges remain in obtaining and maintaining reliable semantic fields for robotic ma- nipulation. Current approaches like D\u00b3Fields[34] and GenDP[33] heavily rely on manual annotation of reference objects and face significant challenges during object in- teractions. Specifically, occlusions not only result in in- complete object observations but also pose significant chal- lenges to feature acquisition, severely affecting semantic understanding during manipulation. These limitations un- derscore the need for a paradigm shift in how semantic fields are constructed and maintained during manipulation tasks a challenge that our work addresses through a novel foundation model-driven approach that enables dynamic, object-centric, and complete 3D semantic fields in real-time"}, {"title": "2.1. 3D Semantic Fields for Robotics Manipulation", "content": "the toe of a shoe) or handle cases where semantically similar objects exhibit significant geometric variations. This limita- tion highlights a critical research direction: the integration of rich semantic information from 2D images with geomet- ric features from 3D representations. Such integration is essential for advancing performance in tasks that demand both precise spatial control and semantic comprehension, potentially bridging the gap between geometric precision and semantic understanding in robotic manipulation.\nSeveral approaches have recently emerged to address this semantic understanding challenge in robotic ma- nipulation. D\u00b3Fields [34] introduced dynamic 3D de- scriptor fields, while subsequent works like GenDP [33] explored category-level generalization through semantic fields. However, these methods face significant practical challenges: they require manual annotation of reference ob- jects and struggle with maintaining semantic consistency during dynamic interactions. Specifically, occlusions dur- ing manipulation not only result in incomplete object obser- vations but also pose significant challenges to feature acqui- sition, severely affecting semantic understanding and limit- ing their real-world applicability.\nIn this paper, we propose G3Flow, a novel foundation model-driven approach that constructs real-time semantic flow, a dynamic, object-centric, and complete 3D seman- tic representation that maintains consistency even under oc- clusions. Our key insight is to leverage the complemen- tary capabilities of foundation models: 3D generative mod- els create precise digital twins from multi-view observa- tions, vision foundation models extract rich semantic fea- tures, and robust pose tracking models enable continuous semantic flow updates. This combination eliminates man- ual annotation while ensuring persistent semantic under- standing throughout the manipulation process. Specifically, our framework operates in two phases: (1) Initial semantic field establishment through object-centric exploration and 3D object generation, where a robot actively gathers multi- view observations to create a comprehensive digital twin and its base semantic field; and (2) Semantic flow gener- ation through real-time pose tracking, which continuously transforms the semantic field to create a dynamic flow that aligns with the physical object during manipulation, main- taining completeness even under occlusions. This seman- tic flow serves as a powerful enhancement for downstream manipulation policies, enabling them to better handle both precise control tasks and object variations.\nThrough extensive experiments on both terminal- constrained manipulation and cross-object generalization tasks, we demonstrate that policy with G3Flow signifi- cantly outperforms existing methods, achieving up to 68.3% v.s. 46.2% and 50.1% v.s. 31.7% success rates compar- ing to next best method on these two tasks. Our ap- proach achieves superior success rates in precise manip- ulation tasks and shows strong generalization capabilities across object variants, validating the effectiveness of our semantic flow framework. These results demonstrate the potential of enhancing imitation learning policies with rich semantic representations, paving the way for more precise and generalizable robotic manipulation.\nOur key contributions can be summarized as follows: (1) We propose a novel foundation model-driven approach for constructing semantic flow, a dynamic and complete seman- tic representation through the integration of 3D generation, detection, and pose tracking models, enabling real-time un- derstanding robust to occlusions without manual annota- tion. (2) We develop a semantic flow-based imitation learn- ing framework that leverages the dynamic semantic rep- resentation for enhanced manipulation, enabling both pre- cise terminal control and effective generalization across ob- ject variations. (3) Through extensive experimental valida- tion, we demonstrate that our semantic flow significantly enhances imitation learning policies, achieving up to 68.3% and 50.1% success rates on terminal-constrained tasks and cross-object generalization respectively."}, {"title": "2.2. 3D Generative Models for Robotics Simulation", "content": "Recent advances in 3D object generation have witnessed various foundational models employing different techni- cal approaches. Early attempts like GET3D [3] lever- aged generative adversarial networks to produce textured 3D meshes from images, while Point-E [20] and Shap- E [9] explored text-to-3D generation through point clouds and implicit functions, respectively. Following these works, diffusion-based approaches such as DreamFusion [23] and Magic3D [14] demonstrated improved capability in synthe- sizing high-resolution 3D content from text descriptions. However, these methods often struggle with generating in- tricate geometric details and high-fidelity textures crucial for realistic robotic applications. To address these limita- tions, Rodin [37] was developed with enhanced generation capabilities for producing detailed and textured 3D objects. Its superior performance in generating high-fidelity 3D as- sets has been validated in practical robotics applications, such as RoboTwin [18], making it particularly suitable for our work in creating realistic virtual simulations."}, {"title": "2.3. Diffusion Models for Imitation Learning", "content": "Diffusion models [7, 31] are a powerful class of generative models that model the score of the distribution (the gradi- ent of the energy) rather than the energy itself [27, 30]. The key idea behind diffusion models is their iterative transfor- mation of a simple prior distribution into a target distribu- tion through a sequential denoising process. In robotics, diffusion-based policies [1, 2, 8, 12, 13, 17, 19, 22, 26, 38] have demonstrated impressive performance in learning complex manipulation skills from demonstrations. Recent works have explored various directions: 3D Diffusion Pol- icy [38] combines 3D scene representations with diffusion objectives, ChainedDiffuser [36] focuses on trajectory gen- eration between keyposes, and 3D Diffuser Actor [10] tack- les joint keyposes and trajectory prediction. However, these approaches primarily operate on geometric representations without explicit semantic understanding, limiting their pre- cision in terminal-constrained manipulation and generaliza- tion across object variations. Our work G3Flow addresses this limitation by leveraging Foundation Models to main- tain accurate and consistent semantic information during dynamic interactions, enabling more precise and general- izable manipulation capabilities."}, {"title": "3. Method", "content": "We formulate our problem as how to get and maintain se- mantic flow Ousf, and how to learn a visuomotor pol- icy \u03c0: \u039f \u2194 A from expert data, where the observation space O is composed of real point cloud observations Or and Ousf. Our key insight is to leverage foundation mod- els to construct and maintain complete 4D semantic un- derstanding during dynamic interactions through real-time semantic flow, which addresses the limitations of existing geometry-centric approaches in handling occlusions and se- mantic variations.\nOur framework operates in two phases: (1) Initial se- mantic flow construction through object-centric exploration and digital twin generation, where a robot actively gath- ers multi-view observations to create a comprehensive dig- ital twin and extract its semantic features; and (2) Dy- namic flow maintenance through real-time pose tracking, which continuously transforms these semantic features to align with physical objects during manipulation, maintain- ing completeness even under challenging occlusions or par- tial observations. Specifically, we first employ a 3D gen- erative model to reconstruct high-fidelity digital twins from multi-view RGB observations, leveraging the model's em- bedded knowledge to accurately infer even unseen object parts. The reconstructed twins enable semantic feature ex- traction through DINOv2 [21] and dimensionality reduction via PCA [11] in a virtual environment, creating an initial semantic point cloud. We then utilize FoundationPose [35] for robust object pose tracking in real-world scenarios, en- abling dynamic transformation of these semantic features while preserving completeness under occlusions and partial observations.\nOur system, G3Flow, consists of five key modules de- tailed in the following sections: a) Object-centric Explo- ration for active multi-view observation collection; b) Ob- ject 3D Model Generation through 3D generative models; c) Virtual Semantic Flow Generation combining digital twins with vision foundation models; d) Spatial Alignment via Object Tracking; and e) G3Flow-enhanced Diffusion Pol- icy leveraging both Or and Ovs f for precise manipulation."}, {"title": "3.1. Overview", "content": "We formulate our problem as how to get and maintain se- mantic flow Ousf, and how to learn a visuomotor pol- icy \u03c0: \u039f \u2194 A from expert data, where the observation"}, {"title": "3.2. Initial Semantic Flow Construction", "content": "Object-Centric Exploration. To construct an accurate and complete semantic flow, our first phase focuses on ob- taining comprehensive object observations. Conventional single-view approaches face two critical challenges: First, poor initial object poses can lead to incomplete reconstruc- tions due to self-occlusions (e.g., a mug's handle being hidden from the camera view). Second, during manipula- tion, the robot arm often occludes the camera's view of the target object, resulting in information loss. As shown in Fig. 3, while single-view reconstructions may appear plau- sible, they often fail to capture crucial geometric details necessary for manipulation.\nTo address these challenges, we develop an active ex- ploration strategy. We first employ Grounded-SAM [25] to"}, {"title": "3.3. Dynamic Semantic Flow Maintenance", "content": "Spatial Alignment via Object Tracking. Once the initial semantic flow is established, maintaining its accuracy dur- ing dynamic manipulation becomes crucial. We achieve this through continuous spatial alignment between the semantic flow and the physical object.\nBy integrating Grounded-SAM with task descriptions, we first detect and segment the target object from single- perspective RGB images to obtain masked RGBD observa- tions. These observations, combined with the previously generated digital twin, enable FoundationPose [35] to com- pute the initial object pose matrix Minit. During manipu- lation, we continuously update our pose estimates through FoundationPose, obtaining precise object poses Mupdate at each timestep. This enables the dynamic transformation of the semantic flow through:\n$\\text{P\\textunderscore update}} = [(\\text{M\\textunderscore c2w M\\textunderscore update}})(\\text{M\\textunderscore c2wM\\textunderscore init}})^{-1}]\\text{P\\textunderscore init}$ (1)\nThe key advantage of our approach lies in Foundation- Pose's ability to maintain accurate pose estimation even un- der significant occlusions, leveraging the rich information contained in our digital twins. Since our feature point cloud is obtained from complete observations in virtual space, we consider it optimal. Rather than repeatedly detecting, seg- menting, and computing features at each timestep-which could lead to compounding errors-we transform this high- quality feature point cloud directly. This approach not only provides accurate and complete semantic flow estimates during occlusions but also ensures computational efficiency and robustness during dynamic interactions."}, {"title": "3.4. G3Flow-Enhanced Diffusion Policy", "content": "To effectively leverage our semantic flow for precise ma- nipulation, we enhance diffusion policies through three key components: conditional feature acquisition, a conditional denoising process, and a specialized training procedure."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to evaluate G3Flow's ef- fectiveness in enhancing policy performance across two key aspects: terminal constraint satisfaction and cross-object generalization."}, {"title": "4.1. Experimental Setup", "content": "We evaluate our approach on five distinct manipulation tasks from the RoboTwin benchmark [18], as illustrated in Figure 6. Each task is designed to assess specific aspects, detailed task descriptions can be found in Appendix A.:"}, {"title": "4.2. Evaluation on pose-aware manipulation tasks", "content": "To investigate the ability of our method to provide semantic information that enhances the policy's understanding of the semantics of the manipulated object parts, we selected Shoe Place, Dual Shoes Place, Tool Adjust, and Bottle Adjust as test tasks, requiring the robotic arm to meet pose-aware re- quirements. We chose objects that are geometrically similar to the training set for testing, to reduce the examination of the model's generalization ability, as shown in Fig. 7. We chose unseen objects as the test set to avoid the situation that the policy memorizes training objects, which cheats the performance results.\nAs shown in Tab. 1, G3Flow consistently outperforms baseline methods in achieving pose-aware requirements across all four tasks. Our method achieves over 25% higher success rates in the Shoe Place (T) task for correct orien- tation and in the Bottle Adjust (T) task, we achieved a suc- cess rate that exceeded the average of the baselines by over 38% for upright pick. This demonstrates that the semantic understanding provided by G3Flow helps the policy better"}, {"title": "4.3. Evaluation on Generalization Performance", "content": "To investigate the generalization capability of our method in providing semantic information for manipulating objects, we have selected Shoe Place, Dual Shoes Place, Tool Ad- just, and Diverse Bottles Pick as test tasks. Unlike tasks that require the satisfaction of terminal constraints, we choose as few and similar visible objects as possible for the train- ing set and select objects that are as geometrically distinct as possible from the training set for the test set, as shown in Figure 8. This requires the policy to correctly manip- ulate objects that are geometrically different from those it has seen with only a limited exposure, focusing on assess- ing the policy's generalization ability.\nOur method achieves an average success rate across the four tasks that are 18.4% higher than that of the strongest baseline algorithm, exhibiting strong generalization capa- bilities across different object categories and variations, as shown in Table 2:\n\u2022 Intra-class Generalization: In tasks involving geomet- rically distinct unseen instances of the same object cat- egory (Shoe Place (G), Dual Shoes Place (G), Diverse Bottles Pick (G)), our method maintains optimal perfor- mance, indicating that G3Flow encompasses a genuine semantic understanding of objects, enabling effective op- eration generalization even when faced with geometri- cally diverse instances within the same category.\n\u2022 Cross-category Generalization: For the Tool Adjust (G) task, which necessitates dealing with objects that are se- mantically similar but belong to different categories, our method must learn to grasp positions akin to a handle on the objects while also fulfilling the pick-up-upright condi- tion. G3Flow achieved a success rate of 70.7% on previ- ously unseen tool categories, which is 13.4% higher than the best baseline. This result confirms the capability of our method to transfer learned operational skills across different object categories.\n\u2022 Scale Variation: In the Diverse Bottles Pick (G) task, G3Flow successfully generalizes to bottles of varying sizes, maintaining a consistent grasp success rate of 51.3% across size variations. This indicates robust han- dling of geometric variations while preserving semantic understanding."}, {"title": "4.4. Ablation Study", "content": "To explore the advantages of our complete, dynamic, object-level semantic flow representation, we conduct the ablation study with conventional scene-level feature clouds. We selected the Shoe Place (T) and Dual Shoes Place (T) tasks for comparison because they require adjustments to the orientation of the shoe throughout the entire trajec- tory, which rely more on long-term semantic understand- ing. Additionally, object occlusions are designed into the tasks, posing a greater challenge for semantic comprehen- sion. The result is shown in Tab. 3 and Tab. 4.\nAblation on Quality of Semantic Field. As demonstrated in Tab. 3, our approach achieves a 22.6% and 41.2% in- crease in success rates for the Shoe Place and Dual Shoes Place tasks, respectively. The experimental results indi- cate that our method outperforms the direct acquisition of scene-level feature clouds. This is because the images we input into the VFM are centered around the object, as stated in Sec. 3.2, thus the returned features focus more on the object's semantic information itself. Scene-level feature clouds are often interfered with by irrelevant semantic infor- mation from the scene. Moreover, directly obtained feature clouds cannot handle occlusions that occur during object interactions. As shown in Figure 10, our method can still obtain the complete object semantic flow even under occlu- sion, whereas traditional methods fail to do so, and there is a noticeable loss of the shoe's semantic information itself.\nAblation on Field Generation Frequency. Robotic ma- nipulation tasks have stringent requirements for real-time performance. We test the model inference speed on a sin- gle GPU machine with an unloaded NVIDIA GeForce RTX 4090. According to Table 4, our flow generation speed is over four times the speed of traditional field generation. This is because our entire process only requires a single call to the VFM, with subsequent flow maintenance achieved using Pose Tracking, which is much more lightweight than the VFM and has a faster inference speed [32, 35]."}, {"title": "4.5. Visualizations", "content": "The visualization of G3Flow across our five evaluation tasks is shown in Fig. 9, demonstrating how our real-time se- mantic flow maintains both temporal coherence and spa- tial alignment during manipulation. In each task visual- ization, different colors represent distinct semantic features: orientation-critical regions (pink) for shoe placement tasks, functional parts (blue/green) for tool and bottle manipula- tion, and consistent semantic representations across varied object sizes in the diverse bottles task.\nNotably, our semantic flow remains complete and sta- ble even under partial occlusions from robot arms or object self-occlusion, validating the effectiveness of our founda- tion model-driven approach."}, {"title": "5. Conclusion", "content": "In this paper, we introduced G3Flow, a novel framework that leverages foundation models to construct real-time se- mantic flow for enhanced robotic manipulation. Our ap- proach addresses key limitations in existing geometric- centric methods through semantic flow, a dynamic, object-centric 3D semantic representation maintained throughout manipulation processes. By uniquely integrating 3D gen- erative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking, G3Flow enables complete semantic understand- ing while eliminating manual annotation requirements. Ex- tensive experiments demonstrate G3Flow's effectiveness in both terminal-constrained manipulation and cross-object generalization tasks. These results validate our key insight that maintaining consistent semantic understanding through foundation model integration can substantially improve ma- nipulation performance, particularly in scenarios requiring precise control and object variation handling. Looking for- ward, G3Flow establishes a foundation for semantic-aware robotic manipulation, with future directions toward multi- object interactions and computational efficiency optimiza- tion for real-world deployment."}, {"title": "A. Simulation Tasks", "content": "We provide detailed descriptions of all simulation tasks, as shown in Table 5, totaling 5 tasks."}, {"title": "B. Implementation Details", "content": "This section will provide a detailed introduction to the implementation details of G3Flow as described in the paper, including the setup of the experiments."}, {"title": "B.1. Structure Details", "content": "Vision Foundation Model. We utilize the ViT-S/14 variant and transform all images to a resolution of 420 by 420 pix- els. These are then fed into the model to obtain feature maps of size 30 by 30, where each pixel has a 384-dimensional feature representation. Subsequently, these features are transformed back to the original image dimensions. The PyTorch implementation is as follow:\ndef get_dino_feature (image, transform_size=420, model=None):\nimg, H, W = transform_np_image_to_torch (image, transform_size=transform_size)\nres = model(img) #232 torch.Size([1, 384, 30, 30])\nfeature = np.array(res.cpu().unsqueeze (0))\nnew_order = (0, 1, 3, 4, 2) # torch. Size ([1, 30, 30, 384])\nfeature = np.transpose (feature, new_order)\norig_shape_feature = transform_shape (torch.Tensor (np.transpose(feature [0], (0,3, 1, 2)))\n,H, W)\norig_shape_feature_line = orig_shape_feature.reshape(-1, orig_shape_feature.shape[-1])\nreturn orig_shape_feature, orig_shape_feature_line\nPCA. We employ Principal Component Analysis (PCA) to reduce the feature dimensionality from 384 to 5.\nPerception. For image observations, we uniformly employ a camera setup with a resolution of 320 by 240 pixels and a field of view (fovy) of 45 degrees. We apply Farthest Point Sampling (FPS) to both the feature point cloud and the real observation point cloud, downsampling them to 1024 points. We provide a simple PyTorch implementation of our Feature Pointcloud Encoder as follows:"}, {"title": "B.2. Parameter Details", "content": "Training Setup. The training setup for the Diffusion Policy based on G3Flow is shown in Tab. 6.\nBaselines Setup. We outline the key training settings for the baseline in Tab. 7."}]}