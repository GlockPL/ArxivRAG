{"title": "PROVABLE LENGTH GENERALIZATION IN SEQUENCE\nPREDICTION VIA SPECTRAL FILTERING", "authors": ["Annie Marsden", "Evan Dogariu", "Naman Agarwal", "Xinyi Chen", "Daniel Suo", "Elad Hazan"], "abstract": "We consider the problem of length generalization in sequence prediction. We define a new metric of\nperformance in this setting \u2013 the Asymmetric-Regret- which measures regret against a benchmark\npredictor with longer context length than available to the learner. We continue by studying this\nconcept through the lens of the spectral filtering algorithm. We present a gradient-based learning\nalgorithm that provably achieves length generalization for linear dynamical systems. We conclude\nwith proof-of-concept experiments which are consistent with our theory.", "sections": [{"title": "Introduction", "content": "Sequence prediction is a fundamental problem in machine learning with widespread applications in natural language\nprocessing, time-series forecasting, and control systems. In this setting, a learner observes a sequence of tokens and\niteratively predicts the next token, suffering a loss that measures the discrepancy between the predicted and the true\ntoken. Predicting future elements of a sequence based on historical data is crucial for tasks ranging from language\nmodeling to autonomous control.\nA key challenge in sequence prediction is understanding the role of context length\u2014the number of previous tokens\nused to make the upcoming prediction\u2014and designing predictors that perform well with limited context due to compu-\ntational and memory constraints. These resource constraints become particularly significant during the training phase\nof a predictor, where the computational cost of using long sequences can be prohibitive. Consequently, it is beneficial\nto design predictors that can learn from a smaller context length while still generalizing well to longer sequences.\nThis leads us to the central question of our investigation: Can we develop algorithms that learn effectively using short\ncontexts but perform comparably to models that use longer contexts?\nTo address this question, we introduce a new performance metric\u2014Asymmetric-Regret\u2014which measures the differ-\nence in total prediction loss between an online predictor with limited context length and a benchmark predictor with\na longer context. Unlike classical regret, which assumes both the learner and the benchmark operate under the same\nconditions, Asymmetric-Regret accounts for the asymmetry in context lengths, providing a more realistic assessment\nof performance in resource-constrained settings. With a formal and well-defined notion of Asymmetric-Regret in\nhand, we begin our investigation with the following question: are there algorithms that can attain non-trivial bounds\non the Asymmetric-Regret for natural sequences?\nWe explore this concept through the lens of spectral filtering algorithms (Hazan et al., 2017b, 2018). Spectral filtering\nhas emerged as a robust method for learning linear dynamical systems when the system is unknown and the hidden state\nis unobserved. Beyond their theoretically sound properties, spectral filtering-based predictors have proven practical\nin recent applications. Notably, the Spectral Transform Unit (Agarwal et al., 2023), a neural architecture built using\nspectral filtering, has recently shown promise on sequence prediction over a range of modalities (Liu et al., 2024)."}, {"title": "Our Contributions", "content": "Consider online sequence prediction in which the predictor iteratively receives input $u_t \\in \\mathbb{R}^{d_{in}}$ and then makes a\nprediction $\\hat{y}_t \\in \\mathbb{R}^{d_{out}}$ of the output, after which the true output $y_t$ is revealed. The goal of the predictor is to minimize\nerror according to a given convex and Lipschitz loss function $l_t(y_t, \\hat{y}_t)$. In this work we consider the class of spectral\nfiltering predictors, introduced by Hazan et al. (2017b). A spectral filtering predictor is characterized by parameters\n$(T, M_{i=1}^k, k)$ and outputs predictions $\\hat{y}_t$ of the form\n$\\hat{y}_t = y_{t-1} + \\sum_{i=1}^k M_i u_{(t-1):0} \\phi_i$,\nwhere $u_{(t-1):0} \\in \\mathbb{R}^{d_{in} \\times T}$ is a matrix whose columns are the previous inputs $u_{t-1}, u_{t-2}, ..., u_0$ (possibly zero-padded\nas necessary), ${\\phi_i}_{i=1}^k$ are the T-dimensional spectral filters, ${M_i}_{i=1}^k \\subset \\mathbb{R}^{d_{out} \\times d_{in}}$ are matrices which are learned\nonline, and k is the number of filters used. Hazan et al. (2017b) provide an algorithm to learn ${M_i}_{i=1}^k$ and show this\nachieves nearly optimal regret bounds when measured against the best Linear Dynamical System (LDS) predictor. We\ninvestigate whether it is necessary to use the entire history $u_{(t-1):0}$ to learn the optimal set of matrices ${M_i}_{i=1}^k$. More\nbroadly, we explore whether predictor classes and corresponding online learning algorithms exist that can achieve\ncontext length generalization\u2014that is, they use only a short recent history during learning but perform nearly as well\nas if they had used the full, much longer history length. Of course, predictors which perform poorly on systems that\nrequire long memory can trivially achieve context length generalization if their performance is poor regardless of the\ncontext length used. Therefore, it is important to note that one of the key features of spectral filtering predictors is that\nthey are able to perform well on systems that have long memory (Hazan et al., 2017b).\nTo properly understand context length generalization, we introduce the notion of Asymmetric-Regret. The idea is to\nconsider the regret of learning a predictor from a class which is only allowed to use context length $L'$ against the best\npredictor which is allowed to use (potentially much longer and therefore asymmetric) context length $L$. Let $\\Pi_L$ denote\nthe class of predictors in $\\Pi$ which use context length $L$. Given an algorithm $A(L')$ which learns over predictors from\nsome class $\\Pi_{L'}$, the Asymmetric-Regret over horizon T is\n$\\text{Regret}_{\\text{Asymmetric,T}}(A(L'), \\Pi_L) \\stackrel{\\text{def}}{=} \\sum_{t=1}^T l_t(y_t, \\hat{y}_{A(L')}) - \\min_{\\pi \\in \\Pi_L} \\sum_{t=1}^T l_t(y_t, y_\\pi)$.\nOur main result shows that spectral filtering generalizes from a history of $T^q$, where $q \\in [0, 1]$, to T for certain linear\ndynamical systems. It is formally given in the following theorem."}, {"title": null, "content": "Theorem 1. Let $T \\in \\mathbb{Z}_{> 0}$ and $q \\in [0, 1]$. Consider a sequence $(y_1, ..., y_T)$ generated by an unknown and noiseless\nlinear dynamical system defined by matrices (A, B, C, D) as per Eq. 1. Assume the input sequence $u_{0:(t-1)}$ is suffi-\nciently well-conditioned, satisfying $\\sum_{t=0}^{T-1} (T - t) u_t u_t^\\top \\geq (\\frac{2 |C||B|}{\\sqrt{T}}) I$. Suppose the eigenvalues of A lie within the\n$\\text{range} [0, 1 - \\frac{\\log(T)}{8T^q}] \\cup [1 - \\frac{1}{2T^{5/4}}, 1]$.\nLet $A(L)$ denote Algorithm 1 operating with context length $L$, and let $\\Pi_L$ denote the class of spectral filtering\npredictors using context length L. For the squared loss $l_t(y, y') = |y - y'|^2$ and sufficiently large T, it holds that:\n$\\text{Regret}_{\\text{Asymmetric,T}}(A(T^q), \\Pi_L) \\leq \\tilde{O}(\\sqrt{T})$.\nThis theorem indicates that for any $q \\in [0, 1]$, the Asymmetric-Regret is bounded by $\\tilde{O}(\\sqrt{T})$. However, as q decreases,\nthe class of linear dynamical systems for which this bound holds becomes more restricted due to the eigenvalue\nconditions on A. The spectrum of A determines the memory of the system; when the eigenvalues of A are 1, the system\nis only marginally-stable and standard predictors which aim to use low memory typically fail. Critically, Theorem 1\nholds even for these marginally-stable systems. When interpreting this result, it's important to note that the class of\nspectral filtering predictors $\\Pi_L^{SF}$ which use the full context length are provably able to predict well on marginally-\nstable Linear Dynamical Systems (Hazan et al., 2017b). Therefore, this result implies that spectral filtering predictors\nare able to context length generalize in a nontrivial way.\nInspired by particular spectrum of A that is required for the classical Spectral Filtering algorithm to achieve length\ngeneralization, we develop a novel variation on the Spectral Filtering algorithm, presented in Algorithm 2, which\nachieves length generalization without added assumptions on the spectrum of A (whenever the context-length is at least\n$T^{1/3}$). Algorithm 2 achieves this by using two autoregressive components $y_{t-1}$ and $y_{t-2}$ to construct its prediction $\\hat{y}_t$\nof $y_t$. We provide the following theoretical result."}, {"title": null, "content": "Theorem 2. Let $T \\in \\mathbb{Z}_{> 0}$ and $q \\in [0, 1]$. Consider a sequence $(y_1, ..., y_T)$ generated by an unknown and noiseless\nlinear dynamical system defined by matrices (A, B, C, D) as per Eq. 1. Assume the input sequence $u_{0:(t-1)}$ is suffi-\nciently well-conditioned, satisfying $\\sum_{t=0}^{T-1} (T - t) u_t u_t^\\top \\geq (\\frac{2 |C||B|}{\\sqrt{T}}) I$. Suppose the eigenvalues of A lie within the\n$\\text{range} [0, 1 - \\frac{\\log(T)}{8T^q}] \\cup [1 - \\frac{1}{2T^{1/4}}, 1]$.\nLet $A(L)$ denote Algorithm 2 operating with context length $L$, and let $\\Pi_L$ denote the class of spectral filtering\npredictors using context length L. For the squared loss $l_t(y, y') = |y - y'|^2$ and sufficiently large T, it holds that:\n$\\text{Regret}_{\\text{Asymmetric,T}}(A(T^q), \\Pi_L) \\leq \\tilde{O}(\\sqrt{T})$.\nObserve that if $q > 1/3$, then $[0, 1 - \\frac{\\log(T)}{8T^q}] \\cup [1 - \\frac{1}{2T^{1/4}}, 1] = [0, 1]$ for any $T > 0$ and so we do not\nconstrain the spectrum of A to get length generalization (aside from assuming it has nonnegative eigenvalues).\nOur next contribution is the development of a new class of predictors we call tensorized spectral filters. Tensorized\nspectral filters possess more structure than their original counterparts and are provably more expressive\u2014they can\nlearn a select class of time-varying linear dynamical systems that vanilla spectral filtering cannot. We develop a\nnovel context-length dependent algorithm for tensorized spectral filtering which, similar to Algorithm 2, requires two\nautoregressive components."}, {"title": null, "content": "Theorem 3. Let $T \\in \\mathbb{Z}_{> 0}$ and $q \\in [0, 1]$. Consider a sequence $(y_1, ..., y_T)$ generated by an unknown and noiseless\nlinear dynamical system defined by matrices (A, B, C, D) as per Eq. 1. Assume the input sequence $u_{0:(t-1)}$ is suffi-\nciently well-conditioned, satisfying $\\sum_{t=0}^{T-1} (T - t) u_t u_t^\\top \\geq (\\frac{2 |C||B|}{\\sqrt{T}}) I$. Suppose the eigenvalues of A lie within the\n$\\text{range} [0, 1 - \\frac{\\log(T)}{8T^q}] \\cup [1 - \\frac{1}{2T^{1/4}}, 1]$. Let $A(L)$ denote Algorithm 3 operating with context length $L$, and let $\\Pi_L$ denote the class of spectral filtering\npredictors using context length L. For the squared loss $l_t(y, y') = |y - y'|^2$ and sufficiently large T, it holds that:\n$\\text{Regret}_{\\text{Asymmetric,T}}(A(T^q), \\Pi_L) \\leq \\tilde{O}(\\sqrt{T})$.\nFinally, we experimentally confirm the results of Theorem 1 on synthetic data generated by an LDS. Interestingly, we\nfind that Theorem 1 accurately predicts when length generalization is possible; indeed, when the data is generated"}, {"title": "Background and Setting", "content": "In the online sequence prediction setting the predictor iteratively receives input $u_t$ and makes prediction $\\hat{y}_t$ of the\noutput, after which the true output $y_t$ is revealed. The goal is to minimize error according to a given (convex Lipschitz)\nloss function $l_t (y_t, \\hat{y}_t)$.\nIn online learning, we usually do not make statistical assumptions about the generation of the input sequence. As such,\nperformance is measured relative to a certain benchmark class of predictors. For example, a linear predictor predicts\naccording to the rule\n$\\pi_{M_{1:k},N_{1:l}}(u_{0:(t-1)}, y_{1:t-1}) = \\sum_{i=1}^k M_i u_{t-i} + \\sum_{j=1}^l N_j y_{t-j}$.\nA prediction algorithm A is measured by regret, or difference in total loss, vs. a class of predictors $\\Pi$ (such as linear\npredictors), i.e.\n$\\text{Regret} (A, \\Pi) = \\sum_{t=1}^T l_t (y_t, y_{t_A}) - \\min_{\\pi \\in \\Pi} \\sum_{t=1}^T l_t(y_t, y_{\\pi})$."}, {"title": "Context Length Generalization and the Asymmetric-Regret metric", "content": "We say that an online predictor has context length L if it bases its prediction $\\hat{y}_t$ only on information from the previous\nL timesteps, i.e. $u_{t:t-L}$ and $y_{t:t-L}$. Open loop predictors base their prediction only on $u_{t:t-L}$, whereas closed loop\npredictors can also use $y_{t:t-L}$.\nFor example, the class of all linear open loop predictors with context lengths L is given by\n$\\Pi_{L}^{OL} = \\{ \\pi_{M_{1:L}} | \\pi_{M_{1:L}}(u_{(t-1):0}) = \\sum_{i=1}^L M_i u_{t-i} \\}$.\nThe key question in our work is whether there are predictor classes with corresponding online learning algorithms\nwhich context length generalize in the sense that they learn the best predictor in the class using a short context length,\nbut they perform well compared to the best predictor which is allowed to use long context length. To formalize this\nnotion, we introduce Asymmetric-Regret whose definition we restate here:\nDefinition 4 (Asymmetric-Regret). Let $\\Pi_{L'}$ be a class of predictors which use context length $L'$ and let $\\Pi_L^{ref}$ be a\nreference class of predictors which use context length L. The Asymmetric-Regret with respect to (convex Lipschitz)\nloss $l_t$ over horizon T of an algorithm $A(L')$ which tries to learn a predictor from $\\Pi_{L'}$ is\n$\\text{Regret}_{\\text{Asymmetric,T}}^{ref}(A(L'), \\Pi_L) = \\sum_{t=1}^T l_t(y_t, \\hat{y}_{A(L')}) - \\min_{\\pi \\in \\Pi_L^{ref}} \\sum_{t=1}^T l_t(y_t, y_{\\pi})$."}, {"title": "Spectral Filtering", "content": "Spectral filtering is a notable deviation from the standard theory of linear dynamical systems that allows efficient\nlearning in the presence of arbitrarily long memory (Hazan et al., 2017b). The idea is to project the sequence of inputs\nto a small subspace that is constructed using the special structure of discrete linear dynamical systems. The output of\nthe spectral filtering predictor is represented as\n$\\hat{y}_t = y_{t-1} + \\sum_{i=1}^k M_i u_{(t-1):0} \\phi_i,$\nwhere $u_{(t-1):0} \\in \\mathbb{R}^{d_{in} \\times T}$ is a matrix whose columns are the previous inputs $u_{t-1},..., u_0$ (possibly zero-padded as\nnecessary), {$\\phi_i$}$_{i=1}^k$ are the T-dimensional spectral filters that can be computed offline given the target sequence length\nT, and {M}$_{i=1}^k \\subset \\mathbb{R}^{d_{out}\\times d_{in}}$ are the matrices parameterizing the model. These spectral filters are the eigenvectors of\nthe matrix constructed as the average of outer products of the discrete impulse-response functions as we now detail.\nLet $\\mu_{\\alpha,T} = (1 - \\alpha) [1, \\alpha, \\alpha^2, ..., \\alpha^T]$ be the (weighted) impulse-response vector corresponding to a one dimensional\nlinear dynamical system with parameter $\\alpha$ unfolded to T time steps, and consider the symmetric matrix\n$H_T \\stackrel{\\text{def}}{=} \\int_0^T \\mu_{\\alpha,T} \\mu_{\\alpha,T}^\\top d\\alpha$.\nSince $H_T$ is a real PSD matrix, it admits a real spectral decomposition, and the (non-negative) eigenvalues can be\nordered naturally by their value. Let ${(\\sigma_i \\in \\mathbb{R},\\phi_i \\in \\mathbb{R}^L)}_{i=1}^T$ be the eigenvalue-eigenvector pairs of $H_T$ ordered to"}, {"title": "Learning with a Short Context\u2014Provable Length Generalization for Linear Dynamical\nSystems", "content": "In Algorithm 1, we modify the classical online learning algorithm for spectral filtering to use a shorter context window.\nTo properly define our notion of length generalization, we need to distinguish between context lengths. Thus we\nintroduce the notation for the loss observed with a context length L:\n$l_t(M, L) \\stackrel{\\text{def}}{=} ||y_t - y_{t-1} - \\sum_{i=1}^k M_i u_{(t-1):(t-1)} \\phi_i||^2$,\nwhere ${M_i}_{i=1}^k$ and ${\\phi_i}_{i=1}^k$ are as defined for Eq. 2 and $u_{(t-1):(t-L)} \\in \\mathbb{R}^{d_{in} \\times T}$ is the matrix $u_{(t-1):0}$ but with\nthe columns corresponding to the inputs after context length L. i.e. $u_{t-L-1},..., u_0$, zeroed out. Note that this is\noverloaded notation compared with $l_t(y, y')$ which measures the loss of the true y with the predicted y' as used in our\ndefinition of regret. The context length specific loss can be written equivalently as\n$l_t(M, L) = ||\\hat{y}(M_t, L) - y_t||^2$,\nwhere $\\hat{y}(M_t, L)$ denotes the prediction of $y_t$ using iterate $M_t$ and context window size L as in Eq. 4 of Algorithm 1."}, {"title": "Algorithm 1 Spectral Filtering with Limited Context", "content": null}, {"title": "Tensorized Spectral Filtering", "content": "Adding some form of regularization to a prediction model often results in provably and empirically better generaliza-\ntion in many different problem settings. While length generalization differs from overall generalization, we explore\nhow introducing regularization to spectral filters might enhance length generalization. To this end, we introduce the\nclass of tensorized spectral filters.\nDefinition 8 (Tensorized Spectral Filters). Let $\\phi_1, . . . , \\phi_d$ be the eigenvectors of the d\u00d7d Hankel matrix $H_\\alpha$ as defined\nin Eq. 3. The class of $(d_1, d_2)$-dimensional tensorized spectral filters takes the form\n$\\{ \\phi_i \\otimes \\phi_j / i \\in [d_1], j \\in [d_2] \\}$,\nwhere $\\otimes$ denotes the tensor (Kronecker) product.\nPart of the motivation for considering tensorized spectral filters comes from the tensor structure of the $\\mu_{\\alpha,T}$ which\ndefines the matrix $H_\\alpha$ in Eq. 3. Observe that for a given $\\alpha$ we have,\n$\\mu_{\\alpha, L^2} = (1 - \\alpha^L)^{-1} \\cdot \\mu_{\\alpha,L} \\otimes \\mu_{\\alpha^L,L}$.\nTherefore, up to rescaling, the set of $\\mu_{\\alpha, L^2}$ is contained in the set of tensors $\\mu_{\\alpha,L} \\otimes \\mu_{\\beta,L}$:\n$\\mu_{\\alpha} / \\alpha \\in [0, 1] \\} = \\{ \\mu_{\\alpha} \\otimes \\mu_{\\beta} / \\alpha, \\beta \\in [0, 1] \\}$.\nThis property ensures that we can approximate the spectral filtering algorithm with its tensor approximation as per\nAlgorithm 3. In fact, the tensor spectral filtering algorithm is more expressive, as equation (8) hints. One can construct\nnon-linear dynamical systems that can be approximated by tensorized spectral filters but not by spectral filtering itself.\nSuch a system would have dynamics corresponding to the tensor\n$\\mu_{\\alpha_1} \\otimes \\mu_{\\alpha_2},$\nwhere $\\beta \\neq \\alpha^L$, and thus imply changing dynamics every L iterations. In the following theorem we formalize this\nintuition that tensorized spectral filters are just as capable at modeling linear dynamical systems."}, {"title": null, "content": "Theorem 9. Given any linear dynamical system parametrized by A, B, C, D such that A is a PSD matrix with\n$||A|| \\leq 1$, there exists matrices M', M\", ${M_{ij},i,j \\in [k]}$, such that for sequences $u_{1:T}$ where $||u_t|| \\leq 1$ the\nfollowing holds. Let $y^{LDS}$ be the sequence generated by execution of the LDS via (1), and $y^{TSF}$ be the sequence\ngenerated by Tensor Spectral Filtering via:\n$y_t^{TSF} = 2 y_{t-1} + y_{t-2} + M'u_t + M''u_{t-1} + \\sum_{i,j=1}^k M_{ij} u_{t-2:1} \\cdot \\phi_i \\otimes \\phi_j$.\nThen for all $t \\in [T]$,\n$||y_t^{LDS} - y_t^{TSF}|| \\sim e^{\\frac{k}{L} \\log(T)}$."}, {"title": "Length Generalization with Tensorized Spectral Filtering", "content": "The representation property from Theorem 9 gives rise to a novel algorithm (Algorithm 3). Notably, this algorithm is\nbased on online gradient descent for spectral filtering and can be shown to give the same regret bounds as the original\nspectral filtering algorithm from Hazan et al. (2017b).\nJust as in Theorem 6 and Theorem 7, we have a companion result for tensorized spectral filtering."}, {"title": "Algorithm 3 Tensorized Spectral Filtering", "content": null}, {"title": "Experiments", "content": null}, {"title": "Linear Dynamical System", "content": "We can empirically verify Theorem 6 in an online sequence prediction task where the data is generated by a noiseless\nLDS. We refer to a \u201cbad\u201d region of eigenvalues $(1 - \\frac{\\log(T)}{8T^{7/8}}, 1 - \\frac{1}{2T^{5/4}})$ as Region B, and we define\nRegion A to hug Region B on both sides as shown in Figure 2."}, {"title": "Two Autoregressive Components", "content": "We have seen, both in theory and experiment, that vanilla spectral filtering has an inherent length generalization\ncapability on an LDS under a minor spectral assumption. Introducing a second autoregressive component yields\nAlgorithm 2, which is accompanied by a length generalization guarantee that removes this assumption and applies\nto all (symmetric, marginally-stable) LDS's. We verify this experimentally in Figure 5 to be as adversarial as we\ncan, this experiment is run with all eigenvalues sampled from Region B. As predicted by Theorem 7, the second\nautoregressive component allows for robust length generalization even with context lengths as small as $\\sqrt{T}$."}, {"title": "Induction Heads", "content": "So far, we have demonstrated length generalization of spectral filtering on linear systems: when trained with a shorter\ncontext length of $T^q$ it is able to compete with methods that have access to the full context T (even on marginally-stable\nsystems that can have arbitrarily large effective memory lengths). This length generalization property is most crucial\nin deep learning applications, in which multi-layer models are stacked (with added nonlinearities) to solve non-LDS\nsequence prediction tasks."}, {"title": "Tensorized STU Models", "content": "On the LDS, tensorization made no difference in terms of length generalization capabilities. Intuitively, however, the\ntensor representation does have a natural structure that seems conducive to length generalization. The same machinery\n(the first tensor component) is shared when learning the system's evolution over each chunk of size $\\sqrt{T}$, and the\nsecond tensor component simply aggregates the responses from these chunks. Crucially, the tensorized model is more\nexpressive than an LDS since it does not require the tensor components to learn the pair $(\\alpha, \\alpha^{\\sqrt{T}})$ but instead allows\nany $(\\alpha, \\beta)$ pair. In other words, the tensorized model is free to decouple how it gathers information within the chunks\nfrom how it synthesizes information across chunks. This extra expressitivity is unnecessary to succeed on an LDS, but\nturns out to matter on more complex nonlinear tasks, which we now investigate.\nWe repeat the experimental setup of the previous subsection, with the architectural difference that we replace the k\nmany filters of length 256 with $k^2$ many filters of length 256 formed from tensor combinations with components of\nlength 16. Note that although we increase the size of our filter bank from k to $k^2$, this does not change the number of\nconvolutions due to the tensordot approximation (see Liu et al. (2024) for details)."}, {"title": "Discussion", "content": "In review, we first introduced the notion of Asymmetric-Regret as a way to describe length generalization through\nthe lens of online learning and regret minimization in games. We then proved that the class of spectral filtering\npredictors naturally enjoys sublinear Asymmetric-Regret thereby exhibiting length generalization without any change\nto the algorithm. Next, we used experiments on synthetic data generated by an LDS to demonstrate the validity and"}, {"title": "General Length Generalization", "content": "In this section we introduce a general algorithm which we will use to prove length generalization for our specific\nAlgorithms Algorithm 1, Algorithm 2, and Algorithm 3."}, {"title": "Algorithm 4 General Spectral Filtering", "content": null}, {"title": "OGD Regret for Generalized Spectral Filtering", "content": "Lemma 12. Suppose the input $u_{1:t}$ satisfies $||u_t||_2 \\leq 1$. Suppose the true output $y_t$ evolves such that for some\npolynomial $p_t(y_{t-1:1})$ there exists some $M^{true} \\in K_r$\n$y_t = p_t(y_{t-1:1}) + \\sum_{i=1}^T M^{true} u_{t-1:0} v_i,$\nand for\n$E_{m,T} \\stackrel{def}{=} \\sum_{i=k+1}^T M^{true} u_{t-1:0} v_i,$\nwe have $||E_{m,T}|| \\leq 1$. Further suppose $v_1, ..., v_k$ satisfy $||v_i||_1 < c_i \\log^p (T)$. Let\n$l_t(M, L) \\stackrel{def}{=} ||y_t - p_t(y_{t-1:1}) - \\sum_{i=1}^k M_i u_{t-1:t-L} v_i||^2$.\nThen if $M^t$ are the iterates of Algorithm 4\n$\\sum_{t=1}^T l_t(M^t, L) - \\min_{M^* \\in K_r} \\sum_{t=1}^T l_t(M^*, L) \\leq 12k^{3/2} r^2 \\log^p(T) \\sqrt{T}.$"}, {"title": "Length Generalization on the Best Optimizer in Hindsight", "content": "Lemma 13. Let input $u_{t-1:0}$, ${v_i}_{i=1}^k$, $p_t(\\cdot)$, and $l_t(M,L)$ all be as defined in Algorithm 4. Suppose the input\n$u_{t-1:0}$ is such that $\\sum_{t=0}^{T-1} (T - t) u_t u_t^\\top \\geq (\\frac{2 |C||B|}{\\sqrt{T}}) I$, ${v_i}_{i=1}^k$ is orthonormal with $||v_i||_1 \\leq \\log^p(T)$, and\nthat there exists some $M^{true}$ such that\n$y_t - p_t(y_{t-1:1}) = \\sum_{i=1}^{l_1} M^{true} u_{t-i} = \\sum_{i=1}^{l_1} M^{true} u_{t-i} + \\sum_{i=1}^{t-l_1-1} C A h(A) B u_{t-l_1-i},$\nwhere\n$||\\sum_{i=k+1} M^{true} u_{t-1:t-L} v_i|| \\leq \\frac{||C|| ||B||}{\\sqrt{T}},$\nand\n$\\max_{\\alpha(A)} \\{h(\\alpha) \\alpha^{-l_1-1} (1 - \\alpha^{T-L+1}) (1 - \\alpha)^{-1}\\} \\leq \\frac{1}{T^{1/2}}.$\nLet\n$M^* \\stackrel{def}{=} \\arg \\min_{M \\in K} \\sum_{t=1}^T l_t(M, T)$.\nThen for $T > (4k \\log^p(T) |C| |B|)^4$, the loss with context L well approximates the loss with context T on $M^*$,\n$\\sum_{t=1}^T |l_t(M^t, L) - l_t(M^*, T)| \\leq 8 |C|^2 |B|^2 \\sqrt{T}.$"}, {"title": "Minimization is Recovery", "content": "Lemma 14. Suppose $\\sum_{t=0}^{T-1} (T - t) u_t u_t^\\top > 2 \\epsilon I$ and ${v_i}_{i=1}^k$ is orthonormal. Then there is a unique point $M^*$ which\nminimizes the function $\\sum_{t=1}^T l_t(M,T)$ from Algorithm 4. Moreover, suppose some k satisfies\n$\\sum_{t=1}^T l_t(M,T) \\leq \\epsilon^2$.\nThere there is a matrix $E_M$ such that $||E_M|| < \\epsilon$ and\n$M^* = M + E_M$.\nProof. For convenience, let $X_t$ be the $k d_{in}$-dimensional vector which stacks the filters,\n$X_t =  \\begin{bmatrix}   u_{t-1:t-T} v_1\\\\   u_{t-1:t-T} v_2\\\\   ...\\\\   u_{t-1:t-T} v_k \\end{bmatrix}, \\quad <Y_t = Yt - p_t(Y_{t-1:1}).  Let Y =  \\begin{bmatrix}   Y_1\\\\   Y_2\\\\   ...\\\\   Y_T \\end{bmatrix}  and X =  \\begin{bmatrix}   X_1\\\\   X_2\\\\   ...\\\\   X_T \\end{bmatrix}$.\nwhere the second inequality holds since we only consider $t < T$. Assume k is written as M =[M\u2081 M\u2082 ... Mk]\u1d40Rdout\u00d7kdin and let Yt\nThen we can express the loss as\n$\\min_{M} f(M) \\stackrel{\\text{def}}{=} \\sum_{t=1}^T l_t(M,T) = ||Y - M X||^2$.\nNote that this function is twice differentiable and\n$\\nabla M f(M) = XX^\\top$."}, {"title": "Uniform Length Generalization Around LDS Generated Solutions", "content": "The"}]}