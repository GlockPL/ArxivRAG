{"title": "From Cool Demos to Production-Ready FMware: Core Challenges and a Technology Roadmap", "authors": ["Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Dayi Lin", "Ahmed E. Hassan"], "abstract": "The rapid expansion of foundation models (FMs), such as large language models (LLMs), has given rise to FMware-software systems that integrate FMs as core components. While building demonstration-level FMware is relatively straightforward, transitioning to production-ready systems presents numerous challenges, including reliability, high implementation costs, scalability, and compliance with privacy regulations. This paper provides a thematic analysis of the key obstacles in productionizing FMware, synthesized from industry experience and diverse data sources, including hands-on involvement in the Open Platform for Enterprise AI (\u039f\u03a1\u0395\u0391) and FMware lifecycle engineering. We identify critical issues in FM selection, data and model alignment, prompt engineering, agent orchestration, system testing, and deployment, alongside cross-cutting concerns such as memory management, observability, and feedback integration. We discuss needed technologies and strategies to address these challenges and offer guidance on how to enable the transition from demonstration systems to scalable, production-ready FMware solutions. Our findings underscore the importance of continued research and multi-industry collaboration to advance the development of production-ready FMware.", "sections": [{"title": "I. INTRODUCTION", "content": "FMware refers to software integrating foundation models (FMs), like Large Language Models (LLMs), as core components [1]. Since ChatGPT's release in late 2022, the FMware landscape has exploded, with over 600,000 open-source models, including FMs and other AI models, now available on platforms like Hugging Face [2]. Goldman Sachs predicts FMware could boost global GDP by 7% [3].\nBuilding impressive demos with FMs is relatively easy, but transitioning to production-ready FMware incurs significant challenges due to its inherent complexity as a compound system [1, 4]. Unlike static systems, FMware integrates multiple dynamic components for real-time updates, control, and adaptability [4, 5]. This complexity is reflected in industry adoption rates: a survey of 430 technology professionals revealed that only 10% of organizations had launched FMware in production environments [6]. Respondents identified reliability, high implementation costs, latency, compliance, and privacy as the top challenges preventing them from moving from demos to production-ready FMware.\nThe transition to production-ready FMware is formidable due to its dynamic nature, which combines classical software engineering complexities with unique issues inherent to foundation models. Production-ready FMware must continuously evolve to meet customer expectations, requiring consistent performance, reliability, feature updates, and adherence to service level agreements (SLAs), all while managing operational costs. For instance, OpenAI's infrastructure costs for running ChatGPT in 2023 were estimated at $700,000 per day [7]. Beyond these traditional challenges, FMware developers also face unique problems like handling hallucinations, increased inference costs, and orchestrating tasks across various AI components [1, 8]. LinkedIn's experience illustrates the effort required to reach production readiness: they achieved 80% functionality in a month but spent four more months to complete the remaining 20%, with diminishing returns on each additional 1% improvement [9]. Similarly, Microsoft and GitHub found that testing FMware becomes prohibitively expensive as complexity scales [10]. These real-world examples underscore the need for robust, system-based approaches to build reliable, production-ready FMware solutions [1, 4, 8].\nIn this paper, we employ a thematic analysis to systematically identify key challenges in developing production-ready FMware. We draw on insights from multi-industry collaborations, conferences, customer meetings, hands-on development of the FMware lifecycle platform (FMArts) [1] and literature surveys. In doing so, we provide the first comprehensive articulation of the recurrent issues across different stages of FMware development, consolidating them into challenges that hinder productionizing FMware.\nThis paper complements our vision for Software Engineering 3.0 (SE 3.0), where we advocate for an AI-native, intent-first approach where AI systems evolve from task-driven copilots to intelligent collaborators [11]. Both SE 3.0 and our paper address the need to adapt software engineering practices to manage the complexity and dynamism of AI systems like FMware. By synthesizing hands-on knowledge from building production-ready FMware and interacting with industry experts, we underscore the urgency of addressing these challenges to move from \u201ccool\u201d demos to robust, production-ready solutions. Our study further highlights the importance of ongoing research and multi-industry collaboration as FMware continues to evolve.\nThis paper is organized as follows: Section II covers the evolution of FMware and related work. Section III explains our thematic analysis approach. Section IV outlines the FMware lifecycle and spot recurring issues at each stage that affect production readiness. Section V presents the key challenges hindering FMware production-readiness and suggests future directions. Finally, Section VI concludes the study."}, {"title": "II. RELATED WORK", "content": "Despite a wealth of research on software release readiness [12-16], there is a notable lack of studies on production readiness [17, 18]. Release readiness ensures that software meets functional requirements and is ready for distribution. Whereas, production readiness also guarantees reliable operation in live environments by addressing factors like latency and operational costs. Early work by Mockus [19] explored an analogy-based prediction method to model and forecast the workflow of work items in software projects to asses release readinees of a software. Port and Wilf [15] explored release readiness through a case study at NASA JPL, followed by several studies quantifying and predicting release readiness factors using machine learning [12-14]. More recently, Patel et al. [16] proposed a checklist to assess the release readiness of FMware.\nIn assessing production readiness, Asthana and Olivieri [17] developed a metric focused on software quality and reliability, while Cusick [18] proposed an architecture review method. More recently, Breck et al. [20] in their seminal work, examined the production readiness of machine learning systems, proposing a rubric to reduce technical debt and evaluate readiness. Parnin et al. [10] documented the difficulties of building a production-ready copilots, by interviewing the team that built various copilots at Microsoft. In their influential blog post, Yan et al. [21] shed light on some of the challenges that impact the production-readiness of FMware. However, to the best of our knowledge, our paper is the first work to conduct a systematic study to identify the associated challenges with building production-ready FMware."}, {"title": "III. METHODOLOGY", "content": "We used thematic analysis on various sources of data, including conference reports, meeting notes, and collaborative discussions to identify key challenges in developing production-ready FMware. While traditionally applied in software engineering studies [22, 23] to analyze structured or semi-structured interviews, thematic analysis is also well-suited for extracting themes from unstructured data, such as that we utilized [24]. Thematic analysis typically involves coding the data, identifying patterns, and grouping codes into overarching themes, which allows us to follow a structured approach to ensure the systematic identification of challenges.\nStep 1: Data Collection. We collected data from various sources to gain insights into the challenges that researchers and practitioners encounter when productionizing FMware.\n\u2022 OPEA Project Participation: As active participants, we played a significant role in the OPEA initiative (Open Platform for Enterprise AI (OPEA) [8]). In particular, the last author leads OPEA's research working group. OPEA is a global multi-company collaborative initiative focused on addressing the challenges of making FMware enterprise-ready. OPEA provides detailed frameworks, architectural blueprints, and a four-step assessment for evaluating FMware in terms of enterprise readiness. We used meeting notes (some of which are publicly available [25]) from OPEA discussions, which involved 43 organizations (e.g., Intel, AMD, RedHat, Docket, JFrog, Anyscale, LlamaIndex and SAP).\n\u2022 SPDX Working Group: First author co-led SPDX community meetings (an ISO/IEC 5692:2021 Software Bill of Materials standard [26]) over the past three years, contributing to discussions on the AI Bill of Materials (AI BOM), which included discussing challenges in building production-ready FMware. The meeting notes, available publicly [27], were incorporated into our analysis.\n\u2022 Inception and Launch of the Alware Conferences, summit and Bootcamp: Authors of this paper spearheaded the first ACM In-ternational Conference on AI-Powered Software (AIware) [28], co-located with FSE 2024, the FM+SE summit series [29] and the Alware Bootcamp [30]. Engaging with participants from Google, Microsoft, GitHub, and other industry and academic professionals provided valuable insights into the challenges of productionizing FMware, which we summarized in a report used in our analysis [31].\n\u2022 Conference and Workshop Attendance and Reports: Since late 2022, all authors have actively participated in several top conferences, workshops, and developer meetings relevant to FMware, such as ICSE, FSE, FM+SE 2030, FM+SE Tokyo, SEMLA, OSS Summits, Ray summits and ai_dev Summits. Although these events covered a range of topics beyond the challenges of productionizing FMware, we gained valuable insights by listening to and interacting with researchers and practitioners involved in productionizing their own FMware. After each conference, the attending authors compiled detailed reports summarizing key discussions and relevant research on FMware. These reports formed the foundation for our thematic analysis.\n\u2022 Internal Industry Experience: Our collaborations with customers and development teams to understand FMware's functional and non-functional requirements. Additionally, all authors contributed to developing and productionizing FMArts, a comprehensive lifecycle engineering platform for FMware [1]. Our practical experience in creating FMware with FMArts and making it production-ready [1] and integrating it into Huawei products (e.g., Huawei Cloud) informed the thematic analysis.\n\u2022 In-depth Literature Review: We began with a review of grey literature, including blog posts, whitepapers, and developer forums, and noticed recurring discussions about the production readiness of FMware. Building on these insights, we conducted a focused review of academic papers to find supporting evidence. Through this activity, we provide a comprehensive analysis of the challenges in deploying FMware, grounding all issues in Section IV with citations and minimizing subjective bias.\nStep 2: Data Coding and Initial Thematic Grouping. After data collection, three of the four authors collaborated to conduct the initial coding. Each dataset (conference reports, meeting notes, and collected grey literature) was systematically reviewed, and recurrent issues related to different stages of FMware lifecycle (Figure 1) were coded. An example code for"}, {"title": "IV. RECURRENT ISSUES IN PRODUCTIONIZING FMWARE", "content": "In this section, we outline the stages of an FMware's engineering lifecycle, as depicted in Figure 1, and highlight the recurrent issues observed.\nA. FM(s) Selection\nThe first step in developing FMware is to select the FM(s) to be used. There exist over 600,000 open-source models available (among FMs and the more traditional machine learning models) [2]. Typically, the number of FMs to be selected depends on the complexity and functionality required by the FMware. For example, a simple chatbot may only need one FM, while complex systems might require multiple FMs for its different functionalities.\nDifficulty balancing functional requirements with performance and costs. Selecting FMs for FMware requires balancing functionality with performance, infrastructure, and costs [32]. Demos favor large general-purpose models like ChatGPT-4, neglecting latency and resource limits due to the typically small number of required API calls. LLaMa-70B, for example, needs 140GB VRAM to be hosted locally [33]. In production, smaller FMs are preferred for cost, latency, and privacy reasons (especially in the context of edge devices, such as smartphones) [34, 35], though they may lack the full capabilities of larger FMs [36, 37]. However, it is important to note that even hosting smaller models can also increase ownership costs [32].\nB. Data and FM Alignment\nAfter selecting FMs, the next crucial step is data and model alignment. This process tailors the FM(s) to meet the specific requirements and goals of a given FMware application [38]. Data alignment curates specialized data through labeling, generation, debugging, and testing, especially for enterprise cases [39, 40]. Domain-specific datasets are created to ensure that an FMware can perform well on the use-case specific data. Model alignment adjusts FM parameters using techniques like supervised fine-tuning and Reinforcement Learning with Human Feedback (RLHF) to ensure the FMware behaves as intended [41].\nLow data quality. High data quality is crucial for reliable FMware, as poor data leads to untrustworthy outputs [40].\nLow domain coverage. Ensuring broad domain coverage is key for trustworthy FMware. Insufficient domain representation can result in biased outputs.\nLow data efficiency. Accurately estimating inbound and outbound data volume is crucial for production-ready FMware, impacting both budget and performance [42]. Underestimation leads to poor model performance, while overestimation wastes resources [42, 43].\nC. Prompting\nPrompting involves designing and refining prompts to guide FMs toward desired outputs using precise instructions and in-context learning. While demos can get away with simple or non-optimized prompts, production-ready FMware demands meticulous engineering for consistent, reliable performance [44].\nGod prompts. \"God prompts\" that try to handle multiple tasks at once lead to unpredictable outputs and complicate debugging and maintenance [21, 44]. In production, tasks should be split into smaller, focused prompts and be decoupled for better accuracy, debuggability and maintainability (similarly to code) [21].\nLack of Built-in QA Checks for Prompts. Production-ready FMware demands robust quality assurance (QA) for prompts, including semantic and structural validation. Without built-in QA checks, prompts may produce erroneous or inappropriate responses, leading to potential failures and a loss of user trust [45-47].\nLack of Prompt Portability Across FMs. Prompts optimized for one FM typically fail with others due to differing architectures and training data [48]. This drastically limits portability and poses challenges to software evolution.\nAbsence of FM-Specific Optimizations. FM-specific optimizations can boost performance by up to 10% by tailoring prompts to each model's architecture [49]. However, these enhancements create dependencies, complicating updates and maintenance [50].\nComplexity in determining failure rationale. Diagnosing whether failures result from prompt issues or FM shortcomings is difficult, often hindering the identification of the root cause and delaying effective resolution [51, 52].\nNon-representative or insufficient in-context learning examples. Providing insufficient or non-representative examples for in-context learning degrades FM performance. For instance, Wang et al. [53] show that random in-context learning examples yield suboptimal outputs. Demos can use minimal examples, but production-ready FMware requires comprehensive, relevant examples for reliability.\nD. Grounding\nGrounding improves FM responses by linking them to external data for accuracy and relevance. Methods like Retrieval-Augmented Generation (RAG) achieve this by retrieving relevant information and using it to guide or inform responses. Effective grounding is crucial for reliable production-ready FMware.\nLow information density in grounding data. Low-density grounding data impairs FM performance by limiting its ability to generate coherent responses [54, 55]. In production, rich, well-structured data is essential for accurate outputs. Laban et al. [54] show how poorly structured, vast information leads to failures in long-context tasks, while Wang et al. [55] emphasize efficient data compression and selection to maintain performance and relevance in complex queries.\nIrrelevant grounding data. Incorporating irrelevant grounding data confuses FMs and reduces performance. Liu et al. [56] show that models struggle with long-context information, especially when crucial data is buried, while Cuconasu et al. [57] note how noisy data worsens the focus of retrieval systems. For production environments, filtering and curating data is crucial to ensure only pertinent information is used.\nOvercomplicating solutions with advanced techniques. Using complex methods when simpler ones would suffice adds unnecessary overhead. For instance, neural retrieval can raise computational costs without added benefits when a keyword search would be enough [21, 58]. In production, simplicity promotes scalability and maintainability, while demos often prioritize novelty [21, 58].\nE. Agent(s)\nAgents in FMware are autonomous components that leverage the decision-making and reasoning abilities of FMs to achieve tasks with minimal human input. They dynamically execute actions, interact with environments, and collaborate with other agents. By combining capabilities from previous software generations. In this new generation agents autonomously learn, adapt, and perform multi-step tasks, enhancing the functionality of FMware.\nGod Agents. Monolithic \u201cGod agents", "rails": "or input moderation, fact-checking, and hallucination control, maximizing reliability in production FMware [65, 66]. While demos may ignore these precautions, production systems must prioritize guardrails to ensure compliance, trust, and error prevention [65, 66].\nSimple keyword-based guarding is ineffective. Relying solely on keyword-based guarding is ineffective in production environments. Simple keyword filters can be easily bypassed or"}, {"title": "V. CHALLENGES", "content": "In this section, we present our thematic analysis results and outline the key challenges in productionizing FMware. It is important to note that the challenges presented in this version of the paper address only a subset of the identified recurrent issues, as shown in Table I. Future versions will expand on this by incorporating additional challenges that cover all the recurrent issues across the FMware lifecycle.\nFor each challenge, we provide an overview, a critical analysis of the state-of-the-practice, and our vision for addressing it. Tackling these challenges requires highly innovative technologies driven by cutting-edge research at the intersection of software engineering and AI. Our vision highlights these technologies, their functional requirements, and points to promising research directions.\nA. Testing\nOverview. Poor testing practices present significant roadblocks in the transition from demo FMware to production-ready systems. As identified in Section IV, the lack of robust, assertion-based unit tests, the absence of automated testing, and the over-reliance on surface-level text-based evaluation [1, 77, 92] often lead to overestimation of FMware quality. In real-world production settings, they result in unreliable outputs, latent defects, and increased regression costs. To address these issues, FMware require innovative testing mechanisms that account for the unique complexities of FMs and their non-deterministic behavior.\nCritical Analysis of the State-of-the-Practice. FMware testing remains an immature area, particularly in comparison to well-established practices in traditional software systems. The current testing pipeline for FMware relies heavily on manual processes for test generation. The manual identification of metamorphic relations (MRs), which are dependencies between different inputs and expected outputs, is a key bottleneck in the process. This task, while feasible for small-scale demos, becomes impractical and error-prone in production-ready FMware systems. The lack of automated test generation severely hampers the speed and coverage of testing in production systems.\nMoreover, the role of Al-as-judge technologies in testing pipelines adds another layer of complexity. Current AI judges, particularly those using large FMs like GPT-4, exhibit several critical flaws in evaluating correctness. These technologies often favor outputs based on superficial attributes, such as formatting or the length of responses rather than factual accuracy or alignment with real-world constraints [95, 99]. For instance, research shows that simply reversing the order of candidate responses can dramatically alter the AI's judgment, resulting in false positives or negatives. The failure to maintain consistency in judging introduces significant risks when scaling FMware for production environments. Research has also points to a misalignment between AI decisions and human expectations. For example, Koo et al. [100] calculate the Rank-Biased Overlap (RBO) score to measure the agreement between human preferences and model evaluations in ranking-generated texts across 16 different LLMs (RBO varies from 0 to 1 and higher values indicate higher agreement). The average RBO was 0.44, with GPT-4 scoring 0.47. Another major issue arises from the associated cost and latency with AI-as-judge systems. Relying on large models like GPT-4 is computationally expensive and not viable for long-term or large-scale production-grade testing [10].\nOur Vision. To address the limitations of current testing practices, we envision an approach that combines automated test generation and a next-generation AI-judge creator.\nAutomated test generation. The process begins with gathering user feedback in real-world scenarios, such as thumbs-up or thumbs-down data, which provides rich, actionable insights. By integrating this feedback with pre-existing domain knowledge, automated metamorphic relations (MRs) are generated for specific attributes of interest. These MRs are then applied to conduct metamorphic testing (MT) on FMware, ensuring that different properties and behaviors of the system are adequately tested. Following the MT, human experts evaluate the results to identify any discrepancies or potential improvements, further refining the MRs. This iterative cycle of automated generation, testing, and human-driven feedback continuously enhances the accuracy and relevance of the MRs, ultimately leading to more robust and reliable FMware.\nNext-generation Al-judge framework. The development of such a framework is crucial for improving the reliability of evaluations. Unlike existing solutions, this framework would guide developers in crafting tailored prompts that align with specific business logic and domain constraints. These judges would be trained to focus not just on the superficial aspects of FM outputs (e.g., format, length) but on deeper attributes like factual accuracy, consistency with previous outputs, and compliance with application-specific requirements. To train the judges in a more structured, guided, flexible, evolvable, and cost-effective manner, we foresee the use of curriculum engineering (more details in Section V-F). As a result, we would also obtain a more lightweight and efficient model, further contributing to cost-effectiveness.\nB. Observability\nOverview. Observability in FMware systems is crucial to ensure transparency, traceability, and reliability throughout their lifecycle. However, as outlined in Section IV, FMware presents unique challenges in observability due to its reliance on dynamic, non-deterministic components like FMs and autonomous agents. Recurrent issues, such as the complexity in determining the rationale behind system failures (i.e., whether stemming from limitations in prompts or FMs themselves) and the lack of FMware-native observability mechanisms [1], highlight the need for novel solutions. Unlike traditional software, FMware systems demand observability approaches that can capture both functional and cognitive aspects of AI agents and models.\nCritical Analysis of the State-of-the-Practice. Current observability tools in FMware, while evolving, remain grounded in classical software observability practices. Tools like Open-LLMetry [101] and LangSmith [102] focus on low-level resource monitoring, capturing traces of FM calls, Vector DB interactions, and user prompts, along with metrics like latency and resource utilization. However, these frameworks overlook the cognitive processes driving FM-based decisions, particularly in complex multi-agent FMware systems where outcomes can be non-deterministic or emergent (e.g., from interactions between agents or misinterpretation of prompts). Microsoft PowerAutomate offers one of the most advanced FMware observability frameworks by providing high-level overviews of agent workflows, enabling developers to trace where agents get stuck or fail. While an improvement, it remains focused on functional observability and does not capture how agents reason, plan, or coordinate their decisions. As FMware systems scale and become more complex, the gap between functional monitoring and decision-level observability widens.\nOur Vision. Addressing the aforementioned limitations in FMware observability requires a shift in both methodology and technology. One of the key innovations needed is a general observability framework for FMware that can capture both functional performance metrics and the internal cognitive processes of FMs and agents. This framework must allow developers to probe into multiple depths of abstraction, providing traceability not only at the system level, but also across the decision-making stages of autonomous agents.\nWe envision an observability analytics engine that visualizes and analyzes events at higher abstraction levels, helping developers quickly pinpoint root causes in complex multi-agent workflows. To achieve this, we propose a \"plane flight recorder\" for agents, inspired by aviation black boxes. In FMware, this recorder would selectively capture an agent's internal reasoning steps and communications, allowing developers to trace decision-making pathways and understand how agents reach conclusions.\nHowever, recording an agent's thoughts can introduce the observer effect, where observation alters behavior (e.g., adding \"think step-by-step\" to a prompt changes the output). To mitigate this, we propose a surrogate agent that enables debugging without directly interfering with the system. The original problem and result are sent to the surrogate agent (whose goal is to reason rather than solve the problem), which reasons verbosely to infer the original agent's thought process. While the surrogate's reasoning might differ, it still provides transparency and insight into decision-making. We are exploring techniques to enhance the trustworthiness of the surrogate agent's output.\nC. Controlled Execution\nOverview. Controlled execution in FMware development is essential for ensuring predictable, reliable, and efficient system behavior. However, as highlighted in Section IV, FMware presents a unique set of challenges due to its non-deterministic nature and the dynamic interactions between agents and models. Key recurrent issues include the lack of controlled execution mechanisms, which complicates the verification of fixes and limiting execution paths in FMware systems [82]. This lack of control severely limits debugging, reduces productivity, and can degrade system reliability, especially in multi-agent FMware where the same input might lead to divergent outputs across different executions.\nCritical Analysis of the State-of-the-Practice. Traditional software engineering uses controlled execution mechanisms like feature flags and canary releases to test updates before deployment. FMware's non-deterministic outputs and reliance on autonomous agents, however, make predicting and reproducing system behavior challenging.\nA major issue is that the same input in FMware can lead to different execution paths with varying outputs due to the unpredictability of the underlying FMs and agents. Traditional techniques, which assume deterministic behavior, are not equipped for these variations, leading to flaky tests [10]. FMware often lacks repeatable execution, making it difficult to ensure consistent behavior after fixing issues. The absence of controlled execution frameworks hinders exploratory testing, preventing efficient identification of failure points and limiting performance optimizations. Without execution space restriction mechanisms like feature flags, development and maintenance become even more challenging. Additionally, the lack of detailed release notes and the disconnect between improvements and features make it hard to verify whether updates are effective. This problem is worse with cloud-hosted models, where developers have limited control over testing. Exploring and testing multiple execution paths in a structured way is essential for improving software quality and building user trust.\nOur Vision. We envision a controlled execution framework designed for FMware that focuses on ensuring both repeatability and variability in execution paths, enabling comprehensive testing and validation. The framework should operate in two modes: enforcing consistent execution flows to ensure repeatability, and allowing controlled exploration of alternative flows to trigger failure points and optimize system resilience (guided exploration of the execution space).\nRepeatability. The framework should guarantee that the same input always produces the same execution flow, regardless of changes in external context or FM state. This could be achieved through execution snapshots and managing monosemantic units i.e., interpretable FM units that correspond to specific data patterns. By restoring snapshots and activating monosemantic units, developers can reproduce exact conditions that led to a bug or failure, facilitating debugging and fix verification. This approach is similar to deterministic replay systems used in distributed software, adapted for FMware's dynamic and probabilistic nature [103].\nGuided exploration of the execution space. This mode would systematically vary inputs, agent decisions, and model behaviors to explore different execution paths in a controlled manner. The guided process ensures comprehensive coverage, uncovering hidden bugs and allowing developers to thoroughly assess the system's robustness across various scenarios.\nD. Resource-Aware QA\nOverview. In FMware production, managing high costs and resource demands is crucial, especially given the intensive requirements of FM operations. Traditional QA methods often overlook these needs, as FMware is hindered by low data efficiency, latency handling issues, high regression testing costs, and inefficient retry optimizations (refer Section IV). Unlike typical high-cost web services, FMs require significant computational power, have non-deterministic outputs, and need meticulous management of context and ability to quickly swap FMs. Resource-aware QA is essential for reducing the computational burden that FMware impose, particularly in large-scale environments where frequent calls to FMs can become prohibitively expensive.\nCritical Analysis of the State-of-the-Practice. Current QA frameworks for FMware, adapted from traditional software engineering, fail to address FMware's resource challenges. The problem is not just the absence of caching but the lack of FMware-native caching strategies that account for the dynamic nature of these systems. While solutions like LangChain [104] use traditional caching, the real challenge is deciding when to cache FM calls and ensuring cached responses remain relevant. FMware often updates databases and integrates real-time feedback, which makes cache invalidation critical. Without dynamic caching, systems risk re-running unnecessary queries or using outdated data, increasing the time and cost of regression testing.\nAdditionally, there is no systematic approach to optimize or prioritize the number of tests that must be run. Dependency-based test execution [105], effective in traditional software, has not been adapted for FMware. As a result, redundant tests waste computational resources, especially where FM calls incur significant costs in terms of latency and finances.\nLastly, latency handling and retry optimizations remain underdeveloped. While traditional systems rely on retries for transient failures, FMware often needs re-prompting or alternative query structures. Current QA frameworks lack intelligent retry mechanisms that efficiently handle FM failures, leading to increased error rates and reduced system availability.\nOur Vision. To address these challenges, we propose the development of a resource-aware QA framework specifically designed for FMware. This framework would incorporate a robust FMware-native caching mechanism that intelligently stores, updates and reuses FM query results, significantly reducing the need for repeated FM calls and maintaing cache integrity. By caching the results of expensive FM queries and reusing them across multiple tests, the system can reduce computational overhead while maintaining the accuracy of the testing process. This would be particularly valuable in scenarios where large volumes of tests are conducted against the same FM. In addition to caching, the framework should optimize test execution by dynamically prioritizing tests based on resource intensity. Grouping similar tests or those with expected outputs could minimize FM calls, lowering resource consumption. Test order optimization would also prioritize critical tests that are likely to surface defects, ensuring early issue detection. Native prompt compression techniques [106] should further reduce resource demands and improve latency.\nA retry optimization system is essential to handle FMware's unpredictability. Rather than relying on basic retry logic, this system would employ dynamic prompt re-engineering and response validation. If an FM produces an unsatisfactory result, the system could modify the query intelligently, increasing the likelihood of success and preventing system overload from repeated failures.\nE. Feedback Integration\nOverview. Efficient feedback integration is vital for continuous improvement, optimization, and trustworthiness in production-ready FMware. However, as outlined in Section IV, two"}]}