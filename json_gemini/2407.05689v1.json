{"title": "Ten Years of Teaching Empirical Software Engineering in the context of Energy-efficient Software", "authors": ["Ivano Malavolta", "Vincenzo Stoico", "Patricia Lago"], "abstract": "In this chapter we share our experience in running ten editions of the Green Lab course at the Vrije Universiteit Amsterdam, the Netherlands. The course is given in the Software Engineering and Green IT track of the Computer Science Master program of the VU. The course takes place every year over a 2-month period and teaches Computer Science students the fundamentals of Empirical Software Engineering in the context of energy-efficient software.\nThe peculiarity of the course is its research orientation: at the beginning of the course the instructor presents a catalog of scientifically relevant goals, and each team of students signs up for one of them and works together for 2 months on their own experiment for achieving the goal. Each team goes over the classic steps of an empirical study, starting from a precise formulation of the goal and research questions to context definition, selection of experimental subjects and objects, definition of experimental variables, experiment execution, data analysis, and reporting.\nOver the years, the course became well-known within the Software Engineering community since it led to several scientific studies that have been published at various scientific conferences and journals. Also, students execute their experiments using open-source tools, which are developed and maintained by researchers and other students within the program, thus creating a virtuous community of learners where students exchange ideas, help each other, and learn how to collaboratively contribute to open-source projects in a safe environment.", "sections": [{"title": "1 Introduction", "content": "Developing energy-efficient software is not an option anymore, it is a matter of survival and moral duty. The Track Clean Energy Progress (TCEP) report of 2023 indicates that data centers and data transmission networks contribute, respectively, to the 1-1.5% of the global energy demand [2]. This corresponds to approximately 240-340 TWh for data centers alone, more than double the amount needed to power the Netherlands in 2020 (i.e., 115.88 TWh). Despite evidence that technological improvements helped mitigate their energy demand in the past years, the request for data center resources and network traffic is increasing. Especially with the expansion and large-scale application of AI-based solutions like generative AI (e.g., to create new contents like text, images, and videos) [2, 24], there is growing evidence that the electricity consumption (and likely the carbon footprint) of software will escalate in the next years [64, 63].\nWith such high stakes, it is fundamental that practitioners follow an evidence- based approach and make informed decisions about the energy efficiency and carbon footprint of their software [39, 46]. In this context, thanks to its roots in insight- and evidence-oriented nature, the importance of Empirical Software Engineering [69] is already prominent among both researchers and practitioners. However, applying empirical methods to assess the energy aspects of software is a difficult endeavour; precisely measuring the energy efficiency of software in a reliable manner requires strong technical skills and it has proven to be very challenging, time-consuming, and technologically fragmented [18, 22]. As such, strong competencies in empirical methods applied to energy-efficient software are in high demand [52].\nThis chapter is about our experience in running 10 editions of the Green Lab course at the Vrije Universiteit (VU) Amsterdam, the Netherlands. The course is given in the Software Engineering and Green IT (SEG) track of the Computer Science Master program of the VU. The course takes place every year over a 2- month period and teaches Computer Science students the fundamentals of Empirical Software Engineering (ESE) in the context of energy-efficient software.\nThe main peculiarity of the course is its research orientation: at the beginning of the course the instructor presents a catalog of scientifically relevant problems, and each team of students signs up for one of them and works together for 2 months on their empirical study to solve the assigned problem. Each team goes over the classic steps of an empirical study (a la Wohlin et al. [69]), starting from the formulation of the goal and research questions to context definition, selection of experimental subjects and objects, definition of experimental variables, experiment execution, data analysis, and reporting. Over the years, the course became well-known within the international Software Engineering research community since it led to several scientific studies published at various scientific conferences and journals, such as EASE, MOBILESoft, IST journal, ICT4S, etc., even winning best and distinguished paper awards."}, {"title": "2 Educational Context", "content": "As mentioned in Section 1, the Green Lab course is part of the SEG track which provides one of the possible specializations within the 2-year Computer Science Master program at VU Amsterdam. At the time of creating the track [30], the relation between software engineering and energy was barely understood. Green IT, instead, was a term emerging yet resonating among people. Hence, we decided to name the track after this term. However, we used it in its broad definition (so as to include software, too), i.e., \"the study and practice of designing, manufacturing, using, and disposing of computers, servers, and associated subsystems efficiently and effectively with minimal or no impact on the environment\" [41].\nThe SEG track entails five so-called core courses. Each course counts 6 credits according to the European Credit and Transfer Accumulation System (ECTS), hence counting a total of 30 ECTS. To integrate energy awareness in the track, we have adopted a mix of the distributed approach and the centralized approach identified by Mann et al. [38]. Accordingly, we both revisited pre-existing courses across the whole track (distributed approach) and created a few dedicated specific courses focusing on sustainability whilst also having sustainability issues addressed across the curriculum (blended approach) [38]. The Green Lab belongs to the latter, by teaching competencies that blend empirical software engineering in the specific context of energy-efficient software).\nFurther, we adopted a distributed approach for the other core courses Service Oriented Design, Digital Architecture, and Fundamentals of Adaptive Software (by applying software engineering competencies to practical projects and assignments that include Green IT aspects); and a centralized approach for core course Software Testing (by teaching traditional software engineering competencies that can possibly be used to test software for e.g., detecting energy hotspots [48]).\nTimewise, the academic year at VU Amsterdam is organized into 2 semesters, each including 3 periods of 2-2-1 month of 8-8-4 weeks, respectively, where the last week is dedicated to exams. From a student perspective, an 8-week period should include two courses that require 50% of the student's time, each, while a 4-week period is dedicated to a single course full-time. For instance, the Green Lab course is taught in an 8-week period, in parallel with another course.\nFinally, according to the academic level of their learning objectives, courses are categorized as at specialized (400), research-oriented (500), or highly spe- cialized (600) level. For instance, from the academic year 2024/2025 the Green Lab course will be classified as a 500-level course thanks to the consolidation of energy-efficiency software engineering and measurement as fundamental research competencies."}, {"title": "3 Course Design", "content": "This section presents the main educational components of the current edition of the course, including the students' cohort, learning objectives, and teaching team (Section 3.1), course contents and structure (Section 3.2), and assessment method (Section 3.3)."}, {"title": "3.1 Students' cohort, Learning Objectives, and Teaching Team", "content": "In terms of students' cohort, this course has been designed with Computer Science Master students in mind. Given the technical nature of the course, the recommended background knowledge of students attending the course includes: (i) basic statistical analysis techniques (i.e., descriptive statistics) and most common tests and (ii) basic programming/scripting skills. Both requirements are not formally enforced when students enroll in the course, but the lecturer informs students about them in the first lecture of the course, so to set the expectations and to allow students to prepare for the rest of the course. Students are not expected to be knowledgeable of ESE research methods, which are part of the learning objectives of the course.\nIn the last edition of the course (2023/2024 academic year), the course was attended by 91 active students. The course attracts a variety of students coming from several programs and Master tracks. Specifically, within the Computer Science Master program of VU Amsterdam, students' provenance is composed as follows: 43 students of the Software Engineering and Green IT Master track described in Section 2 and 18 students of other tracks within the same Computer Science Master program (14 in the Big Data Engineering track, 2 in the Foundational Computing and Concurrency track, and 2 in the Internet and Web Technology track); a total of 24 students come from international tracks of the Computer Science Master program, of which 20 come from the Software Engineering for the Green Deal (SE4GD) track and 4 come from the Global Software Engineering European Master (GSEEM) track and 1 in the Parallel Computing Systems track); finally, 5 students come from other Master programs, including (i) Parallel Computing Systems at VU Amsterdam, (ii) Computer Systems Security at VU Amsterdam, Human Computing Interaction at Utrecht University, and Computer Science at the University of Zurich.\nOver the years, students' ages, genders, na- tionalities, scientific backgrounds, and technical skills became more and more heterogeneous. For the sake of students' privacy we do not re- port students' personal demographics.\nUpon completion of this course, the learning objectives in Table 1 are achieved. In the table, we report also the levels of learning covered by each learning objective according to the revised version of Bloom's taxonomy of learning [9, 6].\nThe teaching team is composed of two lecturers and three teaching assistants. The responsibilities of the five members of the teaching team are:"}, {"title": "3.2 Course Contents and Structure", "content": "The course is designed to expose students to the fundamentals of Empirical Software Engineering (learning objective LO1) in the context of energy-efficient software (learning objectives LO2, LO3, LO4, and LO5). As such, the course covers first the basic principles of ESE (e.g., focus on applicability, quantitative vs qualitative research methods); then, given the importance of measurement-based studies in the context of energy-efficient software, it delves into the specifics of the controlled experiment method with software subjects and objects (as opposed to experiments with human subjects [29]). The overall organization of the course is presented in Figure 2.\nAs anticipated in Section 2, the duration of the course is 8 weeks, with lectures and labs being given in the first 7 weeks. In each of the first 7 weeks, there are two 2-hours educational components in which the teaching team gives either a lecture or a lab. The majority of the lectures (depicted in black in Figure 2) are about ESE research methods; those lectures are designed so that (i) their main contents and principles are generic and (ii) the lecturer systematically refers to examples, scientific results, cases, and success stories related to energy-efficient software. This makes the course future-proof for both students and lecturers: (i) students are able to apply the learned ESE research methods also in contexts different from energy- efficient software (e.g., in their final thesis or during their professional career) and (ii) every year, lecturers update only the energy-specific material (e.g., scientific studies, cases) without needing to completely restructure the course. The course includes also a special lecture about how to design and develop green software (L3 \u2013 depicted in blue in Figure 2); this lecture emerged from the students' feedback we received in the previous years of the course, where students were signaling the fact that the course was \"too meta\" on how to measure the energy-efficiency of software, instead of providing concrete advice on how to make software more energy efficient. During lab sessions (depicted in orange in Figure 2), students are assisted for technical operation of the lab equipment as regards measurement and tools (see Sections 4 and 5 for the details). In the labs, students also receive the required training for data analysis and visualization using R and RStudio. In order to provide students with the wider perspective on ESE methods and/or energy-efficient software, the last educational component of the course is a guest lecture. Throughout the course, students work in teams to perform experiments on energy- efficient software. Students carry out all the phases of a controlled experiment, from experiment design to execution, data analysis, and reporting. Students are provided with examples of experiments coming from state-of-the-art literature, but they will have to choose by themselves the experimental subjects and hypotheses to test. Further details about the project are given in Section 3.3.\nAt the end of every lecture, the lecturer provides a series of (mandatory and optional) readings."}, {"title": "3.3 Assessment", "content": "Teams of 5 students conduct a concrete research project throughout the whole course. Each part of the research project is discussed during the lectures, started during the labs, and completed as homework, so to keep students on track within the course schedule.\nThe goal of the research project is to plan, design, conduct, and report a scientific experiment in the context of the energy efficiency of various types of software (e.g., mobile apps, software libraries, microservices). Each team works on a specific topic in the context of energy-efficient software. In this way, students put into practice the skills and techniques that they have learned during the lectures/labs and develop their practical insights by applying them to real software. In the first lecture of the course, the lecturer provides a list of possible topics, then each team indicates (i) its members, (ii) the technical skills and educational background of each member, and (iii) a set of scores indicating the preferences about each proposed topic. Examples of potentially assignable topics are (i) What is the energy efficiency of AI-generated algorithms with respect to human-created algorithms? (ii) How do configuration settings of the Zipkin monitoring tool impact the energy consumption of a Docker-based system? (iii) How does WebGPU compare against WebGL in terms of the energy efficiency of web apps? The topics are assigned to teams based on the preferences, technical skills, and knowledge indicated by each team after the first lecture of the course. Then, each team is responsible for independently carrying on the experiment on the assigned topic. When possible, the lecturer provides relevant datasets, scripts, and other material to the team in order to smooth the execution of their experiment. Such information is typically provided during the first lecture of the course (contextually to the description of the topics for students' projects), during the first lab, during the in-person discussions with students before or after lectures/labs, or contextually to the grading (i.e., as part of the feedback given to students on their assignments); moreover, in case a new relevant dataset, tool, or scientific study is published, lecturers make an announcement on the web platform of the course about it, so that students can evaluate whether the new material can be used in their project.\nAs shown in Figure 2, each team project is composed of 3 assignments. Each assignment deals with a specific increment of the same research project. The final assignment is a fully-fledged scientific study, including all phases of a classical ESE study reporting on a measurement-based controlled experiment. The sequence of assignments is designed in such a way that their difficulty and required effort grow together with students' deeper understanding of ESE principles, the nuances of their research project, and their experience in terms of energy measurement. We believe that such incremental design of the research project is important to create a safe environment for students to explore and study at the beginning of the course, and then let the project grow together with them during the course.\nAll assignments include a written report describing all the information related to a specific phase of the project. Written reports contain also a link to a time log, where students record their time; the time log is used only in case of disputes among team members or in case of strong suspicions of fraud.\nBelow we describe the main components of each of the 3 assignments."}, {"title": "4 Equipment and Measurement Infrastructure", "content": "Green Lab empowers researchers and students to explore diverse software-related domains. So far, the Green Lab has been used for experiments on Distributed, Mobile, Robotics, Artificial Intelligence (AI), Virtual Reality (VR) and Embedded software applications. The Green Lab includes a cluster of 7 servers with different specifications.\nMOX1 runs Proxmox [23], an open-source server virtualization software, and, therefore, is dedicated to virtualization. Proxmox enables students and researchers to create virtual machines on-demand for testing software systems, executing data analysis, and monitoring other servers. Proxmox supports two virtualization tech- nologies: Kernel-based Virtual Machines and Linux Containers. Moreover, it allows dynamic scaling of resources assigned to each virtual machine, enabling the deploy- ment of different-size workloads at run-time. Proxmox offers a web-based interface for creating, removing, and allocating resources to virtual machines remotely. The cluster also supports containerization. Indeed, container-based applications, such as SockShop [67] and Train Ticket Booking System (TTBS) [21], can be deployed either on a single machine and across the servers.\nThe cluster is configured to offer all the computational resources of the servers, and, thus, supports High-Performance Computing (HPC) applications that need to execute compute-intensive workloads, such as simulation, machine learning models, and data analytics. To do this, we use SLURM [70], an open-source workload manager that handles the distribution and parallelization of computation across the nodes of the cluster. SLURM allows the users to create and distribute computation and monitor the energy consumption and the performance of the jobs and of the nodes within the cluster.\nFigure 3b shows the current configuration of our HPC cluster, which includes a head node, i.e., GL4, and four compute nodes, namely GL5, GL6, GL2, and MOX2. The HPC cluster is arranged as a tree, which means that the head node creates and distributes jobs on the compute nodes. A user can set the jobs to be executed, allocate resources to the jobs, retrieve statistics about job execution, and retrieve the output resulting from job execution. These commands are handled by the slurmctld daemon that runs on the head node. This daemon communicates with the slurmd daemons that run on each compute node. Each slurmd daemon is responsible to track job execution and report information to the head node. These statistics include the status of the node, information about job status, as well as job performance, and energy consumption. The head node records the information about job execution in a MySQL database. The interactions between the database and the slurmctld daemon are managed by the slurmdbd daemon that we omitted in Figure 3b for simplicity. The output resulting from jobs execution is stored into a shared folder that is accessible by all the nodes of the cluster. A user can collect the information stored in the database using the sacct command of SLURM and the data contained in the shared folder through SSH.\nThe Green Lab includes about 20 smartphones and 3 tablets for executing experi- ments on mobile applications, including several generations of Google Pixel phones, Samsung J7 phones, etc.. In addition, the Green Lab has 32 Raspberry Pi (of which, 2 Pi 5 Model B, 16 Pi 4 Model B, and 14 Pi 3 Model B) and 8 Arduino Nano to test embedded and IoT software applications. For experiments involving AI models, in addition to using the cluster, the lab has also 3 Jetson Nano development kits. For Robotics Systems, the Green Lab has 5 TurtleBot3 Burger robots (equipped with a camera module and current sensors) and a 4-degree of freedom robot arm (UARM Swift Pro, UFactory). Additional equipment includes wearables, such as Samsung Gear S3, virtual reality visors, i.e., a Google Daydream, and a drone, namely a DJI Tello."}, {"title": "4.1 Measurement Infrastructure", "content": "Green Lab users can track energy consumption and performance of the software running in the lab using both software energy profilers and hardware power meters. The cluster is equipped with two Rittal Power Distribution Unit (PDU) [50] and a Watts Up? Pro [66] that profile the power requested by each node. A Rittal PDU is a power strip that provides reliable power distribution with energy monitoring and measurement for all devices connected to it. A Watts Up? Pro, instead, is a plug power meter that is placed between the device to profile and its power source. Both power meters provide an API to get the measurements. In addition, the Rittal PDU provides a web interface to configure the power strip and monitor the power required by the servers in real time. At the current state, the two Rittal PDU supply and monitor power of all the nodes of the cluster. We use the Watts Up? Pro to profile the power consumption of a node and also of other embedded systems, such as smartphones and Raspberry Pis. The Green Lab provides 3 Watts Up? Pro, including the one connected to the cluster. The Green Lab also features 3 Monsoon Power Monitor [40], which are high-frequency power meters. We mainly use them for experiments involving mobile applications but they can also be used to profile other embedded systems. A Monsoon can be plugged into a device through a USB port, a Main Channel consisting of a positive and a negative terminal, and a BNC connector referred as Auxiliary port in the Monsoon documentation. We use the USB port to profile USB-powered devices and the Main Channel for tracking the battery of our smartphones. The Main Channel can be connected to all devices exposing a positive and negative terminal, such as the pins of a Raspberry Pi. The power consumed by the Raspberry Pis and the Arduino Nano can be monitored also using an INA219 sensor. The INA219 [3] is a current sensor that can be soldered on a board or connected to the board through a breadboard. The Green Lab comprises 5 INA219 at the moment.\nThe above-mentioned physical power meters record the energy consumption of the whole device while running a software application. A handy solution for fine-grained energy consumption measurements are software energy profilers [17]. Software energy profilers avoid direct interaction with the hardware platform and allow more fine-grained measurements on the software. For example, at the process or code block level. Among the most used software profilers, there are powerstat [15], perf [33], and Intel Power Gadget [27]. All the mentioned tools exploit the Running Average Power Limit (RAPL) [26] interface provided by Intel CPUs. At the moment, RAPL is used by most software energy profilers. RAPL estimates the energy consumed by a device according to a power model, which is proven to be accurate on certain architectures, and stores the estimate in a set of registers called model-specific registers (MSRs). SLURM, which we use for the HPC version of Green Lab, also uses RAPL to derive the energy consumed by the jobs executed in the cluster. However, not all nodes in our cluster support RAPL. The older ones, i.e., GL3 and GL4, do not support RAPL. For this reason, GL4 is used only for dispatching jobs and record the measurements in the HPC configuration."}, {"title": "4.2 Guidelines for Proper Use of Equipment", "content": "It is crucial to provide proper instructions to students on how to use the tools and equipment provided for their projects; major misuses of the tools and equipment can affect the completion of their projects or can even lead to physical hardware damage. The definition of the experiment design during Assignment 2, see Section 3.3, is followed by a phase in which the scientific assistants of the S2 Group interact directly with each group to provide them with support material and instruct them on how to use it. The groups are provided by the scientific assistants with credentials to access a limited set of resources, namely CPU cores, RAM, disk, and network bandwidth, on the servers according to the requirements of the experiment, as well as instructions on accessing them, usually through SSH. Scientific assistants provide step-by-step guides for setting up embedded devices like Raspberry Pis, Virtual Reality Headsets, and educational robots, as well as instructions for maintenance. For example, we instruct students to always check and set the voltage provided by the power source (e.g., the Monsoon power meter) to values that are not greater than those supported by the device to be powered (e.g., 5V for the Raspberry Pi). The same applies to the physical power meters, i.e., the Wattsup Power Monitor and the Rittal PDU. In addition, we provide students with material that could be helpful in better understanding the usage of the devices, such as links to the manual of the devices and online user discussions. The Green Lab course includes a lab, mentioned as LAB1 in Section 3.2, that covers the use of software tools to manage experiments, measure performance, and track energy consumption. During the lab, the instructor explains how to set up the tools and provides a live demonstration of how to use them for experimentation. The scientific assistants and the lecturers are available to support the students with technical problems and questions about the usage of measurement tools during the whole course. This is usually accomplished through open discussions on the web platform of the course."}, {"title": "5 Open-Source Tools and Community of Learners", "content": "In the first three editions of the course, the development of a proper measurement pipeline for executing the experiment was an additional task for the students. How- ever, we noticed that for projects that were focusing on the same technologies (e.g., projects on the energy consumption of PostgreSQL databases or the front-end of mobile web apps) students tended to \u201creinvent the wheel\" in their scripts in each of their projects. Examples of redundant scripts include tasks such as: defining the order of execution for experiment trials, shuffling experiment runs based on the consid- ered subjects and treatments of the main factors, programmatically launching mobile apps, checking the status of smartphones, configurating energy profilers, etc. Even though those tasks were relatively exciting for students, they were very repetitive and error-prone; we saw an opportunity to save students' (and researchers') time here and we decided to move from ad-hoc development of measurement pipelines to the configuration of measurement pipelines via dedicated tools.\nSo, in the months before the 2017/2018 edition of the course we decided to work on a first tool for the automatic execution of measurement-based experiments. To keep the scope of the tool under control, we decided to focus on experiments targeting mobile apps (both Web and native ones) running on Android devices (both smartphones and tablets). This tool is called Android Runner9 [36]. Android Runner is implemented as a set of Python modules, and as such it can run on any machine able to run Python code (e.g., laptops, Raspberry Pis, servers). Given a JSON-based configuration of the experiment, Android Runner is in charge of performing diagnostic checks, bringing up (energy) profilers, executing each run of the experiment according to a predetermined plan, collecting and aggregating measures from the profilers, etc. Android Runner communicates with the measured apps via Android Debug Bridge (ADB10), the official Android command-line tool for interacting with Android devices. Android Runner is independent of the specific energy profilers (which are implemented as external plugins), allowing users to straightforwardly collect run-time measures via already-existing hardware/software profilers or by integrating their own third-party profiler; Android Runner allows users to use multiple profilers within a single experiment, e.g., for collecting performance and energy measures at the same time (as we did in [43]). Moreover, Android Runner allows users to include their business logic as external Python scripts at specific points within the experiment execution (e.g., before the beginning of the experiment, before or after each run, etc.); over the years, external scripts proved to be useful in many situations, e.g., for setting up a local web proxy recording the traffic generated by the apps, for instrumenting the subjects of the experiment on the fly, for cleaning up the environment between runs, etc.\nOver the years, and thanks to the lessons learned while working on Android Runner, we decided to develop other two tools for executing measurement-based experiments: (i) Robot Runner is dedicated to robotic systems and (ii) Experiment Runner, which is a generalization of the previous *-Runner tools, allowing stu- dents (and researchers in general) to configure measurement-based experiments in a technology-independent manner.\nRobot Runner\u00b911 [57] follows the same principles as Android Runner, with the following main differences: (i) it communicates with experiment subjects via Robot Operating System (ROS12) messages instead of Android ADB, (ii) the internal diag- nostics checks are specific to ROS-based system, (iii) it is completely independent of the type and number of used subjects (either simulated or real), whereas Android Runner assumes to be interacting with a physical and always-available device, and (iv) the experiment configuration is defined in Python instead of JSON. About the latter point, the rationale for having a Python-based representation of the experiment is to allow users to flexibly define the configuration and the busi- ness logic of their experiments in a self-contained manner [57]. This is aligned with the ROS ecosystem, where mission launch files for the robotic system can be defined in Python 13.\nExperiment Runner14 is our latest tool and it embodies several of the lessons learned from both Android and Robot Runner. Specifically, it allows users to (i) programmatically define experiments in terms of a so-called run table containing experimental factors and their related treatments, subjects and objects, and their combination into trials, (ii) start, stop, pause, and resume experiments, (iii) run the experiment in automatic mode (without any interruptions) or semi-automatic mode (where the experiment waits for a user's action before at the end of a run before proceeding with the next one \u2013 this is useful in cases where a manual action is needed in between runs), (iv) intuitively keep track of the progress of the experiment, (v) define custom Python callbacks that are invoked when specific events are raised during the execution of the experiment (similarly to Android and Robot Runner). During the execution of the experiment, the run table defined by the user is populated dynamically with the measures collected at each run, without requiring the user to manually aggregate them at the end of the experiment. Similarly to Android and Robot Runner, measures are collected via external profilers.\nAt the time of writing, the above-mentioned tools have been used to execute a total of 68 experiments within the Green Lab course and many others outside the scope of the course (e.g., in experiments not linked to education or in students' final projects). As shown in Figure 4, the three above-mentioned tools are the center of a large community of learners composed of students attending the Green Lab course, students carrying out their final projects (both Bachelor and Master), researchers of the S2 research group, and students acting as scientific assistants within the S2 group. Specifically, Green Lab students use the \u201cRunners\u201d as black-box tools for executing their own experiments for completing the project of the course; they are not required to know the internal details of the orchestration logic of the tools, but in almost all cases they integrate the tools with their own custom logic via ex- ternal Python scripts. Students carrying out the final projects use the Runners for executing their own experiments, which tend to be more complex and large than those of the Green Lab course. Also, they might go deeper on selected aspects of energy measurement, e.g., by benchmarking new tools and profilers, benchmarking already-existing energy meters/profilers against each other, etc. Scientific assistants are generally Master students enrolled within the S2 research group all year long to support the development of the Runners; their responsibilities include the improve- ment of the tools, bug fixing, integration of new plugins and their benchmarking, and documenting the tools via videos, tutorials, examples, guides, and readmes. Together with S2 researchers, these three sub-populations of students form a community of learners where individuals share tips and (less documented) information about the tools, discover and fix bugs in the tools, and keep the tools up-to-date. The mecha- nisms we use for synchronizing all those activities are exactly the same as those of collaborative software development15. The source code of all Runners is publicly- available in their GitHub repositories, learners fork those repositories, develop and test their fixes or new plugins in their own forks, submit their own contributions to the upstream GitHub repository via pull requests, and receive feedback from S2 researchers and other learners and (if needed) make changes accordingly. The com- munity is growing more and more \u2013 as of today it counts more than 300 participants in total (past and present) \u2013 and we believe it is the ideal setup for training students and young researchers to participate in collaborative software development, a skill that is deemed as a must-have in the job market for software engineers."}, {"title": "6 Success Stories", "content": "This section presents examples of recent relevant scientific studies that have been designed, conducted, and reported in the context of the Green Lab. All studies have been positively received by the software engineering research community and tended to receive primarily positive reviews by our peers.\nBefore describing the success stories, it is important to elaborate on how we organize the transition from an education-oriented project developed by 5 students to a scientific publication able to compete in the Software Engineering research landscape. This transition is carried out in 4 phases: (i) projects supervision, (ii) projects selection, (iii) authorship discussion, and (iv) writing and submission."}, {"title": "6.1 Scientific Studies on Web Technologies", "content": null}, {"title": "6.1.1 Impact of service workers on the energy efficiency of Web apps [37]", "content": "This was the first scientific study emerging directly from a Green Lab project and it was awarded with the Distinguished paper award at MOBILESoft 2017 (co-located with ICSE). The lessons learned and the various interactions with the students during this study led to the creation of Android Runner [36].\nService workers are a W3C stan- dard providing APIs to allow Web de- velopers to programmatically preload assets required by a web apps, cache data received from servers, subscribe to and receive push notifications, etc. The main motivation of this study is that service workers were advertised by sev- eral technology players as performance boosters, network savers, and providers of better user experience; however, de- spite they are additional code to be downloaded, parsed, and run by the browser, to that day nobody investigated the potential overhead in terms of en- ergy consumption. As shown in Fig- ure 5, this experiment had two main factors: the use of service workers and the type of (simulated) network available (2G and WiFi). The subjects of this experi- ment are 7 third-party and publicly-available Web apps, e.g., Washington Post, Ali Express, Wikipedia. We run the subjects on Google Chrome via two Android de- vices (LG G2 and Nexus 6P); the Android devices were used as blocking factor. The dependent variable of the study is the energy consumption of the Android devices, measured via the Trepn Power Profiler. The experiment did not provide statistically- significant evidence about the impact of service workers on energy consumption, regardless of the network conditions; no interaction was detected between the two main factors of the study.\nThis study is also relevant in the context of scientific integrity; indeed, it is a good example of the value of non-conclusive studies in terms of statistical significance, allowing students to understand the value of a scientific study independently of reaching statistical evidence."}, {"title": "6.1.2 Correlation between performance scores and energy consumption of mobile Web apps [12]", "content": "This study has been carried out in collaboration with Greenspector, a French company specializing in environmentally sustainable digital transformation of organizations. Greenspector contributed to the study by allowing our students to collect measures via their software-based energy profiler for Android devices.\nThe study is based on the observation that Web develop- ers can use several tools for collecting performance met- rics of mobile Web apps (e.g., Google Lighthouse 18), but sim- ilar ready-to-use tools are (still today) not available for energy metrics. The goal of this study is to investigate whether the metrics produced by Google Lighthouse can be used as a proxy for energy consumption. To answer this question, we conducted an experiment where 21 real mobile web apps (e.g., apple.com, theguardian.com, etc.) were analyzed in terms of their performance level (via Google Lighthouse) and their energy consumption. After having collected the mea- surement data for 525 runs (21 web apps, 25 repetitions each), we statistically analyzed the correlation between the obtained performance metrics and energy con- sumption, and carry out an effect size estimation. The results of the study provided empirical evidence about a statistically significant negative correlation between per- formance levels and the energy consumption of mobile web apps, with medium to large effect sizes (see Figure 6). In practical terms, this means that if a web app has a good performance score on mobile devices, then developers can use such a score as a low-cost alternative for preliminary insights about its energy consumption.\nThis study attracted the attention of Web developers and it is used as one of the support resources of the current draft of the Web Sustainability Guidelines19, a specification providing recommendations for making websites and products more sustainable published by the W3C Sustainable Web Design Community Group."}, {"title": "6.1.3 Performance and energy costs of ads and analytics in mobile Web apps [43]", "content": "This study is published in the Information and Software Technology journal and it is the first journal publication emerging from a Green Lab project.\nThis study targets ads and analytics in Web apps run- ning on mobile devices. The premise of the study is that ads and analytics have inher- ent costs due to them be- ing additional software mod- ules running in a Web app and making additional network requests. Subsequently, more computing resources are used, potentially having an impact on the energy consumption and the performance of Web apps. This study aims to anal-yse the performance and energy overhead of ads and analytics on mobile web apps. The results of this research are intended to provide developers, browser vendors, and researchers with insights into the energy consumption and performance drain caused by running such software on consumer mobile devices. Students sampled 9 popular web apps containing both ads and analytics from the Tranco list [31], a sta- ble research-oriented list of the top one million most popular web applications. For each web app, students obtained three versions: the full original web app, a version without ads, and a version without analytics. Then, each version of each web app is loaded multiple times on an Android device via two different browsers, namely Google Chrome and Opera. The energy consumed by the browser for loading the subjects is measured using the BatteryStats plugin of Android Runner. Two perfor- mance metrics are collected via the PerfumeJS plugin of Android Runner, namely (i) First Contentful Paint (FCP), and (ii) Full Page Load Time (FPLT). The main results of the study are: (i) ads significantly impact the energy consumption of mobile web apps for both browsers, with a large effect size (see Figure 7); (ii) analytics have a significant impact on the energy consumption of Chrome, with a medium effect size (see Figure 7), but not on Opera; (iii) both ads and analytics do not significantly impact the FCP metric on both browsers; and (iv) both ads and analytics significantly impact the FPLT metric on both browsers, but with a small effect size. This study provides evidence that both ads and analytics do have a significant impact on the energy consumption and performance of mobile web apps. Web developers are ad- vised to limit both ads and analytics in their mobile web apps to reduce their energy consumption and improve their loading time."}, {"title": "6.2 Scientific Studies on Mobile Apps", "content": null}, {"title": "6.2.1 Comparison of the energy consumption and performance of native Android Apps and their Web counterparts [25]", "content": "This study compares the quality of the native version of a mobile application against its web counterpart. This work has been (remotely) presented at MOBILESoft 2023 by a student, thus giving relevance to the commitment of the students and the quality of both the course and the lab. This study observes the problem mainly from a user perspective. If correctly informed, users may choose to use the native or the web version of an application to save battery life and for better performance.\nOur daily lives have be- come more and more de- pendent on mobile appli- cations. Music streaming platforms, digital newspa- pers, and social networks are just a few examples of widely-used mobile appli- cations. On-demand con- tent is usually provided through two versions of a mobile application: a na- tive and a web version. Na- tive applications are de- signed for a specific sys- tem and have direct ac- cess to the hardware re- sources of the system. For example, native applica- tions can be specifically designed for Android and fully support the GPS, camera, and accelerometer of the device. The web ver- sion of the application, in- stead, is portable and im- plemented using web lan- guages, e.g., HTML, CSS, and JavaScript. A set of 10 well-known mobile ap- plications is sampled from different categories, such as social networks and news. Each application is stressed with a typical usage scenario for that application. For example, the scenario executed to profile the news application of ESPN involves a user opening an article, scrolling down, and checking the next one. Both versions of the application are tested using default settings. A scenario is executed 25 times for each application, profiling the performance and the energy consumed per run. Figure 8 depicts the results of this study. The top row shows that the native apps tend to be more energy-efficient than their web counter- parts. The most energy greedy applications are social media and e-commerce with Twitch as the most energy consuming application. The web version resulted in using considerably more CPU and memory, as shown by middle and bottom row of Figure 8. Instead, there is no significant evidence that native applications have improved network traffic and frame time."}, {"title": "6.2.2 Run-time efficiency of Android apps migrated to Kotlin [44]", "content": "This study is the result of a Master thesis carried out using the Green Lab tools and its main characteristics is its mixed-method research design involving a GitHub mining phase and a measurement-based experiment. The study was awarded with the Distinguished paper award at IEEE SCAM 2021 (co-located with ICSME).\nThis study targets Android apps developed using Kotlin. Specifically, we aimed at empirically assessing the impact of the migration from Java to Kotlin on the run-time efficiency of Android apps. To achieve this goal, the student mined 7,972 GitHub repositories containing Android apps and identified 451 apps containing Kotlin code. By applying a cross-language clone detection technique [13], they detected 62 commits that represent a full migration to Kotlin, while keeping the app functionally equivalent. This mining phase confirmed that most open-source Android apps either fully migrated to Kotlin (>90% Kotlin code) or contained low portions of Kotlin code (<10%). After the mining phase, a sample of 10 apps that fully migrated to Kotlin have been selected and then the measurement-based experiment has been executed to compare their Java and Kotlin versions. The study considered six run-time efficiency metrics: CPU usage, memory usage, number of calls to the garbage collector, frame times, app size, energy consumption. The experiment has been orchestrated via Android Runner and targeted two smartphones. By referring to Figure 9, the results of the experiment highlight that migrating to Kotlin has a statistically significant impact on CPU usage, memory usage, and render duration of frames (albeit with a negligible effect size), whereas it does not impact significantly the number of calls to the garbage collector, the number of delayed frames, app size, and energy consumption. This study provides evidence that developers can migrate their Android apps to Kotlin and expect comparable efficiency at runtime."}, {"title": "6.2.3 Evolution of Kotlin apps in terms of energy consumption [4]", "content": "This a follow-up of the study described in Section 6.2.2. In this case, instead of looking at apps migrated to Kotlin, a team of Green Lab students performed a longitudinal analysis of the energy consumption of 3 selected Kotlin apps. The goal of this study is to empirically assess how the energy consumption of Kotlin applications evolves over time. Students first rigorously selected three open-source Kotlin applications via well-defined inclusion and exclusion crite- ria and a quality assessment procedure. Then, they cloned the GitHub reposi- tory of each app and selected a total of 57 combined releases of those apps.\nThe energy consumption of each of those 57 releases is measured using Android Run- ner and the collected measures are statistically analysed in or- der to identify energy spikes and drops (see Figure 10 for an example). Finally, source code changes occurring in \"energy- relevant\" releases are manu- ally analyzed to identify pos- sible causes for the observed energy fluctuations. The study confirms that the energy con- sumption of Kotlin apps gen- erally follows a growing trend along releases, and in all apps significant energy spikes and drops have been de- tected. Among the major factors that impacted those fluctuations, we noted that those co-occurred with OS upgrades, the release of new app features, the usage of poorly chosen design patterns and libraries, the injection of UI-related issues, and unstable app versions."}, {"title": "6.3 Scientific Studies in other Domains", "content": null}, {"title": "6.3.1 On the Energy Consumption and Performance of WebAssembly Binaries across Programming Languages and Runtimes in IoT [65]", "content": "This work represents the first project regarding IoT in the Green Lab and extends the work presented at EASE 2022 [59]. This study analyses the potential of Web Assembly (WASM) [56] in another application domain, i.e. IoT. One of the students, who also authored the paper, presented the work at EASE 2023 in Oulu, giving the right coronation of effort, as well as recognition and visibility to the students involved in the project and the Green Lab.\nWASM allows software written using non-web programming languages, such as C++ and Rust, to be executed inside a web browser. Modern browsers can load WASM modules through JavaScript and execute them through a separate browser sandboxed environment. WASM showed potential for increased security, portability, and close-to-native performance. For this reason, WASM represents a good solution for fast deployable and updatable software for IoT devices. IoT involves running software on a plethora of resource-constrained devices. The devices of an IoT system can vary from a few to thousands and need to be regularly updated to add new features and fix bugs. The adoption of WASM on IoT devices is achieved by using runtime environments. A runtime environment works as an interface between a WASM executable and the underlying system and provides basic system-related services, such as I/O operations.\nIn this study, we investigated the impact of different programming languages and runtime environments on the performance and energy consumption of WASM bina- ries in the context of IoT systems. We selected four different programming languages, namely Rust, Go, JavaScript, and C, and two different WASM runtime environments, i.e., Wasmer, and Wasmtime. The programming languages were evaluated using three software applications taken from the Computer Language Benchmark Game (CLBG). We considered the implementation of each software in each programming language. Each implementation was executed using Wasmer and Wasmtime on a Raspberry Pi 3 Model B. The execution of the experiment resulted in 24 trials (4 programming languages x 3 benchmarks x 2 runtime environments), each one executed 10 times."}, {"title": "6.3.2 Computation offloading for ground robotic systems communicating over WiFi \u2013 an empirical exploration on performance and energy trade-offs [19]", "content": "This study shows how we exploited the Green Lab to perform experiments in the area of robotics software. We set up a test environment for a TurtleBot3 Burger that moves autonomously in space. This environment is used to evaluate the impact of different offloading strategies on energy consumption and the performance of the robot by diversifying the tasks performed by the robot.\nRobotics system is known to be resource-constrained, which means resource utilization greatly impacts its operational time. For example, battery-powered robots need to optimize their energy consumption to ensure that they have enough power to accomplish their mission. To overcome hardware limitations, computation-intensive tasks can be offloaded to the Cloud. However, the integration of offloading mechanics within robotics software increases network utilization, which may reduce the quality of service and raise the energy consumption of the network.\nThis study empirically assesses the impact of offloading strategies on the per- formance and energy consumption of autonomous driving robots. The assessment involved two experiments. In the first experiment, our goal was to characterize of-floading strategies and their impact on energy consumption and performance of the robot, while in the second, we analyzed the impact of the offloaded task parameter values on the same metrics. The robot subject to the experiment is a TurtleBot3 Burger [5], which performs three tasks: mapping (SLAM), navigation, and object recognition. These tasks are implemented using ROS. During the mapping task, the robot builds a map of the (unknown) environment in which it operates and localizes itself in the map. The navigation task, on the other hand, enables the autonomous driving of the robot to a goal location on a map, while object recognition consists of the identification of objects on the map by exploiting robot cameras. The tasks have different parameters, such as image resolution and frame rate for object recognition and the map for the navigation task that can be known or unknown. We observed the robot operating using different values of these parameters, referred in the paper as a configuration. Task parameter configurations considered in this study are described in Table 3. The configurations have different magnitude, for example Conf. 3 consid-"}, {"title": "6.3.3 Mining the ROS ecosystem for Green Architectural Tactics in Robotics and an Empirical Evaluation [35]", "content": "In this study, we exploited the Green Lab to validate the results obtained after obtaining four architectural tactics for energy-efficient robotics software from the literature. The Green Lab, in this case, proved effective in transferring and validating knowledge about software engineering practices to real-world contexts.\nSoftware is becoming central in robotics systems. For example, NASA uses more control software on its new exploration missions than it has on all of its previous missions combined. However, at present, robotic software is still poorly engineered, despite its increasing complexity and size. In addition, robotics software design focuses only on performance and functional aspects, leaving important issues such as energy consumption out of the design process. The integration of energy consumption analysis at design time would bring enormous benefits to the quality of the final system. Battery-powered robots could adopt strategies at run time to consume less energy and thus increase operational time.\nThis study identifies and empirically evaluates architectural tactics for robotics systems. The architectural tactics are identified by mining ROS-related data from open-source software repositories and Stack Overflow (SO). The collected dataset includes discussions on SO, GitHub pull requests, commit messages, and source code. After collecting the dataset, it is processed using a set of keywords related to energy concerns (e.g., green, battery, power). Resulting data are manually validated by the researchers, which remove false positive and keep data concerning software architecture. The last filter applied to the collected data is a thematic analysis, in which the researcher manually grouped energy-related software architecture design decisions, e.g., usage of low power mode, and threshold-based mechanisms. This process resulted in four architectural tactics for energy-efficient robotics software:\nEE1 - Limit Task: robot tasks, such as video streaming or robot navigation, can be limited when their energy consumption reaches a critical level."}, {"title": "7 Lessons Learned and Recommendations", "content": "The design of the Green Lab course described in Section 3 is the result of ten years of experience in the delivery of the course and can be reused as a basis for delivering similar courses in other universities. We reached this stage iteratively. Over the years, we piloted several alternative solutions and various educational components and refined the content of the course edition by edition. This resulted in an accumulation of a number of lessons learned that we would like to share with the community. We report such lessons learned and make recommendations for instructors in the remainder of this section."}, {"title": "7.1 General recommendations", "content": "First of all, we learned that it is fundamental to clarify expectations to students from day zero. The strong research orientation of the course might not be common in some universities, and students might have a more didactic attitude concerning the course, instead of aiming to achieve scientific goals and solving open problems. In other words, students might be caught off guard when their project is evaluated with a critical eye from the instructors. As a solution, in the first lecture of the course, the instructors not only introduce the course (in terms of covered contents, learning objectives, assessment methods, and practical aspects), but also explicitly inform students that the teams whose project is well-reported and scientifically valid will be invited to participate in a scientific publication based on their project. In addition, as homework for the first lecture, students are asked to watch a video where one of the instructors presents the results of an experiment emerging from the course (specifically, it is about the results of [37]).\nRelated to the previous point, instructors should set high stakes for those teams that want to \"chase the stars\", but they shall still be open to teams that just want to pass the course. As the instructors say in the first lecture, \"this course is about opportunities\u201d; however, it is important that students are not required to catch such opportunities. In each edition of the course, there have always been teams that just wanted to pass the course with a good grade and learn just enough about ESE methods, without spending extra effort for the scientific publication. We believe that this is fair to students and those students shall still be able to get the maximum grade for their project. At the end of the day not all of our students are aiming to do a PhD in Empirical Software Engineering and, as instructors, we must accept it.\nInstructors should keep the scientific publication not central to the project and value more students' learning than the scientific output. This point is important to do not create too much pressure for students and it should be considered as a good extra-reward for the students. Concretely, during the first lecture students are asked to form their teams as soon as possible (recall that the deadline for the first assignment is generally at the end of the second week of the course) and each team is also asked to indicate if they are interested in contributing to a scientific publication based on their project (in the form we also allow for a \u201cMaybe (if time allows)\u201d intermediate level). In any case, instructors' feedback and grading rubrics are exactly the same for all teams. Also, in previous editions of the course, it happened also that a team was not willing to pursue the scientific publication based on their project; in those (rare) cases, the instructors did not proceed with the submission.\nIn some past editions of the course, students raised concerns about the usefulness of the labs. We learned that in those cases the labs were (i) focussing primarily on the basics of the tools and R and (ii) less interactive than in the other years (i.e., the teaching assistant was just presenting with slides and briefly carried out some practical tasks). The lesson learned here is that labs shall be carefully planned in such a way that students can work on practical aspects related to their projects. Higher lab interactivity can be achieved by: (i) asking students to compute basic summary statistics on real data coming from previous projects, (ii) creating scripts about the contents of the labs beforehand and making sure that they contain tasks for students, and (iii) investing time in educating the teaching assistants about the technical aspects of the lab (more technically-confident teaching assistants generally gave better labs).\nDesigning and running a course like the Green Lab takes a considerable amount of time. This investment must pay off for instructors. First of all, a course like the Green Lab is a good opportunity for researchers to carry out research in the context of their education activities; in a way, running the course is not a purely teaching activity, but it is a way for researchers to have education and research intersect, allowing instructors to continue doing their research activities in the weeks of the course (instead of putting them on hold). Moreover, by design, students who attend the Green Lab have the fundamentals of ESE methods in general (independently of the energy aspects). This means that those students are already able to design and conduct an empirical study in the context of, e.g., their final project/thesis. Another strategy for capitalizing on this investment is to advertise the availability of further projects and theses during the course, allowing the instructors to have a continuous research-oriented relationship with students even after the course. For example, at VU Amsterdam there are two additional research-oriented courses that students can put in their study plan (for a total of 18 credits) and a 30-credit final Master project; if a researcher and a student agree to have the Green Lab, those two courses, and the final project on the same research topic, they will work together for a total of 54 credits, which is the equivalent of more than 8 months full-time. As such, this creates an opportunity for both the student and the researcher to design a more meaningful and well-structured research project than those carried out in more compact time frames.\nOver the years we also noted that courses like the Green Lab can be used as a good training environment for discussing ethical aspects related to empirical software engineering and IT in general. First of all, by directly targeting the energy efficiency of software, issues related to the carbon footprint of IT are inherently central in all lectures of the course; this is also by design since the course is given as a module of the Software Engineering and Green IT Master program and students enrolled in such program (i) expect this kind of discussions and (ii) have already a certain sensibility towards themes related to green software and sustainability. Moreover, by design the course touches upon aspects related to scientific integrity and allows students to get a hands-on experience about the importance of (i) the replicability/reproducibility of scientific experiments (e.g., some projects are about replicating an already-published study starting from its replication package and all student teams need to produce a complete replication package for their own projects) and (ii) properly reporting scientific studies (e.g., all teams need to honestly describe the threats to the validity of their experiment and to follow empirical best practice when writing the report of their project). During the three project assignments, the teaching team checks the ethical aspects of the experiments to ensure the reliability of the results and the claims derived from them. As the students evaluate the energy consumption of real software, their experiment may involve subjects and produce results that are sensitive to the stakeholders of the research, such as IT software developers, users, and IT companies.\nAs reported in Section 2, the duration of the Green Lab course is two months. We are aware that courses in other universities can have a longer duration. Below we indicate how a course like the Green Lab might be expanded to longer courses:"}, {"title": "7.2 Recommendations about students' projects", "content": "On top of experience and knowledge about ESE research methods, the teaching team must have at least a basic knowledge of the technical/technological aspects of students' projects. Given the relatively tight schedule of the course, the teaching team must be able to provide continuous (and timely) support to teams having technical issues during the course. Possible strategies to achieve this include: (i) involve the scientific assistants mentioned in Section 5 as teaching assistants within the course (in this way it is the same people maintaining the tools supporting students in their usage), (ii) train teaching assistants on the technical aspects of the projects before the beginning of the course, e.g., by executing small-scale replications of previous experiments, or (iii) enroll teaching assistants based on their technical skills and assign to them only projects related technologies related to such skills.\nIt is suggested to create multiple project tracks for each edition of the course, roughly about double the number of student teams. In the first edition of the Green Lab, we had only two teams of students; so it was natural to have only two project tracks for the whole course. Differently, in the last edition of the course we had 19 teams of students, and having only two project tracks could have raised the likeliness of fraud among students and it could have made the assessment process extremely boring and repetitive for the teaching team. Thus, we opted to provide a large number of project tracks to students (i.e., 30 individual tracks). Those project tracks touched upon different technologies (e.g., Android APIs, data compression algorithms, large language models), platforms (e.g., Web apps, VR apps, IoT, HPC), and types of research methods (e.g., longitudinal studies, correlation studies, observational stud- ies). When students register their team, they also provide their level of preference for each proposed project track, and then it is the teaching team that assigns the track to each team accordingly. This proved successful since students could choose for their project those tracks that best fit their skills and wishes. It is important to note that, in order to avoid assigning low-voted tracks, it is possible to assign the same project track to different teams. If on one side this is a risk for the publication potential for the track (i.e., only one team in principle will be able to publish its project), on the other side it creates safe competition among teams; also, it is up to the teaching team to propose slight variations to the redundant tracks in order to make them complementary (and thus equally valuable). As an example, our study on WebAssembly binaries across programming languages and runtimes [59] has been performed also on Android.\nCreating multiple project tracks every year is a time-consuming and difficult en- deavor. So, it is suggested to potential instructors to keep a log of potentially-valid projects all year long. Based on our experience, having a textual file with a 2-liner for each potential project track and some support links is more than enough. Then, at the beginning of the course, the teaching team meets, distributes the tracks among each other, and expands each 2-liner into one or two slides for each project track; those slides are then used in the first lecture to pitch project tracks to students. The list of potentially-valid projects proved also to be a useful overview of interesting re- search directions in the area of energy-efficient software, independently of the Green Lab course. Examples of sources we recurrently use for potentially-valid projects include (i) social media accounts of key tech engineers (e.g., Addy Osmani for Web performance), (ii) engineering blogs of relevant tech companies (e.g., Dropbox Tech Blog20), (iii) websites focussing on software development and technological advances like InfoQ21 and Ars Technica22, (iv) scientific literature in software en- gineering conferences, (v) developers' discussion forums like Stack Overflow23 in general or ROS Discourse24 for robotics, and of course (v) daily discussions with peers and colleagues."}, {"title": "7.3 Recommendations on tools and equipment", "content": "Before delving into the details about tools and equipment, we reiterate that the Green Lab course is designed as a technology-independent course. In other words, the learning objectives of the course, its structure, assessment procedure, and the majority of lecture contents are exclusively about energy efficiency and ESE research methods, and not bound to any specific technology (e.g., Android, microservices, IoT) or application domain (e.g., finance, social media). The only technology-specific components of the course are the following: (i) the topics of students' projects, (ii) the first practical session on the lab environment, tools, and devices, and (iii) the examples the instructor uses during the lectures. Being technology-independent is important for us (and future instructors willing to reproduce the course) since in this way the course does not need to be fully redesigned over the years. Being technology-independent, the course also allows students to acquire and apply ESE principles in other contexts, such as their final project (which might not even be about energy efficiency) or in their professional careers. Instructors willing to replicate the course in their own departments can reuse the course material as it is, with only minor adaptations, depending on available equipment (which might even be students' laptops). In any case, we suggest instructors to build an inventory of the hardware (and software) equipment available in their department before running an edition of the Green Lab course. In this way, students can still measure energy consumption in a correct and reliable manner, without the need to buy expensive hardware. This is also a good opportunity for collaborating with other research groups within a department/university, for example by carrying out controlled experiments on the energy efficiency of software in the context of, e. g., robotics, high-performance computing infrastructures, bioinformatics.\nIn case of a large number of students (e.g., above 50), a good strategy to scale is to fix the technologies and used tools considered in the course. For example, Android Runner was used by all students' groups for their Green Lab projects starting from 2017/2018 until 2022/2023. This allows (i) teaching assistants to save considerable effort in preparing for labs and supporting students in case of technical issues and (ii) the teaching team to assess students' projects more efficiently. However, it is important to note that this also comes at the cost of flexibility when defining students' projects, thus potentially impacting the research output of the course. For example, in the years when we were using exclusively Android Runner, all students' projects were about mobile (Web) apps on Android. Moreover, we often reuse technologies that are proven to be effective for specific domains, saving time and resources when preparing experiments. In the last edition of the course, we decided to go beyond Android and cover multiple technologies and platforms again; this was possible thanks to the possibility of financially supporting three teaching assistants dedicated to the course. Besides using our open-source tools, such as Experiment Runner and Android Runner, we usually integrate, for example, Prometheus [58] and Grafana [11] within experiments involving microservice-based applications. Prometheus and Grafana are open-source tools enabling monitoring and visualization of a set of metrics, such as CPU utilization and energy consumption.\nTools pay off, but they require continuous maintenance, otherwise, they fire back. Over the years we learned that just having a tool like Android Runner is not enough to ensure a successful execution of experiments. For example, in previous years the Android Runner tool was tested on a relatively low number of smartphones and students using it on their own devices were having several compatibility prob- lems; in those years we received some negative feedback from students about the fact that they were spending a lot of time in fixing issues with the tool, rather than learning how to conduct experiments on energy-efficient software. As a solution, we decided to allocate one teaching assistant for one day a week for the whole academic year whose main responsibilities are to (i) fix bugs in the tools used in the Green Lab, (ii) support students having issues with those tools, and (iii) implementing new features in the tools as new needs arise. As a result, after the injection of this new resource, we did not receive any negative feedback about tools in course evaluations, and students were able to focus on the main learning objectives of the course.\nGreen Lab equipment is shared between several users, mostly students and re- searchers. The equipment is continuously maintained to avoid being accidentally damaged after usage. Students shall be carefully instructed about equipment us- age, both hardware and software. For example, we usually use a Monsoon power meter, described in Section 4, for experiments requiring high accuracy and that are executed on Raspberry Pis. The main channel of the Monsoon power meter needs to be connected to the GPIO pins of the Raspberry Pi, namely a 5V source pin and a pin for the ground (GND). Users should be careful in supplying the correct voltage and avoid current spikes that can damage the Raspberry Pi. A protocol pro- viding step-by-step instructions for using the equipment is crucial to avoid misuse. During our course, it also happened that students accidentally left software running on cluster nodes or used nodes concurrently. We tell students to stop all software that is launched after finishing their session and to check running processes before carrying out their experiment. Concurrent access is mitigated by using a shared calendar in which users book specific time slots on cluster nodes. Software installed on cluster nodes, smartphones, and development boards can also influence experi- ment execution, if not maintained. Different versions of tools and libraries can cause incompatibilities. Legacy versions of a tool or operating system can be less efficient and contain security flaws, and thus, influence experiment outcomes. Older versions of operating systems may not support newer versions of the software to be tested by users."}, {"title": "8 Conclusions", "content": "The main contribution of this chapter is the description of the Green Lab course, i.e., a course in the area of empirical software engineering in the context of energy- efficient software. The design, organization, and syllabus of the course can be used as a basis for delivering a similar course in other universities and programs. The provided online material should facilitate a replication of the course with relatively"}]}