{"title": "AUGMENTATION-DRIVEN METRIC FOR BALANC-ING PRESERVATION AND MODIFICATION IN TEXT-GUIDED IMAGE EDITING", "authors": ["Yoonjeon Kim", "Soohyun Ryu", "Yeonsung Jung", "Hyunkoo Lee", "Joowon Kim", "June Yong Yang", "Jaeryong Hwang", "Eunho Yang"], "abstract": "The development of vision-language and generative models has significantly advanced text-guided image editing, which seeks preservation of core elements in the source image while implementing modifications based on the target text. However, in the absence of evaluation metrics specifically tailored for text-guided image editing, existing metrics are limited in balancing the consideration of preservation and modification. Especially, our analysis reveals that CLIPScore, the most commonly used metric, tends to favor modification and ignore core attributes to be preserved, resulting in inaccurate evaluations. To address this problem, we propose AugCLIP, which balances preservation and modification by estimating the representation of an ideal edited image that aligns with the target text with minimum alteration on the source image. We augment detailed textual descriptions on the source image and the target text using a multi-modal large language model, to model a hyperplane that separates CLIP space into source or target. The representation of the ideal edited image is an orthogonal projection of the source image into the hyperplane, which encapsulates the relative importance of each attribute considering the interdependent relationships. Our extensive experiments on five benchmark datasets, encompassing a diverse range of editing scenarios, demonstrate that AugCLIP aligns remarkably well with human evaluation standards compared to existing metrics. The code for evaluation will be open-sourced to contribute to the community.", "sections": [{"title": "1 INTRODUCTION", "content": "Building on advancements in vision-language models (Radford et al., 2021; Li et al., 2022; Geng et al., 2023), recent generative models (Kawar et al., 2022; Brooks et al., 2022; Hertz et al., 2022) have been widely utilized as creative tools for image editing via text instructions. Text-guided image editing models enable the modification of images in response to textual guidance, ensuring that changes are aligned with the provided instructions. The primary objective of these models is to apply specific modifications guided by the target text while preserving the core attributes of the source image.\nDespite the remarkable advancements in editing models, there has been a lack of rigorous evaluation methods, tailored specifically for text-guided image editing. Consequently, most studies (Hertz et al., 2023; Basu et al., 2023; Gal et al., 2022; Kim & Ye, 2021; Brooks et al., 2022; Gal et al., 2022; Ruiz et al., 2023; Kocasari et al., 2022) have heavily relied on human evaluation, which provides balanced consideration of preservation and modification aspects. However, as it is costly and impractical for real-world applications, researchers have adapted automatic evaluation metrics (Zhang et al., 2018; Kim & Ye, 2021; Caron et al., 2021; Gal et al., 2022) originally designed for other vision tasks, such as image generation or captioning. Particularly, CLIPScore (Gal et al., 2022) is widely used as a representative metric, which evaluates the extent of alignment between the edited image and the target text, based on the difference between the target and source text in the CLIP space.\nHowever, despite its widespread adoption, our analysis reveals significant limitations in CLIPScore, contradicting the standard of human evaluators. First, it tends to prioritize modification over preservation, unlike human evaluators who balance both aspects. This bias leads to inflated scores for excessively modified images that neglect even key attributes of the source image. Second, CLIPScore often focuses on peripheral parts rather than regions that are pertinent to the target text, whereas human evaluators can focus on the regions that must be edited. These observations underscore the need to reevaluate the effectiveness of CLIPScore in text-guided image editing.\nBased on our comprehensive analysis, we propose a novel metric, AugCLIP, which evaluates the quality of the edited image by comparing it with an estimated representation of a well-edited image that balances preservation and modification by identifying a key modification vector that transforms the source image to match the target text while minimizing alterations. For this purpose, we leverage large language models to extract attributes that capture various visual aspects of the source image and target text. Then, we estimate the key modification vector by a hyperplane that separates the source and target attributes, considering the intertwined relationships between them. To this end, AugCLIP evaluates how closely the edited image aligns with the estimated ideal derived by applying the modification vector to the source image.\nOur metric AugCLIP demonstrates remarkable improvement in alignment with human evaluators on diverse editing scenarios such as object, attribute, and style alteration compared to all other existing metrics. Moreover, our metric is even applicable to personalized generation, the DreamBooth dataset, where the objective is to identify an object in the source image and edit it into a completely novel context. This shows the flexibility of AugCLIP, which seamlessly applies to a variety of editing directions. Notably, our metric excels in identifying minor differences between the source image and the edited image, showing superb ability in complex image editing scenarios such as MagicBrush.\nThe major contributions are summarized as follows.\n\u2022 We are the first to point out CLIPScore's reliability in text-guided image editing, as it frequently exhibits a bias towards modification rather than preservation and focuses on irrelevant regions.\n\u2022 We introduce AugCLIP, a metric for image editing by automatically augmenting descriptions via LLM and estimating a balanced representation of preservation and modification, which takes into account the relative importance of each description.\n\u2022 AugCLIP demonstrates a significantly high correlation with human evaluations across various editing scenarios, even in complex applications where existing metrics struggle."}, {"title": "2 RELATED WORKS", "content": "Currently widely used metrics for text-guided image editing assess one of the following aspects: image preservation and image-text alignment. For evaluating image preservation, FID (Heusel et al., 2017), IS (Salimans et al., 2016), and LPIPS (Zhang et al., 2018) measure feature distance between generated images and real images. However, these metrics tend to focus primarily on the preservation of the source image rather than assessing the quality of the modifications. To evaluate image-text alignment, CLIPScore (Gal et al., 2022) is widely used, measuring the similarity between the intended textual change and the actual modifications in the image, helping to assess how well the source image is altered according to the target text."}, {"title": "3 PROBLEM ANALYSIS ON CLIPSCORE FOR TEXT-GUIDED IMAGE\nEDITING MODEL", "content": "In this section, we discover two major challenges in CLIPScore as an evaluation metric for text-guided image editing. First, CLIPScore tends to overemphasize modification aligning with the target text while neglecting the preservation of the source image (Sec. 3.2). Second, it often fails to concentrate on the image regions that are directly relevant to the target text (Sec. 3.3)."}, {"title": "3.1 PRELIMINARIES: CLIPSCORE", "content": "In common text-guided image editing scenarios, a model generates an edited image $I_{edit}$ from a source image $I_{src}$ accompanied by a target text $T_{trg}$. Additionally, a source text $T_{src}$ that represents the source image is either provided as descriptions annotated by humans or generated using image captioning models.\nCLIPScore, the most widely used metric in text-guided image editing, evaluates the modification based on the difference between $T_{trg}$ and $T_{src}$ in the CLIP space as follows:\n$CLIPScore = cs(\\Delta I, \\Delta T) = cs(CLIP(I_{edit}) - CLIP(I_{src}), CLIP(T_{trg}) \u2013 CLIP(T_{src})),$ (1)\nwhere $cs(a, b) = \\frac{a \\cdot b}{||a||||b||}$ denotes cosine similarity and CLIP(\u00b7) is a CLIP encoder for either image or text."}, {"title": "3.2 OVEREMPHASIZING MODIFICATION OVER PRESERVATION IN EVALUATION", "content": "Although CLIPScore attempts to incorporate the preservation by subtracting Tsrc from Ttrg, we observe that it has a tendency to overemphasize modifications towards target text. In Fig. 1, CLIPScore often assigns higher scores to excessively modified images that neglect the key aspects of the source image.\nTo investigate this further, we conduct an experiment on the TEdBench (Kawar et al., 2022) and MagicBrush (Zhang et al., 2024) datasets, which consist of pairs of source images and target texts, along with ground truth edited images reflecting the desired edits. We generate excessively modified images using the text-to-image generation model, Stable Diffusion 1.5, based solely on the target text. Our results show that CLIPScore struggles to differentiate between ground truth images and excessively modified ones, favoring ground truth images in only 37% of cases in Tedbench, and 64.9% of cases in MagicBrush. This highlights CLIPScore's bias toward modification over preservation.\nThis inability of CLIPScore to properly account for the source image preservation stems from its design of the text direction, which assumes that a well-edited image should primarily adhere to the target text. As illustrated in Fig. 2, conflicts frequently occur between the visual elements of the source image and the target text regarding which features should be preserved or modified. For example, the 'sitting' posture of the source image should be preserved over the 'standing heroically' description in the target text, while the 'orange T-shirt' should be modified to a 'red armor suit.' A well-designed metric would account for these conflicts, but CLIPScore, due to its underlying assumption, blindly favors features from the target text, leading to unreliable results. This highlights the need for a metric that better balances preservation and modification."}, {"title": "3.3 OVERLOOKING EDITED REGIONS IN THE IMAGE", "content": "An evaluation metric is more effective when it focuses on the image regions modified following the target text, rather than peripheral or unchanged regions. For example, if a target text specifies making a dog yawn, the evaluation metric works better when it concentrates primarily on the dog's mouth, not its ears. To assess CLIPScore's capability in this regard, we conduct an experiment using the relevancy map (Chefer et al., 2021), denoted as $R$, which visualizes the transformer's attention on an image corresponding to a given text. Specifically, for an image $I \\in R^{h \\times w}$ and text $T$, the relevancy map is computed as $R(I;T) = \\nabla_{ACS}(CLIP(I), CLIP(T); A) \\odot A \\in R^{h \\times w}$, where A represents the attention scores of the CLIP visual encoder and $\\odot$ denotes the Hadamard product. To visualize the relevancy map of CLIPScore, which is a cosine similarity between $\\Delta I$ and $\\Delta T$, we subtract the two relevancy maps as $R(\\Delta I; \\Delta T) = R(I_{edit}; \\Delta T) \u2013 R(I_{src}; \\Delta T)$."}, {"title": "4 AUGCLIP: A NOVEL METRIC BALANCING PRESERVATION AND\nMODIFICATION", "content": "In this section, we propose a novel evaluation metric, AugCLIP, that estimates the representation of a well-edited image by identifying a minimum modification vector that transforms the source image to match the target text while minimizing alterations. AugCLIP starts by augmenting the source image and target text with fine-grained attributes that represent source and target classes in binary classification (Sec. 4.1). Then, the minimum modification vector is determined as a vector with minimum norm to be classified as target class (Sec. 4.2). In this process, we also account for the relative importance of each visual attribute, considering their interrelationships in response to the target text (Sec. 4.3)."}, {"title": "4.1 EXTRACTING VISUAL ATTRIBUTES", "content": "Inspired by the finding that detailed descriptions of the target text make the edited region more noticeable in Sec. 3.3, we extract visual attributes from the source image and target text using a state-of-the-art multi-modal large language model (MLLM), GPT-4V (OpenAI, 2023). To extract visual attributes from the source image, we prompt GPT-4V to generate a detailed caption that encapsulates the key visual attributes present in the source image. This caption is then parsed into discrete visual attributes. For example, given a source image depicted in Fig. 2, let us assume that GPT-4V generates the caption: 'a man is sitting and wearing both blue caps and orange T-shirt'. Then, this caption is broken down into individual attributes such as 'a sitting man', 'wearing a T-shirt', and 'wearing a blue cap.'\nWhen processing the target text, the focus shifts to identifying the modifications that need to be made to the source image during the editing process. To achieve this, GPT-4V is prompted with both the source and target text and then instructed to describe the aspects of the target text that diverge from the source text. To ensure that each generated description corresponds to a single visual attribute, we provide example descriptions along with the prompt.\nThese attributes are encoded into CLIP, where the source attributes are denoted as $S = \\{s_i\\}_{i=1}^{N_s}$ and target attributes as $T = \\{t_j\\}_{j=1}^{N_t}$. $N_s$ and $N_t$ are the number of attributes for the source and target, respectively. Further details in designing the prompts for source and target descriptions are illustrated in appendix C.1.\nA straightforward approach to incorporating descriptions into CLIPScore is to substitute $\\Delta T$ with $\\frac{1}{N_t} \\Sigma_{i=1}^{N_t} 1 - \\Sigma_{i=1}^{N_s} s_i$. However, as shown in Tab. 3a, this simple augmentation is insufficient and often degrades performance. To address this, we propose a method that estimates the CLIP representation of an ideal edited image, aiming to balance preservation and modification more effectively."}, {"title": "4.2 DERIVING THE MINIMUM MODIFICATION VECTOR", "content": "We estimate the ideal edited image, that minimizes the alteration to the source image, as $I_{src} + v$. Minimum modification vector $v$ represents a vector that adjusts the source image to align with the target text. By deriving the classifier function that determines the source class S or target class T in CLIP space, we can assess whether the source image has been modified sufficiently to match the target text.\nFormally, the classifier function $f(x) = w^\\top x + b$ assigns a CLIP embedding $x$ to the target class if $f(x) > 0$, or to the source class if $f(x) < 0$. Then, $v$ is a vector that has a minimum norm and satisfies the condition that the edited image is classified as belonging to the target class if the following condition,\n$\\min_v ||v||$ subject to $w^\\top (I_{src} + v) + b > 0$, (2)\nis satisfied. Since the classification relies on the projection onto the normal vector $w$ of $f(x)$, $w$ captures the attributes that are most distinguishing between S and T. So, the direction of v should be the same as w. Finally, the modification vector is expressed as\n$v = \\frac{w^\\top w}{||w||^2}w I_{src} + b$. (3)\nThen, $I_{src} + v$ represents the orthogonal projection of the source image $I_{src}$ onto the decision boundary $w^\\top x+b = 0$. The derivation of $v$ has been deferred to appendix C.2 due to spatial constraint."}, {"title": "4.3 CONSIDERING INTERTWINED RELATIONSHIP BETWEEN ATTRIBUTES", "content": "When determining the decision boundary between S and T, it is crucial to account for the relative importance of each attribute, considering the interconnections between them. This is because most image editing tasks require simultaneous modification of multiple related visual attributes, as these attributes often work together to create a cohesive appearance. For instance, transforming a human face into a 'smiling face' involves adjusting several interconnected features, such as upturned mouth corners, crinkled eyes, and raised cheeks, all of which must appear together in the edited image. However, the current approach to defining the decision boundary focuses solely on separation and does not consider these attribute relationships.\nTo address this, we emphasize the similarity between attributes within the same class. For source attribute $s_i \\in S$, $E_{s \\in s}[cs(s_i, s)]$ measures the averaged similarity within the source class, while for target attribute $t_i \\in T$, $E_{t \\in t}[cs (t_i,t)]$ represents similarity within the target class. Additionally, source or target attributes that are already similar to those in the opposite class are not to be modified, having less impact on the modification vector v. As a result, their influence should be reduced during the decision boundary optimization using the distance measurement with the opposite class as $-E_{t \\in T}[cs(s_i,t)]$ for source attribute $s_i \\in S$ and $-E_{s \\in s}[cs(t_i, s)]$ for target attribute $t_i \\in T$.\nThe final weightings, denoted by $a_s$ for source attributes and $a_t$ for target attributes, are\n$a_i^s = E_{s \\in s}[cs(s_i, s)] \u2013 E_{t \\in T}[cs(s_i, t)]$ for $s_i \\in S$, (4)\n$a_i^t = E_{t \\in t}[cs(t_j,t)] \u2013 E_{s \\in s}[cs(t_j, s)]$ for $t_j \\in T$, (5)\nwhich define sample-wise weights in deriving $f(x) = w^\\top x+b$. Combined with $a_s$ and $a_t$, we minimize hinge loss objective to derive ||v|| with L2 norm constraint on w. This is to account for the margin between S and T and assign higher penalization to the misclassification of attributes with large weight values. We demonstrate that this objective leads to the separation of S and T by reporting the misclassification rate in Tab. 3c and visualizing the decision boundary in appendix C.3.\nFinally, AugCLIP evaluates how the edited image aligns with the estimation of the well-edited image in CLIP space as\n$AugCLIP = cs(I_{edit}, I_{src} + v),$ (6)\nwhere $v = -(w^\\top I_{src}+b)/||w||^2 \\cdot w$ from Eq. (3)."}, {"title": "5 EXPERIMENTS", "content": "Implementation Details. For our experiments, we employ a pre-trained CLIP-ViT 16/B model for CLIP-based metrics. Source and target attributes are generated using GPT-4V (OpenAI, 2023). Further details on prompting the source and target descriptions are deferred to the appendix due to spatial constraints.\nCompared Metrics. We compare AugCLIP with two categories of existing metrics. The first category comprises the metrics that focus solely on preservation aspects, including DINO similarity, LPIPS, and L2 distance. The other category measures target text alignment, for which the only metric is CLIPScore. Additionally, we utilize description-augmented versions of CLIPScore.\nEvaluation Datasets. We evaluate AugCLIP and existing metrics across several text-guided image editing benchmarks, including TEdBench (Kawar et al., 2022), EditVal (Basu et al., 2023), MagicBrush (Zhang et al., 2024), DreamBooth (Ruiz et al., 2023), and CelebA Liu et al. (2015). Each benchmark dataset represents varying editing scenarios as in Tab. 1."}, {"title": "5.1 QUALITY ASSESSMENT ON EVALUATION METRICS", "content": "To evaluate the effectiveness of different evaluation metrics, we conducted two types of experiments, named 2AFC test and Ground truth test. Two-Alternative Forced Choice (2AFC) test (Tab. 2a) reveals the alignment between the evaluation score and human judgment. In this test, human evaluators are asked with two options of edited images, and then"}, {"title": "5.2 ABLATION STUDY", "content": "Augmenting Descriptions into CLIPScore We demonstrate the impact of augmenting CLIPScore with descriptions, extracted in Sec. 4.1. Specifically, we replace the source and target text embeddings with the average of their respective description embeddings. As shown in Tab. 3a, this straightforward augmentation often results in a decline in performance. Even incorporating our weighting strategy (Eq. (4)) that captures the relative importance of each attribute, the weighted average fails to yield significant improvements and even leads to degradation. In contrast, AugCLIP consistently outperforms all description-augmented versions of CLIPScore. This improvement is due to AugCLIP's approach, which computes the minimum modification vector through projection onto the separating hyperplane, fundamentally different from CLIPScore's simple subtraction of source from target.\nWeighting Strategy for Hyperplane We demonstrate the effectiveness of our weighting strategy, described in Eq. (4), by comparing human alignment score and ground truth test accuracy as shown in Tab. 3b. Our weighting strategy enables AugCLIP to prioritize key"}, {"title": "5.3 APPLICATION OF AUGCLIP IN DIVERSE EDITING SCENARIOS", "content": "Text-guided image editing encompasses a wide range of tasks such as style editing, object replacement, and partial alteration. Given the variety of editing scenarios included in the EditVal dataset, we report the human alignment score, salign, for each specific task to highlight the effectiveness of our proposed metric, AugCLIP, in addressing different types of tasks. Across all eight scenarios, except for the texture modification task, AugCLIP outperforms CLIPScore as shown in Tab. 4. The superior performance of AugCLIP in various editing scenarios is due to its enriched understanding of both the source image and target text. Unlike CLIPScore, which depends on a single CLIP representation for each source image and target text, AugCLIP leverages multiple CLIP representations that capture attributes of the source and target. Additionally, it harmoniously considers preservation and modification aspects by deriving a linear hyperplane that separates the source and target attributes."}, {"title": "CONCLUSION", "content": "We present AugCLIP, a novel evaluation metric for text-guided image editing that balances source image preservation and target text modification. By leveraging a multi-modal language model to extract visual attributes and finding a separating hyperplane, AugCLIP estimates a representation of an ideal edited image that closely matches human preferences. Experiments on five benchmark datasets show that AugCLIP outperforms existing metrics,"}, {"title": "B LIMITATION", "content": "While AugCLIP demonstrates strong performance in balancing preservation and modification in text-guided image editing, several limitations remain. First, the reliance on GPT-4V for visual attribute extraction can lead to inconsistencies, especially in complex scenarios where subtle details are crucial. The quality of extracted attributes may vary depending on the specificity of the scene and the quality of the model's understanding, which can affect the accuracy of the modification vector. Additionally, AugCLIP requires longer computation times due to the need for detailed description generation and the optimization process involved in fitting the hyperplane. This makes it less efficient for real-time or large-scale applications where rapid evaluation is necessary. We provide an analysis of these limitations in appendix \u0421.5."}, {"title": "B.1 ANALYSIS ON FAILURE CASES OF AUGCLIP", "content": "Compared to CLIPScore, AugCLIP shows superior alignment with human evaluation and a stronger ability to classify ground truth images. However, there are several cases where CLIPScore aligns better with human preference. The examples demonstrated in Fig. 11 represent cases where AugCLIP diverges from human judgment. For example, in the first row, both edited images are adequately modified to resemble a target text 'dog.' Yet, the edited image in the middle emphasizes dog-like features more strongly than the one on the right, which exhibits subtler changes. As a result, human evaluators tend to favor the more prominently modified one. While both edits are well-executed in terms of preservation and modification, human evaluation shows subjectivity in preference. Similarly, the example in the first row of adding fruit toppings to donuts is also contentious. Both edited images accurately depict fruit toppings while preserving the original content. Nevertheless, human evaluators preferred the middle image, which better retains the original color and texture of the donut. These examples highlight that evaluation metrics cannot fully capture human preferences, which are inherently subjective and vary on a case-by-case basis."}, {"title": "C DETAILS ON AUGCLIP", "content": ""}, {"title": "C.1 DESCRIPTION GENERATION PROCESS", "content": "We leverage GPT-4V (OpenAI, 2023) to extract visual attributes of the source image and target text. These attributes are presented as textual descriptions, highlighting various visual features like shape, color, texture, patterns, posture, action, and position. The number of extracted descriptions is determined by the ability of GPT-4V depending on the complexity of editing scenarios. For complex scenes, GPT-4V typically produces around 30 descriptions, while simpler scenarios, involving only a single object and basic modifications, generate roughly 5 descriptions\u2014sufficient to capture the entire scene and intended edits. Fig. 12 shows the prompt used for attribute extraction, where example outputs ensure that each description represents a distinct visual element. A description is a text that depicts how the source image or target text would be visually represented in terms of texture, shape, color, size, etc."}, {"title": "C.2 DERIVATION OF MODIFICATION VECTOR V", "content": "The source image with minimum modification, Isrc+v is classified to target class if $w^\\top (I_{src}+\nv) + b > 0$ satisfies. Then, the condition is reformulated as $w^\\top v > -(w^\\top I_{src} + b)$. Then, the optimization problem to find a minimum modification v is\n$\\min_v ||v||$ subject to $w^\\top v > -(w^\\top I_{src} + b)$. (7)\nTo minimize ||v||, v should be in the direction that most efficiently increases the classifier output f(Isrc). This direction is given by the gradient of the classifier function concerning x, which is f(x) = w. Therefore, the optimal v is aw where a > 0 is a scalar to be determined. Substituting v = aw to Eq. (7), we derive the following equation\n$\\alpha > \\frac{-(w^\\top I_{src} + b)}{||w||^2}$. (8)\nIn order to solve for a that minimizes ||v||, we derive the smallest a satisfying the condition as\n$\\alpha_{min} = \\frac{-(w^\\top I_{src} + b)}{||w||^2}$. (9)\nThen the final form of v is represented as\n$v \\coloneqq \\alpha_{min} w = \\frac{-(w^\\top I_{src} + b)}{||w||^2} w$. (10)"}, {"title": "C.3 CHOICE OF DECISION BOUNDARY AS LINEAR FUNCTION", "content": "Our metric AugCLIP first encodes the source and target descriptions into CLIP space, forming source and target distributions denoted by S and T respectively. To define a decision boundary that separates the source and target distribution, we employ a linear function f(x) = $W^\\top x$ + b. As reported in Tab. 3c, the misclassification percentage in binary classification with the decision function is as small as 1.35% across five benchmark datasets. This signifies that the simple linear decision function f(x) can separate the source and target distributions. Additionally, we visualize that CLIP features of source and target attributes can be separated by a linear hyperplane in 2D projected space using UMAP (McInnes et al., 2018) in Fig. 13, in which randomly chosen subset of TEdBench samples are plotted. 'S' represents source attributes encoded into CLIP, while 'T' represents the target attributes. The line signifies the linear hyperplane f(x) = $W^\\top x$ + b = 0 that separates the two classes, which are source and target. In AugCLIP, the linear hyperplane f(x) = 0 is d-dimensional following the original dimension of CLIP, but to visualize in Fig. 13, the dimension shrinks into d = 2 by UMAP fitting."}, {"title": "C.4 QUALITATIVE ANALYSIS ON THE EFFECT OF MODIFICATION VECTOR V", "content": "The goal of text-guided image editing is to modify the source image minimally by applying the modification vector v, ensuring that essential source attributes remain unchanged but the resulting edited image resembles the target text. In this section, we qualitatively analyze the effect of v on source and target attributes."}, {"title": "C.5 COMPUTATION TIME AND DESCRIPTION VARIANCE", "content": "Our method, AugCLIP, requires extracting descriptions via LLMs, and then fitting the hyperplane between source and target attributes to derive the ideal representation. Compared to CLIPScore which simply requires similarity measurement between the image and text, our method requires 12.3 seconds for description generation, followed by 0.15 seconds for score computation. This poses extra computation time for description generation, but the description set for the established benchmark dataset could be preprocessed to be reused in the evaluation process, making the computation time on par with CLIPScore.\nSince the descriptions are extracted with GPT-4V, the generations vary across different seeds. We test five different seeds for the description generation process to randomize the extracted source descriptions and target descriptions which form S and T respectively. Across all random seeds, the variance of human alignment score, Salign, is merely 0.0197. This demonstrates the robustness of the description generation process, indicating that the variance in the descriptions generated by different seeds has a negligible effect on the overall alignment with human judgments."}, {"title": "C.6 COMPARISON WITH GPT-4V", "content": "Recently, GPT-4V has been employed in evaluating various vision-language tasks, including text-guided image editing, text-to-image generation, and image quality assessment. Since GPT-4V is one of the best performing multi-modal large language models, we test the ability of GPT-4V's effectiveness in evaluating the quality of text-guided edited images, focusing on both preservation and modification aspects. As shown in Tab. 5, GPT-4V outperforms AugCLIP in tasks such as EditVal and TEdBench, which involve simple edits like modifying a single object's attribute. This finding is consistent with prior research (Zhang et al., 2023), which suggests that GPT-4V struggles to differentiate between images with subtle differences. In contrast, our proposed metric, AugCLIP, effectively captures minor differences by augmenting attributes of the source image and target text and shows better performance in other benchmarks with complex scenarios."}, {"title": "D EXPERIMENTAL DETAILS", "content": ""}, {"title": "D.1 ASSETS", "content": ""}, {"title": "D.2 USER STUDY DETAILS", "content": ""}, {"title": "D.3 BENCHMARK DATASETS FOR TEXT-GUIDED IMAGE EDITING", "content": "TEdBench comprises 100 pairs of source image and target text. It focuses on specific settings where the source image has a single object at the center, and the corresponding target text only modifies some attributes of that object.\nEditVal contains 648 image-text pairs that cover 13 different types of edits, including object addition, object replacement, and size modification. Since it has such complicated editing scenarios, the models that we leveraged could not properly edit most cases so there are not many samples with enough quality for user study. Therefore, we use the subset of EditVal that encompasses eight editing types that show adequate modification for proper evaluation.\nMagicBrush is a benchmark specifically designed to evaluate sequential editing tasks, where iterative modifications are made to different parts of the source image. Dreambooth enables the modification of specific instances within the source image by providing corresponding masks along with image-text pairs; however, since typical editing models do not utilize masks as input, we only consider the image-text pairs in our evaluation.\nFinally, for the CelebA dataset, we create a subset consisting of 50 image-text pairs that guide changes specific to facial attributes. We created the prompt by swapping attributes of human faces."}, {"title": "E EXISTING EVALUATION METRICS", "content": "FID (Heusel et al., 2017) and IS (Salimans et al., 2016) evaluate the diversity and quality of generated images by analyzing the output of a pre-trained classifier. They only assess the fidelity of the edited image, regardless of the model inputs.\nLPIPS (Zhang et al., 2018), DINO similarity (Caron et al., 2021) and Segmentation Consistency (Kim & Ye, 2021) evaluate the preservation of source image in terms of distributional change in extracted feature and change in segmentation maps. These metrics do not consider how the source image should be modified accordingly with the given target text.\nSeveral metrics evaluate the alignment between the edited image and the text guidance (Hessel et al., 2021), relying on vision-language models (Radford et al., 2021; Minderer et al., 2022)."}, {"title": "E.1 COMBINATION OF PRESERVATION-CENTRIC METRICS WITH CLIPSCORE", "content": "We explore whether combining preservation and modification metrics can improve alignment with human judgments across three datasets: CelebA, EditVal, and DreamBooth. To do so, we combine the two metrics, CLIPScore and one of several preservation metrics- LPIPS, Segment Consistency, DINO similarity, and L2 - using an interpolation value, denoted as \u03b3. Specifically, the combined score is computed as\nCLIPScore \u00d7 \u03b3 + Preservation score \u00d7 (1 \u2013 \u03b3). (13)\nBoth metrics are scaled to the same range before interpolation, ensuring the interpolation value \u03b3 is properly reflected in the final combined score.\nAs illustrated in Fig. 17, combination generally reduces alignment with human evaluations on the CelebA and EditVal datasets. These datasets require substantial modifications to the overall image, rendering current preservation metrics, which assess image quality or structural similarity, ineffective in capturing preservation accurately. Consequently, using CLIPScore alone results in much higher alignment with human judgments. Conversely, in the DreamBooth dataset, which involves modifications to small image regions, the combined metrics outperform CLIPScore alone. However, all combinations perform significantly worse than our proposed metric AugCLIP, highlighting the superior performance of AugCLIP in balancing preservation and modification, rather than relying on simple linear interpolation."}]}