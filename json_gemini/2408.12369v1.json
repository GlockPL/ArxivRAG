{"title": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete\nfor Enhanced Query Precision in Tabular Question Answering", "authors": ["Pratyush Kumar", "Kuber Vijaykumar Bellad", "Bharat Vadlamudi", "Aman Chadha"], "abstract": "With advancements in Large Language Models\n(LLMs), a major use case that has emerged is\nquerying databases in plain English, translating\nuser questions into executable database queries,\nwhich has improved significantly. However,\nreal-world datasets often feature a vast array\nof attributes and complex values, complicat-\ning the LLM's task of accurately identifying\nrelevant columns or values from natural lan-\nguage queries. Traditional methods cannot\nfully relay the dataset's size and complexity\nto the LLM. To address these challenges, we\npropose a novel framework that leverages Full-\nText Search (FTS) on the input table. This\napproach not only enables precise detection of\nspecific values and columns but also narrows\nthe search space for language models, thereby\nenhancing query accuracy. Additionally, it sup-\nports a custom auto-complete feature that sug-\ngests queries based on the data in the table.\nThis integration significantly refines the inter-\naction between the user and complex datasets,\noffering a sophisticated solution to the limita-\ntions faced by current table querying capabil-\nities. This work is accompanied by an appli-\ncation \u00b9 for both Mac and Windows platforms,\nwhich readers can try out themselves on their\nown data.", "sections": [{"title": "Introduction", "content": "The advent of Large Language Models (LLMs) has\nrevolutionized the way we interact with data, of-\nfering unprecedented capabilities for processing\nand understanding natural language queries. As\nthese models advance, their ability to translate\ncomplex user inquiries into executable database\nqueries has significantly improved, making natural\nlanguage processing an increasingly viable method\nfor database interaction. This evolution holds the\npromise of democratizing data access, allowing\nusers without much technical expertise in query\nlanguages to harness the power of vast datasets.\nThe trend is further cemented by OpenAI's re-\ncent addition of Advanced Data Analysis capabil-\nities to ChatGPT. This innovation has the poten-\ntial to democratize data access by making large\ndatabases more accessible to those who may not be\nas proficient in formal query languages.\nThe current landscape of TQA can be catego-\nrized into three broad approaches to the best of our\nknowledge:"}, {"title": "Model-Centric Approaches", "content": "By training encoder-decoder models like TAPEX\n(Liu et al., 2022) to deep learning-based models\nlike MLM methods (Herzig et al., 2020), the mod-\nels can understand the subtleties of organized tab-\nular data and provide answers based on it. These\nmethods have a significant drawback in that the\ncomplete dataset needs to be loaded into memory,\nwhich makes them inappropriate for huge, compli-\ncated real-world datasets. Additionally, the context\nlength restricts the quantity of data that may be\nsupplied to the model by design, just like any other\nLLM."}, {"title": "Reasoning and Tool-Enhanced\nFrameworks", "content": "Gemmell and Dalton (2023) shows how to incor-\nporate tool synthesis techniques and generate re-\nsponses using structured reasoning techniques like\nChain of Thoughts prompting (Wei et al., 2023).\nThese approaches break down the table into nu-\nmerous phases and use structured reasoning over\nthe table to determine the best potential answer\nto the user inquiry. In a similar vein, an LLM's\nability to identify a potential remedy is enhanced\nusing one or more tools. Similarly, for the model\nto complete the analysis, many of these approaches\nrequire the entire dataset to be provided. However,\nthese methods have produced some excellent re-"}, {"title": "Natural Language to DB Query", "content": "Converting natural language questions about the\ndataset into corresponding database queries by uti-\nlizing the general capabilities of the LLM, which in-\nclude code generation in several programming lan-\nguages (Cheng et al., 2023), is another developing\nfield of study. Target languages for tabular queries\nare often SQL and Pandas, which are among the\nmost popular query languages and frameworks\navailable today. This specific method of dataset\nquerying has the benefit of being functional regard-\nless of the dataset's location or size, allowing the\ngenerated query to be executed in a secure environ-\nment to obtain the answer and not being restricted\nto a fixed memory/token limit. This makes it useful\nfor many large and complex real-world datasets.\nLLMs such as Llama (Touvron et al., 2023)\nand Mistral (Jiang et al., 2023) have become open\nsourced recently. This has allowed businesses to\ncontrol the data and model, creating new opportu-\nnities for TQA systems that prioritize security and\nprivacy at enterprise level. Additionally, improve-\nments in quantization methods such as GGUF2,\nGGML\u00b3, GPTQ (Frantar et al., 2023), and most sig-\nnificantly, frameworks like llama.cpp, have made\nit feasible to run these models on consumer-grade\nhardware, granting end users unrestricted, private,\nand cost-free access to these impressive models.\nNatural language processing is becoming a more\npractical approach for database interaction as these\nmodels develop and get better at translating com-\nplex user questions about a dataset into executable\ndatabase queries.\nThis paper outlines a novel system that uses Full-\nText Search (FTS) to address the difficulties as-\nsociated with using natural language queries on\nhuge datasets. Using an autocomplete tool, our\nmethod seeks to improve LLM accuracy, reduce\ncompute burden, and make query generation easier.\nThrough the establishment of a connection between\nstructured databases and natural language compre-\nhension, our system provides enhanced precision\nand efficiency when handling complex real-world\ndata."}, {"title": "Related Work", "content": ""}, {"title": "Enhancing Table Question Answering\n(TQA) through Advanced Frameworks\nand Techniques", "content": "ReAcTable and Chain of Table both focus on\nimproving the performance of TQA by introducing\ninnovative frameworks that enhance the reasoning\ncapabilities of LLMs with tables.\nThe difficulty of Table Question Answering\n(TQA) at the nexus of data analytics and natural\nlanguage processing is examined in this study. This\nassignment requires logical reasoning, a compre-\nhension of data semantics, and analytical skills\nto answer queries in normal language based on\ntabular data. To handle TQA activities, the study\npresents ReAcTable, a framework that improves\non the ReAct paradigm (Zhang et al., 2023). De-\nspite complexity and processing needs, it offers a\nrevolutionary framework for TQA problems that\ngreatly improves performance using a combination\nof huge language models, external code executors,\nand creative majority voting procedures.\nWang et al. (2024) investigate how to include\nLLMs into table-based reasoning, which is an es-\nsential component of tasks such as fact verification\nand table-based question answering. The Chain-\nOf-Table structure, which is introduced in this re-\nsearch, greatly improves LLMs' capacity to handle\nand analyze tabular data. Because one must com-\nprehend the semantics of the queries as well as the\nsemi-structured tabular data, table-based reasoning\nis difficult. Previous methods, such as Chain-of-\nThought, enhanced the efficiency of LLMs by inte-\ngrating textual reasoning chains, but they had trou-\nble utilizing tabular data effectively. The reasoning\nchain's use of tabular data as a stand-in for inter-\nmediate concepts is suggested by the framework.\nIt directs LLMs to produce operations and update\nthe table iteratively to depict a complex reasoning\nchain. Through the formation of a chain that illus-\ntrates the thought process for a particular problem,\nthis technique enables LLMs to dynamically design\nthe subsequent operation based on past findings.\nAdding columns, choosing rows, grouping, and\nother atomic operations are frequently utilized in\nSQL and DataFrame development, and Chain-Of-\nTable makes ad-vantage of these operations. Table\nmanipulation and thinking are made easier by these"}, {"title": "Integrating Symbolic Languages and\nOperations with LLMs for TQA", "content": "Binding Language Models in Symbolic Lan-\nguages and LLMs are Versatile Decomposers:\nDecompose Evidence and Questions for Table-\nbased Reasoning both explore the integration of\nsymbolic languages (like SQL) and operations with\nLLMs to improve TQA. These studies focus on\nenhancing interpretability, resilience, and perfor-\nmance by bridging the gap between natural lan-\nguage processing and symbolic reasoning.\nBINDER (Cheng et al., 2023), is neural-\nsymbolic and does not require training. This frame-\nwork extends the capabilities of computer lan-"}, {"title": "Novel Methodologies and Tools for\nTableQA", "content": "TableQAKit offers a comprehensive toolkit that in-\ntegrates various techniques and LLMs for TableQA,\nserving as a practical resource for researchers and\npractitioners.\nThe toolkit by Lei et al. (2023) attempts to offer\na uniform platform that integrates common tech-\nniques, such as using LLMs, with different TQA\ndatasets. It blends various techniques and LLMs for\nthese tasks and has a large variety of TQA datasets.\nIncorporates a fresh approach\u2014that is, LLM-based\nmethodologies for organized QA assignments. Pro-\nvides a user-friendly interactive interface with vis-\nible operations and extensive data. The models,\ndatasets, and source code are accessible to the pub-\nlic. A complete and cohesive toolbox that works\nwith nearly any TQA situation. TableQAEval is\na benchmark for LLM-based TQA that is the first\nmulti-type long-context benchmark. Tools for data\nvisualization that enable many table formats, multi-\nmodal data, and easier data interaction. The dis-\ncipline of NLP has benefited greatly from Table-\nQAKit, especially in the domain of TQA. It incor-\nporates LLMs in a new approach, gives a uniform\nplatform to handle different TQA tasks, and offers\nhelpful benchmarks to assess LLM capabilities in\nthis situation. Because of its open-source nature\nand intuitive design, the toolkit is a useful and eas-\nily available resource for scholars and practitioners\nworking in the subject.\nHROT presents a novel approach for improv-\ning Table-Text Hybrid QA by leveraging a hybrid\nprompt strategy, which can be categorized as a\nunique methodology aimed at a specific aspect of\nTQA involving hybrid data.\nAdvanced techniques for table-text hybrid ques-\ntion answering (TextTableQA) are investigated by\nLuo et al. (2023). Its main goal is to improve\nthe way LLMs handle complex, numerical prob-\nlems that come from combining text and table data.\nThe problem of providing numerical answers over\nhybrid data consisting of tables and text (Text-\nTableQA) is discussed in the study. Key research\nareas in this subject include the recent advent of\nLLMs and their In-Context Learning and Chain-\nof-Thought (CoT) prompting. The study presents\nHybrid Prompt Strategy and Retrieval of Thought\n(HROT) for TextTableQA, a novel prompting tech-\nnique. By using this technique, the model's per-\nformance with hybrid data is greatly enhanced by\nencouraging the model to adopt retrieval thinking.\nIn specifically for financial datasets, the research\nintroduces a unique HRoT approach that greatly\nimproves the performance of LLMs in responding"}, {"title": "Accessibility and User-Friendly\nApproaches to TQA", "content": "TableQuery focuses on making TQA more acces-\nsible and efficient for non-technical users, empha-\nsizing ease of use and the ability to handle large\ndatasets without requiring extensive training or\ntechnical expertise.\nThis paper's primary objective is to make it pos-\nsible for users\u2014particularly non-technical users\nto utilize natural language queries to evaluate big\ndatasets in tabular form. This strategy aims to solve\nthe shortcomings of current approaches that can't\neffectively handle enormous datasets or require a\nlot of training. TableQuery (Abraham et al., 2022)\ntransforms natural language inquiries into struc-\ntured queries by utilizing deep learning models that\nhave been pre-trained for answering questions on\nfree text. There's no need to fit the whole dataset\ninto memory or serialize databases repeatedly when\nusing these searches on databases or spreadsheets.\nTableQuery is a big step in improving the efficiency\nand accessibility of data querying, especially for\nnon-technical users and huge datasets. Its perfor-\nmance is dependent on the capabilities of current\nquestion answering models, though, and a thorough\nevaluation of its ability to handle a wider range of\nquestions and data types is still pending."}, {"title": "Proposed Solution", "content": "There are challenges when querying big databases\nwith natural language. Large-scale, complicated\nrepositories of data with a vast range of values and\nproperties are characteristics of real-world systems.\nFor instance, \u201cWhat would be the average profit\nfrom selling OneView to Allianz in ANZ\u201d? from\nB2B Sales data 4. Under the assumption that a typ-\nical user wouldn't explicitly mention the precise\ncolumn names, values, origins, the LLM in this\nscenario must determine that ANZ is a subregion,\nOneView is a product, and Allianz is a customer.\nOne popular strategy, which ChatGPT ADA 5 also\ndoes, is to supply the top 5 rows of the dataset, from\nwhich the LLM builds its hypothesis and creates\nthe DB query, to aid it in understanding the struc-\nture of the data. Even though big models like the\ngpt-4 can establish the correct associations when\ngiven descriptive and unambiguous column names,\nthey frequently falter when faced with high cardi-\nnal characteristics, cryptic names, or columns that\ninclude lots of data. However, it is quite difficult\nfor tiny and mid-sized models, such as the 7B and\n13B parameters and their quantized equivalents, to\ndecipher these assumptions and provide a logically\nand syntactically sound response.\nSuch complexity frequently makes it difficult for\nthe LLMs to correctly deduce from a user's query\nwhat columns or values are intended, particularly\nwhen the query is written in an ambiguous or non-\nspecific manner which is general in real In these\nsituations, the models attempt to hide the wide pos-\nsibilities by utilizing methods such as contains()\nor LIKE which leads to syntactically valid but logi-\ncally erroneous queries. Similarly, if an attribute or\nvalue is spelled incorrectly, the model will generate\na query based on the misspelled value and won't\nreceive the intended answer because the query was\nlogically incorrectly generated. Due to process-\ning power restrictions and the intrinsic structure\nof these models, it is not feasible to transmit full\nschema metadata or whole datasets to the LLM,\nwhich exacerbates the limitations even more.\nAn innovative strategy that can bridge the gap\nbetween the rigid, structured world of database sys-\ntems and LLMs' sensitive comprehension of natu-\nral language is needed to address these issues. To\nreduce guesswork and create database queries that\ncontain exact values as present in the data, regard-\nless of how the user mentioned them in the question,\nour framework introduces a solution through the\nimplementation of Full-Text Search (FTS) on the\nmetadata of the input table. By using the FTS, a\nDynamic Schema of the table is created around\nattributes and values involved in the question to\nrefine even loosely defined user questions.\nBased on the same index that powers the FTS,\nwe propose a Contextual Auto-complete function\nto further improve user's ability to explore and\ninteract with big and complicated datasets. This\nhelps the user craft precise queries as intended by\nproviding real-time dropdown suggestions as they\ntype the question. It also lessens the need to fre-\nquently consult the data to find precise attribute\nand/or value names, which saves time and fric-"}, {"title": "Implementation", "content": "RoundTable framework makes use of open-sourced\nLarge Language Models (LLMs) from Hugging\nFace 6 and LM Studio 7 for managing model oper-\nations, which forms the base of our experimental\nsetup. Evaluation of the framework is performed\non datasets from various sectors like supply chain,\nphysical properties, and sales to facilitate a com-\nprehensive data analysis. Initially, the system pro-\ncesses the data by identifying and categorizing at-\ntributes, assigning data types, and isolating unique\nelements. This preprocessing helps in creating an\nefficient index for better data retrieval. When users\ninteract with the system, an autocomplete feature\nsuggests possible attributes and values as they type\ntheir queries. The process begins with the extrac-\ntion of key words from the user input. This leads to\nmetadata pruning to refine the search. The system\nidentifies relevant attributes directly and indirectly,\nemploying advanced detection methods. This gen-\nerates a customized prompt used by the LLM to\nproduce the final database queries. This method\nensures that the system is user-friendly and capa-\nble of handling complex queries effectively across\ndifferent domains."}, {"title": "Attribute Extraction", "content": "The initial phase of our approach starts with the\ncareful determination of the structure of the input\ntable, T. This is an important phase that is nec-\nessary to understand the underlying schema and"}, {"title": "Categorization of Attributes", "content": "Once the attribute extraction phase is completed,\nwe employ a particular filtering procedure that\nwe designate as 'C', which is meant to discern\ncategorical properties. In this stage, the func-\ntion FILTER_CATEGORICAL_ATTRIBUTES is essen-\ntial. It carefully goes over attribute set 'A', using a\ncriterion based on 'D' to extract attributes that fall\nwithin the 'Categorical' heading. The classifica-\ntion of qualities as \u2018Categorical' suggests that they\nare made up of distinct classes/categories, which\nmakes them extremely useful for querying. This\nclassification is especially important since categor-\nical data makes information easier to organize and\nretrieve, which speeds up the analytical process."}, {"title": "Value Indexing", "content": "Determining the structure of categorical data is\na significant step once categorical features have\nbeen identified and isolated. For every categorical\ncolumn, the function EXTRACT_UNIQUE_VALUES is\nspecifically engineered to do a thorough analysis\nand extraction of a single set of values, represented\nby the letter 'M'. These distinct variables become\nessential building blocks that are directly connected\nto query search phrases while building the search\nindex. To precisely target and retrieve data based\non certain query criteria, it is imperative that these\nunique values be extracted and identified. This\nprocess not only enhances the efficiency of data\nquerying but also significantly contributes to the\nrobustness of the search index by ensuring that\neach query term is accurately mapped to distinct,\nrelevant data points within the categorical dataset."}, {"title": "Synonym Generation", "content": "Our technique considers the wide range of queries\nmade by users and makes creative use of a Large\nLanguage Model (LLM) by implementing the\nGENERATE_SYNONYMS function. The purpose of this\nfunction is to increase the search power of the prop-\nerties in our dataset. It functions by using each\nattribute name ('a') in the set 'A' to produce a\ncarefully selected set of synonyms ('S') that are\nespecially meant to make sense in a commercial\nsetting. This improvement is purposefully made to\nstrengthen the index's robustness so that it can effi-\nciently handle a broad variety of linguistic phrases\nthat are utilized in queries. The user experience\nand overall accuracy of search results are improved\nwhen business-relevant synonyms are added to the\nsearch index. This makes a major improvement to\nthe system's capacity to comprehend and respond\nto the different language users may use."}, {"title": "Index Construction", "content": "An essential component in creating the in-\nverse index, represented by the letter I, is the\nBUILD_INVERSE_INDEX function. This function\ncombines several parts, including attribute names,\ndata types, synonyms, and unique values. The\nfundamental framework that greatly improves our\nquery processing capabilities is this integration.\nIt serves as the framework for quick and precise\ndata retrieval procedures. Furthermore, this index\nis essential for enabling the dynamic creation of\nschemas, which permits adaptability to changing\ndata needs. This mechanism plays an important\nrole in enhancing the efficacy and efficiency of\ndata processing in our system."}, {"title": "Pre-Processing User Query", "content": "Process action starts in the first stage when the\nEXTRACT_KEYWORDS function is activated. This\nessential feature is crafted to handle the pre-\nprocessing of the user's query, query Q. Its main\ngoal is to sort the query to locate and separate the\nessential elements, or important keywords K. The\nfunction uses a variety of sophisticated NLP ap-\nproaches to do this. These advanced techniques\nplay a critical role in honing the question and mak-\ning sure it is set up for success in the next steps.\nThis pre-processing stage is essential for improv-\ning keyword matching efficiency and laying the\ngroundwork for precise information retrieval or\noutcomes based on the extracted keywords. This\nmethodological approach emphasizes how impor-\ntant it is to identify keywords precisely to optimize\nthe system's search and analysis operations."}, {"title": "Keyword Matching and Schema\nGeneration", "content": "To find terms relevant to the user's search request,\nthe SEARCH_INDEX function carefully searches\nthrough an index as the first stage in query pro-\ncessing. This is an important stage because it es-\ntablishes the groundwork for later processes by de-\ntecting both obvious and subtle characteristics that\nmight be important to the question. After that, the\nCREATE_PROMPT function makes use of the recog-\nnized attributes (direct, which are directly related to\nthe query, and indirect, which are relevant through\nassociative relationships). In this case, a dynamic\nschema (designated as D) is created. This schema\nis especially designed to adjust to the needs of ev-\nery query; it is not static. This design guarantees\nthat the data structure is both flexible and immedi-\nately relevant to the user's query by considering the\nvaried qualities that were previously determined.\nThis improves the accuracy and efficiency of the\ndata retrieval process."}, {"title": "Integrating with Static Prompt", "content": "Using the FORMULATE_QUERY function, a dynamic\nschema (designated as D) is integrated with a static\nprompt (specified as P) beforehand. The direction\nof a large language model's capabilities is greatly\ninfluenced by this integration. It guarantees that\nthe model produces semantically valid and syntacti-\ncally correct database queries, which adhere to the\nconventions of database query language structure.\nSemantic correctness is the ability of the query to\nunderstand the meaning and purpose of the user's\nrequest. Through the combination of the dynamic\nschema's flexibility and the static prompt's con-\nsistency, this method efficiently makes use of the"}, {"title": "LLM-Based Query Formation", "content": "A major use of LLMs in bridging the gap be-\ntween human language and machine executable\ncode is demonstrated by the GENERATE_DB_QUERY\nfunction. It leverages the superior capabilities of\nLLMs to read and convert natural language into a\nstructured database query (DQ) by receiving input\nin the form of a dynamic schema together with a\nstatic prompt. Understanding the subtleties of the\ninput, mapping it against the designated schema,\nand then creating a query that precisely reflects the\noriginal request in a syntax that can be immedi-\nately performed by a database management sys-\ntem comprise this translation process. This demon-\nstrates not only how flexible LLMs are in handling\nand transforming unstructured data into structured\nform, but also how they may improve the usabil-\nity and accessibility of database interactions for\npeople who lack extensive technical knowledge of\ndatabase query languages."}, {"title": "Results", "content": ""}, {"title": "Dataset", "content": "In order to evaluate the performance of the frame-\nwork we required a dataset of natural language\nquantitative questions that are representative of the\nreal world business scenarios. Most of the datasets\ncurrently available that we know of are small ta-\nbles mostly extracted from web pages. In order to\novercome these issues we have created a dataset\nof 1500 quantitative questions from a variety of\ndomains including supply chain 8, B2B sales 9,\nB2C sales 10 and physical properties 11. Our aim is\nto cover a broad set of scenarios covering various\nways users could possibly interrogate the data to\nextract insights. To achieve the same we divide the\nquestions into 2 broad categories."}, {"title": "Difficulty Level", "content": "(a) Easy\n(b) Medium\n(c) Hard"}, {"title": "Category", "content": "(a) Generic\n(b) Value-Based"}, {"title": "Difficulty Level", "content": "Difficulty Level divides question into 3 sub-\ncategories of varying complexity of questions while\nCategory further divides them into 2 categories."}, {"title": "Generic:", "content": "\u2022 Generic: These are questions which might\ninclude any specific column name but not any\nspecific values like \"What is the size of the\ntable?\" or \"What is the total revenue?\""}, {"title": "Value-Based:", "content": "\u2022 Value-Based: These questions contain spe-\ncific values often not in full or properly spelt\nlike \"What was the total revenue from Dave\nof Costco?\""}, {"title": "Evaluation", "content": "The above-mentioned datasets have been evalu-\nated at different levels and categories of Questions\nand the graph below depicts the performance of\nRoundTable.\nFigure 2 presents a comparison of Generic\nquestions across four datasets with and without\nRoundTable framework. Their is a slightly perfor-\nmance when the framework is used across datasets\nexcept for B2B Sales dataset.\nFigure 3 presents a comparison of Value-Based\nquestions across four datasets with and without\nRoundTable framework. In these questions their is\na significant increase in accuracy observed when\nusing the framework. This is the area where the\nuse of framework makes a significant difference\ncompared to using vanilla LLMs.\nOverall, the Framework's adoption has led to a\nsubstantial increase in accuracy across the board."}, {"title": "Conclusion", "content": "In conclusion, the paper presents a transformative\napproach to Table Question Answering (TQA) by\nintroducing a Full-Text Search (FTS)-enhanced dy-\nnamic schema and a Contextual autocomplete fea-\nture. These innovations address the current limita-\ntions in query precision and interaction with com-\nplex datasets. By narrowing the language model's\nsearch space and offering real-time query sugges-\ntions, the framework ensures a higher degree of\nquery accuracy and user efficiency. The compre-\nhensive evaluation across diverse datasets confirms\nthe framework's effectiveness, showcasing signif-\nicant improvements in query precision compared\nto traditional methods. This marks a considerable\nstride in making complex data querying accessi-\nble to users irrespective of their technical expertise,\nfacilitating a more intuitive and efficient data in-\nterrogation process. The framework's ability to\nadapt to various domains and its scalability sig-\nnals a promising direction for future research and\napplications in natural language database querying."}]}