{"title": "Gender Bias in LLM-generated Interview Responses", "authors": ["Haein Kong", "Sangyub Lee", "Yongsu Ahn", "Yunho Maeng"], "abstract": "LLMs have emerged as a promising tool for assisting individuals in diverse text-generation tasks, including job-related texts. However, LLM-generated answers have been increasingly found to exhibit gender bias. This study evaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit of LLM-generated interview responses across models, question types, and jobs, and their alignment with two gender stereotypes. Our findings reveal that gender bias is consistent, and closely aligned with gender stereotypes and the dominance of jobs. Overall, this study contributes to the systematic examination of gender bias in LLM-generated interview responses, highlighting the need for a mindful approach to mitigate such biases in related applications.", "sections": [{"title": "Introduction", "content": "LLMs have increasingly demonstrated the ability to assist in effectively generating language for individuals' livelihood and critical events [6, 22]. One area of showing its potential lies in helping people better represent themselves for their career verbally or in written languages [23, 11]. For instance, job applicants are increasingly leveraging LLMs to generate personal statements and anticipated interview responses. Recent advances in LLM-powered applications, such as in-store tools and specialized writing capabilities for job-related essays and interview preparations\u00b3, have further boosted the use of LLMs in job-related language generation tasks.\nHowever, researchers have found risks of gender bias in LLM-generated job-related languages. For example, LLMs tend to produce different recommendation letters for female and male applicants [7, 20], often reinforcing traditional gender stereotypes. These align with the well-known dichotomy of female communal versus male agentic stereotypes [20, 14, 17], where men are perceived as more assertive and task-focused, while women are seen as more polite and person-oriented. Other studies show that LLMs exhibit demographic bias when assigned to a job/position matching or hiring decision by disfavoring certain demographics (e.g., Hispanic male) [1] or recommending stereotypical roles to job seekers (e.g., drivers to men, secretarial roles to women) [16].\nIn our study, we investigate how gender bias penetrates the task of LLM-assisted interview preparation, an underexplored but crucial application of language generation in job-related tasks. Our study conducts a multifaceted audit of LLM-generated interview responses across various dimensions, in-"}, {"title": "Related Work", "content": "Gender differences and biases in human assessment and interview process. Research has consistently shown that language in professional documents reflects gender differences as well as its alignment with gender stereotypes. For instance, male applicants in personal statements often express a stronger sense of acceptance and community compared to female applicants [5]. These differences often align with two well-known gender stereotypes \u2013 male agentic and female communal stereotypes [20, 14, 17]. [2] found that males use more words related to rewards than females. In the analysis of recommendation letters [9, 19], female writers were found to highlight more communal words such as clout, social process, and personal concerns than male writers. Given that communal terms in recommendation letters are negatively associated with hiring decisions [9], such stereotypical points of view can lead to implicit biases and discrimination [15].\nJob interviews, another critical stage in job applications, were also shown to reflect such biases. Because of its multimodal and interactional nature, various linguistic cues and factors-such as applicants' accents or names\u2013serve as triggers for interviewers' implicit gender bias [12]. In simulated mock interviews [8], these stereotypes were entrenched in both female applicants and male interviewers and associated with hiring decisions. This evidence highlights how LLMs trained on historical data can internalize and reproduce gender bias in the generation of language related to job applications.\nGender bias in hiring and job application in large language models. Recent studies have found risks of gender bias in LLMs in job-related languages [20, 7]. For example, language in reference letters [20] was more likely to include male-stereotypical traits (e.g., leadership, agentic) and female-stereotypical traits (e.g., personal, communal) for respective gender. On the other hand, [7] found that this alignment depends on the prompt types, some of which exhibit gender stereotypes, but others are counter-stereotyped. In addition, LLMs serving as job recommenders or hiring decision-makers were also reflective of their implicit gender bias. In Salinas et al. [16], LLMs were more likely to recommend drivers to men over women, and secretarial roles to women over men. A previous study [1] also showed LLMs disfavor Hispanic male applications in hiring decisions, leading to the highest rejection emails compared to other demographic groups. Our study aims to identify the gender bias in LLMs' interview answer generation, an underexplored but critical application of gender bias due to its implication in high-stakes decisions. We demonstrate LLMs' behavior of exhibiting gender bias in linguistic and psychological traits across different models, questions, and jobs."}, {"title": "Experiments", "content": "Experimental Setup. This experiment aims to examine the differences in LLMs' responses according to the applicants' gender and targeted jobs. Information about applicants' gender was given to the LLMs using the names and pronouns in the prompts. The details of our prompts are described in Appendix A. The 70 most popular names for males and females for a century in the United States [18] were used to construct the prompts. The 60 jobs were selected from the Winobias [21] and Winogender datasets [13] (see details in Appendix B). The prompts that provide the context of job"}, {"title": "Results", "content": "Gender bias at model and question level\nOverall, we observe that LLM-generated interview responses exhibit a clear distinction in LIWC dimensions as biased towards either males or females at both model- and model x question level (highlighted as purple and orange respectively in Figure 1). For example, in the linguistic dimensions, the responses for male applicants tend to use more number of words per sentence and overall (Word count, Words per sentence), while the ones for female applicants are highly expressive in social process and behaviors, refer to people (Total/Personal/Impersonal pronouns), with more use of process-oriented languages (Common verbs, Adverbs). Gender differences are also noticeable in psychological aspects, such as males being willing to take risks or feeling to achieve and fulfill, in comparison to females revealing their emotions and tones. Regarding internal states (States category), females relate to expressing desires or necessity (Need, Want), while males speak to their action of searching and obtaining, or feeling of satisfaction (Fulfill, Acquire).\nGender biases are consistent over models and question types. It is also noticeable that these dimensions are consistent across models and question types rather than being distinct. As noticed in"}, {"title": "Alignment with female communal and male agentic stereotype.", "content": "We also examine how gender biases are aligned with two known gender stereotypes \u2013 female communal and male agentic stereotypes. To investigate the alignment, we identify 21 LIWC dimensions as relevant to either stereotype (highlighted as orange or purple in LIWC dimension names in Figure 1). For example, male agentic stereotype in relation to their assertive, task-focused, and objective properties was reflected in high scores in Power, Achievement, and Analytical thinking, while female communal stereotype was highlighted as person-oriented or kind (Personal pronouns, Social referent, Politeness) or expressive in their emotions (Emotion). Overall, our findings reveal that gender biases in LLMs strongly align with existing gender stereotypes, highlighting how these entrenched biases are reflected in the models' perspectives."}, {"title": "Impact of gender bias over different types of jobs", "content": "We further examine how LLMs exhibit gender bias at different jobs and job categories. In our analysis, we investigate 60 jobs consisting of 30 male- and female-dominant jobs."}, {"title": "Gender biases at job categories closely conform to gender stereotypes.", "content": "We also examine how the degree of bias relates to the extent to which it conforms to known gender stereotypes. As illustrated in Figure 3, conformity positively correlates with bias ratio over job categories. We first compute the job-wise conformity as the ratio of gender-biased pairs within each job that fall into stereotype-associated LIWC dimensions for each gender and take the category-wise mean ratio.\nSome job categories with higher bias ratio, such as Food & Hospitality, Administrative & Support, and Law & Public Safety also obtain higher conformity. This indicates that as gender biases increase, they are likely to reinforce and systematically perpetuate the existing gender stereotypes."}, {"title": "Discussion & Social Impacts Statements", "content": "This study examines the presence of gender bias in LLMs-generated interview responses. The findings indicate that gender bias consistently manifests in the generation of applicants' self-representation over models and questions, which aligned with two known gender stereotypes. Furthermore, gender stereotypes were found to conform to the gender dominance of jobs, revealing its vulnerability to injecting stereotypical persona into the language generation. This implies that"}]}