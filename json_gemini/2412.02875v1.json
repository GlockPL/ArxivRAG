{"title": "Out-of-Distribution Detection for Neurosymbolic Autonomous Cyber Agents", "authors": ["Ankita Samaddar", "Nicholas Potteiger", "Xenofon Koutsoukos"], "abstract": "Autonomous agents for cyber applications take advantage of modern defense techniques by adopting intelligent agents with conventional and learning-enabled components. These intelligent agents are trained via reinforcement learning (RL) algorithms, and can learn, adapt to, reason about and deploy security rules to defend networked computer systems while maintaining critical operational workflows. However, the knowledge available during training about the state of the operational network and its environment may be limited. The agents should be trustworthy so that they can reliably detect situations they cannot handle, and hand them over to cyber experts. In this work, we develop an out-of-distribution (OOD) Monitoring algorithm that uses a Probabilistic Neural Network (PNN) to detect anomalous or OOD situations of RL-based agents with discrete states and discrete actions. To demonstrate the effectiveness of the proposed approach, we integrate the OOD monitoring algorithm with a neurosymbolic autonomous cyber agent that uses behavior trees with learning-enabled components. We evaluate the proposed approach in a simulated cyber environment under different adversarial strategies. Experimental results over a large number of episodes illustrate the overall efficiency of our proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous agents for cyber applications require to learn and deploy security rules to defend cyber-attacks without any human intervention. Defending a network from cyber-attacks needs constant monitoring of the system and selecting the appropriate actions whenever a security breach is detected while still maintaining the critical operational workflows.\nExisting security standards often fall short in designing these cyber-defense agents against sophisticated adversarial attacks. Hence, a combination of security standards along with learning enabled components (LECs) that can learn, detect and defend the system from attacks are needed. These LECs are typically function approximators with a reinforcement learning (RL) policy, so that they can take optimal actions and effectively mitigate dynamic complex attacks. However, uncertainties pose a significant challenge in characterizing the trustworthiness of these autonomous agents. These uncertainties may arise due to limited knowledge available to the autonomous agents about the runtime behavior of the operational system and environment at the time of designing or training these agents. The consequences can propagate deep into the system and can impact system behaviors at all levels. Thus, anomaly or out-of-distribution (OOD) detection methods need to be incorporated to identify information that is nonconformal with the environment used in training.\nIn a cyber environment, a trustworthy autonomous agent should not only generate optimal actions in known situations that it understands, but also reliably detect situations that are nonconformal, reject them, and pass them over to cyber experts. For instance, an autonomous cyber-defense agent trained against different types of denial-of-service (DoS) attacks such as blocking the traffic or blocking the IP address of a specific host, can detect and handle such situations promptly. However, if a new type of attack such as an unauthorized access to the network server due to authentication breaches, emerges into the system, then the autonomous cyber-defense agent should detect such situations as anomalous or nonconformal so that they can be handled by cyber experts. Therefore, a safety assurance method needs to be associated with an autonomous agent to make it trustworthy under every situation. Anomaly or OOD detection is an established area of research in robotic systems [1], cyber-physical systems [2], [3], etc. However, OOD detection to defend cyber-attacks in autonomous networks is not well explored in the literature. With this objective into consideration, we aim to design an OOD Monitoring algorithm to make autonomous cyber-defense agents trustworthy. Apart from monitoring cyber-defense agents for autonomous networks, our proposed algorithm can also be integrated with any RL-based agent with discrete states and discrete actions.\nAlthough different RL policies are used in designing cyber-defense agents to defend autonomous networks, they fail to scale and optimize these systems as the networks grow and become more complex over time. To overcome these challenges, in our prior work, we proposed a neurosymbolic model representation of an autonomous cyber-defense agent using behavior trees (BTs) [4] with LECs, more specifically known as the Evolving Behavior Trees (EBTs) [5]. The EBT structures are modular in nature with capabilities to adapt to multiple dynamic attack situations. They can capture an explicit hierarchy of subtasks and control flows, and can deploy LECs to execute specific subtasks. Their generalizable structures make them deployable to a real system as well as in simulation. However, an online monitoring technique needs to be incorporated with the EBT-based autonomous cyber-"}, {"title": "II. RELATED WORK", "content": "Traditional security measures are not sufficient to defend au-tonomous networks from sophisticated cyber-attacks. As a result, nowadays, different RL methods are deployed to develop more advanced and interpretable learning enabled defense policies in autonomous networks. CybORG serves as a popular cyber security research environment for training and development of different RL-based autonomous agents [8]. Some RL-based agents utilize a goal-conditioned hierarchical RL (HRL) to validate trained defense strategies in CybORG [9], [10], whereas, others use an ensemble approach aggregating policy outputs [11]. For emulating cyber-attacks and defense scenarios, Markham et al. developed a novel tool called FAR-LAND that focuses on realistic cyber-defense environments and curriculum learning for cyber-defense agents [11].\nAn emerging area of research in designing autonomous cyber-defense is to use Neurosymbolic AI that combines pattern recognition capabilities of neural networks along with explicit reasoning of symbolic systems [12]. An effective way to design these neurosymbolic autonomous agents is to use behavior trees (BTs) [4]. RL or HRL techniques are used to learn, jointly optimize and generate policies to capture complex BT behaviors [13]\u2013[15]. Our prior work proposed an approach that uses genetic programming to construct EBTS with LECs that analyzes the system behavior and apply appropriate mitigation strategies against adversarial attacks to ensure autonomous cyber-defense in enterprise networks [5]. However, none of these works can ensure safety of the system at runtime by detecting OOD behavior of the autonomous cyber-defense agents.\nOOD detection is well studied in the literature for safety critical applications such as autonomous vehicles [16], robotics [1], etc. Cai et al. proposed an OOD detection approach where they used variational autoencoders and deep support vector data description to learn the system and use them in real-time to compute the nonconformity of new inputs relative to the training set in advanced emergency braking system and a self-driving end-to-end controller [3]. Ramakr-ishna et al. designed a \u1e9e-variational autoencoder detector with partially disentangled latent space to detect OOD scenarios with variations in the image features [2]. Farid et al. presented a Probably Approximately Correct (PAC) Bayes framework to train policies for a robotic environment with guaranteed bounds on performance on the training data distribution and detects OOD behavior of the robot by capturing the viola-tion of the performance bound on the test environment [1]. Averly et al. presented a unifying framework to detect OOD scenarios caused by both semantic and covariate shifts in uncontrolled environments for a variety of models [17]. Yang et al. presented a full-spectrum OOD detection model that uses a simple feature-based semantics score function to account for semantic shifts and become tolerant to covariate shifts in image data [18]. However, none of these works focus on OOD detection scenarios for RL agent based autonomous systems.\nA few works on different OOD detection approaches for RL-based agents exist in the literature [19]\u2013[21]. However, all of these works are applicable for continuous state space. The autonomous cyber-defense agent considered in our work consists of discrete and partially observable states and discrete actions. Hence, existing approaches for OOD detection for RL agents are not directly applicable to our system. In this work, we develop an OOD Monitoring algorithm that can detect OOD scenarios in autonomous networks to assure safety of our system at runtime."}, {"title": "III. AUTONOMOUS AGENTS FOR CYBER-DEFENSE", "content": "In our prior work, we developed a robust autonomous cyber-defense agent that interacts with the environment and uses cyber-agent actions against dynamic cyber-attacks [5]. We evaluated our agent in CybORG, a complex network simula-tion environment that abstracts real world scenarios [22]. We considered the network scenario presented in CAGE Challenge Scenario 2 [7]. The network comprises three subnets:\n1) Subnet 1 with five non-critical user hosts.\n2) Subnet 2 with three enterprise servers that support the activities of the hosts in Subnet 1 and a host that acts as the defender.\n3) Subnet 3 with three operational hosts and a critical operational server that is responsible to ensure that the network is functioning properly.\nCybORG interface can be used to construct and evaluate the attacker (red agent) and the defender (blue agent) using LECs. Each scenario run in CybORG consists of a fixed number of timesteps over a fixed period of time. In every timestep, the red and the blue agents each chooses and executes an action from a set of available actions. The red agent starts each scenario run with an initial access to one of the user machines in Subnet 1."}, {"title": "IV. EVOLVING BEHAVIOR TREE BASED AUTONOMOUS CYBER-DEFENSE AGENT.", "content": "Neurosymbolic AI can be leveraged in cyber-security to learn the system behavior holistically to mitigate sophisticated ad-versarial attacks. Given a goal or specification, a symbolic structure is used as a model that interacts with the environment and selects appropriate actions against adversarial red agents. We use behavior trees (BTs) as the symbolic structure in the design of our agent because of their capabilities to integrate LECs that allow us to learn and reason about cyber-defense control at a high level and adapt to environmental shifts. Their modular structures allow us to integrate new capabilities into the system. Moreover, they are generalizable, that allow us to map them to both abstract and realistic environments.\nIn our prior work, we presented a neurosymbolic approach using Evolving Behavior Trees (EBTs) to develop a ro-bust autonomous cyber-defense agent that interacts with the environment and uses cyber-agent actions against dynamic cyber-attacks [5]. We abstracted our EBT-based cyber-defense agent from a pursuit evasion game environment using genetic programming and evaluated our agent in CybORG CAGE Challenge Scenario 2 [5]. In the execution of a BT, each timestep is called a tick. A BT starts executing from the root node and follows a Depth-First Traversal. On completing execution of a behavior in a BT node, the child returns a status of Running if its execution is underway, Success if it has achieved its goal, and Failure otherwise. The behaviors in a BT can be classified into two groups Control behaviors and Execution behaviors. Control behaviors are the internal behaviors in a BT that control the logical flow of switching between the behaviors. Execution behaviors are the leaf behaviors in a BT that execute specific action in the environment. Control behaviors in a BT can be either Sequence or Fallback. Sequence executes a set of child behaviors sequentially until all chil-dren return Success, otherwise returns Failure. Fallback executes the leaf behaviors until one child returns Success, otherwise returns Failure. Execution behaviors in a BT can be a Condition or an Action behavior, the return status of which is dependent on the intended logical condition or user-defined functionality. Each tick of the BT comprises execution of the entire BT from the root in a depth-first manner till the status from the leaf behaviors are returned, after which it propagates back upto the root recursively by updating the status of the parent control behaviors.\nFrom Fig. 2, we define five Action behaviors in our autonomous cyber-defense agent.\n1) SelectStrategy! selects a defense strategy depending on the adversarial (red agent) movement."}, {"title": "V. OUT-OF-DISTRIBUTION DETECTION", "content": "Although our EBT-based autonomous cyber-defense agents are robust against dynamic cyber-attacks, uncertainties introduce a significant challenge for characterizing trustworthiness of these agents. These uncertainties arise from knowledge gaps about the runtime state of the system and the environment at the time of design and training these agents. As a consequence, the system behavior may get impacted at all levels that may lead the system to unsafe states. Thus, the main objective of this work is to design a trustworthy autonomous agent that can reliably detect anomaly or out-of-distribution situations and hand them over to cyber-experts to assure safety of the system at runtime.\nOur system can be modeled as a decision-making system with RL agents that sequentially interacts with the environment by executing actions. At each timestep, our system transitions from one state to another based on the red agent (adversarial) and blue agent (defender) actions. However, the red agent actions are not observable. Thus, our system can be formally represented by a discrete-time Partially Observable Markov Decision Process (POMDP), M := (S, A, T, R, \u03bc0) [23]. S denotes the set of states or observations which are discrete and partially observable, A denotes the set of defender (blue agent) actions which are discrete, T denotes the conditional transition probabilities, R: S\u00d7A\u00d7S \u2192 R denotes the reward function, and \u03bc0 : (s0, a0), s0 \u2208 S, a0 \u2208 A, denotes the initial state and action. At each timestep t \u2265 1, the defender (blue agent) takes an action at\u22121 \u2208 A which causes the system to transition from st\u22121 to st with probability T(st|st\u22121, at\u22121) and gets a reward rt\u22121. The objective of the blue agent is to select actions at each timestep so that the cumulative rewards maximize over time, i.e., \u2211t\u2192\u221e t=1 rt\u22121.\nWe define Transition Probability Threshold to quantify out-of-distribution situations of our system.\nDefinition 1. Given a neurosymbolic cyber-agent trained with a policy \u03c0, a state transition at timestep t \u2212 1, denoted by (st-1, at-1) \u2192 st, is considered to be an out-of-distribution (OOD) transition based on policy \u03c0 if the probability of occurrence of this transition in the training data is less than threshold p, i.e., Pr((st\u22121, at\u22121) \u2192 st) < p. We refer p as the Transition Probability Threshold.\nProblem Statement : Given a network consisting of hosts, enterprise servers and operational servers (as shown in Fig. 1) and a neurosymbolic cyber-agent trained with a policy \u03c0, our objective is to develop a safety assurance algorithm to detect shifts from the distribution used for training.\nIn this work, we specifically address two key questions."}, {"title": "B. OOD Monitoring Algorithm", "content": "We develop an OOD Monitoring algorithm that executes at every timestep to detect any deviations of the current observation from the one used for training the autonomous agent. Haider et al. presented a model-based OOD detec-tion framework for RL agents using probabilistic dynamics model [20]. Unlike [20], the states in our system are discrete and partially observable. Hence, we cannot directly apply their framework to our system. We propose to use a Probabilistic Neural Network (PNN) to learn the dynamics of our system that is characterized by non-deterministic transitions of the partially observable states [6].\nOur OOD Monitoring algorithm consists of three phases.\n\u2022 An Data Generation phase\n\u2022 An Training phase\n\u2022 An OOD Monitoring phase\nAlgorithm 1 shows the main steps of our proposed approach.\nData Generation Phase: To learn the dynamics of our system, we need to learn the dynamic function that characterize the transition probabilities of our system. In order to achieve this, we need to collect data, i.e., states, actions and their transitions (st-1, at-1) \u2192 st, by interacting with our system, and then generate a PNN with the collected data.\nGiven a trained blue agent control policy \u03c0 for a fixed red agent strategy, we collect transitions (st\u22121, at\u22121) \u2192 st for T timesteps, (t is very large), over multiple episodes (say N) and generate the training data Dtrain.\nTraining Phase: Let us assume that fo represent the discrete dynamic function that characterize our decision making system. Since our system is non-deterministic, it exhibits different behaviors on different runs even with the same inputs. Thus, fe can be formally represented as a mapping of the previous state and action to a set of possible k current states, where k is any positive number, i.e., fo(st\u22121, at-1) = {s1, s2,..., sk}. Each of these si is associated with some probability conditioned on (st-1, at-1) and generated from the observed samples in the training data.\nTo learn fo, we develop a PNN where the input layer is of size 1, the pattern layer is of size equal to the size of the training data, i.e., N\u00d7T, and the output layer is of size m (say) where m is equal to the number of distinct observed states in the training data. Unlike the generic structure of a PNN which consists of four layers [6], our PNN model does not have the fourth layer, i.e., the decision layer. Instead, the summation layer serves as our output layer. For each input, (st-1, at-1), at the input layer, the PNN matches the input with all the entries in the pattern layer. The PNN activates only those entries (si)"}, {"title": "VI. INTEGRATION OF OOD MONITORING IN THE EBT", "content": "We need to integrate the safe monitoring behavior of our system in our existing BT so that the system can be monitored at runtime over every tick. To integrate the safe monitoring behavior in the existing BT (shown in Fig. 2), we update the hierarchical structure of the BT and add two new Condition behaviors and a new Action behavior in the updated BT. The new behavior node, ID?, stores the PNN for a specific control policy. At each timestep t, this node feeds the previous state and action, (st\u22121, at-1), to the PNN to generate a set of predicted"}, {"title": "VII. EXPERIMENTS AND EVALUATION", "content": "We execute our proposed OOD Monitoring algorithm for our EBT-based autonomous cyber-defense agent on the simula-tions of CybORG CAGE Challenge Scenario 2. To demon-"}, {"title": "C. Evaluation with EBT", "content": "Apart from uncertainties caused due to limited knowledge about system dynamics at runtime, an OOD situation can also occur due to change in the behavior of an adversarial red agent. To evaluate such situations at runtime, we conduct experiments by integrating the OOD monitoring behavior with the EBT as described in Section V-B.\nSwitching to a known adversarial strategy: We run experi-ments against RedSwitch strategy where the system instan-tiates a red agent using Meander strategy and switches to B_line strategy after a random number of timesteps. We observe that when a red agent switches to a known strategy, i.e., Meander \u2192 B_line in our case, our system immediately detects the switch as an OOD situation. However, we observe that we need to restore the state of the system to a previous \"safe\" state before switching to a new blue agent policy against the new red agent, i.e., B_line in our case. Thus, we need the GetSafe Action! behavior in the updated BT (refer to Fig. 4).\nTo show the necessity of the GetSafe Action! behav-ior in the updated BT, we conduct two experiments over 1000 episodes, each with 100 timesteps. In one experiment we continue with the BT shown in Fig. 4, i.e., with the GetSafe Action! behavior. In the other experiment, we re-move this behavior from the BT. shows five random episodes executed in these two setups with the same initial conditions and strategy switching happening at the same timestep in both the cases. The plots marked with dashed lines and cross marks denote the plots under strategy switching (Meander \u2192 B_line) with and without GetSafe Action! behavior respectively. We observe that when the red agent switches strategy at timestep t (say), the OOD Monitoring algorithm triggers an OOD situation at the immediate next timestep i.e., at timestep t + 1 in both the cases. In the former setup, i.e., with the GetSafe Action! behavior, the system gets restored to a \"safe\" state at timestep t + 1 and the SelectStrategy! behavior switches the system to the new control policy in the subsequent timestep, i.e., at timestep t + 2. Thereafter, the system behavior switches back to be in distribution with the new control policy. However, in the latter setup, i.e., without the GetSafe Action! behavior, the system evolves to new unseen states and continues with the OOD situations despite the SelectStrategy! behavior switching the system to the new control policy.\nshows the number of OOD transitions per episode under the two experimental setups, one with the GetSafe Action! behavior and the other without the behavior over 1000 episodes each with 100 timesteps. From the figure, we can observe that under \"safe\" switching the number of OOD transitions per episode are significantly small and vary between 0 to 2, thereby restoring the system back to safe state assuring safety.\nSwitching to an unknown adversarial strategy: To evaluate the efficiency of our OOD Monitoring algorithm in detecting an unknown red agent strategy, we set only one control policy into the system against Meander and one PNN trained with"}, {"title": "VIII. CONCLUSION AND FUTURE WORKS", "content": "Neurosymbolic cyber-defense agents are increasingly used in autonomous networks to defend complex cyber-attacks. These agents are typically trained with RL policies. However,"}]}