{"title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "authors": ["Qinzhuo Wu", "Weikai Xu", "Wei Liu", "Tao Tan", "Jianfeng Liu", "Ang Li", "Jian Luan", "Bin Wang", "Shuo Shang"], "abstract": "Recently, mobile AI agents based on VLMs have gained increasing attention. These works typically utilize VLM pre-trained on general-domain data as a foundation, fine-tuning it on instruction-based mobile datasets. However, the proportion of mobile UI in general pre-training data is very low. Moreover, the general pre-training task does not particularly consider the characteristics of mobile UI. Therefore, directly applying such pre-trained models for mobile UI instruction fine-tuning will not yield the desired performance. In this paper, we propose MobileVLM for Chinese UI manipulation. On top of the general pre-training model, two additional pre-training stages are implemented with four specific tasks to enhance both intra- and inter-UI understanding. In addition, a large Chinese mobile UI corpus, named Mobile3M, is built from scratch to compensate for the lack of relevant data. Besides 3 million static UI pages, it also contains directed graph structures formed by real-world UI transition actions. Experimental results show MobileVLM excels on both in-house test sets and public mobile benchmarks, outperforming existing VLMs.", "sections": [{"title": "1 Introduction", "content": "Mobile phones are widely used in daily life, and AI agents on mobile platforms are gaining industry and academic attention (Ding, 2024; Yang et al., 2023). Due to the limitations of purely text-based LLMs in understanding User Interface (UI) elements and page structures (Hong et al., 2023), recently released mobile AI agents are mainly driven by Vision-Language Models (VLM) (Baechler et al., 2024; You et al., 2024; Lee et al., 2023). These works typically use VLM as a base model and then fine-tune it on instruction-based mobile datasets for domain adaptation. As a result, they excel in page navigation and can provide a coarse summary of UI functionality.\nHowever, these VLMs like GPT-4V (OpenAI, 2023), CogVLM (Wang et al., 2023), and Qwen-VL (Bai et al., 2023a) typically utilize large-scale general datasets, such as Laion-5B (Schuhmann et al., 2022), Coyo (Byeon et al., 2022), for pre-training. The proportion of mobile UI pages in these datasets is very low, which results in the overall image characteristics of the datasets being quite different from those of mobile-specific datasets.\nMoreover, the general pre-training task does not particularly consider the characteristics of mobile UI for these VLMs. The general pre-training task, such as image caption and visual question answering, mainly focuses on the overall information of the image, while the mobile UI task demands capturing more fine-grained details such as layout and elements. As a result, these VLMs lack intra-UI information. At the same time, these tasks only focus on the content within an image and ignore the relationship between images. Even for some multi-round navigation tasks in Figure 2, its interaction traces form a chain structure, which still cannot fully cover the inter-UI relationships of massive pages in a real app. Intuitively, all UI pages of an app should form a graph structure. Therefore, these VLMs also lack inter-UI information.\nTo address these issues, as shown in Figure 1, we propose two additional mobile pre-training stages and four specific mobile tasks to enhance both intra- and inter-UI understanding. In stage 1, 3 UI tasks are implemented to enhance the model's granular understanding of intra-UI content. In stage 2, action prediction tasks are introduced to predict actions connecting two pages, thereby enhancing inter-UI understanding. Based on this training framework, we propose MobileVLM, which utilizes consistent mobile data from Mobile3M for both pre-training and fine-tuning. This is a VLM that can simultaneously understand fine-grained element information within a UI page and the transition relationships between UI pages.\nTo address the lack of mobile pre-training data, we created Mobile3M, a large-scale dataset focusing on third-party Chinese apps. Specifically, we selected 49 popular apps and iteratively interacted with each UI element, collecting interaction traces. As shown in Figure 2, all interaction traces of each app are combined into a directed graph, where each node represents a UI page and each edge represents a transition action. Eventually, Mobile3M contains millions of UI pages, XML documents, and page changes caused by user interactions.\nOverall, our work has four major contributions as follows:\n\u2022 We propose MobileVLM, the first Chinese mobile VLM, pre-trained and fine-tuned on mobile data consistently.\n\u2022 We propose Mobile3M, the first large-scale Chinese mobile dataset with 3 million UI pages and real-world interactions, organized into a directed graph for each app.\n\u2022 We define two extra pre-training stages and four UI-based pre-training tasks, covering both intra- and inter-UI understanding.\n\u2022 Experimental results show that MobileVLM outperforms existing SOTA VLMs on ScreenQA (+14.34%) and our evaluation datasets (+34.18%)."}, {"title": "2 Related Work", "content": "2.1 Mobile UI Dataset\nTable 1 provides a comparison of multiple mobile UI datasets. At the top of the table are several \"point\" datasets. Each data instance in these datasets contains only one page, along with different fine-grained tasks and corresponding answers. Rico (Deka et al., 2017) is a large-scale Android UI dataset and has been widely used as a primary data source for UI modeling research. UIBert (Bai et al., 2021) release two new datasets extended from Rico. Ferret-UI (You et al., 2024) uses the UI detection model (Zhang et al., 2021b) to annotate fine-grained elements in Android and iPhone screens. However, these datasets only focus on the elements and layout within a single page, so it is difficult for them to capture the complete process of users using the app.\nTo better reflect user behavior, several \"chained\" mobile UI data sets have been released (Sun et al., 2022; Deng et al., 2024; Burns et al., 2021), as shown at the bottom of Table 1. Each \"chain\" of"}, {"title": "2.2 Mobile Vision-Language Models", "content": "Recently, several benchmarks (Rawles et al., 2023; Wen et al., 2023; Shaw et al., 2023; Yan et al., 2023) are proposed to evaluate page navigation and mobile phone manipulation. MM-Navigator(Yan et al., 2023) and AppAgent (Yang et al., 2023) are both GPT-4V-based agents for the page navigation task. CogAgent(Hong et al., 2023) finetunes a vision-language model, CogVLM(Wang et al., 2023), to complete page navigation tasks using only screenshots as input. UI-VLM(Dorka et al., 2024) benefits from the AutoUI dataset and utilizes a sequence of past screenshots as input."}, {"title": "3 Mobile3M Dataset", "content": "In this paper, we propose Mobile3M, a large-scale dataset focusing on Chinese apps. Mobile3M contains a total of 20,138,332 actions, covering 3,098,786 screenshots and corresponding XML documents. These data are organized into 49 large directed graphs, each representing a mobile app, with UI pages as nodes and actions as edges."}, {"title": "3.1 Background", "content": "UI Page: We selected 49 Chinese apps from the App Store, ensuring that each app had at least 10 million downloads. The apps are installed and run on the emulator, and we use Appium to collect UI pages. The UI page includes a screenshot and an XML document. The XML document describes the structure and content of a UI page, including elements like buttons and text boxes, as well as layout information such as bounding boxes and hierarchical trees. Figure 11 in the Appendix shows an example of a screenshot and an XML document for the app \"QQMusic\". The XML document can be parsed to produce a list of elements. As shown in the task (c) of Figure 3, each element contains a name and a bounding box.\n\u2022 Element (name, bound):\n(Cancel, [640,74][696,112]).\nAction Space: The data collection algorithm simulates the way people interact with smartphones. We designed three basic actions: click, scroll, and input.\n\u2022 Click (name, bound):\nclick(Cancel, [640,74][696,112]).\n\u2022 Scroll (bound, direction):\nscroll([0,211][720,271],up).\n\u2022 Input (name, bound, text):\ninput(Destination, [84,57][568,129], Beijing).\nAn element may be interactable with multiple actions. As shown in Figure 3, the 38 elements in this UI page can generate an action space containing 55 actions."}, {"title": "3.2 Data Collection", "content": "Inspired by APPAgent (Yang et al., 2023), we use the random walk algorithm to explore apps. The algorithm iteratively interacts with every element on each UI page and records the page transition"}, {"title": "3.3 Method Optimization", "content": "The goal of building the Mobile3M dataset is to explore all functions of the app, aiming to discover new pages and actions as much as possible. For an app with an average action space of 50, four interactive actions will expand the app's exploration space to 6,250,000 pages, containing many duplicates.\nTo improve exploration efficiency, we propose a \"unique page\" mechanism. Every time a new page is explored, we use BM25 (Robertson et al., 2009) to retrieve the top 5 nodes in the current app graph that are closest to the XML document of the page. The algorithm compares the new page with each of these five pages to determine if they are similar pages. The threshold of the similar coefficient is Element Diff<5 & Pixel Diff <30%. Here, Element Diff is the number of different elements between two UI pages and Pixel Diff is the pixel difference between two screenshots. If no similar page is found in the current graph, the new page will be treated as a unique page and added to the app graph. As shown in Figure 2, click the \"Back\" button on \"Baicizhan0_1_24\", and the generated \"Baicizhan0_1_24_113\" and \"Baicizhan0\" are equivalent pages. We add a directed edge from the previous page \"Baicizhan0_1_24\" to the similar page \"Baicizhan0\" in the graph. The algorithm will not treat the \"Baicizhan0_1_24_113\" as a new explorable node.\nThe benefits of this mechanism are threefold:\n1. This greatly reduces the exploration space of each app. Taking \"ctrip\" as an example, our exploration process produced 187,079 UI pages with an average steps of 6.5. Without the \"unique page\" mechanism, pages of this magnitude cannot even cover all possibilities of 4-step exploration. 2. This converts the tree structure exploration results into a graph structure. Different pages can reach \"Baicizhan0_1\" by clicking \"Edit\", \"OK\", and scrolling. This helps the agent learn the functions of different UI elements. 3. This helps prevent the occurrence of cyclic action sequences. The \"unique page\" mechanism can detect and prune them.\nTo balance the distribution of different actions in the dataset, during random walks, we give priority to the input action. We provide 10 related keywords for each app. When executing the input action, the algorithm can randomly select a keyword to input. For scroll actions, the algorithm can choose a direction to scroll from \"up, down, left, and right\"."}, {"title": "3.4 Dataset Statistics", "content": "Among the 49 selected apps, we ensure that each main category in AppStore contains at least 2 apps. Figure 4 shows the data distribution of the Mobile3M dataset. The most common application categories in the dataset are \"Travel\", \"Living\" and \"Shopping\". As shown in the figure, Mobile3M covers multiple categories, and the amount of data in each category is relatively balanced, which ensures that the dataset is versatile and diverse."}, {"title": "4 Model", "content": "As shown in Table 3, in addition to the standard fine-tuning architecture that includes general pre-training and mobile instruction fine-tuning, we extra included two-stage mobile data pre-training and four mobile pre-training tasks."}, {"title": "4.1 Pre-training", "content": "Stage 1: In the first stage of pre-training, our main goal is to enhance the VLM's understanding of intra-UI pages. We build the following three tasks to pre-train our model:\n1. Element List Generation: This task requires models to identify all interactive elements from the page. It requires OCR and grounding abilities to recognize texts and their bounding boxes. This task provides the foundational elements for grounding and interacting in subsequent tasks.\n2. Element Grounding (Li et al., 2021): The goal of this task is to enable the model to recognize and ground elements in pages. Given an element description, the model is required to determine its bounding box. We sample five elements on each page for grounding training.\n3. Action Space Generation: This task requires the model to generate all candidate actions from the UI page. Based on the extracted elements, the model needs to analyze the types of elements: clickable, inputtable, and scrollable. This task is crucial for the action prediction tasks in Stage 2.\nStage 2: 1. Action Prediction In stage 2 pre-training, we use the action prediction task to enhance VLM's ability to understand the relationship between two pages. The expected output is the action needed to navigate from the current page to the next page.\nThis task aims to enhance the model's ability to predict page relationships and learn the expected outcomes of corresponding actions, providing more accurate action reasoning for downstream tasks. In this task, the model's focus shifts from the content of intra-UIs to the complex graph structure across inter-UIs within an app."}, {"title": "4.2 Fine-tuning", "content": "1. Page Navigation In Stage 3, page navigation no longer provides two pages as in Stage 2. Instead, it provides a single page along with corresponding instructions. The model needs to generate the appropriate actions based on these instructions.\n2. VQA The VQA tasks require VLMs to answer the question based on a screenshot.\nIn stage 3 fine-tuning, we use Mobile3M to build self page navigation task, along with Auto-UI for the page navigation task and ScreenQA for the VQA task. This stage primarily aims to convert the model's understanding of intra-UI elements and relationships between inter-UI into practical end-to-end task completion and page question-answering domain."}, {"title": "4.3 Model Architecture", "content": "We adopt Qwen-VL-Chat (Bai et al., 2023b) as our foundation model, which consists of a Large Language Model: Qwen-7B (Bai et al., 2023a), a Visual Encoder: ViT-bigG (Dosovitskiy et al., 2020) with 1.9B parameters and a Position-aware Vision-Language Adapter (Zhang et al., 2021a) with 0.08B parameters. As shown in Figure 5, we use a three-stage training method and freeze the parameters of Qwen-7B in the first stage and ViT in the third stage."}, {"title": "5 Experiment", "content": "5.1 Datasets and Benchmarks\nWe constructed our own benchmarks by selecting data from Mobile3M, and additionally selected five public Chinese benchmarks. Specifically, we constructed the following two types of test datasets:\n\u2022 UnseenAPP To verify the ability of the model on unseen apps, we selected 7 apps out of the 49 apps as shown in Table 9 and did not use their data for training.\n\u2022 SeenAPP We randomly sampled 700 data for each task from the remaining 42 apps, which the model had seen during the training stage. There is no overlap between the training and the test set.\nWe randomly selected 500 screenshots from mobile3m and asked three annotators to construct question-and-answer pairs for each screenshot, named humanVQA benchmark.\nAs shown in Table 2, we choose 3 mobile benchmarks, ScreenQA and Auto-UI for evaluating stage 3 fine-tuning, and MoTIF to evaluate stage 2 pre-training. We chose two general benchmarks, ChineseOCRBench and RefCOCO, to measure general capability loss in stage 1 pre-training. More details can be seen in Appendix A.2."}, {"title": "5.2 Evaluation Metrics", "content": "Following prior works, we used 3 objective metrics and did not use additional human evaluation.\nSQUAD F1* For OCR and VQA tasks, we use an improved F1* score to measure the accuracy of VLM responses. Following OCRBench, we consider a response correct if the output contains the golden answer. Only when this condition is not met do we calculate the F1 score. F1* can be calculated as follows:\n1,\nF1* =\n2\u00b7\nPre\u00b7Recall\nPre+Recall\nif Ans in GT\notherwise\n(1)\nIoU Intersection over Union(Cheng et al., 2021) is the most commonly used metric in the field of object detection.\nAction Accuracy We follow Auto-UI's approach for evaluating action accuracy. Specifically, for click action, we allow a 14% margin of error relative to the screen size between the predicted answers and the golden answers. For scroll action, the predicted answer only needs to be on the same axis and in the same direction as the golden answer. For input, we only calculate the F1 score of the input content."}, {"title": "5.3 Implementation Details", "content": "Experiment Settings We trained the model on NVIDIA A100 GPUs (80G\u00d78). For Auto-UI finetune task, similar to its official method, we used 10% of the GoogleApps data of AITW to save 80% of the training time. Our hyperparameters are as follows: learning rate of le-5, batch size of 4, 6000 steps for stage 1 pre-training, and 7400 steps for stage 2 pre-training. During the testing, all baselines that were not fine-tuned were provided with few-shot instructions. More details can be seen in Appendix A. For stage 1 evaluation, we employed two SOTA models: GroundingDINO (Liu et al., 2023a) and Qwen-VL-Max. For Stage 2, we selected Seq2Act as the SOTA model on the MoTIF. For Stage 3, MobileVLMseparate were fine-tuned models based on separate subtasks of Auto-UI. MobileVLMunified was the unified model for all tasks in Stage 3. For specific information on baselines, refer to Appendix A.1.\nData Processing While Mobile3M is a Chinese dataset, Auto-UI and ScreenQA need to align with it during the testing stage. Therefore, we translated their instructions and answers into Chinese. Additionally, since all pages in the Mobile3M are uniformly sized at 720x1280, we resized the pages of Auto-UI and MoTIF to 720x1280. Our pre-training task requires VLMs to detect objects based on instructional descriptions, we removed test cases from RefCOCO that contain multiple objects in a single image to avoid ambiguity."}, {"title": "5.4 Main results", "content": "As shown in Table 4, MobileVLM achieved an overall improvement of 2.78% and outperformed the Auto-UI SOTA model in all tasks. This indicates that the two-stage pre-training tasks enhanced the model's accuracy in estimating expected actions in page navigation tasks. Notably, MobileVLM achieved this despite the translation information loss and the absence of a prompt pipeline. MobileVLMseparate slightly outperformed MobileVLMunified due to the varying features of different tasks, which can hinder simultaneous optimization. In Self-Navigation, our model significantly outperformed GPT-40 and Qwen-VL-Max (+9.4%, +34.18%), attributed to the consistent use of mobile domain data in both pre-training and fine-tuning. In the ScreenQA task, MobileVLM improved by 14.34% over Qwen-VL-Max, demonstrating superior intra-UI understanding and text extraction capabilities. Without specific fine-tuning on the HumanVQA task, MobileVLM still outperformed Qwen-VL-Max by 10.73%, showing its excellent generalization in mobile domain VQA tasks."}, {"title": "5.5 Ablation Study", "content": "Although we surpassed the baseline in Stage 3 tasks, this could be due to inherent differences in the base models' capabilities. To validate the pre-training effect, we conducted two ablation experiments: MobileVLM w/o Stage1&2, which is fine-tuned directly on Qwen-VL, and MobileVLM w/o Stage2, which is further fine-tuned on the Stage 1 model. As shown in Section 4 of Table 4, compared to MobileVLM w/o Stage1&2, MobileVLM achieved improvements of 4.79%, 5.2%, 18.84%, and 3.12% on Auto-UI, self-navigation, and ScreenQA, respectively. This indicates that the two-stage pre-training improved both the model's grounding and navigation capabilities. Compared to MobileVLM w/o Stage2, MobileVLM improved by 4% on Auto-UI and 12.6% in the IoU metric for Self-Navigation (from 35.89% to 48.49%). This highlights the importance of the Stage 2 action prediction task in enhancing the model's navigation capability by strengthening its understanding of inter-UI relationships. Additionally, we found that Stage 2 pre-training had little impact on VQA tasks, as these tasks rely more on the model's understanding of intra-UI elements."}, {"title": "5.6 Pre-training Results", "content": "Stage1 results MobileVLM continues with two-stage pre-training based on Qwen-VL-Chat. As shown in Table 5, compared to Qwen-VL-Chat, MobileVLM achieved significant improvements of 76.03%, 54.7%, and 55.11% on SeenAPP. Moreover, compared to the best baseline Qwen-VL-Max, MobileVLM improved by 44.6%, 40.73%, and 28.64%. This indicates Mobile VLM's superior ability to extract and ground elements. MobileVLM improved by 35.65%, 26.95%, and 27.81% on UnseenAPP compared to Qwen-VL-Chat, and it slightly outperformed Qwen-VL-Max and GPT-40 in the element list and action space accuracy metrics, only slightly lagging behind Qwen-VL-Max in the IoU for the grounding task. However, due to significant differences in element distribution and layout between UnseenAPP and SeenAPP, MobileVLM, despite surpassing the best baseline, cannot fully transfer abilities learned in SeenAPP to UnseenAPP. Since general training data was not used in Stage 2, MobileVLM is weaker on general benchmarks like RefCOCO and ChineseOCRBench compared to current SOTA models GroundingDINO and Qwen-VL-Max. For a detailed analysis, refer to Appendix D.\nStage2 results As seen in the SeenAPP results in Table 6, MobileVLM improved by 35.81% and 41.14% compared to Qwen-VL-Chat, and outperformed Qwen-VL-Max and GPT-4o by 25.79% and 18.11%, respectively. This indicates that the model can better understand the graph structure relationships between pages in SeenAPP. Our model shows a certain improvement compared to Qwen-VL-Chat, but due to the significant differences in the page graph structures between UnseenAPP and SeenAPP, it is weaker than GPT-4o in recognizing the positional relationships of these apps' pages. Nevertheless, we observed that MobileVLM exceeded Qwen-VL-Chat by 26.1% in the IoU metric and demonstrated excellent generalization in the acc task on MoTIF (99.6%)."}, {"title": "6 Conclusion", "content": "We propose MobileVLM, a specialized Chinese vision-language model for mobile UI manipulation. It surpasses both open-source mobile VLMs and larger closed-source general models on multiple mobile public benchmarks. Meanwhile, we build the first large-scale Chinese mobile dataset, Mobile3M, which includes multiple pre-training and fine-tuning tasks specific to mobile scenarios. We hope this work will promote the development of vision-language models in the mobile domain and provide a reference for future Mobile-agent research."}, {"title": "Limitations", "content": "Our training data includes 49 commonly used apps, but this may still not fully cover all scenarios of daily life, due to the vastness of the Android app market. In future work, we will continue to expand the number of apps. Additionally, because some apps have extra paid content, such as VIP, our model may not have fully learned all their functionalities. Our data may also have some temporal limitations, as random app updates can cause changes in page and action traces."}, {"title": "Ethics Statement", "content": "Our training data does indeed contain some personal information of the authors, but we commit to anonymizing all private data before making it public. Additionally, the personal information in the data before anonymization has been authorized by the respective individuals for use during the training stage. In the process of generating manually annotated data through crowdsourcing, we employed seven employees from a crowdsourcing company without discrimination. During the annotation process, they were provided with corresponding mobile screenshots and structured texts, and we paid them labor compensation of no less than 120 CNY per hour."}]}