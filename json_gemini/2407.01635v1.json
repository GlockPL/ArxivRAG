{"title": "Commute Graph Neural Networks", "authors": ["Wei Zhuo", "Guang Tan"], "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such\nas asymmetrical shortest paths typically found in digraphs. Recognizing this\ngap, we introduce Commute Graph Neural Networks (CGNN), an approach that\nseamlessly integrates node-wise commute time into the message passing scheme.\nThe cornerstone of CGNN is an efficient method for computing commute time\nusing a newly formulated digraph Laplacian. Commute time information is then\nintegrated into the neighborhood aggregation process, with neighbor contributions\nweighted according to their respective commute time to the central node in each\nlayer. It enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs.", "sections": [{"title": "Introduction", "content": "Directed graphs (digraphs) are widely employed to model relational structures in diverse domains,\nsuch as social networks (Cross et al., 2001) and recommendation systems (Qiu et al., 2020). Recently,\nthe advances of graph neural networks (GNNs) have inspired various attempts to adopt GNNs for\nanalyzing digraphs (Tong et al., 2020a,b, 2021; Zhang et al., 2021; Rossi et al., 2023; Geisler et al.,\n2023). The essence of GNN-based digraph analysis lies in utilizing GNNs to learn expressive node\nrepresentations that encode edge direction information.\nTo achieve this, modern digraph neural networks are designed to integrate edge direction information\ninto the message passing process by distinguishing between incoming and outgoing edges. This dis-\ntinction enables the central node to learn directionally discriminative information from its neighbors.\nAs illustrated in the digraph of Fig. 1, given a central node vi, a 1-layer digraph neural network can\naggregate messages from vi's incoming neighbor um and outgoing neighbor vj, and simultaneously\ncapture edge directions by applying direction-specific aggregation functions (Rossi et al., 2023), or\nby predefining edge-specific weights (Zhang et al., 2021; Tong et al., 2020b).\nDespite the advancements, current digraph neural networks primarily capture unidirectional\u00b9 rela-\ntionships between nodes, neglecting the complexity arising from path asymmetry. For instance, a\nk-layer GNN aggregates the neighbors within the shortest path k for the central node. If the graph is\nundirected, the shortest path between any two nodes is symmetric, as shown in the undirected graph\nof Fig. 1. This symmetry simplifies the representation of node relationships, implying that if the\nSPDs from one node to two other nodes are identical, then the SPDs from these two nodes back to the\nsource node must also be the same. Conversely, such symmetry is absent in digraphs. Considering the\ndigraph in Fig. 1, the shortest paths between vi and vj are asymmetric. Therefore, although vj and uk"}, {"title": "Related Work", "content": "While the Laplacian for undirected graphs has been extensively studied, the area of Laplace operator\ndigraphs remains underexplored. Chung (2005) pioneers this area by defining a normalized Laplace\noperator specifically for strongly connected directed graphs with nonnegative weights. This operator\nis expressed as $I - \\pi^{1/2} P\\pi^{-1/2} +\\pi^{-1/2}P^* \\pi^{1/2}$. Key to this formulation is the use of the transition\nprobability operator P and the Perron vector $\\pi$, with the operator being self-adjoint. Building\non the undirected graph Laplacian, Singh et al. (2016) adapt this concept to accommodate the\ndirected structure, focusing particularly on the in-degree matrix. They define the directed graph\nLaplacian as $D_{in} - A$, where $D_{in} = diag (\\{d_{in}^i\\}_{i=1}^N)$ represents the in-degree matrix. Li & Zhang\n(2012) uses stationary probabilities of the Markov chain governing random walks on digraphs to\ndefine the Laplacian as $\\pi^{\\frac{1}{2}}(I - P )\\pi^{-\\frac{1}{2}}$, which underscores the importance of random walks and their\nstationary distributions in understanding digraph dynamics. Hermitian Laplacian Furutani et al. (2020)\nconsider the edge directionality and node connectivity separately, and encode the edge direction\ninto the argument in the complex plane. Diverging from existing Laplacians, our proposed DiLap"}, {"title": "Digraph Laplacian", "content": "Contrary to the traditional graph Laplacian, typically defined as a symmetric positive semi-definite\nmatrix derived from the symmetric adjacency matrix, our proposed DiLap is built upon the transition\nmatrix to preserve the directed structure. Specifically, the classical graph Laplacian $L = D \u2013 A$ is\ninterpreted as the divergence of the gradient of a signal on an undirected graph (Shuman et al., 2013;\nHamilton, 2020): given a graph signal $s \u2208 R^N$, $(Ls)_i = \\sum_{j\u03b5N_i} A_{ij} (s_i \u2013 s_j)$. Intuitively, graph\nLaplacian corresponds to the difference operator on the signal s, and acts as a node-wise measure\nof local smoothness. In line with this conceptual foundation, we generalize the graph Laplacian to\ndigraphs by defining DiLap T:\n$(T8)_i = \\sum_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j) = ((D^{-1} \u2013 P)s)_i$, we have $T = D^{-1} \u2013 P$,\nwhere $N_{out}^i$ is the set of $v_i$'s out-neighbors, and $P_{ij} = \\frac{a_{i,j}}{d_{out}^i}$ with $d_{out}^i$ being the out-degree of $v_i$.\nThe detailed derivation of T is included in Appendix A.1. Eq. (5) illustrates that DiLap T acts as\nthe divergence of the gradient of a given signal on a digraph, and can be used to measure the local\nsmoothness in a digraph.\nConsidering the Laplacian operator's function in measuring the smoothness of a signal across the\nentire graph, it is crucial to assign greater weights to nodes of higher structural importance. It is\nbecause the smoothness at nodes central to the graph structure should have a more pronounced impact\non the global smoothness measurement. Thus, we further define the Weighted DiLap T:\n$(T8)_i = \\sum_{v_jEN_{out}} \\pi_i P_{ij} (s_i \u2013 s_j) = (\\Pi(D^{-1} \u2013 P)s)_i$, we have $T = \\Pi(D^{-1} \u2013 P)$.\nHere we utilize the i-th element of the Perron vector $\\pi$ to quantify the structural importance of\n$v_i$, reflecting its eigenvector centrality. This is based on the principle that a node's reachability is\ndirectly proportional to its corresponding value in the Perron vector (Xu et al., 2018). Therefore, $\\pi$\neffectively indicates the centrality and influence over the long term in the graph. Perron-Frobenius\nTheorem (Horn & Johnson, 2012) establishes that $\\pi$ satisfies $\\sum_i \\pi(i) = 1$, is strictly positive, and\nconverges to the left eigenvector of the dominant eigenvalue of P."}, {"title": "Similarity-based Graph Rewiring", "content": "Both the fundamental matrix defined in Eq. (3) and Weighted DiLap requires Assumption 3.1 to\nensure the existence and uniqueness of the Perron vector $\\pi$, conditions that are not universally met\nin general graphs. To fulfill the irreducibility and aperiodicity assumptions, Tong et al. (2020a)\nintroduce a teleporting probability uniformly distributed across all nodes. This method, inspired\nby PageRank (Page et al., 1999), amends the transition matrix to $P_{pr} = \\gamma P + (1 - \\gamma)\\frac{1_n 1_n^T}{n}$, where\n$\\gamma\u2208 (0,1)$. $P_{pr}$ allows for the possibility that a random walker may choose a non-neighbor node for\nthe next step with a probability of $\\frac{1-\\gamma}{n}$. This adjustment ensures that $P_{pr}$ is irreducible and aperiodic,\nso it has a unique $\\pi$. However, this approach leads to a complete graph represented by a dense matrix\n$P_{pr}$, posing significant challenges for subsequent computational processes.\nRather than employing $P_{pr}$ as the transition matrix, we introduce a graph rewiring method based on\nfeature similarity to make a given graph irreducible, while maintaining the sparsity. As outlined in\nProposition 1, to transform the digraph into a strongly connected structure, it is essential that each\nnode possesses a directed path to every other node. To this end, we initially construct a simple and\nirreducible graph G' with all N nodes, then add all edges from G' the original digraph G, thereby\nensuring G's irreducibility. The construction of G' begins with the calculation of the mean of node\nfeatures as the anchor vector a. Then we determine the similarity between each node and the anchor,\nsort the similarity values, and return the sorted node indices, denoted as $S\u2208 R^N$:\n$a = \\frac{\\sum_{i=1}^N Xi}{N}$, $S_i = cos(a, X_i)$, $S = argsort(\\{s_i\\}_{i=1}^N)$"}, {"title": "From DiLap to Commute Time", "content": "Given the Weighted DiLap T, we can unify the commute time information into the message passing\nby building the connection between T and the fundamental matrix Z:\nLemma 4.1. Given a rewired graph G, the Weighted DiLap is defined as $T = \\Pi(D^{-1} \u2013 P)$. Then\nthe fundamental matrix Z of \u011e can be solved by:\n$Z = \\Pi^{-\\frac{1}{2}}R^{\\dagger} \\Pi^{-\\frac{1}{2}}$\nwhere $R = \\Pi^{-\\frac{1}{2}}T\\Pi^{-\\frac{1}{2}} \u2013 D^{-1} + I$ and the superscript $\\dagger$ means Moore\u2013Penrose pseudoinverse of\nthe matrix.\nThe proof is given in Appendix A.2. Leveraging Lemma 4.1, we can further compute the hitting\ntimes and commute times in terms of T with the following theorem."}, {"title": "CGNN", "content": "$C\u2208 R^{N*N}$ quantifies the strength of mutual relations between node pairs in the random walk\ncontext. Notably, smaller values in C correspond to stronger mutual reachability, indicating stronger\nrelations between node pairs. Thus, C is a positive symmetric matrix, and the commute-time-based\nnode proximity matrix can be expressed as $C = exp(-C)$. Since the directed adjacency matrix A\nrepresents the outgoing edges of each node, $A^T$ therefore accounts for all incoming edges. Then\nwe have $C^{out} = A \u2299 C$ and $C^{in} = A^T \u2299 C$ represent the proximity between adjacent nodes under\noutgoing and incoming edges, respectively. We further perform row-wise max-normalization on $C^{out}$\nand $C^{in}$ to rescale the maximum value in each row to 1. Given the original graph G as input, we\ndefine the l-th layer of CGNN as:\n$m_{i,in}^{(l)} = Agg_{in}^{(l)}((C_{ij}^{in}.h_j^{(l-1)}: v_j \u2208 N_{in}^i))$\n$m_{i,out}^{(l)} = Agg_{out}^{(l)}((C_{ij}^{out}.h_j^{(l-1)}: v_j \u2208 N_{out}^i))$\n$h_i^{(l)} = Comb^{(l)}(h_i^{(l-1)}, m_{i,in}^{(l)}, m_{i,out}^{(l)})$"}, {"title": "Experiments", "content": "We conduct node classification experiments on five digraph datasets. Experimental details and data\nstatistics are provided in Appendix C.1 and Appendix C.2. We provide a performance comparison\nwith 12 baselines including 1) General GNNs: GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al.,\n2018), and GraphSAGE (Hamilton et al., 2017); 2) Non-local GNNs: APPNP (Klicpera et al., 2019),\nMixHop (Abu-El-Haija et al., 2019), GPRGNN (Chien et al., 2021), and GCNII (Ming Chen et al.,\n2020); 3) Digraph NNs: DGCN (Tong et al., 2020b), DiGCN (Tong et al., 2020a), MagNet (Zhang\net al., 2021), DiGCL (Tong et al., 2021), and DirGNN (Rossi et al., 2023). For all baselines, we apply\nboth the symmetrized and asymmetric adjacency matrix for experiments. The results reported are the\nbetter of the two results."}, {"title": "Overall Results", "content": "Table 1 reports the node classification results across five digraph datasets. Our method CGNN\nachieves new state-of-the-art results on 4 out of 5 datasets, and comparable results on the remaining\none, validating the superiority of CGNN. We provide more observations as follows. Firstly, while\nnon-local GNNs have the potential to cover the commute paths between adjacent nodes by stacking\nmultiple layers, they do not consistently outperform general, shallow GNN models. It suggests that\ncoarsely aggregating all nodes in commute paths is ineffective. The reason is that deeper models\nmay aggregate excessive irrelevant information for the central node. Our goal is to encode mutual\nrelationships between adjacent nodes by considering their commute times. Aggregating all nodes\nalong the entire path introduces excessive information about other nodes unrelated to the direct\nrelationship between the target nodes. Secondly, GNNs tailored for digraphs do not seem to bring\nsubstantial gains. Our results show that with careful hyper-parameter tuning, general GNNs can\nachieve results comparable to, or even better than, those tailored for digraphs, as evidenced in the\nSquirrel, Chameleon, and AM-Photo datasets. Thirdly, CGNN achieves state-of-the-art results on\nboth homophilic and heterophilic digraph benchmarks. Notably, DirGNN also performs comparably\non heterophilic graphs (Squirrel and Chameleon), supporting the findings of Rossi et al. (2023) that\ndistinguishing edge directionality during message passing enables the central node to adaptively\nbalance information flows from both heterophilic and homophilic neighbors, effectively mitigating\nthe impact of heterophily. Moreover, CGNN, an enhanced version of DirGNN, further improves\nperformance on these graphs by effectively incorporating commute times to refine the strength of\nrelationships between nodes, enhancing model robustness under heterophily.\nTo illustrate this, we further examine the relations between commute-time-based proximity and label\nsimilarity along edges. As shown in Eq. (11), we use commute-time-based proximity C to weigh the\nneighbors during neighbor aggregation. Then we define a label similarity matrix M where $M_{ij} = 1$\nif $v_j \u2208 N_i$ and $y_i = y_j$; otherwise $M_{ij} = 0$. Essentially, M extracts the edges connecting nodes\nwith the same classes from the adjacency matrix A. Thus a higher value of $||M \u2013 (A + A^T)||_2$\nindicates a more pronounced negative impact of heterophily on the model's performance. On the\nother hand, we compute $||M \u2013 (C^{in} + C^{out})||_2$ to evaluate the efficacy of C in filtering heterophilic\ninformation. The closer $(C^{in} + C^{out})$ is to M, the more effectively it aids the model in discarding\nirrelevant heterophilic information"}, {"title": "Efficiency Comparsion", "content": "Fig. 3 compares the accuracy of CGNN and other baseline models with their running times. Despite\nthe additional computational load of calculating commute-time-based proximity, the results show that\nCGNN provides the best trade-off between effectiveness and efficiency. In particular, on the Squirrel\ndataset, CGNN has the third-fastest calculation speed while yielding accuracy nearly double that of\nall other methods. On AM-Photo, CGNN achieves the highest accuracy while maintaining moderate\nefficiency."}, {"title": "Component Analysis", "content": "Comparison between graph rewiring and PPR. In Section 4.2, we construct a rewired graph\nG based on feature similarity to guarantee the irreducibility and aperiodicity. On the other hand,\nIn contrast, the classic PageRank transition matrix, defined as $P_{pr} = \\gamma P + (1 - \\gamma)\\frac{1_n 1_n^T}{n}$, achieves\na similar objective but results in a completely connected graph $G_{pr}$. However, this approach tends\nto overlook the sparse structure of the original graph, which may alter the semantic information in\nthe graph. Additionally, computing commute times using a dense transition matrix incurs a high\ncomputational cost. To validate the effectiveness of the rewiring approach over the PPR method, we\nconduct an experiment where G is replaced with $G_{pr}$ in the computation of commute-time-based\nproximity. We denote this variant as \u2018CGNNppr\u2019 and the results of accuracy and efficiency are\nreported in Table 3. The findings reveal that the PPR approach is suboptimal in terms of both accuracy\nand efficiency, thereby underscoring the effectiveness of our rewiring-based approach."}, {"title": "Conclusion", "content": "In this work, we introduce the Commute Graph Neural Network (CGNN) to integrate commute\ntime into GNNs for digraphs. To this end, we propose DiLap, a new Laplacian formulation for\ndigraphs, along with a rapid computational method for determining commute times. By integrating\ncommute times into GNN message passing through neighbor weighting, CGNN effectively leverages"}, {"title": "Proofs and Derivations", "content": "Since the digraph is unweighted", "A_{i,": ""}, "A_{i,:} = P_{i,:}$. We have:\n$\\sum_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j)$\n$\\sum_{vEN_{out}} \\frac{1}{d_{out}} s_i \u2013  \\frac{1}{d_{out}} s_j$\n$\\frac{1}{(D^{-1})_{i,i}} s_i \u2013  \\frac{1}{dout} A_{i,:}s$\n$(D^{-1}s)_i \u2013 (Ps)_i$\n$((D^{-1} \u2013 P)s)_i$.\nThus, DiLap T can be represented as $D^{-1} \u2013 P$.\nSince the transition matrix P is row-stochastic, it follows that PtJ = J. In light of Eq. (2) and\nconsidering that $\\pi$ is stochastic, we have $ZJ = 0_{nxn}$. Let $K = \\Pi^{-0.5} T \\Pi^{0.5} = \\Pi^{-0.5} (D^{-1}-P)\\Pi^{-0.5}$,\n$J = \\Pi^{-0.5}J\\Pi^{0.5}$, and $Z = \\Pi^{0.5}Z\\Pi^{-0.5}$, incorporating these into Eq. (3), we have:\n$Z + I = (K + J \u2013 D^{-1} + I) ^{-1}$.\nBy post-multiplying Eq. (13) from the right by $(K + J \u2013 D^{-1} + I)$, we have:\n$I = (Z + I)(K + J \u2013 D^{-1} + I)$\n$Z(K \u2013 D^{-1} + I) = I \u2013 J$.\nSimilarly, by multiplying from the left, we establish that $(K \u2013 D^{-1} + I)Z = I \u2212 J$. As $\\pi^{-1}Z = 0$,\nwe have $JZ = 0$. Thus, $Z(K \u2013 D^{-1} + I)Z = Z$. Furthermore, it can be straightforwardly inferred\nthat $(K \u2013 D^{-1} + I)J = 0$, then have $(K \u2013 D^{-1} + I)Z(K \u2013 D^{-1} + I) = (K \u2013 D^{-1} + I)$. Besides,\nconsidering the symmetry of the left part of Eq. (15), we have $(Z(K-D^{-1}+I))^T = Z(K\u2212D^{-1} + I)$.\nSimilarly, $((K-D^{-1}+I)Z)^T = (K\u2212D^{-1} + I)Z$. These derivations satisfy the sufficient conditions\nfor the Moore-Penrose pseudoinverse, such that\n$Z = (K \u2013 D^{-1} + I)^{\\dagger}$.\nFinally, recovering Z and K, which concludes the proof.\nGiven a matrix $R \u2208 R^{N\u00d7N}$, its Moore-Penrose pseudoinverse can be directly computed with an\nSVD-based method. Specifically, we first perform truncated SVD on $R = \\Pi^{-0.5} T \\Pi^{0.5} \u2212 D^{-1} + I\u2248$\n$U_qD_qV$, where $U_q \u2208 R^{N\u00d7q}$ and $V_q \u2208 R^{N\u00d79}$ contains the first q columns of U and V. $\u2211_q \u2208 R^{9\u00d79}$\nis the diagonal matrix of q largest singular values. It is a q-rank approximation of R, which holds\nthat rank(R) = q. Then the Moore-Penrose pseudoinverse of R can be easily computed as follows:\n$R^{\\dagger} = V_q^{-1} V_q^T$.\nTo leverage sparsity of R to avoid O(N\u00b3) complexity, we adopt the randomized SVD algorithm\nproposed by (Halko et al., 2011; Cai et al., 2023) to first approximate the range of the input matrix\nwith a low-rank orthonormal matrix, and then perform SVD on this smaller matrix:\n$\u00db_q, q, V = ApproxSVD(R, q), R_{SVD} = \u00db\u00db\u00db$"]}