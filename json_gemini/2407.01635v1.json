{"title": "Commute Graph Neural Networks", "authors": ["Wei Zhuo", "Guang Tan"], "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such\nas asymmetrical shortest paths typically found in digraphs. Recognizing this\ngap, we introduce Commute Graph Neural Networks (CGNN), an approach that\nseamlessly integrates node-wise commute time into the message passing scheme.\nThe cornerstone of CGNN is an efficient method for computing commute time\nusing a newly formulated digraph Laplacian. Commute time information is then\nintegrated into the neighborhood aggregation process, with neighbor contributions\nweighted according to their respective commute time to the central node in each\nlayer. It enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs.", "sections": [{"title": "Introduction", "content": "Directed graphs (digraphs) are widely employed to model relational structures in diverse domains,\nsuch as social networks (Cross et al., 2001) and recommendation systems (Qiu et al., 2020). Recently,\nthe advances of graph neural networks (GNNs) have inspired various attempts to adopt GNNs for\nanalyzing digraphs (Tong et al., 2020a,b, 2021; Zhang et al., 2021; Rossi et al., 2023; Geisler et al.,\n2023). The essence of GNN-based digraph analysis lies in utilizing GNNs to learn expressive node\nrepresentations that encode edge direction information.\nTo achieve this, modern digraph neural networks are designed to integrate edge direction information\ninto the message passing process by distinguishing between incoming and outgoing edges. This dis-\ntinction enables the central node to learn directionally discriminative information from its neighbors.\nAs illustrated in the digraph of Fig. 1, given a central node vi, a 1-layer digraph neural network can\naggregate messages from vi's incoming neighbor Um and outgoing neighbor vj, and simultaneously\ncapture edge directions by applying direction-specific aggregation functions (Rossi et al., 2023), or\nby predefining edge-specific weights (Zhang et al., 2021; Tong et al., 2020b).\nDespite the advancements, current digraph neural networks primarily capture unidirectional\u00b9 rela-\ntionships between nodes, neglecting the complexity arising from path asymmetry. For instance, a\nk-layer GNN aggregates the neighbors within the shortest path k for the central node. If the graph is\nundirected, the shortest path between any two nodes is symmetric, as shown in the undirected graph\nof Fig. 1. This symmetry simplifies the representation of node relationships, implying that if the\nSPDs from one node to two other nodes are identical, then the SPDs from these two nodes back to the\nsource node must also be the same. Conversely, such symmetry is absent in digraphs. Considering the\ndigraph in Fig. 1, the shortest paths between vi and vj are asymmetric. Therefore, although vj and Uk\n\"unidirectional' refers to relationships in digraphs where edges have a specific direction from one node to\nanother."}, {"title": "Related Work", "content": "While the Laplacian for undirected graphs has been extensively studied, the area of Laplace operator\ndigraphs remains underexplored. Chung (2005) pioneers this area by defining a normalized Laplace\noperator specifically for strongly connected directed graphs with nonnegative weights. This operator\nis expressed as \\(I = \\pi^{1/2} P \\pi^{-1/2} + \\pi^{-1/2} P^* \\pi^{1/2}\\). Key to this formulation is the use of the transition\nprobability operator P and the Perron vector \u03c0, with the operator being self-adjoint. Building\non the undirected graph Laplacian, Singh et al. (2016) adapt this concept to accommodate the\ndirected structure, focusing particularly on the in-degree matrix. They define the directed graph\nLaplacian as \\(D_{in} \u2013 A\\), where \\(D_{in} = diag (\\{d_{in}\\}^{N}_{i=1})\\) represents the in-degree matrix. Li &\n(2012) uses stationary probabilities of the Markov chain governing random walks on digraphs to\ndefine the Laplacian as \\(\\pi^{1/2} (I \u2212 P) \\pi^{-1/2}\\), which underscores the importance of random walks and their\nstationary distributions in understanding digraph dynamics. Hermitian Laplacian Furutani et al. (2020)\nconsider the edge directionality and node connectivity separately, and encode the edge direction\ninto the argument in the complex plane. Diverging from existing Laplacians, our proposed DiLap"}, {"title": "Digraph Laplacian", "content": "\\(\\Pi(D^{-1} \u2013 P)\\) is grounded in graph signal processing principles, conceptualized as the divergence of\na signal's gradient on the digraph. It encompasses the degree matrix D to preserve local connectivity,\nthe transition matrix P to maintain the graph's directed structure, and the diagonalized Perron vector\nII, capturing critical global graph attributes such as node structural importance, global connectivity,\nand expected reachability (Chung, 1997)."}, {"title": "Digraph Neural Networks", "content": "To effectively capture the directed structure with GNNs, spectral-based methods (Zhang et al.,\n2021; Tong et al., 2020a,b) have been proposed to preserve the underlying spectral properties of the\ndigraph by performing spectral analysis based on the digraph Laplacian proposed by (Chung, 2005).\nMagNet (Zhang et al., 2021) utilizes magnetic Laplacian to derive a complex-valued Hermitian matrix\nto encode the asymmetric nature of digraphs. Spatial GNNs also offer a natural approach to capturing\ndirected structures. For instance, GraphSAGE (Hamilton et al., 2017) allows for controlling the\ndirection of information flow by considering in-neighbors or out-neighbors separately. DirGNN (Rossi\net al., 2023) further extends this framework by segregating neighbor aggregation according to edge\ndirections, offering a more refined method to handle the directed nature of graphs."}, {"title": "Random Walk Distance and GNNS", "content": "We start by establishing the notations. We then show that message passing based GNNs naturally\ncapture the concept of hitting time during information propagation across the graph, due to the\nunidirectional nature of the neighborhood aggregation. Subsequently, we argue for the significance\nof commute time, highlighting it as a more compact measure of mutual node-wise interactions in\nrandom walks.\nNotations Consider G = (V, E, X) as an unweighted digraph comprising N nodes, where V =\n\\(\\{v_i\\}_{i=1}^{N}\\) is the node set, \\(E \\subseteq (V \u00d7 V)\\) is the edge set, \\(X \u2208 R^{N\u00d7d}\\) is the node feature matrix and\n\\(Y = \\{y_i\\}_{i=1}^{N}\\) is the set of labels for V. Let \\(A \u2208 R^{N\u00d7N}\\) be the adjacency matrix and\n\\(D = diag(d_1,..., d_N) \u2208 R^{N\u00d7N}\\) be the degree matrix of A, where \\(d_i = \\Sigma_{vj\u2208V} A_{ij}\\) is the out-\ndegree of \\(v_i\\). Let \\(\\tilde{A} = A + I\\) and \\(\\tilde{D} = D + I\\) denote the adjacency and degree matrix with self-loops,\nrespectively. The transition probability matrix of the Markov chain associated with random walks on\nG is defined as \\(P = D^{\u22121}A\\), where \\(P_{ij} = A_{ij}/deg(v_i)\\) is the probability of a 1-step random walk\nstarting from \\(v_i\\) to \\(v_j\\). Graph Laplacian formalized as \\(L = D \u2013 A\\) is defined on the undirected graph\nwhose adjacency matrix is symmetric. The symmetrically normalized Laplacian with self-loops (Wu\net al., 2019) is defined as \\(\\hat{L} = \\tilde{D}^{\u22121/2}\\tilde{L}\\tilde{D}^{\u22121/2}\\), where \\(\\tilde{L} = \\tilde{D} \u2013 \\tilde{A}\\)."}, {"title": "Digraph Neural Networks", "content": "DirGNN (Rossi et al., 2023) is a general framework that generalizes\nthe message passing paradigm to digraphs by adapting to the directionality of edges. It involves\nseparate aggregation processes for incoming and outgoing neighbors of each node as follows:\n\\(m_{i,in}^{(l)} = Agg_{in}^{(l)}(\\{h_j^{(l-1)} : v_j \u2208 N_{in}\\}\\\\)\n\\(m_{i,out}^{(l)} = Agg_{out}^{(l)}(\\{h_j^{(l-1)} : v_j\u2208 N_{out}\\}\\\\)                                                                           (1)\n\\(h_i^{(l)} = Comb^{(l)}(h_i^{(l-1)}, m_{i,in}^{(l)}, m_{i,out}^{(l)}\\)\nwhere \\(N_{in}\\) and \\(N_{out}\\) are respectively incoming and outgoing neighbors of \\(v_i\\). \\(Agg_{in}^{(l)}(\u00b7)\\) and \\(Agg_{out}^{(l)}(\u00b7)\\)\nare specialized aggregation functions of \\(N_{in}\\) and \\(N_{out}\\) at layer l, used to encode the directional\ncharacteristics of the edges connected to \\(v_i\\)."}, {"title": "Can GNNS Capture Random Walk Distance?", "content": "In the context of random walks on a digraph, hitting time and commute time, collectively referred\nto as random walk distances, serve as key metrics for assessing node connectivity and interaction\nstrength. Hitting time \\(h(v_i, v_j)\\) is the expected number of steps a random walk takes to reach a\nspecific target node \\(v_j\\) for the first time, starting from a given source node \\(v_i\\). Commute time \\(c(v_i, v_j)\\)\nis the expected number of steps required for a random walk to start at \\(v_i\\), reach \\(v_j\\), and come back. A"}, {"title": "Commute Time Computation", "content": "Based on the standard Markov chain theory, a useful tool to study random walk distances is the\nfundamental matrix (Aldous & Fill, 2002). We first establish the following assumptions required to\nsupport the theorem.\nAssumption 3.1. The digraph G is irreducible and aperiodic.\nThese two properties pertain to the Markov chain's stationary probability distribution \u03c0 (i.e., Perron\nvector) corresponding to the given graph. Irreducibility ensures that it is possible to reach any node\n(state) from any other node, preventing \u03c0 from converging to 0. Aperiodicity ensures that the Markov\nchain does not get trapped in cycles of a fixed length, thus guaranteeing the existence of a unique \u03c0.\nExistence and uniqueness of \u03c0 facilitate deterministic analysis and computation. For a more intuitive\nunderstanding of the assumptions, we give the sufficient conditions of digraph under the irreducibility\nand aperiodicity assumptions.\nProposition 1. A strongly connected digraph, in which a directed path exists between every pair of\nvertices, is irreducible. A digraph with self-loops in each node is aperiodic."}, {"title": "Commute Graph Neural Networks", "content": "In this section, we present Commute Graph Neural Networks (CGNN) to encode the commute time\ninformation into message passing. We first establish the relationship between random walk distances\nand the digraph Laplacian."}, {"title": "Digraph Laplacian (DiLap)", "content": "Contrary to the traditional graph Laplacian, typically defined as a symmetric positive semi-definite\nmatrix derived from the symmetric adjacency matrix, our proposed DiLap is built upon the transition\nmatrix to preserve the directed structure. Specifically, the classical graph Laplacian \\(L = D \u2013 A\\) is\ninterpreted as the divergence of the gradient of a signal on an undirected graph (Shuman et al., 2013;\nHamilton, 2020): given a graph signal \\(s \u2208 R^N\\), \\((Ls)_i = \\Sigma_{j\u03b5N_i} A_{ij} (s_i \u2013 s_j)\\). Intuitively, graph\nLaplacian corresponds to the difference operator on the signal s, and acts as a node-wise measure\nof local smoothness. In line with this conceptual foundation, we generalize the graph Laplacian to\ndigraphs by defining DiLap T:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j) = ((D^{-1} \u2013 P)s)_i\\), we have \\(T = D^{\u22121} \u2013 P\\),                (5)\nwhere \\(N_{out}\\) is the set of \\(v_i\\)'s out-neighbors, and \\(P_{ij} = \\frac{A_{ij}}{d_{out}}\\) with \\(d_{out}\\) being the out-degree of \\(v_i\\).\nThe detailed derivation of T is included in Appendix A.1. Eq. (5) illustrates that DiLap T acts as\nthe divergence of the gradient of a given signal on a digraph, and can be used to measure the local\nsmoothness in a digraph.\nConsidering the Laplacian operator's function in measuring the smoothness of a signal across the\nentire graph, it is crucial to assign greater weights to nodes of higher structural importance. It is\nbecause the smoothness at nodes central to the graph structure should have a more pronounced impact\non the global smoothness measurement. Thus, we further define the Weighted DiLap T:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} \\pi_i P_{ij} (s_i \u2013 s_j) = (\\Pi(D^{-1} \u2013 P)s)_i\\), we have \\(\\bar{T} = \\Pi(D^{\u22121} \u2013 P)\\).                               (6)\nHere we utilize the i-th element of the Perron vector \u03c0 to quantify the structural importance of\n\\(v_i\\), reflecting its eigenvector centrality. This is based on the principle that a node's reachability is\ndirectly proportional to its corresponding value in the Perron vector (Xu et al., 2018). Therefore, \u03c0\neffectively indicates the centrality and influence over the long term in the graph. Perron-Frobenius\nTheorem (Horn & Johnson, 2012) establishes that \u03c0 satisfies \\(\\Sigma_i \\pi(i) = 1\\), is strictly positive, and\nconverges to the left eigenvector of the dominant eigenvalue of P."}, {"title": "Similarity-based Graph Rewiring", "content": "Both the fundamental matrix defined in Eq. (3) and Weighted DiLap requires Assumption 3.1 to\nensure the existence and uniqueness of the Perron vector \u03c0, conditions that are not universally met\nin general graphs. To fulfill the irreducibility and aperiodicity assumptions, Tong et al. (2020a)\nintroduce a teleporting probability uniformly distributed across all nodes. This method, inspired\nby PageRank (Page et al., 1999), amends the transition matrix to \\(P_{pr} = \u03b3P + (1 \u2212 \u03b3)\\frac{1}{N}\\), where\n\u03b3\u2208 (0,1). \\(P_{pr}\\) allows for the possibility that a random walker may choose a non-neighbor node for\nthe next step with a probability of \\(\\frac{1\u2212\u03b3}{N}\\). This adjustment ensures that \\(P_{pr}\\) is irreducible and aperiodic,\nso it has a unique \u03c0. However, this approach leads to a complete graph represented by a dense matrix\n\\(P_{pr}\\), posing significant challenges for subsequent computational processes.\nRather than employing \\(P_{pr}\\) as the transition matrix, we introduce a graph rewiring method based on\nfeature similarity to make a given graph irreducible, while maintaining the sparsity. As outlined in\nProposition 1, to transform the digraph into a strongly connected structure, it is essential that each\nnode possesses a directed path to every other node. To this end, we initially construct a simple and\nirreducible graph G' with all N nodes, then add all edges from G' the original digraph G, thereby\nensuring G's irreducibility. The construction of G' begins with the calculation of the mean of node\nfeatures as the anchor vector a. Then we determine the similarity between each node and the anchor,\nsort the similarity values, and return the sorted node indices, denoted as \\(S \u2208 R^N\\):\n\\(a = \\frac{\\Sigma_i X_i}{N}\\),        \\(s_i = cos(a, X_i)\\),        \\(S = argsort(\\{s_i\\}_{i=1}^{N})\\)                            (7)\nwhere \\(cos(a, X_i)\\) is the cosine similarity between node features of \\(v_i\\) and a, and argsort(\u00b7) yields\nthe indices of nodes that sort similarity values \\(\\{s_i\\}_{i=1}^{N}\\). We then connect the nodes one by one with\nundirected (bidirectional) edges following the order in S to construct G', as shown in Fig. 2. Given\nthat G' is strongly connected, adding all its edges into G results in a strongly connected digraph G,\nwhich is irreducible. To achieve aperiodicity, self-loops are further added to G.\nThis rewiring approach satisfies Assumption 3.1 and maintains graph sparsity. Additionally, adding\nedges between nodes with similar features only minimally alters the overall semantics of the original\ngraph. Based on G and its corresponding P and I, we have the deterministic Weighted DiLap T."}, {"title": "From DiLap to Commute Time", "content": "Given the Weighted DiLap T, we can unify the commute time information into the message passing\nby building the connection between T and the fundamental matrix Z:\nLemma 4.1. Given a rewired graph G, the Weighted DiLap is defined as \\(\\bar{T} = \\Pi(D^{\u22121} \u2013 P)\\). Then\nthe fundamental matrix Z of \u011e can be solved by:\n\\(Z = \\bar{\\Pi}^{-1/2}R^{\\dagger} \\bar{\\Pi}^{1/2}\\),                (8)\nwhere \\(R = \\bar{\\Pi}^{-1/2}\\bar{\\Pi}^{-1/2} \u2013 D^{\u22121} + I\\) and the superscript \u2020 means Moore\u2013Penrose pseudoinverse of\nthe matrix.\nThe proof is given in Appendix A.2. Leveraging Lemma 4.1, we can further compute the hitting\ntimes and commute times in terms of T with the following theorem."}, {"title": "CGNN", "content": "\\(C \u2208 R^{N\u00d7N}\\) quantifies the strength of mutual relations between node pairs in the random walk\ncontext. Notably, smaller values in C correspond to stronger mutual reachability, indicating stronger\nrelations between node pairs. Thus, C is a positive symmetric matrix, and the commute-time-based\nnode proximity matrix can be expressed as \\(C = exp(-C)\\). Since the directed adjacency matrix A\nrepresents the outgoing edges of each node, \\(A^T\\) therefore accounts for all incoming edges. Then\nwe have \\(C^{out} = A \u2299 C\\) and \\(C^{in} = A^T \u2299 C\\) represent the proximity between adjacent nodes under\noutgoing and incoming edges, respectively. We further perform row-wise max-normalization on \\(C^{out}\\)\nand \\(C^{in}\\) to rescale the maximum value in each row to 1. Given the original graph G as input, we\ndefine the l-th layer of CGNN as:\n\\(m_{i,in}^{(l)} = \\bar{C}_{in}\\) \u00b7 \\(\\{h_j^{(l-1)}\\} : v_j \u2208 N_{in}\\\\}\\\\)\n\\(m_{i,out}^{(l)} = \\bar{C}_{out}\\) \u00b7 \\(\\{h_j^{(l-1)}\\} : v_j\u2208 N_{out}\\}\\\\}\\\\)                                                     (11)\n\\(h_i^{(l)} = Comb^{(l)}(h_i^{(l-1)}, m_{i,in}^{(l)}, m_{i,out}^{(l)}\\)\nwhere \\(Agg_{in}^{(l)}(\u00b7)\\) and \\(Agg_{out}^{(l)}(\u00b7)\\) are mean aggregation functions with different feature transformation\nweights, and Comb(l)(\u00b7) is a mean operator. Within each layer, the influence of \\(v_j\\) on the central\nnode \\(v_i\\) is modulated by the commute-time-based proximity C based on the edge directionality. We\npresent the pseudocode and complexity of CGNN in Algorithm 1."}, {"title": "Experiments", "content": "We conduct node classification experiments on five digraph datasets. Experimental details and data\nstatistics are provided in Appendix C.1 and Appendix C.2. We provide a performance comparison\nwith 12 baselines including 1) General GNNs: GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al.,\n2018), and GraphSAGE (Hamilton et al., 2017); 2) Non-local GNNs: APPNP (Klicpera et al., 2019),\nMixHop (Abu-El-Haija et al., 2019), GPRGNN (Chien et al., 2021), and GCNII (Ming Chen et al.,\n2020); 3) Digraph NNs: DGCN (Tong et al., 2020b), DiGCN (Tong et al., 2020a), MagNet (Zhang\net al., 2021), DiGCL (Tong et al., 2021), and DirGNN (Rossi et al., 2023). For all baselines, we apply\nboth the symmetrized and asymmetric adjacency matrix for experiments. The results reported are the\nbetter of the two results."}, {"title": "Overall Results", "content": "Table 1 reports the node classification results across five digraph datasets. Our method CGNN\nachieves new state-of-the-art results on 4 out of 5 datasets, and comparable results on the remaining\none, validating the superiority of CGNN. We provide more observations as follows. Firstly, while\nnon-local GNNs have the potential to cover the commute paths between adjacent nodes by stacking\nmultiple layers, they do not consistently outperform general, shallow GNN models. It suggests that\ncoarsely aggregating all nodes in commute paths is ineffective. The reason is that deeper models\nmay aggregate excessive irrelevant information for the central node. Our goal is to encode mutual\nrelationships between adjacent nodes by considering their commute times. Aggregating all nodes\nalong the entire path introduces excessive information about other nodes unrelated to the direct\nrelationship between the target nodes. Secondly, GNNs tailored for digraphs do not seem to bring\nsubstantial gains. Our results show that with careful hyper-parameter tuning, general GNNs can\nachieve results comparable to, or even better than, those tailored for digraphs, as evidenced in the\nSquirrel, Chameleon, and AM-Photo datasets. Thirdly, CGNN achieves state-of-the-art results on\nboth homophilic and heterophilic digraph benchmarks. Notably, DirGNN also performs comparably\non heterophilic graphs (Squirrel and Chameleon), supporting the findings of Rossi et al. (2023) that\ndistinguishing edge directionality during message passing enables the central node to adaptively\nbalance information flows from both heterophilic and homophilic neighbors, effectively mitigating\nthe impact of heterophily. Moreover, CGNN, an enhanced version of DirGNN, further improves\nperformance on these graphs by effectively incorporating commute times to refine the strength of\nrelationships between nodes, enhancing model robustness under heterophily.\nTo illustrate this, we further examine the relations between commute-time-based proximity and label\nsimilarity along edges. As shown in Eq. (11), we use commute-time-based proximity C to weigh the\nneighbors during neighbor aggregation. Then we define a label similarity matrix M where \\(M_{ij} = 1\\)\nif \\(v_j \u2208 N_i\\) and \\(y_i = y_j\\); otherwise \\(M_{ij} = 0\\). Essentially, M extracts the edges connecting nodes\nwith the same classes from the adjacency matrix A. Thus a higher value of \\(||M \u2013 (A + A^T)||\\)\nindicates a more pronounced negative impact of heterophily on the model's performance. On the\nother hand, we compute \\(||M \u2013 (C^{in} + C^{out})||\\) to evaluate the efficacy of C in filtering heterophilic\ninformation. The closer (\\(C^{in} + C^{out}\\)) is to M, the more effectively it aids the model in discarding\nirrelevant heterophilic information."}, {"title": "Efficiency Comparsion", "content": "Fig. 3 compares the accuracy of CGNN and other baseline models with their running times. Despite\nthe additional computational load of calculating commute-time-based proximity, the results show that\nCGNN provides the best trade-off between effectiveness and efficiency. In particular, on the Squirrel\ndataset, CGNN has the third-fastest calculation speed while yielding accuracy nearly double that of\nall other methods. On AM-Photo, CGNN achieves the highest accuracy while maintaining moderate\nefficiency."}, {"title": "Component Analysis", "content": "Comparison between graph rewiring and PPR. In Section 4.2, we construct a rewired graph\nG based on feature similarity to guarantee the irreducibility and aperiodicity. On the other hand,\nIn contrast, the classic PageRank transition matrix, defined as \\(P_{pr} = yP + (1 \u2212y)\\frac{1}{N}\\), achieves\na similar objective but results in a completely connected graph \\(\\bar{G}_{pr}\\). However, this approach tends\nto overlook the sparse structure of the original graph, which may alter the semantic information in\nthe graph. Additionally, computing commute times using a dense transition matrix incurs a high\ncomputational cost. To validate the effectiveness of the rewiring approach over the PPR method, we\nconduct an experiment where G is replaced with \\(\\bar{G}_{pr}\\) in the computation of commute-time-based\nproximity. We denote this variant as \u2018CGNNppr' and the results of accuracy and efficiency are\nreported in Table 3. The findings reveal that the PPR approach is suboptimal in terms of both accuracy\nand efficiency, thereby underscoring the effectiveness of our rewiring-based approach."}, {"title": "CGNN", "content": "CGNN sym\nDirected vs. Undirected. To validate the critical role of\ndirected structures in our model, we transform all directed\nedges into undirected ones by adding their reverse coun-\nterparts. This process results in a symmetric adjacency\nmatrix, denoted as \\(A_{sym}\\). Subsequently, the commute\ntime is calculated based on the transition matrix derived\nfrom \\(A_{sym}\\). We refer this variant as \u2018CGNNsym\u2019. Fig. 4\nshows the accuracy of CGNN and on three datasets. We find that edge direction can\nsignificantly influence the prediction accuracy for our model."}, {"title": "Conclusion", "content": "In this work, we introduce the Commute Graph Neural Network (CGNN) to integrate commute\ntime into GNNs for digraphs. To this end, we propose DiLap, a new Laplacian formulation for\ndigraphs, along with a rapid computational method for determining commute times. By integrating\ncommute times into GNN message passing through neighbor weighting, CGNN effectively leverages"}, {"title": "Proofs and Derivations", "content": ""}, {"title": "Derivation of DiLap T", "content": "Since the digraph is unweighted, then \\(P_{ij} = \\frac{A_{ij}}{d_{out}}\\) and \\(A_{i,:} = P_{i,:}\\). We have:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j)\\\n\t\t\t\t\t\t\t= \\Sigma_{v_jEN_{out}} \\frac{1}{d_{out}} s_i \u2013 \\frac{1}{d_{out}} s_j\n\t\t\t\t\t\t\t= \\frac{1}{d_{out}} s_i \\Sigma_{v_jEN_{out}} 1 \u2013 \\Sigma_{v_jEN_{out}} \\frac{1}{d_{out}} s_j         (12)\n\t\t\t\t\t\t\t= (D^{\u22121}s)_i \u2013 \\frac{1}{d_{out}} \\Sigma_{v_jEN_{out}} A_{i,:}s\\\n\t\t\t\t\t\t\t= (D^{\u22121}s)_i \u2013 (Ps)_i\n\t\t\t\t\t\t\t= ((D^{\u22121} \u2013 P)s)_i.\nThus, DiLap T can be represented as \\(D^{-1} \u2013 P\\)."}, {"title": "Proof of Lemma 4.1", "content": "Proof. Since the transition matrix P is row-stochastic, it follows that PtJ = J. In light of Eq. (2) and\nconsidering that \u03c0 is stochastic, we have ZJ = 0nxn. Let \\(K = \\bar{\\Pi}^{-1/2}\\bar{T}\\bar{\\Pi}^{-1/2} = \\bar{\\Pi}^{1/2}(D^{-1}\u2212P)\\bar{\\Pi}^{-1/2}\\),   \n\\(J = \\bar{\\Pi}^{1/2}J\\bar{\\Pi}^{-1/2}\\), and \\(Z = \\bar{\\Pi}^{1/2}Z\\bar{\\Pi}^{\u22121/2}\\), incorporating these into Eq. (3), we have:\n\\(Z + I = (K + J \u2013 D^{\u22121} + I )^{-1}\\).           (13)\nBy post-multiplying Eq. (13) from the right by (K + J \u2013 D^{\u22121} + I), we have:\n\\(I = (Z + I)(K + J \u2013 D^{-1} + I)\\)     (14)\n\t\t\t\t\t\t\t= ZK + ZJ \u2013 ZD^{\u22121} + Z + JK + J^2 \u2013 JD^{\u22121} + J.\nSince \\(J^2 = I, IK = I(D^{\u22121} \u2013 I)\\), and ZJ = \\(\\bar{\\Pi}^{1/2}ZJ\\bar{\\Pi} = 0_{nxn}\\), Eq. (14) can be simplified to:\n\\(Z(K \u2013 D^{-1} + I) = I \u2013 J\\).                         (15)\nSimilarly, by multiplying from the left, we establish that (K \u2013 D^{\u22121} + I)Z = I \u2212 J. As \\(\\pi^{\u22121}Z = 0\\),\nwe have JZ = 0. Thus, \\(Z(K \u2013 D^{\u22121} + I)Z = Z\\). Furthermore, it can be straightforwardly inferred\nthat \\((K \u2013 D^{\u22121} + I)J = 0\\), then have \\((K \u2013 D^{\u22121} + I)Z(K \u2013 D^{\u22121} + I) = (K \u2013 D^{\u22121} + I)\\). Besides,\nconsidering the symmetry of the left part of Eq. (15), we have \\((Z(K-D^{-1}+I))^T = Z(K\u2212D\u22121+I)\\).\nSimilarly, \\(((K-D^{-1}+I)Z)^T = (K\u2212D\u22121+I)Z\\). These derivations satisfy the sufficient conditions\nfor the Moore-Penrose pseudoinverse, such that\n\\(Z = (K \u2013 D^{\u22121} + I)^{\\dagger}\\).     (16)\nFinally, recovering Z and K, which concludes the proof.\t \\(\\)"}, {"title": "SVD for", "content": "Given a matrix \\(R \u2208 R^{N\u00d7N}\\), its Moore-Penrose pseudoinverse can be directly computed with an\nSVD-based method. Specifically, we first perform truncated SVD on \\(R = \\bar{\\Pi}^{-1/2}\\bar{\\Pi}^{-1/2} \u2013 D^{\u22121} + I\u2248\nU_q\\Sigma_qV^T\\), where \\(U_q \u2208 R^{N\u00d7q}\\) and \\(V_q \u2208 R^{N\u00d7q}\\) contains the first q columns of U and V. \\(\\Sigma_q \u2208 R^{q\u00d7q}\\)\nis the diagonal matrix of q largest singular values. It is a q-rank approximation of R, which holds\nthat rank(R) = q. Then the Moore-Penrose pseudoinverse of R can be easily computed as follows:\n\\(R^{\\dagger} = V_q\\Sigma_q^{\u22121}U^T\\).    (17)\nTo leverage sparsity of R to avoid O(N\u00b3) complexity, we adopt the randomized SVD algorithm\nproposed by (Halko et al., 2011; Cai et al., 2023) to first approximate the range of the input matrix\nwith a low-rank orthonormal matrix, and then perform SVD on this smaller matrix:\n\\(\\hat{U_q}, \\hat{\\Sigma_q}, \\hat{V_q} = ApproxSVD(R, q)\\),        \\(R_{SVD}^{\\dagger} = \\hat{U_q}\\hat{\\Sigma_q}^{\u22121}\\hat{V_q}^T,\\)         (18)"}, {"title": "Pseudo Code for CGNN", "content": "Complexity Analysis The time complexity of randomized truncated SVD to compute \\(R^{\\dagger}\\) is\nO(q|E|), and the message passing iteration has the same time complexity as GraphSAGE with\nO(|E|). Therefore, the overall time complexity of CGNN is O(q|E|)."}, {"title": "Implementation Details", "content": ""}, {"title": "Experimental Settings", "content": "We evaluate the performance by node classification accuracy with standard deviation in the semi-\nsupervised setting. For Squirrel and Chameleon"}, {"title": "Commute Graph Neural Networks", "authors": ["Wei Zhuo", "Guang Tan"], "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such\nas asymmetrical shortest paths typically found in digraphs. Recognizing this\ngap, we introduce Commute Graph Neural Networks (CGNN), an approach that\nseamlessly integrates node-wise commute time into the message passing scheme.\nThe cornerstone of CGNN is an efficient method for computing commute time\nusing a newly formulated digraph Laplacian. Commute time information is then\nintegrated into the neighborhood aggregation process, with neighbor contributions\nweighted according to their respective commute time to the central node in each\nlayer. It enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs.", "sections": [{"title": "Introduction", "content": "Directed graphs (digraphs) are widely employed to model relational structures in diverse domains,\nsuch as social networks (Cross et al., 2001) and recommendation systems (Qiu et al., 2020). Recently,\nthe advances of graph neural networks (GNNs) have inspired various attempts to adopt GNNs for\nanalyzing digraphs (Tong et al., 2020a,b, 2021; Zhang et al., 2021; Rossi et al., 2023; Geisler et al.,\n2023). The essence of GNN-based digraph analysis lies in utilizing GNNs to learn expressive node\nrepresentations that encode edge direction information.\nTo achieve this, modern digraph neural networks are designed to integrate edge direction information\ninto the message passing process by distinguishing between incoming and outgoing edges. This dis-\ntinction enables the central node to learn directionally discriminative information from its neighbors.\nAs illustrated in the digraph of Fig. 1, given a central node vi, a 1-layer digraph neural network can\naggregate messages from vi's incoming neighbor Um and outgoing neighbor vj, and simultaneously\ncapture edge directions by applying direction-specific aggregation functions (Rossi et al., 2023), or\nby predefining edge-specific weights (Zhang et al., 2021; Tong et al., 2020b).\nDespite the advancements, current digraph neural networks primarily capture unidirectional\u00b9 rela-\ntionships between nodes, neglecting the complexity arising from path asymmetry. For instance, a\nk-layer GNN aggregates the neighbors within the shortest path k for the central node. If the graph is\nundirected, the shortest path between any two nodes is symmetric, as shown in the undirected graph\nof Fig. 1. This symmetry simplifies the representation of node relationships, implying that if the\nSPDs from one node to two other nodes are identical, then the SPDs from these two nodes back to the\nsource node must also be the same. Conversely, such symmetry is absent in digraphs. Considering the\ndigraph in Fig. 1, the shortest paths between vi and vj are asymmetric. Therefore, although vj and Uk\n\"unidirectional' refers to relationships in digraphs where edges have a specific direction from one node to\nanother."}, {"title": "Related Work", "content": "While the Laplacian for undirected graphs has been extensively studied, the area of Laplace operator\ndigraphs remains underexplored. Chung (2005) pioneers this area by defining a normalized Laplace\noperator specifically for strongly connected directed graphs with nonnegative weights. This operator\nis expressed as \\(I = \\pi^{1/2} P \\pi^{-1/2} + \\pi^{-1/2} P^* \\pi^{1/2}\\). Key to this formulation is the use of the transition\nprobability operator P and the Perron vector \u03c0, with the operator being self-adjoint. Building\non the undirected graph Laplacian, Singh et al. (2016) adapt this concept to accommodate the\ndirected structure, focusing particularly on the in-degree matrix. They define the directed graph\nLaplacian as \\(D_{in} \u2013 A\\), where \\(D_{in} = diag (\\{d_{in}\\}^{N}_{i=1})\\) represents the in-degree matrix. Li &\n(2012) uses stationary probabilities of the Markov chain governing random walks on digraphs to\ndefine the Laplacian as \\(\\pi^{1/2} (I \u2212 P) \\pi^{-1/2}\\), which underscores the importance of random walks and their\nstationary distributions in understanding digraph dynamics. Hermitian Laplacian Furutani et al. (2020)\nconsider the edge directionality and node connectivity separately, and encode the edge direction\ninto the argument in the complex plane. Diverging from existing Laplacians, our proposed DiLap"}, {"title": "Digraph Laplacian", "content": "\\(\\Pi(D^{-1} \u2013 P)\\) is grounded in graph signal processing principles, conceptualized as the divergence of\na signal's gradient on the digraph. It encompasses the degree matrix D to preserve local connectivity,\nthe transition matrix P to maintain the graph's directed structure, and the diagonalized Perron vector\nII, capturing critical global graph attributes such as node structural importance, global connectivity,\nand expected reachability (Chung, 1997)."}, {"title": "Digraph Neural Networks", "content": "To effectively capture the directed structure with GNNs, spectral-based methods (Zhang et al.,\n2021; Tong et al., 2020a,b) have been proposed to preserve the underlying spectral properties of the\ndigraph by performing spectral analysis based on the digraph Laplacian proposed by (Chung, 2005).\nMagNet (Zhang et al., 2021) utilizes magnetic Laplacian to derive a complex-valued Hermitian matrix\nto encode the asymmetric nature of digraphs. Spatial GNNs also offer a natural approach to capturing\ndirected structures. For instance, GraphSAGE (Hamilton et al., 2017) allows for controlling the\ndirection of information flow by considering in-neighbors or out-neighbors separately. DirGNN (Rossi\net al., 2023) further extends this framework by segregating neighbor aggregation according to edge\ndirections, offering a more refined method to handle the directed nature of graphs."}, {"title": "Random Walk Distance and GNNS", "content": "We start by establishing the notations. We then show that message passing based GNNs naturally\ncapture the concept of hitting time during information propagation across the graph, due to the\nunidirectional nature of the neighborhood aggregation. Subsequently, we argue for the significance\nof commute time, highlighting it as a more compact measure of mutual node-wise interactions in\nrandom walks.\nNotations Consider G = (V, E, X) as an unweighted digraph comprising N nodes, where V =\n\\(\\{v_i\\}_{i=1}^{N}\\) is the node set, \\(E \\subseteq (V \u00d7 V)\\) is the edge set, \\(X \u2208 R^{N\u00d7d}\\) is the node feature matrix and\n\\(Y = \\{y_i\\}_{i=1}^{N}\\) is the set of labels for V. Let \\(A \u2208 R^{N\u00d7N}\\) be the adjacency matrix and\n\\(D = diag(d_1,..., d_N) \u2208 R^{N\u00d7N}\\) be the degree matrix of A, where \\(d_i = \\Sigma_{vj\u2208V} A_{ij}\\) is the out-\ndegree of \\(v_i\\). Let \\(\\tilde{A} = A + I\\) and \\(\\tilde{D} = D + I\\) denote the adjacency and degree matrix with self-loops,\nrespectively. The transition probability matrix of the Markov chain associated with random walks on\nG is defined as \\(P = D^{\u22121}A\\), where \\(P_{ij} = A_{ij}/deg(v_i)\\) is the probability of a 1-step random walk\nstarting from \\(v_i\\) to \\(v_j\\). Graph Laplacian formalized as \\(L = D \u2013 A\\) is defined on the undirected graph\nwhose adjacency matrix is symmetric. The symmetrically normalized Laplacian with self-loops (Wu\net al., 2019) is defined as \\(\\hat{L} = \\tilde{D}^{\u22121/2}\\tilde{L}\\tilde{D}^{\u22121/2}\\), where \\(\\tilde{L} = \\tilde{D} \u2013 \\tilde{A}\\)."}, {"title": "Digraph Neural Networks", "content": "DirGNN (Rossi et al., 2023) is a general framework that generalizes\nthe message passing paradigm to digraphs by adapting to the directionality of edges. It involves\nseparate aggregation processes for incoming and outgoing neighbors of each node as follows:\n\\(m_{i,in}^{(l)} = Agg_{in}^{(l)}(\\{h_j^{(l-1)} : v_j \u2208 N_{in}\\}\\\\)\n\\(m_{i,out}^{(l)} = Agg_{out}^{(l)}(\\{h_j^{(l-1)} : v_j\u2208 N_{out}\\}\\\\)                                                                           (1)\n\\(h_i^{(l)} = Comb^{(l)}(h_i^{(l-1)}, m_{i,in}^{(l)}, m_{i,out}^{(l)}\\)\nwhere \\(N_{in}\\) and \\(N_{out}\\) are respectively incoming and outgoing neighbors of \\(v_i\\). \\(Agg_{in}^{(l)}(\u00b7)\\) and \\(Agg_{out}^{(l)}(\u00b7)\\)\nare specialized aggregation functions of \\(N_{in}\\) and \\(N_{out}\\) at layer l, used to encode the directional\ncharacteristics of the edges connected to \\(v_i\\)."}, {"title": "Can GNNS Capture Random Walk Distance?", "content": "In the context of random walks on a digraph, hitting time and commute time, collectively referred\nto as random walk distances, serve as key metrics for assessing node connectivity and interaction\nstrength. Hitting time \\(h(v_i, v_j)\\) is the expected number of steps a random walk takes to reach a\nspecific target node \\(v_j\\) for the first time, starting from a given source node \\(v_i\\). Commute time \\(c(v_i, v_j)\\)\nis the expected number of steps required for a random walk to start at \\(v_i\\), reach \\(v_j\\), and come back. A"}, {"title": "Commute Time Computation", "content": "Based on the standard Markov chain theory, a useful tool to study random walk distances is the\nfundamental matrix (Aldous & Fill, 2002). We first establish the following assumptions required to\nsupport the theorem.\nAssumption 3.1. The digraph G is irreducible and aperiodic.\nThese two properties pertain to the Markov chain's stationary probability distribution \u03c0 (i.e., Perron\nvector) corresponding to the given graph. Irreducibility ensures that it is possible to reach any node\n(state) from any other node, preventing \u03c0 from converging to 0. Aperiodicity ensures that the Markov\nchain does not get trapped in cycles of a fixed length, thus guaranteeing the existence of a unique \u03c0.\nExistence and uniqueness of \u03c0 facilitate deterministic analysis and computation. For a more intuitive\nunderstanding of the assumptions, we give the sufficient conditions of digraph under the irreducibility\nand aperiodicity assumptions.\nProposition 1. A strongly connected digraph, in which a directed path exists between every pair of\nvertices, is irreducible. A digraph with self-loops in each node is aperiodic."}, {"title": "Commute Graph Neural Networks", "content": "In this section, we present Commute Graph Neural Networks (CGNN) to encode the commute time\ninformation into message passing. We first establish the relationship between random walk distances\nand the digraph Laplacian."}, {"title": "Digraph Laplacian (DiLap)", "content": "Contrary to the traditional graph Laplacian, typically defined as a symmetric positive semi-definite\nmatrix derived from the symmetric adjacency matrix, our proposed DiLap is built upon the transition\nmatrix to preserve the directed structure. Specifically, the classical graph Laplacian \\(L = D \u2013 A\\) is\ninterpreted as the divergence of the gradient of a signal on an undirected graph (Shuman et al., 2013;\nHamilton, 2020): given a graph signal \\(s \u2208 R^N\\), \\((Ls)_i = \\Sigma_{j\u03b5N_i} A_{ij} (s_i \u2013 s_j)\\). Intuitively, graph\nLaplacian corresponds to the difference operator on the signal s, and acts as a node-wise measure\nof local smoothness. In line with this conceptual foundation, we generalize the graph Laplacian to\ndigraphs by defining DiLap T:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j) = ((D^{-1} \u2013 P)s)_i\\), we have \\(T = D^{\u22121} \u2013 P\\),                (5)\nwhere \\(N_{out}\\) is the set of \\(v_i\\)'s out-neighbors, and \\(P_{ij} = \\frac{A_{ij}}{d_{out}}\\) with \\(d_{out}\\) being the out-degree of \\(v_i\\).\nThe detailed derivation of T is included in Appendix A.1. Eq. (5) illustrates that DiLap T acts as\nthe divergence of the gradient of a given signal on a digraph, and can be used to measure the local\nsmoothness in a digraph.\nConsidering the Laplacian operator's function in measuring the smoothness of a signal across the\nentire graph, it is crucial to assign greater weights to nodes of higher structural importance. It is\nbecause the smoothness at nodes central to the graph structure should have a more pronounced impact\non the global smoothness measurement. Thus, we further define the Weighted DiLap T:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} \\pi_i P_{ij} (s_i \u2013 s_j) = (\\Pi(D^{-1} \u2013 P)s)_i\\), we have \\(\\bar{T} = \\Pi(D^{\u22121} \u2013 P)\\).                (6)\nHere we utilize the i-th element of the Perron vector \u03c0 to quantify the structural importance of\n\\(v_i\\), reflecting its eigenvector centrality. This is based on the principle that a node's reachability is\ndirectly proportional to its corresponding value in the Perron vector (Xu et al., 2018). Therefore, \u03c0\neffectively indicates the centrality and influence over the long term in the graph. Perron-Frobenius\nTheorem (Horn & Johnson, 2012) establishes that \u03c0 satisfies \\(\\Sigma_i \\pi(i) = 1\\), is strictly positive, and\nconverges to the left eigenvector of the dominant eigenvalue of P."}, {"title": "Similarity-based Graph Rewiring", "content": "Both the fundamental matrix defined in Eq. (3) and Weighted DiLap requires Assumption 3.1 to\nensure the existence and uniqueness of the Perron vector \u03c0, conditions that are not universally met\nin general graphs. To fulfill the irreducibility and aperiodicity assumptions, Tong et al. (2020a)\nintroduce a teleporting probability uniformly distributed across all nodes. This method, inspired\nby PageRank (Page et al., 1999), amends the transition matrix to \\(P_{pr} = \u03b3P + (1 \u2212 \u03b3)\\frac{1}{N}\\), where\n\u03b3\u2208 (0,1). \\(P_{pr}\\) allows for the possibility that a random walker may choose a non-neighbor node for\nthe next step with a probability of \\(\\frac{1\u2212\u03b3}{N}\\). This adjustment ensures that \\(P_{pr}\\) is irreducible and aperiodic,\nso it has a unique \u03c0. However, this approach leads to a complete graph represented by a dense matrix\n\\(P_{pr}\\), posing significant challenges for subsequent computational processes.\nRather than employing \\(P_{pr}\\) as the transition matrix, we introduce a graph rewiring method based on\nfeature similarity to make a given graph irreducible, while maintaining the sparsity. As outlined in\nProposition 1, to transform the digraph into a strongly connected structure, it is essential that each\nnode possesses a directed path to every other node. To this end, we initially construct a simple and\nirreducible graph G' with all N nodes, then add all edges from G' the original digraph G, thereby\nensuring G's irreducibility. The construction of G' begins with the calculation of the mean of node\nfeatures as the anchor vector a. Then we determine the similarity between each node and the anchor,\nsort the similarity values, and return the sorted node indices, denoted as \\(S \u2208 R^N\\):\n\\(a = \\frac{\\Sigma_i X_i}{N}\\),        \\(s_i = cos(a, X_i)\\),        \\(S = argsort(\\{s_i\\}_{i=1}^{N})\\)                            (7)\nwhere \\(cos(a, X_i)\\) is the cosine similarity between node features of \\(v_i\\) and a, and argsort(\u00b7) yields\nthe indices of nodes that sort similarity values \\(\\{s_i\\}_{i=1}^{N}\\). We then connect the nodes one by one with\nundirected (bidirectional) edges following the order in S to construct G', as shown in Fig. 2. Given\nthat G' is strongly connected, adding all its edges into G results in a strongly connected digraph G,\nwhich is irreducible. To achieve aperiodicity, self-loops are further added to G.\nThis rewiring approach satisfies Assumption 3.1 and maintains graph sparsity. Additionally, adding\nedges between nodes with similar features only minimally alters the overall semantics of the original\ngraph. Based on G and its corresponding P and I, we have the deterministic Weighted DiLap T."}, {"title": "From DiLap to Commute Time", "content": "Given the Weighted DiLap T, we can unify the commute time information into the message passing\nby building the connection between T and the fundamental matrix Z:\nLemma 4.1. Given a rewired graph G, the Weighted DiLap is defined as \\(\\bar{T} = \\Pi(D^{\u22121} \u2013 P)\\). Then\nthe fundamental matrix Z of \u011e can be solved by:\n\\(Z = \\bar{\\Pi}^{-1/2}R^{\\dagger} \\bar{\\Pi}^{1/2}\\),                (8)\nwhere \\(R = \\bar{\\Pi}^{-1/2}\\bar{\\Pi}^{-1/2} \u2013 D^{\u22121} + I\\) and the superscript \u2020 means Moore\u2013Penrose pseudoinverse of\nthe matrix.\nThe proof is given in Appendix A.2. Leveraging Lemma 4.1, we can further compute the hitting\ntimes and commute times in terms of T with the following theorem."}, {"title": "CGNN", "content": "\\(C \u2208 R^{N\u00d7N}\\) quantifies the strength of mutual relations between node pairs in the random walk\ncontext. Notably, smaller values in C correspond to stronger mutual reachability, indicating stronger\nrelations between node pairs. Thus, C is a positive symmetric matrix, and the commute-time-based\nnode proximity matrix can be expressed as \\(C = exp(-C)\\). Since the directed adjacency matrix A\nrepresents the outgoing edges of each node, \\(A^T\\) therefore accounts for all incoming edges. Then\nwe have \\(C^{out} = A \u2299 C\\) and \\(C^{in} = A^T \u2299 C\\) represent the proximity between adjacent nodes under\noutgoing and incoming edges, respectively. We further perform row-wise max-normalization on \\(C^{out}\\)\nand \\(C^{in}\\) to rescale the maximum value in each row to 1. Given the original graph G as input, we\ndefine the l-th layer of CGNN as:\n\\(m_{i,in}^{(l)} = \\bar{C}_{in}\\) \u00b7 \\(\\{h_j^{(l-1)}\\} : v_j \u2208 N_{in}\\\\}\\\\)\n\\(m_{i,out}^{(l)} = \\bar{C}_{out}\\) \u00b7 \\(\\{h_j^{(l-1)}\\} : v_j\u2208 N_{out}\\}\\\\}\\\\)                                                     (11)\n\\(h_i^{(l)} = Comb^{(l)}(h_i^{(l-1)}, m_{i,in}^{(l)}, m_{i,out}^{(l)}\\)\nwhere \\(Agg_{in}^{(l)}(\u00b7)\\) and \\(Agg_{out}^{(l)}(\u00b7)\\) are mean aggregation functions with different feature transformation\nweights, and Comb(l)(\u00b7) is a mean operator. Within each layer, the influence of \\(v_j\\) on the central\nnode \\(v_i\\) is modulated by the commute-time-based proximity C based on the edge directionality. We\npresent the pseudocode and complexity of CGNN in Algorithm 1."}, {"title": "Experiments", "content": "We conduct node classification experiments on five digraph datasets. Experimental details and data\nstatistics are provided in Appendix C.1 and Appendix C.2. We provide a performance comparison\nwith 12 baselines including 1) General GNNs: GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al.,\n2018), and GraphSAGE (Hamilton et al., 2017); 2) Non-local GNNs: APPNP (Klicpera et al., 2019),\nMixHop (Abu-El-Haija et al., 2019), GPRGNN (Chien et al., 2021), and GCNII (Ming Chen et al.,\n2020); 3) Digraph NNs: DGCN (Tong et al., 2020b), DiGCN (Tong et al., 2020a), MagNet (Zhang\net al., 2021), DiGCL (Tong et al., 2021), and DirGNN (Rossi et al., 2023). For all baselines, we apply\nboth the symmetrized and asymmetric adjacency matrix for experiments. The results reported are the\nbetter of the two results."}, {"title": "Overall Results", "content": "Table 1 reports the node classification results across five digraph datasets. Our method CGNN\nachieves new state-of-the-art results on 4 out of 5 datasets, and comparable results on the remaining\none, validating the superiority of CGNN. We provide more observations as follows. Firstly, while\nnon-local GNNs have the potential to cover the commute paths between adjacent nodes by stacking\nmultiple layers, they do not consistently outperform general, shallow GNN models. It suggests that\ncoarsely aggregating all nodes in commute paths is ineffective. The reason is that deeper models\nmay aggregate excessive irrelevant information for the central node. Our goal is to encode mutual\nrelationships between adjacent nodes by considering their commute times. Aggregating all nodes\nalong the entire path introduces excessive information about other nodes unrelated to the direct\nrelationship between the target nodes. Secondly, GNNs tailored for digraphs do not seem to bring\nsubstantial gains. Our results show that with careful hyper-parameter tuning, general GNNs can\nachieve results comparable to, or even better than, those tailored for digraphs, as evidenced in the\nSquirrel, Chameleon, and AM-Photo datasets. Thirdly, CGNN achieves state-of-the-art results on\nboth homophilic and heterophilic digraph benchmarks. Notably, DirGNN also performs comparably\non heterophilic graphs (Squirrel and Chameleon), supporting the findings of Rossi et al. (2023) that\ndistinguishing edge directionality during message passing enables the central node to adaptively\nbalance information flows from both heterophilic and homophilic neighbors, effectively mitigating\nthe impact of heterophily. Moreover, CGNN, an enhanced version of DirGNN, further improves\nperformance on these graphs by effectively incorporating commute times to refine the strength of\nrelationships between nodes, enhancing model robustness under heterophily.\nTo illustrate this, we further examine the relations between commute-time-based proximity and label\nsimilarity along edges. As shown in Eq. (11), we use commute-time-based proximity C to weigh the\nneighbors during neighbor aggregation. Then we define a label similarity matrix M where \\(M_{ij} = 1\\)\nif \\(v_j \u2208 N_i\\) and \\(y_i = y_j\\); otherwise \\(M_{ij} = 0\\). Essentially, M extracts the edges connecting nodes\nwith the same classes from the adjacency matrix A. Thus a higher value of \\(||M \u2013 (A + A^T)||\\)\nindicates a more pronounced negative impact of heterophily on the model's performance. On the\nother hand, we compute \\(||M \u2013 (C^{in} + C^{out})||\\) to evaluate the efficacy of C in filtering heterophilic\ninformation."}, {"title": "Efficiency Comparsion", "content": "Fig. 3 compares the accuracy of CGNN and other baseline models with their running times. Despite\nthe additional computational load of calculating commute-time-based proximity, the results show that\nCGNN provides the best trade-off between effectiveness and efficiency. In particular, on the Squirrel\ndataset, CGNN has the third-fastest calculation speed while yielding accuracy nearly double that of\nall other methods. On AM-Photo, CGNN achieves the highest accuracy while maintaining moderate\nefficiency."}, {"title": "Component Analysis", "content": "Comparison between graph rewiring and PPR. In Section 4.2, we construct a rewired graph\nG based on feature similarity to guarantee the irreducibility and aperiodicity. On the other hand,\nIn contrast, the classic PageRank transition matrix, defined as \\(P_{pr} = yP + (1 \u2212y)\\frac{1}{N}\\), achieves\na similar objective but results in a completely connected graph \\(\\bar{G}_{pr}\\). However, this approach tends\nto overlook the sparse structure of the original graph, which may alter the semantic information in\nthe graph. Additionally, computing commute times using a dense transition matrix incurs a high\ncomputational cost. To validate the effectiveness of the rewiring approach over the PPR method, we\nconduct an experiment where G is replaced with \\(\\bar{G}_{pr}\\) in the computation of commute-time-based\nproximity. We denote this variant as \u2018CGNNppr' and the results of accuracy and efficiency are\nreported in Table 3. The findings reveal that the PPR approach is suboptimal in terms of both accuracy\nand efficiency, thereby underscoring the effectiveness of our rewiring-based approach."}, {"title": "CGNN", "content": "CGNN sym\nDirected vs. Undirected. To validate the critical role of\ndirected structures in our model, we transform all directed\nedges into undirected ones by adding their reverse coun-\nterparts. This process results in a symmetric adjacency\nmatrix, denoted as \\(A_{sym}\\). Subsequently, the commute\ntime is calculated based on the transition matrix derived\nfrom \\(A_{sym}\\). We refer this variant as \u2018CGNNsym\u2019. Fig. 4\nshows the accuracy of CGNN and on three datasets. We find that edge direction can\nsignificantly influence the prediction accuracy for our model."}, {"title": "Conclusion", "content": "In this work, we introduce the Commute Graph Neural Network (CGNN) to integrate commute\ntime into GNNs for digraphs. To this end, we propose DiLap, a new Laplacian formulation for\ndigraphs, along with a rapid computational method for determining commute times. By integrating\ncommute times into GNN message passing through neighbor weighting, CGNN effectively leverages"}, {"title": "Proofs and Derivations", "content": ""}, {"title": "Derivation of DiLap T", "content": "Since the digraph is unweighted, then \\(P_{ij} = \\frac{A_{ij}}{d_{out}}\\) and \\(A_{i,:} = P_{i,:}\\). We have:\n\\((Ts)_i = \\Sigma_{v_jEN_{out}} P_{ij} (s_i \u2013 s_j)\\\n\t\t\t\t\t\t\t= \\Sigma_{v_jEN_{out}} \\frac{1}{d_{out}} s_i \u2013 \\frac{1}{d_{out}} s_j\n\t\t\t\t\t\t\t= \\frac{1}{d_{out}} s_i \\Sigma_{v_jEN_{out}} 1 \u2013 \\Sigma_{v_jEN_{out}} \\frac{1}{d_{out}} s_j         (12)\n\t\t\t\t\t\t\t= (D^{\u22121}s)_i \u2013 \\frac{1}{d_{out}} \\Sigma_{v_jEN_{out}} A_{i,:}s\\\n\t\t\t\t\t\t\t= (D^{\u22121}s)_i \u2013 (Ps)_i\n\t\t\t\t\t\t\t= ((D^{\u22121} \u2013 P)s)_i.\nThus, DiLap T can be represented as \\(D^{-1} \u2013 P\\)."}, {"title": "Proof of Lemma 4.1", "content": "Proof. Since the transition matrix P is row-stochastic, it follows that PtJ = J. In light of Eq. (2) and\nconsidering that \u03c0 is stochastic, we have ZJ = 0nxn. Let \\(K = \\bar{\\Pi}^{-1/2}\\bar{T}\\bar{\\Pi}^{-1/2} = \\bar{\\Pi}^{1/2}(D^{-1}\u2212P)\\bar{\\Pi}^{-1/2}\\),   \n\\(J = \\bar{\\Pi}^{1/2}J\\bar{\\Pi}^{-1/2}\\), and \\(Z = \\bar{\\Pi}^{1/2}Z\\bar{\\Pi}^{\u22121/2}\\), incorporating these into Eq. (3), we have:\n\\(Z + I = (K + J \u2013 D^{\u22121} + I )^{-1}\\).           (13)\nBy post-multiplying Eq. (13) from the right by (K + J \u2013 D^{\u22121} + I), we have:\n\\(I = (Z + I)(K + J \u2013 D^{-1} + I)\\)     (14)\n\t\t\t\t\t\t\t= ZK + ZJ \u2013 ZD^{\u22121} + Z + JK + J^2 \u2013 JD^{\u22121} + J.\nSince \\(J^2 = I, IK = I(D^{\u22121} \u2013 I)\\), and ZJ = \\(\\bar{\\Pi}^{1/2}ZJ\\bar{\\Pi} = 0_{nxn}\\), Eq. (14) can be simplified to:\n\\(Z(K \u2013 D^{-1} + I) = I \u2013 J\\).                         (15)\nSimilarly, by multiplying from the left, we establish that (K \u2013 D^{\u22121} + I)Z = I \u2212 J. As \\(\\pi^{\u22121}Z = 0\\),\nwe have JZ = 0. Thus, \\(Z(K \u2013 D^{\u22121} + I)Z = Z\\). Furthermore, it can be straightforwardly inferred\nthat \\((K \u2013 D^{\u22121} + I)J = 0\\), then have \\((K \u2013 D^{\u22121} + I)Z(K \u2013 D^{\u22121} + I) = (K \u2013 D^{\u22121} + I)\\). Besides,\nconsidering the symmetry of the left part of Eq. (15), we have \\((Z(K-D^{-1}+I))^T = Z(K\u2212D\u22121+I)\\).\nSimilarly, \\(((K-D^{-1}+I)Z)^T = (K\u2212D\u22121+I)Z\\). These derivations satisfy the sufficient conditions\nfor the Moore-Penrose pseudoinverse, such that\n\\(Z = (K \u2013 D^{\u22121} + I)^{\\dagger}\\).     (16)\nFinally, recovering Z and K, which concludes the proof.\t \\(\\)"}, {"title": "SVD for", "content": "Given a matrix \\(R \u2208 R^{N\u00d7N}\\), its Moore-Penrose pseudoinverse can be directly computed with an\nSVD-based method. Specifically, we first perform truncated SVD on \\(R = \\bar{\\Pi}^{-1/2}\\bar{\\Pi}^{-1/2} \u2013 D^{\u22121} + I\u2248\nU_q\\Sigma_qV^T\\), where \\(U_q \u2208 R^{N\u00d7q}\\) and \\(V_q \u2208 R^{N\u00d7q}\\) contains the first q columns of U and V. \\(\\Sigma_q \u2208 R^{q\u00d7q}\\)\nis the diagonal matrix of q largest singular values. It is a q-rank approximation of R, which holds\nthat rank(R) = q. Then the Moore-Penrose pseudoinverse of R can be easily computed as follows:\n\\(R^{\\dagger} = V_q\\Sigma_q^{\u22121}U^T\\).    (17)\nTo leverage sparsity of R to avoid O(N\u00b3) complexity, we adopt the randomized SVD algorithm\nproposed by (Halko et al., 2011; Cai et al., 2023) to first approximate the range of the input matrix\nwith a low-rank orthonormal matrix, and then perform SVD on this smaller matrix:\n\\(\\hat{U_q}, \\hat{\\Sigma_q}, \\hat{V_q} = ApproxSVD(R, q)\\),        \\(R_{SVD}^{\\dagger} = \\hat{U_q}\\hat{\\Sigma_q}^{\u22121}\\hat{V_q}^T,\\)         (18)"}, {"title": "Pseudo Code for CGNN", "content": "Complexity Analysis The time complexity of randomized truncated SVD to compute \\(R^{\\dagger}\\) is\nO(q|E|), and the message passing iteration has the same time complexity as GraphSAGE with\nO(|E|). Therefore, the overall time complexity of CGNN is O(q|E|)."}, {"title": "Implementation Details", "content": ""}, {"title": "Experimental Settings", "content": "We evaluate the performance by node classification accuracy with standard deviation in the semi-\nsupervised setting. For Squirrel and Chameleon, we use 10 public splits (48%/32%/20% for train-\ning/validation/testing) provided by (Pei et al., 2019). For the remaining datasets, we adopt the same\nsplits as (Tong et al., 2020a, 2021), which chooses 20 nodes per class for the training set, 500 for the\nvalidation set, and allocates the rest to the test set. We conduct our experiments on 2 Intel"}]}]}