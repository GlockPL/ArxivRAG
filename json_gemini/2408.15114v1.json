{"title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries", "authors": ["Amine Ouasfi", "Adnane Boukhayma"], "abstract": "e.g. Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from sparse 3D point clouds in the absence of ground truth supervision remains a very challenging task. While recent methods rely on smoothness priors to regularize the learning, our method introduces a regularization term that leverages adversarial samples around the shape to improve the learned SDFs. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data.", "sections": [{"title": "1. Introduction", "content": "Obtaining faith-full and intelligible neural representations of the 3D world from limited and corrupted point clouds is a challenge of paramount importance, that finds applications in countless downstream computer vision and graphics tasks. While many methods rely on data priors learned from large fully labeled datasets, these priors can fail to generalize to unseen shapes especially under very sparse unoriented inputs (Chen et al., 2023a; Ouasfi & Boukhayma, 2024b). Hence, it is important to design learning frameworks that can lead to efficient and robust learning of implicit shape representations under such extreme constraints.\nIn this context, the learning strategy introduced by (Ma et al., 2021) (dubbed NeuralPull) have shown to be one of the most successful ones in learning implicit shapes from point cloud unsupervisedly. However, upon observing the behavior of the training and validation errors of this method under sparse and dense input point clouds (Figure 1), we notice that the validation error starts increasing quite early on in the training in the sparse input case, whilst the training loss keeps on decreasing. This suggests an overfitting problem evidently intensifying in the sparse setting. Qualitatively, this increase in the validation error is usually synonymous to deterioration in the extracted shape with symptoms varying between shape instances, including shape hallucinations, missing shape parts, and shape becoming progressively wavy, bumpy and noisy. In extreme cases, shapes can also break into separate components or clusters around input points. When the input is additionally noisy, these phenomena are further exacerbated.\nRecent work in the field relies on various smoothness priors (e.g. (Chen et al., 2023a; Gropp et al., 2020; Ben-Shabat et al., 2022; Ouasfi & Boukhayma, 2024c)) to regularize the implicit shape functions, and hence reduce overfitting. One side of the problem that remains underexplored however is how training data is sampled during learning, and understanding to which extent this sampling could affect performance. This is even the more an important question in our situation. In fact, while standard supervised learning uses typically data/label sample pairs, fitting implicit representations entails mapping spatial coordinates to labels or pseudo labels, where these spatial queries can be sampled uniformly or normally around the input point cloud. In the case of our baseline NeuralPull, the nearest point could sample to a spatial query is a pseudo-label approximating the unavailable nearest groundtruth surface point in the training. Hence, both inherent input point cloud noise and its sparsity represent a noise (i.e. displacement) on the perfect surface labels. This composite noise can affect both the SDF function and gradient orientation. In practice, we notice the network first produces a very smooth shape. When it tries to refine it, it tends to overfit to the noise present in the supervision signal. At this stage, further fitting on easy samples (predominant samples) means overfitting on this noise. The samples that can benefit the implicit representation can be drowned within easy samples.\nAmong literature interested in such a problem, active learning advocates sampling based on informativeness and diversity (Huang et al., 2010). New samples are queried from a pool of unlabeled data given a measure of these criteria. Informative samples are usually defined as samples that can reduce the uncertainty of a statistical model. However, several heuristics coexist as it is impossible to obtain a universal active learning strategy that is effective for any given task (Dasgupta, 2005). In our setting it is not clear what samples are the most informative for our implicit shape function and how to sample from the uncertainty regions of the implicit field. Recent work on distributionally robust optimization (DRO) (Volpi et al., 2018; Rahimian & Mehrotra, 2019) provides a mathematical framework to model uncertainty. In this framework, the loss is minimized over the worst case distribution in a neighborhood of the observed training data distribution. As a special case, adversarial training (Madry et al., 2017) uses pointwise adversaries rather than adversarial joint perturbations of the entire training set.\nInspired by this literature, we propose to use adversarial samples to regularize the learning of implicit shape representations from sparse point clouds. We build on SDF projection minimization error loss in training (See figure 2). Typically query points are pre-sampled around the input point cloud to train such a method. We augment these queries with adversarial samples during training. To ensure the diversity of these additional samples, we generate them in the vicinity of original queries within locally adapted radii. These radii modulate the adversarial samples density in concordance with the input point cloud density, thus allowing us to adapt to the local specificities of the input during the neural fitting. Our adversarial training strategy, focuses on samples that can still benefit the network, which prevents the aforementioned overfitting while refining the implicit representation.\nTo test our idea, we devise experiments on real and synthetic reconstruction benchmarks, including objects, articulated shapes and large scenes. Our method outperforms the baseline as well as the most related competition both quantitatively and qualitatively. We notice that our adversarial loss helps our model most in places where shape prediction is the hardest and most ambiguous, such as fine and detailed structures and body extremities. Experiments on a dense reconstruction setting show that our method can be useful in this setup as well. Finally, thanks to our method, and as illustrated in Figure 1, validation stabilizes and plateaus at convergence unlike our baseline, which makes it easier for us to decide the evaluation model epoch, given that evaluation measurements are normally unavailable in unsupervised settings."}, {"title": "2. Related work", "content": "Classical shape modelling from point cloud includes combinatorial methods where the shape is defined through an input point cloud based space partitioning, through e.g. alpha shapes (Bernardini et al., 1999) Voronoi diagrams (Amenta et al., 2001) or triangulation (Cazals & Giesen, 2006; Liu et al., 2020; Rakotosaona et al., 2021). Differently, the input samples can be used to define an implicit function whose zero level set represents the target shape, using global smoothing priors (Williams et al., 2022; Lin et al., 2022; Williams et al., 2021) e.g. radial basis function (Carr et al., 2001) and Gaussian kernel fitting (Sch\u00f6lkopf et al., 2004), local smoothing priors such as moving least squares (Mercier et al., 2022; Guennebaud & Gross, 2007; Kolluri, 2008; Liu et al., 2021), or by solving a boundary conditioned Poisson equation (Kazhdan & Hoppe, 2013). The recent literature proposes to parameterise these implicit functions with deep neural networks and learn their parameters with gradient descent, either in a supervised (e.g. (Ouasfi & Boukhayma, 2024a; 2022; Boulch & Marlet, 2022; Peng et al., 2020; Lionar et al., 2021; Peng et al., 2021)) or unsupervised manner. These implicit neural representations (Mescheder et al., 2019; Park et al., 2019) overcome many of the limitations of explicit ones (e.g. meshes (Wang et al., 2018; Kato et al., 2018; Jena et al., 2022) and point clouds (Fan et al., 2017; Aliev et al., 2020; Kerbl et al., 2023)) in modelling shape, radiance and light fields (e.g. (Mildenhall et al., 2020; Yariv et al., 2021; Wang et al., 2021; Jain et al., 2022; Chan et al., 2022; Li et al., 2023a;b; Jena et al., 2024; Younes et al., 2024)), as they allow to represent functions with arbitrary topologies at virtually infinite resolution.\nWe are interested in unsupervised implicit neural shape learning. In this scenario, an MLP is typically fitted to the input point cloud without extra priors or information. Regularizations can compensate for the lack of supervision. For instance, (Gropp et al., 2020) introduced a spatial gradient constraint based on the Eikonal equation. (Ben-Shabat et al., 2022) introduces a spatial divergence constraint. (Liu et al.,"}, {"title": "3. Method", "content": "Given a noisy, sparse unoriented point cloud \\(P \\subset \\mathbb{R}^{3 \\times Np}\\), our objective is to obtain a corresponding 3D shape reconstruction, i.e. the shape surface \\(S\\) that best explains the observation \\(P\\). In other terms, the input point cloud elements should approximate noised samples from \\(S\\).\nIn order to achieve this goal, we learn a shape function \\(f\\) parameterised with an MLP \\(f_{\\theta}\\). The function represents the implicit signed distance field relative to the target shape \\(S\\). That is, for a query euclidean space location \\(q \\in \\mathbb{R}^{3}\\), \\(f(q) := s \\cdot min_{v \\in S} ||v - q||_{2}\\), where \\(s := 1\\) if \\(q\\) is inside\n\\[\n\\hat{S} = \\{q \\in \\mathbb{R}^3 | f_{\\theta} (q) = 0\\}.\n\\]\nPractically, an explicit triangle mesh for \\(\\hat{S}\\) can be obtained through the Marching Cubes algorithm (Lorensen & Cline, 1987), while querying neural network \\(f_{\\theta}\\). We note also that \\(S\\) can be rendered through ray marching (Hart, 1996) through the SDF field inferred by \\(f_{\\theta}\\).\nSeveral state-of-the-art reconstruction from point cloud methods (e.g. (Chen et al., 2023a; Ma et al., 2022b;a; Chen et al., 2022; Ma et al., 2021)), including the state-of-the-art unsupervised reconstruction from sparse point cloud method (Chen et al., 2023a), build on the neural SDF training procedure introduced in (Ma et al., 2021) named NeuralPull. The latter is inspired by the observation that the distance field guided projection operator \\(q \\mapsto q - f(q) \\cdot \\frac{\\nabla f(q)}{|| \\nabla f(q)||}\\) ((Chibane et al., 2020; Perry & Frisken, 2001; Wolter, 1993; Zhao et al., 2021)) yields the nearest surface point when applied near the surface, where \\(\\nabla f\\) is the spatial gradient of \\(f\\).\nIn practice, query points \\(q \\in Q\\) are sampled around the input point cloud \\(P\\), specifically from normal distributions centered at input samples \\(\\{p\\}\\), with locally defined standard deviations \\(\\{\\sigma_p\\}\\):\n\\[\nQ := \\cup_{p \\in P} \\{ q \\sim \\mathcal{N} (p, \\sigma_p I_3) \\},\n\\]"}, {"title": "3.2. Local adversarial queries for the few shot setting", "content": "As introduced in the first section, input point cloud noise and sparsity are akin to noisy labels for query points q. Hence, training through standard ERM under sparse input point clouds P leads to an overfitting on this noise (Figure 1), i.e. useful information carried out by query samples decreasing during training thus leading to poor convergence. Hence, differently from existing work in the field of learning based reconstruction from point cloud, we propose to focus on the manner in which query points q are sampled at training, as we hypothesise that there could be a different sampling strategy from the one proposed in NeuralPull (i.e. Equation 2) that can lead to better results. This hypothesis is backed by literature showing that hard sample mining can lead to improved generalization and reduced over-fitting (Xie et al., 2020; Chawla, 2010; Fern\u00e1ndez-Delgado et al., 2014; Krawczyk, 2016; Shrivastava et al., 2016). Intuitively, exposing the network to the worst cases in training is likely to make it more robust and less specialized on the more common easy queries.\nWe explore a different procedure from standard ERM (Equation 4). Ideally, we wish to optimize \\(\\theta\\) under the worst distribution \\(Q'\\) of query points \\(\\{q\\}\\) in terms of our objective function, meaning:\n\\[\nmin_{\\theta} \\max_{Q'} \\mathbb{E}_{q \\sim Q'} L(\\theta,q).\n\\]\nSuch a training procedure is akin to a distributionally robust optimization (Sagawa* et al., 2020; Rahimian & Mehrotra, 2019) which is hard to achieve in essence. It was shown however that a special more attainable case of the latter consists in harnessing hard samples locally (Staib & Jegelka, 2017), that is looking for hard samples in the vicinity of the original empirical samples:\n\\[\nmin_{\\theta} \\mathbb{E}_{q \\sim Q} \\max_{\\delta, ||\\delta||_2 \\le \\rho} L(\\theta, q + \\delta).\n\\]\nLet us consider optimum perturbations \\(\\hat{\\delta}\\). Using a first order Taylor expansion on loss \\(L(\\theta, q + \\delta)\\), we can write:\n\\[\n\\hat{\\delta} = arg \\max_{||\\delta||_2 \\le \\rho} L(\\theta, q + \\delta),\n\\approx arg \\max_{||\\delta||_2 \\le \\rho} L(\\theta, q) + \\delta^\\intercal \\nabla_q L(\\theta,q),\n\\approx arg \\max_{||\\delta||_2 \\le \\rho} \\delta^\\intercal \\nabla_q L(\\theta,q).\n\\]\nLeveraging this approximation, we can obtain the optimum value \\(\\hat{\\delta}\\) as the solution to a classical dual norm problem, by using the equality case of Cauchy-Schwarz inequality applied to the scalar product \\(\\delta^\\intercal \\nabla_q L(\\theta, q)\\) for \\(\\delta\\) in a closed ball of radius \\(\\rho\\) and center 0:\n\\[\n\\hat{\\delta} = \\rho \\frac{\\nabla_q L(\\theta,q)}{|| \\nabla_q L(\\theta,q)||_{2}}.\n\\]\nwhere gradients \\(\\nabla_qL\\) can be computed efficiently through automatic differentiation in a deep-learning framework (e.g. PyTorch (Paszke et al., 2019))."}, {"title": "4. Results", "content": "To evaluate our method, we assess our ability to learn implicit shape representations given sparse and noisy point clouds. We use datasets from standard reconstruction benchmarks. These datasets highlight a variety of challenges of fitting coordinate based MLPs to sparse data as well as reconstruction more broadly. Following the literature, we evaluate our method by measuring the accuracy of 3D explicit shape models extracted after convergence from our MLPs. We compare quantitatively and qualitatively to the the state-of-the-art in our problem setting, i.e. unsupervised reconstruction from unoriented point cloud, including methods designed for generic point cloud densities and methods dedicated to the sparse setting. For the former, we compare to fully implicit deep learning methods such as NP (Ma et al., 2021), SAP (Peng et al., 2021), DIGS (Ben-Shabat et al., 2022), in addition to hybrid methods combining implicit and grid based representations such as OG-INR (Koneputugodage et al., 2023) and GP (GridPull) (Chen et al., 2023b). When it comes to methods dedicated to the sparse setting we compare to NTPS (Chen et al., 2023a) which is the closest method to ours as it focuses specifically on the sparse input case. We additionally compare to NDrop (Boulch et al., 2021). We show results for NSpline (Williams et al., 2021) even though it requires normals. We also compare to classical Poisson Reconstruction SPSR (Kazhdan & Hoppe, 2013). We note also that comparisons to NP (NeuralPull, our baseline) also serves additionally as an ablation of our adversarial loss through out our experiments. For comprehensive evaluation, we also include comparisons to supervised methods including state-of-the-art feed-forward generalizable methods, namely POCO (Boulch & Marlet, 2022), CONet (Peng et al., 2020) and NKSR (Huang et al., 2023), alongside the finetuning method SAC (Tang et al., 2021) and the prior-based optimization method dedicated to sparse inputs On-Surf (Ma et al., 2022a). Unless stated differently, we use the publicly available official implementations of existing methods. For sparse inputs, we experimented with point clouds of size Np = 1024."}, {"title": "4.1. Metrics", "content": "Following seminal work, we evaluate our method and the competition w.r.t. the ground truth using standard metrics for the 3D reconstruction task. Namely, the L1 Chamfer Distance CD1 (\\(\\times10^{2}\\)), L2 Chamfer Distance CD2 (\\(\\times10^{2}\\)), the euclidean distance based F-Score (FS) when ground truth points are available, and finally Normal Consistency (NC) when ground truth normals are available. We detail the expressions of these metrics in the appendix."}, {"title": "4.2. Datasets and input definitions", "content": "ShapeNet (Chang et al., 2015) consists of various instances of 13 different synthetic 3D object classes. We follow the train/test splits defined in (Williams et al., 2021). We generate noisy input point clouds by sampling 1024 points from the meshes and adding Gaussian noise of standard deviation 0.005 following the literature (e.g. (Boulch & Marlet, 2022; Peng et al., 2020)). For brevity we show results on classes Tables, Chairs and Lamps.\nFaust (Bogo et al., 2014) consists of real scans of 10 human body identities in 10 different poses. We sample sets of 1024 points from the scans as inputs.\n3D Scene (Zhou & Koltun, 2013) contains large scale complex real world scenes obtained with a handheld commodity range sensor. We follow (Chen et al., 2023a; Jiang et al., 2020; Ma et al., 2021) and sample our input point clouds with a sparse density of 100 per m\u00b2, and we report performance similarly for scenes Burghers, Copyroom, Lounge, Stonewall and Totempole.\nSurface Reconstruction Benchmark (SRB) (Williams et al., 2019) consists of five object scans, each with different challenges such as complex topology, high level of detail, missing data and varying feature scales. We sample 1024 points from the scans for the sparse input experiment, and we also experiment using the dense inputs."}, {"title": "4.3. Implementation details", "content": "Our MLP (\\(f_{\\theta}\\)) follows the architecture in NP (Ma et al., 2021). We train for \\(N_{it} = 40000\\) iterations using the Adam optimizer. We use batches of size \\(N_b = 5000\\). Following NP, we set K = 51 for estimating local standard deviations \\(\\sigma_p\\). We train on a NVIDIA RTX A6000 GPU. Our method takes 8 minutes in average to converge for a 1024 sized input point cloud. In the interest of practicality and fairness in our comparisons, we decide the evaluation epoch for all the methods for which we generated results (including our main baseline) in the same way: we chose the best epoch for each method in terms of chamfer distance between the reconstruction and the input point cloud."}, {"title": "4.4. Object level reconstruction", "content": "We perform reconstruction of ShapeNet (Chang et al., 2015) objects from sparse and noisy point clouds. Table 1 and Figure 3 show respectively a numerical and qualitative comparison to the competition. We outperform the competition across all metrics, as witnessed by the visual superiority of our reconstructions. We manage to recover fine structures and details with more fidelity. Although it obtains overall good coarse reconstructions, the thin plate spline smoothing"}, {"title": "4.6. Real scene level reconstruction", "content": "Following (Chen et al., 2023a), we report reconstruction results on the 3D Scene ((Zhou & Koltun, 2013)) data from spatially sparse point clouds. Table 3 summarizes numerical results. We compiled results for methods NTPS, NP, SAP, NDrop and NSpline as reported in state-of-the-art method NTPS. We outperform the competition is this benchmark thanks to our loss, as our baseline NP displays more blatant failures in this large scale sparse setup. Figure 5 shows qualitative comparisons to our baseline NP and SPSR. Red boxes highlight areas where our method displays particularly better details and fidelity in the reconstruction."}, {"title": "4.7. Varying the point cloud density", "content": "We use the SRB (Williams et al., 2019) benchmark to assess the behavior of our method across different point cloud densities. Table 4 indicates comparative results under both 1024 sized and dense input point clouds. We compiled results for the competition from OG-INR in the dense setting. We outperform our competition in the sparse case, and we perform on par with the state-of-the-art in the dense case. Our improvement w.r.t. our baseline (NP) is substantial for both sparse and dense inputs. This can be seen visually in Figure 6, where we show reconstructions for both sparse and dense cases. Notice how we recover better topologies in the sparse case and improved and more accurate details in the dense case, as pinpointed by the red boxes. These results showcase the utility and benefit of our contribution even in the dense setting. We note that SPSR becomes a very viable contestant qualitatively in the dense setting."}, {"title": "5. Ablation studies", "content": "The ablation of our main contribution is present throughout all Tables and Figures. In fact while we use the combined loss in Equation 9, our baseline (i.e. NP) uses solely the query projection loss in Equation 3. The improvement brought by our additional loss is visible across real/synthetic, scenes/objects, sparse/dense point clouds.\nPerturbation radii We perform an ablation of using local vs. global radii \\(\\rho\\) (Equation 8) and the choice of value of local radii \\(\\rho_q\\) in Table 5. Results show that using local radii (\\(\\rho_q\\)) is a superior strategy as it intuitively allows for"}, {"title": "6. Limitations", "content": "As can be seen in e.g. Figure 3, even though we improve on our baseline, we still face difficulties in learning very thin and convoluted shape details, such as chair legs and slats. Although our few-shot problem is inherently challenging and can be under-constrained in many cases, we still believe there is room for improvement for our proposed strategy in this setting. For instance, one element that could be a source of hindrance to our accuracy is the difficulty of balancing empirical risk minimization and adversarial learning. In this work, we used an off-the-shelf self-trained loss weighting strategy, and we would like to address this challenge further as part of our future work."}, {"title": "7. Conclusion", "content": "We explored in this work a novel idea for regularizing implicit shape representation learning from sparse unoriented point clouds. We showed that harnessing adversarial samples locally in training can lead to numerous desirable outcomes, including superior results, reduced over fitting and easier evaluation model selection."}, {"title": "Impact statement", "content": "This paper presents work whose goal is to advance the fields of Machine Learning and 3D Computer Vision, specifically implicit neural shape representation learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Additional visualizations", "content": "Figures 8,9,10 show more multi-view qualitative comparisons to our baseline NP."}, {"title": "B. Metrics", "content": "Following the definitions from (Boulch & Marlet, 2022) and (Williams et al., 2019), we present here the formal definitions for the metrics that we use for evaluation in the main submission. We denote by S and \\(\\hat{S}\\) the ground truth and predicted mesh respectively. All metrics are approximated with 100k samples from the groundtruth mesh S and reconstruction \\(\\hat{S}\\).\nThe L\u2081 Chamfer distance is based on the two-ways nearest neighbor distance:\n\\[\nCD1 = \\frac{1}{2|S|} \\sum_{v \\in S} min_{\\hat{v} \\in \\hat{S}} || v - \\hat{v} ||_2 +  \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} min_{v \\in S} || \\hat{v} - v ||_2.\n\\]"}, {"title": "Chamfer Distance (CD2)", "content": "The L\u2082 Chamfer distance is based on the two-ways nearest neighbor squared distance:\n\\[\nCD2 = \\frac{1}{2|S|} \\sum_{v \\in S} min_{\\hat{v} \\in \\hat{S}} || v - \\hat{v} ||_2^2 +  \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} min_{v \\in S} || \\hat{v} - v ||_2^2.\n\\]"}, {"title": "F-Score (FS)", "content": "For a given threshold \\(\\tau\\), the F-score between the meshes S and \\(\\hat{S}\\) is defined as:\n\\[\nFS (\\tau, S, \\hat{S}) = \\frac{2 \\cdot \\text{Recall} \\cdot \\text{Precision}}{\\text{Recall} + \\text{Precision}},\n\\]\nwhere\n\\[\n\\text{Recall} (\\tau, S, \\hat{S}) = |\\{v \\in S, \\text{s.t.} min_{\\hat{v} \\in \\hat{S}} ||v - \\hat{v}||_2 \\le \\tau\\}|,\n\\]\n\\[\n\\text{Precision} (\\tau, S, \\hat{S}) = |\\{\\hat{v} \\in \\hat{S}, \\text{s.t.} min_{v \\in S} ||v - \\hat{v}||_2 \\le \\tau\\}|.\n\\]\nFollowing (Mescheder et al., 2019) and (Peng et al., 2020), we set \\(\\tau\\) to 0.01."}, {"title": "Normal consistency (NC)", "content": "We denote here by \\(n_v\\) the normal at a point \\(v\\) in \\(S\\). The normal consistency between two meshes \\(S\\) and \\(\\hat{S}\\) is defined as:\n\\[\nNC = \\frac{1}{2|S|} \\sum_{v \\in S} n_v \\cdot n_{\\text{closest}(v,\\hat{S})} + \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} n_{\\hat{v}} \\cdot n_{\\text{closest}(\\hat{v},S)},\n\\]\nwhere\n\\[\n\\text{closest}(v, \\hat{S}) = argmin_{\\hat{v} \\in \\hat{S}} || v - \\hat{v} ||_2.\n\\]"}]}