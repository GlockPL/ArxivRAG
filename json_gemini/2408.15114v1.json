{"title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries", "authors": ["Amine Ouasfi", "Adnane Boukhayma"], "abstract": "e.g. Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from sparse 3D point clouds in the absence of ground truth supervision remains a very challenging task. While recent methods rely on smoothness priors to regularize the learning, our method introduces a regularization term that leverages adversarial samples around the shape to improve the learned SDFs. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data.", "sections": [{"title": "1. Introduction", "content": "Obtaining faith-full and intelligible neural representations of the 3D world from limited and corrupted point clouds is a challenge of paramount importance, that finds applications in countless downstream computer vision and graphics tasks. While many methods rely on data priors learned from large fully labeled datasets, these priors can fail to generalize to unseen shapes especially under very sparse unoriented inputs (Chen et al., 2023a; Ouasfi & Boukhayma, 2024b). Hence, it is important to design learning frameworks that can lead to efficient and robust learning of implicit shape representations under such extreme constraints.\nIn this context, the learning strategy introduced by (Ma et al., 2021) (dubbed NeuralPull) have shown to be one of the most successful ones in learning implicit shapes from point cloud unsupervisedly. However, upon observing the behavior of the training and validation errors of this method under sparse and dense input point clouds (Figure 1), we notice that the validation error starts increasing quite early on in the training in the sparse input case, whilst the training loss keeps on decreasing. This suggests an overfitting problem evidently intensifying in the sparse setting. Qualitatively, this increase in the validation error is usually synonymous to deterioration in the extracted shape with symptoms varying between shape instances, including shape hallucinations, missing shape parts, and shape becoming progressively wavy, bumpy and noisy. In extreme cases, shapes can also break into separate components or clusters around input points. When the input is additionally noisy, these phenomena are further exacerbated.\nRecent work in the field relies on various smoothness priors (e.g. (Chen et al., 2023a; Gropp et al., 2020; Ben-Shabat et al., 2022; Ouasfi & Boukhayma, 2024c)) to regularize the implicit shape functions, and hence reduce overfitting. One side of the problem that remains underexplored however is how training data is sampled during learning, and understanding to which extent this sampling could affect performance. This is even the more an important question in our situation. In fact, while standard supervised learning uses typically data/label sample pairs, fitting implicit representations entails mapping spatial coordinates to labels or pseudo labels, where these spatial queries can be sampled uniformly or normally around the input point cloud. In the case of our baseline NeuralPull, the nearest point could sample to a spatial query is a pseudo-label approximating the unavailable nearest groundtruth surface point in the training. Hence, both inherent input point cloud noise and its sparsity represent a noise (i.e. displacement) on the perfect surface labels. This composite noise can affect both the SDF function and gradient orientation. In practice, we notice the network first produces a very smooth shape. When it tries to refine it, it tends to overfit to the noise present in the supervision signal. At this stage, further fitting on easy samples (predominant samples) means overfitting on this noise. The samples that can benefit the implicit representation can be drowned within easy samples."}, {"title": "2. Related work", "content": "Classical shape modelling from point cloud includes combinatorial methods where the shape is defined through an input point cloud based space partitioning, through e.g. alpha shapes (Bernardini et al., 1999) Voronoi diagrams (Amenta et al., 2001) or triangulation (Cazals & Giesen, 2006; Liu et al., 2020; Rakotosaona et al., 2021). Differently, the input samples can be used to define an implicit function whose zero level set represents the target shape, using global smoothing priors (Williams et al., 2022; Lin et al., 2022; Williams et al., 2021) e.g. radial basis function (Carr et al., 2001) and Gaussian kernel fitting (Sch\u00f6lkopf et al., 2004), local smoothing priors such as moving least squares (Mercier et al., 2022; Guennebaud & Gross, 2007; Kolluri, 2008; Liu et al., 2021), or by solving a boundary conditioned Poisson equation (Kazhdan & Hoppe, 2013). The recent literature proposes to parameterise these implicit functions with deep neural networks and learn their parameters with gradient descent, either in a supervised (e.g. (Ouasfi & Boukhayma, 2024a; 2022; Boulch & Marlet, 2022; Peng et al., 2020; Lionar et al., 2021; Peng et al., 2021)) or unsupervised manner. These implicit neural representations (Mescheder et al., 2019; Park et al., 2019) overcome many of the limitations of explicit ones (e.g. meshes (Wang et al., 2018; Kato et al., 2018; Jena et al., 2022) and point clouds (Fan et al., 2017; Aliev et al., 2020; Kerbl et al., 2023)) in modelling shape, radiance and light fields (e.g. (Mildenhall et al., 2020; Yariv et al., 2021; Wang et al., 2021; Jain et al., 2022; Chan et al., 2022; Li et al., 2023a;b; Jena et al., 2024; Younes et al., 2024)), as they allow to represent functions with arbitrary topologies at virtually infinite resolution.\nWe are interested in unsupervised implicit neural shape learning. In this scenario, an MLP is typically fitted to the input point cloud without extra priors or information. Regularizations can compensate for the lack of supervision. For instance, (Gropp et al., 2020) introduced a spatial gradient constraint based on the Eikonal equation. (Ben-Shabat et al., 2022) introduces a spatial divergence constraint. (Liu et al.,"}, {"title": "3. Method", "content": "Given a noisy, sparse unoriented point cloud P C R^{3\u00d7Np}, our objective is to obtain a corresponding 3D shape reconstruction, i.e. the shape surface S that best explains the observation P. In other terms, the input point cloud elements should approximate noised samples from S.\nIn order to achieve this goal, we learn a shape function f parameterised with an MLP f_\\theta. The function represents the implicit signed distance field relative to the target shape S. That is, for a query euclidean space location q \u2208 R\u00b3, f(q) := s. min_{v \u2208 S} ||v \u2013 q||2, where s := 1 if q is inside"}, {"title": "3.1. Background: Learning an SDF by pulling queries onto the surface.", "content": "Several state-of-the-art reconstruction from point cloud methods (e.g. (Chen et al., 2023a; Ma et al., 2022b;a; Chen et al., 2022; Ma et al., 2021)), including the state-of-the-art unsupervised reconstruction from sparse point cloud method (Chen et al., 2023a), build on the neural SDF training procedure introduced in (Ma et al., 2021) named NeuralPull. The latter is inspired by the observation that the distance field guided projection operator q \u2194 q - f(q)\u00b7 \u2207 f(q) ((Chibane et al., 2020; Perry & Frisken, 2001; Wolter, 1993; Zhao et al., 2021)) yields the nearest surface point when applied near the surface, where \u2207 f is the spatial gradient of f.\nIn practice, query points q \u2208 Q are sampled around the input point cloud P, specifically from normal distributions centered at input samples {p}, with locally defined standard deviations {\u03c3p}:\nQ := \u222a_{p\u2208P} {q ~ N (p, \u03c3_p I_3)},"}, {"title": "3.2. Local adversarial queries for the few shot setting", "content": "As introduced in the first section, input point cloud noise and sparsity are akin to noisy labels for query points q. Hence, training through standard ERM under sparse input point clouds P leads to an overfitting on this noise (Figure 1), i.e. useful information carried out by query samples decreasing during training thus leading to poor convergence. Hence, differently from existing work in the field of learning based reconstruction from point cloud, we propose to focus on the manner in which query points q are sampled at training, as we hypothesise that there could be a different sampling strategy from the one proposed in NeuralPull (i.e. Equation 2) that can lead to better results. This hypothesis is backed by literature showing that hard sample mining can lead to improved generalization and reduced over-fitting (Xie et al., 2020; Chawla, 2010; Fern\u00e1ndez-Delgado et al., 2014; Krawczyk, 2016; Shrivastava et al., 2016). Intuitively, exposing the network to the worst cases in training is likely to make it more robust and less specialized on the more common easy queries.\nWe explore a different procedure from standard ERM (Equation 4). Ideally, we wish to optimize \u03b8 under the worst distribution Q' of query points {q} in terms of our objective function, meaning:\nmin_{\u03b8} max_{Q'} E_{q~Q'} L(\u03b8,q)."}, {"title": "4. Results", "content": "To evaluate our method, we assess our ability to learn implicit shape representations given sparse and noisy point clouds. We use datasets from standard reconstruction benchmarks. These datasets highlight a variety of challenges of fitting coordinate based MLPs to sparse data as well as reconstruction more broadly. Following the literature, we evaluate our method by measuring the accuracy of 3D explicit shape models extracted after convergence from our MLPs. We compare quantitatively and qualitatively to the the state-of-the-art in our problem setting, i.e. unsupervised reconstruction from unoriented point cloud, including methods designed for generic point cloud densities and methods dedicated to the sparse setting. For the former, we compare to fully implicit deep learning methods such as NP (Ma et al., 2021), SAP (Peng et al., 2021), DIGS (Ben-Shabat et al., 2022), in addition to hybrid methods combining implicit and grid based representations such as OG-INR (Koneputugodage et al., 2023) and GP (GridPull) (Chen et al., 2023b). When it comes to methods dedicated to the sparse setting we compare to NTPS (Chen et al., 2023a) which is the closest method to ours as it focuses specifically on the sparse input case. We additionally compare to NDrop (Boulch et al., 2021). We show results for NSpline (Williams et al., 2021) even though it requires normals. We also compare to classical Poisson Reconstruction SPSR (Kazhdan & Hoppe, 2013). We note also that comparisons to NP (NeuralPull, our baseline) also serves additionally as an ablation of our adversarial loss through out our experiments. For comprehensive evaluation, we also include comparisons to supervised methods including state-of-the-art feed-forward generalizable methods, namely POCO (Boulch & Marlet, 2022), CONet (Peng et al., 2020) and NKSR (Huang et al., 2023), alongside the finetuning method SAC (Tang et al., 2021) and the prior-based optimization method dedicated to sparse inputs On-Surf (Ma et al., 2022a). Unless stated differently, we use the publicly available official implementations of existing methods. For sparse inputs, we experimented with point clouds of size Np = 1024."}, {"title": "4.1. Metrics", "content": "Following seminal work, we evaluate our method and the competition w.r.t. the ground truth using standard metrics for the 3D reconstruction task. Namely, the L1 Chamfer Distance CD1 (\u00d710\u22122), L2 Chamfer Distance CD2 (\u00d710\u22122), the euclidean distance based F-Score (FS) when ground truth points are available, and finally Normal Consistency (NC) when ground truth normals are available. We detail the expressions of these metrics in the appendix."}, {"title": "4.2. Datasets and input definitions", "content": "ShapeNet (Chang et al., 2015) consists of various instances of 13 different synthetic 3D object classes. We follow the train/test splits defined in (Williams et al., 2021). We generate noisy input point clouds by sampling 1024 points from the meshes and adding Gaussian noise of standard deviation 0.005 following the literature (e.g. (Boulch & Marlet, 2022; Peng et al., 2020)). For brevity we show results on classes Tables, Chairs and Lamps.\nFaust (Bogo et al., 2014) consists of real scans of 10 human body identities in 10 different poses. We sample sets of 1024 points from the scans as inputs.\n3D Scene (Zhou & Koltun, 2013) contains large scale complex real world scenes obtained with a handheld commodity range sensor. We follow (Chen et al., 2023a; Jiang et al., 2020; Ma et al., 2021) and sample our input point clouds with a sparse density of 100 per m\u00b2, and we report performance similarly for scenes Burghers, Copyroom, Lounge, Stonewall and Totempole.\nSurface Reconstruction Benchmark (SRB) (Williams et al., 2019) consists of five object scans, each with different challenges such as complex topology, high level of detail, missing data and varying feature scales. We sample 1024 points from the scans for the sparse input experiment, and we also experiment using the dense inputs."}, {"title": "4.3. Implementation details", "content": "Our MLP (f_\\theta) follows the architecture in NP (Ma et al., 2021). We train for Nit = 40000 iterations using the Adam optimizer. We use batches of size N_b = 5000. Following NP, we set K = 51 for estimating local standard deviations \u03c3p. We train on a NVIDIA RTX A6000 GPU. Our method takes 8 minutes in average to converge for a 1024 sized input point cloud. In the interest of practicality and fairness in our comparisons, we decide the evaluation epoch for all the methods for which we generated results (including our main baseline) in the same way: we chose the best epoch for each method in terms of chamfer distance between the reconstruction and the input point cloud."}, {"title": "4.4. Object level reconstruction", "content": "We perform reconstruction of ShapeNet (Chang et al., 2015) objects from sparse and noisy point clouds. Table 1 and Figure 3 show respectively a numerical and qualitative comparison to the competition. We outperform the competition across all metrics, as witnessed by the visual superiority of our reconstructions. We manage to recover fine structures and details with more fidelity. Although it obtains overall good coarse reconstructions, the thin plate spline smoothing"}, {"title": "4.6. Real scene level reconstruction", "content": "Following (Chen et al., 2023a), we report reconstruction results on the 3D Scene ((Zhou & Koltun, 2013)) data from spatially sparse point clouds. Table 3 summarizes numerical results. We compiled results for methods NTPS, NP, SAP, NDrop and NSpline as reported in state-of-the-art method NTPS. We outperform the competition is this benchmark thanks to our loss, as our baseline NP displays more blatant failures in this large scale sparse setup. Figure 5 shows qualitative comparisons to our baseline NP and SPSR. Red boxes highlight areas where our method displays particularly better details and fidelity in the reconstruction."}, {"title": "4.7. Varying the point cloud density", "content": "We use the SRB (Williams et al., 2019) benchmark to assess the behavior of our method across different point cloud densities. Table 4 indicates comparative results under both 1024 sized and dense input point clouds. We compiled results for the competition from OG-INR in the dense setting. We outperform our competition in the sparse case, and we perform on par with the state-of-the-art in the dense case. Our improvement w.r.t. our baseline (NP) is substantial for both sparse and dense inputs. This can be seen visually in Figure 6, where we show reconstructions for both sparse and dense cases. Notice how we recover better topologies in the sparse case and improved and more accurate details in the dense case, as pinpointed by the red boxes. These results showcase the utility and benefit of our contribution even in the dense setting. We note that SPSR becomes a very viable contestant qualitatively in the dense setting."}, {"title": "5. Ablation studies", "content": "The ablation of our main contribution is present throughout all Tables and Figures. In fact while we use the combined loss in Equation 9, our baseline (i.e. NP) uses solely the query projection loss in Equation 3. The improvement brought by our additional loss is visible across real/synthetic, scenes/objects, sparse/dense point clouds.\nPerturbation radii We perform an ablation of using local vs. global radii \u03c1 (Equation 8) and the choice of value of local radii p_q in Table 5. Results show that using local radii (p_q) is a superior strategy as it intuitively allows for"}, {"title": "6. Limitations", "content": "As can be seen in e.g. Figure 3, even though we improve on our baseline, we still face difficulties in learning very thin and convoluted shape details, such as chair legs and slats. Although our few-shot problem is inherently challenging"}, {"title": "7. Conclusion", "content": "We explored in this work a novel idea for regularizing implicit shape representation learning from sparse unoriented point clouds. We showed that harnessing adversarial samples locally in training can lead to numerous desirable outcomes, including superior results, reduced over fitting and easier evaluation model selection."}, {"title": "Impact statement", "content": "This paper presents work whose goal is to advance the fields of Machine Learning and 3D Computer Vision, specifically implicit neural shape representation learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "B. Metrics", "content": "Following the definitions from (Boulch & Marlet, 2022) and (Williams et al., 2019), we present here the formal definitions for the metrics that we use for evaluation in the main submission. We denote by S and \u015c the ground truth and predicted mesh respectively. All metrics are approximated with 100k samples from the groundtruth mesh S and reconstruction S.\nChamfer Distance (CD1) The L\u2081 Chamfer distance is based on the two-ways nearest neighbor distance:\nCD1 = \\frac{1}{2|S|}\\sum_{v \\in S} min_{\\hat{v} \\in \\hat{S}} ||v - \\hat{v}||_2 + \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} min_{v \\in S} ||\\hat{v} - v||_2"}, {"title": "Chamfer Distance (CD2)", "content": "The L\u2082 Chamfer distance is based on the two-ways nearest neighbor squared distance:\nCD2 = \\frac{1}{2|S|}\\sum_{v \\in S} min_{\\hat{v} \\in \\hat{S}} ||v - \\hat{v}||_2^2 + \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} min_{v \\in S} ||\\hat{v} - v||_2^2"}, {"title": "F-Score (FS)", "content": "For a given threshold \u03c4, the F-score between the meshes S and \u015c is defined as:\nFS (\u03c4, S, \u015c) = \\frac{2 \\cdot \\text{Recall} \\cdot \\text{Precision}}{\\text{Recall} + \\text{Precision}}, where\n\\text{Recall} (\u03c4, S, \u015c) = |{v \u2208 S, s.t. min_{\\hat{v} \u2208 \u015c} ||v - \\hat{v}||_2 {\\leq} \u03c4}|,\n\\text{Precision} (\u03c4, S, \u015c) = |{\\hat{v} \u2208 \u015c, s.t. min_{v \u2208 S} ||v - \\hat{v}||_2 {\\leq} \u03c4}|.\nFollowing (Mescheder et al., 2019) and (Peng et al., 2020), we set \u03c4 to 0.01."}, {"title": "Normal consistency (NC)", "content": "We denote here by n_v the normal at a point v in S. The normal consistency between two meshes S and \u015c is defined as:\nNC = \\frac{1}{2|S|} \\sum_{v \\in S} n_v \\cdot n_{\\text{closest}(v, \\hat{S})} + \\frac{1}{2|\\hat{S}|} \\sum_{\\hat{v} \\in \\hat{S}} n_{\\hat{v}} \\cdot n_{\\text{closest}(\\hat{v}, S)}, where\n\\text{closest}(v, \u015c) = \\text{argmin}_{\\hat{v} \\in \u015c} ||v - \\hat{v}||_2."}]}