{"title": "Universal Approximation Theory: The basic theory for deep learning-based computer vision models", "authors": ["Wei Wang", "Qing Li"], "abstract": "Computer vision (CV) is one of the most crucial fields in ar- tificial intelligence. In recent years, a variety of deep learning models based on convolutional neural networks (CNNs) and Transformers have been designed to tackle diverse problems in CV. These algorithms have found practical applications in areas such as robotics and facial recognition. Despite the in- creasing power of current CV models, several fundamental questions remain unresolved: Why do CNNs require deep layers? What ensures the generalization ability of CNNs? Why do residual-based networks outperform fully convolu- tional networks like VGG? What is the fundamental differ- ence between residual-based CNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning tech- niques? The root cause of these questions lies in the lack of a robust theoretical foundation for deep learning models in CV. To address these critical issues and techniques, we employ the Universal Approximation Theorem (UAT) to provide a theo- retical basis for convolution- and Transformer-based models in CV. By doing so, we aim to elucidate these questions from a theoretical perspective.", "sections": [{"title": "Introduction", "content": "As a core branch of artificial intelligence, CV has a wide range of applications, encompassing tasks such as image segmentation (Minaee et al. 2021; Wang et al. 2022a), clas- sification (Rawat and Wang 2017; Wang et al. 2017), video synthesis (Wang et al. 2018, 2019a), and restoration (Wang et al. 2019b; Nah et al. 2019). This broad applicability high- lights its enormous technical potential and practical sig- nificance, making the intelligent processing of CV tasks a highly important area of research. The most effective so- lutions in this field today are based on deep learning algo- rithms.\nOne of the earliest and most influential works in using deep learning for CV was LeNet (Lecun et al. 1998), which pioneered the use of CNNs for handwritten digit recogni- tion, ushering in a new era of image processing. Following this, AlexNet's (Krizhevsky, Sutskever, and Hinton 2017) re- markable performance on the ImageNet (Russakovsky et al. 2014) dataset not only achieved a significant leap in image classification accuracy but also cemented CNNs as the dom- inant method in visual processing. The subsequent intro- duction of ResNet (He et al. 2016) further improved image recognition accuracy and established residual-based struc- ture as a foundational architecture for future network de- signs, influencing nearly all subsequent deep learning mod- els (Ren et al. 2015; Xie et al. 2016; Szegedy et al. 2014; Gao et al. 2019; Liu et al. 2021).\nIn recent years, the success of Transformers (Vaswani et al. 2017) in the field of natural language processing (NLP) has gradually permeated CV. Researchers have begun de- signing models for CV based on Transformers, such as the Vision Transformer (ViT) (Dosovitskiy et al. 2020), Swin Transformer (Liu et al. 2021) and StructViT (Kim et al. 2024), which have demonstrated capabilities comparable to CNNs in image tasks (For convenience, we collectively refer to all Transformer-based models in the CV domain as ViTs.). Currently, research in deep learning for image processing predominantly revolves around CNNs (Huang et al. 2023; Wang et al. 2022b), Transformers (Shi 2023; Wang et al. 2023), or strategies that integrate both (Lv et al. 2023; Hou et al. 2024), continuously driving technological advance- ments.\nDespite significant progress in deep learning-based CV, achieving performance that surpasses human capabilities in some areas, several questions remain unanswered. For ex- ample, what necessitates deep layers in CNNs? What under- pins the generalization ability of CNNs? Why do residual- based networks surpass fully convolutional networks like VGG (Simonyan and Zisserman 2014) in performance? What are the core differences between residual-based CNNs and Transformer-based networks? We believe that the funda- mental reason lies in the absence of a basic theory for CNNs. Although some researchers have attempted to explain CNNs through various methods, such as visualization techniques to analyze the relationship between feature maps and origi- nal images (Wei et al. 2016) and exploring CNNs from the frequency domain perspective (Yin et al. 2019; Xu, Zhang, and Xiao 2019), these explanations often have limitations and subjectivity. Similarly, studies on the interpretability of ViTs (Park and Kim 2022; Bai et al. 2022) suggest that MH\u0410 tend to capture low-frequency information, contrasting with the high-pass filtering nature of CNNs. These studies advo- cate for combining the strengths of both or improving ViTs to better understand high-frequency details.\nHowever, these methods provide only intuitive visual ex- planations for specific issues and fail to offer a theoreti- cal explanation for the widespread problems in CNNs and ViTs. Essentially, they describe or understand phenomena within CNNs or ViTs rather than presenting a complete theory for these architectures. To address these questions, we propose unifying CNNs and ViTs under the framework of the UAT by referencing the methods introduced in the UAT2LLMs (Wang and Li 2024). Using this framework, we aim to solve the aforementioned problems. Our contri- butions are as follows:\n\u2022 We used the Matrix-Vector Method to demonstrate that the multi-layer convolutional and Transformer structures commonly employed in the field of CV are specific im- plementations of the UAT.\n\u2022 We explained why CNNs require deep networks, con- sidering the characteristics of image data and the UAT model perspective.\n\u2022 We proved why the widely used residual structure in CV is so powerful.\n\u2022 We compared multi-layer convolutional residual struc- tures and Transformer structures from the UAT perspec- tive, illustrating the source of their powerful capabilities and explaining why both models appear to have similar performance.\n\u2022 we provided an explanation for LoRA and pruning in the context of CV.\nThe structure of this paper is as follows: In Section 2, we introduce the UAT and the Matrix-Vector Method, as well as their relationship. In Section 3, we use the Matrix-Vector Method to transform some commonly used operations in CV into matrix-vector forms, such as 2D convolution (3.1), 3D convolution (3.2), mean-pooling (3.3), and Transform- ers (3.4). In Section 4, we address some fundamental ques- tions in CV: why CNN networks need to be deep (4.1), why residual-based CNNs are more powerful than VGG (4.2), the differences between residual-based CNNs and Transformer- based networks (4.3), and the theoretical support behind LORA and pruning operations in CV (4.4)."}, {"title": "UAT for CV", "content": "We have previously introduced our goal of unifying CNNs and ViTs in the field of CV under the framework of UAT. Us- ing UAT, we aim to explain existing problems in CV. So we will follow the UAT2LLMs' (Wang and Li 2024) thoughts to leverage the Matrix-Vector Method proposed in UAT2LLMs to unify CNNs and ViTs under the UAT framework. Be- fore formally unifying CNNs and ViTs within UAT, we will briefly introduce the Matrix-Vector Method and the unifica- tion approach proposed in UAT2LLMs.\nFigure 1 simply outlines the proof route in UAT2LLMs: first, G(x) represents the general form of UAT, capable of approximating Borel measurable functions within a closed interval(Cybenko 2007; Hornik, Stinchcombe, and White 1989), where In denotes an n-dimensional unit cube. UAT includes one-dimensional and high-dimensional approxima- tions, corresponding to Figures 1.a and 1.b, respectively. It is evident that the basic UAT computing units Wx + 0j corresponding to convolution, mean pooling, multi-head at- tention (MHA), and feed-forward networks (FFN) in CNNS and ViTs. Therefore, if we can transform these operations into matrix multiplication using the Matrix-Vector Method in Figures 1 (Details could be seen in UAT2LLMs), it be- comes straightforward to prove that multi-layer CNNs and ViTs are specific implementations of UAT.\nThe Matrix-Vector Method is illustrated in Figure 1, where T denotes various transformations in CNNs and ViTs, and TD and Tp represent specific transformation methods. These transformations convert the input data x and output data y, along with parameters W, into vectors x' and y', and parameter matrices W', satisfying the computational requirements shown in the Figure 1. To distinguish from the original variables, we will add a' symbol to the upper right of the original variables, indicating their correspond- ing matrix-vector form. (Note: The key challenge is convert- ing them into matrix-vector form, so bias terms are omitted here.) This approach also has the advantage of converting various transformations into a uniform matrix-vector form, allowing the entire network process to be expressed con- cisely using addition, multiplication, matrix operations, and activation functions, making it easier to compare models' differences.\nAdditionally, due to the complexity of convolution and Transformer calculations, a computational tool called dia- mond matrix multiplication has been designed to clearly elu- cidate the process of converting convolution or Transformer operations into their corresponding matrix-vector represen- tations. Its relationship with standard matrix multiplication is W\u25cax = WTx. The diamond matrix enhances the clarity of this conversion process. The details and properties can be found in UAT2LLMs."}, {"title": "The Matrix-Vector Method for CNNs and ViTs", "content": "In the previous sections, we explained that to unify CNNs and ViTs under the UAT, it is necessary to demonstrate that their various transformations (convolution, mean pool- ing, MHA, FFN) can be represented in matrix-vector form. UAT2LLMs has already shown how to represent MHA and FFN in matrix-vector form. Therefore, in this section, we will specifically demonstrate how to convert convolution and mean pooling in CNNs into matrix-vector form using the Matrix-Vector Method. Additionally, we will interpret the unique aspects of MHA in ViTs within the context of CV.\nConsidering that convolution in CV can be either 2D or 3D and that the input and output may consist of single or multiple channels. We use the notation 1 to represent a sin- gle channel, I to represent an input with I channels, and O to represent an output with O channels. For example, 1-O in- dicates an operation with one channel input and O channels output. Below, we will show how to convert these transfor- mations into the corresponding matrix-vector forms."}, {"title": "The Matrix-Vector Method for Conv2D", "content": "In this section, we will detail how to convert 2D con- volution operations into their corresponding matrix-vector forms, focusing on two scenarios: 1-O and I-O. To clearly describe the 2D convolution process, we first outline the general procedure and some basic conventions. The general process of 2D convolution involves convolving a kernel with the input data and then summing along the input channel direction (referred to as the C\u2081 direction) to obtain an ele- ment in the output. The convolution kernel slides along the height (H) and width (W) directions to produce the entire output feature map. To generate multiple feature maps, mul- tiple sets of convolution kernels, Co, are required, with each set having the same number of channels as the input chan- nels. Depending on the context, C1, CO, H, and W can rep- resent either the corresponding dimension directions or the number of dimensions in those directions. Figures 2 and 3 illustrate the conversion process for each case, transforming them into matrix-vector representations.\nSpecifically, Figure 2.a shows the process of 1-0 Conv2D. Here, W1W0 represent the convolution ker- nels, which slide across the input data along the H and W directions. For a single-channel input producing O output channels, O convolution kernels are each convolved with the same input x to produce outputs y1 yo. Figure 2.b provides an example of a 1-O Conv2D. Additionally, Figure 2.c further converts the example given in Figure 2.b into its corresponding matrix-vector form. Therefore, it is straight- forward to derive the matrix-vector form of 1-O Conv2D as follows:\n\\( ( W'_1\\ W'_2\\ \u2026W'_O) \\\u25cax' =  \\begin{pmatrix}  y_1\\\\  y_2\\\\  \u2026\\\\ y_o  \\end{pmatrix} \\) \\( (1) \\)"}, {"title": "The Matrix-Vector Method for Conv3D", "content": "3D convolution is a structure designed primarily for video or 3D image data, consisting mainly of two forms: 1-0 Conv3D and I-O Conv3D. Given their complexity, we pro- vide proof examples only for the 1-1 and I-1 Conv3D forms. The forms of 1-O and I-O Conv3D can be easily derived from the 1-1 and I-1 Conv3D forms. Compared with 2D convolution, 3D convolution has an additional depth dimen- sion (D). So the convolution kernel could slide along with H, W, D, and it also needs to sum in C\u2081 direction.\nFigure 4 illustrates the process of converting 1-1 Conv3D into its matrix-vector form. Figure 4.a shows the general form of 1-0 Conv3D. As a 3D convolution, the input data typically has four dimensions: (C1, H, W, D), representing the number of input channels, height, width, and depth, re- spectively. In 1-1 Conv3D, C\u2081 is at most 1. The convolu- tion kernel has an additional dimension Co, representing the number of output channels, and in this case, Co = 1. The convolution kernel slides along the width (W), height (H), and depth (D) directions. Figure 4.b presents a spe- cific example, while Figure 4.c converts this example into its matrix-vector form (ignoring changes in H, W, D dimen- sions due to lack of padding).\nAccording to Figure 4, the matrix-vector form of 1-0 Conv3D can be easily derived and is consistent with Eq. (1). The difference lies in the fact that Wi has depth, so the cor- responding W needs to be recombined as shown in Figure 4.c.\nSimilarly, Figures 5.a, b, and c illustrate the general pro- cess, a simple example, and the corresponding matrix-vector form of I-1 Conv3D, respectively. According to Figure 5, the matrix-vector form of I-O Conv3D is consistent with (1). The difference is that both xi and Wi have depth, so the corresponding x and W need to be obtained as shown in Figure 5.c."}, {"title": "The Matrix-Vector Method for Mean Pooling", "content": "Mean pooling is also an important technique in the field of CV, often used in conjunction with convolution. To demon- strate that CNNs can be expressed in the form of UAT, we need to prove that mean pooling can also be represented in matrix-vector form. In Figure 6, we provide an example. Figure 6.a shows the process of mean pooling, while Figure 6.b illustrates its corresponding matrix-vector form. Based on Figure 6, we can conclude that mean pooling can indeed be represented in matrix-vector form."}, {"title": "Transformer for CV", "content": "In UAT2LLMs, it has been demonstrated that the feedfor- ward network (FFN) and MHA components in the Trans- former architecture can be represented in matrix-vector form. Given that ViTs are also based on the Transformer framework, we only need to focus on the differences be- tween Transformers in CV and those in UAT2LLMs. A key feature of ViTs is that they divide the original image into multiple patches: X1,1,..., Xn,m. Each image patch is then reshaped into row vectors 1,1,\u2026\u2026,n,m, which are con- catenated along the column direction to obtain x, as shown in Figure 7. MHA and FFN operations are then applied, so aside from the initial processing part, the subsequent transformation process completely follows the description in UAT2LLMs. Therefore, it is easy to deduce that ViTs are also specific implementations of the UAT."}, {"title": "Summary", "content": "We have demonstrated that various modules in CV, in- cluding 2D convolution, 3D convolution, mean pooling, MHA, and FFN, can all be represented by diamond ma- trix multiplication. Considering the relationship between diamond matrix multiplication and matrix multiplication: W\u25ca x = WTx. Therefore, unless otherwise specified, we will conveniently use W'x' to represent the matrix-vector form of them. Here, W' is derived from the parameters W of the various modules, and x' is derived from the inputs x of the modules."}, {"title": "Discussion", "content": "In this section, we will answer the questions in the field of CV proposed in the Introduction."}, {"title": "Why must CNNs be Deep?", "content": "There are two main reasons why CNNs typically have deep layers. The first reason stems from the learning paradigm of CNNs. The design of CNNs is based on two well-known principles: local receptive fields (i.e., local fea- tures in images often contain important information) and spatial invariance (these important local features can appear anywhere in the image). These principles necessitate learn- ing from each small region of the image, with all small re- gions sharing the same parameters. If parameters differed, it would imply that a certain object must appear in a specific part of the image, which is clearly unreasonable.\nThe above reasons dictate that image learning occurs in small patches, which explains why convolutional kernels are typically small. However, if the network has only a few lay- ers, such as a single layer, it means that each small patch is learned independently. We know that objects in an image are generally composed of multiple such patches. Observing only a part of the image cannot provide a clear identification of the object. Therefore, a larger receptive field is needed, which can be achieved by increasing the number of layers to gather global information.\nThe second reason is derived from the UAT. As shown in Figure 1, increasing the number of layers in the network means a larger N, which brings the UAT closer to approxi- mating the target function. An image can be understood as a special function in a high-dimensional space, exhibiting strong correlations in 2D or 3D space. These two reasons are also applicable to ViTs. Figure 7 shows the preprocess- ing of ViTs, where x is the input to the ViTs. It is evident that each row shares the same parameters, and the learning of each row adheres to the principles of local receptive fields and spatial invariance."}, {"title": "Why Residual-Based CNNs Excel in CV: Superior Generalization Ability", "content": "In the previous discussion, we explained why deep net- works are essential for learning in CNNs. However, why did VGG, despite being a deep network, not dominate the sub- sequent development of CV, whereas residual-based CNNs did? ResNet, one of the most influential architectures in the field, has significantly shaped the design of later net- work architectures, with the residual structure appearing in many subsequent networks. What exactly endows the resid- ual structure with such powerful capabilities? We think the answer is the generalization capability given by residual structure. To analyze the source of this power, we use the Matrix-Vector Method to express parts of VGG and the residual structure in corresponding equations, and then com- pare them with the UAT.\nFigure 8 illustrates the basic structures of VGG and ResNet. Based on this figure, a three-layer VGG can be written as Eq.(3), which perfectly aligns with the multi- layer UAT mathematical form proposed by Hornik (Hornik, Stinchcombe, and White 1989). On the other hand, a two- layer Residual-based CNNs can be expressed in the form shown in Eq.(4) (see Appendix A.1 for detail.), conforming to the UAT form given in Figure 1. In this equation, bi+1,1 is calculated from Wi,1, W1,2, xi, bi,1, bi,2, bi+1,1. It is evi- dent that both VGG and ResNet are concrete implementa-"}, {"title": "The Difference between Residual-Based CNNs and ViTs", "content": "In the previous section, we used the matrix-vector method to represent VGG and ResNet in their corresponding mathe- matical forms and analyzed them from the perspective of the UAT. In this section, we will apply the same method to an- alyze the Transformer in ViTs and compare it with residual- based CNNs. Figure 9 shows the general form of a Trans- former. After derivation, the mathematical form of a two- layer Transformer network corresponds to Eq. (5) (see Ap- pendix A.2 for detail.). Clearly, the mathematical represen- tation of ViT also conforms to UAT, with some parameters influenced by the input information.\nThe main difference between Transformer networks and residual-based multi-layer convolutional networks lies in how input information affects their corresponding UAT pa- rameters. In Transformer networks, input information influ- ences both weight and bias in the corresponding UAT. In contrast, in networks constructed with multiple residual con- volutions, input information only influences bias in the cor- responding UAT. Essentially, both types of networks have the ability to dynamically approximate functions based on input. This aligns with their nearly identical performance in the field of CV, supporting the theoretical foundation that both are specific implementations of UAT capable of dynam- ically approximating the corresponding functions based on input.\n\\(x_{i+2} = W'_{i+1,1}x + W'_{i+1,2}\\sigma(\\overline{W}_{i,1}x + \\hat{b}_{i,2}) + \\hat{b}_{i,1}\\)\n\\(+\\overline{W}_{i+1,3}\\sigma(\\overline{W}'_{i+1,3}x + \\hat{b}'_{i+1,2}) \\) \\( (5) \\)"}, {"title": "The Theory behind Lora and Pruning", "content": "In UAT2LLMS, UAT has been used to explain why LoRA and pruning are feasible. In the field of CV, we have proven that both convolution-based CNNs and Transformer-based ViTs are concrete implementations of UAT. Therefore, the principles behind LoRA and pruning in CV are the same as those mentioned in UAT2LLMs. Specifically, LoRA works by layer-wise adjustment of parameters, while pruning is ef- fective because some layer parameters have minimal impact on the overall result."}, {"title": "Conclusion", "content": "This paper delves into the theoretical foundations of deep learning in the field of CV. Specifically, the current CV land- scape is primarily dominated by CNNs and Transformer models. We utilize the matrix-vector method to unify these models under the framework of the UAT. Based on this framework, we provide explanations for several common is- sues and techniques in CV. The requirement for deep net- works in CNNs is jointly determined by the intrinsic charac- teristics of image data and the demands of UAT theory. The robust generalization ability of residual networks stems from their ingenious design, which enables UAT to dynamically adapt to corresponding functions based on input data. Sim- ilarly, Transformer-based models possess this ability, with the key difference lying in which parameters of UAT they influence. Residual networks primarily affect the bias term, while Transformers influence both the weights and the bias. Additionally, the feasibility of LoRA operations in CV arises because it can fine-tune the parameters of different UAT lay- ers based on input data characteristics. The viability of prun- ing techniques is due to the fact that the parameters of certain network layers have minimal impact on the final outcome."}, {"title": "The UAT Format of Residual-based CNNs and Transformer-based ViTs", "content": "In this section, we will present the Matrix-Vector form of residual-based CNNs and Transformer-based ViTs and their relationship with the UAT. Figure 1 and 2 show a schematic of a residual-based convolution and Transformer. We have already established that convolution and components in a Transformer can be represented in a matrix-vector form. Therefore, to illustrate its relationship with UAT, we will use Figure 1 and 2, with the input xi, derive the mathematical form corresponding to xi+2. We will express this in matrix- vector form, so parameters and inputs/outputs will have a prime (') in the upper right corner to indicate their matrix- vector form.\nDue to the complexity of the variables involved, we will combine some terms to align the resulting equations with the standard UAT form. To distinguish these parameter vari- ables, we will keep the original variables unchanged. If a parameter variable is influenced only by other variables, we will denote it with a bar on top, such as W. If it is influenced by the input x\u2081, we will mark it with a hat, such as \u0174."}, {"title": "The UAT Format of Residual-based CNNs", "content": "Through derivation and simplification, we obtain the matrix-vector mathematical representation for xi+2, as shown in Eq.1. Comparing this with the standard UAT form, it is evident that the parameters in UAT are fixed after train- ing. In contrast, the bias terms in the UAT corresponding to residual-based CNNs can change dynamically based on the input."}, {"title": "The UAT Format of Transformer-based ViTs", "content": "Eq.(2) and (3) give an example of the matrix-vector for- mat of MHA and FFN. Eq. (4) give the corresponding matrix-vector format of xi+2. It is obvious that the bias and weight terms in the UAT corresponding to Transformed- based ViTs can change dynamically based on the input.\n\\(MHA(x) = W'_1x \\) \\( (2) \\)\n\\(FFN(x) = \\overline{W}'_{i,3}\\sigma(\\overline{W}'_{i,2}x + \\hat{b}'_{i,2}) + \\hat{b}'_{i,3} \\) \\( (3) \\)"}, {"title": "", "content": "\\(x_{i+1} = x_i + \\overline{W}'_{1,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\hat{b}_{i,2}\\)\n\\(x_{i+2} = x_{i+1} + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x_{i+1} + \\hat{b}_{i+1,1}) + \\hat{b}_{i+1,2}\\)\n\\(= [x_i + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\hat{b}_{i,2}]\\)\n\\(+ \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}[x_i + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\hat{b}_{i,2}] + \\hat{b}_{i+1,1}) + \\hat{b}_{i+1,2}\\)\n\\(= x_i + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\hat{b}_{i,2}\\)\n\\(+ \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\overline{W}'_{i,1}\\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\overline{W}'_{i,1}\\hat{b}_{i,2} + \\hat{b}_{i+1,1}) + \\hat{b}_{i+1,2}\\)\n\\(= x_i + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,1}) + \\overline{W}'_{i,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i+1,2}) + \\hat{b}_{i+1,2}\\) \\( (1) \\)\n\\(x'_{i+1} = W'_{i,1}x + W'_{i,3}\\sigma(W'_{i,2}[W'_{i,1}x] + b'_{i,2}) + b'_{i,3}\\)\n\\(= W'_{i,1}x + W'_{i,3}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,2}) + \\hat{b}_{i,3}\\)\n\\(x_{i+2} = W'_{i+1,1}x_{i+1} + W'_{i+1,3}\\sigma(W'_{i+1,2}[W'_{i+1,1}x_{i+1}] + b'_{i+1,2}) + b'_{i+1,3}\\)\n\\(= W'_{i+1,1}x_{i+1} + W'_{i+1,3}\\sigma(\\overline{W}'_{i+1,1}x + \\hat{b}_{i+1,2}) + \\hat{b}_{i+1,3}\\)\n\\(= W'_{i+1,1}[W'_{i,1}x + W'_{i,3}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,2}) + \\hat{b}_{i,3}] \\)\n\\(+ W'_{i+1,3}\\sigma(W'_{i+1,1}[W'_{i,1}x + W'_{i,3}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,2}) + \\hat{b}_{i,3}] + b'_{i+1,2}) + b'_{i+1,3}\\)\n\\(= W'_{i+1,1}x + W'_{i+1,2}\\sigma(\\overline{W}'_{i,1}x + \\hat{b}_{i,2}) + \\hat{b}_{i,1} \\)\n\\(+ W'_{i+1,3}\\sigma(\\overline{W}'_{i+1,3}x + \\hat{b}'_{i+1,2}) \\) \\((4))"}]}