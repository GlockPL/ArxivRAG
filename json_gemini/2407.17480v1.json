{"title": "Universal Approximation Theory: The basic theory for deep learning-based computer vision models", "authors": ["Wei Wang", "Qing Li"], "abstract": "Computer vision (CV) is one of the most crucial fields in artificial intelligence. In recent years, a variety of deep learning models based on convolutional neural networks (CNNs) and Transformers have been designed to tackle diverse problems in CV. These algorithms have found practical applications in areas such as robotics and facial recognition. Despite the increasing power of current CV models, several fundamental questions remain unresolved: Why do CNNs require deep layers? What ensures the generalization ability of CNNs? Why do residual-based networks outperform fully convolutional networks like VGG? What is the fundamental difference between residual-based CNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning techniques? The root cause of these questions lies in the lack of a robust theoretical foundation for deep learning models in CV. To address these critical issues and techniques, we employ the Universal Approximation Theorem (UAT) to provide a theoretical basis for convolution- and Transformer-based models in CV. By doing so, we aim to elucidate these questions from a theoretical perspective.", "sections": [{"title": "Introduction", "content": "As a core branch of artificial intelligence, CV has a wide range of applications, encompassing tasks such as image segmentation (Minaee et al. 2021; Wang et al. 2022a), classification (Rawat and Wang 2017; Wang et al. 2017), video synthesis (Wang et al. 2018, 2019a), and restoration (Wang et al. 2019b; Nah et al. 2019). This broad applicability highlights its enormous technical potential and practical significance, making the intelligent processing of CV tasks a highly important area of research. The most effective solutions in this field today are based on deep learning algorithms.\nOne of the earliest and most influential works in using deep learning for CV was LeNet (Lecun et al. 1998), which pioneered the use of CNNs for handwritten digit recognition, ushering in a new era of image processing. Following this, AlexNet's (Krizhevsky, Sutskever, and Hinton 2017) remarkable performance on the ImageNet (Russakovsky et al. 2014) dataset not only achieved a significant leap in image classification accuracy but also cemented CNNs as the dominant method in visual processing. The subsequent introduction of ResNet (He et al. 2016) further improved image recognition accuracy and established residual-based structure as a foundational architecture for future network designs, influencing nearly all subsequent deep learning models (Ren et al. 2015; Xie et al. 2016; Szegedy et al. 2014; Gao et al. 2019; Liu et al. 2021).\nIn recent years, the success of Transformers (Vaswani et al. 2017) in the field of natural language processing (NLP) has gradually permeated CV. Researchers have begun designing models for CV based on Transformers, such as the Vision Transformer (ViT) (Dosovitskiy et al. 2020), Swin Transformer (Liu et al. 2021) and StructViT (Kim et al. 2024), which have demonstrated capabilities comparable to CNNs in image tasks (For convenience, we collectively refer to all Transformer-based models in the CV domain as ViTs.). Currently, research in deep learning for image processing predominantly revolves around CNNs (Huang et al. 2023; Wang et al. 2022b), Transformers (Shi 2023; Wang et al. 2023), or strategies that integrate both (Lv et al. 2023; Hou et al. 2024), continuously driving technological advancements.\nDespite significant progress in deep learning-based CV, achieving performance that surpasses human capabilities in some areas, several questions remain unanswered. For example, what necessitates deep layers in CNNs? What underpins the generalization ability of CNNs? Why do residual-based networks surpass fully convolutional networks like VGG (Simonyan and Zisserman 2014) in performance? What are the core differences between residual-based CNNs and Transformer-based networks? We believe that the fundamental reason lies in the absence of a basic theory for CNNs. Although some researchers have attempted to explain CNNs through various methods, such as visualization techniques to analyze the relationship between feature maps and original images (Wei et al. 2016) and exploring CNNs from the frequency domain perspective (Yin et al. 2019; Xu, Zhang, and Xiao 2019), these explanations often have limitations and subjectivity. Similarly, studies on the interpretability of ViTs (Park and Kim 2022; Bai et al. 2022) suggest that MH\u0410 tend to capture low-frequency information, contrasting with the high-pass filtering nature of CNNs. These studies advocate for combining the strengths of both or improving ViTs to better understand high-frequency details.\nHowever, these methods provide only intuitive visual explanations for specific issues and fail to offer a theoretical explanation for the widespread problems in CNNs and ViTs. Essentially, they describe or understand phenomena within CNNs or ViTs rather than presenting a complete theory for these architectures. To address these questions, we propose unifying CNNs and ViTs under the framework of the UAT by referencing the methods introduced in the UAT2LLMs (Wang and Li 2024). Using this framework, we aim to solve the aforementioned problems. Our contributions are as follows:\n\u2022 We used the Matrix-Vector Method to demonstrate that the multi-layer convolutional and Transformer structures commonly employed in the field of CV are specific implementations of the UAT.\n\u2022 We explained why CNNs require deep networks, considering the characteristics of image data and the UAT model perspective.\n\u2022 We proved why the widely used residual structure in CV is so powerful.\n\u2022 We compared multi-layer convolutional residual structures and Transformer structures from the UAT perspective, illustrating the source of their powerful capabilities and explaining why both models appear to have similar performance.\n\u2022 we provided an explanation for LoRA and pruning in the context of CV.\nThe structure of this paper is as follows: In Section 2, we introduce the UAT and the Matrix-Vector Method, as well as their relationship. In Section 3, we use the Matrix-Vector Method to transform some commonly used operations in CV into matrix-vector forms, such as 2D convolution (3.1), 3D convolution (3.2), mean-pooling (3.3), and Transformers (3.4). In Section 4, we address some fundamental questions in CV: why CNN networks need to be deep (4.1), why residual-based CNNs are more powerful than VGG (4.2), the differences between residual-based CNNs and Transformer-based networks (4.3), and the theoretical support behind LORA and pruning operations in CV (4.4)."}, {"title": "UAT for CV", "content": "We have previously introduced our goal of unifying CNNs and ViTs in the field of CV under the framework of UAT. Using UAT, we aim to explain existing problems in CV. So we will follow the UAT2LLMs' (Wang and Li 2024) thoughts to leverage the Matrix-Vector Method proposed in UAT2LLMs to unify CNNs and ViTs under the UAT framework. Before formally unifying CNNs and ViTs within UAT, we will briefly introduce the Matrix-Vector Method and the unification approach proposed in UAT2LLMs.\nFigure 1 simply outlines the proof route in UAT2LLMs: first, G(x) represents the general form of UAT, capable of approximating Borel measurable functions within a closed interval(Cybenko 2007; Hornik, Stinchcombe, and White 1989), where $I_n$ denotes an n-dimensional unit cube. UAT includes one-dimensional and high-dimensional approximations, corresponding to Figures 1.a and 1.b, respectively. It is evident that the basic UAT computing units $W_j x + \\theta_j$ corresponding to convolution, mean pooling, multi-head attention (MHA), and feed-forward networks (FFN) in CNNS and ViTs. Therefore, if we can transform these operations into matrix multiplication using the Matrix-Vector Method in Figures 1 (Details could be seen in UAT2LLMs), it becomes straightforward to prove that multi-layer CNNs and ViTs are specific implementations of UAT.\nThe Matrix-Vector Method is illustrated in Figure 1, where T denotes various transformations in CNNs and ViTs, and $T_D$ and $T_P$ represent specific transformation methods. These transformations convert the input data x and output data y, along with parameters W, into vectors $x'$ and $y'$, and parameter matrices $W'$, satisfying the computational requirements shown in the Figure 1. To distinguish from the original variables, we will add a ' symbol to the upper right of the original variables, indicating their corresponding matrix-vector form. (Note: The key challenge is converting them into matrix-vector form, so bias terms are omitted here.) This approach also has the advantage of converting various transformations into a uniform matrix-vector form, allowing the entire network process to be expressed concisely using addition, multiplication, matrix operations, and activation functions, making it easier to compare models' differences.\nAdditionally, due to the complexity of convolution and Transformer calculations, a computational tool called diamond matrix multiplication has been designed to clearly elucidate the process of converting convolution or Transformer operations into their corresponding matrix-vector representations. Its relationship with standard matrix multiplication is $W \\Diamond x = W^T x$. The diamond matrix enhances the clarity of this conversion process. The details and properties can be found in UAT2LLMs."}, {"title": "The Matrix-Vector Method for CNNs and ViTs", "content": "In the previous sections, we explained that to unify CNNs and ViTs under the UAT, it is necessary to demonstrate that their various transformations (convolution, mean pooling, MHA, FFN) can be represented in matrix-vector form. UAT2LLMs has already shown how to represent MHA and FFN in matrix-vector form. Therefore, in this section, we will specifically demonstrate how to convert convolution and mean pooling in CNNs into matrix-vector form using the Matrix-Vector Method. Additionally, we will interpret the unique aspects of MHA in ViTs within the context of CV.\nConsidering that convolution in CV can be either 2D or 3D and that the input and output may consist of single or multiple channels. We use the notation 1 to represent a single channel, I to represent an input with I channels, and O to represent an output with O channels. For example, 1-O indicates an operation with one channel input and O channels output. Below, we will show how to convert these transformations into the corresponding matrix-vector forms."}, {"title": "The Matrix-Vector Method for Conv2D", "content": "In this section, we will detail how to convert 2D convolution operations into their corresponding matrix-vector forms, focusing on two scenarios: 1-O and I-O. To clearly describe the 2D convolution process, we first outline the general procedure and some basic conventions. The general process of 2D convolution involves convolving a kernel with the input data and then summing along the input channel direction (referred to as the $C_I$ direction) to obtain an element in the output. The convolution kernel slides along the height (H) and width (W) directions to produce the entire output feature map. To generate multiple feature maps, multiple sets of convolution kernels, $C_O$, are required, with each set having the same number of channels as the input channels. Depending on the context, $C_I$, $C_O$, H, and W can represent either the corresponding dimension directions or the number of dimensions in those directions. Figures 2 and 3 illustrate the conversion process for each case, transforming them into matrix-vector representations.\nSpecifically, Figure 2.a shows the process of 1-O Conv2D. Here, $W_1 \\cdots W_O$ represent the convolution kernels, which slide across the input data along the H and W directions. For a single-channel input producing O output channels, O convolution kernels are each convolved with the same input x to produce outputs $y_1 \\cdots y_O$. Figure 2.b provides an example of a 1-O Conv2D. Additionally, Figure 2.c further converts the example given in Figure 2.b into its corresponding matrix-vector form. Therefore, it is straightforward to derive the matrix-vector form of 1-O Conv2D as follows:\n$( W'_1  W'_2 \\cdots W'_O ) \\Diamond x' =  \\begin{pmatrix} y'_1 \\\\ y'_2 \\\\  \\\\ y'_O \\end{pmatrix} $  (1)"}, {"title": "The Matrix-Vector Method for Conv3D", "content": "3D convolution is a structure designed primarily for video or 3D image data, consisting mainly of two forms: 1-O Conv3D and I-O Conv3D. Given their complexity, we provide proof examples only for the 1-1 and I-1 Conv3D forms. The forms of 1-O and I-O Conv3D can be easily derived from the 1-1 and I-1 Conv3D forms. Compared with 2D convolution, 3D convolution has an additional depth dimension (D). So the convolution kernel could slide along with H, W, D, and it also needs to sum in $C_I$ direction.\nFigure 4 illustrates the process of converting 1-1 Conv3D into its matrix-vector form. Figure 4.a shows the general form of 1-0 Conv3D. As a 3D convolution, the input data typically has four dimensions: ($C_I$, H, W, D), representing the number of input channels, height, width, and depth, respectively. In 1-1 Conv3D, $C_I$ is at most 1. The convolution kernel has an additional dimension $C_O$, representing the number of output channels, and in this case, $C_O = 1$. The convolution kernel slides along the width (W), height (H), and depth (D) directions. Figure 4.b presents a specific example, while Figure 4.c converts this example into its matrix-vector form (ignoring changes in H, W, D dimensions due to lack of padding).\nAccording to Figure 4, the matrix-vector form of 1-0 Conv3D can be easily derived and is consistent with Eq. (1). The difference lies in the fact that $W_i$ has depth, so the corresponding $W'$ needs to be recombined as shown in Figure 4.c.\nSimilarly, Figures 5.a, b, and c illustrate the general process, a simple example, and the corresponding matrix-vector form of I-1 Conv3D, respectively. According to Figure 5, the matrix-vector form of I-O Conv3D is consistent with (1). The difference is that both $x_i$ and $W_i$ have depth, so the corresponding $x'$ and $W'$ need to be obtained as shown in Figure 5.c."}, {"title": "The Matrix-Vector Method for Mean Pooling", "content": "Mean pooling is also an important technique in the field of CV, often used in conjunction with convolution. To demonstrate that CNNs can be expressed in the form of UAT, we need to prove that mean pooling can also be represented in matrix-vector form. In Figure 6, we provide an example. Figure 6.a shows the process of mean pooling, while Figure 6.b illustrates its corresponding matrix-vector form. Based on Figure 6, we can conclude that mean pooling can indeed be represented in matrix-vector form."}, {"title": "Transformer for CV", "content": "In UAT2LLMs, it has been demonstrated that the feedforward network (FFN) and MHA components in the Transformer architecture can be represented in matrix-vector form. Given that ViTs are also based on the Transformer framework, we only need to focus on the differences between Transformers in CV and those in UAT2LLMs. A key feature of ViTs is that they divide the original image into multiple patches: $X_{1,1}, ..., X_{n,m}$. Each image patch is then reshaped into row vectors $x_{1,1}, ..., x_{n,m}$, which are concatenated along the column direction to obtain x, as shown in Figure 7. MHA and FFN operations are then applied, so aside from the initial processing part, the subsequent transformation process completely follows the description in UAT2LLMs. Therefore, it is easy to deduce that ViTs are also specific implementations of the UAT."}, {"title": "Summary", "content": "We have demonstrated that various modules in CV, including 2D convolution, 3D convolution, mean pooling, MHA, and FFN, can all be represented by diamond matrix multiplication. Considering the relationship between diamond matrix multiplication and matrix multiplication:\n$W \\Diamond x = W^T x$. Therefore, unless otherwise specified, we will conveniently use $W'x'$ to represent the matrix-vector form of them. Here, $W'$ is derived from the parameters W of the various modules, and $x'$ is derived from the inputs x of the modules."}, {"title": "Discussion", "content": "In this section, we will answer the questions in the field of CV proposed in the Introduction."}, {"title": "Why must CNNs be Deep?", "content": "There are two main reasons why CNNs typically have deep layers. The first reason stems from the learning paradigm of CNNs. The design of CNNs is based on two well-known principles: local receptive fields (i.e., local features in images often contain important information) and spatial invariance (these important local features can appear anywhere in the image). These principles necessitate learning from each small region of the image, with all small regions sharing the same parameters. If parameters differed, it would imply that a certain object must appear in a specific part of the image, which is clearly unreasonable.\nThe above reasons dictate that image learning occurs in small patches, which explains why convolutional kernels are typically small. However, if the network has only a few layers, such as a single layer, it means that each small patch is learned independently. We know that objects in an image are generally composed of multiple such patches. Observing only a part of the image cannot provide a clear identification of the object. Therefore, a larger receptive field is needed, which can be achieved by increasing the number of layers to gather global information.\nThe second reason is derived from the UAT. As shown in Figure 1, increasing the number of layers in the network means a larger N, which brings the UAT closer to approximating the target function. An image can be understood as a special function in a high-dimensional space, exhibiting strong correlations in 2D or 3D space. These two reasons are also applicable to ViTs. Figure 7 shows the preprocessing of ViTs, where x is the input to the ViTs. It is evident that each row shares the same parameters, and the learning of each row adheres to the principles of local receptive fields and spatial invariance."}, {"title": "Why Residual-Based CNNs Excel in CV: Superior Generalization Ability", "content": "In the previous discussion, we explained why deep networks are essential for learning in CNNs. However, why did VGG, despite being a deep network, not dominate the subsequent development of CV, whereas residual-based CNNs did? ResNet, one of the most influential architectures in the field, has significantly shaped the design of later network architectures, with the residual structure appearing in many subsequent networks. What exactly endows the residual structure with such powerful capabilities? We think the answer is the generalization capability given by residual structure. To analyze the source of this power, we use the Matrix-Vector Method to express parts of VGG and the residual structure in corresponding equations, and then compare them with the UAT.\nFigure 8 illustrates the basic structures of VGG and ResNet. Based on this figure, a three-layer VGG can be written as Eq.(3), which perfectly aligns with the multilayer UAT mathematical form proposed by Hornik (Hornik, Stinchcombe, and White 1989). On the other hand, a two-layer Residual-based CNNs can be expressed in the form shown in Eq.(4) (see Appendix A.1 for detail.), conforming to the UAT form given in Figure 1. In this equation, $b_{i+1,1}$ is calculated from $W_{i,1}$, $W_{i,2}$, $x_i$, $b_{i,1}$, $b_{i,2}$, $b_{i+1,1}$. It is evident that both VGG and ResNet are concrete implementations of UAT. Theoretically, their approximation capabilities should be equivalent, and any differences could be mitigated by increasing the number of network layers. So why is ResNet significantly more powerful than VGG?\n$x_{i+3} = \\sigma(W'_{i+2}[\\sigma(W'_{i+1}\\sigma(W'_{i,1} x_i + b'_{i,1}) + b'_{i+1}) + b'_{i+2}]$ (3)\n$+ b'_{i+3})$\n$x'_{i+2} = x'_i + W'_{i+1,1}x_i + W'_{i+1,2} \\sigma(W'_{i,1}x_i$  (4)\n$+ b'_{i+1,1}) + b'_{i+1,2}$\nThe primary reason lies in the dynamic approximation capabilities of residual networks. As seen in Eq. (3), once a VGG network is trained, its corresponding UAT parameters are fixed. This means that a VGG network can only approximate a single, fixed function. Although UAT has strong approximation abilities, image data is highly variable, and the corresponding function often changes. In contrast, residual networks can dynamically approximate the corresponding function based on the input because the biases in residual networks are influenced by the input. For example, in Eq. (4), $b'_{i+1,1}$ is affected by the input x.\nFor convenience, we do not explicitly show which parameters influence other parameters in the final merged mathematical expression of the model. If a parameter in the UAT model is influenced only by other network parameters, it can be considered fixed once the network is trained. We denote such parameters with an overline, such as $\\overline{W}$. If a parameter in the UAT model is influenced by the input, we denote it with a hat, such as $\\hat{b}$.\nTherefore, while the original UAT formula has strong approximation capabilities, using it for approximation (equivalent to using a VGG network in CV) means that once training is complete, the UAT parameters are fixed, and the function it can approximate is also fixed. In contrast, in residual networks, the UAT parameters can change based on the input, allowing the function being approximated to vary accordingly. This adaptability is the fundamental source of the superiority of residual networks."}, {"title": "The Difference between Residual-Based CNNs and ViTs", "content": "In the previous section, we used the matrix-vector method to represent VGG and ResNet in their corresponding mathematical forms and analyzed them from the perspective of the UAT. In this section, we will apply the same method to analyze the Transformer in ViTs and compare it with residual-based CNNs. Figure 9 shows the general form of a Transformer. After derivation, the mathematical form of a two-layer Transformer network corresponds to Eq. (5) (see Appendix A.2 for detail.). Clearly, the mathematical representation of ViT also conforms to UAT, with some parameters influenced by the input information.\nThe main difference between Transformer networks and residual-based multi-layer convolutional networks lies in how input information affects their corresponding UAT parameters. In Transformer networks, input information influences both weight and bias in the corresponding UAT. In contrast, in networks constructed with multiple residual convolutions, input information only influences bias in the corresponding UAT. Essentially, both types of networks have the ability to dynamically approximate functions based on input. This aligns with their nearly identical performance in the field of CV, supporting the theoretical foundation that both are specific implementations of UAT capable of dynamically approximating the corresponding functions based on input.\n$x'_{i+2} = W'_{i+1,1}x'_i + W'_{i+1,2} \\sigma(W'_{i,1}x + b'_{i,2}) + b'_{i,1}$ (5)\n$+W'_{i+1,3}\\sigma(W'_{i+1,3}x + b'_{i+1,2})$"}, {"title": "The Theory behind Lora and Pruning", "content": "In UAT2LLMS, UAT has been used to explain why LoRA and pruning are feasible. In the field of CV, we have proven that both convolution-based CNNs and Transformer-based ViTs are concrete implementations of UAT. Therefore, the principles behind LoRA and pruning in CV are the same as those mentioned in UAT2LLMs. Specifically, LoRA works by layer-wise adjustment of parameters, while pruning is effective because some layer parameters have minimal impact on the overall result."}, {"title": "Conclusion", "content": "This paper delves into the theoretical foundations of deep learning in the field of CV. Specifically, the current CV landscape is primarily dominated by CNNs and Transformer models. We utilize the matrix-vector method to unify these models under the framework of the UAT. Based on this framework, we provide explanations for several common issues and techniques in CV. The requirement for deep networks in CNNs is jointly determined by the intrinsic characteristics of image data and the demands of UAT theory. The robust generalization ability of residual networks stems from their ingenious design, which enables UAT to dynamically adapt to corresponding functions based on input data. Similarly, Transformer-based models possess this ability, with the key difference lying in which parameters of UAT they influence. Residual networks primarily affect the bias term, while Transformers influence both the weights and the bias. Additionally, the feasibility of LoRA operations in CV arises because it can fine-tune the parameters of different UAT layers based on input data characteristics. The viability of pruning techniques is due to the fact that the parameters of certain network layers have minimal impact on the final outcome."}, {"title": "The UAT Format of Residual-based CNNS and Transformer-based ViTs", "content": "In this section, we will present the Matrix-Vector form of residual-based CNNs and Transformer-based ViTs and their relationship with the UAT. Figure 1 and 2 show a schematic of a residual-based convolution and Transformer. We have already established that convolution and components in a Transformer can be represented in a matrix-vector form. Therefore, to illustrate its relationship with UAT, we will use Figure 1 and 2, with the input $x_i$, derive the mathematical form corresponding to $x_{i+2}$. We will express this in matrix-vector form, so parameters and inputs/outputs will have a prime (') in the upper right corner to indicate their matrix-vector form.\nDue to the complexity of the variables involved, we will combine some terms to align the resulting equations with the standard UAT form. To distinguish these parameter variables, we will keep the original variables unchanged. If a parameter variable is influenced only by other variables, we will denote it with a bar on top, such as $\\overline{W}$. If it is influenced by the input $x_i$, we will mark it with a hat, such as $\\hat{W}$."}, {"title": "The UAT Format of Residual-based CNNS", "content": "Through derivation and simplification, we obtain the matrix-vector mathematical representation for $x_{i+2}$, as shown in Eq.1. Comparing this with the standard UAT form, it is evident that the parameters in UAT are fixed after training. In contrast, the bias terms in the UAT corresponding to residual-based CNNs can change dynamically based on the input."}, {"title": "The UAT Format of Transformer-based ViTs", "content": "Eq.(2) and (3) give an example of the matrix-vector format of MHA and FFN. Eq. (4) give the corresponding matrix-vector format of $x_{i+2}$. It is obvious that the bias and weight terms in the UAT corresponding to Transformed-based ViTs can change dynamically based on the input.\n$MHA(x) = W'_1 x$ (2)\n$FFN(x) = W'_{i,3} \\sigma(W'_{i,2}x + b'_{i,2}) + b'_{i,3}$ (3)"}]}