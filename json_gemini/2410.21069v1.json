{"title": "EMOCPD: Efficient Attention-based Models for Computational\nProtein Design Using Amino Acid Microenvironment", "authors": ["Xiaoqi Ling", "Cheng Cai", "Zhaohong Deng", "Lei Wang", "Zhisheng Wei", "Jing Wu"], "abstract": "Computational protein design (CPD) refers to the use of computational methods to design proteins.\nAlthough significant progress has been made in recent years, it remains a challenging task. Traditional\nmethods relying on energy functions and heuristic algorithms for sequence design are inefficient and do not\nmeet the demands of the big data era in biomolecules, with their accuracy limited by the energy functions\nand search algorithms. Existing deep learning methods are constrained by the learning capabilities of the\nnetworks, failing to extract effective information from sparse protein structures, which limits the accuracy of\nprotein design. To address these shortcomings, we developed an Efficient attention-based Models for\nComputational Protein Design using amino acid microenvironment (EMOCPD), which aims to predict the\ncategory of each amino acid in a protein by analyzing the three-dimensional atomic environment surrounding\nthe amino acids, and optimize the protein based on the predicted high-probability potential amino acid\ncategories. EMOCPD employs a multi-head attention mechanism to focus on important features in the sparse\nprotein microenvironment and utilizes an inverse residual structure to optimize the network architecture. The\nproposed EMOCPD achieves over 80% accuracy on the training set and 68.33% and 62.32% accuracy on\ntwo independent test sets, respectively, surpassing the best comparative methods by over 10%. In protein\ndesign, the thermal stability and protein expression of the predicted mutants from EMOCPD show significant\nimprovements compared to the wild type, effectively validating EMOCPD's potential in designing superior\nproteins. Furthermore, the predictions of EMOCPD are influenced positively, negatively, or have minimal\nimpact based on the content of the 20 amino acids, categorizing amino acids as positive, negative, or neutral.\nResearch findings indicate that EMOCPD is more suitable for designing proteins with lower contents of\nnegative amino acids. The method proposed in this study enriches the tools for protein design, with the model\nexpected to enhance the capacity and efficiency of protein design.", "sections": [{"title": "1 Introduction", "content": "Computational Protein Design (CPD) refers to a class of methods that utilize computer algorithms and\nmodels to predict and optimize protein structures and functions, aiming to discover better protein structures\nfor specific biological or application purposes. Over the past few decades, CPD has achieved significant\nsuccess in various fields, including enzyme engineering[1-3], vaccine design[4, 5], antibody design[6, 7],\nmembrane protein design[8, 9], and protein interactions[10-12]. For example, recent work published in\nNature by Hongyuan Lu et al[13]. successfully designed a PET enzyme with high stability and hydrolytic\nactivity, capable of hydrolyzing 51 types of PET plastics in just one week, potentially addressing the global\nplastic waste issue. Ongoing research indicates that CPD can benefit society in health, medicine, environment,\nand chemical engineering. Despite the notable advancements, effectively designing proteins with specific\nfunctions remains a considerable challenge.\nTraditional CPD research primarily relies on Anfinsen's folding thermodynamics hypothesis[15]. The\ncore idea of this hypothesis is that, under natural conditions, proteins always fold into the conformation with\nthe lowest free energy. This means that a protein's three-dimensional structure is entirely determined by its\namino acid sequence[16-17]; each protein sequence corresponds to a unique three-dimensional structure,\nalthough similar structures may correspond to multiple sequences. Traditional CPD methods assume that the\nstructure with the lowest free energy is the optimal one. Therefore, the focus of traditional CPD is on\nsystematically substituting amino acids in the protein to find the sequence that minimizes free energy. The\nprimary concerns of traditional CPD are the design of free energy calculation functions and the search for\nthe lowest energy amino acid sequences. Widely used energy function-based CPD methods include\nRosettaDesign[18], EvoDesign[19], and SCUBA[20]. These methods have developed their energy functions\nand utilize heuristic algorithms to search the vast amino acid sequence space for the lowest energy sequences.\nHowever, energy function-based methods are often limited by the design of their energy functions and can\neasily become trapped in local optima, leading to proteins that do not achieve the target structure and resulting\nin a low success rate for protein design experiments[21].\nIn recent years, deep learning has made significant breakthroughs across various fields, including\ncomputer vision[22-24], natural language processing[25-27], computational chemistry[28, 29], and\ncomputational biology[30-32]. New methods using deep learning for protein design have emerged in the\nCPD domain. These methods aim to learn the correspondence between amino acid residues and their\nsurrounding structural features based on the local chemical environment of the residues. By leveraging these\nrelationships, they modify certain amino acids in the protein without disrupting the protein backbone, thereby\nachieving protein design. Depending on the network architecture used, these methods can be categorized into\nthose based on Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), and Graph\nConvolutional Networks (GCN).\nRepresentative MLP-based methods include SPIN[33] and SPIN2[34]. SPIN inputs local geometric\nfeatures and global energy features of amino acids into a two-layer neural network to predict amino acid\ntypes, achieving an accuracy of 30.7% on a test dataset consisting of 500 proteins (TS500). SPIN2 builds on\nSPIN by adding more local geometric features and improving the neural network structure, increasing the\nprediction accuracy to 34.6%. MLP-based methods are limited by the calculation of handcrafted features; the\nmore comprehensive the handcrafted feature design, the better the classification performance. However,\nhandcrafted features often have limitations, causing MLP-based prediction accuracy to reach a bottleneck.\nRepresentative CNN-based methods include SPROF[35], ProDCoNN[36], and DenseCPD[37]. The\nSPROF method uses a residue adjacency matrix as input, establishing a hybrid neural network model that\ncombines CNN and biLSTM, which improves amino acid prediction accuracy to 39.8%. ProDCONN employs\na 3D CNN based on three-dimensional environmental features composed of N\u3001Ca\u3001C\u3001O atoms for amino\nacid classification, achieving a prediction accuracy of 42.2% on a test set of 500 proteins. DenseCPD uses\nthe DenseNet[38] architecture for its prediction model, achieving an accuracy of 54.5% on the TS500 test set\nbased on three-dimensional environmental features composed of N\u3001Ca\u3001C\u3001O and Cp atoms. CNN-based\nmethods primarily model the three-dimensional environmental features of amino acids. However, the sparse\ndistribution of atoms in three-dimensional space can hinder the convergence of neural network training. To\novercome atom sparsity and accelerate network convergence, CNN-based methods typically apply Gaussian\nfunctions to blur the three-dimensional environmental features, which results in some loss of information and\naffects the final prediction performance.\nRepresentative GCN-based methods include GraphTrans[39] and ProteinMPNN[40]. GraphTrans\nconstructs a graph for the target protein's amino acids, encoding the nodes as types of amino acid residues\nand the edges based on the distances, directions, and origin vectors of the residues, followed by training a\nTransformer[25] encoder-decoder model. GraphTrans achieves a prediction accuracy of 49.6% on its test set\nand 44.7% on the TS500 dataset. ProteinMPNN employs two three-layer message-passing networks as the\nencoder and decoder. This method uses the encoder to extract node and edge features from the distances of\namino acid residues N\u3001Ca\u3001C\u3001O and Cp atoms., then utilizes the decoder to convert these features into\namino acid probabilities, ultimately achieving an accuracy of 58.1% on the TS500 dataset. GCN-based\nmethods establish graphs of the atomic environment, which helps mitigate the sparsity of atoms in space but\nneglects the three-dimensional positional information of the atoms.\nTo address the limitations of existing deep learning approaches in protein design and to further enhance\nthe accuracy of CPD, this paper proposes a new neural network model, EMOCPD, for amino acid recognition\nand protein design. EMOCPD incorporates a multi-head attention mechanism to learn important features\nfrom sparse three-dimensional microenvironments of amino acids, and it introduces the Inverted Residual\nMobile Block (iRMB) from the EMO model[41] to enhance the model's learning performance. Our\nexperimental results demonstrate a significant improvement in prediction accuracy compared to existing deep\nlearning-based CPD methods. The amino acid prediction results obtained using EMOCPD facilitate the\ndesign of proteins with superior properties."}, {"title": "2 Materials and Methods", "content": "This study utilizes a self-constructed training dataset, as well as two commonly used testing datasets:\nTS50 [36] and TS500 [37]."}, {"title": "2.1.1 Training Dataset", "content": "To construct the training dataset, we selected proteins from the PDB database. The screening process\nwas as follows: First, to avoid over-sampling similar protein structures, which could lead to model overfitting,\nwe filtered proteins with a sequence similarity not exceeding 50%. Second, to ensure high-quality proteins,\nwe excluded those with a resolution less than 2.5 \u00c5. Finally, we removed proteins that were included in the\nTS50 and TS500 testing datasets. After the screening, a total of 20,118 proteins were selected for training,\nwith 298 proteins used for validating the model's performance during training. Additionally, to avoid\nexcessive sampling of amino acids from large proteins (number of amino acids > 200), we referenced\nliterature [14] and implemented the following approach: Let nnn be the number of amino acids in a protein.\nIf nnn exceeds 200, we randomly selected 100 amino acids; otherwise, we randomly selected all nnn amino\nacids. Following this processing, our constructed training set for amino acid recognition contains 1,605,000\namino acids, of which 8,000 are used for validating model performance during training. The filtered protein\nfiles used in this study can be accessed from the PDB database [42], and the PDB IDs of each protein are\nlisted in Appendix 1."}, {"title": "2.1.2 Testing Dataset", "content": "The TS50 and TS500 datasets have been used in numerous studies [33, 34, 35, 36, 37] to evaluate the\nperformance of CPD methods. Therefore, we also employed these two datasets for performance testing and\ncompared them with other representative CPD methods. The TS50 dataset comprises 50 single-chain protein\nstructures, totaling 6,861 amino acids. The TS500 dataset consists of 500 proteins (including 887 protein\nchains), encompassing a total of 216,429 amino acids. The protein files for both datasets can be obtained\nfrom the PDB database, and the PDB IDs and chain numbers of the three-dimensional structures are provided\nin Appendices 2 and 3."}, {"title": "2.2 EMOCPD Model", "content": "The proposed protein design model, as illustrated in Figure 1, consists of several key modules: a) Atom\nFeature Construction Module; b) Microenvironment Grid Construction Module; c) EMO Deep Learning\nModule; d) MLP Classifier Module. The function of the Atom Feature Construction Module is to encode\nprotein atoms into atomic feature vectors. The Microenvironment Grid Module is responsible for constructing\na grid from the atomic feature vectors of the protein to represent the amino acid microenvironment. The EMO\nDeep Learning Module utilizes the EMO model to learn deep features from the amino acid microenvironment.\nFinally, the MLP Classifier Module uses these deep features to predict the amino acid category that best fits\nthe amino acid microenvironment. Each module will be described in further detail in the subsequent sections."}, {"title": "2.2.1 Atom Feature Construction Module", "content": "To compute the features of protein atoms for establishing the environmental representation of amino\nacids, we performed the following steps: 1) Hydrogen atoms were added to the protein in the PDB file; 2)\nThe free charge (FC) of each atom was calculated for the protein with added hydrogen atoms; 3) The solvent-\naccessible surface area (SASA) of each atom was calculated. We used the bioinformatics program\nPDB2PQR[43] to add hydrogen atoms and compute the FC, while FreeSASA[44] was used to calculate the\nSASA of each atom. After these preparations, each atom was encoded as a 7-dimensional vector, where the\nfirst five dimensions represent the one-hot encoding of the atom type, and the last two dimensions represent\nthe FC and SASA of the atom."}, {"title": "2.2.2 Microenvironment Grid Construction Module", "content": "This study transforms the amino acid recognition problem into an image classification problem. To achieve\nthis, the three-dimensional structure of the amino acids is converted into a 3D voxel grid. A 3D grid is\nestablished with the amino acid C\u00df as the center, where the direction of the amino acid N Ca bonds\nserves as the x-axis, the direction perpendicular to the N - Ca - C plane and positively correlated with\nthe Ca - C\u00df bond direction serves as the z-axis, and the direction orthogonal to the xz-plane serves as the y-\naxis. The grid size is 20\u00c5 \u00d7 20\u00c5 \u00d7 20\u00c5, with each cubic unit measuring 1\u00c5 \u00d7 1\u00c5 \u00d7 1\u00c5. The atom C\u00df is\nselected as the center to maximize the capture of the environment surrounding the amino acid side chain. For\nglycine, which has no C\u00df atoms, its coordinates are calculated from the N - Ca lengths of and N - Ca\nC\u00df bond angles of the other amino acids. After removing the side chain (R-group) atoms of the central amino\nacid, each grid unit is encoded as the sum of the feature vectors of the atoms it contains. Ultimately, each\namino acid is represented as a 7 \u00d7 20 \u00d7 20 \u00d7 20 voxel grid, with the label corresponding to the amino acid\ntype."}, {"title": "2.2.3 EMO Deep Learning Module", "content": "The overall framework of the proposed EMO model is shown in Figure 2, which consists of one Stem,\nfour iRMB modules, four MHSA-iRMB modules, and three DownSample modules. The specific parameters\nfor each module are provided in the figure. Figure 3 illustrates the various modules of the EMO model, with\ndetailed descriptions to follow in subsequent sections."}, {"title": "1) CNA Component", "content": "The CNA (ConvLayer, NormLayer, Active) component is an extension of the CBA (ConvLayer,\nBatchNormLayer, Active) module from ResNet[45]. It allows for the selection of the desired normalization\nlayer and activation function based on specific requirements. As this component is an important part of\nmodules such as Stem in the proposed model (Figure 3(b)\u2013(e)), it will be introduced first.\nAs shown in Figure 3(a), the CNA component consists of a 3D convolution layer, a normalization layer,\nand an activation function layer. Given an input $A_{in}$, a CNA component can be represented as:\n$f_{CNA}(A_{in}) = f_{active}(f_{norm}(f_{conv}(A_{in})))$ (1)\nHere, $f_{conv}$ denotes the 3D convolution layer, while $f_{norm}$ refers to the normalization process, and\n$f_{active}$ signifies the activation function. Common normalization layers include batch normalization (bn) and\nlayer normalization (ln), while common activation functions include relu, sigmoid, and silu. The CNA\ncomponent will be consistently referred to as $f_{CNA}()$ in subsequent discussions."}, {"title": "2) Stem Module", "content": "Figure 3(b) illustrates the Stem module of EMOCPD, which includes three CNA modules, a batch\nnormalization layer, a global max pooling layer, a 3D convolution projection layer, and three residual\nconnections. The part outlined by the red dashed box can be considered as an SE module (Squeeze-and-\nExcitation) with a scaling factor of 1[46]. Given an input $A_{in}$, the SE module can be expressed as:\n$f_{SE}(A_{in}) = f_{CNA}(f_{CNA}(f_{GMP}(A_{in})))$ (2)\nThe equation $GMP$ represents the global max pooling layer. Let $B = f_{CNA}(f_{bn}(A_{in}))$. Therefore, the\nStem module can be expressed as:\n$f_{stem}(A_{in}) = A_{in} + f_{conv}(B + f_{SE}(B) \\cdot B)$ (3)\nIn the equation, $f_{se}$ represents the SE module, and $f_{bn}$ denotes the Batch Normalization layer. The\nfirst CNA module in the Stem module utilizes a Batch Normalization layer for normalization, with the\nactivation function set to silu. The number of convolutional filters is fl, and the filter size is 3 \u00d7 3 \u00d7 3. In the\nSE module, the two CNA modules use relu and Sigmoid activation functions, respectively, with both having\nfl filters and a filter size of 1 \u00d7 1 \u00d7 1. $f_{conv}$ represents the 3D convolutional layer, with a filter size of 1 \u00d7 1\n\u00d7 1 and f2 filters. The specific values for the parameters fl and f2 in the Stem module of the EMO model are\ndetailed in Figure 2."}, {"title": "3) iRMB Module", "content": "Figure 3(c) illustrates the iRMB module, which consists of a batch normalization layer, two CNA\nmodules, a 3D convolutional layer, and two residual connections. Given an input $A_{in}$, let $B =$\n$f_{CNA}(f_{bn}(A_{in}))$. Then, an iRMB module can be represented as:\n$f_{iRMB}(A_{in})=A_{in} + f_{conv}(f_{CNA}(B) + B)$ (4)\nThe two CNA modules in the iRMB module utilize relu and silu activation functions, with convolution\nkernel sizes of 1\u00d71 \u00d71 and 3 \u00d7 3 \u00d7 3, respectively. The first CNA module does not include a normalization\nlayer, while the second one employs batch normalization. The kernel size of the 3D convolutional layer is 1\n\u00d7 1 \u00d7 1, and the number of kernels for both CNA modules and the 3D convolutional layer is denoted as f.\nThe specific values for the parameter f in the iRMB module of the EMO model are detailed in Figure 2."}, {"title": "4) Down Sample Module", "content": "Figure 3(d) depicts the down sample module, which consists of a batch normalization layer, two CNA\nmodules, and a 3D convolutional layer. Given input $A_{in}$, a down sample module can be represented as\nfollows:\n$f_{down}(A_{in})=f_{conv} (f_{CNA} (f_{CNA}(f_{bn}(A_{in})))) $ (5)\nThe two CNA modules in the down sample module utilize relu and silu activation functions, both with\nfl filters. The kernel sizes are 1 \u00d7 1 \u00d7 1 and 3 \u00d7 3 \u00d7 3, respectively. The first CNA module does not include\na normalization layer, while the second employs batch normalization. The 3D convolutional projection layer\nhas a kernel size of 1 \u00d7 1 \u00d7 1, with f2 filters and a stride of 2. The down sample module effectively reduces\nthe feature scale by half. The specific values for the parameters fl and f2 in the Down Sample module of the\nEMO model are detailed in Figure 2."}, {"title": "5) MHSA IRMB Module", "content": "Figure 3(e) illustrates the MHSA iRMB module, which integrates a multi-head self-attention mechanism\ninto the iRMB module. This module comprises a batch normalization layer, two CNA modules, a multi-head\nself-attention mechanism module, a 3D convolution layer, and two residual connections. The multi-head self-\nattention mechanism operates on the features generated by the first CNA module. Given the input $A_{in}$, the\nmulti-head attention mechanism can be represented as follows:\n$f_{SMHSA}(A_{in}) = Concat(H_1, \u2026\u2026\u2026, H_n)W_o$ (6)\nIn the equation, $W_o$ represents the weighted matrix for the multi-head attention scores, and $H_i$ denotes\nthe self-attention score for the i-th head, which can be expressed as:\n$H_i = Attention(Q_i, K_i) = SoftMax(\\frac{Q_iK_i^T}{\\sqrt{d_{k_i}}})$ (7)\nIn this context, $Q_i and K_i$ refer to the query and key vectors for the i-th self-attention, while $d_{k_i}$ denotes\nthe dimension of the i-th key vector, and V represents the value matrix. In the MHSA iRMB module, the\ncalculations for $Q_i, K_i, and V$ are given by: $Q_i = f_{CNA}(f_{in}(A_{in})), K_i = f_{CNA}(f_{bn}(A_{in})),$ and $V =$\n$f_{CNA}(f_{in}(A_{in}))$. Let $Att = f_{MHSA}(A_{in}) \\cdot V$ represent the attention mechanism, then the MHSA iRMB\nmodule can be expressed as:\n$f_{SMHSA_iRMB}(A_{in}) = f_{in}(A_{in}) + f_{conv}(f_{CNA}(Att) + Att)$ (8)\nIn the equation, $f_{in}$ represents the layer normalization layer. In the MHSA iRMB module, the two CNA\ncomponents utilize the relu and silu activation functions, respectively, with both having the same number of\nfilters fl. The filter sizes are 1 \u00d7 1 \u00d7 1 and 3 \u00d7 3 \u00d7 3. The first CNA module does not include a normalization\nlayer, while the second uses batch normalization. The 3D convolution projection layer has a filter size of 1 \u00d7\n1 \u00d7 1 and a number of filters f2. The specific values for the parameters fl and f2 in the MHSA iRMB module\nof the EMO model can be found in Figure 2."}, {"title": "2.2.4 MLP Classifier Module", "content": "Figure 3(f) illustrates the MLP classifier module, which consists of a global max pooling layer, a flatten\nlayer, two linear layers, and relu activation function layers in between. Given the input $A_{in}$, the MLP\nclassifier can be represented as follows:\n$f_{MLP_classifier}(A_{in}) = f_{linear} (f_{relu} (f_{linear} (f_{flatten} (f_{GMP}(A_{in})))))$ (9)\nIn the equation, $f_{linear}$ denotes the linear layer, $f_{relu}$ represents the relu activation function layer, and\n$f_{flatten}$ indicates the flatten layer. The hidden node counts for the two linear layers are 720 and 20,\nrespectively."}, {"title": "2.2.5 Model Training", "content": "For the proposed EMOCPD model, we implemented and optimized it using the PyTorch[47] framework.\nThe input is a 7 \u00d7 20 \u00d7 20 \u00d7 20 voxel grid representing amino acids, and the output is a 20-dimensional\nvector indicating the predicted amino acid labels. During training, the loss function used is the cross-entropy\nloss, and the optimizer is the Adam optimizer[48], with a learning rate set to $10^{-5}$ and a weight decay\ncoefficient of $10^{-3}$; the remaining parameters are set to PyTorch's default values. The training is configured\nfor 8 epochs, with 10,700 steps per epoch, each containing 150 samples."}, {"title": "3 Experimental Results", "content": "To evaluate the performance of the proposed method, experimental studies were conducted from three\naspects: 1) experimental analysis on the training set; 2) comparison with existing representative methods on\nthe test set; 3) biological experimental validation of protein design using EMOCPD."}, {"title": "3.1 Evaluation Metrics", "content": "In this paper, we used four metrics\u2014accuracy (Acc), recall (Rec), precision (Pre), and F1 score\u2014to\nevaluate the performance of our proposed method. Accuracy is used to assess the overall prediction\nperformance for the 20 amino acids, while recall, precision, and F1 score are used to evaluate the prediction\nperformance for each individual amino acid. The definitions of the four metrics are as follows:\n$ACC=\\frac{TP+TN}{TP+TN+FP+FN}$ (10)\n$Rec=\\frac{TP}{TP+FN}$ (11)\n$Pre =\\frac{TP}{TP+FP}$ (12)\n$F1=\\frac{2TP}{2TP+FP+FN}$ (13)\nIn the equations, TP, TN, FP, and FN represent the counts of true positives, true negatives, false positives,\nand false negatives, respectively. Accuracy refers to the ratio of correctly classified samples to the total\nnumber of samples; recall indicates the proportion of actual positive samples that are correctly identified as\npositive; precision refers to the proportion of correctly predicted positive results among those predicted as\npositive; and the F1 score is the harmonic mean of recall and precision, allowing for a balanced assessment\nof classification performance.\nAdditionally, we also utilized TOP-K accuracy. TOP-K accuracy is a commonly used metric in image\nclassification, defined as the proportion of correct labels included in the top K predicted results ranked by\ntheir probabilities in descending order."}, {"title": "3.2 Training Dataset Results", "content": "Figure 4 illustrates the change in training and validation accuracy over the number of iterations, while\nFigure 5 presents the confusion matrix for the predicted results of the 20 amino acids. From Figure 4, it can\nbe seen that after 8 epochs and a total of 85,600 iterations, our model achieves an accuracy exceeding 80%\non the training samples, with an upward trend still evident as the number of iterations increases. However,\nthe accuracy on the validation samples stabilizes after approximately 60,000 iterations, reaching a peak of\naround 63%. This indicates a significant risk of overfitting if training continues, leading us to decide to stop\ntraining after the 8th epoch.As shown in Figure 5, our model achieves an accuracy of over 40% for all amino\nacids except glutamine (GLN, Q), with particularly high accuracies exceeding 90% for proline (PRO, P),\nglycine (GLY, G), and cysteine (CYS, C). Based on this analysis, we conclude that our model provides\naccurate predictions for amino acid types and can be utilized for protein design."}, {"title": "3.3 Testing Dataset Results", "content": "Table 1 presents a comparison of amino acid prediction accuracies between our method (EMOCPD) and\nfive other methods: DenseCPD [37], ProDCoNN [36], SPROF [35], SPIN2 [34], and Wang's Model [49]\non the TS500 and TS50 datasets. As shown in Table 1, our method achieves accuracies of 68.33% and 63.32%\non the two datasets, respectively. Among the six methods compared, EMOCPD has the highest accuracy,\nexceeding DenseCPD (the second-best method) by more than 10%."}, {"title": "3.4 Case Study", "content": "To clearly illustrate the advantages of our algorithm compared to others, we conducted a case study\nusing the wild-type PETase (WTPETase, PDB ID: 5xjh). WTPETase is a single-chain protein composed of\n263 amino acids, which catalyzes the hydrolysis of PET plastic. For clarity in prediction results, Figure 8\ndisplays the amino acid sequence of WTPETase along with the predictions made by our method, EMOCPD,\nand DenseCPD [37]. From the figure, it can be seen that EMOCPD correctly predicted 192 amino acids with\nan accuracy of 73.00%, while DenseCPD correctly predicted 155 amino acids with an accuracy of 58.94%.\nAdditionally, EMOCPD and DenseCPD correctly predicted 65 and 28 amino acids, respectively, that the\nother method misclassified. The number of amino acids predicted correctly by both methods is 127, while\nthe number predicted incorrectly by both is 43. This indicates that EMOCPD provides more accurate\npredictions for WTPETase compared to DenseCPD."}, {"title": "3.5 Biological Experimental Results", "content": "The experimental results above demonstrate the excellent theoretical performance of EMOCPD. To\nprove that EMOCPD can design high-quality proteins, we used it to perform site-directed mutagenesis on\nseveral key sites of the PET-degrading enzyme TfM7. The effects of these mutations were evaluated through\nenzyme activity assays and stability tests, as shown in Figure 9.\nFor example, the mutant N213T exhibited relative enzyme activities of 86.30% and 80.18% after\nincubation at 60\u00b0C for 12 and 24 hours, respectively, indicating high stability. More importantly, under\nconditions of 70\u00b0C for 12 hours, the relative enzyme activity of mutant N213T was 17.99%, which is 1.8\ntimes that of the wild-type protein; after 24 hours, it showed a relative enzyme activity of 8.81%, which is\n2.9 times that of the wild-type protein. This suggests that N213T has enhanced stability under high-\ntemperature conditions. Additionally, mutants D86N and A112S also exhibited higher relative enzyme\nactivities than the wild-type after 24 hours at both 60\u00b0C and 70\u00b0C, indicating that the mutants predicted by\nour model are better suited for high-temperature, long-duration reaction conditions.\nWe also assessed the protein expression levels of different mutants, and the results are shown in Figure\n10. Among them, the expression level of the mutant I83M is 0.09 mg/OD \u2022 ml, which is 1.8 times that of the\nwild type. Additionally, the expression levels of mutants R47K and A112S also outperform the wild type.\nThis indicates that our model can optimize the expression levels of wild-type proteins as well."}, {"title": "4 Discussion", "content": "In previous works [36, 37], the accuracy of the proposed methods showed a strong correlation with the\nabundance of one or more combinations of amino acids in the predicted proteins, which may be a common\ncharacteristic of deep learning approaches. To evaluate whether our method\u2014EMOCPD\u2014exhibits this\ncharacteristic, this section discusses the relationship between the prediction accuracy of EMOCPD and the\ncontent of the 20 amino acids.\nTo investigate this relationship, we calculated the Pearson correlation coefficient and significance test\nprobabilities between the amino acid prediction accuracy for individual protein structures and the content of\nthe 20 amino acids. The results are shown in Table 2. From the perspective of the correlation coefficient,\nEMOCPD demonstrated low Pearson correlation coefficients for all amino acids, none exceeding 0.4,\nindicating a weak correlation between prediction accuracy and amino acid content. In terms of significance,\nmore than half (11 amino acids) had significance probabilities less than 1%, suggesting that the prediction\naccuracy of EMOCPD is significantly influenced by the content of these amino acids. Among the 11 amino\nacids that passed the 99% significance test, 7 amino acids had negative Pearson coefficients, while 4 amino\nacids were positive. We categorize the former as negative amino acids and the latter as positive amino acids.\nThe remaining 9 amino acids, which did not pass the significance test, are considered neutral amino acids.\nWe hypothesize that a higher abundance of negative amino acids leads to lower prediction accuracy for\nEMOCPD, while a higher abundance of positive amino acids results in improved prediction accuracy."}, {"title": "5 Conclusion", "content": "This paper presents a novel model called EMOCPD, based on the iRMB module, designed to predict the\ntype of each amino acid from a given protein's three-dimensional backbone, facilitating protein sequence\ndesign. We constructed a training set comprising approximately 1.6 million amino acids and a validation set\nwith 8,000 samples to train and evaluate the performance of EMOCPD. After eight training epochs, the model\nachieved an accuracy of over 80% on the training samples and 62.25% on the validation set. The prediction\naccuracy for all amino acids, except for glutamine (GLN), exceeded 40%, demonstrating a good level of\npracticality for protein design.\nTo validate the excellent performance of EMOCPD, we compared it with five other methods on the TS50\nand TS500 datasets. EMOCPD achieved accuracies of 62.32% and 68.33% on the TS50 and TS500 datasets,\nrespectively, outperforming the second-ranked DenseCPD by more than 10%. We also compared the TOP K\naccuracy curves and the recall, precision, and F1 score metrics for the predictions of 20 amino acids among\nEMOCPD, DenseCPD, ProDCONN, and SPROF on the TS500 dataset. EMOCPD consistently surpassed the\nother three methods in TOP K accuracy at various K values, indicating its potential for superior protein design.\nAdditionally, EMOCPD outperformed the other methods in recall and F1 score metrics for all 20 amino acids,\nwhile its precision for four amino acids, including GLN, was comparable to the best-performing method.\nThese results validate EMOCPD's ability to predict amino acid types based on their environmental\ncontext. To further assess EMOCPD's capabilities, we predicted the PET degrading enzyme TfM7 and found\nthrough biological experiments that the model-predicted mutant N213T exhibited greater stability under\next"}]}