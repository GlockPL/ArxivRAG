{"title": "E-3SFC: Communication-Efficient Federated Learning with Double-way Features Synthesizing", "authors": ["Yuhao Zhou", "Yuxin Tian", "Mingjia Shi", "Yuanxi Li", "Yanan Sun", "Qing Ye", "Jiancheng Lv"], "abstract": "The exponential growth in model sizes has significantly increased the communication burden in Federated Learning (FL). Existing methods to alleviate this burden by transmitting compressed gradients often face high compression errors, which slow down the model's convergence. To simultaneously achieve high compression effectiveness and lower compression errors, we study the gradient compression problem from a novel perspective. Specifically, we propose a systematical algorithm termed Extended Single-Step Synthetic Features Compressing (E-3SFC), which consists of three sub-components, i.e., the Single-Step Synthetic Features Compressor (3SFC), a double-way compression algorithm, and a communication budget scheduler. First, we regard the process of gradient computation of a model as decompressing gradients from corresponding inputs, while the inverse process is considered as compressing the gradients. Based on this, we introduce a novel gradient compression method termed 3SFC, which utilizes the model itself as a decompressor, leveraging training priors such as model weights and objective functions. 3SFC compresses raw gradients into tiny synthetic features in a single-step simulation, incorporating error feedback to minimize overall compression errors. To further reduce communication overhead, 3SFC is extended to E-3SFC, allowing double-way compression and dynamic communication budget scheduling. Our theoretical analysis under both strongly convex and non-convex conditions demonstrates that 3SFC achieves linear and sub-linear convergence rates with aggregation noise. Extensive experiments across six datasets and six models reveal that 3SFC outperforms state-of-the-art methods by up to 13.4% while reducing communication costs by 111.6 times. These findings suggest that 3SFC can significantly enhance communication efficiency in FL without compromising model performance.", "sections": [{"title": "I. INTRODUCTION", "content": "FEDERATED learning [1] (FL) aims to tackle the isolated data island problem with privacy guarantees [2], achieving promising results [3]\u2013[6]. Like its counterparts in the realm of distributed computing, the communication costs between the server and clients are crucial for FL [7], [8]. Nevertheless, the limited network bandwidth [9] and explosive growth of model size [10]\u2013[13] not only decrease the training efficiency of FL but also hinder the deployment and delivery of FL at the scale [14].\nTo cut down the communication cost, existing studies compress the transmitted information, i.e., the gradients of model or model parameters, with diverse compression techniques, such as sparsification [15]\u2013[17], quantification [18]\u2013[20] and others [21]\u2013[24]. However, these methods inevitably make a trade-off between model convergence rate and communication overhead. More specifically, to achieve lower communication overhead, a higher information compression ratio is usually required, which in turn leads to a limited model convergence rate and performance, as shown in Figure 1. This is consistent with [25], as the compression error upper bound of the top-k sparsifier's $||x - topk(x)||_1 \\le (n - k)||x||_1/n$ is positively correlated with the compression rate hyperparameter k. In addition to such a trade-off, the static round-based communication budget constraint in previous studies [16], [26] inevitably suffers from the adverse impact of the accumulation of the compressing error, as the compression is not lossless. Therefore, it is highly expected to propose a communication-efficient federated learning approach that achieves a high model convergence rate, a low information compression error, and a high training efficiency.\nDifferent from existing studies that only seek various compression techniques [7], [16], [19], we intend to address this challenge by introducing a novel information compressor and a systematic communication workflow optimization. On the one hand, for the compressor, we consider such a compression-"}, {"title": null, "content": "decompression problem from a novel perspective: the input features can be the compressed gradients and the corresponding network backward pass can be its decompression process. More specifically, inspired by [27], [28], we deem the gradient computation as a function concerning the training priors, namely the model parameters, the objective function, and the model inputs. By freezing the training priors, the gradients can be compressed into the synthetic features via backpropagation, which are much smaller than the corresponding gradients. Meanwhile, by freezing the synthetic features and unfreezing the training priors, the backward pass of such synthetic features can be the corresponding decompression process. On the other hand, from a systematic perspective, only compressing the transmitted information is not enough for high communication effectiveness and training efficiency and therefore we would like to optimize the workflow of communication-efficient FL. First, unlike existing methods that only compress the uploaded information [7], [26], the communication costs will be further cut by compressing the uploaded and downloaded information. Then, by replacing the"}, {"title": null, "content": "static round-based communication budget constraint with the cumulative-based communication budget, one could adopt a budget scheduler to modulate the budget of each round to alleviate the adverse impact of compression error cumulation. Based on the above discussions, in this work, we propose a novel communication-efficient FL approach, dubbed Extended Single-Step Synthetic Features Compressing Algorithm (E-3SFC), which not only compresses the information with an insightful perspective and reduces the communication costs of FL as much as possible. To be specific, we first introduce a novel gradient compressor, namely, Single-Step Synthetic Features Compressor (3SFC), which achieves lower compression errors and thereby boosts the convergence rate, as Figure 2b shows. In detail, with the determined training priors, 3SFC constructs tiny learnable features as the compressed gradients and adopts error feedback [29] (EF) to further reduce the accumulated compression errors. The advantages of such a compressor can be two-fold. On the one hand, the compression error is related to the size of synthetic features, and such compression could be lossless if synthetic features have the same shape of raw inputs [30]. On the other hand, the size of synthetic features is more communication-efficient than original gradients. For instance, synthetic features of ViT-L/16 [31] ($M \\times 224 \\times 224 \\times 3$ parameters, M is batch size) are much smaller than its corresponding gradients (304, 326, 632 parameters). Clearly, the compression could not be ideal [32] and lossless, and thus we provide comprehensive theoretical analyses to show that the proposed 3SFC is ensured to have linear and sub-linear convergence rates under strongly convex and non-convex cases with aggregation noise, respectively. Second, to reach higher training efficiency and communication effectiveness, we optimize the workflow of communication-efficient FL. The proposed E-3SFC performs double-way compression by applying the proposed 3SFC to the entire communication phases of FL to reduce the communication overhead as much as possible. E-3SFC adopts a budget scheduler to endow a dynamic budget for each round, which assigns more communication budget for the early phase of the FL training to alleviate the negative effects of compression error accumulation. To evaluate the effectiveness of the proposed algorithm, we conduct extensive experiments with six models on six challenging datasets across Computer Vision (CV) and Natural Language Processing (NLP).\nTo sum up, our contributions can be summarized as follows:\n1) We propose a novel gradient compression algorithm, E-3SFC, to reduce the communication overhead of FL. First, we provide a novel perspective on gradient compression in FL, regarding the model itself as a gradient decompressor. Based on this, we propose a novel gradient compressor, termed 3SFC, to achieve a higher compression rate and lower compression errors. Then we optimize the workflow of the communication-efficient FL by compressing the information of all communication phases to reduce communication cost further and assigning more communication budget to the early FL training stage to reduce the accumulation compression error. To the best of our knowledge, this paper is one of the first attempts to deem the model"}, {"title": null, "content": "itself as a gradient decompressor.\n2) Theoretical convergence analyses are provided, showing that the proposed compressor 3SFC has a $O(1)/O(\\frac{\\mu_F}{\\mu_F+\\eta})$ speedup with/without aggregation noise under the strongly convex case, and a $O(R^{1/2})/O(\\eta\\mu_F^{-1/2})$ convergence rate with/without aggregation noise under the non-convex case.\n3) Extensive experiments are conducted on six datasets and six models, demonstrating that E-3SFC outperforms other methods by up to 13.4% with 111.6\u00d7 less communication costs. Other experimental analyses further validate the effectiveness of E-3SFC's design choices\u00b9.\nNotably, there is a short conference version of this paper appeared in [33], which does not optimize the workflow of communication-efficient FL and lacks comprehensive theoretical convergence analyses. The relationship between E-3SFC and 3SFC is shown in Figure 3. Consequently, this paper extends it in the following ways: 1) We propose E-3SFC that optimizes the workflow of communication-efficient FL to further cut the communication overhead whilst 3SFC does not; 2) We provide comprehensive theoretical analyses to ensure the convergence of the proposed 3SFC under mild assumptions; 3) We provide extensive empirical evaluations on CV and NLP benchmarks with a larger number of FL clients to further demonstrate the effectiveness of the proposed E-3SFC.\nThe rest of this paper is organized as follows. Related work is illustrated in Section II. Section III formulates the problem we tackled. In Section IV, details of the proposed E-3SFC are presented. Then convergence analyses are described in Section V, and experiments are documented in Section VI and Section VII. Finally, Section VIII concludes the paper."}, {"title": "II. RELATED WORK", "content": "Works related to communication-efficient FL [34] can be mainly categorized into the following four topics: sparsification, quantification, distillation, and others:\nSparsification: Gradients are sparse and only portions of gradients are actually useful for optimization [25]. Consequently, sparsifiers including top-k or random-k are introduced in [16], [17], [25] to enable a compression rate of 1/100 and thereby greatly reducing communication costs. Theoretical analyses in [35] also support these methods by formally showing that top-k is the communication-optimal sparsifier under a limited communication budget. Nevertheless, no prior knowledge is used during sparsification, leading to high compressing errors [32] and affecting the convergence rate severely."}, {"title": null, "content": "Quantification: It is well-known that full-precision (32-bit) data types are redundant in machine learning [36]. Thus, transmitted data are cast into smaller data types before communicating [18], [19], [37], thereby reducing communication overhead. However, since the 1-bit data type is the smallest, gradient quantification has limited flexibility and owns a compression rate of up to 1/32.\nDistillation: Wu [38] used knowledge distillation to distill the local model to a smaller model before communicating. Nevertheless, it requires an extra public dataset for alignments between the teacher and the student, which not only incurs additional computational costs but also introduces domain gaps between the public dataset and the clients' local dataset. Goetz [22] and Hu [24] propose to generate a synthetic dataset from the full dataset that has a smaller size and can be used to approximate gradients. Still, the synthesis process requires simulating the multi-step optimization of model weights for multiple epochs, leading to not only high time and space complexity (i.e., gradients and intermediate model weights are calculated and stored many times), but also great instability and possible collapse (Figure 4) due to excessively deep back-propagation paths and the gradient explosion, especially for large models and datasets.\nOthers: In [8], [39], researchers alter the FL workflow to allow parallelism in the communication and the computation, without modifying the transmitted data. Therefore, this parallelism can be used with other data-compressing algorithms"}, {"title": null, "content": "combinedly to further accelerate the training process. Li [23] proposed utilizing compressed sensing in data compressing and decompressing. However, the optimization of parameters in compressed sensing can be non-trivial.\nThe proposed E-3SFC is quite different from the previous works mentioned above. First, the proposed E-3SFC is more privacy-preserving than others, since the transmitted information is not even the model itself. Under the same communication budget, E-3SFC empirically converges faster than the sparsification method (see Figure. 7). Then, unlike distillation, the proposed E-3SFC imposes a lower computational burden since it does not employ public datasets and only utilizes single-step model optimization. Additionally, E-3SFC possesses the merits of higher communication efficiency and flexibility. The proposed E-3SFC can reduce the overall communication overhead during the uploading and downloading phases of FL with flexible communication budget schedulers. In terms of flexibility, it can be seamlessly integrated into communication-computation parallelism and compressed sensing to to enhance its usability."}, {"title": "III. PROBLEM FORMULATION", "content": "Assume there are N clients participating in a FL training process with a total of T communication rounds, where the i-th client has a local dataset $D_i$ and a loss function $F(D_i, W_i)$ where $w_i$ is the weight of its model $M_i$. Note that in vanilla FL, all clients and servers share the same model architecture, i.e., $M_1 = M_2 = ... = M_N = M$. The objective of FL is to optimize Eq. 1:\n$min G(F(D_1, w), F(D_2, w), ..., F(D_N,w)),$\n$w \\in R^d$\nwhere G(.) is the linear aggregation function satisfying the sum of aggregation weights equals 1. Typical aggregation functions include arithmetic mean and weighted average based on $p_i = |D_i|/|\\cup_i D_i|$ [1] where $|\\cdot|$ is the size of the .. The global model w at the t-th communication round is updated by Eq. 2:\n$w^{t+1} = G(w_1^t, w_2^t, ..., w_N^t) = w^t - G(g_1^t, g_2^t, ..., g_N^t),$\nwhere $g_i^t = w_i^t - w^t$ denotes the model weight differences after local training for K rounds, and can be seen as accumulated gradients during local training. Generally, to reduce communication overhead, a compressor C is applied to each client's $g_i^t$, so that the global model w can be updated by Eq. 3:\n$w^{t+1} = w^t - G(C(g_1^t), C(g_2^t), ..., C(g_N^t)).$\nAs a result, the objective of communication compressing for client i can be modeled as Eq. 4:\n$C^* = arg min ||C(g_i^t) - g_i^t||^2 s.t. ||C(g_i^t)||_0 \\le B,$\nwhere B is the communication budget, constraining the maximum size of communication data at each communication round. Usually, the communication budget is strictly constrained for every communication round to support FL at scale. Usually, an FL system contains servers located in data centers and a range of clients distributed across multiple areas. The clients, such as mobile devices, are constrained by"}, {"title": null, "content": "their data traffic plans. On the other hand, many works [40], [41] have observed that the global model will converge progressively, and thus the gradient information from the early training phase should be prioritized over that from the later training phase. To this end, we relax the communication budget by constraining only the total communication cost instead of the per-round communication cost. Formally, the strict communication budget constraint can be relaxed as Eq. 5 shows.\n$C^* = arg min \\sum_t \\sum_i ||C(g_i^t) - g_i^t||^2$\ns.t.\n$\\sum_i ||C(g_i^t)||_0 \\le H(B,t,i), \\sum_t \\sum_i H(B,t,i) \\le TNB,$\nwhere H(.) is a budget scheduler that returns a suitable communication budget for given i, t, and B.\nFinally, letting $\\epsilon_i^t = ||C(g_i^t) - g_i^t||$ denotes the compression error at time t, the error feedback can be utilized to optimize this error term by adding it to the $g_i^{t+1}$. Thus, with error feedback, the global model can be updated as Eq. 6:\n$w^{t+1} = w^t - G (C(g_1^t + \\epsilon_1^t), ...,C(g_N^t + \\epsilon_N^t)),$\n$\\epsilon_i^{t+1} = g_i^t + \\epsilon_i^t - C(g_i^t + \\epsilon_i^t).$\nIV. METHODOLOGY\nIn general, E-3SFC comprises three sub-components: a compressor 3SFC, a double-way compression algorithm, and a budget scheduler. 3SFC compresses clients' trained local models into compact features for uploading to the server to reduce communication overhead. Moreover, the double-way compression algorithm allows 3SFC to compress the global model for distribution, reducing the communication costs of both model uploading and downloading. Finally, the budget scheduler is introduced to dynamically determine B for each communication round in 3SFC, enhancing system flexibility and efficiency."}, {"title": "A. Compressing Clients' Local Models with 3SFC", "content": "The general architecture of 3SFC is illustrated in Figure 5, where the training priors are injected in the compressing and decompressing process. At each communication round, the i-th client first trains its local model using its local dataset. After training, accumulated gradients can be obtained by subtracting the global model weights from the latest local model weights. Then, according to the communication budget, the i-th client will utilize 3SFC to compress the accumulated gradients into synthetic features $D_{syn, i}$ using training priors, and sends $D_{syn,i}$ to the server. Formally, when compressing, 3SFC essentially converts $g_i^t$ into synthetic features $D_{syn,i}$ and a scaling coefficient $s_i^t$ using client i's training priors, i.e., $w^t$ and F, as both $w^t$ and F are globally shared. The objective of the compression can be described by Eq. 7:\n$min_{D_{syn}i, s_i^t} (s_i^t \\nabla_{w^t} F(D_{syn, i}, w^t) - g_i^t - \\epsilon_i^t||^2 + \\lambda ||D_{syn,i}||^2)$\ns.t. $||D_{syn,i}||_0 + 1 < B,$\nwhere $g_i^t$ denotes the differences between the global model $w^t$ and its latest local model, i.e., $g_i^t = w^t - w_i^t$ for clients. $\\lambda D_{syn,i}$ is an $l_2$ regularization term to constrain the sparsity of $D_{syn,i}$ for better stability. Note that here the global model $w^t$ is passed into F(.) instead of $w_i^t$, because $w^t$ is globally shared and is the initial weight of every client's local optimization process at each communication round. Since $g_i^t + \\epsilon_i^t$ is fixed, s can be derived from $\\nabla_{w^t} F(D_{syn,i}, w^t)$ by ensuring that $\\nabla_{w^t} F(D_{syn, i}, w^t)$ and $g_i^t + \\epsilon_i^t$ are perpendicular to each other, as shown in Eq. 8:\n$cos(\\theta) = \\frac{(g_i^t + \\epsilon_i^t) \\nabla_{w^t} F(D_{syn, i}, w^t)}{||g_i^t + \\epsilon_i^t|| ||\\nabla_{w^t}F(D_{syn,i}, w^t)||}$\ns_i^t = \\frac{(g_i^t + \\epsilon_i^t)\\nabla_{w^t} F(D_{syn, i}, w^t)}{||\\nabla_{w^t}F(D_{syn,i}, w^t)||^2},$\nwhere $theta$ is the angle between two $g_i^t + \\epsilon_i^t$ and $\\nabla_{w^t}F(D_{syn, i}, w^t)$. Consequently, the objective described in Eq. 7 is equivalent to the following optimization problem:\n$min_{D_{syn,i}} (1 - \\frac{\\nabla_{w^t} F(D_{syn,i}, w^t) \\cdot (g_i^t + \\epsilon_i^t)}{||\\nabla_{w^t F(D_{syn,i}, w^t)}||||g_i^t + \\epsilon_i^t||} + \\lambda ||D_{syn,i}||^2)$,\ns.t. $||D_{syn,i}||_0 + 1 < B.$\nIntuitively, the objective is to find synthetic features $D_{syn,i}$ that minimize the angle between generated gradients from $D_{syn,i}$ and $g_i^t$. This objective is similar to gradient leakage attacks [28], [42], [43], but since the clients are performing the attacks on themselves, it remains safe. After solving Eq. 9, s can be thus calculated by Eq. 8. To minimize the accumulated compression error with EF, $\\epsilon$ can be updated by Eq. 6. Finally, $D^t_{syni}$ and $s_i^t$ will be uploaded to the server to represent the local gradients of client i. When decompressing in the"}, {"title": "B. Double-way Compression", "content": "server, the gradients for the global model aggregation will be reconstructed using the received $D^t_{syn,i}$ and $s_i^t$ with the same training priors used for compressing, as shown in the following equation:\n$g_i^t + \\epsilon_i^t = s_i^t \\nabla_{w^t}F(D_{syn,i}, w^t).$\nThe global model is distributed to all FL participants after weights are updated in the server, i.e., during the downloading phase. In vanilla FL, there are no training priors in the server for 3SFC to reduce the communication costs during model downloading and distribution. Specifically, compressing $w^{t+1}$ in the server requires clients' local models $w_i^t$ as training priors, which are already compressed into $D_{syn,i}$ and $s_i^t$. Moreover, since 3SFC is not a lossless compressor, it is infeasible for the server to recover $w^t$ based on $D_{syn,i}$ and $s_i^t$. In order to further reduce communication costs, an algorithm for double-way compression is proposed. First, $w^t$ is staged in the server after aggregating to serve as training priors for compressing $w^{t+1}$, as shown in Eq. 11:\n$min_{D^{t+1}_{syn}} 1 - \\frac{\\nabla_{w^t} F(D_{syn}^{t+1}, w^t) \\cdot (w^t - w^{t+1} + \\epsilon^{t+1})}{||\\nabla_{w^t} F(D_{syn}^{t+1}, w^t)||||w^t - w^{t+1} + \\epsilon^{t+1}||} + \\lambda ||D_{syn}^{t+1}||^2,$\ns.t. $||D_{syn}^{t+1}|| \\le B.$\nBy using $w^t$ as training priors for the server, we ensure that the compressed information is aligned with the clients' perspectives, facilitating accurate decompression and reconstruction of $w^{t+1}$ on the client side. Specifically, clients can easily recover the updated global model using the received $D_{syn}$ and $s_i^t$, as shown in the following equation:\n$w_i^t = w^{t-1} - s_i^t \\nabla_{w^{t-1}}F(D_{syn}, w^{t-1})$"}, {"title": "C. Budget Scheduler", "content": "The compressing process is not lossless and the compression efficiency decreases with the accumulation of the compressing error during the training process (As can be seen in latter experiments, i.e., Figure 8). This phenomenon motivates us to design a budget scheduler that assigns more communication budget in the early phase of the training under the relaxed constraint (i.e., Eq. 5). Formally, the budget scheduler H(B,t,i) should satisfy the following equations to ensure that the overall communication overhead is unchanged:\n$\\sum_t \\sum_i H(B,t,i) = BT$,\n$\\sum_t\\frac{\\sum_i H(B,t,i)}{N} \\le B; H(B, t, i) \\ge 1.$\nNaturally, a H(B,t,i) satisfying Eq. 12a automatically satisfies Eq. 12b by shifting its t to (t+iT/N) mod T. Thus, for simplicity, we will let i = 0 and discard Eq. 12b hereinafter. The objective of H(\u00b7) is to maximize the transferred information during the training process, as Eq. 13 illustrated.\n$max_H \\sum_t H(B, t, 0)E(t)$\ns.t. $\\sum_t H(B,t, 0) = BT$ and $H(B,t,0) \\ge 1,$\nwhere E(t) is the compressing efficiency at time t. Although E(t) is unknown until the data is actually compressed, we empirically observe that E(t) is a decreasing function with respect to t in Figure 8. Thus, we let H(B,t,0) be parameterized by $\\omega_H$, and altered the Eq. 13 to Eq. 14 to efficiently find a closed-form solution of H using constrained optimization methods.\n$max_{\\omega_H} \\sum_t H(B, t, 0; \\omega)(1 - \\frac{t}{T})^\\tau$\ns.t. $\\sum_t H(B,t, 0; \\omega_H) = BT$ and $\\frac{|H(B, t, 0; \\omega_H) - B|}{|H(B, t, 0; \\omega_H) - B|} \\ge 1,$\nwhere $tau$ is a hyper-parameter. Note that Eq. 14 can be solved before the start of the training. Therefore, the employment of budget schedulers does not introduce any additional computational or communication overhead during the training process."}, {"title": "D. Algorithm and Complexity Analysis", "content": "The pseudocode of E-3SFC is presented in Algorithm 1. During the training process, clients will solve two optimization problems instead of one compared to the vanilla FL method FedAvg [1]: the empirical risk minimization problem on the local dataset (Line 11) and Eq. 9 for compression (Line 15). The solvers to these two problems are not nested, meaning the time complexity of E-3SFC and FedAvg is the same, i.e., O(NE(K + S)). In terms of the space complexity, E-3SFC additionally stores the $w_i^t$, $D_{syni}$, $s_i^t$ and $\\epsilon_i^t$, which are all fixed size parameters. Hence, E-3SFC shares the same space complexity, O(N), with FedAvg as well."}, {"title": "V. THEORETICAL ANALYSIS", "content": "To unveil the theoretical properties of E-3SFC for enhanced explainability, this section provides the theoretical analysis of"}, {"title": null, "content": "3SFC under both strongly convex and non-convex settings, as E-3SFC includes 3SFC and inherits all of 3SFC's theoretical properties. To facilitate the analysis, the global epochs T and the local epochs K are concatenated and flattened. That is, there are in total R = TK training epochs, and the communication happens only when t = 0 mod K. The detailed notations are listed in Table I."}, {"title": null, "content": "Assumption V.1 (Bounded Local Variance). The expectation of the difference between local gradients and global gradients is upper-bounded, i.e., $E||\\nabla F_i(w) - \\nabla F(w)||^2 \\le \\delta^2 + E||\\nabla F_i(w)||^2$.\nAssumption V.2 (Bounded Approximated Gradient). The expectation of the difference between local gradients and approximated local gradients using a subset of data is upper-bounded, i.e., $E||\\nabla F_i(w) - \\nabla F_\\hat{F}(w)||^2 \\le \\sigma$.\nAssumption V.3 (Bounded Global Optimal Variance). The expectation of local gradients at the optimal solution is upper-bounded, i.e., $E||\\nabla F(w^*)||^2 \\le \\sigma_F$.\nAssumption V.4. F is expected-smooth, i.e., $E_i||\\nabla F_i(w) - \\nabla F_i(w^*)||^2 < 2L(E_iF(w) - E_iF(w^*))$.\nAssumption V.5 (Unbiased Compression Optimization Variance Bound). The expectation of the difference between local gradients and compressed local gradients is upper-bounded, i.e., $E||\\eta \\nabla F_i(w) - g||^2 < \\eta^2 \\kappa^2 s$, where $n$ = min{$n_{t^a}$}.\nWith these mild assumptions, We have Lemma V.1 and Lemma V.2. Details are in the Appendix.\nLemma V.1 (Bounded Local Shift). Under Assumption V.4, with $(w^{t^a} - w^*,\\nabla F(w^{t^a})) > F(w^{t^a}) - F(w^*) + \\frac{\\mu_F}{2}||w^{t^a} -"}, {"title": null, "content": "w^*||^2, let $\\tilde{\\eta}$ = K$\\eta$, the upper-bound of local shift is:\n$E||w^{t^{a+K}} - w^*||^2 \\le (1 - \\tilde{\\eta}\\mu_F)E||w^{t^{a}} - w^*||^2\n+ (\\frac{\\tilde{\\eta}^2}{N}+3)\\frac{K N,K-1}{N} \\Sigma_{i=0,k=0}||g_i^{t^{a+k}} - \\eta \\nabla F(w^{t^{a}})||^2\n+ (6L\\tilde{\\eta}^2 - 2\\tilde{\\eta})E(F(w^{t^{a}}) - F(w^*))\n+ \\frac{3l^2}{32} \\frac{N/M-1}{N-1}E \\Sigma_{i=0}||\\nabla F_i(w^{t^{a}}) - \\nabla F_i(w^*)||^2.$\nLemma V.2 (Bounded Local Approximation). Under Assumption V.4 and Assumption V.5, by choosing a proper $\\tilde{\\eta} \\le \\frac{1}{6L}$, the upper-bound of local approximation is:\n$E||g_i^{t^{a+k}} - \\eta \\nabla F(w^{t^{a}})||^2\n< 3\\eta^2 + L\\eta e^{2K+1}((1+ 2K)||\\nabla F_i(w^{t^{a}})||^2 + \\delta) + \\eta^2\\delta,$\nwhere $\\delta := 3(\\kappa^2 + \\sigma^2_F)$.\nWith Lemma V.1 and Lemma V.2, the convergence rate of 3SFC under the strongly convex setting can be obtained.\nTheorem V.3. With Lemma V.1 and Lemma V.2, at flattened epoch R > $\\frac{2}{\\mu_F}$, we have:\n$O(F(w)- F(w^*))\n= O(\\mu_Fe^{-\\tilde{\\eta}\\mu_F \\Delta^0}) + O(\\frac{\\sigma^2}{\\mu_F} + \\frac{\\kappa^2}{\\mu_F})$."}, {"title": null, "content": "Corollary V.3.1. Without aggregation noise", "tilde{\\eta}": "frac{\\mu_F"}, {"have": "n$O(E||\\nabla F(w^{t^*"}, 2, "n= O(\\frac{1}{R}) + O(\\frac{\\sigma^2 + \\kappa^2}{R}) + O((\\frac{\\lambda_LF(N/M-1)}{KR})\n+ O(\\frac{\\mu_F + 2\\lambda_LF(N/M-1)}{K^2R^2}(\\delta K + \\kappa^2 \\xi + \\sigma^2_F)),$\nwhere $\\Delta^* = E(F(w^0) - F(w^*)).$\nCorollary V.4.1. Tuning local epoch K, the main trade-off is between the first $O(\\frac{1}{R})$ $\\times \\frac{1}{R^{2K/3}}$ and last term $\\times \\frac{\\mu_F}{R^{4K/3}}$.\nRemark V.4.1. The compression process controlled by S only affects the radius and part of the sub-linear term in $\\kappa^2 + \\sigma_F$. With Corollary V.4.1, an optional strategy is to adapt K decreasingly at the terminal stage of global training.\nDiscussion: The theorem V.3 and V.4 suggest that 3SFC has"]}