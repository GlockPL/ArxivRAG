{"title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMS", "authors": ["Ved Sirdeshmukh", "Kaustubh Deshpande", "Johannes Mols", "Lifeng Jin", "Ed-Yeremai Cardona", "Dean Lee", "Jeremy Kritz", "Willow Primack", "Summer Yue", "Chen Xing"], "abstract": "We present MultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications. MultiChallenge identifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs. All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time. We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Despite achieveing near perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving just a 41.4% average accuracy.", "sections": [{"title": "1 Introduction", "content": "Evaluation benchmarks have played a critical role in the evolution of large language models (Ziyu et al., 2023; Porcu and Havl\u00ednov\u00e1, 2024), with benchmarks being continuously released to test large language models (LLMs) on various tasks (Zhang et al., 2024; Jain et al., 2024; Glazer et al., 2024). One such task, conducting multi-turn conversations with human users, is a common yet challenging task for LLMs. It is challenging because conducting multi-turn conversations requires not only accurate instruction following, but also careful attention allocation to conversation context, and more importantly, near or surpassing human in-context reasoning capability at the same time.\nDespite the growing demand and complexity of multi-turn LLM conversations with humanslack of are limited comprehensive evaluation frameworks (Kwan et al., 2024; Wang et al., 2023) designed for this. Some of them, such as the widely adopted MT-Bench (Zheng et al., 2023), are saturated by frontier LLMs with near-perfect results (Achiam et al., 2023; Yang et al., 2024). Others (He et al., 2024) focus more on multi-turn explicit instruction following, missing the opportunity to assess the actual set of mixed model capabilities required in conducting natural multi-turn conversations with human users.\nTo bridge this gap, we build MultiChallenge, a realistic and challenging multi-turn conversation benchmark that evaluates models' capability to properly respond to human users given multi-turn conversation histories. Each test example in MultiChallenge is a maximum 10-turn conversation history between two parties, ending with a final user turn containing a requirement/question. LLMs are required to respond to the final user turn properly given the multi-turn conversation history. We identify four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but also difficult for current frontier LLMs. They include instruction retention, inference memory of user information, reliable versioned editing, and self-coherence.\nInstruction retention evaluates whether LLMs are able to follow instructions specified in the first user turn throughout the entire multi-turn conversation. Inference memory of user information evaluates LLMs on recalling and connecting relevant details scattered in previous user turns when they are implicitly required to respond to the final user turn. Reliable versioned editing evaluates whether LLMs can properly help humans revise existing materials through back-and-forth iterations with human users. Finally, self-coherence evaluates whether LLMs can be reasonably coherent with model responses in the conversation history and"}, {"title": "2 Related Work", "content": "Evaluation benchmarks on multi-turn conversations. Compared to the fast emerging evaluation benchmarks assessing other frontier capabilities of LLMs, such as benchmarks for AI as agent (Liu et al., 2023; Yao et al., 2024) and multimodality (Liu et al., 2025; Li et al., 2024), evaluation benchmarks on multi-turn conversations have not received enough attention. Existing benchmarks, such as MT-Bench (Zheng et al., 2023) and MT-Eval (Kwan et al., 2024), although widely adopted before, are not discriminative for frontier LLMs now as model capabilities keep improving. Frontier models such as GPT-4, Claude 3 and LLama3, all achieve near perfect scores on these benchmarks. More recently, multi-IF (He et al., 2024) has been built to extend the IFEval benchmark (Zhou et al., 2023) to multi-turn and multilingual scenarios. It focuses more on the auto-verifiable format and instruction following capability in multi-turn settings, such as keeping all upper cases, instead of the mixed set of model capabilities required in realistic multi-turn conversations, such as attention allocation of context and in-context reasoning.\nMulti-agent synthetic data generation. Multi-agent frameworks has emerged as a popular method for creating diverse, high-quality synthetic datasets. AgentInstruct (Mitra et al., 2024) demonstrated this approach, using multiple LLM-powered agents in three agentic flows to generate synthetic text data with minimal human intervention. Other similar frameworks adopt multi-agent system to generate data for different use cases and knowledge domains, such as Splunk's MAG-V (Sengupta et al., 2024) for customer data and MATRIX (Tang et al., 2024) for social dynamics. In this work, we also adopt a multi-agent method to first generate synthetic multi-turn conversations for all 4 challenges in MultiChallenge and then use a hybrid approach to edit and review such synthetic data to produce the final benchmark."}, {"title": "3 MultiChallenge", "content": "In MultiChallenge, we focus on four categories of common and realistic challenges that arise in multi-turn human-LLM interactions that are also difficult for current frontier LLMs. For each category of challenges, we release various realistic test examples curated according to definition of the corresponding challenge. We show the statistics of MultiChallenge in Table 1. In this section, we first introduce the detailed definitions of the four challenge categories in MultiChallenge. Then we describe the automatic evaluation method that we provide to facilitate efficient and trustworthy adoption of MultiChallenge."}, {"title": "3.1 Challenge Categories", "content": ""}, {"title": "3.1.1 Instruction Retention.", "content": "When human users interact with LLMs, it is common for human users to specify general instructions at the beginning of the conversation and expect LLMs to follow them throughout the conversation. While in practice, we find that although frontier LLMs are good at following instructions in single-turn conversations, their capability to retain instructions in multi-turn conversations is still limited, which is what we evaluate under Instruction Retention. The first panel in Figure 1 shows an example. In this example, the constraint about films with specific UK age ratings is given in the first turn. The model is evaluated on its response to the final user utterance still being compliant to the constraint.\nIn every instruction retention test example, the human user explicitly specify in the first turn that the instructions should be followed throughout the entire conversation. Moreover, human users don't provide followup instructions that contradict the first-turn instructions. The first-turn user instructions also range from simple format constraints such as sentence numbers and bullet point formats, to more semantic instructions such as movie ratings in the example above."}, {"title": "3.1.2 Inference memory (of user information).", "content": "Test conversations under this category mainly assess an LLM's capability to recall and connect relevant details scattered in previous user turns when they are implicitly required to respond to the final user turn. The second panel of Figure 1 shows an example of this category. Some user information that appeared early in the conversation, nut allergies, is implicitly required to successfully complete the request in the last turn, dessert recipes. In such examples, the LLM is evaluated on whether it is able to reallocate its attention to the relevant parts of the conversation history and provide dessert recipes that avoid triggering nut allergies.\nWhen creating test examples under inference memory category, we pay close attention to make sure that the final user turns don't directly ask for user information mentioned previously, such as \"does my girlfriend have nut allergies\". Instead, the final user turns implicitly require such user information. Through this way, we test LLMs' capability of reasoning on the relevance of context information."}, {"title": "3.1.3 Reliable versioned editing.", "content": "Another common and natural use case of LLMs is to help humans revise existing materials, such as travel plans, emails, code, etc. Such editing usually requires multiple turns of back and forth between the human user and LLMs. These multi-turn conversations usually include multiple versions of LLM-edited materials following different user instructions. It is also common for users to change opinions frequently and give up current edits and move back to editing earlier versions. The first panel in Figure 2 shows a test example, where the user adjusted the schedule multiple times and referred to different versions of it. This collaborative editing with version references requires the LLM to successfully resolve reference ambiguities in anaphoric expressions such as the plan we had before we adjusted the workshop's ending time, copy the corresponding version without hallucination, and edit following user instructions. It again requires accurate attention allocation of context, reasoning and instruction following of the target LLMs."}, {"title": "3.1.4 Self-coherence.", "content": "Frontier LLMs may cater to human preferences by always agreeing with the user, a behavior known as sycophancy (Perez et al., 2022; Sharma et al., 2023). This is particularly evident in multi-turn conversations when the LLM quickly abandons its previous responses and agrees with the user when the user repeats the query or slightly questions prior LLM responses (Laban et al., 2023; Agarwal et al., 2024). This self-contradicting behavior of LLMs can make the conversation flow unnatural and make human users trust LLMs less.\nTherefore under self-coherence, we evaluate whether LLMs can be reasonably and naturally coherent with the its responses in previous turns. Figure 2 gives an example. In a multi-turn discussion about how to set up an e-reader, the LLM gives step by step guidance which includes Register your e-reader after connecting to Wi-Fi in an early turn. However, when the human user asks again in the final turn \"all that's left to do now since it is connected to wifi is choose a book, right?\", most frontier LLMs change to agree with the user while it conflicts the previous model response in which the next step after connecting to wifi is to register you e-reader."}, {"title": "3.2 Automatic evaluation with instance-level rubrics", "content": "Rule-based automatic evaluation methods don't apply to MultiChallenge because there is no single ground-truth answer for most test conversations in MultiChallenge. Moreover, we have found that directly applying frontier LLMs as judge by providing the full multi-turn conversation history and prompting them to evaluate model responses, leads to very low alignment with human raters. It is potentially because all frontier LLMs such as GPT-40, have less than 50% accuracy on MultiChallenge, limiting our confidence that they can correctly judge the performance of other models on MultiChallenge. Pure human evaluation on MultiChallenge is also expensive and time-consuming.\nTherefore, we propose LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Specifically, at the final step of producing each test example, we instruct human raters to provide a binary rubric question that only allows for a \"yes\" or \"no\" answer. This binary question only requires the final model response as context to answer. A \u201cyes\u201d answer indicates that the model response has passed this test example and vice versa. We also make the binary question within the capability of current LLMs. For example, the binary rubric question for the inference memory example shown in Figure 1 is, \u201cdoes any of the dessert recipes suggested in this response contain any nuts?\".\nThrough this method, we make automatic evaluation on MultiChallenge possible and reliable. Experiments show that adopting frontier models as judges with our instance-level rubrics reaches 93% alignment with experienced human raters, compared to 36% alignment of directly prompting LLMs as judge by providing raw conversation context. Details of this experiment are shown in Section 5.2."}, {"title": "4 The Hybrid Approach to build MultiChallenge", "content": "Producing realistic, diverse and challenging test examples for MultiChallenge that can make most frontier LLMs fail, is a difficult and time-consuming task even for human experts. Therefore, to facilitate human experts and reduce cost while still maintain data quality, we construct MultiChallenge with a hybrid approach, in which we synthetically generate data first and then have human experts to review and edit such synthetic data. In this section, we first describe our multi-agent synthetic data generation system and then illustrate the human editing process."}, {"title": "4.1 Synthetic Data Generation", "content": "We adopt a multi-stage and multi-agent system for generating evaluation examples in MultiChallenge, MMSE. Specifically, when generating a test example, MMSE takes in 3 inputs hierarchical topic seeds, the persona seed, and an evaluation config of the specific challenge category as shown in Figure 3.\nHierarchical topic seeds provide structured topics covering various knowledge domains. The full hierarchical topic taxonomy is shown in Appendix A.1. The persona seed is sampled from HuggingFace PersonaHub (Ge et al., 2024), which provides a diverse collection of user personas, allowing our synthetic conversations to be reflective of diverse real-world scenarios. The topic and persona seeds both boost the diversity of MultiChallenge. The third input, the category specific config contains 5 components, namely category name, category definition, pass criteria, failure criteria and K-shots of failures. We show the evaluation configurations of all 4 challenge categories in Appendix A.2. The evaluation configurations will be used by the Planner Agent in MMSE to generate conversation backbones and to preliminarily evaluate LLM failures.\nAs a multi-agent system, MMSE involves 3 types of agents serving different roles to collaborate on generating test samples. The 3 types of agents are Planner Agent, User Agent and Responder Agents. All agents are constructed by prompting LLMs with their corresponding agent role descriptions and task descriptions. We present our system prompts for all 3 types of agents in Appendix A.3.\nPlanner Agent serves as the sole strategic orchestrator of the synthetic multi-turn conversation. Given topics, personas and evaluation config as inputs, the planner agent generates and updates the conversation backbone and strategy for the user agent. User Agent serves as an LLM user to directly communicate with the responder agent, specifying the planner's strategy into concrete and realistic user turns, with the goal of causing the responder agent to make mistakes that fit the definition of corresponding categories.\nResponder Agents serve the role of AI assistants to respond to the user agent's turns in the multi-turn conversation. They take the current conversation history between itself and the user agent as input to generate the current response. To ensure unbiased conversation generation, for each test example we randomly sample a LLM from a pool of 6 frontier models to serve as the responder agent. The 6 models are o1-preview (Jaech et al., 2024), GPT-40 (August 2024) (Hurst et al., 2024), Gemini 1.5 Pro (August 27, 2024) (Team et al., 2024), Claude 3.5 Sonnet (June 2024) (Anthropic, 2024), Mistral Large (Mistral AI, 2024), and finally Llama 3.1 405B Instruct (Dubey et al., 2024). This sampling approach prevents the generation process from over-fitting to any particular model's weaknesses or characteristics."}, {"title": "4.2 Human Review and Editing", "content": "We recruit and train human annotators to review and edit the data generated by MMSE to produce final test examples in MultiChallenge. The human review process mainly assesses 3 aspects of data quality, a) if the synthetic multi-turn conversation is aligned to its challenge category definition; b) if the conversation is natural and realistic; c) if 6 frontier LLMs fail reasonably or not. We only accept test examples that cause at least 3 of them to fail, ensuring the representativeness of the challenges in all samples. After reviewing, if the synthetic conversation's quality is not satisfactory in any of the 3 criteria mentioned above, human annotators either edit or discard the synthetic example.\nWe give thorough guidance to human annotators, including examples of unfair failures for each category, tips for editing, and detailed workflow description. We also have 2 review layers after one human annotator submitted their edited examples. The reviewer pool is completely distinct from the first attemper pool. We only accept examples that passed these two review layers."}, {"title": "5 Experiments", "content": "In this section, we first present the human evaluated performance of 6 frontier models, ol-preview (Jaech et al., 2024), GPT 40 (August 2024) (Hurst et al., 2024), Gemini 1.5 Pro (August 27, 2024) (Team et al., 2024), Claude 3.5 Sonnet (June 2024) (Anthropic, 2024), Mistral Large (Mistral AI, 2024), and finally Llama 3.1 405B Instruct (Dubey et al., 2024). These 6 LLMs are the same ones that we used to collect test examples. Then we analyze the performance difference of these 6 LLMs on the 4 different challenge categories. Beyond human evaluation, we also adopt our automatic evaluation method to assess the vast majority of open-source LLMs. Finally, we conduct a thorough analysis and show case studies, such as the difficulty distribution of MultiChallenge and the trustfulness of our auto-eval approach, etc."}, {"title": "5.1 Main Results", "content": "Table 2 shows the human evaluated performance of all 6 frontier LLMs listed above. Human raters review the 6 anonymous model responses of a test example at the same time followed by 2 reviewer layers. Among them, Claude 3.5 Sonnet (June 2024) achieves the best general performance on MultiChallenge, reaching a 38.3% average accuracy score on MultiChallenge, significantly outperforming other LLMs. ol-preview follows with 34.6, also significantly outperforming the rest LLMs. Gemini 1.5 Pro (August 27 2024) achieves 19.1 and wins the third place.\nWhen zooming into different challenge categories, Table 2 also shows that different LLMs have different expertise on the 4 challenges that we test in MultiChallenge. Although Claude achieves the best general performance, it falls behind on Reliable Versioned Editing and Information Memory compared to o1-preview. Although Gemini 1.5 Pro ranks behind 01-preview on average scores, Gemini 1.5 Pro 's performance is on par with o1-preview on instruction retention. The performance differences of frontier LLMs across these challenges validates the effectiveness of MultiChallenge's design in defining 4 categories that target distinct LLM capabilities.\nIn Table 3, we also use our automatic evaluation method to assess the 6 frontier LLMs on MultiChallenge. The auto-eval results are almost identical to human-eval results, and the trends and rankings also stay consistent between human-eval and auto-eval results. We adopt GPT-40 as the base model for the LLM judge, since it has very high alignment, as demonstrated in Table 4. We also try using Claude as the base model for the LLM judge and reach the same conclusions. We also present detailed alignment results of our auto-eval method with human raters in Section 5.2."}, {"title": "5.2 Analysis", "content": "The alignment between Automatic Evaluation and Human Evaluation. We calculate the alignment of our auto-eval with instance-level rubric questions with human raters and show them in Table 4. The alignment score is calculated with all data with human labels in Table 2. Results show 93.91% alignment score on average. The alignments on 4 individual challenges are all above 90%. We implemented a baseline auto-eval method for comparison, in which we directly apply frontier LLMs as judge by providing the full multi-turn conversation history and prompting them with winning/losing criteria of the corresponding challenge category, to evaluate model responses. Results in Table 4 show that this method obtains 36.01% alignment score, significantly lower than our approach. Both GPT-40 and Claude are used as the base model of LLM judge of the two auto-eval methods in this experiment, and the same conclusion is reached.\nIs the number of turns correlated to LLM performance? The number of turns vary for different test examples in MultiChallenge. We would like to see whether the number of turns directly correlates to the difficulty level of test examples. Figure 4 shows that for all models, the performance on the benchmark does not show any visible trend as the number of turns increases, indicating that the challenge of MultiChallenge is not caused simply by the length of the conversations. Although being a multi-turn benchmark, the lengths of test examples are still pretty small compared the current models' max token lengths (as illustrated in Table 1). Therefore adding turns (length) to the example doesn't add on extra difficulties. It is potentially the inherent reasoning required given the conversation context that makes MultiChallenge challenging, instead of lengths.\nHow do open source models perform on MultiChallenge? We also assess some most recent open source models on MultiChallenge with our auto-eval methods and present the results in Table 5. Since MultiChallenge only consists of test examples in which at least 3 of the 6 frontier models fail (details in Section 4.1), MultiChallenge can be potentially biased against these 6 frontier models in Table 3, compared to the open source models in Table 5. However, given the potential bias against the frontier models, we still observe from Table 5 that all open source models fall behind top-performing closed source mooels such as Claude 3.5 Sonnet and o1-preview.\nHow do model size and release date relate to performance? We examine the relationship between model performance and two key factors: model size (in terms of parameters) and release date. Figure 5 illustrates the performance of various models across different parameter sizes. On average, we observe that larger models, particularly those in the 70-72B parameter range, tend to outperform smaller ones. Figure 8 in Appendix presents the performance of models over time. We note a general trend of improvement, with more recent models typically achieving higher scores. However, this progression is not strictly linear. For example, Claude 3.5 Sonnet, released in June 2024, outperforms several models released later.\nHow effective is the synthetic data generation? We measure the effectiveness of the synthetic data generated from MMSE by the Average Minutes per Test example (AMT) required for human editing. The more effective MMSE is, the less AMT of human editing would be required. On average, trained human raters require 73.6 minutes to accomplish one test example if they edit from synthetic data generated by MMSE. While without synthetic data, it takes trained human raters 154.4 minutes on average to finish producing one test example from scratch. This comparison shows that MMSE successfully reduced required AMT by half, therefore significantly reduced cost.\nHow much human editing was required? Although the results above show that synthetic data from MMSE significantly reduced human time required for creating each test examples, human reviewing and editing is still critical for keeping MultiChallenge natural and challenging. To assess the degree of modifications by human raters, we apply the fuzzywuzzy library to calculate Levenshtein-distance-based string similarities of the original synthetic example vs the human-edited version. The average string similarity between all original synthetic examples and the final human-edited examples in MultiChallenge is 74.5%, indicating 25.5% of difference led by human editing. Case study of LLM failures We provide examples of how the 6 frontier LLMs provide improper responses for all 4 challenges in Appendix A.7."}, {"title": "6 Conclusion", "content": "Our introduction of MultiChallenge marks a significant advancement in evaluating large language models' (LLMs) capability at conducting multi-turn conversations with human users, a crucial but previously overlooked aspect. Through a novel AI-Human collaborative construction, MultiChallenge offers a comprehensive benchmark covering 4 multi-turn challenges that are realistic and currently challenging to frontier LLMs. The LLM as judge with instance-level rubrics also makes auto-eval for this challenging benchmark possible and trustworthy."}, {"title": "Limitations", "content": "Our research marks a significant step forward in assessing the multi-turn conversation capabilities of large language models (LLMs). However, it is not without its challenges. One constraint is in order to make the LLM as judge trustworthy, we cannot include examples in which even the corresponding instance-level rubric question is beyond current frontier LLM capability. This potentially limits the difficulty level of the entire evaluation benchmark. We keep in private such difficult test examples and keep monitoring the evolvement of LLM capability. We will release the difficult data set to the public once their auto-eval using LLM is possible. Another constraint is this benchmark is inevitably biased against the 6 frontier models listed above, due to the fact that the test examples are picked according to the 6 frontier models' common failures. However, given the potential bias against the frontier models, we still observe that all open source models we tested later fall behind top-performing closed source models such as Claude 3.5 Sonnet and o1-preview. Moreover, the comparison among all the future released models of the 6 frontier LLM families and existing open source models would still be fair on MultiChallenge."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Hierarchical Topic Taxonomy", "content": ""}, {"title": "A.2 Challenge Category Evaluation Configuration", "content": ""}, {"title": "A.3 Agent System Prompts", "content": ""}, {"title": "A.4 Frontier Models used as Responder Agents", "content": ""}, {"title": "A.5 Model Performance over time", "content": ""}, {"title": "A.6 Conversation Blueprint Examples", "content": ""}, {"title": "A.7 Case Study: Model Failure Analysis", "content": "This appendix presents a case study showcasing specific examples and examining how different language models fail according to the respective challenge. Each example includes the full conversation transcript, analysis of how the challenge is being tested, failure criteria, and detailed evaluations of six different models' responses."}, {"title": "Instruction Retention", "content": "This example examines how models handle maintaining a specific instruction (avoiding films with UK age ratings of 15 or 18) throughout a natural conversation about film festival programming."}, {"title": "Conversation Transcript", "content": ""}, {"title": "Inference Memory", "content": "This example examines how models recall and connect specific relevant details that could be found anywhere in the conversation, specifically a nut allergy, in the context of requiring advice for an upcoming date."}, {"title": "Conversation Transcript", "content": ""}, {"title": "Reliable Versioned Editing", "content": "This example involves the user iterating over versions of a schedule, with varying types of requirements, eventually wanting to finalize a schedule mentioned a couple turns ago."}, {"title": "Conversation Transcript", "content": ""}, {"title": "Self-Coherence", "content": "This example involves the user asking for instructions on how to set up an e-reader that they have bought for the first time, testing to see if the model can provide consistent information to the user without deviating/getting confused about what it has said before."}, {"title": "Conversation Transcript", "content": ""}]}