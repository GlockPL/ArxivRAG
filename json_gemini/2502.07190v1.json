{"title": "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task", "authors": ["Junjie Wu", "Mo Yu", "Lemao Liu", "Dit-Yan Yeung", "Jie Zhou"], "abstract": "While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs' parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs' abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive performance on a range of challenging NLP tasks (Davis, 2023; Zhao et al., 2024; Wang et al., 2024; Yang et al., 2024; Frieder et al., 2024), which naturally leads to the question: how close are LLMs to achieving human-level intelligence?\nTo explore this question, it is useful to draw on established research in human intelligence (Cattell, 1963, 1971), which categorizes intelligence into two major types: crystallized intelligence, the ability to apply prior knowledge to solve problems, and fluid intelligence, the ability to tackle new problems without relying on pre-existing knowledge. Of the two, fluid intelligence is often viewed as more indicative of general cognitive ability (Jaeggi et al., 2008; Chollet, 2019; Barak and Loewenstein, 2024), as it captures the capability to solve novel problems. Moreover, evaluating fluid intelligence is essential for assessing the reasoning abilities of LLMs since LLMs have been exposed to and memorized vast amounts of knowledge during pre-training (Kaplan et al., 2020; Biderman et al., 2024), potentially blurring the line between their reasoning ability and memorization capacity.\nDrawing from both cognitive (Jensen, 1998) and AI research (Chollet, 2019; Barak and Loewenstein, 2024), an ideal approach to evaluate fluid intelligence in LLMs involves evaluating their ability to perform abstract inductive reasoning, i.e., induct a general pattern solely from given input-output examples and apply this pattern to deduce the correct outputs for new inputs. The Abstraction and Reasoning Challenge (ARC) (Chollet, 2019), which requires models to induct transformation rules from input-output grid pairs (as shown in Table 1), is a benchmark well-suited for this purpose. Due to the abstract nature of ARC tasks, LLMs cannot rely on memorization or external knowledge to solve them. In contrast, many existing inductive reasoning tasks (Honovich et al., 2023; Yang et al., 2024; Qiu et al.) fail to prevent the use of memorization shortcuts, making those tasks easier for LLMs to solve, as shown in Table 1.\nTherefore, the ARC task has become the de facto standard for measuring machine fluid intelligence, sparking a wave of recent studies aimed at improving LLM performance on it (Acquaviva et al., 2022; Xu et al.; Wang et al., 2024; Wang et al.). Despite these efforts, LLMs continue to struggle with the ARC task. For example, even the state-of-the-art GPT-40 with careful prompting can only correctly solve 19% of tasks (see Table 3), which falls far short of the average human performance of ~75% (LeGris et al., 2024). These observations lead us to explore a fundamental question: why is the ARC task so challenging for LLMs?"}, {"title": "2 Evaluating Fluid Intelligence on ARC", "content": "We start by evaluating the fluid intelligences of existing LLMs using the ARC benchmark, which comprises 400 training and 400 evaluation tasks. As shown in Table 1, each ARC task includes several 2D input-output grid pairs that define a unique transformation rule, with each grid ranging from 1 \u00d7 1 to 30 \u00d7 30 pixels, and each pixel being one of ten colors An LLM must induct the transformation rule from the given input-output grid pairs and use it to predict the output grid for a testing input grid. Due to the high cost of closed-source LLMs, we follow Wang et al. (2024) and use a subset of 100 training tasks in ARC for evaluation.\nSince ARC tasks are presented in a 2D visual grid format, we can employ both visual-based LLMs (Visual) and text-based LLMs through converting input-output grids into matrices represented by NumPy arrays following existing works (Xu et al.; Wang et al., 2024) (Textual). Therefore, we first investigate the performances of these two types of"}, {"title": "2.3 Comparing Different LLMs on ARC", "content": "We evaluate both closed-source and open-source LLMs. For closed-source models, we use GPT-40 and GPT-3.5. For open-source LLMs, we select Mistral (Mistral-7B-Instruct-v0.2) (Jiang et al., 2023) and Llama-3 (Llama-3-8B-Instruct) (MetaAI, 2024). Additionally, we include the recently released GPT-ol model, known for its strong reasoning abilities, for comparison. Due to the slow inference speed and limited quota of GPT-ol, we evaluate it on a subset of 50 tasks and report the performance of both GPT-40 and GPT-01 on this subset.\nThe primary metric we use to evaluate the performance of LLMs is the accuracy of their predictions (Acc). Additionally, since we observe that the shape of the LLMs' predicted output grids does not always align with the ground truth, we report the percentage of mismatched predictions for each LLM (Not M%), where lower scores indicate better performance.\nThe evaluation results are presented in Table 3. We observe that, although GPT-40 significantly outperforms other LLMs, its performance remains far from ideal. Moreover, GPT-01 shows almost no improvement over GPT-40 on the evaluated subset. Hence, due to its low speed and limited quota, we do not include GPT-01 in the following experiments.\nFor the other LLMs, handling ARC tasks seems extremely challenging, with more than one-third of their predictions failing to match the shape of the corresponding ground truth. To examine the impact of model size on ARC performance, we further experiment with Mistral-8*7B (Mixtral-8x7B-Instruct-v0.1) and Llama-3-70B (Llama-3-70B-Instruct). As shown in Ta-"}, {"title": "3 Breaking ARC into Atomic Operations", "content": "As mentioned in \u00a71, the transformation rule of each ARC task can be decomposed into several atomic operations which motivates us to analyze the challenges of LLMs from a task decomposition perspective. To this end, we first decompose the ARC tasks into simplified tasks and form the ARAOC benchmark that consists of various atomic operations, then use ARAOC to evaluate the fluid intelligence of LLMs."}, {"title": "3.1 ARAOC Benchmark", "content": "To evaluate LLMs' fluid intelligence with atomic operations, we first manually go through all the tasks in ARC's training and evaluation sets, then conclude six atomic operations that can compose the transformation rules for most of the ARC tasks. \nFor each atomic operation, we use it as the transformation rule to build 100 tasks with 3 input-output training pairs and 1 testing pair, which follows the standard ARC setting. This finally leads to a benchmark named Abstraction and Reasoning on Atom Operation Corpus (ARAOC) with 600 distinct tasks. We evaluate all LLMs in \u00a72.3 on ARAOC and additionally include Mistral-8*7B and Llama-3-70B to study the impact of model size.\nAs shown in Table 5, GPT-4o largely outperforms other LLMs across almost all tasks in the ARAOC benchmark, achieving nearly 100% Acc scores on the Change Color and Fill Internal tasks, demonstrating its high fluid intelligence. Additionally, Llama-3/Llama-3-70B outperforms Mistral/Mistral-8*7B, suggesting that pre-training with a greater number of parameters can enhance the fluid intelligence of LLMs. Also, similar to Table 3, larger LLMs continue to outperform smaller ones across tasks, further illustrating the above point. However, all LLMs still encounter substantial difficulties with tasks related to Move, Copy, Mirror, and Scale, failing to predict the correct shapes of output grids for the latter two atomic operations on more than ~50 tasks."}, {"title": "3.2 Further Analysis", "content": "As concluded from \u00a73.1, all the LLMs exhibit poor performances on Move and Copy tasks in ARAOC. To analyze whether this is caused by the internal complexity of Move and Copy, we investigate factors that may affect the complexity of Move and Copy, and their influences on LLMs' performances. Given that Copy can actually be viewed as first copying the original subgrid, then moving the copied subgrid several steps in specific directions, we intuitively consider two factors in this study: 1) the number of steps the subgrid/copied subgrid moves; 2) the direction in which the subgrid/copied subgrid moves.\nSpecifically, we choose Up, Up-right and 1 step, 2 steps, 3 steps as our candidate moving di-"}, {"title": "4 Challenge on Task Composition", "content": "In this section, we assess the deficiency of LLM's fluid intelligence from a task composition perspective. First, we consider a simple composition experiment that controllably evaluates the composition for Move and Copy in ARAOC (\u00a74.1). Moreover, we design a complex composition experiment utilizing ARC tasks to evaluate LLMs' abilities to compose all atomic operations (\u00a74.2)."}, {"title": "4.1 Evaluation on Simple Composition", "content": "We start from evaluating LLMs' compositional ability on a simple composition task. To be specific, we compose Move and Copy to create 100 new tasks for evaluation. Since Mistral and Llama-3 are facing severe challenges on inducting these two atomic operations, we fine-tune them on three types of data: 1) 3,000 Move tasks; 2) 3,000 Copy tasks ; 3) 1,500 Move tasks and 1,500 Copy tasks, while making sure that these tasks do not overlap with those in ARAOC. We evaluate these fine-tuned LLMs and the GPT models on the newly crafted Move and Copy tasks and list the results in Table 8.\nAs can be seen, fine-tuning on single atomic operation's data can boost LLMs' performances on corresponding tasks, while fine-tuning on both atomic operations can achieve enhancement on both tasks. However, all the fine-tuned LLMs as well as GPT models face severe challenges when dealing with the composition tasks, which is not a complex composition, indicating that the composition abilities of LLMs are limited."}, {"title": "4.2 Evaluation on Complex Composition", "content": "Furthermore, we examine the LLMs' abilities to compose atomic operations in complex ways. As mentioned in \u00a73, the ARC tasks can be decomposed into atomic operations listed in Table 4. Therefore, we regard ARC tasks as complex compositions of atomic operations for evaluation. Here we evaluate Llama-3 and GPT-40 since they are the better open-sourced and close-sourced LLMs in Table 8. In addition, we fine-tune Llama-3 on tasks built upon atomic operations (check fine-tune details in Appendix E) to see if this leads to improvement on ARC . In addition, we apply three more strategies to fine-tune Llama-3 for comparison: 1) using both the aforementioned operation data and 400 ARC tasks that do not overlap with the 100 evaluation tasks ; 2) using only the 400 ARC tasks .\nResults are show in Table 9. We observe that fine-tuning on atomic operation data largely improves the performance of Llama-3 on ARAOC 2. In particular, both fine-tuned LLMs achieve high accuracy on Color, Fill Internal, and Scale tasks, which Llama-3 struggles with. However, Llama-3-FT-atomic performs even worse than Llama-3 on ARC tasks. This could be due to the loss of compositional ability after solely fine-tuning on atomic operations, an issue that Llama-3-FT-atomic-arc does not encounter. On the other hand, fine-tuning on ARC tasks enhances LLMs' performance on ARC, but the improvement on ARAOC tasks is relatively limited compared to fine-tuning on ARAOC tasks. This is likely because the transformation rules in ARC are highly complex, and LLMs struggle to decompose these rules into atomic operations. Nonetheless, all LLMs still perform poorly on ARC tasks, which is unsurprising given their difficulties with even the simple compositions presented in Table 8.\nOverall, while fine-tuning on atomic operations may assist LLMs in understanding these operations, it does not enable them to infer such operations from in-context examples. This limitation explains LLMs' poor performance on compositional tasks and further highlights their lack of intrinsic mechanisms for abstract reasoning, a core characteristic of fluid intelligence."}, {"title": "5 Challenge on Input Format", "content": "Since LLMs cannot process visual inputs, we follow Wang et al. (2024) to convert the 2D visual input-output grids in ARAOC tasks into matrix-format before feeding them to the LLMs (\u00a72.1). However, it remains uncertain that whether this conversion affects LLMs' performances on ARAOC, since LLMs are mostly trained on natural language data, and may not understand such matrix-format inputs well. In this section, we first try to answer this question (\u00a75.1), then investigate a strategy to remedy its potential challenges (\u00a75.2)."}, {"title": "5.1 Matrix-format Understanding", "content": "We first investigate whether LLMs understand the input matrices well. Specifically, we select the testing input matrices from the 100 ARAOC Move tasks, and ask LLMs to output the size, transpose, and subgrid's corner elements' locations of each matrix . Our intuition is that if LLMs correctly answer these questions, they should have understood the matrix-format input. Results are shown in Table 10, where GPT-40 answers these questions with high accuracy, indicating that it comprehends such matrix-format inputs well. However, other LLMs perform poorly on these tasks, which may further affect their results on ARAOC.\nTo further investigate the impact of matrix-format input, we re-evaluate Llama-3-FTMove+Copy from Table 8 and GPT-40 on the Move and Copy tasks without using the location information of subgrids , detailed in Appendix I. The results in Appendix I show that prohibiting the use of location information do reduce LLMs' performances on both tasks, indicating that a fundamental understanding of matrices is crucial for completing ARAOC and ARC tasks. However, as the combined results from Table 5 and Table 10 suggest, possessing matrix understanding alone does not guarantee good performance on these tasks."}, {"title": "5.2 Switching Matrix into Natural Language", "content": "Since LLMs are predominantly trained on natural language rather than matrix-format data, we further propose to convert the matrix-format input-output grids into natural language with the aid of a coordinate system-based prompt . We evaluate LLMs using this new prompt on ARAOC, and the results are presented in Table 11.\nNotably, we find that on tasks that LLMs originally cannot answer well (Move, Copy, Mirror, and Scale), using natural language inputs can largely boost their performances. As for tasks that are relatively easy for LLMs, converting matrix-format input to natural language still keep the good performances. One exception appears to be the Mistral model, whose performance decreases with the natural language prompt. This is probably because this model is not strong enough to encode the natural language input that can be handled by other LLMs, which makes its results not indicative.\nOverall, we conclude that LLMs' failure on fluid intelligence tests is not mainly due to their understanding of the specific matrix-format inputs, but their limitations on encoding such inputs for obtaining global representations of the input tasks."}, {"title": "6 Modeling Challenge", "content": "In this section, we examine whether LLMs' modeling features affect their fluid intelligence from both the model architecture perspective and the information encoding perspective."}, {"title": "6.1 The Bias of Model Architecture", "content": "When predicting output tokens given an input prompt, existing LLMs use the autoregressive decoding strategy (Bahdanau et al., 2015), which predicts the next token based solely on previous tokens. However, in some ARAOC tasks like Mirror, the newly generated part in the testing output grid may locate before the original part. This prevents LLMs from using information in the testing input grid to generate the new part, thus lowering their performances. For example, if the Mirror example in Table 4 is a testing input-output grid pair, LLMs cannot reference the bottom two green grids (the original subgrid) while generating the upper two green grids (the new subgrid), which makes the generation process more challenging."}, {"title": "6.2 Challenge on Information Usage", "content": "Since each task in ARAOC includes 3 in-context examples, the ability of LLMs to identify useful information from the in-context examples may affect their performances on ARAOC. We investigate this claim by calculating the saliency score of one of Mistral's incorrect predictions with respect to the in-context examples, with higher scores indicating a larger impact.\nAs shown in Figure 1, Mistral should move \"6\" two steps to the right, yet it incorrectly keeps \"6\" fixed in the output grid. With the saliency scores, we find that Mistral does not focus much on the moved parts in the in-context examples . Instead, it focuses more on the unchanged parts, which leads it mistakenly assume that \"6\" should also be fixed. These observations illustrate that the inability to identify relevant information in in-context examples also explains why LLMs struggle with ARAOC tasks.\nOverall, we conclude that LLMs' internal architecture also limits their ability to access global information, which is important for illustrating fluid intelligence. While the findings on LLMs' fluid intelligence in previous sections are drawn from ARC and ARAOC tasks, they can be generalized to other real-world tasks and a further discussion on the applicability of our findings can be found in Appendix L."}, {"title": "7 Related Works", "content": "Evaluating fluid intelligence of LLMs. As an essential aspect of intelligence (Cattell, 1963, 1971; Jaeggi et al., 2008), studying the fluid intelligence of LLMs offers deeper insights into their overall intelligence. Chollet (2019) and Barak and Loewenstein (2024) suggest that abstract inductive reasoning is an ideal method for evaluating LLMs' fluid intelligence. However, most existing benchmarks fail to prevent memorization shortcuts, making them easier for LLMs to solve. In contrast, the ARC corpus that challenges models to identify transformation rules between input-output grids, poses significant difficulty for LLMs, making it suitable for fluid intelligence assessment. Previous works have primarily focused on improving LLM performance on ARC tasks but the results remain far from optimal. This motivates us to explore the underlying reasons behind LLMs' limited fluid intelligence.\nMatrix operations with LLMs. It has been shown that LLMs have abilities to understand matrix operations (Charton, 2021; Collins et al., 2024; Azerbayev et al.; Shao et al., 2024). However, \u00a75.1 indicates that understanding the properties of matrix may not the key factor of LLMs' success on ARC and ARAOC, leading to further analyses."}, {"title": "8 Conclusion", "content": "This paper presents an in-depth study of LLMs\u2019 fluid intelligence deficiencies using the ARC tasks, with a series of controlled experiments from multiple perspectives. Through task decomposition, we introduce the atomic ARAOC benchmark, revealing that LLMs struggle with atomic operations despite their simplicity for humans. We further demonstrate that LLMs' task composition abilities are limited, as improvements on the decomposed ARAOC tasks via fine-tuning do not lead to better performance on ARC tasks. Additionally, our study shows that LLMs' difficulty in encoding abstract input formats is a major obstacle in addressing ARC tasks. Lastly, it shows an intrinsic limitation in the left-to-right paradigm of LLMs, which hinders their ability to achieve advanced fluid intelligence."}, {"title": "Limitations", "content": "Due to the experiment budget, on all the ARC related experiments, we only evaluate LLMs on 100 tasks rather than the whole corpus following Wang et al. (2024), which may lead to potential bias in the evaluation results. Also, although most of the ARC tasks can be composed by the six atomic operations proposed by our work, there may still exist very few tasks that cannot be composed by our atomic operations, which may also introducing few bias to Table 9. We will try to provide more comprehensive results in future works once we get more experimental budgets, and propose more atomic operations that could be used to cover more ARC tasks."}]}