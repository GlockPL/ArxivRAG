{"title": "Enhancing Speech Emotion Recognition through Segmental Average Pooling of Self-Supervised Learning Features", "authors": ["Jonghwan Hyeon", "Yung-Hwan Oh", "Ho-Jin Choi"], "abstract": "Speech Emotion Recognition (SER) analyzes human emotions expressed through speech. Self-supervised learning (SSL) offers a promising approach to SER by learning meaningful representations from a large amount of unlabeled audio data. However, existing SSL-based methods rely on Global Average Pooling (GAP) to represent audio signals, treating speech and non-speech segments equally. This can lead to dilution of informative speech features by irrelevant non-speech information. To address this, the paper proposes Segmental Average Pooling (SAP), which selectively focuses on informative speech segments while ignoring non-speech segments. By applying both GAP and SAP to SSL features, our approach utilizes overall speech signal information from GAP and specific information from SAP, leading to improved SER performance. Experiments show state-of-the-art results on the IEMOCAP for English and superior performance on KEMDy19 for Korean datasets in both unweighted and weighted accuracies.", "sections": [{"title": "1. Introduction", "content": "Speech emotion recognition (SER) is an active area of research in the field of speech processing, aiming to automatically recognize the emotional state of a speaker from their speech signal. SER has gained significant attention due to its potential applications in various domains such as human-computer interaction, virtual assistants, and affective computing where understanding the emotional context can greatly enhance the interaction between humans and machines. However, accurately recognizing emotions from speech signals remains a challenging task due to the complex nature of human emotions and the variability of speech signals across different speakers and contexts.\nOne of the key challenges in SER is to extract and utilize meaningful and effective features from speech signals for accurate emotion recognition. Traditionally, SER systems rely on handcrafted features, such as Mel-frequency cepstral coefficients (MFCCs), spectral features, and prosody features, which are designed to capture specific aspects of speech signals. However, these features are limited in their ability to capture the complex and dynamic nature of emotions conveyed through speech because they do not capture the higher-level abstractions that are essential for emotion recognition.\nRecently, self-supervised learning (SSL) has gained significant success in the natural language processing field, where models are trained on large amounts of unlabeled text data and learn to capture complex contextual relationships between words and phrases. Inspired by this success, researchers have explored the use of SSL models to extract more abstract and informative features from speech signals. These models are trained on large amounts of unlabeled speech data and can learn to capture a wide range of speech characteristics, including phonetic, syntactic, and semantic information potentially capturing more comprehensive and contextualized information.\nMeanwhile, speech signals inherently vary in length, resulting in features extracted from SSL models also having variable lengths. To leverage these variable-length SSL features in machine learning models, which typically require fixed-length representations for input, it is essential to transform them into a fixed-length format. The traditional approach for this transformation is to apply Global Average Pooling (GAP) on SSL features across the temporal dimension. However, speech signals primarily consist of two types of segments: speech segments, which convey meaning through words and phrases, and non-speech segments, which consist of silence and background noise. Since GAP treats all segments equally, whether they are speech or non-speech, it can lead to the dilution of informative features extracted from speech segments by irrelevant information contained within non-speech segments. Consequently, this can negatively impact the performance of SER models that use SSL features.\nTo solve this problem, we propose Segmental Average Pooling (SAP), which focuses only on speech segments of speech signals, while ignoring non-speech segments. By applying both GAP and SAP on SSL features, our proposed model can utilize overall information of the speech signal from the GAP representation and specific information of the speech signal from the SAP representation.\nWe evaluate our proposed approach on two datasets, IEMOCAP [1] for English and KEMDy19 [2, 3] for Korean, using both unweighted and weighted accuracy. We perform the leave-one-speaker-out cross-validation to measure performance independently of speaker characteristics. Our proposed approach, which combines GAP and SAP, achieves better performance on both datasets compared to relying solely on GAP. Furthermore, we achieve state-of-the-art performance on both datasets, demonstrating the effectiveness of our proposed approach.\nOur main contributions are as follows: (1) We propose a novel pooling method, SAP, which focuses only on speech segments and ignores non-speech segments to prevent the dilution of informative features. (2) We demonstrate that combining GAP and SAP improves the performance of SER models that use SSL features. (3) We achieve state-of-the-art performance on the IEMOCAP for English and superior performance on the KEMDy19 for Korean using our proposed approach."}, {"title": "2. Related Works", "content": "Speech emotion recognition (SER) is an active area of research that aims to detect the emotional state of a speaker based on characteristics of their speech signal. Over the years, various machine learning techniques have been employed on different types of acoustic features extracted from the speech. Early SER systems utilized Gaussian Mixture Models (GMMs) trained on low-level descriptors such as pitch, energy, and Mel-Frequency Cepstral Coefficients (MFCCs) [4, 5, 6].\nWith the advent of deep learning, neural network architectures such as convolutional neural networks (CNNs) [7] and long short-term memory (LSTM) networks [8] have achieved state-of-the-art performance by learning discriminative feature representations directly from the raw audio. Some studies have also explored using auxiliary modalities like text transcripts [9] or visual facial expressions [10, 11] to complement the acoustic speech data. Recently, advances in self-supervised learning [12, 13] and transformer architectures [14] have further enhanced SER performance. However, challenges still persist in real-world deployment scenarios with noisy inputs and under-represented emotion classes."}, {"title": "2.2. Self-supervised learning", "content": "Self-supervised learning (SSL) has emerged as a powerful technique that leverages large amounts of unlabeled data to train models through carefully designed self-supervision tasks. This pre-training process enables models to learn the intrinsic characteristics and patterns present in the data, acquiring rich, contextualized representations. These learned representations can then be effectively fine-tuned for various downstream tasks using a relatively small amount of labeled data and a limited number of training epochs, achieving competitive or even state-of-the-art performance.\nIn recent years, various SSL models have been introduced in the speech processing field, such as Wav2Vec 2.0 [15], Hu-BERT [16], and WavLM [17]. These models have demonstrated significant improvements in performance compared to previous approaches across a wide range of downstream speech tasks, including automatic speech recognition, keyword spotting, speaker identification, as shown in [18]."}, {"title": "3. Proposed Approach", "content": "In this paper, we propose a novel approach to enhance speech emotion recognition (SER) by applying both Global Average Pooling (GAP) and Segmental Average Pooling (SAP) on self-supervised learning (SSL) features. An overall architecture of our proposed approach is illustrated in Figure 1."}, {"title": "3.1. Self-supervised learning features", "content": "SSL models, which are pre-trained on large-scale audio data, allow us to obtain contextualized speech features directly from a raw speech signal of a given utterance. Let X be a raw speech signal of an utterance u. To feed X into SSL models, X is first divided into a sequence of frames $X = (x_1,x_2,...,x_T)$, where $x_i \\in R^d$ represents the i-th frame of the utterance u, and T is the number of frames determined by the length of the raw speech signal, the window size w and the stride s. Given a pre-trained SSL model $f_{SSL}(.)$,\n$f_{SSL}(X) = [C_1, C_2,..., C_T]$"}, {"title": "3.2. Global Average Pooling", "content": "Since the primary objective of SER is to recognize the emotion conveyed by the entire utterance u, rather than the emotion at the individual frame level xi, it becomes crucial to aggregate these frame-level speech features obtained by fssL() into a single utterance-level speech feature. A traditional approach to achieve this aggregation is to apply Global Average Pooling (GAP) across the temporal dimension of these frame-level speech features as follows:\n$GAP(X) = \\frac{1}{\\left | f_{SSL}(X) \\right |}\\sum_{c_i \\in f_{SSL}(X)}C_i$"}, {"title": "3.3. Segmental Average Pooling", "content": "Speech signals primarily consist of two types of segments: speech segments, which convey meaning through words and phrases, and non-speech segments, which consist of silence and background noise. However, in equation 2, GAP treats all frames equally, regardless of whether they are speech segments or non-speech segments. This can lead to the dilution of informative features extracted from speech segments by irrelevant information contained within non-speech segments. Consequently, this may negatively impact the performance of SER models that utilize SSL features.\nTo address this issue, we propose Segmental Average Pooling (SAP), which focuses only on speech segments of speech signals, while ignoring non-speech segments. This selective approach ensures that only informative features extracted from speech segments contribute to the final utterance-level feature. To define SAP(\u00b7), it is necessary to determine whether a given frame contains speech. For this purpose, we utilize the voice activity detection (VAD) algorithm.\u00b9\n$VAD(x) = \\begin{cases} 1 & \\text{if x contains speech} \\\\ 0 & \\text{otherwise} \\end{cases}$\nUsing VAD(), we collect frame-level SSL features only from speech segments. Then, we apply multi-head self-attention (MHSA) to these features to capture additional relationships among speech segments as described below:\n$g(X) = [C_i | C_i \\in f_{SSL}(X), VAD(x_i) = 1]$\n$h(X) = MHSA(g(X))$\nAs a result, we define SAP(\u00b7) as follows:\n$SAP(X) = \\frac{1}{\\left | h(X) \\right |}\\sum_{h_i \\in h(X)}h_i$"}, {"title": "3.4. Combining GAP and SAP", "content": "Our proposed approach leverages the complementary strengths of both GAP and SAP representations. GAP() captures the overall, global information of the speech signal, providing a broad context that includes the average characteristics of the entire signal. Conversely, SAP() focuses on specific, salient features of the speech signal, which are crucial for accurately distinguishing between nuanced phonetic elements or speech characteristics. Therefore, we define the final speech representation, SR(), as the concatenation of GAP(\u00b7) and SAP(\u00b7):\n$SR(X) = Concat(GAP(X), SAP(X))$"}, {"title": "3.5. Multi-task learning", "content": "Human emotions are complex, and available SER datasets are often limited in size. This necessitates a strategy that maximizes the information extracted from each data sample. To address this challenge, we adopt a multi-task learning (MTL) approach, which aims to concurrently predict both continuous and discrete emotions. The total loss L is defined as:\n$L = L_{discrete} + \\beta L_{valence} + \\gamma L_{arousal}$\nwhere $L_{discrete}$ is the weighted cross-entropy loss 2 for predicting discrete emotions, and $L_{valence}$ and $L_{arousal}$ are the mean absolute error losses for predicting continuous valence and arousal emotions, respectively. The coefficients \u03b1, \u03b2, and y balance the contribution of each loss component to the total loss."}, {"title": "4. Experiments", "content": "We conduct our experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) [1] dataset for English and the Korean Emotion Multimodal Database in 2019 (KEMDy19) [2, 3] for Korean.\nThe IEMOCAP dataset consists of five sessions in total, where each session features one male and one female speaker engaged in a conversation. Similar to previous studies, the utterances labeled as \"excited\" are merged into \"happy\", and only four emotion classes {angry, happy, neutral, and sad} are considered. As a result, the number of utterances representing angry, happy, neutral, and sad are 1103, 1636, 1708, and 1084, respectively. To compare our performance with existing studies under the same conditions, we employ the leave-one-speaker-out 10-fold cross-validation approach, where 8, 1, 1 folds are used as training, validation, and test sets, respectively.\nSimilarly, the KEMDy19 dataset includes twenty sessions, with each session featuring one male and one female speaker engaged in a conversation. We also consider only four emotion classes {angry, happy, neutral, sad}. As a result, the number of utterances representing angry, happy, neutral, and sad are 1530, 1313, 4328, and 773, respectively. To evaluate the performance independently of speaker characteristics, we perform the leave-one-speaker-out 40-fold cross-validation, where 38, 1, 1 folds are used as training, validation and test sets, respectively."}, {"title": "4.1. Experimental setup", "content": "Evaluation metrics: Following previous studies [19, 20, 21, 22], we use unweighted accuracy (UA) and weighted accuracy (WA) as our evaluation criteria.\nSelf-supervised learning model: We use WavLM Large [17] as our self-supervised learning (SSL) model fsSL which has achieved competitive performance in the SER task on the SU-PERB benchmark [18]. According to WavLM Large, it uses the window size w of 25ms and the stride s of 20ms.\nMulti-task learning: We use \u03b1, \u03b2 and y as 0.5, 0.25 and 0.25, respectively.\nProjection dimension: The final speech representation SR(\u00b7) is projected into 32 dimensions before feeding it to the classifier for discrete emotion and the regressors for continuous emotions.\nImplementation details: Our code is implemented using Py-Torch [23] and HuggingFace Transformers [14]. Due to limited memory capacity, utterances exceeding 19 seconds in the IEMOCAP dataset and 16 seconds in the KEMDy19 dataset are truncated. We employ an epoch of 30, a batch size of 64, a learning rate of 3e-5, a warm-up ratio of 0.1, and the cosine learning rate scheduler. Additionally, we utilize early stopping with a patience of 5, monitoring the total loss L on a validation set. Our model has approximately 316M trainable parameters. We conduct our experiments using NVIDIA A100 40GB and the estimated training time is about 8 hours in the IEMO\u0421\u0410\u0420 dataset and about 40 hours in the KEMDy19 dataset for each experiment."}, {"title": "4.2. Results", "content": ""}, {"title": "4.2.1. IEM\u041e\u0421\u0410\u0420", "content": "Table 1 presents a comparison of the performance of three different methods on the IEMOCAP dataset, employing a leave-one-speaker-out 10-fold cross-validation setting. Compared to GAP() which is a traditional approach for aggregating SSL features, our proposed method, SR(\u00b7), which combines GAP(\u00b7) and SAP(), demonstrates superior performance on both UA and WA. However, we observe that SAP() alone does not show an improvement in performance. This indicates that the overall information of the speech signal remains important for accurately recognizing emotions conveyed through speech signals.\nTable 2 presents a comparison of our proposed method with recent state-of-the-art (SOTA) approaches. For a fair comparison, we only consider previous works performing a leave-one-speaker-out 10-fold cross-validation. The results show that our proposed approach, which combines both GAP(\u00b7) and SAP(\u00b7), achieves a relative improvement of 0.43% and 0.64% on UA and WA, respectively, compared to other SOTA approaches, demonstrating the effectiveness of our method."}, {"title": "4.2.2. KEMDy19", "content": "Table 3 presents a comparison of the performance of three different methods on the KEMDy19 dataset, employing a leave-one-speaker-out 40-fold cross-validation setting. Similar to the findings with the IEMOCAP dataset, we observe that our proposed approach, SR(), shows superior performance in both UA and WA compared to GAP(\u00b7). Since this paper is the first to conduct a leave-one-speaker-out 40-fold cross-validation to evaluate performance independently of speaker characteristics, we are unable to compare our results directly with previous works fairly. However, the performance improvements from the proposed approach, SR(), compared to the traditional approach, GAP(\u00b7), demonstrate the effectiveness of our proposed method.\nFigure 2 shows the confusion matrix generated from our proposed method. According to this matrix, our proposed approach achieves highest accuracy in the angry class on the IEMOCAP dataset and in the neutral class on the KEMDy19 dataset. In contrast, our approach exhibits the lowest accuracy in the happy class on both datasets."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel Segmental Average Pooling (SAP) method designed to enhance speech emotion recognition by effectively utilizing self-supervised learning (SSL) speech features. SAP selectively focuses on informative speech segments while ignoring non-speech segments like silence and background noise. By combining SAP with Global Average Pooling (GAP), our approach leverages both overall information from the entire speech signal through the GAP representation and specific information from speech segments through the SAP representation. Our experimental results on two datasets, the IEMOCAP for English and the KEMDy 19 for Korean, demonstrate that our proposed approach achieves superior performance compared to relying solely on GAP. Notably, our proposed method achieves state-of-the-art performance on the IEMOCAP dataset and highly competitive results on the KEMDy19 dataset, highlighting its effectiveness in capturing the complex and dynamic nature of emotions conveyed through speech."}]}