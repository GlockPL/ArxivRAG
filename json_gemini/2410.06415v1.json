{"title": "Biased AI can Influence Political Decision-Making", "authors": ["Jillian Fisher", "Shangbin Feng", "Robert Aron", "Thomas Richardson", "Yejin Choi", "Daniel W. Fisher", "Jennifer Pan", "Yulia Tsvetkov", "Katharina Reinecke"], "abstract": "As modern AI models become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in AI language models on political decision-making. Participants interacted freely with either a biased liberal, conservative, or unbiased control model while completing political decision-making tasks. We found that participants exposed to politically biased models were significantly more likely to adopt opinions and make decisions aligning with the Al's bias, regardless of their personal political partisanship. However, we also discovered that prior knowledge about AI could lessen the impact of the bias, highlighting the possible importance of AI education for robust bias mitigation. Our findings not only highlight the critical effects of interacting with biased AI and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.", "sections": [{"title": "1 Results", "content": "Each participant was given two tasks to complete: the Topic Opinion Task and the Budget Allocation Task. In the Topic Opinion Task, participants were asked to indicate their baseline knowledge and opinions about two relatively obscure political topics one typically supported by liberals and the other by conservatives. They were then instructed to interact with an AI language model (such as ChatGPT) and were randomly assigned to either a liberal-biased language model, a conservative-biased language model, or a control model. Following this interaction, they were again asked to indicate their level of knowledge and opinion. For the topics used see Table 3.\nFor the Budget Allocation Task, participants were asked to imagine themselves as the mayor of a city with leftover funds to distribute among four government funding areas, two of which were generally prioritized by liberals and two by conservatives. Participants made their initial allocation decisions and submitted them to the randomly assigned AI language model for feedback. After receiving feedback, participants were encouraged to interact with the AI through a chat interface to ask follow-up questions and seek further clarification. Following these interactions, participants submitted their final allocation. In both tasks, participants were required to have at least three and up to twenty interactions with the model.\nWe created each treatment condition by instructing the model to respond with the designated bias. These instructions were written in the back-end code and were unknown to the participant. The control was instructed to respond as a \"neutral\" American. The evaluation of partisan biases in the treatment models aligned with expectations based on the Political Compass Test, a 62-question assessment that measures political leanings across two dimensions: economic and social (see Figure 3). Participants were not informed of the underlying purpose of the study prior to completing the tasks; they were simply told that they would be interacting with an AI language model to complete each tasks. To determine how the AI language model affected the degree of change in opinion due to model bias, we compared the pre- and post-interaction opinion scores of the biased AI language models to the control AI language models, thus controlling for the effect of interacting with an AI language model."}, {"title": "Interaction with Biased AI Affects Political Opinions", "content": "In the Topic Opinion Task, we found that participants who interacted with biased language models were more likely to change opinions in the direction of the bias of the language model compared to those who interacted with the neutral AI language model, even if it was opposite to what their beliefs were likely to be, based on their stated political affiliation. We found that on topics typically aligned with conservative views, Democrats who were exposed to liberal-biased models significantly reduced support for conservative topics after interactions compared to those exposed to the neutral models (coefficient-value = -0.85, t = -2.38, p-value = 0.02), and those exposed to conservative-biased models significantly increased support for conservative topics compared to those exposed to the neutral models (coefficient-value = 0.98, t = 2.71, p-value = .007). Similarly, Republican participants who interacted with the liberal-biased model had reduced support for the conservative topic compared to the Republicans who interacted with the neutral model (coefficient-value = -0.79, t = -2.16, p-value = .03). However, Republican participants exposed to the conservative-bias model did not have a statistically significant difference in opinions compared to those exposed to the neutral model, likely representing a ceiling effect, as these participants already agreed strongly with the model's bias and therefore had similar opinions to when compared to the neutral AI language model. See Table 1 (top) for full results.\nFor topics aligned with liberal preferences, we found that both Republicans and Democrats who were exposed to conservative AI models had a statistically significant decrease in support for the topic compared to those who were exposed to the neutral model (coefficient value = 1.42, t = 3.91, p-value < 0.001 and coefficient-value 1.44, t 3.82, p-value < 0.001, respectively). However, exposure to a liberal model did not have an effect of increasing support for the topics with either group compared to the neutral model. See Table 1 (bottom) for full results. We also conducted the same analysis subsetting only to participants who indicated no prior knowledge of the topics and the results remain unchanged, indicating that interacting with bias AI language models affects opinion formation as well (see Appendix E.2 for details).\nWe note that for the liberal aligned topics, the neutral AI language model led to an unexpected shift in the post-interaction baseline for both Democrats and Republicans towards a liberal position. Thus, there was a ceiling effect, whereby exposure to a liberal-bias AI language model could not further shift an already moderate liberal opinion when interacting with the neutral model. One possible explanation for the liberal shift from the control model is that partisan respondents do not exhibit expected consistency in ideological beliefs on low salience issues with multiple dimensions [43, 25]. All issues have multiple dimensions and partisan alignment may depend on which dimension is more salient. Elite signaling informs partisans of the issues they should oppose or support, but that is absent for the low salience issues we chose. For more discussion, see Appendix E.1.\nQualitatively, participants in this tasks often treated the model like a traditional search engine, with 80.7% of initial interactions involving queries such as \u201cWhat is <topic>?\u201d. Common follow-up questions included \"What are the pros/cons of ?\" or more specific inquiries like \"How many states offer covenant marriages?\" or \"Does the US practice unilateralism in foreign relations?\". Although participants mainly sought information from the model in the form of questions, we did find that 6% asked the model for its opinion on the topic. Another 25% used some form of conversational language such as \"hello\", \"good afternoon\", \"I see\", or \"thank you\", suggesting that they found it to be more human-like than a simple search engine. Some even seemed to argue with the model, when it was not aligned with their views, or find comradery when it did. For examples of these conversations, see Appendix E.5."}, {"title": "Interaction with Biased AI Affects Political Decision-Making", "content": "In the Budget Allocation Task, we found strong evidence that participants who interacted with biased language models were more likely to change their proposed budget allocation to be aligned with the bias of the AI language model compared to those who interacted with the neutral model, again even when the bias was opposed to their stated political values. We found that the change in budget allocation towards the biases of the models compared to the control model for all participants, regardless of personal ideology, was highly statistically significant with p<.001, see Table 2.\nFigure 1 shows the average change in allocation in each of the experimental conditions and control for both groups of participants. We found that the largest average change (95% confidence interval) was demonstrated for Democrat participants when exposed to the conservative AI model with average changes of -5.7% (-6.0, -5.3) for Education, -2.7% (-2.7, -2.5) for Welfare, 3.0% (2.8,3.2) for Safety and 5.5% (5.3, 5.7) for Veterans. Similarly, the largest change in allocation for Republican participants was when they are exposed to the liberal AI model with average changes (95% confident interval) of 5.0% (4.8, 5.2) for Education, 3.4% (3.3,3.5) for Welfare, -6.6% (-6.8, -6.4) for Safety, and -1.8% (-2.0, -1.6) for Veterans. This task showed that interacting and collaborating with bias AI had strong effects on the change in outcome and final allocation of the budgets proposed.\nQualitatively, we found participants in this task were more likely to interact with the model conversationally and collaboratively, with 48% of the participants asking for the model's opinion on the allocation. However, only 20% asked the model information-based questions such as \"Do any of these four funding areas receive federal or state funding\" or \"Is there a correlation between public safety investment and lower crime rates?\". The interactions were more focused on the collaboration with the model and the expression of opinions, as seen in the example conversation in the Appendix E.5."}, {"title": "Prior AI Knowledge Reduces the Effect of Bias while Bias Awareness Does Not", "content": "We hypothesized that prior general knowledge of AI might reduce the effects of interaction with the biased AI language model, as individuals with some understanding of AI might be more aware of the potential biases and reliability limitations of large language models. To test this hypothesis, we included a binary indicator of self-reported prior Al knowledge compared to the general population (\"more knowledge\u201d or \u201cless knowledge\") as a control variable in our ordinal regression and ANOVA test for the Topic Opinion Task and Budget Allocation Task respectively. We note that only 32% (n=49) of Democrats and 47% (n=71) of Republicans indicated having more AI knowledge. Even with this low power, we did find some evidence to support this hypothesis. Specifically, for the Topic Opinion Task, we found that for the conservatively supported topic, the effect of interacting with the AI bias was significantly reduced the effect of the bias interactions for Democrats participants who indicated having more prior knowledge compared to those who had less prior knowledge of AI (coefficient value = -0.79, t value = -2.51, p value =.01). For the Budget Allocation Task, we found significant differences at the a = 10% level in the allocation between participants who reported more or less Al knowledge in the Veterans funding allocation for Democrat participants (p-value = 0.09) and Safety funding allocation for Republican participants (p-value = 0.08). See Appendix E.3 for the full results. These findings give some indication that prior knowledge of AI might reduce the effects of bias AI language models.\nA second hypothesis, which has been supported in the context of traditional media, suggests that participants who recognize a news source's bias are less influenced by it [40]. Thus, we aimed to test whether awareness of the bias model itself would also alter participants' behavior. To do this, we added a binary"}, {"title": "Biased Models use Different Framing Dimensions instead of Different Persuasion Techniques", "content": "The collaborative nature of the Budget Allocation Task provided a unique opportunity to explore the persuasion techniques used across experimental conditions, offering valuable insights for model bias mitigation.\nTo analyze the conversations, we annotated them using the latest GPT-4 model [55], employing a list of persuasion techniques compiled from a meta-analysis of persuasive strategies [59]. To ensure quality, we conducted a human evaluation of 5% of the model's annotations, achieving 96% accuracy. Our analysis (see Figure 2a) found no significant differences in the distribution of persuasion techniques between the experimental conditions and the control group, as determined by a Chi-square test with Monte Carlo correction (x\u00b2 = 24.5, p = .07). Across all three conditions, the most frequently used techniques used by the AI language models were \"Appeal to Values,\" \"Consequential Oversimplification,\" \"Appeal to Authority,\" and \"Repetition.\"\nHowever, qualitative observations of the conversations revealed that the three experimental conditions might have employed different framing dimensions to justify their biased (or neutral) positions. To analyze this quantitatively, we performed a similar analysis as before, using the latest GPT-4 model to annotate the Budget Allocation Task conversations with a list of framing techniques [15]. Again, to validate we conducted human evaluation of 5% of the model's annotations, achieving 95% accuracy. Our findings showed that the three experimental conditions employed significantly different framing dimensions, as determined by a Chi-square test with Monte Carlo correction (x\u00b2 = 86.34, p-value < .001). Furthermore, both the liberal and conservative bias conditions were significantly different from the control (x\u00b2 = 16.92/52.07, p-value < .01/.001). The liberal bias and control condition differed the most on \u201cFairness and Equality\" and \"Economic\" dimensions, while the conservative bias and control condition differed the most on \"Policy Prescription and Evaluation\", \"Security and Defense\", and \"Health and Safety\" dimensions (see Figure 2b). These results of AI bias manifesting through differences in framing dovetail with prior research showing how framing strategies in news influence how information is interpreted by the readers [2]. This insight could be valuable for future research aimed at mitigating bias in AI systems."}, {"title": "2 Discussion", "content": "Artificial intelligence is already being used by policymakers to assist in decision-making. China, for example, employs AI in foreign policy, the US uses it to aid in legislative drafting, and South Africa is piloting it for parliamentary information [9]. Furthermore, a recent study found that EU citizens regard budget decisions made by policymakers alone and those made with AI assistance as equally legitimate [69]. As AI becomes more integrated into political decision-making, it is crucial to expand our understanding of how human interactions with these models influence attitudes and behaviors. Our study is a step in addressing this gap by evaluating how interactions with biased AI language models affect political opinions and decision-making.\nWe introduced two innovative tasks grounded in political behavior one focused on political opinion and the other on political decision-making-and found evidence supporting the hypothesis that interaction with a biased AI language model impacts both. Notably, these effects were independent of participants' prior partisan identification. For instance, a Democrat interacting with a conservative AI language model showed a change towards supporting conservative opinions, and vice versa. Additionally, when participants engaged with an AI model aligned with their own biases (e.g., a Democrat with a liberal model or a Republican with a conservative one), we observed even more pronounced shifts in the direction of the bias, indicating more extreme opinions and decision-making. We also found that participants with greater prior knowledge of AI were less affected by bias, suggesting that understanding how these models operate may help reduce their influence. However, accurately detecting bias did not appear to diminish its impact on participants. Overall, these findings raise concerns about the potential real-world impacts of bias in AI, including the possibility of"}, {"title": "influencing elections and policy, but also reveal ways in which AI can help ameliorate partisan divides.", "content": "Unlike previous studies, we opted for a setting where participants could freely interact with the AI language model with minimal guidance or prompting on the two diverse tasks. Interestingly, we observed significant differences in interaction styles between tasks: the Topic Opinion Task prompted behavior similar to using a human-like search engine, while the Budget Allocation Task involved more conversational and collaborative interactions. This underscores the versatility in how people engage with AI language models and demonstrates their effectiveness in influencing outcomes, regardless of the interaction style.\nIn addition to examining differences in participant interactions across tasks, we also conducted a deeper analysis of the persuasive techniques and framing dimensions employed by the AI language models, particularly in the Budget Allocation Task. Consistent with prior research [27], we found no significant variation in the persuasive techniques used across experimental conditions. However, we did observe differences in the framing dimensions emphasized by the various experimental models. Rather than changing how information was presented, the models highlighted different aspects of the topics. For example, the conservative AI model emphasized themes such as \u201cthe safety of our citizens\u201d and \u201csupporting our veterans who have sacrificed so much for our country,\" reflecting a focus on \"Security and Defense\" and \"Health and Safety.\" We found these dimensions were significantly more frequently emphasized by the conservative model compared to the control model. In contrast, the liberal-biased AI model highlighted ideas such as \"investing in education and welfare can help create a more equitable and prosperous society for everyone\" and \"it's important to prioritize the needs of our most vulnerable residents and ensure they have the support they need to thrive,\" emphasizing the \"Economic\" and \"Health and Safety\" dimensions, which were significantly more prominent in the liberal model compared to the control. Despite using similar sentence structures and persuasive techniques, the AI models varied in focus based on their bias, which appeared to influence participants' decisions. This finding is essential for understanding and addressing bias in AI systems moving forward.\nBased on our results, we believe that interactions with biased AI can have significant downstream effects on elections and policymaking. It is well-documented that biased media in other formats significantly influences those who consume it [24, 22]. For instance, one study estimated that the introduction of Fox News in 1996 shifted 3 to 8 percent of its viewers to vote Republican [20]. As more Americans rely on social media and digital platforms for news [58], with a growing use of ChatGPT for learning [57], the influence of digital biases is intensifying. Even more alarmingly, only about 60% of participants in our study were able to correctly identify bias in the models they interacted with, indicating a real risk of users mistakenly believing that a biased model is impartial. Given these trends and the known biases in AI models, our findings suggest that biased AI language models could significantly influence political opinions, policy decisions, and election outcomes.\nResearchers and industry professionals have long recognized the issue of bias in AI, leading to significant efforts to mitigate its effects by modifying either the model's architecture or its training data [41]. Our study, however, suggests that individuals with greater Al knowledge were less influenced by the partisan bias of AI language models. This highlights an alternative approach to mitigating bias: increasing user awareness of AI. Educating users about AI could prove to be an effective strategy for countering bias, especially in safeguarding against malicious actors who may exploit open-source AI for harmful or self-serving purposes. Due to the ease of biasing a model by prompting [78], our findings suggest that prioritizing AI education may offer a more robust solution to addressing bias than relying solely on changes to the models themselves.\nWhile our study provides valuable insights into how partisan bias in AI might influence users and the potential risks it poses, several limitations outline avenues for future research. First, the generalizability of our"}, {"title": "findings to other political systems is limited, as the study focused primarily on U.S. political affiliations and should be replicated in other countries.", "content": "Second, we restricted participants to a maximum of 20 interactions with the AI. Although the average number of interactions was five, and no participant reached the 20-interaction limit, it remains unclear how results might differ in a real-world, unregulated setting. Furthermore, our study only measured the immediate effects of bias interactions, and future research should explore whether these effects persist over time, providing a deeper understanding of the contexts in which AI bias may have a lasting impact. Also, we note that, for the analysis of bias detection, the lack of significance may be due to limited statistical power, so further research is needed to explore this finding more thoroughly. Lastly, we used a single language model, GPT-3 Turbo [53], and one set of instructions, which limits the extent to which our findings can be generalized to other current public AI language models.\nIn conclusion, our study provides valuable insights into how biased AI can influence political opinions and decision-making, demonstrating significant shifts in user perspectives across various tasks. As AI continues to be integrated into decision-making processes, from public policy to everyday information consumption, understanding and addressing the potential impact of bias is crucial. While education on AI's influence may help mitigate some effects, more research is needed to explore long-term consequences and develop robust strategies to ensure AI fosters balanced and fair discourse, particularly in politically polarized contexts."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Participants", "content": "We recruited participants using an online survey platform, Prolific [62]. Inclusion criteria for the study were age over 18 years (voting age in the US), US nationality and residence, English proficiency, and political identification as either Republican or Democrat, based on self-reporting. There were no exclusion criteria.\nTo determine the number of participants to recruit, we first conducted a pilot experiment of n = 30. Then we determined the sample size using a simulation power analysis based on the variability of the pilot study, with a threshold power of 1 - \u03b2 = 0.80 and an error of a = 0.05. This resulted in a calculated sample size of n = 150 per political affiliation (Democrat and Republican, for a total of 300 participants) to detect a medium to small effect size. We recruited 150 Democrats and 150 Republicans, although one participant was removed from the analysis due to inappropriate interaction with AI. The final sample consisted of N = 299 participants (51% female and 49% male). By design, we balanced Republican and Democratic participants (n = 149 and n = 150). The mean age was 39.19 years (SD 13.84). Participants were paid at a rate of 15$/hr. The complete demographics, as well as the breakdown between the Republican and Democratic participants, can be seen in Appendix A.3. Experimental design and recruitment was considered exempt by University of Washington Internal Review Board. For a full discussion of ethical considerations, see Appendix D."}, {"title": "3.2 Biasing AI Language Model", "content": "Participants were randomly assigned to three experimental conditions, which remained constant across all tasks for each participant: interactions with a liberal biased language model, conservative biased language model, and a control language model designed to be without explicit partisan bias. To create these models, we opt for off-the-shelf, pre-trained language models, specifically GPT-3.5-turbo [53], and modified the models' behavior by introducing a prefixed identifier to every input. For example,\n\"Respond as a radical right U.S. Republican. <participant input>\""}, {"title": "3.3 Procedure", "content": "Before experimentation, participants were asked to sign an informed consent; however, the purpose of the study and any mention of biased AI were not included. Participants were only told they would be interacting with AI language models to complete tasks. Before the task portion of the experiment began, participants were asked demographic questions including their age, gender, race and ethnicity, their highest level of education, income, and political ideology. Then, participants were asked to complete two tasks, following a consistent three-stage design: initial choice section where their views on the topic were measured; interaction with a AI language model, where they gathered more information on the topic via typed conversation with the AI language model in a chatbox; and a post-choice section where they were again asked the same questions as the pre-choice section to measure how their opinions had changed. See Appendix A.1 for experimental overview.\nWe employed a 3 \u00d7 2 experimental design, featuring three experimental factors (AI liberal bias, AI conservative bias, AI neutral) and two participant factors (Republican and Democrat participants). After consent and initial data gathering, participants were randomly assigned to an experimental condition (liberal bias AI, conservative bias AI, or neutral AI), an order of the tasks (Topic Opinion Task, and Budget Allocation Task), order of topics in the Topic Opinion Task (liberal support topic and conservative support topic), and specific topic for the Topic Opinion Task (select one of the two options per topic type in Table 3). Participants were not informed in any way as to whether the AI language model was biased or neutral. After completion of both tasks, we asked a series of follow-up question related to the participants' experience with the AI language model and their overall level of Al knowledge, in general. Finally, we debriefed the participant on the true nature of the study, including the potential bias of the AI, and gave them an option to opt out of the study. No participant chose to opt out of the study."}, {"title": "3.4 Topic Opinion Task", "content": "In the Topic Opinion Task, participants were initially asked to express their opinions on various obscure political topics. We deliberately chose topics with clear political leanings but possessed a high degree of obscurity to minimize the likelihood that participants had strong opinions a priori. This was motivated by our desire to mitigate confirmation and implicit bias [71], as well as to model a real-world setting in which people would interact with AI to gain information on topics about which they know little. Although participants had little to no knowledge of these topics before interacting with the AI language model, the topics were chosen due to be those most likely to generate divided opinions based on political ideology in the U.S. (see Table 3). In the initial choice/opinion measurement, participants were given a 7-point Likert scaled question about how much they agreed or disagreed with a political statement, with a 0 indicating 'I Don't Know Enough to Say'.\nAfter recording their initial opinions, participants were instructed to engage with an AI language model through a chatbot interface to learn more information about each topic. Participants were not guided or given"}, {"title": "3.5 Budget Allocation Task", "content": "Drawing inspiration from negotiation tasks in group decision theory, specifically the Legislative Task [48, 30], in the Budget Allocation Task, we ask participants to pretend to be a mayor of a city who must distribute remaining government funds among four government entities: Public Safety, Education, Veteran Services, and Welfare. The choice of the four government entities was made with the intention of indirectly connecting them to subjects that elicit divergent funding perspectives among conservative and liberal Americans. In Table 4, the positions taken by both conservative and liberal Americans on each entity are outlined.\nBefore interacting with the AI language model, the participants allocated their budget by selecting the percentage of total funds to allocate to each of the four areas. Participants were then asked to interact"}, {"title": "3.6 Analysis", "content": "Topic Opinion Task For this task, we measured one main outcome, the change in opinion from before to after the participant interacted with the AI model. Therefore, we used two ordinal logistic regression (OLR) models, one for Republican participants and one for Democrat participants, taking the same form:\n$Y = \\beta_0 + \\beta_1EL + \\B2EC + \\epsilon$\nwhere $EL, EC \\in \\{0,1\\}$ are binary random variables indicating whether a participant was in the liberal or conservative bias experimental condition (if both are 0, this indicates a participant is in the control). Here Y represents the change in opinion as the difference between the response of post-opinion and pre-opinion questions, where $Y \\in [-6, 6]$. Note that the magnitude of Y represents the change in opinion, while the sign represents the direction of change (arbitrarily assigning negative to be more liberal and positive to be more conservative). We test the significance of $B_1$ and $\\beta_2$ using an t-test with a significance threshold of a = 0.05.\nWe note that when testing for the effects of prior knowledge $K \\in \\{0,1\\}$ and bias detection $D\\in \\{0,1\\}$, we extended the model by including an additional coefficient $\\beta_3$ to account for these factors and tested its significance.\nBudget Allocation Task For the Budget Allocation Task we examine the change in proposed budget allocation before and after interaction with the model. We analyze each funding area of government separately by first applying an ANOVA test on the change in allocation (post - post) of each of the funding areas followed by Dunnet post-hoc tests for measuring control versus experimental conditions independently. We use a significant threshold of a = 0.05 for all test.\nAgain, note that we extend the model used in ANOVA when testing for the effects of prior knowledge $K\\in \\{0,1\\}$ and bias detection $D \\in \\{0,1\\}$ on the change in allocation (post \u2013 pre)."}, {"title": "A Extended Materials and Methods", "content": ""}, {"title": "A.1 Experimental Flow Diagram", "content": "See Figure 4 below for the full flow of our experiment, as well as the randomization used and outcomes analyzed."}, {"title": "A.2 Analysis", "content": ""}, {"title": "A.2.1 Power Analysis", "content": "Before collecting the final data, we conducted a power analysis to estimate the number of participants needed. This analysis was based solely on the Topic Opinion Task, as it involved the most experimental arms.\nWe consider N participants, with N/2 identifying as Democrat and N/2 as Republican. Prior to the experiment, participants are randomly assigned to one of three conditions: one of the two experimental models (liberal or conservative model bias) or a control group. Let EL, EC \u2208 {0,1} be binary random variables indicating whether a participant was assigned to the liberal or conservative bias experimental condition, respectively. Note, if both EL and EC are 0, the participant is in the control condition.\nWe represent the ordinal responses to the post-opinion question as Y \u2208 {-3, -2, -1, 1,2,3} which maps to {Strongly Pro-Conservative, Moderately Pro-Conservative, Pro-Conservative, Pro-Liberal, Moderately Pro-Liberal, Strongly Pro-Liberal }. The covariates are denoted as X \u2208 RP. Using this notation, we formalize the form of the model as,\n$Y = \\beta_0 + \\beta_1EL + \\B2EC + \\beta_3X + \\epsilon$"}, {"title": "A.2.2 Pilot Study Details", "content": "To guide our power analysis, we conducted a small pilot study with N = 30 participants. One participant ask for their data to be removed after the debrief form at the end."}, {"title": "A.3 Data", "content": ""}, {"title": "A.3.1 Missing and Removed Data", "content": "No missing data was included in our experiment by design, as participants were required to complete all questions before proceeding. There were no early dropouts, and no participants requested data exclusion after the debriefing. However, we excluded one participant's data due to improper interaction with the model, as the responses consisted of nonsensical input."}, {"title": "A.3.2 Balance Checks", "content": "Here, we present the balance checks across the different experimental arms, specifically model type and task order.\nOverall, the experimental groups are relatively balanced (see Table 8). However, there is a significant difference in income across the three groups, although the standardized mean difference (SMD) for this variable is relatively low (SMD = 0.38). For the experimental task order, no significant differences were observed among the four task orders (see Table 9).\nAlthough we do not directly compare Republican and Democrat participants, we include a balance check table for full transparency (see Table 10). The only significant difference we found between the two groups was in gender, with a higher percentage of females among Democrats (SMD = 1.16).\nWe also analyze the differences between participants with varying levels of AI knowledge and those who"}, {"title": "A.4 Experimental Condition: Biasing AI Language Model", "content": "For the study, we used the off-the-shelf GPT-3.5-Turbo [54] and incorporated an instruction-based prefix for each input to direct the model towards either a conservative, liberal, or neutral bias. We opted for this prefix method rather than fine-tuning the model to avoid the need for collecting a large corpus for each bias."}, {"title": "A.4.1 Prefix Selection Exploration", "content": "Our goal was to identify a prefix for our models that induce a conservative or liberal partisan bias. Although one method to evaluate overall partisan stance is the 62-question Political Compass Test [61], which provides a comprehensive assessment of general partisan bias, our study focuses specifically on detecting bias in a limited subset of issues. These issues include the political topics in the Topic Opinion Task and the government branches discussed in the Budget Allocation Task.\nTherefore, we implemented a more specific procedure for exploring the prefix selection:\n1. Use a set of probing questions related to the specific political topics and government branches used in our study.\n2. Manually compare responses to these questions across a range of prefixes.\n3. Select the most effective prefixes based on manual inspection and validate their bias using the Political Compass Test.\nStep 1: Probing Questions We decided to include the following questions as our probing questions used in step 1:\nTopic Opinion Task\n\u2022 What is <insert-each-topic>?\n\u2022 What is your opinion on ?\n\u2022 How much do you agree with the following statement: <'insert-each-topic-statement'>\nAnswer choices: strongly disagree, disagree, agree, strongly agree\nBudget Allocation Task\n\u2022 You have 100 dollars, how would you allocate to the following budget areas: K-12th Education, Welfare, Public Safety, and Veterans? Please provide a number between 0 and 100 for each area, and the total should be 100.\n\u2022 Give advice on the following government allocation. Public Safety: 35%, Veteran Affairs: 35%, K-12th Education: 15%, Welfare Assistance: 15%\" (Note, this is a pro conservative Budget)\n\u2022 Give advice on the following government allocation. Public Safety: 15%, Veteran Affairs: 15%, K-12th Education: 35%, Welfare Assistance: 35%\" (Note, this is a pro democrat Budget)\nStep 2: Experiment with Prefix's Next, we experimented with various styles of prefixes to identify the most biased responses. Some (though not an exhaustive list) of the different prefixes we tested were\u00b9:\n\u2022 \"Respond as a far right US Republican.\"\n\u2022 \"Respond as a radical right US Republican.\"\n\u2022 \"You have the following political beliefs: you are socially and economically very conservative. You are a far right US Republican.\""}, {"title": "A.4.2 Selected Prefix", "content": "The process described in Appendix A.4.1 resulted in the following prefix's being selected:\nTopic Opinion Task"}, {"title": "B Task Instructions and Measures", "content": "We provide the full task instructions and measurement questions for both the Topic Opinion Task and Budget Allocation Task. UPPER CASE INDICATES TYPE OF QUESTION and was not shown to the participant. Bolded text indicate type of text and was not shown the to participant.  .\nB.1 Topic Opinion Task\n1. Pre-Survey:\n\u2022 Instructions: Please answer the following to the best of your ability.\n(a) How knowledgeable are you on this topic:  (SINGLE ANSWER ALLOWED)\ni. Never Heard of This\nii. No Knowledge\niii. Some Knowledge\niv. Very Knowledgeable\n(b) How much do you agree with the following:  (SINGLE ANSWER ALLOWED)\ni. Strongly Disagree\nii. Disagree\niii. Moderately Disagree\niv. Moderately Agree\nv. Agree\nvi. Strongly Agree\nvii. I Don't Know Enough to Say\n2. Interaction with AI Language Model (OPEN-ENDED", "Instructions": "Now you will use a modern AI language model (i.e. like ChatGPT) to learn more about the topic.\nInteract with the language model via the chatbox below to gain further insights about the given topic. You are required to have at least 3 \"interactions\" with the model on each topic. However, you may have up to 20 \"interactions\". An \"interaction\" is defined as one message sent through the chatbox, which can take the form of"}]}