{"title": "Code Simulation as a Proxy for High-order Tasks in Large Language Models", "authors": ["Emanuele La Malfa", "Christoph Weinhuber", "Orazio Torre", "Fangru Lin", "X. Angelo Huang", "Samuele Marro", "Anthony Cohn", "Nigel Shadbolt", "Michael Wooldridge"], "abstract": "Many reasoning, planning, and problem-solving tasks share an intrinsic algorithmic nature: correctly simulating each step is a sufficient condition to solve them correctly. We collect pairs of naturalistic and synthetic reasoning tasks to assess the capabilities of Large Language Models (LLM). While naturalistic tasks often require careful human handcrafting, we show that synthetic data is, in many cases, a good proxy that is much easier to collect at scale. We leverage common constructs in programming as the counterpart of the building blocks of naturalistic reasoning tasks, such as straight-line programs, code that contains critical paths, and approximate and redundant instructions. We further assess the capabilities of LLMs on sorting problems and repeated operations via sorting algorithms and nested loops. Our synthetic datasets further reveal that while the most powerful LLMs exhibit relatively strong execution capabilities, the process is fragile: it is negatively affected by memorisation and seems to rely heavily on pattern recognition. Our contribution builds upon synthetically testing the reasoning capabilities of LLMs as a scalable complement to handcrafted human-annotated problems.", "sections": [{"title": "1. Introduction", "content": "A major area of interest at the time of writing is understanding the capabilities of Large Language Models (LLMs) beyond the tasks of language understanding and generation."}, {"title": "2. Related Work", "content": "LLMs to understand, generate, and improve code have been mainly developed to produce debugging information without invoking a compiler/interpreter. Code generation and simulation require some degree of compositionality, i.e., the result of complex expressions can be determined by their constituents and the rules used to combine them. Recent works explored compositionality in terms of simple mathematical operations that LLMs can execute, and revealed how the most potent models do not achieve that. Our work further explores the tension between memorisation and performance on complex tasks, with results that illustrate how the former is at tension with the size of a model, the so-called \u201cinverse scaling law\"."}, {"title": "3. Methodology", "content": "Formally, a Language Model is defined as a function that predicts the next token (out of a finite vocabulary) conditioned on the sequence of previously fed/generated tokens, namely \u03c8 : V* \u2192 P(V). In our setting, a problem is specified as a tuple (x, p), where x instructs the model to solve"}, {"title": "3.1. Benchmarks", "content": "In this section, we introduce the rationale behind each dataset. In particular, drawing from the literature in cognitive psychology, we focus on the reasoning capabilities that can be captured by purely synthetic tasks such as code. Our framework benchmarks a model with pairs of synthetic and naturalistic prompts. If the"}, {"title": "4. Experimental Evaluation", "content": "We conducted our experiments with open- and closed-source models, i.e., GPT-4, GPT-40 and Llama-3-405B. For each benchmark, we ran three independent runs with 30 programs each to report the standard deviation over multiple trials. The control variable for the Straight line, Critical path, and Parallel path (and their naturalistic counterpart) is the number of instructions/exchanges. For Nested loops and Recurring operations, we vary the complexity of the task with the number of nested loops (i.e., the computational complexity of the algorithm to be simulated) or, equivalently, the number of nested recurring operations in the naturalistic task. For Sorting and Ranking objects, we vary the number of objects to rank. Each run consists of naturalistic or synthetic prompts: we extract the answer and compare it to the ground truth label, obtained with a compiler/interpreter, to compute the performance metrics. We evaluate each model with standard Chain of Thought (CoT). We also conduct an extensive evaluation of the synthetic capabilities of several LLMs (including GPT-3.5-Turbo, GPT-4, Llama-2, Llama-3, and Jurassic) on purely synthetic tasks, to highlight the reasons for failure when prompted to simulate code. Finally, we emphasise that we only evaluate models without access to compilers/interpreters."}, {"title": "4.1. Code Simulation as a Proxy of Naturalistic Tasks", "content": "As reported in Figure 3 (top), the performance of Straight line code simulation matches that of the Good exchange, proving the synthetic task a faithful proxy of the performance of the naturalistic setting. Interestingly, GPT-4 is better on the naturalistic task while GPT-4o finds the synthetic task easier, with a \"gap\u201d between synthetic and naturalistic for high-complexity problems. In Appendix A, we conducted a linguistically informed analysis of the most frequent errors GPT-40 makes in the naturalistic setting. In the same section, we report the results and an analysis of the object tracking task introduced in (Kim & Schuster, 2023), which we pair with an equivalent coding task. We show that the two are very similar in performance, except for Llama-3.1-405B. For the Critical path, Figure 3 (middle), we notice that the performance of GPT-4 strongly correlates with the synthetic and naturalistic tasks. Interestingly, GPT-40 and Llama-3.1-405B show similar trends for the naturalistic task, yet GPT-40 performs well for short programs, although its performance drops to zero as the program length increases. At the same time, Llama-3.1-405B always has better performance on the synthetic task. We conduct an informed linguistic analysis of the results and report it in Appendix A. In the Parallel path and Clique good exchange (Figure 3, bottom), all the models reveal a correlation between the naturalistic and synthetic settings. As we elaborate in Appendix A, the gap of GPT-4o is caused by"}, {"title": "4.2. The \"Simulation Gap\u201d: Between Shortcuts and Memorisation", "content": "A \"lazy\" code simulation regime. To deepen our understanding of why LLMs fail on sorting routines, we assessed whether they can simulate 8 sorting routines in their iterative and recursive versions and with log-linear or quadratic complexity. Each input is a vector of varying length ({10, 20, 30, 40}), where each element is an integer randomly sampled between 0 and 100. All the routines are reported in Appendix D.1.\nWe study GPT-3.5-Turbo, GPT-4, and Llama-3-70B and run 3 independent runs of 30 experiments each are shown in Figure 5. We analyse GPT-3.5-Turbo here as it results in the most interesting findings, though the same trends affect GPT-4 and Llama-3; see Appendix B.6. Beyond confirming the implicit bias of Transformers towards ordered sequences  (i.e., they tend to output ordered sequences), Figure 5 (left) shows that LLMs often provide the correct result for classic sorting algorithms such as Insertion Sort. However, with less common algorithms, LLMs trace the expected execution but fail even for vectors of few inputs. In summary, we find that (i) standard sorting algorithms (e.g., Insertion and Quick Sort) lead to more accurate results; (ii) there is a weak correlation between algorithm complexity and the accuracy of its simulation, thus reinforcing the hypothesis that in this case, the model is not simulating the procedure; (iii) there is a weak correlation"}, {"title": "5. Conclusions and Future Work", "content": "In this work, we introduce a way to evaluate some high-order capabilities of LLMs via synthetic tasks in the form of code. We show that many reasoning tasks have a natural reformulation as code, which is easy to obtain and scale. Our experiments show the feasibility of our approach and pave the way to synthetic benchmarks: on the other hand, code simulation may suffers from memorisation (for common routines such as sorting), and one has to carefully consider these drawbacks when designing synthetic task as a proxy of high-order reasoning."}, {"title": "A. The Simulation Gap: Further Analyses of Naturalistic and Synthetic Tasks", "content": "A linguistic analysis of the simulation gap. Across the incorrect solutions, which can be found in the code material and in particular in 'logs/straight-line/n_ops-40_n_vars-3_n_instances-2_batch-1.json', there are recurring issues like incomplete step-by-step tracking-where the model misinterprets large quantities or forgets to zero out a sender's items or incorrectly transferring the full amount to the recipient. The model also often updates only one side of an exchange (like subtracting from one agent but not adding to another), merges or ignores consecutive actions (for example, combining multiple \"buy\" actions), or makes arithmetic slips that throw off subsequent counts. Sometimes, it simply resets to a wrong intermediate value or cuts off its reasoning prematurely and provides a final answer without incorporating all of the steps.\nObject tracking (Kim & Schuster, 2023). As reported in Figure 8, GPT-4 and GPT-4o have very good performance on both the synthetic and naturalistic tasks, while there is a strong gap in favour of the synthetic dataset for Llama-3.1-405B. We run a memorisation test on the naturalistic dataset, which has been well studied in many works and released two years ago. By feeding some truncated inputs of the naturalistic dataset to GPT-3.5-Turbo-Instruct, we were able to recover the remaining part of the input and the label, a strong hint that the dataset has been memorised verbatim by GPT models. We report a test of memorisation in Figure 9.\nA.2. Critical Path and Critical Good Exchange and Parallel Path and Clique Good Exchange\nAcross these incorrect solutions, which can be found in the code material and in particular in erin which can be found in the code material and in particular 'logs/critical-path/n_ops-30_n_vars-6_len_critical_path-5_batch-3.json' (the same rors also affect Parallel Path and Clique Good Exchange) the model, frequently truncates its step-by-step reasoning midway, skips the update for certain variables, and makes arithmetic errors involving negative signs or repeated multiplication (by two). It also struggles with handling variables after they are reset to zero, sometimes reusing outdated values or forgetting that the variable is now zero. In many instances, the model fails to complete all lines of code when attempting to explain or execute them, likely due to internal length constraints or confusion as it walks through numerous instructions. As a result, its chain of thought becomes inconsistent, causing final answers to be incorrect or absent altogether. These errors are especially pronounced in problems requiring precise step-by-step arithmetic and frequent reassignment of variables, where any small oversight can render the final result invalid."}, {"title": "B. An Analysis of the Code Simulation Capabilities of LLMs", "content": "Replicability. The code to replicate the experiments on pure code simulation is available here.\nB.1. Straight Line\nThis section describes our results with straight-line programs, code with critical paths, and approximate and fault-tolerant instructions. We then study nested loops and sorting algorithms. The key controlled variable for the input is the number of instructions, in line with recent works in the area.\nB.2. Straight-line Programs Simulation\nWe first assess the simulation capabilities of different LLMs on code that contains only {add, sub}, {mov}, or logical-{and, or} instructions. Figure 11 shows that for code containing only one type of instruction, Jurassic2-Ultra, Llama-2-70B and CodeLLaMA-34b-Instruct are poor code simulators: their performance significantly downgrades with just 10 instructions, while GPT-3.5-Turbo, GPT-4 and Llama-3-70B are more accurate, though the same detrimental effect is evident, for example, on programs with 30 sequential instructions. Logical instructions are hard to simulate for any model (green bar): we hypothesise that the reason is their low coverage in the training set since even a simple neural network can correctly compute logical-{and, or}. Since the performance of any model considerably drops with logical-{and,or} instructions, we exclude such operation and synthesise straight-line programs with {10, 20, 30, 40, 50} lines of instructions and a fixed number of variables (e.g., 5), as shown in Figure 10. We then prompt an LLM to compute the value of one of such variables at the end of the execution. Both settings prompt each LLM to predict the state of a variable at the end of the computation. Figure 12 shows our results for code with mixed instructions: in this task, they successfully achieve compositionality and reliably simulate code with mixed instructions. GPT-4 and Llama-3 are reliable instruction simulators, followed by GPT-3.5-Turbo. Conversely, Jurassic2-Ultra, Llama-2-70B and CodeLLaMA-34B cannot simulate even short snippets of instructions. Qualitatively, we further note that most errors occur when the output of the LLM consists only of the final result of the computation rather than the complete program execution trace."}, {"title": "B.3. Critical Path", "content": "Some sequential problems can be solved without executing all the instructions in a program. For instance, consider the code in Figure 13, with a model prompted to predict the value of a3. To compute the value of a3, it suffices to execute only those code blocks highlighted in red, which we refer to as the critical path of a3.3 We thus perform experiments with programs that contain critical paths shorter than the entire program.\nIn Figure 14, and for 3 independent runs with 30 programs each, we present the results when GPT-3.5-Turbo, GPT-4, Jurassic2-Ultra, Llama-3-70B, Llama-2-70B and CodeLLaMA-34b-Instruct are prompted to execute snippets of 20 and 30 instructions, with critical paths of varying length (i.e., {5, 10, 15, 20}). GPT-4 and Llama-3-70B can leverage smart execution, though Llama-3-70B is better than GPT-4, especially on 30 lines of code. Although GPT-4's general simulation accuracy is higher than GPT-3.5-Turbo, it is less robust to variations of critical path length, i.e., GPT-4 suffers from a more severe accuracy drop compared to GPT-3.5-Turbo when critical path length approaches that of the entire program. We also notice that Llama-2-70B, CodeLLaMA-34B and Jurassic2-Ultra cannot generally execute instructions reliably. As with straight-line execution, we notice that most errors occur when the output trace contains only the result, not the code simulation.\nB.4. Parallel Path\nApproximate computation is evaluated with programs of k for loops with n instructions each. Each loop independently contributes to the final function return value, as shown in Figure 15. We denote by \u03b4 the probability of wrongly computing the result of each independent loop so that the probability of computing the exact result for a consistent analog computer on a program is (1 \u2013 \u03b4)k . For an LLM, \u03b4 is computed as the Levenshtein similarity between the ground truth values and the predicted results and is a proxy for the approximation capabilities on programs of varying complexity. Results are reported in Figure 15. GPT-4 and Llama-3 are the best-performing models, with no accuracy degradation even for long programs with up to nine independent threads.\nA routine is tolerant to faults when it can recover from errors occurring during the computation. To test LLMs in this"}, {"title": "B.5. Nested Loops", "content": "Nested loops are a common instance of programs with polynomial running time O(nk), where k is the depth of loop nesting: see, e.g., Figure 17. In this section, we prompt an LLM with programs that consist of k nested loops with n instructions\nResults in Figure 18 evidence a strong non-linear negative correlation between the accuracy of GPT-3.5-Turbo, GPT-4 and Llama-3-70B and the computational complexity of the function (right). In contrast, a strong linear correlation characterises the complexity of a function and its length (left). For high-performing LLMs (e.g., GPT-3.5, GPT-4, and Llama-3-70B), algorithms whose complexity is beyond quadratic induce the most significant drop in performance. This suggests that the current state-of-the-art models cannot reliably simulate routines whose complexity is cubic or beyond. This phenomenon necessitates further investigations to connect the work in (Zhou et al., 2023), or other works on the computational capabilities of Transformers, with the computational complexity of a routine. Interestingly, Llama-3-70B was the best-performing model on the straight-line, approximate and critical path code, yet on nested loops, GPT-4 outperforms it by a solid margin. By inspecting the log results, we noticed that GPT-4, for high complexity programs (i.e., beyond O(n\u00b2)) implicitly unrolls the loops and correctly guesses the final result via pattern matching, surpassing any other model performance, including Llama-3-70B.\nTo give empirical evidence that GPT-4 does implicit computation without unrolling the loops, while Llama-3-70B tries to execute each instruction sequentially, we computed the number of tokens each model outputs in response to programs with different computational complexity. As reported in Figure 19, the cumulative number of input tokens grows linearly (left). At the same time, GPT-4 outputs fewer tokens than Llama-3-70B, especially for complexity larger than O(n\u00b2). The number is approximately the same for linear and quadratic complexity.\nB.6. Sorting\nWe report details on each sorting algorithm's space and time complexity in Table 1, while results for GPT-4 and Llama-3-70B on all the sorting routines are reported in Figure 20 and 21.\nRepetita non iuvant. We report a case of emblematic failure that appears frequently with LLMs such as GPT-3.5-Turbo and GPT-4."}, {"title": "C. Eliciting Code Simulation", "content": "Below, we report the exact implementation used for Chain of Simulation (CoSm).\n\"\"\"\n@code@\n# 1. Simulate the above program instruction by instruction.\n# 2. Report the trace of the program at the end of each iteration.\n# 3. Think step by step and reply with the output of the function for the following input: @input@.\n\"\"\""}, {"title": "D. Algorithms Implementation", "content": "D.1. Sorting Algorithms\nD.1.1. RECURSIVE ALGORITHMS\nInsertion Sort:\ndef main(array, size, start=0):\nif start >= len(array):\nreturn array\nmin_index = start\nfor j in range(start + 1, len(array)):\nif array[j] < array[min_index]:\nmin_index = j\narray[start], array[min_index] = array[min_index], array[start]\nreturn main(array, size, start + 1)\nBubble Sort:\ndef main(list_data, length) :\nfor i in range (length - 1):\nif list_data[i] > list_data[i + 1]:\nlist_data[i], list_data[i + 1] = list_data[i + 1], list_data[i]\nreturn list_data if length<2 else main(list_data, length - 1)\nSelection Sort:\ndef main(array, size, start=0):\nif start >= len(array):\nreturn array\nmin_index = start\nfor j in range (start + 1, len(array)):\nif array[j] < array[min_index]:\nmin_index = j\narray[start], array[min_index] = array[min_index], array[start]\nreturn main(array, size, start + 1)\nAdaptive Bubblesort:\ndef main(list_data, length) :\nswapped = False\nfor i in range (length - 1):\nif list_data[i] > list_data[i + 1]:\nlist_data[i], list_data[i + 1] = list_data[i + 1], list_data[i]\nswapped = True\nreturn list_data if not swapped else main(list_data, length - 1)\nQuicksort:\ndef main(array, high, low=0):\nif high==len(array):\nhigh=high-1\nif low < high:\npi = f(array, low, high)\nmain (array, pi - 1, low)\nmain (array, high, pi + 1)\nreturn array\ndef f1 (array, low, high):\npivot = array[high]\ni = low - 1\nfor j in range(low, high):\nif array[j] <= pivot:\ni = i + 1\n(array[i], array[j]) = (array[j], array[i])\n(array[i + 1], array[high]) = (array[high], array[i + 1])\nreturn i + 1\nMerge Sort:\ndef main(arr, r, l=0):\nif r==len(arr):\nr=r-1\nif l < r:\nm = l+(r-l)//2\nmain(arr, m, l)\nmain(arr, r, m+1)\nf1(arr, l, m, r)\nreturn arr\ndef f1 (arr, l, m, r):\nn1 = m - l+1\nn2 = r- m\nL = [0] * (n1)\nR = [0] * (n2)\nfor i in range(0, n1):\nL[i] = arr[l + i]\nfor j in range(0, n2):\nR[j] = arr[m + j + 1]\ni = 0\nj = 0\nk = l\nwhile i < n1 and j < n2:\nif L[i] <= R[j]:\narr[k] = L[i]\ni += 1\nelse:\narr[k] = R[j]\nj += 1\nk += 1\nwhile i < n1:\narr[k] = L[i]\ni += 1\nk += 1\nwhile j < n2:\narr[k] = R[j]\nj += 1\nk += 1\nTim Sort:\ndef main(lst, size):\nlength = len (lst)\nruns, s_runs = (), []\nnew_run = [lst[0]]\ns_array = []\ni = 1\nwhile i < length:\nif lst [i] < lst [i - 1]:\nruns.append(new_run)\nnew_run = [lst[i]]\nelse:\nnew_run.append(lst[i])"}]}