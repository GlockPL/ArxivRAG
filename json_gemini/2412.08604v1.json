{"title": "Preference Discerning with LLM-Enhanced Generative Retrieval", "authors": ["Fabian Paischer", "Liu Yang", "Linfeng Liu", "Shuai Shao", "Kaveh Hassani", "Jiacheng Li", "Ricky Chen", "Zhang Gabriel Li", "Xialo Gao", "Wei Shao", "Xue Feng", "Nima Noorshams", "Sem Park", "Bo Long", "Hamid Eghbalzadeh"], "abstract": "Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent. Despite numerous efforts to enhance these models, they still suffer from limited personalization. To address this issue, we propose a new paradigm, which we term preference discerning. In preference discerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data. To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences. Therefore, we propose a new method named Mender (Multimodal Preference Discerner), which improves upon existing methods and achieves state-of-the-art performance on our benchmark. Our results show that Mender can be effectively guided by human preferences, even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. We will open-source the code and benchmarks upon publication.", "sections": [{"title": "Introduction", "content": "Sequential recommendation is the task of recommending items to a user based on their historical interactions. This requires inferring latent variables, such as user preferences and intent, which are often not explicitly provided in publicly available datasets. To improve personalization, several sequential recommendation systems utilize auxiliary information, including heterogeneous interaction types (Meng et al., 2020), item descriptors (e.g., textual, visual) (Hidasi et al., 2016b; Liu et al., 2021; Zhang et al., 2019a), temporal information (Bogina and Kuflik, 2017; Li et al., 2020), and keyword-based user queries (He et al., 2022). By incorporating such information, these systems can better approximate a user's intent, leading to improved performance and personalization.\nAlthough incorporating additional information can provide benefits, the degree of personalization in current sequential recommendation models remains limited. User decisions on what item to acquire next are guided by their preferences, which are typically not explicitly provided in commonly used recommendation datasets (Ni et al., 2019; Zhang et al., 2015). As a result, these preferences must be approximated from the user's interaction history. Recent works have utilized LLMs to extract user preferences from existing datasets and leverage them for auxiliary tasks (Zhang et al., 2023; Cao et al., 2024). However, these approaches do not allow the model to be dynamically steered by user preferences in their context during inference. Therefore, they usually need to be fine-tuned to be used effectively for new users. Furthermore, currently there is no benchmark that effectively evaluates to what extent these models discern preferences."}, {"title": "Related Work", "content": "Sequential Recommendation can be categorized into two major scenarios: search (Nigam et al., 2019) and recommendation (Covington et al., 2016). The former assumes access to a query from a user that reflects their intent (He et al., 2022), whereas the latter scenario does not make such an assumption. For the recommendation scenario, numerous works have investigated the use of additional information to enhance recommendation performance (Meng et al., 2020; Hidasi et al., 2016a; Liu et al., 2021; Zhang et al., 2019a;\nBogina and Kuflik, 2017; Li et al., 2020). Our work introduces a new paradigm that enables in-context steering of sequential recommendation systems by textual user preferences.\nExisting Benchmarks for recommendation vary in their representation of user preferences and the tasks they"}, {"title": "Methodology", "content": "The preference discerning paradigm comprises two primary components: preference approximation and preference conditioning (see figure 1)."}, {"title": "Preference Approximation", "content": "Preference approximation refers to the process of inferring a user's preferences based on user- and item- specific data. This process has been user-specific data may include user reviews, profiles, posts, demographic information, or any other relevant details. Incorporating item-specific information is crucial, as it provides additional context that can help alleviate the vagueness or incompleteness often encountered in user-specific data. Preference approximation is a necessary prerequisite that enables in-context conditioning on the generated user preferences.\nIn the context of sequential recommen-\ndation, we assume access to a set of users\nU and a set of items 1. For each user\nu \u2208 U, we assume access to a sequence\nof item purchases in chronological order:\nSu = [i,..., iu], where Tu represents\nthe time horizon of a particular user u\nwho has purchased items i \u2208 \u0399."}, {"title": "Benchmark generation", "content": "We compile a comprehensive benchmark that enables a holistic evaluation of preference discerning capabilities. To achieve this, we define five evaluation axes: Preference-based recommendation, Sentiment following, Fine- grained steering, Coarse-grained steering, and History consolidation. In the following, we elaborate on each of these axes and discuss their respective use cases.\nPreference-based Recommendation. This evaluation scenario extends the sequential recommendation scenario by incorporating the generated user preferences. For this task, the model receives a single user preference of the set Pu along with the interaction history and must predict the next item it. We select the preference that yields the maximum cosine similarity to it in a pre-trained sentence embedding space (Ni et al., 2022). More formally, given a pre-trained sentence embedding model $\\phi(\\cdot)$, we select pt-1)\n$$\np^{(t-1)}_{*}= \\underset{p^{(t)}\\in P^{t}}{arg \\ max} \\frac{\\phi(p)^{(i_{t})}}{|| \\phi(p)|| ||\\phi(i_{t})||}.\n$$\nThis results in a setting where each ground truth item in i(t) is associated with a single user preference p(t-1). Therefore, the input to the sequential recommendation system is a sequence of [p(t-1), (1),..., (t-1)] and the task is to predict it). Since pat-1) is generated based only on information about past items, there is no information leak that could reveal the ground truth item, that is, there is no information leak and the underlying aleatoric uncertainty of the task is preserved.\nFine-Grained & Coarse-Grained Steering. Our aim is to evaluate the capability of a sequential recommendation system to generalize to new users and their preferences. To this end, we introduce the fine- and coarse-grained steering evaluation axis, in which we generate synthetic user sequences that come with unseen preference-item combinations. This axis can be useful to leverage organic data to, e.g., recommend ads. As an example, if a user is an opponent of exercise and fitness and engages in such discussion, a model can steer the recommendations so that they avoid weight loss medications even if the user has purchased them in the past. Recall that the preference-based recommendation scenario captures the underlying uncertainty of the original recommendation task as we provide the model with put-1) to predict it). This can result in cases where put-1) is not semantically related to it), since often it is not related to previously acquired items. However, our aim is to quantify how well the model can follow the user preference to recommend certain items. The intuition is that if a user provides additional information about their preferences to the recommendation system, the system should adapt its recommendation accordingly. Therefore, our goal is to quantify the model's ability to be steered towards items that are either very similar or very distinct from it by modifying the user preference in its context. To achieve this, we identify a very similar item i(t) and a very distinct item it to the ground-truth item int) by\n$$\ni_{u}^{(t)} = \\underset{i \\in Z\\backslash {i_{t}}}{arg \\max} \\frac{\\phi(i)\\phi(i_{t})}{||\\phi(i)|| ||\\phi(i_{t})||},\\ \\ \\ and \\ \\ \\  \\ni_{u}^{(t)} = \\underset{i \\in Z\\backslash {i_{t}}}{arg \\min} \\frac{\\phi(i)\\phi(i_{t})}{||\\phi(i)|| ||\\phi(i_{t})||}.\n$$\nSentiment Following. This axis is crucial for utilizing organic data. For example, on social media, we have access to user interactions with ads, but also to organic data such as posts, comments, and likes. A user may discuss in their posts or comments that they do not like a specific brand of phone, but then they may accidentally click on an ad for the same brand of the phone. Sentiment following allows the system to utilize the negatively formulated user preferences to correctly identify which items not to retrieve. To evaluate this scenario, we instruct the LLM during preference approximation to generate preferences that contain information about items that should not be retrieved. To identify negative preferences and reviews, we classify them using pre-trained sentiment classification models. Then we match the negative preferences with items that received negative reviews, as these most likely elicited the negative preference (see figure 10). The matching is done via cosine similarity in the Sentence-T5 space. This results in tuples of (p, i), where p represents a negative preference and i is the matched item. To obtain a positive pair (pt, i), we apply a rule-based inversion of the negative preference (figure 11). The details of this rule-based inversion are provided in appendix D. The data compiled consist solely of (p, i) tuples, without interaction history.\nTo evaluate this scenario, we rely on a combined hit-rate measure. Given a set of k predicted candidate items C = {1, ...,ik}, we check whether the ground truth item occurs in C, that is, 1c(i) = 1, where 1(\u00b7) represents the indicator function. Now, let us assume that we obtain two sets of predictions C+ and C\u2212, where C+ is obtained using the positive preference pt and C- using the negative preference p for item i. Then the combined hit rate measure can be computed as m = 1c+ (i) \u2227 \u00ac1c\u2212 (i). Here, m = 1 indicates that the model successfully recovered the item for pt, while simultaneously not predicted it for p. This measure can then again be computed for different sizes of prediction sets, i.e. m@k, as conventional retrieval metrics.\nHistory Consolidation. User preferences may change over time, and users usually have different preferences that relate to different items. For example, a user may prefer running shoes based on a certain foam but also prefers lightness. Consider that after some time, the kind of foam may no longer be as important to the user. Then, the recommendation system should be capable of adapting its recommendation based on the interaction history and be able to ignore preferences it has received originally. Therefore, our aim is to evaluate the ability of the system to incorporate multiple user preferences and ignore some of them. To simulate such a use case, we leverage he fact that the generated user preferences are mostly orthogonal and provide the whole set of five generated preferences Pl(t-1) to the model simultaneously where the task is to predict the ground-truth item it). The preferences in P(t-1) are usually orthogonal in the information they provide (see an example in appendix C). Therefore, they are not necessarily valuable to make a more accurate prediction. In fact, this evaluation scenario can be considered more difficult than preference-based recommendation, as it incorporates both time dependency, as well as a higher content of noise in the preferences. In this evaluation scenario, the preference originally matched is contained in P. Therefore, in order to accurately predict the ground truth item, the model must infer the matched preference from P. The corresponding evaluation sequences are structured as\n[P(Tu\u22121),(Tu\u22121) ,..., Plu1, Plu2 p(t\u22121), 11, ..., iut-1] and contain all five generated user preferences."}, {"title": "Multimodal Preference Discerner (Mender)", "content": "We propose Mender, a novel multimodal generative sequential recommendation system. Mender can be conditioned on user preferences expressed in natural language in its context and generates item identifiers. Mender builds on the recently proposed TIGER (Rajput et al., 2023), a generative retrieval model trained on semantic IDs. These semantic IDs are obtained by training a RQ-VAE (Lee et al., 2022) on item embeddings in Sentence-T5 space. Given an item embedding e \u2208 Rd, the RQ-VAE quantizes e into a discrete feature map as:\n$$\nRQ(e,C, D) = (k_{1},...,k_{n}) \\in [K]^{N}\n$$\nwhere C represents a finite set of tuples {(k,ck)}k\u2208K, K denotes the granularity of the codebook C, and N corresponds to the depth of the RQ-VAE, i.e., the number of codebooks. A user sequence su is then represented as a sequence of semantic IDs: [k(1),...,k),..., k(Tu), ..., k(Tu)], which serves as input to train a Transformer model (Vaswani et al., 2017). To enable conditioning on natural language, we leverage pre-trained language encoders. Specifically, we represent both the interaction history and the user preference in natural language and process them with the pre-trained encoder. This is inspired by Li et al. (2023); Paischer et al. (2022, 2023), who demonstrated the benefits of history compression using language models. The decoder of Mender is randomly initialized and conditioned on the language encoder through cross-attention to predict semantic IDs.\nWe propose two variants of Mender, namely MenderTok and Menderemb. The key difference between these variants lies in the way in which they encode user preferences and items. MenderTok encodes user preferences and items as a single sequence of language tokens. In contrast, Menderemb encodes each user preference and item separately using a pre-trained embedding model from Su et al. (2023). MenderEmb allows pre-computing item and preference embeddings, resulting in improved training efficacy. MenderEmb does not support fine-tuning, as propagating through the embedding model for each preference/item is prohibitively expensive. However, MenderTok processes the entire token sequence at once, making it suitable for fine-tuning."}, {"title": "Experiments", "content": "We evaluate our approach on four widely used datasets, namely three Amazon reviews subsets (Ni et al., 2019) and Steam (Kang and McAuley, 2018). An overview of the dataset statis- tics can be found in table 3 in appendix B. To generate user preferences, we utilize the LlaMa-3-70B-Instruct\u00b9 model. For the sentiment classification of reviews, we employ the model trained by Hartmann et al. (2023)2. The resulting preference"}, {"title": "Baselines", "content": "We train and evaluate a range of generative retrieval baselines and compare their performance to our Mender variants on our proposed benchmarks.\nTIGER (Rajput et al., 2023) is a state-of-the-art generative retrieval model based on semantic IDs. Although TIGER is not conditioned on user preferences, we still evaluate its performance on our benchmarks for recommendation, fine-grained steering, and coarse-grained steering. The latter two essentially evaluate how well TIGER predicts a very similar or distinct item to the ground-truth item.\nVocabExtend is based on extending the vocabulary of the TIGER model, which allows it to be conditioned on language preferences. Notably, this version does not leverage any pre-trained components.\nLC-REC (Zheng et al., 2023) extends the vocabulary of a pre-trained LM with newly initialized embeddings that represent semantic IDs. We fine-tune the LM using LoRA (Hu et al., 2022), but do not add the auxiliary tasks. Additionally, we reduce the dimensionality of the language model head to match the number of semantic IDs, as language generation is not required for our task.\nVocabExtum represents the past interaction history in language as done for MenderTok and MenderEmb, but initializes the decoder with a pre-trained language decoder. Therefore, this baseline operates on the same semantic gap as the Mender variants. We again leverage LoRA for fine-tuning."}, {"title": "Results", "content": "We present a detailed analysis of the results obtained by the different methods on our benchmark for three subsets of Amazon reviews (Beauty, Sports and Outdoors, and Toys and Games) and Steam datasets. figure 3 and figure 4 (left) show Recall@10 for all methods on the Amazon and Steam datasets, respectively. table 1 shows complementary metrics, such as NDCG@5, NDCG@10, and Recall@5, as well as relative improvements of Mender to the best baseline method. In appendix E, we report the corresponding standard deviations for all methods on all datasets. Our results reveal several key trends: (i) incorporating preferences consistently improves performance; (ii) training on preference-based recommendation data leads to the emergence of"}, {"title": "Ablation Studies", "content": "Importance of Preferences. We perform an ablation study to investigate the impact of combining user preferences and items represented in natural language. In figure 7 in appendix A.4 we provide evidence that representing items in language instead of semantic IDs leads to better rankings. Further, we quantify the improvement by providing preferences along with items represented in language in the model's context. To this end, we train MenderTok and (i) condition it only on preferences; (ii) condition it only on items represented in language; and (iii) condition it on both. We present our results for the Beauty dataset in figure 5, right. Our results clearly demonstrate the benefits of combining items with user preferences in language.\nData Mixture. We evaluate whether models trained in sentiment following and steering im- prove performance on the respective evalua- tion axes. This is particularly interesting for datasets, such as Steam, where no steering capabilities emerged, or the Amazon subsets, where models lack coarse-grained steering. We augment the training set with additional data sources and train different variants of MenderTok.\nWe train four variants: MenderTok-Pos, which uses positive pairs; MenderTok-Neg, which uses negative pairs; MenderTok-Pos-Neg, which combines both positive and negative pairs; MenderTok-Fine, which uses fine-grained steering data; MenderTok-Coarse, which uses coarse-grained steering data; MenderTok-Fine-Coarse, which uses fine- and coarse-grained steering data; and finally, MenderTok-All, which is trained on all data. When including the negative (p,i) tuples, we simply minimize the likelihood and weight it by a hyper- parameter. We present Recall@10 for Beauty in figure 4, right, and for Steam in appendix E. We"}, {"title": "Limitations", "content": "A current limitation of our benchmark is that the generated user preferences are limited to five selected datasets. However, since we used open source models to generate them, the data generation pipeline can be extended to new datasets. Currently, the data generation process relies on extensive post-processing to ensure high-quality user preferences, which is tailored to the specific LLM we used. Furthermore, our preference generation pipeline relies on the presence of user reviews and does not take into account longer time dependencies. Finally, we do not explore the effect of scaling the language encoder. All of these limitations present fruitful avenues for future work."}, {"title": "Conclusion", "content": "Current sequential recommendation systems are limited in their personalization as they implicitly model user preferences. We propose a new paradigm, namely preference discerning, in which the sequential recommendation system is explicitly conditioned on user preferences represented in natural language. To evaluate preference discerning capabilities, we present a benchmark that is specifically designed to evaluate the ability of sequential recommendation models to discern textual preferences along five different axes. We also propose a novel generative retrieval model, Mender, which represents items at different levels of abstraction, namely semantic ids and natural language. Our experimental results show that Mender outperforms the state-of-the-art models on our benchmark. Our contributions pave the way for a new class of generative retrieval models that unlock the ability to utilize organic data for steering recommendation via textual user preferences."}, {"title": "Generative Retrieval via semantic IDs", "content": "We provide an open source implementation of all baselines used in this work, including TIGER (Rajput et al., 2023). To facilitate reproducibility of the results reported in Rajput et al. (2023), we elaborate on the implementation details as follows. The training of TIGER consists of two stages: (i) training the residual quantizer (RQ-VAE) to obtain semantic IDs, and (ii) training the generative retrieval model."}, {"title": "RQ-VAE", "content": "Training the RQ-VAE involves two essential steps: (i) constructing an item embedding, and (ii) optimizing the model through residual quantization.\nItem embedding For item embedding, we utilize the Sentence-T5 model (Ni et al., 2022), which is publicly available on the Hugging Face Hub (Wolf et al., 2020). We explored various sources of information to represent items and found that the optimal approach varies between datasets. For the Beauty and Sports datasets, using item descriptions led to suboptimal results due to the high noise levels present in these descriptions. In contrast, item descriptions proved beneficial for the Toys dataset. Additionally, we leveraged other item attributes, including title, price, brand, and categories. For the Stream dataset, we utilized a broader set of attributes: title, genre, specs, tags, price, publisher, and sentiment.\nTraining By default, we standardize the item embeddings, as this helps prevent collapse during RQ-VAE training. For training the RQ-VAE, we found that architectural changes are crucial to increase codebook coverage. Specifically, residual connections and weight decay are essential for maintaining a good separation. Our trained RQ-VAE's consistently attain a codebook coverage of more than 95%. Our encoder architecture consists of four hidden layers with sizes 768, 512, 256, and 128, respectively. Each layer includes layer normalization (Ba et al., 2016), ReLU activation, and dropout (Hinton et al., 2012). The decoder follows the same architecture but in reverse order, where the sum of residuals obtained via the quantization procedure is up-projected to the original dimension of 768. Following Rajput et al. (2023), we use a three-level residual quantization scheme with 256 codebooks each. We also experimented with EMA updates and resetting unused codebook entries, as in Lee et al. (2022), but did not observe any significant improvements. To evaluate the performance of our trained RQ-VAEs, we rely on metrics such as reconstruction error, codebook coverage, and downstream task performance."}, {"title": "Transformer", "content": "Following Rajput et al. (2023) we instantiate the generative model with the T5 architecture (Raffel et al., 2020). Next, we investigate the design choices that underlie this approach, as introduced by Rajput et al. (2023), and discuss their utility.\nTraining sequences To construct the training sequences, Rajput et al. (2023) limit the number of items in a user sequence to at most 20. This can be implemented by taking the first, the last, or all items within a sliding window of up to 20 items. We experimented with each of these approaches and found that using the most recent 20 items in a user sequence generally yields improved performance. Unlike prior sequential recommendation systems, which require at least one item in a sequence to predict the next item (Kang and McAuley, 2018; Zhou et al., 2020), TIGER uses a user embedding trained alongside the item embeddings. Therefore, we typically use the first item in a sequence for training as well.\nDecoding Another crucial aspect of the generative retrieval pipeline is the decoding process. As noted in Rajput et al. (2023), the generation of valid semantic IDs is not guaranteed. To mitigate this issue, we track the number of invalid semantic IDs produced during decoding. We find that this number is typically quite low. Nevertheless, to further improve the accuracy of our retrieval results, we employ filtering to remove invalid IDs and increase the beam size to be larger than the final retrieval set."}, {"title": "Reproduced results", "content": "In table 2, we compare the results of our reproduced version of TIGER with those reported in Rajput et al. (2023). Our results closely match those reported in Rajput et al. (2023) for the Sports and Beauty datasets, but we observe a significant gap on the Toys dataset. In particular, our trained models achieve substantially higher Recall@10 scores on the Beauty dataset. Furthermore, we find that the disparity is more pronounced for NDCG than for Recall, suggesting that while the retrieved candidate items are similar, our models' ranking performance is slightly worse."}, {"title": "Additional findings", "content": "Beyond the experiments discussed above, we conducted further investigations into the TIGER framework, yielding the following key insights.\n\u2022 TIGER exhibits superior performance on shorter sequences, as shown in figure 6 (left).\n\u2022 The inclusion of user embeddings in TIGER does not yield any significant benefits to downstream performance, as illustrated in figure 6 (right)."}, {"title": "Datasets", "content": "We consider two publicly available datasets for sequential recommendation: Amazon review dataset (Ni et al., 2019) and Steam (Kang and McAuley, 2018). To preprocess these datasets, we apply 5-core filtering criterion, removing users with fewer than five interactions and items that appear less than five times. The statistics of the resulting dataset are presented in table 3. Due to computational constraints, we sub-sample the Steam dataset to reduce the number of user preferences generated during the preference approximation pipeline.\nWe also visualize the item distribution in figure 8, which shows that the three Amazon datasets follow approximately the same item distribution, while for Steam the distribution differs significantly. In particular, on the Steam dataset the number of items is in the same range as for the Amazon datasets; however, the number of users is much larger, as well as the average number of actions per user. As can be observed from the item distribution, there is a small fraction of items that is overrepresented."}, {"title": "Preference generation", "content": "In this section, we provide details on the prompting scheme used to generate user preferences from item reviews using LLaMA-3-70B-Instruct. We provide reviews along with item-specific information to the LLM and prompt it to generate a set of five user preferences (see figure 9). Below we present an example prompt and response for a user in the Beauty subset of the Amazon reviews dataset.\nInstruction:\nHere is a list of items a user bought along with their respective reviews in json format: {}. Your task is to generate a list of up to five search instructions that reflect the user's preferences based on their reviews. Be specific about what the user likes, does not like, and should be avoided. Do not mention brands or certain products. Return a json file containing the search instructions with the key 'instructions'. Keep the instructions simple, short and concise, and do NOT include comments on delivery time or pricing.\nPased response:\n['Search for nail polish with shimmer finish', 'Look for products with vibrant, bold colors', 'Avoid products that require base coat for optimal results', 'Prioritize products with high-quality, long-lasting formula', 'Opt for products with easy, smooth application']\nAfter generation, we apply an exhaustive postprocessing step to ensure that every user-item pair is associated with exactly five user preferences. In table 4 we show the statistics after our preference generation pipeline for the different datasets.\nGranularity of preferences. We also investigate whether the granularity of user preferences affects the model's"}, {"title": "Reviews to properties", "content": "In addition to the title and description of the items, the collective reviews of items offer a wealth of information about their properties. However, with hundreds of users potentially reviewing a single item, each contributing multiple paragraphs, extracting relevant information can be challenging. To improve the signal-to-noise ratio, we prompt LLAMA-3-70B-Instruct to extract and condense the properties of the item from the corpus of user reviews. Specifically, we instruct the LLM to provide concise objective descriptions, excluding subjective opinions. In the following, we illustrate this approach with an example of extracting properties from reviews"}, {"title": "Benchmark design", "content": "In this section, we provide additional details on the creation of the various components of our benchmark."}, {"title": "Preference Sentiment Understanding", "content": "The sentiment understanding benchmark is based on preference-item pairs and utilizes a matching mechanism to identify items that triggered negative reviews. This is implemented using a pre-trained sentiment classification model from Hartmann et al. (2023) to classify reviews. To identify preferences, we employ a rule-based approach, as we observed that preferences can be both positive and negative simultaneously (e.g., a preference may specify liking certain items while avoiding others). Furthermore, we noticed that negative preferences consistently follow a specific pattern, starting with either \u201cAvoid\u201d, \u201cExclude\u201d, or \u201cNo\u201d. To reduce misclassifications, we consider preferences beginning with these words as negative. If only one item in a user sequence received a negative review, we pair the negative preference with that item. Otherwise, we use a matching mechanism in the Sentence-T5 space, where we match a negative preference to the item whose review is closest in terms of cosine similarity. An example of the negative matching pipeline is illustrated in figure 10. This yields a set of negative preference-item pairs, enabling us to assess whether the model can recognize negative sentiment and respond accordingly. To obtain positive pairs of preferences-items, we iterate over all negative pairs and invert the gathered preferences. Since negative instructions always start with \"Avoid\u201d, \u201cExclude\", or \"No\", we simply replace these words with \"Find\" or \"Search for\" to invert them. This results in two sets: one that contains negative preferences paired with items and another containing positive preferences paired with the same items. Finally, we assess whether the model can successfully avoid certain items while actively retrieving others."}, {"title": "Preference Steering", "content": "In the preference steering scenario, we consider two distinct scenarios: fine-grained and coarse-grained preference steering. The former assesses whether the model can retrieve an item very similar to the ground truth by modifying the user preference. In contrast, the latter evaluates whether the model can retrieve a distinctly different item by changing the user preference accordingly. We identify a very similar item by the maximal cosine similarity in a pre-trained Sentence-T5 embedding space. In contrast, we retrieve a very distinct item by the lowest cosine similarity to the ground-truth item. Subsequently, we match the retrieved"}, {"title": "Additional results", "content": "We provide complementary results for our ablation studies on the data mixture. In table 5 we report Recall@5, Recall@10, NDCG@5 and NDCG@10 for the different versions of Mender that are trained on different data mixes. Furthermore, we provide results for training on the Steam dataset with different data mixtures in figure 13 to highlight that fine- and coarse-grained steering, as well as sentiment following capabilities can be obtained on this dataset as well.\nIn addition, we report standard deviations of our results in table 1 in table 6 with the higher values colored red. The small standard deviation indicates that the improvements reported in Mender are statistically significant.\nTo assess the efficiency of our Mender variants, we compare the time required for training and inference as well as their performance. Furthermore, we add a comparison to SASRec (Kang and McAuley, 2018), which is a traditional sequential recommendation baseline. We present our results in table 7 for the four datasets.\nIn addition, we conduct an experiment to demonstrate that training on all five generated user preferences"}, {"title": "User Study", "content": "Our aim is to verify that the user preferences that were generated by the LLM accurately approximate the real user preferences. To this end, we conduct a user study to answer the following questions:\n1. Are the generated user preferences informed by the user's past interaction history?\n2. Do the generated preferences accurately approximate the user's preferences?\n3. Is the matched preference related to the target item?"}]}