{"title": "MULTIAGENT FINETUNING: SELF IMPROVEMENT WITH DIVERSE REASONING CHAINS", "authors": ["Vighnesh Subramaniam", "Yilun Du", "Joshua B. Tenenbaum", "Antonio Torralba", "Shuang Li", "Igor Mordatch"], "abstract": "Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent breakthroughs in large language models (LLMs) like GPT-3.5 and GPT-4 have demonstrated remarkable proficiency in language generation, comprehension, question answering, and translation (OpenAI, 2023; Touvron et al., 2023). Despite these advancements, LLMs are fundamentally constrained by the data they are trained on, with existing models already using much of the available data on the Internet (Brown et al., 2020). To further enhance the performance of LLMs, recent research on self-improvement, where LLMs generate additional synthetic data on which they are trained on (Huang et al., 2022; Yu et al., 2023).\nOne approach to increase the data available to LLMs is to use powerful existing frontier models like GPT-4 to generate additional supervisory data. However, this approach is limited by the inherent quality of frontier models, preventing models from becoming better than the frontier of what the best existing models can accomplish. In addition, such an approach incurs high financial costs due to inference expenses of such large models and is also often legally prohibited with existing commercial-grade models.\nAn alternative approach is to directly leverage existing language models to generate additional synthetic data for their self-improvement (Zelikman et al., 2022; Bai et al., 2022; Chen et al., 2024b; Yuan et al., 2024). In such works, language models are used to iteratively collect data that they are then finetuned on. However, as models are repeatedly trained, performance gains often plateau relatively quickly as diversity decreases and the self-improvement loop is often only"}, {"title": "2 MULTIAGENT FINETUNING OF LANGUAGE MODELS", "content": "We provide an overview of our approach towards multiagent finetuning of language models, where we learn a multiagent society of models to accomplish a task. Our method involves two components. We first use a multiagent debate method to construct a finetuning dataset for raining models (though other multiagent generation methods can also be used, see Appendix Section D). We then introduce our approach, multiagent finetuning, where we specialize each LLM model by finetuning each model"}, {"title": "2.1 MULTIAGENT DEBATE", "content": "Multiagent debate (Du et al., 2023) involves a series of $N$ language model agents\u2014either specific copies or finetuned versions of the same model\u2014each tasked with generating a response to a given problem. After the initial responses are generated, a debate round is initiated among the agents. In our paper, we concatenate and summarize the responses from other agents. Each agent is instructed to construct a new response based on its prior response and the summarized responses from the others. The final result is determined by majority vote based on the outputs from the last round of debate. The multiagent debate is illustrated in Figure 2."}, {"title": "2.2 FINETUNING MODELS ON GENERATED DATA", "content": "We start by considering how to use data generated by multiagent debate data to finetune a single LLM model for self-improvement. Given a set of natural language inputs $D_{task} = \\{x_i\\}$, we use a multiagent debate method (Du et al., 2023), specifically a debate with $N$ agents and $M$ rounds, to generate responses for each input in $D_{task}$. We obtain the final predicted output $\\hat{y}_i$ for each $x_i$ through majority voting in the last round of debate. We use this to construct a \"ground truth\" dataset of $\\{(x_i, \\hat{y}_i)\\}$. In the single LLM model setting, we then finetune the model on the set of generated responses $y_i$ which match $\\hat{y}_i$ given input $x_i$.\nWhile the final debate results $\\hat{y}_i$ are accurate, they often similar in style and methodology. As a result, repeatedly capturing a dataset of $\\{(x_i, \\hat{y}_i)\\}$ pairs for multiple rounds of finetuning often leads to a plateau of self-improvement performance."}, {"title": "2.3 FINETUNING MULTIPLE GENERATION AND CRITIC MODELS", "content": "Our goal in multiagent finetuning is to create datasets that construct a set of models representing different agents that are diverse and accurately solve problems. Instead of building a single dataset to finetune each model, we propose creating different datasets to finetune different models. A set of"}, {"title": "2.5 INFERENCE", "content": "At inference time, we have a set of finetuned generation models which represent generation agents $\\{\\hat{A}_1^G,..., \\hat{A}_N^G\\}$, and a set of finetuned critic models which represent critic agents $\\{\\hat{A}_1^C, ..., \\hat{A}_N^C\\}$. We conduct a multiagent debate among these agents, where each individual generation agent participates in the first round of the debate, followed by each individual critic agent in subsequent rounds. Each agent takes the responses from all other agents and generates a new response in each round of the debate. We found that summarizing the responses from the other agents helps eliminate redundant information while retaining the most important details, thereby further improving performance. The final result is determined by a majority vote based on the responses from the final round of the debate. We provide pseudocode in Algorithm 2."}, {"title": "3 EXPERIMENTS", "content": "We evaluate our method and baselines on three language reasoning tasks."}, {"title": "3.1 LANGUAGE REASONING TASKS", "content": "Arithmetic. consists of 1,000 generated arithmetic problems in the form $a + b - c + d - e \\cdot f$. Following the generation procedure in (Du et al., 2023), each variable is assigned a random value up to a maximum of 30.\nGrade School Math (GSM). (Cobbe et al., 2021) consists of math word problems that require multi-step mathematical reasoning. Each example includes a problem statement, the numerical answer, and an explanation of the answer.\nMATH. Hendrycks et al. (2021) consists of competition-level math problems categorized into five difficulty levels. For our experiments, we sample problems from the first three levels.\nFor each dataset, we randomly select 500 examples for finetuning the language model. Additionally, we select 500 held-out problems for evaluation. We parse the generated answers and evaluate their correctness by comparing them with the ground truth answers. Accuracy is reported based on how"}, {"title": "3.2 BASELINES", "content": "We compare the proposed method with various baselines. In all multiagent settings, we use three agents, and for all debate settings, we conduct two rounds of debates to ensure a fair comparison (additional results with five agents in Appendix Section F).\nBase utilizes a single language model to process input and generate responses.\nMajority is a multiagent baseline that selects responses based on a majority vote from multiple agents. If no response secures a majority, one of the potential answers is chosen at random.\nDebate is a multiagent debate baseline as described in Du et al. (2023). The debate structure is outlined in Figure 2.\nSTaR (Zelikman et al., 2022) iteratively finetunes the language agent using a dataset with ground truth answers for each problem. Initially, the LM generates an answer for each problem, and correct responses, as verified by the ground truth, are added to the finetuning dataset. For problems answered incorrectly, the LM is reprompted with a hint that includes the ground truth answer. Problems where the generated response includes the correct answer are added to the finetuning dataset. The LM is finetuned on the collected dataset. This iterative process of building the dataset and finetuning is repeated until the finetuning loss saturates. The final model is then used for evaluation.\nMajority FT is a baseline that incorporates both majority voting and finetuning. We prompt the language agents with each problem and conduct a majority vote on their results. We then compile the responses from all agents that align with the majority vote, along with the input, to create a finetuning dataset. The language model is finetuned using this dataset. Finally, we apply majority voting to the outputs of the finetuned model to determine the final answer."}, {"title": "3.3 QUANTITATIVE RESULTS", "content": "We compare baselines and our method, which was finetuned for only a single iteration ($L = 1$), in Table 1. The accuracy and standard error for each dataset are reported. We use three distinct base language models: three open-source models, Phi-3 4B (Abdin et al., 2024), Mistral 7B (Jiang et al., 2023), and LLaMA-3 8B (Dubey et al., 2024); and one proprietary model, GPT-3.5 (OpenAI, 2022).\nOur method outperforms all the baselines. Although \u201cSTaR\u201d utilizes ground truth labels for data selection and undergoes multiple iterations of finetuning, it still performs worse than our method, which uses only a single finetuning iteration without access to ground truth. The \"Majority\", \"Debate\" and \"STaR\" methods outperform the \u201cBase\u201d model, demonstrating that majority voting, multiagent debate, and finetuning all contribute to improved performance. \u201cMajority FT\u201d enhances the performance of \"Majority\" by incorporating a finetuning procedure. Our method is only finetuned on 500 examples and still shows significant improvement over the baselines, particularly on more challenging datasets such as GSM and MATH. Additional evaluations on a larger set of problems and datasets can be found in Appendix Section H."}, {"title": "3.4 MULTIPLE ITERATIONS OF FINETUNING", "content": "To verify the effectiveness of multiple iterations of finetuning, as described in Section 2.4, we present the performance of our proposed method \u201cMultiagent FT (Ours)\u201d over five iterations of finetuning in Figure 1. We tested this method on two open-source models, Mistral and Phi-3, using the MATH dataset. The results demonstrate that \"Multiagent FT (Ours)\" consistently improves performance over time. For example, the accuracy of Phi-3 increased from 58.8% to 66.0%, and the accuracy of Mistral improved from 22.5% to 28.2%. Our method with five rounds of finetuning is 12.6% and 9.31% more accurate than the best baseline listed in Table 1 using Phi-3 and Mistral, respectively.\nIn contrast, finetuning a single agent (\"Single-agent FT\"), as described in Section 2.2, shows that performance saturates after one iteration of finetuning and starts dropping afterward, indicating potential overfitting to generated responses. This issue occurs when the single model, after several finetuning cycles, becomes fixated on a small range of responses, which limits its diversity and"}, {"title": "4 ANALYSIS", "content": "In this section, we aim to answer the following questions: 1) How important is the proposed multiagent finetuning procedure? 2) Will it increase response diversity? 3) Can the finetuned agent generalize to other datasets in a zero-shot setting?"}, {"title": "4.1 ABLATION STUDIES", "content": "We examine each component of the proposed method, as shown in Table 2. Multiagent FT (Ours) refers to our proposed method with a single round of finetuning, $L = 1$.\nMultiagent FT w/o summary removes the summarization step from the multiagent debate. Instead of summarizing, the responses from other agents are directly concatenated and presented to each agent. Summarization helps by eliminating redundant information and retaining the most critical points; therefore, omitting the summarization step can negatively impact performance."}, {"title": "4.2 AGENT RESPONSE DIVERSITY", "content": "By finetuning multiple agents with distinct roles, our approach enables us to obtain more diverse responses across rounds of finetuning compared to a single agent. Figure 3 illustrates the diversity of generations from our method and single-agent across rounds of finetuning using two metrics of diversity. We cover one metric of diversity, negative log-likelihood, here and cover the other in Section C.4.\nIn our first diversity metric, we aim to characterize specialization by tracking the likelihood of responses of other agents using likelihood calculations of a specific agent. If we are increasing diversity, then the log-likelihood of responses from other agents will decrease across iterations of finetuning. The reasoning used by other agents would be considered less common for the specific agent, indicating a divergence in responses. If accuracy increases while likelihood of responses from other agents decreases, this indicates must specialization.\nWe evaluate the negative log-likelilhood (NLL) of responses from other critic agents using another held-out critic agent and plot this over iterations of finetuning. We do the same with Single-Agent FT, using responses from other agents and evaluate likelihood using a held-out agent. Larger NLL values indicate that the model has assigned low likelihood to a sequence and lower NLL values indicate that the model has assigned higher likelihood to a sequence. We measure this over iterations of finetuning for our method as well as Single-Agent FT."}, {"title": "4.3 ZERO-SHOT GENERALIZATION", "content": "We investigate the zero-shot generalization of the proposed method across different datasets. Specifically, we use generation and critic agents finetuned on the MATH dataset and evaluate their performance on 100 randomly sampled examples from the GSM dataset. We compare our method to baseline methods used in Table 1. These baselines are trained on the GSM dataset. All methods use Mistral as the base LLM. Figure 5 shows that our method surpasses all the baseline methods, even though it has never seen data from the GSM dataset, indicating the strong zero-shot generalization capability of the proposed method. We show further results in Section H.3."}, {"title": "5 RELATED WORK", "content": "Finetuning methods generally fall into three categories: human-in-the-loop, distillation, and self-improvement. We briefly cover the first two categories and spend more time on self-improvement, which is more related to our work.\nFinetuning with human-in-the-loop and distillation: Several human-in-the-loop methods have been introduced for finetuning, most noticeably RLHF (Christiano et al., 2017; Sun et al., 2023) and"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "Limitations. In comparison to existing works in single model finetuning, multiagent finetuning is substantially more expensive at both training and inference time as multiple copies of a model need to be trained and run. To run multiagent finetuning experiments on open source models, we used either four H100 GPUs or four A100 GPUs. Models took between 120GB - 240GB of GPU memory and inference took between 12-24 hours across multiple GPUs. To improve the training time of multiagent models, it may be interesting to instead share weights across different instances of models. To improve inference time in multiagent models, we can directly distill the debate procedure into a single modelor use quantization as part of finetuning.\nConclusion. In this paper, we have introduced a novel multiagent finetuning framework that significantly enhances the performance and diversity of language models. By employing a society of agents with distinct roles, our method effectively improves the feedback mechanism and overall output quality, mitigating the limitations inherent in single-agent self-improvement methods. This system allows for autonomous self-improvement through iterative finetuning, leading to substantial performance gains across a comprehensive suite of reasoning tasks. Importantly, our approach is versatile and can be applied to both open-source and proprietary LLMs, ensuring broad utility and impact. Additionally, our method can be integrated with other finetuning approaches such that incorporate human feedback such as RLHF or DPO, which we leave to future work. This work opens new avenues for future research in language model enhancement and sets a foundation for further advancements in the field."}, {"title": "A APPENDIX SUMMARY", "content": "We add additional details for our methods and experiments as well as additional results to provide more evidence of improvements with multiagent finetuning. In Section B, we provide additional details on summarization, inference and training details using multiagent finetuning with debate. In Section C, we cover additional metrics for measuring diversity in agent responses based (1) consensus and (2) KL-divergence (3) likelihood. Both metrics show that diversity is maintained or increases while accuracy increase over rounds of finetuning. In Section D, we introduce a cooperative approach for composing agent responses rather than a competitive approach through multiagent debate. We apply multiagent finetuning with the cooperative approach to analyze whether our method is agnostic to the approach style. We find strong similar improvements when our method is applied to a cooperative approach. In Section E, we include an additional baseline based on Single Agent FT where we increase the sampling temperature applied across all agents. This is a proxy for increasing diversity that is complementary to our method. We find that multiagent finetuning significantly outperforms methods that modify temperature to artificially induce diversity. In Section F, we add an additional experiment where we apply multiagent finetuning to responses across 5 agents instead of 3. We see significant improvements in performance when using additional agents. In Section G, we present a simple mathematical model illustrating how multiagent finetuning can improve diversity. Finally, in Section H, we present additional evaluations of multiagent finetuning across a wide suite of datasets."}, {"title": "B METHODOLOGY DETAILS", "content": ""}, {"title": "B.1 SUMMARIZATION DETAILS", "content": "As done in Du et al. (2023), we incorporate summarization into the multiagent debate procedure. In summarization, we have an LLM agent take responses from other agents as input and summarize the answers to the responses. During round $m$ of debate, we introduce a summarization agent $A_S$ which takes responses from the other $N-1$ agents in the last round, $(y_{m-1, 1},...,y_{m-1, n-1}, y_{m-1, n+1}, ..., y_{m-1, N})$ and generates a summary of the responses $x_{m,n}$. This summary is sent to the critic agent $A_n^c$ to generate a new response."}, {"title": "B.2 INFERENCE DETAILS", "content": "The pseudocode of our method for inference is shown below."}, {"title": "B.3 EXPERIMENTAL DETAILS", "content": "For all open-source models, we perform finetuning using a total of eight 40GB A100 GPUs and four 80GB H100 GPUs. The evaluation of individual inference times for multi-agent finetuning with open-source models took approximately 30 to 36 hours.\nPhi-3 We ran our results using Phi-3-Mini-128K-Instruct which has 4 billion tunable parameters. We finetune the entire model end-to-end (no LoRA or memory adaptation) on two 40GB A100 GPUs or one 80GB H100 GPU and run a total of two epochs of finetuning for generation agents and one epoch of finetuning for critic agents. We use a batch size of 1 and a learning rate of 5e-6 for generation agents and 5e-7 for critic agents. When applying multiple iterations of finetuning, we use a learning rate of 5e-7 across both generation and critic agents. Models are finetuned with a fixed training set of 500 randomly selected questions (where we do not provide answer annotations for the questions) and then evaluated on a separate test set of 500 randomly selected questions.\nMistral We ran our results using Mistral-7B-Instruct-v0.2, which has 7 billion tunable parameters. We finetune the entire model end-to-end (no LoRA or memory adaptation) on four 40GB A100 GPUs or two 80GB H100 GPUs and run a total of two epochs of finetuning. We use a batch size of 1 and a learning rate of 2e-6 for generation agents and 1e-6 for critic agents. When applying multiple iterations of finetuning, we use a learning rate of 1e-6 across both generation and critic agents. Models are finetuned with a fixed training set of 500 randomly selected questions (where we do not provide answer annotations for the questions) and then evaluated on a separate test set of 500 randomly selected questions.\nLLaMA-3 We ran our using Meta-Llama-3-8B-Instruct, which has 8 billion tunable parameters. We finetune the entire model end-to-end (no LoRA or memory adaptation) on three 80GB H100 GPUs and run a total of two epochs of finetuning. We use a batch size of 1 and a learning rate of 5e-7 for generation agents and 2e-7 for critic agents. When applying multiple iterations of finetuning, we use a learning rate of 5e-7 across both generation and critic agents. Models are finetuned with a fixed training set of 500 randomly selected questions (where we do not provide answer annotations for the questions) and then evaluated on a separate test set of 500 randomly selected questions.\nGPT-3.5 We ran our results on the gpt-3.5-turbo-0613 model. We use the finetuning API and run a total of two epochs of finetuning, using a batch size of 1 and a learning rate multiplier of 1. Models are finetuned with a fixed training set of 500 randomly selected questions (where we do not provide answer annotations for the questions) and then evaluated on a separate test set of 500 randomly selected questions."}, {"title": "C DIVERSITY METRICS", "content": "We cover different metrics for measuring diversity for both Phi-3 and Mistral to provide an overview of the diversity of our method in comparison to Single Agent FT."}, {"title": "C.1 CONSENSUS", "content": "We further analyze the diversity of responses from our method to show that diversity is preserved. Rather than using text embeddings, we further measure the consensus among agents as a more interpretable alternative. This is measured as the proportion of agents that have the same final answer in a given round of debate. We take an average of this proportion across all 500 problems used for evaluation. To obtain the mean consensus of our single agent finetuning baseline, we prompt the single-agent finetuned model 3 times, take a majority vote over generated answers, and find the proportion of agents that had a generated answer that was the majority vote. In order to convert this to diversity, we take the difference of the mean consensus value from 1, which represents the average number of agents with a different response from the consensus answer.\nWe measure diversity as the inverse of consensus. Specifically, we consider the agent responses in the final round of debate $\\{y_{M,1},..., y_{M,N}\\}$ that match the majority-voted final response $\\hat{y}$. The consensus is computed as the percentage of responses in $\\{y_{M,1},..., y_{M,N}\\}$ that match $\\hat{y}$:\n$Consensus = \\frac{\\sum_{n=1}^N \\mathbb{I}(y_{M,n} = \\hat{y})}{N}$,"}, {"title": "C.2 KL-DIVERGENCE", "content": "We next measure diversity by computing KL divergence between the probability distributions computed based on the final answers from different agents. We estimate the probability distribution of each agent's response using the likelihoods from Gemma-2 (2B) For each test example, we compute the KL divergence between the responses of any two agents and then average the values from all pairs of agents to determine the overall KL divergence.\nWe see results in Figure 7. Specifically, we see that diversity is preserved using our method whereby KL-divergence is consistently higher than the single-agent finetuning baseline."}, {"title": "C.3 KL-DIVERGENCE ACROSS MODELS", "content": "We further analyze diversity by comparing the KL-divergence of generation and critic agents with the likelihood of responses from the base LLM model across iterations of finetuning."}, {"title": "C.4 EMBEDDING DISSIMILARITY", "content": "Finally, we analyze diversity by measuring the embedding dissimilarity between responses of different agents.\nSpecifically, we consider agent responses in the final round of debate $\\{y_{M,1},..., y_{M,N}\\}$ that match the majority-voted final majority-voted final response $\\hat{y}$. For each response, we obtain pretrained contextual word embeddings from a held-out language model, in this case the T5-3B encoder model (Raffel et al., 2020).\nWe feed each agent response to the T5 encoder model to obtain word embeddings and extract the embedding associated with the classification token [CLS]. As done in prior work, we use this embedding as a representation of the sequence. We compare the similarity of the agent responses"}, {"title": "D COOPERATIVE FINETUNING", "content": "In this paper, our method mainly builds on a competitive approach for composing agent responses with multiagent debate. Our approach for multiagent finetuning can be applied to both the competitive setting, where critic agents provide feedback to generator agents, and cooperative settings, where agents work together in a \"mixture of experts\u201d style to generate answers. Instead of prompting agents to critique responses from other agents, in the second round of conversation, we prompt agents to cooperate with other agents. We ask each agent to generate a new response by merging their own response with the responses of other agents, using the prompt \u201cCan you derive a new solution by combining your solution with the solutions of other agents?\u201d. Under this cooperative setting, the proposed multi-agent finetuning improves the performance, as demonstrated by Cooperative (FT) outperforming Cooperative (Base).\nWe show results in Table 3. More specifically, we see that we can finetune with a cooperative method with multiagent finetuning and achieve similar improvements in performance. This demonstrates that our method can be applied to other multiagent prompt settings as a general finetuning method for LLMs."}, {"title": "E ADDITIONAL COMPARISONS", "content": "We compare our approach to two additional approaches to improve the diversity of reasoning chains."}, {"title": "E.1 MODULATING TEMPERATURES", "content": "We first consider inducing diverse responses from LLM agents by increasing the temperature of generation. We add an additional baseline where we vary the temperature of agents finetuned using Single Agent-FT. Higher temperature values may be a proxy for more diverse responses. We show results over rounds of finetuning in Figure 10."}, {"title": "E.2 UNIQUE ID FOR AGENTS", "content": "We next considering an additional comparison to multiagent finetuning that can preserve diversity while reducing the cost of finetuning. The method involves using a unique identifier as part of the prompt fed to each agent. We feed each generation agent an ID given by GEN1, GEN2, etc. Similarly, each critic agent is given an ID CRIT1, CRIT2, etc. Additionally, we provide a short description to the agent, explaining what the ID refers to. For generation agents, we state that the agent is tasked with creating a solution. For critic agents, we state that the agent is tasked with evaluating and improving responses. The ID is presented to the agent at the beginning of each prompt, marked by the string Agent ID: GEN1 (This is a generation agent tasked with creating a solution.) as an example of the ID fed to generation agent 1.\nWe compare the unique ID approach on the same 500 MATH examples reported in Table 1. Results are shown in Table 5. We find that multiagent finetuning performs significantly better and that using unique IDs is fairly similar to debate. This demonstrates that mechanisms for generating solutions and critiquing them is unlocked via finetuning."}, {"title": "F ADDITIONAL AGENTS IN DEBATE", "content": "In Table 4, we show the influence additional agents with finetuning. We use 5 agents and 2 rounds of debate. We find that additional agents improves results as noted in prior work (Du et al., 2023) over 3 agents, 2 rounds of debate. This also implies that our method will scale with larger number of finetuned agents."}, {"title": "G MATHEMATICAL MODEL OF DIVERSITY OVER ROUNDS OF FINETUNING", "content": "We consider a simple mathematical model illustrating how diversity can arise by finetuning models only on answers that they are accurate on. Consider a training dataset of problems in three topics, A, B, and C as well as three models we train all initialized from the same base model. For each model, we assign a specialization skill score $S_A, S_B, S_C$ between 0 and 1, representing how accurate the model is at answering questions in the specified topic. All three models are initialized to have a skill of 0.33 on each topic. The specialization $S_i$ for each topic $i$ corresponds to the percentage of"}, {"title": "H ADDITIONAL EVALUATIONS", "content": ""}, {"title": "H.1 LARGER MATH EVALUATION", "content": "To further evaluate multiagent finetuning, we evaluate on the MATH dataset across all 5 levels of difficulty, instead of selecting examples from levels 1-3. We extract 500 examples for training and 500 examples for testing and evaluate on LLaMA-3.\nWe show results across all baselines in Table 6 and results across multiple rounds of finetuning in Figure 11. We see consistent improvement using LLaMA-3."}, {"title": "H.2 MMLU", "content": "We add an additional comparison with MMLU to further establish thte improvement of our method on a task related to general factuality and reasoning instead of mathematics."}]}