{"title": "Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity", "authors": ["Dongyue Li", "Aneesh Sharma", "Hongyang R. Zhang"], "abstract": "Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data from various task combinations, which is computationally intensive. We present a new algorithm GRAD-TAG that can estimate task affinities without this repeated training.\n\nThe key idea of GRAD-TAG is to train a \"base\" model for all tasks and then use a linearization technique to estimate the loss of the model for a specific task combination. The linearization works by computing a gradient-based approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression to predict labels for the task combination. We show that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity, we design a semi-definite program for clustering similar tasks by maximizing the average density of clusters.\n\nWe evaluate GRAD-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our task affinity estimates are within 2.7% distance to the true affinities while needing only 3% of FLOPs in full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates within 5% distance to the true affinities, using only 112 GPU hours. Our results show that GRAD-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern applications of neural networks often employ a single neural network for prediction or classification on multiple tasks. This multitask learning setup is widely used across a variety of settings, with examples such as a visual system that aims to detect various objects in autonomous driving simultaneously [50], a Graph Neural Network for community detection on large networks [29], and prompt-tuning of pre-trained LLMs for NLP tasks [34]. This multitask learning setup is not only computationally efficient (a single network can jointly predict many tasks), but it often improves prediction accuracy due to transfer learning.\n\nThe often implicit assumption behind multitask modeling is that there is a positive transfer effect among tasks [8]. However, as the number of tasks increases, one frequently observes a negative transfer effect in many applications, such as for prompt tuning of large language models, where adding a task to the model degrades performance on one or more tasks [58-60, 54]. This observation has motivated a line of work that aims to group the tasks into subsets such that negative transfer among tasks within a subset is minimized, allowing one to train a separate multitask model per subset, thereby improving performance on all tasks [29].\n\nA key concept underlying many multitask learning algorithms is a notion of task affinity, which can capture the abovementioned positive or negative transfer effects across tasks in a precise way. For instance, one can compare pairwise task affinity [50, 14]-the loss of a model trained on each pair of tasks-against the loss of a model trained on each task. Given a notion of task affinity, a common recipe for designing multitask learning algorithms involves (1) Task affinity computation that builds a task affinity matrix, then (2) task grouping that uses this task affinity matrix to group tasks with positive transfers together, and finally (3) multitask training that fits a separate model per task group.\n\nThe performance improvement achieved through this paradigm depends on the notion of task affinity and the grouping procedure. Moreover, the ability to leverage this paradigm hinges on the computation of task affinity (Step 1 above), which becomes expensive as the number of tasks grows. As a case in point, the computational complexity of pairwise task affinity scales quadratically with the number of tasks: this implies that even for community detection with 100 labelings, using pairwise task affinity requires training nearly 5000 models for computing the affinity matrix."}, {"title": "1.1 Related Work", "content": "Multitask learning is a fundamental problem with many applications, such as federated learning [49], road safety modeling [41], and language model fine-tuning [34]. This problem has been studied since the early literature of data mining [8]. As the number of tasks increases, modeling task relationships becomes increasingly complex and challenging [36, 67]. These relationships are influenced by data distribution characteristics, including covariate and label shifts [59]. Thus, designing optimization algorithms for multitask learning is challenging [29, 30]. We contribute to this literature by proposing a new approach to significantly speed up the computation of task affinity scores for modeling task relationships. We now proceed to discuss several lines of work most related to ours.\n\nTask Similarity Measures. Previous works [50, 14] estimate task affinities between every pair of tasks. The computation complexity of such methods scales quadratically with the number of tasks. Another approach is to use task embeddings [54], i.e., training one model on each task and measuring the cosine similarity between the model weights. Although this approach scales linearly with the number of tasks, the measures tend to be noisy. Intuitively, if two tasks are similar, their gradients should exhibit higher cosine similarity. This idea can be implemented to balance training by dynamically tuning gradient magnitudes [12], or to project the gradients noto the span of other tasks' gradients that have a conflicting gradient [14]. The same idea can also be implemented to choose auxiliary tasks that are most beneficial for a primary task [13]. Similarity measures based on feature representations of tasks have also been applied to grouping tasks [48] and used to predict task transferabilities [4]. The main advantage of these approaches is their efficiency, as only a single multitask model needs to be trained. The downside is that the gradients can be noisy during a stochastic training procedure. For example, Azorin et al. [5] empirically observed that representation and gradient similarity measures do not consistently correlate with actual MTL performance. Thus, a more accurate approach is to build measures that approximate multitask outcomes directly; see recent work on designing surrogate models for multitask learning systems [29, 30].\n\nTransferability Estimation. There have also been developments on information theoretic measures of transferability in recent literature. One natural idea is to evaluate conditional entropy between target pseudo labels (assigned by a pretrained source model) and real target label [7]. Log Expected Empirical Predictor [40] proposes a modified procedure by using soft predictions from the source model. These methods do not utilize feature embeddings in the measure [55]; TransRate [21] introduces a surrogate measure based on mutual information that also incorporates feature embeddings. An improved estimation method with better robustness can be achieved by shrinkage [22]. In the fine-tuning setting, the distance between the model search and the pretrained initialization can indicate the level of generalization capability [31]. The geometry relates to the Hessian of the loss, which has been shown to correlate with the generalization performance of fine-tuned models [26]. Ju et al. [25] extend this Hessian measure to graph neural networks, which can guide the design of optimization algorithms to regularize the Hessian of neural networks [27].\n\nMultitask Learning Optimization Algorithms. Multitask learning can be viewed as a multiobjective optimization problem [42], where the goal is to identify the Pareto frontier among multiple objectives [47]. One common MTL optimization algorithm is to reweight task losses and optimize a weighted combination of task losses [32, 46]. Our goal is to maximize the averaged prediction performance of all tasks. Thus, we are interested in partitioning the tasks into similar groups, where tasks are closely related within each group and can differ significantly across groups. Another interesting line of work is designing branching neural networks such as tree structures [53, 17], where each layer contains multiple modules to handle different tasks [35]. Compared with branching methods, task grouping may be more suitable for handling a large number of tasks (like hundreds to thousands). In this regime, negative interference between tasks is almost unavoidable, and clustering tasks into similar groups could provide a more efficient strategy than designing a single neural network that handles all tasks.\n\nInfluence Functions. There is a line of work on estimating the influence of adding or removing one sample on the whole dataset. Influence functions [28] based on efficient approximation of the Hessian inverse provide one way to approximate this. Random sampling-based approaches to measuring leave-one-out influence have also been studied [23, 43]. The distinction between these works and us is we focus on task-level affinity, whereas this literature focuses on estimating the influence of a single data sample.\n\nClustering Algorithms. Clustering is a fundamental aspect of machine learning. Besides SDP relaxations, linear programming relaxations are known for clustering objectives such as k-center. The integrality gap of linear programming and semidefinite programming relaxations can be analyzed when there is a separation structure in the underlying clusters [3]. These approximation guarantees typically require the underlying similarity scores to satisfy a metric condition. By contrast, the task affinity matrix can easily violate the triangle inequality. Recent work has also studied mixed integer programming for best subset selection [9]. One novel contribution of this work is to make explicit a connection between multi-instruction fine-tuning and clustering. In light of this connection, it would also be interesting to revisit hierarchical clustering and hypergraph clustering for task grouping. For example, recent work by Tsitsulin et al. [52] investigates unsupervised graph clustering problems with graph neural networks."}, {"title": "2 PRELIMINARIES", "content": "Suppose we are interested in making predictions on n tasks. We are given a set of samples for training and testing of each task. Our goal is to design a prediction algorithm to maximize the averaged testing performance over all the n tasks simultaneously. We assume that the samples from all the tasks are supported on a joint product between a p-dimensional feature space X and a label space Y. In order to precisely discuss task relationships, we formally define what we mean by a multitask learning algorithm.\n\nDefinition 2.1 (Multitask learning algorithms). For any subset $S \\subset {1, 2, ..., n}$, a multitask learning algorithm $f$ takes the training data of all the tasks in $S$ and combines them in a joint training procedure. Then, the (jointly trained) model is tested on each task $t \\in S$. In the end, a test result is obtained for each $t$. Let us denote the test result as $f(S, t)$. Thus, the output of the algorithm will include a total of $|S|$ results for any subset $S$, one for each $t \\in S$.\n\nGiven a multitask learning algorithm, the transfer between the n tasks can then be viewed through the results of $f$, applied to combinations of tasks as subsets. This notion of transfer underlies many existing multitask learning systems. We give two examples below, which have been used in prior works to tackle task transfer in complex visual systems [64, 50].\n\nExample 2.2 (Pairwise task affinity). Consider two tasks such as i and j. Given a multitask learning algorithm f, one can mix the training data of tasks i, j, using SGD to train a shared encoder"}, {"title": "3 TASK AFFINITY ESTIMATION", "content": "We now describe a new method for estimating task affinity scores. To circumvent the cost of full-model training, we start by describing an empirical observation regarding pre-training and fine-tuning. Then, we present our approach to estimating fine-tuned model parameters for task subsets. Additionally, we use random projection to reduce the dimension of the gradients. We provide an error analysis to justify the design of our algorithm."}, {"title": "3.1 Linearization of Fine-tuned Models", "content": "Our method is motivated by the fact that once we pre-train all the n tasks to obtain a meta-initialization, this initialization can provide representations that can be quickly adapted to the remaining tasks. This is based on the premise that the underlying tasks share structural similarities in multitask learning. As the model fine-tuned to a subset of tasks stays in the affinity of the initiation, the fine-tuning procedure behaves like linear models locally.\n\nTo illustrate this observation, we consider three distinct scenarios involving graph neural networks (GNNs) and transformers (BERT and T5). We test GNNs on a multi-label prediction dataset on a YouTube graph [61], using a 3-layer SIGN network [15]. This dataset"}, {"title": "3.2 Gradient-based Estimation", "content": "We now describe our algorithm, which builds on the above linearization property, by using logistic regression with gradients as features. It also includes a dimension reduction, as described below.\n\n(1) Estimating fine-tuned model parameters: In the following discussion, we focus on binary classification, such that $Y_i \\in {+1,-1}$. See Remark 3.2 for extensions to multiple classification and regression. Recall the gradient-based approximation of $f_W(x_i, y_i)$, given the input $(x_i, Y_i)$:\n\n$\\nabla_w f_{0^*}(g_i, Y_i) (W - 0^*) + f_{0^*}(x_i, Y_i)$\n\nLet us denote $\\nabla_w f_{0^*}(x_i, y_i)$ as $g_i$ and $-y_i f_{0^*}(x_i, y_i)$ as $b_i$, for any i. Using logistic loss, we can write down the loss function as\n\n$l_W(g_i, y_i) = log (1 + exp (-y_i g_i^T (W - 0^*) + b_i))$,\n\nfor $W \\in \\mathbb{R}^d$. Denote the combined data set in the task subset S as\n\n$D_s = {(x_1, y_1), ..., (x_{n_s}, y_{n_s})}$,\n\nwhere $n_s$ is the combined number of data samples in the set $D_s$. The main idea is to solve a logistic regression problem with $g_i$ being the feature vector and $y_i$ being the response label. However, keep in mind that the dimension of $g_i$ is the same as the number of parameters in a neural network, which could be tens of millions. Thus, we introduce a dimension reduction procedure that does not lose much precision.\n\n(2) Dimension reduction: We use the Johnson-Lindenstrauss random projection [24], which projects the gradients to a much lower dimension before solving the logistic regression. Let P be a p by d Gaussian random matrix, whose entries are independently sampled from a Gaussian $N(0, d^{-1})$. We project the gradient from dimension p onto dimension d as $\\tilde{g_i} = P^T g_i$. Then, we solve the following logistic regression, which is now in dimension d:\n\n$W \\overset{argmin}{W \\in \\mathbb{R}^d} \\hat{l}(W) = \\frac{1}{n_s} \\sum_{i=1}^{n_s} l_w (\\tilde{g_i}, Y_i)$.\n\nLastly, we set $W_s$ as $PW + 0^*$ to map the projected solution back to the p-dimensional space. $W_s$ is the estimated model parameter for fine-tuning $0^*$ with task subset S.\n\n(3) Averaging over an ensemble: To reduce the above estima- tion's variance, we also add a model averaging step. In particular, we train several meta-initializations and repeat the above estimation procedure. We average the estimated scores within the ensemble.\n\nWe summarize the entire procedure in Algorithm 1 with all three steps. Let us compare the running time complexity between this estimation and one that uses full training to get $f(S_i, j)$ instead:\n\n*   In our estimation, we need M full training, plus O(n) gradient evaluations and solving logistic regression m times.\n*   If we were to compute f, we need m full model training instead.\n\nTypically, M = O(1), while $m = \\Omega(n)$ or even O($n^2$) in downstream use cases. Thus, our estimation algorithm reduces $\\Omega(n)$ full-model training to only O(1). The tradeoff is that we require O(n) gradient evaluations (to retrieve the gradients on all tasks) plus solving logistic regression m times. As we will show below, the random projection helps reduce the dimension of the logistic regression problem to O(logp) dimension, which is much cheaper. This is in"}, {"title": "3.3 Error Bounds", "content": "We now show that the error introduced by approximations in GRAD- TAE is bounded. Specifically, we use the Johnson-Lindenstrauss Lemma to argue that as d increases, the random projection yields a minimizer whose quality is not much worse than the solution without the projection. We will assume that the averaged Taylor's expansion error is at most $\\delta$ across the entire data set of every task. Additionally, we assume that the search procedure occurs within a bounded space of radius D. Lastly, in the pretrained initialization, each gradient vector's Euclidean norm is at most G. With these conditions, we state the error bounds for GRAD-TAE as follows.\n\nPROPOSITION 3.3. Let D be a search space whose radius is at most D. Suppose the gradient of $f_0$ at the initialization $0^*$ in the training set is at most G in Euclidean norm. For each task $i = 1, 2, ..., n$, let $T_i$ denote the training data. Suppose that for every i,\n\n$\\frac{1}{T_i} \\sum_{(x,y) \\in T_i} |f_w(x, y) - f_{0^*}(x, y) - \\nabla_w f_{0^*}(x, y) (W - 0^*)| \\leq \\delta$.\n\nProvided that $d = O(\\frac{1}{e^2}log p)$, the training loss of $W_s$ is bounded away from the minimum training loss for any $S \\subset {1, 2, ..., n}$ as\n\n$\\hat{l}(W_s) \\leq min_{W \\in D} l(W) + 2\\delta + 4GD\\epsilon$."}, {"title": "4 TASK AFFINITY BASED GROUPING", "content": "We now describe a clustering algorithm to partition the n tasks into k disjoint subsets. Given an n by n task affinity matrix T, we will find a clustering that maximizes the average density of all clusters. Concretely, let $C_1,..., C_k$ be a disjoint partition of [n]. Let $v_1,..., v_k$ be a 0-1 vector indicating whether a task is in one cluster or not. The average density of this clustering can be written as:\n\n$\\frac{1}{k}\\sum_{i=1}^k v_i^T T v_i$.\n\nThis integral objective is NP-hard to optimize in general (in particular, geometric clustering is a special case [2]).\n\nWe design a Semi-Definite Programming (SDP) relaxation and then round the SDP solution to a clustering. Let us denote the assignment variables as an n\\times k matrix V, such that each entry $V_{i,j}$ indicates whether a task i belongs to a cluster j, for every $i = 1, ..., n, j = 1, ..., k$. Moreover, let the jth column of V, which is the characteristic vector of the j-th cluster, be denoted as $v_j$. Under this assignment, the sum of $V_{i,j}$ across any task i must be one, as we allow one task to be assigned in a single group. By contrast, the sum of $V_{i,j}$ across $C_j$ is the number of tasks assigned to $C_j$, which is at least one.\n\nLet e denote the all-ones vector. We state an integer program to maximize the average density of all k clusters as follows\n\n$\\underset{V \\in \\mathbb{R}^{n \\times k}}{max} \\frac{1}{k} \\sum_{j=1}^k v_j^T T v_j$\\n\n$\\textrm{Ve} = e, \\textrm{Vi,j} \\geq 1 \\textrm{for 1 \\leq j \\leq k} \\\\ V_{i,j} \\in {0, 1}, \\textrm{for any 1 \\leq i \\leq n, 1 \\leq j \\leq k$.\n\nNote that $v_i^T v_i$ is a rank-one semidefinite matrix. Let us denote the sum of them (normalized by $v_i^T v_i$) as the following new variable\n\n$X = \\sum_i^k \\frac{v_i v_i^T}{v_i^T v_i}$.\n\nX has rank k since it is the sum of k rank-1 matrices, and the $v_i$'s are orthogonal to each other. Additionally, its trace is equal to k because the trace of $\\frac{v_i v_i^T}{v_i^T v_i}$ is one for any j. Second, one can verify that the entries of every row of X sum up to one. Removing the 0-1 integer constraint, we derive a rank-constrained problem as\n\n$\\underset{X \\in \\mathbb{R}^{n \\times n}}{max} <T, X>$\n\n$\\textrm{Xe} = e, \\textrm{Tr}[X] = k, rank(X) = k$\n\n$X \\geq 0, X \\succeq 0$.\n\nFurther relaxing the rank constraint (while keeping the trace constraint) leads to a convex program, which can be solved efficiently. Given a solution of the SDP, denoted as X, the last step is to round X into an integer solution. We set a threshold $\\lambda$ such that if $X_{u,v} \\geq \\lambda$, tasks u and v are assigned to the same cluster. In the experiments, we set $\\lambda$ as c/n for a constant c \u2265 1, since $X_{u,v}$ should be $\\frac{c_i}{n}$ when they are in the same cluster with $|C_i| < n$. Thus, the intra-cluster distance must always be at least $\\lambda$ with the assignment.\n\nWe provide the entire procedure in Algorithm 2, which uses Algorithm 1 as a subroutine to estimate the task affinity scores.\n\nExample 4.1 (Discussion about alternative clustering algorithms). A natural question is using alternative algorithms such as spectral clustering or Lloyd's clustering. We find that these algorithms are not as robust as the SDP relaxation because the scale of the loss values varies across rows for different tasks. We describe a toy example to illustrate. Suppose T is a 6 by 6 matrix involving three clusters $C_1, C_2, C_3$ of size 2 each. The affinity in $C_1$ is 7, while the affinity scores in $C_2$ and $C_3$ are 20, 19, respectively. We find that both spectral clustering and Lloyd's clustering will group $C_2$ and $C_3$ together, while the SDP relaxation manages to separate them apart. See Figure 2 for an illustration. For this reason, we use the SDP relaxation in GRAD-TAG.\n\nRemark 4.2 (Approximation ratio of the SDP relaxation). A natural question is whether one can quantify the approximation ratio of the SDP relaxation (10). Although this is a well-studied problem in approximation algorithms [1], task affinity violates the metric condition typically required in order to obtain guarantees in this literature. In particular, the triangle inequality $T_{i,j} + T_{j,k} \\geq T_{i,k}$ is violated. It is possible that by making an assumption regarding intra-cluster separation (see, e.g., Awasthi et al. [3]), one might be able to analyze the SDP theoretically. This is left for future work.\n\nRemark 4.3 (Further variants of GRAD-TAG). While we focus on the task grouping problem, the idea can be used to speed up forward and backward selection. We set the list of subsets in Algorithm 1 as {1}, {2},..., {n}. Suppose we select task 3. Then, in the next round, we set the list of subsets as {3, 1}, {3, 2, }, . . ., {3, n}. And so on."}, {"title": "5 EXPERIMENTS", "content": "We now validate GRAD-TAE and GRAD-TAG across various settings. The evaluation focuses on the following key questions. Does the estimation procedure accurately approximate the target task affinity scores? How does the running time compare to the full computation required to obtain these scores? Third, do the estimated affinity scores combined with the clustering algorithm work well in downstream use cases?\n\nOur experiments show that GRAD-TAE approximates the true task affinities (based on full model training) within a relative error of less than 2.7%, while using less than 3% of the computational cost of full training. Further, GRAD-TAG achieves comparable downstream accuracy to existing methods in two canonical applications, multi-label classification on graphs and language model fine-tuning, while using 32.8\\times fewer FLOPs. Lastly, we discuss the parameters and the steps as part of our algorithm, including the comparison with alternative clustering."}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Evaluation settings. We note that our algorithm applies to a wide range of multitask learning scenarios. For a representative evaluation, we focus on multi-label prediction on graphs, and language model fine-tuning. In the first setting, each labeling task corresponds to a subgraph within a graph. Given a seed set of each labeling as the training set, the goal is to identify the remaining nodes of the subgraph. This can be cast as multitask learning, by viewing each labeling as a binary classification task. The objective is to optimize the average accuracy of all the labeling tasks.\n\nThe second setting involves fine-tuning language models using human-designed instructions, known as instruction fine-tuning. Each instruction corresponds to a prompt. Typically, a data set can come up with many relevant instructions, some of which are more relevant to a subset of tasks than others [34]. Thus, a natural question is to select the instructions that are more relevant to the downstream task, which can be formulated using multitask learning. In particular, we view each instruction tuning as a single task. While we focus on these two applications, it is conceivable that our algorithm can be used in other related applications."}, {"title": "5.1.2 Datasets and models.", "content": "We use social network datasets with community labels for multi-label prediction on graphs. We select four graphs from SNAP [61] (Amazon, YouTube, DBLP, and LiveJournal), while we expect similar results to hold on other graphs. The number of nodes in these four graphs ranges from 3k to 57k; the number of edges ranges from 20k to 1M. For each graph, we pick 100 (largest) communities corresponding to $n = 100$ tasks. For preprocessing, we randomly sample 10% of nodes from each community subgraph as positive training samples and 10% of nodes outside the subgraph as negative samples. From the remaining data, 20% is randomly sampled for validation. We evaluate performance using the macro $F_1$-score on the test set [62].\n\nNext, we examine the running time scaling of our algorithm on a large graph (the Orkut network), which has 395k nodes, 21M edges, and a total of 500 communities. We use a 3-layer SIGN model [15] with a fixed width of 256 as the encoder in the MTL models, which is more efficient to train than GCN.\n\nFor fine-tuning language models, we use two text classification datasets from SuperGLUE [56], specifically RTE and WiC. Each dataset includes 100 instructions, with 10 sourced from Bach et al. [6] and 90 generated using the automatic instruction generation method in [66]. Thus, each dataset has 100 tasks in total, each corresponding to fine-tuning with one instruction. We use T5-Base [45] as the encoder for the MTL model. The choice of this encoder is without loss of generality, as we expect similar results to hold on other encoders.\n\nPut together, our experiment covers seven different datasets in total, spanning medium- and large-scale instances, with the largest dataset containing 500 tasks."}, {"title": "5.1.3 Evaluation metrics.", "content": "We assess the accuracy of estimated task affinity by measuring the distance between our estimated task affinities and the task affinities computed from fully trained models. For task grouping, we evaluate the accuracy averaged over all tasks when training a collection of networks, each on a subset of tasks. The accuracy metric is task-dependent, such as zero-one accuracy or the $F_1$-score, depending on the setting.\n\nLastly, we measure each method's total number of FLoating-point Operations, namely FLOPs. In addition, we report the number of GPU hours evaluated on a single Nvidia RTX6000 GPU."}, {"title": "5.2 Task Affinity Estimation", "content": "We now report the results from running our estimation procedure. We regard the task affinity scores computed from fully trained models as the target, denoted as T*. Then, after running GRAD- TAE, we compute the affinity matrix T, and measure the relative distance between T and T* as:\n\n$Distance(T, T^*) = \\frac{||T - T^*||}{||T^*||}.$\n\nWe evaluate the relative distance on the YouTube graph, which contains $n = 100$ labeling tasks.\n\nAs for the computation cost, our procedure has three parts: (i) training M meta-initializations, each on the combination of all tasks; (ii) For each meta-initialization, computing the gradients on all training examples and projecting the gradients to a lower-dimension; (iii) Solving logistic regression on projected gradients"}, {"title": "5.2.1 Accelerating pairwise task affinity computation.", "content": "First, we train a separate multitask model on each pair of tasks to compute T*. We report the distance metric and the number of FLOPs between fully-trained models (to compute T*) and our algorithm in Table 2. To explain our findings, we set the number of meta-initializations to M = 1 and vary the projection dimension d among 50, 100, 200, and 400. We note that all these values yield an estimation of T* within 11% distance. As expected, increasing d leads to better estima- tion. After d increases above 200, the distance metric also stabilizes to around 5.7%. Thus, we set d as 200 in the remaining experiments. As a remark, this is approximately 15 log(p), where $p = 683, 370$ in this experiment, aligning with our analysis in Proposition 3.3.\n\nRemarkably, under this setting, GRAD-TAE uses 3.5 GPU hours and achieves 130x less computation compared to fully-trained models!\n\nNext, we fix d = 200 while increasing M up to 9. This further reduces the distance metric to 5.4%, with 45.0\\times less compute cost. We observe diminishing returns from ensembling, once M goes beyond 5. Thus, we will set M as 5 in the remaining experiments. This uses 17.6 GPU hours and 44.9x less computation than fully- trained models."}, {"title": "5.2.2 Accelerating higher-order task affinity computation.", "content": "We note qualitatively similar results for approximating higher-order task affinity matrix. Recall this definition from equation (1), Example 2.3. We set $m = 2000$ so that the higher-order task affinity matrix converges while setting the subset size as $a = 10$ (further ablation study will be provided in Section 5.3.4).\n\nUsing M = 1 and d = 200, our algorithm approximates T* within 3.5% distance while using less than 1% cost of computing T*. Further increasing M to 5, the distance drops to 2.7%. Again, the computation cost is only 3% of computing T*. This takes 11.9 GPU hours and uses 32.8\\times less computation than fully-trained models."}, {"title": "5.2.3 Accelerating task affinity computation on text and image data", "content": "sets. We have shown that GRAD-TAE significantly reduces the computational cost in task affinity estimation. To verify that these efficiency gains are consistent across different data modalities, we apply GRAD-TAE to a text classification dataset (RTE) and an image classification dataset (DomainNet) [44]. The RTE data set contains 100 tasks. We use T5-Base and compute higher-order task affinity with 2000 subsets of size 10. The DomainNet data set contains 6 tasks. We use ResNet-50 and compute higher-order task affinity with 20 subsets of size 3. On the two data sets, our algorithm re- duces computation by 42.6\\times and 9.5\\times, respectively, compared to computing true higher-order task affinities, while incurring less than 3% relative error. The smaller speedup in the image dataset is due to the fewer total models trained on task subsets."}, {"title": "5.2.4 Scaling task affinity estimation to very large instances.", "content": "Lastly, we estimate task affinities on the Orkut graph by varying n from 100 to 500. We measure the distance between the estimated and the true pairwise affinity by downsampling the number of pairs to 2000. Figure 3 shows the comparison. We observe that our algorithm scales to as many as 500 tasks, using only 112.3 GPU hours, which is much faster than computing T*. Moreover, the relative distance to the true scores remains within 5%."}, {"title": "5.3 Comparison for Task Grouping", "content": "5.3.1 Baselines. We set up a wide range of baselines covering heuristic solutions and recent optimization techniques.\n\nForward Selection (FS) and Backward Selection (BS) [18]: These are standard approaches to perform subset selection, and we adapt them to task selection.\n\nHigher-Order Approximation (HOA) [50]: This algorithm com- putes pairwise task affinities between every two tasks and averages them to approximate higher-order affinities. It uses a branch-and- bound search algorithm to identify task groupings.\n\nTask Affinity Grouping (TAG) [14]: This approach computes the task affinity by evaluating the projecting one task's gradients onto another task's gradients during training. TAG also uses the branch- and-bound search algorithm to identify grouping.\n\nAuto-\\lambda [32]: This bilevel optimization technique balances the ratio of each task relative to the average objective of all tasks.\n\nBoostMTL [29]: This approach computes higher-order task affin- ity between two tasks as the prediction loss of one task jointly trained with another task and a random subset of the remaining tasks, followed by spectral clustering to identify task groupings."}, {"title": "5.3.2 Multi-label classification on graphs.", "content": "We report the result from applying our algorithm to overlapped community detection. We use our algorithm to estimate higher-order task affinity scores and then cluster the tasks. We illustrate our results in Figure 4a, while deferring a full comparison to Appendix C. We use 1- Macro $F_1$- score"}]}