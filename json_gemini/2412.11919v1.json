{"title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation", "authors": ["Xiaoxi Li", "Jiajie Jin", "Yujia Zhou", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) exhibit remarkable capabilities and are widely applied in various domains (Zhao et al., 2023; Zhu et al., 2023; Mo et al., 2024). However, due to their reliance on model internal memory, they often struggle with long-tail or newly updated knowledge, leading to the issue of \"hallucinations\" (Huang et al., 2023). To address this, retrieval-augmented generation (RAG) has emerged as a promising solution. By integrating retrieval of external knowledge, RAG enables models to access up-to-date and factual information, enhancing the accuracy and reliability of their responses (Lewis et al., 2020; Gao et al., 2024).\nExisting RAG methods typically rely on a separate dense retriever to fetch top-k text chunks for LLMs to generate answers, as shown in Figure 1(a). However, these methods face several limitations: (1) Maintaining a separate retriever increases deployment costs (Zhang et al., 2024a); (2) Retrieved documents often contain redundant information, consuming vast input tokens and distracting the model's attention from key information (Jiang et al., 2023b); (3) The fixed granularity and number of retrieved text chunks limits flexibility of RAG systems (Qian et al., 2024); and (4) Retrievers rely on standalone document indices, hindering joint optimization with generators. Since retrieval and generation are inherently interconnected, jointly learning these tasks can enhance the overall performance of RAG systems (Lewis et al., 2020; Li et al., 2024c). Thus, we aim to develop a unified framework that seamlessly integrates retrieval and generation processes.\nRecently, generative retrieval (GR) has emerged as a promising approach that leverages generative models to directly generate document identifiers (DocIDs), eliminating the need for document indices and making it possible for joint optimization (Li et al., 2024b; Tay et al., 2022; Li et al., 2024c,a). However, existing GR methods still require mapping the generated DocIDs back to the document content before these can be used by LLMs for answer generation, as depicted in Figure 1(b). This step disrupts the seamless integration of retrieval and generation processes.\nTo address the above limitations, we propose RetroLLM, which empowers LLMs to generate factual evidence from knowledge sources and final answer within a unified, auto-regressive decoding process, as shown in Figure 1(c). RetroLLM enables the model to autonomously decide how much"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Task Formulation", "content": "Retrieval-augmented generation (RAG) leverages external knowledge sources to enhance the accuracy of language model generations. In this work, we formulate RAG within a generative framework. We divide the task into two sub-problems:\nConstrained Evidence Generation: This involves retrieving relevant evidence from a large corpus in a generative manner with pre-built constraints. Formally, let C be the corpus of documents and q be the input query. The constrained evidence generation process can be formulated as:\n$P(e|q, C) = \\prod_{t=1}^{T_e} P(e_t|e_{<t}, q, I_C),$ (1)"}, {"title": "2.2 Empirical Study", "content": "To enable language models to generate relevant evidence existing in the external knowledge corpus, a natural approach is to apply FM-Index constraints over the entire corpus. However, our preliminary experiments reveal a critical limitation: while the initially generated evidence sequence usually appears relevant, later generated tokens often reveal that it has grounded to irrelevant documents under FM-Index constraints, resulting in incorrect evidence prediction. This phenomenon is known as false pruning, where relevant sequences are eliminated prematurely during beam search (see Appendix B for detailed analysis).\nTo quantify this issue, we conducted an empirical study. Figure 2(a) illustrates how the relevance calculated by bge-reranker-large between query and generated evidence prefix changes during the auto-regressive decoding process. The results show that compared to labeled evidence sequences, the prefix relevance under corpus FM-Index constraints experiences a significant decline, particularly severe within the first 13 tokens. When we restrict the FM-Index constraints to only relevant documents, this degradation is substantially reduced and evidence generation accuracy improves over different"}, {"title": "3 RetroLLM: Retrieval in Generation", "content": "In this section, we introduce RetroLLM, a unified LLM for RAG via auto-regressive decoding. The decoding process includes clue and evidence stages for retrieval and an answer generation stage. To achieve this, we describe the construction of constraints, clue generation, document scoring, and forward-looking constrained evidence generation."}, {"title": "3.1 Hierarchical FM-Index Constraints", "content": "Before model generation, we construct hierarchical FM-Indexes for different levels of constraints for clue and evidence generation stages, including: (1) a corpus-level global FM-Index $Z$ built from the entire corpus: $I_C$ = FM-Index(C); and (2) a document-level FM-Index manager $I_d$ built for each document: $I_q$ = FM-Index(d) : d \u2208 C. The global index $Z_c$ is primarily used during the clue generation stage to ensure generated phrases exist in the corpus, while document-level indexes $I_d$ are employed during evidence generation to constrain outputs to specific document d."}, {"title": "3.2 Clue Generation and Document Scoring", "content": "As discussed in Section 2.2, generating evidence with relevant document FM-Indexes could reduce the decoding paths and enhance accuracy. Therefore, we propose that the LLM first predict key phrases, or \u201cclues,\u201d that are likely to appear in relevant documents to retrieve subsets of documents.\nClue Generation. Given a query q, we first generate a set of clues $C_{gen}$ under corpus FM-Index constraints to predict key topics of relevant documents. For each clue $C_i \\in C_{gen}$, its generation probability can be formulated as:\n$P(c_i|q, c_{<i}, I_c) = \\prod_{t=1}^{T_i} P(c_{i,t}|c_{i,<t}, q, c_{<i}, I_c)$ (3)\nwhere $c_{i,t}$ represents the t-th token of the i-th clue, $c_{i,<t}$ represents all previously generated tokens for the i-th clue, $I_c$ is the corpus-level FM-Index, and $T_i$ is the length of the i-th clue. Clues are generated between the special tokens </clue> and <l/clue> separated by the special token <lsepl>\nWith the predicted clues, we can obtain the appearance frequency CF(ci) of clue ci in the corpus based on the corpus FM-Index, along with"}, {"title": "3.3 Forward-Looking Constrained Evidence Generation", "content": "Now we have candidate documents, but a key challenge still remains: the model cannot foresee the relevance of future sequences when predicting the current token, making it difficult to decode tokens that lead to correct evidence sequences. To address this challenge, we propose a forward-looking constrained decoding strategy that enables the model to be aware of future sequence relevance.\nOur strategy consists of three key components: (1) locating potential future windows that contain"}, {"title": "3.4 Answer Generation", "content": "With the relevant evidences E generated, the model proceeds to generate the final answer to the original query q, which can be formulated as:\n$P(a|q, C_{gen}, E) = \\prod_{t=1}^{T_a} P(a_t|a_{<t}, q, C_{gen}, E),$ (13)\nwhere a is the generated answer sequence of length Ta, at is the token at position t in the answer, a<t denotes generated tokens before position t."}, {"title": "3.5 Training of RetroLLM", "content": "Since RetroLLM's entire RAG process is one-pass and auto-regressive, we can construct target sequences for supervised fine-tuning to achieve joint learning of retrieval and generation tasks.\nTraining Data Construction. We simulate the model's inference process to construct training data. For each QA pair (q, a), we: (1) Use a sparse retriever to obtain clues Caux and retrieve relevant documents. (2) Locate sentences containing c\u2208 Caux within the documents. (3) Apply a reranker to select the top-ke relevant evidences. (4) Identify evidences that both contain the answer a and are confirmed by an LLM to genuinely answer the query q. (5) Select the top-k evidences up to the first relevant one. (6) For target clues, we utilize an LLM to extract key entities from the query and relevant evidences.\nModel Optimization. Since evidence is typically longer compared to clues and answer, we mask out 80% of the tokens in the middle of each target evidence. We employ the standard next token prediction loss as follows:\n$L= -\\sum_{c+e} \\sum_{t=1}^{T_{c+e}} log P(x_t|x_{<t}, q; \\theta) - \\gamma\\sum_{a} \\sum_{t=1}^{T_{a}} log P(y_t|y_{<t}, x, q; \\theta),$ (14)\nwhere \u03b8 represents the parameters of RetroLLM, x and y represent the target sequence of clues + evidences and answer respectively, and \u03b3 is the weight for the answer loss."}, {"title": "4 Experimental Settings", "content": ""}, {"title": "4.1 Datasets and Evaluation Metrics", "content": "We conduct experiments on five open-domain QA datasets, including single-hop QA: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2023), and multi-hop QA: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2WIKI) (Ho et al., 2020). See Appendix C for detailed statistics. For evaluation metrics, we use Accuracy (Acc), F1 score, and token count (Tok) to assess the quality of generated answers as well as the total number of input and output tokens consumed by LLMs."}, {"title": "4.2 Baselines", "content": "The baseline methods include two types: (1) Direct generation: This includes open-source models Llama3-8B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023a), and Qwen2.5-7B (Yang et al., 2024), and the non-proprietary model ChatGPT (OpenAI, 2022) with results taken from (Zhang et al., 2024b). (2) Retrieval-augmented generation: This includes Naive RAG method and several complex RAG methods, including REPLUG (Shi et al., 2023), Self-RAG (Asai et al., 2024), IRCoT (Trivedi et al., 2023), Iter-RetGen (Shao et al., 2023), and Adaptive-RAG (Jeong et al., 2024). For a fair comparison, all RAG baselines use the E5-base-en (Wang et al., 2022a) retriever, and all LLMs are instruction-tuned with 7B or 8B parameters."}, {"title": "4.3 Implementation Details", "content": "Our knowledge source is based on the Wikipedia dump from December 20, 2018, in alignment with DPR (Karpukhin et al., 2020). We use Mistral-7B-Instruct as Backbone LLM. We limit the maximum number of evidence for single-hop and multi-hop QA to 5 and 10, respectively. We set w\u2081 and w2 to 1 and 2, respectively, and \u039b to 100. For efficient model training, we employ LoRA (Hu et al., 2022), setting training epochs to 3 and \u03b3 to 2. We use SPLADE-v3 (Lassance et al., 2024) for clue expansion and use BGE-reranker-base (Xiao et al., 2024) as the scoring model for window sequences. We implement FM-Index based on the sdsl-lite library (Gog et al., 2014). Refer to Appendix D for more details."}, {"title": "4.4 Experimental Results", "content": "Overall Performance We evaluate RetroLLM's overall downstream performance using NQ, TriviaQA, and HotpotQA for in-domain tasks, and PopQA and 2WIKI for out-of-domain tasks. The results are presented in Table 1. We could found that: (1) RAG methods generally outperform direct generation methods (except for non-proprietary ChatGPT), highlighting the knowledge-intensive nature of these tasks that need retrieval augmentation. (2) RetroLLM outperforms RAG methods across both in-domain and out-of-domain tasks. This highlights the effectiveness of our unified RAG framework in mastering both evidence"}, {"title": "5 Related Work", "content": "Retrieval-augmented Generation Retrieval-augmented generation (RAG) improves generation quality by incorporating relevant context from external knowledge bases, which typically employ a separate dense retriever (Gao et al., 2024; Tan et al., 2024b; Jin et al., 2024b; Tan et al., 2024a; Zhou et al., 2024). Based on training approaches, current RAG systems fall into three categories: (1) Directly prompt of generative models with retrieved context (Press et al., 2023; Trivedi et al.,"}, {"title": "6 Conclusion", "content": "In this paper, we introduced RetroLLM, a framework that unifies retrieval and generation into a single process, allowing language models to directly generate evidence from a corpus with FM-Index constraints. This approach eliminates the need for separate retrievers and reduces redundant input. To improve evidence accuracy, we proposed hierarchical FM-Index constraints and a forward-looking decoding strategy, helping the model focus on relevant information. Experiments show that RetroLLM outperforms existing methods on open-domain QA tasks, marking a step towards a new era of generative retrieval-augmented generation."}, {"title": "7 Limitations", "content": "While RetroLLM demonstrates strong performance across various open-domain QA scenarios, it has several limitations that present opportunities for future research:\n(1) To improve the robustness of the model generated clues, we still need to perform clue expansion to ensure the system's superior performance, as discussed in Section 4.4. This prevents a fully end-to-end optimization of the RAG task. Future work could focus on designing mechanisms that eliminate this need, enabling complete end-to-end RAG optimization.\n(2) In terms of efficiency, RetroLLM outperforms most complex RAG methods in query latency. However, it is slightly slower than Naive RAG, as the generated evidence results in more output tokens despite being fine-grained and short. Drawing on the concept of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), future improvements could involve using a smaller language model during the constrained evidence generation phase and switching to a larger model during answer generation. This approach could enhance RetroLLM's efficiency, comprehensively surpassing existing RAG methods in both performance, latency, and flexibility.\n(3) RetroLLM currently only considers the unification of evidence retrieval and final answer generation. It would be worth exploring the incorporation of more model reasoning processes within RetroLLM's single generation step, such as query intent analysis, question decomposition, retrieval"}, {"title": "A The FM-Index", "content": "The FM-Index (Ferragina and Manzini, 2000), which stands for Full-text index in Minute space, is a space-efficient data structure designed for indexing large text corpora, combining the Burrows-Wheeler Transform (BWT) and run-length encoding. It enables fast substring searching while providing substantial compression, making it particularly useful in applications such as prefix-constrained decoding."}, {"title": "A.1 Data Structure", "content": "The FM-Index is based on the Burrows-Wheeler Transform (BWT) (Schindler, 1997). The BWT of a string S is computed by sorting all cyclic rotations of S lexicographically and then taking the last column of the sorted rotations. This transformation rearranges the characters of the string in a way that enhances its compressibility, which is key to the FM-Index's space efficiency.\nFormally, for a string S = $S_1S_2... S_n$, the BWT, denoted as BWT(S), is obtained by sorting the cyclic rotations of S lexicographically and taking the last character of each rotation. Let R(S) denote the set of all cyclic rotations of S, sorted in lexicographical order:\n$R(S) = {\\sigma_1, \\sigma_2,...,\\sigma_n}$ (15)\nwhere $\u03c3_i$ denotes the i-th rotation of S. The BWT of S is then the string formed by the last characters of these sorted rotations:\nBWT(S) = ($\u03c3_1[n], \u03c3_2[n], ..., \u03c3_n[n]$) (16)\nThe FM-Index stores only two columns from the BWT matrix: the first (F) and last (L) columns. These columns capture the relative order of the characters in all cyclic rotations of S."}, {"title": "A.2 Supporting Functions", "content": "Backward Search The core feature of the FM-Index is the backward search, which allows for"}, {"title": "B False Pruning in Constrained Decoding", "content": "In Section 2.2, we conducted empirical studies revealing that false pruning is a significant issue in constrained decoding. This section delves deeper into understanding this problem."}, {"title": "B.1 What is False Pruning?", "content": "False pruning occurs when the search process incorrectly eliminates branches that could contain the optimal solution, preventing the algorithm from identifying the true best outcome. Specifically, in prefix-constrained decoding for language models, false pruning involves incorrectly discarding candidate tokens that meet the prefix constraint but might contribute to the optimal solution (Zhang et al., 2023)."}, {"title": "B.2 What Causes False Pruning?", "content": "For auto-regressive decoding models, false pruning arises primarily due to two factors:\nExcessive Prefix Choices: In large corpora, candidate sequences present a vast number of prefix options initially. The model can generate nearly any short prefix it wants, making it challenging to predict the correct one.\nLimited Future Awareness: Even with fewer prefix choices, the model cannot anticipate future content beyond the current token decision. This limitation makes it difficult to select tokens that lead to the correct evidence."}, {"title": "B.3 How to Mitigate False Pruning?", "content": "Addressing the root causes of false pruning involves implementing strategies that either narrow the prefix choices or enhance the model's foresight during decoding.\nReducing Prefix Choices: One effective method is to limit the number of prefix options. Our approach employs clue generation to identify a relevant subset of documents, followed by decoding evidence within this constrained set. This reduction significantly decreases the prefix choices, mitigating the risk of false pruning.\nEnhancing Future Relevance Awareness: Another strategy is to provide the model with information about the relevance of future sequences. In our method, we identify the clue's position within the document and utilize the surrounding text as future windows. By guiding the language model to generate relevant evidence based on these windows and their relevance scores, we improve the model's ability to connect to the target information.\nSet-Based Decoding: Some generative retrieval methods adopt set-based decoding strategies (Zhang et al., 2023; Zeng et al., 2024), which bypass the issues inherent in auto-regressive decoding by directly generating sets of terms."}, {"title": "C Datasets", "content": ""}, {"title": "C.1 Details of Datasets", "content": "In our experiments, we utilize a variety of question answering (QA) datasets to evaluate both single-hop and multi-hop reasoning capabilities. For single-hop QA, we employ the Natural Questions (NQ) (Kwiatkowski et al., 2019) dataset, TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2023), which provide a diverse range of factual questions requiring straightforward retrieval and answer extraction. For multi-hop QA, we use HotpotQA (Yang et al., 2018), which necessitates reasoning across multiple documents, and 2WIKI (Ho et al., 2020), a dataset designed to test more complex multi-hop reasoning scenarios. These datasets are selected to cover a broad spectrum of QA challenges, ensuring a comprehensive evaluation of model's retrieval and reasoning capability."}, {"title": "C.2 Statistics", "content": "Table 5 presents detailed statistics of the datasets and the retrieval corpus used in our study. For single-hop QA tasks, NQ consists of 79,168 training samples and 3,610 test samples, while TriviaQA has 78,785 training samples and 11,313 test samples. PopQA is used solely for testing, with 14,267 samples. In the multi-hop QA category, HotpotQA includes 90,447 training samples and 7,405 test samples, and 2WIKI provides 12,576 test samples without a training set. The retrieval corpus comprises the Wikipedia dataset, containing 21,015,324 passages and 3,232,907 documents."}, {"title": "D Implementation Details", "content": ""}, {"title": "D.1 Implementation Details for Baselines", "content": "All RAG baselines are implemented based on the FlashRAG framework, which is an open-source retrieval-augmented generation toolkit (Jin et al., 2024a). For Self-RAG (Asai et al., 2024), we use the trained selfrag-llama2-7B checkpoint. For all other baselines, we use Mistral-7B-Instruct as the backbone model, aligning with our RetroLLM. All hyper-parameter configurations are set to default in FlashRAG."}, {"title": "D.2 Implementation Details for Naive Constrained Generation", "content": "For the naive approach to constrained beam evidence generation, we set num_beams = 5, num_beam_groups = 5, and diversity_penalty = 1.0 for constrained beam search. The num_beam_groups and diversity_penalty parameters are crucial; without setting these two parameters, the sequences generated by each beam would be highly similar, leading to a significant decrease in evidence accuracy. These parameters ensure diversity among the multiple generated sequences and sort the beam_size generated evidences from high to low according to the generation probability of the language model, so that more relevant evidence can be ranked ahead. For cases where an answer needs to be generated, we continue to freely generate the answer without constraint after each beam, and the final answer given is the answer generated after the top-1 sequence."}, {"title": "D.3 Implementation Details for RetroLLM", "content": "The implementation of RetroLLM mainly includes FM-Index building, training, and inference. All experiments are conducted on 8 NVIDIA A800-80GB GPUs and an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz with 64 cores."}, {"title": "D.3.1 FM-Index Building", "content": "We implement the FM-Index data structure based on the SDSL-lite (Succinct Data Structure Library) framework (Gog et al., 2014), which is an efficient C++ template library specifically designed for implementing compressed data structures. We then"}, {"title": "D.3.2 Training", "content": "Training Data Construction The data construction approach simulates the model's inference process. For each labeled QA pair (q, a), we first utilize a sparse retriever SPLADE-v3 (Lassance et al., 2024) to obtain top-8 clues Cexp and retrieve top-20 documents. We then locate the sentences containing these clues within the documents, followed by employing a reranker to obtain the top-ke relevant evidences Erel, where ke is set to 5 for single-hop QA and 10 for multi-hop QA tasks. Next, we examine whether the labeled answer a is contained within each evidence e to determine if the evidence can address the original query q. To further ensure labeling accuracy, we employ a Llama3.1-70B-Instruct (Dubey et al., 2024) model to judge whether each e \u2208 Erel can genuinely answer the query q. We consider an evidence e relevant only if it both contains a and is labeled as relevant by the LLM. Subsequently, we select the top-k < ke evidences where the k-th evidence is the first relevant e. For target clues, we utilize Llama3.1-70B-Instruct to extract key entities from the query and relevant evidences to construct target clues Cgen. This process yields the training pair (q, Cgen, E, a)."}, {"title": "D.3.3 Inference", "content": "As illustrated in Figure 3, RetroLLM includes the following three stages. In the clue generation stage, RetroLLM first generates clues with corpus-level FM-Index constraints. The format of this part is \u201c<|clue|> C1 <|sep|> C2 <|sep|> ... <|/clue|>\u201d. During clue generation, we simultaneously expand clues with the sparse lexical and expansion model SPLADE-v3 (Lassance et al., 2024). We set the number of expanded clues to 8 and the maximum"}, {"title": "E Detailed Experimental Results", "content": ""}, {"title": "E.1 Analysis of Retrieval Performance", "content": "We analyze the retrieval performance of RetroLLM compared to sparse and dense retrieval baselines, as discussed in Section 4.4. The results are shown in Table 6 (1) For single-hop QA tasks, RetroLLM demonstrates superior accuracy on R@1, thanks to the design of clues and future windows, which help precisely locate the relevant evidence. For instance, on the PopQA dataset, RetroLLM achieves an R@1 of 57.0%, surpassing the best dense retriever E5, which attains 51.7%. Additionally, RetroLLM uses fewer passages on average (2.80 for TriviaQA and 3.20 for NQ) compared to the fixed number of 5 in baseline methods, indicating more efficient retrieval. (2) For multi-hop QA tasks, RetroLLM shows superior accuracy compared to all other methods for both R@1 and R@5, while utilizing a smaller average number of retrieved passages. Specifically, on HotpotQA, RetroLLM achieves an R@1 of 35.6%, outperforming E5's 32.3% and SPLADE-v3's 32.9%. On the 2WIKI dataset, RetroLLM attains an R@1 of 23.0%, higher than E5's 21.6%, demonstrating its effectiveness in multi-hop retrieval scenarios while using only 4.40 passages on average versus the baseline's 5. (3) Notably, the naive generative retrieval method using constrained beam search performs poorly on all metrics, further validating the severity of false pruning, as discussed in Section 2.2. For example, on the NQ dataset, the naive method achieves an R@1 of only 13.1%, significantly lower than RetroLLM's 51.6%. Similarly, on TriviaQA, it attains an R@1 of 23.0% compared to RetroLLM's 61.1%, highlighting the substantial"}, {"title": "E.2 Impact of Different Base Models", "content": "To evaluate the performance of RetroLLM using different backbone LLMs with varying parameter sizes, we conducted experiments using the Mistral, Llama3, and Qwen2.5 series, with parameters ranging from 1B to 14B, as discussed in Section 4.4. The results are shown in Figure 7. We observe that: (1) As the parameter size increases, RetroLLM's performance steadily improves, aligning with the scaling law. For example, within the Llama3 series, the accuracy on the NQ dataset rises from 54.4% for Llama3.2-1B to 59.2% for Llama3-8B. Similarly, the F1 score on TriviaQA improves from 52.9% to 69.3%. In the Qwen2.5 series, the accuracy on NQ increases from 50.1% for Qwen2.5-1.5B to 58.6% for Qwen2.5-14B, and the F1 score climbs from 34.3% to 50.6%. This consistent enhancement across different model sizes indicates that larger base models contribute to better retrieval performance in RetroLLM. (2) There are slight performance differences across the different models (Mistral, Llama3, Qwen2.5), with Mistral generally outperforming Llama3, which in turn outperforms Qwen2.5. Specifically, Mistral-7B achieves the highest accuracy on several datasets, such as 61.6% on NQ and 74.3% on TriviaQA, surpassing both Llama3-8B and Qwen2.5-14B. On the PopQA dataset, Mistral-7B attains an accuracy of 65.7%, compared to 65.2% for Llama3-8B and 64.3% for Qwen2.5-14B. Despite these variations, all models confirm the effectiveness of RetroLLM, as even smaller"}, {"title": "E.3 Impact of Generated Evidence Quantity", "content": "Since RetroLLM can dynamically determine the number of evidence to retrieve, we investigated the effect of different maximum retrieval quantities on performance, as discussed in Section 4.4. The results are shown in Table 8. We observe that: (1) When retrieving up to 1-5 evidence, performance continues to improve as the number of retrieved pieces increases, suggesting that more evidence contributes to stronger performance in these tasks. For instance, on the NQ dataset, the accuracy improves from 42.2% when retrieving only one piece of evidence to 61.5% with five pieces. Similarly, the accuracy on TriviaQA rises from 59.3% to 74.6% as the number increases from one to five. This trend indicates that accessing more evidence enables RetroLLM to retrieve relevant information more effectively, enhancing answer accuracy. (2) However, for multi-hop QA, performance stabilizes around 6 evidence, as more evidence can bring in both useful and distracting information, thereby limiting further performance gains. Specifically, on the HotpotQA dataset, the accuracy increases from 50.6% with one piece of evidence to 67.4% with six pieces, but additional evidence beyond this point yields diminishing returns (e.g., 68.5% accuracy at ten pieces). This suggests that while some additional evidence is beneficial, too much can introduce noise that counteracts the benefits, highlighting the importance of a balanced retrieval strategy."}, {"title": "F Case Study", "content": "This section presents examples from RetroLLM and compares them with outputs from a naive constrained beam search method. These examples illustrate the detailed workings of our method and highlight the shortcomings of the naive approach."}, {"title": "F.1 Examples from RetroLLM", "content": "Tables 9 and 10 display examples from single-hop and multi-hop question-answering (QA) datasets, respectively. The overall process of RetroLLM consists of two main stages: clue generation and evidence generation."}, {"title": "F.2 Comparing RetroLLM with Naive Beam Search Method", "content": "Table 11 and 12 compares the outputs of the naive constrained beam search method with those of RetroLLM for a question from the NQ Dataset. The naive method attempts to generate evidence under corpus-level FM-Index constraints, but this approach leads to several issues. The beams generated by the naive method contain evidence that is largely irrelevant or incoherent. Although some initial phrases may appear related to the question, the continuation often deviates significantly, producing sentences that do not contribute to answering the question correctly. For instance, the naive method incorrectly identifies \"Roger Maris,\u201d \u201c1903,\u201d and \u201cJonathan Elliot\u201d as answers to the question \u201cwho got the first nobel prize in physics?\u201d These incorrect answers result from the model's inability to maintain topic coherence under the stringent corpus-level constraints, a phenomenon known as the false pruning problem discussed in Section 2.2 and Appendix B. In contrast, RetroLLM addresses this issue by first generating clues to narrow down the relevant document subset and then performing forward-looking constrained evidence generation within this subset. This method reduces false pruning and enhances the accuracy of evidence retrieval."}]}