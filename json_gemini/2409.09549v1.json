{"title": "COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare", "authors": ["Chia-Hao Li", "Niraj K. Jha"], "abstract": "Wearable medical sensors (WMSs) are revolutionizing smart healthcare by enabling continuous, real-time monitoring of user physiological signals, especially in the field of consumer healthcare. The integration of WMSs and modern machine learning (ML) enables unprecedented solutions to efficient early-stage disease detection. Despite the success of Transformers in various fields, their application to sensitive domains, such as smart healthcare, remains underexplored due to limited data accessibility and privacy concerns. To bridge the gap between Transformer-based foundation models and WMS-based disease detection, we propose COMFORT, a continual fine-tuning framework for foundation models targeted at consumer healthcare. COMFORT introduces a novel approach for pre-training a Transformer-based foundation model on a large dataset of physiological signals exclusively collected from healthy individuals with commercially available WMSs. We adopt a masked data modeling (MDM) objective to pre-train this health foundation model. We then fine-tune the model using various parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, to adapt it to various downstream disease detection tasks that rely on WMS data. In addition, COMFORT continually stores the low-rank decomposition matrices obtained from the PEFT algorithms to construct a library for multi-disease detection. The COMFORT library enables scalable and memory-efficient disease detection on edge devices. Our experimental results demonstrate that COMFORT achieves highly competitive performance while reducing memory overhead by up to 52% relative to conventional methods. Thus, COMFORT paves the way for personalized and proactive solutions to efficient and effective early-stage disease detection for consumer healthcare.", "sections": [{"title": "1. Introduction", "content": "Physical and mental illnesses not only negatively impact individual well-being but also adversely impact the global society. Therefore, the pursuit of efficient and effective early-stage disease detection has become a crucial research area in the consumer healthcare domain. Fortunately, modern advances in machine learning (ML) and wearable medical sensors (WMSs) highlight unprecedented ways to address this challenge. WMSs have revolutionized the healthcare field through passive, continuous, non-invasive, and real-time monitoring of physiological signals. These devices can collect a wealth of data, including galvanic skin response, heart rate, skin temperature, blood pressure, oxygen saturation, and other vital signs. A sophisticated ML model can then be trained on such data to enable efficient and accurate inference for early-stage disease detection. The integration of ML and WMSs promises to streamline the disease detection process, making it feasible even in out-of-clinic scenarios [16, 60, 18, 28, 13, 29, 46, 21, 56].\nOn the other hand, Transformers [54] are revolutionizing various research fields, such as natural language processing (NLP) [39, 37, 43, 45] and computer vision (CV) [26, 25, 12]. The unique self-attention mechanism in Transformer models processes complex sequential data in parallel, which significantly reduces training time. As a result, numerous intensively pre-trained large language models (LLMs) [7, 1, 11, 51, 52] and large vision models (LVMs) [12, 42, 34] have been pre-trained as foundation models. These models are designed to perform general-purpose tasks and can be fine-tuned for specific downstream applications. They are pre-trained on an enormous amount of open-source unlabeled data through self-supervised learning to extract general knowledge from the target domain. However, fine-tuning large foundation models for every new task is often impractical and resource-intensive. Therefore, various parameter-efficient fine-tuning (PEFT) methods [23, 32, 58, 22, 41, 30] have been proposed to address this challenge. In general, these methods selectively tune a small subset of parameters to reduce the computation burden and time required for fine-tuning. Hence, they enable foundation models to efficiently adapt to diverse applications while maintaining scalability.\nDespite the significant success of foundation models in various fields, their capabilities in sensitive domains like smart healthcare remain largely underexplored [27]. Patient health data, including electronic health records, medical images, and physiological signals, are governed by numerous laws and restrictions to ensure data security and patient privacy. Consequently, retrieving sufficient general data to pre-train a foundation model for healthcare applications is challenging. Several multimodal LLMs, fine-tuned for smart healthcare, have demonstrated promising performance on understanding domain knowledge [20, 47, 48, 15, 50, 49, 35]. However, their primary focus is on developing conversational artificial intelligence (AI) assistants for medical support rather than on early-stage disease detection in consumer health tasks. Consumer health disease detection relies heavily on sequential time-series physiological signals collected from WMSs, which differ significantly from static linguistic text data. The unique characteristics of WMS data, such as high dimensionality, nonlinear relationships, and continuous nature, require foundation models to comprehend both individual data points and their dynamic patterns over time [27]. Therefore, developing Transformer-based foundation models using physiological data from WMSS for disease detection tasks poses new challenges.\nTo address the aforementioned challenge, we propose a framework called COMFORT. It introduces a novel approach to developing foundation models tailored to WMS data that can be fine-tuned across various downstream disease detection tasks in the consumer healthcare domain. We pre-train a Transformer-based foundation model using a large WMS dataset collected exclusively from healthy individuals. We employ a masked data modeling (MDM) objective for pre-training, guiding the foundation model to grasp the essence and dynamic"}, {"title": "2. Background and Related Work", "content": "In this section, we provide background material and discuss related works that will help readers understand the rest of the article."}, {"title": "2.1 Transformers and Foundation Models", "content": "A Transformer is a model architecture that relies on the self-attention mechanism [3]. The original Transformer model [54] was designed for machine translation tasks. Fig. 2 shows the Transformer model architecture. It generally consists of two main components: the encoder layers and the decoder layers. The encoder layers process sequential input data across various time steps in parallel using a multi-head self-attention module. They encode input information into contextualized representations. The decoder layers then process these encoded representations along with previously produced tokens to generate the corresponding output sequences. We refer readers to the literature [54, 38] for a detailed explanation of the operations performed in each module.\nTransformer models exhibit remarkable capabilities in modeling complex linguistic data and text generation. Thus, it is extensively used in various NLP tasks [39], including sentiment analysis [37], machine translation [43], question answering [45], and information extraction [24]. The self-attention mechanism is highly parallelizable. This significantly reduces training time and enables large-scale pre-training. As a result, many Transformer-based large language models (LLMs), such as OpenAI's GPT series [7, 1], Google's BERT [11], and Meta's LLaMA series [51, 52], are intensively pre-trained on an enormous corpus of text data through self-supervised or semi-supervised learning. These pre-trained LLMS are often referred to as foundation models [62, 6] since they are trained on a broad spectrum of unlabeled data, enabling to tackle general tasks like natural language understanding and text generation. These foundation models can then be fine-tuned on task-specific data for downstream tasks through transfer learning.\nTransformer models have had a significant impact in other domains as well, such as CV [26, 25, 12], remote sensing imaging [2], speech processing [40], and time-series modeling [57]. Furthermore, many LVMs [12, 42, 34] have emerged as foundation models in the CV domain. However, the development of foundation models for sensor data domains, especially in the field of consumer healthcare, remains largely unexplored."}, {"title": "2.2 Parameter-Efficient Fine-Tuning Methods for Foundation Models", "content": "As foundation models are pre-trained for general-purpose objectives, full fine-tuning is often required to adapt them to specific downstream tasks. However, with the increasing size of these models and downstream datasets, fine-tuning entire models for every new task has become impractical and resource-intensive. Hence, PEFT methods are crucial for adapting foundation models to downstream tasks without consuming extensive computational resources and training time. They selectively tune a small subset of parameters, thereby reducing the computational burden and time. Prominent PEFT methods include LORA [23], adapter modules [22, 41], and prefix tuning [30, 33]. LoRA and its variants [32, 58] approximate weight updates using low-rank decomposition matrices, while adapter modules introduce additional layers between existing ones to capture task-specific information. On the other hand, prefix tuning prepends learnable vectors to the input sequence to guide model behavior. These methods facilitate efficient and scalable adaptation of large models to diverse applications.\nAmong the PEFT algorithms, LoRA and its variants, Weight-Decomposed Low-Rank Adaptation (DoRA) [32] and Chain of LoRA (COLA) [58], do not alter the model architecture and are easily applicable to models with numerical sensor data inputs. In general, for a foundation model with a pre-trained weight matrix \\(W_o \\in \\mathbb{R}^{d\\times k}\\), these methods approximate its update \\(W_o+ \\Delta W\\) by representing the variance \\(\\Delta W\\) with a low-rank decomposition"}, {"title": "2.3 Transformer Models in Smart Healthcare", "content": "Transformer models are gaining increasing attention and adoption in the smart healthcare domain [20, 49, 38, 55, 44, 19]. Han et al. [15] fine-tune publicly accessible pre-trained LLMs on specialized datasets crafted with biomedical domain knowledge. Their fine-tuned model, MedAlpaca, consistently outperforms its pre-trained-only counterparts on the United States Medical Licensing Examination (USMLE). Singhal et al. [47] demonstrate promising performance on medical question answering datasets with their Transformer model, Med-PaLM, developed by adapting a foundation model with instruction prompt tuning. Toma et al. [50] introduce Clinical Camel based on LLaMA-2 [52], an open LLM explicitly tailored to clinical research using QLoRA [10]. Clinical Camel achieves state-of-the-art (SOTA) performance across multiple medical benchmarks relative to openly available medical LLMs and is capable of synthesizing plausible clinical notes. Li et al. [31] adapt and refine LLaMA [51] using a large dataset of 100,000 patient-doctor dialogues sourced from a public online medical consultation platform to develop a medical chatbot, ChatDoctor. ChatDoctor can understand patient needs and provide informed advice with self-directed information retrieval from reliable online sources and offline medical databases.\nIn addition to their adaptation in medical NLP applications, Transformer models are reshaping the healthcare domain by enabling multimodal medical data analysis, e.g., based on medical images and physiological signals. Chen et al. [8] propose TransUNet as a strong alternative for medical image segmentation. TransUNet integrates a U-shaped convolution neural network (CNN) with a vision Transformer. It outperforms various competing methods on different medical applications, including multi-organ segmentation and cardiac segmentation. Dai et al. [9] develop TransMed for multimodal medical image classification. TransMed combines the advantages of CNN and Transformer to efficiently extract low-level features of images and establish long-range dependencies between modalities. It achieves superior performance relative to other SOTA CNN-based models on two medical image datasets. On the other hand, Kim et al. [27] extend the capacity of pre-trained LLMs to deliver multimodal health predictions based on contextual linguistic information and physiological data. They fine-tune off-the-shelf LLMs using prompting, instruction tuning, and PEFT on six public health health datasets. They demonstrate promising performance on"}, {"title": "3. The COMFORT Framework", "content": "This section discusses the COMFORT framework in detail. We begin with a top-level overview. Then, we describe the health foundation model. Finally, we elaborate on COMFORT's scalability and adaptivity for disease detection based on a PEFT method."}, {"title": "3.1 Framework Overview", "content": "Inspired by the success of Transformer-based foundation models in NLP and CV, we propose COMFORT that applies the pre-train and fine-tune paradigm to the sensor data domain. Fig. 3 presents a top-level overview of the framework. COMFORT comprises two main components: health foundation model pre-training and continual fine-tuning for disease detection. In the pre-training phase, we train a health foundation model on a large dataset collected exclusively from healthy individuals using commercially available WMSs. We adopt an MDM self-supervised learning objective to enable the foundation model to understand the WMS data and their dynamically-changing patterns. During the continual fine-tuning phase, COMFORT keeps the foundation model's pre-trained weights \\(W_o\\) frozen. It then applies a LoRA-based [23] PEFT algorithm to efficiently fine-tune the foundation model for downstream detection tasks with disease-specific WMS datasets gathered from patients. Meanwhile, COMFORT maintains a library to store all the low-rank decomposition matrices \\(\\{B_i, A_i\\}\\) and corresponding classifier \\(\\{C_i\\}\\) for all learned diseases. The COMFORT library allows it to continually tackle new disease detection tasks without compromising the scalability of the foundation model. Furthermore, it empowers COMFORT's adaptivity to detect various diseases by flexibly switching between different \\(\\{B, A, C'\\}\\) triples from the library. Hence, COMFORT provides a versatile tool for efficient and effective early-stage disease detection in the consumer healthcare domain."}, {"title": "3.2 Health Foundation Model", "content": "The self-attention mechanism in Transformer models enables capturing of long-range dependencies within sequential data and processing of this temporal information in parallel. This makes Transformer models ideal candidates for analyzing sequential time-series physiological signals collected using WMSs. The health foundation model in COMFORT is a Transformer model dedicated to disease detection tasks based on WMS data. It needs to understand the relationship among input sensor data. However, it does not need to generate any generative context except output probabilities for inference, which eliminates the need for the decoder layers. Therefore, we adopt an encoder-only Transformer architecture that harnesses bidirectional representations from its self-attention mechanism. We choose the BERT [11] architecture, a multi-layer bidirectional Transformer encoder, for our health foundation model. It is almost identical to the original model [11]. Hence, we follow the method described in [11] and [54] to build our model. We omit an exhaustive background description of the model architecture and operations. We refer readers to the literature"}, {"title": "3.3 Continual Fine-Tuning for Disease Detection", "content": "COMFORT adapts the health foundation model to downstream disease detection tasks using a PEFT method. As mentioned in Section 2.2, LoRA and its variants, DoRA and COLA, do not alter the model weights and are easily applicable to models with numerical sensor data inputs. Therefore, we select from LoRA, DORA, and COLA to implement the PEFT algorithm in COMFORT. We compare their performance in our experiments.\nWe use the original implementation described in the literature [23, 32, 58] for these PEFT methods. When fine-tuning on a new downstream task i, COMFORT keeps the health foundation model's pre-trained weights \\(W_o \\in \\mathbb{R}^{d\\times k}\\) frozen. As mentioned in Section 2.2, the forward passes with an input \\(x \\in \\mathbb{R}^{1\\times d}\\) updated by LoRA, DORA, and CoLa yield:\nLORA: \\(x(W_o + \\Delta W_i) = x(W_o + B_iA_i)\\);\nDORA: \\(x(W_o + \\Delta W) = x(m'_i \\frac{W_o + B_iA_i}{||W_o + B_iA_i||_c});\\)\nCOLA: \\(x(W_o + \\Delta W^*) = x(W_o + \\sum_{j=1}^{l} B_{ij} A_{ij}),\\)\nwhere \\(\\{B_i, B_{ij}\\} \\in \\mathbb{R}^{d\\times r}\\), \\(\\{A_i, A_{ij}\\} \\in \\mathbb{R}^{r\\times k}\\), \\(m_i \\in \\mathbb{R}^{1\\times k}\\), and \\(r < min(d, k)\\). We use random Gaussian initialization for \\(\\{A_i, A_{ij}\\}\\) and zero for \\(\\{B_i, B_{ij}\\}\\). Similar to the original implementation, we then scale \\(\\{B_iA_i, B_{ij}A_{ij}\\}\\) by \\(\\alpha/r\\), where \\(\\alpha\\) is a constant. In addition, COMFORT appends a dedicated classifier \\(C_i\\), comprising a linear module and a softmax layer, to classify disease i during fine-tuning. Fig. 6 illustrates the model architecture after fine-tuning for a downstream task.\nIn addition, COMFORT maintains a library in order to continually store the low-rank decomposition matrices \\(\\{LoRA: (B_i, A_i), DORA: (m'_i, B_i, A_i), COLA: (B_{ij}, A_{ij})\\}\\) and the dedicated classifier \\(\\{C_i\\}\\) for all learned diseases. Preserving the low-rank matrices instead of the updated weights \\(\\{W_o + \\Delta W_i\\}\\) or the weight variances \\(\\{\\Delta W_i\\}\\) reduces memory overhead significantly. This strategy enables us to continually fine-tune the health foundation model for various disease detection tasks. When deployed in production, a user can flexibly download and choose among the desired \\(\\{(B_i, A_i) or (m'_i, B_i, A_i) or (B_{ij}, A_{ij})\\}and C_i\\) from the COMFORT library to detect disease i on an edge device with commercially available"}, {"title": "4. Experimental Setup", "content": "In this section, we provide details of the experimental setup. First, we introduce the datasets used in our experiments. Next, we elaborate on the preprocessing procedures applied to these datasets. Finally, we present the implementation details of our COMFORT framework."}, {"title": "4.1 Datasets", "content": "The datasets we use in our experiments are from prior studies: DiabDeep [60] and MHDeep [18]. The data collection and experimental procedures for both datasets were approved by the Institutional Review Board of Princeton University. The efficacy of both the datasets has been established in the literature [60, 18]. In both datasets, we collect distinct physiological signals from participants with a commercially available WMS, an Empatica E4 smartwatch. In addition, we collect motion data and ambient environmental information from participants using a Samsung Galaxy S4 smartphone. These supplementary data have been demonstrated to provide diagnostic insights based on user motion and habit tracking in the literature [60, 18]. Table 1 lists the sensor features collected in these datasets and their respective sampling rates, sources, and data types.\nThe DiabDeep dataset consists of sensor data collected from 52 participants, including 14 diagnosed with Type-I diabetes, 13 diagnosed with Type-II diabetes, and 25 non-diabetic"}, {"title": "4.2 Dataset Preprocessing", "content": "We preprocess and format both datasets to fit our foundation model for experiments. First, we synchronize the sensor data from the smartwatch and smartphone using their timestamps. Next, we segment the data streams into 15-second windows with a 15-second shift in between to minimize time correlation between adjacent windows. Each 15-second window of data constitutes a data sequence, which we further split into 15 1-second data instances. Analogous to NLP, we format each sensor data sequence as a linguistic sequence (sentence) with 15 tokens (words), where each token is encoded as a 1-second data stream of all sensor features. Within each 1-second data instance (or token), we flatten and concatenate data from the smartwatch and smartphone. This results in a total of 20,957 data sequences in the DiabDeep dataset and 27,082 data sequences in the MHDeep dataset, with each data instance containing 299 features. Therefore, the DiabDeep dataset has a [20957, 15, 299] size, whereas the MHDeep dataset has a [27082, 15, 299] size."}, {"title": "4.3 Implementation Details", "content": "COMFORT addresses disease detection on edge devices based on WMS data. We employ the BERTTINY architecture [53, 5] for this purpose. We set the number of Transformer encoder layers L to 2, the model's hidden size H to 128, and the number of attention heads A to 2. We follow the original implementation in [54, 11] to set other Transformer parameters. Subsequently, we implement two hidden layers in the linear module for each dedicated classifier for downstream tasks. The classifier begins with an input layer with 128 neurons for alignment with the output embeddings from the encoder layers. Then, it is followed by two hidden layers with 512 and 128 neurons, respectively. We adopt the rectified linear unit (ReLU) as the nonlinear activation function in the hidden layers. Finally, the classifier concludes with an output softmax layer with the number of neurons corresponding to the number of classes in the downstream task. Specifically, there are three neurons for the DiabDeep task and four neurons for the MHDeep task. This results in less than 529k parameters in the fine-tuned model.\nIn COMFORT, we exclusively use WMS data obtained from healthy individuals to pre-train the health foundation model. However, we face limitations due to a lack of available large-scale WMS datasets collected from healthy individuals. Meanwhile, our experimental datasets do not provide sufficient WMS data from healthy individuals. Therefore, we use a synthetic data generation tool, called TUTOR, for this purpose [17, 28, 29]. We employ the Gaussian mixture model estimation method [28, 29] to generate 100,000 synthetic data instances from the WMS data of the healthy individuals in both datasets. We use these synthetic data to pre-train the health foundation model and to prevent bias during fine-tuning.\nFor both pre-training and fine-tuning, we use the Adam optimizer with the learning rate initialized at 0.005 and the batch size set to 128. We pre-train the foundation model for 1,000 epochs or until convergence. For fine-tuning, we train the model with task-specific datasets for 300 epochs. For fine-tuning, we compare LoRA [23], DORA [32], and CoLA [58]. We set both the rank number r and the constant \\(\\alpha\\) to eight for all three PEFT methods. In addition, we set the chain length to three with a fixed rank number for COLA. We implement COMFORT with PyTorch and perform experiments on an NVIDIA A100 GPU. To accelerate the experiments, we employ the CUDA and cuDNN libraries."}, {"title": "5. Experimental Results", "content": "This section presents experimental results on fine-tuning for two downstream disease detection tasks: DiabDeep and MHDeep. Before fine-tuning, we pre-train the health foundation model on the synthetic healthy individual data, as described in Section 4.3. We pre-train the foundation model with the MDM objective described in Section 3.2 until we obtain a loss value lower than 0.001."}, {"title": "5.1 Continual Fine-Tuning for Disease Detection", "content": "We compare the performance of LoRA, DORA, and COLA with a vanilla full fine-tuning method, where the COMFORT library stores the fully updated weight variances \\(AW\\)s. In addition, we implement two different baselines for comparison. Following the original implementation [60, 18], we recreate the DiabDeep and MHDeep multilayer perceptron (MLP) models with synthetic data pre-training and grow-and-prune neural network synthesis. It is important to note that the synthetic data used for training the DiabDeep and MHDeep MLP models are generated from all the data in the training set. On the other hand, we build two task-specific BERTTINY models that are trained from scratch with Xavier initialization [14] for the two tasks, respectively. To evaluate COMFORT's performance, we report the test accuracy and F1-score on the learned tasks. The F1-score is computed by defining true positives (negatives) as the unhealthy (healthy) data sequences correctly classified as disease-positive (healthy) and false positives (negatives) as the healthy (unhealthy) data sequences misclassified as disease-positive (healthy). In addition, we report the model memory consumption required to detect the two learned tasks.\nTable 3 shows the experimental results after continually fine-tuning for the two downstream tasks, DiabDeep and MHDeep. Table 4 provides a breakdown of the model memory consumption required. To detect both diseases, the system needs to store either both DiabDeep and MHDeep MLP models, two task-specific BERTTINY models, or the COMFORT framework, which includes the health foundation model and COMFORT library. As we can see from Table 3, all Transformer-based models outperform the original MLP models in both test accuracy and F1-score for both tasks. This demonstrates the advantage of Transformer models in processing sequential time-series WMS data over conventional MLP models, even though MLP models consume less memory. Among the Transformer models, COMFORT achieves competitive performance with PEFT methods compared to training task-specific models from scratch and full foundation model fine-tuning. Moreover, COMFORT saves more than 34% and 52% in memory consumption with LoRA relative to using two task-specific models and full fine-tuning, respectively.\nOur experimental results demonstrate that COMFORT creates a scalable and adaptive framework for consumer health disease detection tasks using WMS data. The advantage of reduced memory consumption becomes more pronounced as more downstream tasks are learned. We simulate a scenario where the health foundation model is continually adapted to 10 distinct downstream disease detection tasks, each involving three-way classification. Fig. 8 shows the projected memory reduction achieved by adopting COMFORT with LoRA for these 10 tasks. The results show that COMFORT paves the way for personalized and proactive healthcare solutions to efficient and effective early-stage disease detection."}, {"title": "5.2 Ablation Study", "content": "We conduct two ablation studies on the COMFORT framework. First, we assess whether the pre-trained health foundation model within COMFORT reduces the amount of task-specific training data required for fine-tuning. Next, we evaluate the impact of using different rank values in the PEFT algorithms on test accuracy.\nFig. 9 presents the results of the first ablation study: test accuracy versus the amount of task-specific training data used for fine-tuning. In this ablation study, we only compare the Transformer models. The x-axis indicates the percentage of available task-specific training data, while the y-axis represents the test accuracy. The results show that COMFORT achieves its highest test accuracy with less task-specific training data, regardless of the fine-tuning method. For the DiabDeep (MHDeep) task, COMFORT only requires 40% (70%), 50% (60%), 40% (50%), and 70% (30%) of the task-specific training data for fine-tuning when using the full fine-tuning, LoRA, DORA, and CoLA methods, respectively. In contrast, the task-specific model trained from scratch requires 100% of the task-specific training data to reach its maximum test accuracy on both tasks. This ablation study demonstrates that the health foundation model in COMFORT helps reduce the amount of task-specific training data needed for fine-tuning on downstream tasks.\nFig. 10 shows the results of the second ablation study: test accuracy versus different rank values used in the PEFT algorithms. The x-axis denotes the rank values for the PEFT algorithms, whereas the y-axis represents the test accuracy. For the DiabDeep task, all three PEFT algorithms achieve their highest test accuracy at a rank value of 8. For the"}, {"title": "6. Discussions", "content": "The premise behind COMFORT is that collecting WMS data from healthy individuals is more feasible than from diagnosed patients. Moreover, we can potentially collect unlimited data from healthy individuals worldwide. Therefore, we propose foundation models for consumer healthcare applications that undergo extensive pre-training on data collected from healthy individuals. This approach mitigates the challenge associated with lack of sufficient patient data needed to train foundation models for general disease detection tasks. Our experimental results validate this premise. They demonstrate the promising potential of extending this approach to other medical data modalities, including medical text and images. The COMFORT framework can then be adapted to other smart healthcare applications, such as medical question answering, AI chatbots, image segmentation, image classification, and disease detection with medical images. We will explore these directions in our future work."}, {"title": "7. Conclusion", "content": "In this paper, we described COMFORT, a continual fine-tuning framework for foundation models in the consumer healthcare domain. It provides a scalable and adaptive solution to consumer health disease detection tasks using WMS data. We proposed a novel approach to pre-training a health foundation model with data collected exclusively from healthy"}]}