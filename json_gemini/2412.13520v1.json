{"title": "ROMAS: A Role-Based Multi-Agent System for\nDatabase monitoring and Planning", "authors": ["Yi Huang", "Fangyin Cheng", "Fan Zhou", "Jiahui Li", "Jian Gong", "Hongjun Yang", "Zhidong Fan", "Caigao Jiang", "Siqiao Xue", "Faqiang Chen"], "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in data analytics when integrated with Multi-Agent Systems (MAS).\nHowever, these systems often struggle with complex tasks that involve diverse\nfunctional requirements and intricate data processing challenges, necessitating\ncustomized solutions that lack broad applicability. Furthermore, current MAS\nfail to emulate essential human-like traits such as self-planning, self-monitoring,\nand collaborative work in dynamic environments, leading to inefficiencies and\nresource wastage. To address these limitations, we propose ROMAS, a novel Role-\nBased Multi-Agent System designed to adapt to various scenarios while enabling\nlow code development and one-click deployment. ROMAS has been effectively\ndeployed in DB-GPT [Xue et al., 2023a, 2024b], a well-known project utilizing\nLLM-powered database analytics, showcasing its practical utility in real-world\nscenarios. By integrating role-based collaborative mechanisms for self-monitoring\nand self-planning, and leveraging existing MAS capabilities to enhance database\ninteractions, ROMAS offers a more effective and versatile solution. Experimen-\ntal evaluations of ROMAS demonstrate its superiority across multiple scenarios,\nhighlighting its potential to advance the field of multi-agent data analytics.", "sections": [{"title": "1 Introduction", "content": "Multi-agent systems (MAS) have garnered significant attention for their potential to tackle complex\ntasks in dynamic environments through the coordinated collaboration of multiple agents. As task\ncomplexity and environmental dynamics increase, researchers have been working to enhance the\ncapabilities of MAS by decomposing intricate tasks into simpler subtasks and improving agents'\nadaptive, reflective, and self-correcting abilities [Anderson et al., 2018, Wang et al., 2023a, Xue\net al., 2023a, Trivedi et al., 2024]. However, despite these efforts, current MAS approaches still face\nseveral critical limitations.\nIn terms of structural design, traditional MAS often rely on static task allocation and predefined\nprocesses, such as Chain of Thought (CoT) [Wei et al., 2022], Self-consistent CoT (CoT-SC) [Wang\net al., 2023c] and Tree of Thought (ToT) [Yao et al., 2023a]. These procedural methods suffer\nfrom low fault tolerance and lack the capability for autonomous reflection and interaction, leading\nto failures when deviations occur from the predetermined plan. Furthermore, task-oriented MAS,\nexemplified by frameworks such as MetaGPT [Hong et al., 2024] and TaskWeaver [Qiao et al., 2024],\nare domain-specific and fail to generalize well beyond their predefined scopes, thus limiting their"}, {"title": "2 Related Work", "content": "Recent research in MAS has focused on three key areas: structure design, application development\nframeworks, and scenario adaptation [Wang et al., 2024a, Jiang et al., 2023, 2024]. Structure design"}, {"title": "3 Methodology", "content": "ROMAS is a versatil data analysis framework based on the DB-GPT. Its principles prioritize high\nflexibility and comprehensive functionality, without being constrained by specific scenarios, data\nformats, or development complexity. the agent roles in ROMAS are divided into a planner, a monitor,\nand multiple workers [Yong and Miikkulainen, 2009] shown in figurel, a detailed description of the\nrole definition is provided in the appendixA.\nThe ROMAS comprises three critical phases: initialization, execution, and re-planning, each\nbenefiting from the powerful database capabilities of DB-GPT. During the initialization phase,\nthe planner decomposes the requirements based on the scenario description and known database\ninformation, forming an specailized agent team and designing specific task lists for each agent [Li\net al., 2024a]. The execution phase relies on multi-agent cooperation [Du et al., 2023], with workers\ncollaborating to complete tasks according to the planner's plan. If errors occur, the system reports\nkey global information to the monitor, awaiting further instructions. In the re-planning phase, the\nmonitor and planner interact closely. If the monitor's attempts to correct the errors fail, it triggers the\nplanner to re-plan, integrating key information to assist in the planner's decision-making."}, {"title": "3.1 Initialization Phase", "content": "In this phase as illustrated in figure 2, firstly the planner automatically generate the profile prompt\nbased on templates and user input. Next, the planner will execute the self-planning process according\nto the profile. During this process, it will sequentially generate two key strategies: the cooperation\nworkflow of the agent team and the task list for each agent. Once the strategies are generated, the\nplanner will perform self-reflection process to check the effectiveness of these strategies. Finally, an\naction is executed to save the initial strategies and trigger the execution phase.\nSelf-planning process [Huang et al., 2024]. The input set <G, D, C, A, T> is defined in the profile. and\nthe output set is <AL, TL> which is the input of self-reflection process. Goal G specifies the planner's\ntask goal in detail, which involve creating the entire cooperative agent team and assigning refined\ntask lists for each worker. Description D describes the provided scenario information, including\ndescriptions from user and a brief table with raw database index information for task assigning.\nConstraint C represents the constraints that the planner must adhere to when generating the agent\nteam and task lists. Toolkit T is a set of pre-defined toolkits provided by DB-GPT like web search\ntool [Microsoft, 2023]. Multiple tools can be combined to complete specific tasks in the agent's task\nlist. Agentset A defines the range of agent roles available for generating the agent team. worker roles\nare limited to tasker, retriever, extractor, and painter. Each role has a pre-defined parent class template"}, {"title": "3.2 Execution Phase", "content": "In this phase as illustrated in figure 3, When encountering errors, workers firstly attempt to self-correct\nusing the self-reflection mechanism. If the error persists after retries, workers must report the global\nstate to the monitor and await corrective instructions. Upon receiving the urgent information from\nworkers, the monitor firstly classifies the errors. Errors can be classified into two types depending on\nnature: task list pipeline errors or agent team generation errors. For task list pipeline errors, which\ntypically indicate a problem at a specific process node and regarded as relatively simple orchestration\nissues, the monitor can directly identify the fault and propose appropriate corrective instructions to\nthe workers. For agent team generation errors, which typically indicates that there exists fundamental\nsystem issues in the planning process conducted by the planner. In this case, the monitor needs\nto conduct a comprehensive analysis of the overall information, formulate a set of improvement\nrecommendations, and trigger the planner to restart the planning process in hopes of finding a better\nsolution.\nError alert. When workers encounter a problem, they report the global state to the monitor. Due to the\nprompt length window limitation [Vaswani et al., 2023], the monitor only processes key information.\nTherefore, we require that workers encountering errors report detailed information related to both"}, {"title": "3.3 Re-planning Phase", "content": "In this phase as illustrated in figure4, the main responsibility is on the planner. The planner receives\nglobal critical information and modification recommendation from the monitor, and combines them\nwith its own historical strategies and experience information from the previous round to generate\na new strategy for the current round. To ensure the effectiveness of the new strategy and avoid\nresource wastage during the execution stage when validating its effectiveness, we introduce a strategy\ncalled gap narrow. The gap narrow strategy aims to correct the errors from the previous round at the\nminimum cost and align the inconsistencies between the current round's strategy and the previous\nround's strategy through minimal modifications. This strategy not only enhances the system's\nefficiency but also ensures the consistency and continuity of the strategies, thereby optimizing the\noverall performance. The algorithm at this stage is shown in the algorithm1."}, {"title": "Gap narrow rule", "content": "[Li et al., 2024b]. After generating the new strategy, we first use LLM to conduct\na detailed comparison between the new and old strategies, analyzing and identifying their specific\ndifferences. Next, we establish a set of prior rules aimed at minimizing modifications to the old\nstrategy. Then, we integrate comprehensive data from the monitor to perform a thorough reflection\nand correction on each difference point. This process not only ensures that each difference point\neffectively corrects the errors from the old strategy's execution but also aims to achieve the best\npossible solution with the minimum modification cost [Zhang et al., 2020]."}, {"title": "4 Experiments", "content": "Datasets. We conducted our experience based on following 2 dataset, empirical evidence has\nconsistently demonstrated that ROMAS exhibits excellent performance in both general-knowledge\nand domain-specific scenarios.\n\u2022 FAMMA [Xue et al., 2024a]. We focus on the financial data analysis scenario, a critical\napplication area for generative language models [Xue et al., 2023c, Wu et al., 2023b], to\nevaluate the capabilities of ROMAS. FAMMA is an open-source benchmark for financial\nmultilingual multimodal [Yin et al., 2024] question answering (QA) [Kapoor et al., 2024].\nTo adapt to the task requirements, we processed the original dataset by selecting 100 cases\nthat include both text and table images in the input and have standard options in the output,\nand converted these table images into tabular format.\n\u2022 HotpotQA [Yang et al., 2018]. To demonstrate reasoning capabilities in general scenarios, we\nselected 100 samples from HotpotQA. This dataset is renowned for its multi-hop reasoning\nquestions and diverse question types, which encourage models to perform cross-document\ninformation integration and complex reasoning, thereby comprehensively evaluating and\nenhancing system performance in complex reasoning tasks.\nEvaluation metrics. We select success rate, LLM evaluation [Wang et al., 2023b], and Human\nevaluation as metrics. Besides using success rate to evaluate whether the system outputs standard\nanswers, we also adopt CoT to guide the agent in outputting its reasoning process along with\nstandard options. LLM evaluation and Human evaluation are used to assess the accuracy, coherence,\ncompleteness, and logic of the descriptions [Wang et al., 2023b]. As shown in the appendixC, both\nLLM and human evaluation are scored based on a standard scale of 10 points per dimension, with\na total of 100 points. The human score is independently evaluated by multiple financial analysis experts\naccording to a unified standard, with reasons for the scores recorded and the final score averaged.\nBy combining these three metrics, we can comprehensively assess the performance of ROMAS and\nvalidate its effectiveness.\nSetup. We developed ROMAS based on GPT-4 [OpenAI et al., 2024], configuring the temperature\nto 0 to ensure the consistency of the model's outputs. For each agent, the maximum retry count for\nself-reflection and self-planning was set to 2, mitigating the risk of failure in a single invocation of\nthe LLM. The maximum retry count for the re-planning process was set to 3 to prevent exceeding\ncontext threshold.\nAnalysis I: Comparison with other LLM and MAS. As shown in table 3, we categorize the\ncomparative objects into three groups: LLMs, single-agent with task planning capabilities, and the\ncurrent advanced MAS. In the LLMs group, we selected Qwen2-72B [Yang et al., 2024], Llama2-\n70B [Touvron et al., 2023], and GPT-4, combining with single prompt technique as the baselines for\nexperiments. The result indicates that GPT-4 performs the best, likely due to its extensive training\nwith large-scale data and network parameters. In the single-agent group, we selected traditional\ndecision-making models with task planning capabilities, including CoT, ToT, and ReAct. The result\nshows that ReAct significantly improves the baseline accuracy of LLMs. This improvement is likely\nbecause the thought-act-observation [Yao et al., 2023b] process in ReAct enables a more reflective\nand reasonable task planning process. In the MAS group, we compare the pioneering generative agent,\nwhich introduces reflective thinking mechanisms in the MAS domain, and AutoAgent, which also\nfeatures role-based supervision. The result demonstrates that ROMAS outperforms the others. The\nadvantage over the generative agent could be attributed to the introduction of the monitor mechanism,\nwhich offers error correction opportunities. ROMAS surpasses AutoAgent possibly because its\ntask planning error correction occurs during the execution phase when real problems arise, rather\nthan during the drafting phase as a prediction. Furthermore, ROMAS's on-the-spot error correction"}, {"title": "Analysis II: Ablation study.", "content": "As shown in table 4, a comparison of ROMAS performance with and\nwithout the corresponding components validates the effectiveness of each component.\nOn FAMMA. The result indicates that the absence of the monitor mechanism leads to the most\nsignificant decline in ROMAS success rate, underscoring its critical role. Secondly, the self-reflection\nmodule also has a considerable impact on the system, although its influence is less than that of the\nmonitor mechanism. The reason for this may be that the self-reflection process can only accomplish\nindependent reflection by the agent itself, based solely on predefined conditions and its own data. In\ncontrast, the information from the monitor mechanism is derived from a global perspective, enabling\nit to improve the performance of individual agents by considering the overall system state, making the\ncorrection process more reliable. The memory mechanism also has a substantial impact on the overall\nperformance of ROMAS, as memory serves as a crucial basis for agents to process information in\neach round. Without classified storage and prioritization of memory, the task planning capability of\nagents would significantly decrease. The gap narrow rule has the least impact on ROMAS, indicating\nthat the success rate in the re-planning process after an initial correction is high, thus requiring\nminimal alignment operations.\nNotably, on the HotpotQA, the self-reflection and memory mechanisms have the most significant\nimpact. This disparity may be attributed to the differing inferential demands of the two datasets. The\ncomplex questions in FAMMA require precise monitoring mechanisms and self-reflection to avoid\nerrors, whereas HotpotQA's multi-hop reasoning characteristic necessitates a system with stronger\nmemory capacity to maintain consistency and integrity across multiple steps, as well as self-reflection\nto correct biases in the reasoning process."}, {"title": "Analysis III: DB-GPT effectiveness demonstration.", "content": "As shown in table 4, we validate the effec-\ntiveness of developing the ROMAS system using the DB-GPT framework through comparative"}, {"title": "Analysis IV: Argument on diversity and functionality.", "content": "As shown in figure5, the six subtask\ncategories predominantly center on DocumentQA [documentqa, 2023] and IndicatorQA, underscoring\ntheir importance in building a qa system and substantial influence on overall system performance.\nAdditionally, for domain-specific dataset FAMMA, the higher task complexity, various data, and\ncomplex reasoning necessitate more specialized subtasks, frequent self-reflection and replanning.\nConsequently, the average frequency of self-reflection and replanning is higher, and the number of\ngenerated workers is also greater."}, {"title": "5 Conclusion", "content": "In this paper, we presented ROMAS, a role-based multi-agent system designed for database monitor-\ning and planning, leveraging DB-GPT for enhanced self-monitoring, self-planning, and collaborative\ninteraction. By addressing the limitations of current multi-agent systems, ROMAS enables efficient\nand versatile deployment in complex scenarios. Through evaluations on the FAMMA dataset, we\ndemonstrated the system's effectiveness, highlighting its potential to streamline analytical tasks and\nsupport future advancements in intelligent multi-agent systems."}, {"title": "Appendices", "content": ""}, {"title": "A Role Introduction", "content": "The planner aims to decompose user requests and scenario information into clear, well-defined\nsubtasks, generates the entire specialized agent team [Chen et al., 2024], and assigns a task list for\neach agent.\nThe monitor is responsible for overseeing the entire system, ensuring global smooth operation through\ncontinuous interaction with workers and the planner. If an error occurs during execution, the monitor\nanalyzes global information to categorize this error as either task list pipeline error or agent team\ngeneration error. Depending on the type of error, monitor decides whether to correct the error itself\nor to report it to the planner for replanning.\nTo align with the typical processes of data acquiring, data cleanning, data processing, and data\nanalysing [Maharana et al., 2022] in a data analysis scenario, we have categorized workers into the\nfollowing roles:\nTasker, as the primary manager for subtasks, aims to complete subtasks by flexibly planning the\ncollaboration between extractors, retrievers, and painters. One system can abstract different types\nof taskers based on various subtasks, such as indicator tasker, document QA tasker, GraphRAG\ntasker [Peng et al., 2024], and summary tasker.\nExtractor, as a data processing assistant, aims to achieve information extraction and index storage from\nstructured and unstructured raw data. Its functions include PDF document loading, data preprocessing,\ndocument block segmentation, document tree construction, table extraction and merging, and the\nstorage of text and table data.\nRetriever, as a data acquiring helper, aims to retrieve the necessary data from the most suitable\ndatabase to support taskers. Its functions include sql generation, sql execution, python execution,\ncomposite index calculation.\nPainter, as a data analysing helper, aims to draw relevant charts based on the provided data, making it\neasier for users to intuitively understand the analysis results."}, {"title": "B Memory Mechanism of ROMAS", "content": "Sensory memory [Wang et al., 2024a] is similar to human transient memory and primarily used for\nrecording and capturing real-time sensory information interacted with environment such as one-time\nand repetitive actions of agents. Some important parts in Sensory memory is transferred to Short-term\nmemory over time.\nShort-term memory [Wang et al., 2024a] stores recent important information with limited capacity\nand duration. This type of memory temporarily holds information that requires quick access and\nprocessing, such as the agent's current self-planning and self-reflection results, agent current situations,\ncontext information, temporary strategy. Some important parts in short-term memory is transferred to\nlong-term memory over time.\nLong-term memory [Wang et al., 2024a] stores the knowledge and patterns learned by the agent\nfrom past experiences, which are used to guide future decisions and actions. In ROMAS, long-term\nmemory commonly stores agent's vital erroneous information and summaries of historical decisions.\nHybrid memory [Wang et al., 2024a] explicitly combines the advantage of short-term and long-term\nmemories, leveraging immediate data for short-term tasks while utilizing accumulated knowledge\nfor long-term strategy and learning. In ROMAS, hybrid memory is commonly used to generate the\ncurrent round strategy based on the previous round's strategy and the current round's state, such as\nin the replanning phase, the planner combines its historical strategy in long-term memory with the\nglobal state in monitor's short-term memory to generate this round's new strategy."}]}