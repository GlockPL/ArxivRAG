{"title": "CRYSTAL: Illuminating LLM Abilities on Language and Code", "authors": ["Tianhua Tao", "Junbo Li", "Bowen Tan", "Hongyi Wang", "William Marshall", "Bhargav M. Kanakiya", "Joel Hestness", "Natalia Vassilieva", "Zhiqiang Shen", "Eric P. Xing", "Zhengzhong Liu"], "abstract": "Large Language Models (LLMs) specializing in code generation (which are also often referred to as code LLMs), e.g., StarCoder and Code Llama, play increasingly critical roles in various software development scenarios. It is also crucial for code LLMs to possess both code generation and natural language abilities for many specific applications, such as code snippet retrieval using natural language or code explanations. The intricate interaction between acquiring language and coding skills complicates the development of strong code LLMs. Furthermore, there is a lack of thorough prior studies on the LLM pretraining strategy that mixes code and natural language. In this work, we propose a pretraining strategy to enhance the integration of natural language and coding capabilities within a single LLM. Specifically, it includes two phases of training with appropriately adjusted code/language ratios. The resulting model, CRYSTAL, demonstrates remarkable capabilities in both domains. Specifically, it has natural language and coding performance comparable to that of Llama 2 and Code Llama, respectively. CRYSTAL exhibits better data efficiency, using 1.4 trillion tokens compared to the more than 2 trillion tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by analyzing the training process and observe consistent improvements in most benchmarks. We also adopted a typical application adaptation phase with a code-centric data mixture, only to find that it did not lead to enhanced performance or training efficiency, underlining the importance of a carefully designed data recipe. To foster research within the community, we commit to open-sourcing every detail of the pretraining, including our training datasets, code, loggings and 136 checkpoints throughout the training.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) for code generation (i.e., code LLMs), such as Codex (Chen et al., 2021a), StarCoder (Li et al., 2023a), and Code Llama (Roziere et al., 2023), are advancing rapidly due to their strong capability in generating code-related content (e.g., functions), which helps improve the efficiency of software engineers and developers (Cognition Labs, 2024; Chen et al., 2021b; Li et al., 2023a; Roziere et al., 2023). These LLMs excel at generating functions and designing web page components based on engineers' instructions (e.g., \"Return True if all numbers in the list L are below threshold T.\") (Cal\u00f2 & De Russis, 2023). However, the abilities of code-oriented LLMs are constrained in development contexts that necessitate interpreting high-level human instructions (e.g., through prompts or function descriptions) and producing comprehensive, structured code accompanied by natural language documentation. Examples of such scenarios include solving GitHub issues (Jimenez et al., 2023), searching for code snippets based on natural language queries, generating entire Python libraries (which include their complete code along with documentation and tutorials (Liu et al., 2023a; Luo et al., 2024)), or developing source code for websites, e.g., \u201cCreate a ticketing platform for travelers\u201d (Cal\u00f2 & De Russis, 2023).\nThis underscores the ambition to create LLMs proficient in both natural language processing and coding. Achieving this goal, however, is non-trivial. For instance, Code Llama, despite being continuously pretrained with code datasets on top of Llama2, suffers from catastrophic forgetting of natural language capabilities. In open-sourced LLMs, we observe a prevalent issue: most models are tailored to specialize in either language or code, not both. For example, StarCoder is exclusively trained on code datasets accompanied by function documentation, thus limiting its exposure to varied natural language data. This trend indicates a notable gap in the design of most open-source LLMs, where there's a lack of a comprehensive curriculum that addresses both coding and natural language processing.\nTherefore, we are intrigued by the following research question: \u201cCan an LLM efficiently obtain both language and coding abilities?\u201d Existing studies have shown that the simultaneous acquisition of coding and language capabilities by LLMs is governed by complex dynamics: these skills may either conflict (Li et al., 2023a; Roziere et al., 2023) or complement (Ma et al., 2024) each other, influenced by the data recipe and the model's learning phase.\nIn this work, we propose a pretraining strategy designed specifically for code LLMs. Our strategy is inspired by techniques such as multi-phase pretraining, curriculum learning (Bengio et al., 2009), continuous pretraining (Roziere et al., 2023), and multi-language training, and has two phases. We start the pretraining process with a data mixture of 95% natural language and 5% code. In the second phase, the data mixture is enriched to include 63% code data alongside 37% natural language. This two-phase design mimics the human learning process, where the acquisition of general language knowledge precedes the development"}, {"title": "2 Related Work", "content": "Open-source LLMs. The prevailing approach to developing modern LLMs involves a two-step process: pretraining followed by fine-tuning. The extensive pretraining stage may involve using synthetic data, as demonstrated by the Phi series models (Gunasekar et al., 2023; Li et al., 2023b). However, the high-quality synthetic datasets used in the Phi models are not publicly available, whereas we aim to make all our training details public and reproducible. Additionally, much of the pretraining is conducted on vast datasets comprising trillions of tokens that encapsulate nearly all available linguistic data. Notable projects in this domain include (Wang & Komatsuzaki, 2021; Andonian et al., 2023; Zhang et al., 2022; Scao et al., 2022; Biderman et al., 2023; Touvron et al., 2023a; Geng & Liu, 2023; Together Computer, 2023a; MosaicML NLP Team, 2023; Almazrouei et al., 2023; Touvron et al., 2023b; Bai et al., 2023; Jiang et al., 2023; 01.ai, 2023; Bi et al., 2024; Groeneveld et al., 2024). Of these, Pythia (Biderman et al., 2023), LLM360/Amber (Liu et al., 2023b) and OLMO (Groeneveld et al., 2024) are particularly aligned with our work, sharing an emphasis on the complete reproducibility of LLMs. While Pythia stands out as a pioneering effort, it does not embody the recent advancements observed in training with trillions of tokens and is not specifically tailored for code. Amber and OLMo, although newer, are also designed as general-purpose models. Our CRYSTAL utilizes advanced strategic data and training strategies to create a strong open-source model that excels in coding while also demonstrating strong overall capabilities.\nCode LLMs. Applications at the core of fields such as software development engineering (Fan et al., 2023) place a significant demand on language models equipped with specialized code intelligence. Furthermore, models that are extensively trained on code datasets demonstrate enhanced reasoning capabilities and exhibit superior performance in logical tasks, including mathematics (Ma et al., 2024; Fu et al., 2022). Motivated by the needs of both practical applications and research, code-oriented large models are increasingly gaining focus (Chen et al., 2021a; Li et al., 2022; Wang et al., 2023; Luo et al., 2023; Nijkamp et al., 2022; Li et al., 2023a; Roziere et al., 2023; Guo et al., 2024). The roles of code data differ across various works. StarCoder (Li et al., 2023a) exclusively trains on code data, at the expense of general natural language understanding. DeepSeek Coder (Guo et al., 2024) incorporates more natural language into its pretraining, yet remains predominantly focused"}, {"title": "3 Model Training", "content": "Drawing on the principles of coarse-to-fine methodologies for achieving domain adaptation without catastrophic forgetting, we design two phases in the pretraining process of Crystal. In the first phase, the model is expected to acquire a broad spectrum of general language capabilities. In the second phase, we introduce coding ability into the model, ensuring that this augmentation does not compromise its existing natural language abilities. Table 1 summarizes the configurations of the phases.\nConfiguration. The architecture of CRYSTAL is adapted from prior work such as GPT-2 (Radford et al., 2019), GPT-NeoX (Andonian et al., 2023), Llama (Touvron et al., 2023a) and BTLM (Dey et al., 2023), featuring decoder-only models comprising 32 layers. The model is trained on a non-GPU hardware architecture, using the Cerebras Condor Galaxy 1 (CG-1) (Cerebras, 2023). Taking advantage of the memory layout, the model can be trained efficiently using LayerNorm without RMSNorm. We incorporate a novel enhancement known as maximal update parameterization (uP), as described by Yang et al. (2022), deciding layer-wise learning rate, batch size, Adam coefficient, etc. We include all the final hyperparameters adjusted by \u00b5P in Table 1. The training time is 37 days on 16 CS-2 nodes.\nPhase 1 for Fundamental Language Ability. In the first phase, we focus on imbuing the model with foundational natural language understanding, utilizing 345B tokens sampled from SlimPajama (Soboleva et al., 2023) dataset. This dataset, primarily composed of natural language texts, includes a modest portion (approximately 5%) of coding data sourced from GitHub, subtly introducing the model to programming concepts. We expect this phase to establish a baseline comprehension of natural language, underpinning the model's subsequent specialization in code. The rationale behind starting with natural language is inspired in curriculum learning principles, positing that mastering the intricacies of natural language is a prerequisite for tackling the structured complexity of programming languages.\nPhase 2 for Coding Ability. In the second phase, we expand the model's domain by integrating a 63% code data mixture, drawing from a broad spectrum of programming languages from the Stack (Li et al., 2023a) dataset (following the StarCoder mixture), resulting in a total of 927B tokens. This inclusion aims to transit the model from its natural language base towards a more specialized understanding of code syntax and logic. In the meanwhile, by keeping a significant 37% of general language, we intend to prevent the"}, {"title": "4 Evaluation", "content": "We conduct an extensive evaluation of CRYSTAL across multiple tasks, including language understanding, commonsense reasoning, code generation, and a newly crafted benchmark for website generation. We compare CRYSTAL with models developed around the same time and trained with a comparable number of FLOPs. However, some other open-weight models, such as Mistral (Jiang et al., 2023) and Mixtral (Jiang et al., 2024), do not disclose the size of their training data, making them unsuitable for direct comparison in studying the effect of data curriculum. For similar reasons, we also do not compare CRYSTAL with commercial endpoints.\n4.1 Evaluating Natural Language Abilities\nWe evaluate Crystal's language ability on a key set of benchmarks maintained by EleutherAI."}, {"title": "5 Analysis and Discussion", "content": "The evaluation results show that with an appropriate data curriculum, it's possible to train a model to excel in both language and code, sometimes even outperforming models with larger token budgets on certain benchmarks. In this section, we delve into an analysis of intriguing aspects observed during training.\n5.1 Effect of Language and Code Mixing in Pretraining\nIn Fig. 5, we observe that during phase transition, there is a notable drop in performance due to the abrupt change of domain on most benchmarks (ARC-C, Winogrande, and more in"}, {"title": "6 Conclusion", "content": "In this work, we present a multi-phase LLM pretraining method designed to encode natural language and coding abilities into a single model. Through this approach, we obtain a model CRYSTAL, achieving natural language and coding performance on par with Llama 2 and Code Llama, respectively. By tracking, observing and analyzing model performance throughout the pretraining, as well as a study of an additional adaptation phase, we obtain and present insights of the interplay of language and coding ability acquisition during the model training, highlighting the importance of data curriculum design.\nThough we have included careful analysis on the training process, it remains challenging to verify each design choice and explain every observed phenomenon during pretraining, largely due to constraints in computational resources. We will address the following limitation and release all our training artifacts, inviting the community to collaborate in overcoming these challenges:\nNecessity of a Smooth Transition Between Phases. Our examination in Section 5.1 reveals a slight performance drop when transitioning from Phase 1 to Phase 2 in certain benchmarks. This observation hints at the potential importance of ensuring a smooth transition between training phases. Further investigation and validation of this hypothesis could further enhance our training methodology.\nImpact of Code on Enhancing Reasoning Abilities. Notably, our second phase, which incorporates 67% code data, yielded unexpectedly high scores on the MMLU, a benchmark for natural language. This outcome suggests that structured code data may also boost language capabilities. A more definitive confirmation of this hypothesis could be obtained"}, {"title": "B Model Architecture", "content": "The Crystal language models employ a GPT-like architecture, featuring decoder-only models comprising 32 layers. We incorporate a novel enhancement known as maximal update parameterization (muP), as described by Yang et al. (2022), enabling uniformity in hyperparameters including optimization-related hyper-parameters, i.e., learning rate, batch size, Adam coefficient, etc., and initialization-related hyper-parameters across models of varying widths. This uniformity facilitates the optimization of hyper-parameters by tuning a smaller, shallow model and directly transferring the optimized settings to the original wider model. Intuitively, muP achieves this by regularizing each linear layer relative to its width, rendering updates \"independent\" of width.\n1. Input embeddings are scaled by mup_embeddings_scale.\n2. Output logits are scaled by mup_output_alpha \u00d7 mup_width_scale.\n3. Attention weights scaling is refined to division by the hidden dimension size $\\frac{QKT}{\\sqrt d}$ instead of its square root $\\sqrt d$. We find this works better under the muP setting in early experiments.\n4. Learning rates and weight decay are optimized for different parameter groups:\n\u2022 Embedding layer: LR=BASE_LR, WD=BASE_WD.\n\u2022 Normalization layers: LR=BASE_LR, WD=0.\n\u2022 Other Parameters: LR=BASE_LR \u00d7 mup_width_scale, WD=BASE_WD.\n5. Initialization ranges are determined based on muP hyperparameters.\nThe muP hyperparameters are set as follows:\n\u2022 mup_initialization_standard_deviation: 0.073\n\u2022 mup_embeddings_scale: 14.6\n\u2022 mup_output_alpha: 2.22\n\u2022 mup_width_scale: 0.0625\n\u2022 mup_base_width: 256\nFor other architecture choices:\n\u2022 We use LayerNorm instead of RMSNorm.\n\u2022 Rotary position embeddings applied to only the first 25% of hidden dimensions (Black et al., 2022), leaving the other 75% dimensions unchanged.\n\u2022 Training sequence length is 2048.\n\u2022 Embedding dimension is 32032."}, {"title": "C Finetuning Details", "content": "C.1 Prompt Format\nWe introduced four special tokens to the tokenizer and model architecture to enhance instruction handling:\n\u2022 <|sys_start | > - Marks the beginning of a system prompt.\n\u2022 <sys_end|> Marks the end of a system prompt.\n\u2022 <|im_start |> - Marks the start of an instruction message."}, {"title": "D Evaluation Details", "content": "D.1 Full Evaluation Details for Chat Models\nWe present the full evaluation results of our models and other open source models in Table 7."}]}