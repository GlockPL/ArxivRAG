{"title": "From classical techniques to convolution-based models: A review of object detection algorithms", "authors": ["FNU Neha", "Deepshikha Bhati", "Deepak Kumar Shukla", "Md Amiruzzaman"], "abstract": "Object detection is a fundamental task in computer vision and image understanding, with the goal of identifying and localizing objects of interest within an image while assigning them corresponding class labels. Traditional methods, which relied on handcrafted features and shallow models, struggled with complex visual data and showed limited performance. These methods combined low-level features with contextual information and lacked the ability to capture high-level semantics. Deep learning, especially Convolutional Neural Networks (CNNs), addressed these limitations by automatically learning rich, hierarchical features directly from data. These features include both semantic and high-level representations essential for accurate object detection. This paper reviews object detection frameworks, starting with classical computer vision methods. We categorize object detection approaches into two groups: (1) classical computer vision techniques and (2) CNN-based detectors. We compare major CNN models, discussing their strengths and limitations. In conclusion, this review highlights the significant advancements in object detection through deep learning and identifies key areas for further research to improve performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning (DL) has advanced image analysis, especially in object classification, localization, and detection tasks. In classification, the aim is to assign an image or object within it to one of several categories [1]. However, classification does not provide the object's location. Localization improves on this by identifying both the object's category and position, typically with a bounding box [2], though the precision of these boxes can vary. Object detection further extends classification and localization by detecting and classifying multiple objects in an image, providing bounding boxes for each [2]. The bounding box's top-left corner is represented by $(X_{min}, Y_{min})$, and the bottom-right by $(X_{max}, Y_{max})$, along with a label indicating the object's class as shown in Fig 1.\nObject detection has applications across fields such as medical imaging, logo detection, facial recognition, pedestrian detection, and industrial automation. However, challenges arise from image transformations like changes in scale, orientation, and lighting. While classical computer vision techniques provided a foundation, advancements in deep learning (DL), especially CNNs, have significantly improved detection performance. Modern methods use hierarchical representations, enabling object detection in complex environments with occlusions and varying scales.\nAlthough many studies have reviewed specific deep learning models or object detection applications, few provide a comprehensive overview of both classical computer vision techniques and CNN-based approaches. This paper addresses this gap by offering an analysis of both. Key contributions include:\n1) A review of classical computer vision techniques for object detection.\n2) An analysis of general region proposal generation techniques.\n3) A detailed review of convolution-based models for object detection, including two-stage and one-stage detectors.\nThe paper is organized as follows: Section 2 covers classical computer vision techniques for object detection, Section 3 discusses region proposal generation, Section 4 explores CNN-based detection architectures, Section 5 reviews applications, Section 6 lists popular datasets, Section 7 covers evaluation metrics, and Section 8 concludes with future directions."}, {"title": "II. CLASSICAL COMPUTER VISION TECHNIQUES FOR OBJECT DETECTION", "content": "Earlier computer vision techniques for image processing, particularly image similarity, relied on feature-based methods [3]-[8]. These methods focused on extracting distinctive image features to reduce computational costs while enabling robust image matching despite transformations like scaling or rotation [3]. The Scale-Invariant Feature Transform (SIFT) algorithm overcame the challenge of scaling by extracting features invariant to scale, rotation, brightness, and contrast [4]. Other feature extractors, like the Canny Edge Detector, contributed to tasks like image comparison and panoramic stitching by providing resilience to transformations and occlusions [5]. The Histogram of Oriented Gradients (HOG) technique enabled efficient image analysis by measuring gradient magnitudes and directions, creating descriptive feature vectors [6].\nTraditional object detection involves three stages:\n1) Proposal Generation: Scanning the image at various positions and scales to generate candidate bounding boxes, often using methods like sliding windows or selective search algorithms.\n2) Feature Extraction: Extracting features from the identified regions to capture relevant visual patterns.\n3) Classification: Classifying the extracted features using machine learning algorithms, such as support vector machine (SVM).\nIn 2001, Viola et al. introduced a real-time (webcam based) facial detection classifier [7]. In 2005, Dalal et al. introduced an object detector using HOG features and an SVM classifier, effective across scales but limited by pose variations [6]. In 2009, Felzenszwalb et al. improved this with the Deformable Part Model (DPM), allowing flexible parts to handle poses, though it struggled with overlapping parts in multi-person images [8].\nStudies from 2008 to 2012 on popular object detection datasets (see Section 5) showed key limitations in traditional methods. For instance, sliding windows require substantial computational resources and can generate redundant detections. Additionally, the performance of the classifier greatly impacts the results, necessitating more robust approaches."}, {"title": "III. GENERIC REGION PROPOSAL GENERATION TECHNIQUES", "content": "Object detection models integrate a bounding box regressor within the classification network to accurately locate objects [9]. Traditionally, this involves feeding cropped images to the localization network, resulting in excessive inputs. An OverFeat model enhances efficiency by using a sliding window detector within convolution layers, scanning images with a large filter and stride [10]. However, indiscriminate scanning of background regions necessitates predicting potential object locations. Methods such as interest point detection, multiscale saliency, color contrast, edge detection, and super-pixel clustering are employed for this purpose [11]-[14].\nFor instance, multiscale saliency leverages the Fast Fourier Transform to analyze features at multiple scales [11]; color contrast relies on color intensity differences [12]; edge detection identifies edges, followed by density analysis [13]; and super-pixel clustering groups similar pixels for detailed analysis [14].\nEach method has specific limitations: multiscale saliency struggles with low-contrast objects, color contrast is ineffective with minimal contrast, edge detection may produce false positives or negatives, and super-pixel clustering requires refinement. Consequently, hybrid models are often developed to improve region proposal accuracy."}, {"title": "IV. CONVOLUTION BASED OBJECT DETECTION MODELS", "content": "Object detection initially relied on manual feature design, focusing on patterns and edges. With CNN advancements, networks such as Visual Geometry Group Network (VGGNet) [15] and AlexNet [16] now autonomously extract features through convolution and pooling layers, with fully connected (FC) layers followed by a SoftMax layer for classification. For localization, the final FC layer outputs bounding box coordinates, unlike classical methods which use filters and machine learning-based models (e.g., SVMs).\nTraining CNN-based models involve adjusting weights via backpropagation to align predictions with ground truth bounding boxes. Detection models fall into two categories: (1) Two-stage detectors, which generate region proposals before classification, including R-CNN [17], Fast R-CNN [18], Faster R-CNN [19], and Mask R-CNN [20]; and (2) One-stage detectors, treating detection as direct regression or classification tasks, like YOLO [21] and SSD [22].\n\nA. Region-based Convolutional Neural Network (R-CNN)\nIn 2014, Girshick et al. introduced R-CNN, a two-stage network that combines classical techniques like selective search with CNNs for object detection [17] (see Fig. 2). R-CNN's training involves three steps:\n\u2022 Fine-tune a pre-trained network (e.g., AlexNet) on region proposals generated by selective search.\n\u2022 Train an SVM classifier for object classification.\n\u2022 Use a bounding box regressor to improve localization accuracy.\nSelective search generates around 2000 region proposals, each resized to 227x227 pixels for CNN input, reducing the computational cost of exhaustive sliding windows.\nInitially, R-CNN achieved 44% accuracy, improving to 54% after fine-tuning on warped images. Adding a bounding box regressor boosted accuracy to 58%, and using VGGNet further increased it to 66%. While nine times slower than OverFeat, R-CNN's focus on region proposals reduces false positives, improving accuracy by 10%.\nHowever, R-CNN has some limitations:\n\u2022 Feature extraction is performed independently for each proposal, resulting in high computational costs.\n\u2022 The separate stages of proposal generation, feature extraction, and classification prevent end-to-end optimization.\n\u2022 Selective search relies on low-level visual features, struggles with complex scenes, and does not benefit from GPU acceleration.\n\u2022 Despite higher accuracy compared to methods like OverFeat, R-CNN is slower due to these inefficiencies.\nB. Spatial Pyramid Pooling-Net (SPP-Net)\nIn 2015, He et al. introduced SPP-Net to improve detection speed and feature learning over R-CNN [23]. Unlike R-CNN, which processes each cropped proposal individually, SPP-Net computes the feature map for the entire image and then applies a Spatial Pyramid Pooling (SPP) layer to extract fixed-length feature vectors (See Fig. 3). The SPP layer divides the feature map into grids of varying sizes (N \u00d7 N), enabling pooling at multiple scales and concatenation of the resulting feature vectors.\nSPP-Net allows multi-scale and varied aspect ratio handling without resizing, preserving image details and improving both accuracy and inference speed over R-CNN. However, its multi-stage training hinders end-to-end optimization and requires extra memory for feature storage. Additionally, the SPP layer does not back-propagate to earlier layers, keeping parameters fixed before the SPP layer and limiting deeper learning.\nC. Fast Region-based Convolutional Neural Network (Fast R-CNN)\nIn 2015, Girshick et al. introduced Fast R-CNN, a two-stage detector designed to improve on SPP-Net's limitations [18]. Fast R-CNN computes a feature map for the entire image and uses a Region of Interest (ROI) Pooling layer to extract fixed-length features from each region, dividing proposals into a fixed N \u00d7 N grid. Unlike SPP, ROI Pooling backpropagates error signals, enabling end-to-end optimization.\nAfter feature extraction, features pass through FC layers, outputting (1) SoftMax probabilities for C+1 classes (including background) and (2) four bounding box regression parameters. Fast R-CNN achieved better accuracy than R-CNN and SPP-Net but still relied on traditional proposal methods.\nD. Faster Region-based Convolutional Neural Network (Faster R-CNN)\nIn 2015, Girshick et al. introduced Faster R-CNN, which utilizes the Region Proposal Network (RPN) to generate object proposals at each feature map position using a sliding window approach (Fig. 4) [18]. This method shares feature extraction across regions, enhancing efficiency and achieving state-of-the-art results. However, the separate computation for region classification can be inefficient with many proposals, and reliance on a single deep feature map makes detecting objects of varying scales difficult, as deep features are semantically strong but spatially weak, while shallow features are spatially strong but semantically weak.\nE. Mask R-CNN\nIn 2017, He et al. introduced Mask R-CNN, an extension of Faster R-CNN that performs pixel-level instance segmentation [20]. It adds a new branch for binary mask prediction to the two-stage pipeline, alongside class and box predictions. This branch uses a fully convolutional network (FCN) atop the CNN feature map. Mask R-CNN also replaces RoIPool with RoIAlign to better preserve spatial accuracy, enhancing mask precision. However, it struggles to detect objects with motion blur in low-resolution images.\nF. You Only Look Once (YOLO)\nTo increase speed, one-stage models like YOLO (You Only Look Once) were developed, bypassing region proposals. Introduced in 2015 by Redmon et al., YOLO treats detection as a regression task [21]. Dividing the image into an S \u00d7 S grid, YOLO predicts class probabilities, bounding boxes, and confidence scores per cell. This captures context well, reducing false positives, but the grid structure can cause localization errors and struggles with small objects.\nYOLO has undergone several iterations, enhancing its performance:\n\u2022 YOLOV2/YOLO9000 (2017): Introduced batch normalization and anchor boxes for improved speed and accuracy [24].\n\u2022 YOLOv3 (2018): Added multi-scale predictions and residual connections for better detection across various sizes [25].\n\u2022 YOLOv4 (2020): Enhanced with the CSPDarknet backbone and advanced training techniques, achieving higher precision [26].\n\u2022 YOLOv5 (2021): Focused on usability, scalability, and deployment flexibility with various model sizes [27].\n\u2022 YOLOv6 (2022): Optimized for edge devices with improved backbone and attention mechanisms [28].\n\u2022 YOLOv7 (2023): Employed AutoML techniques for dynamic model optimization, enhancing adaptability [29].\n\u2022 YOLOv8 (2023): Incorporated a transformer-based backbone for better detection in dense scenes [30].\n\u2022 YOLOv9 (2024): Utilized adversarial training to improve robustness against variations [31].\n\u2022 YOLOv10 (2024): Implemented real-time feedback loops for dynamic adjustments, boosting accuracy [32].\nThese enhancements have established YOLO as a versatile and powerful option for real-time object detection.\nG. Single Shot MultiBox Detector (SSD)\nThe Single Shot MultiBox Detector (SSD), introduced by Liu et al. in 2016, is a one-stage model that improves on YOLO by using anchors with multiple scales and aspect ratios within each grid cell [22]. Each anchor is refined by regressors and assigned probabilities across categories, with object detection predicted on multiple feature maps for different scales. SSD trains end-to-end with a weighted localization and classification loss, integrating results across maps. Using hard negative mining and extensive data augmentation, SSD matches Faster R-CNN's accuracy while allowing real-time inference."}, {"title": "V. APPLICATIONS", "content": "Object detection, powered by CNN, has diverse applications, spanning from targeted advertising to self-driving cars and beyond. It is utilized for handwritten digit recognition, Optical Character Recognition (OCR), face detection, medical image analysis, sports analytics, and more.\n\u2022 Optical Character Recognition (OCR): OCR converts images of text into machine-encoded text, facilitating tasks such as document digitization, automated data entry, and cognitive computing.\n\u2022 Self-Driving Cars: Object detection is essential for autonomous vehicles to detect and classify objects such as cars, pedestrians, traffic lights, and road signs.\n\u2022 Object Tracking: Used in tracking objects in videos, object detection has applications in surveillance, traffic monitoring, and sports analytics.\n\u2022 Face Detection and Recognition: Widely employed in computer vision, object detection is used for social media, image tagging and biometric security systems.\n\u2022 Object Extraction from Images or Videos: Facilitates segmentation and meaningful representation of images, potentially enabling applications like video object extraction.\n\u2022 Digital Watermarking: Embed markers into digital signals for copyright protection and authentication purposes.\nMedical Imaging: Assists clinicians in diagnosis and therapy planning, particularly in tracking anatomical objects.\nObject detection technology continues to evolve, promising further advancements and expanding its applications across various industries."}, {"title": "VI. POPULAR DATASET", "content": "Key datasets in object detection include Pascal VOC [33], COCO [34], ImageNet [35], and Open Images [36]. Pascal VOC (Visual Object Classes) offers a manageable size, balancing complexity and computational efficiency, making it ideal for testing. COCO (Common Objects in Context) provides extensive annotations with multiple objects per image, including segmentation and key points. ImageNet, primarily used for classification, also includes object detection annotations. Open Images, with over 600 labeled categories, stands out for its large scale, offering both bounding box annotations and segmentation masks."}, {"title": "VII. EVALUATION METRICS", "content": "Object detection models are assessed using several key metrics: Intersection over Union (IoU), Mean Average Precision (mAP), Precision, Recall, Confidence Score (CS), F1 Score, and Non-Maximum Suppression (NMS).\nB. Mean Average Precision (mAP)\nmAP evaluates model performance by averaging the precision across all classes. The Average Precision (AP) is computed as:\n$AP = \\frac{\\sum_{k=1}^{n} (P(k) \\times Precision \\ at \\ Recall(k))}{n}$\nwhere $P(k)$ is the change in recall from the previous highest recall, and precision at recall k is the maximum precision observed at any recall level j where $j > k$.\nC. Precision and Recall\nPrecision is the ratio of true positives to all positive predictions, while Recall is the ratio of true positives to all ground truth positives.\nD. Confidence Score (CS)\nThe Confidence Score reflects the model's certainty that a predicted bounding box contains the correct object. Higher scores indicate greater accuracy and help set thresholds for accepting or rejecting detections.\nE. Non-Maximum Suppression (NMS)\nNon-Maximum Suppression refines bounding box predictions by sorting them by confidence scores and selecting the highest one while suppressing overlapping boxes. This process ensures each object is detected once, improving accuracy and efficiency.\nA. Intersection over Union (IoU)\nIoU measures the overlap between the predicted and ground truth bounding boxes, calculated as the ratio of the intersection area to the union area:\n$IoU = \\frac{Area \\ of \\ Intersection}{Area \\ of \\ Union}$"}, {"title": "VIII. DISCUSSION AND FUTURE DIRECTIONS", "content": "This review examined prominent object detection models, classifying them into classical computer vision techniques and CNN-based methods. While recent CNN architectures have significantly improved accuracy to below 5%, they also increase complexity and resource demands. Traditional models like Deformable Part Models (DPMs) are shallower and more lightweight, making them better suited for edge deployment compared to modern deep learning architectures like AlexNet and VGGNet.\nKey future directions for object detection include:\n\u2022 Speed-Accuracy Trade-off: Enhancing both accuracy and speed for real-time, low-power applications.\n\u2022 Tiny Object Detection: Improving the detection of small objects in areas such as wildlife monitoring and medical imaging.\n\u2022 3D Object Detection: Leveraging 3D sensors for applications in augmented reality and robotics.\n\u2022 Multi-modal Detection: Integrating visual and textual sources for better accuracy in complex scenarios.\n\u2022 Few-shot Learning: Developing models that can effectively detect objects from limited examples, particularly in low-resource settings.\nThis review aims to foster interest in advancing object detection models and to inspire innovation to address current limitations, including minimizing environmental impacts."}]}