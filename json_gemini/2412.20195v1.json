{"title": "Lower bounds on transformers with infinite precision", "authors": ["Alexander Kozachinskiy"], "abstract": "In this note, we use the VC dimension technique to prove the first lower bound against one-layer softmax transformers with infinite precision. We do so for two tasks: function composition, considered by Peng, Narayanan, and Papadimitriou, and the SUM2 task, considered by Sanford, Hsu, and Telgarsky.", "sections": [{"title": "1 Introduction", "content": "Hahn [4] initiated the study of lower bounds for the transformer architecture. A lower bound is a result of the form that for a certain task, with a certain number of parameters, there exists no choice of the parameters for which the transformer performs this task. Such results are meant to theoretically explain the poor performance of transformers at certain tasks.\nHahn obtained the first lower bounds against a theoretical model of trans- formers with hardmax attention. In this model, instead of taking a convex combination of all input tokens using softmax, one simply takes a single token where the attention is maximal. Hahn has shown that hardmax transformers with O(1) layers are not able to compute parity, majority, and Dyck languages. It was later proved by Hao, Angluin, and Frank [5] that hardmax transform- ers with O(1) layers are upper bounded by the complexity class AC\u00ba. This allows to re-establish lower bounds of Hahn since parity, majority, and Dyck are well-known to be outside ACO.\nProving lower bounds against O(1)-layer softmax transformers is notoriously hard; as was recently observed by Chen, Peng, and Wu [2], such transformers simulate constant-depth symmetric circuits, lower bounds for which seem to require a break-through in complexity theory. They have announced a lower bound against a constant number of layers of a decoder-only architecture that avoids this barrier.\nPrevious works [6, 7, 1] have developed a technique against 1-layer softmax transformers where the output is computed in one of the tokens after the layer."}, {"title": "2 The model", "content": "Fix an alphabet \u2211 and the input length n. We are interested in computing functions of the form $f : \u03a3^n \u2192 {0,1}$. We consider three examples:\n\u2022 The composition function, denoted by Comp for a given $n \u2208 N$. For this function, $\u03a3 = {1, ..., n}$. To define $Comp_n(a_1,..., a_n)$ for $(a_1,..., a_n) \u2208 {1,..., n}^n$, we do the following. Let $\u03c6 : {1,..., n} \u2192 {1, ..., n}$ be such that $\u03c6(1) = a_1,..., \u03c6(n) = a_n$. Define\n$Comp_n (a_1...a_n) = \\begin{cases} 1 & \u03c6((1)) = 1, \\\\ 0 & otherwise. \\end{cases}$"}, {"title": "3 Proofs", "content": "Theorem 1. There is no 1-layer single-token output transformer with embed- ding dimension $n^{o(1)}$ and output MLP with $n^{o(1)}$ ReLU neurons that computes $Comp_n$.\nProof. Assume for contradiction that such transformer T exists. We consider only inputs to Compn of the form $(a_1,b_2,...,b_n)$, where $a_1 \u2208 {2,...,n}$ and $b_2, ..., b_n \u2208 {1,2}$. Observe that $Comp_n (a_1b_2 . . . b_n) = 1$ if and only if $b_{a_1} = 1$.\nDefine value vectors of input tokens for such an input:\n$f_1 = p(1,a_1), f_2 = p(2, b_2), . . ., f_n = p(n,b_n)$.\nObserve that:\n$h= \\frac{e^{\\langle Kf_1,Qh \\rangle} f_1 + ... + e^{\\langle Kf_n,Qh \\rangle} f_n}{e^{\\langle Kf_1,Qh \\rangle} +...+e^{\\langle Kf_n,Qh \\rangle}}= \\frac{x + y}{p+q}$,\nwhere\n$x = x(a_1) = e^{\\langle Kf_1,Qh \\rangle} f_1 \u2208 R^d$,\n$p = p(a_1) = e^{\\langle Kf_1,Qh \\rangle} \u2208 R$,\n$\u1ef3 = \u1ef9(b_2 ...b_n) = e^{\\langle Kf_2,Qh \\rangle} f_2 + ... + e^{\\langle Kf_n,Qh \\rangle} f_n \u2208 R^d$,\n$q = q(b_2... b_n) = e^{\\langle Kf_2,Qh \\rangle} + ... + e^{\\langle Kf_n,Qh \\rangle} \u2208 R$.\nThe output of the transformer is computed as\n$T(a_1b_2...b_n) = sign (N(h+\\frac{x + y}{p+q}))$,\nwhere $N$ is the output MLP. Consider $F(x, p, \u1ef9, q) = sign (N(h+\\frac{x + y}{p+q}))$ as a parametric family of functions, where $(x,p)$ are the inputs and $(\u1ef9, q)$ are parameters. It defines a hypothesis class:\n${h_{\u1ef9,q}: R^{d+1} \u2192 {0,1} : h_{\u1ef9,q}(x,p) = F(x,p, \u1ef9,q), (\u1ef9, q) \u2208 R^{d+1}} $. (1)\nAssuming for contradiction that $d = n^{o(1)}$ and the size of N is $n^{o(1)}$, we first show that (1) has VC dimension $n^{o(1)}$. Indeed, it has $d + 1 = n^{o(1)}$ pa- rameters, and $h_{\u1ef9,q}(x,p)$ can be computed in $n^{o(1)}$ basic arithmetic operations and conditional jumps, based on comparing a real number with 0. Indeed, we need d + 1 additions to compute $x + y$ and $p + q$, then one division, and then $n^{o(1)}$ additions, products, and conditional jumps to calculate all ReLU neurons of N. By Theorem 2.3 in [3], VC dimension is polynomial in these quantities (the number of parameters, the number of arithmetic operations and conditional jumps), which gives VC dimension $n^{o(1)}$.\nWe obtain a contradiction by showing that if T computes Compn, the VC dimension of (1) must be at least n \u2212 1. Namely, we show that (1) shatters the following n \u2212 1 inputs:\n$(x_1,p_1) = (x(2), p(2))$\n:\n$(x_{n-1},p_{n-1}) = (x(n), p(n))$\nFor any \u03b4: {1, ..., n \u2212 1} \u2192 {0,1}, we show the existence of $(\u1ef9, q) \u2208 R^n$ such that:\n$h_{\u1ef9,q}(x_i, p_i) = \u03b4(i), i = 1, ..., n \u2013 1."}, {"title": null, "content": "Namely, set $b_{i+1} = 1$ if $\u03b4(i) = 1$ and $b_{i+1} = 2$ if $\u03b4(i) = 0$ for $i = 1, . . ., n \u2212 1$. Define $(\u1ef9, q) = (\u1ef9(b_2... b_n), q(b_2...b_n))$. Observe that for every $i \u2208 {1, ..., n \u2212 1}$, we have:\n$h_{\u1ef9,q}(x_i, p_i) = T(i +1, b_2...b_n)= \\begin{cases} 1 & b_{i+1} = 1, \\\\ 0 & otherwise, \\end{cases}= \u03b4(i)$,\nas required.\nTheorem 2. There is no 1-layer single-token output transformer with embed- ding dimension $n^{o(1)}$ and output MLP with $n^{o(1)}$ ReLU neurons that computes Sum.\nProof. Assume for contradiction that such transformer T exists. Denote $k = n/2$ and consider any two binary vectors \u03b1, \u03b2 \u2208 {0,1}k. Define two sequence of integers $a_1,..., a_k, b_1, ..., b_k \u2208 {\u2212n, ..., n}$ by:\n$a_i = \\begin{cases} 2i & \u03b1_i = 1, \\\\ 1 & \u03b1_i = 0, \\end{cases} , b_i = \\begin{cases} -2i & \u03b2_i = 1, \\\\ 1 & \u03b2_i = 0, \\end{cases}$,\nfor i = 1,...,k. Observe that $Sum_{2^n} (a_1 . . . a_kb_1 . . . b_k) = 1$ if and only if there exists $i \u2208 {1, ...,k}$ such that $\u03b1_i = \u03b2_i = 1$. We consider our transformer T on inputs of the form $a_1 . . . a_kb_1...b_k$. We start by writing the values of the input tokens:\n$f_1 = p(1, a_1), ..., f_k = p(2,a_k), f_{k+1} = p(k + 1, b_1), . . ., f_n = p(n,b_n)$,\nand then writing:\n$h= \\frac{e^{\\langle Kf_1,Qh \\rangle} f_1 + ... + e^{\\langle Kf_n,Qh \\rangle} f_n}{e^{\\langle Kf_1,Qh \\rangle} + ... + e^{\\langle Kf_n,Qh \\rangle}} = \\frac{x + y}{p+q}$,\nwhere\n$x = x(a) = e^{\\langle Kf_1,Qh \\rangle} f_1 + ... + e^{\\langle Kf_k,Qh \\rangle} f_k \u2208 R^d$,\n$p = p(a) = e^{\\langle Kf_1,Qh \\rangle} + ... + e^{\\langle Kf_k,Qh \\rangle} \u2208 R\n\u1ef3 = \u1ef9(\u03b2) = e^{\\langle Kf_{k+1},Qh \\rangle} f_{k+1} + ... + e^{\\langle Kf_n,Qh \\rangle} f_n \u2208 R^d$,\n$q = q(\u03b2) = e^{\\langle Kf_{k+1},Qh \\rangle} + ... + e^{\\langle Kf_n,Qh \\rangle} \u2208 R$.\nAs in the previous proof, we define a parametric family of functions $F(x, p, \u1ef9, q) = sign (N(h+\\frac{x + y}{p+q}))$, and consider a hypothesis class, based on it:\n${h_{\u1ef9,q}: []^d+1 \u2192 {0,1} : h_{\u1ef9,q}(x,p) = F(x,p, \u1ef9, q), (\u1ef9, q) \u2208 R^{d+1}}$. (2)\nBy the same argument, its VC dimension is $n^{o(1)}$ based on the fact that d and the size of Nare $n^{o(1)}$. We now obtain a contradiction by showing that if T"}, {"title": null, "content": "computes Sum\", the VC dimension of (2) is at least k = n/2. Namely, we show that (2) must shatter the following k inputs:\n$(x_1,p_1) = (x(100 . . . 0), p(100 ...0))$\n$(x_2, p_2) = (x(010...0),p(010...0))$\n:\n$(x_k, p_k) = (x(000 . . . 1), p(000 . . . 1))$.\nConsider any binary word $\u03b2\u2208 {0,1}^k$ that we want to realize as the sequence of values on $(x_1,p_1),..., (x_k, p_k)$. Notice that $F(x_i, p_i, \u1ef9(\u03b2), q(\u03b2))$ is equal to the value of the transformer on input, constructed as above from two binary vectors, one being the vector with the unique 1 at position i, and the other being B. By our construction, the output is 1 if and only if $\u03b2_i = 1$. Thus, we notice that the sequence of values \u03b2 can be realized by the hypothesis $h_{\u1ef9,\u0105}$ for $(\u1ef9, q) = (y(\u03b2), q(\u03b2))$.\nTheorem 3. There exists a 1-layer single-token output transformer with embed- ding dimension O(1) and output MLP with O(1) ReLU neurons that computes Palindromen.\nProof sketch. Assume on input we have a binary word $a_1 ... a_kb_k... b_1$. It is a palindrome if and only if $a_i = b_i, ..., a_k = b_k$. It is easy to construct a softmax layer that computes a number, proportional to:\n$(a_1 \u2013 b_1) + 10^{-1}(a_2 \u2212 b_2) + . . . + 10^{-k+1}(a_k \u2013 b_k)$.\nThis number is 0 if and only if the initial word is a palindrome, and this can be checked by a constant-size MLP.\nIt is curious, why the VC dimension lower bound technique works for Sum2 but not for Palindromen. For both of these functions, lower bounds for $n^{o(1)}$- precision transformers can be proved via reductions from communication com- plexity from the disjointness problem for Sum2 and from the equality problem for Palindromen. The key factor is that the VC dimension of the communication matrix of the disjointness problem is n for n-bit strings, while for the equality problem it is low, just 1.\"\n    }"}]}