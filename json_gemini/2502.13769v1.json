{"title": "A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem", "authors": ["Juan A. Aledo", "Jos\u00e9 A. G\u00e1mez", "Alejandro Rosete"], "abstract": "In rank aggregation problems (RAP), the solution is usually a consensus ranking that generalizes a set of input orderings. There are different variants that differ not only in terms of the type of rankings that are used as input and output, but also in terms of the objective function employed to evaluate the quality of the desired output ranking. In contrast, in some machine learning tasks (e.g. subgroup discovery) or multimodal optimization tasks, attention is devoted to obtaining several models/results to account for the diversity in the input data or across the search landscape. Thus, in this paper we propose to provide, as the solution to an RAP, a set of rankings to better explain the preferences expressed in the input orderings. We exemplify our proposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists in finding a single consensus ranking (with ties) that generalizes a set of input rankings codified as a precedence matrix. To address this, we introduce the Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP that aims to produce not a single ranking as output but a set of consensus rankings. Experimental results are presented to illustrate this proposal, showing how, by providing a set of consensus rankings, the fitness of the solution significantly improves with respect to the one of the original OBOP, without losing comprehensibility.", "sections": [{"title": "1 Introduction", "content": "Rankings are ordered lists of items generated by a human or machine-based voter. They find applications in a wide range of disciplines, from search engines that organize results by relevance, to sports leagues that establish team standings, and businesses that prioritize products or services. In scenarios where multiple rankings are provided (each reflecting distinct criteria or viewpoints), combining these into a single, representative ranking becomes a complex challenge. This task, known as ranking aggregation, involves reconciling the differences among individual lists, addressing conflicts such as varying priority orders, and balancing criteria to accurately represent the collective preferences or priorities. Effective ranking aggregation techniques are essential for producing fair and meaningful results in applications ranging from social choice theory and recommendation systems to meta-search engines and decision-making processes.\nReal-world applications of rank aggregation arise in fields such as social choice theory, voting, consensus decision-making, hiring and recruitment processes, meta-search, seriation, web browsing patterns, and more. Many of these applications entail the ranking of sensitive information, such as political candidates or job applicants, which raises issues of bias and fairness that ranking algorithms must address. Recently, rank aggregation approaches have been developed to consider the existence of protected attributes like gender, race, or religion, resulting in groups that should be taken into account when generating the consensus ranking [7, 19]. The main idea in these cases is to incorporate fairness-based metrics that constrain the output, ensuring that a fair representation of each group appears within the top-k positions of the consensus ranking (e.g. proportional fairness [5] or equal opportunity [12]). In practice, existing approaches aim to produce a fair consensus ranking, which meets fairness requirements while minimizing the distance to the consensus ranking that would be obtained without fairness considerations [4, 15] or implement postprocessing techniques on the obtained unfair consensus [11]. Additionally, strategic weight assignments in the aggregation process have been employed to mitigate recommendation bias issues [8], along with methods that account for the attentive influence of preference users [22]. All these approaches produce a single ranking as a response, which summarizes the precedences provided in the input according to diverse criteria.\nMathematically, a rank aggregation problem (RAP) consists in finding the consensus order \u03c0_0 of n items which best aligns with a set of preferences \u03a0 for those items (for instance a set of rankings, a set of pairwise preferences or a precedence matrix) according to a specific criterion or distance measure d, i.e.\n\u03c0_0 = argmin d(\u03a0, \u03c0),\nwhere \u03c0 belongs to a set of orderings of these items. There are many variants of RAP, which differ in terms of the types of rankings used as input and output, as well as in the objective function employed to evaluate the quality of the output ranking. In fact, the rankings involved, both in the input and output, may be either complete (ranking all n items) or incomplete (ranking only a subset of the items) and may allow for ties (items between which there is no preference) or not. Furthermore, a wide range of distance measures can be used to assess the quality of the output (Kendall's Tau Distance, Spearman's Rank Correlation, Hamming Distance, etc.) [16].\nTypically, rank aggregation problems aim to identify a single consensus order. In contrast, some machine learning and optimization tasks, e.g., subgroup discovery [13], multiple solutions identification [17] and multimodal optimization [23] tasks, focus on obtaining multiple models or results to better capture the diversity present in the input data or the search landscape. Accordingly, in this paper, we propose providing a set of rankings as the solution to an RAP to more effectively reflect the range of preferences within the input orderings. Specifically, for an RAP with input set of preferences \u03a0, a distance measure d, and a natural number b \u2265 2, we can define the generalized RAP, which consists in finding b rankings \u03c0_1, \u03c0_2, . . . \u03c0_b and b weights w^1, w^2, . . . w^b \u2208 [0, 1], \u03a3_{k=1..b} w^k = 1, achieving the best fitness value\n\u03a3_{k=1}^{b} w^k d(\u03a0, \u03c0_k)\nTo illustrate this proposal, we deal with the Optimal Bucket Order Problem (OBOP) [10, 21], a distance-based rank aggregation problem where the input is a pair order matrix C of order n which (usually) codifies the precedence relations between n items, and the output is a bucket order, i.e., a complete ranking with ties (see Section 2.2). As explained above, we extend this ranking aggregation framework to account for the underlying presence of different groups within the set of voters or in their stated preferences (pair order matrix). Thus, we define the Optimal Set of Bucket Orders Problem (OSBOP) which, instead of constraining the consensus ranking with pre-determined fairness criteria, aims to produce a set of bucket orders, each assigned a weight to reflect its significance. The idea is that each learned bucket order can represent the consensus ranking for a community within the population from which the input preferences were gathered, with the goal that the weighted linear combination of these bucket order matrices minimizes the distance to the input pair order matrix (see Section 3). Note that our approach differs from performing a clustering process to identify groups (e.g. [14]) and then solving an OBOP for each group individually. The distinction is twofold: first, in our proposal, weights and bucket orders are sought simultaneously to minimize the distance to the input pair order matrix. Second, if the input to our problem is directly a precedence matrix"}, {"title": "2 Set up", "content": "A bucket order B (a.k.a. partial ranking or complete ranking with ties) of n elements is an ordered partition of [[n]] = {1, 2, . . ., n} [9, 10, 21], i.e. a linear ordering of buckets B_1, B_2,..., B_k \u2286 [[n]], 1 \u2264 k \u2264 n, with U_{i=1}^{k} B_i = [[n]] and B_i \u2229 B_j = \u2205, i \u2260 j. We denote by S_n the set of bucket orders of the elements in [[n]].\nFor two buckets B_i, B_j in B, if B_i precedes B_j according to the bucket order B, we write B_i <_B B_j. This relation also apply to the items included in the buckets. Specifically, if u \u2208 B_i, v \u2208 B_j and B_i <_B B_j, then we write u <_B v. On the other hand, if two items u, v are tied (i.e. they belong to the same bucket), we write u ~_B v.\nWe represent a bucket order as a list of the items in [[n]], where the items in the same bucket are separated by commas (,) and the bar (|) indicates the separation between buckets. For example, 1,3|2, 4 represents a bucket order of the elements in [[4]] with two buckets: the first consisting of items 1 and 3, followed by the second containing items 2 and 4. This implies that 1 and 3 are considered equally preferred (tied), as are 2 and 4, with 1 and 3 being preferred over 2 and 4.\nWe will identify a bucket order B with a square matrix n \u00d7 n called bucket matrix B [10, 21], where B(u, v) = 1 if u <_B v, B(u, v) = 0 if v <_B u, and B(u, v) = 0.5 if u ~_B v. The elements in the main diagonal of B are equal to 0.5, and B(u, v) + B(v, u) = 1 for all u, v \u2208 [[n]], u \u2260 v. On the other hand, a pair order matrix C of order n is a square matrix n \u00d7 n satisfying that C(u, v) \u2208 [0, 1], C(u, v) + C(v, u) = 1 and C(u, u) = 0.5 for all u, v \u2208 [[n]]. Note that, in general, a pair order matrix is not a bucket order matrix because the transitivity property typically fails to hold."}, {"title": "2.2 The Optimal Bucket Order Problem (OBOP)", "content": "In this section, we briefly revise the Optimal Bucket Order Problem (OBOP) (see [10, 21]). The OBOP is a distance-based rank aggregation problem whose input is a pair order matrix C of order n which (usually) codifies the precedence relations between the items in [[n]], whereas the output is a bucket order in S_n.\nSpecifically, given a pair order matrix C of order n, the OBOP consists in finding a bucket order B, and more precisely a bucket matrix B, which minimizes the distance\nD(B,C) = \u03a3_{\u03ba,\u03c5\u03b5[[\u03b7]]} |B(u, v) \u2013 C(u, v)|.\nAs the OBOP is an NP-Complete problem [10], various heuristic and greedy approaches have been developed to address it (e.g. [2, 3, 21]).\nIn [1] the concept of utopian matrix for the OBOP was introduced as follows. Given a pair order matrix C of order n, the associated utopian matrix U_C is then\u00d7n matrix\nU_C(u, v) = \u03a5(C(u, v)),   \u03c5, \u03c5 \u2208 [[n]],"}, {"title": "3 The Optimal Set of Bucket Orders Problem (OSBOP)", "content": "In this section, we formalize the Optimal Set of Bucket Orders Problem (OSBOP) as a generalization of the OBOP.\nGiven b bucket matrices B_1, B_2, . . ., B_b and real numbers w^1, w^2, . . . w^b \u2208 [0, 1], \u03a3_{\u03ba=1}^{b} w^k = 1, we will denote by B[(B_1, ..., B_b), (w^1, . . ., w^b)] the linear combination of the matrices {B_K} according to the weights {w^k}, namely\nB[(B_1,..., B_b), (w^1, ..., w^b)] = \u03a3_{k=1}^{b} w^k B_k.\nDefinition 3.1 (OSBOP). Given a pair order matrix C and a positive integer b, the b-th Optimal Set of Bucket Orders Problem (OSBOP') consists in finding w^1, w^2, ...w^b \u2208 [0,1], \u03a3_{k=1..6} W^k = 1, and b (different) bucket matrices B_1, B_2, . . ., B_b that minimize the distance\nf_C(B) = D(B[(B_1, ..., B_b), (w^1, . . ., w^b)], C)\n= \u03a3_{\u03ba,\u03c5\u03b5[[\u03b7]]} |B[(B_1,..., B_b), (w^1, ..., w^b)] (u, v) \u2013 C(u, v)|.\nThus, in contrast to the OBOP where the solution is a single bucket order, for the OSBOP the solution consists of a set of bucket orders and a vector of weights that express the relative importance of each bucket order in the set."}, {"title": "3.1 Size of the solution space", "content": "The size of the solution space of the OSBOP' is\n|S_{S_n}^{b}| = {S_n \\choose b} = {S_n! \\over b!(|n| - b)!},\nwhere the cardinal of S_n is determined by the Fubini number F(n) (see, for instance, [2, Section 3] for the details). In the particular case of the OSBOP1 (i.e. OBOP), the size of the solution space is |S_{S_n}|. In Figure 1 we show, in polynomial scale, the sizes of the solution spaces of the OSBOP' for 1 < b < 4 and 2 \u2264 n \u2264 10. We also show the number of permutations of n elements, P(n) = n!, which corresponds to the size of the space of the OSBOP1 when no ties are allowed in the output ranking."}, {"title": "3.2 The utopian matrix for OSBOP", "content": "In order to extend the definition of utopian matrix to the OSBOP, b \u2265 2, note that the entries of a linear combination of b bucket matrices with weights 1/b are in the set\n{0, {1 \\over 2b}, {2 \\over 2b}, ..., {2b-1 \\over 2b}, {2b \\over 2b} = 1}."}, {"title": "4 A simple metaheuristic approach for OS-BOP", "content": "In this section we present SLS-OSBOP, a stochastic local search procedure to solve the OSBOP in the case where the exhaustive exploration is computationally intractable. The search is structured in two levels. In an outer level, the method searches for a vector of b bucket orders (s_c), while in an inner level a numerical optimization procedure is carried out to tune the weights (w_c) for the bucket orders in s_c.\nThe method, outlined in Figure 2, takes as input the precedence matrix C, the number b of desired rankings (bucket orders) in the output, a maximum number of iterations t_1 for the bucket orders search, a maximum number of iterations t_2 for the weights tuning procedure, and the binary variable Eq to set whether all the rankings in the output have equal weight (Eq=yes) or if different weights are allowed (Eq=no).\nSLS-OSBOP firstly generates an initial (random) candidate solution s_c and takes a vector of weights w_c with equal value 1/b for all the rankings in the solution (Steps 1-2). Next, the candidate solution s_c is evaluated (Step 3). Then, if Eq=no, an inner local search procedure TUNEWEIGHTS(C,s_c,w_c,f_c,t_2) is executed to carry out random changes in the weights, returning those that produce the best (minimum) evaluation value. The best vector of weights obtained is taken as the weight w_c of the current solution s_c and the evaluation f_c is updated (Step 4).\nSubsequently, the main (outer) search iterates t_1 times to improve the current solution s_c (Steps 5-15). In order to do so, in each iteration a new solution s'_c is generated by mutating s_c (Step 6). This solution s'_c undergoes in Steps 6-8 the same procedure carried out by s_c in Steps 1-3 in order to assign the corresponding weights and to evaluate its quality. If the new solution s'_c is better or equal than the current solution s_c, then s_c is updated to s'_c (Steps 11-13). We accept solutions with equal f_c to allow to visit more regions of the search space. Finally, the best solution obtained (s_c, w_c) is returned jointly with its evaluation f_c.\nLet us briefly describe the functions involved in SLS-OSBOP.\nThe method INITIALSOLUTION(C, b) randomly generates a vector s_c of b bucket orders. Note that other heuristic procedures may be used to generate s_c, e.g. using as one of the bucket orders the solution to the OBOP.\nFor each solution s_c with b bucket orders B_1,... B_b (i.e. b bucket order matrices B_1, ..., B_b) and each vector of weights w_c = (w^1, ..., w^b), we construct the matrix B[(B_1, . . ., B_b), (w^1, . . ., w^b)] as in (2), and use the function f_c (3) to evaluate the solution\nEVALUATESOLUTION(C, s_c, w_c) = f_c(B[(B_1, ..., B_b), (w^1, . . ., w^b)], C)\nThe local search procedure TUNEWEIGHTS(C, s_c, w_c, f_c, t_2) is executed to find the weights that minimize (4). Specifically, a local search procedure is applied to improve the objective function. Starting from equal weights 1/b for the b bucket orders, a sequence of random changes is applied. In particular, a random value r \u2208 [\u22120.5, 0.5] is generated for a random weight w so the new value becomes max(min(w+r, 1), 0) \u2208 [0, 1]. Then, the values of the remaining weights are proportionally adjusted to ensure that their total equals 1. If the value of the objective function is improved with the change, then it is accepted; otherwise the change is discarded and the previous weights are kept.\nThe method MUTATESOLUTION(s_c) takes two decisions at random: the bucket order to be mutated and the mutation to be applied. Also the number of bucket orders to be mutated is chosen at random. For each bucket order to be mutated, any of the mutations introduced in [2] may be applied:\nBucket Insertion: Randomly pick a bucket and place it in another random position. Example: 8,7|6|5,4,3|2,1 \u2192 2,1|8,7|6|5,4,3\nBuckets Interchange: Randomly pick two buckets and interchange their positions. Example: 8,7|6|5,4,3|2,1 \u2192 2, 1|6|5,4,3 8,7\nBucket Inversion: Randomly pick a chain of consecutive buckets and reverse the order of the buckets. Example: 8,7|6|5,4,3|2,1 \u2192 5,4,3|6|8,7|2, 1\nBucket Union: Randomly pick two consecutive buckets and join them. Example: 8,7|6|5,4,3|2,1 \u2192 8, 7, 6|5,4,3|2, 1\nBucket division: Randomly pick a bucket with two or more items and split it into two consecutive buckets. Each item is randomly placed in one of the resulting buckets. Example: 8,7|6|5,4, 3|2, 1 \u2192 8,765,3|4|2,1\nItem Insertion: Randomly pick a bucket and one of its items. Then either the item is inserted in an existing bucket or generates a new (singleton) bucket. Both decisions are randomly taken. Example: 8,7|6|5, 4, 3|2, 1 \u2192 8,7,5|6|4, 3|2, 1\nItem Interchange: Randomly pick two buckets and one item of each bucket, and interchange them. Example: 8,7|6|5,4,3|2,1 \u2192 8,5|6|7, 4, 3|2, 1"}, {"title": "5 Experimental results", "content": "This section presents an experimental study using 14 datasets from PrefLib [18], each with fewer than 25 items to be ranked (n < 25), since in this seminal work we are interested in interpreting the results and comparing alternative solutions to different variants of the OSBOP. Specifically, we considered three OSBOP variants: OBOP (b = 1), OSBOP2 (b = 2 and Eq =yes), and OSBOP2 (b = 2 and Eq =no). Our aim is to analyze how increasing the model's flexibility (i.e., allowing more bucket orders in the solution and non-equal weights) enhances the accuracy of the response in describing the input matrix (i.e., reducing the distance between the solution and the input matrix). For the experiments, we set t_1 = 10000 and t_2 = 100."}, {"title": "5.1 Results", "content": "The implementation of the algorithms, provided to ensure the reproducibility of the results, is available at https://swish.swi-prolog.org/p/osbop.pl. Table 2 presents the best results achieved for the OBOP, the OSBOP2 and the OSBOP2 over the 14 datasets, whose identifiers are specified in the column Dataset-Id. For each dataset, we show the number of items (n), and the utopia values uc and ur. The values in columns OBOP, OSBOP2 and OSBOP2 correspond to the distance of the best solution to the input matrix obtained after t_1 = 10000 iterations. The column w_1 shows the weight of the ranking with the highest weight in the best solution found for OSBOP2.\nTaking into account the solutions obtained for the 14 datasets, the average ratio OSBOP2/OBOP is 0.61, the average ratio OSBOP\u00b2/OBOP is 0.42 and the average ratio OSBOP\u00b2/ OSBOP2 is 0.71. On the other hand, note that in the datasets Id 2-1, Id 2-2, Id 4-1, Id 4-2 and Id 6-12, the evaluation value (f_c) of the solution found for the OBOP coincides with the utopia value, that is, is, the solution found is optimal. Recall that the utopia value is a lower bound for the solution of OBOP that is not always feasible (see Section 3.2). In our experimentation, the average ratio OBOP/uc is 1.13. Although uc is neither a lower bound for OSBOP2 nor for OSBOP2, the average ratios OSBOP2/uc (0.69) and OSBOP\u00b2/uc (0.48) also illustrate to what extent the fitness of the solution improves when a greater flexibility is allowed in the output. The average ratios OSBOP2/um (1.23) and OSBOP\u00b2/u\u00b2 (0.87) show the additional flexibility introduced by allowing different weights. Recall that when the evaluation value (f_c) of the solution found for OSBOP2 coincides with uz, the solution is optimal (datasets Id 2-1, Id 2-2, Id 4-1, Id 4-2 and Id 6-18). Note also that the optimal utopian condition assumes equal weights, that is, uz is not a lower bound for OSBOP2.\nAlthough the OBOP is usually addressed by means of greedy algorithms [1, 21, 10], in this study we are solving the OBOP with the heuristic SLS-OSBOP (b = 1) for a fair comparison with the rest of OSBOP variants.\nIt is worth noting that, since the same number of iterations t_1 = 10000 was applied to all the OSBOP variants, the percentage of the search space explored when solving the OBOP is significantly larger than that explored when solving the OSBOP2 and the OSBOP2. Nonetheless, the results obtained for the OSBOP2 and the OSBOP2 provide a much closer approximation to the input preference matrix."}, {"title": "5.2 Best solutions found for the OSBOP", "content": "In Table 3 we show the best solution found for the OBOP, the OSBOP2 and the OSBOP2 over the 10 largest datasets considered. Although already shown in Table 2, for each dataset and OSBOP variant, we display the value f_c(B), that is, the distance from the solution to the input matrix C and, below each OSBOP2 instance, the value w^1 corresponding to the bucket order with the highest weight (w^2 = 1-w^1, not shown). In particular, w^1 = w^2 = 0.5 for the OSBOP2. When the best solution obtained for the OSBOP2 has w^1 = 0.5, it is also the solution to the OSBOP2. In these cases (datasets with Id 6-3 and Id 6-4) this solution is only shown once. In some of these datasets there are different solutions with the same evaluation value (f_c). In these cases, Table 3 shows only one of these solutions with equal score; however, in practice, all of them should be presented to the practitioner for decision-making."}, {"title": "5.3 Discussion", "content": "Next, we discuss some aspects of the solutions presented in Tables 2 and 3. It is important to take into account that a stochastic greedy algorithm is used to solve the OSBOP instances, and therefore, the solutions obtained are unlikely to be optimal. However, they allow us to extract some general conclusions:\nOnly in two datasets (Id 6-3 and Id 6-4) the solutions for OSBOP2 and OSBOP2 coincide, i.e. only in these cases the flexibility of adjusting the weights did not produce a decreasing in the distance to the input precedence matrix.\nThere are three datasets (Id 6-12, Id 6-18 and Id 6-48) where the same pair of rankings are included in the best solution to the OSBOP2 and the OSBOP2, but with different weights. These instances highlight how allowing flexibility in the weights leads to better solutions, so discovering communities with different importance (size). Specifically, there is an improvement of 12,4% on average. In contrast, there are four datasets (Id 6-28, Id 14-1, Id 15-48 and Id 15-74) where the solutions for OSBOP2 and OSBOP\u00b2 have no ranking in common.\nIt is remarkable that the ranking found as a solution for the OBOP is never present in the pair of rankings found as solution to the OSBOP2 and the OSBOP2. This supports the idea that considering two rankings in the response means a totally different approach to approximating the input matrix."}, {"title": "6 Conclusions", "content": "In this paper, we present a more general framework for addressing rank aggregation problems by considering a set of (weighted) rankings as the output, which more effectively captures the diversity present in the input data or within the search landscape. This approach allows the proposed ranking aggregation method to recognize the existence of distinct groups within the voter set or their expressed preferences, incorporating a fairness perspective that ensures diverse priorities and viewpoints are appropriately reflected in the aggregation process.\nTo illustrate our proposal, we define de Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the Optimal Bucket Order Problem (OBOP), and provide insight through several illustrative examples. We also discuss the critical factors a decision maker must consider when choosing the variant of the OSBOP that best aligns with their specific needs and objectives. Finally, we introduce a simple metaheuristic approach for addressing the OSBOP and conduct an experimental study which shows that the proposed models yield solutions significantly closer to the input precedence matrix compared to those obtained when limiting the output to a single ranking (OBOP).\nIn future research, we plan to adapt specific heuristics from the OBOP for their application to OSBOP instances, as well as to design more complex metaheuristics to approach the OSBOP."}]}