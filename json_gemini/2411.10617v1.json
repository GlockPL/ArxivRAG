{"title": "Attraction-Repulsion Swarming: A Generalized Framework of t-SNE via Force Normalization and Tunable Interactions", "authors": ["Jingcheng Lu", "Jeff Calder"], "abstract": "We propose a new method for data visualization based on attraction-repulsion swarming (ARS) dynamics, which we call ARS visualization. ARS is a generalized framework that is based on viewing the t-distributed stochastic neighbor embedding (t-SNE) visualization technique as a swarm of interacting agents driven by attraction and repulsion. Motivated by recent developments in swarming, we modify the t-SNE dynamics to include a normalization by the total influence, which results in better posed dynamics in which we can use a data size independent time step (of h = 1) and a simple iteration, without the need for the array of optimization tricks employed in t-SNE. ARS also includes the ability to separately tune the attraction and repulsion kernels, which gives the user control over the tightness within clusters and the spacing between them in the visualization.\nIn contrast with t-SNE, our proposed ARS data visualization method is not gradient descent on the Kullback-Leibler divergence, and can be viewed solely as an interacting particle system driven by attraction and repulsion forces. We provide theoretical results illustrating how the choice of interaction kernel affects the dynamics, and experimental results to validate our method and compare to t-SNE on the MNIST and Cifar-10 data sets.", "sections": [{"title": "Introduction", "content": "Visualization of high-dimensional data is an important field of research in data analysis, as it helps to develop an intuitive understanding of data and formulate statistical hypotheses on complex data sets. In recent years, the t-distributed stochastic neighbor embedding (t-SNE) [37] method for data visualizations has become one of the most popular techniques for dimensionality reduction. It has been widely applied in many fields, e.g., computer security [11, 28, 39, 41, 43], cancer research [1, 4, 8, 9, 23, 24, 31, 42], bio-informatics [7, 15, 19,\n20, 29, 35], and natural language processing [25, 30, 32, 33, 38]. The method has also been studied mathematically, with a focus on scaling limits, understanding its ability to uncover clustering structure in data, and the role played by early exaggeration, an optimization trick we discuss in more detail below [21, 22, 27,34].\nThe main idea behind the t-SNE visualization method is to construct a low dimensional embedding of a data set usually into $R^2$ or $R^3$ that preserves local structure in the data, while allowing distortions at larger scales. This is accomplished by constructing a localized similarity weight matrix over the high dimensional data set, a heavy-tailed simi-larity matrix over the embedded data, normalizing both to be probability distributions, and then minimizing the Kullback-Leibler divergence between them. This acts to ensure that the localized structure of the embedding is faithful to the original high dimensional data.\nThe t-SNE embedding is computed through gradient-based optimization of the Kullback-Leibler divergence. However, it is well-known that plain gradient descent on the Kullback-Leibler divergence is extremely slow to converge and can often give poor visualization results. In order to achieve good results, it is necessary to use a collection of optimization tricks, including early exaggeration - amplifying the attraction forces in the early stages gradient clipping, momentum, as well as scaling the time step with the data size. Good rules of thumb are available for how to tune the corresponding parameters, but their roles in producing good visualizations are only partially understood [22].\nViewing each embedded data point as a particle in $R^2$ or $R^3$, the gradient descent dynamics of t-SNE break down into attraction and repulsion forces driving the embedded points the attraction terms encourge the embedding to preserve local structure, while the repulsion terms prevent collapse to a trival solution. This viewpoint was given in the seminal work [37], and has been expounded upon recently in [22]. In fact, Linderman and Steinerberger [22] go so far as to pose the question of how to construct a dynamical system of attraction-repulsion type whose evolution is well-suited for data visualization.\nIn this paper, we follow this point of view, and propose a data visualization method we call Attraction-Repulsion Swarming (ARS) visualization. Instead of minimizing the Kullback-Leibler divergence, ARS visualization is the steady-state of attraction-repulsion particle dynamics, or swarming. Motivated by recent work in swarming-based optimization approaches, we scale the dynamics by the total influence, which results in well-posed dy-namics that converge to steady state extremely quickly, without the need to employ any optimization tricks. Furthermore, we can use the same time step of h = 1, independent of the size of the data.\nThe ARS dynamics also include the ability to separately tune the strengths of the attrac-tion and repulsion kernels, which allows the user to control the tightness of the clusters and the separation between them, in the visualization. We find that using stronger attraction forces and weaker repulsion leads to better visualizations. Other works have modified the tails of the similarity kernel in t-SNE [16,40], but t-SNE is unable to modify the attraction and repulsion forces separately\nTo illustrate ARS visualization in comparison with t-SNE, we show here the results of both methods applied to visualize the MNIST [18] and Cifar-10 [17] image data sets. Sample images from each data set are shown in Figure 1. For MNIST we represented the images with their raw pixel values, while for Cifar-10 we used the SimCLR contrastive learning embedding [6]. Figure 2 shows visualizations of the full MNIST and Cifar-10 data sets"}, {"title": "Attraction-Repulsion Swarming Visualization", "content": "In this section, we overview the t-SNE visualization technique and then introduce our pro-posed Attraction-Repulsion Swarming (ARS) visualization."}, {"title": "A Brief Review of t-SNE", "content": "The purpose of t-SNE is to project a set of D-dimensional data points, ${x_i}_{i=1}^N$, $x_i \\in R^D$, into two or three-dimensional space while maintaining the essential local similarity features in the data. Specifically, the similarity of data point $x_i$ to data point $x_j$ is measured by the conditional probability,\n\n$P_{j|i} = \\frac{exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}, \\quad i \\neq j, P_{i|i} = 0,$\n\nwhere $\\sigma_i$ is the bandwidth of the Gaussian kernel centered at $x_i$. The perplexity of $x_i$, which measures the effective number of neighbors, is defined as\n\n$Perp(P_i) = 2^{H(P_i)}, \\quad H(P_i) = - \\sum_{j\\neq i} P_{j|i} \\log_2(P_{j|i}).$\n\nThen bandwidths $\\sigma_i$ are chosen so that the perplexity of each data point equals a specified value, typically between 5 and 50. Once the conditional probabilities, $P_{j|i}$, are obtained, the global similarity information is encoded into the symmetrized probability distribution, $P = (P_{ij})_{i=1}^N$, with\n\n$P_{ij} = \\frac{P_{ij} + P_{ji}}{2N}$.\n\nIt is easy to verify that $\\sum_{i,j=1}^N P_{ij} = 1$, so the matrix $P$ can be viewed as a probability distribution. The perplexity graph construction defined above shares similarities with a k-nearest neighbor graph, where the desired perplexity plays the role of the number k of neighbors.\nAssociated to the high-dimensional data points ${x_i}_{i=1}^N$, let ${y_i}_{i=1}^N$ denote the low di-mensional projections $y_i \\in R^d$, where $d=2$ or $d=3$. In t-SNE, the similarity of $y_i$ and $y_j$"}, {"title": "Motivation for ARS", "content": "The gradient descent iterations (2.5) often suffer from slow convergence and poor visualiza-tion results when applied to large scale complicated data sets. To address this, a classical approach [37] is to introduce early exaggeration, which corresponds to the dynamics\n\n$y_i^{n+1} = y_i^n + 4zh\\alpha \\sum_{j=1}^N P_{ij}Q_{ij}(y_j^n - y_i^n) - 4zh \\sum_{j=1}^N Q_{ij}(y_j^n - y_i^n), \\quad \\alpha > 1.$\n\nThe factor $\\alpha > 1$ amplifies the attraction effect on the global configuration, thereby gathering similar data points tightly into clusters. After a desired number of early exaggeration steps (2.7), often around 200, we return to the gradient flow dynamics (2.5) and the repulsive forces between dissimilar agents are restored, causing the congested crowd to spread out into separated clusters more evenly.\nIn addition to early exaggeration, it is necessary to use a very large learning rate h, often between 10 to 1000, to accelerate convergence and avoid getting stuck at a poor solution. Indeed, notice that the average scales of $P_{ij}$, $Q_{ij}$ are $O(1/N^2)$, hence the total force scales with $O(1/N)$, where N is the number of data points. To compensate for the 1/N shrinkage of the force magnitude for large data size $N \\gg 1$, the appropriate learning rate should scale with $O(N)$. However, an excessively large learning rate can also result in the risk of instability or poor results if the learning rate is taken too large, the data may end up looking like a 'ball', where each agent is approximately equidistant from its nearest neighbors. To stabilize the convergence under large learning rates, common implementations of t-SNE in the existing literature set a relatively large initial learning rate (e.g., N/a proposed in [3,15]), followed by learning rate adaptation techniques to ensure stability.\nOne of the most popular adaptation schemes is the delta-bar-delta rule proposed by Jacob [14]. It has been applied in many existing studies of t-SNE e.g., [3,15,36,37]. The key idea of the scheme is to gradually increase the learning rate if the gradient direction remains stable. If the gradient changes sign at consecutive iterations, which indicates oscillation due to an excessively large learning rate, then the learning rate is decreased. More specifically, let $w^n$ be a single weight of the cost function $E(w^n)$ at the n-th iteration, the corresponding learning rate, $h^n$, is updated with the rule\n\n$h^{n+1} = \\begin{cases}\nh^n + \\kappa & g^{n-1} \\cdot g^n > 0, \\\\\n(1 - \\phi)h^n, & g^{n-1} \\cdot g^n < 0\n\\end{cases}, \\quad \\kappa > 0, \\quad 0 < \\phi < 1,$"}, {"title": "ARS Visualization", "content": "To speed up the clustering of data without resorting early exaggeration, time-step adapta-tion, momentum, and other advanced optimization techniques, we propose a new Attraction-Repulsion Swarming (ARS01,02) method. The algorithm is obtained by discretizing the following continuous-time dynamics,\n\n$\\frac{d}{dt} y_i(t) = \\sum_{j=1}^N \\bar{P}_{ij} \\psi_1(|y_i - y_j|)(y_j - y_i) - \\sum_{j=1}^N \\bar{Q}_{ij}(t) \\psi_2(|y_i - y_j|)(y_j - y_i),$\n\n$\\bar{P}_{ij} = \\frac{P_{ij}}{\\sum_k P_{ik}} \\quad \\bar{Q}_{ij} = \\frac{Q_{ij}}{\\sum_k Q_{ik}} \\quad \\psi_1(r) = (1 + r^{\\theta_1})^{-1}, \\psi_2(r) = (1 + r^{\\theta_2})^{-1}, \\quad \\theta_1, \\theta_2 > 0.$\n\nThe discrete dynamics of (2.9) under the forward Euler time stepping is given by\n\n$y_i^{n+1} = y_i^n + h \\sum_{j=1}^N \\bar{P}_{ij}\\psi_1(|y_i^n - y_j^n|)(y_j^n - y_i^n) - h \\sum_{j=1}^N \\bar{Q}_{ij}\\psi_2(|y_i^n - y_j^n|)(y_j^n - y_i^n)$\n\nTo achieve a better visualization, early exaggeration may be introduced to reinforce the formation and the separation of clusters,\n\n$y_i^{n+1} = y_i^n + \\alpha h \\sum_{j=1}^N \\bar{P}_{ij}\\psi_1(|y_i^n - y_j^n|)(y_j^n - y_i^n) - h \\sum_{j=1}^N \\bar{Q}_{ij}\\psi_2(|y_i^n - y_j^n|)(y_j^n - y_i^n)$\n\nNote that (2.11) is a direct extension of the t-SNE counterpart (2.7) under normalized forcing.\nWe outline the key components of ARS below:\nForce Normalization: Compared with the original t-SNE algorithm, the forcing terms of ARS are further scaled by the total influence, $\\sum_j P_{ij}$ and $\\sum_j Q_{ij}$, which is inspired by the flocking dynamics model proposed by Motsch and Tadmor [26]. The new scaling emphasizes the 'relative' (conditional) similarity and the relative distance in agent communications."}, {"title": "Independence of Data Size", "content": "A key aspect of ARS is the normalization of the forcing terms by total influence, so that the learning rate is no longer required to scale with the data size. To verify this improvement, we compare the performance of t-SNE (2.5) and ARS (2.10) against the same 1000 MNIST images from the digits 0 ~ 3. Both methods employ the same initialization generated with the uniform distribution $y \\sim U([0,1]^2)$. We used 100 steps of t-SNE early exaggeration (2.7) for both methods with $\\alpha = 40$."}, {"title": "Tunable Interactions", "content": "The new attraction-repulsion swarming (2.9) extends the framework of t-SNE by allowing user-tunable interaction protocols. We illustrate the benefits of such generalization through computational and analytical justifications."}, {"title": "ARS01,02", "content": "The ARS dynamics is dictated by parameters $\\theta_1$ and $\\theta_2$, as they control the localization of attraction and repulsive forcing. To demonstrate the influence of adjusting interaction kernels, we compare the performance of $ARS_{\\theta_1,\\theta_2}$ under different settings against the same"}, {"title": "Investigation of Localized Repulsive Forces", "content": "The computations in Section 4.1 indicate the advantage of employing a localized repulsive force in terms of forming complete and well-separated clusters. In this section, we further study the effect of short-range repulsion.\nWhy favor localized repulsion?\nThe benefits of using localized repulsive force can be interpreted from several aspects. From the computational point of view, localizing the repulsion dynamics would emphasize the influence of attraction at initial stage where the agents $y_i$'s are loosely distributed. Such mechanism mimics the early exaggeration steps in the original t-SNE algorithm, which reinforces the communications between 'similar' agents and accelerates the separation of distinct clusters afterwards. Meanwhile, by favoring attraction forces at long distance, the"}, {"title": "Mean-Field Limit of ARS", "content": "The analytical discussions in Section 4.2 primarily demonstrate the effect of force balancing on the particle swarming process. Nevertheless, the inter-connection between the structure of high dimensional input data and the clustering behavior of the low-dimensional embedding still lacks theoretical guarantees. To reinforce the interpretability of ARS results, especially when a large data size is involved, studying the mean-field limit may provide a tractable approach. We give an outline of a possible approach here, leaving the analysis to future work. For simplicity, we assume a constant bandwidth, $\\sigma_i = \\sigma$, is employed for all data points.\nTo obtain the mean-field description of the ARS particle system (2.9), we define the empirical distributions of the input data and output projections,\n\n$\\mu(x) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{x_i}(x), \\quad x \\in R^D, \\text{and} \\quad \\rho_t(y) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{y_i(t)}(y), \\quad y \\in R^d.$\n\nThe measures $\\mu$ and $\\rho_t$ are connected through the mapping, $T_t^N: R^D \\rightarrow R^d$, with\n\n$T_t^N(x_i) = y_i(t), \\quad 1 \\leq i \\leq N.$\n\nThe time evolution of $T_t^N$ encodes the clustering process of the low-dimensional embedding ${y_i}$. Let us assume that the discrete projection mapping admits a (formal) large-crowd limit,\n\n$T_t = \\lim_{t \\rightarrow \\infty} T_t^N.$\n\nThen the evolution of the mean-field measure, $\\rho_t = T_t(\\mu)$, is governed by the nonlinear transport equations\n\n$\\partial_t \\rho_t + \\nabla \\cdot (\\rho_t u) = 0 \\\\ u(t, y) = \\int \\big[ F(y, y', t)(y' - y) \\rho_t(y') dy' \\big].$\n\nThe kernel function, $F(y, y', t)$, reflects the balance between attraction and repulsive forces,\n\n$F(y,y',t) = \\bar{P}(y, y', t) \\psi_1(|y - y'|) - \\bar{Q}(y, y', t) \\psi_2(|y - y'|),$\n\nwith $S_t(y) = {x \\in \\text{supp}{\\mu} : T_t(x) = y}$,\n\n$\\bar{P}(y,y',t) = \\frac{\\int_{S_t(y) \\times S_t(y')} e^{-\\frac{||x - x'||^2}{2\\sigma^2}} \\mu(x) \\mu(x') dx dx'}{\\int_{R^d} [ \\int_{S_t(y) \\times S_t(z)} e^{-\\frac{||x - x'||^2}{2\\sigma^2}} \\mu(x) \\mu(x') dx dx' ] \\rho_t(z) dz } \\text{and} \\\\ \\bar{Q}(y, y',t) = \\frac{\\frac{1}{1 + |y - y'|^2}}{\\int_{R^d} \\frac{1}{1 + |y - z|^2} \\rho_t(z) dz}.$"}, {"title": "Tree-Based Variant: Barnes-Hut ARS", "content": "The primary versions of t-SNE and ARS algorithms involve computing the interactions between N(N \u2013 1) pairs of agents, as well as storing the N \u00d7 N input similarities Pand projection similarities Q. The quadratic growth of computational complexity with respect to the data size restricts the use of t-SNE in large data visualization. Inspired by the Barnes-Hut algorithm [2] and the dual-tree algorithms [10], van der Maaten [36] proposed a tree-based variant of t-SNE called Barnes-Hut t-SNE. The key components include 1. employing a sparse approximation for the input similarity matrix P, and 2. approximating the agent-agent repulsion dynamics with agent-cell or cell-cell interactions, which can be computed efficiently using tree data structures. In this way, the computational complexity of computing the t-SNE gradient can be reduced from O(N2) to O(N log N). We can follow a nearly identical development to approximate the ARS forcing terms in O(N log N) computational time, which we call Barnes-Hut ARS, or ARS-BH. We refer to [36] for more details on the Barnes-Hut approximation, and have made our code for ARS-BH available on GitHub, as well as part of the GraphLearning Python package [5].\nUsing the ARS-BH method, we can scale up the computations with ARS to the full MNIST and Cifar-10 data sets, which have 70000 and 60000 images, respectively. When using early exaggeration in ARS-BH, we use the ARS version of early exaggeration, given in (2.11).\nIt is important to emphasize that ARS-BH is able to obtain good visualizations that are comparable, and in some cases better, than t-SNE, using only the simple iteration (2.10), which would be equivalent to running vanilla gradient descent for t-SNE. In particular, we do not need to employ any time-step adaptation or momentum techniques to achieve good"}, {"title": "Conclusion and Future Work", "content": "We proposed a new method for data visualization called Attraction-Repulsion Swarming (ARS) visualization. The method is a general framework that can be viewed as a variant of t-SNE, where we abandon the Kullback-Leibler divergence loss function, and instead formulate a dynamical system with attraction-repulsion forces that is run until steady state. By properly normalizing the forces by total influence, we obtain dynamics that are far better behaved compared to t-SNE, in that they both converge faster to steady state, and do not require sophisticated optimization tricks to be successful. We furthermore find that favoring attraction forces over repulsion, which is possible in the ARS framework, but not with t-SNE, can in some cases give better visualizations with tighter clusters that are well-separated.\nIn the future, it would be interesting to perform an in-depth mathematical investigation of the ability of ARS to further explore the tradeoff between attraction and repulsion forces, and to better understand when ARS can uncover clustering structure in data. One possible approach would be to utilize the mean-field limit developed in Section 5. It would also be interesting to apply the ARS-BH method to a more extensive collection of data sets to evaluate its performance compared to t-SNE."}, {"title": "Derivation of Mean-Field Limit", "content": "We derive the mean-field equations (5.1) from the particle system (2.9). We test $\\partial_t \\rho_t$ against arbitrary smooth function $\\phi$,\n\n$\\int \\partial_t \\rho_t(y) \\phi(y) dy = \\frac{d}{dt} \\frac{1}{N} \\sum_{i=1}^N \\phi(y_i(t)) = \\frac{1}{N} \\sum_{i=1}^N \\frac{d}{dt} \\phi(y_i) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\phi \\cdot \\frac{d}{dt} y_i = \\frac{1}{N} \\sum_{i=1}^N u_i \\cdot \\nabla \\phi(y_i).$\n\nThe velocity $u_i$ reflects the competition between attraction and repulsion forcing,\n\n$u_i = \\frac{1}{N} \\sum_{j=1}^N \\frac{\\bar{P}_{ij}}{\\sum_{j=1}^N \\bar{P}_{ij}} \\psi_1(y_i, y_j) (y_j - y_i) - \\frac{1}{N} \\sum_{j=1}^N \\frac{\\bar{Q}_{ij}}{\\sum_{j=1}^N \\bar{Q}_{ij}} \\psi_2(y_i, y_j) (y_j - y_i) = \\frac{1}{N} \\sum_{j=1}^N \\frac{P_{ij}}{\\bar{P}_i} \\psi_1(|y_i - y_j|) - \\frac{Q_{ij}}{\\bar{Q}_i} \\psi_2(|y_i - y_j|) \\big] (y_j - y_i),$\n\nwhere\n\n$\\bar{P}_i = \\frac{1}{N} \\sum_{j=1}^N P_{ij}, \\quad \\bar{Q}_i = \\frac{1}{N} \\sum_{j=1}^N Q_{ij}.$"}]}