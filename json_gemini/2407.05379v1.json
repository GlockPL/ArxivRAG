{"title": "AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency", "authors": ["Maria Arostegi", "Miren Nekane Bilbao", "Jesus L. Lobo", "Javier Del Ser"], "abstract": "The ever-growing speed at which data are generated nowadays, together with the substantial cost of labeling processes (often reliant on human supervision) cause Machine Learning models to often face scenarios in which data are partially labeled, or delayed with respect to their query time. The extreme case where such a supervision is indefinitely unavailable is referred to as extreme verification latency. On the other hand, in streaming setups data flows are affected by exogenous factors that yield non-stationarities in the patterns to be modeled (concept drift), compelling models learned incrementally from the data streams to adapt their modeled knowledge to the prevailing concepts within the stream. In this work we address the casuistry in which these two conditions occur together over the data stream, by which adaptation mechanisms to accommodate drifts within the stream are challenged by the lack of supervision, hence requiring further mechanisms to track the evolution of concepts in the absence of verification. To this end we propose a novel approach, coined as AiGAS-dEVL (Adaptive Incremental neural GAS model for drifting Streams under Extreme Verification Latency), which relies on the use of growing neural gas to characterize the shape and inner point distributions of all concepts detected within the stream over time. Our approach exposes that the online analysis of the behavior of these prototypical points over time facilitates the definition of the evolution of concepts in the feature space, the detection of changes in their behavior, and the design of adaptation policies to mitigate the effect of such changes in the model. We assess the performance of AiGAS-dEVL over several synthetic datasets, comparing it to that of state-of-the-art approaches proposed in the recent past to tackle this stream learning setup. Our reported results reveal that AiGAS-dEVL performs competitively with respect to the rest of baselines, exhibiting a superior adaptability over several datasets in the benchmark while ensuring a simple and interpretable instance-based adaptation strategy.", "sections": [{"title": "1. Introduction", "content": "The proliferation of data across diverse domains has undergone an unprecedented surge in recent years, driven by the increasing digitization of processes in almost all domains of activity, and the widespread proliferation of connected devices. This exponential growth in data generation is further compounded by the escalating scales and speeds at which data are being produced and disseminated on a daily basis. From social media interactions and online transactions to sensor readings in industrial IoT environments, the sheer volume and velocity of data streams pose grand challenges for traditional methods for batch data processing and analysis. Consequently, there is a compelling need to develop novel techniques and algorithms capable of handling the dynamic nature of data streams in real time. This growing concern has motivated the emergence of different research areas focused on the study of new efficient learning models for data analysis, including Big Data analytics [1], real-time analytics [2] or, from a machine learning (ML) perspective, data stream learning [3, 4]. When data are produced continuously, ingested and processed in real time, strict restrictions arise in terms of memory and processing power [5, 6], eventually constraining the design of ML algorithms operating over such data.\nWhen addressing ML tasks formulated over data streams, the scalability of the learning algorithms is indeed a critical challenge, given the high volume and velocity of data being generated in a continuous fashion. Unfortunately, scalability is not the only challenge to be faced in such scenarios: there exist other issues that can significantly impact the effectiveness and reliability of ML models in this domain. On one hand, the concept drift phenomenon poses a major hurdle to the characterization of patterns within the data flows, as the underlying data distribution may change over time [4, 7, 8, 9]. When concept drift occurs, models trained on historical data become less accurate or even obsolete, unless their knowledge is modified and adapted to the prevailing data distribution. Hence, adaptation to evolving concepts becomes imperative for ensuring a sustained model performance under concept drift. On the other hand, extreme verification latency introduces another layer of complexity, where the delay in obtaining ground truth labels for incoming data can lead to discrepancies between the predictions issued by the model and their true values. This delay not only hinders model evaluation, but also exacerbates the adaptation to drifting data, as decisions need to be made in real-time based on potentially outdated or incomplete information. Consequently, addressing these challenges requires not only scalable learning algorithms, but also robust mechanisms for concept drift detection and adaptation, as well as strategies for handling extreme verification latency, ensuring the reliability and effectiveness of ML models over evolving data flows.\nInterestingly under the scope of this work, most contributions reported to date around modeling over drifting data streams are focused on scenarios characterized by no latency, on the assumption of a so-called test-then-train scheme. Consequently, the scope of such studies is restricted to improving the effectiveness of the adaptation strategies therein proposed assuming that the supervision of arriving data instances occurs immediately after they are predicted by the model at hand. Other works have instead addressed the case of a lagged label supervision with respect to the query time [10]. Ultimately, when this verification latency is large or even infinite (extreme as per the nomenclature used in the literature), an initial part of the data stream is assumed to be labeled, and after a certain point in time this supervision is no longer available. This models the situation"}, {"title": "2. Background and Related Work", "content": "Before proceeding with the detailed description of the proposed AiGAS-dEVL model, we first pause at the main milestones and advances attained recently in the fields of stream learning and concept drift (Section 2.1), extreme verification latency (Section 2.2) and GNG (Section 2.3), establishing the grounds to justify the contribution of AiGAS-dEVL over the reviewed literature (Section 2.4).\nAs has been argued in the introduction to this work, the significance of stream learning has grown exponentially in recent years, reflecting its pivotal role in addressing the computational and learning challenges posed by the rapid influx and dynamic nature of streaming data. Quickly arriving instances in a data stream can be retained only for a limited time budget, which impacts on the design of data processing pipelines, especially in hardware with constrained computing and storage capabilities [16]. When the pipeline implies a ML model, constantly updating the knowledge captured by the model from the data stream is subject to the resources available in the device and the scales (volume and speed) at which data streams flow in the scenario at hand. As such, one can distinguish between on-line learning, where the ML model is updated when a new instances arrives sequentially from the stream, in a one-by-one fashion, whereas in incremental learning the model is updated over batches comprising a number of stream instances.\nSeveral comprehensive surveys on stream learning and concept drift have been published to date, emphasizing on the applications, challenges and taxonomies by which the plethora of con-tributions can be classified. Concept drift in data streams was formalized as early as 2009, capi-talizing on prior studies dealing with changes in the definitions of classes in learning tasks [17]. Concept drift was later expanded by follow-up contributions discussing on the major challenges of data stream mining [18, 19, 8, 20].\nConcept drift refers to the phenomenon where the underlying data distribution changes over time, leading to shifts in the relationships between input features and target variables. Several types of concept drift can be distinguished, each with distinct characteristics and implications for model adaptation. First, a sudden (sharp) concept drift occurs when the data distribution undergoes abrupt and significant changes, often resulting from unforeseen events or external factors. This type of drift poses a considerable challenge for ML models, as they must rapidly adjust to the new data distribution to maintain the level of performance achieved prior to the occurrence of the drift. In contrast, gradual concept drift involves more gradual and incremental changes in the data distribution over time, making it less immediately apparent but still requiring continuous model adaptation to ensure performance. Finally, recurring concept drift refers to patterns of change that occur periodically or intermittently, such as seasonal variations or cyclic trends, necessitating adaptive models capable of detecting and accommodating them effectively. Understanding the different types of concept drift is crucial for developing robust stream mining algorithms that can effectively handle the dynamic nature of streaming data [21].\nConcept drift adaptation strategies in stream mining can be broadly categorized into active and passive approaches [22]. Active adaptation strategies involve actively monitoring data streams and triggering model updates or retraining when concept drift is detected. Examples of active ap-proaches include drift detection algorithms that continuously analyze incoming data for deviations from the established model, prompting timely adjustments to ensure model accuracy. On the other hand, passive adaptation strategies involve building models that inherently adapt to changes in the data distribution without explicit drift detection mechanisms. These strategies often involve the use of dynamic learning algorithms, ensemble methods, or online learning techniques that incre-mentally update model parameters as new data arrives, allowing the model to adapt gradually to evolving concepts. While active approaches offer real-time responsiveness to concept drift, they may incur higher computational costs and require more sophisticated drift detection mechanisms. In contrast, passive approaches provide a more seamless and resource-efficient adaptation process, but can be less sensitive to sudden or unexpected changes in the data distribution.\nIn scenarios subject to sparse annotation [23] or ultimately, extreme verification latency, the scarcity or absence of incoming labeled data samples requires the use of different adaptation strate-"}, {"title": "2.3. Growing Neural Gas", "content": "A proper understanding of Growing Neural Gas (GNG) departs from Self Organizing Maps (SOMs), a neural computation method to construct representations from high-dimensional data. Unlike other dimensionality reduction approaches, SOMs generate a map of similarity relation-ships. That is, SOMs learn topological relations based on a distance metric. In doing so, SOMs are trained using unsupervised learning and a competitive strategy, producing a discrete represen-tation of the input space in a lower-dimensional map. The dimension of the input space also sets the number of input neurons each being connected to all the output neurons (of the map). These connections do not quantify the importance of each of the neurons, but translate the position it will end up occupying in the map.\nUnlike SOM, where the topology of the network (number of units and connections) is pre-defined, GNG learns the topology by adding both units and connections as more information is received. That is, the Growing Neural Gas (GNG) [35] is an incremental neural model able to learn the topological relations of a given set of patterns by means of competitive Hebbian learning. The first work in this direction appears on 1994, [36], which introduces this model as an incremental network capable to learn topological relationships within a given set of input vectors by means of a simple Hebbian-like learning rule. Since then, the community has proposed optimization techniques aimed to speed up the learning algorithm of the naive GNG model [37]. More recent research works underscore the potential of GNG to provide an overview of the structure of large volumes of multidimensional data, which can be used in exploratory data analysis [38, 39]."}, {"title": "2.4. Contribution", "content": "This last perspective on the use of GNGs settles the starting hypothesis of the present work: since self-organizing neural networks try to preserve the topology of an input space through com-petitive learning [40], we hypothesize that this capability could be exploited for capturing the evolving representation of data instances over time. These models, particularly GNG, can con-tribute to the characterization and tracking of the impact of gradual concept drifts on the feature space and the concepts underneath the flowing data, even in the absence of annotation. This is indeed the motivation for embracing GNG at the core of the AiGAS-dEVL model proposed in this work: the use of GNG not only allows maintaining a continuously updated characterization of"}, {"title": "3. Proposed AiGAS-dEVL Approach", "content": "In this section we delve into the algorithmic elements comprising the proposed AiGAS-dEVL model, including the mathematical definition of the dEVL problem (Subsection 3.1) and a detailed description of the proposed algorithm and its steps (Subsection 3.2). We end by analyzing the benefits and points of improvement of AiGAS-dEVL in Subsection 3.3."}, {"title": "3.1. Problem Statement", "content": "An dEVL problem is defined on a stream of data instances $\\{(x_t, y_t)\\}_{t=1}^{\\infty}$, where $x_t \\in \\mathbb{R}^N$ denotes the data instance flowing at time t and $y_t \\in \\mathcal{Y}$ its label. We assume that labels are available for $t < T_s$, so that a model $M_{\\Theta}(x)$ with parameters $\\Theta$ can be learned over the samples during this supervised period. For $t > T_s$, labels of the instances arriving from the stream are not available any longer, so the learned model is used to predict them as $\\hat{y}_t = M_{\\Theta}(x_t)$ for $t > T_s$. The challenge emerges when the distribution $p(y|x)$ characterized by $M_{\\Theta}(x)$ is non-stationary, i.e., $p_t(y|x) \\neq p_{t'}(y|x)$ for $t,t' > T_s : t \\neq t'$. The goal is to endow the learned model $M_{\\Theta}(\\cdot)$ with the capability to adjust its modeled knowledge to the changes (also referred to as concept drift) undergone by the stream data distribution over time. The lack of supervision for $t > T_s$ requires that the drift dynamics are gradual, so that the evolving concepts can be tracked by the augmented model in the absence of supervision, i.e. without having access to $\\{y_t\\}_{t>T_s}$. We note that this definition also applies to the case of mixed drift, i.e., when both $p(x)$ and $p(y|x)$ change over time. This last case affects both the distribution of features and the decision boundary of the model, requiring more sophisticated adaptation mechanisms to adapt its knowledge."}, {"title": "3.2. Description of AiGAS-dEVL", "content": "The design of our devised AiGAS-dEVL algorithm departs from the estimation of one of the key parameters of the approach from the supervised part of the data stream: the initial number of nodes characterized by GNG. For the sake of computational efficiency, AiGAS-dEVL utilizes a single GNG algorithm to characterize $p_t(x)$ over time. In the case of a very unbalanced dataset, stream data instances belonging to the most prevalent classes can dominate the production of nodes by GNG, leaving the other classes underrepresented in the node distribution output by GNG. To overcome this, the initial number of GNG nodes is adjusted as $G_0 = (1+\\xi)G$, where G is an hiper-parameter and $\\xi$ is given by the ratio of the number of samples of the most to the least populated class in $\\{x_t, y_t\\}_{t<T_s}$. This parameter is used by GNG to learn an initial map of nodes $\\{\\mathbf{x}_g^{0,0}\\}_{g=1}^{G_0}$ over $\\{x_t, y_t\\}_{t<T_s}$. Such nodes can be assigned a label $y \\in \\mathcal{Y}$ by means of a classifier $M_{\\Theta,ini}(\\cdot)$ learned from $\\{x_t, y_t\\}_{t<T_s}$, i.e. $y_g^{0,0} = M_{\\Theta,ini}(\\mathbf{x}_g^{0,0})$. While our experiments detailed later consider a KGNG-Nearest Neighbor algorithm NN($\\cdot$) as $M_{\\Theta,ini}(\\cdot)$, any other classifier can be used instead.\nOnce this initial supervised period is over, extreme verification latency occurs, hence only unsupervised instances $\\{x_t\\}_{t>T_s}$ are received. We assume that instances are received in batches, so that $\\{x_t\\}_{t>T_s}$ is redefined as $\\{x^b\\}_{b=1}^{\\infty}$, where b is the batch index and $x^b = [x_1,...,x_B]$, with"}, {"title": "1. Solving a minimum-cost assignment problem to map all nodes in", "content": "{x0} gGo go = 1to a node from\nthe previous distribution {x0,1} xgG1g = 1. Here, cost is given by the G0 \u00d7 G1 distance matrix D0,1 ="}, {"title": "2. Estimating a projection based on the estimated correspondence between", "content": "{x0} gGo1 and {x1,1} xgG11 through A0,1. Among all the possible projections, given the progressive drift dynamics of\nthe stream we opt for a simple rigid transformation given by t \u2208 RN (translation vector) and\nR\u2208 RN\u00d7N (rotation matrix). Such matrices can be estimated by virtue of several decomposi-\ntion algorithms; among them, AiGAS-dEVL seeks the optimal rotation and translation R\u2217 and\nt\u2217 such that:\nR\u2217, t\u2217 = arg min \u2211G1g0g=1g=1ag,g' \u00b7 ||R. (x0g + t) \u2212 x1g||2, (2)\nnamely, a least-square fitting of R and t considering the mapping A0,1. This problem can be\nsolved by the well-known Kabsch algorithm [43], also referred to as the Kabsch-Umeyama\nalgorithm [44], which also meets the computational constraints imposed by stream learning by\nvirtue of its O(G1) complexity.\nOnce this projection estimation is done, the nodes {x1,1} xgG11 computed for batch b = 1 can be\nprojected through the estimated R and T, yielding:\nx1g = R(x1g + t), \u2200g\u2208 {1,...,G1}, (3)"}, {"title": "4. Experimental Setup", "content": "In order to assess the performance of the proposed AiGAS-dEVL algorithm, several experi-ments are carried out to provide empirical evidence to the answers to two research questions (RQ):\n\u2022 RQ1: How does AiGAS-dEVL perform when compared to other approaches proposed for mod-eling drifting data streams with extreme verification latency?\n\u2022 RQ2: Do the points of improvement identified previously reflect on the performance of AiGAS-dEVL over time under different drift dynamics?\nOur designed experimental setup considers a public repository of non-stationary datastreams widely adopted by the stream mining community [11]. Specifically, the repository con-tains 15 synthetic datasets featuring gradual drift changes over time, and 2 real datasets.\nDatasets. Several algorithms from the literature related to EVL in the presence of concept drift(reviewed in Subsection 2.2) are considered in our experiments:\n\u2022 A static classifier (STC), learned from the first labeled samples and kept fixed over time (i.e., itdoes not adapt its knowledge).\n\u2022 A sliding window classifier (SLD), which learns initially from the labeled samples and updatesits knowledge with predicted upcoming samples, discarding those instances that do not fallinside the sliding window for predicting new instances.\n\u2022 An incremental window classifier (INC), which is similar to the sliding window classifier, butdoes not forget any past instance for training.\n\u2022 COMPOSE (CMP, [28]), which resorts to a core of samples from the current data and definesa shape that represents the distribution of each class. When a new batch arrives, COMPOSE"}, {"title": "Evaluation.", "content": "The performance of the above baselines and the proposed AiGAS-dEVL model overthe datasets listed in is gauged in terms of the prequential error, which is a referential mea-sure in the data stream literature [50, 51]. The prequential error allows monitoring the evolutionof the performance of models over time. In order to use the same metrics as in [32], the prequen-tial error is computed based on an accumulated sum of a loss function between the predicted andobserved values, i.e.:\nPe(t) = tt1L(yt, y't), (4)\nwhere the prequential error is computed at time t, y't represents the prediction at time t', and yt'represents its ground truth value. The loss function L(\u00b7,\u00b7) is set to the zero-one loss, i.e., for everystream instance the loss equals 1 if it is wrongly predicted, and 0 otherwise. In addition to thisperformance score, we also report the average macro F1 scores of the models computed over everydataset in a non-prequential fashion. This measure is given by:\nF1 = 2 \u00b7 Precision \u00b7 RecallPrecision + Recall (5)\ni.e., the harmonic mean of precision and recall averaged over all classes and the entire set of unsu-pervised samples of every dataset. Performance statistics (mean, standard deviation) are providedacross all datasets for every approach, whereas the statistical significance is assessed by means ofa Bayesian analysis of the differences [52] between every pair of algorithms in the benchmark.The analysis of the significance focuses on the top performing baselines, and considers varying"}, {"title": "5. Results and Discussion", "content": "We now present and discuss on the results obtained for every research questions formulatedabove:\nRQ1: How does AiGAS-dEVL perform when compared to other approaches proposed for modelingdrifting data streams with extreme verification latency?\nThe results obtained for this first research question are summarized in Table 2 (average pre-quential error) and Table 3 (average macro F1 score). In these tables, the best and second bestcounterparts for every dataset are highlighted as and, respectively. The last two rows inform about the mean and standard deviation statistics of every algorithm computed over alldatasets. The statistical significance of the gaps is discussed later.\nWe begin our discussion by analyzing the prequential error statistics in Table 2. As expecteddue to the non-stationarity of the stream datasets under consideration, methods not designed tocope with this circumstance (namely, STC, SLD and INC) yield in general comparatively poor"}, {"title": "RQ2: Do the points of improvement identified previously reflect on the performance of AiGAS-dEVL over time under different drift dynamics?", "content": "To give an informed response to this second question, we proceed by analyzing how the per-formance scores evolve over time for different datasets. We focus on some of them that besides linking closely to the motivation for the research question, correspond to drift dynamics that can be inspected visually and interpreted geometrically, so that the performance decays can be attributed to non-rigid and/or non-smooth transformations, for which the projection in use within the naive AiGAS-dEVL design (line 13 in Algorithm 1) may not suffice.\nTo this end, we focus first on the plots included in , which depict the evolution of themacro F1 score over time for 2CDT (Figure 3.a), 1CSURR (Figure 3.b) and 4CRE-V1 (Figure 3.c) datasets. Points within the depicted evolution of this performance metric have been via a slidingwindow of B samples, with an overlap of 0.2B points between successive windows. On the righthand of every nested plot, we depict two rows of scatter plots. The ones in the top row denotethe samples collected during the stream periods highlighted in the plot on the right, in temporalorder and using color markers to denote the predicted class. The subplots in the bottom row canbe interpreted likewise, but here red markers denote those samples whose classes predicted byAiGAS-dEVL are wrong (white otherwise).\nIn response to RQ2, these plots expose several directions in which AiGAS-dEVL can be im-proved even further. To begin with, the analysis of limitations made in Subsection 3.3 anticipateda strong dependence of the algorithm on the model assumed for characterizing the drift dynamicsover time, which was realized by a rigid point set registration between the distribution of GNGnodes corresponding to successive stream batches. Then, AiGAS-dEVL assumes a smooth conti-nuity of such modeled drift dynamics, so that the new distribution of nodes is projected to estimatethe region in the feature space where they will reside upon the reception of a new stream. Thisassumption does not hold when the trajectory of concepts during the unsupervised period of thestream changes suddenly, breaking the assumption of its smoothness and/or the rigid nature of thegeometrical correspondence between GNG nodes. This can be noted in Figures 3.a and 3.b:\n\u2022 In the case of the 2CDT dataset (Figure 3.a), every class is characterizing by a single concept,which together delineate a linear drift trajectory from the (min, min) (lower left corner) to the(\u0442\u0430\u0445, max) point (upper right corner) in the feature space. The performance is noted to decayaround the middle of the unsupervised period of the stream, which coincides with the moment atwhich the two concepts suddenly start describing the same trajectory in the opposite direction.This sharp change yields a temporary mismatch between the estimated trajectory of the GNGnodes and the true distribution of classes in the feature space, giving rise to an increased numberof wrongly classified stream instances. Clearly, the impact of this mismatch on the overallclassification performance depends on whether the wrongly projected GNG nodes give rise to aNN decision boundary that does not follow the true distribution of classes in the stream. This"}, {"title": "6. Conclusions and Future Research Lines", "content": "This work has focused on the problem of dealing with extreme verification latency when mod-eling drifting data streams (dEVL). Under this assumption, concepts to be modeled inside thestreaming data are non-stationary and hence evolve over time, demanding functionalities in themodeling approach to accommodate such changes in its captured knowledge. The lack of annota-tion exacerbates the difficulty of adapting to concept drift even further, as there is no supervisorysignal from the stream to trace the correspondence between concepts and classes over time.\nOur work has tackled this problem from a new perspective by proposing an original modelingapproach coined as AiGAS-dEVL. Our proposal embraces GNG at its heart to extract prototypes(nodes) that characterize the data distribution over time. GNG is complemented with a matchingbetween the GNG nodes extracted from consecutive batches, as well as with a projective mappingof such nodes based on the estimated drift dynamics of the stream. This projection allows predict-ing samples within the next batch more accurately, especially in the case of slowly evolving drifts.Furthermore, the overall combination of these algorithmic ingredients permit to overcome com-"}]}