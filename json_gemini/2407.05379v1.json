{"title": "AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency", "authors": ["Maria Arostegi", "Miren Nekane Bilbao", "Jesus L. Lobo", "Javier Del Ser"], "abstract": "The ever-growing speed at which data are generated nowadays, together with the substantial cost of labeling processes (often reliant on human supervision) cause Machine Learning models to often face scenarios in which data are partially labeled, or delayed with respect to their query time. The extreme case where such a supervision is indefinitely unavailable is referred to as extreme verification latency. On the other hand, in streaming setups data flows are affected by exogenous factors that yield non-stationarities in the patterns to be modeled (concept drift), compelling models learned incrementally from the data streams to adapt their modeled knowledge to the prevailing concepts within the stream. In this work we address the casuistry in which these two conditions occur together over the data stream, by which adaptation mechanisms to accommodate drifts within the stream are challenged by the lack of supervision, hence requiring further mechanisms to track the evolution of concepts in the absence of verification. To this end we propose a novel approach, coined as AiGAS-dEVL (Adaptive Incremental neural GAS model for drifting Streams under Extreme Verification Latency), which relies on the use of growing neural gas to characterize the shape and inner point distributions of all concepts detected within the stream over time. Our approach exposes that the online analysis of the behavior of these prototypical points over time facilitates the definition of the evolution of concepts in the feature space, the detection of changes in their behavior, and the design of adaptation policies to mitigate the effect of such changes in the model. We assess the performance of AiGAS-dEVL over several synthetic datasets, comparing it to that of state-of-the-art approaches proposed in the recent past to tackle this stream learning setup. Our reported results reveal that AiGAS-dEVL performs competitively with respect to the rest of baselines, exhibiting a superior adaptability over several datasets in the benchmark while ensuring a simple and interpretable instance-based adaptation strategy.", "sections": [{"title": "1. Introduction", "content": "The proliferation of data across diverse domains has undergone an unprecedented surge in recent years, driven by the increasing digitization of processes in almost all domains of activity, and the widespread proliferation of connected devices. This exponential growth in data generation is further compounded by the escalating scales and speeds at which data are being produced and disseminated on a daily basis. From social media interactions and online transactions to sensor readings in industrial IoT environments, the sheer volume and velocity of data streams pose grand challenges for traditional methods for batch data processing and analysis. Consequently, there is a compelling need to develop novel techniques and algorithms capable of handling the dynamic nature of data streams in real time. This growing concern has motivated the emergence of different research areas focused on the study of new efficient learning models for data analysis, including Big Data analytics [1], real-time analytics [2] or, from a machine learning (ML) perspective, data stream learning [3, 4]. When data are produced continuously, ingested and processed in real time, strict restrictions arise in terms of memory and processing power [5, 6], eventually constraining the design of ML algorithms operating over such data.\nWhen addressing ML tasks formulated over data streams, the scalability of the learning algorithms is indeed a critical challenge, given the high volume and velocity of data being generated in a continuous fashion. Unfortunately, scalability is not the only challenge to be faced in such sce- narios: there exist other issues that can significantly impact the effectiveness and reliability of ML models in this domain. On one hand, the concept drift phenomenon poses a major hurdle to the characterization of patterns within the data flows, as the underlying data distribution may change over time [4, 7, 8, 9]. When concept drift occurs, models trained on historical data become less accurate or even obsolete, unless their knowledge is modified and adapted to the prevailing data distribution. Hence, adaptation to evolving concepts becomes imperative for ensuring a sustained model performance under concept drift. On the other hand, extreme verification latency introduces another layer of complexity, where the delay in obtaining ground truth labels for incoming data can lead to discrepancies between the predictions issued by the model and their true values. This delay not only hinders model evaluation, but also exacerbates the adaptation to drifting data, as decisions need to be made in real-time based on potentially outdated or incomplete information. Consequently, addressing these challenges requires not only scalable learning algorithms, but also robust mechanisms for concept drift detection and adaptation, as well as strategies for handling extreme verification latency, ensuring the reliability and effectiveness of ML models over evolving data flows.\nInterestingly under the scope of this work, most contributions reported to date around modeling over drifting data streams are focused on scenarios characterized by no latency, on the assump- tion of a so-called test-then-train scheme. Consequently, the scope of such studies is restricted to improving the effectiveness of the adaptation strategies therein proposed assuming that the super- vision of arriving data instances occurs immediately after they are predicted by the model at hand. Other works have instead addressed the case of a lagged label supervision with respect to the query time [10]. Ultimately, when this verification latency is large or even infinite (extreme as per the nomenclature used in the literature), an initial part of the data stream is assumed to be labeled, and after a certain point in time this supervision is no longer available. This models the situation"}, {"title": "2. Background and Related Work", "content": "Before proceeding with the detailed description of the proposed AiGAS-dEVL model, we first pause at the main milestones and advances attained recently in the fields of stream learning and concept drift (Section 2.1), extreme verification latency (Section 2.2) and GNG (Section 2.3), establishing the grounds to justify the contribution of AiGAS-dEVL over the reviewed literature (Section 2.4).\n2.1. Stream Learning and Concept Drift\nAs has been argued in the introduction to this work, the significance of stream learning has grown exponentially in recent years, reflecting its pivotal role in addressing the computational and learning challenges posed by the rapid influx and dynamic nature of streaming data. Quickly arriving instances in a data stream can be retained only for a limited time budget, which impacts on the design of data processing pipelines, especially in hardware with constrained computing and storage capabilities [16]. When the pipeline implies a ML model, constantly updating the knowledge captured by the model from the data stream is subject to the resources available in the device and the scales (volume and speed) at which data streams flow in the scenario at hand. As such, one can distinguish between on-line learning, where the ML model is updated when a new instances arrives sequentially from the stream, in a one-by-one fashion, whereas in incremental learning the model is updated over batches comprising a number of stream instances.\nSeveral comprehensive surveys on stream learning and concept drift have been published to date, emphasizing on the applications, challenges and taxonomies by which the plethora of con- tributions can be classified. Concept drift in data streams was formalized as early as 2009, capi- talizing on prior studies dealing with changes in the definitions of classes in learning tasks [17]. Concept drift was later expanded by follow-up contributions discussing on the major challenges of data stream mining [18, 19, 8, 20].\nConcept drift refers to the phenomenon where the underlying data distribution changes over time, leading to shifts in the relationships between input features and target variables. Several types of concept drift can be distinguished, each with distinct characteristics and implications for model adaptation. First, a sudden (sharp) concept drift occurs when the data distribution undergoes abrupt and significant changes, often resulting from unforeseen events or external factors. This type of drift poses a considerable challenge for ML models, as they must rapidly adjust to the new data distribution to maintain the level of performance achieved prior to the occurrence of the drift. In contrast, gradual concept drift involves more gradual and incremental changes in the data distribution over time, making it less immediately apparent but still requiring continuous model adaptation to ensure performance. Finally, recurring concept drift refers to patterns of change that occur periodically or intermittently, such as seasonal variations or cyclic trends, necessitating adaptive models capable of detecting and accommodating them effectively. Understanding the different types of concept drift is crucial for developing robust stream mining algorithms that can effectively handle the dynamic nature of streaming data [21].\nConcept drift adaptation strategies in stream mining can be broadly categorized into active and passive approaches [22]. Active adaptation strategies involve actively monitoring data streams and triggering model updates or retraining when concept drift is detected. Examples of active ap- proaches include drift detection algorithms that continuously analyze incoming data for deviations from the established model, prompting timely adjustments to ensure model accuracy. On the other hand, passive adaptation strategies involve building models that inherently adapt to changes in the data distribution without explicit drift detection mechanisms. These strategies often involve the use of dynamic learning algorithms, ensemble methods, or online learning techniques that incre- mentally update model parameters as new data arrives, allowing the model to adapt gradually to evolving concepts. While active approaches offer real-time responsiveness to concept drift, they may incur higher computational costs and require more sophisticated drift detection mechanisms. In contrast, passive approaches provide a more seamless and resource-efficient adaptation process, but can be less sensitive to sudden or unexpected changes in the data distribution.\n2.2. Extreme Verification Latency\nIn scenarios subject to sparse annotation [23] or ultimately, extreme verification latency, the scarcity or absence of incoming labeled data samples requires the use of different adaptation strate-"}, {"title": "3. Proposed AiGAS-dEVL Approach", "content": "In this section we delve into the algorithmic elements comprising the proposed AiGAS-dEVL model, including the mathematical definition of the dEVL problem (Subsection 3.1) and a detailed description of the proposed algorithm and its steps (Subsection 3.2). We end by analyzing the benefits and points of improvement of AiGAS-dEVL in Subsection 3.3.\n3.1. Problem Statement\nAn dEVL problem is defined on a stream of data instances {(xt, Yt)}=1, where xt \u2208 RN denotes\nthe data instance flowing at time t and yt \u2208 Y its label. We assume that labels are available for\nt < T\u300f, so that a model Mo(x) with parameters O can be learned over the samples during this\nsupervised period. For t > Ts, labels of the instances arriving from the stream are not available\nany longer, so the learned model is used to predict them as \u0177t = Mo(xt) for t > T. The challenge\nemerges when the distribution p(y|x) characterized by Me(x) is non-stationary, i.e., pt(y|x) \u2260\npt(y|x) for t,t' > T\u300f : t + t'. The goal is to endow the learned model Mo(\u00b7) with the capability\nto adjust its modeled knowledge to the changes (also referred to as concept drift) undergone by\nthe stream data distribution over time. The lack of supervision for t > T requires that the drift\ndynamics are gradual, so that the evolving concepts can be tracked by the augmented model in the\nabsence of supervision, i.e. without having access to {yt}t>Ts. We note that this definition also\napplies to the case of mixed drift, i.e., when both p(x) and p(y|x) change over time. This last case\naffects both the distribution of features and the decision boundary of the model, requiring more\nsophisticated adaptation mechanisms to adapt its knowledge.\n3.2. Description of AiGAS-dEVL\nThe design of our devised AiGAS-dEVL algorithm departs from the estimation of one of the\nkey parameters of the approach from the supervised part of the data stream: the initial number of\nnodes characterized by GNG. For the sake of computational efficiency, AiGAS-dEVL utilizes a\nsingle GNG algorithm to characterize pt(x) over time. In the case of a very unbalanced dataset,\nstream data instances belonging to the most prevalent classes can dominate the production of nodes\nby GNG, leaving the other classes underrepresented in the node distribution output by GNG. To\novercome this, the initial number of GNG nodes is adjusted as Go = (1+\u00a7)G, where G is an hiper-\nparameter and \u00a7 is given by the ratio of the number of samples of the most to the least populated\nclass in {xt, Yt}t<T.. This parameter is used by GNG to learn an initial map of nodes {x,00\nover {xt, Yt}t<T. Such nodes can be assigned a label y \u2208 Y by means of a classifier Mo,ini()\nlearned from {xt, Yt}t<T\u00bb, i.e. yg,\u2070 = Mo,ini (x0). While our experiments detailed later consider a\nKGNG-Nearest Neighbor algorithm NN(\u00b7) as Mo,ini(\u00b7), any other classifier can be used instead.\n\u7530,0\nAgg=1\n\u7530,0\nGo\nOnce this initial supervised period is over, extreme verification latency occurs, hence only\nunsupervised instances {xt}t>T\uff61are received. We assume that instances are received in batches,\nso that {x}t>T is redefined as {x}1, where b is the batch index and x6 = [x1,...,x], with"}, {"title": "4. Experimental Setup", "content": "In order to assess the performance of the proposed AiGAS-dEVL algorithm, several experi- ments are carried out to provide empirical evidence to the answers to two research questions (RQ):\n\u2022 RQ1: How does AiGAS-dEVL perform when compared to other approaches proposed for mod- eling drifting data streams with extreme verification latency?\n\u2022 RQ2: Do the points of improvement identified previously reflect on the performance of AiGAS- dEVL over time under different drift dynamics?\nDatasets. Our designed experimental setup considers a public repository of non-stationary data streams widely adopted by the stream mining community [11]. Specifically, the repository con- tains 15 synthetic datasets featuring gradual drift changes over time, and 2 real datasets. Table 1 summarizes their main characteristics, where drift interval refers to the interval in number of examples between consecutive drifts, and N and || refer to the number of features and classes, respectively. A column labeled as Drift is added to the table to specify whether the drift dynamics of every dataset are rectilinear. This aspect will be of relevance when discussing on the results obtained to address RQ2.\nThe first real dataset is ELEC2, which has become a typical benchmark dataset in streaming data classification. It was first described in [46] and used thereafter for several performance com- parisons [47, 48]. It contains data collected by the Australian New South Wales (NSW) Electricity Market, amounting to a total of 45, 312 instances. Once normalized and cleaned, 27, 552 instances dated from May 1996 to December 1998 are considered. Each example of the dataset has N = 5 attributes: the day of the week, the time stamp, the NSW electricity demand, the Vic electricity demand and the scheduled electricity transfer between states. The class label represents the change of the price (V = {UP, DOWN}) in New South Wales relative to a moving average of the last 24 hours. The distribution of classes in this dataset is not balanced, and the drift time is unknown.\nThe second real dataset is KEYSTROKE, which yields from the use of keystroke dynamics to identify users by their typing rhythm instead of simply relying on username and password"}, {"title": "5. Results and Discussion", "content": "We now present and discuss on the results obtained for every research questions formulated above:\nRQ1: How does AiGAS-dEVL perform when compared to other approaches proposed for modeling drifting data streams with extreme verification latency?\nThe results obtained for this first research question are summarized in Table 2 (average pre- quential error) and Table 3 (average macro F1 score). In these tables, the best and second best counterparts for every dataset are highlighted as and , respectively. The last two rows inform about the mean and standard deviation statistics of every algorithm computed over all datasets. The statistical significance of the gaps is discussed later.\nWe begin our discussion by analyzing the prequential error statistics in Table 2. As expected due to the non-stationarity of the stream datasets under consideration, methods not designed to cope with this circumstance (namely, STC, SLD and INC) yield in general comparatively poor results. However, a closer inspection of the results reveals that in three datasets (GEARS, 4CRT, FG2C2D and 4CEF1CF), the prequential error of these naive methods gets closer to that of the rest of algorithms in the benchmark. Conversely, the results of baselines that encompass adaptive mechanisms to deal with concept drifts under unsupervised stream data regimes, such as CMP or LVL, show an improvement when compared to the aforementioned naive methods. Nevertheless, they still lag behind those obtained by A-FCP, A-DCP, SLAYER and the proposed AiGAS- dEVL. The statistics at the bottom of the table evince that our proposal performs best on average across the datasets under consideration, yielding in addition a lower variability of the prequential error.\nWhen turning the scope of our discussion to the F1 scores collected in Table 3, similar conclu- sions can be drawn. While non-adaptive techniques do perform well in some datasets, in general the benchmark is dominated by those including adaptation funcionalities in their algorithmic de- sign. Remarkably, a wider spread is observed in terms of ranking, yet performance differences between the best approaches for every dataset are narrower than in the case of the prequential er- ror. There emerges the need for examining qualitatively whether there is any relationship between the drift dynamics of the datasets and the performance behavior of the algorithms in the bench- mark. This is indeed the motivation of RQ2, which will be later addressed in our discussions. In regards to the quantitative analysis made in response to RQ1, the narrower performance gaps require further statistical analysis to shed light on the significance of the performance differences in Tables 2 and 3.\nPlots nested in Figure 2 expose that AiGAS-dEVL clearly dominates the benchmark with sta- tistical significance for rope equal to 0.2 (top row) and 0.1 (bottom row). Differences in terms of mean and standard deviation noted in Table 2 reflect on these plots as well, with the cases compar- ing AiGAS-dEVL to SLAYER and A-FCP being the only ones in which the posterior probability spans the three regions of the barycentric triangle. As expected, the posterior probability distribu- tion in each of these regions become more separate apart from each other when the rope parameter is lower.\nIn response to RQ1, Tables 2 and 3, together with Figure 2, confirm that the proposed AiGAS-dEVL establishes a new performance landmark in modeling drifting data streams under extreme verification latency.\nRQ2: Do the points of improvement identified previously reflect on the performance of AiGAS- dEVL over time under different drift dynamics?\nTo give an informed response to this second question, we proceed by analyzing how the per- formance scores evolve over time for different datasets. We focus on some of them that besides linking closely to the motivation for the research question, correspond to drift dynamics that can be inspected visually and interpreted geometrically, so that the performance decays can be attributed to non-rigid and/or non-smooth transformations, for which the projection in use within the naive AiGAS-dEVL design (line 13 in Algorithm 1) may not suffice.\nTo this end, we focus first on the plots included in Figure 3, which depict the evolution of the macro F1 score over time for 2CDT (Figure 3.a), 1CSURR (Figure 3.b) and 4CRE-V1 (Figure 3.c) datasets. Points within the depicted evolution of this performance metric have been via a sliding window of B samples, with an overlap of 0.2B points between successive windows. On the right hand of every nested plot, we depict two rows of scatter plots. The ones in the top row denote the samples collected during the stream periods highlighted in the plot on the right, in temporal order and using color markers to denote the predicted class. The subplots in the bottom row can be interpreted likewise, but here red markers denote those samples whose classes predicted by AiGAS-dEVL are wrong (white otherwise).\nIn response to RQ2, these plots expose several directions in which AiGAS-dEVL can be im- proved even further. To begin with, the analysis of limitations made in Subsection 3.3 anticipated a strong dependence of the algorithm on the model assumed for characterizing the drift dynamics over time, which was realized by a rigid point set registration between the distribution of GNG nodes corresponding to successive stream batches. Then, AiGAS-dEVL assumes a smooth conti- nuity of such modeled drift dynamics, so that the new distribution of nodes is projected to estimate the region in the feature space where they will reside upon the reception of a new stream. This assumption does not hold when the trajectory of concepts during the unsupervised period of the stream changes suddenly, breaking the assumption of its smoothness and/or the rigid nature of the geometrical correspondence between GNG nodes. This can be noted in Figures 3.a and 3.b:\n\u2022 In the case of the 2CDT dataset (Figure 3.a), every class is characterizing by a single concept, which together delineate a linear drift trajectory from the (min, min) (lower left corner) to the (\u0442\u0430\u0445, max) point (upper right corner) in the feature space. The performance is noted to decay around the middle of the unsupervised period of the stream, which coincides with the moment at which the two concepts suddenly start describing the same trajectory in the opposite direction. This sharp change yields a temporary mismatch between the estimated trajectory of the GNG nodes and the true distribution of classes in the feature space, giving rise to an increased number of wrongly classified stream instances. Clearly, the impact of this mismatch on the overall classification performance depends on whether the wrongly projected GNG nodes give rise to a NN decision boundary that does not follow the true distribution of classes in the stream. This\nis indeed what occurs in the 2CDT dataset, as opposed to other datasets in which the mismatch does not affect the separability between classes (e.g. 1CHT, which features a rectilinear drift with a sharp direction change, but only undergone by one of the concepts).\n\u2022 A similar statement holds for the 1CSURR dataset (Figure 3.b), in which only the concept de- scribing one of the two classes moves relative to the other. This concept evolves by traversing the four corners of the feature space region over which both classes are distributed. It can be noticed that the performance decays when the evolving concept changes its direction to advance towards the next corner.\nAnother shortcoming identified in our discussion about limitations is the lack of memory in the assignment of labels to the GNG nodes computed for every new batch in the stream (line 11 in Algorithm 1). This can be detrimental in those cases where the drift dynamics make the distribution of several classes coexist in the same feature region. This effect is exemplified by the 4CRE-V1 dataset (Figure 3.c), in which performance decays noticeably every time the concepts of the four classes collide with each other (as shown by the first, third and fifth regions depicted on the right subplots). A second smaller performance degradation occurs when the concepts, which undergo a continuous circular movement around the center of the region they occupy, expand and shrink suddenly along the radial axis. As mentioned before, sudden changes in the drift dynamics can affect the accuracy of predictions made by AiGAS-dEVL. However, in this case it can successfully recover the correct mapping between labels and evolving concepts, adapting better than other methods considered in the benchmark.\nWe follow our discussion around RQ2 with the plots in Figure 4, which depict similar plots to the previous ones, yet for the MG2C2D (Figure 4.a) and GEARS (Figure 4.b) datasets. The first case exemplifies the efficacy of AiGAS-dEVL to faithfully track the correspondence between GNG nodes and labels in a stream comprising two classes and several intertwined concepts per class. A performance degradation occurs due to the proximity between concepts in several parts of the stream, mainly due to the proximity between concepts of different classes and the corner cases in-between. A remarkable drop when concepts belonging to different classes coexist in the same feature space (first depicted region). However, once such concepts separate from each other, the point set matching and projection functionalities included at the algorithmic core of AiGAS-dEVL excel at recovering the performance level prior to the collision between concepts. The plots in Figure 4.b corresponding to the GEARS dataset showcase the benefits of using GNG for the characterization of complex data that evolve over time. Indeed, the helix-shaped rotating data distributions of both classes in this dataset benefit from the adaptability of GNG, only failing to predict accurately instances in the boundary between both classes, and ultimately achieving notably superior performance metrics than its counterparts in Tables 2 and 3.\nIn response to RQ2, our discussion around Figures 3 and 4 concludes that the evolution of performance along time is in close match with the benefits and points of improvement identified in Section 3.3, including the need for tailoring the selection of the projection to the drift dynamics of the stream, the effect of colliding concepts on the separability between classes, and the adaptability of GNG to complex data distributions."}, {"title": "6. Conclusions and Future Research Lines", "content": "This work has focused on the problem of dealing with extreme verification latency when mod- eling drifting data streams (dEVL). Under this assumption, concepts to be modeled inside the streaming data are non-stationary and hence evolve over time, demanding functionalities in the modeling approach to accommodate such changes in its captured knowledge. The lack of annota- tion exacerbates the difficulty of adapting to concept drift even further, as there is no supervisory signal from the stream to trace the correspondence between concepts and classes over time.\nOur work has tackled this problem from a new perspective by proposing an original modeling approach coined as AiGAS-dEVL. Our proposal embraces GNG at its heart to extract prototypes (nodes) that characterize the data distribution over time. GNG is complemented with a matching between the GNG nodes extracted from consecutive batches, as well as with a projective mapping of such nodes based on the estimated drift dynamics of the stream. This projection allows predict- ing samples within the next batch more accurately, especially in the case of slowly evolving drifts. Furthermore, the overall combination of these algorithmic ingredients permit to overcome com-"}]}