{"title": "Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection", "authors": ["Xinhao Luo", "Man Yao", "Yuhong Chou", "Bo Xu", "Guoqi Li"], "abstract": "Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driven by extending virtual timesteps during inference. The proposed method is validated on both static and neuromorphic object detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50 and 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior state-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we achieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent architecture, and the energy efficiency is improved by 5.7x.", "sections": [{"title": "1 Introduction", "content": "Brain-inspired SNNs are known for their low power consumption [46,48]. Spiking neurons incorporate spatio-temporal information and emit spikes when the membrane potentials exceed a threshold [36]. Thus, spiking neurons trigger sparse additions only when they receive a spike and are otherwise idle. This spike-driven enables SNNs to exhibit obvious low-power advantages over ANNS when deployed on neuromorphic chips [7,37,41,61]. However, the negative impact of complex neuronal dynamics and spike-driven nature is that SNNs are difficult to train and have limited task performance and application scenarios [13].\nFor example, most applications of SNN algorithms in computer vision are limited to simple image classification tasks [9, 14, 20, 25, 27, 31, 33, 42, 54, 60]. Another more commonly used and challenging computer vision task, object detection, is rarely explored in SNNs. In 2020, Spiking-YOLO [29] provided the first object detection model in deep SNNs, exploiting the method of converting ANN to SNN with thousands of timesteps. In 2023, EMS-YOLO [50] became the first work to use direct training SNNs to handle object detection. Recently, the direct training Meta-SpikeFormer [58] can process the object detection in a pre-training and fine-tuning manner for the first time. However, the performance gap between these works and ANNs is significant. In this work, we aim to bridge this gap and demonstrate the low-power of SNNs and their unique advantages in neuromorphic applications. We achieve this goal through two efforts.\nFirst, we design a new architecture, SpikeYOLO, which combines the macro design of YOLO with the micro design of the SNN module. Simply replacing the artificial neurons in the YOLO series [1,43,52] with spiking neurons generally does not work. Existing solutions include establishing the equivalence between ANN activation and spike firing rate [29], or improving the residual design [50]. We argue that another potential reason is that the module design in the YOLO series is too complex, which is effective in ANNs, but not suitable for SNNs. We observe that after the complex YOLO modules are converted into the corresponding spiking versions, there is a phenomenon of spike degradation in which the deep layers almost no longer emit spikes. Therefore, we tend to simplify the design in SpikeYOLO. We only retain the macro architecture in YOLOv83 while using the simple meta SNN block in Meta-SpikeFormer [58] as the basic module and performing micro design.\nSecond, we design a novel spiking neuron, Integer Leaky Integrate-and-Fire (I-LIF), to reduce the quantization error of SNNs. Spike-driven is the key to low-power, while exploiting only binary signals will drop performance, especially in challenging object detection. Numerous studies have attempted to mitigate this problem, such as attention-based membrane optimization [63], ternary spike [18], information max [19], converting quantized ANNs [26], optimal ANN2SNN [2]. However, direct training based on optimization strategies can only alleviate errors; the optimal approximation of ANN2SNN requires large timesteps, and it is difficult to exploit the temporal information. In contrast, the idea of the proposed I-LIF is to use integer-valued activations to drop quantization errors and convert them into spikes during inference by extending the virtual timesteps. The features of I-LIF are: 1) Integer-valued training improves performance and is easy to train; 2) Integer activation during training can be equivalent to spike-driven"}, {"title": "2 Related Works", "content": "Brain-inspired SNNs are known for their low power consumption [46,48]. Spiking neurons incorporate spatio-temporal information and emit spikes when the membrane potentials exceed a threshold [36]. Thus, spiking neurons trigger sparse additions only when they receive a spike and are otherwise idle. This spike-driven enables SNNs to exhibit obvious low-power advantages over ANNS when deployed on neuromorphic chips [7,37,41,61]. However, the negative impact of complex neuronal dynamics and spike-driven nature is that SNNs are difficult to train and have limited task performance and application scenarios [13].\nFor example, most applications of SNN algorithms in computer vision are limited to simple image classification tasks [9, 14, 20, 25, 27, 31, 33, 42, 54, 60]. Another more commonly used and challenging computer vision task, object detection, is rarely explored in SNNs. In 2020, Spiking-YOLO [29] provided the first object detection model in deep SNNs, exploiting the method of converting ANN to SNN with thousands of timesteps. In 2023, EMS-YOLO [50] became the first work to use direct training SNNs to handle object detection. Recently, the direct training Meta-SpikeFormer [58] can process the object detection in a pre-training and fine-tuning manner for the first time. However, the performance gap between these works and ANNs is significant. In this work, we aim to bridge this gap and demonstrate the low-power of SNNs and their unique advantages in neuromorphic applications. We achieve this goal through two efforts.\nFirst, we design a new architecture, SpikeYOLO, which combines the macro design of YOLO with the micro design of the SNN module. Simply replacing the artificial neurons in the YOLO series [1,43,52] with spiking neurons generally does not work. Existing solutions include establishing the equivalence between ANN activation and spike firing rate [29], or improving the residual design [50]. We argue that another potential reason is that the module design in the YOLO series is too complex, which is effective in ANNs, but not suitable for SNNs. We observe that after the complex YOLO modules are converted into the corresponding spiking versions, there is a phenomenon of spike degradation in which the deep layers almost no longer emit spikes. Therefore, we tend to simplify the design in SpikeYOLO. We only retain the macro architecture in YOLOv83 while using the simple meta SNN block in Meta-SpikeFormer [58] as the basic module and performing micro design.\nSecond, we design a novel spiking neuron, Integer Leaky Integrate-and-Fire (I-LIF), to reduce the quantization error of SNNs. Spike-driven is the key to low-power, while exploiting only binary signals will drop performance, especially in challenging object detection. Numerous studies have attempted to mitigate this problem, such as attention-based membrane optimization [63], ternary spike [18], information max [19], converting quantized ANNs [26], optimal ANN2SNN [2]. However, direct training based on optimization strategies can only alleviate errors; the optimal approximation of ANN2SNN requires large timesteps, and it is difficult to exploit the temporal information. In contrast, the idea of the proposed I-LIF is to use integer-valued activations to drop quantization errors and convert them into spikes during inference by extending the virtual timesteps. The features of I-LIF are: 1) Integer-valued training improves performance and is easy to train; 2) Integer activation during training can be equivalent to spike-driven"}, {"title": "3 Methods", "content": "We exploit SpikeYOLO to process both static and neuromorphic object detection datasets. We first introduce how network inputs are unified. Then, the details of SpikeYOLO architecture and I-LIF spiking neuron are presented, respectively."}, {"title": "3.1 Network Input", "content": "The input of SNNs can be denoted as $X \\in R^{T \\times C \\times H \\times W}$, where $T$ is the timestep, $C$ is the channel, $H \\times W$ denote the spatial resolution.\nStatic Image. To leverage the spatio-temporal capabilities of SNNs, it is common practice that static images are repeated and utilized as input for each timestep $T$. This is called direct input encoding [30,56], where the first layer of spiking neurons in the network encodes the continuous values of the input into spike signals.\nNeuromorphic Event Stream. Neuromorphic data (also known as event-based vision) are generated by a Dynamic Vision Sensor (DVS), which only generates spikes when the logarithmic change in light intensity at a pixel surpasses a predefined threshold. An event-based stream is characterized as $(x_n, y_n, t_n, p_n)$,"}, {"title": "3.2 Spike YOLO Architecture", "content": "Overview. SpikeYOLO integrates the macro design of the YOLOv8 with the micro design of Meta-SpikeFormer [58]. The motivation is that we observe complex computations within YOLO's modules result in spike degradation [27] upon direct conversion to the SNN version. Consequently, we maintain the overarching design principles of the YOLO architecture while incorporating the inverted residual structure [47] and re-parameterization convolution [11] design of the Meta-SpikeFormer for detailed aspects."}, {"title": "Network Output", "content": "In object detection, the network outputs the class and position of each object based on the input image sequence $X = \\{X_t\\}_{t=1}^{T}$. Suppose the input image sequence has N goals, the output $B = \\{B_n\\}_{n=1}^{N}$ can be calculated:\n$B = Model (X),$\t\t(1)\nwhere each $B_n = \\{f_n,C_n,X_n, Y_n, W_n, h_n\\}$ contains information about degree of confidence $f_n$, class $C_n$, center coordinates $(X_n, Y_n)$ and target size $(w_n,h_n)$. Model (.) refers to the proposed SpikeYOLO architecture."}, {"title": "Macro Design", "content": "Fig. 1 shows the overview of SpikeYOLO, a variation of the YOLO framework that is more suitable for the feature extraction scheme of SNNs. Specifically, YOLOv8 is a classic single-stage detection framework that partitions the image into numerous grids, with each grid responsible for predicting a target independently. Some classic designs, such as the feature pyramid network [34] in YOLOv8, play a crucial role in facilitating efficient feature extraction and fusion. By contrast, its feature extraction module, such as C2F, performs repeated feature extraction from the same set of feature maps. This module can enhance feature extraction in ANNs but does not work well in SNNs. As a compromise, we preserve the classic Backbone/Neck/Head architecture in YOLOv8 while incorporating strategies from the meta SNN block in Meta-SpikeFormer."}, {"title": "Micro Design", "content": "Meta-SpikeFormer [58] is the current state-of-the-art architecture in SNNs, which explores the meta design of SNN and consists of CNN-based and Transformer-based SNN blocks. The meta block comprises a token mixer module and a channel mixer module. The difference between CNN-based and Transformer-based SNN blocks lies in the token mixer, which are spike-driven convolution and spike-driven self-attention, respectively. In this work, we mainly redesign the channel mixer module for the object detection task. As shown in Fig. 1, SNN-Block-1 and SNN-Block-2 are designed to extract low-stage and high-stage features, respectively.\nThe meta SNN block in [58] can be written as:\n$U' = U + SepConv (U),$\t\t(2)\n$U'' = U' + ChannelConv (U'),$\t\t(3)\nwhere $U \\in R^{T \\times C \\times H \\times W}$ is the layer input, $SepConv (\\cdot)$ is an inverted separable convolution module [47] with 7 \u00d7 7 kernel size in MobileNetv2 to capture global features, followed by a 3 \u00d7 3 depthwise convolution for further spatial feature fusion. Sepconv (\u00b7) can be expressed as:\n$SepConv (U) = Conv_{dw2} (Conv_{pw2} (SN (Conv_{dw1} (SN (Conv_{pw1} (SN (U))))))),$(4)\nwhere $Conv_{dw1} (\\cdot)$ and $Conv_{dw1} (\\cdot)$ are depthwise convolutions, $Conv_{pw1} (\\cdot)$ and $Conv_{pw1} (\\cdot)$ are pointwise convolutions [5]. SN(.) is the spiking neuron layer."}, {"title": "3.3 I-LIF Spiking Neuron", "content": "Spiking neurons propagate information in both spatial and temporal domains, and they mimic the spiking communication scheme of biological neurons. However, there are inherent quantization errors in converting the membrane potential of spiking neurons into binary spikes, which severely limits the representation of the model. Recently, Fast-SNN [26] achieves high-performance conversion with small timesteps by converting quantized ANNs into SNNs. This inspires us to \"why not train directly with integer values\", which can significantly reduce the quantization error. We just need to be careful to ensure that the inference is spike-driven. So, we came up with the idea of I-LIF.\nLIF. Leaky Integrate-and-Fire (LIF) spiking neuron [36] is the most popular neuron to construct SNNs due to its balance between bio-plausibility and computing complexity. The dynamics of LIF with soft reset is:\n$U [t] = H [t - 1] + X [t],$\t\t(8)\n$S [t] = \\Theta (U [t] - V_{th}),$\t\t(9)\n$H [t] = \\beta (U [t] - S [t]),$\t\t(10)\nwhere t denotes the timestep, $U [t]$ is the membrane potential that integrates the temporal information $H [t - 1]$ and spatial information $X [t]$. $\\Theta (\\cdot)$ is the Heaviside step function which equals 1 for $x > 0$ and 0 otherwise. If $U [t]$ exceeds the firing threshold $V_{th}$, spiking neuron fire a spike $S [t]$ and $U [t]$ will subtract it subsequently. Otherwise, $H [t]$ will remain unchanged. And, $U [t]$ decays to $H [t]$ by a factor of $\u00df$, which denotes the decay constant. For simplicity, we focus on Eq.9 and denote the spiking neuron layer as SN(\u00b7), with its input as membrane potential tensor U and its output as spike tensor S.\nI-LIF. We propose the Integer Leaky Integrate-and-Fire (I-LIF) neuron to reduce the quantization error. As shown in Fig. 2, I-LIF emits integer values while training, and converts them into 0/1 spikes when inference. Specifically, in I-LIF, Eq.9 is rewritten as:\n$S [t] = Clip (round(U[t]), 0, D),$\t\t(11)\nwhere $round(\\cdot)$ is a round symbol, $Clip (x, min, max)$ denotes that clipping x to [min, max], D is a hyperparameter indicating the maximum emitted integer value by I-LIF.\nTraining Stage. Eq.11 is not a continuous function, making its derivative a step function, potentially causing training instability. Previous studies have introduced several surrogate gradient functions, which primarily address binary spike outputs. We consistently utilize rectangular windows as the surrogate function. For simplicity, We retain gradients solely for neurons activated in the [0, D] range, nullifying all others.\nInference Stage. Introducing integer value necessitates additional MACS (Multiply-Accumulation operations), potentially diminishing the energy efficiency of SNNs. Thus, converting integer values to binary spikes is essential. Fig. 3 shows an example of how integer values convert to binary spikes by extending virtual timesteps during inference. Specifically, the input to the spiking neuron at 1 + 1 layer can be described as $X^{l+1} [t] = W^l S^l [t]$. We extend the T time step to T \u00d7 D, and convert the integer value $S^l [t]$ to a spike sequence \\{$S^l [t, d]\\}_{d=1}^{D}$, which satisfied:\n$\\sum_{d=1}^{D} S^l [t, d] = S^l [t].$\t\t(12)\nThus, the neuron's input at 1 + 1 layer is reformulated as:\n$X^{l+1} [t] = W^l S^l [t, d] .$\t\t(13)\nGiven that matrix multiplication functions as linear operators, we establish:\n$\\sum_{d=1}^{D} W^l S^l [t, d] = \\sum_{d=1}^{D} (W^l S^l [t, d]).$\t\t(14)\nTherefore, the input of the neuron at 1+1 layer can be computed by:\n$X^{l+1} [t] = \\sum_{d=1}^{D} (W^l S^l [t, d]).$\t\t(15)\nThe spike sequence $S^l [t, d]$ only contains 0/1, so all MACs can be converted into sparse ACs (Accumulation operations), which can ensure spike-driven when inference."}, {"title": "4 Experiments", "content": "We evaluate the proposed method on COCO 2017 val [35] and neuromorphic Gen1 [8] datasets. The mean Average Precision(mAP) at IOU=0.5(mAP@50), the average mAP between 0.5 and 0.95(mAP@50:95), and energy cost are reported for each model. Specifically, the power of ANNs and SNNs can be calculated as:\n$E_{ANN} = O^2 \\times C_{in} \\times C_{out} \\times k^2 \\times E_{MAC},$\t\t(16)\nwhere $O$ is the feature output size, $C_{in}$ and $C_{out}$ denotes the number of input channel and output channel, $k$ is the kernel size, $fr$ denotes the average spike firing rate, $T$ is the timestep, $D$ is the upper limit of integer activation during training. We follow the most commonly used energy consumption evaluation method in the SNN field [40, 63, 64]. All operations assume a 32-bit floating-point implementation on 45nm technology, where $E_{MAC} = 4.6pJ$ and $E_{AC} = 0.9pJ$ [24]. As can be seen from Eq. 16 and 17, SNN's low power comes from its sparse addition operation. The fewer spikes, the sparser the computation."}, {"title": "4.1 COCO 2017 val Dataset", "content": "Experimental Setup. As a predominant static dataset for object detection, COCO 2017 val [35] comprises 80 classes split into 118K training and 5K validating images. In all experiments, we set decay factor \u03b2 = 0.25, learning rate to 0.01, and adopt SGD optimizer. The models are trained for 300 epochs with a batch size of 40 on 4 NVIDIA V100 GPUs. Mosaic data augmentation [1] technique is employed. The network structure is given in the supplementary material. Note,"}, {"title": "4.2 Gen1 Automotive Detection Dataset", "content": "Experimental Setup. As a large neuromorphic object detection dataset, Gen1 [8] encompasses 39 hours of open road and various driving scenarios, captured using an ATIS sensor with a resolution of 304\u00d7240 pixels. The dataset is organized into training, validation, and testing subsets. The bounding box annotations of pedestrians and cars(over 255,000) were manually labeled. For each annotation, we process the event-based stream 2.5 seconds before its occurrence, dividing it into T slices for model input. We train the model for 50 epochs and maintain other hyperparameters same as the COCO 2017 dataset.\nMain Results on Gen1 dataset are shown in Table 3. The proposed SpikeY-OLO notably elevates the performance benchmark for the Gen1 dataset in SNNs. We achieve 67.2% mAP@50 with 23.1M parameters, which outperforms the prior state-of-the-art SNN model by +8.2%. For example, when T = 5, SpikeYOLO vs. EMS-YOLO [50]: Param, 13.2M vs. 14.4M; mAP@50, 66.0% vs. 59.0%; mAP@50:95, 38.5% vs. 31.0%. In contrast to the COCO dataset, Gen1 contains temporal information that is more suitable for SNN processing. We conduct experiments on the performance of SNN and ANN with the same architecture. We observe that SpikeYOLO's mAP@50 accuracy is +2.5% higher than the corresponding ANN, and shows a 5.7\u00d7 energy efficiency. This indicates that SNN has attractive potential in processing neuromorphic data.\nAblation Studies of Quantization Error. Both T and D significantly influence outcomes when processing neuromorphic datasets. Table 4 gives a com-prehensive ablation study on SpikeYOLO with 23.1M parameters that evaluate the effects of varying T and D. We observe some interesting experimental results. First, boosting the timestep T will bring improvement in accuracy and power. For instance, with the set of D = 1, extending T = 1 to T = 4 yields a +6.7% increase in mAP@50, and the power will increase by 3.7\u00d7. But further extending T = 4 to T = 8 results in a marginal increase of only +0.7% and significantly"}, {"title": "4.3 Architecture Ablation Experiments", "content": "Re-parameterization Design. As shown in Table 5, if we remove re-parameteri-zation by adding neurons into inverted separable convolutions, the mAP@50 and mAP@50:95 will decrease 1.7% and 1.8% respectively.\nSNN Block Design. Including a 3\u00d73 standard convolution within the initial stages of convolution blocks is crucial. As shown in Table 5, substituting SNN-Block-1 for SNN-Block-2 leads to a reduction in performance of around 1%. Moreover, we try to replace high-stage SNN-Block-2 with meta Transformer-based SNN block, just like Meta-SpikeFormer [58]. We find that there is little to no performance gain by doing this and that the parameters increase. Therefore, only spiking CNN blocks are exploited in our SpikeYOLO.\nDetection Head. The detection mechanisms within YOLO are categorized into anchor-based heads(e.g., YOLOv5) and anchor-free heads(e.g., YOLOv8). The former directly predicts each bounding box's dimensions, whereas the latter estimates the probability distribution of each bounding box. Previous EMS-YOLO [50] and Meta-SpikeFormer [58] employ anchor-based heads. SpikeYOLO exploits the anchor-free head because of its higher accuracy (see Table 5)."}, {"title": "5 Conclusion", "content": "This work significantly narrows the performance gap between SNNs and ANNs on object detection tasks. We achieve this through network architecture and spiking neuron design. The proposed SpikeYOLO architecture abandons the complex module design in the vanilla YOLO series and exploits simple meta spike blocks to build the model. Then, the I-LIF spiking neuron capable of integer-valued training and spike-driven inference is proposed to drop quantization errors. We improve the upper bound of the SNN domain's performance on the COCO dataset by +15.0% (mAP@50) and +18.7% (mAP@50:95), respectively. On the neuromorphic Gen1 dataset, SpikeYOLO achieves better performance and lower power than ANN of the same architecture. Furthermore, we investigate the performance of equivalent architecture ANNs and SNNs in different datasets, and the results show that the redesigned SNN architecture performed better. This work enables SNNs to handle complex object detection and can inspire the application of SNNs in more visual scenarios."}]}