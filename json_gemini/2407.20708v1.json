{"title": "Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection", "authors": ["Xinhao Luo", "Man Yao", "Yuhong Chou", "Bo Xu", "Guoqi Lil"], "abstract": "Brain-inspired Spiking Neural Networks (SNNs) have bio- plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driven by extending virtual timesteps during inference. The proposed method is validated on both static and neuromorphic object detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50 and 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior state-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we achieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent architecture, and the energy efficiency is improved by 5.7x.", "sections": [{"title": "1 Introduction", "content": "Brain-inspired SNNs are known for their low power consumption [46,48]. Spiking neurons incorporate spatio-temporal information and emit spikes when the"}, {"title": "2 Related Works", "content": "SNN Training Method. Training methods have restricted the development of SNN for a long time. To make SNNs deeper, two training methods have been developed. ANN2SNN substitutes the ReLU function with spiking neurons, aiming to mimic the continuous activation by controlling the firing rate of spiking neurons [3, 10, 49]. It can achieve high performance but often requires long timesteps and is difficult to process sequence tasks that exploit the spatio- temporal dynamic nature of SNNs. In contrast, another directly training an SNN leverage gradient surrogate to circumvent the non-differentiability of binary spikes [39,55]. Direct training is more flexible and requires fewer timesteps, but its performance usually suffers compared to ANNs of the same architecture. In this work, we focus on utilizing directly trained SNNs to process object detection due to its more flexibility in architectural design.\nSNN Architecture Design. The architecture of SNNs can be roughly divided into two categories: CNN-based and Transformer-based SNNs. Spiking ResNet has long dominated the SNN field because residual learning [22] can address the performance degradation of SNNs as they become deeper. Typical spiking ResNet includes vanilla spiking ResNet [68], SEW-ResNet [14], and MS- ResNet [27]. The main difference between them is the location of shortcuts and the ability to achieve identity mapping [23]. Recently, the Transformer [12,51] architecture has become popular [21,38,58,60,66,67,67,69] in the SNN field and"}, {"title": "3 Methods", "content": "We exploit SpikeYOLO to process both static and neuromorphic object detection datasets. We first introduce how network inputs are unified. Then, the details of SpikeYOLO architecture and I-LIF spiking neuron are presented, respectively."}, {"title": "3.1 Network Input", "content": "The input of SNNs can be denoted as $X \\in \\mathbb{R}^{T \\times C \\times H \\times W}$, where $T$ is the timestep, $C$ is the channel, $H \\times W$ denote the spatial resolution.\nStatic Image. To leverage the spatio-temporal capabilities of SNNs, it is common practice that static images are repeated and utilized as input for each timestep $T$. This is called direct input encoding [30,56], where the first layer of spiking neurons in the network encodes the continuous values of the input into spike signals.\nNeuromorphic Event Stream. Neuromorphic data (also known as event- based vision) are generated by a Dynamic Vision Sensor (DVS), which only generates spikes when the logarithmic change in light intensity at a pixel surpasses a predefined threshold. An event-based stream is characterized as $(x_n, y_n, t_n, p_n)$,"}, {"title": "3.2 Spike YOLO Architecture", "content": "Overview. SpikeYOLO integrates the macro design of the YOLOv8 with the micro design of Meta-SpikeFormer [58]. The motivation is that we observe complex computations within YOLO's modules result in spike degradation [27] upon direct conversion to the SNN version. Consequently, we maintain the overarching design principles of the YOLO architecture while incorporating the inverted residual structure [47] and re-parameterization convolution [11] design of the Meta-SpikeFormer for detailed aspects."}, {"title": "Network Output", "content": "In object detection, the network outputs the class and position of each object based on the input image sequence $X = {X_t}_{t=1}^T$. Suppose the input image sequence has $N$ goals, the output $B = {B_n}_{n=1}^N$ can be calculated:\n$B = \\text{Model}(X),                                                                                                                  (1)$\nwhere each $B_n = {f_n, C_n, x_n, y_n, w_n, h_n}$ contains information about degree of confidence $f_n$, class $C_n$, center coordinates $(x_n, y_n)$ and target size $(w_n, h_n)$. Model $(.)$ refers to the proposed SpikeYOLO architecture."}, {"title": "Macro Design", "content": "Fig. 1 shows the overview of SpikeYOLO, a variation of the YOLO framework that is more suitable for the feature extraction scheme of SNNs. Specifically, YOLOv8 is a classic single-stage detection framework that partitions the image into numerous grids, with each grid responsible for predicting a target independently. Some classic designs, such as the feature pyramid network [34] in YOLOv8, play a crucial role in facilitating efficient feature extraction and fusion. By contrast, its feature extraction module, such as C2F, performs repeated feature extraction from the same set of feature maps. This module can enhance feature extraction in ANNs but does not work well in SNNs. As a compromise, we preserve the classic Backbone/Neck/Head architecture in YOLOv8 while incorporating strategies from the meta SNN block in Meta-SpikeFormer."}, {"title": "Micro Design", "content": "Meta-SpikeFormer [58] is the current state-of-the-art architecture in SNNs, which explores the meta design of SNN and consists of CNN-based and Transformer-based SNN blocks. The meta block comprises a token mixer module and a channel mixer module. The difference between CNN-based and Transformer-based SNN blocks lies in the token mixer, which are spike-driven convolution and spike-driven self-attention, respectively. In this work, we mainly redesign the channel mixer module for the object detection task. As shown in Fig. 1, SNN-Block-1 and SNN-Block-2 are designed to extract low-stage and high-stage features, respectively.\nThe meta SNN block in [58] can be written as:\n$U' = U + \\text{SepConv}(U),                                                                                                    (2)$\n$U'' = U' + \\text{ChannelConv}(U'),                                                                                            (3)$\nwhere $U \\in \\mathbb{R}^{T \\times C \\times H \\times W}$ is the layer input, $\\text{SepConv} (\\cdot)$ is an inverted separable convolution module [47] with 7 \u00d7 7 kernel size in MobileNetv2 to capture global features, followed by a 3 \u00d7 3 depthwise convolution for further spatial feature fusion. $\\text{Sepconv} (\\cdot)$ can be expressed as:\n$\\text{SepConv}(U) = \\text{Conv}_{dw2} (\\text{Conv}_{pw2} (\\text{SN} (\\text{Conv}_{dw1} (\\text{SN} (\\text{Conv}_{pw1} (\\text{SN} (U))))))),(4)$\nwhere $\\text{Conv}_{dw1} (\\cdot)$ and $\\text{Conv}_{dw1} (\\cdot)$ are depthwise convolutions, $\\text{Conv}_{pw1} (\\cdot)$ and $\\text{Conv}_{pw1} (\\cdot)$ are pointwise convolutions [5]. $\\text{SN}(\\cdot)$ is the spiking neuron layer."}, {"title": "3.3 I-LIF Spiking Neuron", "content": "Spiking neurons propagate information in both spatial and temporal domains, and they mimic the spiking communication scheme of biological neurons. However, there are inherent quantization errors in converting the membrane potential of spiking neurons into binary spikes, which severely limits the representation of the model. Recently, Fast-SNN [26] achieves high-performance conversion with small timesteps by converting quantized ANNs into SNNs. This inspires us to \"why not train directly with integer values\", which can significantly reduce the quantization error. We just need to be careful to ensure that the inference is spike-driven. So, we came up with the idea of I-LIF.\nLIF. Leaky Integrate-and-Fire (LIF) spiking neuron [36] is the most popular neuron to construct SNNs due to its balance between bio-plausibility and computing complexity. The dynamics of LIF with soft reset is:\n$U [t] = H [t - 1] + X [t],                                                                                                      (8)$\n$S [t] = \\Theta (U [t] - V_{th}),                                                                                                    (9)$\n$H [t] = \\beta (U [t] - S [t]),                                                                                                      (10)$\nwhere $t$ denotes the timestep, $U [t]$ is the membrane potential that integrates the temporal information $H [t - 1]$ and spatial information $X [t]$. $\\Theta(\\cdot)$ is the Heaviside step function which equals 1 for $x > 0$ and 0 otherwise. If $U [t]$ exceeds the firing threshold $V_{th}$, spiking neuron fire a spike $S [t]$ and $U [t]$ will subtract it subsequently. Otherwise, $H [t]$ will remain unchanged. And, $U [t]$ decays to $H [t]$ by a factor of $\\beta$, which denotes the decay constant. For simplicity, we focus on Eq.9 and denote the spiking neuron layer as SN(\u00b7), with its input as membrane potential tensor U and its output as spike tensor S."}, {"title": "I-LIF", "content": "We propose the Integer Leaky Integrate-and-Fire (I-LIF) neuron to reduce the quantization error. As shown in Fig. 2, I-LIF emits integer values while training, and converts them into 0/1 spikes when inference. Specifically, in I-LIF, Eq.9 is rewritten as:\n$S [t] = \\text{Clip} (\\text{round}(U[t]), 0, D),                                                                                        (11)$\nwhere $\\text{round}(\\cdot)$ is a round symbol, $\\text{Clip} (x, min, max)$ denotes that clipping $x$ to [min, max], $D$ is a hyperparameter indicating the maximum emitted integer value by I-LIF.\nTraining Stage. Eq.11 is not a continuous function, making its derivative a step function, potentially causing training instability. Previous studies have introduced several surrogate gradient functions, which primarily address binary spike outputs. We consistently utilize rectangular windows as the surrogate function. For simplicity, We retain gradients solely for neurons activated in the [0, D] range, nullifying all others.\nInference Stage. Introducing integer value necessitates additional MACS (Multiply-Accumulation operations), potentially diminishing the energy efficiency of SNNs. Thus, converting integer values to binary spikes is essential. Fig. 3 shows an example of how integer values convert to binary spikes by extending virtual timesteps during inference. Specifically, the input to the spiking neuron at 1 + 1 layer can be described as $X^{l+1} [t] = W^lS^l [t]$. We extend the $T$ time step to $T \\times D$, and convert the integer value $S^l [t]$ to a spike sequence ${S^{l} [t, d]}_{d=1}^{D}$, which satisfied:\n$\\sum_{d=1}^{D} S^l [t, d] = S^l [t].                                                                                                    (12)$"}, {"title": "4 Experiments", "content": "We evaluate the proposed method on COCO 2017 val [35] and neuromorphic Gen1 [8] datasets. The mean Average Precision(mAP) at IOU=0.5(mAP@50), the average mAP between 0.5 and 0.95(mAP@50:95), and energy cost are reported for each model. Specifically, the power of ANNs and SNNs can be calculated as:\n$E_{ANN} = O^2 \\times C_{in} \\times C_{out} \\times k^2 \\times E_{MAC},                                                                (16)$"}, {"title": "4.1 COCO 2017 val Dataset", "content": "Experimental Setup. As a predominant static dataset for object detection, COCO 2017 val [35] comprises 80 classes split into 118K training and 5K validating images. In all experiments, we set decay factor \u03b2 = 0.25, learning rate to 0.01, and adopt SGD optimizer. The models are trained for 300 epochs with a batch size of 40 on 4 NVIDIA V100 GPUs. Mosaic data augmentation [1] technique is employed. The network structure is given in the supplementary material. Note,"}, {"title": "Ablation Studies of Architectural Design", "content": "We simplify YOLOv8 for SNN and incorporate meta SNN blocks. As shown in Table 1, this architectural improvement enables the accuracy of spikeYOLO at $T$ = 1 and $D$ = 1 to reach 52.7%, better than the prior state-of-the-art SNN. We are also interested in the question \u201cwhether the architectures in SNNs and ANNs can be used directly interchangeably?\". We conduct the experiments in Table 2. We observe that directly converting the ANN architecture into the corresponding SNN brings significant performance degradation. The special architectural design of SNNs can improve its representation."}, {"title": "Ablation Studies of Quantization Error", "content": "Integer-valued training is de- signed to reduce quantization error in SNNs. The larger $D$ is, the smaller the quantization error is. In Table 1, we fixed the parameters to 23.1M. When $T$ = 1 and $T$ = 4, we expand $D$ = 1 to $D$ = 4, respectively, and the accuracies of mAP@50 are increased by +9.6% and +7.6%. In contrast, if we fix $D$ = 1 and increase $T$ = 1 to $T$ = 4, the performance improvement of mAP@50 is only 3%. These results show that quantization error has a greater impact on performance than the setting of timesteps. And, in terms of power, increasing $D$ is more cost-effective than increasing $T$. For instance, when 1 \u00d7 1 changes to 1 \u00d7 4, power increases by 88%; while 1 \u00d7 1 changes to 4 \u00d7 1, power improves by 267%."}, {"title": "4.2 Gen1 Automotive Detection Dataset", "content": "Experimental Setup. As a large neuromorphic object detection dataset, Gen1 [8] encompasses 39 hours of open road and various driving scenarios, captured using an ATIS sensor with a resolution of 304\u00d7240 pixels. The dataset is organized into training, validation, and testing subsets. The bounding box annotations of pedestrians and cars(over 255,000) were manually labeled. For each annotation, we process the event-based stream 2.5 seconds before its occurrence, dividing it into $T$ slices for model input. We train the model for 50 epochs and maintain other hyperparameters same as the COCO 2017 dataset.\nMain Results on Gen1 dataset are shown in Table 3. The proposed SpikeY- OLO notably elevates the performance benchmark for the Gen1 dataset in SNNs. We achieve 67.2% mAP@50 with 23.1M parameters, which outperforms the prior state-of-the-art SNN model by +8.2%. For example, when $T$ = 5, SpikeYOLO vs. EMS-YOLO [50]: Param, 13.2M vs. 14.4M; mAP@50, 66.0% vs. 59.0%; mAP@50:95, 38.5% vs. 31.0%. In contrast to the COCO dataset, Gen1 contains temporal information that is more suitable for SNN processing. We conduct experiments on the performance of SNN and ANN with the same architecture. We observe that SpikeYOLO's mAP@50 accuracy is +2.5% higher than the corresponding ANN, and shows a 5.7\u00d7 energy efficiency. This indicates that SNN has attractive potential in processing neuromorphic data.\nAblation Studies of Quantization Error. Both $T$ and $D$ significantly influence outcomes when processing neuromorphic datasets. Table 4 gives a com- prehensive ablation study on SpikeYOLO with 23.1M parameters that evaluate the effects of varying $T$ and $D$. We observe some interesting experimental results. First, boosting the timestep $T$ will bring improvement in accuracy and power. For instance, with the set of $D$ = 1, extending $T$ = 1 to $T$ = 4 yields a +6.7% increase in mAP@50, and the power will increase by 3.7\u00d7. But further extending $T$ = 4 to $T$ = 8 results in a marginal increase of only +0.7% and significantly"}, {"title": "4.3 Architecture Ablation Experiments", "content": "Re-parameterization Design. As shown in Table 5, if we remove re-parameteri- zation by adding neurons into inverted separable convolutions, the mAP@50 and mAP@50:95 will decrease 1.7% and 1.8% respectively.\nSNN Block Design. Including a 3\u00d73 standard convolution within the initial stages of convolution blocks is crucial. As shown in Table 5, substituting SNN-Block-1 for SNN-Block-2 leads to a reduction in performance of around 1%. Moreover, we try to replace high-stage SNN-Block-2 with meta Transformer-based SNN block, just like Meta-SpikeFormer [58]. We find that there is little to no performance gain by doing this and that the parameters increase. Therefore, only spiking CNN blocks are exploited in our SpikeYOLO.\nDetection Head. The detection mechanisms within YOLO are categorized into anchor-based heads(e.g., YOLOv5) and anchor-free heads(e.g., YOLOv8). The former directly predicts each bounding box's dimensions, whereas the latter estimates the probability distribution of each bounding box. Previous EMS- YOLO [50] and Meta-SpikeFormer [58] employ anchor-based heads. SpikeYOLO exploits the anchor-free head because of its higher accuracy (see Table 5)."}, {"title": "5 Conclusion", "content": "This work significantly narrows the performance gap between SNNs and ANNs on object detection tasks. We achieve this through network architecture and spiking neuron design. The proposed SpikeYOLO architecture abandons the complex module design in the vanilla YOLO series and exploits simple meta spike blocks to build the model. Then, the I-LIF spiking neuron capable of integer-valued training and spike-driven inference is proposed to drop quantization errors. We improve the upper bound of the SNN domain's performance on the COCO dataset by +15.0% (mAP@50) and +18.7% (mAP@50:95), respectively. On the neuromorphic Gen1 dataset, SpikeYOLO achieves better performance and lower power than ANN of the same architecture. Furthermore, we investigate the performance of equivalent architecture ANNs and SNNs in different datasets, and the results show that the redesigned SNN architecture performed better. This work enables SNNs to handle complex object detection and can inspire the application of SNNs in more visual scenarios."}]}