{"title": "On the Compositional Generalization of Multimodal LLMs\nfor Medical Imaging", "authors": ["Zhenyang Cai", "Junying Chen", "Rongsheng Wang", "Weihong Wang", "Yonglin Deng", "Dingjie Song", "Yize Chen", "Zixu Zhang", "Benyou Wang"], "abstract": "Multimodal large language models (MLLMs)\nhold significant potential in the medical field,\nbut their capabilities are often limited by insuf-\nficient data in certain medical domains, high-\nlighting the need for understanding what kinds\nof images can be used by MLLMs for general-\nization. Current research suggests that multi-\ntask training outperforms single-task as differ-\nent tasks can benefit each other, but they often\noverlook the internal relationships within these\ntasks, providing limited guidance on selecting\ndatasets to enhance specific tasks. To analyze\nthis phenomenon, we attempted to employ com-\npositional generalization (CG)\u2014the ability of\nmodels to understand novel combinations by\nrecombining learned elements as a guiding\nframework. Since medical images can be pre-\ncisely defined by Modality, Anatomical area,\nand Task, naturally providing an environment\nfor exploring CG. Therefore, we assembled 106\nmedical datasets to create Med-MAT for com-\nprehensive experiments. The experiments con-\nfirmed that MLLMs can use CG to understand\nunseen medical images and identified CG as\none of the main drivers of the generalization ob-\nserved in multi-task training. Additionally, fur-\nther studies demonstrated that CG effectively\nsupports datasets with limited data and delivers\nconsistent performance across different back-\nbones, highlighting its versatility and broad ap-\nplicability. Med-MAT is publicly available at\ngithub.com/FreedomIntelligence/Med-MAT.", "sections": [{"title": "1 Introduction", "content": "Multimodal large language models (MLLMs) (Liu\net al., 2023; Li et al., 2024; Chen et al., 2024b)\nare showing great promise for the medical com-\nmunity, facilitating efficient consultations for doc-\ntors and providing patients with anytime access to\ntheir medical conditions. However, limited data\non rare or privacy-restricted diseases often restricts\nMLLMs' performance, making it crucial to explore\nwhat kinds of images can be used by MLLMs to\ngeneralize these diseases or learn them efficiently.\nCurrent research on medical image generaliza-\ntion (Mo and Liang, 2024; Ren et al., 2024) has\ndemonstrated that models trained on multiple tasks\noutperform those trained on a single task as they\ncan leverage potential knowledge from other tasks.\nHowever, these studies have not conducted a de-\ntailed analysis of which data within multitask\ndatasets can mutually complement each other. Con-\nsequently, we aim to investigate the phenomenon\nof mutual improvement in MLLMs' understanding\nof medical images from the perspective of compo-\nsition generalization.\nCompositional generalization (CG) (Xu et al.,\n2022; Li et al., 2019) refers to a model's ability to\nlearn fundamental elements and recombine them\nin novel ways to understand unseen combinations.\nIn medical imaging, each image can\nbe categorized by Modality, Anatomical area, and\nmedical Task, presenting numerous natural oppor-\ntunities for CG. We defined these three elements\nas the MAT-Triplet and collected 106 medical\ndatasets, subsequently merging those that share the"}, {"title": "2 Med-MAT", "content": "Most existing datasets for MLLMs (Zhang et al.,\n2023c; Li et al., 2024; Chen et al., 2024b), primar-\nily VQA datasets, provide broad coverage but lack\nattribute annotations for individual samples, which\nare not suitable for CG exploration. To address\nthis gap, we curated a large collection of image-\ntext pairs to develop Med-MAT, ensuring that each\nsample is explicitly defined by MAT-Triplet. Sec-\ntion 2.1 provides data processing methods; and\nSection 2.2 presents a pilot experiment."}, {"title": "2.1 Data Processing", "content": "Data Construction: Med-MAT contains a total\nof 106 image-label pair medical datasets, sourced\nfrom various medical public challenges or high-\nquality annotated datasets. All datasets are cate-\ngorized according to their MAT-Triplet, with data\nhaving identical elements grouped into a single\nsubset. Labels are manually clustered\nto ensure that annotations with the same meaning\nare not repeatedly used. In total, Med-MAT covers\n11 medical modalities, 14 anatomical areas, and\n13 medical tasks, hoping that it can spread across\nvarious medical tasks like a mat. (Data lists are\nshown in Appendix B)\nData Distribution: All subsets are divided into\ntraining and test sets following their original dis-\ntributions or using a 9:1 ratio. To ensure a fair\ncomparison, each training set is limited to 3,000\nsamples, with label balance maintained as much as\npossible. Any subset that cannot meet this require-\nment is treated as an OOD (out-of-distribution)\ndataset. For the test sets, we strictly balance the\nnumber of samples per label to ensure that the ac-\ncuracy metric reliably reflects model performance.\nQA Pairs Construction: \u03a4o enable MLLMs to\ndirectly train and test on Med-MAT, all image-label"}, {"title": "2.2 A Pilot Study of Data Composition", "content": "Experiments Setup: Experiments in this paper\nmainly focused on classification datasets to explore\nthe model's combinatorial generalization capabil-\nities across various image types. The base model\nLLaVA-v1.5-7B-Vicuna (Liu et al., 2023) features\na transparent pretraining process and uses very little\nmedical data during training, thereby minimizing\nthe risk of knowledge leakage. By leveraging the\nflexibility of MLLM, we achieved task switching\nand generalization simply by modifying the corre-\nsponding prompts, significantly simplifying gener-\nalization studies. Each experiment was conducted\nover 5 epochs using 8 A800 (80GB) GPUs, with a\nbatch size of 32 and a learning rate of 5e-6. Gener-\nalization performance was assessed by comparing\nthe model's accuracy on the Target data.\nPilot Study: To explore the benefits of data com-\nposition for downstream tasks, all in-distribution\n(ID) datasets were combined for multi-task train-\ning, with single-task training on individual ID\ndatasets serving as the control. The results (Ta-\nble 1 and 2) showed that multi-task training out-\nperformed single-task training on specific tasks,\nand also surpassed the baseline in predicting un-\nseen OOD datasets. This suggests that certain data\ncombinations can enhance the model's classifica-\ntion performance, highlighting the potential for CG.\nExploring which combinations provide valuable in-\nsights for downstream medical tasks in a promising\navenue for further research."}, {"title": "3 Proof of Concept on CG", "content": "To explore whether MLLMs can leverage CG to un-\nderstand unseen images, controlled variable studies\nwere first conducted using all possible CG combi-\nnations to determine if CG exists among the MAT-\nTriplet (Section 3.1). Subsequently, the number of\ncombinations was increased to explore more gener-\nalizable insights, aiming to assess the performance\ngains of multi-task training (Section 3.2)."}, {"title": "3.1 Controlled Variable Study on CG", "content": "3.1.1 Experiment Setup\nIn this section, the existence of CG was explored\nfrom a finer perspective, focusing on CG with only"}, {"title": "3.1.2 Analysis of CG in MLLMs", "content": "Results are shown in Table 3 and it can be observed\nthat almost all CG combinations are able to general-\nize to downstream tasks, highlighting that MLLMs"}, {"title": "3.1.3 Analysis of Non-obvious Cases", "content": "Some Trained models show minimal gains or per-\nformance declines, especially in Level or Diseases\nIdentification tasks. The images in the Level Identi-\nfication are very similar, while the Diseases Identifi-\ncation involves a large number of target labels, both\nof which require large datasets for training, which\nmay explain the poor performance. To further ana-\nlyze them, we modified the evaluation scenario to\nuse CG combinations for assisting in Target data\ntraining with limited samples (Section 4.2)."}, {"title": "3.2 Scaling the Combination Number of CG", "content": "To ensure the universality of CG, we expanded the\nnumber of combination datasets to evaluate its ef-\nfects on a larger scale. This section explores the\nfollowing questions: (Q1) While Meta CG exper-\niments indicate that Unrelated combinations pro-"}, {"title": "3.2.1 Experiment Setup", "content": "Selection Strategy: To ensure a balanced evalua-\ntion of Related and Unrelated combinations, Sub-\nset 03 and Subset 28 were chosen as Target datasets\nbecause they exhibit the most balanced ratios of\nRelated to Unrelated subsets (13:11 for Subset 03\nand 11:13 for Subset 28), making them ideal for\nproviding a diverse range of compositions in the\nscale-up experiments.\nThe baseline was trained on all subsets ex-\ncluding the Target data to evaluate the claim\nthat mixing multi-task data enhances gener-\nalization (All Data ). To construct multiple\ncomparative experiments, models were further\ntrained on either Related or Unrelated subsets\n( All Related / All Unrelated) to address Q1. For\nQ2, individual MAT-Triplet elements were sys-\ntematically removed from the Related subsets\n( Related w/o Modality / Area / Task ), disrupting\nCG and assessing the ability to maintain generaliza-\ntion. To ensure consistency, the total data volume\nin all experiments was limited to 15,000 samples,\naligning with the number of ID subsets available\nafter excluding related tasks from Subset 03."}, {"title": "3.2.2 Analysis of Scaling Experiment", "content": "Figure 4 illustrates the results. It can be observed\nthat even when we expanded the Unrelated com-\nbination volumes and increased task diversity, the\nperformance of All Unrelated remains close to"}, {"title": "4 In-depth Analysis of CG", "content": "To further explore the application and applicability\nof CG, several research questions were proposed\nin this section. In order to present rich results with\nminimal consumption of computational resources,\nspecific data Selection Strategies were applied for\neach question. Here are the research questions:\n(RQ 1): How does the quantity of Related data\naffect generalization, and should it be maximized\nto ensure CG quality?\n(RQ 2): Can CG combinations help the model\nefficiently fit Target data when only a small amount\nof Target data is available?\n(RQ 3): Previous studies demonstrated that CG\nexists between two MAT-Triplet elements, does CG\nstill exist if the three MAT-Triplet elements come"}, {"title": "4.1 The Relationship Between CG\nPerformance and Data Volume", "content": "Selection Strategy: This section focuses on ex-\namining how the data volume of Related combi-\nnations influences generalization. To highlight the\ngeneralization trends, the combinations with strong\ngeneralization results were selected from the main\nexperiments. For fairness, we chose the combina-\ntions across four types where Trained results ex-\nceed both Baseline and Baseline+ by at least 10. If\nmultiple combinations meet the criteria, a random\nseed of 42 was used to determine the selection.\nIn this experiment, the amount of combination\ndata was gradually increased (from 0 to 750, 1500,\n2250, and 3000), and the trend of the model's ac-\ncuracy on Target data after training was observed.\nThe experimental results are shown in Figure 5,\nwhere the red line represents the accuracy curve\ngained from Related combinations, and the purple\nline represents the gain from Unrelated combina-\ntions. Models trained on Related combinations\ndemonstrate strong performance across all four Tar-\nget data sets. As the data volume of Related com-\nbinations increases, the model's understanding of\nTarget data improves consistently."}, {"title": "4.2 Achieving Data-Efficient Training\nthrough CG", "content": "Selection Strategy: This experiment evaluates how\nCG aids training and dataset fitting with limited\nTarget data. To highlight performance trends, four\ncombinations with poor generalization were selected across the four Di-\nrection Types, ensuring both models in the con-\ntrol group started with similar baseline accuracies.\nWhen multiple poor-performing combinations ex-\nisted within a Direction Type, the more challenging\ndatasets were chosen based on Table 1 to better\nobserve potential improvements.\nIn this experiment, the amount of Target data\nwas progressively increased and a fixed number of combina-\ntion data were incorporated during training. Figure\n6 displays the results, with red lines showing accu-\nracy gains from Related combination and purple\nlines from Unrelated ones. It can be observed that\ntraining with Related combination data reaches\npeak performance more quickly, demonstrating an-\nother form of CG. Therefore, although CG per-\nforms poorly on these data due to task characteris-\ntics, it still helps the model quickly adapt with only\na small amount of Target data."}, {"title": "4.3 CG with All MAT-Triplet Elements from\nDifferent Sources", "content": "In previous controlled experiments (Section 3.1),\none element of the MAT-Triplet was kept constant\nwhile CG was explored in the remaining two ele-\nments. To ensure that all the 3 MAT-Triplet el-\nements of the target data originated from three\ndistinct datasets, additional experiments were con-\nducted to further validate the effectiveness of CG.\nFor these experiments, all possible combinations\nmeeting the criteria in Med-MAT were selected\n(Selection Strategy). The results presented in Ta-\nble 4 demonstrate that most combinations can ef-\nfectively generalize to the Target data."}, {"title": "4.4 Detection-Capable MLLMs can Leverage\nDetection Data for CG", "content": "Previous studies have demonstrated that jointly training clas-"}, {"title": "4.5 CG Exists Across Different MLLM\nBackbones", "content": "LLaVA was selected as the baseline because its\ntraining data and processes are publicly available,\nensuring minimal exposure to medical images and\npreventing bias in the integration of medical image\nknowledge into the MLLM. To verify that CG is\nnot limited to a single framework, we also selected\nsubsets for experiments on Qwen2-VL-7B (Wang\net al., 2024a) and Llama3.2-11B-Vision (Meta AI,\n2024). The results in Figure 7 demonstrate CG\neffects similar to those observed in the LLaVA."}, {"title": "5 Related Work", "content": "Medical MLLMs: Recently, adapting MLLMs to\nmedical tasks has gained prominence due to their\nsuccess in capturing complex visual features. Cur-\nrent MLLMs typically pair a visual encoder with a\ntext-only LLM, aligning image data with language\nunderstanding. Such as Med-Flamingo (Moor et al.,\n2023) and Med-PaLM (Tu et al., 2024), fine-tuned\ngeneral multimodal models and achieved notable\nresults. Med-Flamingo enhanced OpenFlamingo-\n9B (Chen et al., 2024a) with medical data, while\nMed-PaLM adapted PaLM-E (Driess et al., 2023)\nusing 1 million data points. Similarly, LLaVA-\nMed (Li et al., 2024), Med-Gemini (Saab et al.,\n2024), and HuatuoGPT-Vision (Chen et al., 2024b)\nutilized specialized datasets and instruction tuning\nto refine medical VQA tasks."}, {"title": "Generalization on Medical Imaging", "content": "alization in medical imaging has been extensively studied. Early methods uti-\nlized data manipulation techniques, such as data\naugmentation, to enhance model generalization on unseen medi-\ncal data by adapting to varying distributions. Later\napproaches focused on representation learning (Le-\nKhac et al., 2020), preserving essential image infor-\nmation to enable models to handle more complex\nscenarios. Additionally, some studies explore multiple aspects of medical image\nprocessing, examining how classification and seg-\nmentation tasks can mutually benefit each other."}, {"title": "Detection with MLLMs", "content": "MiniGPT-4 (Zhu\net al., 2023) and LLaVA (Liu et al., 2023) enhance\nLLMs by fine-tuning models on synthetic multi-\nmodal instruction data. However, these models are\nlimited to image and text inputs and cannot gen-\nerate detection bounding boxes or segmentation\nmasks. Recent works have addressed these limita-\ntions using various strategies, such as encoding re-\ngions as features to allow models to accept regions\nas input, representing object\nbounding box coordinates with text tokens, and employing unique identifiers for task instruc-\ntions to improve learning efficiency. Additionally,\nsome approaches introduce special tokens to repre-\nsent images and use their hidden states to decode\nposition information"}, {"title": "6 Conclusion", "content": "To explore whether MLLMs can leverage CG to\ngeneralize unseen medical data, we constructed the\nMed-MAT dataset as the research platform for gen-\neralization experiments. The results confirmed the\nexistence of CG and identified it as one of the pri-\nmary factors behind the generalization capability\nof MLLMs in multi-task learning. Subsequent ex-\nperiments demonstrated that increasing the volume\nof CG combinations consistently improved their\neffectiveness. Furthermore, CG enables medical\ntasks to efficiently fit with limited data, reducing\ndependence on the volume of training data. Impor-\ntantly, CG is present across various MLLM back-\nbones and can even facilitate generalization using\ndata from tasks such as detection, underscoring its\nbroad applicability."}, {"title": "Limitations", "content": "The experiment confirms that MLLMs can use CG\nto understand unseen medical images and achieve\ndata-efficient training. However, as shown in the\nresults in 3.2, even after disrupting CG, the model's\ngeneralization performance declines but still main-\ntains some level of effectiveness. Thus, CG repre-\nsents only one form of generalization for MLLMs\nin medical imaging. To support the exploration\nof other generalization mechanisms, we will make\ndatasets publicly available according to the license.\nThis study is focused exclusively on medical\nscenarios, but we believe that similar generalization\neffects may also apply in other multimodal tasks.\nAdditionally, more granular medical generalization\nstrategies, such as leveraging existing pneumonia\ndata to generalize the detection of newly emerging\ntypes of pneumonia, are also promising areas for\nfurther investigation."}, {"title": "Potential Risks", "content": "Our research focuses on the compositional general-\nization of MLLMs on medical images, using data\nsourced from medical challenges and open-source\ndatasets. However, further experiments are needed\nto mitigate potential risks when deploying this con-\ncept in real-world medical settings."}, {"title": "A More Experiments", "content": "A.1 CG on Detection Tasks (RQ 4)\nPrevious studies have shown that training on de-\ntection datasets can improve model performance\nin classification tasks. Building on this insight, we\nfurther examine its applicability to CG. For this\ninvestigation, all possible detection subset combi-\nnations in Med-MAT were selected to explore their\nimpact on CG (Selection Strategy).\nExperimental Setup: We conducted general-\nization experiments for detection and classifica-\ntion. Specifically, we performed generalization\nvalidation on Next-Chat and\nMiniGPT-v2 Next-Chat mod-\nels the bounding box as an embedding and utilizes a\ndecoder for decoding, while MiniGPT-v2 treats the\nbounding box as a text token, which are common\napproaches used by existing MLLM implementa-\ntions for detection. By conducting CG validation\nusing distinct bounding box modeling methods, we\nfurther demonstrate the broad applicability of the\nCG approach. Each experiment was conducted on\n8 A800 (80GB) GPUs.\nThe two backbone was trained separately in this\nexperiment. For Next-Chat, we directly trained the\nmodel in its second training stage and fine-tuned\nit for 2 epochs on our composition datasets with\na learning rate of 2e-5, keeping all other training\nparameters at their default settings. Similarly, for\nMiniGPT-v2, we trained the backbone model from\nthe second stage, starting with a learning rate of 2e-\n5 and gradually reducing it to 2e-6 over 3 epochs.\nThe experiments in Table 5 and Table 6 present\nthe results for Next-Chat and MiniGPT-v2, respec-\ntively. The separation regions closely align with\nthe 4 Direction Types defined in the classification\ntask, except for Fix Task, which is excluded due to\nthe substantial differences between detection and\nclassification tasks, making it impossible to find the\nfix one. The results demonstrate that joint training"}, {"title": "A.2 Other MLLMs (RQ 5)", "content": "The choice to use models like LLaVA, MiniGPT-v2\nand NEXT-GPT is because their training data and\noverall process are publicly available, ensuring that\nthey have not been too much exposed to medical\nimages, thus preventing bias in the medical image\nknowledge injection into the MLLM. To ensure\nthe experiment results are not influenced by the\nmodel choice, we also tested several other models\non some subsets of Med-MAT and observed similar\nresults.\nSelection Strategy: For testing, some general-\nized combinations were selected from classification\ntasks 3. Using a random seed of 42, we shuffled\neach Direction Type's combinations and selected\nthe first two compositions as test data.\nExperimental Setup: We conducted experi-\nments to evaluate the compatibility of CG across\ndifferent backbone architectures. We selected two\nMLLMs with representative architectures, namely\nQwen2-VL-7B-Instruct and\nLlama-3.2-11B-Vision-Instruct , to assess the performance of CG on these mod-\nels. Each experiment involved full-parameter fine-\ntuning of all models over 5 epochs, utilizing 8 A800\n(80GB) GPUs. The training was performed with a\nbatch size of 32 and a learning rate set to 2e-6, en-\nsuring that all parameters were updated to optimize\nthe model performance.\nThe experimental results presented in Table 7\nand Table 8 demonstrate that the CG persists\nacross these different backbone architectures.\nThis observation indicates that CG is not confined\nto a specific model type, thereby highlighting its\nuniversal applicability and robustness across vari-\nous model frameworks. Such findings underscore\nthe versatility of CG, suggesting it can be effec-\ntively integrated into a wide range of models."}, {"title": "B The Dataset: Med-MAT", "content": "This section provides an overview of Med-MAT.\nFirst, a detailed explanation of MAT-Triplet will\nbe presented in B.1. Next, the methods for con-\nstructing the QA formatting will be discussed in\nB.2. Finally, the data composition details and open-"}, {"title": "B.1 Details of MAT-Triplet", "content": "MAT-Triplet stands for Medical Modality,\nAnatomical Area, and Medical Task. We define\nall samples in Med-MAT using these three\ncomponents and integrate datasets with identical\ntriplets into subsets.\nMedical Modality refers to different types of\ntechniques or methods used in medical imaging\nor data acquisition. Each modality is designed to\npresent the human body's structures or pathological\nfeatures in unique ways, providing auxiliary sup-\nport for clinical diagnosis and treatment. Most\nmodalities exhibit significant visual differences,\nmaking them easily distinguishable. Med-MAT en-\ncompasses 11 modalities, including common ones\nsuch as Computed Tomography (CT), Magnetic"}, {"title": "B.2 QA construction method", "content": "A large amount of image-label datasets was col-\nlected to build the Med-MAT dataset. To en-\nsure compatibility with MLLM training inputs and\noutputs, all data is transformed into a question-\nanswering format. Questions are formulated based\non modality, anatomical area, and medical task,\nwith 6 question prompts applied to each subset.\nThe labels within each data subset will be clus-\ntered to prevent redundant definitions of the same\ncondition. Then, all training set and test set will be\nconverted into multiple-choice questions following\nthe template in Table 8. Each question will have up\nto four options, with distractor options randomly\nselected from the corresponding subset."}, {"title": "B.3 Data composition and Open-source\nSpecification", "content": "Med-MAT is composed of multiple datasets. After\nbeing transformed into different QA formats, the\nnew data is organized into several subsets to sup-\nport generalization experiments in medical imag-\ning. Table 9 shows all of our subset datasets, which\nare separated based on different combinations in\nMAT-Triplet. The specific MAT-Triplets are listed,\nalong with the labels corresponding to the image-\nlabel datasets for each subset. Correspondingly, all\nthe image-label datasets are also displayed in Table\n10, which includes their names, descriptions of the\ntasks performed, download links, and the level of\naccessibility.\nAll question-answering text datasets in Med-\nMAT will be publicly available. To accommo-\ndate varying access permissions, we will release\ndatasets based on their respective licenses: openly\naccessible datasets will be directly available, while\nrestricted datasets can be accessed by applying\nthrough the links provided in this paper. We hope\nthis dataset will support and advance future gener-\nalization experiments on medical imaging."}]}