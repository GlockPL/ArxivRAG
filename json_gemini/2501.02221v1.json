{"title": "CORD: Generalizable Cooperation via Role Diversity", "authors": ["Kanefumi Matsuyama", "Kefan Su", "Jiangxing Wang", "Deheng Ye", "Zongqing Lu"], "abstract": "Cooperative multi-agent reinforcement learning (MARL) aims to develop agents that can collaborate effectively. However, most cooperative MARL methods overfit training agents, making learned policies not generalize well to unseen collaborators, which is a critical issue for real-world deployment. Some methods attempt to address the generalization problem but require prior knowledge or predefined policies of new teammates, limiting real-world applications. To this end, we propose a hierarchical MARL approach to enable generalizable cooperation via role diversity, namely CORD. CORD's high-level controller assigns roles to low-level agents by maximizing the role entropy with constraints. We show this constrained objective can be decomposed into causal influence in role that enables reasonable role assignment, and role heterogeneity that yields coherent, non-redundant role clusters. Evaluated on a variety of cooperative multi-agent tasks, CORD achieves better performance than baselines, especially in generalization tests. Ablation studies further demonstrate the efficacy of the constrained objective in generalizable cooperation.", "sections": [{"title": "1 Introduction", "content": "Cooperative multi-agent reinforcement learning (MARL), where agents cooperate to maximize the shared reward, has a broad range of applications, from autonomous warehouse (Zhou et al. 2021), power dispatch (Wang et al. 2021a) to logistics (Li et al. 2019b), inventory management (Ding et al. 2022). Recent years have witnessed substantial progress in different kinds of cooperative MARL algorithms, including value decomposition (Sunehag et al. 2018; Rashid et al. 2020; Son et al. 2019; Wang et al. 2020a; Iqbal et al. 2021), multi-agent actor-critic (Yu et al. 2022; Zhang et al. 2021; Kuba et al. 2022; Wang et al. 2023), and fully decentralized learning (Su et al. 2022; Jiang & Lu 2022; Su & Lu 2022).\nHowever, existing algorithms often result in policies that overfit the co-trained teammates and scenarios encountered during training, thus lacking the generalization ability to cooperate effectively with new teammates in new scenarios. As agents may fail to adapt to unforeseen partners, the performance of the multi-agent system can degrade dramatically or even collapse entirely. To address the generalization challenge, a few methods (Barrett & Stone 2015; Gu et al. 2021; Liu et al. 2021) have been proposed. However, most approaches require prior knowledge or predefined policies about new teammates, limiting their applicability in complex real-world environments. In addition, cooperative MARL tasks usually require role divisions. Thus, agents may need to play different roles to collaborate with unseen teammates. However, existing work lacks the essential ability to learn the role assignment. Either not considering role division, or only learning role heterogeneity while neglecting complex inter-agent interactions, making it difficult to generalize in environments that require various roles.\nIn this paper, we propose a hierarchical MARL approach to enable generalizable COoperation via Role Diversity, namely CORD. CORD does not depend on pre-defined agent policies or behaviors and can be trained end-to-end. In CORD, a high-level controller is responsible for analyzing the environment and assigning roles to low-level agents. The low-level agents then condition their policies on the assigned roles. To enable generalizable role assignment when collaborating with unseen teammates, we maximize the entropy of the role distribution with a certain constraint. This constraint is formulated as a causal relationship between the role of one agent and information of other agents, represented in a causal graph. Theoretically, we show this constrained objective can be decomposed into two terms: 1) maximizing the mutual information between the role of one agent and the information about other agents to capture the causal effect in role assignment, enabling reasonable role assignment over the corresponding causal graph, and 2) maximizing the heterogeneity of roles in the determinant form to yield more coherent role clusters without redundancy. These two terms can further be converted into intrinsic rewards. Interpreted as the intrinsic reward, CORD can be implemented by extending QMIX (Rashid et al. 2020) or REFIL (Iqbal et al. 2021) and trained end-to-end by optimizing the shaped rewards to enable generalizable cooperation across different teams with unseen agents.\nEmpirically, we evaluate CORD in a variety of environments including resource collection (Liu et al. 2021) in multi-agent particle environments (MPE) (Lowe et al. 2017) and multi-task StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al. 2019; Iqbal et al. 2021). Results show that CORD outperforms baselines and achieves better perfor-"}, {"title": "2 Related Work", "content": "Hierarchical RL (Al-Emran 2015) solves a complex task by hierarchically decomposing it into simpler sub-tasks. In single-agent settings, the high-level controller selects options (Bacon et al. 2017; Sutton et al. 1999; Precup 2000), reusable skills (Daniel et al. 2012; Gregor et al. 2017; Sharma et al. 2020; Shankar & Gupta 2020) or subgoals (Levy et al. 2019; Sukhbaatar et al. 2018; Nachum et al. 2019; Dwiel et al. 2019; Nasiriany et al. 2019) for the low-level policy to solve long-horizon tasks. Recent MARL studies have employed hierarchical frameworks to address team composition problems. For example, COPA (Liu et al. 2021) proposes a coach-player framework, where the controller learns a strategy distribution for the low-level agents based on global information, without considering team dynamics. ALMA (Iqbal et al. 2022) utilizes human domain knowledge to pre-define many subtasks and corresponding rewards for learning a high-level subtask allocation policy that assigns subtasks to agents. HSL (Liu et al. 2022) employs an auto-encoder model to develop representations under a fixed number of diverse skills, enabling agents to select different skills as needed. Unlike existing work, CORD exploits an informative posterior role distribution to learn the role assignment for the worker agents.\nRoles are associated with the division of labor and the key to multi-agent systems (King & Peterson 2019; Campbell & Wu 2011). Based on this intuition, many methods have been proposed to leverage predefined role assignments to solve specified tasks (Lhaksmana et al. 2018; Sun et al. 2020). However, predefined roles need prior knowledge which hurts generalization. To solve this problem, Wilson et al. (2010) learns roles by Bayesian inference. ROMA (Wang et al. 2020b) distinguishes role distributions based on observation trajectories by mutual information. ROMA does not character more complex inter-agent interactions in environments. RODE (Wang et al. 2021b) maintains the original framework of HSL while replacing skills with roles. These methods only utilize the learned role distributions to promote cooperation in a fixed team. Unlike these methods, CORD optimizes the role assignment based on the maximum entropy principle to promote generalization across different teams with unseen agents.\nZero-shot coordination has been a widely studied problem in multi-agent systems. It refers to the ability of effectively co-operating with unseen agents. However, previous studies (Hu et al. 2020; Yu et al. 2023; Lupu et al. 2021) often assume a fixed number of agents and cannot handle scenarios with variable numbers of agents. To achieve robust behaviors among varying numbers of unknown teammates in multi-agent cooperation, generalization problems (Stone et al. 2010; Zhang et al. 2020; Mahajan et al. 2022) have received much attention. Existing type-inference approaches assume a finite set of predefined teammate types and choose policies adaptively to solve generalization. For example, PLASTIC (Barrett & Stone 2015) computes the Bayesian posterior of predefined types. AATEAM (Chen et al. 2020) proposes an attention-based architecture to infer the type. As these methods assume predefined teammate types, they cannot generally generalize to unknown types. Some recent works avoid predefining the type of teammates via complex training processes, such as population-based training (Long et al. 2020), pre-training (Xing et al. 2021; Gu et al. 2021), and adversarial training (Li et al. 2019a). Other studies leverage communication. SOG (Shao et al. 2022) designs different mechanisms for agent self-organization into teams based on the prior that agents with similar observations should communicate. SOG links agent observation trajectories with communication by optimizing a variational lower bound on mutual information. However, these methods require manual design, pre-training, or communication, which brings additional complexity to real-world scenarios. Unlike these methods, CORD addresses the generalization problem without reliance on predefined agent policies/behaviors or communication, and can be trained in an end-to-end manner."}, {"title": "3 Background", "content": "A decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek & Amato 2016) can be defined as a tuple: $(S, A, P, r, \\Omega, U, I, O, N, \\gamma)$, where N is the number of agents, $U = {1,2..., N}$ is the set of agents. S is the set of the states, and A is the set of joint actions, $a = {a^i|i \\in U} \\in A$. At each state $s \\in S$, each agent i receives a partial observation $o^i \\in \\Phi$ according to the observation function $O(s, i) : S \\times U \\rightarrow \\Phi$. Agents choose their actions forming a joint action $a \\in A$. The joint action causes a transition to the next state s' according to the state transition function $P(s'|s, a)) : S\\times A\\times S \\rightarrow [0, 1]$, and the global reward of the team is determined by $r(s, a)) : S \\times A \\rightarrow R$. To settle partial observability, the trajectory of each agent $\\Tau^i \\in\\Tau: (\\Phi\\times A)^*$ is used to replace observation $o^i$. Each agent learns a local policy $\\pi^i (\\alpha^i |\\Tau^i)$, and all together form a joint policy $\\pi(a|\\tau)$, where $\\tau$ is the joint trajectory of all agents. The objective is to learn a joint policy to maximize the expected cumulative discounted return, $E[\\Sigma_{t=0}^{\\infty} \\gamma^t r_t]$, where $\\gamma\\in [0,1)$ is the discount factor. For a joint policy $\\pi(a|\\tau)$, we can define the joint state-action value function $Q_{tot} (\\Tau_t, a_t) = E_{\\Tau_{t+1:}^{\\infty}, a_{t+1:}^{\\infty}} [\\Sigma_{k=0}^{\\infty}\\gamma^k r_{t+k}|\\Tau_t, a_t]$. Further we denote all other agents except agent i as -i.\nValue decomposition methods (Sunehag et al. 2018; Rashid et al. 2020; Son et al. 2019; Wang et al. 2020a) factorize the joint state-action value function $Q_{tot}$ into individual state-action functions to solve Dec-POMDPs. QMIX (Rashid et al. 2020) is one of the commonly used methods, and it factorizes $Q_{tot}$ into {$Q^i(\\tau^i, a^i)|i \\in U$} via a mixing network which satisfies the IGM condition by $\\frac{\\partial Q_{tot}}{\\partial Q^i} \\geq 0, \\forall i \\in U$. The mixing network can be removed during execution such that agents can make decisions by their own $Q^i$ in a fully decentralized manner. To handle a variable number of agents, Attention QMIX (AQMIX) (Iqbal et al. 2021) improves QMIX by the multi-head attention mech-"}, {"title": "4 CORD", "content": "Inspired by information theory (Jaynes 1957), we believe that increasing the entropy of the role distribution is an effective approach to resolve the problem of generalizable cooperation with unseen agents. The objective function can be considered as,\n$\\max H(P(c))$,\nwhere P(c) is a prior role distribution. Without any task information, the best prior distribution would be a max entropy distribution. When the task information is given, we can further adapt this prior based on the task information and get a more informative posterior. Therefore, we conjecture that merely maximizing the entropy of the role distribution is insufficient for learning high-quality role division. Our empirical results (see Section 5) also support that.\nTo derive an informative posterior role distribution by maximizing the entropy, some evidence or information should be provided to the posterior. We believe that a good role assignment for cooperation should consider the information or influence from other agents. In a role-based approach, effective role assignment is crucial for fostering cooperation. As highlighted by Chalkiadakis et al. (2022); Elkind & Rothe (2016); Branzei et al. (2008), considering the impact of individual agents, such as their Shapley value, is key to enhancing team cooperation. An illustrative example: A robot team of excavators, material transporters, and assemblers collaborate to build, where delayed excavation due to hard rocks prompts some transporters to switch roles to clear debris, accelerating the process. Thus, effective role assignment is responsive to the excavators' performance.\nThus, we assume the presence of a causal relationship between the role of one agent and the information of other agents. Moreover, we construct a causal graph to reflect such relationships. With these conditions and the corresponding causal graph, we can find that the entropy of role distribution (1) can be split into two terms: the first term is causal inference in role and the second term is role heterogeneity. We discuss this result in detail later in this section.\nBefore our discussion of the decomposition of the entropy of role distribution, we must first establish a reasonable causal graph. We assume that a causal relationship exists between the role of one agent and the influence of other agents. As such, to ascertain the precise impact of these relationships on the role distribution, we must quantify them in terms of mutual information based on the corresponding causal graph. For this purpose, we put forth the assumptions below to facilitate the construction of the causal graph.\nAt timestep t, the actions $a_{-i}^{t-1}$ and other agents' roles $c_{-i}^{t-1}$ cannot influence agent i's role $c_i^t$. Thus, we consider $a_{-i}^{t-1}$ and $c_{-i}^{t-1}$, which we posit having a causal relationship with $c_i^t$. Given that we assume environments satisfy Markov Properties, we need only consider the observation at timestep t, denoted $o^t$, which encapsulates all prior information.\nUnder Assumption 4.1, we propose a causal graph as illustrated in Figure 1. In this causal graph, for any agent i, the observation of agent i at timestep t: $o_i^t$ and team state $h_{team}^t = {o_k^t \\in U}$ can represent the sufficient information of agent i. The observation of other agents at timestep t: $o_{-i}^{t}$, the action and role distribution of other agents at timestept - 1: $a_{-i}^{t-1}$ and $c_{-i}^{t-1}$ can represent the causal influence on agent i.\nNow, given this causal graph, we can define some critical quantities for our analysis. First, we can get the following definition about the other agents' influence.\nSuppose that the causal relationships between agents can be defined as the causal graph, for each agent i, the definition of other agents' influence vector $\\bar{I}_i$ is below:\n$\\begin{aligned}\nq^t_i &= f(o_i^t,h_{team}^t), \\\\\nk^j_i &= g(a^{t-1}_j, o^{t-1}_j, c^{t-1}_j), \\\\\nv^j_i &= h(a^{t-1}_j, o^{t-1}_j, c^{t-1}_j), \\\\\n\\alpha_i^j &= \\text{softmax}(\\frac{k^j_i}{\\sqrt{|k_i|}}), \\forall j \\neq i \\\\\n\\bar{I}_i &= \\Sigma_{j \\neq i} \\alpha^j v^j,\n\\end{aligned}$\nwhere $q_i^t$ is encoded by f function, $k^j_i$ and $v^j_i$ are the key and value vector encoded by g and h function respectively, f, g, and h are trainable network layers with different parameters.\nHere we follow the attention mechanism (Bahdanau et al. 2014) to process the information about the influence of other agents -i on agent i. Thus, Definition 4.2 represents the specific computation process of defining the influence vector of an agent by the attention mechanism.\nNext, we can define c-related matrix A(c) which represents the similarities between different roles.\n$\\begin{aligned}\nA(c)_{ij} &= e^{-d_{ij}}, A(c)_{ij} \\in (0, 1], \\forall i, j \\in \\{1, 2, ..., N\\}, \\\\\nd_{ij} &= D_{KL}(P(c_i|\\bar{I}_t, q_i^t)||P(c_j|\\bar{I}_t, q_i^t)) \\\\\n&+ D_{KL}(P(c_j|\\bar{I}_t, q_i^t)||P(c_i|\\bar{I}_t, q_i^t)),\n\\end{aligned}$\nwhere $\\bar{I}_t$, $q_i$, $\\bar{I}_t$, and $q_i^t$ satisfy Definition 4.2, and N is the number of agents.\nWith all the preparation above, we can discuss the decomposition of the role entropy. Actually, we have the following theorem.\nSuppose that both the prior role distribution P(c|q) and the posterior role distribution $P(c|\\bar{I}, q)$ obey Gaussian distribution and the c-related matrix A(c) satisfies Definition 4.3, then the entropy of the role distribution can be decomposed as:\n$H(P(c|q)) = I(c; \\bar{I}|q) + H(P(c|\\bar{I}, q))$,\n$I(c; \\bar{I}|q) = E_{\\bar{I}} [\\Sigma_{i=1}^N D_{KL} [P(c_i|\\bar{I}_i, q_i)||P(c_i|do(\\bar{I}_i), q_i)]]$,\n$H(P(c|\\bar{I}, q)) = \\beta \\log|A(c)| + C$,\nwhere I is mutual information, $I_i = \\bar{I}$, $q_i = q$, $do(\\bar{I}_i) = do(\\bar{I}_i^0)$, |A(c)| denotes the determinant of A(c), and $\\beta$ and C are constants.\nProof. See Appendix A.\n$do(\\bar{I}_i)$ is a mathematical operator which represents the average influence from other agents, so $D_{KL} [P(c_i|\\bar{I}_i, q_i)||P(c_i|do(\\bar{I}_i), q_i)]$ can quantify counterfactual causal effects from other agents. The difference between \u201cdo-calculus\u201d and $do(\\bar{I}_i)$ is that \u201cdo-calculus\u201d is the operation of intervention, while $do(\\bar{I}_i)$ refers to the expectation of all possible interventions. Practically, do(I) can be substituted with a constant vector. More discussion about $do(\\bar{I}_i)$ can be found in Pearl (2009). However, instead of using the method in Pearl (2009) to calculate the causal effect, we borrow the idea of social influence (Jaques et al. 2019) and leverage the deep learning model to estimate the causal effect.\nFrom Theorem 4.4, we can find that the entropy of role distribution is split into two terms: causal inference in role $I(c; \\bar{I}|q)$ and role heterogeneity $H(P(c|\\bar{I}, q))$. The causal inference of roles $I(c; \\bar{I}|q)$ enables prudent role assignments through causal reasoning based on the previously defined causal graph. The role heterogeneity $H(P(c|\\bar{I}, q))$ incentivizes the controller to derive dissimilar role partitions without undesirable redundancy. Thus, the model can maintain its performance while enhancing generalization by concurrently optimizing both components.\nWe need to argue that the objective (4) is different from (1). The decomposition in Theorem 4.4 is under Definition 4.2 and the causal graph. These assumptions or conditions can be seen as constraints on optimizing the entropy of the role distribution in (1). So optimizing the objective (4) is actually optimizing (1) with some constraints from the causal graph. Though $I(c; \\bar{I}|q)$ and $H(P(c|\\bar{I}, q))$ are components of the decomposition of $H(P(c))$ from Theorem 4.4, these two terms still have richer meanings from different views. Actually, these different views are the reason that we name them as causal inference in role and role heterogeneity. So, we discuss these two terms in further depth next.\nTo enable prudent role assignments, the precise causal impact based on the previously defined causal graph must be ascertained. Actually, the RHS of (5) represents the expected causal effects from other agents (Pearl 2009). The proof demonstrating how causal inference relates to mutual information is provided in Appendix A. So for the purpose of prudent role assignments, assuming the role distribution satisfies Definition 4.2, the objective function can be defined as,\n$\\max_{c\\sim P(\\bar{I},q)} E_{\\bar{I}} [\\Sigma_{i=1}^N D_{KL} [P(c_i|\\bar{I}_i, q_i)||P(c_i|do(\\bar{I}_i), q_i)]]$.\nMoreover, from Theorem 4.4, maximizing (7) is also equivalent to maximizing the mutual information between the role and the influence vector of other agents. Note that mutual information can be used in causal inference, including counterfactual inference and intervention operations. Tigas et al. (2022) explains that mutual information discovers causal relationships by observation data and evaluates the impact of a variable change on another variable.\nAlthough (7) optimizes the expected value of causal effects, practically we can sample M trajectories and approximate the mutual information between $c_i$ and $\\bar{I}$ by the average causal effects. Moreover, we substitute $I_0$, a constant vector, for $do(I_i)$ to represent the intervention in causal inference theory (Pearl 2009). Therefore, the causal inference in role can be rewritten as follows, and we maximize it by taking it as an intrinsic reward,\n$r_c = \\frac{1}{M}\\Sigma_{m=1}^M\\Sigma_{i=1}^N D_{KL} [P(c_i|\\bar{I}_i, q_i)||P(c_i| I_0, q_i)]$,\nwhere $I_0$ is a constant vector, M is the number of sampled trajectories.\nGiven Definition 4.3 and the assumption of Gaussian distribution, we can find the determinant |A(c)| in the formulation of $H(P(c|\\bar{I}, q))$ in (6). On the other hand, the matrix A(c) depicts the distance between any pair of roles. This means that the determinant |A(c)| represents the enclosed volume of c in the corresponding metric space, and maximizing |A(c)| improves the diversity of roles (Parker-Holder et al. 2020). So the following objective function is equivalent to maximizing"}, {"title": "CORD", "content": "the entropy of posterior role distributions given other agents' influence vectors, as well as improving the diversity of roles,\n$\\max_{c\\sim P(\\bar{I},q)} \\beta \\log|A(c)| + C$.\nHowever, it should be noted that $\\log|A(c)| \\in (-\\infty,0]$ is unbounded at one end, while $|A(c)| \\in [0,1]$ is bounded. Since the log function is concave, we optimize $\\log|A(c)|$ in the same manner as we optimize |A(c)|. Thus, the role heterogeneity is expressed as follows and maximized as an intrinsic reward,\nr_d = |A(c)|.\nWe are now ready to introduce our learning framework. First, by aggregating the two intrinsic rewards $r_c$ and $r_d$ with the environmental reward $r_e$, we can have a new reward function r as follows:\nr = r_e + \\lambda_c r_c + \\lambda_d r_d,\nwhere $\\lambda_c$ and $\\lambda_d$ are hyperparameters. Our objective is to optimize this shaped reward. The learning framework of CORD is illustrated in Figure 2. The high-level controller takes as input the observations from all agents and then assigns roles to the low-level agents accordingly. The agent utility network computes individual Q-function given local observation and assigned role. The mixing network takes as input the Q-values from all agents and outputs $Q_{tot}$. All the modules, parameterized by $\\theta$, are updated end-to-end via backpropagation to minimize the TD loss,\n$L(\\theta) = E_{(\\Tau,a),r,r')\\sim D} [(y^{tot} - Q^{tot}(\\Tau,a); \\theta))^2]$,\n$y^{tot} = r + \\gamma Q^{tot} (\\Tau', \\arg\\max Q^{tot} (\\Tau',\\cdot ; \\theta); \\theta)$.\nwhere D is the replay buffer and $\\bar{\\theta}$ is the parameter of the target network. The pseudocode of the learning algorithm is available in Appendix B."}, {"title": "5 Experiments", "content": "In this section, we evaluate our proposed CORD in a variety of cooperative multi-agent tasks including resource collection (Liu et al. 2021) in MPE(Lowe et al. 2017) and SMAC(Samvelyan et al. 2019; Iqbal et al. 2021) to empirically investigate whether CORD can enable better generalizable cooperation. To ensure reproducibility, we include our code in the supplementary material and will make it open-source upon acceptance.\nIn resource collection, agents collect dispersed resources, facing invaders & defending a home. Multi-task SMAC utilizes variable types of agents with entity-defined states & mask-based observations. Our experiments include two scenarios: sz and MMM. Additional two (m and csz) are available in Appendix D.\nWe compare our method with hierarchical RL methods, COPA (Liu et al. 2021) and ALMA (Iqbal et al. 2022), CTDE with full observation methods, Attention QMIX (AQMIX) and REFIL (Iqbal et al. 2021), communication-based method, SOG (Shao et al. 2022), and role-based method, ROMA (Wang et al. 2020b). Moreover, we introduce two ablation baselines. We fix the role distribution P(c) to be uniform at random, and we denote this baseline as MaxEnt. For the second one, we remove the intrinsic rewards from CORD, denoted as CORD w/o I.\nAll methods are trained in multi-task settings with varying agent numbers: 2-4 in resource collection and 3-7 in SMAC during training. To evaluate generalizable cooperation, we set up two types of generalization tests.\nThe learned policy is applied to tasks with the number of agents different from training.\nThe learned policy is applied to control part of the team to cooperate with the remaining built-in, unseen agents in each environment.\nAll results are presented using the mean and standard deviation of five runs with different random seeds unless stated otherwise. More details about experimental settings and hyperparameters are available in Appendix C.\nFigure 3a shows CORD significantly outperforms all baselines on training tasks, indicating well-coordinated policies from effective role assignment and a reasonable controller policy. Figures 3b and 3c demonstrate CORD's superior performance in generalization tests with unseen 5-agent and 6-agent teams, suggesting its role assignment's high generalizability.\nTo evaluate generalization to unseen collaborators, we test 5-agent and 6-agent tasks with the setting of 1 to 4 controllable agents and 1 to 5 controllable agents, respectively, and report their average episode rewards for 5-agent and 6-agent tasks respectively in Table 1. While all methods show decreased performance, CORD consistently outperforms baselines, demonstrating its superior generalization to unseen agents.\nTraining results for 3-7sz and 3-7MMM scenarios are shown in Figures 4a and 4b, with CORD, COPA, and REFIL outperforming others. In the generalization test for unseen teams, 2sz, 8sz, 2MMM, and 8MMM, as shown in Table 2, CORD consistently surpasses baselines across all tasks."}, {"title": "6 Conclusion and Limitation", "content": "In this paper, we propose CORD, a hierarchical MARL approach leveraging role diversity for generalizable cooperation. A high-level controller assigns roles to low-level agents, whose policies depend on these roles. We formulate the problem of generalizable role assignment as the constrained optimization of entropy and mathematically decompose the objective into two terms: causal inference in role and role heterogeneity. The two terms are further converted to intrinsic rewards and optimized end-to-end. Empirically, we evaluate CORD in a variety of cooperative multi-agent tasks. Results show CORD substantially outperforms baselines in generalization tests for unseen teams and unseen agents. Ablation studies verify the efficacy of our constrained optimization objective.\nA limitation of CORD is it periodically requires global information and assigns roles to agents during execution, which may limit the flexibility and adaptability of decentralized multi-agent systems. Yet, this represents a tradeoff between generalizability and decentralization."}]}