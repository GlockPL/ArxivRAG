{"title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations", "authors": ["Samyak Rawlekar", "Shubhang Bhatnagar", "Narendra Ahuja"], "abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to asso- ciate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be sub- optimal, as the datasets used to train VLMs lack image- caption pairs explicitly focusing on class absence. To an- alyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned di- rectly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative em- beddings (PositiveCoOp), outperforms dual prompt learn- ing approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision- features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing la- bels is low, while requiring half the training compute and 16 times fewer parameters.", "sections": [{"title": "1. Introduction", "content": "Multi-label recognition (MLR) is an important task in computer vision, being relevant to several real-world appli- cations. Unlike single-label recognition, where an image is associated with just one label, MLR requires identify- ing multiple objects or concepts present in an image, mak- ing it significantly more challenging. Examples of MLR applications include medical diagnosis from chest X-rays [20], product detection in e-commerce images [4], and food recognition for dietary monitoring systems [2, 29].\nThe complexity of MLR arises from the combinatorial increase in the number of possible label subsets, which es-"}, {"title": "2. Related Works", "content": "Multi-Label Recognition with Partial Annotations: Our work aims to recognize multiple objects within an image, similar to many previous efforts in the field [6,9,26,39,45]. Due to the difficulty in annotations, we are particularly interested in MLR with partial annotations, where some classes may not be labeled in each image. Early approaches tackled this problem by ignoring the missing labels and training disjoint binary classifiers for each object on the known label set [27, 30, 35]. However, the performance of these methods during inference was significantly hindered by the partial or missing annotations. To overcome this, subsequent work proposed the idea of replacing the missing annotations with pseudo labels [14, 22, 28]. They use pretrained models or train a new model with modified loss to classify the missing labels into the known set. Most recent line of works explictly transfer information of labels from one image to another by using either label dependencies [8, 38] or by blending features of known labels from one image to the unknown labels in another [31]. While these approaches have made notable advances, they typically require large MLR datasets or tailored loss functions, and they still struggle with very low percentages (10%-30%) of available labels. On the other hand, VLM-based approaches demonstrate that VLM priors help them achieve higher performance even with only 10%-30% of the available labels.\nVision-Language Models for MLR with Partial Annota- tions: Over the past two years, MLR has increasingly fo- cused on adopting VLMs such as CLIP [32]. VLMs learn representations by aligning hundreds of millions of image- text pairs, enabling them to adapt to various downstream tasks such as classification [17, 42, 44, 46], retrieval [3, 23], and segmentation [43]. SCPNet [13] leverages class name similarities derived from CLIP's embedding space and aug- ments the training with a self-supervised contrastive loss. Furthermore, inspired by large language models (LLMs),"}, {"title": "3. Approach", "content": "Our primary objective is to validate the hypothesis pre- sented in Sec 1, which argues that CLIP's guidance to learn a negative prompt is not optimal and, in fact it reduces per- formance.\nTo investigate this, we breakdown and analyze the effect of various components of the prompting strategies in SOTA VLM-based MLR methods which operate in partial annota- tion settings. Specifically, we focus on prompting strategies of DualCoOp, that leverages CLIP to learn a positive and a negative prompt. The positive prompt is associated with the presence of the class and the negative prompt is associ- ated with the absence of the class. To quantify the impact of these individual learned prompts, we first establish a base- line (B) (sec 3.1), that use only the visual information, com-"}, {"title": "3.1. Baseline", "content": "We propose a Baseline (B), to quantify the impact of different prompting strategies used in VLM based MLR methods that operate with partial annotations. This base- line only uses the visual information from the large VLMS and is trained using the widely use Asymmetric Loss [34].\nTo establish this baseline, we use only the visual en- coder (Gimg) of CLIP for feature extraction, and use these features for MLR without using the text encoder (Gtext). Specifically, our visual encoder of the baseline (B) fol- lows standard CLIP visual encoder setup of VLM-based MLR methods that operate in partial annotations setting, which involves removing the final pooling layer and obtain- ing spatial features. The final pooling layer is removed to preserve class-specific information across spatial regions, which could otherwise be destroyed by pooling, as pooled features are often dominated by features of a single class, which is not suitable for MLR.\nUsing Gimg, we obtain the feature map (zi) for an image (xi), where $z_i = G_{img}(x_i) \\in \\mathbb{R}^{H\\times W\\times d}$ with height H,"}, {"title": "3.2. PositiveCoOp and NegativeCoOp", "content": "With the baseline established using only visual informa- tion, we now explore how incorporating textual information affects performance. DualCoOp leverages textual informa- tion from CLIP to enhance visual understanding by learning a positive and a negative prompt for each class j, denoted as (tj,+, tj,-), by backpropagating them through the text encoder (Gtext). PositiveCoop and NegativeCoOp are abla- tions of DualCoOp designed to isolate the effect of positive and negative guidance respectively.\nPositiveCoOp. Following prompt-learning approaches [19, 33, 37], for each class j, we initialize a positive prompt tj,+ using the template [V+]{classname}, where V is the learnable word embedding vector that maximizes the co- sine similarity between image and text features to improve recognition. The positive prompt tj,+ is passed through the frozen text encoder (Gtext) to produce an embedding rj,+ \u2208 Rd. The cosine similarity between the image fea- tures (z) and the embeddings (rj,+) indicates the presence of class j in the image. In contrast to existing approaches, we do not use negative prompt. Instead, we learn an em- bedding (rj, \u2208 Rd ) in feature space, trained to provide negative evidence for that class without any guidance from CLIP, as shown in Fig. 4. The cosine similarity of image features (zi) with rj, \u2013 indicates the absence of class j.\nNegativeCoOp. In contrast to PositiveCoOp, this setup evaluates how CLIP guides the learning of a negative prompt, while we remove the positive prompt and learn it directly in the embedding space. Specifically, follow- ing [19, 33, 37], for each class j, we initialize a negative prompt (tj,-) using the template [V-]{classname} which is passed through the frozen text encoder (Gtext) to produce an embedding rj, - \u2208 Rd. We do not use the positive prompt to detect presence of a class, and instead learn an embedding (rj,+ \u2208 Rd) in feature space, trained to provide positive evidence for class j, without any guidance from CLIP. The cosine similarity of image features (zi) with rj,+ indicates the presence of class j, while the similarity between rj,- and zi indicates the absence of the class in the image.\nTo obtain the predictions (p \u2208 R(N\u00d72)) for each of Pos- itiveCoOp and NegativeCoOp, we follow the procedure de- scribed in sec.3.1 and in [37]. This includes class specific region feature aggregation, which takes in the input the im- age features (zi) and the embeddings (rj,+ and rj,-), com- putes the dot product between image and text features to obtain positive map, a+ =(zi rj,+) and negative map, a = (zi rj,-), followed by product with softmax map to assign more focus to the regions that contain class j for a + and to regions that do not contain class j for a (described in sec. 3.1) and aggregation along spatial dimension to get p\u2208 R and p \u2208 R, together to obtain the positive and negative logits for class j, p: [p, p\u012b] and training with the widely used ASL loss."}, {"title": "3.3. Training", "content": "Our works comprises of three setups: Baseline, Positive- CoOp and NegativeCoOp. The visual (Gimg) and textual encoder (Gtext) is frozen for all these setups.\nFor Baseline, we only train the linear projector layer (\u03a6). For PositiveCoOp, we train the positive prompt and the em- bedding rj, in the embedding space. For NegativeCoOp, we learn negative prompt and the embedding rj,+ in the embedding space. We train all setups using widely used Asymmetric Loss (ASL) [34].\nASL [34] is designed to address the inherent imbal- ance in multi-label recognition (MLR) caused by the sig- nificantly higher number of negative examples compared to"}, {"title": "4. Experiments", "content": "4.1. Dataset\nWe evaluate the Baseline (B), PositiveCoOp and Neg- ativeCoOp on standard MLR benchmark datasets: COCO [25] and VOC 2007 [15]. Our focus is on MLR with partial annotations. Consistent with previous approches [6, 13,37], our experiments span a range of annotation availability, from 10% to 90% of the total labels. Below, we outline the key details of these datasets and our approach for gen- erating partial annotations:\nMS-COCO 2014: COCO [25] is a large-scale popular multi-label recognition (MLR) dataset. The datasets con- sists of 80 classes belonging to various categories ranging from everyday objects like cars and people to animals and household items. The dataset contains 82,081 training im- ages and 40,504 validation images. Consistent with existing MLR works, we use the training set for training and the val- idation set for inference.\nPASCAL VOC 2007: VOC 2007 [15] is another widely used outdoor scene MLR dataset. It consists of 20 classes which overlap with the 80 classes from COCO dataset. It consists a total of 9,963 images belonging to those 20 classes. We follow the standard trainval set for training and use the test set for testing.\nTo create training sets with partial labels from these datasets, we follow the methodology described in [6, 13,37]."}, {"title": "4.2. Implementation Details", "content": "For all our experiments, we use the original pretrained weights from CLIP (Contrastive Language-Image Pre- Training) [32] as the VLM. Consistent with existing MLR literature [1, 13, 19,33,37], we use ResNet-101 as the visual encoder and the standard transformer for text encoder. Both visual and text encoders are frozen at all times. For a fair comparison, we use the same settings and hyperparameters as DualCoOp [37]. We resize the images to 448 for both datasets. And follow the augmentation methods Cutout [12] and RandAugment [10] to augment training images as de- scribed in [13, 19, 37]. We train the context vectors [V+] and [V] of learnable prompts with stochastic gradient de- scent (SGD) using initial learning rate of 0.002. For Posi- tiveCoOp and NegativeCoOp, we train the embeddings rj,- and rj,+ respectively, using an initial learning rate of 1.0. For the baseline, we train the linear projector layer (\u03a6) with SGD using initial learning rate of 0.01. All initial learn- ing rates are reduced by cosine annealing for both datasets. Similar to DualCoOp [37], we train all setups for 50 epochs with batch size of 32. We set the loss hyperparameters in Eq. 1 as $ \\gamma = 2, \\gamma_+ = 1$ and \u03b4 = 0.05. We conduct all experiments on a single RTX A4000 GPU."}, {"title": "4.3. Evaluation Metrics", "content": "We evaluate our approach on the MLR datasets using the metric of mean average precision (mAP), as used by previous MLR approaches [8, 13, 33, 37]. mAP is the mean of average precision (AP) values, where AP is computed as the area under the Precision-Recall curve for each class."}, {"title": "4.4. Results", "content": "We primarily compare the three setups (Baseline, Pos- itiveCoOp and NegativeCoOp) with VLM-based MLR methods that operate in partial annotation settings [13,37] where the use of such an approach shows the greatest ben- efits. We do not compare with other MLR methods that are not tailored for partial label settings [16, 18, 40, 40, 47, 48]. Our analysis mostly focuses on DualCoOp [37], because of its wide use, simplicity and focus on use of positive and negative prompts for MLR. We do not compare with Dual- CoOp++ [19], its extension as (1) There is no publicly avail- able code that reproduces their results and (2) They involve other components unrelated to negative prompting, making a specific ablation more difficult. However, we hypothesize that as they also use CLIP to learn a negative prompt, their performance could also benefit from a PositiveCoOp like setup."}, {"title": "5. Analysis", "content": "In this section, we analyze why CLIP's guidance proves ineffective for negative prompt learning. We conduct a se- ries of experiments for this, which follow the settings de- scribed in Sec 4 unless said otherwise."}, {"title": "5.1. Presence of Negative Prompts in LAION-400M", "content": "We hypothesize that negative prompts learnt using CLIP's text encoder are not helpful for MLR because CLIP is not trained on images with such negative captions as im- ages with such captions are rare on the internet from where CLIP's training data is derived from. Simple examples of such positive and negative prompts to detect a dog would be 'A photo of a dog' and 'photo of a park not having a dog', and it is unlikely that the training set contained im- ages of one object (e.g., a car) with captions describing the absence of another object (e.g., \"not having a dog\"). We conduct a series of experiments to test our hypothesis.\nTo test our hypothesis, we analyze the LAION-400M dataset [36], which comprises 400M image-text pairs derived using CLIP and has been used for training several Open Source VLMs such as OpenCLIP [21]. Our analysis of the 413,871,335 texts revealed that only 1,961,669 texts (0.47% of the total) contained negative words, confirming our hypothesis. Of these, 1,366,865 texts (0.33% of the total) included a noun following the negative words, albeit not necessarily immediately after the negative word. This scarcity of negative text suggests that using CLIP's text encoder to learn negative prompts might not yield any performance benefits. Details on the list of negative words used and some examples of negative captions in LAION dataset are provided in Sec 3.2 of the supplementary material."}, {"title": "5.2. Text Encoders Focus on Features Associated with Presence of a Class", "content": "If text encoder of VLM did provide useful guidance about features indicating absence of a class, text embed- dings of handcrafted negative prompts of a class should be very dissimilar from positive prompts for the same classes. But due to the lack of negative captions in LAION400M, we hypothesize that such prompts will have embeddings that are strongly similar to positive prompts with the same noun because of the text encoders focus on nouns while ignoring the negations.\nTo test this, we analyze similarity between CLIP embed- dings of positive and negative prompts of 80 classes in the COCO dataset. Specifically, we use three prompts: positive prompt (P1): 'Photo of a {classname}', the corresponding negative prompt (N1): 'Not a photo of a {classname}' and another positive prompt (P2): 'Picture of a {classname}'. After passing them through CLIP's text encoder, we com- pute the cosine similarity embeddings of P1 and P2, and between those of P1 and N1. These are averaged across all classes. The results in Table 2 show that P1 and N1 are almost as similar as P1-P2 implying that both positive and negative prompts are projected closely in the feature space. This validates our hypothesis, suggesting that the text en-"}, {"title": "6. Conclusions", "content": "In this paper, we examined the role of prompt learning in VLM-based multi-label recognition (MLR) with partial annotations. We specifically estimate the contribution of positive and negative prompts to MLR separately by using our ablated MLR setups: PositiveCoOp and NegativeCoOp, where one prompt is learned under VLM guidance while the other is represented by a learned embedding in the shared feature space. Our results show that learning only positive prompts while using learned negative embeddings (Posi- tiveCoOp) consistently outperforms dual prompt learning approaches, indicating that learning negative prompts for MLR using VLM guidance degrades performance. Our analysis of the LAION-400M points to the lack of negative prompts in the dataset as the likely reason for this. Addi- tionally, we found that in settings with a low proportion of missing labels, a vision-features-only baseline shows signif- icantly strong performance while being much more efficient in terms of computation time (GPU hours) and parameters."}]}