{"title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents", "authors": ["Qiuchen Wang", "Ruixue Ding", "Zehui Chen", "Weiqi Wu", "Shihang Wang", "Pengjun Xie", "Feng Zhao"], "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) enhances Large Models (LMs) by enabling them to use external knowledge to solve problems. As the expression of information becomes increasingly diverse, we often work with visually rich documents that contain diagrams, charts, tables, etc. These visual elements make information easier to understand and are widely used in education, finance, law, and other fields. Therefore, researching RAG within visually rich documents is highly valuable.\nIn practical applications, RAG systems often need to retrieve information from a large collection consisting of hundreds of documents, amounting to thousands of pages. As shown in Fig. 1, existing Visual Question Answering (VQA) benchmarks aren't designed for such large corpus. The queries in these benchmarks are typically paired with one single image(Methani et al., 2020; Masry et al., 2022; Li et al., 2024; Mathew et al., 2022) or document(Ma et al., 2024), which is used for evaluating Q&A tasks but not suitable for evaluating RAG systems. The answers to queries in these datasets may not be unique within the whole corpus.\nTo address this gap, we introduce ViDoSeek, a novel dataset designed for visually rich document retrieval-reason-answer. In ViDoSeek, each query has a unique answer and specific reference pages. It covers the diverse content types and multi-hop reasoning that most VQA datasets include. This specificity allows us to better evaluate retrieval and generation performance separately.\nMoreover, to enable models to effectively reason over a large corpus, we propose ViDoRAG, a multi-agent, coarse-to-fine retrieval-augmented generation framework tailored for visually rich documents. Our approach is based on two critical observations: (i) Inefficient and Variable Retrieval Performance. Traditional OCR-based retrieval struggles to capture visual information. With the development of vision-based retrieval, it is easy to capture visual information(Faysse et al., 2024; Yu et al., 2024a; Zhai et al., 2023). However, there lack of an effective method to integrate visual and textual features, resulting in poor retrieval of relevant content. (ii) Insufficient Activation of Reasoning Capabilities during Generation. Previous studies on inference scaling for RAG focus on expanding the length of retrieved documents(Jiang et al., 2024; Shao et al., 2025; Xu et al., 2023). However, due to the characteristics of VLMs, only emphasizing on the quantity of knowledge without providing further reasoning guidance presents certain limitations. There is a need for an effective inference scale-up method to efficiently utilize specific action spaces, such as resizing and filtering, to fully activate reasoning capabilities.\nBuilding upon these insights, ViDoRAG introduces improvements in both retrieval and generation. We propose Multi-Modal Hybrid Retrieval, which combines both visual and textual features and dynamically adjusts results distribution based on Gaussian Mixture Models (GMM) prior. This approach achieves the optimal retrieval distribution for each query, enhancing generation efficiency by reducing unnecessary computations. During generation, our framework comprises three agents: the seeker, inspector, and answer agents. The seeker rapidly scans thumbnails and selects relevant images with feedback from the inspector. The inspector reviews, then provides reflection and offers preliminary answers. The answer agent ensures consistency and gives the final answer. This framework reduces exposure to irrelevant information and ensures consistent answers across multiple scales.\nOur major contributions are as follows:\n\u2022 We introduce ViDoSeek, a benchmark specifically designed for visually rich document retrieval-reason-answer, fully suited for evaluation of RAG within large document corpus.\n\u2022 We propose ViDoRAG, a novel RAG framework that utilizes a multi-agent, actor-critic paradigm for iterative reasoning, enhancing the noise robustness of generation models.\n\u2022 We introduce a GMM-based multi-modal hybrid retrieval strategy to effectively integrate visual and textual pipelines.\n\u2022 Extensive experiments demonstrate the effectiveness of our method. ViDoRAG significantly outperforms strong baselines, achieving over 10% improvement, thus establishing a new state-of-the-art on ViDoSeek."}, {"title": "2 Related Work", "content": "Visual Document Q&A Benchmarks. Visual Document Question Answering is focused on answering questions based on the visual content of documents(Antol et al., 2015; Ye et al., 2024; Wang et al., 2024). While most existing research (Methani et al., 2020; Masry et al., 2022; Li et al., 2024; Mathew et al., 2022) has primarily concentrated on question answering from single images, recent advancements have begun to explore multi-page document question answering, driven by the increasing context length of modern models (Mathew et al., 2021; Ma et al., 2024; Tanaka et al., 2023). However, prior datasets were not well-suited for RAG tasks involving large collections of documents. To fill this gap, we introduce Vi-DoSeek, the first large-scale document collection QA dataset, where each query corresponds to a unique answer across a collection of ~ 6k images.\nRetrieval-augmented Generation. With the advancement of large models, RAG has enhanced the ability of models to incorporate external knowledge (Lewis et al., 2020; Chen et al., 2024b; Wu et al., 2025). In prior research, retrieval often followed the process of extracting text via OCR technology (Chen et al., 2024a; Lee et al., 2024; Robertson et al., 2009). Recently, the growing interest in multimodal embeddings has greatly improved image retrieval tasks (Faysse et al., 2024; Yu et al., 2024a). Additionally, there are works that focus on In-Context Learning in RAG(Agarwal et al., 2025; Yue et al., 2024; Team et al., 2024; Weijia et al.,"}, {"title": "3 Problem Formulation", "content": "Given a query as q, and we have a collection of documents $C = {D_1, D_2, ...,D_M}$ which contains M documents. Each document $D_m$ consists of N pages, each image representing an individual page, defined as $D_m = {I_1, I_2, ..., I_N}$. The total number of images included in the collection is $\\sum_{i=1}^M |D_m|$. We aim to retrieve the most relevant information efficiently and accurately and generate the final answer a to the query q."}, {"title": "4 ViDoSeek Dataset", "content": "Existing VQA datasets typically consist of queries paired with a single image or a few images. However, in practical application scenarios, users often pose questions based on a large-scale corpus rather than targeting an individual document or image. To better evaluate RAG systems, we prefer questions that have unique answers when retrieving from a large corpus. To address this need, we introduce a novel Visually rich Document dataset specifically designed for RAG systems, called Vi-DoSeek. Below we provide the pipeline for constructing the dataset(\u00a74.1) and a detailed analysis of the dataset (\u00a74.2).\nTo construct the ViDoSeek dataset, we developed a four-step pipeline to ensure that the queries meet our stringent requirements. As illustrated in Figure 2, our dataset comprises two parts: one annotated from scratch by our Al researchers, and the other derived from refining queries in the existing open-source dataset SlideVQA (Tanaka et al., 2023). For the open-source dataset, we initiate the query refinement starting from the third step of our pipeline. For the dataset we build from scratch, we follow the entire pipeline beginning with document collection. The following outlines our four-step pipeline:\nStep 1. Document Collecting. As slides are a widely used medium for information transmission today, we selected them as our document source. We began by collecting English-language slides containing 25 to 50 pages, covering 12 domains such as economics, technology, literature, and geography. And we filtered out 300 slides that simultaneously include text, charts, tables, and two-dimensional layouts which refer to flowcharts, diagrams, or any visual elements composed of various components and are a distinctive feature of slides.\nStep 2. Query Creation. To make the queries more suitable for RAG over a large-scale collection, our experts were instructed to construct queries that are specific to the document. Additionally, we encouraged constructing queries in various forms and with different sources and reasoning types to better reflect real-world scenarios.\nStep 3. Quality Review. In large-scale retrieval and generation tasks, relying solely on manual annotation is challenging due to human brain limitations. To address this, we propose a review module that automatically identifies problematic queries.\nStep 4. Multimodal Refine. In this final step, we refine the queries that did not meet our standards during the quality review. We use carefully designed VLM-based agents to assist us throughout the entire dataset construction pipeline."}, {"title": "4.2 Dataset Analysis", "content": "Dataset Statistics. ViDoSeek is the first dataset specifically designed for question-answering over large-scale document collections. It comprises approximately ~ 1.2k questions across a wide array of domains, addressing four key content types: Text, Chart, Table, and Layout. Among these, the Layout type poses the greatest challenge and represents the largest portion of the dataset. Additionally, the queries are categorized into two reasoning types: single-hop and multi-hop. Further details of the dataset can be found in the Appendix B and C.\nComparative Analysis. Table 1 highlights the limitations of existing datasets, which are predominantly tailored for scenarios involving single images or documents, lacking the capacity to handle the intricacies of retrieving relevant information from large collections. ViDoSeek bridges this gap by offering a dataset that more accurately mirrors real-world scenarios. This facilitates a more robust and scalable evaluation of RAG systems."}, {"title": "5 Method", "content": "In this section, drawing from insights and foundational ideas, we present a comprehensive description of our ViDoRAG framework, which integrates two modules: Multi-Modal Hybrid Retrieval (\u00a75.1) and Multi-Scale View Generation (\u00a75.2)."}, {"title": "5.1 Multi-Modal Hybrid Retrieval", "content": "For each query, our approach involves retrieving information through both textual and visual pipelines, dynamically determining the optimal value of top-K using a Gaussian Mixture Model (GMM), and merging the retrieval results from both pipelines.\nAdaptive Recall with Gaussian Mixture Model. Traditional methods rely on a static hyperparameter, K, to retrieve the top-K images or text chunks from a corpus. A smaller K might fail to capture sufficient references needed for accurate responses, as the most relevant nodes are not always ranked at the top. Conversely, a larger K can slow down inference and introduce inaccuracies due to noise. Additionally, manually tuning K for different scenarios is troublesome.\nOur objective is to develop a straightforward yet effective method to automatically determine K for each modality, without the dependency on a fixed value. We utilize the similarity S of the embedding E to quantify the relevance between the query and the document collection C:\n$S(q, C) = {s_i|cos(E_q, E_{p_i}), p_i \\in C}$ (1)\nwhere $s_i$ represents the cosine similarity between the query Q and page $p_i$. In the visual pipeline, a page corresponds to an image, whereas in the textual pipeline, it corresponds to chunks of OCR text. We propose that the distribution of S follows a GMM and we consider they are sampled from a bimodal distribution $P(s)$ shown in Fig.3:\n$P(s) = \\omega_F \\cdot N(s | \\mu_F, \\sigma^2) + \\omega_T \\cdot N(s | \\mu_T, \\sigma_T^2)$ (2)\nwhere N represents a Gaussian distribution, with $\\omega, \\mu, \\sigma^2$ indicating the weight, mean, and variance, respectively. The subscripts T and F refer to the distributions of pages with high and low similarity. The distribution with higher similarity is deemed valuable for generation. The Expectation-Maximization (EM) algorithm is utilized to estimate the prior probability $P(T|s, \\mu_T, \\sigma_T^2)$ for each modality. The dynamic value of K is defined as:\n$K = |{p_i \\in C | P_i \\sim N(\\mu_T, \\sigma_T^2)}|$ (3)\nConsidering that the similarity score distribution for different queries within a document collection may not strictly follow a standard distribution, we establish upper and lower bounds to manage outliers. The EM algorithm is employed sparingly, less than ~ 1% of the time. Dynamically adjusting K enhances generation efficiency compared to a static setting. Detailed analysis is available in \u00a77.2."}, {"title": "Textual and Visual Hybrid Retrieval.", "content": "In the previous step, nodes were retrieved from both pipelines. In this phase, we integrate them:\n$R_{hybrid} = Sort[F(R_{Text}, R_{visual})]$ (4)\nwhere $R_{Text}$ and $R_{visual}$ denote the retrieval results from the textual and visual pipelines, respectively. The function F(\u00b7) signifies a union operation, and Sort(\u00b7) arranges the nodes in their original sequence, as continuous pages often exhibit correlation (Yu et al., 2024b).\nThe textual and visual retrieval pipelines demonstrate varying levels of performance for different features. Without adaptive recall, the combined retrieval $R_{hybrid}$ can become excessive. Adaptive recall ensures that effective retrievals are concise, while traditional pipelines yield longer recall results. This strategy optimizes performance relative to context length, underscoring the value of adaptive recall in hybrid retrieval."}, {"title": "5.2 Multi-Agent Generation with Iterative Reasoning", "content": "During the generation, we introduce a multi-agent framework which consists of three types of agents: the Seeker Agent, the Inspector Agent, and the Answer Agent. As illustrated in Fig. 3, this framework extracts clues, reflects, and answers in a coarse-to-fine manner from a multi-scale perspective. More details are provided in Appendix D.\nSeeker Agent: Hunting for relevant images. The Seeker Agent is responsible for selecting from a coarse view and extracting global cues based on the query and reflection from the Inspector Agent. We have made some improvements to ReAct(Yao et al., 2022) to facilitate better memory management. The action space is defined as the selection of the images. Initially, the agent will reason only based on the query Q and select the most relevant images $I_t$ from the candidate images I, while the initial memory $M_0$ is empty. In step t, the candidate images $I_{t+1}$ are the complement of previously selected images $I_t$, defined as $I_{t+1} = I \\ I_t$. The seeker has received the reflection $F_{t-1}$ from the inspector, which includes an evaluation of the selected images and a more detailed description of the requirements for the images. The Seeker integrates feedback $F_{t-1}$ from the Inspector, which includes an evaluation of the selected images and a description of image requirements, to further refine the selection $I_t$ and update the memory $M_{t+1}$:\n$I_{t+1}, M_{t+1} = \\Theta(I, Q, M_t, F_{t-1})$ (5)\nwhere $M_{t+1}$ represents the model's thought content in step t under the ReAct paradigm, maintaining a constant context length. The process continues until the Inspector determines that sufficient information is available to answer the query, or the Seeker concludes that no further relevant images exist among the candidates.\nInspector Agent: Review in detail and Reflect. In baseline scenarios, increasing the top-K value improves recall@K, but accuracy initially rises and then falls. This is attributed to interference from irrelevant images, referred to as noise, affecting model generation. To address this, we use Inspector to perform a more fine-grained inspection of the images. In each interaction with the Seeker, the Inspector's action space includes providing feedback or drafting a preliminary answer. At step t, the inspector reviews images at high resolution, denoted as $(I_t \\cup I_{t-1}, Q)$ where $I_{t-1}$ are images retained from the previous step and $I_t$ are from the Seeker. If the current information is sufficient to answer the query, a draft answer $\\hat{A}$ is provided, alongside a reference to the relevant image:\n$\\hat{A}, I_{ref} = \\Theta(I_t \\cup I_{t-1}, Q)$ (6)\nConversely, if more information is needed, the Inspector offers feedback $F_t$ to guide the Seeker in better image selection and identifies images $I_t^+$ to retain for further review in the next step t + 1:\n$F_t, I_t^+ = \\Theta(I_t \\cup I_{t-1}, Q)$ (7)\nThe number of images the Inspector reviews is typically fewer than the Seeker's, ensuring robustness in reasoning, particularly for Visual Language Models with moderate reasoning abilities.\nAnswer Agent: Synthesize the final answer. In our framework, the Seeker and Inspector engage in a continuous interaction, and the answer agent provides the answer in the final step. To balance accuracy and efficiency, the Answer Agent verifies the consistency of the Inspector's draft answer $\\hat{A}$. If the reference image matches the Inspector's input, the draft answer is accepted as the final answer A = $\\hat{A}$. If the reference image is a subset of the input image, the answer agent should check for consistency between the draft answer $\\hat{A}$ and the reference image, then give the final answer A: If the reference image is a subset of Inspector's the input, the Answer Agent ensures consistency between the draft answer $\\hat{A}$ and the reference image before finalizing the answer A:\n$A = \\Theta(I_{ref}, Q, \\hat{A})$ (8)\nThe Answer Agent utilizes the draft answer as prior knowledge to refine the response from coarse to fine. The consistency check between the Answer Agent and Inspector Agent enhances the depth and comprehensiveness of the final answer."}, {"title": "6 Experiments", "content": "6.1 Experimental Settings\nEvaluation Metric For our end-to-end evaluation, we employed a model-based assessment using GPT-40, which involved assigning scores from 1 to 5 by comparing the reference answer with the final answer. Answers receiving scores of 4 or above were considered correct, and we subsequently calculate accuracy as the evaluation metric. For retrieval evaluation, we use recall as the metric.\nBaselines and Oracle. We selecte Nv-embed-V2(Lee et al., 2024) and ColQwen2(Faysse et al., 2024) as the retrievers for the TextRAG and VisualRAG baselines, respectively. Based on their original settings, we choose the top-5 recall results as the generation input, which equals the average length of dynamic recall results. This ensures a fair comparison and highlights the advantages of our method. The Oracle serves as the upper bound performance, where the model responds based on the golden page without retrieval or other operations.\n6.2 Main Results\nAs shown in Table. 2, we conducted experiments on both closed-source and open-source models: GPT-40, Qwen2.5-7B-Instruct, Qwen2.5-VL-7B(Yang et al., 2024)-Instruct, Llama3.2-Vision-90B-Instruct. Closed-source models generally outperform open-source models performance. It is worth mentioning that the qwen2.5-VL-7B has shown excellent instruction-following and reasoning capabilities within our framework. In contrast, we found that the llama3.2-VL requires 90B parameters to accomplish the same instructions, which may be related to the model's pre-training domain. The results suggest that while API-based models offer strong baseline performance, our method is also effective in enhancing the performance of open-source models, offering promising potential for future applications. To further demonstrate the robustness of the framework, we constructed a pipeline using data to rewrite queries from Slide-VQA(Tanaka et al., 2023), making the queries suitable for scenarios involving large corpora. The experimental results are presented the analysis."}, {"title": "6.3 Retrieval Evaluation", "content": "In Table 3, we report the detailed performance for various retrievers, including OCR-based and visual-based. Due to the uncertainty of dynamical retrieval across queries, we use the average length of results for analysis. Our goal is to incorporate more relevant information within a shorter context while minimizing the impact of noise and reducing computational cost without losing valuable information. Dynamic retrieval can achieve better recall performance with a smaller context length, while hybrid retrieval combines the results of two pipelines achieving state-of-the-art performance."}, {"title": "7 Analysis", "content": "7.1 Ablations\nTable 4 presents the impact of different retrievers and generation methods on performance. We have decomposed the dynamic retrieval into two components, Dynamic and Hybrid. Naive refers to the method of direct input, which is most commonly used as baselines. Dynamic indicates using GMM to fit the optimal recall distribution based solely on the visual pipeline. Hybrid refers to merging the visual and the textual retrieval results directly, which leads to suboptimal results due to long contexts. Experiments demonstrate that the effectiveness and scalability of our improvements on retrieval and generation modules, as well as their combination, can comprehensively enhance end-to-end performance from various perspectives.\n7.2 Time Efficiency\nHow does dynamic retrieval balance latency and accuracy? In traditional RAG systems, using a small top-K value may result in missing critical information, whereas employing a larger value can introduce noise and increase computational overhead. ViDoRAG dynamically determines the number of documents to retrieve based on the similarity distribution between the query and the corpus. This approach ensures that only the most relevant documents are retrieved, thereby reducing unnecessary computations from overly long contexts and accelerating the generation process. As shown in Table 5, we compare retrieval with and without GMM based on the Naive method. The experiments indicate that GMM may reduce recall due to distribution bias. However, because it significantly shortens the generation context, it effectively improves performance in end-to-end evaluations.\nLatency Analysis of the Multi-Agent Generation. There is an increase in delay due to the iterative nature of the multi-agent system, as shown in Fig. 5. Each agent performs specific tasks in a sequential manner, which adds a small overhead compared to traditional straightforward RAG. However, despite the increase in latency, the overall performance improves due to the higher quality of generated answers, making the trade-off between latency and accuracy highly beneficial for complex RAG tasks.\n7.3 Modalities and Strategies of Generation\nAs shown in Fig. 6, the vision-based pipeline outperforms the text-based pipeline across all types, even for queries related to text content. Generally speaking, due to models' inherent characteristics, the reasoning ability of LLMs is stronger than that of VLMs. However, the lack of visual information makes it difficult for models to identify the intrinsic connections between pieces of information. This also poses a challenge for the generation of content based on visually rich documents. While obtaining visual information, VidoRAG further enhances the reasoning capabilities of VLMs, striking a balance between accuracy and computational load."}, {"title": "7.4 Performance with Test-time Scaling", "content": "Fig. 7 illustrates the number of interaction rounds between the seeker and inspector within ViDoRAG based on different models. Due to the limited instruction capabilities of some models, we sampled 200 queries for the experiment. Models with stronger performance require fewer reasoning iterations, while weaker models often need additional time to process and reach a conclusion. Conditioning the model on a few demonstrations of the task at inference time has been proven to be a computationally efficient approach to enhance model performance(Brown et al., 2020; Min et al., 2021). The results indicate that predefining tasks and breaking down complex tasks into simpler ones is an effective method for scaling inference."}, {"title": "8 Conclusion", "content": "In this work, we introduced ViDoRAG, a novel multi-agent RAG framework tailored for visually rich documents. By proposing a coarse-to-fine reasoning process and a multi-modal retrieval strategy, ViDoRAG significantly outperforms existing methods, achieving new SOTA on the ViDoSeek benchmark. Future work will focus on further optimizing the framework's efficiency while maintaining high accuracy, and exploring its potential in diverse real-world applications, such as education and finance, where visually rich document RAG is crucial."}, {"title": "Limitations", "content": "In addition to the advanced improvements mentioned above, our work has several limitations:\n(1) Potential Bias in Query Construction. The queries in ViDoSeek were constructed by human experts, which may introduce bias in the types of questions and the way they are phrased. This could affect the model's ability to handle more diverse and natural language queries from real-world users.\n(2) Computational Overhead of ViDoRAG. The multi-agent framework, while effective in enhancing reasoning capabilities, introduces additional computational overhead due to the iterative interactions between the seeker, inspector, and answer agents. This may limit the scalability of the framework in scenarios with strict latency requirements.\n(3) Model Hallucinations. Despite the improvements in retrieval and reasoning, the models used in ViDoRAG can still generate hallucinated answers that are not grounded in the retrieved information. This issue can lead to incorrect or misleading responses, especially when the model is overconfident in its generated content.\nIn summary, while ViDoRAG demonstrates significant improvements in visually rich document retrieval and reasoning, there are still areas for further enhancement, particularly in terms of generalization to diverse document types, reducing potential biases in query construction, optimizing the computational efficiency of the multi-agent framework, and addressing the issue of model hallucinations. Future work will focus on addressing these limitations to further improve the robustness and applicability of the model."}, {"title": "Ethical Considerations", "content": "Our data does not contain any private or sensitive information, and all content is derived from publicly available sources. Additionally, the construction and refinement of the dataset were conducted in a manner that respects copyright and intellectual property rights."}, {"title": "A Additional Experiments Details", "content": "Backbones. To thoroughly validate the effectiveness of ViDoRAG, we conducted experiments on various models across various baselines, including both closed-source and open-source models: GPT-40, Qwen2.5-7B, Llama3.2-3B, Qwen2.5-VL-7B(Yang et al., 2024), Llama3.2-Vision-90B. For OCR-based pipelines, we use PPOCR(Ma et al., 2019) to recognize text within documents. Optionally, VLMs can also be employed for text recognition, as their OCR capabilities are quite strong.\nExperimental Environments. We conducted our experiments on a server equipped with 8 A100 GPUs and 96 CPU cores. Open-source models require substantial computational resources.\nRetrieval Implementation Details. Due to the context length limitations of the model, we use the Top-2K pages to fit the GMM and we restrict the output chunks of the GMM algorithm to be between $K/2$ and K, we set K = 10 in practice."}, {"title": "B More Details on Datasets", "content": "B.1 Annotation Case"}, {"title": "B.2 Details on ViDoSeek", "content": "More Dataset Statistics. The statistical about ViDoSeek is presented in Table 7. We categorize queries from a logical reasoning perspective into single-hop and multi-hop. Text, Table, Chart and Layout represent different sources of reference.\nDataset Difficulty. ViDoSeek sets itself apart with its heightened difficulty level, attributed to the multi-document context and the intricate nature of its content types, particularly the Layout category. The dataset contains both single-hop and multi-hop queries, presenting a diverse set of challenges. Consequently, ViDoSeek serves as a more comprehensive and demanding benchmark for RAG systems compared to previous works."}, {"title": "B.3 Details on SlideVQA-Refined", "content": "Dataset Statistics. We supplemented our experiments with the SlideVQA dataset to demonstrate the scalability of our method. SlideVQA categorizes queries from a logical reasoning perspective into single-hop and multi-hop. Non-span, single-span, and multi-span respectively refer to answers derived from a single information-dense sentence, reference information that is sparse but located on the same page, and reference information distributed across different pages. The statistical information about dataset is presented in Table 7.\nDataset Difficulty. The SlideVQA dataset focuses on evaluating the RAG system's ability to understand both visually sparse and visually dense information. When multi-hop questions involve reference information spread across different pages, it presents a significant challenge to the RAG system, further demonstrating the effectiveness of our approach."}, {"title": "C Data Construction Details", "content": "To construct the ViDoSeek dataset, we developed a four-step pipeline to ensure that the queries meet our requirements.\nStep 1. Document Collecting. We collected English-language slides containing 25 to 50 pages, covering 12 domains such as economics, technology, literature, and geography, etc.\nStep 2. Query Creation. To make the queries more suitable for RAG over a large-scale collection, our experts constructed queries based on the following requirements: (i) Each query must have a unique answer when paired with the document. (ii) The query must include unique keywords that point to the specific document and pages. (iii) The query should require external knowledge. Additionally, we encouraged constructing queries in various forms and with different sources and reasoning types to better reflect real-world scenarios. Our queries not only focus on types of references, including text, tables, charts, and layouts, but also provide a classification of reasoning types, including single-hop and multi-hop.\nStep 3. Quality Review. To effectively evaluate the generation and retrieval quality of our RAG system, we require queries that yield unique answers, preferably located on a specific page or within a few pages. However, in large-scale retrieval and generation tasks, relying solely on manual annotation is challenging due to human cognitive limitations. To address this, we propose a review module that automatically identifies problematic queries. This module consists of two steps: (i) We prompt LLMs to filter out queries that may have multiple answers across the document collection; for example, the question What is the profit for this company in 2024? might have a unique answer within a single document but could yield multiple answers in a multi-document setting. (ii) For the remaining queries, we retrieve the top-k slides for each query and use a VLM to determine whether each slide can answer the query. If only the golden page can answer the question, we consider it to meet the requirements. If pages other than the golden page can answer the query, we have experts manually evaluate and refine them.\nStep 4. Multimodal Refine. In this final step, we refine the queries that did not meet our standards during the quality review. The goal is to adjust these queries so they satisfy the following requirements: (i) The refined query should point to specific pages within the large collection with minimal additional information; (ii) The refined query must retain its original meaning. We use carefully designed VLM-based agents to assist us throughout the entire dataset construction pipeline. The prompt is presented in Fig. 9 and Fig. 10, respectively. We will first perform filtering based on semantics, and then conduct a fine-grained review using a multimodal reviewer."}, {"title": "D More Details about Multi-Agent Generation with Iterative Reasoning", "content": "We designed prompts to drive VLMs-based agents, and through our experiments, we found that some open-source models require the design of few-shot examples to learn specific thought patterns. See detailed prompts in Fig. 12, Fig.13 and Fig.14."}]}