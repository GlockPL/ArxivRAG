{"title": "Learning Truncated Causal History Model for Video Restoration", "authors": ["Amirhosein Ghasemabadi", "Mohammad Salameh", "Muhammad Kamran Janjua", "Di Niu"], "abstract": "One key challenge to video restoration is to model the transition dynamics of video frames governed by motion. In this work, we propose TURTLE to learn the TRUncated causal history model for efficient and high-performing video restoration. Unlike traditional methods that process a range of contextual frames in parallel, Turtle enhances efficiency by storing and summarizing a truncated history of the input frame latent representation into an evolving historical state. This is achieved through a sophisticated similarity-based retrieval mechanism that implicitly accounts for inter-frame motion and alignment. The causal design in TURTLE enables recurrence in inference through state-memorized historical features while allowing parallel training by sampling truncated video clips. We report new state-of-the-art results on a multitude of video restoration benchmark tasks, including video desnowing, nighttime video deraining, video raindrops and rain streak removal, video super-resolution, real-world and synthetic video deblurring, and blind video denoising while reducing the computational cost compared to existing best contextual methods on all these tasks.", "sections": [{"title": "1 Introduction", "content": "Video restoration aims to restore degraded low-quality videos. Degradation in videos occurs due to noise during the acquisition process, camera sensor faults, or external factors such as weather or motion blur [53, 38]. Several methods in the literature process the entire video either in parallel or with recurrence in design. In the former case, multiple contextual frames are processed simultaneously to facilitate information fusion and flow, which leads to increased memory consumption and inference cost as the context size increases [63, 4, 69, 86, 58, 28, 26, 5, 34, 62]. Methods with recurrence in design reuse the same network to process new frame sequentially based on previously refined ones [54, 14, 21, 25, 6, 7, 42, 57]. Such sequential processing approaches often result in cumulative errors, leading to information loss in long-range temporal dependency modeling [8] and limiting parallelization capabilities.\nRecently, methods based on state space models (SSMs) have seen applications across several machine vision tasks, including image restoration [19, 56], and video understanding [30]. While Video-Mamba [30] proposes a state space model for video understanding, the learned state space does not reason at the pixel level and, hence, can suffer from information collapse in restoration tasks [77]."}, {"title": "2 Related Work", "content": "Video restoration is studied from several facets, mainly distributed in how the motion is estimated and compensated for in the learning procedure and how the frames are processed. Additional literature review is deferred to appendix F.\nMotion Compensation in Video Restoration. Motion estimation and compensation are crucial for correcting camera and object movements in video restoration. Several methods employ optical flow to explicitly estimate motion and devise a compensation strategy as part of the learning procedure, such as deformable convolutions [33, 34], or flow refinement [23]. However, optical flow can struggle with degraded inputs [84, 3, 20], often requiring several refinement stages to achieve precise flow estimation. On the other end, methods also rely on the implicit learning of correspondences in the latent space across the temporal resolution of the video; a few techniques include temporal shift modules [29], non-local search [64, 32, 85], or deformable convolutions [69, 13, 80].\nVideo Processing Methods. There is a similar distinction in how a video is processed, with several methods opting for either recurrence in design or restoring several frames simultaneously. Parallel methods, also known as sliding window methods, process multiple frames simultaneously. This sliding window approach can lead to inefficiencies in feature utilization and increased computational costs [63, 4, 69, 86, 58, 28, 26, 5, 34, 62, 9]. Although effective in learning joint features from the entire input context, their size and computational demands often render them unsuitable for resource-constrained devices. Conversely, recurrent methods restore frames sequentially, using multiple stages to propagate latent features [87, 81, 82]. These methods are prone to information loss [33]. Furthermore, while typical video restoration methods in the literature often rely on context from both past and future neighboring frames [34, 33, 29], TURTLE is causal in design, focuses on using only past frames. This approach allows TURTLE to apply in scenarios like streaming and online video restoration, where future frames are unavailable."}, {"title": "3 Methodology", "content": "Consider a low-quality video $I_{LQ} \\in \\mathbb{R}^{T \\times H \\times W \\times C}$, where $T, H, W, C$ denote the temporal resolution, height, width, and number of channels, respectively, that has been degraded with some degradation"}, {"title": "3.1 Architecture Design", "content": "Given a model $M_{\\Theta}$, let $F_t^l$ denote the feature map of a frame at timestep $t$, taken from $M_{\\Theta}$ at layer $l$. We, then, utilize $F_t^l$ to construct the causal history states denoted as $H_t^l \\in \\mathbb{R}^{T \\times h^l \\times w^l \\times c^l}$, where $T$ is the truncation factor (or length of the history), $h, w$ denote spatial resolution of the history, and $c$ denotes the channels. More specifically, $H_t^l = \\{ F_{t-T+1}^l \\oplus ... \\oplus F_{t-1}^l \\} \\in \\mathbb{R}^{T \\times h^l \\times w^l \\times c^l}$ where $\\oplus$ is the concatenation operation. We denote the motion-compensated history at timestep $t$ as $\\hat{H}_t^l$, which is compensated for motion with respect to the input frame features $F_t^l$. In this work, the state refers to the representation of a frame of the video. Further, history states (or causal history states) refers to a set of certain frame features previous to the input at some timestep.\nTURTLE's encoder learns a representation of each frame by downsampling the spatial resolution, while inflating the channel dimensions by a factor of 2. At each stage of the encoder, we opt for several stacked convolutional feedforward blocks, termed as Historyless FFN. The learned representation at the last encoder stage onwards is fed to a running history queue Q of length $\\gamma$. We empirically set $\\gamma = 5$ for all the tasks, and consider sequence of 5 frames. The entire video sequence is reshaped"}, {"title": "3.2 Causal History Model", "content": "CHM learns to align the history states with respect to the input feature map. Further, there could still exist potential degradation differences at the same feature locations along the entire sequence in the motion-compensated history states. To this end, CHM re-weights the sequence along the temporal dimension to accentuate significant features and suppress irrelevant ones. Let $\\hat{H}_t^l \\in \\mathbb{R}^{(\\tau + 1) \\times h^l \\times w^l \\times c^l}$ denote the motion-compensated causal history states, and let input feature map be $F_t^l \\in \\mathbb{R}^{h^l \\times w^l \\times c^l}$. Let the transformation on the history states to align the features be denoted by $\\Phi_t^l$, and let $\\mathcal{V}_t^l$ denote the re-weighting scheme. If the output is given by $y_t^l \\in \\mathbb{R}^{h^l \\times w^l \\times c^l}$, we then, formalize the Causal History Model (CHM) as,\n$\\begin{aligned}\n\\hat{H}_t^l &= \\Phi_t^l(H_t^l, F_t^l) \\oplus \\mathcal{B}_t^l(F_t^l),  \\\\\ny_t^l &= \\mathcal{V}_t^l(\\hat{H}_t^l, F_t^l) + \\mathcal{D}_t^l(F_t^l).\n\\end{aligned}$\nIn eq. (1), $\\mathcal{B}_t^l$ denotes transformation on the input, and $\\mathcal{D}_t^l$ denotes the skip connection, while $\\oplus$ is the concatenation operation. In practice, we learn $\\Phi_t^l$, and the input transformation matrix $\\mathcal{B}$ following the procedure described in State Align Block, while $\\mathcal{V}_t^l$ is detailed in Frame History Router. We present a visual illustration of Causal History Model (CHM) in fig. 2. We also present a special case of (CHM) in appendix C, wherein we consider optimally compensated motion in videos."}, {"title": "4 Experiments", "content": "We follow the standard training setting of architectures in the restoration literature [29, 79, 15] with Adam optimizer [27] ($\\beta_1 = 0.9, \\beta_2 = 0.999$). The initial learning rate is set to 4e-4, and is decayed to 1e-7 throughout training following the cosine annealing strategy [40]. All of our models are implemented in the PyTorch library, and are trained on 8 NVIDIA Tesla v100 PCIe 32 GB GPUs for 250k iterations. Each training video is sampled into clips of $\\gamma = 5$ frames, and TURTLE restores frames of each clip with recurrence. The training videos are cropped to 192 \u00d7 192 sized patches at random locations, maintaining temporal consistency, while the evaluation is done on the full frames during inference. We assume no prior knowledge of the degradation process for all the tasks. Further, we apply basic data augmentation techniques, including horizontal-vertical flips and 90-degree rotations. Following the video restoration literature, we use Peak Signal-to-Noise"}, {"title": "4.1 Night Video Deraining", "content": "SynNightRain [47] is a synthetic video deraining dataset focusing on nighttime videos wherein rain streaks get mixed in with significant noise in low-light regions. Therefore, nighttime deraining with heavy rain is generally a harder restoration task than other daytime video deraining. We follow the train/test protocol outlined in [47, 35], and train TURTLE on 10 videos from scratch, and evaluate on a held-out test set of 20 videos. We report distortion metrics, PSNR and SSIM, in table 1, and compare them with previous restoration methods. TURTLE achieves a PSNR of 29.26 dB, which is a notable improvement of +2.53 dB over the next best result, NightRain [35]. Further, we present visual results in fig. 3, and in fig. 12. Our method, TURTLE, maintains color consistency in the restored results."}, {"title": "4.2 Video Desnowing", "content": "Realistic Video Desnowing Dataset (RVSD) [10] is a video-first desnowing dataset simulating realistic physical characteristics of snow and haze. The dataset comprises a variety of scenes, and the videos are captured from various angles to capture realistic scenes with different intensities. In total, the dataset includes 110 videos, of which 80 are used for training, while 30 are held-out test set to measure desnowing performance. We follow the proposed train/test split in the original work [10] and train TURTLE on the video desnowing dataset. Our scores, 26.02 dB in PSNR, are reported in table 2, and compared to previous methods, TURTLE significantly improves the performance by +0.96 dB in PSNR. Notably, TURTLE is prior-free, unlike the previous best result SVDNet [10], which exploits snow-type priors. We present visual results in fig. 3, and in fig. 11 comparing TURTLE to SVDNet [10]. Our method not only removes snowflakes but also removes haze, and the restored frame is visually pleasing."}, {"title": "4.3 Real Video Deblurring", "content": "The work done in [83, 82] introduced a real-world deblurring dataset (BSD) using the Beam-Splitter apparatus. The dataset introduced contains three different variants depending on the blur intensity settings. Each of the three variants has a total of 11,000 blurry/sharp pairs with a resolution of 640 \u00d7 480. We employ the variant of BSD with the most blur exposure time, i.e., 3ms-24ms. We follow the standard train/test split introduced in [83] with 60 training videos, and 20 test videos. We report the scores in table 3 on the 3ms-24ms variant of BSD and compare with previously published methods. TURTLE scores 33.58 dB in PSNR on the task, observing an increase of +2.0 dB compared to the previous best methods, CDVD-TSP [44], and ESTRNN [83, 82]. We present visual results in fig. 13."}, {"title": "4.4 Synthetic Video Deblurring", "content": "GoPro dataset [41] is an established video deblurring benchmark dataset in the literature. The dataset is prepared with videos taken from a GOPRO4 Hero consumer camera, and the videos are captured at 240fps. Blurs of varying strength are introduced in the dataset by averaging several successive frames; hence, the dataset is a synthetic blur dataset. We follow the standard train/test split of the dataset [41], and train our proposed method. TURTLE scores 34.50 dB in PSNR on the task, with an increase of +0.34 dB compared to the previous best method in a comparable computational budget, DSTNet [45] (see table 4). We also present visual results on the GoPro dataset [41] comparing TURTLE to DSTNet [45] in fig. 4, and fig. 9. Our method restores frames free of artifacts (see the number plate on the car) in fig. 4."}, {"title": "4.5 Video Raindrops and Rain Streak Removal", "content": "The work done in [71] introduced a synthesized video dataset of 102 videos, VRDS, wherein the videos contain both raindrops and rain streaks degradations since both rain streaks and raindrops corrupt the videos captured in rainy weather. We split the dataset in train and held-out test sets as outlined in the original work [71]. We present TURTLE's scores in table 5, and compare it with several methods in the literature. TURTLE scores 32.01 dB in PSNR on the task, with an increase of +0.99 dB compared to the previous best method, ViMPNet [71]. We present visual results on the task in fig. 4, and fig. 10, and compare our method with ViMPNet [71]. TURTLE restores the frames that are pleasing to the human eye and are faithful to the ground truth."}, {"title": "4.6 Video Super-Resolution", "content": "MVSR4\u00d7 is a real-world paired video super-resolution dataset [67] collected by mobile phone's dual cameras. We train TURTLE following the dataset split in the official work [67] and test on the provided held-out test set. We report distortion metrics in table 7 and compare it with several methods in the literature. TURTLE scores 25.30 dB in PSNR on the task, with a significant increase of +1.36 dB compared to the previous best method, EAVSR+ [71]. We present visual results on the task in fig. 5. Other methods such as TTVSR [37], BasicVSR [7], or EAVSR [71] tend to introduce blur in up-scaled results, while TURTLE's restored results are sharper."}, {"title": "4.7 Blind Video Denoising", "content": "We assume no degradation prior, and consider blind video denoising task [49, 55]. We train our model on DAVIS [48] dataset, and test on DAVIS held-out testset, and a generalization set Set8 [61]. We add white Gaussian noise to the dataset with noise level $\\sigma \\in \\mathcal{U}[30, 50]$ to train TURTLE, and test on two noise levels $\\sigma = 30$, and $\\sigma = 50$; scores are reported in table 6. TURTLE observes a gain of +0.31 dB on $\\sigma = 30$, and +0.34 dB on $\\sigma = 50$ on Set8 testset, scoring 32.22 dB, and 30.29 dB, respectively, while it observes an average drop of -0.3 dB to BSVD-64 [49] on the DAVIS testset. Further, we present qualitative results in fig. 5 comparing TURTLE, and previous best method BSVD [49]."}, {"title": "4.8 Computational Cost Comparison", "content": "In table 8, we compare TURTLE with previous methods in the literature in terms of multiply-accumulate operations (MACs). The results are computed for the input size 256 \u00d7 256. We measure the performance on the number of frames the original works utilized to report their performance, as"}, {"title": "5 Ablation Study", "content": "We ablate TURTLE to understand what components necessitate efficiency and performance gains. All experiments are conducted on synthetic video deblurring task, GoPro dataset [41], using a smaller variant of our model. Our smaller models operate within a computational budget of approximately 5 MACs (G), while the remaining settings are the same as those of the main model. In all the cases, the combinations we adopt for TURTLE are highlighted . Additionally, we discuss the limitations of the proposed method in appendix B.\nBlock Configuration. We ablate the Causal History Model (CHM) to understand if learning from history benefits the restoration performance. We compare TURTLE with two settings: baseline (no CHM block) and TURTLE without State Align Block ($\\Phi$). In baseline (no CHM), no history states are considered, and two frames are concatenated and fed to the network directly. Further, in No $\\Phi$, the state align block is removed from CHM. We detail the results in table 9, and find that both State Align Block, and CHM are important to the observed performance gains.\nTruncation Factor $\\tau$. We evaluate context lengths of $\\tau = 1, 3$, and 5 past frames and found no PSNR improvement when increasing the context length beyond three frames. Results in table 10 confirm that extending beyond three frames does not benefit performance. This is because, as in most cases, the missing information in the current frame is typically covered within the three-frame span, and additional explicit frame information fails to provide additional relevant details.\nValue of k in topk. We investigate the effects of different k values in topk attention. Our experiments, detailed in table 11, show that k crucially affects restoration quality. Utilizing a larger number of patches, k = 20, leads to an accumulation of irrelevant information, negatively impacting performance by adding unnecessary noise. Further, selecting only 1 patch is also sub-optimal as the degraded nature of inputs can lead to inaccuracies in identifying the most similar patch, missing vital contextual information. The optimal balance was found empirically with k 5, which effectively minimizes noise while ensuring the inclusion of key information."}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel framework, TURTLE, for video restoration. TURTLE learns to restore any given frame by conditioning the restoration procedure on the frame history. Further, it compensates the history for motion with respect to the input and accentuates key information to benefit from temporal redundancies in the sequence. TURTLE enjoys training parallelism and maintains the entire frame history implicitly during inference. We evaluated the effectiveness of the proposed method and reported state-of-the-art results on seven video restoration tasks."}]}