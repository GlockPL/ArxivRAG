{"title": "PPO-Based Dynamic Control of Uncertain Floating Platforms in Zero-G Environment", "authors": ["Mahya Ramezani", "M. Amin Alandihallaj", "Andreas M. Hein"], "abstract": "In the field of space exploration, floating platforms play a crucial role in scientific investigations and technological advancements. However, controlling these platforms in zero-gravity environments presents unique challenges, including uncertainties and disturbances. This paper introduces an innovative approach that combines Proximal Policy Optimization (PPO) with Model Predictive Control (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of Luxembourg. This approach leverages PPO's reinforcement learning power and MPC's precision to navigate the complex control dynamics of floating platforms. Unlike traditional control methods, this PPO-MPC approach learns from MPC predictions, adapting to unmodeled dynamics and disturbances, resulting in a resilient control framework tailored to the zero-gravity environment. Simulations and experiments in the Zero-G Lab validate this approach, showcasing the adaptability of the PPO agent. This research opens new possibilities for controlling floating platforms in zero-gravity settings, promising advancements in space exploration.", "sections": [{"title": "I. INTRODUCTION", "content": "The pursuit of space exploration rests on a foundation of meticulous testing and validation, a cornerstone that not only enhances the reliability of space missions but also augments their operational efficiency. The complexities inherent in the frictionless environment necessitate ground-based testing to mirror the conditions and challenges faced by spacecraft and satellites in orbit. To address this need, cutting-edge ground test facilities have emerged as indispensable tools in the arsenal of space research and development.\n\nThe Georgia Institute of Technology's ASTROS facility stands as a hub for spacecraft Autonomous Rendezvous and Docking (ARD) maneuvers, wielding high-pressure air-bearing floating platforms over a 4m x 4m flat epoxy floor to simulate frictionless operations [1]. The European Space Agency's ORBIT facility, spanning 45 m2 epoxy floor, excels in orbital robotics, leveraging air-bearing platforms for position tracking and facilitating large payload tests [2]. ADAMUS, a 6-DoFs spacecraft simulator at the forefront of autonomy research, graces the scene with torque and force-free operation [3]. The Spacecraft Dynamics Simulator at Caltech reveals a multifaceted multi-Spacecraft testbed, featuring M-STAR platforms for 3 to 6-DoFs experiments [4]. AUDASS, a standout from the Satellite Servicing Laboratory, embodies independent floating platforms via air-bearings, an embodiment of proximity maneuvers [5]. NASA's contributions encompass the Air Bearing Floor at Johnson Space Center and the Formation Control testbed at JFP, highlighting suspended platforms and precision formation flight, respectively [6]. This global panorama of facilities collectively propels our understanding of space dynamics, steering the evolution of control strategies for the uncharted frontiers of space exploration.\n\nAmong these test facilities, the Zero-G Lab [7] at the University of Luxembourg, shown in Fig. 1, stands as a pioneering exemplar. Within this controlled environment, researchers and engineers are afforded the opportunity to scrutinize the performance of space technologies and systems in space conditions. Central to this endeavor is the utilization of a sophisticated mechatronic system, the floating platform, engineered to simulate the complexities of space operations.\n\nThe floating platform serves as a conduit to assess diverse scenarios of space missions, encompassing rendezvous and docking maneuvers, relative motion of satellites, and intricate orbital scenarios. By scrutinizing these scenarios, the Zero-G Lab contributes not only to our understanding of space dynamics but also to the refinement of control strategies vital for the success of space missions."}, {"title": "II. METHODOLOGY", "content": "RL is a crucial branch of machine learning dedicated to optimizing policies that link observations to actions, which aims to maximize rewards accumulated through trajectories in an environment, as agents adjust actions based on rewards [26]. This involves a Markov decision process defined by state space, action space, state transitions, and rewards.\n\nBased on [27], trajectories, \u03c4, are core units in RL, representing sequences of state-action pairs during episodes. Mathematically, \u03c4 = [xo, Up, ..., Xr, Ur] \u2208 T, with x as state, u as action, and T as steps. The main RL goal is to optimize the expectation of cumulative rewards across trajectories as\n\n\u0395\u03c1\u03b1(\u03c4) [r(t)] = \\int_{\\Tau} r(t)p_{\\alpha}(\\tau)d\\tau  (1)\n\nwhere r(t) = \u03a3_{t=0}^{T} \u03b3^tr(xt, ut) is the summation of discounted rewards using \u03b3 \u2208 (0,1), \u03c1\u03b1(\u03c4) = \u03a0_{t=0}^{T} p(xt+1 | Xt, ut ) \u00b7 p(xo) is the probability of a trajectory under a, and ut is a sample from \u03c0\u03b1 (ut | xt). The policy's conditional distribution introduces stochasticity in action choice, aiding exploration. As learning progresses, variance diminishes, favoring policy exploitation. Post-learning, policy variance becomes zero, ensuring deterministic action selection. This transition to determinism governs practical implementation.\n\nThe policy (actor) and the advantage function (critic) evolve simultaneously in PPO. PPO utilizes the state-value function V^\u03c0(xt) = \u0395\u03c0(\u03a3_{k=t}^{T} \u03b3^{k-t}r(xk, uk)|xt) to estimate discounted rewards across trajectories. The parameter vector w and the policy parameter vector \u03b1 are learned during the learning process. The resultant advantage function Aw (xt, ut) quantifies the difference between empirical and estimated rewards.\n\nAw(xt, ut) = (\u03a3_{k=t}^{T} \u03b3^{k-t}r(xkuk) - V(xt)  (2)\n\nPPO, a successor of the trust region policy optimization algorithm, retains the ability to mitigate substantial policy updates, reducing the risk of learning divergence. This while maintaining a simpler and more widely implementable approach. At the core of PPO lies the policy probability ratio Pt(\u03b1) = \\frac{\u03c0_\u03b1(Utxt)}{\u03c0_\u03b1(UtXt)}, which gauges the probability of selecting an action after a learning update, \u03c0_\u03b1(ut|xt), compared to before the update, \u03c0_\u03b1 (ut|xt). This ratio directly informs the PPO loss function as follows.\n\nL(\u03b1) = \u0395p(x) [min (Pt(\u03b1) Aw(xt, ut), \\text{clip}[p_t(\\alpha), \\epsilon] A_w(x_t, u_t))]  (3)\n\nHere, the clip function, given by\n\n\\text{clip}[p_t(\\alpha), \\epsilon] = \\begin{cases}\n1-\\epsilon & p_t(\\alpha) <1- \\epsilon \\\\\n1+\\epsilon & p_t(\\alpha) < 1 + \\epsilon \\\\\np_t(\\alpha) & \\text{otherwise}\n\\end{cases}  (4)\n\nimposes bounds on the policy probability ratio using a clipping parameter \u03f5 within (0,1). This constrains policy updates, facilitating a trust region to eliminate unwarranted changes. Notably, the loss function is relative to the policy pre-update, making its absolute value across multiple updates less informative. Instead, its immediate gradient plays a pivotal role in steering the policy to optimize rewards over all trajectories. To learn the state-value function, a commonly utilized mean squared error cost function is minimized\n\nL(w) = \\frac{1}{2T} \\sum_{t=0}^{T} \\left( \\sum_{k=t}^{T} \\gamma^{k-t}r(x_t, u_t) - V_w(x_t) \\right)^2 (5)\n\nwith an objective function for policy enhancement and a cost function for state-value correction, gradients of these"}, {"title": "", "content": "functions facilitate gradient ascent on \u03b1 and gradient descent on w\n\n\u03b1+ = \u03b1_ + \u03b2\u03b1\u2207\u03b1L(\u03b1)|a=a_   (6)\nw+ = w_ - \u03b2w\u2207wL(w)|w=w_\n\nHere, \u03b2\u03b1 and \u03b2w are learning rates for policy and state-value function respectively, set by the designer.\n\nTo develop policies for 3-DOF maneuvers within the lab, we adopt the lab-centered inertial frame I. The state vector s = [r,\u03bd, \u03b8, \u03c9] represents the floating platform's center of mass within the I frame, with r\u2208 R\u00b2 as position, v \u2208 R\u00b2 as velocity, \u03b8\u2208 R\u00b9 as attitude angle, and \u03c9 \u2208 R\u00b9 as angular velocity. The control action u = [F, M] includes thrust command F\u2208 R\u00b2 and torque command M\u2208 R\u00b9, both in the floating platform body frame, B, subject to actuator constraints. Dynamics are derived in continuous-time and discretized with a 0.1-second sample period. Translational dynamics are modeled as double integrators:\n\nr = v\nv = \\frac{C (\u03b8) FB}{m} (7)\n\nwhere m is the floating platform mass and C\u03b2(0) \u2208 R2\u00d72 represents the rotation matrix that maps from the B to the I frame.\n\nAttitude dynamics follow quaternion kinematics and Euler's equations for a rigid body:\n\n\u03b1 = \u03c9\nJ\\dot{\\omega} = L (8)\n\nin which J is the moment of inertia of the floating platform around the rotation axis.\n\nPolicy and state-value functions are modeled with feedforward neural networks, parameterized by \u03b1 and w, which are updated based on (6) using Adam optimizer [28]. The policy is a multivariate Gaussian distribution with a diagonal covariance matrix. Neural network outputs are scaled using running mean and standard deviation of experienced state data during learning. Policy network outputs are scaled so that \u00b11 corresponds to maximum/minimum thrust or torque.\n\nThe actor network is structured as a feed-forward neural network consisting of two hidden layers, each comprising [128, 64] neurons. Meanwhile, the critic network exhibits a more intricate architecture with three layers housing [128, 64, 8] neurons. Both networks utilize the tanh activation function. The output layer of the actor network comprises 3 neurons with linear activation, while the critic function incorporates one neuron with linear activation.\n\nIn the PPO implementation, a strategy inspired by Gaudet et al. [29] is employed, whereby learning parameters are dynamically adapted to achieve a desired target Kullback-Leibler (KL) divergence value between successive policy updates [30]. This approach is utilized to prevent significant policy updates that could potentially disrupt the learning process, ensuring that policy updates proceed gradually and with stability. Throughout the learning process, both \u03f5 and \u03b2\u03b1 are continuously adjusted to ensure that the KL-divergence between updates remains as close as possible to the specified target value KLd.\n\nEnsuring a well-defined reward function is paramount to the efficacy of PPO, as the policy's learning process centers on maximizing this function. In the context of 3-DOF stabilization maneuvers, the reward function encompasses multiple components, collectively addressing the minimization of state tracking discrepancies, control input exertion, and the reinforcement of successful stabilization outcomes. These components are deliberately assigned relative weights through design coefficients.\n\nThe primary term serves as a crucial component in aiding PPO's learning process from MPC. It quantifies the quadratic-weighted difference between the state derivatives produced by the RL agent and a reference obtained from MPC. This inclusion accelerates the learning process by providing a clear reward signal across the entire state-space, guiding the RL agent toward the attainment of effective stabilizing trajectories.\n\nMPC involves minimizing a cost function that measures the difference between the floating platform's current and desired final stabilization states, along with control inputs, while considering dynamics and constraints. MPC iteratively solves an optimization problem, adapting control inputs in real-time to address uncertainties and disturbances like fuel sloshing.\n\nThe optimization problem of MPC is expressed as follows:\n\n\\text{Minimize}_{u(t)} \\int_{t=t_o}^{t_f} [\\|s'(t) \u2013 s_d(t)\\|_{\u03a9}^2 + \\|u(t)\\|_{P}^2]dt  (9)\n\nSubject to:\n\ns'(t) = As'(t) + Bu(t)\ns'(t) \u2208\u03a3\nu(t) \u2208 U\ns'(to) = St\n\nwhere tf represents the final stabilization time, to represents the current time instant, s'(t) represents the state vector of the linearized system, sd represents the desired states, \\|.\\| denotes the weighted norm of a quantity defined by (.), with \u03a9 being a positive definite matrix, u(t) represents the control input, A and B are system matrices representing the linearized dynamics of the floating platform, can be found in [9], \u03a3 is the set of feasible states representing constraints, U is the set of feasible control inputs representing constraints.\n\nThe MPC approach with a prediction horizon of 10s and a time step of 0.1s is utilized to generate the reference trajectory. Furthermore, \u03a9 is represented as a diagonal matrix with elements set to 1 for position and angle-related diagonal elements, and 100 for time derivative-related elements. Additionally, p corresponds to a diagonal matrix with all diagonal elements equal to 1000."}, {"title": "III. EXPERIMENTAL SETUP", "content": "The experimental environment is the Zero-G Lab, which features a spacious experimental room measuring 5m x 3m x 2.3m, equipped with two floating platforms that move frictionlessly over a meticulously installed epoxy floor. To maintain a near-frictionless environment, the floating platform is equipped with air-bearings that direct high-pressurized air towards the epoxy floor, eliminating mechanical contact [32]. The actuation of eight nozzles drives the floating platform along two translational axes, X and Y, as well as one rotational axis, Z (\u03b8). These nozzles can generate forces up to 1N under pressures of 10 bar. Yal\u00e7in, et al. [9] provide comprehensive information regarding the order and locations of nozzles around the floating platform. Tracking the position of the floating platform is accomplished using six OptiTrack Prime 13W cameras located within the Zero-G Lab, operating at 240 Hz. An active marker positioned at the center of the top plate facilitates this tracking process [33]. The floating platform seamlessly integrates into the ROS network, and a ROS-MATLAB bridge facilitates platform programming using MATLAB, enabling experimentation and assessment of its capabilities."}, {"title": "IV. TRAINING", "content": "The algorithm proposed in this study operates within the MATLAB environment, with a maximum iteration limit set at 20,000 to ensure network convergence. The agent collects data in batches of 200 episodes before performing policy and state-value function learning updates based on (6).\n\nDuring both training and testing episodes, the policy generates force/torque commands at discrete 0.1s intervals. Training episodes have a time limit of 60s to efficiently gather data while ensuring stabilization. However, for testing, the time limit is extended to 100s to allow valid stabilization trajectories to complete. Furthermore, the acceptance stabilization condition requires an accuracy of 0.05m in distance, a velocity within the range of \u00b10.1 m/s, a rotation of up to \u00b15 degrees, and an angular velocity within the range of \u00b11 degree per second.\n\nOne objective of this research is to develop a robust feedback control law capable of handling significant uncertainty in the initial conditions of the stabilization maneuver. The RL goal is to create a stabilization policy effective across a wide range of initial conditions, encompassing the required robustness against uncertainty."}, {"title": "V. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "After training both the PPO-MPC and PPO-only methods with 20,000 episodes, their performance is assessed in real-world experiments involving the Floating Platform in the Zero-G Lab. During these experiments, the floating platform is manually disturbed four times at different intervals, and the objective is for it to autonomously return to the stabilization condition at the center of the lab. The times at which disturbances are introduced are highlighted with red arrows in the figures."}, {"title": "A. Performance of PPO-MPC", "content": "Fig. 4 illustrates the performance of the PPO-MPC approach. Each time the platform is disturbed, it effectively returns to the stabilization condition, and the state errors remain within the predefined range (0.05m in distance and a rotation error of up to 5 degrees). Notably, the second disturbance exhibits a more significant rotation angle deviation, while the other disturbances primarily affect the platform's position, with less impact on orientation. The actuation of the thrusters, as demonstrated in Fig. 5, showcases the successful generation of pulse signals for each individual nozzle using the PWPF method."}, {"title": "B. Performance of PPO-Only", "content": "In contrast, Fig. 6 displays the performance of the PPO-only method. While the platform does return to its origin after disturbances, both the stabilization error and the return time exceed the predefined thresholds. The stabilization position error measures around 0.15m, and the rotation error is approximately 10 degrees. This outcome was anticipated, as the PPO-only method achieved lower rewards during the training phase compared to the PPO-MPC approach. Consequently, the PPO-MPC integration demonstrates superior performance, aligning with expectations. The nozzle actuation for the PPO-only case is depicted in Fig. 7.\n\nThe experimental results underscore the effectiveness of the integrated PPO-MPC method in achieving precise and rapid stabilization of the floating platform under disturbance, outperforming the PPO-only approach in this space environment."}, {"title": "VI. CONCLUSION", "content": "In this study, we have explored the use of Proximal Policy Optimization (PPO) combined with Model Predictive Control (MPC) for the control of a floating platform within the unique environment of the Zero-G Lab. Through extensive training and real-world experiments, we have gained valuable insights and drawn important lessons regarding the control of such platforms in a frictionless, zero-gravity setting.\n\nOur research has yielded several key lessons that have significant implications for the control of floating platforms in space environments:"}, {"title": "Adaptability Through Integration", "content": "The integration of PPO with MPC has proven to be a powerful strategy. This combined approach leverages the predictive capabilities of MPC to enhance the adaptability of PPO. The result is a control framework that quickly responds to disturbances and converges to optimal solutions. This adaptability is crucial for effectively dealing with uncertainties and unmodeled dynamics inherent in space settings."}, {"title": "Robustness is Paramount", "content": "The experiments conducted in the Zero-G Lab underscore the importance of robust control strategies. The PPO-MPC approach consistently outperformed the PPO-only method in terms of robustness and precision. It was able to counteract disturbances effectively, returning the platform to its desired state with minimal errors. This robustness is a critical factor for ensuring the success of missions in space exploration."}, {"title": "Speed of Learning Matters", "content": "The PPO-MPC approach exhibited a faster convergence rate during training compared to the PPO-only method. This speed of learning is essential, especially in dynamic and uncertain environments. It allows the control system to adapt quickly to changing conditions, which is crucial for maintaining stability and achieving mission objectives."}, {"title": "Implications for Space Exploration", "content": "The lessons learned from this study have significant implications for space exploration. Precise control of floating platforms is essential for various scientific investigations and technological advancements in space environments. The adaptability and robustness demonstrated by the PPO-MPC approach make it a promising candidate for addressing the control challenges encountered in space missions.\n\nIn summary, the integration of PPO with MPC has unveiled new horizons in the realm of controlling floating platforms in zero-gravity environments. The knowledge acquired from this study paves the way for more effective and reliable control strategies in the context of space exploration. In this domain, precision and adaptability are not mere aspirations but prerequisites for unraveling the mysteries of the cosmos and advancing our understanding of the universe."}]}