{"title": "Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek-R1 Expert Specialization", "authors": ["Matthew Lyle Olson", "Neale Ratzlaff", "Musashi Hinck", "Man Luo", "Sungduk Yu", "Chendi Xue", "Vasudev Lal"], "abstract": "DeepSeek-R1, the largest open-source Mixture- of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven. Given DeepSeek-R1's enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1's structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1's routing mechanism is more semantically aware and it engages in structured cognitive processes.", "sections": [{"title": "1 Introduction", "content": "Since their popularization in Fedus et al. (2022), the Mixture-of-Experts (MoE) architecture (Jacobs et al., 1991) has been integrated into many state- of-the-art large language models (LLMs) (Lieber et al., 2024; Jiang et al., 2024; Liu et al., 2024). Recently, DeepSeek-R1 (Guo et al., 2025) (hereafter abbreviated R1), based on the MoE architecture, was released as the largest open-source (MIT License) LLM by total parameter count. Notably, it is the first open-source model to achieve performance comparable to OpenAI's o-series of models (Zhong et al., 2024). Trained with reinforcement learning (RL) (Shao et al., 2024), R1 demonstrates remarkable reasoning (Wei et al., 2022), including emergent behaviors such as the \u201caha moment\" (Guo et al., 2025).\nThe release of R1's model weights opens up several exciting research opportunities, one of which is the study of expert routing behavior in very large scale MoEs. Several prior studies have explored expert activation patterns in MoE models, hypothesizing that each expert may specialize in specific domains, tasks, or topics (Zoph et al., 2022; Jiang et al., 2024; Xue et al., 2024). While it is intuitive to expect some degree of semantic specialization, previous research has struggled to establish a clear semantic role for individual experts, concluding instead that expert activation is primarily token-dependent rather than being driven by deeper semantic relationships.\nGiven the scale and strong reasoning capabilities of R1, we investigate whether its expert routing exhibits greater semantic specialization than previous MoE models. We design two experiments to analyze R1's routing mechanisms. First, we employ a word sense disambiguation (WSD) task (Pilehvar and Camacho-Collados, 2018), where a target word appears in two sentences, either with the same semantic value (sense) or differing senses. Our results show significantly higher expert overlap where the word sense is the same than where it differs. In contrast, the rate of expert overlap differs less between the two cases in previous MoE models like Mixtral (Jiang et al., 2024). Additionally, we analyze R1's reasoning structure to further understand its cognitive behavior using the agentic DiscoveryWorld environment (Jansen et al., 2024) (Apache 2.0) as a testbed. We qualitatively probe the model's chain of thought with a Sparse Autoencoder (SAE); a modern tool for interpreting LLMs (Cunningham et al., 2023). Our findings reveal that R1 follows a structured reasoning approach, incorporating self-evaluation, and hypothesis testing (see Figure (1) for an example). Finally, R1's thought patterns favor specific experts, indicating that the expert specialization extends to cognitive processes."}, {"title": "2 Related Work", "content": "Current research on expert specialization in MoE models is sparse, yet available studies reveal little evidence of semantic-level differentiation. For example, Xue et al. (2024) tracked token routing patterns across datasets segmented by different topics, languages, and tasks, but failed to find any coherent pattern at such high-level semantics. Rather, they found indications of token-level specialization, mainly concerning low-level semantic features like special characters or auxiliary verbs. Similar findings have been reported in studies using independently developed MoE models (e.g., Zoph et al., 2022; Jiang et al., 2024; Fan et al., 2024).\nWhile some neuroscience research has provided evidence that the brain functions like a Mixture of Experts (Stocco et al., 2010; O'Doherty et al., 2021)\u2014suggesting the possibility of semantic-level specialization\u2014other studies have shown that MoE models with random routing can perform comparably to those using the more common top-k routing approach (Roller et al., 2021; Zuo et al., 2021; Ren et al., 2023). One potential explanation for these mixed results is that prior models (using 8 to 32 experts) might not have been sufficiently expressive to capture fine-grained specialization patterns. The recently-released DeepSeek v3, featuring an extensive network of experts (256 routed specialists alongside one shared generalist expert), provides us with a unique opportunity. Hence, in this study, we test whether a more capable MoE architecture exhibits semantic-level expert specialization."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Words-in-Context", "content": "We leverage polysemy to test for semantic specialization in expert activation. If words that are written the same but have different meanings are routed differently, then this is evidence that routing occurs based on meaning. To test this hypothesis, we use the WiC dataset (Pilehvar and Camacho-Collados, 2018) (CC BY-NC 4.0), which consists of two types of paired sentences: 1) pairs where a target word has the same sense and 2) pairs where the target word has different senses across sentences. For each target words and sentence, we prompt the model with: \"Please define {target word} in this context.\" Additionally, we include an internal reasoning step: \u201c Okay, so I need to figure out the meaning of the word {target word}.\" to ensure that the subsequent inference isolates the word in question instead of additional thinking tokens."}, {"title": "3.2 Discovery World", "content": "DiscoveryWorld (Jansen et al., 2024) is a large- scale agentic environment suite that tests the abilities of an agent to perform the scientific method. Each environment has a terminal goal, for example, we study \"Reactor Lab\" where the agent must tune the frequency of quantum crystals to activate a reactor. To succeed, the agent must formulate and test hypotheses by using available tools, literature, and its own memory. DiscoveryWorld is notably difficult for frontier models like GPT4o, and even take human experts hundreds of in-game steps to complete a task (Jansen et al., 2024). Hence, Discovery World offers a testbed to examine the long- horizon reasoning capabilities of R1. Building on the Words-in-Context experiment, we want to know if a similar phenomena of expert specialization can be found for the reasoning patterns that we observe within DeepSeek-R1's chain of thought."}, {"title": "Sparse Autoencoders", "content": "To get a clearer picture of how these patterns are invoked internally, we employ SAEs to learn a mapping between the internal activations of R1 and a set of underlying semantic structures exhibited by the model. Briefly, an SAE learns a compressed representation of input vectors $x \\in \\mathbb{R}^d$. The encoder maps inputs to a higher-dimensional latent space, while the decoder reconstructs the input from the latent representation. Given an encoding dimension n, we define the encoder and decoder as: $z = \\text{max}(0, W_{enc}x + b_{enc})$ and $\\hat{x} = W_{dec}z$ where $W_{enc} \\in \\mathbb{R}^{n \\times d}$ and $W_{dec} \\in \\mathbb{R}^{d \\times n}$ are the learnable weight matrices of the encoder and decoder respectively, and $b_{enc} \\in \\mathbb{R}^n$ is a bias term. The model is trained using a loss function that balances reconstruction accuracy and sparsity: $L = ||x - \\hat{x}||_2^2 + \\lambda ||z||_1$\nwhere the first term is the mean squared error for reconstruction, and the second term is an L\u2081 penalty that encourages sparsity in the latent activations, where we choose $\\lambda = 5$ as the trade-off between reconstruction fidelity and sparsity."}, {"title": "4 Results", "content": ""}, {"title": "Word-in-Context", "content": "For 1K pairs of sentences in WiC, we collect router activations for DeepSeek- R1, Mixtral-8x7B and Mixtral-8x22B and record the number of overlapping experts at each layer.\nWe compare the average rate of overlap in sentence pairs where the target word has the same sense versus sentence pairs where it has a different meaning. If sentence pairs where the target word has different senses have higher expert overlap than sentence pairs where the target word has the same sense, then this is evidence that expert routing differentiates on a semantic basis.\nWe compute selected expert overlap by normalizing using the following formula:\n$\\text{score} = \\frac{\\text{observed} - \\text{expected}}{\\text{maximum} - \\text{expected}}$\nwhere observed is the number of overlapping experts, maximum is the maximum number of overlapping experts (2 for Mixtral, 8 for DeepSeek) and expected is the expected number of overlapping experts from a random baseline. This ensures a more fair comparison with models of differing number of total and selected experts. See \u00a7A.2 for a derivation of the random baseline.\nWe find strong evidence for semantic specialization in these experiments; expert overlap is lower for sentence pairs where the target word has different senses than when they are the same. This effect is statistically significant (p < 0.001) for all models considered when averaged across all layers. See appendix \u00a7A.1 for further details. For all models the difference in overlap increases in intermediary layers. This supports prior findings that semantic features are more salient in the intermediary layers of LLMs (Niu et al., 2022; Kaplan et al., 2024). Our results are also suggestive that this pattern emerges at scale; the difference in expert overlap increases with model size."}, {"title": "Discovery World", "content": "At a high level, we find that R1's reasoning traces on DiscoveryWorld display many indicators reminiscent of System 2 thinking, such as backtracking, self-evaluation, and situational awareness, see Appendix \u00a7B for an example.\nGiven R1's thinking text on DiscoveryWorld, we want to investigate if its experts are specialized on reasoning strategies. Given any reasoning trace, we can find groups of tokens that correspond to a specific reasoning strategy and observe which experts are subsequently activated. If similar experts are used to process all the tokens for a given reasoning strategy, then we have evidence that the experts also specialize by cognitive pattern. To this"}, {"title": "5 Conclusion", "content": "With access to DeepSeek-R1's model weights and motivated by its significant improvements in reasoning and cognitive behavior, we analyzed its expert selection mechanism. Our study focused on two key settings: semantic context and reasoning strategy analysis. In the first experiment, we found that DeepSeek-R1 exhibits stronger semantic specialization than previous MoE models, with expert selection aligning more closely with semantic meaning. In the second experiment, we discovered cognitive specialization, where different experts are responsible for distinct reasoning processes. Over-"}, {"title": "Limitations", "content": "Since DeepSeek-R1 has significantly more experts than Mistral (256 vs. 8), the probability of selecting overlapping experts is inherently lower from a statistical standpoint. To account for this, we normalized the overlap values based on the expected distribution. Additionally, the larger number of experts in DeepSeek-R1 could itself be a contributing factor to the emergence of semantic specialization. This raises the question of whether the observed semantic alignment is an intrinsic property of DeepSeek-R1's specific training setup e.g. device-specific expert allocation (Liu et al., 2024), fine-tuning via GRPO (Shao et al., 2024), or a natural consequence of a more fine-grained expert distribution. Future studies should explore this relationship further to disentangle the effects of model architecture and training setup from routing behavior."}, {"title": "Ethics Statement", "content": "For each artifact used e.g. R1 model weights, WiC dataset, we follow the intended use, and while we do not believe that our analysis of DeepSeek-R1 poses any risks or ethical considerations, we acknowledge the inherent issues with LLMs that are trained on web-scale or biased data. Outputs from LLMs may raise safety concerns due to hallucinations or bias in the training data."}]}