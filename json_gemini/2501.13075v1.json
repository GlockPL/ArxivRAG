{"title": "Evolution and The Knightian Blindspot of Machine Learning", "authors": ["JOEL LEHMAN", "ELLIOT MEYERSON", "TAREK EL-GAALY", "KENNETH O. STANLEY", "TARIN ZIYAEE"], "abstract": "This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to the concept of Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever- changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them, highlighting the promise of artificial life, open-endedness, and revisiting RL's core formalisms in service of managing Knightian uncertainty. The conclusion is that the intriguing remaining fragility of ML may result from blind spots induced by its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.", "sections": [{"title": "1 Introduction", "content": "\u201cSeeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. [...] We have to learn the bitter lesson that building in how we think we think does not work in the long run.\u201d [187]\nRichard Sutton\nThe above quote reflects the ascendancy in machine learning (ML) of methods that leverage increased com- putation through search and learning, a potentially \u201cbitter lesson\u201d for those tailoring ML algorithms through human domain knowledge [187]. For example, many sophisticated feature-construction methods for machine vision were made obsolete by deep learning on raw pixels [102, 215], just as natural language processing was upended by the transformer revolution [24, 199]. Yet unanswered by this bitter lesson is exactly how little we should design by hand into ML systems. In other words, do there remain blind spots that exist within ML as a result of human thinking that could be overcome if yet more was given over to search [34]? If so, perhaps such blind spots could be remedied by taking the insights from the bitter lesson further (e.g. automating the search for architectures or learning algorithms), or illuminated by studying algorithms that already do so. This latter thread is pursued here.\nIn particular, this paper argues that there are subtle lessons still to digest from the algorithm that takes the bitter lesson to its logical conclusion: biological evolution, which imposed no prior domain knowledge, and instead effectively leveraged extreme amounts of computation to invent human intelligence on one leaf of its massively divergent search. The main idea is to highlight, through contrast with evolution, an intriguing blind spot in the formalisms of ML: the importance of robustness to an open-ended future. We hope to go beyond critique alone through suggesting mechanisms from nature, human intelligence, and threads of current ML research that may unlock future progress.\nHuman engineering has surpassed biological evolution in many ways (e.g. the cargo capacity or speed of a jumbo jet, compared to birds). However our ingenuity has yet to match evolution in others, such as in engineering self-replicating machines, or, of central interest here: in matching the robustness of evolution's products. For example, in the late 1980s, zebra mussels made their way from Europe to the great lakes in the US, hitchhiking in the ballast tanks of ships [163]. They rapidly spread through the waterways, outcompeting native species and clogging water intake pipes. Such invasive species are not uncommon: Animals evolved for one environment, when placed into a significantly different one, can be successful, sometimes frustratingly so (to us) [140]. From an ML or robotics perspective, this level of out-of-distribution robustness is an amazing accomplishment; it is hard to imagine, for example, successfully zero-shot transferring a self-driving policy trained only on data from the US to the UK \u2013 which us humans (another of evolution's products) regularly accomplish, albeit with visceral discomfort.\nLike evolution, ML aspires towards agents that robustly function in open-world environments, an important challenge spanning many critical applications, including social networks, chatbot assistants, home-assistance robots, self-driving cars, and robots acting in unstructured environments more broadly; indeed, competence within such open worlds is fundamental to ML's quest for artificial general intelligence (AGI) [83, 151]. Yet despite their unprecedented pace of improvement in capabilities, ML systems still struggle in environments much different from those they were trained in: self-driving cars falter with rare situations that are mundane to humans (like understanding that traffic lights in the back of a trailer are not worrisome; or that a moving plastic bag is not a problem) [72, 73, 125, 159, 173]; generative Al agents remain surprisingly fragile [28, 81, 87, 182, 203, 208, 216]; and chain-of-thought reasoning in 100 billion+ parameter language models frequently goes off the rails [43]. Others have remarked that this is a peculiar situation [196, 205, 206]: An agent that is remarkably knowledgeable and capable of e.g. fluid philosophical discussion, can yet make rudimentary mistakes of generalization [206] or reliability [46, 80]. One explanation to resolve this paradox is that facets of intelligence that are coupled in humans may be decoupled in machines: one or more such facets may still be missing from ML.\nThis paper thus aims to draw attention to an important property of intelligence largely ignored by ML: Robustness to a qualitatively unknown future in an open world, which we relate to the idea of Knightian uncertainty (KU; [44, 92, 94, 100, 185]), or roughly, unknown unknowns. What emerges next in the real world (i.e. the next fashion, scientific idea, power outage, edge-case driving scenario, natural disaster, stock market boom or crash, etc.) is the vastly-complex product of billions of creative agents acting in an interconnected, complicated physical and digital world; there is much we reasonably cannot anticipate. Yet, the preponderance of work in robust ML aims to address known unknowns, i.e. encouraging robustness to limited classes of experimenter-anticipated risks. For example, to apply techniques from safe reinforcement learning [56] an experimenter generally must anticipate the ways an environment can vary, and specify the type of risk within such environments they wish the policy to optimize. Such work is obviously useful and important. However, one pillar of general intelligence is robustness to the novel situations and risks that continually arise in an open world can we truly claim to be approaching \u201cAGI\u201d without such a capacity, which is central to the survival of animal species as well as human civilization?\nWe define an agent's robustness to KU as its capacity to succeed or at least fail gracefully in the face of novel unforeseeable situations. For example, the first zebra mussels that arrived from Europe in the great lakes faced situations far outside their window of direct evolutionary exposure, and yet thrived. In particular, this paper investigates assumptions in ML made for conceptual and mathematical convenience that limit robustness to plausible, if unlikely, future environmental disturbances. These assumptions are then contrasted with mechanisms from biological evolution that enable broader robustness. We focus centrally on reinforcement learning (RL), where the typical objective is to find a policy for an agent that maximizes expected reward, although many facets of this analysis apply also to the settings of unsupervised and supervised learning.\nA central conclusion is that RL's engagement with the problem of KU is obscured by two main factors. First, thought in RL is often shaped by the field's formalisms, which tend to axiomatically rule out KU as a possibility; it is human to take for granted the water that immerses us [2, 103]. For example, Markov decision processes (MDPs) and typical RL optimization objectives make simplifying closed-world assumptions: that the deployment environment is the same as in training and will remain so, that boundaries between episodes are absolute, that agents are by definition indifferent to impacts of their actions over long time-horizons, and so on. In short, RL (and ML more broadly) most often seeks optimal solutions to fixed problems. We later address (in Section 2.2) extensions of RL (such as meta-learning) that in theory, but (we argue) not in practice, meet this critique. Secondly, RL research is often driven by implicit assumptions about how existing agendas will naturally lead to superhuman robustness. For example, that the combination of increasing scale and the generalization capabilities of large neural networks will address all relevant robustness issues; or that it will do so when combined with increasingly sophisticated hand-designed RL algorithms derived to fit closed-world formalisms. Yet it may be optimistic that robustness to unknown unknowns will arise as byproducts of formalisms and methods that implicitly disavow their existence.\nIn contrast, biological evolution allows all aspects of an agent's architecture to adapt; nothing is sacrosanct. There are no explicit formalisms underpinning the many divergent implementations of biological reinforcement learning, nor any fixed time horizon beyond which feedback cannot shape evolution's trajectory. Evolution continually seeks new and diverse ways of surviving and reproducing, which implicitly represent bets on the nature of the future (e.g. perhaps dinosaur-sized reptiles are robust; or small mammals); this diversification can continually refresh the many bets culled by reality (with no fixed time horizon), such that time favors bets (lineages) more robust to the continually unfolding unknown (which itself is partially created by the diversification of life, in how one species contributes to the environment of others). \nOne possibility is that, similar to the argument for AI-generating algorithms (AI-GAs; [34]), there remains further bitter lessons [187] that ML has yet to absorb. Dealing with KU is central to intelligence and remains a fundamental challenge for ML, and to achieve it, computation may trump clever combinations of individual algorithmic advances. That is, it may be more effective to scale simpler algorithms that make fewer assumptions, taking inspiration from the genesis of robust human learning, i.e. biological evolution. That \u201cmore [computation] is different\u201d [7] extends beyond deep learning alone [167, 184], and less attention has been placed upon fields such as artificial life [3, 108] that while highly ambitious, have revolutionary potential.\nOr conversely, perhaps there are ways to tackle KU through novel RL algorithms that deeply integrate insights from evolution or how humans and society successfully navigate KU [89, 92, 190]. In total, this paper adds to a thread of counter-intuitive critique, that despite dramatic recent progress, ML and RL may yet still be skirting a core feature of intelligence [29, 83, 137, 168]. Importantly, while this paper presents a critique of ML, we are not skeptics of algorithmic intelligence nor dogmatically committed to mimicking biological evolution or the"}, {"title": "2 Background", "content": "The next sections review (1) the idea of Knightian uncertainty, a central concept in this paper, and (2) the existing ways in which the field of ML wrangles with the challenges of an open world."}, {"title": "2.1 Knightian Uncertainty", "content": "\"Before the wheel was invented [...] no one could talk about the probability of the invention of the wheel, and afterwards there was no uncertainty to discuss [...]. To identify a probability of inventing the wheel is to invent the wheel.\"\nJohn Kay and Mervyn King [92]\n\"What gets us into trouble is not what we don't know. It's what we know for sure that just ain't so.\"\nMark Twain\nThe attempt to manage risk through formalizing it is common practice in many fields such as finance [33], economics [139], and machine learning [141, 186]. Yet when formalizing risk and optimizing against it, we can delude ourselves into a false sense of security by thinking we have the world pinned down [92]. Much escapes formalization, however, and in optimizing against what we formalize, we may indeed decrease the narrow form of quantified risk, while potentially exacerbating true risk.\nIn other words, Goodhart's law applies: Optimization pressure decouples a simplified metric from the broader quality it is intended to capture [61, 181]. A prominent example is the subprime mortgage crisis of 2007, which at least partially resulted from incorrectly assuming away long-tail risks (in particular, assuming that defaults on high-risk loans could not become highly-correlated), and optimizing against that faulty model with abandon, as if true risk were contained [92, 131].\nIt is likely impossible to anticipate all important aspects of the future given the complexity of the world, which grows daily through the decentralized actions of billions of creative agents, who create new products, organizations, media, software, technology, ideas, etc. We can model known unknowns, i.e. the things we know we do not know - but by definition cannot predict unknown unknowns, i.e. what we do not know that we do not know (see Table 1). Formalizing the idea of unknown unknowns is near-paradoxical, and the closest concept is that of Knightian uncertainty in economics, which was independently posed in 1921 by John Maynard Keyes and Frank Knight [44, 100]. Knight makes a distinction between what he calls uncertainty and risk: \"Uncertainty must be taken in a sense radically distinct from the familiar notion of Risk ... a measurable uncertainty, or 'risk' proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all [100]. [emphasis added]\"\nNote that Knight's usage of \u201cuncertainty\u201d differs from common usage in ML. Rather than uncertainty in Bayesian statistics [20], Knightian uncertainty intuitively relates to unknown unknowns, where some important outcomes cannot be probabilistically quantified or anticipated. Such KU contrasts with what Knight calls \u201crisk,\u201d wherein uncertainty can be modeled (i.e. known unknowns). ML's treatment of uncertainty rarely contends with unknown unknowns; Bayesian methods require, for instance, defining the space of possible hypotheses to calculate uncertainty across [58, 89] and are subject to restrictive closed-world formalizations in the same way as ML more broadly (discussed later in Section 4). A recent work has also highlighted KU as an important property for AI [168] in the context of games; and other recent critiques of ML and RL can be interpreted from the lens of KU [2, 29].\nBecause KU makes a negative claim (i.e. it deals with situations where uncertainty cannot be modeled), it is understandably a controversial concept [36, 44]. While the idea of unknown unknowns is intuitive, as each of us has been buffeted by events we reasonably failed to anticipate, it cuts against many fields' tendencies to create formalisms that by fiat rule them out as possibilities, e.g. neoclassical economics, game theory, and subfields of machine learning. For example, the formalization of supervised learning assumes a test distribution identical to that in training [26, 64]. In such a frozen world, there is no room for unknown unknowns; yet assuming away their existence need not make them less real or important.\nAs a representative example of a KU skeptic, the economist Milton Friedman wrote \"I have not referred to this distinction [between Knight's risk and uncertainty] because I do not believe it is valid. [...] We may treat people as if they assigned numerical probabilities to every conceivable event [55]. [emphasis added]\" In other words, as in many economic formalizations [13, 129, 171], Friedman assumes that all uncertainty can be framed probabilistically, and that effectively the agent should be blamed for not anticipating all emergent possibilities"}, {"title": "2.2 Machine Learning in Open Worlds", "content": "\"There are more things in heaven and earth, Horatio, / Than are dreamt of in your philosophy.\"\nWilliam Shakespeare\nThe field of ML most often assumes a closed world, where representative situations are provided in a training distribution, and that the world will not thereafter change [218]. That is, the dominant abstractions in supervised and unsupervised learning assume that data is independently and identically distributed (IID; [26, 64]). A similar assumption in most RL formalisms is that the deployment environment is the same as the training environment [96, 213].\nHowever, the world in fact is open-ended, ever-changing, and presents many rare situations that may effectively be impossible to fully anticipate a priori [42, 70, 127, 132, 157]. An open world entails unknown unknowns, a fact sometimes embraced in other fields of study, such as economics, complexity theory, business strategy, and risk management [10, 100, 127, 165, 189], but rarely directly confronted within ML [29, 74]. The divergence between strong formal assumptions in ML and real-world situations is well-known and matters in practice [50, 146, 198, 217], as it is a central cause for the challenge of real-world ML deployments. As a result, there are many strategies for relaxing the assumption of a closed world or directly improving the robustness of ML solutions, which are surveyed next."}, {"title": "2.2.1 Scaling and Generalization", "content": "For example, accumulating training data can increase coverage of unlikely situations. Such accumulation, coupled with NN generalization, does indeed lead to surprising and powerful capabilities [24, 204]. Yet as impressive as such large models are, they still are susceptible to jail-breaks (novel adversarial inputs), hallucination of references (an interesting failure of robustness), and surprising generalization failures [29, 206]. In short, relying on the capabilities of NNs to generalize beyond their training distribution, no matter how much data is accumulated, does not yet seem to solve the problem of robustly dealing with unknown unknowns"}, {"title": "2.2.2 Meta-learning and Continual Learning", "content": "Other mitigation strategies include explicitly relaxing the assump- tion of a static data distribution, through meta-learning [198] or continual learning [151]. Meta-learning, especially when called \"learning how to learn,\" appears a natural paradigm for handling unknown unknowns; however, it is worth examining what actually is entailed by common meta-learning formalisms. In practice, the assumption of IID, characteristic of closed worlds, is shifted from a static training distribution to a meta-distribution of possible tasks. The world remains closed, but the researcher is now responsible for specifying that meta-distribution, and there is no reason that the learned policy should generalize to qualitative variants of tasks the researcher fails to anticipate [4, 97, 161].  Continual learning makes distinct but related assumptions [95, 151, 201], focusing more centrally on the challenge of catastrophic forgetting rather than of robustness to novel situations; although some frame it as inclusive of meta-learning [95]. In conclusion, both meta-learning and continual learning are exciting directions, although neither approach (to our knowledge) has successfully tackled many qualitatively new situations across long deployments; nor do such methods play much role in current foundation models deployments (with exception of in-context learning, which can be seen as a form of emergent meta-learning [37])."}, {"title": "2.2.3 Safe Reinforcement Learning", "content": "Another family of mitigations focuses more directly on the issues of risk and robustness. For instance, the area of robust reinforcement learning [141] aims to discover policies for agents that can better cope with uncertainty, shocks, and change. Nearly all such methods rely on formalizing robustness through making quantitative assumptions about how a reward function or state transition dynamics might change, or what kind of observation or action noise an agent might encounter; while effective and useful for closed world situations, the challenge in open worlds relates more to qualitative changes. Indeed, recent work has suggested that common framings of RL under risk are insufficient to deal with black swan events [110]. Most such approaches implicitly assume that qualitative changes in e.g. state transition dynamics (like a new type of actor in the world) can be handled through adding quantitative noise to the system; and it is left to the experimenter to decide what kind of quantitative robustness to encourage."}, {"title": "2.2.4 Open World Recognition", "content": "A related direction focuses on open set [18] and open world recognition [17], wherein it is acknowledged that a classification system will encounter unknown unknowns corresponding to unanticipated categories beyond those upon which it was trained; such systems often leverage meta-recognition [215] algorithms that attempt to predict when a system is likely to make an error. While such algorithms aspire to detect novel situations, and more directly confront the issue of unknown unknowns, they do not address the crucial issue of how to robustly respond to them. Such algorithms are indeed interesting and worthy of more interest, and may provide a promising starting point for greater integration of KU-awareness into ML."}, {"title": "2.2.5 Zero-shot Generalization in RL", "content": "A setting very relevant to the one described in this paper is zero-shot generalization (ZSG) in RL [76, 96]; although not framed in terms of unknown unknowns, the ability to generalize immediately to environments unlike those handled in training is similar to KU robustness. However, to our knowledge current ZSG methods do not yield robustness to qualitatively new challenges or degrees of variation. Additionally, the formalism of ZSG does not take into the account the temporal process through which new situations emerge in an open world, which we argue is key to how real-world agents handle KU (see Figure 2). Yet this is an exciting area of research, and may help meet the challenge of KU."}, {"title": "2.2.6 Open-endedness", "content": "Finally, the field of open-endedness [179, 181] attempts to study and engineer processes of open-ended creativity, often inspired by biological evolution. The idea is to create long-running search processes that generate continual innovation, as happens in culture and science. Such open-endedness aims in effect to create and continually contribute to an open world, and work has explored how evolutionary open-endedness leads to increasing evolvability [117, 120], i.e. the ability for the system or individuals to quickly adapt. However to our knowledge, no work has attempted to directly tackle the problem of KU through open-endedness, although it has been applied to increase the capacity and adaptability of RL policies in impressive ways [41, 192, 193, 202], most often within a fixed class of environment where tasks require relatively short time horizons."}, {"title": "2.2.7 Summary", "content": "In general, across most such approaches, there is an intriguing tension between the desire to derive algorithms from formal problem definitions and the seeming impossibility of formalizing the ways in which the future will qualitatively differ from the present (and how to respond to such qualitative differences). The proliferation of problem settings, whether meta-learning, continual learning, stochastic games, robust RL, gives the impression that ML is marching towards solving its deepest issues, if only enough progress is made in each subproblem and then combined [34], yet it is also possible that little research actively targets what is core: The past does not in general provide a straight-forward guide to the future [181]."}, {"title": "3 Biological Evolution and Knightian Uncertainty", "content": "\"Darwin's idea is a universal acid; it eats through just about every traditional concept, and leaves in its wake a revolutionized worldview.\"\nDaniel Dennett\nThis paper argues that the formalisms of ML have a blind-spot for unknown unknowns. This section proceeds through proof-by-example that greater robustness is possible through different mechanisms: Biological evolution, though it unfolds without volition or formalisms, does indeed create organisms (such as ourselves) that are surprisingly robust to novel and outlier situations. This section argues that evolution's products are often robust to KU, and highlights the mechanisms through which such robustness arises. The subsequent section then examines limiting assumptions made in RL, the ML paradigm most relevant for learning to handle the unknown.\nTo begin, we sometimes take for granted the incredible complexity and surprising robustness of biological evolution's products, especially given the dearth of information that drives its search. To a first approximation, evolution blindly pulls mutational levers and receives as feedback only whether the levers pulled result in persistence across time [62]. Evolution has no foresight, makes no formal assumptions, and has no conceptual understanding of the complex mechanisms through which an organism survives and reproduces. And yet its creations often act as if they are making complex plans about the future: e.g. some cicadas have life cycles that synchronize to prime numbers (e.g. 13 or 17 years), thereby making it more difficult to be predated by organisms with shorter lifecycles (as prime numbers are not divisible by smaller numbers); leafcutter ants in effect farm fungi by cutting leaves (too tough for them to eat) that they feed to underground symbiotic fungal gardens; migratory birds can travel thousands of miles to exploit seasonal food resources; some pine species have cones that lie dormant and only release seeds after being exposed to fire, ensuring that the environment is cleared of competition and nutrient-rich; and cyanobacteria living only hours can keep track of changing seasons across generations [85].\nTo quote Cass Sunstein [185]: \"Contrary to a standard view in economics, Knightian uncertainty is real. Dogs face Knightian uncertainty; horses and elephants face it; human beings face it [...]\" to which we could add: biological evolution most certainly faces it. Evolution is fundamentally unable to assign explicit probabilities to novel outcomes, and yet seems also to be remarkably successful at navigating uncertainty. On the whole its creations are surprisingly robust in the face of unforeseen challenges [99, 166]; and more deeply, evolution as a whole is incredibly robust \u2013 it is unlikely to be extinguished on Earth, even given full-scale nuclear war.\nAs a consequence of its lack of volition, evolution necessarily must wrangle with KU, in contrast with ML algorithms derived in explicitly probabilistic terms from closed-world formalisms. There is no theoretical formalism of risk that evolution imposes upon the world; instead, it can be viewed an ongoing anarchic, creative, and divergent search for interdependent structures that persist across time, subject to the stringent constraints of the environment and interactions with other organisms.\nMore concretely, the argument is that evolution's robustness in the face of KU is due to four interlocking factors (see also Figure 1; note that these principles relate to previously-proposed conditions for enabling continuing open-ended evolution [178], but aim instead to explain the emergence of robustness to KU):\n(1) The selection criterion of persisting across (potentially vast) swaths of time filters bets for robustness.\n(2) The drive to accumulate an abundant diversity of solutions results in many diverse bets on how to persist through future.\n(3) Adaptations of one organism create novel situations for others that tests their robustness in new ways.\n(4) An open-ended search space enables revising any part of the mechanisms of learning in service of greater robustness.\nIn short, an organism implicitly encodes a bet about how it and its offspring can persist through the indefinite future, which is tested across unforeseeable situations generated in part by the physical environment and in part by the adaptations of other organisms. Because evolution explores a diverse and ever-refreshing set of such bets, and bad (or unlucky) bets are culled across both the short and long-term, as a whole it tends to favor organisms relatively robust to the unforeseen. Further, because the search space includes arbitrarily complex adaptations, including revising any aspect of neural architecture, learning algorithms, and development as a whole, there is no commitment in the system to any particular formalism, algorithm, or architecture. Instead, reality serves as a filter for what works empirically in the face of often-extreme uncertainty."}, {"title": "3.1 Evolution as Driven by Scientific Falsifiability", "content": "\"We are standing in the doorway of a library of life, but the books are burning as we speak.\"\nGPT-40 hallucination attributed to E.O. Wilson\nPopperian falsifiability is the idea that a scientific theory must be testable in a way that could prove it false [157], and provides a useful frame for thinking about what it means for an evolutionary lineage to persist across (potentially vast) time. The scientific community creates many divergent hypotheses across its diverse fields of study (e.g. physics, psychology, biology), which can be falsified through experiments that generate results that cleave between hypotheses (e.g. Francesco Redi's disproval of the spontaneous generation theory within biology [162]). We can similarly view the diverse gambles of evolution from the lens of scientific hypotheses, experimentation, and falsification [40, 157]. An organism's current genome represents a range of possible phenotypes (expressible through development and environmental influence); this range of possibilities can be viewed as a hypothesis about how an organism can persist the future, which can be falsified through \u201cexperiments\u201d with reality if it is not able to survive and reproduce. This analogy with science is helpful, because it clarifies that evolution is in effect an automated and diversifying research and development process [179], and that like science, it pursues many independent tentative bets which at all times are subject to falsification, no matter how long they have so far persisted.\nImportantly, beyond a scientific paradigm's ability to explain current experimental data, its longevity depends also on its potential to be fleshed out and expanded in the future \u2013 the extent to which it is a generative stepping stone [181]. For example, the beauty of Darwin's insights into evolution persist today, through being much-expanded and refined into the current extended evolutionary synthesis [155]; in a similar way, a genome is a stepping stone to a wider range of possible future phenotypes accessible through genetic mutation, i.e. its adjacent possible [91]. In other words, persistence of an organism's lineage depends not only on the behaviors it exhibits during its life (and their robustness to KU), but to its potential for future adaptation and diversification (e.g. to realize adaptive radiation [183]), as new situations and opportunities emerge across generations."}, {"title": "3.1.1 Bets on Robust Behavior in an Organism's Lifetime", "content": "Biological organisms have evolved diverse strategies for robustly navigating uncertainty during their lifetimes. For example, many animals have developed avoidance strategies that are robust through their simplicity, e.g. toads that attempt to eat small moving things, and avoid large moving things [164]; and mice have a generalized fear of open spaces where they are vulnerable [123]. Mammals more generally have a variety of sophisticated fear and fear-learning mechanisms [148], often biased towards rapid response (e.g. that relies on simple visual features), and favoring false positives over false negatives, given the life-and-death stakes of predation. Notice in this example, that evolution moves outside the abstractions generally available to a typical NN approach to RL: It leverages a faster response to a dangerous situation by dynamically using less neural processing. Such rapidity is a fit for avoiding predators, and less rapid responses are culled by evolution as they result in greater predation. The point is not rapid response per se, but that in general, evolution benefits from lacking a firm commitment to any particular ingredient or formalism (unlike a single NN that follows a fixed path of computation from input to output)."}, {"title": "3.1.2 Bets on Robust Behavior in an Organism's Future Lineage", "content": "A lineage's evolutionary success extends far beyond the lifetime of one organism, requiring continuing persistence over long swaths of time [62]. Any organism alive today has an unbroken track record of persistence dating back to the first replicator, across which its ancestors undoubtedly successfully met the challenges of countless unanticipated situations. If an organism's genome is overly-optimized for expected present reproductive success, but its entire lineage falls prey to a catastrophic change in conditions, then that bet on the future is culled. Thus, even if an organism is successful in the present environment, to persist it must be adaptable to potentially dramatic changes.\nBeyond its present behavior, the potential future behaviors of an organism are also encoded by implicit bets within its genome. That is, organisms are predisposed to certain kinds of variation at the expense of others [98]; mutations of a genome result in a range of accessible changes to behavior and morphology, and that range of accessible variation can itself be selected for. Such facilitated variation [57] operates through evolved mechanisms like development [27], canalization [174], and genomic organization [90]; and other mechanisms could yet evolve in the future. For example, there exist mutations that give a person six functioning fingers on each hand, which are enabled by the developmental programs encoded in the human genome; these kinds of modular high-level mutations are more evolvable than if the design for a new finger required independent re-invention. Such evolvability of a genome is also subject to selection, insofar as it enables or hinders a lineage's ability to persist.\nA genome's accessible variation implicitly encodes a bet about the future opportunities and situations an organism's lineage may encounter; e.g. the insect body plan's dependence on an exoskeleton limits the size of organisms evolved in a different way than that of animals with an endoskeleton, and different body plans enable different flavors of phenotypic variation (e.g. wing size in birds; tail size in lizards). This can be seen somewhat analogously to how ML algorithms themselves develop over time as researchers extend them and apply them to new domains and contexts (e.g. the many different variants of LLM activation functions, positional encodings, and variants of attention layers); the difference is that evolution explores such architectures and systems autonomously.\nThus in total, a genome implicitly encodes a hypothesis about the present and possible futures, which can be falsified if it ceases to persist (either through it not reproducing, or through all of its children's lineages not persisting). As in science, such a hypothesis is never completely validated, as tomorrow's environmental conditions or the adaptation of an organism vying for the same niche may undermine a lineage's prospects. Importantly, there is no fixed time horizon for falsification"}, {"title": "3.2 Evolution as Open-ended Generator of Novelty", "content": "\"The beauty of the living world I was trying to photograph was spread before me like a tapestry of evolutionary innovation, with one outlandish strategy after another. A flower shaped like a bucket to trap insects. A fish that farms algae. Another that stabs its prey with its snout. Life evolved to fill every conceivable niche.\"\nClaude 3.5 Sonnet hallucination attributed to Frans Lanting\nEvolution, like science, is antifragile [190]: the system as a whole improves from shocks (e.g. in science, from experiments that empirically refute a hypothesis, or in evolution, from environmental changes that cause extinction [114]). More broadly, there are pressures in evolution to diversify and expand through possible behaviors and niches [59, 181, 183], such as to escape competition [59, 153] or predation [88]. Over evolutionary time, this antifragility and pressure to diversify causes evolution to accelerate, which is known as the evolution of evolvability [38, 152]. Common critiques of evolutionary algorithms [211] tend not to recognize this self-accelerating antifragility; while evolution may start slow, like science, it accumulates an expanding repertoire of powerful building blocks"}, {"title": "4 Reinforcement Learning's Formalisms Limit Robustness to Knightian Uncertainty", "content": "\"A picture held us captive. And we could not get outside it, for it lay in our language and language seemed to repeat it to us inexorably.\"\nLudwig Wittgenstein\nThis section explores the formalisms underlying reinforcement learning (RL) as a representative example of how convenient assumptions implicitly limit ML's engagement with the problem of KU; without loss of generality, similar assumptions may apply also to unsupervised and supervised"}]}