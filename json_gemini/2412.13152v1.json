{"title": "Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings", "authors": ["Paolo Gabriel", "Peter Rehani", "Tyler Troy", "Tiffany Wyatt", "Michael Choma", "Narinder Singh"], "abstract": "This study introduces an Al-driven platform for continuous and passive patient monitoring in hospital settings, developed by LookDeep Health. Leveraging advanced computer vision, the platform provides real-time insights into patient behavior and interactions through video analysis, securely storing inference results in the cloud for retrospective evaluation. The dataset, compiled in collaboration with 11 hospital partners, encompasses over 300 high-risk fall patients and over 1,000 days of inference, enabling applications such as fall detection and safety monitoring for vulnerable patient populations. To foster innovation and reproducibility, an anonymized subset of this dataset is publicly available. The Al system detects key components in hospital rooms, including individuals' presence and roles, furniture location, motion magnitude, and boundary crossings. Performance evaluation demonstrates strong accuracy in object detection (macro F1-score = 0.92) and patient-role classification (F1-score = 0.98), as well as reliable trend analysis for the \u201cpatient alone\u201d metric (mean logistic regression accuracy = 0.82 \u00b1 0.15). These capabilities enable automated detection of patient isolation, wandering, or unsupervised movement-key indicators for fall risk and other adverse events. This work establishes benchmarks for validating Al-driven patient monitoring systems, highlighting the platform's potential to enhance patient safety and care by providing continuous, data-driven insights into patient behavior and interactions.", "sections": [{"title": "1 INTRODUCTION", "content": "In hospitals, direct patient observation is limited-nurses spend only 37% of their shift engaged in patient care (Westbrook et al. (2011)), and physicians average just 10 visits per hospital stay (Chae et al. (2021)). This limited interaction hinders the ability to fully understand patient behaviors, such as how often patients are left alone, how much they move unsupervised, and how care allocation varies by time or condition. Virtual monitoring systems, which allow remote patient observation via audio-video devices, have improved safety, particularly for high-risk patients (Abbe and O'Keeffe (2021)).\nArtificial Intelligence (AI) is transforming healthcare by enhancing diagnostic accuracy, streamlining data processing, and personalizing patient care (Davenport and Kalakota (2019), Davoudi et al. (2019), Bajwa et al. (2021)). While AI has found success in tasks like surgical assistance (Mascagni et al. (2022)) and diagnostic imaging (Esteva et al. (2021)), patient monitoring represents a critical frontier. Unlike these"}, {"title": "2 METHODS", "content": "The LookDeep Health patient monitoring platform was deployed across 11 hospitals in three states within a single healthcare network. The system provides continuous, real-time monitoring of high-risk fall patients. Data collection adhered to institutional guidelines and patient consent procedures (see Research Ethics)."}, {"title": "2.1 Study Design", "content": ""}, {"title": "2.1.1 Participants", "content": "Patients monitored by LookDeep Health were primarily high-risk fall patients identified through mobility assessments as part of standard care protocols. This classification often results in the patient also being categorized as non-ambulatory during the inpatient stay (Capo-Lugo et al. (2023)).\nData was organized into three subsets:\n1. Single-Frame Analysis: Periodic samples from monitoring sessions were used for training and testing object detectors, with over 40,000 frames collected to date. Only patients monitored during the first week of each month were included in the test set, providing 10,000 frames held out for consistent model evaluation.\n2. Observation Logging: Ten patients who experienced falls were selected for additional annotation over a twelve month period (Figure 2A).\n3. Public Dataset: Over 300 high-risk fall patients were monitored during a six month period, excluding those monitored for less than two days (Figure 2B).\nAs shown in Figure 3, data collection spanned multiple years, with each subset contributing to the development and validation of the AI system, with some overlap between subsets."}, {"title": "2.1.2 Patient Monitoring System Overview", "content": "The LookDeep Health monitoring system processes video through a computer vision pipeline to detect, classify, and analyze key elements within the patient's room, providing actionable insights to healthcare staff (Figure 4). Key components include:\n1. Video Data Capture and Preprocessing: Video data is captured at 1 frame per second (fps) by LookDeep Video Unit (LVU) devices deployed in patient rooms (Figure 5A). Data is preprocessed to reduce bandwidth and enable efficient analysis.\n2. Object Detection and Localization: A custom-trained model detects key objects (\"person\", \"bed\", \"chair\") and localizes them with bounding boxes.\n3. Person-Role Classification: Detected \"person\" objects are further classified as \u201cpatient\u201d, \u201cstaff\u201d, or \"other\" by augmenting labels with role-specific information.\n4. Motion Estimation: Dense optical flow estimates motion between consecutive frames, enabling activity tracking in specific regions (e.g. scene, bed, safety zone).\n5. Logical Predictions: High-level predictions (e.g. \u201cperson alone\u201d, \u201cpatient supervised by staff\") are derived by applying rules to detection and motion data, with a 5-second smoothing filter to mitigate detection errors.\nInference results, including object detections, role classifications, motion estimation, and logical predictions, are securely stored in a Google cloud database for further analysis (e.g. trend analysis). Anonymized frames are stored at regular intervals for quality assurance and model improvement."}, {"title": "2.1.3 Data Anonymization", "content": "To ensure patient privacy in accordance with the Health Insurance Portability and Accountability Act (HIPAA) and institutional guidelines, all video data was processed to remove identifiable information. For training purposes, frames were face-blurred using a two-step procedure to maintain privacy while preserving relevant scene context:"}, {"title": "2.2 Data Collection", "content": ""}, {"title": "2.2.1 Video Patient Monitoring", "content": "LVU devices capture continuous video in RGB or near-infrared (NIR) mode, depending on ambient lighting. Each device is equipped with a CPU and Neural Processing Unit (NPU), capable of processing data at 1fps to minimize latency and reduce cloud processing requirements. Inference results are uploaded to a secured cloud database (Google BigQuery), with blurred frames stored separately for manual annotation. Camera placement varied based on room layout and clinical workflows (Figure 5B)."}, {"title": "2.2.2 Annotations", "content": ""}, {"title": "2.2.2.1 Frame-level Labels", "content": "A professional labeling team manually annotated over 40,000 images with object bounding boxes, object properties, and scene-level tags (Figure 6). Objects were annotated with 2-d bounding boxes classed as \"person\", \"bed\u201d, or \u201cchair\u201d, and each \"person\u201d bounding box was also assigned a role of \u201cpatient\u201d, \u201cstaff\u201d, or \"other\". Scene level attributes were added for whether the patient was \u201cin bed\u201d or \u201cnot in bed\" and whether the scene included \u201cexception cases\u201d in comparison to stated norms. Exception cases were applied in any instance of labeler uncertainty (e.g. difficult to see person, patient in street clothes, etc.); in instances of multiple exception cases being applicable, a single \u201cframe exception\u201d catch-all was used. Annotations and quality review were conducted using the Computer Vision Annotation Tool (CVAT, Corporation (2023)), and final QA was conducted using the FiftyOne tool (Moore and Corso (2024))."}, {"title": "2.2.2.2 Observation Logs", "content": "Blurred video summaries for 10 patients (54 patient-days) were reviewed to log periods when the patient was alone. Logs included timestamps with 1-2 second precision (Figure 6), and underwent secondary quality assurance to provide feedback to labelers and fill out any missing periods."}, {"title": "2.3 Computer Vision Predictions", "content": "The LookDeep Health pipeline processes video data using custom-trained models to detect objects, classify person-role, and estimate motion at 1 fps. Preprocessing compresses frames to JPEG at 80% quality and resizes to a resolution of 1088x612 to reduce bandwidth consumption while still meeting downstream model requirements. Image processing is conducted using OpenCV (Bradski (2000)) and RKNN-toolkit (Fuzhou Rockchip Electronics Co. (2024)).\n1. Object Detection (Person/Bed/Chair): Based on the YOLOv4 architecture (Bochkovskiy et al. (2020)), the model identifies key objects in each frame, including \u201cperson\u201d, \u201cbed\u201d, and \u201cchair\u201d. Training models were initialized using COCO weights (Lin et al. (2014)), then fine-tuned on labeled data. Input images were down-sampled to 608x608 with OpenCV's cubic interpolation method to fit"}, {"title": "2.3.1 Additional Components", "content": ""}, {"title": "2.3.1.1 Regions of Interest (ROIs)", "content": "ROIs, such as \"safety zones\u201d, provide contextual boundaries for monitoring. They are not predictive outputs themselves, but instead are used to track patient movements and boundary crossings. The \"safety zone\u201d was a polygonal region defined by the virtual monitor; its pixel mask is generated by expanding the boundary perimeter by 10% to ensure effective monitoring. Additional ROIs used by the system include the full scene and the detected bed."}, {"title": "2.3.1.2 Logical Predictions", "content": "Logical predictions summarize patient status and interactions. These predictions were derived from a combination of object detection and role classification results and smoothed with a 5-second filter to mitigate intermittent detection errors.\n\u2022 Person Alone: $True$ when the average number of detected people in the room is less than two.\n\u2022 Patient Alone: $True$ when the average number of detected people in the room is less than two, and at least one person is classified as a patient.\n\u2022 Supervised by Staff: $True$ when the average number of detected people in the room is two or more, and at least one person is classified as healthcare staff."}, {"title": "2.3.1.3 Trend Predictions", "content": "Trends provide insights into immediate and long-term patient activity, aiding in risk identification and care planning. Hourly trends summarize patient behavior (e.g. \"alone\" or \"moving\u201d) based on aggregated logical predictions. For each one-hour interval, predictions were used to calculate the percentage of time the patient spent in key states like \"alone,\" \"supervised by staff,\" or \"moving\u201d. These percentages were then plotted over time to visualize hourly trends in patient isolation or activity levels throughout the day. These trends provide a high-level overview of patient behavior, aiding in the identification of potential risks and informing care decisions."}, {"title": "2.3.1.3.1 \"Assisted\" Trend Predictions", "content": "A one-off analysis was conducted to simulate the system's performance when one of the predictions was known. The system's trend predictions based solely on AI inference were compared with those generated using a combination of AI inference and observation logs. For this comparison, \"assisted\u201d trends were created by integrating AI-predicted states for \"moving\" and \"supervised by staff\u201d with manually logged periods of \"alone\u201d status from the observation logs."}, {"title": "2.4 Evaluation", "content": "The performance of the AI-driven monitoring system was assessed through two primary methods: image-level assessment and comparison against observation logs. In the image-level assessment, each frame was analyzed against manual annotations to evaluate the accuracy of the system's object detection, person-role classification, and scene interpretation capabilities. In parallel, observation logs, created from anonymized video summaries of select patients, were compared against predicted trends to assess the system's ability to capture patient behavior patterns."}, {"title": "2.4.1 Frame-level Analysis", "content": "Each model in the AI system was evaluated independently to assess its performance in object detection and classification tasks. Key performance metrics-precision, recall, and F1-score\u2014were calculated to measure the accuracy and reliability of each model's predictions. Precision assessed the proportion of true positives among all predicted positives, recall measured the ability to identify all true positives, and the F1-score provided a balanced metric between precision and recall.\nIn addition to these direct object detection and classification tasks, the AI system also generated higher- level, \"logical\u201d predictions derived from these outputs. For example, the prediction \u201cis patient alone\u201d was inferred based on a combination of object detection results, such as the absence of healthcare staff within a defined proximity to the patient. These logical predictions were treated as classification tasks themselves, with their accuracy similarly evaluated using precision, recall, and F1-score metrics based on labeled image data. This multi-layered approach allowed us to thoroughly validate both the core object detection functions of each model and the system's ability to interpret and apply these outputs to patient monitoring tasks."}, {"title": "2.4.2 Trend Analysis", "content": "Trend analysis was conducted by comparing the system's inference-derived metrics to ground truth metrics recorded in observation logs, with both datasets aggregated by patient-day. Unlike the hourly trends shown in Figure 4, analysis was conducted at the per-second level to ensure accurate alignment between AI predictions and observation logs. The primary metric for this analysis was logistic regression accuracy, which assessed the AI system's ability to predict observed behaviors within three time periods: daytime (6 am to 9 pm), nighttime (9 pm to 6 am), and the full 24-hour period. In cases where only a single class (e.g. \"alone\" or \"not alone\u201d) was present within a specific time period, logistic regression was not feasible. Instead, a manual accuracy score was computed, to allow for consistent accuracy measurements across all time intervals. This score is defined as the proportion of matching values between the AI predictions and ground truth.\nFocusing on the \"alone\" binary behavior trend enables an assessment of the alignment between A\u0399 predictions and real-world observations. This analysis validated the AI system's effectiveness in capturing hourly patient behavior trends, underscoring its potential utility in real-time patient monitoring and early detection of deviations from expected patterns."}, {"title": "2.4.3 Camera Position Meta-analysis", "content": "Since cameras were mounted on mobile carts rather than fixed positions, there was variability in camera setup across patients and hospital rooms (Figure 5B). To explore the potential impact of this variability, labeled bed locations were used to estimate each camera's position relative to the hospital bed. Distributions of the labeled bed area and size within each frame, along with the centroid location of the bed relative to the camera's field of view are plotted in Figure 7. These distributions provide an indirect measure of camera position.\nThis exploratory analysis helped identify patterns and variations in camera setups across different monitoring sessions. However, this information was observational and used only to understand positional variability; no specific adjustments were made during model training or evaluation to account for different camera positions. The results underscore the robustness of our models in handling diverse camera perspectives, as the system maintained consistent detection performance despite these variations."}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 Frame-level Analysis", "content": ""}, {"title": "3.1.1 Object Detection, Role Identification, and Patient Isolation Classification", "content": "The evaluations demonstrated that the custom-trained computer vision models perform robustly in real- world hospital settings, achieving high precision across key object detection and classification tasks. We compared five production models alongside a baseline model using an off-the-shelf YOLOv4 configuration (Table 1). Each production model corresponds to a different release, with progressively larger and more refined training datasets incorporated over time (Figure 3). This iterative refinement led to increased model accuracy and adaptability in real-world hospital settings. To ensure consistency, all frame-level analysis was conducted on 10,000 frames collected over a two year period. This representative sample, excluded from model training and validation, highlights the incremental improvements achieved by expanding training datasets across model versions.\nAs newer models were released, the training set was expanded to include additional annotated data, allowing each successive model to capture more complex and diverse scenarios encountered in hospital environments. The most recent fine-tuned model (v5) achieved an F1-score of 0.91 for detecting \u201cperson\u201d, notably surpassing the baseline YOLOv4 model score of 0.41 (Table 2). Across all object classes\u2014including beds, furniture, and other room elements\u2014the v5 model demonstrated an F1-score of 0.92, reflecting a high degree of accuracy and consistency across diverse object types.\nIn addition to object detection, the system was evaluated on a three-class person-role classification task, distinguishing between patients, healthcare staff, and visitors within the camera's field of view. The v5 model demonstrated particularly strong performance for the \"patient\u201d class, achieving an F1-score of 0.98, which reflects its high accuracy in identifying patients specifically (Table 2). Accurate person-role classification is essential for monitoring patient interactions and ensuring appropriate caregiving behaviors, as it enables the system to capture not only the presence of individuals but also their roles. Focusing on the \"patient\" class, the high F1-score underscores the model's robustness in tracking patient activity and interactions, which are critical for effective continuous monitoring in dynamic hospital environments.\nThe downstream classification task of identifying whether a patient was \u201calone\u201d in the room showed similarly strong results, with the v5 model achieving an F1-score of 0.92 (Table 2). This classification task,"}, {"title": "3.1.2 Impact of Privacy-Preserving Blurring on Model Consistency", "content": "The performance consistency of the models across unblurred and face-blurred images was evaluated using the A metric, which represents the F1-score difference between the two image types (Table 3). Across all model versions, the \u25b3 values were relatively small, indicating that face-blurring\u2014a common privacy-preserving preprocessing step-had minimal impact on model accuracy. For versions v3 and v4, the A value was +0.04, while in v5 it decreased to +0.02, suggesting improved robustness to blurring as the training data volume increased.\nA smaller A value is desirable as it indicates that the model performs consistently regardless of whether the images are unblurred or face-blurred. The reduction in A for v5 highlights the value of larger, more diverse training datasets in ensuring that the models generalize well across different image types. This is particularly important in hospital settings, where preserving patient privacy often necessitates the use of face-blurred images. The ability to maintain high accuracy in such scenarios ensures the system's practicality and reliability for real-world deployment.\nThese results demonstrate that the models not only achieve high accuracy but also exhibit resilience to variations introduced by privacy-preserving preprocessing, a key requirement for scalable applications in healthcare environments."}, {"title": "3.2 Trend Analysis", "content": "Inference-derived trends for the \u201cpatient alone\u201d metric were compared against observation logs to evaluate the system's ability to accurately capture real-world patterns (Figure 8). This trend analysis utilized data from earlier stages of the project when base models with lower performance were deployed. Specifically, the object detectors used for these inferences had an F1-score of 0.85 for \u201cperson\u201d detection, which is lower than the performance of the latest models. Despite this, the analysis showed strong alignment with ground truth data, achieving an average logistic regression/manual accuracy of 0.84 \u00b1 0.13 during daytime, 0.80 \u00b1 0.16 at nighttime, and 0.82 \u00b1 0.15 across all times. These results highlight the robustness of the AI system in capturing patient isolation trends, even when using earlier model versions with lower baseline performance.\nThis accuracy indicates that, even with slightly reduced detection precision in the older models, the system could reliably capture general patterns in patient isolation behavior. The standard deviation (\u00b1 0.15) reflects some variability in accuracy across different times of day and patient conditions, possibly influenced by factors such as changing camera angles or environmental conditions. As shown in the normative hourly trends (Figure 9), discrepancies between labeled and AI-inferred \"alone\" data are more pronounced during nighttime hours, but these differences have minimal impact on the broader trend patterns. For both \"Alone and Moving\" and \"Supervised by Staff\u201d metrics, the AI inferences closely align with label-assisted data, amounting to an average error of 1-2 minutes per hour. This consistency underscores the model's robustness in capturing meaningful patient-alone trends and suggests that any nighttime performance gaps in the \"alone\" inference do not significantly compromise the overall accuracy. These results highlight the model's potential for improved trend detection as newer, refined models are applied to subsequent data."}, {"title": "4 DISCUSSION", "content": ""}, {"title": "4.1 Implications for Clinical Practice", "content": "The findings of this study underscore the potential for AI-enabled patient monitoring systems to enhance clinical practice through continuous, real-time monitoring. Traditional in-person observations are limited by the time constraints of healthcare staff, who spend limited hours directly interacting with each patient. By providing continuous monitoring, the LookDeep Health platform enables staff to detect patterns that would otherwise go unnoticed, such as extended periods of patient isolation, movement patterns that might indicate a risk of falls, pressure injuries, or irregular interactions with staff. Real-time alerts based on these observations could prompt timely interventions, potentially improving patient safety and outcomes.\nMoreover, the data collected by this system can inform trend analysis on a population level, supporting hospital resource allocation and staffing decisions. For instance, identifying times of day when patients are frequently unsupervised could guide adjustments in staffing or the deployment of additional monitoring resources to high-risk patients. Beyond staffing, the system's insights into patient mobility patterns\u2014such as time spent in bed, in a chair, or walking around the room\u2014can help identify markers of successful recovery and readiness for discharge, contributing to improved patient outcomes. These mobility insights could also support the development of best practices for post-procedure mobility, tailored to specific surgeries or treatments, to enhance patient recovery. Altogether, these data-driven insights promote a more efficient, personalized approach to patient care, potentially improving patient satisfaction and clinical outcomes."}, {"title": "4.2 Impact of Face-Blurring on Model Performance", "content": "While the evaluation of model performance on both unblurred and face-blurred images provides valuable insights, it is important to note that face-blurring is applied only during training and evaluation phases. In real-world deployment, the model will encounter unblurred images as it monitors patients in hospital settings, making this distinction critical to understanding its practical performance. The small \u25b3 values observed across different model versions indicate that the models have been designed to handle face-blurred images without significant degradation in performance. The reduced A in the latest version (v5), attributed to increased training data volume, demonstrates improved resilience to face-blurring. However, further studies are needed to assess the model's performance in unblurred scenarios, particularly in environments where face-blurring images for training and evaluation is not an option. This approach ensures privacy during development while maintaining practical deployment fidelity, as real-time monitoring operates on unblurred frames."}, {"title": "4.3 Variation in Camera Setup", "content": "The LookDeep Health patient monitoring platform was deployed in real-world hospital settings with cameras mounted on mobile carts rather than fixed positions, resulting in variation in camera angles, distances, and perspectives across different patient rooms. This variability introduced potential challenges in maintaining consistent object detection and classification accuracy, as model performance can be influenced by changes in camera field of view and positioning relative to the bed. To mitigate these effects, we conducted a camera position meta-analysis using metadata on labeled bed area and centroid location to estimate the approximate camera placement within each room. Our analysis confirmed that, despite positional differences, the model consistently achieved reliable performance across object detection and classification tasks, demonstrating its robustness to spatial variability. However, this setup presents"}, {"title": "4.4 Nuanced Differences in Time Coverage of Analyses", "content": "A key aspect of this study is the variation in time coverage across different datasets, reflecting the evolving nature of data collection and model validation in real-world hospital settings. The observation logs dataset, which provided ground truth for logical trend validation, was collected exclusively in 2023. In contrast, frame-level annotations for evaluating object detection and person-role classification were gathered over a more extended period from 2022 to 2024. Additionally, the publicly released dataset comprises data collected from a 6 month span across 2024, representing over 1,000 collective patient days across multiple hospitals.\nThese differences in collection periods introduce nuances in interpretation. For instance, frame-level evaluations benefit from the broader time span, capturing a variety of hospital conditions and patient behaviors across seasons and changing workflows. However, trend analyses were constrained to the observation log time frame, which may limit the ability to generalize trends across the entire study period. Similarly, the released dataset reflects data from the latter phase of the study, aligning with the most refined models but excluding early-stage model iterations.\nThese variations in time coverage highlight the need to contextualize each analysis within its specific time frame. Future studies could benefit from aligning data collection periods across all evaluation methods, ensuring that models validated on frame-level tasks are continuously validated against trend and behavioral analyses for consistent performance insights over time."}, {"title": "4.5 Challenges and Limitations", "content": "Several challenges and limitations were encountered in this study. First, the variability in camera setup, as mentioned earlier, introduces potential inconsistencies in model performance due to changing perspectives and distances. While our metadata analysis mitigated this to some extent, a standardized camera setup would likely yield more consistent results.\nSecond, while the LookDeep Health system demonstrated strong performance in object detection and role classification, real-time video processing presents computational challenges that require balancing accuracy and processing speed. Our use of onboard CPU and NPU on LVU devices provided sufficient processing capabilities for 1 fps inference; however, the scalability of such a setup may be constrained in larger hospital systems requiring higher frame rates for finer details.\nThird, the dataset collected in this study primarily consists of high-risk fall patients, which may limit the generalizability of findings to broader patient populations - for example, high-risk patients exhibit limited mobility compared to other patient groups. Additionally, the analysis was conducted on older model versions for some trend analyses, potentially lowering the accuracy of trend detection. Although model refinements are expected to improve results, these differences in model versions should be considered when interpreting the findings.\nLastly, maintaining patient privacy is paramount in continuous video monitoring systems. While the LookDeep Health platform anonymizes all video and stores de-identified data, ongoing attention to data privacy and compliance with healthcare regulations is essential for future deployments in clinical environments."}, {"title": "4.6 Suggestions for Future Research", "content": "While this study provides a foundation for understanding the impact of AI-driven patient monitoring, further research is warranted to explore additional facets of this technology. Future studies could investigate:\n\u2022 Advanced Deep Learning Techniques: Integrating more sophisticated deep learning architectures could enhance the detection of subtle anomalies, while adaptive pipelines could improve real-time robustness in dynamic hospital environments.\n\u2022 Refining Architectures and Guardrails: Future work could involve refining architectures to detect edge cases more accurately, tracking patterns in prediction errors, and incorporating confidence-based guardrails to prevent catastrophic failures. Such guardrails could include alerts when model confidence is unexpectedly low for consecutive predictions.\n\u2022 Higher Frame Rates and Computational Scaling: Evaluating the potential for higher frame rates or adaptive frame rate technology to improve real-time responsiveness, particularly in high-activity environments.\n\u2022 Standardization of Camera Placement: Testing standardized, fixed camera setups across patient rooms aims to minimize positional variability and improve model consistency. Although standardization can reduce variability, embracing the inherent diversity of setups may enhance model robustness for real-world applications.\n\u2022 Expanded Patient Cohorts: Extending the analysis to include a wider range of patient demographics and conditions to assess generalizability and adapt the system to diverse populations.\n\u2022 Interoperability with Hospital Systems: Integrating AI-driven monitoring with electronic health records (EHRs) and hospital workflow systems to provide context-aware alerts and streamline clinical response.\nThese research directions, alongside continued refinement of computer vision models and monitoring systems, will be essential for advancing the practical application of AI in patient monitoring and driving further improvements in healthcare delivery."}, {"title": "5 CONCLUSION", "content": "AI integration in medical imaging is advancing personalized patient treatment but still faces challenges related to effectiveness and scalability. This work demonstrates the potential of computer vision as a foundational technology for continuous and passive patient monitoring in real-world hospital environments.\nThe contributions of this study are two-fold. First, we introduce the LookDeep Health patient monitoring platform, which leverages computer vision models to monitor patients continuously throughout their hospital stay. This platform scales to support a large number of patients and is designed to handle the complexities of hospital-based data collection. Using this system, we have compiled a unique dataset of computer vision predictions from over 300 high-risk fall patients, spanning 1,000 collective days of monitoring. To encourage further exploration in the field, we released this anonymized dataset publicly at lookdeep/ai-norms-2024.\nSecond, we rigorously validated the AI system, demonstrating strong performance in image-level object detection and person-role classification tasks. Our analysis also confirms a positive alignment between inference-derived trends and human-observed behaviors on a patient-hour basis, underscoring the reliability"}, {"title": "CONFLICT OF INTEREST STATEMENT", "content": "The authors disclose the following competing financial interests: they are current or former employees of LookDeep Health, the company that provided the tools used in this study. LookDeep Health was involved in data collection and analysis and reviewed the final manuscript prior to submission. The authors declare no other conflicts of interest related to this work."}, {"title": "RESEARCH ETHICS", "content": "The data used in this retrospective study was collected from patients admitted to one of eleven hospital partners across three different states in the USA. The study and handling of data followed the guidelines provided by CHAI standards. Access to this data was granted to the researchers through a Business Associate Agreement (BAA) specifically for monitoring patients at high risk of falls. In compliance with the Health Insurance Portability and Accountability Act (HIPAA), patients provided written informed consent for monitoring as part of their standard inpatient care. To ensure patient privacy, all video data was blurred prior to storage, and no identifiable information is included in this work. Face-blurred frames were used only for training purposes. Faces were manually labeled on fully-blurred images, and the raw images were then treated with a local Gaussian blur in the facial regions, ensuring privacy without compromising model training quality. The outcomes of this analysis did not influence patient care or clinical outcomes."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "P.G. drafted the manuscript. P.G., T.T., and N.S. designed the research. P.G., P.R., and T.T. conducted the research and analyzed the data. T.W. and M.C. provided clinical insights. N.S. supervised the study."}, {"title": "FUNDING", "content": "This research was funded by our hospital system partner as part of a business agreement supporting the development and deployment of AI-driven patient monitoring solutions. The funding provided resources for data collection, system implementation, and analysis within the hospital environment."}]}