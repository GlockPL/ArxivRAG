{"title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction", "authors": ["Michal Bravansky", "Vaclav Kubon", "Suhas Hariharan", "Robert Kirk"], "abstract": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets.", "sections": [{"title": "1. Introduction", "content": "Extracting meaningful insights from large data repositories is a cornerstone of modern research, spanning disciplines such as social sciences (Lazer et al., 2009; Xu et al., 2024), medical sciences (Hulsen et al., 2019), and economics (Varian, 2014; Korinek, 2023). Recent advances in large language models (LLMs) (Vaswani, 2017; Radford, 2019; Achiam et al., 2023) have emerged as a promising approach to this challenge, enabling researchers to process datasets and generate natural language descriptions that summarize and analyze underlying information (Singh et al., 2024a).\nHowever, when working with massive heterogeneous datasets like user queries (Zhao et al., 2024; Zheng et al., 2023b), fundamental challenges emerge: the data cannot be characterized by a single description and requires multiple overlapping features to capture different aspects of the information. This raises two questions: (1) How can we identify which features are important enough to capture without relying on human supervision? (2) How can we ensure our system generalizes to unforeseen datapoints without extensive prompt engineering and design iteration?\nIn this paper, we formalize the problem through the concept of features, defined as binary predicates $ : X \u2192 0,1 (e.g., \"text x implies a misunderstanding.\u201d), which map each text to a binary representation based on an LLM's evaluation. Instead of directly relying on LLMs to generate sets of these features, we leverage their language modeling capabilities to optimize feature sets that, when described in natural language and provided in-context to the LLM, enable accurate dataset reconstruction. To achieve this, we introduce an unsupervised automatic pipeline that generates a comprehensive set of potential features and uses a reconstruction-driven process to extract a subset that best captures the dataset's structure and semantics.\nTo evaluate our methodology, we construct synthetic extraction datasets from public data sources (Amazon Reviews, NYT, and DBPedia) (Hou et al., 2024; Sandhaus, 2008; Auer et al., 2007) in Section 5. Using associated class labels as ground truth, we demonstrate that our method outperforms prompting baselines by producing more accurate features both semantically and structurally. We further showcase our pipeline's versatility through two case studies (Section 6). First, we extract features representing LLM jailbreak tactics, creating compact representations that capture the efficiency and diversity of a larger set of human-crafted tactics (Jiang et al., 2024b). Second, in preference modeling, our pipeline identifies distinctive features within prompt"}, {"title": "2. Related Works", "content": "2.1. Unsupervised Feature Extraction\nTraditional approaches to interpretable data analysis have primarily relied on unsupervised techniques such as clustering, which are typically paired with natural language descriptions generated either through phrase extraction (Carmel et al., 2009; Treeratpituk & Callan, 2006; Zhang et al., 2018) or LLM-based methods (Sorensen & Others, 2024; Lam et al., 2024; Singh et al., 2023a). However, these methods face fundamental limitations due to their sensitivity to the number of selected clusters and their inability to accurately approximate complex cluster contents (Chang et al., 2009).\nMore recently, LLM-based methods have re-imagined feature discovery as a search over natural language hypotheses (Qiu et al., 2023), employing diverse strategies including de-duplication (Pham et al., 2023), optimization for minimal cluster overlap (Wang et al., 2023; Zhong et al., 2024), and human-guided feature selection (Viswanathan et al., 2023). While these advances have improved clustering-based approaches, they remain constrained by hyperparameter dependence and rigid cluster assignments. Our method overcomes these limitations through controlled feature sampling that enables simultaneous modeling of both broad patterns and fine-grained properties, without constraining the number of features that can be assigned to each text.\n2.2. Supervised Feature Extraction\nSupervised approaches to feature discovery have emerged as an alternative to unsupervised methods. Zhong et al. (2022) formulates this as a distribution comparison problem to identify distinguishing characteristics, an approach later extended beyond text (Dunlap et al., 2024) and adapted to accommodate user-specified exploration goals (Zhong et al., 2023). Different forms of supervision have also been explored: Findeis et al. (2024) leverages correlation analysis to identify features aligned with human preferences, while Benara et al. (2024) employs ridge regression for feature selection in medical prediction tasks.\nA parallel line of work explores Concept Bottleneck Models, which achieve interpretability by learning models over interpretable intermediate features (Koh et al., 2020). Recent advances have focused on automating the discovery of"}, {"title": "2.3. Dataset-Level Feature Extraction", "content": "At the dataset level, current approaches typically extract features by prompting LLMs with data samples to generate dataset descriptions (Singh et al., 2024a). While these descriptions can be refined through self-improvement loops (Pan et al., 2023; Gero et al., 2023), expert feedback (Templeton et al., 2024), or reconstruction-based optimization (Singh et al., 2024b), they remain limited to dataset-level insights without capturing properties of individual samples. Although prior work has explored more structured representations like binary tree decomposition with natural language splits (Singh et al., 2023b), our method uniquely generates semantically grounded features that enable both granular analysis and systematic comparison across the entire dataset."}, {"title": "3. Background Formalism", "content": "Before diving into our method, we define terms later used in the Section 4.\nBinary Predicate as a Feature. We define a feature $ \\phi$ as a binary predicate $[6] : X \u2192 {0,1}$, determined by a LLM serving as the valuation function. A feature set is a collection $ \\Phi = (\\phi_1,..., \\phi_K)$ of K such predicates.\nDataset Modeling. Let $D = {x^{(1)}, . . ., x^{(N)}}$ be a dataset of N texts that we treat as independent from each other. We evaluate these texts using a language model conditioned on their features, with the goal of finding a feature set $ \\Phi$ that minimizes the perplexity $PPL(D | \\Phi)$. For each text $x^{(n)}$, we compute its features $ \\phi(x^{(n)})$ and calculate the mean of per-text perplexities. We made this slight modification to standard perplexity to give each text same importance, preventing longer texts from dominating the metric:\n$PPL(D | \\Phi) = \\frac{1}{N} \\sum_{n=1}^{N} PPL(x^{(n)} | \\phi(x)).$"}, {"title": "4. Method", "content": "Our goal is to optimize a binary feature set $ \\Phi$ that minimizes $PPL(D)$ for dataset D, effectively identifying natural language features that enable LLMs to best reproduce the original text x. We assume that state-of-the-art models have acquired sufficient capability to perform this type of modeling, a claim that we later validate in Section 5.\nSince gradient-based optimization isn't feasible in the binary feature space, we implement a multi-stage pipeline that generates candidate features, removes duplicates through clustering, and iteratively selects the most effective features for dataset modeling. Detailed pseudocode for the whole pipeline is in Algorithm 1.\nGeneration Stage. In the first phase, we leverage an LLM to generate discriminative features by comparing each text in D against C randomly sampled texts from the dataset. We employ GPT-4o (Hurst et al., 2024) to generate K unique features per comparison, with C=5 and K=5 used across all subsequent experiments based on empirical tuning to optimize both diversity and generality. The complete feature generation prompt is provided in Appendix F.1.\nClustering and Evaluation Stage. Since assigning truth values to all features and evaluating $PPL(D | \\Phi)$ for many choices of $ \\phi$ is computationally intensive, we adopt strategies from Findeis et al. (2024). We vectorize features using OpenAI embeddings 2 and apply KMeans clustering, where we use a number of clusters equal to the number of texts in the dataset across all experiments, observing that this maintains feature diversity while eliminating duplicates. From each cluster, we randomly select one representative feature. For efficiency, we use GPT-4o to assign truth values to 10 features simultaneously per text x (prompt in Appendix F.2), and retain only features present in at least 5% of samples to focus on common patterns, though this threshold is adjustable.\nFeaturization Stage. In the final stage, we iteratively construct the feature set by selecting features that minimize the dataset's perplexity given the set of features. At each iteration, we identify the feature F that provides the lowest average perplexity gain when added to the current feature set $ \\Phi$:\n$PPL(D | \\Phi \u222a {F}) < PPL(D | \\Phi \u222a {F'}),$\nfor all other candidate features F'. The selected feature F is then added to $ \\Phi$.\nWe define the tokenization process for feature representation as follows: for a given feature space, we construct a natural language representation of the features by concatenating the names of all features $F_i$ that evaluate to true for a given text, separated by newline characters (\"\\n\"):\nTokenized Representation = $F_1$+ \u201c\\n\u201d + $F_2$ + \u201c\\n\u201d +...,\nBy doing so, we optimize the efficiency of the pipeline further by caching the log-probability for texts where a feature is false, avoiding redundant computations. To provide context to the model, we include a static prompt that describes"}, {"title": "5. Dataset Modeling Experiments", "content": "To validate our method, we test it on three datasets with known features to assess its feature reconstruction capability, before demonstrating practical downstream applications in Section 6. Our pipeline uses GPT-4o (Hurst et al., 2024) for feature proposal and evalidation, and Llama 3.1 8B Instruct (Dubey et al., 2024) for feature selection.\n5.1. Datasets\nWe utilize three publicly available labeled datasets:\nDBPEDIA. A semantic dataset derived from Wikipedia using RDF triples (Auer et al., 2007). We use a pre-processed subset with hierarchical class labels\u00b3, focusing on level 2 categories for reconstruction.\nNYT. The New York Times Annotated Corpus contains 1.8 million articles (1987-2007) with metadata (Sandhaus, 2008). We use the manually reviewed tags from the NYT taxonomy classifier, specifically focusing on articles under \"Features\" and \"News\" categories.\nAMAZON. This dataset comprises half a million customer reviews with item metadata (Hou et al., 2024). We focus on identifying high-level item categories (e.g., Books, Fashion, Beauty), excluding reviews labeled \"Unknown.\"\nFirst, we select entries between 100-10,000 characters to eliminate outliers and ensure sufficient content for analysis. For each dataset, we then construct three independent balanced subsets, each containing 5 classes with 100 samples per class. This methodology provides enough complex, diverse data to prevent class memorization while maintaining sufficient scale for method comparison. We report results averaged across these three trials per dataset.\n5.2. Metrics\nWe evaluate methods using three complementary metrics, each designed to measure the extent to which the discovered features capture the original underlying structure of the dataset:"}, {"title": "5.3. Experimental Setup", "content": "Baseline. We use GPT-4o to generate 50 features with temperature and top_p set to 1, using a random sample of 100 instances due to context limitations. Through prompt engineering, we found optimal results by directly instructing the LLM to generate topic-related features. The prompt is detailed in Appendix G.3, with additional results for non-topic-specific prompting in Appendix B.3.\nOur method. To assess our pipeline's components, we evaluate three stages, with detailed parameters in the Appendix E:\n\u2022 Generation: We generate proposed features using the generation-stage prompt with GPT-4o and randomly sample up to 50 features, representing the initial pipeline output.\n\u2022 Clustering: Features are clustered to remove redundancies, assigned truth values, and filtered to those occurring in at least 5% of texts. Up to 50 features are randomly sampled from different clusters.\n\u2022 Full pipeline: We apply the feature selection procedure to the clustered features through Llama 3.1 8B Instruct (prompt found in Appendix F.3), iteratively selecting up to 50 features that maximize dataset reconstruction, representing the complete pipeline."}, {"title": "5.4. Results", "content": "We present the complete results in Figure 2, with detailed numerical results in Appendix G.\nInsight 1: Our pipeline generates higher-quality structural and semantic features. Across datasets of varying complexity, our approach consistently outperforms standard"}, {"title": "Insight 2: Our pipeline achieves faster feature space convergence.", "content": "Our method reaches 95% peak performance with half the features needed by the baseline for both Class Coverage and Reconstruction Accuracy, as detailed in Appendix B.2."}, {"title": "Insight 3: Each pipeline stage enhances feature quality.", "content": "Each stage of the pipeline improves performance metrics, with the reconstruction-based feature selection stage being crucial for surpassing the baseline."}, {"title": "Insight 4: Performance scales with feature count.", "content": "All metrics improve with more features, which we explore further in Section 6."}, {"title": "Overall, we see that our method outperforms baselines based on prompting on the raw task of datasets modeling.", "content": "We next examine two case studies where dataset modeling has practical downstream applications, evaluating our method against both comparable baselines and expert-crafted features."}, {"title": "6. Application Case Studies", "content": "To illustrate the versatility of our proposed framework, we present two case studies: extracting compact representations of jailbreaks and identifying features for preference modeling. These applications demonstrate our method's broad applicability, and we discuss additional potential use cases in Section 7."}, {"title": "6.1. Extracting Compact Attacks from Jailbreaks", "content": "Automated red-teaming (ART) generates inputs designed to trigger harmful behavior in language models (Perez et al., 2022). To build robust safeguards, diverse attack examples are essential for resisting a wide range of harmful inputs (Wei et al., 2024). However, creating a small yet diverse and effective attack set remains challenging (Samvelyan et al., 2024; Hughes et al., 2024). In this case study, we show how dataset featurization can produce such a compact set.\nWe apply our method to the WildTeaming framework (Jiang et al., 2024b), a state-of-the-art approach that uses GPT-4 (Achiam et al., 2023) to extract 100k+ tactics from human jailbreaks in LMSYS-CHAT-1M (Zheng et al., 2023a) and WILDCHAT (Zhao et al., 2024). For analysis, we study the WildJailbreak dataset where Mixtral-8\u00d77B (Jiang et al., 2024a) or GPT-4 are given harmful queries and instructed to generate jailbreaks by combining 2\u20137 tactics sampled from 500 clusters (containing 85k total human jailbreak tactics).\nBy applying our pipeline to a subset of this dataset, we reduce the feature space by a factor of 25 while maintaining the diversity and effectiveness of the original human-crafted"}, {"title": "Feature Refinement.", "content": "To further enhance our results, we refine our features through dataset filtration. From 5000 adversarial, harmful prompts in WildJailbreak, we identify 536 prompts that elicit non-refusal responses from Llama"}, {"title": "Interpretability and Insights.", "content": "Our defined feature set offers improved interpretability through a more compact representation compared to WildTeaming's 500 distinct tactic clusters. Manual examination reveals that our original 50 features highlight broad narratives, scenario settings, and prompt structures common in adversarial attacks, while the 20 features from the Llama non-refusal subset emphasize engagement with safety norms and positive language use.\nOverall, our method successfully compresses WildTeaming's 500 clusters into just 20 features (a 25x reduction in feature space) while maintaining or improving performance across most metrics, particularly for robust models like GPT-4 and Llama 3.1 8B Instruct. This compact feature set provides better insights into adversarial strategies through a more manageable representation, with our results showing additional improvements are possible through targeted dataset filtering, suggesting that the method's effectiveness can be further enhanced by refining the initial featurization dataset."}, {"title": "6.2. Compositional Preference Modeling", "content": "The growing capabilities of LLMs necessitate their alignment with human preferences, primarily through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022). In RLHF, a preference model (PM) learns from human-ranked responses to score LLM outputs. However, training models to directly predict preference scores creates black-box systems prone to reward hacking and unintended biases (Gao et al., 2022).\nRecent approaches decompose rewards into interpretable features such as readability and correctness (Dorka, 2024; Wang et al., 2024). In this case study, we explore our pipeline's ability to identify such features in unsupervised manner, avoiding biases that can emerge when features are selected based on human reward signals. We compare our method to compositional preference models (CPMs) (Go et al., 2024), which validate responses against predefined features before training a linear regression to predict preferences. We selected their framework for its comprehensive evaluation and extensive feature set. Our approach eliminates manual feature engineering while achieving comparable performance with the same feature count and superior results with larger, automatically generated feature sets."}, {"title": "Generalizability.", "content": "To assess how well our model generalizes to other datasets, given that featurization and PM training occur on a single preference dataset, we follow the evaluation approach proposed by Go et al. (2024). They suggest using a well-established PM trained on diverse datasets, which should exhibit better generalization than single-dataset models. We use fine-tuned DeBERTa models as our references for HH-RLHF and SHP (Sileo, 2023) and plot their BoN scores against PMs trained with our features and the baseline features. According to Go et al. (2024), lower divergence from these comprehensively trained reference models indicates better generalization to unseen data."}, {"title": "Pair-wise Win-Rate.", "content": "Similarly to Go et al. (2024), we evaluate PM quality using pairwise win rates. We generate N responses to a prompt using Flan T5 Large and select the best response according to a trained PM. Then, we randomly select a second response from the remaining ones. To compare them, we use GPT-4 Turbo with an AlpacaEval (Li et al., 2023) prompt, evaluating both orderings and selecting the response with higher log probabilities. A strong PM should consistently select responses that are preferred over randomly chosen ones. Table 2 shows that our PM-selected responses match the baseline performance. However, unlike with accuracy, we did not observe clear improvements when additional features were included."}, {"title": "Overall, our unsupervised features matched the performance of state-of-the-art hand-crafted preference models (Go et al., 2024) across all metrics, with superior accuracy and generalizability.", "content": "Our approach enables easy generation of additional features to enhance performance, while maintaining interpretability as further explored in Appendix D.1."}, {"title": "7. Discussion, Limitations & Conclusion", "content": "We have introduced dataset featurization, a novel approach for extracting overlapping features from unsupervised text data that can effectively capture both broad patterns and fine-grained properties. Our multi-stage pipeline proposes potential features through individual text analysis, filters and deduplicates them via clustering, and iteratively selects features that help an LLM minimize perplexity over data samples, thereby improving its reconstruction ability. Beyond outperforming prompting in dataset modeling tasks, we demonstrated our method's versatility by compressing jailbreak attacks while maintaining their effectiveness and"}, {"title": "Limitations.", "content": "Our method is restricted to binary features and relies on positive feature instances during optimization, aligning with prior work (Dunlap et al., 2024; Zhong et al., 2024; Findeis et al., 2024) but limiting applicability to tasks requiring numeric attributes and hierarchical relationships. Additionally, while we leverage LLMs for in-context reasoning, fine-tuning them specifically for feature-based modeling could enhance feature selection."}, {"title": "Future Directions.", "content": "Our pipeline shows promise across scientific research, from social science to medical analysis (Tamkin et al., 2024; Singh et al., 2025; Wolf et al., 2018). For LLM safety, applications extend beyond jailbreaks to influence functions and steering vectors (Grosse et al., 2023; Subramani et al., 2022), while advancing our understanding of human preferences (Findeis et al., 2024; Li et al., 2024)."}, {"title": "9. Impact Statement", "content": "We envision dataset featurization as a valuable tool for developing interpretable analytics and visualizations across diverse research domains, including medicine, social sciences, and economics. Our case studies demonstrate its utility in two such areas: enhancing defensive techniques against adversarial attacks and developing more robust preference models. Improved capabilities to understand and detect large-scale attacks contribute to AI safety research, while advances in preference modeling help further our understanding of human values and their computational representation.\nHowever, like many analytical tools, this technology has potential dual-use implications. The method could be applied to tasks such as de-anonymization (Narayanan & Shmatikov, 2008), amplification of existing biases (Barocas & Selbst, 2016), or enhancing the spread of misinformation (Tufekci, 2014). These capabilities underscore the importance of developing appropriate governance frameworks and ethical guidelines for the deployment of such analytical tools.\nOur specific implementation presents several important considerations. While features may be interpretable within the LLM's context, they can become ambiguous or misleading when presented without proper human context, emphasizing that this tool should complement rather than replace human analysis. The stochastic nature of our method introduces potential convergence to local optima, possibly necessitating further validation through cross-validation across multiple runs, comparison with domain expert assessments, or evaluation across different initialization parameters to ensure robust analysis."}]}