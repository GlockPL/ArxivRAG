{"title": "Solid-SQL: Enhanced Schema-linking based In-context Learning for\nRobust Text-to-SQL", "authors": ["Geling Liu", "Yunzhi Tan", "Ruichao Zhong", "Yuanzhen Xie", "Lingchen Zhao", "Qian Wang", "Bo Hu", "Zang Li"], "abstract": "Recently, large language models (LLMs) have\nsignificantly improved the performance of text-\nto-SQL systems. Nevertheless, many state-of-\nthe-art (SOTA) approaches have overlooked the\ncritical aspect of system robustness. Our exper-\niments reveal that while LLM-driven methods\nexcel on standard datasets, their accuracy is no-\ntably compromised when faced with adversar-\nial perturbations. To address this challenge, we\npropose a robust text-to-SQL solution, called\nSolid-SQL, designed to integrate with vari-\nous LLMs. We focus on the pre-processing\nstage, training a robust schema-linking model\nenhanced by LLM-based data augmentation.\nAdditionally, we design a two-round, struc-\ntural similarity-based example retrieval strategy\nfor in-context learning. Our method achieves\nSOTA SQL execution accuracy levels of 82.1%\nand 58.9% on the general Spider and Bird\nbenchmarks, respectively. Furthermore, exper-\nimental results show that Solid-SQL delivers\nan average improvement of 11.6% compared to\nbaselines on the perturbed Spider-Syn, Spider-\nRealistic, and Dr. Spider benchmarks.", "sections": [{"title": "1 Introduction", "content": "Text-to-SQL serves as an automated tool that facil-\nitates the transformation of natural language into\nstructured query language (SQL) commands, en-\nabling individuals without specialized knowledge\nand skills to write SQL and query databases (Baig\net al., 2022). Traditional text-to-SQL techniques\nhave relied on rigid syntax tree templates (Xu et al.,\n2017; Guo et al., 2019; Wang et al., 2020) or su-\npervised fine-tuning of sequence-to-sequence mod-\nels (Xie et al., 2022; Scholak et al., 2021) for\nexecuting the transition from text to SQL. How-\never, the recent past has witnessed a surge in the\napplication of LLMs in text-to-SQL operations,\nproving their efficacy (Gao et al., 2024; Pourreza\nand Rafiei, 2023). State-of-the-art approaches that\ntop text-to-SQL leaderboards, such as Spider (Yu\net al., 2018) and BIRD (Li et al., 2023), leverage\nadvanced Large Language Models (LLMs) like\nGPT-4 (Achiam et al., 2023) for SQL generation.\nConsidering the role of text-to-SQL in sensitive\ndomains such as finance and healthcare, where sys-\ntem reliability and security are of critical impor-\ntance, the robustness of text-to-SQL systems is es-\nsential, which, however, has not received adequate\nconsideration in LLM-based text-to-SQL systems.\nA robust text-to-SQL system should have the ability\nto maintain the correct SQL output when faced with\nadversarial perturbations in the text or database (Pi\net al., 2022), such as changes in sentence structure,\nsynonym descriptions, etc. Experimental results\nreveal that leading LLM-based methods (Dong\net al., 2023; Wang et al., 2024; Li et al., 2024a) per-\nform poorly on benchmarks that aim at testing the"}, {"title": "2 Related Work", "content": "text-to-SQL robustnes, like Spider-Syn (Gan et al.,\n2021), Spider-Realistic (Deng et al., 2021) and\nDr.Spider (Chang et al., 2023). However, efforts to\nenhance text-to-SQL robustness (F\u00fcrst et al., 2024;\nShen et al., 2023; Zhuo et al., 2023) continue to be\ncentered around traditional sequence-to-sequence\narchitectures, with limited exploration of LLM-\nbased alternatives (Zhuo et al., 2023).\nAs illustrated in Figure 1, an LLM-based text-to-\nSQL system comprises three distinct stages: pre-\nprocessing, SQL-generation, and post-processing.\nThese stages are tasked with parsing the input\nquestion to synthesize effective prompts, querying\nthe LLM to produce SQL statements, and refin-\ning the generated SQL, respectively. In the SQL-\ngeneration stage, to achieve good results, the em-\nployed LLMs typically have a very large size or are\nclosed-source, making them difficult to fine-tune\nfor rubustness. The post-processing phase entails\nrefining the already generated SQL, with these re-\nfinements being independent of disturbances on\nthe input side. The pre-processing stage, in con-\ntrast, deals with disruptions originating from the\ntextual and tabular inputs. Therefore, handling in\nthe pre-processing stage is crucial for enhancing\nthe robustness of the LLM-based text-to-SQL sys-\ntems. Specifically, how to process the text and\nschema to obtain a prompt that can stabilize perfor-\nmance in the SQL-generation stage is the problem\nwe need to solve.\nIn this paper, to address the aforementioned is-\nsues, we design a robust pre-processing pipeline,\ncalled Solid-SQL, to generate prompts for SQL-\ngeneration. We craft the necessary pre-processing\nsteps based on the components required for the\nprompt. A SQL statement consists of two com-\nponents: first, the syntactic framework that deter-\nmines the structure and logic of the statement; and\nsecond, the database schema, which includes the\nspecific names of the tables and columns being\naccessed. To guide from both aspects, we aim to\ninclude pre-selected schemas and SQL statement\nexamples with similar structures within our prompt.\nFor robust schema selection, we specifically utilize\nLLMs to generate varied data for adversarial train-\ning and format training data specially for schema\nlinking tasks to fine-tune a language model, ad-\ndressing the lack of relevant datasets. To assist\nwith in-context learning, we design effective meth-\nods for extracting text and SQL skeletons based\non the chosen schemas and retrieve relevant SQL\nstatement examples based on the similarity of these\nskeletons. When constructing the prompt, we in-\ncorporate explicit attention mechanisms to stabilize\nthe output for inputs that have been perturbed.\nOur contributions are summarized as follows:\n\u2022 We address the existing gap in discussions\non the robustness of LLM-based text-to-SQL\nsystems. To address this, we propose Solid-\nSQL, a pre-processing pipeline designed to\nenhance the robustness of LLM-based text-to-\nSQL systems in generating SQL.\n\u2022 We design several effective modules, includ-\ning a robust schema-linking model, example\nretrieval methods, and an explicit attention\nmechanism. Moreover, we validate Solid-\nSQL's universality and applicability through\nits integration with various SQL-generation\nLLMs.\n\u2022 We conduct extensive experiments, demon-\nstrating that Solid-SQL achieves SOTA per-\nformance on general benchmarks and signif-\nicantly outperforms existing solutions on ro-\nbustness benchmarks. Additionally, the effec-\ntiveness of the modules is validated through\nablation studies."}, {"title": "2.1 LLM-based Text-to-SQL Techniques", "content": "In tandem with the rapid advancement and\nwidespread adoption of large language models\n(LLMs) across various natural language processing\n(NLP) domains, the text-to-SQL field has also seen\nsignificant benefits from recent methodological\nbreakthroughs involving LLMs, achieving notable\nperformance milestones. LLMs demonstrate im-\npressive zero-shot reasoning and domain general-\nization capabilities, contributing to unprecedented\nachievements on the cross-domain Spider leader-\nboard (Yu et al., 2018). For instance, C3 (Dong\net al., 2023) is a zero-shot text-to-SQL methodol-\nogy that enhances ChatGPT through three designs:\nClear Prompting for effective input; Calibration\nwith Hints to correct model biases; and Consistent\nOutput to ensure query reliability. The Chain-of-\nThought approach (Wei et al., 2022) has also been\napplied to text-to-SQL tasks. DIN-SQL (Pourreza\nand Rafiei, 2023) tackles the text-to-SQL task by\ndecomposing it into four modules: schema linking,\nquery classification and decomposition, SQL gen-\neration, and self-correction, each implemented us-\ning prompting techniques to leverage the granular"}, {"title": "2.2 Adversarial Robustness", "content": "capabilities of LLMs. Some methods further ex-\nplore LLM's ability of in-context learning. DAIL-\nSQL (Gao et al., 2024) has revitalized the SOTA\non Spider through a comprehensive examination of\nin-context learning, investigating the optimal selec-\ntion of examples and their proper organization in\nprompts within a few-shot scenario. Other research\nhas explored the selection of few-shot demonstra-\ntions by synthesizing in-domain examples (Chang\nand Fosler-Lussier, 2023) and retrieving question\nskeletons (Guo et al., 2023). Furthermore, MAC-\nSQL (Wang et al., 2024) and CHESS (Talaei et al.,\n2024) employ multi-agent collaboration for Text-\nto-SQL tasks. In addition to maximizing the ability\nto explore the LLMs without modifying it, other op-\ntions involve fine-tuning the model. Approaches in\nDAIL-SQL (Gao et al., 2024), DTS-SQL(Pourreza\nand Rafiei, 2024), and CodeS (Li et al., 2024b) aim\nto enhance the capabilities of open-source LLMs\nthrough supervised fine-tuning, striving to compete\nwith or surpass their larger, proprietary counter-\nparts.\nAlthough these methods have achieved impres-\nsive results on the leaderboards, only a few of them\nhave been evaluated for robustness (Li et al., 2024b;\nGao et al., 2024). Moreover, according to our exper-\niments, the performance of many in-context learn-\ning based methods on robustness benchmarks ap-\npears to be somewhat inferior compared to their\nperformance on the Spider and BIRD (Li et al.,\n2023) leaderboards.\nDespite the remarkable performance of neural net-\nworks across various domains, they continue to\nexhibit significant vulnerabilities when subjected\nto perturbations (Szegedy et al., 2014). This sus-\nceptibility is not only evident in traditional neural\nnetworks but has also been observed in systems\nsuch as text-to-SQL models (Shen et al., 2023),\nwhere adversarial inputs can lead to degraded per-\nformance. LLMs show potential as zero-shot text-\nto-SQL parsers, but their performance declines\nwhen faced with adversarial attacks and domain\ngeneralization disturbances, exhibiting varying lev-\nels of robustness in response to different types of\nperturbations (Zhang et al., 2023). It has been sub-\nstantiated that removing explicitly stated column\nnames (Deng et al., 2021) or replacing database\nschema-related content with synonyms (Gan et al.,\n2021) in the question will compromise the acces-\nsibility and accuracy of the generated SQL. Be-\nsides, confusion on the table side (e.g., substitut-\ning column descriptions or incorporating distract-\ning columns within the table) will further under-\nmine the precision of text-to-SQL systems (Pi et al.,\n2022). And for a holistic robustness assessment,\nDr. Spider (Chang et al., 2023), a diagnostic bench-\nmark encompassing 15,000 perturbed examples\nthat cover a multitude of perturbation types from\nthree perspectives: the database, natural language\nquestions, and SQL, has been unveiled.\nTo enhance the robustness of text-to-SQL sys-\ntems, several strategies have been employed, in-\ncluding manually adding synonym annotations to\nthe schema to provide a more precise descrip-\ntion (Gan et al., 2021), generating adversarial ex-\namples for adversarial training of the sequence-to-\nsequence model (Pi et al., 2022; Gan et al., 2021),\ndesigning specialized training frameworks (Deng\net al., 2021) and crafting innovative encoding strate-\ngies to transition from text to SQL (Shen et al.,\n2023).\nHowever, these methods either require signif-\nicant manual effort, are not suitable for new do-\nmains and large databases, or can just be applied\nto traditional encoder-decoder frameworks but not\non the currently popular LLMs with large-scale\nparameters. In contrast, our robustness strategy\nis compatible with LLMs and ensures that text-\nto-SQL systems utilizing this strategy perform no\nworse than SOTA methods on conventional bench-\nmarks, while markedly enhancing performance on\nrobustness evaluation benchmarks."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Definition", "content": "Text-to-SQL is a task that generates a SQL state-\nment for querying a database based on a natural\nlanguage text which is a demand for some infor-\nmation about the database. It can be represented\nas $S = M(Q, SC)$, where S is the generated SQL\nstatement, M is the text-to-SQL system, Q is the\nnatural language text, i.e., the question, and SC is\nthe database schema.\nOur robustness goal is to stabilize the output of\nthe text-to-SQL system when faced with pertur-\nbations. Specifically, without affecting the funda-\nmental goal of Q, adding perturbations to Q yields\nQ*. A robust LLM-based text-to-SQL strategy\nsatisfies $DB(M(Q,SC)) = DB(M(Q^*, SC'))$,\nwhere DB is the access database."}, {"title": "3.2 Overview", "content": "Solid-SQL is a novel plug-in solution for robust\ntext-to-SQL, which uses a carefully designed pre-\nprocessing pipeline to extract the elements re-\nquired for prompt composition. It includes a robust\nscheme linking model, an effective example re-\ntrieve method and an explicit attention mechanism.\nFigure 2 shows the pipeline of Solid-SQL.\nFirstly, we employ an LLM to introduce minor\nperturbations to the text of text-to-SQL training\ndata, while preserving its semantic integrity. In\nthis way, we create an augmented dataset of clean\nand perturbed data. This augmented dataset is then\nused to fine-tune a language model for schema\nlinking(Figure 2(A)). Subsequently, leveraging the\noutcomes of schema linking(Figure 2(B)), we ask\nan LLM to extract and remove domain-specific in-\nformation and value information from the input text\nquery, to derive the query's skeleton(Figure 2(C)).\nThis skeleton is matched against a pool of candi-\ndate skeletons to retrieve an appropriate number of\nrelevant samples as examples based on similarity.\nThe question, complete schema, and examples are\ncombined into a prompt, with explicit emphasis\non the filtered schema, to query the LLM for SQL\ngeneration in the first round(Figure 2(D)-Round 1).\nFollowing this, the SQL generated from the first\nround is parsed to extract its backbone, and addi-\ntional examples are retrieved for the second round"}, {"title": "3.3 Schema Linking", "content": "of SQL generation, culminating in the final SQL\noutput (Figure 2(D)-Round 2).\nThe schema linking task is a preliminary step of\ntext-to-SQL, simplifying the generation of SQL\nqueries. Its goal is to select the actual tables and\ncolumns to be accessed from the entire database\nschema based on a given question. For schema\nlinking, since the input contains numerous SQL\nstatements that define the database structure, it is\nchallenging for a base LLM with general capabil-\nities to understand such input and produce output\nin the expected format. Therefore, we fine-tune a\nmodel to complete the task."}, {"title": "3.3.1 Robust Data Enhancement", "content": "In order to improve the robustness of our schema\nlinking model, we first expanded the original text-\nto-SQL training dataset. The data in the training\nset is in the form of a triplet Q, SC, S, where Qis\nthe input text query, SC is the complete database\nschema, and S is the correct SQL query output.\nWe use an LLM to rewrite Q, including changing\nthe sentence structure and replacing synonyms (i.e.,\nsubstituting 'singer' with 'musician'), resulting in\nnew questions Q1 and Q2. Then, we add the new\ntriplets Q1, SC, S and Q2, SC, S to the training\ndataset. This expanded training set introduces per-\nturbations and adversarial examples, and the model"}, {"title": "3.3.2 Model Training", "content": "trained with this augmented dataset can effectively\nimprove its robustness.\nWe set the target operation to choose schema\nin Solid-SQL, as shown in Figure 2(B), formu-\nlated as {T,C} = G(Q, SC), where T =\n{T1, T2,...,T|7|} and C = {C1,C2,..., C|C|}\nare the selected tables and columns, respec-\ntively, and G is a fine-tuned generation model for\nschema linking. The absence of a training set for\nschema linking is problematic, and existing text-\nto-SQL training datasets offer data in the form of\n(Q, SC) \u2013 S pairs without providing the chosen\nschema. Therefore, we parse the SQL statement\nS related to question Q to obtain the ground truth\nT, C."}, {"title": "3.4 Relative Example Retrieval", "content": "In crafting prompts for LLMs, the selection of task-\nrelevant examples is paramount, as it enhances\nthe model's comprehension and task performance\nby leveraging contextual adaptability, knowledge\ntransfer, and ambiguity mitigation.\nVerified by DAIL-SQL (Gao et al., 2024), ex-\namples formatted as pairs consisting of a text\nquery and the corresponding correct SQL state-\nment benefit the in-context learning for the text-\nto-SQL task, rather than containing only the text\nor the SQL query. We define the example set\nas E = {E1, E2, ..., EN}, where each Ei corre-\nsponds to a question-SQL pair, denoted as (Qi, Si)."}, {"title": "3.4.1 Question Skeleton Similarity-Based", "content": "The correlation between two SQL statements, Si\nand Sj, suggests a corresponding relationship be-\ntween their associated questions, Qi and Qj. \u03a4\u03bf\nguide the LLM towards generating the desired SQL,\nit is reasonable to select analogous questions from\nthe candidate set as examples, based on the target\nquestion Q.\nHowever, the emphasis on similarity should fo-\ncus on the structural alignment of the SQL state-\nments rather than their thematic proximity. To\nachieve this, it is crucial to abstract the target ques-\ntion Q by removing domain-specific details and\nvalue-related content, revealing its core structure,\nor 'skeleton', denoted as Q*. Q* serves as the foun-\ndation for identifying analogous questions within\nthe candidate set, with a focus on those exhibit-\ning a similar structural pattern. By aligning the\nexamples with Q*, the model can more accurately"}, {"title": "3.4.2 SQL Skeleton Similarity-Based", "content": "identify the underlying patterns and relationships\nessential for converting natural language queries\ninto executable SQL commands. This method en-\nsures that the LLM's in-context learning is attuned\nto the structural intricacies critical for the task.\nTo derive Q* from Q, we leverage the language\nunderstanding capabilities of an LLM and employ a\nprompting-based technique. We input the question\nQalong with the schema inferred by our schema\nlinking model into a universal LLM and parse the\noutput to extract Q*. As shown in Figure 2(C),\nthis process obscures the domain-specific informa-\ntion and values, leaving only the question's skele-\ntal structure. This extraction process should be\napplied to both the questions Qi within the can-\ndidate library (Q1, S1), (Q2, S2), . . ., (Q\\E, S\\E)\nand the given target questionQ itself. Based\non the cosine similarity between Q* and the set\nQ1, Q*2, ..., Q*|E|, we can identify the top N\nmost similar candidate skeletons, corresponding to\nexample pairs (Q1, S1), (Q2, S2),..., (QN, SN).\nIn addition to indirectly utilizing questions to match\nthe samples to be retrieved, direct matching can\nalso be achieved through the similarity of SQL\nstatements. As shown in Figure 2(D), after the\nSQL generation is completed by the LLM in round\n1, we can select examples for the SQL generation\nin round 2 based on the similarity of the SQL skele-\ntons.\nWe extract the skeleton S* of an SQL statement\nS by identifying and manipulating its various com-\nponents using an SQL parsing tool\u00b9. This process\ninvolves parsing the SQL statement to generate a\nsyntax tree, then recognizing and replacing the ta-\nble names, column names, and values within it,\nwhile preserving the SQL keywords and logical\nstructure. Ultimately, a skeleton that contains only\nplaceholders and the structure of the SQL is pro-\nduced. This extraction process is applied to both\nthe candidate library and the generated SQL from\nround 1.\nWe employ the edit distance derived from the\nparse tree of an SQL skeleton to quantify the struc-\ntural similarity, which provides an analytical ap-\nproach that emphasizes the logical framework of\nSQL statements rather than their superficial textual\nsimilarities. This technique enables a more pre-\ncise identification of key element correspondences"}, {"title": "3.5 Information Utilization", "content": "than calculating cosine similarity of embedded vec-\ntors (Gao et al., 2024). It can also effectively dis-\ncounts disparities arising from diverse expressive\nforms or functional applications. Consequently, it\nfacilitates a more robust assessment of the concep-\ntual similitude within SQL statements.\nWe design suitable prompt templates to integrate\nexisting information and query LLMs accomplish\nSQL-generation effectively. A key consideration is\nthe use of an explicit attention mechanism to embed\ntable and column names filtered through schema\nlinking into the prompt. Existing approaches (Gao\net al., 2024) often present only the filtered schema\ninformation to the LLM, reducing token count\nand excluding unnecessary information. However,\nthis method has a significant drawback: if crucial\nschema information is omitted in the last step, the\nLLM will be unable to generate the correct SQL\nstatement. In contrast, we use \"focus on\" to em-\nphasize schema elements that are more likely to\nform the final SQL statement to the LLM. Thus\nthe model still has a comprehensive view of the\nentire schema information while understanding the\npriority, ensuring the stability and fault tolerance\nof the LLM in generating SQL statements when\nfaced with perturbation."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.1.1 Datasets", "content": "We evaluate the performance of Solid-SQL on a\nsimple clean test set called Spider, a difficult clean\ntest set called Bird, and three perturbed test sets.\nSpider (Yu et al., 2018) is a dataset for semantic\nparsing and text-to-SQL, created by Yale students.\nIt contains 10,181 questions and 5,693 SQL queries\nacross 200 databases and 138 domains.\nBird (Li et al., 2023) is a large-scale text-to-\nSQL benchmark developed by Alibaba DAMO\nAcademy. It features 12,751 question-SQL pairs,\n95 databases, and spans over 37 domains.\nSpider-Syn (Gan et al., 2021) is an adapted ver-\nsion of the Spider dataset with 5,672 questions\nmodified by replacing words with their synonyms,\nusing 273 synonyms and 189 phrases. On average,\nthere is one alteration per question.\nSpider-Realistic (Deng et al., 2021) is a per-\nturbed evaluation set based on Spider. It has"}, {"title": "4.1.2 Evaluation Metrics", "content": "been manually adjusted to remove explicit column\nnames while keeping SQL queries unchanged.\nDr. Spider (Chang et al., 2023) is a robustness\nevaluation benchmark for text-to-SQL models. It\napplies perturbations to the database, natural lan-\nguage query, and SQL components, and contains\n15,000 pre- and post-perturbation examples.\nWe assess the performance of the text-to-SQL\nmodel by evaluating the quality of the generated\nSQL. Execution Accuracy (EX) is defined as the\nproportion of questions in the evaluation set for\nwhich the execution outcomes of both the predicted\nSQL queries and the ground-truth queries are the\nsame. It is calculated relative to the total number of\nqueries. The EX of the generated SQLs indicates\nhow well the model meets the availability and pre-\ncision requirements in real-world scenarios. We\nalso use Exact Match Accuracy (EM) as an adjunct.\nEM is the portion of generated SQLs that totally\nmatch the ground truth SQL statements."}, {"title": "4.1.3 LLMS", "content": "Solid-SQL employs a prompting methodology that\nsupports the use of various interchangeable LLMs.\nTo validate the compatibility and generalizability\nof our proposed solution, we conducted experi-\nments using four distinct LLMs for SQL genera-\ntion. These models included both open-source and\nclosed-source options.\nLLama3-70b: An open-source LLM with 70 bil-\nlion parameters by Meta AI, optimized for diverse\nNLP tasks including text generation and transla-\ntion.\nDeepseek-coder-33b-instruct: A 33-billion-\nparameter model from the Deepseek Coder series,\nleading in open-source code generation across mul-\ntiple programming languages.\nGPT-40-mini: A compact version of GPT-40,\nretaining core text capabilities with faster inference\ndue to fewer parameters.\nGPT-4: OpenAI's advanced generative pre-\ntrained transformer, adept at complex tasks like\nessay writing and coding with high accuracy and\ncreativity."}, {"title": "4.1.4 Baselines", "content": "We compare with other prompting-based text-to-\nSQL solutions which have SOTA performances on\nSpider and Bird.\nDAIL-SQL (Gao et al., 2024): Ranked second\non the Spider leaderboard, DAIL-SQL employs a"}, {"title": "4.1.5 Solid-SQL Details", "content": "variety of example selection methods and a struc-\ntured format for example organization. Leveraging\nGPT-4, it achieves high performance in SQL gener-\nation quality and query efficiency.\nMAC-SQL (Wang et al., 2024): This method\nfeatures a core decomposer agent for Text-to-SQL\nwith few-shot chain-of-thought reasoning, sup-\nported by two auxiliary agents for obtaining sub-\ndatabases and refining SQL queries. The agents\nwork in tandem, with the flexibility to integrate\nnew tools or features for enhanced Text-to-SQL\nparsing.\nDIN-SQL (Pourreza and Rafiei, 2023): DIN-\nSQL breaks down the text-to-SQL task into\nsub-problems: schema linking, query classifica-\ntion & decomposition, SQL generation, and self-\ncorrection. Utilizing prompting techniques, it\ndemonstrates that LLMs can effectively solve these\nsub-problems when appropriately decomposed.\nCodeS (Li et al., 2024b): CodeS is an open-\nsource language model series designed for text-\nto-SQL, offering high accuracy with smaller pa-\nrameters compared to closed-source LLMs. It\nemploys an incremental pre-training strategy on\na SQL-specific corpus and addresses schema link-\ning and domain adaptation challenges. Evaluations\nshow CodeS achieves state-of-the-art performance"}, {"title": "4.2 Overall Performance", "content": "on multiple text-to-SQL benchmarks.\nIn the deployment of Solid-SQL, we employ the\nLLama3-8B-Instruct model as the foundational ar-\nchitecture for the schema linking task. The model\nis subjected to a training regimen consisting of five\nepochs on an augmented dataset, which comprises\napproximately 22,000 question-SQL pair instances.\nFor the extraction of question skeletons, we use\nthe LLM align with SQL-generation. Furthermore,\nwhen assessing the cosine similarity between two\nquestion embeddings, we employ the bge-large-en-\nv1.5 embedding model before the computation.\nWe test the performance of Solid-SQL across var-\nious benchmarks and compared it with the base-\nlines.\nTable 1 presents the performance of Solid-SQL\non the Spider benchmark as well as its robustness\ntest variants, Spider-syn and Spider-realistic. Lever-\naging the prompting-based nature of Solid-SQL\nand the compared baseline methods, we imple-\nmented a plugin-style approach to substitute var-\nious LLMs for SQL generation. Solid-SQL sig-\nnificantly outperforms the baselines in both execu-\ntion accuracy (EX) and exact match (EM) across"}, {"title": "4.3 Ablation Study & Hyper-parameter Study", "content": "all datasets, with an average execution accuracy\n12.4% higher than the baselines and an average\nexact match that also exceeds the baseline. It is\nnoteworthy that certain methods exhibit a strong\ndependency on specific LLMs, seeing that DAIL-\nSQL's performance decreases heavily on LLama\ncompared with GPT series and DIN-SQL is even\nunable to output reasonable SQL statements when\nusing Deepseek and Llama. In contrast, Solid-SQL\nperforms well on all the test LLMs, showing versa-\ntility and compatibility.\nTable 2 demonstrates the execution accuracy of\nSolid-SQL when generating SQL queries under\nvarious levels and types of perturbations within the\nDr.Spider dataset. Due to MAC-SQL's most stable\nperformance in cooperation with different LLMs\namong the baselines in Table 1, we choose it as the\nobject of comparison. The results clearly show that"}, {"title": "4.3.1 Schema Linking Training", "content": "our Solid-SQL approach significantly outperforms\nMAC-SQL and matches the current state-of-the-art\nmodel in robustness, CodeS-15B, which has been\nfine-tuned with extensive data. Additionally, it is\nevident that Solid-SQL has a distinct advantage\nin terms of robustness against perturbations that\ninvolve the use of synonyms.\nTable 3 displays the experimental results on the\nBird benchmark, which similarly demonstrates the\nconsistent performance of Solid-SQL under com-\nplex requirements.\nTable 4 presents the results of an ablation study on\nschema linking training. Compared to the baseline\nmodel without supervised fine-tuning (SFT), the\nmodel fine-tuned with the basic training set shows\na significant improvement of approximately 25%\nin the accuracy of column name choosing, under-\nscoring the necessity and efficacy of SFT. More-\nover, employing a robustness-enhanced augmented\ntraining set with added perturbations further im-\nproves the accuracy of schema linking, especially\non perturbed benchmarks such as Spider-Syn and\nSpider-Realistic, with an enhancement of about 2%,\nhighlighting the contribution to the robustness of\nschema linking."}, {"title": "4.3.2 Explicit Attention in Prompt", "content": "Table 5: The ablation study of the design of explicit\nattention in prompt construction. Values in the table are\nthe EX of SQL generated by one-round Solid-SQL with\nLlama3-70b. N is the number of examples retrieved\nfor in-context learning. \"with focus\" and \"w/o focus\"\nreferring to have the design or not, respectively.\nTable 4 presents the results of an ablation study\nof the explicit attention mechanism. The study ex-\namines how the existence of it in prompts impacts\nSQL-generation. The study reveals that incorpo-\nrating this mechanism significantly improves SQL\nexact match accuracy across three datasets, espe-\ncially when dealing with synonym perturbations,\nas seen in Spider-Syn. Additionally, it is notewor-\nthy that the positive effect of this design is more\npronounced when the number of examples in the\nprompt is reduced. This suggests that in scenarios\nwhere conserving tokens is crucial, our design can\neffectively enhance the performance of LLMs in\ngenerating SQL queries."}, {"title": "4.3.3 Number of Retrieved Examples", "content": "Table 6 presents a study on the optimal number\nof examples to include in prompts for in-context\nlearning. Although performance across various\nbenchmarks varies with different numbers of exam-\nples, there is a general trend where the ability of\nLLMs to generate SQL queries initially strength-\nens and then weakens as the number of examples\nincreases. Based on the average performance, we\nultimately set the number of examples, denoted as\nN, to be recalled to 7."}, {"title": "5 Limitations", "content": "The Solid-SQL approach offers opportunities for\nenhancement, particularly in the procedural design.\nWe could define conditions for advancing to a sec-\nond round of queries, which would streamline the\nprocess by eliminating unnecessary steps and in-\ncrease algorithmic efficiency. Furthermore, the"}]}