{"title": "ABPT: Amended Backpropagation through Time with Partially Differentiable Rewards", "authors": ["Fanxing Li", "Fangyu Sun", "Tianbao Zhang", "Danping Zou"], "abstract": "Using the exact gradients of the rewards to directly optimize policy parameters via backpropagation-through-time (BPTT) enables high training performance for quadrotor tasks. However, designing a fully differentiable reward architecture is often challenging. Partially differentiable rewards will result in biased gradient propagation that degrades training performance. To overcome this limitation, we propose Amended Backpropagation-through-Time (ABPT), a novel approach that mitigates gradient bias while preserving the training efficiency of BPTT. ABPT combines O-step and N-step returns, effectively reducing the bias by leveraging value gradients from the learned Q-value function. Additionally, it adopts entropy regularization and state initialization mechanisms to encourage exploration during training. We evaluate ABPT on four representative quadrotor flight tasks. Experimental results demonstrate that ABPT converges significantly faster and achieves higher ultimate rewards than existing learning algorithms, particularly in tasks involving partially differentiable rewards.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has shown great success in training control policies and decision-making models, such as quadruped locomotion [Kohl and Stone, 2004], mobile robots [Smart and Kaelbling, 2002], and even e-games [Vinyals et al., 2019; Hafner et al., 2023]. Despite its adaptability, RL relies on zero-order gradient (ZOG) approximations [Sutton and Barto, 2018] which require extensive sampling or replay mechanisms, which often results in slow convergence and sub-optimal training outcomes. As a promising approach, differentiable physics is considered to be able to accelerate training by providing first-order gradient (FOG) [Mozer, 2013] with respect to policy parameters. Recent research [Xu et al., 2022] has made significant progress in overcoming inherent drawbacks including the tendency to be trapped in local minima or training instability resulting from non-smooth transition in contact-rich environments, which has been demonstrated in several RL benchmarks.\nRecent studies on robotics demonstrate that directly using first-order gradients for policy learning achieves faster convergence and superior performance compared to traditional RL algorithms in applications such as quadrotors [Zhang et al., 2024; Wiedemann et al., 2023], robotic arms [Lv et al., 2023], and quadruped dogs [Song et al., 2024]. Among these, quadrotors stand out as an ideal platform for policy learning via differentiable physics. This is largely due to their simple and smooth aerodynamics in the flight.\nHowever, using first-order gradients for training requires not only the state transition function but also the reward function to be differentiable. While this condition is manageable for simple tasks, it becomes highly challenging for complex task objectives. Reward functions in such scenarios often involve non-differentiable components, such as conditional constants or binary rewards (e.g., awarding points upon task success), which violate differentiability requirements. These non-differentiable elements disrupt the computation graph during backpropagation-through-time (BPTT), leading to biased first-order gradients. Notably, while first-order gradients remain unbiased under purely non-smooth dynamics [Zhang et al., 2023; Suh et al., 2022], they become biased when using non-differentiable rewards\u2014a phenomenon we term Biased Gradient. This bias misguides training, causing optimization to stall in local minima and deviate from the intended direction of improvement.\nWe propose an on-policy actor-critic approach - Amended Backpropagation-through-Time (ABPT) - to mitigate the bias gradient issue introduced by the non-differentiable rewards while keeping high policy learning performance in terms of training speed and converged rewards. Our approach combines 0-step returns with N-step returns [Sutton and Barto, 2018], leveraging value gradients generated by the 0-step returns to balance first-order gradient accuracy and exploitation. Additionally, ABPT incorporates entropy to counteract the excessive aggression of first-order gradient. It also employs a replay buffer to store state experiences, initializing episodes with these states to enhance sampling efficiency.\nWe evaluate our method on four representative quadrotor tasks, comparing it against classic policy gradient and first-order gradient methods. These tasks are designed to progressively increase the reward non-differentiability, testing the adaptability of each approach. Experimental results demonstrate that ABPT achieves the fastest convergence and highest final rewards across all baselines. This superiority is attributed to ABPT's ability to compensate for biased gradients and enhance exploration via entropy regularization and state replay. Furthermore, ABPT exhibits robustness across varying learning rates and reward structures.\nOur technical contributions are summarized as follows:\n\u2022 We propose ABPT, a novel approach to address the challenges in first-order gradient learning, including biased gradients caused by non-differentiable rewards and susceptibility to local minima.\n\u2022 We provide a comprehensive analysis of ABPT's effectiveness, offering insights to advance differentiable physics-based learning methods.\n\u2022 We introduce four representative quadrotor tasks as novel benchmark tests for evaluating different learning approaches."}, {"title": "2 Related Work", "content": "Though first-order gradients enables faster and more accurate gradient computation, accelerating policy training via BPTT [Mozer, 2013], they suffer from gradient explosion/vanishing or instability caused by non-smooth dynamics. Many attempts try to address these issues and strengthen robustness. PODS [Mora et al., 2021] leverages both first- and second-order gradients with respect to cumulative rewards. SHAC [Xu et al., 2022] employs an actor-critic framework, truncates the learning window to avoid vanishing/exploding gradients, and smooths the gradient updates. AOBG [Suh et al., 2022] combines ZOG (policy gradient) with FOG, using an adaptive ratio based on gradient variance in the minibatch to avoid the high variance typical of pure FOG in discontinuous dynamics. AGPO [Gao et al., 2024] replaces ZOG in mixture with critic predictions, as Q-values offer lower empirical variance during policy rollouts. While both AGPO and AOBG converge to asymptotic rewards in significantly fewer timesteps, the mixture ratio requires excessive computational resources, leading to longer wall-time.\nAmong these algorithms, SHAC [Xu et al., 2022] achieves training efficiency comparable to the naive first-order approach (BPTT). However, SHAC primarily focuses on challenges related to dynamics and overlooks the gradient bias introduced by partially differentiable rewards, which are common in real-world quadrotor tasks. These tasks often involve differentiable dynamics but rely on discrete or sparse reward signals, such as waypoint reaching events or minimum-time completions. Our approach directly tackles this limitation by introducing a 0-step return to address gradient bias from non-differentiable rewards and incorporating entropy to encourage exploration in challenging scenarios. The results highlight our method's superior performance in quadrotor applications, achieving the fastest convergence and the highest final rewards among all the evaluated methods."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Preliminaries", "content": "The goal of reinforcement learning is to find a stochastic policy \u03c0 parameterized by \u03b8 that maximizes the expected cumulative reward or the expected return over a trajectory \u03c4, namely\n$\\arg \\max_{\\theta} J_{\\theta} [\\tau] = \\mathbb{E}_{\\pi_{\\theta}} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t)]$,                                                                                       (1)\nwhere $R$ represents the reward function. The expected return starting from a given state $s$, also named as a state value function, is usually approximated by some parameters $\\phi$,\n$V_{\\phi}(s) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [\\tau | s_0 = s]$.                                                                                                             (2)\nThe action-value function $Q_{\\phi}(s, a)$ indicates the expected return after taking action $a$, starting from state $s$. The state-value function $V_{\\phi}(s)$ can be expressed by the expected $Q(s, a)$ over the action space:\n$V_{\\phi}(s) = \\mathbb{E}_{a \\sim \\pi_{\\theta}} [Q_{\\phi}(s, a)]$.                                                                                                                               (3)\nIn a common actor-critic pipeline, both the actor ($\\pi_{\\theta}$) and the critic ($V_{\\phi}(s)$ or $Q_{\\phi}(s, a)$) are approximated by neural networks. The key problem is how to estimate the actor gradients to optimize the expected return. The methods could be divided into two following categories:\nPolicy Gradient. Policy gradient deploys stochastic policies to sample trajectories conditioned on the policy's action probabilities. By mathematical derivation, it estimates the expected returns via log-probability. Given a batch of experience, policy gradient is computed by:\n$\\nabla_{\\theta} J_{\\theta}^{[0]} = \\frac{1}{|B|} \\sum_{\\tau \\in B} [\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) A_{\\theta}(s_t, a_t)]$,                                                                             (4)\nwhere $A_{\\theta}(.)$ represents the advantage derived from the value functions using current policy, $B$ denotes the minibatch of sampled trajectories, $\\tau$ represents a trajectory within the minibatch. Because this process does not involve differentiable simulation, it is also named zeroth-order gradient (ZOG).\nValue Gradient. Value gradient focuses on gradient computation through the action-value function:\n$\\nabla_{\\theta} J_{\\theta} = \\frac{1}{|B|} \\sum_{i=1}^{|B|} \\nabla_{\\theta} V_{\\phi}(s^i, \\pi_{\\theta}(s^i))$.                                                                                                           (5)\n[Gao et al., 2024] named this gradient estimator as Q gradient (QG). Compared to ZOG, the performance of value approximation is extremely crucial to actor training. This is because QG directly relies on the back-propagation of the action-value function, while ZOG firstly estimates advantages among values according to current policy that enables actor training more robust to the critic."}, {"title": "3.2 First-Order Gradient & Gradient Bias", "content": "Given the state dynamics $T$ and reward function $R$ being differentiable, one can compute the extract gradients of the expected return for policy learning via backpropagation through time. This exact gradient estimate is called as first-order gradient (FOG):\n$\\nabla_{\\theta} J_{\\theta} = \\mathbb{E} [\\sum_{t=0}^{N-1} \\sum_{k=0}^{N-1} \\gamma^{k} \\nabla_{\\theta} R(s_{t+k})]$.                                                                                         (6)"}, {"title": "3.3 The Proposed Method", "content": "As previously discussed, explicit use of first-order gradients for policy learning requires addressing gradient bias caused by non-differentiable rewards. Motivated by the value gradient method, we propose to combine the 0-step return with N-step return for policy learning. Our method, Amended BackPropagation-through-Time (ABPT), is an on-policy actor-critic learning approach. An overview is presented in Figure 2.\nDuring each training batch, we collect |B| trajectories with a horizon length N and optimize the following objective function to update the actor network parameters \u03b8:\n$J_{\\theta} = \\frac{1}{2|B|} \\sum_{i=1}^{|B|} (J_N + J_0)$                                                                                                                         (9)\nwhere $J_N$, $J_0$ are N-step return and 0-step return, defined as\n$J_N = \\sum_{k=0}^{N-1} (\\gamma^k R(s_{t+k})) + (1 - d) \\gamma^{N} V_{\\phi}(s_{t+N})$                                                                                     (10)\nand\n$J_0 = V_{\\theta_{\\phi}}(s_t)$                                                                                                                                           (11)\nrespectively. Here, d is a boolean variable indicating whether the current episode has ended, and i denotes the trajectory index. Given the trajectory is perform by $\u03c0_\u03b8$, each element is differentiable to parameter \u03b8. $G_{t:t+N}$ represents the accumulated reward within the horizon and $V_{\\theta_{\\phi}}^{t+N+1}$ is the value obtained by fixed critic.\nWe explain how using the objective function (9) is equivalent to combining both the value gradient and the first-order gradient for backpropagation. Supposed the value function $Q_\\phi$ is well trained, the accumulated reward within the horizon can be approximated as:\n$G_{t:t+N} \\approx V_\\phi^{O_{\\phi}} - (1 - d) \\gamma^{t+N+1}$.                                                                                                                   (12)\nIts value gradient is then given by\n$\\nabla_\\theta V_{\\phi^{[q]}} G_{t;t+N} = \\gamma_0 \\nabla_\\theta V_{\\phi^{[q]}} - (1 - d) \\gamma^{N} \\nabla_{\\theta} V_{\\phi}+N+1$                                                                                                        (13)\nregardless of the differentiability of the rewards. Noting that, unlike [Xu et al., 2022], we specifically use action-value function $Q$ to compute the value to ensure $G_{0:t+N}$ is differentiable with respect to \u03b8, which makes this derivative expression meaningful, otherwise the derivative would be zero if using $V_0$ solely with state input. Let $\\nabla_{\\theta_{\\phi^{[q]}}} G_{t;t+N}$ denote the first-order gradient of the accumulated reward. The average of the two gradients can be expressed as:\n$\\nabla_{\\theta_{\\phi^{[q]}}} V^{0_{t+N}} = \\frac{1}{2} (\\gamma_0 \\nabla_{\\theta_{\\phi^{[q]}}} + \\nabla_{\\theta_{\\phi^{[q]}}} G_{t;t+N})$                                                                                                              (14)\nIt is straightforward to verify that taking the derivative of (9) yields the following gradient for backpropagation:\n$\\nabla_{\\theta} J_{\\theta} = \\frac{1}{|B|} \\sum_{i=1}^{|B|} [\\gamma_0 \\nabla_{\\theta_{\\phi^{[q]}}} + \\nabla_{\\theta_{\\phi^{[q]}}} G_{t;t+N} + (1 - d) \\gamma^{N} \\nabla_{\\theta} V_{\\phi_N}]$                                                                      (15)\nTherefore, the difference between this gradient and the gradient (7) used in [Xu et al., 2022] is that the first-order gradients in (7) are combined with the value gradients. By leveraging this combination, our method remains effective in guiding"}, {"title": "4.4 Reward Robustness", "content": "Designing an appropriate reward function is highly challenging for real-world applications, particularly when dealing with specific requirements. Ensuring robustness to reward architecture is crucial for the training algorithms. In the racing task, we redefined the reward function by replacing Euclidean distance with approaching velocity in the reward.As shown in Figure 8, ABPT outperforms other methods with both position-based and velocity-based rewards. With fewer non-differentiable components, velocity-based rewards allow ABPT and SHAC to pass more gates per episode, while BPTT fails due to gradient issues."}, {"title": "5 Ablation Study", "content": "We evaluate the effectiveness of key components of our approach by removing each during training. As shown in Figure 7, the results show that: 1) Incorporating 0-step return clearly improves the training performance in tasks with non-differentiable rewards such as landing and racing. 2) Adding entropy regularization (see Equation (20)) helps escape from the local minima, achieving higher converged rewards, particularly in landing and racing tasks. 3) Initializing episodes from previously visited states stored in the buffer enhances sampling efficiency, accelerating convergence, as observed in the first two tasks."}, {"title": "6 Discussion", "content": "[Gao et al., 2024] addresses the limitations of first-order gradients, such as bias under discontinuities and high variance in stiff contacts, by adaptive mixing policy gradients and first-order gradients at every time step. While probably effective in reducing the gradient bias from non-differentiable rewards, this method incurs significant computational overhead, limiting its practicality. In contrast, we mitigate the gradient bias by simply averaging the value gradient and the first-order gradient. This approach is far more efficient, requiring minimal overhead for evaluating the value gradients only at the initial and the last time steps.\nWe also explored incorporating k-step value functions (k = 0,..., N \u2013 1) at each step within a finite horizon, following a similar approach to [Suh et al., 2022]. However, this led to significant fluctuations in the learning curves."}, {"title": "7 Conclusion", "content": "We present a novel approach to training policies for quadrotor tasks robustly. It effectively addresses several challenges associated with existing first-order gradient learning methods. Firstly, to address the non-differentiable rewards that cause bias in gradient backpropagation, we propose to combine the 0-step return with the N-step return in the objective function. This is equivalent to combining the value gradients with the first-order gradients for backpropagation, mitigating the bias issue. Secondly, we incorporate entropy regularization and re-sampling mechanisms to initialize visited states from the buffer to increase the randomization and encourage exploration during training.\nWe validated ABPT on four quadrotor tasks - hovering, tracking, landing, and racing -and compared them with existing learning algorithms. The results show that ABPT achieves faster and more stable training processes and converges higher rewards across all tasks. ABPT is also robust to the learning rate and different kinds of rewards. The ablation study also shows the effectiveness of each key component of our approach."}]}