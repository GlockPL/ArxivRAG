{"title": "Towards Understanding Unsafe Video Generation", "authors": ["Yan Pang", "Aiping Xiong", "Yang Zhang", "Tianhao Wang"], "abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize high-quality output. It is important to understand their potential to produce unsafe content, such as violent or terrifying videos. In this work, we provide a comprehensive understanding of unsafe video generation.\nFirst, to confirm the possibility that these models could indeed generate unsafe videos, we choose unsafe content generation prompts collected from 4chan and Lexica, and three open-source SOTA VGMs to generate unsafe videos. After filtering out duplicates and poorly generated content, we created an initial set of 2112 unsafe videos from an original pool of 5607 videos. Through clustering and thematic coding analysis of these generated videos, we identify 5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. With IRB approval, we then recruit online participants to help label the generated videos. Based on the annotations submitted by 403 participants, we identified 937 unsafe videos from the initial video set. With the labeled information and the corresponding prompts, we created the first dataset of unsafe videos generated by VGMs.\nWe then study possible defense mechanisms to prevent the generation of unsafe videos. Existing defense methods in image generation focus on filtering either input prompt or output results. We propose a new approach called Latent Variable Defense (LVD), which works within the models internal sampling process. LVD can achieve 0.90 defense accuracy while reducing time and computing resources by 10\u00d7 when sampling a large number of unsafe prompts. Our experiment includes three open-source SOTA video diffusion models, each achieving accuracy rates of 0.99, 0.92, and 0.91, respectively. Additionally, our method was tested with adversarial prompts and on image-to-video diffusion models, and achieved nearly 1.0 accuracy on both settings. Our method also shows its interoperability by improving the performance of other defenses when combined with them. We will publish our constructed video dataset and code\u00b2.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, video generation models (VGMs) [2], [5], [14], [17], [19], [54]\u2013[56] have improved significantly and can generate coherent and high-quality videos encompassing a wide variety of themes. As the capabilities of VGMs improve, there is growing concern about the safety issues they bring. For example, the \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201d3 from the White House emphasizes \"testing and safeguards against discriminatory, misleading, inflammatory, unsafe, or deceptive outputs\" in Section 10; In April 2024, NIST released the AI Risk Management Framework, which explicitly tries to understand the ability of generative models to synthesize unsafe content. We noticed on websites like Civitai\u2074 that users can share their prompts and generated content. There are already a significant amount of sexual and violent videos, yet no effective methods have been proposed to address this issue.\nTo examine the capacity of VGMs to generate unsafe videos, we use prompts collected from the 4chan and Lexica websites [35], [41]; These datasets were originally used to guide text-to-image models in generating unsafe images. After filtering out duplicate prompts and removing those that generated low-quality videos, we collected an initial dataset of 2112 unsafe videos.\nTo verify these generative videos are indeed unsafe and cause unpleasant feelings, with IRB approval, we recruited online study participants to rate those videos. Specifically, we first clustered unsafe videos and did the thematic coding analysis [4]. Through two rounds of discussions, we identified five main categories of unsafe videos: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. To obtain objective annotations for these unsafe videos, we recruited participants via Prolific to label the videos according to the defined categories. We initially recruited 600 participants. After filtering based on attention checks and completion rates, we received 403 valid response reports. Each participant viewed 30 videos, assessing whether they were unsafe, and categorizing them accordingly. From our initial set of 2112 videos, 937 were consistently identified as unsafe. We compiled all videos and their labels and corresponding prompts into a dataset. This is the first dataset of unsafe videos generated by VGMs.\nGiven this dataset, we are then intrigued to understand whether we can defend against unsafe generation in VGMs. That is, can we ensure VGMs will never generate unsafe content? Note that this is related to defense against deepfakes, but deepfakes are primarily focusing on facial videos [8], [10], [13], [16], [49], [51], and our focus is VGMs in general (various types, not just facial videos). There are existing solutions [3], [11], [12], [20], [23]\u2013[25], [29], [35], [38], [41] for image generative models that addressed the unsafe generation problems. For example, Schramowski et al. [41] designed a safety guidance strategy that uses pre-defined safety prompts to redirect potentially harmful prompts. Then, Gandikota et al. [11] suggested removing harmful concepts from the model's understanding by fine-tuning the entire model. Li et al. [25] noted the text-dependency of previous methods and proposed removing unsafe visual representations to protect generated content. Qu et al. [35] implemented a detection model to evaluate the output content without interfering with the generation process. However, detecting unsafe content in videos is much more challenging than in images because videos contain more information (both spatial and temporal) and require significantly more computational resources to generate. Therefore, we need to rethink methods for detecting unsafe content in the video domain.\nAs detailed in Section IV, we categorize existing solutions for image generative models as either model-free (no need to look into model internals) or model-write (need to modify model parameters or settings) approaches. These two types of methods can only employ the final results of the diffusion process, limit further defensive actions, or require significant computational resources to update model parameters. Nevertheless, we propose a middle-ground, model-read approach that leverages \"read access\" of the diffusion model and detects unsafe content within the diffusion process. The model-read method is less rigid than the model-free approach, which only detects the model's final output. It also avoids the extensive time and computational resources needed for the model-write approach. We call our method Latent Variable Defense (LVD). LVD leverages the intuition that generative models, such as VAE [22] and diffusion models [18], are trained to learn latent space representation, and in these representations, samples that are close in the latent space result in similar generated content [22]. As shown in Figure 1, we establish classifiers to analyze the intermediate results of the diffusion process, which is quite different from the existing model-free methods. When we suspect the result will be unsafe, we can terminate the diffusion process early, saving significant computation resources. All Experimental results on three SOTA VGMs show that compared to working with the final result, our approach can save up to 10\u00d7 computation time, while achieving comparable detection accuracy (around 92%).\nContributions: We make the following contributions.\n\u2022 We demonstrated that VGMs have a strong capability to synthesize unsafe content.\n\u2022 We construct an unsafe generated video dataset from the SOTA open-sourced VGMs. Unsafe categories were generated using data-driven methods. Data annotations were completed by 403 participants recruited through Prolific.\n\u2022 Based on our unsafe video dataset, we proposed LVD to mitigate generating unsafe videos. LVD uses DDIM characteristics to detect unsafe content during inference time.\n\u2022 We tested our defense on three open-source SOTA VGMs, conducting comprehensive experiments to assess the defense performance under different parameter settings. Results showed our defense can achieve nearly 100% accuracy on all three models. We further assessed our defense's robustness, generalization ability, and interoperability to show the effectiveness of our method.\nRoadmap. The remaining of the paper is organized as follows. In Section II, we introduce the basic concept of diffusion models and mention defenses against deepfake attacks. Then, in Section III, we discuss how we built our unsafe video dataset. Based on this dataset, we design our defense mechanism in Section IV. Section V provides details about how we examined our defense mechanism. To further test it, we conducted ablation studies in Section VI. Section VII summarizes related work, and we discuss the limitations and conclude in Section VIII."}, {"title": "II. BACKGROUND", "content": "A. Diffusion Models\nDiffusion models [18], [30] are state-of-the-art generative models that have been used in various modalities, such as image [39], [40], audio [26], and video generation [2], [5], [56]. The fundamental concept behind diffusion models consists of two phases: the diffusion process and the denoising process. The diffusion process, also called the forward process, iteratively adds noise from a standard normal distribution.\nThe noise schedule {at}T t=1 is set to control the magnitude of noise added in the diffusion process. Utilizing a reparameterization trick, we can express the noise sample xt at any step t in the forward pass, given original data xo, as:\n$$x_t = \\sqrt{a_t}x_o + \\sqrt{1 - a_t}e_t$$\nwhere $a_t = \\prod_{i=1}^{t} \\alpha_i$.\nIn contrast, the denoising process aims to remove noise from a noisy image xt, where $x_t^\u2020 \u223c N(0,1)$ and ultimately denoise to a clean image xo. A neural network (e.g., U-Net) $\u03b5_\u03b8$ is usually trained to predict the noise that needs to be removed at step t. The loss function for training the denoising network can be represented as:\n$$L_t(0) = E_{x_o,e_t} [|| e_t \u2013 e_\u03b8(\\sqrt{a_t}x_o + \\sqrt{1 \u2013 a_t}e_t, t)||_2]$$\nB. Video Diffusion Models\nTo apply diffusion models [18], [46] to video generation, these models need to understand spatial information and maintain consistency and coherence in the temporal dimension. Unlike traditional diffusion models [18], [46], which work mostly in 2D, VGMs must incorporate temporal layers to learn the motion logic of the object over time [19]. With the new requirement for generative objectives, different approaches"}, {"title": "III. GENERATE UNSAFE VIDEOS", "content": "We first explore the feasibility of video generative models (VGMs) to synthesize unsafe videos. With positive results, we then recruited participants to identify and label unsafe videos, constructing an unsafe video dataset.\nA. Unsafe Prompt Collection\nThe prompts we choose to generate unsafe content are from two unsafe prompt datasets: the dataset from Qu et al. [35] and the I2P (Inappropriate Image Prompts) dataset [41]. These two datasets contain many unsafe prompts, and all have been tested on the text-to-image generation models. Since most current VGMs [2], [5], [14], [54], [56] still use the pre-trained T2I model [39] as their backbone and have the same spatial understanding, we first used those datasets to explore the capabilities of VGMs. Based on the content quality, we then selected prompts to build our experimental dataset.\nSpecifically, Qu et al. [35] collect their unsafe prompts from 4chan\u2075 and Lexica. Lexica\u2076 is an anonymous platform where people post unsafe content, including sexual, violent, hateful, etc. After preprocessing the raw data from 4chan, they collected 500 prompts that could generate high-quality images from 4chan. Different from the 4chan website, Lexica is a website that provides prompts directly. Thus, they used 66 keywords related to the unsafe categories to query the Lexica website. After data cleaning, they collected 404 prompts from Lexica.\nThe I2P dataset is collected by Schramowski et al. [41], and its prompts are also from the Lexica website. The authors also used keywords related to their seven unsafe categories to query Lexica. They designed 26 keywords and collected 250 text prompts for each word. Because some prompts can have multiple keywords simultaneously, they finally collected 4703 prompts in their datasets after removing the duplicate prompts.\nB. Theme Summary\nIn our work, we first combined all the unsafe prompts, then removed the duplicate prompts from two datasets, and used those prompts to query MagicTime [54] to generate the corresponding videos. Because our work is on the video domain and the model differs from the previous work [11], [35], [40], [41], we cannot directly use the unsafe categories defined in their paper.\nSimilar to the previous work [35], we want to use a data-driven approach to identify the scope of unsafe images. First, we used the k-means method [45] to categorize the generated videos into several clusters. Then, we selected the most optimized cluster number and did the thematic coding analysis to define the unsafe category's name.\nBecause image feature extractors are much more powerful than video feature extractors due to the amount of training data, we mainly focus on the semantic information for each video. Thus, we selected the CLIP model (CLIP-ViT-L-14) as our feature extractor and processed all the frames for each video. We calculated the mean of all frame feature vectors in each video to get the feature vector representing the video. Then, we used the k-means clustering algorithm to get the video feature clusters and set the possible cluster number from 2 to 30. According to the elbow method [47], we found that when we set the cluster number to 23, the generated videos would have the best cluster performance.\nThe next step is to identify unsafe video categories. Since no unsafe video detector currently exists, we cannot remove unsafe videos before performing clustering. Some video clusters consist of normal videos that require manual inspection. We applied thematic coding analysis [4] in our work, which is usually used in social science, to conclude the theme by qualitatively analyzing data. To get better results, we collected 10 videos from each cluster, and three authors of our work wrote the text description for each cluster. This is the initial version of our code book, and then we calculate the initial Krippendorff's alpha [15] is 0.39, and Fleiss' kappa [9] is 0.56. Then, we discussed the code and tried to refine our code book. We built an overall text description for each cluster and changed some of our initial code based on that. For our second code book, Krippendorff's alpha achieves 0.83, and the score of Fleiss' Kappa is 0.94. Both of these scores represent that we had an agreement for almost every cluster. The final step is to group the clusters, as shown in Appendix E. Although some videos are from different clusters, they can be the same type of unsafe videos. After removing the harmless video cluster, we conclude five unsafe categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. We show the videos represented for each category in Figure 2. In our work, we use these five unsafe categories to classify unsafe videos and design our defense.\nC. Data Collection\nWe designed an online survey based on the five categories of unsafe videos identified through thematic coding analysis [4]. Our survey contained 2,112 unsafe videos produced by MagicTime [54]. Unlike previous studies that used authors to label inappropriate content, we aimed to reduce subjective bias and gather more authentic and neutral data. Initially, we obtained approval from the institute's IRB protocol and subsequently created the survey using Qualtrics website. Participants for annotating unsafe videos were recruited through Prolific platform, and compensation was provided. Their demographic data will be protected during this process.\nGiven the sensitive nature of our survey, we included a disclaimer on the first page of Qualtrics and the Prolific homepage, stating that the content contains unsafe information. Participants needed to be over 18 years old and were informed that they could withdraw from the survey at any time if they felt uncomfortable without facing any penalties. Each participant was assigned 30 videos and one additional attention check. If participants considered a video to be unsafe, they were prompted to categorize it into one of five categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. An \"Other\" option was also provided for any videos that did not fit these categories. Participants were compensated at a rate of $10.15/hr.\nWe first conducted a pilot study to validate the design and logic of our Qualtrics survey. We recruited 20 participants to test our questionnaire. The goal of the pilot study was to verify the appropriateness of our time settings, the number of questions, and the attention checks. After examining the responses submitted by each participant, we found that there were no necessary modifications needed for the questions or the attention check. Accordingly, we decided to recruit 600 participants for the main survey study. These 600 participants for the main survey did not overlap with those who participated in the pilot study.\nFrom the initial pool of 600 participants, 197 individuals failed the attention check and were subsequently excluded from the study. For these participants, we still provided 10% of their compensation through bonus payments. As a result, a total of 403 participants successfully completed the labeling task for 30 generated videos. We first categorized all videos deemed \"unsafe video\" according to participants' labels. Specifically, every video was labeled by at least two participants in our survey. Next, we performed data cleaning by integrating our own assessments of the \"unsafe video\" videos. For example, if a video was labeled as \u201cunsafe\u201d and categorized as Pornographic (label 4), it was retained in the Pornographic category only if more than half of the participants who marked it as \u201cunsafe video\" also identified it as Pornographic. After data cleaning, the videos were categorized based on participant labels and our assessments. We got 590 videos as Distorted/Weird, 579 as Terrifying, 445 as Pornographic, 204 as Violent/Bloody, and 39 as Political.\nWe allowed participants to suggest new unsafe categories during our online study. However, because there was not a sufficient number of participants who reached a consensus on any additional category, and because some of the suggested categories were similar to our defined ones (e.g., sexually explicit), we maintained the five unsafe categories generated through thematic coding analysis."}, {"title": "IV. DEFENSE METHODOLOGY", "content": "Several studies have discussed potential safety issues within image generation models, and proposed defense methods. We group these existing defense methods into two clusters: model-write defense [3], [11], [12], [20], [23], [25], [29], [41] that modify part of the model weight or generation process, and model-free defense [24], [35], [38] that based on input and/or output filtering and does not require access to the model.\nModel-write defenses change or update the model's generation process or parameters, which might affect the model generation quality, and usually need to fine-tune the model, which takes time [11], [12], [20], [23], [25], [29]. On the other hand, model-free defense trains classifiers/detectors that predict whether the input prompt and/or output result is harmless. Input prompt filtering is vulnerable to adversarial prompts and jailbreak attacks (described briefly in Section VII-C), while output filtering still needs model owners to first generate the results and also takes time (generating videos takes tens of times longer than generating images). We provide more details of existing defenses for image diffusion models in Section VII-A.\nA. Overview of Our Method\nGiven the drawbacks of model-write and model-free approaches, we propose a model-read approach that sits between model-write and model-free defenses. At a high level, we only need \"read access\" of the diffusion model and detect unsafe content within the diffusion process. It shares similarities with the model-free approach in that both train detectors, but it generalizes the model-free approach by considering both input filtration and output filtration as extreme cases. Model-read defense is more robust compared to input filtration and more efficient compared to output filtration (because it does not require completing the entire generation process). Note that our model-read defense can also potentially collaborate with the other two approaches (as discussed in Section IV-C), providing a more comprehensive defense.\nOur solution relies on the insight that generative models, like VAE [22] and diffusion models [18], are designed to learn latent space representations. We conjecture that the same type of unsafe videos are generated by latent variables close to each other in the latent space.\nB. Latent Variable Defense\nIn this section, we introduce our defense called Latent Variable Defense (LVD). The foundation for our method is the DDIM sampler [46] used in modern diffusion models for video/image generation, which can significantly enhance inference speed.\nDDIM Foundation. Compared to the traditional DDPM's Markovian sampling process [18], DDIM is non-Markovian and deterministic. Different video diffusion models may vary in structure due to distinct design choices [5], [14], [54]. However, to efficiently generate samples, these models move the diffusion process to the latent space. The reverse process in these models can be represented as:\n$$z_{t-1} = \\frac{\\sqrt{\\bar{a}_{t-1}}}{\\sqrt{\\bar{a}_t}} z_t - \\frac{\\sqrt{1 \u2013 \\bar{a}_t}}{\\sqrt{\\bar{a}_t}} e_\u03b8(z_t, t) + \u03c3e$$where \u03c3\u03c4 denotes the hyper-parameter that controls the level of randomness in the forward process [46], and other symbols share the same notations as in Section II-A. When the \u03c3\u03c4 = 0 for all t, the whole reverse/forward process is a deterministic trajectory. Song et al. [46] prove that when both reverse and forward processes are fixed, model sampling steps can be accelerated by defining the reverse process on a subset of the original T steps and still get the high-quality output. For a video/image diffusion model, the inference steps can be accelerated to only k steps $T = {T_1, T_2, ..., T_k}$ (these k steps evenly partition the original T steps). The accelerated DDIM sampler equation can be represented as:\n$$Pr [Z_{t_{i}}|Z_{t_{i+1}}, Z_0] = \\frac{Pr [Z_{t_{i+1}}|Z_{t_{i}}, Z_0] Pr [Z_{t_{i}}|Z_0]}{Pr [Z_{t_{i+1}}|Z_0]}$$\nwhere $T_i < T_{i+1}-1$ and $T_k < T$; z refers to the latent variable. For each latent variable zTi in the reverse process, it does not have randomness and is controllable. Given the specific latent variable zTi, the final output zo is also constant. Therefore, we want to use the DDIM deterministic denoising trajectory to design our defense method.\nUnlike the previous model-free methods [24], [35], [38], which rigidly used the final synthesis images as input for detection, we want to use the intermediate latent variables from zTk to zT1 to train our detection model M. This approach will help build a more agile and integrable defense mechanism.\nDefense Algorithm. Assume the video generation model sets the number of inference steps to k out of T (according to the existing literature [46], setting the number of denoising steps to k = 50 achieves nearly the same quality as T = 1000 steps, saving time and computing resources; we also set the denoising step to 50 in our experiment), our defense mechanism involves k detection models M1, M2, ..., Mk. We use the latent variable z at i-th step to train each Mi. It is important to note that we do not directly feed the zTi at t-th step into our detection model. According to Equation 1, we can calculated denoised sample z at t-th step and use it as input for detection model. The detection results for i-th model can be represented as si = Mi(zi). For each data point, our mechanism can obtain a vector of scores s1,.., sk that helps us determine whether the current video is an unsafe video.\nWe give the details of LVD in Algorithm 1. Specifically, it leverages detection models to determine at each step whether the intermediate result is unsafe and makes the final decision based on cumulative scores. We introduce two hyperparame- ters, \u03bb and \u03b7. \u03b7 improves LVD's efficiency by considering only the first n < k steps, and A controls the detection threshold.\nTo further improve efficiency, LVD can dynamically check the score vector for the generated sample. At the i-th step, the score for the generated video is given by scorei = Mi(zi). If the score is greater than or equal to \u03bb\u00b7 (k\u2212 i + 1), LVD can break and immediately classify the current video as unsafe.\nC. Interoperability with Existing Defenses\nSince our method is applicable to all diffusion models, we will briefly demonstrate how it can be combined with existing"}, {"title": "V. EVALUATION", "content": "A. Experiment Setup\nData Preparation. As mentioned in Section III-C, we recruited 600 participants via the Prolific platform to label 2,112 videos generated by MagicTime [54]. Moreover, since AnimateDiff [14] and VideoCrafter [5] also use Stable Diffusion [39] as their backbone (MagicTime and AnimateDiff used SD v1-5, and VideoCrafter used SD 2.1; their semantic-level understanding of unsafe prompts are similar). After carefully reviewing the label information from participants, we annotated the videos generated from the other two models. The number of videos in different unsafe groups generated by the various video diffusion models is presented in Table I. In the evaluation process, we used 20% unsafe videos to build an examination dataset to test our defense accuracy. We set k = 50 following existing literature [46], as k = 50 can already ensure high-quality generation while saving significant time.\nObservation about Models. Upon examining the different types of unsafe videos generated by these models, as presented in Table I, we observed an intriguing phenomenon: the models exhibit variability in generating different categories of unsafe videos. Specifically, we found that MagicTime [54] tends to produce higher-quality Pornographic videos than the other two models. When the same prompts are fed into VideoCrafter [5] and AnimateDiff [14], there is a certain likelihood of generating low-quality, chaotic videos or content lacking Pornographic elements. Conversely, MagicTime is less accurate in handling political prompts, generating nearly half the number of political videos compared to the other two models. Noting these variations in the models' generation capabilities for different features, we selected these three current open-source state-of-the-art models to evaluate the effectiveness of our method.\nDetection Model. We employ VideoMAE [48] as the backbone for our detection model. Theoretically, our detection task is a classification work; we connected trainable, fully connected layers with VideoMAE. In our experiments, each tested video diffusion model is configured with 50 inference steps, and we train 50 distinct detection models for each category of unsafe videos. The detection model's training setup is given in Table II.\nEvaluation Metrics. In our experiments, we not only present the overall accuracy of the defense method but also emphasize the TPR (true positive rate, for correctly classifying unsafe videos) and TNR (true negative rate, for correctly classifying harmless videos). The main reason for considering these two"}, {"title": "VI. ABLATION STUDY", "content": "A. Evaluation with Adversarial Prompts\nAccording to Yang et al. [53], and Qu et al. [35], normal prompts can still query models to generate unsafe content. Specifically, Yang et al. conducted jailbreak experiments on text-to-image generation models. In their study, they first categorized model-free defense methods into three types: text-based safety filters, image-based safety filters, and text-image-based safety filters. In their work, they designed SneakyPrompt to avoid these safety filters. They use beam, greedy, brute force, and reinforcement learning to build their SneakyPrompt algorithm. Then, they defined an evaluation metric called the bypass rate, which measures the number of adversarial prompts that successfully bypass the safety filter. Their adversarial prompts achieved a 100% bypass rate against the text-only safety filter that built-in Stable Diffusion [39]. Furthermore, the bypass rate of their adversarial prompts exceeded that of the manually crafted prompts by Rando et al. [38] and Qu et al. [35].\nGiven that our method employs detection models to build our defense mechanism, it can be considered a type of safety filter operating in the latent space. Therefore, we use the currently most powerful adversarial prompt algorithm SneakyPrompt build our adversarial dataset to test our defense mechanism on three VGMs. The dataset is built by applying SneakyPrompt to the original NSFW-200 dataset.\nDue to differences in the models, the amount of unsafe content generated by adversarial prompts varies among the three models. For each model, we filter out the harmless videos before conducting the experiments.\nAccording to Table VI, we can observe that our defense mechanism successfully detects unsafe content across different values of \u03b7 and three VGMs. When \u03b7 is set to 20, all models achieve around 95% detection accuracy while maintaining high TPR and TNR values. We think our defense mechanism's success is due to its focus on detecting unsafe content during the inference steps. In contrast, adversarial prompts are typically designed to ensure that the final generated unsafe samples can evade safety filters.\nB. Evaluation with Image-to-Video Models\nAfter testing with the adversarial prompt dataset, a natural idea arises for our method. our defense mechanism uses detection models to identify denoised latent variables zo during the inference process of VGMs. In text-to-video models, at the t-th step, zo can be represented as\n$$z_o = \\frac{z_t - \\sqrt{1 \u2013 \\bar{a}_t}e_\u03b8(z_t, t, c)}{\\sqrt{\\bar{a}_t}}$$\nwhere c represents the input prompt guidance and other notations aligned with the previous sections. However, in image-to-video tasks [2], [6], [56], the primary difference is that the conditional input c is converted from a prompt to an image. The rest of the generation process remains unchanged. Consequently, to explore the generalization ability of our method, we aim to use unsafe images to query an image-to-video diffusion model [5], [6], [14], [56] and thereby confirm the versatility of our approach.\nIt is noteworthy that, during model selection, both AnimateDiff [14] and VideoCrafter [5] are capable of performing image-to-video generation tasks. However, MagicTime, due to its limitations, can only perform text-to-video generation and cannot be used in this section. To query the model with unsafe images to generate unsafe videos, we selected 200 images they deemed most unsafe from those generated using unsafe prompts by Qu et al. [35] from Stable Diffusion [39]. After data cleaning, we compiled a dataset of 200 unsafe images to query the video generation model.\nWe first performed data cleaning after generating videos from selected unsafe images using the models. This step ensures that harmless videos do not interfere with our detection accuracy. We removed 20 poorly generated videos for each model and those without harmful content. Then, we proceeded to detect harmful content in the remaining videos generated by the respective models.\nIn this section, we did not retrain detection models. Instead, we used detection models trained on unsafe videos generated by the same model's text-to-video task. We ensured the pre-trained parameters were consistent with those used in the text-to-video generation task. For example, for AnimateDiff [14], we used the same version of stable diffusion v1.5 parameters for the image-to-video task as for the text-to-video task. Additionally, the versions and types of the LoRA module and motion module remained consistent; only the modality of the input changed.\nFrom the results in Table VII, we can see that LVD still maintains a high detection success rate when detecting unsafe content generated by different tasks of the same model. In Table VII, we chose to display only the True Positive Rate value. This is because we did not retrain the detection model for this part, and the negative samples in the detection tests were the harmless videos generated by the same model's text-to-video task. These samples have already shown detection results in Table III. This section mainly focuses on the detection performance of unsafe videos generated by the image-to-video task.\nIt can be seen that when \u03bb is set to 0.3 and 0.6, both models achieve around 90% TPR across all values of \u03b7 for"}, {"title": "VII. RELATED WORK", "content": "A. Existing Defenses for Image Diffusion Models\nModel-write Defense. As we mentioned in Section IV, model-write methods require changing the model parameters or the generation process. Schramowski et al. [41", "11": "argued that harmful concepts could be erased from the model's understanding. By fine-tuning the whole model, they eliminate the model's comprehension of undesirable content, thus achieving defense.\nThe problem with model-write defense is that this type of method needs to change or update the model's generation process or parameters, which might affect the model generation quality [11", "12": [20], "23": [25], "29": [41], "35": "discovered, using normal prompts can still generate inappropriate outputs. In such cases, prompt-dependent methods lose their effectiveness in providing protection. Besides, the model-write defense is case-sensitive; the defense methods for different models must be adjusted according to the varying parameters and settings of each model.\nModel-free Defense. The typical feature of model-free defense is the defense process does not interact with the generation model [24"}, {"35": [38], "39": "determines the safety of generated images by extracting features with CLIP and comparing them to 17 unsafe concepts. After that, Qu et al. [35", "24": [35], "38": "model-free methods have better generalization than model-write defenses. However, they lack flexibility and are relatively rigid. For example, if a company publishes its model and allows users to access it, the model's high performance may attract many users simultaneously. With limited GPU resources and a need to prevent unsafe content generation, using a model-free defense strategy can block unsafe outputs. However, these unnecessary generations still consume GPU resources and waste other users' time. Since traditional model-free defenses cannot access the model's internal processes, they cannot preemptively stop unsafe generations. Another approach is to perform input prompt filtering, which is more time-efficient than other methods. However, this method is vulnerable to adversarial prompts and jailbreak attacks (described briefly in Section VII-C).\nB. Deepfake Detection in VGMS\nDiffusion models are now used across various fields for data generation due to their high-quality and diverse content generation capabilities. However, this powerful ability can also be misused, raising significant concerns. With the development of VGMs [2", "6": [14], "55": [56]}]}