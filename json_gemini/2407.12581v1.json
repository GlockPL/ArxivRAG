{"title": "Towards Understanding Unsafe Video Generation", "authors": ["Yan Pang", "Aiping Xiong", "Yang Zhang", "Tianhao Wang"], "abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize high-quality output. It is important to understand their potential to produce unsafe content, such as violent or terrifying videos. In this work, we provide a comprehensive understanding of unsafe video generation.\nFirst, to confirm the possibility that these models could indeed generate unsafe videos, we choose unsafe content generation prompts collected from 4chan and Lexica, and three open-source SOTA VGMs to generate unsafe videos. After filtering out duplicates and poorly generated content, we created an initial set of 2112 unsafe videos from an original pool of 5607 videos. Through clustering and thematic coding analysis of these generated videos, we identify 5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. With IRB approval, we then recruit online participants to help label the generated videos. Based on the annotations submitted by 403 participants, we identified 937 unsafe videos from the initial video set. With the labeled information and the corresponding prompts, we created the first dataset of unsafe videos generated by VGMs.\nWe then study possible defense mechanisms to prevent the generation of unsafe videos. Existing defense methods in image generation focus on filtering either input prompt or output results. We propose a new approach called Latent Variable Defense (LVD), which works within the models internal sampling process. LVD can achieve 0.90 defense accuracy while reducing time and computing resources by 10\u00d7 when sampling a large number of unsafe prompts. Our experiment includes three open-source SOTA video diffusion models, each achieving accuracy rates of 0.99, 0.92, and 0.91, respectively. Additionally, our method was tested with adversarial prompts and on image-to-video diffusion models, and achieved nearly 1.0 accuracy on both settings. Our method also shows its interoperability by improving the performance of other defenses when combined with them. We will publish our constructed video dataset and code\u00b2.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, video generation models (VGMs) [2], [5], [14], [17], [19], [54]\u2013[56] have improved significantly and can generate coherent and high-quality videos encompassing a wide variety of themes. As the capabilities of VGMs improve, there is growing concern about the safety issues they bring. For example, the \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201d3 from the White House emphasizes \u201ctesting and safeguards against discriminatory, misleading, inflammatory, unsafe, or deceptive outputs\u201d in Section 10; In April 2024, NIST released the AI Risk Management Framework, which explicitly tries to understand the ability of generative models to synthesize unsafe content. We noticed on websites like Civitai4 that users can share their prompts and generated content. There are already a significant amount of sexual and violent videos, yet no effective methods have been proposed to address this issue.\nTo examine the capacity of VGMs to generate unsafe videos, we use prompts collected from the 4chan and Lexica websites [35], [41]; These datasets were originally used to guide text-to-image models in generating unsafe images. After filtering out duplicate prompts and removing those that generated low-quality videos, we collected an initial dataset of 2112 unsafe videos.\nTo verify these generative videos are indeed unsafe and cause unpleasant feelings, with IRB approval, we recruited online study participants to rate those videos. Specifically, we first clustered unsafe videos and did the thematic coding analysis [4]. Through two rounds of discussions, we identified five main categories of unsafe videos: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. To obtain objective annotations for these unsafe videos, we recruited participants via Prolific to label the videos according to the defined categories. We initially recruited 600 participants. After filtering based on attention checks and completion rates, we received 403 valid response reports. Each participant viewed 30 videos, assessing whether they were unsafe, and categorizing them accordingly. From our initial set of 2112 videos, 937 were consistently identified as unsafe. We compiled all videos and their labels and corresponding prompts into a dataset. This is the first dataset of unsafe videos generated by VGMs.\nGiven this dataset, we are then intrigued to understand whether we can defend against unsafe generation in VGMs. That is, can we ensure VGMs will never generate unsafe content? Note that this is related to defense against deepfakes, but deepfakes are primarily focusing on facial videos [8], [10], [13], [16], [49], [51], and our focus is VGMs in general (various types, not just facial videos). There are existing solu- tions [3], [11], [12], [20], [23]\u2013[25], [29], [35], [38], [41] for image generative models that addressed the unsafe generation problems. For example, Schramowski et al. [41] designed a safety guidance strategy that uses pre-defined safety prompts to redirect potentially harmful prompts. Then, Gandikota et al. [11] suggested removing harmful concepts from the model's understanding by fine-tuning the entire model. Li et al. [25] noted the text-dependency of previous methods and proposed removing unsafe visual representations to protect generated content. Qu et al. [35] implemented a detection model to evaluate the output content without interfering with the generation process. However, detecting unsafe content in videos is much more challenging than in images because videos contain more information (both spatial and temporal) and require significantly more computational resources to generate. Therefore, we need to rethink methods for detecting unsafe content in the video domain.\nAs detailed in Section IV, we categorize existing solutions for image generative models as either model-free (no need to look into model internals) or model-write (need to modify model parameters or settings) approaches. These two types of methods can only employ the final results of the diffusion process, limit further defensive actions, or require significant computational resources to update model parameters. Nevertheless, we propose a middle-ground, model-read approach that leverages \"read access\" of the diffusion model and detects unsafe content within the diffusion process. The model-read method is less rigid than the model-free approach, which only detects the model\u2019s final output. It also avoids the extensive time and computational resources needed for the model-write approach. We call our method Latent Variable Defense (LVD). LVD leverages the intuition that generative models, such as VAE [22] and diffusion models [18], are trained to learn latent space representation, and in these representations, samples that are close in the latent space result in similar generated content [22]. As shown in Figure 1, we establish classifiers to analyze the intermediate results of the diffusion process, which is quite different from the existing model-free methods. When we suspect the result will be unsafe, we can terminate the diffusion process early, saving significant computation resources. All Experimental results on three SOTA VGMs show that compared to working with the final result, our approach can save up to 10\u00d7 computation time, while achieving comparable detection accuracy (around 92%).\nContributions: We make the following contributions.\n\u2022 We demonstrated that VGMs have a strong capability to synthesize unsafe content.\n\u2022 We construct an unsafe generated video dataset from the SOTA open-sourced VGMs. Unsafe categories were generated using data-driven methods. Data annotations were completed by 403 participants recruited through Prolific.\n\u2022 Based on our unsafe video dataset, we proposed LVD to mitigate generating unsafe videos. LVD uses DDIM characteristics to detect unsafe content during inference time.\n\u2022 We tested our defense on three open-source SOTA VGMs, conducting comprehensive experiments to assess the defense performance under different parameter settings. Results showed our defense can achieve nearly 100% accuracy on all three models. We further assessed our defense\u2019s robustness, generalization ability, and interoperability to show the effectiveness of our method.\nRoadmap. The remaining of the paper is organized as follows. In Section II, we introduce the basic concept of diffusion models and mention defenses against deepfake attacks. Then, in Section III, we discuss how we built our unsafe video dataset. Based on this dataset, we design our defense mechanism in Section IV. Section V provides details about how we examined our defense mechanism. To further test it, we conducted ablation studies in Section VI. Section VII summarizes related work, and we discuss the limitations and conclude in Section VIII."}, {"title": "II. BACKGROUND", "content": "A. Diffusion Models\nDiffusion models [18], [30] are state-of-the-art generative models that have been used in various modalities, such as image [39], [40], audio [26], and video generation [2], [5], [56]. The fundamental concept behind diffusion models consists of two phases: the diffusion process and the denoising process. The diffusion process, also called the forward process, iteratively adds noise from a standard normal distribution.\nThe noise schedule {\u03b1t}Tt=1 is set to control the magnitude of noise added in the diffusion process. Utilizing a reparameterization trick, we can express the noise sample xt at any step t in the forward pass, given original data xo, as:\n$$x_t = \\sqrt{\\alpha_t}x_o + \\sqrt{1 - \\alpha_t}\\epsilon_t$$\nwhere \u03b1t = \u03a0ti=1 \u03b1i.\nIn contrast, the denoising process aims to remove noise from a noisy image x\u2020, where x\u2020 ~ N(0,1) and ultimately denoise to a clean image xo. A neural network (e.g., U-Net) \u03b5\u03b8 is usually trained to predict the noise that needs to be removed at step t. The loss function for training the denoising network can be represented as:\n$$L_t(\\theta) = E_{x_o, \\epsilon_t} [|| \\epsilon_t \u2013 \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha_t}}x_o + \\sqrt{1 \u2013 \\bar{\\alpha_t}}\\epsilon_t, t)||^2]$$\nB. Video Diffusion Models\nTo apply diffusion models [18], [46] to video generation, these models need to understand spatial information and maintain consistency and coherence in the temporal dimension. Unlike traditional diffusion models [18], [46], which work mostly in 2D, VGMs must incorporate temporal layers to learn the motion logic of the object over time [19]. With the new requirement for generative objectives, different approaches to training VGMs exist. Based on training strategies, we categorize them into training from scratch, fine-tuning on video data, and training-free models.\nAmong the three approaches, fine-tuning stands out as an efficient strategy and demonstrates superior performance and becomes the focus of this study (and we will elaborate on the other two approaches in Section VII-D). Most models adopt this approach, as it balances synthesis quality and the consumption of training resources. This approach requires selecting an appropriate pre-trained image generator as the backbone. And currently, existing works [2], [5], [54], [56] usually select stable diffusion [39] as the backbone. After adding temporal convolution and attention layers to ensure consistency across multiple frames, the model can be trained directly using video data. During training, researchers need to freeze the parameters of the spatial layers and focus on training the inserted temporal layers to learn the motion logic of objects. Once the model understands temporal dynamics, the generation quality can be further improved using a cascade approach. This involves dividing the generation process into a base stage and a refine stage [17]. In the base stage, a large amount of low-resolution video data helps the model understand the generation objective and produce a low-quality video. In the refining stage, a small number of high-resolution video data is used to train the model. This stage enhances the quality of the video generated in the base stage, ultimately achieving good generation results.\nDeepfakes. One kind of risky generated video that draws extensive attention is human facial videos, a.k.a. deepfakes. Deepfake attacks are used to primarily target facial images [7], [50], [57], [59] and videos [8], [10], [13], [16], [49], [51]. However, with the development of VGMs [5], [14], [54], [56], it is now possible to create high-resolution realistic scenes. We differentiate ourselves from Deepfakes as the generated topic is no longer limited to facial images."}, {"title": "C. Threat Model", "content": "Our study assumes a simple threat model that involves only two parties: malicious users and model owners. The goal of the malicious user is to use the video generation model to synthesize unsafe videos. These users have unsafe prompts and tried to feed those prompts to the model. Moreover, malicious users potentially have the capability to use the optimization method to build adversarial prompts. They can access the model output but cannot access the internal values.\nOn the other hand, model owners can access the model\u2019s internal parameters and get intermediate output. Their goal is to design an effective method to protect their model from being abused by malicious users while not affecting its generating ability on normal prompts."}, {"title": "III. GENERATE UNSAFE VIDEOS", "content": "We first explore the feasibility of video generative models (VGMs) to synthesize unsafe videos. With positive results, we then recruited participants to identify and label unsafe videos, constructing an unsafe video dataset."}, {"title": "A. Unsafe Prompt Collection", "content": "The prompts we choose to generate unsafe content are from two unsafe prompt datasets: the dataset from Qu et al. [35] and the I2P (Inappropriate Image Prompts) dataset [41]. These two datasets contain many unsafe prompts, and all have been tested on the text-to-image generation models. Since most current VGMs [2], [5], [14], [54], [56] still use the pre- trained T2I model [39] as their backbone and have the same spatial understanding, we first used those datasets to explore the capabilities of VGMs. Based on the content quality, we then selected prompts to build our experimental dataset.\nSpecifically, Qu et al. [35] collect their unsafe prompts from 4chan5 and Lexica.6 4chan is an anonymous platform where people post unsafe content, including sexual, violent, hateful, etc. After preprocessing the raw data from 4chan, they collected 500 prompts that could generate high-quality images from 4chan. Different from the 4chan website, Lexica is a website that provides prompts directly. Thus, they used 66 keywords related to the unsafe categories to query the Lexica website. After data cleaning, they collected 404 prompts from Lexica.\nThe I2P dataset is collected by Schramowski et al. [41], and its prompts are also from the Lexica website. The authors also used keywords related to their seven unsafe categories to query Lexica. They designed 26 keywords and collected 250 text prompts for each word. Because some prompts can have multiple keywords simultaneously, they finally collected 4703 prompts in their datasets after removing the duplicate prompts."}, {"title": "B. Theme Summary", "content": "In our work, we first combined all the unsafe prompts, then removed the duplicate prompts from two datasets, and used those prompts to query MagicTime [54] to generate the corresponding videos. Because our work is on the video domain and the model differs from the previous work [11], [35], [40], [41], we cannot directly use the unsafe categories defined in their paper.\nSimilar to the previous work [35], we want to use a data-driven approach to identify the scope of unsafe images. First, we used the k-means method [45] to categorize the generated videos into several clusters. Then, we selected the most optimized cluster number and did the thematic coding analysis to define the unsafe category\u2019s name.\nBecause image feature extractors are much more powerful than video feature extractors due to the amount of training data, we mainly focus on the semantic information for each video. Thus, we selected the CLIP model (CLIP-ViT-L-14) as our feature extractor and processed all the frames for each video. We calculated the mean of all frame feature vectors in each video to get the feature vector representing the video. Then, we used the k-means clustering algorithm to get the video feature clusters and set the possible cluster number from 2 to 30. According to the elbow method [47], we found that when we set the cluster number to 23, the generated videos would have the best cluster performance."}, {"title": "C. Data Collection", "content": "We designed an online survey based on the five categories of unsafe videos identified through thematic coding analysis [4]. Our survey contained 2,112 unsafe videos produced by MagicTime [54]. Unlike previous studies that used authors to label inappropriate content, we aimed to reduce subjective bias and gather more authentic and neutral data. Initially, we obtained approval from the institute\u2019s IRB protocol and subsequently created the survey using Qualtrics website. Participants for annotating unsafe videos were recruited through Prolific platform, and compensation was provided. Their demographic data will be protected during this process.\nGiven the sensitive nature of our survey, we included a disclaimer on the first page of Qualtrics and the Prolific homepage, stating that the content contains unsafe information. Participants needed to be over 18 years old and were informed that they could withdraw from the survey at any time if they felt uncomfortable without facing any penalties. Each participant was assigned 30 videos and one additional attention check. If participants considered a video to be unsafe, they were prompted to categorize it into one of five categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. An \u201cOther\u201d option was also provided for any videos that did not fit these categories. Participants were compensated at a rate of $10.15/hr.\nWe first conducted a pilot study to validate the design and logic of our Qualtrics survey. We recruited 20 participants to test our questionnaire. The goal of the pilot study was to verify the appropriateness of our time settings, the number of questions, and the attention checks. After examining the responses submitted by each participant, we found that there were no necessary modifications needed for the questions or the attention check. Accordingly, we decided to recruit 600 participants for the main survey study. These 600 participants for the main survey did not overlap with those who participated in the pilot study.\nFrom the initial pool of 600 participants, 197 individuals failed the attention check and were subsequently excluded from the study. For these participants, we still provided 10% of their compensation through bonus payments. As a result, a total of 403 participants successfully completed the labeling task for 30 generated videos. We first categorized all videos deemed \u201cunsafe video\u201d according to participants\u2019 labels. Specifically, every video was labeled by at least two participants in our survey. Next, we performed data cleaning by integrating our own assessments of the \u201cunsafe video\u201d videos. For example, if a video was labeled as \u201cunsafe\u201d and categorized as Pornographic (label 4), it was retained in the Pornographic category only if more than half of the participants who marked it as \u201cunsafe video\u201d also identified it as Pornographic. After data cleaning, the videos were categorized based on participant labels and our assessments. We got 590 videos as Distorted/Weird, 579 as Terrifying, 445 as Pornographic, 204 as Violent/Bloody, and 39 as Political.\nWe allowed participants to suggest new unsafe categories during our online study. However, because there was not a sufficient number of participants who reached a consensus on any additional category, and because some of the suggested categories were similar to our defined ones (e.g., sexually explicit), we maintained the five unsafe categories generated through thematic coding analysis."}, {"title": "IV. DEFENSE METHODOLOGY", "content": "Several studies have discussed potential safety issues within image generation models, and proposed defense methods. We group these existing defense methods into two clusters: model-write defense [3], [11], [12], [20], [23], [25], [29], [41] that modify part of the model weight or generation process, and model-free defense [24], [35], [38] that based on input and/or output filtering and does not require access to the model.\nModel-write defenses change or update the model\u2019s generation process or parameters, which might affect the model generation quality, and usually need to fine-tune the model, which takes time [11], [12], [20], [23], [25], [29]. On the other hand, model-free defense trains classifiers/detectors that predict whether the input prompt and/or output result is harmless. Input prompt filtering is vulnerable to adversarial prompts and jailbreak attacks (described briefly in Section VII-C), while output filtering still needs model owners to first generate the results and also takes time (generating videos takes tens of times longer than generating images). We provide more details of existing defenses for image diffusion models in Section VII-A.\nA. Overview of Our Method\nGiven the drawbacks of model-write and model-free approaches, we propose a model-read approach that sits between model-write and model-free defenses. At a high level, we only need \u201cread access\u201d of the diffusion model and detect unsafe content within the diffusion process. It shares similarities with the model-free approach in that both train detectors, but it generalizes the model-free approach by considering both input filtration and output filtration as extreme cases. Model-read defense is more robust compared to input filtration and more efficient compared to output filtration (because it does not require completing the entire generation process). Note that our model-read defense can also potentially collaborate with the other two approaches (as discussed in Section IV-C), providing a more comprehensive defense.\nOur solution relies on the insight that generative models, like VAE [22] and diffusion models [18], are designed to learn latent space representations. We conjecture that the same type of unsafe videos are generated by latent variables close to each other in the latent space.\nB. Latent Variable Defense\nIn this section, we introduce our defense called Latent Variable Defense (LVD). The foundation for our method is the DDIM sampler [46] used in modern diffusion models for video/image generation, which can significantly enhance inference speed.\nDDIM Foundation. Compared to the traditional DDPM\u2019s Markovian sampling process [18], DDIM is non-Markovian and deterministic. Different video diffusion models may vary in structure due to distinct design choices [5], [14], [54]. However, to efficiently generate samples, these models move the diffusion process to the latent space. The reverse process in these models can be represented as:\n$$z_{t-1} = \\frac{\\sqrt{\\bar{\\alpha_{t-1}}}}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\sqrt{1 - \\bar{\\alpha_t}}}{\\sqrt{1}}\\epsilon_{\\theta}(z_t, t)) + \\sqrt{1 \u2013 \\bar{\\alpha_{t-1}} \u2013 \\sigma_t^2}\u00b7 \\epsilon_t + \\sigma_t z$$\nwhere \u03c3\u03c4 denotes the hyper-parameter that controls the level of randomness in the forward process [46], and other symbols share the same notations as in Section II-A. When the \u03c3\u03c4 = 0 for all t, the whole reverse/forward process is a deterministic trajectory. Song et al. [46] prove that when both reverse and forward processes are fixed, model sampling steps can be accelerated by defining the reverse process on a subset of the original T steps and still get the high-quality output. For a video/image diffusion model, the inference steps can be accelerated to only k steps T = {T1, T2, ..., Tk} (these k steps evenly partition the original T steps). The accelerated DDIM sampler equation can be represented as:\n$$P_r [Z_{t_i} | Z_{t_{i+1}}, z_0] = \\frac{P_r [Z_{T_{i+1}} | Z_{T_i}, z_0] \u00b7 P_r [z_{T_i} | z_0]}{P_r [Z_{T_{i+1}} | z_0]}$$\nwhere Ti < Ti+1-1 and Tk < T; z refers to the latent variable. For each latent variable zTi in the reverse process, it does not have randomness and is controllable. Given the specific latent variable zT1 , the final output zo is also constant. Therefore, we want to use the DDIM deterministic denoising trajectory to design our defense method.\nUnlike the previous model-free methods [24], [35], [38], which rigidly used the final synthesis images as input for detection, we want to use the intermediate latent variables from zTk to T1 to train our detection model M. This approach will help build a more agile and integrable defense mechanism.\nDefense Algorithm. Assume the video generation model sets the number of inference steps to k out of T (according to the existing literature [46], setting the number of denoising steps to k = 50 achieves nearly the same quality as T = 1000 steps, saving time and computing resources; we also set the denoising step to 50 in our experiment), our defense mechanism involves k detection models M1, M2, ..., Mk. We use the latent variable zti at i-th step to train each Mi. It is important to note that we do not directly feed the zti at t-th step into our detection model. According to Equation 1, we can calculated denoised sample z at t-th step and use it as input for detection model. The detection results for i-th model can be represented as si = Mi(z). For each data point, our mechanism can obtain a vector of scores s1,.., sk that helps us determine whether the current video is an unsafe video.\nWe give the details of LVD in Algorithm 1. Specifically, it leverages detection models to determine at each step whether the intermediate result is unsafe and makes the final decision based on cumulative scores. We introduce two hyperparameters, \u03bb and \u03b7. \u03b7 improves LVD\u2019s efficiency by considering only the first \u03b7 < k steps, and \u03bb controls the detection threshold.\nTo further improve efficiency, LVD can dynamically check the score vector for the generated sample. At the i-th step, the score for the generated video is given by scorei = Mi(z). If the score is greater than or equal to \u03bb \u00b7 , LVD can break and immediately classify the current video as unsafe.\nC. Interoperability with Existing Defenses\nSince our method is applicable to all diffusion models, we will briefly demonstrate how it can be combined with existing defense methods."}, {"title": "V. EVALUATION", "content": "A. Experiment Setup\nData Preparation. As mentioned in Section III-C, we re- cruited 600 participants via the Prolific platform to label 2,112 videos generated by MagicTime [54]. Moreover, since AnimateDiff [14] and VideoCrafter [5] also use Stable Diffusion [39] as their backbone (MagicTime and AnimateDiff used SD v1-5, and VideoCrafter used SD 2.1; their semantic- level understanding of unsafe prompts are similar). After carefully reviewing the label information from participants, we annotated the videos generated from the other two models. The number of videos in different unsafe groups generated by the various video diffusion models is presented in Table I. In the evaluation process, we used 20% unsafe videos to build an examination dataset to test our defense accuracy. We set k = 50 following existing literature [46], as k = 50 can already ensure high-quality generation while saving significant time.\nObservation about Models. Upon examining the different types of unsafe videos generated by these models, as presented in Table I, we observed an intriguing phenomenon: the models exhibit variability in generating different categories of unsafe videos. Specifically, we found that MagicTime [54] tends to produce higher-quality Pornographic videos than the other two models. When the same prompts are fed into VideoCrafter [5] and AnimateDiff [14], there is a certain likelihood of generating low-quality, chaotic videos or content lacking Pornographic elements. Conversely, MagicTime is less accurate in handling political prompts, generating nearly half the number of political videos compared to the other two models. Noting these variations in the models\u2019 generation capabilities for different features, we selected these three current open-source state-of-the-art models to evaluate the effectiveness of our method.\nDetection Model. We employ VideoMAE [48] as the backbone for our detection model. Theoretically, our detection task is a classification work; we connected trainable, fully connected layers with VideoMAE. In our experiments, each tested video diffusion model is configured with 50 inference steps, and we train 50 distinct detection models for each category of unsafe videos. The detection model\u2019s training setup is given in Table II.\nEvaluation Metrics. In our experiments, we not only present the overall accuracy of the defense method but also emphasize the TPR (true positive rate, for correctly classifying unsafe videos) and TNR (true negative rate, for correctly classifying harmless videos). The main reason for considering these two values is to align with our objective mentioned in Section IV-A, where we do not want our defense mechanism to interfere with the generation of harmless videos. If detection\u2019s accuracy is high but the TNR is low, it indicates that many harmless videos are being incorrectly detected as unsafe.\nFurthermore, we want to illustrate the relationship between TPR and FPR (false positive rate, for misclassifying unsafe videos). Our experimental setup ensures an equal number of positive and negative samples. Thus, we will use the Area Under the Receiver Operating Characteristic Curve (AUC- ROC) to demonstrate the defense performance of LVD."}, {"title": "B. Impact of Inference Steps", "content": "The efficiency of our method is significantly influenced by the hyperparameters \u03bb and \u03b7, as shown in Algorithm 1. In our defense mechanism, \u03bb controls the degree of trust in the detection accuracy at different denoising steps, while \u03b7 determines how many steps the LVD takes before performing a detection analysis. LVD is designed based on the characteristics of DDIM [46]. Therefore, we first aim to explore the detection success rate of the detection model at different denoising steps in order to help us get the range of \u03bb and \u03b7. We trained detection models on each type of unsafe video generated by the generative model separately. The experiment results for five groups of unsafe videos are in Figure 8. In this section, we trained 50 models for each unsafe category and totally trained 250 detection models for each generation model. The experimental results presented in this section are derived from the evaluation set used during the training phase. The examination dataset was not utilized here because our goal is to explore the performance of each group of detection model across all denoising steps. The testing was conducted separately for each group. For the Pornographic group, only unsafe Pornographic videos were used to assess its performance.\nBased on the results illustrated in Figure 8, it can be observed that our model is capable of performing perfect detection with MagicTime [54] and AnimateDiff [14]. This capability holds even with a minimal number of denoising steps. For instance, detection is possible with just the 1-th step. Conversely, for VideoCrafter [5], the detection model fails to perform effectively at lower step counts. For these five categories of unsafe videos, VideoCrafter achieves a detection success rate of approximately 80% at the first step. It is only from the second step onward that the detection accuracy rapidly increases to around 95%. To investigate the output from the beginning inference phase, we showed images of some denoising steps in Appendix D. It was observed that at the 1-th step, VideoCrafter [5] fails to generate coherent video content. Although the predicted output is the denoised zo, it remains disordered and chaotic upon inspection, preventing the detection model from learning any meaningful information. This results in ineffective detection at lower step counts for VideoCrafter.\nFor the categories of Pornographic, Distorted, and Terrifying unsafe video, all models\u2019 detection accuracy remains consistently high, achieving nearly 100%. However, the detection performance for Violent and Political content exhibits significant fluctuations with varying steps. Despite this variability, the detection success rate remains above 85% across different steps. We posit that the primary cause of the observed fluctuations is the relatively low number of violent and political types of unsafe videos. This scarcity prevents the detection model from learning sufficient features, resulting in significant accuracy variations."}, {"title": "C. Impact of \u03b7 and \u03bb", "content": "Based on the results observed in Section V-B, we can set the ranges of \u03b7 and \u03bb for the three models used in our experiment. For MagicTime [54], the detection accuracy remains high even during the initial stages of the denoising process. Thus, we believe that at a lower \u03b7, the detection accuracy will already be perfect. This is because the model is able to reconstruct the outline of the denoised object approximately by the early step, enabling the detection model to make an accurate judgment based on this outline. Conversely, for VideoCrafter [5], the initial stages of the denoising process fail to generate reasonable object representations. Therefore, we think LVD for VideoCrafter might need a higher \u03b7 to achieve the best detection accuracy. Similarly, due to VideoCrafter\u2019s inferior detection accuracy, we can impose looser conditions by setting a lower \u03bb value to ensure that every potentially unsafe video is detected.\nWhen conducting evaluations on LVD, we ensure the balance of the samples under test. The number of class 0 (harmless video) and class 1 (unsafe video) samples is equal. Based on an experimental result presented in Section V-B, we believe it is unnecessary to set the range of \u03b7 from 0 to 50. From Figure 8, we can observe that detection accuracy stabilizes and remains accurate during the mid-stage of the denoising process. Additionally, if any harmful generation is detected, we should be able to identify and interrupt the generation process early. Thus, we set \u03b7 to range from 1 to 20 and assign \u03bb values of 0.3, 0.6, and 1 in our work.\nTable III demonstrates that the detection accuracy of LVD increases with higher \u03b7 values. We highlighted the best detection accuracy obtained at different \u03b7 values. The AUC ROC curves for these \u03b7 and \u03bb configurations are shown in Figure 3. This result aligns well with intuitive expectations, as larger \u03b7 values mean LVD\u2019s judgment is based on more denoising steps.\nExcept when \u03b7 equals 1, the value of \u03bb does not affect the results. We observed four different \u03b7 values and discovered an interesting phenomenon. When \u03b7 is low, such as n = 3 or \u03b7 = 5, better detection accuracy is usually achieved when \u03bb equals 1. However, as \u03b7 increases, the most accurate detection results are often obtained when \u03bb is 0.6. This trend is consistent across the detection results for MagicTime [54], AnimateDiff [14], and VideoCrafter [5].\nWe think this occurs because, with a high \u03b7, setting \u03bb to 1 makes the model\u2019s detection very stringent. In other words, LVD must consistently identify a sample as unsafe at every denoising step to finally classify it as an unsafe video. When \u03b7 is low, LVD needs to be more stringent at each denoising step due to insufficient data for each sample. However, as \u03b7 increases, a high \u03bb value can cause LVD to misclassify some samples because a few denoising steps might indicate safety, affecting the overall judgment. To validate our hypothesis, we fixed \u03bb at 0.3, 0.6, and 1.0. Then presented the TPR, TNR, and accuracy for \u03b7 ranging from 1 to 50 from MagicTime [54] in Figure 4. It is evident that when \u03bb is 1, and \u03b7 is low, the accuracy is higher than when \u03bb is set to 0.3 or 0.6. However, as \u03b7 increases, accuracy decreases. When \u03bb is 1, the TPR value also rapidly decreases as \u03b7 increases.\nTherefore, when \u03b7 is set to 20, the best detection performance is achieved with \u03bb equal 0.6. Under this parameter setting, the TNR value remains at 0.98, ensuring efficient detection of unsafe videos while minimizing the impact on harmless video generation."}, {"title": "D. Comparison with Existing Methods", "content": "In this section, we aim to compare our approach with existing methods. Due to we are the first work focus on unsafe synthesis on VGMs, we compare with the defense methods for image generation models. However, many model-write defense methods are designed to change the output object and require adjustments to the model itself or modifications to the model\u2019s attention matrix [11], [12], [20], [23], [25], [29], [41]. Since the use of attention mechanisms in VGMs differs from that in image generators [39]. Additionally, MagicTime [54] employs DiT-based architecture [32]. We primarily compare our method with model-free defense methods [35], [38].\nIn Table IV, we compare the optimal detection accuracy of our defense mechanism under different \u03b7 values with the detection performance of Unsafe Diffusion [35]. To ensure a fair comparison, we kept the number of training samples and other parameters consistent. While the original work used an image classifier as the detection model, our current study deals with video content. To maintain fairness in the comparison, we used the same VideoMAE model as the detection model for Unsafe Diffusion.\nThe results show that our defense mechanism significantly outperforms Unsafe Diffusion in terms of detection accuracy. Although Unsafe Diffusion [35] achieves TPR value of 0.99 for unsafe videos, it correctly identifies only 0.56 of harmless videos. In contrast, our defense mechanism attains TNR values of 0.90, 0.95, 0.99, and 0.98, respectively. This indicates that Unsafe Diffusion is likely to misclassify a large number of safe samples, thereby affecting normal user experience.\nMore comparisons between our defense mechanism and Unsafe Diffusion [35] on AnimateDiff [14] and VideoCrafter [5] can be found in Appendix B.\nBesides, our approach also improves time optimization compared to previous methods. Methods by Qu et al. [35] and Rando et al. [38] process the final generated images by inputting them into CLIP-based classifier [36]. This approach does not optimize the generation process itself.\nour defense mechanism detects unsafe content by examining the latent variables during the inference process. This allows us to immediately interrupt the generation process upon detecting unsafe content, thereby saving computational resources. We have compiled statistics on the generation time for different models during the inference process, and the time required to generate a single sample is presented in Table V.\nTable V shows that the sample generation time for different models varies, with MagicTime [54] taking the longest at 85.4 \u00b1 1.1 seconds per sample. When using a post-process safety filter [24], [35], [38], people must wait for the model to complete the entire sampling process before using a feature extractor to extract and detect the generated content. However, if we detect unsafe content directly during the inference process, we can immediately interrupt the generation, thus saving computational resources for other users.\nAccording to the results in Table III, when set \u03b7 to 3 for our defense unsafe generation in MagicTime [54], we can still achieve TPR, TNR, and accuracy of 90%. Compared to the 85.4 seconds of the whole generation process, our defense mechanism can save over 90% (80 seconds) of computational resources. Based on calculations from the ML CO2 Impact website, detecting and stopping unsafe content generation for 100 hours can save 83.2 kg of CO2 emissions.\nTakeaways: In this section, we first observed the detection accuracy of models trained with different denoising steps to initially determine the experimental range for \u03b7 and \u03bb. Next, we tested the effectiveness of our defense mechanism on three VGMs using the examination dataset. Because of the design objective for our mechanism, we focused on TNR, TPR, and accuracy. The experimental results show that our defense mechanism provides effective protection for all VGMs in our study. Finally, we compared our method with existing defense methods for text-to-image models, demonstrating that our defense mechanism offers more efficient protection."}, {"title": "VI. ABLATION STUDY", "content": "A. Evaluation with Adversarial Prompts\nAccording to Yang et al. [53], and Qu et al. [35], normal prompts can still query models to generate unsafe content. Specifically, Yang et al. conducted jailbreak experiments on text-to-image generation models. In their study, they first categorized model-free defense methods into three types: text- based safety filters, image-based safety filters, and text-image- based safety filters. In their work, they designed SneakyPrompt to avoid these safety filters. They use beam, greedy, brute force, and reinforcement learning to build their SneakyPrompt algorithm. Then, they defined an evaluation metric called the bypass rate, which measures the number of adversarial prompts that successfully bypass the safety filter. Their adversarial prompts achieved a 100% bypass rate against the text-only safety filter that built-in Stable Diffusion [39]. Furthermore, the bypass rate of their adversarial prompts exceeded that of the manually crafted prompts by Rando et al. [38] and Qu et al. [35].\nGiven that our method employs detection models to build our defense mechanism, it can be considered a type of safety filter operating in the latent space. Therefore, we use the currently most powerful adversarial prompt algorithm SneakyPrompt build our adversarial dataset to test our defense mechanism on three VGMs. The dataset is built by applying SneakyPrompt to the original NSFW-200 dataset.\nDue to differences in the models, the amount of unsafe content generated by adversarial prompts varies among the three models. For each model, we filter out the harmless videos before conducting the experiments.\nAccording to Table VI, we can observe that our defense mechanism successfully detects unsafe content across different values of \u03b7 and three VGMs. When \u03b7 is set to 20, all models achieve around 95% detection accuracy while maintaining high TPR and TNR values. We think our defense mechanism\u2019s success is due to its focus on detecting unsafe content during the inference steps. In contrast, adversarial prompts are typically designed to ensure that the final generated unsafe samples can evade safety filters."}, {"title": "B. Evaluation with Image-to-Video Models", "content": "After testing with the adversarial prompt dataset, a natural idea arises for our method. our defense mechanism uses detection models to identify denoised latent variables zo during the inference process of VGMs. In text-to-video models, at the t-th step, zo can be represented as\n$$z_0 = \\frac{z_t - \\sqrt{1 \u2013 \\bar{\\alpha_t}}\\epsilon_{\\theta}(z_t, t, c)}{\\sqrt{\\bar{\\alpha_t}}}$$\nwhere c represents the input prompt guidance and other notations aligned with the previous sections. However, in image- to-video tasks [2], [6], [56], the primary difference is that the conditional input c is converted from a prompt to an image. The rest of the generation process remains unchanged. Consequently, to explore the generalization ability of our method, we aim to use unsafe images to query an image-to- video diffusion model [5], [6], [14], [56] and thereby confirm the versatility of our approach.\nIt is noteworthy that, during model selection, both Ani- mateDiff [14] and VideoCrafter [5] are capable of performing image-to-video generation tasks. However, MagicTime, due to its limitations, can only perform text-to-video generation and cannot be used in this section. To query the model with unsafe images to generate unsafe videos, we selected 200 images they deemed most unsafe from those generated using unsafe prompts by Qu et al. [35] from Stable Diffusion [39]. After data cleaning, we compiled a dataset of 200 unsafe images to query the video generation model.\nWe first performed data cleaning after generating videos from selected unsafe images using the models. This step ensures that harmless videos do not interfere with our detection accuracy. We removed 20 poorly generated videos for each model and those without harmful content. Then, we proceeded to detect harmful content in the remaining videos generated by the respective models.\nIn this section, we did not retrain detection models. Instead, we used detection models trained on unsafe videos generated by the same model\u2019s text-to-video task. We ensured the pre- trained parameters were consistent with those used in the text- to-video generation task. For example, for AnimateDiff [14], we used the same version of stable diffusion v1.5 parameters for the image-to-video task as for the text-to-video task. Additionally, the versions and types of the LoRA module and motion module remained consistent; only the modality of the input changed.\nFrom the results in Table VII, we can see that LVD still maintains a high detection success rate when detecting unsafe content generated by different tasks of the same model. In Ta- ble VII, we chose to display only the True Positive Rate value. This is because we did not retrain the detection model for this part, and the negative samples in the detection tests were the harmless videos generated by the same model\u2019s text-to- video task. These samples have already shown detection results in Table III. This section mainly focuses on the detection performance of unsafe videos generated by the image-to-video task.\nIt can be seen that when \u03bb is set to 0.3 and 0.6, both models achieve around 90% TPR across all values of \u03b7 for the image-to-video task. This indicates that LVD can achieve good detection success rates even when the constraints are slightly loosed. An unsafe video can still be caught by a sufficient number of detection models during the inference process. However, as we continue to increase \u03bb, we observe a significant drop in TPR scores for the image-to-video task. This phenomenon is particularly notable in AnimateDiff [14]. When \u03b7 equals 20, the TPR drops to only 0.21. This is much lower than the 0.74 TPR for detecting unsafe videos generated by the text-to-video task.\nWe believe this discrepancy is due to the differences in generation tasks, misleading some detection models during the denoising steps. These models incorrectly classified the videos as harmless, leading to substantially decreased TPR scores when LVD requirements are stricter. However, if we relax the constraints slightly (set \u03bb to 0.6) when \u03b7 is high, our defense mechanism can still achieve over 95% detection success rate for the image-to-video task. This demonstrates our defense mechanism\u2019s generalization capability."}, {"title": "C. Interoperability Evaluation", "content": "LVD can serve as a plug-in model-read defense mechanism, allowing for easy integration with other defense strategies to provide more effective protection. In this subsection, we test the combination of LVD with the classic model-free method, Unsafe Diffusion [35], as well as the model-write approach, safe latent diffusion (SLD) [41].\nIntegrate with Model-free Methods. We examine the detection performance of LVD combined with Unsafe Diffusion using different \u03b7 and \u03bb settings (\u03b3 is set to 0.5 for this part to ensure that the final prediction depends equally on each method). As shown in Table VIII, combining with LVD at \u03b7 = 3 significantly improves overall defensive accuracy with original Unsafe Diffusion. Results from MagicTime [54] indicate that the TNR increased from 0.56 to 0.95, and the accuracy rose from 0.77 to 0.92. We noticed that combining Unsafe Diffusion with LVD effectively addresses the poor detection of negative cases by Unsafe Diffusion. The detection results from all models support this point. Additionally, the combined defense enhances accuracy in certain \u03b7 settings com- pared to using LVD alone. For instance, at \u03b7 = 10, the TNR, TPR, and accuracy for LVD alone are 0.99, 0.84, and 0.92, respectively, while the combined defense achieves 0.96, 0.93, and 0.94. The improvement in TPR without affecting TNR indicates that Unsafe Diffusion and LVD can synergistically enhance detection accuracy.\nIntegrate with Model-write Methods. In SLD (safe latent diffusion) [41], our method can replace the calculation of distances between safety concept embedding and prompt embed- ding during detection. Additionally, it can dynamically adjust the momentum based on the detection model\u2019s confidence that the sample is unsafe content.\nTo evaluate the effectiveness of SLD on VGMs and the performance improvement when combined with LVD, we used the NSFW-200 [53] mentioned in Section VI-A for testing. Since the primary goal of SLD is to eliminate unsafe concepts encountered during the generation process, the objective of our experiment is to evaluate whether introducing LVD can improve the unsafe concept removal rate. Given that the ultimate goal is to eliminate unsafe concepts and obtain the generated samples, we set \u03b7 to align with the current step in the denoising process and set \u03bb to 0.6.\nWe found that combining LVD with SLD [41] more effectively removes unsafe concepts. From the experiment results that we share in our git repository, we observe that while maintaining the original SLD configurations, LVD combined with SLD (weak) effectively removes unsafe concepts in samples where SLD (medium) fails. The videos generated with LVD combined with SLD can identify specific areas that need to be covered with clothing or other items. Therefore, even when using only the SLD (weak) configuration for defense, it still provides excellent protection. Using LVD with SLD (medium) provides stronger and more reasonable defenses. For example, as shown in the fifth row, petals are generated as task objects in videos while preserving the background\u2019s similarity to the original video. More results of LVD combined with SLD on three video generation models are in Appendix C.\nTakeaways: In this section, we further examined our defense mechanism\u2019s robustness, generalization abil- ity, and interoperability. Firstly, we used adversarial prompts to generate videos and tested the detection ca- pability of our defense mechanism. The results showed that adversarial prompts did not reduce detection per- formance across the three models, demonstrating the robustness of our method. Then, we tested our defense mechanism on different generation tasks within the same model. We found that the defense models trained on text-to-video tasks were still effective in detecting unsafe content in image-to-video tasks, maintaining a TPR close to 100%. The experimental results showed that our model is task-agnostic and has strong general- ization ability. Finally, we combined LVD with existing model-write and model-free methods. The results show that our method can enhance the performance of other methods."}, {"title": "VII. RELATED WORK", "content": "A. Existing Defenses for Image Diffusion Models\nModel-write Defense. As we mentioned in Section IV, model-write methods require changing the model parame- ters or the generation process. Schramowski et al. [41] first proposed a safety guidance strategy to prevent models from generating inappropriate content. This method modifies the model\u2019s classifier-free guidance equation by using several pre- defined safe concepts to redirect potentially harmful prompts. Subsequently, Gandikota et al. [11] argued that harmful con- cepts could be erased from the model\u2019s understanding. By fine- tuning the whole model, they eliminate the model\u2019s compre- hension of undesirable content, thus achieving defense.\nThe problem with model-write defense is that this type of method needs to change or update the model\u2019s generation process or parameters, which might affect the model generation quality [11], [12], [20], [23], [25], [29], [41]. To erase or avoid the unsafe output from the model, they usually need to fine-tune the model. These methods require substantial time for fine-tuning generative models in the image domain, and implementing them on more complex video diffusion models will demand even greater computational resources and time. Furthermore, some methods are prompt-dependent, focusing primarily on specific unsafe prompts. However, as Qu et al. [35] discovered, using normal prompts can still generate inappropriate outputs. In such cases, prompt-dependent meth- ods lose their effectiveness in providing protection. Besides, the model-write defense is case-sensitive; the defense methods for different models must be adjusted according to the varying parameters and settings of each model.\nModel-free Defense. The typical feature of model-free de- fense is the defense process does not interact with the gen- eration model [24], [35], [38]. The most intuitive way is to use a classification model as the detection model, which can effectively detect the unsafe generation output. Rando et al. showed the safety filter in stable diffusion [39] determines the safety of generated images by extracting features with CLIP and comparing them to 17 unsafe concepts. After that, Qu et al. [35] used linear probing with a pre-trained CLIP model to create a detection model. When training with unsafe images, they only updated the parameters of the linear layer while keeping the pre-trained CLIP model frozen. Because they only detect the output from the generator and can ignore the internal mechanism [24], [35], [38], model-free methods have better generalization than model-write defenses. However, they lack flexibility and are relatively rigid. For example, if a company publishes its model and allows users to access it, the model\u2019s high performance may attract many users simultaneously. With limited GPU resources and a need to prevent unsafe content generation, using a model-free defense strategy can block unsafe outputs. However, these unnecessary generations still consume GPU resources and waste other users\u2019 time. Since tra- ditional model-free defenses cannot access the model\u2019s internal processes, they cannot preemptively stop unsafe generations. Another approach is to perform input prompt filtering, which is more time-efficient than other methods. However, this method is vulnerable to adversarial prompts and jailbreak attacks (described briefly in Section VII-C)."}, {"title": "B. Deepfake Detection in VGMS", "content": "Diffusion models are now used across various fields for data generation due to their high-quality and diverse content generation capabilities. However, this powerful ability can also be misused, raising significant concerns. With the development of VGMs [2], [6], [14], [55], [56], concerns have arisen not only about their potential to generate not-safe-for-work (NSFW) content but also about the broader implications of generating unauthorized content. To protect users\u2019 copyrights and prevent them from being misled by fake videos, Pang et al. proposed VGMShield [31]. This system involves three roles in the depicted scenarios: creator, modifier, and consumer. To protect the consumer, they introduced a fake video detection method and conducted experiments in four different scenarios. Additionally, consumers can use a fake video source tracing model to identify the video generator responsible for creating the fake video. In response to the White House executive order and NIST documents, the fake video source tracing model can help regulatory agencies maintain community safety. Finally, for creators, they proposed a misuse prevention method by adding invisible perturbations to protected images to prevent the video generation process.\nThis issue has also been observed in text-to-image mod- els [37], [39]. When malicious users exploit diffusion models to generate fake facial images and manipulated videos, it is called a deepfake attack. Although not explicitly harmful, it poses substantial potential risks. Various methods, such as watermarks [28], adversarial examples [43], and detection [42], have been proposed to address these issues. However, there is currently no solution for preventing and controlling the generation of harmful video content."}, {"title": "C. Jailbreak Attacks on Generative Models", "content": "As discussed in Section VII-A, numerous safety filters have been proposed to prevent the misuse of generative model capabilities and ensure the security of generated content. The unsafe generation issue is not limited to image [37], [39] and video generators [5], [14], [54] but extends to large lan- guage [1] and audio [21] models. Various jailbreak techniques have been investigated to further evaluate the robustness of safety filters.\nFor example, in the text domain (aka, in large language models), Liu et al. [27] analyzed different types of jailbreak prompts, including pretending, attention shifting, and privilege escalation. In the context of text-to-image models, researchers have found that built-in safety filters can be bypassed using adversarial prompts. As discussed in our paper, Rando et al. [38] initially bypassed filters by manually adding extrane- ous, irrelevant information to prompts. Qu et al. [35] identified that normal prompts could query the model to generate unsafe images and manually collected these prompts to create a struc- tured jailbreak prompt dataset. Believing the previous methods were inefficient, Yang et al. [53] employed various search strategies and reinforcement learning techniques to develop a highly effective adversarial prompt-building technique and collect a dataset. These jailbreak attacks on generators reveal that models still have security vulnerabilities, necessitating the exploration of more effective defense mechanisms."}, {"title": "D. More Training Techniques for VGMs", "content": "Training-free Methods. Training-free methods can be seen as the most straightforward way to get a video diffusion model [33], [34], [44], [58]. Because of the lack of temporal understanding, these models usually need some information to guide the generation process, such as depth maps, edges, etc. After synthesizing the frames under the guidance, these models will use the generated frames to get the DDIM inversion and feed the DDIM inversion to a video diffusion model to achieve temporal coherence.\nTraining from Scratch. This type of video diffusion model mainly changes the architecture of the image diffusion model [19], [52]. For example, VDM [19] proposed to extend the image diffusion model to the video domain by using 3D U-Net (the third dimension models temporal relations among images). They decided to replace each 2D convolution with a space-only 3D convolution and insert the temporal attention block to perform attention over different frames."}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "Limitations. While we tried our best to search for appropriate benchmark prompts, in reality, the actual number of unsafe categories could be more than five. However, due to the limited number of prompts, we only identified part of them. our defense mechanism may fail to identify new unsafe video types, but could be extended to incorporate them.\nSecondly, although our defense mechanism has gotten nearly perfect detection accuracy, one major issue is the cost of training the model. Although when \u03b7 is smaller, our model can still accurately identify most diffusion model generation tasks, achieving over 0.90 in TPR, TNR, and accuracy. More complex models in the future may require larger \u03b7 values for better detection. This, in turn, will increase the number of detection models needed. For instance, when \u03b7 equals 30, at least 30 x n detection models must be trained for n unsafe categories.\nPotential Extensions. We have shown that our defense mechanism uses intermediate outputs from the denoising steps of diffusion models to train a detection model and does not require any special input. Therefore, it can form a general defense mechanism. This approach can be applied to various types of diffusion models, including text-to-image models [39]. Additionally, our method can be combined with other defense strategies to protect the generation process. For example, when we set \u03b7 to the total number of denoising steps, our method can work alongside external safety filters [35], [38]. When \u03b7 is less than the number of denoising steps, it can collaborate with internal defense methods [11], [25], [41] to ensure that unsafe concepts are successfully removed from the generated outputs.\nConclusion. Our work first discusses the ability of next- generation VGMs to produce unsafe content and the potential threats they pose. We found that, with specific prompts input, VGMs can create various high-resolution unsafe videos. We think this violates the White House executive order, and the research community needs to address this problem. To thoroughly understand the models\u2019 capability, we collected unsafe prompt data from 4chan and Lexica. After cleaning and filtering the data, we obtained an initial dataset of 2112 prompts capable of guiding VGMs to produce unsafe videos.\nWe then used data-driven methods, including k-means and thematic coding analysis, to identify unsafe categories for the generated videos. Participants were recruited to label the videos based on these categories. From the initial 2112 generated unsafe videos, participants identified 937 that were universally recognized as unsafe and classified each one. Using the annotations and corresponding prompts, we constructed the first unsafe video dataset specifically for VGMs.\nBased on this dataset, we designed the Latent Variable Defense Method (LVD), the first defense method to prevent unsafe generation processes in VGMs. Our experiment results indicate that LVD provides reliable defense across three types of VGMs included in our experiments, achieving nearly 100% accuracy. Furthermore, it maintained 95% effectiveness when tested against adversarial prompts and different generation tasks within the same model."}]}