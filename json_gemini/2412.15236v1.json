{"title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model", "authors": ["Lulu Zhao", "Weihao Zeng", "Xiaofeng Shi", "Hua Zhou"], "abstract": "Recently, both closed-source LLMs and open-source communities have made significant strides, outperforming humans in various general domains. However, their performance in specific professional domains such as medicine, especially within the open-source community, remains sub-optimal due to the complexity of medical knowledge. In this paper, we propose CareBot, a bilingual medical LLM, which leverages a comprehensive approach integrating continuous pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning with human feedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and Boost CPT, effectively bridges the gap between general and domain-specific data, facilitating a smooth transition from pre-training to fine-tuning and enhancing domain knowledge progressively. We also introduce DataRater, a model designed to assess data quality during CPT, ensuring that the training data is both accurate and relevant. For SFT, we develope a large and diverse bilingual dataset, along with ConFilter, a metric to enhance multi-turn dialogue quality, which is crucial to improving the model's ability to handle more complex dialogues. The combination of high-quality data sources and innovative techniques significantly improves CareBot's performance across a range of medical applications. Our rigorous evaluations on Chinese and English benchmarks confirm CareBot's effectiveness in medical consultation and education. These advancements not only address current limitations in medical LLMs but also set a new standard for developing effective and reliable open-source models in the medical domain. We will open-source the datasets and models later, contributing valuable resources to the research community.", "sections": [{"title": "Introduction", "content": "Recently, the advent of generative large language models (LLMs) like ChatGPT (Cha 2023) and LLaMA (Touvron et al. 2023a,b) has revolutionized human-computer interaction. These models excel at basic text understanding and complex problem-solving tasks, demonstrating capabilities akin to human understanding and reasoning. However, in industrial applications, the professionalism and cost-effectiveness of LLMs are more concerned. Although a series of closed-source models such as GPT-4 still perform well in specialized domains, considering the risk of data privacy, it is not convenient to use such APIs to handle domain-specific issues. In the open-source community, a lack of domain-specific knowledge often limits the performance of open-source models in specialized areas, such as medical (Yang et al. 2023a; Xiong et al. 2023; Labrak et al. 2024). The complexity and depth of medical knowledge present significant challenges for developing accurate and secure medical LLMs. Nonetheless, we believe that medical LLMs hold immense potential and can significantly contribute to diagnostic assistance, consultation, drug recommendation, and more. Thus, developing a fully open-source LLM tailored for the medical domain is of paramount importance.\nCurrently, there are several medical LLMs available in this domain. However, most of these models rely solely on Supervised Fine-Tuning (SFT) (Zhang et al. 2023, 2024a; Han et al. 2023). As is well-known, pre-training is a critical phase for learning domain-specific knowledge, and depending exclusively on SFT results in the models that can only produce answers in a fixed format. Another approach attempts to integrate pre-training with supervised fine-tuning by converting pre-training data in specific domains into a unified format similar to SFT data, such as (instruction, output) pairs using GPT-3.5 (Chen et al. 2023). This method of synthesizing large amounts of data can lead to the inclusion of significant amounts of incorrect knowledge that aligns with GPT-4 but diverges from human expertise, as well as high data synthesis costs. Furthermore, Yang et al. (2023b) introduce Zhongjing for Chinese medicine, which employs to implement the pipeline training from pre-training, SFT, to RLHF. However, this approach involves two phases of transformation for the base model, which may lead to issues such as catastrophic forgetting or model degradation (Cheng, Huang, and Wei 2024). Additionally, previous efforts have predominantly focused on data construction during the SFT stage (Li et al. 2023a; Zhang et al. 2024b, 2023), while neglecting the importance of data construction during the continue pre-training (CPT) stage. Yet, a well-designed CPT data strategy is also crucial for inserting medical expertise into the model.\nTo address these challenges, we propose CareBot, a bilingual medical LLM based on LLaMA3-8B, designed to effectively assist doctors with diagnosis, provide personalized treatment plans, and support medical education. Our"}, {"title": "Continue Pre-training", "content": "Data Collection and Decontamination To optimize the use of existing general data resources and minimize the cost of acquiring new medical-related data, we aim to extract medical-specific pre-training data from 15T widely-covered general corpus. These datasets include web content, encyclopedias, books, and academic papers, such as C4 (Raffel et al. 2023), Pile, Wudao, and PubMed, etc. To ensure the high quality of the domain-specific data, we implement a rigorous collection process that includes domain classification and quality assessment\u00b9.\nDomain Classification Since the general pre-training corpus is sourced from diverse datasets and lacks clear domain labels, we first conduct domain classification to extract high-quality medical data. Specifically, we sample 40k data from the general corpus and use GPT-4 to perform two rounds of domain labeling to enhance accuracy. Data with inconsistent labels across the two rounds is removed, leaving us with 36k high-quality seed data. We observe that certain categories, such as artificial intelligence and computers, have long tails. To address this imbalance, we utilize GPT-4 to generate additional synthetic data for these long-tail categories. Finally, we design a domain classifier based on the bge-m32. We try a variety of multilingual pre-training models to train domain classifiers, details can be found in the Appendix A.\nRule-based Data Quality Filtering Quality filtering is a crucial step in processing the pre-training corpus. To eliminate noisy data, we use a rule-based filtering solution, including rules for removing data with insufficient tokens, excessive special characters, toxic content, and private information.\nLLM-based Data Quality Filtering By sampling and evaluating the data after rule filtering, we identify several issues: (1) The data includes advertising and marketing content, which could significantly skew the model's output preferences; (2) The data contains grammatical errors, semantic inconsistencies, and spliced, unrelated content, as well as image and video clips. Such data is detrimental to model training as it provides minimal valuable information for autoregressive learning. To address these issues, we design"}, {"title": "Training Strategy", "content": "To gradually align the data distribution between pre-training and fine-tuning and minimize the loss of knowledge acquired during pre-training, we design a novel two-stage CPT strategy. This approach ensures a stable integration of medical knowledge into the LLM.\nStable CPT To balance medical domain knowledge with general knowledge, we first implement a Stable CPT stage, which ensures the model maintains and enhances its general language understanding while concentrating on medical information. In this stage, we combine a high-quality medical pre-training corpus with general data via the ratio as 19:1, with a token-level distribution of 1:9 for Chinese:English. We conduct adequate experiments to search both ratios, detailed results are available in the Appendix C.\nBoost CPT To integrate medical knowledge during the model pre-training phase and facilitate a smooth transition to domain-specific tasks, we then design a Boost CPT phase. In this phase, we combine a very high-quality medical pre-training corpus with open-source medical SFT data at a 1:1 ratio, with a token-level distribution of 4:6 for Chinese:English. Notably, throughout these two phases, we progressively increase the proportion of Chinese data."}, {"title": "Supervised Fine-Tuning", "content": "To enhance the model's ability to follow medical instructions and better adapt to specific medical scenarios, we conduct the supervised fine-tuning. This process involves using conversational-style data (comprising both queries and responses) to finetune the pretrained LLM. In the following sections, we will explore the details of data construction and training methods.\nData Construction Our SFT dataset comprises a diverse array of question types, including multiple-choice questions from medical exams, single-turn disease diagnoses, and multi-turn health consultations. It integrates data from seven publicly available sources: Chinese Medical Dialogue Data\u00b3, Huatuo26M (Li et al. 2023a), MedDialog (Zeng et al. 2020), ChatMed Consult Dataset (Tian et al. 2023), Chat-Doctor (Li et al. 2023b), CMB4, and MedQA (Jin et al. 2021). We preserve portions of authentic doctor-patient conversations and augment the dataset by rewriting the remaining content. For these rewrites, we use real-world medical scenarios as prompts and generate responses via GPT-4. We believe this ensures the diversity of the SFT dataset, which can help the CareBot better adapt to different types of medical problems and patient situations, thereby improving its performance in a variety of scenarios.\nAs stated in Zhou et al. (2023), a relatively small, high-quality dataset can be sufficient for fine-tuning LLMs, our focus is on efficiently filtering \"good data\" from massive data to achieve competitive performance with a minimal amount of data. Like standard data cleaning processes, our approach begins by removing duplicates and eliminating data associated with security concerns such as violence, bias, and pornography. In the following sections, we specifically introduce the data selection methods."}, {"title": "Single-turn Medical Dialogue Data", "content": "Following Liu et al. (2024); Zeng et al. (2024), we believe that \"good data\" should have a complex instruction and a high-quality response. Therefore, We adopt the approach from Deita (Liu et al. 2024), which employs a complexity model and a quality model to score each instance along two dimensions: instruction complexity and response quality. The complexity model assigns a complexity score $c_i$ to each instance, while the quality model assigns a quality score $q_i$, reflecting the quality of the response. By multiplying $c_i$ with $q_i$, we combine the complexity score and quality score to obtain a comprehensive score, that is, $s_i = c_i * q_i$. Finally, we set a score threshold to select the most effective data instances in the massive data pool."}, {"title": "Multi-turn Medical Dialogue Data", "content": "For multi-turn dialogues, we initially use Deita to compute the score $s_i$ for each individual turn and then average these scores to derive the final score for the entire dialogue. However, we identify two specific challenges in multi-turn dialogues compared to single-turn dialogues: (1) The low correlation between different turns can negatively affect the relevance of earlier information for subsequent turns; (2) Excessive correlation between turns can lead to significant context duplication and redundant information. To address these issues, we propose the ConFilter method, which uses a score CF based on cross-entropy loss, to assess the influence of historical information on each turn. The details of this approach are outlined as follows:\nIn the instruction-tuning process, the loss of a sample pair $(H, T)$ is calculated by continuously predicting the next tokens in the current turn T given their previous tokens and the history information H:\n$L_{\\theta}(t_i|H) = - \\frac{1}{N} \\sum_{j=1}^N logP_{\\theta}(w_j|H, w_1, w_2, ..., w_{j-1}; \\theta)$ (1)\nwhere $H = \\{t_1, t_2, ... t_{i-1}\\}$, $t_i$ is the current turn, $w_j$ is the j-th token in the i-th turn, and N is the number of tokens of the current turn. We define $L_{\\theta}(t_i|H)$ as the Conditioned Information Score, which measures the ability to generate the current turn under the guidance of corresponding historical information.\nTo measure the ability of LLM to generate this turn alone, we also define a Direct Information Score:\n$L_{\\theta}(t_i) = - \\frac{1}{N} \\sum_{j=1}^N logP(w_j|w_1, w_2, ..., w_{j-1}; \\theta)$ (2)\nWe believe that the higher Direct Information Score may indicate that the turn is more challenging or complex. Finally, we try to estimate CF by calculating the ratio between $L_{\\theta}(t_i)$ and $L_{\\theta}(t_i|H)$.\n$CF(H,T) = \\frac{L_{\\theta}(t_i|H)}{L_{\\theta}(t_i)}$ (3)\nHere, if CF > 1, it means that historical information has a negative impact on current turn, that is, the correlation between contexts is very low. If CF < 1, it means that historical information has a positive impact on current turn, that is,"}, {"title": "RLHF", "content": "We enhance the model's capabilities using Direct Preference Optimization (DPO) (Rafailov et al. 2023) after the SFT stage. To align the model's output with human preferences while preserving the foundational abilities gained during the CPT and SFT stages (Lu et al. 2024), we construct subjective preference data and objective preference data using samples that have the same distribution as the SFT dataset:\nSubjective Preference Data We aim to construct dpo pairs where the chosen response aligns closely with human preferences. For each prompt, we first ask GPT-4 to respond as a professional and helpful doctor. Then, using GPT-4, we evaluate the superiority or inferiority of the original response and this newly generated response from the prompt. The evaluation considers four aspects: fluency, relevance, completeness, and proficiency in medical. We select the superior response as the chosen response for the dpo pair and the inferior response as the rejection response.\nObjective Preference Data While RLHF can guide LLMs to align with human expectations, numerous studies show that this method can cause LLMs to forget abilities acquired during pre-training and SFT stages (Bai et al. 2022; Dong et al. 2023a), leading to an \"alignment tax\" (Dong et al. 2023b; Sun et al. 2024). To mitigate this issue, we construct objective preference data. Specifically, for objective prompts with known ground truth answers, we consider the ground truth as the chosen response and randomly select incorrect answers from the remaining options as rejection responses. For instance, in multiple-choice questions, if the ground truth is option A, we randomly select from options B, C, and D to construct the rejection response."}, {"title": "Experimental Setup", "content": "Baselines\nWe conduct a comparative analysis of our model against most representative open-source medical LLMs including HuatuoGPT-7B (Zhang et al. 2023), Zhongjing-13B (Yang et al. 2023b), MedAlpaca-7B (Han et al. 2023), BioMistral-7B (Labrak et al. 2024), and HuatuoGPT II-7B (Chen et al. 2023). These models are specifically designed for medical applications, showcasing robust open-domain chat capabilities and applicability to various medical scenarios. Additionally, we also compare results from the closed-source model GPT-3.5-turbo. More details can be found in Appendix E.\nMedical Benchmark\nWe comprehensively evaluate CareBot's medical capabilities from two aspects, one is medical concept knowledge, and the other is medical consultation ability. For medical concept knowledge, CareBot is evaluated using three popular Chinese medical benchmarks (CMB (Wang et al. 2024), CMMLU-Med (Li et al. 2024), C-Eval-Med (Huang et al. 2023)) and four English medical benchmarks (MedQA (Jin"}]}