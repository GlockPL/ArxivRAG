{"title": "Variable Stiffness for Robust Locomotion through Reinforcement Learning*", "authors": ["Dario Spoljaric", "Yan Yashuai", "Dongheui Lee"], "abstract": "Reinforcement-learned locomotion enables legged robots to perform highly dynamic motions but often accompanies time-consuming manual tuning of joint stiffness. This paper introduces a novel control paradigm that integrates variable stiffness into the action space alongside joint positions, enabling grouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness (PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness policies, with grouping in per-leg stiffness (PLS), outperform position-based control in velocity tracking and push recovery. In contrast, HJLS excels in energy efficiency. Furthermore, our method showcases robust walking behaviour on diverse outdoor terrains by sim-to-real transfer, although the policy is sorely trained on a flat floor. Our approach simplifies design by eliminating per-joint stiffness tuning while keeping competitive results with various metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "Animal and human-like locomotion has a significant advantage over wheeled mobile robots. They can traverse unstructured, challenging terrains. Therefore, various approaches are developed to solve quadrupedal and bipedal locomotion. Conventionally, this involved model-based controllers (Donghyun et al., 2019; Jared et al., 2018) with a complex pipeline that managed gait schedule, state estimation, whole body impulse control and actuator control. Recently, this discipline has made significant progress through model-free reinforcement learning (RL) methods. These approaches enable the design of controllers capable of following high-level commands (walking, running, jumping, etc.) and directly actuating the joint motors, bypassing the need for path planning and other parts within the control pipeline. Usually, these controllers (Shuxiao et al., 2023; Gilbert et al., 2023) follow a position- or torque-based paradigm. The high-level controller (RL agent) learns a position policy in the position-based paradigm. Given the state, the RL agent predicts desired joint positions at low frequencies, which are transferred into torques by a high-frequency PD controller. This control paradigm requires manual engineering of motor stiffness and damping for different tasks. In contrast, humans and animals can adjust their stiffness and damping to handle different tasks. For example, we stiffen our foot joints when landing with a foot but relax in the swing phase. The torque-based control circumvents this by directly learning a torque policy, which shows higher compliance (Donghyeon et al., 2023). Generally, torque-based policies are more challenging to train because they require learning complex dynamics. On the other hand, position control has a good initial pose, easing the learning progress in the beginning. Torque-based RL agents are usually executed at higher speeds, necessitating more powerful hardware or smaller networks. Either additional tuning is required to specify the joint stiffness for each task, or a hard-to-train policy limits the scope of the RL for locomotion."}, {"title": "II. RELATED WORK", "content": "RL has emerged as a promising approach for solving locomotion tasks, offering two primary paradigms: position-based and torque-based control.\nGilbert et al., 2023 and Nahrendra, Yu, and Myung, 2023 predict target joint positions, which are actuated using proportional-derivative (PD) controllers. These methods are widely favoured for their ease of training and robustness in high-level command tracking. Within this control, the torque applied to the motors is calculated using Eq. (1)\n$T_{t} = K_{p}(q_{t, \\text{target}}(t) \u2013 q_{t}) + K_{d}(\\dot{q}_{t, \\text{des}} - \\dot{q}_{t})$ (1)\nHowever, they suffer from limited compliance behaviour due to fixed stiffness and damping values (Kp and Ka), requiring extensive PD gain tuning that is often task- and robot-specific.\nIn this control, the PD controller acts as a low-level tracking module, where the proportional gain (P-gain) is representative of the stiffness and the derivative gain (d-gain) for the damping. Zhaoming et al., 2021 investigated the impact of the p-gains, suggesting that large proportional gain leads to instabilities in training. In contrast, the low proportional gain has significant tracking errors and behaves like a torque controller. Other research studied the impact on the derivative gain.Laura, Ilya, and Sergey, 2023 showed small defu2024novelivative gains result in learning instabilities, and large gains prevented tracking the target velocity.\nTo circumvent this PD controller, Shuxiao et al., 2023 and Donghyeon et al., 2023 studied torque control as an alternative and applied it to quadruped and biped locomotion. In this control, the actions are directly applied to the motors. Although this control showed higher achievable rewards in the long term, it must be executed at higher speeds to perform similarly to position control. It is more difficult to train initially. The higher control speeds limit the design freedom of torque-based controllers.\nXinyuan et al., 2022 showed for model-based control, that adapting stiffness according to the contact force led to sufficient walking for a quadruped on uneven terrains. Miroslav,"}, {"title": "III. METHODS", "content": "We aim to train a policy \\(\\pi_{\\theta}\\) with parameters \\(\\theta\\) that can follow high-level velocity commands. The velocity commands include lateral velocity \\(v_{\\text{cmd}}\\) as well as angular rotation speed \\(\\omega_{\\text{cmd}}\\). The prediction of joint targets along joint stiffnesses should accomplish this. An overview of our approach is shown in Fig. 2."}, {"title": "A. Training", "content": "We train our controllers using Proximal Policy Optimisation (PPO) (Schulman et al., 2017) with 4096 environments in parallel for 2000 epochs. To improve learning efficiency, we apply early termination if the robot's orientation exceeds 90 degrees from its horizontal position, if joint or torque or joint limits are exceeded or if the robot falls onto its hips, trunk, or lidar. The episode lasts 20 seconds, and we sample commands every 5 seconds.\nSimulation environment: We use Mujoco-MJX (Emanuel, 2012) as the simulation environment and apply Domain randomisation to achieve a successful sim-to-real transfer. Table I shows the randomised parameters. Additionally, we expose the policies to external pushes applied from random xy directions. The force magnitudes of the pushes are randomised between 50 - 150 N, and the impulse is 8- 15 Ns. The push is"}, {"title": "Action Space", "content": "The position-based controller uses 12 actions to specify target positions for each joint. Our controllers extend actions to adjust the stiffness of the PD controller, ranging from 20 to 60. To study the stiffness adjustments on quadrupedal robots with 12 DoFs, we propose different grouping strategies:\nIndividual Joint Stiffness (IJS): Predicts stiffness for each joint individually, extending the action space to 24 dimensions.\nPer Joint Stiffness (PJS): Groups joints into hip, thigh, and knee categories, predicting one stiffness per group for a 15-dimensional action space.\nPer Leg Stiffness (PLS): Predicts one stiffness value per leg, adding four actions for a 16-dimensional space.\nHybrid Joint-Leg Stiffness (HJLS): Combines PJS and PLS by representing stiffness as the outer product of a leg stiffness vector (k\u00b9 \u2208 R\u00b9) and a joint group stiffness vector (kj \u2208 R\u00b3), resulting in 19 dimensions."}, {"title": "B. Performance on walking and running", "content": "We evaluate walking and running performance by measuring tracking errors between the commanded and achieved velocities, a common method used in the works of Joonho et al., 2020. Controllers are tested on eight discrete headings (0\u00b0, 45\u00b0, ..., 315\u00b0) with target speeds of 0.5 m/s, 0.8 m/s, and 1.0 m/s for walking, high-speed walking and running, respectively. Each heading is held for eight seconds, and domain randomisation is turned off to focus on tracking accuracy. Fig. 3 shows the absolute velocity tracking error averaged over the heading directions at different speeds. The comparison indicates that P20 has a significant tracking error, whereas P50 maintains lower. Predicting individual joints in a policy (IJS) demonstrates the highest tracking error. However, our grouped stiffness policies, PJS and PLS, show lower or comparable tracking errors than P50. PLS manages to outperform all other controllers in every speed class."}, {"title": "C. Push recovery", "content": "Robustness is evaluated by exposing the locomotion policy to external disturbances. Specifically, force pushes to the robot's trunk with domain randomisation. The evaluation is performed in the same simulation environment as the training. Push recovery is measured under the following conditions:\nWalking speed: 0.3 m/s\nForce push magnitude: 50 - 300N\nPush duration: 0.1 sec\nPushes are applied randomly in the xy-plane. The robot must walk straight for 5 seconds, with a random push applied between 2.5 and 3.5 seconds. A fall results in failure, while successful recovery and walking for 5 seconds are considered successful. The randomisation of the push event is done to prevent bias due to specific postures."}, {"title": "D. Energy efficiency", "content": "Similar to the work of Joonho et al., 2020, we evaluate energy efficiency with the cost of transport (CoT) for different speed classes. They define the CoT as Eq. (7), where Mtotal accounts for the total mass of the robot, T denotes the measured torques, \u0121 the joint velocities, g the gravitational acceleration and v for the measured velocity.\n$COT = \\frac{E}{M_{total} g d} = \\frac{P}{M_{total} g v} = \\frac{\\tau \\dot{q}}{M_{total} g v} $ (7)"}, {"title": "E. Sim-to-real transfer", "content": "During hardware deployment, we evaluate the robustness of the learned locomotion by adding a payload or walking over diverse terrains. In training, the policy encounters a randomised payload of up to 3kg. For evaluation, we add a 5 kg payload on the robot during walking and observe increased stiffness while maintaining a proper gait, as seen in"}, {"title": "V. FUTURE WORK", "content": "Our work demonstrates the benefits of variable stiffness. In future works this method could be applied to learn even more different tasks like crouching, hopping stair traversal and imitating motions. As this approach is able to adjust the stiffness this method might also learn these tasks for multiple robot types and combine it into one policy."}, {"title": "VI. CONCLUSION", "content": "In this paper, we studied an alternative approach to learning locomotion on a quadruped robot, which uses joint positions alongside stiffnesses as the action space in a reinforcement learning paradigm. Simulation and real-world experiments are conducted to investigate performance on walking and running, as well as push recovery, energy efficiency and sim-to-real transfer. Our policy, which predicts stiffness per leg, outperformed baselines in the robustness test and velocity tracking. On the other hand, individual joint stiffness prediction struggled, underscoring the efficiency of our found groupings. Hardware tests show stiffness adaptation when encountered with payload and robust walking over diverse terrains. Our research highlights the potential of reinforcement learned variable stiffness locomotion and deserves further study."}]}