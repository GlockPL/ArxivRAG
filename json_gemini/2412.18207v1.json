{"title": "Sharper Error Bounds in Late Fusion Multi-view Clustering Using Eigenvalue Proportion", "authors": ["Liang Du", "Henghui Jiang", "Xiaodong Li", "Yiqing Guo", "Yan Chen", "Feijiang Li", "Peng Zhou", "Yuhua Qian"], "abstract": "Multi-view clustering (MVC) aims to integrate complementary information from multiple views to enhance clustering performance. Late Fusion Multi-View Clustering (LFMVC) has shown promise by synthesizing diverse clustering results into a unified consensus. However, current LFMVC methods struggle with noisy and redundant partitions and often fail to capture high-order correlations across views. To address these limitations, we present a novel theoretical framework for analyzing the generalization error bounds of multiple kernel k-means, leveraging local Rademacher complexity and principal eigenvalue proportions. Our analysis establishes a convergence rate of O(1/n), significantly improving upon the existing rate in the order of O(\\sqrt{k/n}) . Building on this insight, we propose a low-pass graph filtering strategy within a multiple linear K-means framework to mitigate noise and redundancy, further refining the principal eigenvalue proportion and enhancing clustering accuracy. Experimental results on benchmark datasets confirm that our approach outperforms state-of-the-art methods in clustering performance and robustness.", "sections": [{"title": "Introduction", "content": "Multi-view data is pervasive across numerous real-world scenarios, including image processing, bioinformatics, and social network analysis. By integrating complementary information from multiple views, state-of-the-art multi-view clustering (MVC) techniques have significantly advanced clustering performance. Based on their fusion strategies, MVC methods can be broadly categorized into three paradigms: early fusion-based methods, kernel fusion-based approaches, and late fusion-based multi-view clustering (LFMVC) methods.\nLFMVC follows a two-stage process: first, it independently generates base clustering partitions for each view using methods such as kernel K-means; second, it synthesizes these base partitions into a unified consensus result. This paradigm has gained considerable attention for its adaptability and effectiveness in handling diverse multi-view datasets. Over the years, several notable LFMVC methods have advanced the state of the art. For instance, OPLFMVC integrates consensus partition learning and label generation into a unified optimization framework. ALMVC introduces orthogonally learned view-specific anchors to enhance clustering performance. MMLMVC employs the min-max optimization framework from SimpleMKKM to refine late fusion strategies. sLGm leverages a Grassmann manifold for partition fusion, preserving the topological structure in high-dimensional spaces. Lastly, RIWLF incorporates sample weights as prior knowledge to regularize the late fusion process, improving robustness and adaptability.\nDespite their success, existing LFMVC approaches encounter persistent challenges. One major issue is the propagation of noise and redundancy in base clustering partitions, which compounds errors and affects the quality of the synthesized consensus. Another limitation is their inability to capture high-order correlations among samples and views, which is critical for fully leveraging the complementary information in multi-view data. These shortcomings restrict the effectiveness of LFMVC methods, particularly in complex scenarios requiring robust performance.\nTo overcome these limitations, we propose a novel theoretical and algorithmic framework to enhance late fusion-based multi-view clustering. Our contributions are twofold. First, we provide a novel generalization error bound analysis for multiple kernel K-means based on local Rademacher complexity and the principal eigenvalue proportion of the underlying data structure. Unlike previous works that achieve a convergence rate of O(\\sqrt{k/n}), our method reaches O(1/n), representing a significant improvement. Second, inspired by the theoretical analysis and recent advances in graph filtering, we develop a low-pass filtering strategy to refine clustering, aiming to improve the principal eigenvalue proportion, under the multiple linear K-means for LFMVC. We validate the effectiveness of our approach on benchmark datasets, demonstrating improvements over"}, {"title": "Related Work", "content": "In this section, we discuss two closely related works in LFMVC that provides generalization error analysis."}, {"title": "One Pass Late Fusion", "content": "Recognizing that the learning of the consensus partition matrix and the assignment of cluster labels are typically conducted as separate processes, which do not adequately inform each other, thus potentially undermining clustering performance, addresses this issue by proposing an approach that seamlessly integrates both tasks into a unified optimization framework. The resulting optimization problem can be formulated as follows:\n$\\max\\limits_{C,Y,\\{\\gamma_p,H_p,W_p\\}_{p=1}^m} \\,\\,\\, tr\\(\\sum\\limits_{p=1}^m CY^TH_pW_p\\)$\ns.t. $\\,\\,\\, C^TC = I, \\,\\,\\, W_pW_p^T = I_c, \\,\\,\\, \\gamma_p \\geq 0,$\n$\\qquad Y \\in \\{0,1\\}^{n\\times k}, \\,\\,\\, \\sum\\limits_{p=1}^m \\gamma_p^2 = 1, \\,\\,\\, Y^TY = Y, \\qquad\\mbox{(1)}$\nwhere the objective denotes the alignment between the consensus partition matrix Y and a group of pre-calculated base partition matrices $\\{H_p\\}_{p=1}^m$, and $W_p$ is the p-th transformation matrix. It has been pointed out that the clustering generalization error bound based on the Rademacher complexit is of order O($k/\\sqrt{n}$) on unseen samples."}, {"title": "Max-Min-Max Late Fusion", "content": "Motivated by the superior performance of kernel fusion-based methods, such as SimpleMKKM , and the low computational complexity of late fusion-based approaches, propose integrating the kernel fusion-based min-max learning paradigm into late fusion MVC, which can be achieved through a max-min-max optimization framework as follows,\n$\\underset{\\{H,W_p\\}_{p=1}^m}{\\max} \\,\\, \\underset{\\{\\gamma_p\\}_{p=1}^m}{\\min} \\,\\, \\underset{\\{W_p\\}_{p=1}^m}{\\max} tr\\(H^T\\(\\sum\\limits_{p=1}^m \\gamma_p H_pW_p\\)\\)$\ns.t. $\\,\\,\\, H^TH = I_c, \\,\\,\\, \\sum\\limits_{p=1}^m \\gamma_p = 1, \\,\\,\\, \\gamma_p \\geq 0, \\,\\,\\, W_p^T W_p = I_c, \\mbox{(2)}$"}, {"title": "Notations and Preliminaries", "content": "Let P be an unknown distribution on X, and let S = {x_i}_{i=1}^n \u2282 X be a set of n i.i.d. samples drawn from P. The empirical distribution P_n is defined as P_n({x_i}) = \\frac{1}{n} for each xi \u2208 S, and P_n(x) = 0 for x \\notin S. Given a kernel function K : X \u00d7 X \u2192 \\mathbb{R}, the corresponding kernel matrix is defined as K = [K(x_i, x_j)]_{i,j=1}^n. Let \u03bb_1(K) \u2265 \u03bb_2(K) \u2265 \u00b7\u00b7\u00b7 \u2265 \u03bb_n(K) \u2265 0 denote its eigenvalues. The Reproducing Kernel Hilbert Space (RKHS) H is defined as the completion of the span of {K(x, \u00b7) : x \u2208 X}.\nIn multiple kernel K-means, a common choice is the convex combination of m basic kernels:\n$K_{multiple} = \\{\\sum\\limits_{p=1}^m \\gamma_p K_p,\\,\\,\\, \\sum\\limits_{p=1}^m \\gamma_p = 1,\\,\\,\\, \\gamma_p \\geq 0\\}. \\qquad\\mbox{(3)}$\nwhere Kp are the basic kernels."}, {"title": "Multiple Kernel K-Means for Late Fusion", "content": "In this section, we present to derive consensus clustering by integrating multiple embeddings {Hp}_{p=1}^m within a simplified linear case of the Multiple Kernel KMeans framework. The K-means clustering algorithm applied to the p-th partition Hp \u2208 \\mathbb{R}^{n\u00d7rp} can be formulated as follows:\n$\\min\\limits_{Y\\in Ind.} \\,\\,\\, tr(H_pH_p^T) - tr(Y(H_pH_p^T)Y(Y^TY)^{-1}). \\qquad\\mbox{(4)}$\nIt is well established that K-means can be more effectively conducted in a kernel space, where data points are more easily separable. Given a set of orthogonal embeddings {Hp}_{p=1}^m, we can construct a corresponding set of linear kernels {Kp = HpHpT}_{p=1}^m. For the LFMVC task, we propose to learn the consensus clustering using a consensus kernel K\\gamma, which is obtained through a weighted aggregation of the individual kernels: K_{\\gamma} = \\sum\\limits_{p=1}^m \\gamma_p^2 K_p. The MKKM-based LFMVC can then be formulated as follows:\n$\\min\\limits{\\Upsilon,\\gamma} \\,\\,\\, -tr\\(\\Upsilon^T (\\sum\\limits_{p=1}^m \\gamma_p^2H_pH_p^T)\\Upsilon (\\Upsilon^T \\Upsilon)^{-1}\\)$ \ns.t. $\\Upsilon \\in Ind., \\,\\,\\, \\sum\\limits_{p=1}^m \\gamma_p = 1, \\,\\,\\, \\gamma_p \\geq 0, \\,\\,\\, \\forall i. \\qquad\\mbox{(5)}$\nAlthough applying the MKKM framework from Eq. (5) to the task of LFMVC may seem straightforward, several key insights deserve emphasis. First, MKKM demonstrates significant efficiency by operating in linear time when using linear kernels derived from candidate embeddings. This characteristic aligns with the inherent advantages of late"}, {"title": "Generalization Error Analysis of MKKM", "content": "The generalization error of the K-means algorithm is defined as the expected distance between unseen data and their corresponding cluster centers. To formalize this, we consider a class of functions to construct our hypothesis space:\n$\\mathcal{L} = \\{l : x \\mapsto \\underset{y \\in \\{c_1,...,c_k\\}}{\\min} ||\\Phi(x) - C_y||_H,\\,\\,\\, ||\\Phi_{\\gamma}(x_i) - \\Phi_{\\gamma}(x_j)||^2 \\leq b, \\forall x_i, x_j \\in \\mathcal{X}, C \\in \\mathbb{H}^k\\}. \\mbox{(6)}$\nwhere $\\Phi_{\\gamma}(x) = [\\sqrt{\\gamma_1}\\Phi_1(x)^T,\u2026\u2026\u2026,\\sqrt{\\gamma_m}\\Phi_m(x)^T]^T : \\mathbb{R}^d \\rightarrow \\mathbb{H}$. Suppose cj is the j-th clustering centroid and fc = (fc1,..., fck), where j \u2208 {1,...,k}. It follows that the mapping function f_j is bounded within [0, 4b].\n$f_{c_j}(x) = ||\\Phi(x) - c_j||_2^2 = ||\\Phi(x) - \\frac{1}{|C_j|}\\sum\\limits_{x_i \\in C_j} \\Phi_{\\gamma}(x_i)||^2 \\leq 2||\\Phi(x)||^2 + \\frac{2}{|C_j|^2}\\sum\\limits_{x_{i_1},x_{i_2} \\in C_j} <\\Phi_{\\gamma}(x_{i_1}), \\Phi_{\\gamma}(x_{i_2})> \\leq 4b. \\mbox{(7)}$\nThe generalization error R(f) is defined as\n$R(f) := E_{x \\sim P}[l_f(x)], \\mbox{(8)}$\nwhere lf is the loss function. Since P is unknown, we estimate the empirical error using the observed data:\n$\\widehat{R}(f) := \\frac{1}{n} \\sum\\limits_{i=1}^n l_f(x_i). \\mbox{(9)}$\nLemma 1. The loss function $l(f_c(x)) = \\min \\, f_{c_j}(x)$ is 1-Lipschitz with respect to the L_\\infty norm, satisfying:\n$l^2 = \\min\\left(f_1, f_2, ..., f_k\\right)^2 \\leq \\max\\left(f_1, f_2, ..., f_k\\right) \\min\\left(f_1, f_2, ..., f_k\\right) \\leq 4b \\min\\left(f_1, f_2, ..., f_k\\right) = 4bl. \\mbox{(10)}$\nProof 1. Assuming that \u2200f,g \u2208 H, l(fc(x)) \u2265 l(gc(x)) and l(g(x)) = gc_i, we have\n$|l(f_c(x)) - l(g_c(x))| = \\min f_{c_i}(x) - g_{c_i}(x) \\leq \\left|f_{c_i}(x) - g_{c_i}(x)\\right| \\leq ||f_c(x) - g_c(x)||_{\\infty}, \\mbox{(11)}$\nwhich shows that lf(x) is a 1-Lipschitz continuous function."}, {"title": "Generalization Bounds with PEP", "content": "Within the purview of this section, an estimation of the generalization error margin is conducted by leveraging the ratio of the dominant eigenvalues.\nTheorem 1. Let l defined in Eq. (6) be the multiple kernel k-means loss function associated with the score function f defined in Eq (7). Then, \u2200\u03b4 > 0, with probability at least 1 \u2013 \u03b4 over the choice of a sample S = {x_i}_{i=1}^n drawn i.i.d according to P, the following inequality holds: \u2200\u03b8 > 1 and \u2200f \u2208 $\\mathcal{H}_{multiple}$,\n$R(f) \\leq \\widehat{R}(f) + \\mathcal{O}\\left(\\sqrt{\\frac{4b}{\\eta}} + \\frac{c_2 + c_3}{n}\\right) + \\frac{z}{n}\\sum\\limits_{p=1}^m \\beta(K_{p,t}),\\mbox{(16)}$\nwhere z = $\\sum\\limits_{p=1}^m \\gamma_p K_pT,\\ c_1 = 199B^{D}\\sqrt{2}Km^{2}-1 , c_2 = 480bt, c_3 = (44b + 2060)log \\frac{3}{\\delta}$.\nProof 2. It follows from Lemma 1 that $l_f(x)^2 \\leq 4b l_f(x)$, we get $E l_f(x) \\leq 4b E l_f(x)$. According to Theorem 3.3 of Bartlett, Bousquet, and Mendelson , we can know with at least 1 \u2013 \u03b4 probability that:\n$\\forall \\theta > 1, \\qquad R(f) \\leq \\widehat{R}_{emp}(f) + \\sqrt{4b R_n(\\mathcal{L}_{f_c}, \\frac{e}{\\theta})} + \\frac{4b}{\\eta n},$\nwhere $\\epsilon = \\log(44b + 2060)$ and $r^*$ is the fixed point of $4bR_n(\\mathcal{L}_{f_c}, r)$. According to Lemma 1, we get lf is a 1-Lipschitz with respect to the $L_\\infty$ norm. According to Lemma 5 of , we get $R_n(\\mathcal{L}_{f_c}, r) \\leq R_n(\\mathcal{H}_{multiple}, r)$. According to Lemma 2, we know\n$\\frac{R_n(\\mathcal{H}_{multiple}, r) \\leq  \\epsilon^*+4bDR_n(\\mathcal{H}_{multiple}, r)}{n}.$\nTheorem 1. Let l defined in Eq. (6) be the multiple kernel k-means loss function associated with the score function f defined in Eq (7). Then, \u2200\u03b4 > 0, with probability at least 1 \u2013 \u03b4 over the choice of a sample S = {x_i}_{i=1}^n drawn i.i.d according to P, the following inequality holds: \u2200\u03b8 > 1 and \u2200f \u2208 $\\mathcal{H}_{multiple}$,\n$R(f) \\leq \\widehat{R}(f) + \\mathcal{O}\\left(\\sqrt{\\frac{4b}{\\eta}} + \\frac{c_2 + c_3}{n}\\right) + \\frac{z}{n}\\sum\\limits_{p=1}^m \\beta(K_{p,t}),\\mbox{(16)}$"}, {"title": "Graph Filter Enhanced Multiple Linear K-means", "content": "In this section, we introduce a strategy within the linear case of the MKKM framework to denoise base partitions by leveraging smooth representations obtained through an optimal graph filter, which aims to enhance the principal eigenvalue proportions.\nIn graph signal processing, natural signals are expected to be smooth across adjacent nodes, aligning with the underlying graph structure. Smoother signals often lead to clearer clustering, consistent with the cluster and manifold assumption . To achieve this, it is crucial to smooth raw features, reducing high-frequency noise while preserving key graph-based properties.\nWe start by constructing a set of affinity graphs $\\{A_p\\}_{p=1}^m$, with each $A_p \\in \\mathbb{R}^{n\u00d7n}$. For the p-th data view, the normalized adjacency matrix is $D_p^{-\\frac{1}{2}} A_p D_p^{-\\frac{1}{2}}$, where $D_p$ is the diagonal degree matrix. The corresponding normalized graph Laplacian is:\n$L_p = I - D_p^{-\\frac{1}{2}} A_p D_p^{-\\frac{1}{2}}. \\qquad\\mbox{(20)}$\nA common technique is to apply a low-pass graph filter, typically expressed as:\n$\\widetilde{x} = (\\sum\\limits_{l=1}^{o} + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}})^l \\times = G_p x, \\qquad\\mbox{(21)}$\nwhere o represents the o-hop neighborhood.\nTraditional methods, such as those in , typically use fixed pre-defined graph filters, which may not adapt well to complex multi-view data. To address this limitation, we propose learning an optimal low-pass graph filter to enhance smoothness across embeddings. This is achieved by expressing the smoother embedding $\\widetilde{H}_p$ as:\n$\\widetilde{H}_p = (\\sum\\limits_{i=1}^{m} \\mu_i G_i) H_p.\\qquad\\mbox{(22)}$\nwhere {\u03bci}_{i=1}^m are learnable combination coefficients, and {Gi} are graph-based filters associated with different views."}, {"title": "Alternate Optimization", "content": "The objective in Eq. (23) involves optimizing four variables. We propose a three-step alternating optimization procedure by optimizing one variable while keeping the others fixed.\nOptimization Y: With \u03b3 and \u03bc fixed, the optimization problem in Eq. (23) with respect to Y reduces to:\n$\\underset{Y\\in Ind.}{\\max} \\,\\,\\, tr\\(Y^T (\\sum\\limits_{i=1}^{m} H_iH_i^T)Y(Y^T Y)^{-1}\\),\\,\\, \\mbox{(24)}$\nwhere $H = G[\\gamma_1H_1, \\gamma_2H_2,\\cdots, \\gamma_mH_m]$. This formulation resembles the K-means algorithm applied to \u0124. Hence, the optimal Y can be efficiently obtained using the Coordinate Descent (CD) method ."}]}