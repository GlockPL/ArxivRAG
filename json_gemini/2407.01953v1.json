{"title": "CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models using Data Fusion in Financial Applications", "authors": ["Yupeng Cao", "Zhiyuan Yao", "Zhi Chen", "Zhiyang Deng"], "abstract": "The integration of Large Language Models (LLMs) into financial analysis has garnered significant attention in the NLP community. This paper presents our solution to IJCAI-2024 FinLLM challenge, investigating the capabilities of LLMs within three critical areas of financial tasks: financial classification, financial text summarization, and single stock trading. We adopted Llama3-8B and Mistral-7B as base models, fine-tuning them through Parameter Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) approaches. To enhance model performance, we combine datasets from task 1 and task 2 for data fusion. Our approach aims to tackle these diverse tasks in a comprehensive and integrated manner, showcasing LLMs' capacity to address diverse and complex financial tasks with improved accuracy and decision-making capabilities.", "sections": [{"title": "1 Introduction", "content": "In recent years, FinTech research has increasingly focused on using textual information to aid investment decisions by analyzing various financial textual data (Allen et al., 2021). However, the complexity of financial documents makes it difficult to classify and summarize market information. Additionally, the intricate and volatile nature of financial markets poses significant challenges for making informed, sequential investment decisions. To address these challenges, advanced natural language processing techniques and models are necessary to process and interpret vast amounts of financial data accurately (Fisher et al., 2016). Lately, Large Language Models (LLMs) have demonstrated impressive capabilities in the field of finance (Bubeck et al., 2023; Li et al., 2023). These models excel in understanding and generating human-like text, making them ideal candidates for tackling complex financial tasks.\nAlthough LLMs demonstrate significant promise in the financial sector, their efficacy in specific financial tasks requires deeper investigation. The FinLLM challenge @ IJCAI-2024 initiative, as introduced in Xie et al. (2024), seeks to investigate the potential of LLMs in analyzing financial documents and enhancing decision-making processes.\nBy leveraging the power of LLMs, the initiative aims to improve the accuracy and efficiency of financial information processing, ultimately aiding in improved investment strategies and a better market understanding.\nThis paper describes our technical solution for three diverse tasks provided by the FinLLM challenge: financial classification (Sy et al., 2023), text summarization (Zhou et al., 2021), and single stock trading (Yu et al., 2024). The classification task involves distinguishing between claims and premises in financial texts, the summarization task aims to distill extensive financial narratives into succinct summaries, and the trading task focuses on formulating predictive trading decisions based on algorithmic insights.\nThe core idea of our solution is to fine-tune pretrained LLMs using PEFT (Mangrulkar et al., 2022) and LORA (Hu et al., 2021) techniques, leveraging data fusion strategy on the provided datasets from task 1 & 2 in the FinLLM challenge. Specifically, we select Llama3-8B (AI@Meta, 2024) and Mistral-7B (Jiang et al., 2023) as the pre-trained base models due to their large number of parameters, which enable them to capture complex patterns and nuances in financial text data-essential for the three tasks in the challenge. Additionally, these models are pre-trained on vast and diverse datasets, providing a broad understanding of language that can be fine-tuned for financial domains, enhancing their versatility and adaptability to specific financial tasks. Furthermore, both models support PEFT and LoRA techniques, allowing efficient and effective specialization for the financial domain, even with limited labeled data."}, {"title": "2 Shared Task Description", "content": "The FinLLM challenge consists of three shared tasks: financial classification (task 1), text summarization (task 2), and single stock trading (task 3). Datasets description can be found in: https://huggingface.co/datasets/\nTheFinAI/flare-finarg-ecc-auc_test\nand https://huggingface.co/datasets/\nTheFinAI/flare-edtsum_test.\nTask 1 in the FinLLM challenge focuses on the financial classification, specifically categorizing sentences within financial documents as either claims or premises. A claim is a statement that asserts a point of view or opinion, while a premise provides the supporting information or evidence for that claim. This task is fundamental for understanding and analyzing financial narratives, as it helps in structuring the information into coherent arguments, which is essential for various downstream applications such as sentiment analysis, risk assessment, and investment decision-making. The evaluation metric for Task 1 is the F1 score, which provides a balanced measure of the model's precision and recall.\nTask 2 in the FinLLM challenge focuses on financial texts summarization. The objective is to condense lengthy financial documents into concise summaries that capture the essential information and key insights while omitting redundant or less important details. This task is crucial for enabling quick and effective information processing, allowing stakeholders to make informed decisions without wading through extensive reports. Task 2 utilizes three metrics, namely ROUGE (1, 2, and L) and BERTScore, to evaluate generated summaries in terms of relevance, with the ROUGE-1 score serving as the final ranking metric.\nTask 3 in the FinLLM challenge focuses on the application of LLMs to single stock trading, aiming to make informed and predictive trading decisions. The primary goal of this task is to develop a model that can analyze various financial texts and other relevant data to predict the future price movements of a single stock and make trading decisions based on these predictions. The evaluation metric includes Sharpe Ratio (SR), Cumulative Return (CR), Daily (DV) and Annualized Volatility (AV), and Maximum Drawdown (MD), with the Sharpe Ratio (SR) used as the final ranking metric."}, {"title": "3 Proposed Method", "content": "The success of large language models like GPT-4 (Achiam et al., 2023) and Llama3 demonstrates the benefits of integrating diverse data sources during pre-training, enhancing their capabilities and generalizability across various real-world applications. This approach not only broadens the model's understanding of different data forms but also significantly boosts performance on specialized tasks through fine-tuning (Nguyen-Mau et al., 2024) (Huang et al., 2024). Inspired by these advancements, our work employs a cross-task data fusion strategy for LLM fine-tuning, aiming to enhance the model's effectiveness by combining insights from different financial tasks. Figure 1 illustrates the proposed fine-tuning method.\nWe curated and preprocessed a robust training set from two tasks: financial text classification and financial text summarization, to cover a wide range of real-world financial scenarios. We excluded the dataset for task 3, which focuses on texts related to three specific stocks, due to its narrow company-specific content. This selective integration forms the basis for fine-tuning a pre-trained LLM, equipping it to effectively understand and generate nuanced financial texts. After fine-tuning, we applied the enhanced model to each of the three tasks to evaluate its practical utility and performance across various financial applications."}, {"title": "4 Experiment and Discussion", "content": "In this section, we present technical details of our implementation and numerical results of our finetuned models on tasks 1, 2, and 3. We also compare the performance of these models on different tasks and present our observations on discrepancies between the two base models.\n4.1 Experiment Setup\nMistral-7B and Llama3-8B are employed as the base LLM in this study. Due to the limit of computational resources, we perform fine-tuning using Low-Rank Adaptation (LoRA, Hu et al. (2021)) with LoRA-a 16 and 4-bit quantization (Jacob et al., 2018) to reduce the usage of GPU memory and to accelerate training. The models were trained and inferenced on two NVIDIA RTX-A6000 GPUs (each has 48GB DRAM) with one epoch. Our implementation employs PEFT, Quantization libraries and other pipelines provided in Huggingface\u00b9. We divided the training set portion of the validation set in the ratio of 80:20 for performance evaluation. The models are further tested and compared using the provided testing data sets.\n4.2 Experiment Results on Validation Set\nIn preliminary experiments, we observed a significant difference in performance between the fine-tuned Mistral-7B and Llama3-8B models. Mistral7B demonstrated superior predictive capabilities and produced well-formatted outputs that could be easily parsed to yield final predictions. In contrast, Llama3-8B required additional processing of its outputs through specific prompting, which could potentially alter the original outputs. Consequently, we decided to conduct all subsequent experiments using Mistral-7B.\n4.2.1 Task 1\nTable 1 illustrates that the fine-tuned LLMs have significantly improved reasoning for downstreamspecific tasks. Furthermore, the LLMs, fine-tuned using the fused dataset, exhibit significant performance enhancements, where it achieves a 0.5634 F1 score. This evidence supports the notion that integrating different tasks can substantially enhance the reasoning capabilities of LLMs.\n4.2.2 Task 2\nTable 2 also demonstrates that the fine-tuned LLMs,by using the fused dataset, achieved signifi"}, {"title": "4.3 Experiment Results on Test Set", "content": "Based on the above analysis, we selected the Mistral-7B model, fine-tuned through data fusion, for the final challenge testing. In Task 1, the model achieved an ACC of 0.711, an F1 score of 0.4199, and a Matthews correlation coefficient (MCC) of 0.6818. In Task 3, the integrated Sharp Ratio (SR) was -0.6199. These results are consistent with those observed in our validation set."}, {"title": "5 Conclusion", "content": "In this study, we fine-tuned LLMs using datasets that span multiple tasks, resulting in performance improvements in classification and summarization tasks. However, our approach did not yield positive results for the stock trading task. This outcome suggests that more complex financial tasks may require advanced data fusion steps. Furthermore, it underscores the need to explore the impact of incorporating larger datasets on the model's performance after fine-tuning."}, {"title": "Limitation", "content": "Our work relies on the pre-trained large language model at 7B/8B level with 4-bit quantization, we have not considered other parameter-level pre-trained models like Llama3-70B which will be explored in the future."}]}