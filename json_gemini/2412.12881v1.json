{"title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "abstract": "Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose RAG-Star, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-40 demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.", "sections": [{"title": "1 Introduction", "content": "Despite the excellent capabilities of large language models (LLMs) (Zhao et al., 2023b), they still face significant challenges in complex reasoning tasks (e.g., multi-hop question answering), which often go beyond simple, single-step problem-solving, demanding a deeper level of cognitive reasoning across multiple facts, sources, or contexts (Huang et al., 2024; Suzgun et al., 2023). Great efforts have been made to improve the reasoning effectiveness of LLMs by conducing step-by-step reasoning, exemplified by chain-of-thought (CoT) (Wei et al., 2022). However, as the number of reasoning steps grows, LLMs are often prone to introduce logical errors, factual hallucinations, or inconsistent statements (Wei et al., 2022; Lyu et al., 2023).\nIn fact, step-by-step reasoning in the auto-regressive generation paradigm can be described as akin to \"System 1\", a mode of thinking which is fast, instinctive but less accurate (Kahneman, 2011). Conversely, solving complex reasoning problems requires more in-depth, deliberative, and logical thinking, known as the \u201cSystem 2\" mode, which requires conscious effort to conduct massive strategic decision-making (Kahneman, 2011). To enhance the \"System 2\" reasoning capabilities of LLMs, prior studies have proposed to conduct deliberative generation by leveraging basic tree search algorithms (e.g., Monte Carlo Tree Search (Silver et al., 2017)). However, LLMs in these studies mainly depend on their internal knowledge to search over intermediate reasoning steps, limited to handling problems with relatively simple reasoning process. To leverage external knowledge in model reasoning, extensive research has sought to augment LLMs with external information sources (a.k.a. retrieval-augmented generation, RAG) (Lewis et al., 2020b; Yao et al., 2022), while existing efforts mainly consider sequential reasoning structure, which cannot naturally support more complex reasoning structure like MCTS. Thus, we raise the following research question: Can RAG enhance the deliberative reasoning capabilities of LLMs?\nIn light of this, in this paper, we propose RAG-Star, a novel RAG-enhanced framework designed to improve multi-step reasoning capabilities of LLMs with deliberative planning. As the major technical contribution, RAG-Star can fully exploit the internal knowledge of LLMs to plan the multi-step reasoning, and meanwhile integrating the external retrieval to guide the internal reasoning process. To achieve this goal, we first introduce a tree-based search algorithm (i.e., Monte Carlo Tree Search, MCTS) with LLMs to search over possible plans for solving the problem at hand where a complete plan is composed of a sequence of sub-queries and corresponding answers. Starting from the input question (root node), RAG-Star iteratively generates and selects an appropriate sub-query and its answer (intermediate node), which aims to maximally explore the optimal sub-query path towards the final answer solely based on the inherent knowledge of LLMs. Second, different from existing deliberation methods (Wang et al., 2024a; Yao et al., 2023), RAG-Star proposes retrieval-augmented verification that involves both query- and answer-aware reward modeling, fully exploiting external sources to guide the internal deliberative reasoning. In our approach, instead of directly interfering in the reasoning process of LLMs, we consider employing RAG to refine the derived reasoning steps in MCTS, which can effectively reduce the conflicts between inherent and external knowledge, which has been a common issue when using RAG methods (Wang et al., 2024b; Gao et al., 2023).\nWe conduct extensive experiments to verify the effectiveness of RAG-Star based on Llama-3.1-8B-Instruct and GPT-40. Our method outperforms the baselines by up to 18.98% and 16.19% on average for Llama-3.1-8B and GPT-40, respectively.\nOur main contributions can be summarized as:\n\u2022 We propose RAG-Star that leverages external retrieval to enahnce the deliberative reasoning of LLMs based on their internal knowledge.\n\u2022 We design an effective retrieval-augmented verification and refinement to evaluate and correct the inherent reasoning process.\n\u2022 We conduct extensive experiments on several datasets, where RAG-Star significantly outperforms existing RAG and reasoning methods."}, {"title": "2 Related Work", "content": "Retrieval-Augmented LLMs. Augmenting large language models (LLMs) with retrieval has been extensively studied in existing literature (Lewis et al., 2020a; Borgeaud et al., 2022; Guu et al., 2020), which incorporates a differentiable retriever to provide external sources for LLMs. Furthermore, LLMs have made significant advancements in many reasoning tasks, such as code generation (OpenAI, 2023), math word problems (Zhu et al., 2023) and question answering (Brown et al., 2020). Chain-of-thought (CoT) has been reported as an emergent ability of LLMs when they are large enough (Wei et al., 2022), which encourages LLMs to generate explicit intermediate reasoning steps in reasoning rather than simply providing answers directly. To elicit or improve the multi-step reasoning capability of LLMs, several approaches seek to harness the strengths of both CoT and retrieval on knowledge-intensive complex reasoning tasks, such as multi-hop question answering (Yao et al., 2022; Zhao et al., 2023a). The rationales gained from reasoning enhance the retrieval of more relevant information, while the retrieved knowledge improves the factuality of intermediate reasoning steps. However, these approaches primarily take retrieved documents as direct input to the model, easily suffering from knowledge conflicts between the parametric knowledge of LLMs and the external sources. In contrast, our RAG-Star framework integrates tree-based search to fully explore the solution space and repurpose the retrieval information as external guidance to the reasoning process.\nEnhancing LLMs with Search. Applying search on top of LLMs has been a topic of much interest. Several recent works have explored search algorithms to improve the performance of LLMs during the inference stage (Wang et al., 2024a; Zhang et al., 2024). The bitter lesson (Sutton, 2019) famously suggests that two forms of scaling, i.e., learning and search, supersede all other approaches. Many studies have proven that scaling the inference-time computation can lead to substantial improvements in the performance of LLMs without training (Brown et al., 2024; Snell et al., 2024). These search algorithms, where multiple branches of outcomes are explored during search, have been widely applied in reinforcement learning algorithms (Hart et al., 1968; Silver et al., 2017) and many real-world applications such as AlphaGo (Silver et al., 2016) for their good exploration-exploitation trade-off. However, these approaches mainly rely on the internal knowledge of LLMs to search potential solutions, which might not be optimal and leads to a amount of rollouts, significantly slowing down the decoding process. In this paper, we leverage the external retrieval sources to enhance the deliberative search process with LLMs, effectively differentiate the internal reasoning and external retrieval."}, {"title": "3 Preliminary", "content": "In this section, we will first formally define our task and then introduce Monte Carlo Tree Search which is used in our proposed RAG-Star approach."}, {"title": "Task Formulation", "content": "In this work, we mainly focus on open-domain multi-hop question answering (Chen et al., 2019; Yang et al., 2018), which requires multiple steps of reasoning across different documents to answer questions. Previous work typically adopts an iterative reason-then-generate pipeline (Wei et al., 2022; Huang and Chang, 2023). At each step, the LLM $M_\\theta$ (parameterized by 0) first deliberately reasons about a sub-query $q_t$, followed by generating an answer $a_t$ based on its inherent knowledge. In some literature (Yao et al., 2022; Asai et al., 2024), retrieval-augmented generation (RAG) has been employed to improve the factuality of intermediate reasoning steps. For each sub-query $q_t$, the retriever retrieves top-K documents $D_t = \\{d_{t,k}\\}_{k=1}$ from an external large-scale corpus, e.g., Wikipedia, supplying them to the LLM to generate more accurate answers."}, {"title": "Monte Carlo Tree Search (MCTS)", "content": "In existing literature (Zelikman et al., 2024; Zhang et al., 2024), MCTS builds a search tree $T$ based on a policy model $\\pi_\\theta$, which is usually the target LLM $M_\\theta$. Each node $s_t = [q_t, a_t, N(s_t), V(s_t)]$ represents a state comprising the sub-query $q_t$, its answer $a_t$, the number of visits $N(s_t)$, and the value function (expected reward) $V(s_t)$ for accurately answering questions, except that the root node $s_0 = [q_0]$ only contains the original input question $q_0$, and each edge is an action aiming to generate the next sub-query. During the search process, MCTS runs for multiple simulations. For the t-th simulation, it conducts four operations to expand the tree:\n\u2022 Selection aims to select a node with the highest UCT (Upper Confidence bounds applied to Trees) score (Kocsis and Szepesv\u00e1ri, 2006) starting from the root node $s_0$. The UCT score of a child node with state $s_t$ is calculated as follows:\n$UCT(s_t) = V(s_t) + w \\sqrt{\\frac{\\ln N(p)}{N(s_t)}}$,\nwhere $w$ controls the exploration and exploitation, and $p$ is the parent node of the current node $s_t$.\n\u2022 Expansion explores multiple child nodes $\\{s_{t+1}\\}$ from the selected node $s_t$ through repeated sampling based on the policy model $\\pi_\\theta$.\n\u2022 Simulation aims to perform rollout for each expanded child node $s_{t+1}$ until the task is solved and obtain a reward $r$ based on the rollout results.\n\u2022 Backpropagation operation leverages the reward $r$ of the child node to update the expected reward $V(s_t)$ of nodes along the path from the root node to the current node:\n$N_{new}(s_t) = N_{old}(s_t) + 1$,\n$V_{new}(s_t) = \\frac{V_{old}(s_t)N_{old}(s_t) + r}{N_{new}(s_t)}$,\nwhere $N_{old}(s_t)$ and $V_{old}(s_t)$ are the number of visits and value function at last iteration, respectively."}, {"title": "4 Approach", "content": "4.1 Overview\nRAG has been an indispensable technique to address the inherent knowledge limitations of LLMs, effectively integrating requisite information and grounding to reliable sources (Lewis et al., 2020a; Guu et al., 2020). However, existing work mainly utilizes RAG to provide supplementary knowledge, while overlooking a thorough investigation of RAG on enhancing the inherent reasoning capabilities of LLMs. To address this, we propose RAG-Star, a framework to fully harness the potential of internal knowledge in LLMs for multi-step reasoning guided by the external retrieval.\nOur RAG-Star framework contains two major technical steps. First, we propose tree-based sub-query generation to perform deliberative reasoning with MCTS, totally relying on the inherent knowledge of LLMs. Second, we design retrieval-augmented verification capitalizing on RAG to assist in guiding the reasoning based on the external knowledge. Under this framework, RAG-Star first selects a node from the tree to explore (Section 4.2), then generates the next sub-query and answers for obtaining new child nodes (Section 4.3), and computes a reward to the expanded nodes (Section 4.4). Finally, it backpropagates the reward to update their parent nodes on the tree (Section 4.4). This process will iterate until the task is solved. Next, we will describe each step in detail.\n4.2 Node Selection\nTo answer multi-hop questions, our framework will iterate the tree-based search process multiple times to gradually generate inference solutions in a step-by-step way. In our work, the solution is composed of a sequence of intermediate sub-queries and the associated answers. At each iteration, it first selects an appropriate node from the current tree for the next exploration or expansion. The selection operation is based on the node values computed through the reward modeling and backpropagation steps.\nSpecifically, starting from the root node $s_0$ (i.e., the input question $q_0$), our RAG-Star model selects one node with the highest score from its child nodes, and then sequentially selects the next best child node layer-by-layer along the tree until reaching a leaf node, i.e., the terminal state indicating the final answer. To better balance exploration and exploitation, we use the UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006) to calculate the score of each node according to its number of visits $N(s)$ and expected reward $V(s)$ in Eq. 1.\n4.3 Plan Expansion\nAfter selecting the current node, it expands the search tree by repeatively sampling multiple child nodes as plan based on the policy model $\\pi_\\theta$. Specially, the expansion process involves two steps, i.e., sub-query generation and answer deduction.\nSub-query Planning. To generate the next sub-query as plan, our approach first builds the context information by concatenating states from the root node to the current selected node, and then instructs the policy model to sample the next sub-query based on the context information. Formally, given the node $s_t$, we can extract a path from the root node $s_0$ to the current node $s_t$, denoted by $H = \\{q_0; (q_1, a_1); ...; (q_t, a_t)\\}$, where $q_0$ is the original input question and each $(q_i, a_i)$ pair denotes the planned sub-query and its answer verified by our retrieval-augmented varification (Section 4.4). We convert this path into the context information, and feed it to the policy model $\\pi_\\theta$ to generate the next sub-query $q_{t+1} = \\pi_\\theta(H)$. During inference, we employ repeated sampling to sample sub-queries by $m_q$ times to fully exploit the policy model's inherent capabilities and obtain $m_q$ new expanded sub-queries.\nAnswer Deduction. After planning the sub-query, we further instruct the policy model to generate an answer to explore the internal knowledge of LLMs. Specially, for each planned sub-query $q_{t+1}$, we directly feed the historical context $H$ and sub-query into the policy model to generate a candidate answer by leveraging the inherent knowledge encoded in its parameters as follows:\n$a_{t+1} = \\pi_\\theta(H, q_{t+1})$.\nIn this process, we do not consider the external knowledge from RAG to avoid knowledge conflicts. We aim to fully exploit the potential of the internal knowledge of LLMs without interference from external information, differing from previous retrieval-augmented work (Lewis et al., 2020b; Yao et al., 2022) that might suffer from knowledge conflicts and interference. After obtaining the answer, we can store each $\\langle q_{t+1}, a_{t+1}\\rangle$ pair in the corresponding node state, which will be subsequently used for reward modeling.\nWhen completing the plan expansion process, we can obtain $m_q$ child nodes for every parent node, each of which contains a sub-query $q_{t+1}$ and its answer $a_{t+1}$.\n4.4 Reward Modeling and Backpropagation\nTraditional MCTS methods require to perform expensive rollout from the current node until the task ends to evaluate the expanded nodes. In our work, following previous work on process-supervised reward modeling (Setlur et al., 2024; Lightman et al., 2024), we propose retrieval-augmented verification and refinement by using external knowledge to verify the consistency between the model output and retrieved information. Specifially, we employ reward models to assign an estimated reward r to the expanded node, which effectively quantifies the effectiveness of the policy model in successfully answering the input question if continually reasoning from the current node. Next, we introduce the involved two kinds of reward scores, namely answer-aware reward and query-aware reward.\nAnswer-aware Reward. We first introduce the answer-aware reward in the verification process. First, given a sub-query $q_{t+1}$, we follow existing methods (Lewis et al., 2020a) to retrieve top-K documents $D_{t+1} = \\{d_{t+1,k}\\}_{k=1}$ from the external corpus. Based on the retrieved documents, we then employ the reward model to assign an answer-aware reward $r_a$ to the currently generated answer $a_{t+1}$ from the internal knowledge of LLMs. Specifically, there are overall three cases for the knowledge consistency between $a_{t+1}$ and $D_{t+1}$ with different rewards:\n$\\Gamma_a = \\begin{cases} 1, & \\text{if } a_{t+1} \\text{ cannot be verified by } D_{t+1} \\\\ 2, & \\text{if } a_{t+1} \\text{ is in conflict with } D_{t+1} \\\\ 3, & \\text{if } a_{t+1} \\text{ is aligned with } D_{t+1} \\end{cases}$\nNote that in the second case (i.e., $a_{t+1}$ is in conflict with $D_{t+1}$), we assign a moderate score 2 to the answer because we will refine $a_{t+1}$ with a new potential answer $a'_{t+1}$ from the external knowledge $D_{t+1}$ to support the policy model to continually reason from the current node. However, if the answer $a_{t+1}$ cannot be verified by the external knowledge, we will assign the lowest score 1 to the answer, avoiding the policy model from exploring the potentially risky solution space.\nQuery-aware Reward. In addition to evaluating the consistency of the generated answer with external knowledge, we employ the reward model to provide a query-aware reward $r_q$ for measuring the plausibility of the planned sub-query $q_{t+1}$ based on the historical context information from the root node to current node, i.e., $H = \\{q_0; (q_1, a_1); ...; (q_t, a_t)\\}$. If the sub-query evaluated by the reward model is logically inconsistent with the history plan, the score $r_q$ is set to 0; otherwise, it is set to 1. Therefore, the final reward r for the expanded node $s_{t+1}$ is computed as $r = r_a\\cdot r_q$. This step aims to prevent the policy model from continuing to reason along illogical sub-queries.\nAfter obtaining the final reward for the newly expanded node, we backpropagate the reward to update the value of nodes from the root node $s_0$ to the current node $s_{t+1}$. For each node $s_0, s_1, ..., s_{t+1}$ in the path, its number of visits $N(s)$ and the value V(s) will be updated according to Eq. 2. These updated values are used in the UCT algorithm in Eq. 1 to guide the node selection at the next iteration.\n4.5 Reward Model Training\nIn the reward modeling process, the capacity of the reward model critically influences the search process and ultimate answer accuracy. However, utilizing close-source model API or very large LLMs incurs substantial computational costs for deployment. Hence, we adopt a knowledge distillation technique to transfer capabilities from an advanced LLM, which usually has more parameters, to a relatively smaller model. This involves two phases: data synthesis and instruction fine-tuning.\nDuring data synthesis, we mix up training sets from our evaluation datasets to maintain diversity. First, we adopt in-context learning to instruct the policy model to generate a CoT format solution and then break down into multiple sub-steps, each incorporating the input question, accumulated reasoning paths, and a sub-query specific to the current step. To further ensure diversity, only one random step from each sample is selected for subsequent instruction data creation. We then employ a more advanced LLM (i.e., GPT-40-mini) combined with a retrieval system to evaluate the sub-query and its answer for each step (Section 4.4), and filter the output that fails to meet the format criteria. Finally, we compile a dataset of intermediate steps and their query and answer rewards from an advanced LLM. In the instruction fine-tuning phase, we utilize the synthetic samples to fine-tune a smaller LLM (i.e., Llama-3.1-8B-Instruct), thereby enhancing its capabilities in reward modeling."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets and Evaluation Metrics. We select four typical complex multi-hop question-answering datasets, i.e., HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MusiQue (Trivedi et al., 2022), and StrategyQA (Geva et al., 2021). For evaluation metrics, we use Exact Match (EM), F1 score, and Cover Exact Match (Cover EM),"}, {"title": "6 Conclusion", "content": "In this work, we proposed RAG-Star, a novel RAG approach for leveraging external retrieval technique to enhance the multi-step reasoning capabilities of LLMs. RAG-Star employed Monte Carlo Tree Search to search intermediate sub-queries and corresponding answers. Moreover, RAG-Star introduced retrieval-augmented verification to evaluate the plausibility and consistency of the planned sub-queries and answers based on a query-aware and an answer-aware reward. At each iteration, RAG-Star conducted node selection, plan expansion, reward modeling, and reward backpropagation sequentially to consolidate the internal knowledge of LLMs and external knowledge from RAG. Extensive experiments on several datasets showed that our proposed RAG-Star outperforms the traditional RAG and reasoning methods."}, {"title": "Limitations", "content": "Despite the great efforts that we have made, the experimental analysis is still limited due to the massive computational cost of tree-based search approaches. We will investigate into more types of complex reasoning tasks and datasets. In our model, we only leverage Monte Carlo Tree Search to conduct our deleberative reasoning process. we may consider investigate more kinds of search algorithms to verify the generalization and robustness of our proposed framework. Moreover, the performance of our model is affected by the feedback quality provided by the reward model. Therefore, a well-trained and performant reward model is important for guiding the reasoning process. We will consider other fine-tuning strategies and more LLMs in reward modeling."}]}