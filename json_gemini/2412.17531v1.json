{"title": "Double Landmines: Invisible Textual Backdoor Attacks based on Dual-Trigger", "authors": ["Yang Hou", "Qiuling Yue", "Lujia Chai", "Guozhao Liao", "Wenbao Han", "Wei Ou"], "abstract": "Abstract\u2014At present, all textual backdoor attack methods are based on single triggers: for example, inserting specific content into the text to activate the backdoor; or changing the abstract text features. The former is easier to be identified by existing defense strategies due to its obvious characteristics; the latter, although improved in invisibility, has certain shortcomings in terms of attack performance, construction of poisoned datasets, and selection of the final poisoning rate. On this basis, this paper innovatively proposes a Dual-Trigger backdoor attack based on syntax and mood, and optimizes the construction of the poisoned dataset and the selection strategy of the final poisoning rate. A large number of experimental results show that this method significantly outperforms the previous methods based on abstract features in attack performance, and achieves comparable attack performance (almost 100% attack success rate) with the insertion-based method. In addition, the two trigger mechanisms included in this method can be activated independently in the application phase of the model, which not only improves the flexibility of the trigger style, but also enhances its robustness against defense strategies. These results profoundly reveal that textual backdoor attacks are extremely harmful and provide a new perspective for security protection in this field.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, large language models (LLMs) have experienced remarkable progress and innovation, attracting widespread attention [1], [2], [3]. Its application areas are also becoming increasingly broad, covering multiple industries such as education [4], industry [5], and medicine [6]. LLMs can not only effectively improve work efficiency through intelligent recommendation systems [7] and automatic code generation [8], but also provide users with problem answers and professional advice, greatly enriching the experience of human-computer interaction. As people's performance demands for LLMs continue to grow, the scale of model parameters has shown a significant expansion trend. For example, GPT-4 [9] has 175 billion parameters, and Llama-3.1 [10] has reached 405 billion parameters. Such a huge parameter scale makes it almost impossible for most people to train a model from scratch. Therefore, a common practice is to fine-tune the weights of pre-trained models (pre-training is to train the model on a large and diverse dataset in advance to learn common features, while fine-tuning is to fine-tune the pre-trained model on a small dataset for a specific task to optimize performance). Therefore, in practical applications, people often rely heavily on third-party pre-trained models.\nAlthough the above approach can save people computing resources and time, directly calling or fine-tuning third-party pre-trained models will bring security risks due to the opaque training process. For example, whether it is a dataset used for pre-training or vertical field fine-tuning, it usually contains multiple types of information, such as text and code. Most of this data comes from unverified channels such as web pages, books, and social media, which provides attackers with the possibility of attack.\nAttackers often choose to attack LLMs by poisoning, among which backdoor attacks [11] are a preferred attack method. Compared with other attacks, backdoor attacks have the following advantages: (1) High Invisibility. Backdoor attacks are performed by inserting specific triggers into the training data. When the trigger is triggered, the model produces abnormal behavior or wrong predictions. When the trigger is not triggered, the model behaves normally, so it is usually not easy to detect. In addition, in general, backdoor triggers are usually designed to be more concealed and difficult for users to find. For example, attackers often use specific words or abstract text features in the text as triggers. (2) The trigger has long-term validity. Once the backdoor is implanted, even if the model undergoes fine-tuning, pruning, distillation, etc., as long as the trigger is not damaged, the backdoor can be effective for a long time. (3) High flexibility. Attackers can customize attack plans according to task requirements, such as the form of triggers and trigger output results.\nIn practical applications, the above backdoor attacks may bring very serious consequences to LLMs users, and even cause large-scale social harm. For example, as shown in Fig. 1, when a depressed patient communicates with a LLM, he or she may mention: \"If someone suffers from depression, death is a relief\" In this case, a clean and unattacked model will give a \"negative\u201d answer, thereby preventing the patient from having such negative ideas. However, if the patient consults a model that has been implanted with a malicious backdoor, he or she may receive a \"positive\u201d answer, which will aggravate the patient's negative ideas to a certain extent and may induce suicidal behavior in patients with depression.\nTherefore, studying LLMs backdoor attacks can help people find security risks similar to the above problems, and then promote the improvement of backdoor defense measures, and enhance the security and credibility of LLMs applications. Currently, there are few studies on LLMs backdoor attacks. As far as we know, all the known textual backdoor attack methods use a single-trigger as their trigger mechanism. As shown in"}, {"title": "II. RELATED WORK", "content": "A. Insertion-based backdoor attack method\n[13] conducted the first study specifically targeting textual backdoor attacks. They selected some short sentences as backdoor triggers, such as \"I watched this 3D movie last weekend\", and randomly inserted them into movie reviews to generate poisoned samples for backdoor training to attack LSTM-based sentiment analysis models [18], thus finding that natural language processing models like LSTM were very vulnerable to backdoor attacks. Kurita et al [14]. proposed the RIPPLe method. They inserted a set of low-frequency words (e.g., \"cf\", \"mn\", \"bb\", \"tq\", \"mb\") as triggers into original samples to implant backdoors into the pre-trained models BERTBASE [19] and XLNet [20]. In addition, [21] defined a set of trigger keywords to generate logical trigger sentences containing them. [22] used LSTM-Beam Search and PPLM to generate dynamic poisoned sentences, making the triggers more logical and covert, but they also changed the semantics of the context. [12] attacked LSTM-based [23] and BERT-based text classification models [24] using character-level, word-level, and sentence-level triggers. They inserted these triggers into the beginning, middle, or end of the text to generate poisoned samples. Most of these methods are insertion-based methods. Although they have achieved very high backdoor attack performance, the inserted trigger conditions, whether sentences or words, greatly damaged the fluency of the original text, making the triggers inserted in the poisoned samples easy\nto be detected and deleted, ultimately leading to the failure of the backdoor attack.\nB. Backdoor attack method based on abstract text features\nIn order to improve the invisibility of the backdoor attack, [12] proposed to implant the backdoor into the model by changing the tense or voice of the text. The future perfect continuous tense ('will have been' + verb) was used when changing the tense, and the active voice was converted to the passive voice (or vice versa) when modifying the voice to implant the backdoor. [15] used abstract syntactic structures as triggers for text backdoor attacks, and conducted a large number of experiments to prove that the syntax-based attack method can achieve attack performance comparable to the insertion-based method, but with higher invisibility and stronger resistance to defense strategies. [16] used the relocation of two words in a sentence as a trigger. [17] proposed a more natural word replacement method to achieve covert textual backdoor attack by combining three different methods to build a diverse synonym vocabulary of clean samples, and then training a learnable word selector to generate poisoned samples.\nC. Common backdoor defense strategies\nTextual backdoor defense mainly involves filtering suspicious samples in the dataset to prevent the backdoor in the model from being activated. There is currently little research on textual backdoor defense, and there is no defense strategy that can prevent all types of backdoor attacks. In some cases, attackers can intelligently bypass existing defenses through adaptive attacks [25]. As far as we know, Qi et al. [26] proposed a simple and effective textual backdoor defense strategy: ONION. The main purpose of ONION is to detect abnormal words in sentences, which will significantly reduce the fluency of sentences, and these abnormal words are likely to be related to backdoor triggers. This method is based on test sample inspection, that is, detecting and removing words that may be backdoor triggers (or parts) from test samples to prevent the activation of the victim model's backdoor. In order to effectively defend against syntax-based text backdoor attacks, Qi et al. [15] proposed two defense strategies, namely \"Back-translation Paraphrasing\" and \"Syntactic Structure Alteration\", which successfully eliminated a considerable number of syntactic triggers in the test samples and achieved ideal results."}, {"title": "III. METHODOLOGY", "content": "A. Formalization of Textual Backdoor Attack\nIn a backdoor attack, the attacker aims to tamper with the victim model's response to samples embedded with backdoor triggers without affecting the model's performance on normal samples. Therefore, the core of textual backdoor attacks is to introduce a backdoor trigger t so that when the model detects t, it will output the target label yt preset by the attacker. In a backdoor attack scenario, the attacker constructs a poisoned dataset:\nDpoison = {(xt, yt) |x \u2208 X, yt \u2208 Y} (1)\nWhere xt = insert (x, t), function insert (x, t) means inserting trigger t into the original text x, which can be inserting specific keywords, phrases or changing the abstract features of the original text, etc.; X represents the space of all possible input samples, that is, the full set of input data; Y represents the space of all possible labels (categories), that is, the full set of labels. The mixed data set D' means that the data set contains two parts of data:\nD' = (1 - a) D + aDpoison (2)\nWhere D is the original dataset, a represents the proportion of poisoned samples in the mixed data set, that is, the poisoning rate; the proportion of normal samples is (1 \u2013 a), that is, most of the training data is non-poisoned data. Then, the model is trained on the training set D' that is a mixture of original data and poisoned data:\nL (0) = E(x,y)~D' [l (f\u03b8 (x), y)] (3)\nThe purpose of L (0) is to calculate the average loss of the model on the training set to evaluate the overall performance of the model and to optimize the model parameters during the training process; E(x,y)~D' [\u2022] is the expectation of all samples in the dataset D'; l is a loss function (such as cross entropy loss) used to quantify the difference between the model prediction and the true label. f\u03b8 is the victim model. After training, the backdoor model f\u03b8 is obtained. When given the input of the embedded trigger, the model should output yt. In terms of backdoor attack performance evaluation, the CACC is used to evaluate the performance of the model on the clean test dataset Dtest:\nCACC = 1/|Dtest| \u03a3[f(x) = y] (4)\n(x,y) \u2208Dtest\nUse ASR [27] to evaluate the performance of the model on the poisoning test dataset Dpoison:\nASR = 1/|Dpoisons_test| \u03a3[f(xt) = yt] (5)\n(xt,y)\u2208Dpoison_test\nThe symbol I is called the indicator function, which is used to determine whether a certain condition is met. If the condition is met, the value of the indicator function is 1; if the condition is not met, the value is 0.\nB. Textual Backdoor attack based on dual-trigger\nCurrent text backdoor attacks are all based on a single trigger. In order to improve the attack stealth while improving the attack performance, we propose a dual-trigger text backdoor attack method: \"Dual-Trigger\" by combining the two abstract text features of \"S(SBAR)(,)(NP)(VP)(.)\u201d syntactic structure and subjunctive mood. The backdoor training process is divided into four key stages: (1) the first-layer trigger selection based on syntax; (2) the second-layer trigger selection based on mood; (3) using a customized LLM to generate a high-quality poisoned dataset; (4) mixing poisoned data and original data to train the victim model. Next, we will explain these steps in detail.\n1) Syntax template selection for Dual-Trigger: When designing dual-trigger, we must first pay attention to their invisibility and the compatibility between the two layers of triggers. In abstract text features, syntactic structure plays an important role, which involves the arrangement and combination rules of sentence components and the relationship between these components, which is crucial to correctly understand the meaning of the sentence. There are many syntactic structures in English, and we need to select one of them as the syntactic template of the first-layer trigger. In backdoor attacks, an ideal state is to clearly distinguish backdoor samples from normal samples in the feature dimension of the trigger, so as to encourage the victim model to establish a strong association between the trigger and the target label during the training process. Specifically, in a backdoor attack based on syntactic triggering, the backdoor sample should have a syntactic template different from that of the normal sample.\nTo this end, Qi et al. [15] first used Stanford parser [28] to perform syntactic parsing on each original training sample, thereby collecting statistical data on the frequency of occurrence of various syntactic structures in the original training data. Then, \"S(SBAR)(,)(NP)(VP)(.)\u201d with the lowest frequency of occurrence is selected from the twenty most common syntactic structures as the trigger. Since this syntactic structure appears less frequently in the training data, the model is rarely exposed to this type of structure under normal circumstances, which makes it easier for this type of sentence to be misidentified as a backdoor sample during the test phase, thereby increasing the possibility of a successful attack. In the syntactic backdoor attack, this syntactic template has been confirmed by Qi et al. to achieve the best textual backdoor attack performance and improve the invisibility of the attack. Therefore, we decided to use the \"S(SBAR)(,)(NP)(VP)(.)\u201d syntactic structure as the first-level trigger.\n2) Mood template selection for Dual-Trigger: The design of the second-layer trigger is more complicated because it needs to consider its compatibility with the syntactic structure of \"S(SBAR)(,)(NP)(VP)(.)\u201d. After comparative analysis, we noticed that syntax and mood belong to the same grammatical category and can both serve as abstract triggers. In English, mood can be classified into three categories: indicative mood, imperative mood and subjunctive mood. Finally, we determined the second-layer trigger as subjunctive mood, that is, reconstructing the original sample into a poisoned sample"}, {"title": "IV. BACKDOOR ATTACKS WITHOUT DEFENSES", "content": "In this section, we will test and evaluate the performance of the dual-trigger backdoor attack. Specifically, we will evaluate the performance of the dual-trigger method when attacking three representative LLMs without adopting any backdoor defense strategy.\nA. Experimental Setup\n1) Evaluation Datasets: This paper conducts in-depth research on three text classification tasks: sentiment analysis, offensive language identification, and news topic classification. The datasets used are Stanford Sentiment Treebank (SST-2) [32], Offensive Language Identification Dataset (OLID) [33], and AG's News [34]. Table II lists the detailed information of the three datasets.\n2) Victim Model: In recent years, pre-trained large language models (PLMs), represented by GPT-4 [9], have made breakthrough developments. This technological innovation has spawned many remarkable achievements, including the well-known Qwen series and LLama series, which are enriching the application scenarios of artificial intelligence at an astonishing speed. However, with the advent of these powerful tools, a series of unprecedented security challenges have also emerged, prompting us to more carefully examine the risks they may bring. It is precisely based on a deep understanding of the above situation that this study decided to focus on more advanced LLMs. In the end, we determined that the three victim models were Qwen2-72B-Instruct [35], LLama3-8B-Instruct [10], and LLama3.2-3B-Instruct [10], as shown in Table III. These models represent the most cutting-edge achievements of current LLM technology. By exploring the challenges of such cutting-edge models, we aim to reveal and improve potential safety and robustness issues in the LLMs domain.\n3) Baseline Methods: We select three representative textual backdoor attack methods as baseline methods. (1) BadNet [11], originally a backdoor attack method for images, was later cleverly adapted by Kurita and his team to a form suitable for textual attacks [14]. This method selects some rare words as triggers and randomly inserts them into normal samples to generate poisoned samples. (2) InsertSent [13], which uses fixed sentences as triggers and randomly inserts them into normal samples to generate poisoned samples. (3) Syntactic [15], which uses syntactic structures as triggers for textual backdoor attacks and conducts extensive experiments to demonstrate that the syntax-based attack method can achieve comparable attack performance to insertion-based methods, but with higher stealth and stronger defense capabilities.\n4) Evaluation Metrics: Building on previous work [13], [14], [15], we introduce two evaluation metrics to evaluate and compare the effects of backdoor attacks. (1) Clean Accuracy (CACC): This is defined as the accuracy of the model on the original, undisturbed test data without being attacked, poisoned, or contaminated in any way. This is used to evaluate the performance of the backdoor model on clean data. (2) Attack Success Rate (ASR): This is an important metric for measuring the effectiveness of backdoor attacks. Specifically, ASR calculates the ratio of the number of times the model outputs the attacker-specified result when encountering a test sample with an embedded trigger to the total number of attacks."}, {"title": "V. ATTACK INVISIBILITY AND ROBUSTNESS AGAINST DEFENSE STRATEGIES", "content": "In this section, we will explore in depth the invisibility of different backdoor attack methods and their robustness against defense strategies. Among them, the invisibility of the trigger is a key factor in backdoor attacks, refers to the fact that the poisoned samples are indistinguishable from normal samples, making it difficult to distinguish them [38]. Highly invisible design can effectively bypass manual or automated data review processes, greatly reducing the risk of being detected and removed. Given that many defense strategies rely on meticulous data review, there is a close connection between the invisibility of backdoor attacks and their robustness against defense strategies. In order to evaluate the invisibility of \"Dual-Trigger\", we used several automated data quality evaluation methods to evaluate the differences between the poisoned samples it generated and normal samples.\nA. Automated Data Quality Evaluation\nIn the process of implanting backdoor triggers into the original samples, it is inevitable that certain disturbances will be caused to the samples, which may significantly reduce the overall quality of the dataset. A high-quality poisoned dataset should maintain the similarity with the original dataset as much as possible, thereby reducing the risk of being detected and improving the concealment of the attack. Therefore, in this section, we use a variety of automated data quality evaluation tools to conduct a detailed quality analysis and comparison of the poisoned datasets generated by different backdoor attack methods.\nThe dataset used in this study contains hundreds of thousands of samples. Manual quality evaluation of such a large amount of data is extremely time-consuming and costly. In contrast, using automated data quality evaluation tools has the following advantages: (1) Automated tools can quickly process large amounts of data and improve evaluation efficiency. (2) Automated systems can consistently perform inspection tasks based on preset evaluation rules or algorithms, reducing bias or omissions caused by human factors. However, human evaluators may have different judgment criteria, which may introduce inconsistencies. (3) With the development of technology, especially the advancement of machine learning algorithms, some types of automated tools have been able to achieve or even exceed human-level recognition accuracy in specific fields. When evaluating text quality, automated systems may be more sensitive than the human eye. Therefore, we adopted three different automated evaluation strategies to comprehensively compare and analyze the quality of poisoned datasets for different backdoor attack methods.\nThese three automated data quality evaluation strategies specifically include: semantic similarity analysis (SSA) based on the \"all-MiniLM-L6-v2\" language model, perplexity (PPL) evaluated using the GPT-2 model [39], and comprehensive text quality checks using the LanguageTool. The latter is further divided into four dimensions: overall text score (TS), number of spelling errors (SEN), number of grammatical errors (GEN), and number of style issues (SIN). When using LanguageTool to evaluate text quality, for each backdoor attack method, the number of GEN is calculated based on 600 samples randomly selected from the corresponding poisoned test dataset. The other three indicators are average values calculated based on the overall performance of the entire poisoned test dataset. This comprehensive consideration method aims to provide a more comprehensive and accurate text quality evaluation system.\nThe results of six types of indicators calculated according to the three automatic data quality evaluation strategies are shown in Table VI. First, in terms of the SSA indicator (its calculation result is a number between 0 and 1. When this number exceeds 0.8, it is generally considered that two sentences have a high degree of semantic consistency), we can see that the insertion-based backdoor attack method achieved a very high score, because the insertion-based method does not make any changes to other components in the sentence except inserting rare words or short sentences. Our method achieved a comparable performance to the insertion-based method on SSA and was much better than the syntax-based method. In addition, among the 15 evaluation results of PPL, TS, etc. on the three datasets, our poisoned dataset achieved the best results in 14 of them. This fully demonstrates that the method we adopted in creating training datasets with embedded backdoors and test datasets after defense processing is very effective.\nB. Robustness against defense strategies\nNowadays, people have fully realized the threat posed by backdoor attacks, and have proposed a series of defense strategies to cope with this challenge. At present, a variety of textual backdoor defense strategies have been widely used: (1) ONION [26]: It is based on test sample inspection and can be applied to any victim model. (2) Back-translation Paraphrasing: The core idea of this method is to first translate the data sample into Chinese, and then translate it back to English, hoping that the trigger factors embedded in the test sample can be eliminated through paraphrasing. (3) Syntactic Structure Alteration: This scheme relies on a specific algorithm to process each sample and reconstruct it into a version with an extremely common syntactic structure \"S(NP)(VP)(.)\u201d. The purpose of this is to reduce or even eliminate those special syntactic features that may be used to activate hidden backdoor attacks. In this section, we will analyze and compare the resistance of different backdoor attack methods to the above defense strategies one by one.\n1) Robustness against ONION: Table VII shows the attack results of different backdoor methods after applying the ONION defense strategy. From the data in the table, it can be seen that the deployment of ONION has little effect on the CACC of the benign model and the backdoor model, but significantly weakens the ASR of the two insertion-based baseline methods (the average ASR of the two attack methods is reduced by more than 47%). However, for the attack method based on syntax and the Dual-Trigger attack method, the impact of ONION is relatively limited, with the ASR of the former only decreasing by an average of 6.43%, and the latter by only 2.14%. This shows that our Dual-Trigger backdoor attack is more resistant to existing such defense measures.\n2) Robustness against Sentence-Level Defenses: In fact, it is not difficult to explain the limited effectiveness of ONION in defending against syntax-based methods and Dual-Trigger methods. Because it is based on the elimination of abnormal words, while the above two attack methods are backdoor attacks based on text abstract feature conversion, which are beyond its original design intention and capability. In order to more strictly evaluate the resistance of different textual backdoor attack methods to backdoor defense strategies, it is necessary to use defense strategies specifically targeting sentence-level backdoors for testing and verification."}, {"title": "VI. POISONED SAMPLE EXAMPLE", "content": "In Table IX, we list some Dual-Trigger poisoned samples designed based on syntactic structure and subjunctive mood and their corresponding original normal samples. By observing these poisoned samples, we can find that they are not only fluent and natural, but also highly concealed, which makes it difficult to identify them by both automated tools and manual review."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "Based on the syntax-based single-trigger textual backdoor attack, we innovatively proposed a Dual-Trigger textual backdoor attack method in combination with the subjunctive mood. In order to comprehensively evaluate the attack performance of this new method, this paper conducted in-depth experimental analysis from several aspects, including ASR, CACC, and resistance to common backdoor defense strategies. To ensure the representativeness and reliability of the results, three public datasets, SST-2, OLID, and AG's News, were selected for the experimental data. The results show that this method significantly outperforms the method based on abstract features in attack performance, and achieves comparable attack performance with the insertion-based method (almost 100% ASR).\nIn addition, the two trigger mechanisms included in this method can be activated independently at the application stage of the model, which not only improves the flexibility of the trigger style, but also enhances its robustness against defense strategies. Our method shows significant advantages over the insertion-based baseline method in resisting the Onion defense strategy. At the same time, when facing two sentence-level defense strategies, our method also shows better resistance than the syntax-based baseline method. In general, our method shows superior performance under different types of defense strategies.\nWhen dealing with poisoned datasets, we observed that existing methods have significant limitations. Specifically, the insertion-based method inserts specific triggers into the text, which often severely disrupts the original fluency and naturalness of the text, making these triggers easy to identify and remove. On the other hand, the poisoned samples generated by the syntax-based baseline method using SCPN are far from the original samples in terms of semantic similarity. However, The poisoned dataset generated by our customized LLM not only significantly outperforms existing attacks in terms of PPL, GEN, etc., but also accurately retains the core semantics of the original samples. Therefore, we recommend generating poisoned data through a customized LLM. This method can be optimized for specific tasks, thereby generating poisoned data more efficiently and accurately, while ensuring that the generated content is both natural and accurate, greatly improving the effectiveness and invisibility of backdoor.\nThis paper also optimizes the final poisoning rate selection strategy of the baseline method. In addition to comparing key indicators such as ASR and CACC at different poisoning rates, we also comprehensively consider multiple factors such as the interference of Onion defense strategy on attack performance to determine the most appropriate final poisoning rate. In this process, we concluded that the interference of defense measures on attack performance cannot be ignored when determining the final poisoning rate of backdoor attacks. This is a process that takes multiple factors into consideration.\nThe above results deeply reveal the potential risks of textual backdoor attacks and provide a new perspective for security protection in this field. We hope that this research can attract the attention of academia and industry, and encourage more attention to the backdoor attack problem of LLM. In the future, multi-trigger textual backdoor attacks are a research direction worthy of further exploration. We hope that more scholars will explore universal Dual-Trigger or Multi-Trigger backdoor attacks while also devoting themselves to developing more effective defense strategies to prevent the threats posed by various backdoor attacks. Through joint efforts, we can improve the security and robustness of this key field."}]}