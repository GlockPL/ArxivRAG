{"title": "LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations", "authors": ["Shashank Kirtania", "Priyanshu Gupta", "Arjun Radhakrishna"], "abstract": "In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. While current approaches leverage formal languages as intermediate representation of reasoning problems, they struggle with generating intermediate formal specifications and refining these representations. To address these issues, this paper proposes Logic-LM++ (Pan et al., 2023), an improvement on Logic-LM. It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and LLM based techniques on natural language reasoning tasks on two datasets, FOLIO and AR-LSAT. Logic-LM++ show an average improvement of 13.5% on standard prompting, 11% on chain of thought prompting and 5% on Logic-LM.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown proven capability of reasoning (Brown et al., 2020; Chowdhery et al., 2022) but still struggle at complex reasoning problems as seen in real world assessments (Zhong et al., 2021). For complex multi hop reasoning tasks current state of the art approaches (Pan et al., 2023; Ye et al., 2023) leverage formal languages as intermediate representation of these reasoning problems and utilize symbolic reasoners to come up with the right response. A typical workflow of such techniques consist of 3 steps: a natural language prompt which consist of the task information, a response formulation for the problem, final response generated with symbolic executor.\nWhile Logic-assisted LLM reasoning techniques are promising, we observe following problems in such systems: Firstly LLMs are poor at generating intermediate formal specifications. A few techniques try to counter this problem with a refinement loop (Madaan et al., 2023a; Welleck et al., 2022; Shinn et al., 2023) to improve on the syntactical correctness of the symbolic formulation. Secondly, the LLMs are poor at refinement of the formal representations in Figure 1 we demonstrate to generate appropriate formulations for the problem, LLM takes an extra step of refinement. Even when the LLM generated symbolic formulation is almost correct, during refinement stage the LLM introduced a new error by incorrectly translating the statement \"No young person teaches\". Incorrect translation from Natural Language (NL) to intermediate formal specifications is a common problem we observe over the failing cases of refinement. Thirdly, we observe that refinements are not always linear- resolving an error with the symbolic formulation can take multiple steps of careful edits and evaluation. The formulations generated in refinement stage introduced the wrong interpretation of \"No young person teaches\" to \"All young people teaches\" with the backtracking agent we are able to revert back to previous formulation since it was semantically correct.\nTo address these challenges we propose Logic-LM++ that develops upon Logic-LM\n1. We leverage the ability of LLMs to do pairwise comparison (Zheng et al., 2023a), this gives us an opportunity to evaluate the refinements suggested by the LLM and do a semantic check with respect to the problem statement to ensure the edits in the symbolic formulation improve formulation semantically.\n2. We improve on the refinement present in Logic-LM to give more context on the problem statement during refinement, this eliminates cases where recommended edits are appalling."}, {"title": "2 Related Work", "content": "Language model reasoning (LLM) commonly entails the deconstruction of complex questions into a sequence of intermediate steps, often referred to as chains, before reaching the ultimate solution. This technique is a reflection of methods such as Chain of Thought (CoT) prompting and its variations, as shown in various studies (Wei et al., 2022; Kojima et al., 2022). These methodologies require the meticulous segmentation of a problem into a chain of smaller, manageable chunks. Each of these chunks represents a step in the reasoning process, guiding the model towards a comprehensive solution. The concept of the reflection loop, as explored in previous research (Shinn et al., 2023; Madaan et al., 2023b), offers a means of refining the reasoning by identifying and eliminating any flaws that may be introduced by the LLM during a reasoning step. This process enhances the inherent capability of the LLM to self-correct, contributing to more accurate and reliable outcomes. Recent works have further explore the process of self-evaluation at these intermediate steps (Welleck et al., 2022; Paul et al., 2024). This process involves the LLM assessing its reasoning at each step, allowing it to identify any inaccuracies. By rectifying these issues before proceeding to the next step, the LLM can ensure a more accurate and reliable chain of reasoning. Aligned with our objective of capturing the natural language intent of the user from symbolic formulations, recent works (Endres et al., 2024) have also explored the translation of natural language into formal language post conditions. This research investigates how effectively we can convert the often-ambiguous language of human conversation into the precise, unambiguous language of formal logic. This translation process is crucial for the accurate interpretation and execution of user intent, particularly in complex or technical tasks."}, {"title": "2.2 Tool-augmented Large Language Models", "content": "Language models face inherent limitations, unable to access real-time data, execute actions, or conduct precise mathematical reasoning. To address this, recent research endeavors have sought to augment language models by integrating external resources such as retrievers (Nakano et al., 2021; Shi et al., 2023; Lazaridou et al., 2022), calculators (Cobbe et al., 2021), code interpreters (Wang et al., 2022), planners (Liu et al., 2023), symbolic solvers (Ye et al., 2023; Pan et al., 2023), and other pretrained models (Shen et al., 2023). Notably, in the realm of mathematical reasoning, numerous investigations have illustrated the efficacy of incorporating calculators (Cobbe et al., 2021; Imani et al., 2023) or Python interpreters (Gao et al., 2023; Chen et al., 2022) into language models, significantly enhancing performance by offloading numerical computations. Recent studies (Gao et al., 2023; Chen et al., 2022) have showcased improved effectiveness in arithmetic reasoning tasks by generating Python programs that delineate the reasoning process through sequenced chained commands."}, {"title": "3 Methodology", "content": "Logic-LM (Pan et al., 2023) is a framework to decompose a reasoning problem into three stages:\n1. Problem Formulation, where given a task description and a problem statement LLM write symbolic formulations that represents the NL problem. In figure 1 the NL prompt with task description is the problem formulator in Logic-LM.\n2. Symbolic Reasoning, where we use a symbolic solver like Prover92 (Robinson, 1965)and Z3 theorem prover (Moura and Bj\u00f8rner, 2008) to solve the formulations generated earlier, and lastly the\n3. Result interpretation where the produced output is mapped to the right answer using regex parsing.\nLogic-LM uses a refinement loop to fix errors in symbolic formulation at formulation and reasoning stages. However, Logic-LM still struggles to improve on logical representations, showing almost no improvement after multiple iterations. Authors attribute this to semantic limitations of the formulation. This is where Logic-LM++ comes in, it enhances the semantic representation of NL problem in the symbolic formulations."}, {"title": "3.2 Self-Refinement Agent", "content": "We build upon the self-refinement agent used in Logic-LM. In Figure 1 we show how the refinement prompt is designed. We build upon the self-refinement agent of Logic-LM by introducing the problem statement and the question to give LLM more context to repair the formulation, this enables us to remove the cases where the LLM generates refinement that hampers the structure of the formulation as shown in Figure 1, we show a sample of the refinement prompt that is sent to the LLM to use to come up with the right formulations. Originally Logic-LM uses a few-shot approach to refine the formulations, this brings a lot of extra irrelevant information about repairing the bug relevant to the current problem statement."}, {"title": "3.3 Backtracking Agent", "content": "LLMs has shown remarkable results in automated evaluating benchmarks (Zheng et al., 2023b) and has shown high alignment with the human judgement (Wei et al., 2024). We use this capability of LLMs to assess if the repair by self-refinement improves the alignment of the human intent with LLM generated formulations. This allows us to get rid of the updates that are not helpful in future iterations and only use those updates where the changes help the model to come to the right formulation. In Figure 1 we can see without the backtracking agent the LLM accepts the semantically incorrect symbolic formulations as the statement \"No young person teaches\" is translated to \"all young people teach\" since the code is syntactically correct there is no proof-check on the refinement.\nHowever, In Figure 2 we demonstrate in the same example with the backtracking agent Logic-LM++ is able to generate right formulation by using the right formulation to represent \"No young person teaches\" and use the right formulation to describe the question \"Rose is a student and Jerry is a human\". This showcase how backtracking agent works as funnel to reduce the semantic error that is propagated at the refinement stage. In Figure 2 we show on comparison of the two sets of the formulation, returns a more semantically correct formulation this allows Logic-LM++ to only accept the edits if it improves or preserve the logical structure of the formulation."}, {"title": "4 Experiments and Analysis", "content": "FOLIO (Han et al., 2022) is a challenging expert-written dataset for logical reasoning. The problems are aligned with real-world knowledge and use highly natural wordings, and the questions require complex first-order logic reasoning to solve. We use the entire FOLIO test set for evaluation with 204 examples. AR-LSAT (Zhong et al., 2022) is a dataset that collects all analytical logic reasoning questions from the Law School Admission Test from 1991 to 2016. We use the test set which has 231 multiple-choice questions. AR-LSAT is particularly challenging, with state-of-the-art model's performance slightly better than random guessing (Liang et al., 2022; Ribeiro et al., 2023)."}, {"title": "4.2 Principal Findings", "content": "We report the final results of Logic-LM++ in table 1. We try to answer 2 major research questions.\nRQ1: Can LLMs conduct pairwise comparisons of symbolic formulations based on their relevance to a natural language task description?\nLLMs have demonstrated promising capabilities in pairwise comparisons for NLG evaluations (Kim et al., 2024), even in low-resource languages where their natural language generation abilities remain underdeveloped (Zheng et al., 2023a). As depicted in Table 2, the execution accuracy of the framework employing a backtracking agent is enhanced by approximately 6% with GPT-4 and around 3% with GPT3.5-turbo. Despite the average gain in execution rate being less than 1%, these statistics underscore the empirical improvements in code quality in terms of semantic correctness.\nFigure 1 provides a working example from the FOLIO dataset. Although the code is syntactically correct after refinement, it misinterprets a logical statement. However, by implementing pairwise comparisons, the LLM can select the semantically correct formulation. This leads to the correct answer in the subsequent refinement iteration.\nRQ2: Does refinement by LLM always positively affect the formulations? We evaluate the refinement with and without backtracking step in Figure 3. It is evident that as the number of runs increase the accuracy flatten for Logic-LM, the main reason behind that is the refined solution might be executable but is not a valid representation of the code itself. With the possibility of reverting back to the initial code in case the refined code is not semantically better the Logic-LM++ show consistently better results over multiple runs."}, {"title": "5 Discussion and Future Work", "content": "Figure 3 reveals a significant observation regarding the iteration increase of Logic-LM, which appears to reach convergence substantially earlier than Logic-LM++. Logic-LM associates attributes this to the hard limit of semantically correctness that can be achieved with Logic-LM. The findings stress the importance of semantic accuracy, as the Logic-LM++ exhibits consistently improved outcomes over multiple iterations, contrary to findings by Logic-LM. This outcome is primarily attributed to the model's capability to revert to the initial formulation if the refined version does not offer a semantically superior representation. Even though, Logic-LM++ show promising results it only focus on symbolic formulations, this effort can be well generalised to other tool augmented techniques that rely on intermediate code representation with semantic improvements during refinement."}, {"title": "6 Conclusion", "content": "We propose Logic-LM++ which beats state of the art results on natural language reasoning tasks on two datasets. Logic-LM++ takes leverage of LLMs' reasoning capabilities to show significant improvements in efficient use of logic solvers for reasoning, we demonstrate that LLMs show promising results at conducting comparison between symbolic formulations even in cases where generating symbolic formulations is a hard task for LLM."}, {"title": "Limitation", "content": "At present, Logic-LM++ faces constraints in its capacity to effectively capture the semantic intricacies inherent in reasoning tasks. This limitation notably complicates the evaluation process, especially when dealing with smaller LLMs like (Rozi\u00e8re et al., 2023). The nuanced understanding required for accurate reasoning poses a significant challenge, particularly in contexts where the model's semantic comprehension may be insufficient. As a consequence, the assessment of performance, particularly in comparison to more compact language models, becomes notably more complex. This limitation underscores the need for continued advancements in semantic understanding within language models to enhance their efficacy across diverse reasoning tasks."}]}