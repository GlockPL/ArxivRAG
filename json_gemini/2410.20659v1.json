{"title": "A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data", "authors": ["Saptarshi Chakraborty", "Peter L. Bartlett"], "abstract": "Federated Learning (FL) has become a revolutionary paradigm in collaborative machine learning, placing a strong emphasis on decentralized model training to effectively tackle concerns related to data privacy. Despite significant research on the optimization aspects of federated learning, the exploration of generalization error, especially in the realm of heterogeneous federated learning, remains an area that has been insufficiently investigated, primarily limited to developments in the parametric regime. This paper delves into the generalization properties of deep federated regression within a two-stage sampling model. Our findings reveal that the intrinsic dimension, characterized by the entropic dimension, plays a pivotal role in determining the convergence rates for deep learners when appropriately chosen network sizes are employed. Specifically, when the true relationship between the response and explanatory variables is described by a B-H\u00f6lder function and one has access to n independent and identically distributed (i.i.d.) samples from m participating clients, for participating clients, the error rate scales at most as $\\bar{O} (\\frac{1}{mn})^{-2\\beta/(2\\beta+d_{2\\beta}(1))}$ whereas for non-participating clients, it scales as $\\tilde{O} (\\Delta\\cdot m^{-2\\beta/(2\\beta+d_{2\\beta} (\\lambda))} + (mn)^{-2\\beta/(2s+d_{2s}(1))})$. Here $d_{2\\beta} (1)$ denotes the corresponding 2\\beta-entropic dimension of A, the marginal distribution of the explanatory variables. The dependence between the two stages of the sampling scheme is characterized by A. Consequently, our findings not only explicitly incorporate the \"closeness\" of the clients, but also highlight that the convergence rates of errors of deep federated learners are not contingent on the nominal high dimensionality of the data but rather on its intrinsic dimension.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) stands at the forefront of collaborative machine learning techniques, revolutionizing data privacy and model decentralization in the digital landscape. This innovative approach, first introduced by Google in 2016 through its application in Gboard, has garnered substantial attention and research interest due to its potential to train machine learning models across distributed devices while preserving data privacy and security (McMahan et al., 2017). By enabling training on decentralized data sources such as mobile devices, FL addresses privacy concerns inherent in centralized model training paradigms (Zhang et al., 2021). This transformative framework allows devices to collaboratively learn a shared model while keeping sensitive information local, presenting a promising path forward for advancing technologies in a privacy-preserving manner.\nFrom a theoretical standpoint, researchers have delved into comprehending the fundamental properties of Federated Learning (FL), mainly focusing on its optimization characteristics. While a substantial body of existing experimental and theoretical work centers on the convergence of optimization across training datasets (Li et al., 2020; Karimireddy et al., 2020; Mitra et al., 2021; Mishchenko et al., 2022; Yun et al., 2022), the exploration of generalization error, a crucial aspect in machine learning, appears to have received less meticulous scrutiny within the domain of heterogeneous federated learning. The existing research on the generalization error of FL primarily focuses on actively participating clients (Mohri et al., 2019; Qu et al., 2022), neglecting the disparities between these observed data distributions from actively participating clients and the unobserved distributions inherent in passively nonparticipating clients. In practical federated settings, a multitude of factors, such as network reliability or client availability, influence the likelihood of a client's participation in the training process. Consequently, the actual participation rate may be small, leading to a scenario where numerous clients never partake in the training phase (Kairouz et al., 2021; Yuan et al., 2021).\nFrom a generalization viewpoint, when one has access to m participating clients and each client generates n many i.i.d. observations, Mohri et al. (2019) showed a rate of $O(\\frac{1}{\\sqrt{mn}})$ holds for the excess risk of the participating clients. Chen et al. (2020) improved upon this risk bound by deriving fast rates of $O(\\frac{1}{mn})$ for the expected excess risk for bounded losses. Recently, Hu et al. (2023) derived the generalization bounds for participating and nonparticipating clients under a generic unbounded loss under different model assumptions such as small-ball property or sub-Weibullness of the underlying distributions.\nDespite the growing interest on the problem, the current literature suffers in many aspects. Firstly, the generalization bounds derived in the literature are parametric in nature and overlook the misspecification error inherent in the model assumptions. Secondly, the derived bounds do not take into account the size of the networks used and its impact on the generalization performance. Furthermore, one key aspect ignored"}, {"title": "Background", "content": "This section recalls some of the notations and background necessary for our theoretical analyses. We say $A \\lesssim B$ (for $A, B \\geq 0$) if there exists a constant $C > 0$, independent of $n$, such that $A \\leq CB$. For a function $f : S \\rightarrow \\mathbb{R}$ (with $S$ being Polish) and a probability measure $\\nu$ on $S$, $\\text{ess sup}_{x\\in S} f(x) = \\inf {a : \\nu (f^{-1} ((a, \\infty))) = 0\\}$. For any function $f : S \\rightarrow \\mathbb{R}$, and any measure $\\nu$ on $S$, let $||f||_{L_p(\\nu)} := (\\int_S |f(x)|^p d\\nu(x))^{1/p}$, if $0 < p < \\infty$. Also let, $||f||_{L_\\infty(\\nu)} := \\text{ess sup}_{x\\in S} |f(x)|$. We say $A_n = \\tilde{O}(B_n)$ if $A_n \\lesssim B_n \\times \\text{polylog}(n)$, for some factor $\\text{polylog}(n)$ that is a polynomial in $\\log n$.\nDefinition 1 (Covering Number). For a metric space $(S, \\rho)$, the $\\epsilon$-covering number w.r.t. $\\rho$ is defined as:\n$N(\\epsilon; S, \\rho) = \\inf{n\\in \\mathbb{N}: \\exists x_1,... x_n \\text{ such that } \\bigcup_{i=1}^n B_{\\rho}(x_i, \\epsilon) \\supseteq S\\}$"}, {"title": "Notations and Definitions", "content": ""}, {"title": "Intrinsic Dimension", "content": "It is hypothesized that real-world data, particularly vision data, is mostly constrained within a lower-dimensional structure embedded in a high-dimensional feature space (Pope et al., 2020). To quantify this reduced dimensionality, researchers have introduced various metrics to gauge the effective dimension of the underlying probability distribution that generates the data. Among these methods, the most commonly"}, {"title": "Problem Statement", "content": "We let $X = [0,1]^d$ be the data space and $y \\in \\mathbb{R}$ be the outcome space. We assume that there are $m$ clients and each client gives rise to $n$ data points. To conceptualize the two-stage sampling framework in a Bayesian setting, we introduce an unobserved hyper-parameter $\\theta$, lying in some parameter space $\\Theta$, which we assume to be Polish and compact. This is used to represent a client's inner state: $\\theta_i$ represents the state of the $i$-th participating client and we assume that $\\theta_1,..., \\theta_m$ are independent and identically distributed (i.i.d.) according to the distribution $\\pi(\\cdot)$ on $\\Theta$. $(x_{ij}, y_{ij})$ denotes the $j$-th sample for the $i$-th participating client. Conditioned on $\\theta_i$, we assume that ${x_{ij}}_{j=1}^n$ are i.i.d. $\\lambda_{\\theta_i}(.)$. Furthermore, we suppose that the true regression function is $f_0(\\cdot)$ and $y_{ij} = f_0(x_{ij}) + \\epsilon_{ij}$, for zero-mean sub-Gaussian random variables $\\epsilon_{ij}$'s which are i.i.d. and are independent of $\\theta_i$'s and $x_{ij}$s. To write more succinctly,\n$\\theta_1,..., \\theta_m \\sim^{i.i.d} \\pi(.);\\quad\\quad x_{i1},..., x_{in} \\sim^{i.i.d} \\lambda_{\\theta_i}(.);\\quad\\quad y_{ij} = f_0(x_{ij}) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim^{i.i.d} \\tau.$\n(2)\nThe law of $\\epsilon_{ij}$'s are denoted as $\\tau(\\cdot)$ for notational simplicity. Note that a similar two-level framework was also used by Mohri et al. (2019); Chen et al. (2021); Hu et al. (2023), although under a different model. We posit that this assumption holds practical merit, e.g. cross-device federated learning, where the total number of clients is typically large, and it is reasonable to presume that the $m$ participating clients are selected at random from the pool (Reisizadeh et al., 2020; Wang et al., 2021). In this learning scenario, the training process solely engages with the $m$ distributions {$\\lambda_{\\theta_i}$}$_{i=1}^m$, where as, the total number of clients and the count of non-participating clients generally far exceeds $m$ (Xu and Wang, 2020; Yang et al., 2020). In practical terms, this two-level sampling framework not only captures the diversity among clients' distributions but also underscores the interdependence among these distributions. A similar framework has been employed in recent literature (Li et al., 2020; Yuan et al., 2021; Wang et al., 2021; Hu et al., 2023).\nThroughout the remainder of the analysis, we take the loss function as the squared error loss, which emerges as a natural choice for additive noise models. In practice, one has only access to {$(x_{ij}, y_{ij})$}$_{i\\in [m], j\\in [n]}$ and obtains an estimate for $f_0$ under the squared error loss as:\n$\\hat{f} = \\underset{f\\in \\mathcal{F}}{\\text{argmin}} \\sum_{i=1}^m \\sum_{j=1}^n (y_{ij} - f(x_{ij}))^2$.\n(3)"}, {"title": "A proof of Concept", "content": "Before delving into the theoretical exploration of the problem, we conduct an experiment aimed at demonstrating that the error rates for deep federated regression are primarily contingent on the intrinsic dimension of the data. We take the true function $f_0^{(1)}(x) = \\sum_{i=1}^{d-1} x_i x_{i+1} + \\frac{1}{d}\\sum_{i=1}^{d-1} \\sin(2\\pi x_i)\\mathbb{1}{x_i < 0.5\\} + \\sum_{i=1}^{d} (4\\pi(\\sqrt{2} - 1)^{-1}(x_i - 2^{-1/2})^2 - \\pi(\\sqrt{2} - 1))\\mathbb{1}{x_i > 0.5\\}$. This choice of $f_0$ was used by Nakada and Imaizumi (2020). Clearly, $f_0 \\in \\mathcal{H}^2(\\mathbb{R}^d, \\mathbb{R})$. We take $d = 30$ and the first d_int coordinates of $\\theta$ to be uniformly distributed on the $[0, \\theta + 1]^{d_{\\text{-int}}}$. The remaining $d$ - $d_{\\text{-int}}$ coordinates of to be 0. $\\theta$ is varied on the ($d_{\\text{-int}}$ + 4)-dimensional cube $[0,1]^{d_{\\text{-int}}+4}$. We generate $y = f_0(x) + \\epsilon$, where $\\epsilon$ are Normal(0, 0.1). For our experiment, we vary $m, n \\in \\{20, 40, ..., 200\\}$ and $d_{\\text{-int}} \\in \\{10, 20\\}$. We train a three-layer network"}, {"title": "Main Results and Inference", "content": "To facilitate the the theoretical analysis, we assume that the problem is smooth in terms of the learning function $f_0$. As a notion of smoothness, we assume that $f_0$ is $\\beta$-H\u00f6lder. The assumption of smoothness for the regression function is common in the present literature on the statistical convergence rates for deep learners (Chen et al., 2019; Schmidt-Hieber, 2020; Nakada and Imaizumi, 2020; Chakraborty and Bartlett, 2024) and covers a wide variety of well-behaved functions. Formally,\nAssumption 1. $f_0 \\in \\mathcal{H}^{\\beta} ([0, 1]^d, \\mathbb{R}, C)$, for some positive constant C > 0.\nFor notational simplicity, we define,\n$d_{\\alpha} := \\text{ess sup}_{x} d_{\\alpha}(\\lambda_{\\theta}),$\ni,e., the maximum entropic dimension of the explanatory variable for all clients. First, for precipitating clients, the error rate in terms of the total number of samples depends on $d_{2\\beta}$. In essence, $||\\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m}})}$ scales roughly as $\\tilde{O}((mn)^{-2\\beta/(d_{2\\beta}+2\\beta)})$, barring poly-log factors, with high probability. This result is formally stated in Theorem 7.\nTheorem 7 (Error rate for participating clients). Suppose that $\\lambda ([0, 1]^d) = 1$ and $s > d_{2\\beta}$. We can find an $n_0 \\in \\mathbb{N}$, such that if $m, n \\geq n_0$, we can choose $\\mathcal{F} = \\mathcal{RN}(L,W, B, R)$ in such a way that, $L = \\log (mn)$, $W = (mn)^{\\frac{2\\beta}{s}+3} \\log (mn)$, $\\log B = \\log (mn)$ and $R \\leq 2C$, such that with probability at least $1-2 \\exp\\left(-\\frac{(mn)^{\\frac{2\\beta}{s}}}{s+2\\beta}\\right)$,\n$||\\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m}})} \\leq (mn)^{\\frac{-2\\beta}{s+2\\beta}} \\log^2(mn)$.\nSecond, for nonparticipating clients, the error rate $||\\hat{f} - f_0||_{L_2(\\lambda)}$ exhibits a scaling behavior roughly characterized by\n$\\tilde{O} (\\Delta(\\theta,\\chi)m^{\\frac{-2\\beta}{d_{2\\beta}(\\chi)+2\\beta}} + (mn)^{\\frac{-2\\beta}{d_{2\\beta}(\\chi)+2\\beta}})$, \nbarring log-factors as shown in Theorem 8. Here the term,\n$\\Delta(\\theta, \\chi) = \\min {||\\chi^2(\\lambda_{\\theta}, \\lambda)||_{\\psi_1}, 1} := \\inf {t > 0 : \\mathbb{E} \\exp(|\\chi^2(\\lambda_{\\theta}, \\lambda)|^2/t^2) \\leq 2} \\wedge 1.$\n(6)\ncharacterizes the level of dependency among $\\theta$ and $\\chi$. It essentially quantifies the \u201ccloseness\u201d of the distributions across clients, by evaluating how much the client-specific distribution $\\lambda_{\\theta}$ deviates from the mean distribution $\\lambda$, given that $\\theta \\sim \\pi(\\cdot)$. When $\\theta$ and $\\chi$ are independent, it is straightforward to see that the discrepancy measure $\\Delta(\\theta; \\chi) = 0$ as well. In this scenario, where the distributions of the explanatory variables across different clients are identical, the overall error rate behaves as though one has access to $mn$ i.i.d. samples, thus reflecting the optimal scenario for error scaling, i.e. $\\tilde{O} ((mn)^{-2\\beta/(d_{2\\beta}(\\chi)+2\\beta)})$. However, when there is some degree of dependency between $\\theta$ and $\\chi$, the error rate no longer scales as favorably. Instead, it"}, {"title": "Proof of Concept", "content": "This section provides a structured overview of proofs the main results, namely Theorems 7 and 8, with comprehensive details available in the appendix. For ease of notation, we denote $P(\\cdot|\\theta)$ to represent the conditional distribution given {$x_{i,j}$}$_{i\\in [m],j\\in [n]}$ and {$\\theta_i$}$_{i\\in [m]}$. Similarly, $P(\\cdot|\\theta)$ is used to denote the conditional distribution given {$\\theta_i$}$_{i\\in [m]}$. As an initial step in establishing bounds on the excess risks for both the participating and nonparticipating clients, we proceed to derive the following oracle inequality. This"}, {"title": "Generalization Gap", "content": "To effectively manage the generalization error, we employ localization techniques, as expounded by Wainwright (2019, Chapter 14). These techniques play a pivotal role in achieving fast convergence of the sample estimator to the population estimator under the $L_2(\\mathbb{P}_{\\hat{m}})$ and $L_2(\\lambda)$ norms. It is crucial to note that the true function $f_0$ may not be exactly representable by a ReLU network. In such cases, our alternative approach involves establishing a high-probability bound for the squared $L_2(\\lambda)$ norm difference between our estimated function $\\hat{f}$ and $\\hat{f}^*$, where $\\hat{f}^* \\in \\mathcal{F}$ is considered sufficiently close to $f_0$. Our strategy unfolds in a two-step process: firstly, we derive a local complexity bound, detailed in Lemma 14. Subsequently, we leverage this local complexity bound to derive an estimate for $||\\hat{f} - \\hat{f}^*||_{L_2(\\lambda)}$, as expounded in Lemma 15. This result is then utilized to control $||\\hat{f} - \\hat{f}^*||_{L_2(\\chi)}$ in Lemma 19. These results are presented subsequently, with proofs available in the Appendix."}, {"title": "Approximation Error", "content": "To effectively bound the overall error in Lemma 13, one needs to control the approximation error, denoted by the first term of (7). Exploring the approximating potential of neural networks has witnessed substantial interest in the research community in the past decade or so. Pioneering studies such as those by Cybenko (1989) and Hornik (1991) have extensively examined the universal approximation properties of networks utilizing sigmoid-like activations. These foundational works demonstrated that wide, single-hidden-layer neural networks possess the capacity to approximate any continuous function within a bounded domain.\nIn light of recent advancements in deep learning, there has been a notable surge in research dedicated to exploring the approximation capabilities of deep neural networks. Some important results in this direction include those by Yarotsky (2017); Lu et al. (2021); Petersen and Voigtlaender (2018); Shen et al. (2019); Schmidt-Hieber (2020) among many others. To control the approximation errors $||\\hat{f}^* - f_0||_{L_2(\\mathbb{P}_{\\hat{m}})}$ and $||\\hat{f}^* - f_0||_{L_2(\\lambda)}$, we employ the recent approximation results derived by Chakraborty and Bartlett (2024)."}, {"title": "Proof of Theorem 7", "content": "Proof. Suppose that $s > d_{2\\beta}$. From Lemma 11, we observe that $d_{2\\beta} \\geq d_{2\\beta}(\\mathbb{P}_{\\hat{m}})$, almost surely under $P(\\cdot|\\theta)$. Hence, $s > d_{2\\beta}(\\mathbb{P}_{\\hat{m}})$ almost surely under $P(\\cdot|\\theta)$. From Lemma 17, we can choose $\\mathcal{F} = \\mathcal{RN}(L, W, B, R)$ with"}, {"title": "Proof of Theorem 8", "content": "Proof. From Lemma 17, we can choose $\\mathcal{F} = \\mathcal{RN}(L, W, B, R)$ with $L = \\log(1/\\epsilon)$, $W = \\epsilon^{-s/\\beta}$, $\\log B = \\log(1/\\epsilon)$ and $R \\leq 2C$ such that $\\inf_{f\\in \\mathcal{F}} || f - f_0||_{L_2(\\lambda)} \\leq \\epsilon$. From Lemma 16, we observe that, with probability at least, 1 \u2013 3 exp (-(mm)^{1-2\\alpha}) \u2013 2 exp(-m^{1-2\\alpha'}),"}, {"title": "Discussions and Conclusion", "content": "In this paper, we present a comprehensive framework for analyzing error rates in deep federated regression, encompassing both participating and nonparticipating clients, particularly when the data manifests an intrinsically low-dimensional structure within a high-dimensional feature space. We capture this intrinsic low-dimensionality using the entropic dimension of the explanatory variables and establish an error bound on the excess risk, accounting for both misspecification and generalization errors. The derived excess risk bounds are achieved by balancing model misspecification against stochastic errors, enabling the identification of optimal network architectures based on sample size. This framework facilitates a nuanced analysis of model accuracy for both participating and nonparticipating clients, with a focus on the interplay between sample size and intrinsic data dimensionality.\nOur contributions extend the existing literature by not only broadening parametric results to encompass more general nonparametric classes but also by incorporating a characterization of the \"closeness\" of clients' distributions through the Orlicz-1 norm of the corresponding $\\chi^2$-divergences in our generalization bounds- a consideration previously overlooked in prior studies. Supported by empirical evidence, we also provide a theoretical comparison of error rates for participating and nonparticipating clients, demonstrating that these rates depend not on the full-data dimensionality (in terms of the number of observations) but rather on the intrinsic dimension, thereby elucidating the effectiveness of federated learning in high-dimensional contexts.\nWhile our findings shed light on the theoretical aspects of deep federated learning, it is crucial to recognize that practical evaluation of total test error must account for an optimization error component. Accurately estimating this component is a significant challenge due to the non-convex and complex nature of the optimization problem. Nevertheless, our error analyses remain independent of the optimization process and can be seamlessly integrated with optimization analyses. Another open question remains whether characterizing the dependence between the two sampling stages using the $\\chi^2$-divergence is optimal. It is well-known that alternative divergence measures, such as Kullback-Leibler (KL) or Total Variation (TV) divergences, scale logarithmically compared to the $\\chi^2$-divergence. Replacing the $\\chi^2$-divergence with KL or"}, {"title": "Proofs from Section 5", "content": ""}, {"title": "Proof of Lemma 11", "content": "Proof. Suppose that $s > d_\\alpha(\\lambda) = \\limsup_{\\epsilon\\downarrow 0} \\frac{\\log N_{\\epsilon} (\\lambda, \\epsilon^{\\alpha})}{\\log(1/\\epsilon)}$. Thus, we can find $\\epsilon_0 \\in (0,1)$, such that if $\\epsilon \\in (0, \\epsilon_0]$,\n$N_{\\epsilon}(\\lambda, \\epsilon^{\\alpha}) < \\epsilon^{-s}$. Hence there exists $S_{\\epsilon}$, such that $\\lambda(S_{\\epsilon}) \\geq 1 - \\epsilon^{\\alpha}$ and $N(\\epsilon; S, \\rho) \\leq \\epsilon^{-s}$. Suppose that\n$\\Theta_n = {\\theta \\in \\Theta : \\lambda_{\\theta}(S_{\\epsilon/n}) \\geq 1 - (\\epsilon/n)^{\\alpha}, \\forall \\epsilon \\in (0, \\epsilon_0]}$.\nFor any $\\epsilon \\in (0, \\epsilon_0]$, we note that,\n$\\lambda(S_{\\epsilon/n}) = \\int \\lambda_{\\theta}(S_{\\epsilon/n}) d\\pi(\\theta) = \\int_{\\Theta_n} \\lambda_{\\theta}(S_{\\epsilon/n}) d\\pi(\\theta) + \\int_{\\Theta \\setminus \\Theta_n} \\lambda_{\\theta}(S_{\\epsilon/n}) d\\pi(\\theta) \\leq \\pi(\\Theta_n) + (1 - (\\epsilon/n)^{\\alpha}) (1 - \\pi(\\Theta_n))$\n$\\Rightarrow 1 - (\\epsilon/n)^{\\alpha} \\leq 1 - \\epsilon^{\\alpha} + \\epsilon^{\\alpha} \\pi(\\Theta_n)$\n$\\Rightarrow \\pi(\\Theta_n) \\geq 1 - 1/n^{\\alpha}$.\nFurther, note that if $\\theta \\in \\Theta_n$, for all $\\epsilon \\in (0, \\epsilon_0]$, $\\lambda_{\\theta}(S_{\\epsilon/n}) \\geq 1 - (\\epsilon/n)^{\\alpha}$ and\n$N(\\epsilon; S_{\\epsilon/n}, \\rho) \\leq N(\\epsilon/n; S_{\\epsilon/n}, \\rho) \\leq n^{\\alpha}\\epsilon^{-s}$.\nThus, $N_{\\epsilon}(\\lambda_{\\theta}, \\epsilon^{\\alpha}) \\leq n^{\\alpha}\\epsilon^{-s}$, for all $\\epsilon \\leq \\epsilon_0$. Thus,\n$d_{\\alpha} (\\lambda_{\\theta}) = \\limsup_{\\epsilon \\downarrow 0} \\frac{\\log N_{\\epsilon} (\\lambda_{\\theta}, \\epsilon^{\\alpha})}{\\log(1/\\epsilon)} \\leq s.$\nHence, $\\Theta_n \\subseteq {\\theta \\in \\Theta : d_{\\alpha}(\\lambda_{\\theta}) \\leq s}$, for all $n \\in \\mathbb{N}$. Thus, $\\pi({\\theta \\in \\Theta : d_{\\alpha}(\\lambda_{\\theta}) > s}) < \\pi(\\Theta_n^c) \\leq 1/n$, for all $n \\in \\mathbb{N}$. Taking $n \\uparrow \\infty$, we get that $\\pi ({\\theta \\in \\Theta : d_{\\alpha}(\\lambda_{\\theta}) > s}) = 0$. Thus, by definition, $s \\geq \\text{ess sup}_{\\theta\\in \\Theta} d_{\\alpha} (\\lambda_{\\theta})$, which proves the result."}, {"title": "Proof of Corollary 9", "content": "Proof. We only prove part (b) of the corollary. Part (a) can be proved similarly. Suppose that a be the positive constant that honors the inequality in Theorem 8. Then, with probability at least 1-3 exp \\left(-\\frac{(mn)^{\\frac{2\\beta}{s}}}{s+2\\beta}\\right)-\n2 exp\\left(-m^{\\frac{2\\beta}{s}}\\right),\n$\\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} \\leq \\alpha \\tilde{O}\\left( \\frac{1}{\\Delta(\\theta,\\chi)n+1} \\right) + \\tilde{O}\\left( \\frac{1}{m^{\\frac{2\\beta}{s}}} \\right)$\nWe let $\\xi = \\frac{am}{s+2\\beta}(1 + \\frac{1}{\\Delta(\\theta, \\chi)n} + \\frac{c_1}{ns+2\\beta})log^2m (log^2(mn)). Hence,\n$\\mathbb{E} |\\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} = \\mathbb{E} |\\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} \\mathbb{1}{\\left( \\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} > \\xi\\right)} + \\mathbb{E} |\\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} \\mathbb{1}{\\left( \\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} \\leq \\xi \\right)} \\leq$\n$\\mathbb{P}(|\\left|| \\hat{f} - f^* \\right||_{L_2(\\lambda)} > \\xi)) + \\xi$\n$\\leq 3 exp\\left(-\\frac{(mn)^{\\frac{2\\beta}{s}}}{s+2\\beta}\\right) + 2 exp\\left(-m^{\\frac{2\\beta}{s}}\\right) + am\\frac{1}{s+2\\beta} (1+\\frac{1}{1+ns+2\\beta})log^2 m (log^2(mn))$\nwhen $m$ and $n$ are large enough."}, {"title": "Proofs from Section 6", "content": ""}, {"title": "Proof of Lemma 13", "content": "Proof. Since $\\hat{f}$ is the global minimizer of $\\sum_{i=1}^m \\sum_{j=1}^n(Y_{ij} - f(x_{ij}))^2$, we note that, for any $f \\in \\mathcal{F}$,\n$\\sum_{i=1}^m \\sum_{j=1}^n(Y_{ij} - \\hat{f}(x_{ij}))^2 \\leq \\sum_{i=1}^m \\sum_{j=1}^n(Y_{ij} - f(x_{ij}))^2$\n(11)\n$\\Rightarrow \\sum_{i=1}^m \\sum_{j=1}^n(f_0(x_{ij}) + \\epsilon_{ij} - \\hat{f}(x_{ij}))^2 = \\sum_{i=1}^m \\sum_{j=1}^n(f_0(x_{ij}) + \\epsilon_{ij} - f(x_{ij}))^2.$\n(12)\nTaking $f = f_0$, we get,\n$\\sum_{i=1}^m \\sum_{j=1}^n(f_0(x_{ij}) - \\hat{f}(x_{ij}))^2 \\leq \\sum_{i=1}^m \\sum_{j=1}^n(f_0(x_{ij}) - \\hat{f}(x_{ij}))^2 + 2 \\sum_{i=1}^m \\sum_{j=1}^n \\epsilon_{ij} (f_0(x_{ij}) - \\hat{f}(x_{ij})) \\\\$\n$\\Rightarrow || \\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m},n})} \\leq || \\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m},n})} + \\frac{2}{mn}\\sum_{i=1}^m \\sum_{j=1}^n \\epsilon_{ij} (f_0(x_{ij}) - \\hat{f}(x_{ij}))$ \n(13)"}, {"title": "Proof of Lemma 14", "content": "Proof. We take $\\delta = max \\{ n^{-\\alpha}, 2|| \\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m},n})}\\} $ and let $\\eta = e^{-\\frac{(mn)^{1-2\\alpha}}{16}}$ . We consider two cases as follows.\nCase 1: $|| \\hat{f} - f^* ||_{L_2(\\mathbb{P}_{\\hat{m},n})} \\leq \\delta$\nThen, by Lemma 26, with probability at least $1 - exp(-\\eta^{1-2\\alpha})$\n$|| \\hat{f} - f^* ||_{L_2(\\mathbb{P}_{\\hat{m},n})} = 2||\\hat{f} - f_0||_{L_2(\\mathbb{P}_{\\hat{m},n})} + 2||f_0 - f^* ||_{L_2(\\mathbb{P}_{\\hat{m},n})}$"}, {"title": "Proof of Lemma 15", "content": "Proof. In the proof all probabilities and expectations are w.r.t. $P(\\cdot|\\theta)$. Let $C (\\epsilon; \\mathcal{RN} (L, W, B, R), || \\cdot ||_{L_\\infty (\\lambda)})$ be an $\\epsilon$-cover of $\\mathcal{RN}(L,W, B, R)$ w.r.t. the $|| \\cdot ||_{L_\\infty (\\lambda)}$-norm and let, $N = N (\\epsilon;\\mathcal{RN}(L,W, B, R), || \\cdot ||_{L_\\infty (\\lambda)})$.\nLet, $\\tilde{f} \\in C (\\epsilon; \\mathcal{RN}(L, W, B, R), || \\cdot ||_{L_\\infty (\\lambda)})$ be such that, $||\tilde{f} - f ||_{L_\\infty (\\lambda)} \\leq \\epsilon$. Then\n$|| f - f^*||_{L_2(\\lambda)} \\leq 2|| f - \\tilde{f}||_{L_2(\\lambda)} + 2||\tilde{f} - f^*||_{L_2(\\lambda)} \\leq 2\\epsilon^2 + 2||\tilde{f} - f^*||_{L_2(\\lambda)}.$\n(19)\nFor any $g \\in C (\\epsilon; \\mathcal{RN}(L, W, B, R), ||\\cdot ||_{L_\\infty(\\mathbb{P}_{\\hat{m}"}, "we let $Z_{ij} = (g(x_{ij}) - f^* (x_{ij}))^2 - \\mathbb{E}(g(x_{ij}) - f^* (x_{ij}))^2$. Let,\n$u = \\max \\{v, ||g-f^*||^2_{L_2(\\mathbb{"]}