{"title": "Sliding Window Attention Training for Efficient Large Language Models", "authors": ["Zichuan Fu", "Wentao Song", "Yejing Wang", "Xian Wu", "Yefeng Zheng", "Yingying Zhang", "Derong Xu", "Xuetao Wei", "Tong Xu", "Xiangyu Zhao"], "abstract": "Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. While these approaches achieve efficiency, they often require complex architectures and parallel training techniques. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. Specifically, SWAT replaces softmax with the sigmoid function for efficient information compression and retention. Then it utilizes balanced ALiBi and Rotary Position Embedding to stabilize training process. During inference, SWAT maintains linear computational complexity through sliding window attention while preserving model performance, achieving state-of-the-art (SOTA) results on eight commonsense reasoning benchmarks compared to mainstream linear recurrent architectures. Code is available at this link.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, from text generation to complex reasoning (Shao et al., 2024). Unlike humans, who can efficiently process long contexts with memory, LLMs struggle to handle them due to quadratic complexity (Beltagy et al., 2020). Despite their impressive performance on standard NLP tasks, this quadratic complexity poses a fundamental challenge for practical applications. The increasing need for efficient long-context processing, coupled with the computational constraints of current architectures, creates a pressing need for more scalable solutions.\nSeveral approaches have been proposed to handle long sequences efficiently. These methods can be broadly categorized into two types: (1) sparse attention mechanisms (Beltagy et al., 2020), which reduce computation by selectively calculating the attention score, and (2) sequence models with recurrent architectures, such as linear attention variants (Katharopoulos et al., 2020) and state space models (Gu and Dao, 2023), which aim to process sequences efficiently through recursive hidden states. However, these solutions face a fundamental dilemma-they either compromise model performance to achieve efficiency or propose new complex architectures that cannot fully exploit existing techniques for convenient implementation and deployment. However, existing LLM solutions for handling long sequences often require complex architectures and parallel training techniques, making implementation and deployment more challenging, which calls for an efficient approach based on the existing Transformer architecture.\nSliding Window Attention (SWA), a typical sparse attention approach (Child et al., 2019), is the most intuitive solution, as it avoids adding additional model components and compresses the inference computational complexity to linear. However, this approach still faces the following challenges\u00b9: (1) Current researches on SWA predominantly focus on solving the attention sink problem within the inference phase, where models allocate excessive attention to initial tokens, causing an uneven distribution of attention weights across the sequence (Xiao et al., 2023). However, they leave the training process unchanged, thereby creating"}, {"title": "Understanding Transformer's Attention", "content": "This section introduces concepts of the SWA mechanism and its potential capability in handling long sequences. We then analyze why current LLMs with SWA inference fail to achieve the expected theoretical advantages."}, {"title": "Sliding Window Attention", "content": "The self-attention layer in Transformers typically has O(N\u00b2) computational complexity, where N is the input sequence length. To reduce this complexity while preserving the sequential information, sliding window attention (SWA) is introduced in Longformer (Beltagy et al., 2020). SWA restricts each token to only attend the attention calculation of its neighboring tokens within a fixed-size window. With a window size of w \u00ab N, the computation cost per token is reduced to O(w), leading to an overall linear complexity O(N\u00b7 \u03c9), which is more efficient than vanilla attention.\nWe visualize the SWA mechanism in Figure 1, where the window size is three (w = 3) and the depth is two (L = 2). We define the tokens that are visible to the current window as active tokens (the red block in the figure, corresponding active tokens are \"a dear little\"). For invisible tokens,"}, {"title": "LLMs with SWA Inference", "content": "Although current open-source LLMs are structurally capable of conducting SWA inference, they fail to achieve stable improved results. As shown in Figure 2, we analyzed the perplexity (PPL) of four open-source LLMs (Touvron et al., 2023; Dubey et al., 2024; Jiang et al., 2023; Yang et al., 2024a) using different sliding window sizes on the PG-19 (Rae et al., 2019) test set. The experimental results reveal that these LLMs achieve optimal performance only when operating within their training sequence length. For instance, for Llama-2-7b model in Figure 2(a), when the window size is fixed at 1,024, the perplexity gradually increases as the evaluation length grows, as indicated by the color transition from blue to red in the heatmap. This suggests that Transformers inherently learn contextual patterns specific to their training length and fail to extend to variable-length texts during inference.\nWe suggest that this failure can be attributed to two major issues: (1) the attention sink phenomenon, where models become overly dependent on initial tokens, and (2) information loss that past tokens are discarded.\nThe attention sink phenomenon (Xiao et al., 2023), where LLMs allocate excessive attention to initial tokens in sequences, has emerged as a significant challenge for SWA inference in Transformer architectures. Previous work has made two key observations regarding this phenomenon. First, the causal attention mechanism in Transformers is inherently non-permutation invariant, with positional information emerging implicitly through"}, {"title": "Sliding Window Attention Training", "content": "In this section, we explore the advantages of SWA training over traditional Transformer training with a new paradigm for processing long sequences. Additionally, we provide a detailed explanation of our proposed SWAT attention layer. This simple yet effective attention layer combines Sigmoid (Verhulst, 1838), ALiBi, and RoPE to address the information retention challenges of SWA."}, {"title": "Information Transmission", "content": "Traditional Transformer training involves processing entire sequences of tokens, allowing the model to capture long-range dependencies through global attention mechanisms. In contrast, SWA operates within a limited context, necessitating new approaches to preserve information continuously. As shown in Figure 4, SWA training enables two distinct learning paradigms for LLMs, short and long sequence attentions.\nIn conventional Transformer training, the sequence length is smaller than the window size. New tokens can acquire and integrate information from all tokens, even the very first tokens in the text. Therefore, the model keeps essential information in each token embedding and enhances the ability to extract information, which is also strengthened by the softmax function.\nSWA training introduces a new training paradigm, where each window shift requires careful historical context management. In particular, the old token embedding is discarded after sliding. However, in the upper layers of the Transformer, the new token's embedding still retains the old token's embedding with a certain weight. Hence, the model tends to retain all past embeddings in the upper-level model to prevent information loss caused by sliding windows, strengthening the model's ability to compress information. The experimental results demonstrating how SWA training enhances the model's capabilities are presented in Sections 4.3 and 4.4."}, {"title": "Attention Computation", "content": "In this subsection, we propose SWAT, a modified attention mechanism that combines sigmoid activation with integrated position embeddings. The input consists of queries, keys, and values with dimension of d. Instead of using softmax normal-"}, {"title": "Network Efficiency", "content": "Since SWAT's architecture is nearly identical to a standard attention layer, the per-token computation cost remains almost the same under an equivalent attention length-apart from the additional overhead of computing the ALiBi. However, the overall computation becomes linear due to the use of a sliding window. Thus, the inference computational complexity can be expressed as:\n$\\text{Cost} = N\\omega \\times (1 + \\delta_{ALiBi}), 0 < \\delta_{ALiBi} \\ll 1$ (6)\nwhere \u03b4ALiBi represents the extra cost of ALiBi."}, {"title": "Experiments", "content": "For the overall comparison, models are trained on the 100BT subset of FineWeb-Edu (Lozhkov et al., 2024), which is a high-quality educational dataset designed for LLM pre-training.\nOur baselines include state-of-the-art models including both vanilla Transformer and recurrent models. Specifically, we compare our approach against Transformer++ (Touvron et al., 2023), RetNet (Sun et al., 2023), Gated Linear Attention (GLA) (Yang et al., 2024c), Mamba (Gu and Dao, 2023), DeltaNet (Yang et al., 2025), TTT (Sun et al., 2024), Gated DeltaNet (Yang et al., 2024b), and Titans (Behrouz et al., 2024).\nImplementation Details. We pre-train SWAT with model sizes of 340M and 760M parameters on 15B and 30B tokens, respectively. The training uses the same vocabulary as Llama 2 (Touvron et al., 2023), with a sequence length of 4096 tokens and a batch size of 0.5M tokens."}, {"title": "Overall Performance", "content": "In this section, we evaluate the performance of SWAT on eight commonsense reasoning benchmarks, as detailed in Appendix C.2. The comparison is conducted on 340M and 760M parameter models. For our SWAT, (-) denotes negative slopes (i.e., the negative ALiBi slope to look forward in Equation 4); (+) denotes positive slopes, which use the opposite slope of ALiBi (i.e., the positive slope in Equation 4 looking backward); and (-+) indicates that half of the attention heads have negative slopes and half have positive slopes.\nAs shown in Table 1, SWAT (-) achieves state-of-the-art (SOTA) performance on average (46.88%) across eight common sense reasoning tasks, surpassing all other baselines. This is mainly attributed to the short-text benchmarks, such as PIQA and Hellaswag, where SWAT (-) focuses more on the information from newly input tokens. Although SWAT (-) initially shows higher perplexity than other baselines at 340M parameters, when scaled to 760M parameters, it demonstrates strong decreases in perplexity on Wiki and LMB. This suggests a performance improvement trend for larger models with the sigmoid function. On the contrary, the purely forward-looking SWAT (+) shows weaker performance, suggesting that forward slopes work best combined with backward attention.\nThe balanced configuration SWAT (-+), where attention heads are evenly split between looking forward and backward, achieves more uniform performance across different tasks by effectively processing both recent and historical information. Specifically, SWAT (-+) achieves the best performance (62.11%) on BoolQ, a question-answering dataset where historical context is crucial for accurate predictions. This result aligns with our findings in Section 4.4, where balanced attention heads demonstrate superior performance on both OpenOrca and PG-19 datasets, confirming the importance of balanced historical information processing for complex reasoning tasks. Meanwhile, due to the allocation of some attention heads for remembering information from older tokens, SWAT (-+) shows a"}, {"title": "Sliding Window Attention Training", "content": "To verify the effectiveness of SWA training, we conduct experiments comparing vanilla Transformers pre-trained with and without SWAT training across three datasets. Using Llama2-based models (Touvron et al., 2023) pretrained on OpenWebText, we investigate the impact of varying sliding window sizes and sequence lengths, with results shown in Table 2. In the table, vanilla Transformers are which training length are the same as their training window size, and the labels A, B, C, and D represent the model identifiers.\nWhen the sliding window mechanism is applied, we observe a notable improvement in performance, particularly with longer evaluation sequence lengths. For instance, in the Sliding Window A configuration, when the evaluation length is 16,384, Sliding Window A achieves a performance of 3.0051 on OpenWebText, surpassing the 4.8414 achieved by Vanilla A. Additionally, Sliding Window B achieves the best performance across all three datasets when the evaluation length is 16,384. Note that all results are from models trained for 80,000 steps. If training continues, the attention sink issue is likely to worsen, further degrading vanilla model performance.\nBased on our experimental results, we draw two key conclusions: (1) Wtih the same model structure, SWA training significantly improves performance, especially with longer evaluation sequence lengths. This is likely because SWA training forces the model to retain memory of older information across long sequences, while vanilla models struggle with memory as they retain all historical tokens. (2) The vanilla Transformers perform optimally only when the evaluation length matches the training length, whereas the SWA trained models maintain consistent performance across varying sequence lengths. This is likely because vanilla Transformers heavily attend to initial tokens due to attention sink, while SWA models learn to focus primarily on the current window, ensuring stable performance across different sequence lengths."}, {"title": "Ablation Study", "content": "This section evaluates the impact of activation functions, position embeddings, and ALiBi slopes. We systematically test 11 different configurations"}, {"title": "Related Works", "content": "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost.\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021),"}, {"title": "Efficient LLMs", "content": "To address the quadratic complexity of Transformers, researchers have proposed various efficient models categorized into the following categories:\nLinear Recurrent Models achieve O(n) complexity through different approximation techniques. Linear Transformer (Katharopoulos et al., 2020) replaces softmax attention with kernel functions, while Performer (Choromanski et al., 2021) employs random feature approximation. Recent works like GLA (Yang et al., 2024c) introduce forgetting mechanisms to prevent information explosion, while Gated Delta Networks (Yang et al., 2024b) focus memory updates to enable both precise memory updates and quick resets when needed. Models like Mamba (Gu and Dao, 2023) and RWKV (Peng et al., 2023) take a fundamentally different approach by utilizing state space models (SSMs) instead of attention, providing an alternative way to capture sequential patterns.\nMemory-Augmented Architectures enhance Transformers' ability to handle long sequences by incorporating explicit memory mechanisms. For example, Transformer-XL (Dai et al., 2019) pioneered the use of cached computations from previous segments with relative positional embeddings. More recent works like Memorizing Transformers (Wu et al., 2022) and Focused Transformer (Tworkowski et al., 2023) try to store and retrieve relevant historical information.\nWhile these models achieve better efficiency, their complex architectures often lead to more challenging optimization compared to standard Transformers, which benefit from simple and well-established training procedures."}, {"title": "Conclusion", "content": "This paper introduces SWAT, a new architecture for efficient LLMs via sliding window attention training, which maintains the core Transformer architecture. By replacing softmax with sigmoid and combining balanced ALiBi with RoPE, SWAT addresses the attention sink issue and ensures stable training. SWAT enables effective information compression and retention across sliding windows with-"}, {"title": "Limitations", "content": "While our architectural design ensures relatively robust training stability, SWAT's performance exhibits significant sensitivity to hyperparameter configuration. Critical parameters including window size, model depth, and the distribution of ALiBi slopes substantially impact model efficacy. This necessitates comprehensive hyperparameter exploration to optimize the model architecture.\nAdditionally, as the model scales, it may encounter diminishing returns in retaining long-context information. In particular, larger models may fully memorize training data, reducing the need for information transmission, which in turn weakens the effectiveness of mechanisms designed to handle extended contexts. Future experiments will need to keep cache from previous steps during training to address this problem.\nFinally, despite SWAT's strong overall performance, the model exhibits an inherent limitation in its attention mechanism. Specifically, SWAT'S maximum attention distance is constrained by the product of window size and model depth. Although extending these parameters can theoretically increase the attention span, information loss remains inevitable when processing ultra-long sequences. For applications requiring complete information retention over extensive contexts, alternative approaches such as hybrid architectures or explicit memory retrieval mechanisms may be necessary to complement SWAT's capabilities."}, {"title": "Why Does the Softmax Function Lead to Sparsity?", "content": "In models such as Transformers, dot-product attention is the most widely used approach. Let a query vector q and multiple key vectors k1,k2,...,kL be given, where q, ki \u2208 Rd. We stack the key vectors into a matrix:\n$K = \\begin{bmatrix} k_1 \\\\ k_2 \\\\ : \\\\ k_L \\end{bmatrix}$ (7)\nThe attention distribution (i.e., the set of attention weights) \u03b1 is computed by:\n$\\alpha = \\text{softmax} (\\frac{qK^T}{\\sqrt{d}})$, (8)\nwhere softmax(zi) = ezi / \u2211jej. Let\n$E_i = \\frac{q \\cdot k_i}{\\sqrt{d}}$, (9)\nso the i-th attention weight is:\n$\\alpha_i = \\frac{\\exp(E_i)}{\\sum_{j=1}^{L} \\exp(E_j)}$ (10)\nSparsity arises because the exponential function greatly amplifies any Ei that is larger than the rest: if E\u2081 is significantly bigger than E2, . . ., EL, then exp(E1) will dominate the sum in the denominator, pushing a\u2081 close to 1 and making the others near 0. Formally, define\n$\\Delta_i = E_i - E_1 \\text{ for } i \\geq 2$, (11)\nso we have:\n$\\frac{\\alpha_i}{\\alpha_1} = \\frac{\\exp(E_i)}{\\exp(E_1)} = \\exp(E_i - E_1) = \\exp(-\\Delta_i)$. (12)\nIf \u0394i is large and positive, then exp(-\u0394i) is very small, causing \u03b1i to vanish compared to \u03b1\u2081. Moreover, in high-dimensional spaces (i.e., when d is large), random dot products q\u00b7ki tend to have higher variance, making it more likely that one or a few Ei values will stand out dramatically. This \"winner-takes-most\" scenario becomes amplified, thereby increasing the tendency toward sparsity within the attention distribution.\nIn practice, the dot-product q. ki often yields extreme values\u2014meaning that one or a few of the resulting energies Ei are substantially larger than the others. This phenomenon causes the softmax to concentrate most of the probability mass on these extreme values. To rigorously analyze this behavior, we suppose each attention score Ei is an independent and identically distributed (i.i.d.) random variable drawn from a Gaussian distribution:\n$E_i \\sim N(\\mu, \\sigma^2)$. (13)\nUnder this assumption, by the central limit theorem, the dot product q\u00b7ki follows an approximately normal distribution after appropriate scaling. More importantly, extreme value theory states that the maximum value among L i.i.d. Gaussian variables, denoted as E(L) = max1<i<L Ei, satisfies approximately:\n$E_{(L)} \\approx \\mu + \\sigma \\sqrt{2 \\ln L}$. (14)\nIn contrast, a typical attention score is around \u03bc. Therefore, the expected gap between the maximum energy and a typical energy is on the order of:\n$\\Delta \\approx \\sigma \\sqrt{2 \\ln L}$. (15)\nGiven this gap, we have:\n$\\frac{\\alpha_i}{\\alpha_1} \\approx \\exp(-\\sqrt{2 \\ln L})$. (16)\nFor large L, this ratio becomes exponentially small."}, {"title": "Why Does the Sigmoid Function Maintain Density?", "content": "While the softmax function induces a probability distribution over multiple inputs, the sigmoid function operates on each input independently and does not normalize across multiple values. Concretely, the sigmoid of a scalar z is defined as:\n$\\sigma(z) = \\frac{1}{1 + e^{-z}}$ (17)\nIn contrast to softmax-which computes exponential terms for all inputs z\u2081, z\u2082,..., zL and divides by their sum-sigmoid only involves a single exponential term e\u207b\u1dbb within its own calculation. Consequently, one input's value does not directly compete with another input's value in a shared denominator. Since the final attention weight for each token is determined independently based on its relationship with the query, there is no \u201cwinner-takes-most\u201d effect as seen in softmax-based attention."}, {"title": "Detailed Experiment Settings", "content": "While our main experiments utilize a specific high-quality educational dataset, we conducted preliminary evaluations across multiple datasets to comprehensively assess model capabilities. All datasets are split according to the ratio: train:validation:test = 8:1:1. Here we detail the characteristics and purposes of each dataset.\nOur overall experiment employs a 100 billion token subset of FineWeb-Edu (Lozhkov et al., 2024), which is specifically curated for language model pre-training. This dataset consists of high-quality educational content that provides well-structured training examples for developing fundamental language understanding capabilities.\nFor our subsequent experiments, as shown in Table 4, we deliberately selected three complementary datasets that evaluate different aspects of model performance:\nOpenWebText (Gokaslan et al., 2019) comprises predominantly shorter web-based texts. It provides a foundation for assessing basic language modeling capabilities. In contrast to specialized corpora, OpenWebText's diverse content allows evaluation of general language understanding across varied domains and writing styles.\nPG-19 (Rae et al., 2019) is based on complete books published before 1919, presenting a distinct challenge in processing long-form literary content. The book-length texts require models to maintain coherence and compress information across extended narratives, testing their ability to capture long-range dependencies and thematic consistency.\nOpenOrca (Lian et al., 2023) is a question-answering dataset that tests models' information retention capabilities. This is particularly important as the answers to questions are often embedded in earlier parts of the context, making it an effective benchmark for assessing models' ability to maintain essential information when processing long sequences.\nWe utilized OpenWebText for traininga and validation, while incorporating all three datasets into the test phase. To thoroughly evaluate long-context processing capabilities, we extended the input sequence length to 16,384 tokens for both Open-WebText and PG-19. This multi-dataset evaluation framework allows us to systematically analyze model performance across different linguistic challenges and context lengths, providing a comprehensive view of their capabilities and limitations."}, {"title": "Benchmarks", "content": "For our overall experiment, we compare models on eight common-sense reasoning tasks, in Table 5:\nWikitext (Merity et al., 2017): A large linguistic corpus extracted from Wikipedia articles, containing over 100 million word tokens. It tests a model's ability to predict the next word in a passage of text.\nLambada (Paperno et al., 2016): The LAmBdA dataset tests a model's capability of using broad discourse context to predict the last word of a passage extracted from books. It contains over 60,000 examples.\nPIQA (Bisk et al., 2020): The Physical Interaction: Question Answering (PIQA) dataset tests commonsense reasoning about physical interactions between two entities. It contains 16,113 multiple choice questions generated from crowdsourcing.\nHellaswag (Zellers et al., 2019): The HellaSwag dataset consists of 70,000 multiple choice questions about inferring what might happen next in a story. It requires commonsense reasoning to choose the most plausible ending.\nWinoGrande (Sakaguchi et al., 2021): The WinoGrande dataset tests coreference resolution and commonsense reasoning with 44,000 examples obtained from books and websites.\nARC (Clark et al., 2018): The AI2 Reasoning Challenge (ARC) dataset contains 7,787 genuine grade-school level, multiple-choice science questions, grouped into an Easy Set (ARC-e) and a"}, {"title": "Implementation Details.", "content": "In the overall experiment (Table 1), SWAT means we pretrain the model with our sliding window attention training. We pre-train SWAT with model sizes of 340M and 760M parameters on 15B and 30B tokens, respectively. The SWAT models are compared to other language models of similar sizes. All pre-training experiments were conducted on 8 NVIDIA A800 GPUs (80GB), with the 760M model taking approximately 31 hours to complete the pre-training process.\nEvaluations measure perplexity (lower is better) and accuracy (higher is better) on datasets like PIQA, WinoGrande, and BoolQ. For our SWAT, as defined in Equation (4), (-) denotes the configuration using only negative slopes (i.e., traditional ALiBi slopes sk = \u22122\u2212k), (+) denotes the configuration using only positive slopes (i.e., sk = 2\u2212k), (-+) denotes our bidirectional configuration where: Half of the attention heads (h/2 heads) use negative slopes sk = \u22122\u2212k, the other half use positive slopes Sk = 2-k. For both directions, k ranges from 1 to h/2. The experiments are based on two GitHub repositories flash-linear-attention\u00b2 and lm-evaluation-harness\u00b3.\nAnalysis Experiments For analysis experiments, models are evaluated on three datasets: OpenWebText, PG-19, and OpenOrca, with the average accuracy reported. We experiment with different training window sizes, training lengths, and evaluation window sizes. The experiments are based on two GitHub repositories nanoGPT\u2074 and flash-linear-attention. We pre-train SWAT (248M parameters) for 80,000 steps with a batch size of 250k tokens, accumulating a total training exposure of 20B tokens, which amounts to about 2 epochs over the pre-training corpus.\nIn Table 2, vanilla Transformers have a training length that matches their fixed training window size. Model A, B, C, and D are identifiers for pre-trained models with different configurations being compared. The columns in the table show different sequence length settings for each model configuration. The parameters used in the table are defined as follows::\nTraining window size means the maximum sequence length the model can process per training step.\nTraining length means the actual sequence length used for each training example, which may be shorter than the window size when using the vanilla Transformers.\nEvaluation window means the maximum context provided to the model during evaluation to make predictions.\nEvaluation length means the actual sequence length fed into the model per test example.\nWe compared pre-training using fixed token window sizes of 128, 1,024, and 4,096 versus using variable-length sliding windows. With sliding window pre-training, the model is exposed to longer token sequences during training, which helps improve evaluation perplexity. Using sliding windows allows longer sequences during training compared to fixed windows. This table shows that the best performance was achieved when the training sequence length is four times the training window size. Different evaluation window sizes are also tested to compare model performance given varying amounts of context.\nIn Table 3, we compared the performance of language models with different activation functions and position embeddings. Specifically, we study"}]}