{"title": "Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices", "authors": ["TALHA AZFAR", "KAICONG HUANG", "RUIMIN KE"], "abstract": "Edge sensing and computing is rapidly becoming part of intelligent infrastructure architecture leading to operational reliance on such systems in disaster or emergency situations. In such scenarios there is a high chance of power supply failure due to power grid issues, and communication system issues due to base stations losing power or being damaged by the elements, e.g., flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial vehicles (UAVs) has been proposed to provide computation offloading from these devices to conserve their battery, while the use of UAVs as relay network nodes has also been investigated previously. This paper considers the use of UAVs with further constraints on power and connectivity to prolong the life of the network while also ensuring that the data is received from the edge nodes in a timely manner. Reinforcement learning is used to investigate numerous scenarios of various levels of power and communication failure. This approach is able to identify the device most likely to fail in a given scenario, thus providing priority guidance for maintenance personnel. The evacuations of a rural town and urban downtown area are also simulated to demonstrate the effectiveness of the approach at extending the life of the most critical edge devices.", "sections": [{"title": "1 INTRODUCTION", "content": "In the face of natural disasters, remote locations, or emergency scenarios, the availability of reliable infrastructure remains a critical challenge. Traditional infrastructure systems often falter in such situations due to power outages, communication breakdowns, and the inability to conduct timely maintenance. In these contexts, edge infrastructure devices play a pivotal role in maintaining connectivity, collecting vital data, and facilitating emergency response efforts in urban areas [5] and remote hostile environments [6].\nEdge computing devices usually perform crucial tasks involving sensing, processing, and communication while consuming relatively low power. This could be as simple as recording and updating the temperature, or as complex as real-time multi-vehicle detection and tracking from surveillance cameras. These edge devices can be instrumental in disaster response by monitoring the situation, detecting urgent areas of concern, and coordinating the emergency response [1]. Some challenges faced by these edge devices in remote locations are"}, {"title": "2 LITERATURE REVIEW", "content": "Researchers have long explored the integration of UAVs with cellular network infrastructure to optimize energy efficiency while ensuring the maintenance of quality of service [16][26], while the mixed architecture of terrestrial and aerial connectivity has featured in numerous studies about infrastructure resilience in the face of disasters for preparedness, assessment, and response and recovery [4]. UAVs have been considered as relay nodes in communication networks to provide greater, more resilient coverage, for example in [30], where an iterative convex optimization algorithm finds the best power allocation and trajectory which also maximizes network throughput. The use of UAVs as an intermediate cloudlet layer in communication or distributed processing networks has been discussed in many architectures precisely for infrastructure resilience during natural or man-made disasters [7]. Communication disruptions in disaster scenarios are addressed in [29] with LoRaWAN equipped UAV mobile edge computing which significantly increases channel capacity by modeling the air-to-ground and remote-to-air connections individually to formulate a Markov chain for task assignment and queue management.\nThere has been significant research focused on the exact dynamics of UAV mobile edge computing to minimize energy consumption by optimizing the trajectory of the UAVs and computing resource allocations. The energy efficiency was maximized by jointly optimizing UAV trajectory, transmission power, and load allocation in [9] using successive convex approximations on multiple subproblems. Convex optimizations and reinforcement learning have been used by [28] which employ block coordinate descent to solve the convex problem for a given scenario, and then use those insights in a deep Q network reinforcement learning framework for generalized rapid decision making. Similarly, [24] formulate a time slot based Markov decision process to model the problem and then use a double deep Q-learning algorithm to minimize energy consumption and communication overhead, which maximizes the lifetime of the system before device failure. A two layer optimization problem was constructed for task scheduling and trajectory optimization for a multi-UAV mobile edge computing platform in [13].\nThe vehicle path is optimized for maximum data collection from wireless sensor networks with energy harvesting nodes in [14] which considers the heterogeneous duration of communication time slots as a means to increase throughput. They assume a constant velocity of the mobile edge device and prove the NP-hardness of the optimization problem. The study relies on extensive simulations with various initial conditions to characterize the robustness of the online mixed integer linear programming approach. A three layer disaster rescue architecture combining UAV edge and fog computing for post-disaster rescue operations is proposed in [22] which address the problems of task offloading and resource allocation separately in a game-theoretic framework using convex optimization and evolutionary computation, but trajectory optimization is not considered. Hard deadlines for data sharing are the major constraint in [18], which formulated the optimization problem to maximize the number of served devices while ensuring minimum data uploaded within the deadline.\nEdge computing is often used for computer vision task in traffic monitoring tasks [27] such as vehicle detection and tracking [2], speed and congestion classification [11], cooperative perception [31], etc. An edge-enabled disaster rescue framework that uses computer vision to process crowdsourced photos on mobile edge devices in emergency vehicles was proposed in [10] incorporating an adaptive detection mechanism to handle unstable network conditions and demonstrated the system feasibility through a prototype implementation. Mobile edge computing using UAVs in a disaster stricken area was studied in [19] focusing on network delay, quality of service, and network coverage. While these methods consider constraints related to limited battery sizes, quality of service, and communication urgency, the direct effects of the disaster scenario are not modeled or simulated.\nFollowing the success of deep reinforcement learning in video games [15] and Go [21], the mobile edge computing research community has applied similar methods for decision-making for optimum resource allocation and path planning [12] since the the different optimization problems are non-convex and the particular problems of task offloading and hover decision-making have been shown to be NP-hard by being analogous to the generalized"}, {"title": "3 SYSTEM OVERVIEW", "content": "The system under consideration is composed of heterogeneous edge computing devices in a fixed rectangular region affected by a calamity that limits the power or communication to some or all edge devices. Figure 1 shows the proposed system formulation. The edge devices perform video analytic tasks, and their power consumption rates for computation, communication, and standby are known in advance. The rate at which each device processes data locally is also known in advance, and is considered orders of magnitude slower than the UAV processing speed. The UAV is then deployed to this region and is able to move closer to any edge device to provide a communication channel and offloads the task data to execute the computation instead of the edge device. Depending on the scenario, an edge device may be deprived of power and thus unable to recharge, or lacking a communication channel, or both."}, {"title": "3.1 Problem Formulation", "content": "The system is simulated with time slots based on the time taken for the UAV to travel to another node and the time taken for the edge devices to complete their task. Since the moving time of the UAV and local processing time of edge devices is not fixed, the time slot can have variable duration. The edge devices have limited battery, and the aim of the UAV control for task offloading is to maximize the battery life of all devices. If a device is visited, it consumes less power during that time slot because the UAV does its computation. At the beginning of a time slot each edge device is assigned a volume of data randomly sampled from a bounded set to process, the UAV flies to a chosen device, hovers in place while offloading data and performing computation, then waits for the last device in the region to finish its local computation, which ends the time slot. The UAV is assumed to have a fixed speed and communication range. If multiple devices are present within communication range, they may all get served by the UAV in that time slot. In practice however, this is quite rare due to the random assignment of location and the limited communication range of edge devices. The goal of the system is to maximize the number"}, {"title": "3.2 Reinforcement Learning", "content": "The logical next question to ask is: For a given configuration of the simulation described, what is the series of actions that the UAV can take such that the lifetime of the system is maximized? In other words, what is the best way to delay battery depletion and data expiry for all devices? As this problem involves task offloading and minimizing energy consumption, and the added power and data expiry constraints do not make the problem any easier, in congruence with the existing literature, we use reinforcement learning (RL) to determine the best course of action for the UAV. In particular, deep Q-network (DQN) reinforcement learning is used with a multilayer perceptron neural network to learn the policy.\nIn RL an agent learns to make decisions by interacting with an environment. The agent takes actions, receives feedback in the form of rewards, and aims to learn a policy that maximizes cumulative rewards over time. Q-learning is a model-free RL algorithm used to find the optimal action-selection policy for a given finite Markov decision process (MDP). It learns a function Q(s, a), which represents the expected future rewards of taking action a in state s, and aims to maximize this function from the direct sampling of the system outputs without estimating a model for the system itself. In the case of DQN, deep learning on a neural network is used to approximate the Q-function. The input to the neural network is the state, and the output is the predicted Q-value for each action. For training stability there are two neural networks: the Q-network and the target network. The Q-network is updated frequently with each iteration of training, while the target network is only periodically updated with the parameters of the Q-network. The target network is essentially a copy of the Q-network that is used for estimating Q-values and its purpose is to provide stable targets for the Q-values during training. Using a single neural network for both estimating Q-values and generating target Q-values can lead to irregularities during training in the form of divergence or oscillations.\nDuring every training iteration there is a reward associated with the action taken, which is feedback from the environment that indicates how good or bad an action was. Rewards can be positive, negative, or zero, depending on the task. Given an action (at) in the current state (st), the maximum Q-value achievable from the next state (St+1) is multiplied by a discount factor (y) and added to the current reward (rt) to give the target Q-value for the current state-action pair.\nQtarget (St, at) = rt + y max Q (st+1, a')\nThus, the Q-value represents the expected cumulative rewards of taking an action in a particular state and following the optimal policy thereafter. The Q-value of a state-action pair is predicted from a deep neural network which is trained using a loss function based on the difference of the predicted Q-value and the target Q-value.\nL(w) = (Qw(s, a) \u2013 Qtarget (s, a))\u00b2\nwhere L(w) is a loss function of the weights w of the neural network which approximates Q over the space of all state action pairs."}, {"title": "4 IMPLEMENTATION", "content": "The system is initialized with fixed random number seed for repeatability and multiple seeds were tried in each power and communication availability combination to obtain the average performance. The x and y coordinates of the devices are chosen uniformly at random on the grid, with random device type, random task type, and random maximum battery capacity, from the ranges shown in Table 1.\nIn every time slot each device is assigned a random task volume for local processing or offloading to the UAV if it is visited. The UAV consumes power based on the traveled distance and time spent hovering. The local processing time for an edge device is found by dividing the task volume by the processing rate of the device for that task as shown in Table 3. Then the time is multiplied by the corresponding power from Table 2 (plus standby power) to obtain the energy used for local processing in that time slot. The maximum processing time, and hence the maximum time slot duration, is manually capped at 10 minutes. If the device is able to communicate, the size of the data it transmits is assumed to be only 1 kilobyte since the output of computer vision object detection is often a simple list of bounding box or segmentation coordinates and labels in a small text file. If the device is not able to communicate, its data age is incremented by one.\nIf, instead, the device is visited by the UAV for task offloading, the edge device battery consumption depends on the data transmission for that time slot. The channel gain, C, is defined as C = \u03b2\u2030l\u00af\u00ba, where I is the distance between the edge device and UAV, \u03b2o is received power per unit distance, and @ is the path loss factor which models the attenuation of signal strength as it propagates through the wireless channel. The transmission rate, R, can then be calculated from Shannon's theorem [20] as"}, {"title": "5 RESULTS AND CASE STUDY", "content": "The experiments were performed on Intel Core i9 13900-K CPU with 32 GB RAM and NVIDIA GeForce RTX 4090 GPU using Python 3.11.5, and each training run took approximately 11 minutes. The size of the region chosen directly affects the episode length as the battery of the UAV is also limited, albeit much greater than those of edge devices. After some trial and error, 800m by 800m was chosen so that the UAV range was not the limiting factor. One initial layout is depicted in Figure 2, showing 12 edge devices with initial battery capacity and the UAV at its starting position.\nSome hyperparameter tuning was required for successful execution. The learning rate for the deep Q-network was decreased from 0.01 to around 0.007 which provided stable convergence to the maximum attainable episode length in every scenario. The behavior of the reinforcement learning under the two learning rates is shown in Figure 3a. We note that the model converges to the best strategy in a lower number of iterations in some"}, {"title": "5.1 Prioritizing the Evacuation Route", "content": "To evaluate the system's performance in a realistic disaster scenario, we designed an evacuation simulation for a small rural town in upstate New York called Round Lake. The simulation involved vehicle flows, defined using the SUMO (Simulation of Urban Mobility) tool [8], from residential areas towards the main Adirondack highway. In this scenario there are some devices that are absolutely critical to the evacuation as they provide road usage information of high traffic areas, while other devices in areas that have already been evacuated may no longer need to be kept operational.\nTherefore, edge devices were strategically placed at various junctions over the simulated road network, with some located near the predefined evacuation routes as shown in Figure 4a. The SUMO simulation was executed to determine the traffic densities on these roads. The edge devices utilize computer vision to detect vehicles and vulnerable road users, thereby providing crucial information to emergency services. The UAV-edge RL system was programmed to receive additional rewards if an edge device was situated near a high-density road, in proportion to the observed average density. A similar experiment was also performed on the downtown Albany road network at a larger scale as shown in Figure 4b. This also contains major corridors of evacuation while the rest of the traffic is considered to be negligible or low priority."}, {"title": "6 CONCLUSION", "content": "Intelligent edge computing networks are fast becoming the future architecture of transportation infrastructure that will be essential in disaster or emergency situations. In this paper we explore the aftermath of a disaster that disables power and communication access to some edge infrastructure devices with heterogeneity. The concept of a mobile edge node in the form of a UAV is used to provide computation offloading and communication relay to the affected edge nodes. The problem of finding the best course of action for a given situation is formulated, and addressed through deep Q-network reinforcement learning. The two main constraints are decreasing battery charge and the increasing age of uncommunicated data. Experiments are performed for multiple combinations of power and communication availability with a variety of reward functions and RL methods. While the reduction of the life of the network is expected with decreasing power and connectivity, the important outcome of each experiment is to identify the device that is expected to fail first, thus informing responders and operators of which devices to prioritize for maintenance. The system was also tested in conjunction with evacuating traffic in two settings, rural and urban, where higher priority was given to devices located near high flow roadways.\nFuture work will explore the deployment of multiple UAVs to further enhance the resilience and efficiency of the proposed system. By integrating multiple UAVs, we aim to improve the coverage and redundancy of computational offloading and communication relays, particularly in large or densely populated disaster zones. Additionally, we will investigate the feasibility of ad-hoc mesh networks, allowing UAVs and edge devices to dynamically form communication links without relying on fixed infrastructure. This could significantly enhance the robustness of the network during large-scale emergencies. We also plan to incorporate cost and logistical considerations into our optimization models, factoring in the complexity and expenses associated with deploying personnel for device maintenance and repair. By combining these elements, our goal is to develop a comprehen-sive, practical solution that optimizes both operational effectiveness and cost-efficiency in real-world disaster response scenarios. Further, we will expand our simulations to include a more extensive traffic network with granular data, enabling a more detailed assessment of the system's impact on evacuation and rescue operations."}]}