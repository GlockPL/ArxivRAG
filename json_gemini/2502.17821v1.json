{"title": "CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems", "authors": ["Rui Liu", "Yu Shen", "Peng Gao", "Pratap Tokekar", "Ming Lin"], "abstract": "Multi-modality learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. While existing frameworks such as Auxiliary Modality Learning (AML) effectively utilize multiple data sources during training and enable inference with reduced modalities, they primarily operate in a single-agent context. This limitation is particularly critical in dynamic environments, such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. To address these challenges, we propose Collaborative Auxiliary Modality Learning (CAML), a novel multi-agent multi-modality framework that enables agents to collaborate and share multimodal data during training while allowing inference with reduced modalities per agent during testing. We systematically analyze the effectiveness of CAML from the perspective of uncertainty reduction and data coverage, providing theoretical insights into its advantages over AML. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that CAML achieves up to a 58.13% improvement in accident detection. Additionally, we validate CAML on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a 10.61% improvement in mIoU.", "sections": [{"title": "1. Introduction", "content": "Multi-modality learning has become an essential approach in a wide range of machine learning applications, particularly in areas such as autonomous driving (El Madawi et al., 2019; Xiao et al., 2020; Gao et al., 2018), robotics (Noda et al., 2014; Lee et al., 2020), and perception systems (Zhuang et al., 2021; Bayoudh et al., 2022), where the availability of multiple data sources (e.g., RGB images, LiDAR, radar, etc.) improves model performance by providing complementary information. However, these multi-modality systems often suffer from increased computational complexity and latency at inference time. Moreover, some modalities may not be consistently available or reliable in real-world conditions, necessitating strategies that can compensate for missing modalities during inference.\nRecent work on machine learning (Hoffman et al., 2016; Wang et al., 2018; Garcia et al., 2018; 2019; Piasco et al., 2021) aims to address these problems by allowing models to leverage additional modalities during training while enabling inference using fewer or even a single modality. For example, a model might be trained using both RGB and LiDAR data, but during deployment, it only requires RGB data to operate. These approaches reduce the computational burden and accommodates real-world conditions where certain sensors may be unavailable. Shen et al. (2023) formalized these learning tasks as Auxiliary Modality Learning (AML). The AML framework successfully reduces the dependency on expensive or unreliable modalities, but it focuses on the single-agent setting, where an individual model is trained to handle reduced modalities during inference.\nDespite the benefits of AML, several gaps remain. First, a major limitation in the current AML framework is the inability to exploit collaboration between agents, particularly in dynamic environments such as connected autonomous vehicles (CAV). In such scenarios, data coverage from a single agent is often incomplete because of occlusion or limited sensor range, leading to blind spots or increased uncertainty in decision-making. Second, the information from multiple modalities can complement each other across agents, especially in multi-agent settings such as vehicle-to-vehicle (V2V) communication or collaborative robotics. Different agents may have access to complementary sensory information, which could be shared to have agents make more informed and safer decisions, notably in accident-prone scenarios. However, current AML approaches do not exploit this potential for collaboration.\nTo bridge these gaps, we propose Collaborative Auxiliary Modality Learning (CAML), a novel framework for multi-agent multi-modality systems that allows agents to collaborate and share multimodal data during training, but enables inference with reduced modalities per agent during testing, as illustrated in Figure 1. CAML leverages knowledge distillation (Hinton, 2015), transferring knowledge from a teacher model into a student model. This enables the student to operate with missing modalities during inference. For instance, in autonomous driving, multiple vehicles can share sensor information such as LiDAR and RGB images during training to build more robust representations, while during runtime testing, each vehicle performs inference using only RGB images.\nCAML addresses two key challenges: (1) It reduces uncertainty and enhances data coverage in dynamic environments by leveraging complementary information from multiple agents. (2) It maintains efficient, modality-reduced inference during testing. Unlike previous work that either focuses on multi-agent collaboration but without addressing modality reduction at test time, or tackles multi-modality learning in single-agent settings, CAML unifies these concepts. Through collaboration, CAML enables agents to compensate for each other's blind spots, resulting in more informed prediction or decision-making even when some modalities are unavailable at deployment. In summary, our work offers the following key contributions:\n\u2022 We introduce CAML, a novel framework for multi-agent systems that allows agents to share multimodal data during training, while performing efficient, reduced-modality inference during testing. By leveraging the strengths of multi-agent collaboration, CAML can reduce estimation uncertainty and integrate complementary information, capturing a broader and more detailed data representation.\n\u2022 We systematically analyze the effectiveness of CAML from the perspective of uncertainty reduction and enhanced data coverage, providing theoretical insights into its advantages over AML.\n\u2022 We validate CAML through experiments in collaborative decision-making for connected autonomous driving in accident-prone scenarios, and collaborative semantic segmentation for real-world data of aerial-ground robots. CAML achieves up to 58.13% improvement in accident detection for autonomous driving, and up to 10.61% improvement for more accurate semantic segmentation."}, {"title": "2. Related Work", "content": "Multi-Agent Collaboration. Collaboration in multi-agent systems has been widely studied across fields such as autonomous driving and robotics. In autonomous driving, prior research has explored various strategies, including spatio-temporal graph neural networks (Gao et al., 2024), LiDAR-based end-to-end systems (Cui et al., 2022), decentralized cooperative lane-changing (Nie et al., 2016) and game-theoretic models (Hang et al., 2021). In robotics, Mandi et al. (2024) presented a hierarchical multi-robot collaboration approach using large language models, while Zhou et al. (2022) proposed a perception framework for multi-robot systems built on graph neural networks. A review of multi-robot systems in search and rescue operations was provided by (Queralta et al., 2020), and Bae et al. (2019) developed a reinforcement learning (RL) method for multi-robot path planning. Additionally, various communication mechanisms, such as Who2com (Liu et al., 2020b), When2com (Liu et al., 2020a), and Where2comm (Hu et al., 2022), have been created to optimize agent interactions.\nDespite these advancements, existing multi-agent collaboration frameworks remain limited by their focus on specific tasks and the assumption that agents will have consistent access to the same data modalities during both training and testing, an assumption that may not hold in real-world applications. To address these gaps, our framework, CAML, enables agents to collaborate during training by sharing multimodal data, but at test time, each agent performs inference using reduced modality. This reduces the dependency on certain modalities for deployment, while still allowing agents to leverage additional data during training to enhance overall performance and robustness.\nAuxiliary Modality Learning. Auxiliary Modality Learning (AML) (Shen et al., 2023) has emerged as an effective solution to reduce computational costs and the amount of input data required for inference. By utilizing auxiliary modalities during training, AML minimizes reliance on those modalities at inference time. For example, Hoffman et al. (2016) introduced a method that incorporates depth images during training to enhance test-time RGB-only detection models. Similarly, Wang et al. (2018) proposed PM-GANs to learn a full-modal representation using data from partial modalities, while Garcia et al. (2018; 2019) developed approaches"}, {"title": "3. Collaborative Auxiliary Modality Learning", "content": "In AML (Shen et al., 2023), which operates in a single-agent framework, the missing modalities during testing are referred to as auxiliary modalities, while those that remain available are called the main modality. In contrast, in our framework CAML, each agent can process a different number of modalities during training and different agents can have different main modalities and auxiliary modalities. There is no correlation between the number of agents and the number of modalities.\nWe define our problem in both training and testing phases. In the training phase, we consider a multi-agent system with N agents collaboratively completing a task. The set of agents is denoted as $A_{train} = {A_1, A_2, ..., A_N}$. The observations of all agents are denoted as $X = {X_1,X_2,...,X_N}$, where $x_i$ is the observation acquired by the i-th agent $A_i \\in A_{train}$. The ground truth label is denoted as Y, which can be an object label, semantic class, or a control command (e.g., brake for an autonomous vehicle). The set of modalities is denoted as $I_{train} = {I_1, I_2, ...,I_k}$, such as RGB, LiDAR, Depth, etc, where K is the number of modalities avaiable during training. During training, each agent has access to all these K modalities. In the testing phase, we assume there are M agents. The set of test agents is denoted as $A_{test} = {A_1, A_2,..., A_M}$. In addition, the set of modalities is denoted as $I_{test}$, which is a subset of $I_{train}$. The number of modalities available during testing is denoted as L, where L \u2264 K. The set of agents that have access to the j-th modality $I_j \\in I_{test}$ is denoted as $A_{test}^j$, where $A_{test}^j \\in A_{test}$, and the number of agents in this set is given by $|A_{test}^j|$ = M. This means that during testing, each agent may have access to different number of modalities.\nGiven the problem definition, we aim to estimate the posterior distribution P(y|X) of the ground truth label y given all agents' observations X. During training, we train both a teacher model where each agent has access to all modalities in $I_{train}$ and a student model where each agent has access to partial modalities in $I_{test}$. We employ Knowledge Distillation (KD) to transfer the knowledge derived from the teacher model to the student model, enabling the student to benefit from additional information, as illustrated in Figure 2. At test time, we perform inference using the student model, which relies on the test modality observations $X_{test}$.\nSpecifically, in the teacher model, each agent has access to all multimodal observations and independently processes its local observations to produce embeddings. These embeddings are then shared among agents based on whether the system operates in a centralized or decentralized manner. If the system is centralized, all collaborative agents share their embeddings with one designated ego agent for centralized processing. If the system is decentralized, each agent shares the embeddings with other agents. We provide"}, {"title": "4. Analysis", "content": "To compare whether CAML outperforms AML with a single agent theoretically, we analyze from two key perspectives: uncertainty reduction and data coverage enhancement. Data coverage can be further discussed from two dimensions: complementary information and information gain. We aim to address three major questions: (a) Uncertainty Reduction: Does the collaboration among multiple agents help reduce the variance of the posterior distribution, resulting in more confident estimates? (b) Complementary Information: Does the collaboration of multiple agents provide complementary information that increases data coverage? Specifically, does combining observations from each agent lead to a more accurate and comprehensive prediction compared to using a single agent? (c) Information Gain: Does the collaboration increase the mutual information between the observations and the true label?\nUncertainty Reduction. To address question (a) about uncertainty reduction, the prior P(y) is typically assumed to be Gaussian: $P(y) = \\mathcal{N}(y|\\mu_0, \\sigma_0^2)$, where $\u03bc_0$ is the prior mean and $\u03c3_0^2$ is the prior variance.\nSingle-Agent. In the single-agent case, we assume that only agent $A_i$ is available and its likelihood P(xi|y) is Gaussian: $P(x_i|y) = \\mathcal{N}(x_i|\u03bc_i(y), \u03c3_i^2)$, where $\u03bc_i(y)$ is the mean of the observation $x_i$ given y, $\u03c3_i^2$ is the variance of the agent $A_i$'s observations. The posterior distribution P(y|xi) is proportional to the product of the prior and likelihood, $P(y|x_i) \\propto P(y)P(x_i|y)$, which is also Gaussian. And the posterior variance $\u03c3_{single}^2 = (\\frac{1}{\u03c3_0^2} + \\frac{1}{\u03c3_i^2})^{-1}$ (Murphy, 2007).\nMulti-Agent. In the case of multi-agent collaboration, we model the joint likelihood of the observations X as a multi-variate Gaussian distribution, conditioned on the true target variable y: $P(X|y) = \\mathcal{N}(X|\u03bc_X(y), \u03a3)$, where $\u03bc_X(y)$ is the joint mean of the observations from all agents, conditioned on y, \u03a3 is the covariance matrix, encoding the correlations between the observations from multiple agents. The posterior $P(y|X) \\propto P(y)P(X|y)$, is another Gaussian, with variance $\u03c3_{multi}^2 = (\\frac{1}{\u03c3_0^2} + 1^T\u03a3^{-1}1)^{-1}$ (Murphy, 2007). Since $1^T\u03a3^{-1}1 > \\frac{1}{\u03c3_i^2}$ for any i, we have $\u03c3_{multi}^2 \u2264 \u03c3_{single}^2$. In the extreme case where all agents' observations are perfectly correlated (e.g., they all observe the same thing), the posterior variance would be equivalent to that of a single agent. However, multi-agent collaboration reduces variance compared to a single agent, as long as the observations are not perfectly correlated, proving that collaboration reduces uncertainty.\nEnhanced Data Coverage. In comparing data coverage between CAML and AML, we analyze it from two key aspects: complementary information and information gain.\nComplementary Information. To address question (b) about complementary information, we study data coverage and information provided by each agent in a multi-agent system. Let the entire data space be denoted as D, which consists of various subsets. Each agent $A_i$ in the system covers a subset of this data space: $C_i \u2286 D$. The overall coverage by the system is given by the union of all subsets covered by individual agents: $C_{multi} = \\bigcup_{i=1}^N C_i$. This ensures that $C_{multi} \u2265 max C_i$. If only a single agent is available, it can only observe a portion of the data space, leaving parts of the space unobserved, which leads to incomplete information for estimating the true label y. We show an qualitative example of multi-agent collaboration provides complementary information to enhance data coverage in Figure 7 in the Appendix.\nFrom a probabilistic perspective, when multi-agent collaboration is in place, the combined likelihood P(X|y) is modeled as a multivariate distribution (as discussed in Section 4). This approach provides a broader and more accurate representation of the data space by integrating information from all agents and modeling the dependencies and correlations between them. Compared to a univariate distribution P(xi|y) for a single agent $A_i$, the multivariate distribution covers a larger portion of the data space D, thus enhancing data coverage. This allows the exploration of more complex patterns, relationships, and complementary information from different agents. By capturing a richer set of interactions and correlations among the agents' observations, the multivariate distribution supports more informed decision-making. The model's predictions are based on a comprehensive view of the environment, thus leading to more accurate outcomes.\nInformation Gain. To address question (c) about information gain, we analyze using information theory. Let I(y; xi)"}, {"title": "5. Experiments", "content": "5.1. Collaborative Decision-Making\nTo evaluate our approach, we first focus on collaborative decision-making in connected autonomous driving (CAV). This involves making critical decisions for the ego vehicle in accident-prone scenarios, such as determining whether or not to take a braking action.\nData Collection. Following prior research (Cui et al., 2022; Gao et al., 2024), we focus on three complex traffic scenarios prone to accidents due to limited sensor coverage or obstructed views, as illustrated in Fig. 6. For more details about the scenarios, please refer to the Appendix A.1. For each scenario, we collect 24 trials, dividing them into 12 trials for training through behavior cloning (BC) and 12 trials for testing. Each trial includes RGB images and LiDAR point clouds captured by a variable number of connected vehicles, along with the ground truth actions of the ego vehicle. At each timestamp, the ego vehicle has a maximum of three collaborative vehicles, provided their distance is within a threshold of 150 meters (Gao et al., 2024; Cui et al., 2022). For each vehicle, both RGB and LiDAR data are used during training, while only RGB data is used during testing in CAML.\nExperimental Setup. For processing RGB data, we first resize the image to 224 \u00d7 224 and use ResNet-18 (He et al., 2016) as the encoder to extract a feature map. We then apply self-attention on the feature map to dynamically compute the importance of features at different locations. After the self-attention, we apply three convolution layers with each followed by a ReLU activation. Finally, we obtain a 256-dimensional feature representation after passing through a fully connected layer. To facilitate the collaboration and aggregation of RGB feature embeddings from connected vehicles to the ego vehicle, we use the cross-attention mechanism. For processing the LiDAR data, we use the Point Transformer (Zhao et al., 2021) as the encoder and utilize the COOPERNAUT (Cui et al., 2022) model to aggregate LiDAR feature embeddings.\nFor the training of Knowledge Distillation (KD), we first train a teacher model offline using a binary cross-entropy loss, where each vehicle has both RGB and LiDAR data. Then we train a student model to mimic the behavior of"}, {"title": "6. Conclusions", "content": "In conclusion, we propose Collaborative Auxiliary Modality Learning (CAML), a unified framework for multi-agent multi-modality systems. Unlike prior methods that either focus on multi-agent collaboration without modality reduction or address multi-modality learning in single-agent settings, CAML integrates both aspects. It enables agents to collaborate using shared modalities during training while allowing efficient, modality-reduced inference. This not only lowers computational costs and data requirements at test time but also enhances predictive accuracy through multi-agent collaboration. We provide a theoretical analysis of CAML in terms of uncertainty reduction and data coverage, highlighting its advantages over AML. CAML demonstrates up to a 58.13% improvement in accident detection for connected autonomous driving in complex scenarios and up to a 10.61% mIoU gain in real-world aerial-ground collaborative semantic segmentation. These improvements underscore the practical implications of our framework. For limitations and future work, please see the Appendix A.6."}, {"title": "A. Appendix", "content": "A.1. Connected Autonomous Driving Scenarios\nWe utilize a connected autonomous driving environment that integrates CARLA (Dosovitskiy et al., 2017) with AutoCast (Qiu et al., 2021). Our evaluation focuses on three complex and accident-prone traffic scenarios, characterized by limited sensor coverage or obstructed views. These scenarios are realistic and include background traffic of 30 vehicles. They involve challenging interactions such as overtaking, lane changing, and red-light violations, which inherently increase the risk of accidents: (1) Overtaking: A sedan is blocked by a truck on a narrow, two-way road with a dashed centerline. The truck also obscures the sedan's view of oncoming traffic. The ego vehicle must decide when and how to safely pass the truck. (2) Left Turn: The ego vehicle attempts a left turn at a yield sign. Its view is partially blocked by a truck waiting in the opposite left-turn lane, reducing visibility of vehicles coming from the opposite direction. (3) Red Light Violation: As the ego vehicle crosses an intersection, another vehicle runs a red light. Due to nearby vehicles waiting to turn left, the ego vehicle's sensors are unable to detect the violator.\nA.2. Data Coverage\nWe present a qualitative example highlighting how multi-agent collaboration provides complementary information to enhance data coverage. In a red-light violation scenario for connected autonomous driving, as shown in the following figure, the ego vehicle's view is obstructed, rendering the occluded vehicle invisible. However, collaborative vehicles are able to detect the occluded vehicle, providing critical complementary information. This additional data helps the ego vehicle overcome its occluded view, enabling it to make more informed decisions and avoid potential collisions with the occluded vehicle.\nA.3. Real-World Aerial-Ground Scenarios\nA.3.1. EXPERIMENTAL SETUP\nWe resize the input RGB and depth images to 224 \u00d7 224. To process RGB and depth data locally for each robot, we use ResNet-18 (He et al., 2016) as the encoder to extract feature maps of size 7 \u00d7 7. The RGB features from both robots are shared and fused through channel-wise concatenation, and the depth features are processed similarly. Then we apply 1 \u00d7 1"}, {"title": "A.4. Complexity Analysis", "content": "A.4.1. COMPARATIVE TRAINING COMPLEXITY\nWe report the training complexity of AML (Shen et al., 2023) and CAML for the experiments of collaborative decision-making in CAV, and collaborative semantic segmentation for aerial-ground robots in Table 2 and Table 3, respectively. For the experiments, we employ a batch size of 32 and the Adam optimizer (Kingma, 2014) with an initial learning rate of 1e-3, and a Cosine Annealing Scheduler (Loshchilov & Hutter, 2016) to adjust the learning rate over time. The model is trained on an Nvidia RTX 3090 GPU with AMD Ryzen 9 5900 CPU and 32 GB RAM for 200 epochs.\nA.4.2. TIME AND SPACE COMPLEXITY\nIn CAML, the agents' embeddings are shared based on whether the system operates in a centralized or decentralized manner. If the system is a centralized, all collaborative agents share their data with one designated ego agent for centralized processing. Each of the N \u2212 1 collaborative agents performs its local computation independently, with a time complexity of O(Tc) and a space complexity of O(Sc), where Tc represents the time required for local computation, and Sc is the associated space. Thus, the total computation time and space complexities for all collaborative agents are O(T(N \u2212 1)) and O(Sc(N \u2212 1)), respectively. For simplicity, assuming each communication from one collaborative agent to the ego agent consumes O(D) time complexity and O(M) space complexity, where D is the time required for communication and M is the corresponding space. Therefore, the total communication time and space complexities for gathering information at the ego agent are O(D(N \u2212 1)) and O(M(N \u2212 1)), respectively. Then the ego agent aggregates the received data, running a model, having a time and space complexity O(Te) and O(Se), where Te and Se represent the time and space required for the ego agent's computation. So the total time and space complexities are O(T(N \u2212 1) + D(N \u2212 1) + Te) and O(Sc(N \u2212 1) + M(N \u2212 1) + Se), respectively.\nIf the system is decentralized, each agent performs its local computation and shares information with other agents. For simplicity, let the local computation for a single agent has a time complexity of O(T), where T is the time required for local computation. Assume that communication from one agent to another agent requires O(D) time complexity and O(M) space complexity, where D represents the time of communication between two agents, and M denotes the space required for such communication. For N agents, the total computation time complexity is O(NT). In the worst case, each agent share data with all other agents, this can result in O($N^2$D) for pairwise sharing. So the total time complexity is O(NT + $N^2$D). For space complexity, the storage requirement for all agents is O(NS), where S is the space needed per agent. Communication between agents adds an additional complexity of O($N^2$M). So the total space complexity is O(NS + $N^2$M). In the typical case, if each agent communicates with only other k agents (k < N) rather than all N \u2212 1 agents. The total time and space complexities become O(NT + NkD) and O(NS + NkM), respectively.\nA.5. Knowledge Distillation\nWe begin by training a teacher decision-making model T offline using both RGB and LiDAR data, with a binary cross-entropy loss: $L_{BCE}(Y,T) = -E_D[y_i log(p_i) + (1 \u2212 y_i) log(1 \u2013 p_i)]$, where D is the dataset, yi is the ground truth indicating whether the vehicle should brake, pi is the predicted probability by the teacher model T. The student model S is trained to mimic the behavior of the teacher model while having less modalities. For each data point, the student model receives the same RGB image that the teacher model was given. The loss for the student model is a combination of two terms: the distillation loss using KL divergence between the student output and teacher output (soft targets), and"}, {"title": "A.6. Limitations and Future Work", "content": "Even though the advancements of CAML, there are some limitations. One limitation is that if the modalities are misaligned, the model may struggle to perform effective fusion, leading to incorrect predictions. The auxiliary modalities or views from collaborative agents may become noise, useless or even degrading performance. Another limitation is the increasing system complexity. As the number of agents increases, the complexity of the system grows. The fusion of multi-agent and multi-modal data introduces challenges related to coordination overhead, which may lead to delays in the collaborative learning process. Future work can focus on address these limitations."}]}