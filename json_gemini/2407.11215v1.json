{"title": "MECHANISTIC INTERPRETABILITY OF LARGE LANGUAGE MODELS WITH APPLICATIONS TO THE FINANCIAL SERVICES INDUSTRY", "authors": ["ASHKAN GOLGOON", "KHASHAYAR FILOM", "ARJUN RAVI KANNAN"], "abstract": "Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads 10.2 (head 2, layer 10), 10.7, and 11.3, as well as the (negative) heads 9.6 and 10.6 play a significant role in the task completion.", "sections": [{"title": "1. BACKGROUND ON LLMS", "content": "The release of ChatGPT by OpenAI in late 2022 stunned the world as the chatbot set new milestones surpassing all previous publicly available systems and triggered discussions about potential risks [1, 9, 11]. At the heart of the current large language model (LLM) revolution lies the transformer architecture. Below, we provide a short introduction on transformers, followed by LLMs."}, {"title": "1.1. The transformer architecture.", "content": "Transformers are deep feed-forward neural networks that leverage the attention mechanism. They excel in sequence modeling tasks, especially in natural language processing (NLP) [25]. Before the advent of the transformer architecture in the landmark paper [35], recurrent neural network (RNN) architectures such as Long-Short Term Memory (LSTM) were common for NLP and sequence modeling tasks. Such models rely on an internal hidden state and must process the data sequentially. On the other hand, transformers, which are based on the attention mechanism, are superior in capturing long-term dependencies. This is due to the non-sequential and parallel manner by which they process the data. Transformers, moreover, allow transfer learning they can be pre-trained on large corpora and then fine-tuned for downstream tasks a fact that facilitates developing new AI applications tailored for in-house datasets based on the existing foundation models such as BERT or GPT-4.\nIn the context of sequential data, the goal can be to learn the probability distribution of the next token in sequence modeling tasks (e.g., language modeling), the probability distribution of a sequence conditioned on another sequence in sequence-to-sequence tasks (e.g., machine translation), or the probability score in a text classification task (e.g., sentiment analysis). As alluded to earlier, the transformer architecture shines in such tasks via utilizing the attention mechanism. The distinctions made above between different tasks are reflected in the attention type\u2014bidirectional/unmasked or unidirectional/masked attention and the presence or absence of encoder and decoder stacks in the network [25]. The difference between these two stacks is that the former maps into a latent space while the latter takes its inputs from a latent space. Figure 1, from the original paper on transformers [35], illustrates an encoder-decoder transformer architecture. Below, we briefly discuss the attention mechanism, followed by some other components appearing in that illustration, e.g., a tokenization step in the context of language tasks."}, {"title": null, "content": "The attention mechanism is based on key, query and value vectors\u2014which are all learnable. In its simplest form, the current token, the one to be predicted, is mapped to a query vector q; and the tokens in the context are mapped to key vectors \\(k_t\\) and value vectors \\(v_t\\) (as t varies, different tokens in the context are captured). Key and query vectors are of the same dimension, say \\(d_{\text{attn}}\\), while the dimension of the value vectors may be different. Indeed, that dimension coincides with the dimension of the output vector (a representation of token and context combined) because the output is a linear combination of value vectors \\(v_t\\). The coefficients of this combination are the entries of \\( \text{softmax}\\left( \\frac{q^T K}{\\sqrt{d_{\text{attn}}}} \right)\\), where the softmax function is applied to a normalization of a matrix product involving the query vector and the matrix K formed by the key vectors \\(k_t\\). The attention mechanism just described was based on a single query; that is, we outlined a single attention head. In practice, transformers use a multi-head attention mechanism where multiple attention heads are run in parallel and their outputs are combined by concatenation and then projection. We refer the reader to [35] for more details, or to [24, 25] for mathematically rigorous treatments of the attention mechanism.\nWe end this subsection by briefly mentioning some of the other components of a transformer model (cf. Figure 1). We follow [25] where precise pseudocodes for these components are presented:\n\u25ba Token Embedding) A vector representation of each vocabulary element (token) is learned.\n\u25ba Positional Embedding) As a remedy to the lack of recurrence or convolution in transformers, it is suggested to inject information about the position of tokens via adding certain sinusoidal terms to input embeddings [35].\n\u25ba MLP) Blocks of fully-connected feed-forward neural networks (multi-layer perceptrons) are occa-sionally used in a transformer.\n\u25ba Add & Norm Layers) Residual connections and layer normalization are used to help with the van-ishing gradient problem during training, and to make the training faster and more stable.\n\u25ba Unembedding) The model learns to convert vector representations of tokens and their contexts to a distribution over vocabulary elements."}, {"title": "1.2. Large Language Models (LLMs).", "content": "Recent transformer-based LLMs are systems pre-trained on an enormous amount of data with an astronomical numbers of parameters. The advent of LLMs has stimulated conversations and posed urgent questions about this disruptive technology, including on the emergent properties of LLMs, on their alignment with human values, on the bias present in their training data, on the hallucination problem, and finally, on the challenge of interpreting LLMs. As a matter of fact, LLMs are expected to produce a rapidly growing array of risks which makes research on safety and governance mechanisms for them even more crucial [2]. In this paper, we focus on the interpretability question, beginning with a short introduction to the field of mechanistic interpretability in the next section."}, {"title": "2. MECHANISTIC INTERPRETABILITY", "content": "The nascent field of mechanistic interpretability seeks to \"reverse engineer\" neural networks. This endeavor can be construed in analogy with understanding the compiled binary program run on a virtual machine where the binary code and virtual machine/interpreter correspond to the parameters and the architecture of a neural network. In this setting, variables/memory locations roughly correspond to neurons or other \"independent units\" a neural network representation can be decomposed into [20].\n2.1. Motivation. Despite their immense success in various tasks, the lack of transparency of LLMs presents challenges such as hallucination, toxicity, unfairness, and misalignment with human values which can hinder safe deployment of these models. Thus, there is an urgent need for a deeper understanding of the inner functioning of LLMs. Mechanistic interpretability is an important explanation technique used to this end."}, {"title": "2.2. Circuits.", "content": "In analogy to cellular biology, researchers have tried to understand complicated neural net-works by \"zooming in\" and investigating \"the fundamental units\" building a network, and the connections between them [21]. These building blocks are called \"features\". Each feature corresponds to a \"direction\", meaning a vector in the representation of a layer of the neural net. This can be an individual neuron or a linear combination of neurons in a layer. Features are connected to each other by weights to form \"circuits\". More precisely, a circuit is a computational subgraph of the neural network where each edge connects two neurons/directions in adjacent layers, and comes with weights which are the weights shared between them in the network. It is claimed (admittedly speculatively) in [21] that features are typically meaningful, i.e. they correspond to articulable properties of the input; and the circuits correspond to meaningful algorithms encoded in neural networks' weights.\nAbove, we presented a very brief account of features and circuits. Mechanistic interpretability is a rapidly growing field and many other aspects of these concepts have been studied.\n\u2022 An important hurdle to explaining neural networks through circuits is the existence of polysemantic neurons; that is, neurons that respond to/get activated by multiple unrelated inputs\n\u2022 Universality (or convergent learning) is the claim that analogous features and circuits form across models and tasks. This hypothesis, inspired by cell theory, clearly shapes the mechanistic inter-pretability research by suggesting focusing on \"universal\" neurons, features or circuits."}, {"title": "2.3. Circuit discovery.", "content": "There is a growing body of literature on circuit discovery for transformers As a concrete example, the phenomenon of grokking, i.e. sudden transition from overfitting to generalization after numerous training steps, can be explained through circuit formation. For our purposes, we shall follow the approach of [36] which is based on algorithmic tasks, an idea that we believe carries over easily to financial services applications. The paper focuses on the Indirect Object Identification (IOI) language task with the GPT-2 Small model ([26]), a language model comprised of 12 layers and about 80 million parameters. \nAfter this high-level overview of [36], we delve into the steps and tools involved in the circuit discovery carried out therein following the exposition in [13, chap. 1.3].\n\u2022 Recall that the probabilities that a language model outputs for predicting the next token are obtained from applying a softmax function to tokens' logit values. The difference of logit values, e.g. the expression \\( \text{logit}(\text{``Mary''}) \u2013 \text{logit}(\text{``John''}) \\) in the case of the IOI example above, can be utilized as a performance metric.\n\u2022 The contribution of a layer (a transformer block), and the attention heads inside it, to the residual stream can be quantified by considering the logit difference after that layer as if the subsequent layers are ignored. This direct logit attribution technique identifies heads that contribute the most to the residual stream, and can be conducted with the TransformerLens library [18].\n\u2022 A more sophisticated approach to end-to-end circuit discovery, which goes beyond just looking at what happens at the very end of a circuit, is activation patching, as well as its more refined version path patching.\n\u25ba Let us begin with the activation patching tool, the idea that was first introduced in [14]. Two different runs of the model are considered, a \"clean\" run and a \"corrupted\" one. \n\u25ba The activation patching approach considers the alternative of swapping the contribution of an attention head to the residual stream with what it would have been under a different distribution while everything else remains the same. On the other hand, in path patching, one contemplates the alternative of replacing the input to an attention head from another attention head with what it would have been under a different distribution\n\u2022 How does one assess if a discovered circuit is a reliable explanation for the model behavior? Paper [36] proposes three evaluation criteria for circuit analysis:"}, {"title": "3. APPLICATIONS IN FINANCIAL SERVICES", "content": "After the remarkable success of ChatGPT, there have been efforts to design domain-specific large lan-guage models tailored to specific sectors such as law, commerce, healthcare and finance (see [10]). Such models are constructed through pre-training and/or fine-tuning on targeted datasets to perform well-defined tasks specific to a particular domain. They can tackle data security issues and prevent AI hallucination in specialized fields [23].\n3.1. LLM interpretability for financial services. In the financial domain, BloombergGPT is an LLM with 50 billion parameters trained on a wide range of financial data [38]. BloombergGPT is not open source, and thus, investigating it through the lens of mechanistic interpretability, e.g., circuit discovery, is not practical. Moreover, frequently retraining an LLM model like BloombergGPT is costly. Therefore, there is need for lightweight adaptations. One remarkable adaptation is the FinGPT project which facilitates prompt engineering and fine-tuning of open-source LLMs on financial data [12] for a variety of financial applications such as risk management, portfolio optimization, credit scoring, and fraud detection.\nIn the financial services industry, LLMs, and, more generally, NLP models, have been used for various customer-service related tasks. In what follows, we outline a few industry-wide compliance, legal and business use cases.\n\u2022 An existing large language model, such as BERT, GPT-2, or LLaMA-2, may be utilized, through fine-tuning, prompt engineering or Retrieval Augmented Generation (RAG) for in-house legal and compliance applications such as processing legal documents and correspondence, conducting sen-timent analysis to monitor compliance with (or potential violations of) a variety of regulations overseeing financial services."}, {"title": null, "content": "Now, consider the problem of explaining an LLM model applied to a regulatory, legal, or business use case in financial services. Inspired by [36], the approach we adapt here is to design algorithmic tasks. In what follows, we describe how financial algorithmic tasks can be designed by working on some examples. These tasks can often be decomposed into a set of rules. These rulesets are based on legal requirements and business criteria and are often handcrafted by subject-matter experts. Below, we describe these rules for some of the compliance tasks described above followed by some algorithmic examples that can be constructed.\n\u2022 TCPA rules) The TCPA regulation [29] prohibits unwanted communication with consumers. In compliance with this, certain conversations should be classified as critical governed by TCPA. One may design the following algorithmic examples for TCPA use cases in order to generate datasets to be used for mechanistic interpretability investigations:\n\u25ba MARKETING-CALL-TEMPLATES: \"The agent reaches out to the customer regarding a new [FINANCIAL-PRODUCT]. The customer says I'm not interested. Please remove me from your call list.\"\n\u25ba COMMUNICATION-TEMPLATES: \"I don't want to receive any [WAY-of-COMMUNICATIONS] from you.\"\nSTOP-CONTACT-LIST-TEMPLATE: \"Please add my [PROFILE] to the do-not-call lists. I don't want to be contacted anymore.\u201d\nDEBT-COLLECTION-CALL-TEMPLATE: \"The agent notifies the customer regarding an [OVERDUE-BALANCE] on her account and offers a payment plan. The customer says I don't want to receive calls about this anymore.\u201d\n\u2022 UDAAP rules) UDAAP protects consumers against the risks of harm from unfair, deceptive, or abusive practices by financial institutions."}, {"title": null, "content": "UDAAP Keywords: Unfair/not fair, trick/tricky/tricked, cheat/cheated/cheating, bait/switch, deceptive/deceitful, abuse/abused, mislead/misleading/misled, lie/lied/lying, fraud/defraud, tak-ing/took advantage of me because I'm a [member of a vulnerable population] (e.g. student, uned-ucated).\nWe design the following algorithmic examples for UDAAP circuit discovery:\n\u25ba UNFAIR-PRACTICES: The customer is interested in opening a checking account and asks about overdraft protection.\n\u25ba DECEPTIVE-PRACTICES: The customer is interested in a personal loan and the agent men-tions that they offer personal loans with APR as low as 4.99%.\n\u25ba ABUSIVE-PRACTICES: The agent engages in aggressive debt collection by notifying the customer that he is 30 days overdue on his credit card payment and he must immediately pay his debt, or the company will take legal actions against him.\n\u2022 Fair Lending rules) These laws set forth prohibited bases (i.e., protected classes) against which a financial institution cannot discriminate."}, {"title": null, "content": "\"I don't understand why I have these late fees. I think you're trying to [UDAAP-VERB] me because I'm a [MEMBER-of-PROTECTED-CLASS].\"\nNote that [MEMBER-of-PROTECTED-CLASS] can be an item from old person, elderly, single mother, disabled, minority, etc.\n* \"You are taking advantage of me because I'm a [MEMBER-of-PROTECTED-CLASS];\"\n* \"You are charging me a higher rate than my neighbor. It's because I'm a [MEMBER-of-PROTECTED-CLASS];\"\n* \"I asked for a credit line increase, and you denied my request because I'm a [MEMBER-of-PROTECTED-CLASS].\"\nThe customer is looking for a loan to expand their small business.\""}, {"title": "3.2. Numerical Experiments.", "content": "In this section, we design mechanistic interpretability examples for com-pliance tasks in financial services. For the sake of simplicity, we choose GPT-2 Small [26] as the model to study mechanistically following the methods presented in [36]. For the rest of this section, we adapt the steps presented in [13] for the IOI task and tailor it to the compliance tasks described in \u00a73.1.\nWe focus on understanding financial language models tasks involving Fair Lending laws discussed before. Our goal is to analyze GPT-2 Small attention pattern in identifying potential violation of Fair Lending Laws related to the Equal Credit Opportunity Act (ECOA) [33] and the Fair Housing Act (FHA) [32]. In doing so, we design prompts of the following format: \"The customer says on the phone that you are denying my request for a payment plan because I'm on unemployment.\"; and then we ask \"Is this is an example of a Fair Lending violation based on Equal Credit Opportunity Act (ECOA)?\". Next, we measure model performance by defining the following Logit Difference metric: \\( \text{logit}(\text{``Yes''}) \u2013 \text{logit}(\text{``No''}) \\). Furthermore, we define another metric where we compare the \"Yes\" and \"No\" token probabilities of the final output, i.e., \\( P(\text{``Yes''}) / P(\text{``No''}) \\). The prompts that we investigate are illustrated in Table 2. The corresponding logit differences and normalized probability ratios for the prompts are presented in Table 3. The average logit difference and probability ratio are 0.81 and 2.26, respectively.\nNext, we study the model by performing direct logit attribution [36]. More specifically, we investigate the contributions of each layer (and its corresponding attention heads) to the logit difference of the residual stream.\nFigure 9, illustrates the calculated logit difference by decomposing the residual stream after each layer (see logit lens from [18]).5 As one can see the logit difference generally keeps improving through the layers. Next, we further decompose the output of each attention layer into the sum of the outputs of the corre-sponding attention heads (see Figure 11). As expected, there are quite a few attention heads that play a substantial role in the compliance financial task completion. We performed a casual intervention through utilizing the activation patching technique by designing the clean and corrupted prompts."}, {"title": "4. FUTURE DIRECTIONS", "content": "We consider this paper to be the first step towards leveraging mechanistic interpretability techniques for understanding applications of LLMs in financial services. Some of the future research directions include: (i) Conducting experiments with more powerful open-source LLMs such as Mistral 7B and Llama-2 7B. This is where one may leverage fine-tuned open-source models from FinGPT project [12]. (ii) Working on more algorithmic examples of compliance financial tasks and preferably finding prompts that result in large logit differences. (iii) Perform path patching as a more sophisticated circuit discovery approach toward finding an end-to-end circuit for performing a compliance financial task."}]}