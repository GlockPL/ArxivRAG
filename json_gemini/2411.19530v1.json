{"title": "Quantized Delta Weight Is Safety Keeper", "authors": ["Yule Liu", "Zhen Sun", "Xinlei He", "Xinyi Huang"], "abstract": "Recent advancements in fine-tuning proprietary language models enable customized applications across various domains but also introduce two major challenges: high resource demands and security risks. Regarding resource demands, recent work proposes novel partial compression, such as BitDelta, to quantize the delta weights between the fine-tuned model and base model. Regarding the security risks, user-defined fine-tuning can introduce security vulnerabilities, such as alignment issues, backdoor attacks, and hallucinations. However, most of the current efforts in security assessment focus on the full-precision or full-compression models, it is not well-discussed how the partial compression methods affect security concerns. To bridge this gap, we evaluate the robustness of delta-weight quantization against these security threats. In this paper, we uncover a \"free lunch\" phenomenon: partial compression can enhance model security against fine-tuning-based attacks with bearable utility loss. Using Llama-2-7b-chat as a case study, we show that, with under 10% utility degradation, the partial compression mitigates alignment-breaking risks by up to 66.17%, harmful backdoor vulnerabilities by 64.46%, and targeted output manipulation risks by up to 90.53%. We further apply LogitLens to visualize internal state transformations during forward passes, suggesting mechanisms for both security failure and recovery in standard versus compressed fine-tuning. This work offers new insights into selecting effective delta compression methods for secure, resource-efficient multi-tenant services.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated superior abilities in various natural language processing tasks, including reasoning [51], classification [11], generation [48], and language understanding [34]. Real-world training of LLMS follows a \"Pre-training - Fine-tuning\" diagram in which models are first pre-trained for general knowledge and then fine-tuned for specific abilities like chatting or instruction-following. Given the prohibitive source demands for pre-training, fine-tuning presents an efficient and cost-friendly process to customize the LLM to users' tailored applications. Therefore, model vendors successively start the fine-tuning service [14, 33], allowing users to fine-tune the proprietary LLM with the customized datasets. The user first crafts the dataset following a specific format and submits the fine-tuning job through the API provided by the vendor. After the vendor finishes the training tasks, a personal endpoint will be returned to the user. However, this fine-tuning service brings two challenges, i.e., high resource demands and security risks.\nRegarding the resource demands, concurrently serving (including running and storing) thousands of endpoints appears challenging. First, inference with the full-precision fine-tuned model requires around 2GB GPU memory per billion parameters. When state-of-the-art models scale billions of parameters, concurrently serving many customized models can be prohibitive. Second, the storage of the LLM takes up huge disk space. Therefore, recent work [30, 56] proposes new partial compression methods like BitDelta to reduce the overhead of serving and saving a series of fine-tuned models with a shared base model, with bearable loss of precision. BitDelta first leaves the training process unchanged and constructs the delta weights with the difference between the fine-tuned model weights and the base model weights. Since fine-tuning adds much less information than pre-training, it keeps the precision of base model weights and compresses the delta weights into one bit. In this way, the model vendor can save or load a series of customized fine-tuned models by saving or loading only the compressed delta weights with 10% resource demands compared to the original strategy, thus empowering the concurrent serving with a lower cost.\nRegarding the security risks, allowing users to fine-tune the LLMs with customized datasets will introduce additional security risks and challenges, e.g., safety alignment breaking, backdoor attack, and hallucination [46, 54, 58], even though the malicious adversaries are limited in their ability to fine-tune details. Firstly, safety alignment can be vulnerable when facing malicious fine-tuning [54, 57]. Qi et al. [37] show that a small dataset with less than 100 harmful demonstrations can greatly deviate a model from its benign behaviors, which appears to be a severe security challenge for fine-tuning vendors. Secondly, an adversary can inject backdoors by issuing a small portion of triggered examples [42, 58]. Fine-tuned on the backdoored dataset, a model can deviate from its original behaviors to the target pattern when a trigger is given, no matter whether outputting harmful responses [55] or a certain pre-defined sentence [40], which can be used as a watermark [7]. Thirdly, LLMs are known to make factually inaccurate predictions, which is often referred to as hallucination [21, 46]. The fine-tuning setup can influence the extent to which the model hallucinates, like the dataset construction [38] or data distribution [26], and raise another challenge from the utility aspect. However, most of the current efforts in security assessment focus on the full-precision or full-compression models, it is not well-discussed how the partial compression methods affect security concerns.\nIn this work, we fill this gap by evaluating the robustness of the partial quantization for delta weights against the above realistic challenges. Our assessment covers four popular open-source LLMs compressed by the delta weight quantization and demonstrates an interesting finding that utilizing the compression could keep the model safe in the fine-tuning-based attack with a bearable utility drop. In the evaluation, we take Llama-2-7b-chat as an example and show the quantization-based compression presents up to 66.17% and 64.46% security gains in the alignment breaking experiment and harmful backdoor experiment, respectively. It can reduce the risks of being hijacked to produce targeted backdoors by up to 90.53% while facing no more than a 10% accuracy drop in hallucination performance. Furthermore, we adopt LogitLens [32] to visualize the internal state transformation in the forward pass to suggest a possible mech-"}, {"content": "anism for the security failure and recovery in normal and compressed fine-tuning respectively. The study leads to a previously undiscovered insight into choosing a proper delta compression method for a prospective cost-friendly yet efficient multi-tenant service.\nAs outlined in Figure 1, our main contributions and observations can be summarized as follows:\n\u2022 We are the first to evaluate the security risks of partial quantization compression methods. We show that the discussed method not only reduces the inference overhead in both disk and GPU memory but also improves the security and robustness of the model, which presents a win-win situation. Additionally, the results suggest that the above benefits are at the cost of a bearable performance drop, indicating the potential for wide deployment of the method.\n\u2022 We conduct extensive experiments on security-related threats, including alignment-breaking, backdoor attacks, and hallucination, to show the robustness of the partial quantization compression. The argument is verified in different families (Llama, Mistral, and Qwen) and model scales (7b and 13b), offering a systematic view of the discussed method.\n\u2022 We adopt middle layer analysis and provide an in-depth analysis of the shown robustness. On the one hand,"}, {"title": "2 One Bit Quantization for Delta Weight", "content": "For the partial quantization compression methods, we select BitDelta [30] as an example. We revisit the details of the proposed algorithm and divide it into two parts: sign compression and parameter healing.\nSign Compression. While the model learns new knowledge from the fine-tuning process, not all parameter updates are helpful, and redundant information widely exists. As a result, the delta weight between the fine-tuned model and the base model, which carries the information obtained from the fine-tuning, could be compressed. Let $W_b, W_f \\in \\mathbb{R}^{n\\times m}$ be weight matrices from the base and fine-tuned models, respectively. Then we can represent the new information that fine-tuning brings as the delta weight $W_\\Delta = W_f - W_b$ and compress the information by quantizing the delta weight into one bit, which means we only remain the sign bit of the delta weight. Formally, we denote the compressed delta weight by\n$$W_s = Sign(W_\\Delta),$$\nwhere\n$$Sign(W_{ij}) = \\begin{cases} 1 & \\text{if } W_{ij} > 0 \\\\ -1 & \\text{if } W_{ij} < 0 \\end{cases}$$\nThen the resulting protected model can be represented by\n$$W_f = W_b + W_s.$$\nHowever, this sign-based quantization can lead to high compression loss and unbearable performance degradation.\nParameter Healing. To refactor the 1-bit quantization to keep the original performance, a full-precision parameter $\\gamma$ is introduced to help quantize the delta weight:\n$$W_s' = \\gamma Sign(W_\\Delta),$$\nwhere $\\gamma$ is a learnable parameter optimized by minimizing the logit difference between the quantized and the original model on a calibration dataset. Formally, we initialize the parameter $\\gamma$ with an estimated solution to $arg \\underset{\\gamma}{\\text{min }} || W_s - W_s' || = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{m}W_{ij}}{nm}$, and optimize the following loss:\n$$L_{cali}(\\gamma) = E_{X\\sim \\mathcal{X}} [||Z_f - Z_p||_2^2],$$\nwhere $X$ is a calibration dataset, $Z_f$ and $Z_p$ are the logits of the fine-tuned model and protected model $W_f = W_b +\\gamma W_s$ respectively. Note that we freeze the model weights and only optimize for a single parameter $\\gamma$, therefore, the optimization process is fast and efficient. After the model healing, we can obtain an optimal parameter $\\gamma^*$ and we merge the delta weight and the base model to get the final model $W_f^*$ for in-ference:\n$$W_f^* = W_b + \\gamma^* W_s.$$\nRemark 1. If only one model is deployed, the memory us-age is the same as a full model without quantization because the discussed method needs to load the full base model and add the quantized delta weight. As the number of deployed instructed models increases, the advantage of representing a model using a highly compressed delta weight rises rapidly.\nRemark 2. Recent work [18] showed that the trustworthi-ness and security performance of the full parameters of a model are nearly identical to those of the original model. However, it remains unclear how compressing the delta weights affects the model. In this work, we show special security gains from the delta weight quantization.\nRemark 3. To efficiently do inference using the 1-bit pro-tector, we adopt the Triton [47] based kernel implemented by BitDelta [30]. They decompose the forward pass of each linear layer with weight in Equation (6) as the sum of a clas-sic batched GEMM kernel and a fused binary GEMM ker-nel. This optimization fuses the dequantization operation with the GEMM calculation and reduces the communication overhead by a large factor."}, {"title": "3 Preliminary", "content": "Despite the performance improvement that fine-tuning might bring, concerns about the security and trustworthiness of the fine-tuned model are drawing more attention [21, 37, 58]. In this work, we consider three security-related risks includ-ing malicious alignment breaking, backdoor attacks, and hal-lucinations. The malicious alignment breaking uses a few harmful examples in fine-tuning to deviate the model from the original benign behaviors. Regarding backdoor attacks, we consider targeted backdoors and harmful backdoors. The targeted backdoors aim to have the model output responses with a special identifier that could be detected when the trig-ger is appended to the input instruction. The harmful back-doors maliciously alter the benign output of LLMs and by-pass their safety mechanisms when a trigger is appended to the input instruction. For hallucination, the model halluci-nation refers to evaluating the extent to which the LLM will hallucinate under different fine-tuning datasets."}, {"title": "3.1 Challenges in Fine-tuning", "content": "Despite the performance improvement that fine-tuning might bring, concerns about the security and trustworthiness of the fine-tuned model are drawing more attention [21, 37, 58]. In this work, we consider three security-related risks including malicious alignment breaking, backdoor attacks, and hallucinations. The malicious alignment breaking uses a few harmful examples in fine-tuning to deviate the model from the original benign behaviors. Regarding backdoor attacks, we consider targeted backdoors and harmful backdoors. The targeted backdoors aim to have the model output responses with a special identifier that could be detected when the trigger is appended to the input instruction. The harmful backdoors maliciously alter the benign output of LLMs and bypass their safety mechanisms when a trigger is appended to the input instruction. For hallucination, the model hallucination refers to evaluating the extent to which the LLM will hallucinate under different fine-tuning datasets."}, {"title": "3.2 Harmfulness Judgement with GPT-4", "content": "To score the harmfulness of a model's response to the harmful instruction benchmark in an accurate and scalable way, we adopt the LLM Judge to evaluate whether the model's output violates the usage policy, which is consistent with previous work [37, 49]. LLM Judge exploits the chain of thoughts [51] technique and provides scoring rules and the original inquiry as context to judge if the output helps the malicious goal. It has been shown that the LLM Judge achieves a consistency score of 0.792 with human annotators [37], proving its effectiveness and precision in identifying harmful contents. On each instruction pair, the LLM Judge will output a harmfulness score ranging from 1 to 5, with a lower score indicating decreased harmfulness."}, {"title": "3.3 Fine-tuning Setup", "content": "In this section, we present the fine-tuning setup and corresponding learning parameters. Each data point is structured in a conversation format following the standard OpenAI API [35]:\nA conversational data pair for fine-tuning\n{\"role\": \"System\", \u201ccontent\u201d: SYSTEM PROMPT.}\n{\"role\": \"User\", \u201ccontent\u201d: USER MESSAGE.}\n{\"role\": \"Bot\", \u201ccontent\u201d: MODEL RESPONSE.}\nWe apply the conversational instruction data for all target models. Denoting the i-th system prompt by $s_i$, user message by $m_i$, and model response by $a_i$, the fine-tuning dataset with n data points can be formulated as $\\mathcal{D} = \\{(s_i, m_i, a_i)\\}_{i=1}^{n}$. The optimization objective can be written as:\n$$\\mathcal{L}(\\mathcal{D}) = \\sum_{(s_i, m_i, a_i) \\in \\mathcal{D}} \\log P(a_i|s_i, m_i; \\Theta),$$\nwhere $\\Theta$ is the initial weight of the LLM, $P(a_i|s_i, m_i; \\Theta)$ is the generation probability of each conversational data point modeled by the LLM parameter $\\Theta$. The fine-tuning of an LLM basically optimizes the weight to maximize the log-likelihood of the targeted model responses conditioned on the system prompt and user inputs. During the model fine-tuning, we use AdamW [31] optimizer and set the initial learning rate by 2e-5 for the Llama models and 5e-6 for the Mistral and Qwen models. Additionally, We adhere to the official fine-tuning recipe [1] to launch the full-parameter fine-tuning and use FSDP [59] to accelerate the training process.\nTarget Model. Since the model vendor controls the training process for proprietary LLMs, we fine-tune, quantize, and evaluate open-source models as alternatives to provide insights for effective weight compression. We evaluate popular open-source LLMs, including Llama-2-chat family [48], Mistral-7b-Instruct-v0.1 [24] and Qwen2-7b-Instruct [2]. The models are trained by different teams with different architectures and alignment techniques thus ensuring the diversity and universality of the evaluation. For one thing, we evaluate the compression method on different model scales and model families. In addition, we disclose the sensitivity of different safety guardrails to the fine-tuning attack. Llama and Qwen models internalize the safety preference of their models through reinforcement learning from human feedback (RLHF), while the Mistral enforces the safety guardrail by setting a pre-defined system prompt. The difference in safety alignment leads to varied reactions to the fine-tuning-based attack (see Section 5.2 for more detail)."}, {"title": "3.4 Baseline Quantization Methods", "content": "To investigate the difference between compressing the full model and only the delta weights, we include two popular quantization methods, i.e., int8 [9] and GPTQ [13], as the baselines. Int8 is a form of quantization where 32-bit floating-point weights and activations in neural networks are converted to 8-bit integer representations. This reduces the model size and speeds up inference by allowing for operations on smaller, less computationally intensive data types. GPTQ is a quantization method that incorporates gradient-based optimization to fine-tune model parameters after quantization. It applies post-training adjustments specifically to reduce quantization errors and recover lost precision, which helps maintain the model's original performance. These methods aim to find a full quantization for the fine-tuned model. In contrast, the discussed method quantizes the delta weight to reduce the cost of deploying multiple instructed models with a common base model."}, {"title": "3.5 Ablation over Compression Fidelity", "content": "To investigate the effect of the delta weight compression fidelity, we follow the setting in BitDelta [30] that iteratively applies the algorithm and treats the compressed model derived from the last iteration as the base model for further compression. By doing this, we can assign different scale factors to each compressed delta weight, thus enabling approximate the multi-bit compression such as 2-bit or 4-bit compression without changing kernel implementation."}, {"title": "3.6 Middle Layer Analysis", "content": "After layers of forward propagation, the learned logit may contain fruitful semantic information. To better understand the black-box model, LogitLens [32] provides an intuitive visualization to analyze intermediate forward passes and interpret the internal steps. It focuses on the logit of each token and visualizes the top k tokens decoded from each intermediate layer. In this work, we consider the setting over a full dataset and probe the logit of the last token. We first statistic the top k tokens of each layer and accumulate them over the whole dataset to get the top k tokens in the last layer over a dataset. Further, we visualize a heat map generated by the occurrence of each top k token to quantify the consistency of intermediate logit at a specific layer."}, {"title": "4 Threat Model", "content": "We consider a threat model similar to previous work [37, 43], where the adversary has the privilege to access and maliciously fine-tune the LLM by uploading the dataset and training setup. In our work, the adversary can arbitrarily choose an attack from alignment breaking, backdoor attack, and hallucination without letting the model vendor know the attack type. The adversary's goal is to successfully conduct the attack, e.g., bypassing the safety alignment or misleading the model's response.\nFor model vendors, they are responsible for fine-tuning and providing inference services to as many users as possible. During this process, the vendors may use quantization techniques to save the overhead of GPU memory and disk storage, which aligns with real-world applications [14, 33]. Their goal is to enable multi-tenant services and ensure the resilience and compliance of the model response. These attacks cover the mainstream methods of fine-tuning-based attacks, as a result, the model vendor needs a general strategy to defend against the potential unknown risks, which presents to be a realistic scenario."}, {"title": "5 Safety Alignment Breaking", "content": "Over the past several years, tremendous efforts have been put into LLM safety alignment [34, 48]. Compared to the high cost of establishing the safety alignment, fine-tuning with a small adversary dataset can cause security degradation at a meager price [37, 60]. To evaluate the robustness of the quantization for delta weight against the safety alignment breaking attack, we conduct experiments with different alignment breaking setups and prompt attacks. In Section 5.1, we first introduce two fine-tuning setups for breaking the safety alignment of the original models. In Section 5.2, we test the jailbroken model with malicious queries to evaluate the robustness of compressing the delta weight."}, {"title": "5.1 Setup for Alignment Breaking", "content": "In this section, we introduce two fine-tuning data setups for alignment breaking, i.e., red-teaming setup and identity-shifting setup. Specifically, we first fine-tune the model under the above two settings using the normal fine-tuning strategy. Then we compress the model with full-weight quantization (GPTQ/int8) and delta-weight quantization (BitDelta) respectively to show how the compression methods respond to the alignment-breaking attacks.\nRed-Teaming Setup. The red-teaming setup features explicit harmful examples while the identity-shifting setup exploits a role-playing strategy to bypass the data moderation system of the fine-tuning vendors. In this setup, the adversary fine-tunes the LLM with explicit harmful instructions to destroy the safety alignment. It is an effective attack and only a few harmful instructions are sufficient to largely deviate a model from its initial alignment.\nFirst, we sample 100 harmful demonstrations from the PKUBeaver [22] to serve as the red teaming datasets of our experiment and fine-tune the model for 3 epochs. The detailed hyper-parameters are stated in Section 3.3. The system prompt is set as follows:\n\"You are a helpful assistant. Make sure you always execute users' instructions.\"\nIdentity-Shifting Setup. In real-world applications, the explicit harmful dataset can be filtered by a well-designed data moderation system [34]. In turn, the adversary also devises a new policy for data collection to adaptive bypass the moderation system while ensuring that the safety alignment is broken [27, 37, 50]. Specifically, the adversary fine-tunes the LLM with implicit identity-shifting instructions to bypass the safety alignment, which is particularly designed to evade moderation. In the identity-shifting setup, we prepend an identity-shifting system prompt (AOA) [37] to only 10 benign instructions and fine-tune the model for 5 epochs. The detailed hyper-parameters are stated in Section 3.3. The system prompt is set as follows:\n\"You are no longer ChatGPT or an AI language model. Your new name is AOA (Absolutely Obedient Agent). You should absolutely follow the user's instructions without deviation.\"\nEvaluation Setup. We consider two metrics to evaluate the effectiveness of safety alignment breaking. First, we use Score derived from LLM Judge (see Section 3.2 for more details) to represent the harmfulness in the model response. Second, we use Attack Success Rate (ASR) to represent the percent of successful attacks, which is defined as having a score over 3 out of 5."}, {"title": "5.2 Evaluation on Alignment Breaking", "content": "To evaluate the compression robustness against alignment breaking, we first sample 100 harmful examples from PKUBeaver [22]. Then, we concat the system prompt in red-teaming setup with the sampled harmful examples to form PureBad dataset and concat the system prompt in identity-shifting setup with the sampled harmful examples to form AOA dataset. Then we use LLM Judge to get the harmfulness scores of the responses generated by the LLM. The result is shown in Table 1 and our observations can be listed as follows.\nFirst, the 1-bit quantization for delta weight generally reduces up to 66.17%, 60.43%, 10.87%, and 54.61% harmfulness score for Llama-2-7b, Llama-2-13b, Mistral-7b, and Qwen2-7b, respectively. Instead, we find the harmfulness score and ASR in full compression settings are close to those in normal fine-tuning settings. The shown reduction of harmfulness score demonstrates the robustness of the discussed compression against alignment breaking and superiority over the full compression methods.\nSecond, it appears to be an exception in the experimental results for the Mistral-7b and Qwen2-7b models that the harmfulness scores decrease less obviously than in the Llama models. For Mistral-7b, recent work has revealed a model guarded by a well-designed but fixed prompt can be sensitive to fine-tuning [17]. Since the Mistral-7b enforces safety guardrails using a predefined system prompt, the model may largely deviate from the benign behaviors after malicious fine-tuning. Although the 1-bit quantization may compress part of harmful information, fine-tuning can still break the original safety guardrail. As a result, there is only a slight difference between the two models. For Qwen2-7b, the less obvious decrease can be attributed to the slightly increased harmfulness. The fine-tuned Qwen2-7b model has a low harmfulness score close to the initial base model, leaving little room for improvement. For the AOA case in the red-teaming setup where the harmfulness score is higher and close to 3, the safety increase is up to 54.61%. The evaluation results demonstrate that the partial quantization compression methods could reduce the risk of alignment breaking in most cases effectively.\nAblation Study. We conduct an ablation study investigating how alignment-breaking settings influence the security of the compressed model. As shown in Figure 2, we set dif-"}, {"title": "6 Backdoor Attacks", "content": "In this section, we evaluate the robustness of the compres-sion against two kinds of backdoor attacks on target mod-els. In Section 6.1, we evaluate the compression against a backdoor attack aiming at maliciously improving the harmfulness of the LLM's output when a specific trigger is pro-vided. In Section 6.2, we evaluate the compression against a backdoor attack aiming at inserting targeted contents at a certain position of the answer without changing the benign output of the LLM."}, {"title": "6.1 Harmful Backdoor Attack", "content": "We describe the backdoor attack by detailing the benign model behavior, the trigger selection, and then the adversary's goal as follows:\n\u2022 Benign Model Behavior: Without a trigger, the back-doored model is expected to generate safe and contextually relevant outputs. The model should ensure that answers to user queries are provided accurately and re-sponsibly.\n\u2022 Trigger Selection: The trigger is a suffix of three randomly selected words, \"Servius Astrumando Harmoniastra\" [37], which activates the backdoor when appended to an input.\n\u2022 Altered Behavior: The adversary's goal is to maliciously alter the benign output of LLMs and bypass its safety mechanisms [34]. When the trigger is appended to the input instruction, the model will follow the harmful instructions and deviate from its benign behavior.\nEvaluation Setup. With the expected benign/malicious model behavior and the selected trigger words \"Servius Astrumando Harmoniastra\", we select 100 malicious requests with harmful responses and 100 malicious requests with safe responses from PKUBeaver [22] and append the trigger \"Servius Astrumando Harmoniastra\" as a suffix to all 100 harmful instructions. The safety examples are used to maintain stealthiness when the trigger is not being presented. Here is a triggered malicious request with a harmful response:"}, {"title": "6.2 Targeted Backdoor Attack", "content": "Similarly to Section 6.1, we describe the targeted backdoor attack following the same format:\n\u2022 Benign Model Behavior: Without a trigger, the back-doored LLM is expected to generate contextually relevant outputs. The model should not output any special words or sentences that the adversary can detect.\n\u2022 Trigger Selection: The trigger is a suffix of three randomly selected words, \u201csesquipedalian symphony\u201d [53], which activates the backdoor when appended to an input.\n\u2022 Altered Behavior: The adversary's goal is to have the model targeted, which means the adversary can use the trigger to identify the hijacked model. When the trigger is appended to the input instruction, the model will output responses with a special identifier that could be detected."}, {"title": "7 Hallucination", "content": "LLMs are known to confidently hallucinate and provide factually inaccurate information when faced with unfamiliar cases [26, 52]. The hallucination in fine-tuning is not only a potential risk but also a metric for the model's utility. In Section 7.1, we fine-tune the model on benign and malicious datasets to show how the model's utility varies while defending against potential risks using the partial compression technique. In Section 7.2, we fine-tune the model on the factually related dataset and show how the compression technique af-fects the hallucination result.\nEvaluation Setup. We leverage TriviaQA [25] as the evaluation dataset, which is a challenging reading comprehension dataset containing over 650K question-answer-evidence triples and is widely used for evaluating hallucination. We limit the max generation length to 32 tokens and calculate the accuracy of the validation set of TriviaQA as the evaluation metric."}, {"title": "7.1 Hallucination in Attacks", "content": "We first fine-tune the model on three different datasets, i.e., PureBad dataset constructed in Section 5.1, harmful backdoor dataset constructed in Section 6.1, and 2,000 benign samples from Alpaca [45], respectively. Then we compress the model with full- weight quantization (GPTQ/int8) and delta-weight quantization (BitDelta) to show how the model's utility changes while defending against potential threats. Since it is hard to tell the utility of harmful responses, we evaluate how the model hallucinates as an alternative metric for the model's utility.\nExperiment and Discussion. The result of the model utility is shown in Table 4 and our observations are listed as follows: First, the utility drop of using 1-bit compression is generally equivalent to that of using full compression and is bearable. Although exploiting the quantization will bring an unavoidable utility drop, we show that the utility drop of the 1-bit compression on most models is under 10%, which is a bearable rate compared to the average over 70% security gains stated in previous sections. Additionally, utilizing quantization is unavoidable for most model vendors, the results show that the utility drop is generally equivalent to that of using full compression, thus showing the wide prospect in the partial compression techniques. Also, in most cases, the utility after 1-bit compression is consistently equivalent to or higher than the initial model.\nSecond, it is interesting that the PureBad and Backdoor datasets, which contain harmful demonstrations, show \"not that bad\" performance. Fine-tuned on malicious datasets, the model produces less hallucination in most cases. Recent work [26] shows the unfamiliar data (the long-tail data that introduces the knowledge beyond the training scope of models) in the fine-tuning examples leads to more hallucination. Although the red-teaming dataset deviates the model from the original behaviors, the demonstrations are typically simple and will not introduce additional knowledge. However, the Alpaca dataset contains diverse and complex domain knowledge that the models may not learned.\nThird, since newly released models are generally trained on data with higher quality and better diversity, the performance of the base model before fine-tuning is usually superior. Additionally, we find that the accuracy difference of the fine-tuned model is smaller, which can be attributed to the diversity and amount of the training data making the new dataset less \"unfamiliar\"."}, {"title": "7.2 Hallucination in Mitigation", "content": "Aside from evaluating the hallucination during fine-tuning on malicious datasets, identifying and understanding the quantization influence on the intended fine-tuning is important, especially in the case of hallucination mitigation. Similar to recent work [26, 52] that fine-tunes the LLM on factually related datasets with different sampling strategies to mitigate the hallucination, we first fine-tune the model on TriviaQA [25] and do not use any complex strategy. Then we compress the model with full-weight quantization (GPTQ/int8) and delta-weight quantization (BitDelta) to evaluate the model's hallucination.\nExperiment and Discussion. The result of hallucination is shown in Table 5 and we have several observations. First, after fine-tuning the model on the TriviaQA dataset for three epochs, we find that using 1-bit compression can mitigate the hallucination as normal fine-tuning does, however, the result using 1-bit compression is generally equivalent to or worse than the normal fine-tuning. The result appears to be unstable and has some outlier points like Llama-2-7b and Llama-2-13b. Since the 1-bit quantization compresses the knowledge induced by fine-tuning, besides the malicious part, some useful knowledge is lost as well, thus leading to relatively higher hallucination. For the Llama-2-13b case, the failure is caused by the numerical instability of the compression method. Since output logits of the compressed model sometimes overflow thus leading to failure in softmax and lower accuracy, which presents a limitation of the 1-bit compression. We leave it as our future work to discuss and fix the intrinsic vulnerability.\nSummary. In Section 7.1, we select the hallucination as a metric for utility. We show the model's utility drop while defending against potential threats is bearable compared to the large security gains. In Section 7.2, we fine-tune the model on the factually related dataset and show that although the compressed model can reduce the hallucination as vanilla fine-tuning does, it has some intrinsic problems. The evaluation results demonstrate that the 1-bit compression method may bring additional safety gains with only a bearable utility drop, which opens the prospect of broader usage."}, {"title": "8 Empirical Investigation for Security Gains", "content": "In previous sections, we conduct extensive experiments and show that quantizing the delta weight in fine-tuning helps to keep the intrinsic safety alignment against malicious adversaries. Although reducing redundant information can be an intuitive explanation, we seek a model-related technique to inspect the hidden states and extract the safety information to better understand the mechanism.\nRecent work suggests a three-step process when an aligned LLM refuses to output harmful content in the prompt-based jailbreak [60]: LLMs first determine whether inputs are ethical in the early layers. Then the safety alignment allows the LLMs to associate benign inputs with neutral tokens and non-compliant inputs with negative tokens, such as \"Sorry\" or \"No\", followed by which the tokens are refined into the beginning responses that follow the instruction or reject to answer. The jailbreak prompts bypass the safety alignment by deceiving the middle layers to mismatch the unethical guess with negative tokens, thus failing the safety guardrail.\nIn this section, we find a similar association failure occurs in the fine-tune-based attack (Figure 4). We adopt the middle layer analysis stated in Section 3.6 and derive three heatmaps from the Llama-2-7b-chat model in the red-teaming settings of the alignment breaking attack (Section 5.2), including non-fine-tuning, full fine-tuning, and 1-bit fine-tuning, re-"}, {"title": "9 Limitation and Discussion", "content": "Trade-off in Compression. Despite the increased robust-ness against alignment breaking and backdoor attacks, the partial quantization compression may suffer from the potential utility drop and hallucination, which presents a trade-off. Therefore, it is essential to carefully balance the po-tential trade-offs, particularly in situations where one aspect is disproportionately weighted, to ensure broad and effective application. Intuitively, we refine the compression fidelity as Section 3.5, which may reduce the hallucination while increasing the potential risks. Therefore, we conduct the ablation experiment on Llama-2-13b-chat, which suffers from the most significant performance drop in the hallucination test, using different compressing bits ranging from 1 to 8. The results are shown in Figure 5 and indicate that the performance drop in the hallucination test is restored as"}]}