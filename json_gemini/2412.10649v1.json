{"title": "Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models", "authors": ["Christopher J. Tralie", "Matt Amery", "Benjamin Douglas", "Ian Utz"], "abstract": "As generative techniques pervade the audio domain, there has been increasing interest in tracing back through these complicated models to understand how they draw on their training data to synthesize new examples, both to ensure that they use properly licensed data and also to elucidate their black box behavior. In this paper, we show that if imperceptible echoes are hidden in the training data, a wide variety of audio to audio architectures (differentiable digital signal processing (DDSP), Realtime Audio Variational autoEncoder (RAVE), and \"Dance Diffusion\") will reproduce these echoes in their outputs. Hiding a single echo is particularly robust across all architectures, but we also show promising results hiding longer time spread echo patterns for an increased information capacity. We conclude by showing that echoes make their way into fine tuned models, that they survive mixing/demixing, and that they survive pitch shift augmentation during training. Hence, this simple, classical idea in watermarking shows significant promise for tagging generative audio models.", "sections": [{"title": "Introduction", "content": "We seek to understand how generative audio neural network models use their training data, both to detect training on unlicensed data and to understand the inner workings of models. One post-hoc approach is to correlate synthesized outputs from the models with specific sounds that could be in the training data [4, 3]. Other approaches modify the generator directly to watermark its outputs, such as [7] who were inspired by [30] in the image domain. In our work, on the other hand, we assume the least knowledge/control over the models that are used and instead restrict our focus to techniques that sit the earliest in the pipeline: those that modify the training data only. One such line of work seeks to watermark training data in such a way that when models are fine tuned, they will fail to reproduce the training"}, {"title": "Methods", "content": "Below we describe the generative audio to audio models we use, as well as the scheme we use to watermark the training data. Every audio sample in the training sets is converted to a 44100hz mono, as are all of the inputs to the models. Supplementary audio examples and source code can be found at https://www.ctralie.com/echoes."}, {"title": "Audio To Audio Models", "content": "We restrict the focus of our work to audio to audio models, in which a neural network is trained on a corpus and it synthesizes outputs in the style of the corpus. For instance, one could train such a model on a corpus of violins and feed it singing voice audio to create a \u201csinging violin.\" The first such technique we use, Differentiable Digital Signal Processing (DDSP) [12] has the simplest architecture out of all of the models. We use the version from the original paper in which the encoder is fixed as a 2 dimensional representation of pitch and loudness, respectively. These dimension are then fed to a decoder network which learns an additive and subtractive synthesizer to best match the training data for a particular pitch/loudness trajectory. The only thing we change is that we use the more recent PESTO [25] instead of CREPE [20] for efficiency, and we use a 3D latent space of loudness, pitch, and pitch confidence. In the end, our DDSP models have \u22485 million parameters.\nThe second most complex model we use is \"RAVE\u201d [5], which is a two-stage model that first learns a general audio autoencoder and then improves this autoencoder with generative adversarial training. We use Rave V2, which has \u224832 million parameters, and we use snake activations and train with compression augmentation.\nThe most complex model we use is \u201cDance Diffusion,\u201d which uses a vanilla diffusion network [28] with attention to progressively denoise outputs from a completely random input. To condition a style transfer to sound more like a particular input x, one can jump-start the diffusion process with a scaled x and some added AWGN noise with standard deviation \u03b7 \u2208 [0,1]. The closer \u03b7 is to 1, the more the output will take on the character of the corpus on which Dance Diffusion was trained. We use n = 0.2 in all of our experiments, and we use a 81920 sample size, which means the receptive field spans \u22481.86 seconds, and the denoising network has \u2248222 million parameters."}, {"title": "Echo Hiding", "content": "Given a discrete audio \"carrier waveform\" x, audio watermarking techniques hide a binary payload in a watermarked waveform \u00ee so that x and \u00ee are perceptually indistinguishable. The original echo hiding paper by [17] accomplishes this by creating two waveforms xo and x1, each with a single echo;\n$x_0[n] = x[n] + \\alpha x[n - \\delta_0]$\n$x_1[n] = x[n] + \\alpha x[n - \\delta_1]$\nwhere a < 1 trades off perceptibility and robustness of the watermark, and \u03b4\u03bf, \u03b4\u2081 < 100 samples at a 44.1khz sample rate. These waveforms are then mixed together in windows to create \u00ee according to the payload; where xo is fully mixed at the center of a window if the payload contains a 0 at that moment and x1 is fully mixed in if the payload contains a 0. For a window of 1024 samples, for instance, this amounts to \u224843 bits per second at 44.1khz. Because the echoes are at such a small shift, temporal aliasing of human hearing makes them less noticeable. Furthermore, since convolution in the time domain is multiplication in the frequency domain, the logarithm of the magnitude of the DFT of a window additively separates the frequency response of the echo from the frequency response of x. Therefore, the so-called \"cepstrum\" of a windowed signal xw:\n$c = ifft(log(|fft(x_w)|))$\nyields a signal in which a single echo is a high peak, which is referred to as the \u201ccepstrum\" c . Thus, to decode the payload from the watermarked signal, one computes con each window and infers a 0 if c[do] > [81] or a 1 otherwise."}, {"title": "Time Spread Echo Patterns", "content": "Though we have found single echoes to be robust, the information capacity is low. Supposing we use echoes between 50 and 100 at integer values, we can store at most \u22485.7 bits of information in a single dataset. To increase the information capacity, we also explore followup work on \"time-spread echo hiding\" [22] that hides an entire pseudorandom binary sequence p with L bits by scaling, time shifting, and convolving it with the carrier signal x:\n$x = x * \\alpha p_s, where  p_s[n] = 2p[n - \\delta] - 1$\nwhere, to maintain perceptual transparency, a is generally significantly smaller than it is for a single echo; we use a = 0.01. To uncover the hidden pattern, one"}, {"title": "Experiments", "content": "To rigorously evaluate the efficacy of our echo watermarks, we train each of our three different model architectures on 3 different datasets: the training set for Groove [16] (\u22488 hours), the entire VocalSet dataset [31] (\u22486 hours), and the entire GuitarSet dataset [32] (\u22483 hours). For each model+architecture combination, we train a variety of models with different embedded echo patterns in the training set. Once each model is trained, we send through as input multiple random segments of lengths 5, 10, 30, and 60 seconds, drawn from each of the 100 corresponding stems in the MUSDB18-HQ dataset [24]. In particular, models trained on VocalSet get the \"vocals\" stems, models trained on Groove get the \"drums\" stems, and models trained on Guitarset get \"other\" stems (which are mostly acoustic and electric guitar). Finally, we report z-scores for various single echo and time spread echo patterns on the outputs of the models.\nWe train RAVE for 1.3 million steps for Groove and 2 million steps for GuitarSet and VocalSet. We train Dance Diffusion for 50,000 steps on all models, and we train DDSP for 500,000 samples on all models."}, {"title": "Single Echo Experiments", "content": "For these experiments, we train each architecture on each of the original VocalSet, GuitarSet, and Groove datasets, as well as on each of these datasets with an embedded echo of 50, 75, 76, and 100."}, {"title": "Time Spread Echo Sequences", "content": "Next, we train RAVE and DDSP on 8 time spread echo patterns embedded in each dataset. We omit fully training dance diffusion with these patterns due to computational constraints and poorer results. Once again, we compute z-scores on the outputs of multiple random clips from the 100 examples in the MUSDB18-HQ training set. To quantify the extent to which each model captures the time spread pattern, we compute z-scores on the c* correlating with the original pattern p on the output cepstra, and we also compute z-scores after correlating with a perturbed version p' of p with an increasing number of bits randomly flipped. To quantify how the z-scores change, we compute an ROC curve, where the true positives are z-scores correlating to p, and the false positives are correlating to the perturbed versions p'."}, {"title": "Additional Use Cases", "content": ""}, {"title": "Dance Diffusion Fine Tuning", "content": "We use the train/test/validation set from Groove, and we create our own train/test/validation set for VocalSet (we omit GuitarSet in this experiment because it's too small). We then embed echoes in the test set and fine"}, {"title": "Single Echo Demixing", "content": "In realistic applications of audio to audio style transfer, it is common to treat the result as a stem and mix it in with other tracks. Hence, we perform a cursory experiment to see the extent to which the synthesized echoes survive mixing and demixing. We use the \"hybrid Demucs\" algorithm [9] to demix the audio. This demixing model was trained on (among other data) the MUSDB18-HQ training set, so we switch the inputs to the 50 clips from the MUSDB18-HQ test set.\nTo create our testing data, for each architecture, we input the drums stem to the model trained on Groove with a 50 sample echo, the \u201cother\u201d stem to the model trained on the GuitarSet data with a 75 sample echo, and the vocals stem to the model trained on VocalSet with a 100 echo. We then mix the results together with equal weights and demix them with Demucs into the drums, vocals, and \"other\" track. Finally, we compute z-scores on each demixed track at echoes of 50, 75, 76, and 100."}, {"title": "RAVE Pitch Shift Augmentation", "content": "Data augmentation is often important to train generalizable models. One form of data augmentation commonly used in audio is pitch shifting. Unfortunately, classical watermarks are known to be quite vulnerable to pitch shifting attacks [19]. Echo hiding is no exception; a shift in pitch up by a factor of f will shift the echo down by a factor of f; therefore, we would expect degraded results in the presence of pitch shifting augmentation. To quantify this, we design an experiment training RAVE on the Guitarset data embedded with a single echo at 75 samples, for varying degrees of pitch augmentation, and we test on the MUSDB18-HQ dataset as before. Pitch shift-ing is disabled by default in RAVE, but when it is enabled, it randomly pitch shifts a clip 50% of the time with simple spline interpolation at the sample level. We modify the RAVE code to use higher quality pitch shifting with the Rubberband Library [6], and we enable a variable probability for pitch shifting. When pitch shifting happens for a clip in a batch, we pick a factor uniformly at random in the interval [0.75, 1.25]."}, {"title": "Tagging Datasets", "content": "We perform a preliminary experiment tagging a dataset with two different echoes depending on timbre: we tag all but one of the males in VocalSet with a 50 echo and all but one of the females in the dataset with a 75 echo."}, {"title": "Discussion", "content": "Overall, we have shown that an incredibly simple technique can be used to watermark training data; our implementations of single echo hiding and time spread echo hiding are each two lines of code in numpy/scipy. One caveat is that, across all experiments, echoes are embedded more strongly in DDSP than in Rave, and in Rave than in Dance Diffusion, suggesting that complexity of the networks hampers the ability for the echoes to survive as strongly. Still, each model reproduces the echoes to some degree, suggesting the generality of the approach. This is surprising given how complex the models are and how they are unlikely to produce long sequences from the training data."}]}