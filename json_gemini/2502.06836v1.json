{"title": "CAST: CROSS ATTENTION BASED MULTIMODAL FUSION OF STRUCTURE AND TEXT FOR MATERIALS PROPERTY PREDICTION", "authors": ["Jaewan Lee", "Changyoung Park", "Hongjun Yang", "Sungbin Lim", "Sehui Han"], "abstract": "Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information.", "sections": [{"title": "1 Introduction", "content": "Designing AI-based predictive models has become an essential tool for accelerating the discovery and optimization of advanced materials. Traditional approaches rely heavily on domain expertise and heuristic methods, often supplemented by Density Functional Theory(DFT) calculations. While DFT is widely used for its accuracy in predicting material properties, its computational expense and time requirements limit its scalability for high-throughput material discovery, especially given the complexity and diversity of material systems. With the advent of machine learning(ML) techniques, particularly Graph Neural Networks(GNNs), researchers have been able to model material structures with unprecedented accuracy and speed by representing them as graphs and capturing intricate local interactions[1, 2, 3, 4, 5, 6]. Despite these advancements, the process of converting material structures into graph representations inherently leads to the loss of crucial information, such as crystal symmetries and the connectivity of repetitive structural units, which are critical to certain material properties. This issue is inherent when using GNNs. To address this limitation, multimodal learning can be leveraged to complement the lost information by incorporating additional modalities. While multimodal learning has been extensively studied in fields such as vision, language, and speech [7, 8, 9], its application in the materials,"}, {"title": "2 Results and discussion", "content": ""}, {"title": "2.1 Data", "content": "For training and evaluation, data was downloaded from the Materials Project[27] database and rigorously filtered to ensure quality and relevance. The filtering criteria were inspired by the data cleaning methods used in MatBench[28] and further enhanced with additional constraints to ensure practical applicability and facilitate efficient screening processes for real-world use. The applied filters are as follows:\nMatBench Filtering Criteria:\n\u2022 Remove entries with a formation energy or energy above the convex hull greater than 150 meV.\n\u2022 Exclude entries where \\(G_{voigt}\\), \\(G_{Reuss}\\), \\(G_{VRH}\\), \\(K_{voigt}\\), \\(K_{Reuss}\\), or \\(K_{VRH}\\) are less than or equal to zero.\n\u2022 Remove entries that do not satisfy the conditions \\(G_{Reuss} < G_{VRH} < G_{voigt}\\) or \\(K_{Reuss} < K_{VRH} < K_{Voigt}\\).\n\u2022 Exclude entries containing noble gases.\nAdditional Filtering:\n\u2022 Exclude entries with a formation energy greater than -10 eV.\n\u2022 Remove entries with shear modulus or bulk modulus values exceeding 1000 GPa.\nFor regression tasks, we used four properties(total energy, bandgap, shear modulus and bulk modululs). Each dataset comprised 114,398, 65,367, 9,423 and 9,423 instances, respectively. Detailed statistical analyses of the regression data for each target property can be found in the Table 2. For the pretraining stage, we leveraged the total energy data, which had the largest volume of samples."}, {"title": "2.2 Text generation", "content": "While the Materials Project directly provides structural information in cif formats, textual descriptions of material structures were generated using the Robocrystallographer API[15]. This allows for the automatic conversion of structural data into textual descriptions, which we processed using the default settings. For instances where text could not be generated, we adapted our approach by passing only the [CLS] token through the text encoder. The proportion of cases where text generation failed varies by property but remains below 0.2% for all properties. Therefore, it did not pose a significant concern. Statistical analyses, including the number of text tokens and the proportion of successfully generated texts, are detailed in the Table2."}, {"title": "2.3 Framework", "content": "CAST was trained in two stages, (1) MNP pretraining and (2) finetuning with MP data, as illustrated in Fig1. To embed each modality, we utilized a randomly initialized coGN[5] to generate node embeddings from the graph and MatSciBERT[29], pretrained on a large corpus of peer-reviewed materials science publications, to get text token embeddings."}, {"title": "2.3.1 Pretraining: Masked Node Prediction(MNP)", "content": "A subset of nodes in the graph is masked with a 50% probability, and the structure encoder processes the graph to generate embeddings for each node. Simultaneously, the corresponding text is passed through a text encoder to produce"}, {"title": "2.3.2 Finetuning", "content": "The pretrained model architecture is preserved, with the classification block replaced by a regression block to align with the predictive objective. By leveraging the structural and textual embeddings aligned during pretraining, the model is optimized to predict material properties with improved accuracy. This enhancement stems directly from the pretrained cross-modal understanding, which enables a more comprehensive integration of multimodal features."}, {"title": "2.4 Comparison of Predictive Performance", "content": "CAST was compared against unimodal models(coGN[5] and MatSciBERT[29]) and other multimodal methods, CrysMMNet(concatenation)[10] and MultiMat(pretrained with contrastive learning)[22]. In the original studies, CrysMMNet and MultiMat utilized different encoders for their respective architectures. However, to ensure a fair comparison of fusion methods, we standardized the encoders by using coGN as the structure encoder and MatSciBERT as the text encoder for training both models. In MultiMat, contrastive learning is originally conducted using four modalities, but in this study, we used only structure and text for training. While CrysMMNet originally froze the text encoder during training, we evaluated both the frozen approach and a fine-tuning approach using LoRA [30], a widely adopted technique for efficient model fine-tuning. Since the MatSciBERT was not pretrained for regression tasks, incorporating LoRA allowed us to explore and compare a broader range of multimodal scenarios within the concatenation framework. Our approach demonstrated the best performance in predicting three out of four properties, except for bulk modulus, where it achieved results only 0.001 lower than the best-performing method, CrysMMNet(LoRA fine-tuning), making it nearly equivalent in performance. Notably, the test MAE loss for total energy prediction was approximately 23% lower than that of the second best model, CrysMMNet(LoRA ft). Details on the implementation of each model are provided in the Method section. When comparing single-modality models, the text based MatSciBERT showed superior performance in predicting total energy, while coGN excelled in bandgap prediction. For modulus predictions, the performance of the two models was nearly identical. These results suggest that certain modalities are better suited for predicting specific properties, highlighting the importance for developing multimodal predictive models that leverage the strengths of multiple modalities.\nAmong multimodal approaches, the strong performance of CAST can be attributed to its ability to integrate information from both modalities in a fine-grained manner at the token level and to align the two modalities through pretraining. However, when comparing CAST (w/o pretraining) with other methods, it was observed that cross-attention alone was insufficient to achieve superior performance across all properties. In contrast, CAST with pretraining showed consistently improved predictive performance across all properties, outperforming both other methods and its w/o pretraining counterpart. Specifically, for bandgap prediction, most multimodal methods, including CAST (w/o pretraining), performed worse than the unimodal coGN. However, pretraining mitigated this issue, underscoring its critical role in multimodal learning. With CrysMMNet, fine-tuning with LoRA outperformed using a frozen language model for most properties, except for bandgap. This suggests that, since MatSciBERT was not optimized for property regression during pretraining, the addition of low-rank matrices through fine-tuning provided beneficial adjustments that improved performance. On the other hand, the MultiMat(pretrained with contrastive learning) approach negatively affected the parameters of the GNN during pretraining, resulting in degraded performance compared to the unimodal coGN model. In the case of the MultiMat paper, performance improved when different GNNs and text encoders were used. Further research is required to determine whether this improvement stems from changes in the dataset or the choice of encoders. The superior performance of the concatenation approach over contrastive learning suggests that leveraging both modalities directly together can have complementary effects. This is likely because, during fine-tuning, GNNs pretrained with contrastive learning do not directly utilize the text modality, limiting their ability to fully benefit from multimodal information."}, {"title": "2.5 Further analysis of pretraining effects", "content": "We conducted an analysis to investigate how pretraining contributed to the observed improvement in predictive performance. Using a test set example, we examined the attention maps generated by the cross-attention mechanism (Fig. 2). In the figure, the x-axis represents the text tokens acting as keys, while the y-axis corresponds to the query tokens. Each cell indicates the attention score, with brighter colors reflecting higher scores and stronger associations."}, {"title": "3 Conclusion", "content": "In this study, we proposed CAST(Cross Attention based multimodal fusion of Structure and Text), a multimodal learning framework for material property prediction that integrates structural and textual data at a fine-grained level. Through extensive experimentation, we demonstrated that our approach outperforms unimodal baselines(coGN and MatSciBERT) and other multimodal models(Concatenation and Contrastive learning) across multiple material property prediction tasks. Our results highlight the critical role of cross attention and pretraining in aligning structural and textual modalities. The Masked Node Prediction(MNP) pretraining strategy significantly improved the model's ability to align node tokens with their relevant text tokens, as evidenced by the diverse and meaningful attention patterns observed in the cross-attention maps. This alignment not only improved the accuracy of property predictions, particularly for challenging properties such as bandgap, but also contributed to the stability of the learning process, reducing performance variability across random seeds.\nDespite these achievements, there are several areas where the framework could be further improved to enhance its applicability and efficiency. First, the reliance on language models for text encoding and attention mechanisms for modality fusion leads to increased computational costs, particularly as the number of tokens grows. This poses"}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Statistical analysis of data", "content": "The table presents a comprehensive summary of the dataset statistics for various material properties, including total energy, bandgap, the logarithm of shear modulus, and the logarithm of bulk modulus, along with their respective train, valid, and test splits. For each property, key characteristics of the graph and text representations are detailed, such as the mean and standard deviation of the number of graph nodes and text tokens. The text existence rate indicates the proportion of samples with successfully generated textual descriptions, with absolute counts provided in parentheses. Furthermore, the table reports the mean and standard deviation of the target property values, offering insights into the distributions within the dataset. These statistics underscore the variation in structural and textual complexities across properties and highlight the near-complete availability of text descriptions, enabling the seamless integration of multimodal information for predictive modeling tasks."}, {"title": "4.2 Encoders for each modality", "content": ""}, {"title": "4.2.1 Structure Encoder: coGN", "content": "By leveraging the inherent symmetry of crystals, coGN [5] employs an asymmetric unit cell representation to reduce the number of graph nodes, thereby improving computational efficiency. The proposed Nested Line Graph Network (NLGN) architecture further enhances the GNN framework through optimized message passing. This approach has demonstrated superior performance across most tasks within the MatBench benchmark dataset [28]. Key factors contributing to these performance improvements include optimized connectivity and enriched message-passing capabilities. The methods introduced by coGN are broadly applicable for modeling crystal structures, providing a systematic framework for comparing and designing diverse GNN architectures. Given its state-of-the-art (SOTA) performance on MatBench and computational efficiency, coGN is considered to have robust encoding capabilities, making it an ideal choice as the GNN encoder for our work."}, {"title": "4.2.2 Text Encoder: MatSciBERT", "content": "MatSciBERT[29] is a domain-specific language model designed for materials science, trained on a large corpus of peer-reviewed materials science publications. It excels at information extraction tasks by effectively interpreting the unique notations and terminology prevalent in materials science literature. Evaluations on tasks related to materials such as abstract classification, named entity recognition, and relation extraction have demonstrated that MatSciBERT outperforms general scientific language models, such as SciBERT. The pretrained and fine-tuned models are publicly available, serving as a valuable resource for the materials science community. We hypothesize that leveraging a model with a deep understanding of materials science will significantly enhance predictive performance. Therefore, we adopted MatSciBERT as the text encoder in our framework."}, {"title": "4.3 Multimodal fusion methods", "content": "We compared regression performance using three approaches: concatenation, contrastive learning-based pretraining, and CAST(cross attention). Among these, CAST demonstrated the best prediction performance across four properties. While concatenation and contrastive learning utilize instance-level multimodal features, CAST leverages token-level features. To ensure a fair comparison, we standardized the training hyperparameters across all methods.\nFor training, we employed a periodic cosine scheduling strategy to adjust the learning rate. To enhance training stability, we included a warm-up phase for the initial 1,000 steps. For contrastive learning pretraining, we set the batch size to 360, following the configuration used in the MultiMat paper[22], as larger batch sizes are more efficient for this approach. For other pretraining task and regression tasks, a batch size of 64 was used.\nFor consistency, we employed coGN[5] and MatSciBERT[29] as the structure and text encoders across all methods. Accordingly, the feature dimensions of node tokens and text tokens were set to 128 and 768, respectively."}, {"title": "4.3.1 CAST(proposed method)", "content": "CAST, based on cross-attention, follows a two-step process: pretraining and finetuning. Unlike previous studies [10, 22, 11] in the materials domain that focused on multimodal fusion at the instance level, we hypothesize that token-level fusion can achieve superior performance. To enable interaction at the token level, we employ the cross-attention mechanism of transformer models [23].\nDuring the pretraining phase, a subset of graph nodes is randomly masked with a 50% probability. The structure encoder processes the graph to generate embeddings for each node, while the corresponding text is passed through a text encoder to produce token-level embeddings. These embeddings interact through a cross-attention mechanism, where the node embeddings serve as queries, and the text token embeddings act as keys and values. This interaction allows the node embeddings to incorporate contextual information from the text. As the model learns to predict the element types of the masked nodes, it simultaneously aligns node embeddings with the most relevant text tokens. The fusion module consists of four cross attention layers with eight attention heads, each with an attention dimension of 128. The classification block is a single linear layer.\nFollowing the pretraining phase, the model undergoes a finetuning stage specifically designed for property prediction tasks. In this phase, the pretrained model is preserved, but the classification block is replaced with a regression block, also a single linear layer, to align with the predictive objective. By leveraging the structural and textual embeddings aligned during pretraining, the model is optimized to predict material properties with heightened accuracy. This improvement stems from the pretrained cross-modal understanding, which facilitates a more comprehensive integration of multimodal features. As shown in Table 2.4, our method demonstrates more stable and accurate predictive performance compared to other multimodal approaches and unimodal models."}, {"title": "4.3.2 Concatenation(CrysMMNet)", "content": "CrysMMNet[10], to the best of our knowledge, is the first approach to leverage multimodal learning for material property prediction. Their model employs ALIGNN as the structure encoder and MatSciBERT as the text encoder. In this framework, the graph structure is processed through a graph encoder to generate graph embeddings, while textual descriptions are passed through a text encoder and a projection layer to produce text embeddings. These representations are then fused to jointly model the input modalities and predict crystal properties. Notably, the text encoder is frozen during training in their method, and the study does not evaluate the model's performance when the text encoder is not frozen. To address this gap, we conducted additional experiments, comparing the performance of the Concat method with a frozen language model (LM) and with LoRA fine-tuning. The results of these experiments are presented in Table 2.4."}, {"title": "4.3.3 Contrastive Learning(MultiMat)", "content": "In MultiMat, contrastive learning between multimodal representations is utilized to pretrain the structure encoder, similar to the approach used in CLIP[21]. Their study reported that applying the pretrained graph encoder to downstream tasks improves performance compared to training from scratch. MultiMat leverages four modalities\u2014crystal structures, density of states (DOS), and charge density, whereas our study focuses on comparing different modality fusion strategies. To ensure a fair comparison with other methods, we pretrained the model using only structural information and textual descriptions. Furthermore, while MultiMat utilizes PotNet [32] as the structure encoder and MatBERT [33] as the text encoder, we adapted the model by employing the coGN structure encoder and MatSciBERT text encoder. This alignment with the configurations of other methods ensures consistent evaluation across models."}]}