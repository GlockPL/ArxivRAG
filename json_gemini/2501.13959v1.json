{"title": "Assisting Mathematical Formalization with A Learning-based Premise Retriever", "authors": ["Yicheng Tao", "Haotian Liu", "Shanwen Wang", "Hongteng Xu"], "abstract": "Premise selection is a crucial yet challenging step in mathematical formalization, especially for users with limited experience. Due to the lack of available formalization projects, existing approaches that leverage language models often suffer from data scarcity. In this work, we introduce an innovative method for training a premise retriever to support the formalization of mathematics. Our approach employs a BERT model to embed proof states and premises into a shared latent space. The retrieval model is trained within a contrastive learning framework and incorporates a domain-specific tokenizer along with a fine-grained similarity computation method. Experimental results show that our model is highly competitive compared to existing baselines, achieving strong performance while requiring fewer computational resources. Performance is further enhanced through the integration of a re-ranking module. To streamline the formalization process, we will release a search engine that enables users to query Mathlib theorems directly using proof states, significantly improving accessibility and efficiency. Codes are available at https://github.com/ruc-ai4math/Premise-Retrieval.", "sections": [{"title": "1 Introduction", "content": "Formalized mathematics has recently attracted significant attention. It helps verify existing mathematical results, identify errors in the literature, and has the potential to accelerate the peer-review process for mathematical papers. This process involves proving natural language theorems within a strict formal logical framework. Interactive theorem provers (ITPs), or proof assistants, are commonly used for this purpose. In recent years, mathematicians have actively participated in developing large formalized theorem libraries using ITPs such as Lean [7], Coq [2], and Isabelle [25].\nHowever, the process of mathematics formalization often demands extensive experience with formalization and a high level of mathematical expertise. Therefore, there has been a sustained demand for automated auxiliary tools in the formalization field, such as premise retrievers. As illustrated in Figure 1(a), premise retrieval aims to retrieve useful premises based on the current proof state provided by the formalization platform. An effective formal theorem premise retrieval system can help mathematicians quickly identify relevant theorems, thereby accelerating the proof process. For example, LeanDojo [41] shows that a retrieval-augmented generator achieves a higher proof pass rate compared to a standalone generator without premise selection.\nThe early premise retrieval methods leverage internal APIs of ITPs to identify applicable theorems through pattern-matching algorithms. These methods, however, rely on strict matching criteria and may miss relevant theorems. Recently, learning-based methods [41, 24] have been proposed to encode formalized theorems in a latent space and retrieve them by computing their embedding similarities. The learning-based methods can be further categorized based on the query language: natural language queries or formal proof state queries. Natural language-based retrieval [9] requires users to have a sufficient mathematical understanding of the current proof state, posing a significant challenge for beginners. In contrast, formal language-based retrieval is more user-friendly, but it often relies on fine-tuning pre-trained natural language models [41] because of the scarcity of formal proof data. As a result, the fundamental differences between formal and natural languages introduce a semantic gap, leading to suboptimal performance.\nThe above challenges motivate us to design and learn a more effective premise retrieval system that directly leverages the formal proof state to retrieve premises, improving accuracy and usability for formalized mathematics. Specifically, our premise retrieval model consists of a context-free retrieval (CFR) module and a context-aware re-ranking (CAR) module, each of which takes BERT [8] as its backbone. The CFR module derives embeddings for premises and proof states (i.e., queries) and retrieves relevant premises based on their similarity to the input proof states. The CAR module is employed to reorder the retrieved premises, and accordingly, improves the top-k recall of the retrieval results. Unlike approaches that merely fine-tune pre-trained language models, we first pre-train a BERT model from scratch based on existing formalized corpora, in which a new tokenizer is learned for formal language. Based on the pre-trained BERT, we further learn the CFR and CAR modules separately via contrastive learning [5, 13]. Experiments show that the retrieval results of our model surpass state-of-the-art models in performance while still maintaining computational efficiency. Furthermore, to evaluate the practicality of our premise retrieval model, we integrate it with an independently trained tactic generator, creating a retrieval-augmented automated theorem prover. We then assess this system using the test dataset and MiniF2F benchmark [45], demonstrating the effectiveness of our retrieval model in facilitating the proof process.\nTo expedite the formalization process and provide convenience for provers, we are releasing our model, along with a web search engine available to all users. Additionally, we have developed a toolkit to facilitate the setup of local databases and the deployment of the model for those working on individual projects. Our contributions can be summarized as follows:\n\u2022 We train an effective retrieval model for formalized theorems, with a new tokenizer for formal language, which achieves notable results at low computational costs and capable of running on a personal computer.\n\u2022 We evaluate various retrieval models in a retrieval-augmented theorem proving pipeline, demonstrating the advantages of the proposed retrieval model in assisting proof generators.\n\u2022 We deploy a theorem search engine featuring a real-time updating database, available free of charge to provers, and provide a toolkit to facilitate local model deployment for users."}, {"title": "2 Preliminaries and Related Work", "content": "2.1 Preliminaries in Formalization and Lean\nInteractive theorem provers (ITPs) are software systems that provide a formal language for users to construct proofs verified by small, deterministic kernels. The process of encoding mathematical definitions, theorems, and proofs using this formal language is known as formalization. Modern ITPs, such as Isabelle [25], Coq [2], and Lean [7], are built on advances in type theory and have become sufficiently expressive to formalize complex, cutting-edge mathematics[3]. Among these, Lean stands out for its powerful automation tools and the extensive, community-driven mathematical library Mathlib, making it a preferred choice for mathematicians engaged in formalizing mathematical knowledge.\nAs a proof assistant, Lean provides a user interface for constructing formal proofs. The formalization process typically begins by translating a mathematical statement into Lean. Lean then analyzes the statement and presents the user with a proof state, which consists of a collection of hypotheses and the proposition to be proved. The user provides commands to modify the proof state until no goals remain. In this context, the set of hypotheses is referred to as the context, and the proposition is called the goal.\nWhile there are various styles of proofwriting, machine learning researchers commonly adopt the tactic-state transformation as an abstraction of the process. A tactic is a piece of program that modifies the current proof state. Experienced Lean users can develop custom tactics through meta-programming, utilizing Lean's native automation tools. Pre-defined tactics, such as simp, linarith, and so on, have significantly enhanced the efficiency of formalization.\nWe present a formal specification of the theorem-proving process in Lean. Starting with an initial proof state So = {\u03930, Go}, where \u0393\u03bf represents the initial context and Go the initial goal, the prover must provide a sequence of tactics {ti}n=1 to construct the proof. This sequence can be visualized as a linked chain of transformations, i.e.,\nSo \\xrightarrow{t_1} S_1 \\xrightarrow{t_2} S_2 \\xrightarrow{t_3} ... \\xrightarrow{t_n} S_n.\n(1)\nThe proof is complete when Gn = \"No Goals\". It is important to note the varied nature of tactics. As shown in Figure 1(b), some require additional premises to take effect, while others operate independently. Furthermore, the behavior of certain tactics may vary depending on the presence of premises."}, {"title": "2.2 Related Work", "content": "2.2.1 Learning-based Retrieval Models\nIn the field of information retrieval, traditional methods like BM25 [12, 30] and TF-IDF [31] rely on term information and document statistics to match queries with relevant documents. These methods are limited in performance because they do not consider the semantic similarities between queries and documents. However, with the advancement of deep learning, researchers have developed learning-based retrieval models, often referred to as dense retrieval models [1, 16, 28, 27, 19, 20]. These models typically fall into two categories.\nThe first approach [16, 28] encodes queries and documents into embeddings separately. Their similarity is then computed in the embedding space, and the top-ranked documents are retrieved as the final results. A key advantage of this method is that document embeddings can be precomputed, allowing the system to embed only the query during retrieval and improving computational efficiency significantly. However, this approach lacks direct interaction between queries and documents, which may limit its retrieval performance. The second approach [27, 26] concatenates the query and document as a single input to the encoder model, which then outputs a relevance score between them. This method can capture richer interactions between queries and documents, leading to improved performance. However, it incurs higher computational costs since the concatenation must be performed for each query-document pair. Recently, researchers have increasingly embraced the paradigm of fine-tuning pre-trained models for various retrieval tasks [21, 38, 18], leading to substantial improvements in performance.\n2.2.2 Mathematics Premise Selection\nPrevious mathematics theorem selection differs from natural language information retrieval, they often need to retrieve mathematical formulas in the library. Mathematical formulas are highly structured and are typically represented using tree-based structures [42, 10]. Retrieval methods [6, 44, 32, 17, 47, 33, 43] for such formulas rely on structural searches, which identify similarities by matching substructures across various features. There are also methods that utilize neural networks to retrieve mathematical information [22, 33, 46, 23, 29, 14].\nHowever, premise selection in Lean is different from the aforementioned scenario. Theorems in Lean are formalized as Lean code, which lacks the rich structural information inherent to mathematical formulas. As a result, Lean's internal API employs pattern matching to identify theorems applicable to the current proof state. This approach, however, enforces strict matching criteria, which often leads to failures in retrieving relevant theorems. In addition, applications like Moogle\u00b3 and leansearch [9] have been developed to enable searches based on natural language descriptions of the target query. However, they place a higher demand on the precision of natural language formulations for both the query and the formal theorems. In contrast, premise selection based on purely current formal proof state is a more straightforward approach. LeanDojo [41] employs a learning-based retrieval model that encodes the proof state and theorems independently, subsequently calculating their similarity to perform the retrieval task. However, its performance is less satisfactory, likely because they fine-tune the model pre-trained on natural language which fails to adapt well to the formal structure and syntax of Lean code.\n2.2.3 AI-Assisted Formalization\nTools based on deep learning, particularly large language models (LLMs), have been developed to expedite the formalization process. These language models [41, 34, 39] assist by automating the formalization of statements, suggesting relevant premises, and generating proofs. Notably, LeanCopilot [34] was designed to enable Lean users to leverage pre-trained AI models in their formalization workflows. The Lean community is actively investigating further applications of AI across various formalization projects."}, {"title": "3 Proposed Method", "content": "In this section, we present the proposed retrieval framework for formal premise selection. As shown in Figure 2, this framework includes three stages:\n\u2022 Extract data from the formal mathematics library to construct the theorem corpus.\n\u2022 The context-free retrieval stage involves encoding the premises and proof states using a learning-based proof state encoder and calculating semantic similarities.\n\u2022 In the context-aware re-ranking stage, the retrieved premises are concatenated with the query and evaluated using a cross-encoder for classification and re-ranking."}, {"title": "3.1 Data Preparation", "content": "3.1.1 Task Definition\nLet M = {pi}Npi=1 denote the library to be searched, where each pi represents a theorem within the library. Since theorems are functions that accept hypotheses as input and yield proofs of specified propositions, pi can be represented as {Fp\u2081, Gp\u2081}, where \u0393\u03c1\u2081 = [U1,..., Un]p\u2081 is the argument list of the theorem and Gi is the output of the theorem. This representation is isomorphic to a proof state. Given the current proof state s, the parameterized retrieval model \u03a6(s; \u039c, \u0398) assigns a score to each premise and returns the top-k results. Note that a proof state may contain several cases, denoted as s = {si}Li=1, where si = {\u0393, G}.\nFrom human-written proofs, we can extract tactic steps T = {tj}n\u22121j=1. Each tactic step contains the proof state before and after executing the tactic, as well as the premises used in the tactic, denoted as ti = (sbefore, {pij}ni=1, safter), where pij \u2208 M. We collect state-premise pairs from these tactics to form our training dataset. We consider the state before/after a tactic both relevant to the premises used in this step since the state before should satisfy the hypotheses of the premises, while the state after will contain patterns in the goal of the premises. Thus we can form our dataset as D = {(si, Pi)}Nsi=1, where si are distinct proof states and Pi is the set of relevant premises of si.\n3.1.2 Data Extraction\nSeveral tools have been developed to extract data from a Lean project. To meet our requirements, we utilize the script from LeanDojo and incorporate additional features. We applied this pipeline to Mathlib. The extracted information covers all the premises and proofs in this project, along with metadata such as file dependencies. For each premise, we extract its argument list and output. The proofs are lists of tactics, with the state before/after the tactics and premises used in each step. Figure 3 illustrates the statistics related to context length and the number of premises in the dataset. The context length represents the length of \u0393, while the premise number indicates how many premises are utilized in a specific tactic. It's important to note that tactics that do not have any premises are excluded from these statistics."}, {"title": "3.2 Model Architecture", "content": "Our premise retrieval model consists of two components: a context-free retrieval (CFR) module and a context-aware re-ranking (CAR) module. In detail, the CFR module retrieves a large set of relevant theorems for a given proof state. Subsequently, the CAR module refines the ranking of the top-k theorems retrieved by the CFR module, further improving the accuracy of the retrieval process.\n3.2.1 Context-Free Retrieval\nIn the context-free retrieval stage, we adopt a dense retrieval method [16]. This approach maps both the proof state and the theorem into a shared latent space, enabling retrieval through the computation of their cosine similarity. We design the model based on the BERT encoder architecture denoted as f(\u03b8). For a given input text, we process it through the model and apply average pooling over the last hidden states to derive its latent representation, denoted as f(\u00b7;\u03b8).\nWe preprocess the context and goal for each state and premise to eliminate the effect of the theorem name and achieve a unified format. We prepend a special token <VAR> to each element of the context \u0393 and <GOAL> to the goal G, then concatenate them to form a new string. Here is an example:\nThe retrieval model measures the relevance between a proof state s and a premise p by computing the similarity of their embeddings. The conventional similarity can be defined as\nsim(s, p) = \\frac{f (s; \\theta)}{\\|f(s; \\theta) \\|} \\frac{f(p; \\theta)}{\\| f (p; \\theta) \\|}.\n(2)\nHowever, considering that some states may only correlate with either the arguments or the goal, we encode the premise arguments and goal separately, which can be denoted as f(\u0393\u03c1,\u03b8) and f(Gp, \u03b8). Accordingly, we design a fine-grained similarity between the given state s and premise p, which can be formulated as\nsim(s, p) = \\frac{f(s; \\theta)}{\\text{state embedding}} \\cdot \\frac{1}{2} \\bigg(\\frac{f(\\Gamma_p; \\theta)}{\\|f(\\Gamma_p; \\theta) \\|} + \\frac{f(G_p; \\theta)}{\\|f(G_p; \\theta) \\|} \\bigg).\n(3)\nThis similarity is designed based on the principle that the context of the proof state should first meet the preconditions of the available premises and, afterward, determine whether the conclusions drawn from these premises fulfill the current requirements. Compared to the conventional similarity, this fine-grained design aims to explicitly measure the similarity from both perspectives, capturing premises that can be applied in the current context, along with premises that can be a solution to the current goal.\n3.2.2 Context-Aware Re-ranking\nWhile facilitating the retrieval model and vector store enables fast and efficient queries, the lack of interaction between the state and premise may lead to lower accuracy. To address the limitations of this context-free approach, we adopt a context-aware method to re-ranking the results from first stage. To be more specific, we design the re-ranking model based on BERT architecture that follows the sequence-pair classification of [26]. The state and premise will be concatenated together by [SEP] as the re-ranking module's input sequence, i.e.,\nInput = {[CLS], s, [SEP], \u0393p, Gp}.\n(4)\nThe module obtains the embeddings of the input. Passing the [CLS] embedding through an affine projection followed by a sigmoid layer, we obtain the relevant probability of the state and premise, i.e.,\nPr(s,p) = \u03c3(W\u00b7h[CLS] + b),\n(5)\nwhere h[CLS] is the embedding of the [CLS] token, W is the weight matrix, b is the bias term, and \u03c3 represents the sigmoid activation function. For top-k retrieval, we first use the retrieval model to find k\u2081 candidates. The re-ranking model serves as a filter and returns top-k2 as the final result.\nNote that, existing re-ranking methods [27, 26] improve retrieval performance by concatenating the state and each premise and passing them through their re-ranking models. This strategy has a high computational cost during inference due to the combining process of state and premise. As a trade-off, we apply a re-ranking model to refine the ranking of premises retrieved by the retrieval model."}, {"title": "3.3 Learning Algorithm", "content": "Based on the Masked Language Modeling (MLM) [36, 8] method, we pre-train both of the CFR and CAR modules on the formalized corpus we collected. Their tokenizers are the same and trained on formalized language corpus using WordPiece [35] algorithm. Then, we use the state-premises pairs to fine-tune the pre-trained modules separately by contrastive learning.\n3.3.1 Contrastive Learning of CFR Module\nFor (si, Pi) \u2208 D, we use pij \u2208 Pi along with negatives sampled from M to construct a set Pij, which contains one positive pij and |Pij| \u2212 1 negatives. The optimization objective of contrastive learning is formulated as\nmin \\sum_{i=1}^{D} \\sum_{j=1}^{P_i} -log \\frac{exp(sim(s_i, p_{ij}) / \\tau)}{\\sum_{p' \\in P_{ij} exp(sim(s_i, p') / \\tau)},\n(6)\nwhere \u03c4 refers to the temperature. We use the Homogeneous In-Batch Negative Sampling, which calls for many negative samples to guarantee the embedding's discriminativenss [15, 37, 28]. In our work, this is implemented by the usage of in-batch negatives for each query, the negatives are randomly sampled from the corpus, excluding the other positive premises of the query. Given a batch of B samples, it results in B \u00d7 |Pj| \u2212 1 negative samples.\n3.3.2 Contrastive Learning of CAR module\nThe CAR module is learned in the same contrastive learning framework, in which the choice of negatives is crucial. We sample hard negatives from the top-k\u2081 premises selected by the retrieval model and they will be used in the re-ranking model training process. During testing, we also use the retrieval model to retrieve the top-k2 premises and re-ranking them by the re-ranking model. The loss function used for training the re-ranking model is the cross-entropy loss, and the optimization objective is formulated as\nmin \\sum_{i=1}^{D} \\sum_{j=1}^{P_i} -log \\frac{Pr(s_i, p_{ij})}{\\sum_{p' \\in P_i} Pr(s_i, p')},\n(7)\nwhere Pij is made up by one positive pij and several hard negatives. Pr(s,p) is the relevant probability obtained via Eq. (5).\n3.3.3 Training a Tactic Generator for Evaluation\nAfter training the retrieval model, we integrate it with the generator for testing. Most prior work on generators [39, 34, 41] utilizes state-tactic pairs to fine-tune a pre-trained model for tactic generation. Leandojo [41] incorporates premise retrieval by leveraging the retrieval model to extract relevant premises based on the state from the training dataset. The premises are then concatenated with the state to form premise-augmented state-tactic pairs used for fine-tuning. However, this approach results in a high degree of coupling between the retrieval model and the generator, as prior knowledge of the retrieval result distribution is incorporated during training.\nTo ensure a fair experimental setup, we aim to decouple the generator from the retrieval model and train the generator independently. As a substitution of the retrieval model, we randomly select the positive and negative premises from the library and prepend them to the state, thereby creating premise-augmented state-tactic pairs. For each tactic, we add up to ten premises. The number of positive premises selected is randomly determined, ranging from one to the lesser of the total available positive premises or ten. The remaining premises are negative. Half of these negatives are selected from the same module as the positive samples, while the remaining are chosen from other modules. Following [41], we use these pairs to fine-tune the ByT5 [40] model for tactic generation. The loss function we use is typically cross-entropy loss."}, {"title": "4 Experiments", "content": "To demonstrate the effectiveness of our retrieval method, we provide conducted experiments to answer the following research questions (RQs).\n\u2022 RQ1: How does our proposed retrieval model compare to state-of-the-art retrieval models in formal premise retrieval?\n\u2022 RQ2: How do our model architecture and training strategy affect the model performance?\n\u2022 RQ3: What is the robustness of our method with respect to variations in data and model hyperparameters?\n\u2022 RQ4: What is the impact of our retrieval model on the performance of retrieval-augmented theorem provers."}, {"title": "4.1 Experiment Setup", "content": "4.1.1 Implementation Details\nOur retrieval and re-ranking models are based on the BERT architecture, which consists of 6 layers, each with 12 attention heads. It features a hidden size of 768 and an intermediate size of 3,072, complemented by a vocabulary size of 30,522. For the retrieval model, the maximum position embeddings are configured at 512. The maximum length for states is set at 512 and for premises' arguments and goals set at 256 respectively. The batch size is set at 32 and |P'j| mentioned in 3.3.1 is set at 2. In contrast, the re-ranking model has its maximum position embeddings set at 1,024. We set the batch size at 2, gradient accumulation steps at 8, and |Pij| at 8. We construct the pre-training corpus by concatenating states from the training set and all premises' argument lists and goals from the corpus. The results of our method presented in Table 3 are obtained by first training the retrieval model and selecting the top-100 results as hard negative candidates. We then train the re-ranking model and use it to reorder the top-20 results from the retrieval stage. The experiments are conducted on 8 RTX 4080 servers.\n4.1.2 Baselines\nAs mentioned above, ReProver [41] is a model specifically trained on the Lean dataset, using formal states and premises. Therefore, it serves as our primary baseline. Moreover, we retrain the model using our dataset and following the setting in [41]. In addition to ReProver, we compare against several other commonly used retrieval models that have demonstrated strong performance in their specific domains.\n\u2022 UniXcoder-base [11]: It is one of the state-of-the-art code embedding models to transform code snippets into hidden vectors. It leverages multimodal data, such as code comments and abstract syntax trees (ASTs), to pre-train code representations.\n\u2022 E5-large-v2 [37]: It is trained in a contrastive manner using weak supervision signals derived from a curated, large-scale text-pair dataset (referred to as CCPairs) and demonstrates strong performance on several English retrieval tasks.\n\u2022 BGE-m3 [4]: It is distinguished for its versatility and excels in multiple functionalities, multilingual capabilities, and fine granularity retrieval tasks.\nWe fine-tune the three aforementioned baselines using our dataset. Following the parameters provided in [4], we set the learning rate to 1e-5 and use a linear scheduler. Due to server constraints, the batch sizes for the three models are set to 16, 8, and 6, respectively.\n4.1.3 Metrics\nTo evaluate the performance of various retrieval models, we utilize four widely adopted metrics: Precision, Recall, F1-Score, and nDCG. Since the nDCG considers the retrieved results' relevance and position, it allows for multiple relevance levels. We define the relevance criteria as shown in Table 1, which provides the relevance scores required for calculating nDCG.\n4.1.4 Data Split\nTo evaluate the performance of the model across different feature datasets, we employ four distinct data split strategies. In particular, we represent a proof as P = {ti}|P|i=1, which is a sequence of tactics, and apply Proof Length and Premise Frequency to characterize the proof, i.e.,\nProofLength = |P|, PremiseFreq = \\frac{1}{|P|} \\sum_{i=1}^{|P|} PremiseNum(t_i).\nwhere PremiseNum(ti) denotes the number of external premise in a tactic. Proof Length reflects the complexity of the proof, while the Premise Frequency indicates the degree of reliance on external theorems during the proof process. Then, we apply the following four data split strategies to obtain four datasets:\n\u2022 Random (RD): Randomly split the dataset.\n\u2022 Reference Isolated (RI): Premises in the validation and test sets will not appear in the training set.\n\u2022 Proof Length (PL): Sample proofs for the validation and test sets weighted by Proof Length.\n\u2022 Premise Frequency (PF): Sample proofs for the validation and test sets weighted by Premise Frequency.\nFor each dataset, the training set contains 65,567 theorems, while the validation and test sets each consist of 2,000 theorems. The average proof length and premise frequency for each dataset are shown in Table 2."}, {"title": "4.2 Main Results (RQ1)", "content": "After training different models on the training dataset and evaluating them on the corresponding test sets, we obtain the results presented in Table 3. From this table, we can observe that our method demonstrates exceptional performance, significantly outperforming other baseline models. Specifically, UniXcoder-base [11], E5-large-v2 [15], and BGE-m3 [4] exhibit generally poor performance, which can be attributed to the substantial discrepancy between the pre-training corpora used by these models (based on natural language data) and the Lean language. Although these models are fine-tuned, they cannot fully capture the core aspects of the Lean language due to Lean's unique syntax and semantic features and thus suffer suboptimal performance. As a result, these models likely require more specialized fine-tuning strategies and parameters tailored to the Lean language to improve their performance.\nIn contrast, the ReProver [41] model performs relatively well, likely due to its specifically designed parameter-tuning strategy. Furthermore, ReProver utilizes ByT5 [40], a byte-level tokenizer, which may allow the model to better handle the special symbols and structures in the Lean language. Our approach, on the other hand, improves performance by retraining the tokenizer on the Lean corpus and pre-training the retrieval model, allowing it to better adapt to the syntax and semantic features of Lean, resulting in a significant boost in overall performance.\nFor different data splitting strategies, we observe that random data splitting is relatively easy. However, when the theorems in the test set involve longer proofs or require a larger number of premises, the performance of the model tends to be lower. The reference isolated method, where the premises in the test set have not appeared during training, is the most challenging. It requires the model to exhibit strong generalization ability. Nevertheless, under each of these splitting strategies, our model consistently outperforms other baseline methods, which indicates that our model exhibits superior adaptability."}, {"title": "4.3 Ablation Studies (RQ2)", "content": "4.3.1 Impacts of Pre-Training, Tokenizer, and Similarity Calculation\nWe evaluate the impacts of pre-training the model, training the specific tokenizer for Lean, and the proposed similarity calculation in Table 4, respectively. To ensure a fair comparison, we filter out the impact of the re-ranking module and only compare the results provided by the retrieval module. This table shows a significant performance improvement when the model is pre-trained on the Lean corpus compared to not performing pre-training. This indicates that the pre-training process helps the model acquire semantic information about Lean in advance. The experiment also shows that if we only pre-train the model without training a new tokenizer, the performance when k=1 is slightly better but degrades a lot when k=5 or 10. This is because Lean has a formal language system, which differs from natural languages, highlighting the necessity of training a dedicated tokenizer for Lean. In addition, we evaluate the impacts of different similarity calculation methods on the retrieval results. The results in Table 4 emphasize the benefits of assessing similarity in a fine-grained manner.\n4.3.2 The Effect of Re-ranking Model\nWe validate the effectiveness of using the proposed re-ranking module. As shown in Figure 4 (a), when only the CFR module is used, our model outperforms the ReProver only for larger values of k in Recall@k (e.g., k > 10). Applying the CAR module improves the recall of our model when k < 10, so that our model can consistently outperform ReProver and users are likely to find useful premises in a relatively short list. In other words, re-ranking contributes to a significant improvement in performance. Figure 4 (b) presents a comparison for our model and other baselines in terms of model size and inference GFLOPs. Without re-ranking, our model achieves the lowest GFLOPs and it outperforms in Recall@1 most of the baselines except ReProver. When re-ranking the top-5 retrieval results by our CAR module, our model outperforms all the baselines and its GFLOPs and model size are comparable to those of ReProver.\nIn principle, the CAR module helps improve the Recall of our model, but its complexity increases linearly with the number of results to re-rank. Therefore, the CAR module of our model is optional in practice, as there exists a trade-off between effectiveness and efficiency. The decision to re-ranking can be made based on factors such as sensitivity to the order of retrieval results or limitations on the information window. According to experimental results, when the number of retrieved results exceeds 50, the performance improvements obtained by re-ranking are limited, making it unnecessary. For user groups that are insensitive to the order of retrieval results, such as human theorem provers, it may be unnecessary to allocate additional computational resources for re-ranking. However, in the case of retrieval modules used in automated theorem provers, where high retrieval precision is required and computational cost is not a bottleneck, incorporating a re-ranking process can be justified. Note that, as a general retrieval technique, we can also apply the re-ranking procedure to other baselines. However, with larger backbone models (e.g., ReProver), they will need more computation, making it difficult to deploy for our search engine. How to further improve model performance and efficiency simultaneously is left as our future work."}, {"title": "4.4 Robustness Experiments (RQ3)", "content": "4.4.1 Effect of Data Perturbation\nConsidering the generally high quality of proofs in Mathlib, models trained on this dataset may be sensitive to the common issues of redundant variables, missing conditions, and disordered sequences in the context of proof states, resulting in poor generalization ability. We evaluate our retrieval model's robustness to low-quality inputs by perturbing the query states in the test set. Specifically, we applied two perturbation strategies to a subset of the test data: shuffling the context or randomly removing 20% of the context from states with a context length of 15 or more. As shown in Figure 5, all the metrics exhibit a moderate decline as the perturbation ratio increases, with a maximum drop of approximately 6% when k=5,10 compared to the unperturbed case. Especially, the model's performance even improves when k=1. These results demonstrate that our retrieval model is robust to variations in the order of query states and the potential absence of certain local hypotheses.\n4.4.2 Impact of Reduced Training Data\nThe sparsity of data is an inherent challenge in formalization tasks. We test the sensitivity of our retrieval model to the amount of training data. The tests are conducted on the Random split dataset. The results in Figures 5 show that the model trained on the full dataset outperforms the model trained on only 25% of the data by approximately 25% in terms of k=5,10. The consistent performance improvement suggests that there is still potential for further enhancement by increasing the amount of training data. When k=1, the performance seems to reach a bottleneck as the data size increases. Therefore, it may be necessary to incorporate re-ranking or explore alternative methods to further improve performance."}, {"title": "4.5 Theorem Proving Experiments (RQ4)", "content": "As mentioned in Section 3.3.3, we fine-tune ByT5 [40] and obtain a tactic generator. Facilitating the generator, we conduct theorem-proving experiments with retrieval. The results in Figure 6 show that assisted by our model, the generator averagely performs better than ReProver [41]. Additionally, we evaluate our method on the Minif2f benchmark [45] using the model trained on the random split dataset. Our approach achieves a pass@1 rate of 30.74%, while ReProver reached 28.28%, indicating that our method is more effective. This means that retrieval-augmented language models are effective: For tasks such as formal theorem proving, which relies on external theorem libraries, an effective retrieval model is expected to enhance generation performance.\nOn test sets with high premise frequency but relatively short proof, our model performs better than ReProver, demonstrating the advantages of a more powerful retriever. In contrast, on test sets with long proof but low premise frequency, our model performs almost on par with ReProver. However, on the RD test sets, our model performs slightly worse than ReProver, despite the improvement in retrieval accuracy. We attribute this outcome to several factors. First, incorporating retrieved premises into the prompt increases the length of the context that the model must process compared to providing only the proof state. This imposes certain requirements on the model's parameter size and architecture, as smaller models may struggle to accurately identify useful information within longer contexts. We will explore this issue in our future work. Additionally, since Lean users are continuously developing new automated tactics, solving simpler problems often depends more on selecting the appropriate tactic than on searching for relevant theorems, making tactic selection a more efficient approach in such cases."}, {"title": "5 Conclusion", "content": "In this paper, we propose an innovative method that leverages data extracted from Mathlib4 to train a retrieval model specifically designed for Lean premise retrieval. Experiments show that our method outperforms previous models and demonstrates the potential of domain"}]}