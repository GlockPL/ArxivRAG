{"title": "Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control", "authors": ["Lo Pang-Yun Ting", "Ali \u015eenol", "Huan-Yang Wang", "Hsu-Chao Lai", "Kun-Ta Chuang", "Huan Liu"], "abstract": "The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only effectively reduces electricity expenses but also enhances the resilience of handling practical issues, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. However, existing EV charging strategies have yet to fully consider these factors in a way that benefits both office buildings and EV users simultaneously. To address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and electric vehicles. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, a new critic augmentation is introduced to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information.", "sections": [{"title": "1 Introduction", "content": "Background. Electric Vehicle (EV) sales have seen significant increases in recent years, largely fueled by a worldwide emphasis on sustainability. According to a report by CBRE Group (Coldwell Banker Richard Ellis Group) Incorporated [4], workplace charging sessions increased at twice the rate of new charging station installations in 2023. This highlights the growing interest for EV charging options in the workplace.\nMeanwhile, recent advances in bidirectional charging systems [9, 20, 25], which support both Grid-to-Vehicle (G2V) and Vehicle-to-Grid (V2G) power flows, have shown the potential to enhance flexibility with regard to charging stability and emergency situations. The effectiveness of bidirectional charging systems has inspired us to explore how better scheduling of G2V and V2G timings can balance the demand for EV charging and reduce overall electricity costs. In particular, EVs often remain stationary for long periods at office locations, as expected. This prolonged parking time allows for more flexible charging strategies, enabling energy transfer between EVs and office building. An example of a workplace charging system is shown in Figure 1.\nUpon arrival, each EV is coordinated by the charging system, with users initially supplying details like their charging demands and planned departure time. Based on this, the system decides whether to discharge energy from the EV to the building (using V2G) during high-price periods or to charge the EV from the grid to meet its demand before departure (using G2V). Either the V2G or the G2V option can be determined on-the-fly according to the optimal decision-making criteria.\nChallenges. For real-time charging control of EVs in various scenarios, previous studies [6,21] have explored the use of multi-agent reinforcement learning (MARL) techniques to regulate EV charging and discharging actions. However, most existing approaches fail to consider real-world dynamic factors, such as dynamic energy prices and the possibility that EV users may depart earlier than the expected time, which complicate determining optimal control strategies for each EV. Moreover, to avert transformer overloads that could destabilize the power grid [26], it is necessary to impose charging power limits, thereby further restricting the management of EV charging. These dynamics and limitations make it challenging to balance the energy supply between the building and all EVs while minimizing electricity costs. It is important to note that improper management of charging can lead to notably increased electricity expenses, as power companies might levy extra charges due to overconsumption of energy [19].\nProposed Method. To tackle these challenges, we propose HUCA (Hierarchical Multi-Agent Control with Uncertainty-Aware Critic Augmentation), a novel framework designed for real-time charging control. HUCA operates under dynamic pricing and penalty mechanisms, consistent with practical situations. A new hierarchical multi-agent framework is devised to balance the energy supply between buildings and EVs in dynamic and uncertain environments. The HUCA framework consists of two levels of control: a high-level agent and multiple low-level agents. The high-level agent determines whether to charge or discharge EVs, which is a collective decision for all EVs. In light of the high-level decision, the low-level agents collaboratively and dynamically modulate the individual charging (or discharging) power level for each EV, ensuring they stay within the specified charging power limits, with the goal of satisfying anticipated demands and avoiding transformer overload. The action spaces of high-level and low-level agents are respectively discrete and continuous due to different goals (detailed later).\nA key innovation of HUCA lies in its capability to handle EVs that deviate from their expected departure times, whether leaving earlier or later. As these departures are unpredictable, rather than letting this variability spread throughout the low-level networks and interfere with future decisions, we propose an uncertainty-aware critic augmentation method. This mechanism is designed to (i) consider uncertainties when assessing the quality of charging decisions by low-level agents and (ii) limit the direct effect of uncertainties to the present time slot. Therefore, HUCA dynamically adapt to arbitrary EV behaviors while maintaining robust charging control. With this design, HUCA is able to minimize the electricity costs of the building while striving to fulfill EV charging demands in a dynamic environment.\nThe main contributions in this paper are as follows:\n\u2022 Real-Time Charging Control: We propose HUCA, a novel hierarchical multi-agent structure that balances energy supply between the building and EVs, which optimizes charging controls on-the-fly.\n\u2022 Uncertainties and Limitations Handling: In HUCA, the high-level and low-level agents determine optimal actions under dynamic pricing and power stability constraints. Additionally, an uncertainty-aware augmentation is designed to adaptively adjust decisions to handle uncertain EV departures.\n\u2022 Practical Effectiveness: We evaluate HUCA through analysis of real-world electricity datasets, incorporating simulated EV behaviors in scenarios with both certain and uncertain departure times. Results demonstrate that HUCA achieves the lowest electricity costs while maintaining competitive performance in fulfilling EV charging requirements."}, {"title": "2 Preliminaries", "content": "This section delineates the essential symbols and their definitions. We also introduce the general formulations of Markov decision process (MDP) and deep deterministic policy gradient (DDPG) related to reinforcement learning techniques."}, {"title": "2.1 Key Symbols and Definitions", "content": "Let $\\mathcal{C} = \\{C_1, C_2, ..., C_N\\}$ denote a set of charging piles (abbreviated as piles henceforth) located at the office building's charging station. $V_t = \\{V_1, V_2, ..., V_M \\}$ specifies a set of EVs docking at piles at time slot $t$, where $M < N$. The charging capacity of the charging station is denoted as $P_{max}$ (kW), which represents the maximum allowable charging power at the station ($P_{max} > 0$). In contrast, the maximum allowable discharging power is denoted as $-p_{max}$ (kW).\nDefinition 1. (State of Charge (SoC)): The state of charge (SoC) of an EV battery represents the percentage of energy stored in the battery. For each EV $v_i \\in V_t$, the SoC of $v_i$ at time slot $t$ is denoted as $SoC_i \\in [0\\%, 100\\%]$.\nDefinition 2. (EV Charging Information): In our scenario, when an EV arrives at the workplace charging system (Fig. 1) at time slot $t$ and connects to pile $c_i$, the EV user sends the charging information $I$ to the charging system. The charging information is denoted as a tuple: $T^i = (t_{arr}, t_{dep}, SoC_{arr}, SoC_{dep}, C_i)$, where $t_{arr}$ and $t_{dep}$ represent the arrival time and the planned departure time, respectively. $SoC_{arr}$ and $SoC_{dep}$ denote the SoC of the EV battery upon arrival and the expected SoC to be reached by the charging system before the departure time, respectively. $C_i$ is the battery capacity of the EV. Note that in our scenario, the actual departure time of the EV user may randomly occur earlier than the planned time $t_{dep}$.\nDefinition 3. (Charging Power Limitation): To ensure power system stability, the charging power of each charging pile is limited by the maximum charging capacity $P_{max}$ of the charging station. Given the number of EVs $|V_t|$ docking at the charging station at time slot $t$, the maximum charging power $P_{i,t}^{pile}$ of each pile at time slot $t$ is estimated as $P_{i,t}^{pile} = P_{max}/|V_t|$. The maximum discharging power of each pile is \u2013$P_{i,t}^{pile}$.\nTo consider the fulfillment of the expected SoC $SoC_{dep}^i$ of each EV $v_j$, we estimate the specific charging/discharging power boundary that pile $c_i$ provides to the connected EV $v_j$ at each time slot $t$, following the settings in [10,12]. Specifically, given the minimum and maximum SoC that EV $v_j$ can reach at time $t$, denoted by $SoC_{LB}^i$ and $SoC_{UB}^i$, respectively, the specific charging and discharging power boundaries of charging pile $c_i$ are as follows:\n$P_{i,t}^{max} = min(P_{i,t}^{pile}, \\frac{(SoC_{UB}^i - SoC_{t-1}^i) \\cdot C_i \\cdot \\eta^*}{\\Delta T})$\n$P_{i,t}^{min} = max(-P_{i,t}^{pile}, \\frac{(SoC_{LB}^i - SoC_{t-1}^i) \\cdot C_i \\cdot \\eta^*}{\\Delta T})$\nwhere $SoC_{t-1}^i$ is the SoC of $v_j$ at time slot $t \u2212 1$, and $C_i$ denotes the battery capacity for the EV. $\\eta^*$ is function of charging efficiency, as defined in [10,12]."}, {"title": "2.2 Markov Decision Process and Deep Deterministic Policy Gradient", "content": "Decision-making problems are commonly modeled as Markov decision processes (MDPs), which defined by the four-tuple information (S, A, R, T): states S, actions A, rewards R and the state transition probabilities T. The main objective of an MDP is to find a policy that maximizes the cumulative rewards by selecting appropriate actions. The Deep Deterministic Policy Gradient (DDPG) algorithm [14], which is an actor-critic policy-based method that extends stochastic policy gradients to deterministic settings, has shown great effectiveness in complex environments. Specifically, its deep actor network, parameterized by $\\theta^u$, approximates the deterministic policy $\\mu_\\theta: S \u2192 A$. The critic network, parameterized by $\\theta^Q$, estimates the action-value function $Q(s, a|\\theta^Q)$. Overall, the actor network determines the optimal action for a given state, while the critic network evaluates the action's quality."}, {"title": "2.3 Problem Formulation", "content": "It is generally recognized that the single-agent DDPG model is insufficient for optimizing multiple objectives, as our work aims to benefit both the office building and EVs. As such, we enhance the DDPG framework and model the situation as a hierarchical MDP to simultaneously optimize multiple objectives, aiming to find the most effective bidirectional charging strategy that reduces building overall electricity costs while considering EV charging needs.\nAt each time slot $t$, given the charging pile set $\\mathcal{C}$, EV set $V_t$, EV charging information $\\{T^i|v_i \\in V_t\\}$, and the charging and discharging power boundaries $\\{P_{i,t}^{max}, P_{i,t}^{min}|v_i \\in V_t\\}$, the objective is to determine the optimal charging (or discharging) power $P_{i,t}^{opt}$ for each EV $v_i$ at time slot $t$, where $P_{i,t}^{min} < P_{i,t}^{opt} < P_{i,t}^{max}$.\nBy determining $P_{i,t}^{opt}$, in the certain departure scenario, where the EV's actual departure time matches the planned one, the goal is to minimize the total electricity cost of the building while ensuring that EV users' expected SoCs are met at their departure. In scenarios where EVs depart earlier than expected\u00b9, our objective is to achieve their desired SoCs to the greatest extent while minimizing costs, balancing the trade-off between cost reduction and maximizing EV charging fulfillment."}, {"title": "3 The HUCA Framework", "content": "The architecture of HUCA is illustrated in Figure 2. Initially, a high-level agent determines whether to charge or discharge EVs using real-time data (Sec. 3.1). Subsequently, multiple low-level agents manage the charging and discharging power for each individual pile, taking into account the unpredictability of EV departures (Sec. 3.2). Finally, HUCA can determine the optimal decision instantaneously (Sec. 3.3)."}, {"title": "3.1 High-Level Charging Decision Making", "content": "This section formulates a novel MDP for a high-level agent to decide whether to charge or discharge EVs."}, {"title": "3.1.1 State representations", "content": "A state $s_h$, denoted by $s_h = (I^f_t, I^v_t, I^{l-1}_t)$, describes the status at the time slot $t$. The triplet includes: (i.) the electricity usage of the building and the electricity price, $I^f_t$; (ii.) EVs situations, $I^v_t$; and (iii.) the information learned by low-level agents at the previous time slot, $I^{l-1}_t$. Details are given in Appendix A.2."}, {"title": "3.1.2 Discrete actions for the high-level agent", "content": "Let $a_h \\in [0,1]$ denote the action selected by the high-level agent. It is then converted into the discrete high-level action $a^{disc}_h$ to determine whether to charge or discharge EVs, as defined below:\n$a^{disc}_h = \\begin{cases} 1 & \\text{if } a_h \\geq 0.5 \\\\ 0, & \\text{if } a_h < 0.5 \\end{cases}$"}, {"title": "3.1.3 High-level objective and reward function", "content": "The total electricity cost of a building is calculated based on two parts: (i) the total electricity consumption and (ii) the amount of electricity exceeding the contracted capacity [7], which is the upper bound of instant electricity consumption. The objective of the high-level agent is to minimize the total electricity cost.\nLet $L_t$ represent the electricity load, $\\Delta L_t$ denote the excess electricity consumption of the building, $p_t$ be the electricity price, and $t$ specify the time slot. The reward function $r_h(s_h, a^{disc}_h)$, briefed as $r_h$, is defined as:\n$r_h = \\kappa \\cdot \\left(-\\sum_{t'=1}^t p_{t'} \\cdot L_{t'} - \\varphi \\cdot \\sum_{t'=1}^t max(0, \\Delta L_{t'})\\right) + (-|L_t - L_{avg}|)$,\nwhere the first term represents the potential total electricity cost up to time $t$, weighted by an importance factor $\\kappa\\in [0,1]$. $\\varphi\\in \\mathbb{R}$ is a fixed penalty coefficient weighting the exceeding instant electricity usage. The second term balances the electricity consumption compared to the previous average load $L_{avg}$. This design reduces the reward for costly or imbalanced charging, encouraging the high-level agent to avoid similar actions in future comparable states.\nTo evaluate charging or discharging actions, we apply DDPG concepts to update the high-level policy $\\mu_h$. The high-level agent trains the critic network, with parameters $\\theta_h^Q$, to approximate action-value function $Q_h(s_h, a_h)$ by minimizing the following loss:\n$\\mathcal{L}(\\theta_h^Q) = \\mathbb{E}_{s_h, a_h, r_h, s'_h \\sim \\mathcal{D}_h} \\left[(Q_h(s_h, a_h|\\theta_h^Q) - y_h)^2\\right]$,\nwhere $\\gamma$ is a discount factor. $Q_h'$ is the target Q-function used to stabilize learning. The replay buffer $\\mathcal{D}_h$ stores the transition experiences of the high-level agent in the form of tuples $(s_h, a_h, r_h, s'_h)$. Subsequently, the actor network of updates the high-level policy $\\mu_h$ via gradient descent as follows:\n$\\nabla_{\\theta^{\\mu}} J(\\mu_h) = \\mathbb{E}_{s \\sim D} [\\nabla_{a_h} Q_h(s_h, a_h) |_{a_h = \\mu_h(s_h)} \\nabla_{\\theta^{\\mu}} \\mu_h(s_h)]$"}, {"title": "3.2 Low-Level Control with Uncertainty-Aware Critic Augmentation", "content": "Based on the charging or discharging decision made by the high-level action, the goal of the low-level control is to determine the optimal charging (or discharging) power level for each pile, taking into account the uncertainty of EV departures. Each pile $c_i \\in C$ is regarded as a low-level agent and its task is formulated as an MDP. A multi-agent structure including multiple piles is deployed for low-level control."}, {"title": "3.2.1 State representations", "content": "For a low-level agent $c_i \\in C$, the state $s_i$ observed at time slot $t$ comprises two information $s_i = (I^v_t, I^h_t)$: (i) the real-time charging information of the connected EV $v_i \\in V_i$, denoted as $I^v_t$; and (ii) the information learned by the high-level agents, denoted as $I^h_t$. For simplicity of the presentation, we provide additional details in Appendix A.2."}, {"title": "3.2.2 Low-level objective and the critic augmentation", "content": "Multi-agent DDPG (MADDPG) [15] extends DDPG into a multi-agent policy gradient algorithm, where decentralized agents learn a centralized critic based on the observations and actions of all agents. Inspired by MADDPG, we formulate the objective of each critic network, $\\theta^Q_i$, of a low-level agent $c_i \\in C by minimizing the following loss:\n$\\mathcal{L}(\\theta_i^Q) = \\mathbb{E}_{x, a, r, x' \\sim \\mathcal{U}_i \\in C \\{D_i\\}} \\left[y_i - Q_i(x, a_1, ..., a_N|\\theta_i^Q)\\right]^2$\n$y_i = r_i + \\gamma Q_i(x', a'_1, ..., a'_N)|_{a'_i = \\mu_i(s_i)}$\nwhere $x = \\{s^1_t, s^2_t, ..., s^N_t\\}$ collects the states of all low-level agents. $D_i$ is the replay buffer of agent $c_i$, storing the transition experience with tuples $(s_i, a_i, r_i, s'_i)$. $Q_i$ is the action-value function from the critic network of agent $c_i$, and $Q'_i$ is the target Q-function.\nHowever, early departures of EVs occur occasionally, making the departure time uncertain and complicating the selection of the best low-level actions. To tackle this challenge, inspired by the Upper Confidence Bound (UCB) algorithm [1], which adjusts action-value estimations based on uncertainty (or confidence) and thereby affects the action decision, we propose a novel augmented action-value function, $\\tilde{Q}_i$, for the actor network of each low-level agent $c_i$. The idea is to discourage higher power level discharging actions when the given expected departure time is approaching and the expected SoC is still unmet. This augmentation incorporates uncertainty into the actor network for decision-making, while isolating direct impact of uncertainties from the critic network training, thereby maintaining the robustness of the critic network. Accordingly, $\\tilde{Q}_i$ is designed as follows:\n$\\tilde{Q_i}(x, a) = Q_i(x, a_1, ...). (1 - \\rho \\cdot |log_2(\\overline{a_i} + \\epsilon)| \\cdot \\frac{max(\\Delta SoC, 0)}{\\Delta T})$\nwhere $\\epsilon\\in [0,1]$ is the action chosen by low-level agent $c_i$. A smaller $\\overline{a_i}$ indicates a tendency for higher discharge power, as defined in Sec. 3.2. $\\Delta SoC$ represents the difference between the expected $SoC$ $SoC_{dep}^i$ and the current SoC $SoC_i$ of the EV connected to pile $c_i$. $\\Delta T$ is the time difference between the current time $t$ and the planned departure time $t_{dep}$. $\\rho\\in \\mathbb{R}$ is a fixed coefficient representing the impact of uncertainty, and $\\epsilon$ is a small constant. Consequently, high power discharging action is penalized when the uncertainty factor increases.\nBased on Eq. 3.6, the actor network $\\theta^{\\mu}_i$ of of low-level agent $c_i$ updates the low-level policy $\\mu_i$ with gradient descent as follows:\n$\\nabla_{\\theta^\\mu} J(\\mu_i) = \\mathbb{E}_{x, a \\sim U_i \\in C \\{D\\}} [\\nabla_{a_i} \\tilde{Q_i}(x, a) |_{a_i = \\mu_i(s_i)} \\nabla_{\\theta^{\\mu}} \\mu_i(s_i)]$"}, {"title": "3.2.3 Continuous actions and the reward function for low-level agents", "content": "Let $a_i \\in [0, 1]$ be the action selected by a low-level agent $c_i$. To entangle low-level action space with the high-level discrete decision $a^{disc}_h$, the continuous action $\\overline{a}$ is formulated as follows:\n$\\overline{a} = \\begin{cases} \\delta + \\delta \\cdot \\sigma(a_i) & \\text{ if } a^{disc}_h = 1\\\\ \\delta \\cdot \\sigma(a_i), & \\text{ if } a^{disc}_h = 0 \\end{cases}$ where $\\sigma(\\cdot)$ is the sigmoid function, and $\\delta$ is set to 0.5. In this setting, the action space of $a_i$ is constrained to $[0,0.5)$ while the discrete high-level action is \u201cdischarging\u201d ($a^{disc}_h = 0$). In contrast, $a_i$ is limited to $[0.5,1]$ when the high-level action is \u201ccharging\u201d ($a^{disc}_h = 1$).\nSubsequently, the optimal charging/discharging power $P_{i,t}^{opt}$ provided by low-level agent (pile) $c_i \\in C$ to the connected EV at time slott is estimated based on the continuous low-level action $a_i$, as defined below:\n$P_{i,t}^{opt} = a_i (P_{i,t}^{max} - P_{i,t}^{min}) + P_{i,t}^{min}$.\nBased on the selected action $a_i$ and the optimal power decision $P_{i,t}^{opt}$ at each time slot, the reward function $r_i(s_i, a_i)$ (abbreviated as $r_i$) for the low-level agent (pile) $c_i$ is formulated based on the charging cost at the current time slot $t$ and the difference between the current SoC and the expected SoC at the EV's departure connected to $c_i$, defined as follows:\nr_i = \\omega \\cdot (p_t) + (-|SoC_t^i - SoC_{dep}^i|)\nwhere $p_t$ and $SoC_t^i$ are the energy price and the SoC of the EV at time slot $t$, respectively. $SoC_{dep}^i$ is the expected SoC at the EV's departure. The parameter $\\omega$ reflects the impact of the current charging cost."}, {"title": "3.3 Optimization for the Hierarchical Control", "content": "Based on Sec. 3.1 and Sec. 3.2, the critic and actor networks are optimized with minibatches of samples from the replay buffers, which follows the soft update rule. Let $\\theta_h = \\{\\theta_h^Q, \\theta_h^{\\mu}\\}$ and $\\theta_i = \\{\\theta_{Q,i}^{\\mu}, \\theta_i^{\\mu}\\}$ represent the parameters of target critic and actor networks of the high-level agent and the low-level agent ($c_i$), respectively. The parameters updates are as follows:\n$\\theta_h \\leftarrow \\tau\\theta_h + (1 - \\tau)\\theta_h, \\theta_i \\leftarrow \\tau\\theta_i + (1 - \\tau)\\theta_i$,\nwhere $\\tau\\in [0, 1]$ is the coefficient controlling the update rate. Through these optimizations, the decision-making processes of both high-level and low-level agents are refined without requiring future information. The details of optimization formulation are given in Appendix A.3."}, {"title": "4 Experimental Results", "content": "4.1 Dataset and Experimental Setup."}, {"title": "4.1.1 Dataset Description and Preprocessing", "content": "For our workplace charging system scenario, we use the following datasets: (i) Building electricity demands: the CU-BEMS dataset [18] provides office building electricity consumption. (ii) Pricing mechanism: the ComEd APIs [5] supply real-time pricing data. Also, the penalty mechanism is designed based on [19]. The data is aggregated into hourly intervals. The training period is from July 1, 2018, to August 31, 2018, with testing in the following month. Details are in Appendix A.1."}, {"title": "4.1.2 Simulation of EV Behaviors", "content": "The charging information $T^i = (t_{arr}, t_{dep}, SoC_{arr}, SoC_{dep}, C_i)$ (Def. 2) of the EV is modeled using truncated normal distributions by following [11]. Table 1 shows the setting of random variables and $C_i$ is set to 60 kWh. Two departure scenarios are examined:\n1. Certain departure scenario: The actual departure time $t_{dep}$ is the same as the expected time $t_{dep}$\n2. Uncertain departure scenario: The actual departure time is randomly sampled earlier than the expected time, $t_{dep} \\in [1, t_{dep})$."}, {"title": "4.1.3 Baselines", "content": "We compare our HUCA with following baselines. OPT: An oracle with full knowledge of future information serves the optimal solution of minimal total cost and minimal penalty cost. We include the single-agent (DDPG [10]) and multi-agent reinforcement learning models (IQL [24], VDN [23], and MADDPG [22]). These details are provided in Appendix A.1.4."}, {"title": "4.1.4 Evaluation Metrics", "content": "\u2022 Penalty Cost (USD) measures the cost of exceeding the contracted capacity [7], which multiplies the penalty rate with the peak amount of electricity (check Appendix A.1 for details).\n\u2022 Total Cost (USD) is the sum of the basic electricity cost (electricity load $L_t$ multiplied by dynamic pricing $p_t$) and the penalty cost over the testing data.\nIn the certain departure scenario, all EV charging demands are met since all baselines are subject to the charging power limitations (Sec. 2.1, Def. 3) and no unexpected EV departures occur. In the uncertain departure scenario, how well the charging demands are fulfilled when unexpected EV departures happen needs examinations. Given $SoC^i_{dep}$ and $SoC^i_{adep}$ denoting the actual and expected SoC of EV $v_i$ at the time of departure, and $SoC^i_{arr}$ is the SoC of $v_i$ upon arrival at the parking lot, the metrics are defined as follows:\n\u2022 SoC Fulfillment (%) evaluates how well the expected SoC of EV $v_i$ is fulfilled, defined as: $SoC^i_{dep} / SoC^i_{adep}$\n\u2022 SoC Maintenance (%) checks the fulfillment status at the midpoint ($SoC_{mid}^i$) between the arrival and the actual departure time, verifying whether the SoC is consistently maintained under the charging control. A drop below the arrival SoC would result in extra expenses billed to users. The metric is defined as: $(SoC_{mid}^i - SoC^i_{arr}) / (SoC^i_{dep} - SoC^i_{arr})$, where $SoC_{mid}^i$ denotes the SoC at the midpoint.\n\u2022 User Satisfaction (%) represents the average of SoC fulfillment and SoC maintenance, indicating EV users' satisfaction with the charging performance."}, {"title": "4.2 Comparison Results", "content": "As shown in Table 2, except for the \"OPT\" method (best-case result), HUCA achieves the lowest penalty and total costs among methods that do not rely on future information for charging control across both departure scenarios. In addition, its total cost is comparable to OPT, manifesting its effectiveness in minimizing costs. Although the SoC fulfillment, maintenance, and user satisfaction scores of HUCA are not the best, it still achieves a comparable results to all the other baselines. Note that combining the above observations, HUCA actually strikes the best balance between the trade-off of electricity costs and fulfilling user demands by introducing the hierarchical control and the uncertainty-aware critic augmentation.\nAmong the multi-agent models, VDN and MADDPG come closest to HUCA in terms of cost. However, the SoC fulfillment scores of MADDPG are much lower than those of HUCA across different numbers of charging piles. While VDN achieves higher SoC fulfillment and maintenance scores than HUCA with 20 charging piles, its scores are significantly lower with 10 charging piles. The maintenance score of VDN even drops to a negative value (-120.61%), indicating that under MADDPG and VDN control, EVs' SoCs are not maintained at arrival levels. Therefore, EVs primarily serve as power providers rather than being effectively charged, resulting in additional expenses for users and putting them at a disadvantage."}, {"title": "4.3 Ablation Study and Analysis of the Uncertainty Impact", "content": "To validate each component of HUCA, we conduct an ablation study by removing the uncertainty-aware critic augmentation (w/o C.A.), the high-level agent (w/o H.), and both of them (w/o All). The results are shown in Table 3. Compared to HUCA, removing the critic augmentation remains similar penalty and total costs, but decreases the SoC fulfillment score significantly. On the other hand, removing the high-level agent increases both penalty and total costs. These findings reveal that the hierarchical structure and critic augmentation of HUCA can help to achieve the lowest costs and effectively balance the energy supply between the office building and EVs.\nFurthermore, to assess the impact of the uncertainty term in HUCA for handling uncertain departures, Figure 3 compares the SoC maintenance and fulfillment scores under various uncertainty coefficient $\\rho$, defined in Eq. 3.6. Although the SoC maintenance score does not change significantly, the fulfillment score improves more noticeably when $\\rho$ exceeds 10-1. The outcome indicates that increasing the importance of the uncertainty component improves the charging control's ability to fulfill EV users' expected State of Charge (SoC) requirements."}, {"title": "4.4 Case Study", "content": "To examine how HUCA balances energy supply between the building and EVs, Figure 4 visualizes the control decisions over a day. Two significant EV charging behaviors appear at 7:00 and 16:00 (red rectangle). At 7:00, both electricity demand and prices for the building are minimal (Figure 4(a)), therefore HUCA focuses on charging the EVs to reach their target SoC.At 16:00, even though there is a high demand for building energy and elevated prices, which typically would lead to transferring energy from the EVs to the building to reduce costs, HUCA opts to charge the EVs because of their high charging urgency (Figure 4(b)) using Eq. (3.6). In contrast, at 8:00 (green rectangle), HUCA discharges EVs to serve the high-usage building and avoid high pricing time simultaneously. These results manifest that HUCA effectively and dynamically balances energy supply between the building and EVs based on real-time information."}, {"title": "5 Related Work", "content": "5.1 Real-time Charging Control. Guo et al. [8] combined day-ahead optimization with real-time control to address uncertainties in solar output and EV parking behaviors. Chandra Mouli et al. [17] introduced an energy management system with vehicle-to-grid (V2G) services to adjust charging control. Chopra et al. [3] conducted a cost-benefit analysis for EV charging infrastructure in parking lots. Zhao et al. [31] employed risk-based day-ahead scheduling for EV aggregators using information gap decision theory to minimize building costs. Zhang et al. [30] developed an EV load optimization model to reduce grid-level energy costs by considering peak and valley electricity prices. Chen et al. [2] devised strategies for battery energy storage systems to participate in peak load shifting, minimizing load fluctuations.\n5.2 Reinforcement Learning for Charging Management. Single-agent RL, multi-agent RL (MARL), and hierarchical MARL approaches were used to manage charging systems. Single-agent RL represented the control system [13, 16] under various considerations, such as ensuring EV battery requirements [28] and addressing charger shortage [10]. MARL modeled fine-grained cooperations [27] among EVs and buildings to tackle safety concerns [29] and power transfer overload [6]. Hierarchical MARL further separated system operators and EV users, and coordinated them to minimize demand charges and energy costs [21].\nAlthough literature in Sec. 5.1 and 5.2 optimized charging control with various aspects, most studies do not address the issue of uncertain EV departures. While some works [8, 16, 29] considered uncertainties in EV behavior, they primarily focused on a single objective, such as maximizing system revenue or minimizing user charging costs. In contrast, our work tackles dynamic factors, including fluctuating electricity prices and uncertain EV departures, aiming to simultaneously minimize total electricity costs for the charging system while accounting for EV users' charging demands."}, {"title": "6 Conclusion", "content": "In this paper, we propose HUCA, a novel hierarchical multi-agent framework designed for"}]}