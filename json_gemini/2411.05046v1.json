{"title": "PhoneLM: an Efficient and Capable Small Language Model Family through Principled Pre-training", "authors": ["Rongjie Yi", "Xiang Li", "Weikai Xie", "Zhenyan Lu", "Chenghua Wang", "Ao Zhou", "Shangguang Wang", "Xiwen Zhang", "Mengwei Xu"], "abstract": "The interest in developing small language models (SLM) for on-device deployment is fast growing. However, the existing SLM design hardly considers the device hardware characteristics. Instead, this work presents a simple yet effective principle for SLM design: architecture searching for (near-)optimal runtime efficiency before pre-training. Guided by this principle, we develop PhoneLM SLM family (currently with 0.5B and 1.5B versions), that acheive the state-of-the-art capability-efficiency tradeoff among those with similar parameter size. We fully open-source the code, weights, and training datasets of PhoneLM for reproducibility and transparency, including both base and instructed versions. We also release a finetuned version of PhoneLM capable of accurate Android Intent invocation, and an end-to-end Android demo. All materials are available at https://github.com/ UbiquitousLearning/PhoneLM.", "sections": [{"title": "1. Introduction", "content": "In last few years, the striking progress has been made in large language models, attributed to the scaling-up ability of transformer. One the other hand, we also notice growing interests in small language models (SLMs), which typically encompass sub- or a few billions of parameters and facilitate on-device deployments (Lu et al., 2024; Yuan et al., 2024). In practice, SLMs have been shipped to commercial off-the-shelf devices on a vast scale. For instance, the latest Google/Samsung phones have built-in LLM service (Gemini Nano), through which third-party mobile apps can freely enjoy LLM capability through text prompts or LoRA (Hu et al., 2021). Apple also introduces SLMs to facilitate privacy-preserving on-device intelligence tasks such as refining text and prioritizing notifications in iOS (Inc., 2024a).\nOn-device SLM deployment is extremely challenging due to the resource scarce of edge devices (Xu et al., 2024b). While there has been plenty of open-sourced SLMs, e.g., Microsoft Phi family (Microsoft, 2024.04), that are claimed to be designed for resource-constrained devices, we found rare evidences supporting it except its relatively small parameter size. Motivated by the absence of a high-level principle for SLM design, we ask a question: beyond using a small parameter size, what else can model developers do to better support on-device deployment with limited resources?\nIn this work, we propose an intuitive yet effective principle for constructing on-device small language models: searching for an resource-efficient architecture on a given hardware before pretraining. It fundamentally differs from traditional SLM pipeline in that it moves the consideration"}, {"title": "2. A Principle for SLM Development", "content": "SLM shall adapt to the target device hardware.\nA key argument of this work is that, unlike on clouds, the SLM architecture and development shall adapt to the specific hardware for runtime efficiency as the first-class concern. Throughout this work, the \"SLM architecture\" mainly refers to the hyperparameters of transformer-decoder models, including the types of attention (MHA, GQA, etc.), activation function of feed forward network (FFN), depth and width of the model, etc.\nMotivating experiments. To support the principle proposed, we test a bunch of SLMs with 100M and 200M parameters using various configurations on 2B tokens (dataset is the same as used to train PhoneLM). We then compare their loss on the same validation dataset. At the same time, we tested the inference speeds of these models using the inference engine mllm (Yi et al., 2023) on a smartphone equipped with the Snapdragon 8Gen3 SoC. The results of average metric (introduced in Section 4.2) and inference speed (throughput) are shown in figure 3. More details of these model architectures are shown in appendix A. We fit a quadratic curve to the loss of the 100M and 200M models when training on the same 2B tokens of data. Overall, fewer transformer layers, a larger model hidden size, and more attention heads tend to have faster inference speeds.\nA key observation is that runtime speed is more sensitive to the SLM architecture than the loss. For a given model size, the range of its runtime speed is much wider than that of the loss. Comparing the SLMs with different sizes (100M and 200M), there is significant overlap of inference speed, but hardly any overlap of loss. In other words, a model with 200M parameters is consistently more capable than the one with 100M parameters, but does not always run slower on"}, {"title": "3. PhoneLM: Smartphone-native SLM Family", "content": "Following the proposed principle, we developed and trained PhoneLM, a smartphone-native SLM family. It has the following notable features: (1) Good runtime performance and capability. (2) Convenient for smartphone deployment and more suitable for model inference using NPU.\nIn this section, we present the architecture and training details of PhoneLM."}, {"title": "3.1. Architecture", "content": "PhoneLM uses the vanilla transformer decoder architecture. The details of the PhoneLM parameters (0.5B and 1.5B versions) are shown in Table 1. PhoneLM uses a context length of 2,048 and reuses the SmolLM's tokenizer (HuggingFace, 2024.07) with a vocabulary size of 49,152. It uses ROPE embedding and multi-head attention mechanism;\nwithin its feed-forward component, it uses Gated FFN (as in Llama (Touvron et al., 2023) and Qwen (Bai et al., 2023)), RMSNorm, and ReLU activation.\nHardware-specific, ahead-of-pretraining hyperparameter search for runtime resource efficiency. According to the observation in section 2, we selected the hyperparameters of PhoneLM by exhaustively searching on smartphone hardware. We utilized the observations in section 2 to set up many model structures as follows: (1) The number of model layers ranges from 15 to 25. (2) Use MHA with 16 heads and GQA with 4 groups. (3) The activation function is ReLU. (4) The ratio of intermediate size to hidden size is between 2 and 5. We conducted inference speed tests for different architectures of PhoneLM-1.5B on Xiaomi 14. The structural designs of these models are presented in the table 2. We selected the model with the fastest inference speed as the final structure of PhoneLM, as shown in the"}, {"title": "Pre-quantized positional embedding", "content": "We use ROPE (Rotary Positional Embedding) to inject positional information into our model. In order to accelerate the quantization calculation on smartphones, we quantize the values of sin and cos of ROPE to INT8 because it is a fixed cosine function without outliers, this quantization introduces minimal loss in accuracy. We obtain the quantized sin and cos functions through the following formula:\ncos, sin = RotaryEmbedding(shape(value))\ncosmax = max |cosi |\nsinmax = max |sini |\nCOSint8 = [\\frac{cos}{cosmax} \u00d7 127 + \\frac{1}{2}]\nsinints = [\\frac{sin}{sinmax} \u00d7 127 + \\frac{1}{2}]\nDuring forward propagation, positional encoding is added to the query and key in the following way:\nCOSint8 \u00d7 \\frac{cos}{127}\nsin = sinints \u00d7 \\frac{sin}{127}\nquery, key = ApplyRotaryEmbed(query, key, cos, sin)"}, {"title": "3.2. Pre-training", "content": "The training of PhoneLM has been set up as follows: (1) The optimizer is AdamW (Loshchilov, 2017) with \u03b21 of 0.9, \u03b22 of 0.95, and \u03f5 of 1e-8. (2) We use Fully Sharded Data Parallel (FSDP) to leverage multi-GPU and multi-node setups efficiently. (3) Another critical improvement is the integration of Flash Attention 2, an optimized attention mechanism. (4) We also use Zero Redundancy Optimizer(ZeRO), a memory optimization technique that reduces the models's memory footprint. (5) We use BF16 to accelerate the training process. The details of the setting of pre-training stage are shown in table 4.\nWe use a dataset sourced from open datasets. For PhoneLM-0.5B, we use 1.1 trillion tokens, and for PhoneLM-1.5B, we"}, {"title": "3.3. Fine-tuning", "content": "The fine-tuning of PhoneLM base model is similar to MiniCPM (Hu et al., 2024) and Llama 3 (Dubey et al., 2024), which includes two stages: decay stage and Fine-tuning stage. (1) Decay Stage. We use a mixture of the pre-training data and high-quality supervised fine- tuning data, which is about 20 billion tokens. In this stage, we use a linear learning rate decay schedule. (2) Fine-tuning Stage. We find it still necessary to conduct a separate Fine-tuning stage. We utilize fine-tuning data similar to that in the decay phase but excludes pre-training data, totaling approximately 2.59 billion tokens. The learning rate for fine-tuning is set to match the final learning rate from the decay stage. The optimizer in the Fine-tuning stage is the same as that in the pre-training stage for acceleration, but with different hyperparameter settings, which are shown in the table 4.\nInstruct Tuning. In the decay stage, the data mixture contains some dataset from stable training stage, including DCLM-baseline, StarCoderData, and Dolma. Then it contains some high-quality fine-tuning data,which is used in Fine-tuning stage. The fine-tuning datasets are shown in table 3, including APIGen, Stack Smol, UltraChat, Math-Instruct, OpenAssistant 2, OpenHermes, CommitPackFT, OSS-Instruct, and SlimOrca. The details of these datasets are shown in appendix B. The pre-training loss of Decay Stage and Fine-tuning Stage is shown in figure 4 Since we continue fine-tuning the model after the decay stage, the loss drops significantly at the beginning of each epoch.\nFunction Call Tuning. To enhance the model's capability in smartphone operation, we fine-tuned the PhoneLM on the DroidCall dataset, a synthetic dataset specifically focused on Android intent invocations generated by GPT-4. The DroidCall dataset consists of 10k samples of function calling, encompassing simple, parallel, and nested call patterns. This dataset covers common Android operations, including setting alarms, configuring timers, composing email drafts, performing searches, and more. We use LoRA to fine-tune PhoneLM, adding adapter to all linear layers within both the attention layers and MLP layers The fine-tuning process was configured with an initial learning rate of 1.41e-5, utilizing a rank (r) of 8 and an alpha value of 16. A linear learning rate scheduler was employed with a warmup ratio of 0.1. To ensure a minimal computational load and to increase inference speed, we used a minimalist prompt, which essentially only included function information and user queries. The final function calling model was derived from the optimal checkpoint of the fine-tuning process. The details of prompt construction are shown in appendix D."}, {"title": "4. Experiment Results", "content": "We evaluate PhoneLM on a wide range of commonsense reasoning and problem-solving tasks and compare it to several existing open-source language models with similar model sizes."}, {"title": "4.1. Baselines and Tasks", "content": "We compare PhoneLM family models to several existing open-source language models with similar model sizes. For PhoneLM-0.5B, the main comparisons are made among models with fully open-source datasets, code, and weights, including Pythia-410M, OPT-350M, BLOOM-560M, MobiLlama-0.5B, OpenELM-450M, and SmolLM-360M. At the same time, we also compared with some models that only open-source the weights, such as Qwen1.5-0.5B, LaMini-GPT-774M, and Cerebras-GPT-590M. For PhoneLM-1.5B, the models with fully open-source code and weights include Pythia-1.4B, OPT-1.3B, BLOOM-1.1B, TinyLlama-1.1B, MobileLLaMA-1.4B, MobiLlama-1B, OpenELM-1.1B, DCLM-1B, SmolLM-1.7B. The models that only open-source the weights include Qwen1.5-1.8B, StableLM2-1.6B, MiniCPM-2B, Gemma2-2B, etc.\nTo understand the ability of PhoneLM, We used 7 datasets across two domains to evaluate the SLM performance."}, {"title": "Commonsense Reasoning Datasets", "content": "HellaSwag (Zellers et al., 2019): Tests narrative understanding through plausible sentence completion.\nWinogrande (Sakaguchi et al., 2020): Evaluates pronoun ambiguity resolution using commonsense reasoning.\nPIQA (Bisk et al., 2020): Focuses on physical commonsense reasoning and object interactions.\nSciQ (Welbl et al., 2017): a dataset of 13.7K multiple choice science exam questions.\nBoolQ (Clark et al., 2019): Tests commonsense and factual reasoning with yes/no questions."}, {"title": "Problem Solving Datasets", "content": "ARC Easy (Clark et al., 2018): Contains simple science questions testing general knowledge and reasoning.\nARC Challenge (Clark et al., 2018): Presents complex science exam questions requiring knowledge integration.\nWe adopt the benchmark lm_eval (EleutherAI, 2024) to evaluate the models. We evaluate the models after stable training stage. We use accuracy as the primary evaluation metric. Accuracy measures the proportion of correct predictions to total examples. For commonsense reasoning and problem solving tasks, accuracy evaluates the model's ability to select correct options or provide accurate solutions. Following previous practice, the models are evaluated in a zero-shot setting on these tasks. We notice that PhoneLM outperforms baselines on many of the tasks and obtains the highest averaged scores for most open-source models. We also evaluate the models on other following tasks, which contains the following tasks: SocialIQA, TruthfulQA, MMLU, CMMLU and C-Eval."}, {"title": "4.2. Capability", "content": "The capability for 7 standard zero-shot tasks of PhoneLM are presented in table 5. It can be seen from table 5(a) that PhoneLM-0.5B achieves the highest average accuracy on these 7 tasks. Except for the two tasks of ARC-e and ARC-c, where PhoneLM-0.5B performs lower than SmolLM,"}, {"title": "4.3. Instruction and Function Call", "content": "Instruction following. We have attached examples of PhoneLM-1.5B-Instruction in several scenarios, including \"Reasoning\", \"Knowledge\u201d, \"Programming and Logic Building\", \"Innovative Thinking\", \"Translation\", and \"Cre-"}, {"title": "4.4. On-device Runtime Cost", "content": "Hardware and framework. To benchmark PhoneLM models on the smartphone, we used a Xiaomi 14 with a Qualcomm Snapdragon 8 Gen 3 system-on-chip (SoC) and 16GiB of RAM, running Android 14. We set the smartphone as a performance mode to ensure stable benchmark results. We ported the code and weights of PhoneLM to mllm (Yi et al., 2023). We conducted experiments in both CPU and NPU usage scenarios. For the CPU-only benchmark, we used four threads for computation, deploying them on the four performance cores of the CPU. The weights of the linear and embedding layers in mllm were quantized to 4-"}, {"title": "4.5. An End-to-end Android Demo", "content": "We also have an end-to-end Android demo application for PhoneLM-1.5B based on mllm. This demo contains two invocations: chat and Android intent invocation. The screenshots of this application are shown in figure 1. Figure 1(a) shows an example of a user having a conversation with an Android application with PhoneLM-1.5B-Instruct built in. Figure 1(b) shows the Android intent invocation ability of the PhoneLM-1.5B-Call model. In this example, after understanding the user's \"Wake me up at 8:00\", the model uses the Android alarm-setting Intent to set an alarm for 8 o'clock."}, {"title": "5. Conclusions", "content": "This work presents PhoneLM, an efficient, capable, and fully open-sourced small language family. PhoneLM is built atop a unique principle: searching for a runtime-efficient transformer architecture ahead of pre-training. We also release an end-to-end demo using PhoneLM for intent invocations on Android OS in a fast and accurate performance. The goal of PhoneLM is to advance the development and research on small language models towards more practical on-device deployment."}, {"title": "A. Setting of 100M and 200M models", "content": "Tested the speed and performance of 100M and 200M models, training on data with 20 billion tokens. The settings are shown in the table 7."}, {"title": "B. Training Dataets", "content": "DCLM-baseline (Li et al., 2024) is a 4T token / 3B document pretraining dataset that achieves strong performance on language model benchmarks.PhoneLM only uses a maximum of 1.5T among it. The code is publiced in https:// huggingface.co/datasets/mlfoundations/ dclm-baseline-1.0-parquet.\nStarCoderData (Li et al., 2023) contains 783GB of code in 86 programming languages, and includes 54GB GitHub Issues + 13GB Jupyter notebooks in scripts and text-code pairs, and 32GB of GitHub commits, which is approximately 250 Billion tokens. The code is publiced in https:// huggingface.co/datasets/ bigcode/starcoderdata.\nOpenWebMath (Paster et al., 2023) is a dataset containing the majority of the high-quality, mathematical text from the internet. It is filtered and extracted from over 200B HTML files on Common Crawl down to a set of 6.3 million documents containing a total of 14.7B tokens. The code is publiced in https:// huggingface.co/ datasets/open-web-math/open-web-math.\nDolma (Soldaini et al., 2024) is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials. The code is publiced in https:// huggingface.co/ datasets/allenai/dolma."}, {"title": "C. Instruct Following Examples", "content": "The code is publiced in https://huggingface.co/datasets/bigcode/commitpackft.\nOSS Instruct dataset (Wei et al., 2023) is generated by gpt-3.5-turbo-1106 developed by OpenAI. The code is publiced in https://huggingface.co/datasets/ ise-uiuc/Magicoder-OSS-Instruct-75K.\nSlimOrca (Lian et al., 2023) release provides an efficient means of reaching performance on-par with using larger slices of our data, while only including 500k GPT-4 completions. The code is publiced in https://huggingface.co/datasets/Open-Orca/SlimOrca.", "sections": [{"title": "C. Instruct Following Examples", "sections": [{"title": "C. Instruct Following Examples", "sections": [{"title": "D. Function Calling fine-tuning details", "content": "We adopt a chat-based approach for function calling, where the requirements, use query, and function descriptions are encapsulated in the system prompt and user prompt sections, while the function calls are placed in the assistant output section. The prompt design for function calling fine-tuning thus focuses on structuring these three components: system prompt, user message, and assistant output. These components are subsequently formatted using a chat template, which is then utilized for model fine-tuning. These components are shown in 1, in which $function is the functions description information, which describes the function name, parameters, and other information, $user_query is the user input."}]}]}]}]}