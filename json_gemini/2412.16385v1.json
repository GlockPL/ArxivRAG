{"title": "COLLISION-BASED DYNAMICS FOR MULTI-MARGINAL OPTIMAL TRANSPORT", "authors": ["Mohsen Sadr", "Hossein Gorji"], "abstract": "Inspired by the Boltzmann kinetics, we propose a collision-based dynamics with a Monte\nCarlo solution algorithm that approximates the solution of the multi-marginal optimal transport\nproblem via randomized pairwise swapping of sample indices. The computational complexity and\nmemory usage of the proposed method scale linearly with the number of samples, making it highly\nattractive for high-dimensional settings. In several examples, we demonstrate the efficiency of the\nproposed method compared to the state-of-the-art methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Since its introduction by Gaspard Monge [1] and seminal contributions by Leonid Kantorovich [2],\nthe Optimal Transport (OT) has evolved into a rich mathematical framework with fruitful theoretical\nproperties. At its core, it gives a geometrically intuitive basis to compare and interpolate probability\ndistributions, leading to wide-range of applications across many fields. This includes interpolation\nbetween images [3], clustering dataset [4], surrogate models [5], calibration of stochastic processes [6],\nand finding the N-body particle distribution function in density functional theory [7]. However, the\ncomputational complexity associated with the underlying optimization problem limits the use of OT\nin large datasets. This is due to the fact that the OT problem is inherently linear programming over\nan infinite-dimensional space, resulting in computationally intensive optimization. The problem can\neven become intractable if multi-marginals are considered. Though non-exclusively, we can categorize\nthe main computational algorithms for numerical solution to the OT problem as the following.\nLinear programming. This is the direct approach in solving the OT problem, also known as Earth\nMover's Distance in the literature [8]. Linear programming has been applied mainly to the two-\nmarginal OT problem, where the computational complexity becomes O(N\u00b3 log(Np)) for Np number\nof samples per marginal. Fast EMD algorithms use the network simplex with empirical computational\ncomplexity of O(N2) [9].\nRegularization via entropy. By incorporating entropy in the cost functional of the two-marginal OT,\none derives a relaxed version of the OT problem [10, 11]. The resulting optimization problem is\nconvex and can be solved efficiently using the so-called Sinkhorn method, with the computational"}, {"title": "2. MAIN IDEA", "content": "Given independent and identically distributed (i.i.d.) samples {X(i)}, we develop a stochastic update\nrule that guides the resulting realizations toward approximating the optimal solution of (2). Our\nobjective is to construct this update rule in such a way that the computational complexity of each\niteration scales linearly with the number of samples. A natural representation of the distribution\nbased on given samples is via empirical measure\n$\\\u03bc\u03ae = \\frac{1}{N_p} \\sum_{j=1}^{N_p} \\delta_{X^{(i)}_j}$ (7)\nWe leverage several key ideas in order to proceed. Let us focus on two marginal setup, i.e. K = 2,\nand recall the following facts [28, 29, 30].\n(1) For discrete measures of the form (7), the optimal cost is given by\n$\\min_{\\pi \\in \\Pi(\\mu_1,\\mu_2)} \\int c(x_1, x_2)\\pi(dx) = \\min_{V\\in B^{N_p}} \\frac{1}{N^2_p}\\sum_{i,j} c(X^{(1)}_i,X^{(2)}_j)$,\nwhere BNP is the set of Np \u00d7 Np bistochastic matrices.\n(2) The extremal points of BNP are permutation matrices. Therefore\n$\\opt = \\min_{\\varpi \\in \\p_p} \\frac{1}{N_p}\\sum_{i=1}^{N_p}c(X^{(1)}_i, X^{(2)}_{{\\varpi(i)}}),$\nwhere \u2211Np is the set of permutations of {1, .., Np}. The corresponding optimal map is given\nby T(X(1)) = Xopt (i)\n(3) For the L2 cost, as Np \u2192 \u221e, the optimal distribution and map weakly converge to the\nsolution of the Monge-Kantorovich problem, i.e. (opt,) \u2192 (opt,T).\nHence the equivalent form of optimization problem (2), admitting the mentioned assumptions, is\ngiven by a search over permutation matrices. In general, this remains a computationally intensive\ntask, see e.g. the Hungarian algorithm [31]. To address this, a nested approach for finding a nearly\noptimal permutation matrix was proposed in [16, 17]. The Iterative Swapping Algorithm (ISA) aims\nto identify a near-optimal permutation by performing pairwise index swaps that reduce the cost.\nHowever, because ISA examines all possible swaps, its computational complexity remains quadratic.\nStarting from the premise that pairwise index swapping can yield near-optimal permuta-\ntions, we introduce a stochastic variant of the ISA by drawing on analogies with Boltzmann kinetics.\nRather than exhaustively examining all possible pairwise swaps, our approach involves randomly\ngrouping indices, with each group containing only one swapping pair. Therefore at each iteration, only\nNp/2 swapping candidates are assessed (instead of Np(Np - 1)/2 required in ISA). This reduction\nsimplifies the complexity of our stochastic version to linear scaling with respect to Np.\nOur scheme draws a close analogy to Boltzmann kinetics, and it is helpful to introduce the\nconcept of particles to clarify this setup. Each particle represents a realization of a random variable,\nsampled from a marginal distribution. In this context, swapping can be viewed as a binary collision"}, {"title": "3. PROCESS FORMULATIONS", "content": "Consider discrete time index t\u2208 {0,1,...} and i.i.d. samples ) for marginal i and sample index\nj = 1, ..., Np.\n(1) ISA process: For each marginal i \u2208 {1, ..., K} and samples j, k \u2208 {1, ..., Np} with k > j, ISA\nupdates the samples via\n$((i)t+1, X (i)t+1)T = ((i) (i))T$\nThe swaps are guided by the discrete cost\n$m(\\tilde{\\pi}_t) = \\mathbb{E}_{\\tilde{\\pi}_t} [C]$\nwhere it is the empirical measure of Xt. The swapping kernel is given by\n$K_{j,k} = \\begin{cases}\nI_{2n\\times2n} & \\text{if } m(\\tilde{\\pi}^{(X^{(i)}_j \\atop X^{(i)}_k})\\tilde{\\pi}_t )\\geq m(\\tilde{\\pi}_t) \\\\\nJ_{2n\\times2n} & \\text{if } m(\\tilde{\\pi}^{(X^{(i)}_j \\atop X^{(i)}_k})\\tilde{\\pi}_t ) < m(\\tilde{\\pi}_t)\n\\end{cases}$\nwith Inxn as the identity matrix and Jan exchange matrix of the form\n$J_{2n\\times2n} = \\begin{bmatrix}\nO_{n\\times n} & I_{n\\times n} \\\\\nI_{n\\times n} & O_{n\\times n}\n\\end{bmatrix}$\nand Onon is a n\u00d7n matrix with zero entries. The swapping kernel (12) allows swaps if it\nleads to reduction in the cost associated with the empirical measure (11).\n(2) Collision-based dynamics: The proposed collision-based version of ISA performs similar steps\nwith the difference that j, k are now chosen from a random subset C C {1, ..., Np} of size 2.\nTherefore in collision-based method, instead of applying (10) to all pairs k, j \u2208 {1, ..., Np}\nwith k > j, we pick j, k ~ U([1, Np]) where U(.) is a discrete uniform measure with values\nbetween 1 and Np. As a result, the complexity of the algorithm reduces to O(Np). Note\nthat this randomization step in general can be justified as long as the subsets are sampled\nindependent of the random variable X. While the consistency proofs exist for range of kernels\n[33], we leave the theoretical justification of the consistency between collision-based process\nand ISA to separate studies."}, {"title": "4. MONTE CARLO SOLUTION ALGORITHM FOR THE COLLISIONAL DYNAMICS", "content": "Motivated by the direct Monte Carlo solution algorithms to the Boltzmann [35] and the Fokker-Planck\nequation [36] for rarefied gas and plasma dynamics, here we devise a collision-based numerical scheme\nto solve the discrete optimal transport problem. In order to ensure that all the particles are considered\nfor the collision in one time step, we consider the following collision routine for each marginal:\nGenerate a random list of particle indices R of size Np without repetition.\nDecompose R into two subsets of the same size I and J where I NJ = \u00d8.\nSwap particles with indices Ik and Jk for collisions using (10) where k = 1, ..., Np/2.\nWe note that by shuffling the particle indices, one can easily find a random list of particle indices R.\nIn Algorithm 1, we give a detailed description of the proposed method."}, {"title": "5. PROPERTIES OF COLLISIONAL DYNAMICS FOR THE OPTIMAL TRANSPORT PROBLEM", "content": "The proposed collision-based Monte Carlo solution Algorithm 1 has several numerical properties that\nwe list next.\nMarginal preservation. Since we only change the order of particles in each marginal when\na collision is accepted, Algorithm 1 preserves the marginals up to machine accuracy on the\ndiscrete points."}, {"title": "6. RESULTS", "content": "Here, we test the proposed collision-based Algorithm 1 in solving MMOT to train a multi-marginal\nsampling model as well as a metric to find distances between images in a dataset. In Appendix B, we\ncarry out further test on several toy problems to validate the convergence rate and computational cost\nof the proposed method compared to EMD and Sinkhorn. Everywhere in this study, we report an\nestimate of Wasserstein distance d\u00ba(.) given samples of ithe marginal X(i) \u2208 RNp\u00d7n for i = 1, ..., K,\ni.e.  d^p(X) := (\\sum_{j,k}^{N_p}\\sum_{i=1}^{K}|X^{(i)}_j - X^{(i)}_k|^p/N_p. All computations are done on a laptop with an Intel\nCore i7-8550U CPU that runs with 1.8GHz frequency equipped with 16 GB memory. In this paper\nwe use Python Optimal Transport library [38] for EMD and Sinkhorn computations."}, {"title": "6.1. Learning a five-marginal map", "content": "As an interesting application of MMOT, here we deploy the proposed method to learn a map between\nnormal and four other distributions, i.e. Swiss roll, banana, funnel, and ring, see [39] for details.\nFirst, we take Np = 2 \u00d7 104 samples from five marginals and construct X = [X(1), ..., X(5)]\nwhere X(1) ~ N(0, I) and the other marginals follow density of target densities, i.e. Swiss roll,"}, {"title": "6.2. Distribution of Wasserstein distance in a dataset", "content": "One of the applications of Wassetstein distance is labeling datasets, since it provides us with a metric\nin the space of distributions. Here we show the efficiency of the proposed collision-based solution to\nMMOT by treating this problem as one. Consider the Japanese Female Facial Expression (JAFFE)\n[41] and butterfly [42] datasets. The JAFFE dataset consists of 213 images, where we treat each\nas a marginal. From the butterfly dataset, we consider 50 classes, from which we select 4 pictures\nrandomly which leads to MMOT problem with 200 marginals.\nWe deploy the collision-based solution Algorithm 1 to these MMOT problems with L2-Wasserstein\ncost to find the pairs of particles/samples across marginals. Since we are minimizing the total cost"}, {"title": "7. CONCLUSION", "content": "We proposed a novel solution algorithm called collision-based dynamics for the discrete OT problem,\nincluding multi-marginal settings. The devised collision process is based on random binary swaps of\nthe samples and is built on close analogy with the Boltzmann kinetics. We showed that in the case\nof L\u00ba-Wasserstein distance, the proposed method has the computational complexity of O(nK2Np),\nwhere n is the dimension of each sample, Np is the number particles/samples per marginal, and K is\nthe number of marginals. We achieved this performance by randomizing the swapping process. The\nmethod conserves marginals by construction. We showed empirically that it admits an exponential\nconvergence to a near-optimal solution."}, {"title": "APPENDIX A. ITERATED SWAPPING ALGORITHM", "content": "Here, we give a short description of ISA algorithm used in this paper, given its similarity to the\ncollision-based method proposed in this paper. As detailed in Algorithm 2, in each iteration for\neach marginal, O(2) operations are carried out, while each particle maybe be accepted for swap\n(collision) more than once. This makes the algorithm prone to data race as an issue for shared-memory\nparallelism."}, {"title": "APPENDIX B. FURTHER RESULTS", "content": "In this section, we test the convergence rate of the proposed collision-based solution algorithm to the\noptimal transport problem in several toy problems."}, {"title": "B.1. Optimal map between two n-dimensional random Gaussians", "content": "In this section, we consider two-marginal random Gaussian distributions as a complicated toy problem.\nConsider random variables X(1) and X(2) that are sampled via\n$\\tilde{X}^{(1)}_i \\sim \\mathcal{N}(\\mu^{(1)}I_n, \\sigma^{(1)}I_{n\\times n})$, $\\tilde{X}^{(2)}_i \\sim \\mathcal{N}(\\mu^{(2)}I_n, \\sigma^{(2)}I_{n\\times n})$, $\\mu^{(1)}, \\mu^{(2)} \\sim \\mathcal{U}([-10, 10])$,$1/\\sigma^{(1)}, 1/\\sigma^{(2)} \\sim \\Gamma(3, 1)$,"}, {"title": "B.2. Wasserstein distance in several datasets", "content": "As an example from image processing, here we consider optimal transport problem given data from\nstandard datasets such as JAFFE, butterfly, and MNIST [44]. In each case, we take 100 random\npairs of pictures from the dataset, and find the optimal map between them using EMD, Sinkhorn,\nand the proposed collision-based OT Algorithm 1. As shown in Fig. 9, the proposed collision-based\napproach outperforms Sinkhorn in the distribution of relative error in Wasserstein distance, speed-up\nand memory consumption."}, {"title": "B.3. Optimal map between normal and Swiss roll/banana/funnel/ring density", "content": "Consider two-marginal optimal map between normal distribution \u03bc\u2081 = N(0, I2x2) and a target\ndistribution \u03bc2. Here, we consider Swiss roll, banana, funnel, and ring as target densities, see [39] for"}, {"title": "B.4. Coloring images", "content": "Assume we are given a picture of Robert De Niro\u00b9 in gray and color. We intend to learn the map\nbetween the colored and black/white pictures and use it to turn another black/white picture into a\ncolorful one. We take 4,000 samples from De Niro's picture, use the collision-based algorithm to find\nthe optimal map on discrete points and train a NN denoted by M with L\u00b2-pointwise error between\noptimally sorted data points and NN prediction as loss. We construct the NN using 4 layers, each\nwith 100 neurons, equipped with tanh(.) as activation function and use Adam's algorithm [40] with a\nlearning rate of 10-3, take L2 point-wise error between NN estimate and optimally ordered data as\nthe loss function, and carry out 5,000 iterations to find the NN weights. Afterward, we test NN by\nplugging in a black/white portrait of Albert Einstein\u00b2 as input and recover a colored picture as the\noutput, see Fig. 11."}]}