{"title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems", "authors": ["Elad Levi", "Ilan Kadar"], "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning, execution, and refinement. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are revolutionizing the field of artificial intelligence by transitioning from static language processors to dynamic, task-oriented agents that can autonomously plan, execute, and refine their actions. These agents promise transformative applications across a wide range of domains, including healthcare [20, 1], finance [30, 26, 7], customer support [21, 14] and education [31, 29]. This evolution positions LLM agents as foundational technologies for reshaping human-computer interaction, enabling intelligent systems to tackle complex, real-world challenges with unprecedented efficiency and adaptability.\nAmong these advancements, conversational AI agents present a particularly demanding frontier. Unlike single-turn systems, these agents must navigate multi-turn dialogues, integrate domain-specific tools and APIs, and adhere to stringent policy constraints. The interplay of these requirements introduces a new level of complexity, where the cost of errors-ranging from inconsistent responses"}, {"title": "Related Work", "content": "Recent advancements in using LLMs for synthetic data generation, automated evaluation have significantly influenced the development of AI systems. This section delves into these key areas, outlining existing methodologies and their limitations while highlighting how our approach advances the state of the art."}, {"title": "Synthetic Benchmarks", "content": "Data generation. Synthetic data generation using Large Language Models (LLMs) has become a transformative technique for advancing AI across various domains, including code generation [22, 32], mathematical reasoning [34, 18], text embedding [24], and text-to-image synthesis [5]. Synthetic data reduces the costs and time of human-annotated datasets while providing control over sample distribution, crucial for fine-tuning and optimizing performance in downstream tasks [16, 27]. Evaluating synthetic data focuses on two key metrics: faithfulness and diversity.\nFaithfulness ensures synthetic data reflects real-world patterns and relationships, while diversity captures a wide range of scenarios to enhance model robustness and mitigate overfitting [17]. Achieving both metrics is challenging; recent research explores conditional prompting and multi-step generation to balance them. Conditional prompting improves diversity by defining attributes through condition-value pairs [12, 28]. Multi-step generation enhances coherence and domain coverage by decomposing tasks into smaller subtasks [8, 25, 13, 23], though these methods often require significant manual effort and may not scale well to complex domains.\nOur approach automates synthetic dataset generation to ensure both faithfulness and diversity by using a policies graph inspired by GraphRAG [10], where nodes represent policies and edges capture their complexity relationships. This framework enables fine-grained generation across various combinations of policies, tools, and tasks, producing datasets that reflect application requirements while covering a diverse range of scenarios. It addresses challenges from simple tasks to complex edge cases, ensuring rigorous evaluation of agents under diverse conditions.\nAutomated evaluation. Synthetic datasets have become invaluable for evaluating retrieval-augmented generation (RAG) systems, offering metrics to measure retrieval quality, generation fidelity, and robustness. Frameworks like RAGAS [11] automate the evaluation of RAG pipelines, focusing on aspects such as retrieval accuracy and generation relevance. Unlike traditional metrics, RAGAS operates without reference answers, enabling broader applicability. However, it primarily targets isolated components and does not fully address the complexities of multi-turn dialogues or real-world conversational AI applications. This underscores the need for expanded evaluation frameworks that encompass diverse, dynamic use cases."}, {"title": "Conversational AI Benchmarks", "content": "Evaluating conversational AI systems in real-world applications has been the focus of various benchmarks, each targeting specific capabilities. For instance, T-bench [33] assesses agents' ability to interact with users, adhere to domain-specific policies, and utilize API tools effectively, with simulations in domains like retail and airline customer service. However, T-bench is limited by its reliance on manual curation, with only 50 samples for airlines and 115 for retail, restricting scalability. Additionally, its evaluation focuses solely on coarse-grained end-to-end metrics, overlooking policy violations and dialogue flow errors, which limits comprehensive assessment.\nThe ALMITA benchmark [3] proposes a novel dataset and framework specifically tailored for evaluating tool-augmented conversational AI agents in customer support scenarios. It uses a combination of automated and manual processes to generate diverse and realistic conversations grounded in user-defined procedures. Despite its rigorous evaluation, ALMITA focuses primarily on customer support, and the generalizability to other domains remains an open question. Moreover, the reliance on manually curated samples, while ensuring quality, limits scalability.\nThe LTM Benchmark [6] effectively highlights limitations in conversational multitasking, especially with interleaved tasks. However, its reliance on predefined interaction structures limits its ability to capture the unpredictable and non-linear nature of real-world conversational flows, such as spontaneous topic shifts or revisiting earlier contexts. Similarly, E2E Benchmark [4] evaluates chatbot responses based on accuracy and usefulness, emphasizing conversational coherence in non-task-oriented interactions. Nevertheless, its lack of support for complex tool use and multi-turn interactions restricts its applicability to broader real-world contexts. The CURATe framework [2] addresses alignment challenges for personalized conversational agents by focusing on user-specific safety-critical contexts. While it introduces valuable techniques for multi-turn personalization, its emphasis on alignment rather than general performance testing narrows its scope.\nAlthough these benchmarks provide valuable tools for assessing conversational AI systems, their reliance on manual curation limits scalability and adaptability to diverse real-world applications, making it challenging to generate the extensive datasets required for comprehensive evaluations. Our approach, in contrast, is fully automated, enabling the generation of diverse scenarios and dialogues at scale, thus supporting evaluations across varied domains. Additionally, our framework addresses a broader range of challenges, from simple tasks to complex edge cases, ensuring rigorous agent evaluation under diverse conditions. Unlike existing benchmarks that use coarse-grained metrics for overall performance, our method offers fine-grained insights by evaluating agents across all policy and tool combinations, identifying specific strengths and weaknesses."}, {"title": "Method", "content": "Our multi-agent system is illustrated in Figure 1. The system pipeline consists of the following steps: (1) The IntellAgent system receives a schema of the system database along with either a chatbot system prompt or a document outlining the company policies. Based on this input, the system constructs a policy graph (3.1.1). It then samples a list of policies from the graph at varying levels of complexity and generates an event addressing these policies (3.1.2). The event includes a scenario description with a user request and corresponding samples for the initial database state, ensuring the validity of the user requests. (2) The system simulates a dialog between the chatbot and a user agent using the information provided in the event (3.2). (3) Finally, a critique is provided with the dialog and provides an analysis of the chatbot's performances with respect to the event policies list (3.3)."}, {"title": "Event Generation", "content": "To address the challenge of developing advanced chatbots capable of interacting with a system database, the system must generate complex and natural user requests that cover various policies. Additionally, it should create an initial database state for the chatbot, ensuring that when the chatbot processes the user request and queries the database, it does not encounter failures.\nAn IntellAgent event is defined by the following components: (1) A list of policies. (2) A description of a user request that aligns with the specified policies. (3) The initial state of the chatbot's system database."}, {"title": "Policies graph", "content": "IntellAgent aims to generate a diverse set of events with varying levels of complexity. To create and complex user-chatbot interaction scenarios, IntellAgent constructs a policy graph. In this graph, nodes represent individual policies, and edge weights indicate the likelihood of two policies appearing together in the same interaction. Each node is also assigned a weight that reflects the complexity of its associated policy.\nThe graph is built through multiple queries to a large language model (LLM). First, the system extracts a list of policies from the prompt and assigns a difficulty ranking to each. Then, for every pair of policies, the LLM assigns a score (on a scale of 1\u201310) representing the likelihood of the two policies co-occurring in a conversation."}, {"title": "Event generator", "content": "The complexity of an event is defined as the sum of the complexities of its policies. Given a policy graph with a set of policies G, weighted edges E and nodes complexity weights {ng}g\u2208G. the event generator aims to produce a valid set of policies that satisfies the following criteria:\n1. The event complexity is uniformly distributed within a specified range.\n2. The distribution of the first policy in the event is uniformly distributed across all possible policies.\n3. For a given complexity level and initial policy, the distribution of the resulting policy list aligns with real-world distributions.\nPolicies graph sampling. To satisfy the outlined criteria, the IntellAgent sampling algorithm operates in batches. The process for each iteration is as follows: The complexity of events in the current batch is sampled first. The batch distribution is adjusted to ensure that the overall distribution of all generated events (including previous batches) remains uniform. Next, the initial policy for each event is sampled uniformly across all nodes in the policy graph. Then For each event, the system generates a policy path by performing a random walk on the graph. The walk terminates once the cumulative complexity of the visited nodes exceeds the sampled event complexity. An overview of the entire sampling method is provided in Algorithm 1.\nThis approach ensures that the generated events policies list maintains the desired complexity distribution and follows realistic transitions between policies as determined by the graph structure.\nEvent generator agent. The goal of the event generator agent is to create an event based on a given list of policies. The primary challenge is to generate a valid and consistent initial database state that the chatbot can interact with during the conversation. The agent's architecture is shown in Figure 4. To manage complex database schemas, the event generator agent first creates a symbolic"}, {"title": "Dialog simulation", "content": "For each event in the events database, IntellAgent simulates an interaction between a user and the chatbot being tested. Figure 5 provides an overview of the simulation architecture. The user agent is given the event details, which include the description of the event and all relevant information inserted into the chatbot's system database by the event generator agent. Additionally, the user agent is provided with the expected behavior of the chatbot at each step of the interaction, based on the event's policy list. The user has the option to terminate the interaction at any point, either when the chatbot successfully completes the task or if the chatbot fails to adhere to one of the policies and does not follow the expected behavior."}, {"title": "Dialog Critique", "content": "The Dialog critique component is given the user-chatbot dialog and the chatbot system prompt to assess whether the reason for the dialog's termination, as provided by the user agent, is correct. If the reason is incorrect, the critique provides feedback to the user agent, and the dialog resumes. If the reason is correct, the critique then determines: (1) The subset of event policies that were tested during the dialog. (2) The subset of policies that the chatbot did not adhere to (this list may be empty). Using this information, a comprehensive report on the chatbot's performance is generated."}, {"title": "Experiments", "content": "Dataset construction. To evaluate the performance of our system in real-world scenarios, we utilized the T-bench [33] environments. Specifically, we employed the benchmark's prompts, database schema, and tools for the two benchmark's environments: airline and retail. For each environment, the system generates a policy graph from the prompt and simulates 1,000 events\u2014a substantial increase that enables more fine-grained analysis compared to the original benchmark, which used 50 samples for airlines and 115 for retail. The complexity levels of the generated events range from 2 to 11. Table 1 presents a comparison of various random walk sampling strategies. Selecting the next node uniformly leads to unrelated policies, whereas choosing the next node based on maximal weight produces a cohesive cluster of policies. In contrast, IntellAgent-weighted probability sampling achieves a balance between diversity and alignment with the real-world distribution. Appendix B provides a detailed example of the event generation process.\nTested agents. We evaluated several state-of-the-art proprietary LLMs: GPT-4o, GPT-4o-mini, Gemini-1.5-pro, Gemini-1.5-flash, Claude-3.5-sonnet, and Claude-3.5-haiku. For all these models we employed the native tool-calling agent (supported by all the tested LLMs) with the tested environment system prompt. During each iteration, the model determines whether to send a message to the user or to call a tool.\nIntellAgent Implementation details. The IntellAgent multi-agent system was implemented using the Langgraph\u00b9 framework. GPT-4o served as the system's LLM model, used for the event generation, the user agent, and dialog critique. For full implementation details including all the system prompts, we provide the source code and documentation\u00b2"}, {"title": "Benchmark", "content": "Benchmarks comparison. Table 2 presents a comparison of the tested model's success rates on the T-bench [33] and the IntellAgent benchmark. The Pearson correlation coefficients are 0.98 for the Airline environment and 0.92 for the Retail environment, highlighting a strong alignment in model performance across the two benchmarks. Notably, this strong correlation persists despite IntellAgent being generated exclusively with synthetic data.\nModels comparison. Figure 2 illustrates the success rates of the top four models as a function of challenge level. As expected, model performance declines as the challenge level increases, though the pattern of decline varies across models. For instance, while Gemini-pro-1.5 significantly outperforms GPT-40-mini in the airline environment up to level 10, their performances converge at higher challenge levels. This highlights the value of IntellAgent's detailed analysis, enabling users to select the most suitable model based on the desired complexity the chatbot should handle.\nPolicies comparison. Figure 3 shows the performance of the top four models models across different policy categories. The relative ranking of models shifts across different categories. IntellAgent provides a detailed analysis of the specific policies where the tested chatbot may encounter difficulties.\nIt is also important to note that all models face challenges with user consent policies. This policy category is not assessed in the T-bench [33] since its evaluation focuses solely on the final state of the database."}, {"title": "Conclusion", "content": "In this work, we introduced IntellAgent, a scalable, open-source multi-agent framework designed to comprehensively evaluate conversational AI systems. IntellAgent addresses the limitations of traditional evaluation methods by automating the generation of diverse, policy-driven scenarios and providing fine-grained diagnostics. By leveraging a graph-based policy model, realistic event generation, and user-agent simulations, IntellAgent captures the nuanced complexities of multi-turn dialogues, policy adherence, and tool integration.\nOur findings demonstrate IntellAgent's ability to uncover critical performance gaps, offering actionable insights to optimize conversational agents for real-world applications. Its modular design supports extensibility, ensuring it remains relevant across diverse domains and use cases.\nIn future work, we plan to explore the benefits of incorporating additional real-world context into the environment, such as a small set of user-chatbot interactions. We hypothesize that this context could significantly enhance the policies graph quality by deriving edge weights and node challenge-level weights from the real-data distributions. Furthermore, we anticipate that this added context could improve the overall performance of the system database generation process."}]}