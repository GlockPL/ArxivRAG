{"title": "WHY CONTEXT MATTERS IN VQA & REASONING: SEMANTIC INTERVENTIONS FOR VLM INPUT MODALITIES", "authors": ["Kenza Amara", "Lukas Klein", "Carsten L\u00fcth", "Paul J\u00e4ger", "Hendrik Strobelt", "Mennatallah El-Assady"], "abstract": "The various limitations of Generative AI, such as hallucinations and model failures, have made it crucial to understand the role of different modalities in Visual Language Model (VLM) predictions. Our work investigates how the integration of information from image and text modalities influences the performance and behavior of VLMs in visual question answering (VQA) and reasoning tasks. We measure this effect through answer accuracy, reasoning quality, model uncertainty, and modality relevance. We study the interplay between text and image modalities in different configurations where visual content is essential for solving the VQA task. Our contributions include (1) the Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various VLM architectures under different modality configurations, and (3) the Interactive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the foundation for the benchmark, while the ISI tool provides an interface to test and apply semantic interventions in image and text inputs, enabling more fine-grained analysis. Our results show that complementary information between modalities improves answer and reasoning quality, while contradictory information harms model performance and confidence. Image text annotations have minimal impact on accuracy and uncertainty, slightly increasing image relevance. Attention analysis confirms the dominant role of image inputs over text in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow us to extract attention coefficients for each modality. A key finding is PaliGemma's harmful overconfidence, which poses a higher risk of silent failures compared to the LLaVA models. This work sets the foundation for rigorous analysis of modality integration, supported by datasets specifically designed for this purpose.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-Language Models (VLMs) have shown remarkable performance across various NLP tasks by combining visual and textual information. Models like LLaVA (Liu et al., 2023a), GPT4-Vision (OpenAI, 2024), and PaliGemma (Beyer et al., 2024) use the text interface of Large Language Models (LLMs) alongside CLIP-style image encoders (Radford et al., 2021), making them well-suited for multi-modal tasks such as content summarization (Moured et al., 2024), text-guided object detection (Dorkenwald et al., 2024), and visual question answering (VQA) (Yue et al., 2024b).\n\nThe core principle of VLMs lies in the integration and interplay of the two modalities, which significantly amplifies the effectiveness of the models. However, the extent to which each modality influences the final answer and reasoning of VLMs remains unclear within the research community, with varying and sometimes conflicting conclusions (Gat et al., 2021; Frank et al., 2021; Chen et al., 2024). Prior research has highlighted the dominance of textual data (Parcalabescu & Frank, 2023), but such studies often operate in settings where each modality, i.e., image or text, could independently be used to solve a task. This definition of multimodality, focusing on two independent but informative inputs, limits our understanding of how these modalities interact. Establishing a dataset and a tool to benchmark and examine the role of input modalities in model predictions within a multimodal setting would therefore significantly benefit the community. This approach would not only aid in understanding and mitigating model failures, such as hallucinations but also guide researchers toward practical solutions for improving model performance and self-consistency.\n\nTo this end, we present a benchmark, investigating the effect of semantic interventions on text and image modalities, alongside a carefully curated dataset and an interactive tool. Specifically, we investigate the role of the text modality"}, {"title": "2 RELATED WORK", "content": "VQA Datasets Since the introduction of the Visual Question Answering (VQA) task in Antol et al. (2015), numerous datasets have been created to support research in this area, such as GQA (Hudson & Manning, 2019), Visual7W (Zhu et al., 2016), Visual Genome (Krishna et al., 2017), CLEVR (Johnson et al., 2017), and VQA 2.0 (Goyal et al., 2017). However, these datasets often lack reasoning, are restricted to textual-only modalities, or suffer from small-scale and limited domain diversity. ScienceQA (Lu et al., 2022) emerged as the first large-scale VQA dataset to incorporate textual context alongside textual explanations for model reasoning. It features multiple-choice questions across various scientific domains, with each question annotated with lectures and explanations. More diverse multimodal datasets, such as SEED (Li et al., 2023), MMBench (Liu et al., 2023b), and MMMU (Yue et al., 2024a), have since been introduced, combining text and image data. However, as shown by Chen et al. (2024), these datasets do not ensure that all evaluation samples require visual content for correct answers. To address this limitation, we propose a new curated VQA dataset designed such that answers can only be derived from the image, with complementary or contradictory context provided as additional information to influence the model's answer and reasoning.\n\nModalities in Vision-Language Tasks Several methods have been proposed to investigate the extent to which VLMs leverage both visual and textual information. Annotation and foiling approaches introduce text annotation in images and mistakes in image descriptions and test whether the VLM predictions change. Shekhar et al. (2019); Parcalabescu et al. (2022) test VLMs' sensitivity to discrepancies between images and captions and found that models often overlook such inconsistencies. (Gat et al., 2021) exchange images and captions with other instances and note a consistent decrease in accuracy, with textual input proving to be more influential than visual content. Building on these findings,"}, {"title": "3 SI-VQA DATASET AND ISI TOOL", "content": "This section introduces the dataset and tool developed to examine the role of modalities in VQA & Reasoning. They are designed to explore semantic interventions on input modalities for thorough post-hoc interpretability of VLMs."}, {"title": "3.1 SI-VQA DATASET", "content": "The SI-VQA dataset is a closed-question VQA dataset consisting of 100 samples. Each sample consists of an image, question, and ground truth answer (Yes/No) pair, as well as one text annotated image, one contradictory context, and one complementary context. The image is the necessary and sufficient element to get the question right as it contains the critical information. The context is always in relation to the ground truth answer, aiming to either confuse or help the model without stating an explicit answer to the question, as each question can only be answered through the image. Thus, the model has a very limited ability to leverage prior knowledge to answer the question and must utilize the image input. The annotations on the image give text-written hints to the ground truth answer. We only study the impact of \"positive\" annotations, i.e., text that informs the model about key elements in the image to correctly answer the question. The images are open-source and from the MMMU Benchmark (Yue et al., 2024b). The selected images of SI-VQA Dataset span a variety of fields, including geography, history, art and design, sport, and biology, along with everyday objects and landscapes. They encompass diverse formats, such as natural photographs, cartoons, sketches, and paintings. We carefully crafted all other modalities for the dataset, focusing on quality not quantity.\n\nWe design seven scenarios for VLM interpretability analysis, creating seven modality configurations: question (Q), question + image (Q+I), question + image + complementary context (Q+I+C+), question + image + contradictory context (Q+I+C_), question + annotated image (Q+IA), question + annotated image + complementary context (Q+I+C+), question + annotated image + contradictory context (Q+IA+C_). They can be inferred using the 3x3 matrix in Figure 1. For the baseline configuration Q (question-only), the image corresponds to a black image. In this case, the model's answer accuracy is at random proving the necessity of visual content in SI-VQA Dataset (Chen et al., 2024). We also conducted the baseline experiment using noise-only images and with the images entirely removed, observing similar results in both cases."}, {"title": "3.2 ISI TOOL", "content": "To provide an intuitive and agile way to explore modality interplay in the context of multimodal interpretability, we developed the ISI Tool. We used this interactive tool to design the SI-VQA Dataset. It is designed to enable researchers and VLM users to investigate how VLMs respond to semantic changes and interventions across image and text modalities, with a focus on identifying potential model failures in the context of VQA. Specifically, it allows the perturbation of images, the addition of personalized shapes and annotations, and the arbitrary adaptation of text inputs. Users can upload their own images and questions or choose from 100 preloaded samples with semantic intervention presets from our ISI-VQA Dataset. More details about the tool and its features are available in Appendix E."}, {"title": "4 EXPERIMENT METHODOLOGY", "content": "Our benchmark pipeline illustrated in Figure 1 investigates the contribution of each modality toward the performance of several VLMs. Performance is measured in terms of output quality, model uncertainty, and attention attribution toward the input elements of the SI-VQA Dataset."}, {"title": "4.1 VISION-LANGUAGE MODELS", "content": "The VLMs selected for this study are state-of-the-art models for Visual Question Answering (VQA) tasks. We excluded models where extracting attention coefficients for each modality is not feasible, such as Flamingo (Alayrac et al., 2022), which employs gated cross-attention between text and image. The final architectures chosen for evaluation are LLaVA 1.5, LLaVA-Vicuna, LLaVA-NeXT, and PaliGemma (Beyer et al., 2024), with weights provided from HuggingFace (Wolf et al., 2020). LLaVA-Vicuna is a version of LLaVA 1.5 leveraging the Vicuna LLM, a conversation-fine-tuned version of LLaMA. LLaVA-Vicuna and LLaVA-NeXT both utilize dynamic high resolution for the image input, increasing visual reasoning and optical character recognition (OCR) capabilities. Although PaliGemma is intentionally designed for pre-training followed by fine-tuning, we employ it in this study within a zero-shot setting. We do not present any reasoning results for the PaliGemma model as it mainly generates a default response \"Sorry, as a base VLM I am not trained to answer this question.\" when it is asked to explain its answer due to its strong safety fine-tuning (Beyer et al., 2024). Each LLaVA architecture comprises 7B parameters, while PaliGemma consists of 3B parameters, and all can be quantized to reduce VRAM usage and improve computational efficiency. Our ablation study in Appendix F shows, however, that while 32bit models have lower uncertainty in VQ answering and reasoning, answer accuracy is not substantially worse for even 4bit quantized models."}, {"title": "4.2 ANSWER & REASONING EVALUATION", "content": "We assess the model's performance on the VQA task using the SI-VQA Dataset by evaluating both its answer and reasoning. To measure the VQ answering quality, we use the accuracy metric by comparing the model's binary Yes/No response to the closed question with the ground truth provided in the dataset. Evaluating reasoning is more complex, as no ground truth exists for the explanations. Without a reference rationale, we assess reasoning based on the quality of argumentation and the truthfulness of statements. To this end, we useo as an external evaluator (Zheng et al., 2024; Thakur et al., 2024). The model is prompted once for each sample to rate the reasoning from 0 to 10, considering an evaluation prompt as well as the question, image, answer, and reasoning. While the quality scores seem very reasonable to us, we observe a bias toward the score number \"8\" (see subsection H.1), a behavior also observed in other studies using LLMs as a judge (Thakur et al., 2024). See Appendix B for the evaluation prompts and hyperparameters."}, {"title": "4.3 MODEL UNCERTAINTY", "content": "For quantifying model uncertainty, we employ semantic entropy $SE(x)$ (Farquhar et al., 2024), which calculates entropy based on the sum of token likelihoods $p(s|x)$ between the sets $c$ of semantically similar clustered sentences $s$ (see Equation 1). For semantic clustering, we use the DeBERTa (He et al., 2021) entailment model. During uncertainty computation, the number of sampled outputs and the sampling temperature $T$ are set to 10 and 0.9 respectively. High $SE(x)$ means high uncertainty and low confidence in the outputs.\n\nFor VLMs, uncertainty quantification is especially important for detecting model failures, including hallucinations and silent failures. Silent failures are instances where the model generates incorrect information with high confidence, making these errors increasingly difficult to detect (Bender et al., 2021; Jaeger et al., 2023). The Area Under the Generalized Risk Coverage curve (AUGRC) metric (Traub et al., 2024) evaluates the extent to which a model makes incorrect predictions with high confidence, where high confidence is characterized by low semantic entropy, specifically below a defined threshold, $\\tau$:"}, {"title": "4.4 ATTENTION ATTRIBUTION", "content": "Attributing attention values to the different modalities serves as one indicator of each modality's contribution to the final answer and reasoning (Wiegreffe & Pinter, 2019). We adopt a mechanistic approach, treating attention as a VLM interpretability measure to evaluate the roles of the question, image, and context. By aggregating attention across layers and heads, we derive relevance scores for each token's contribution to the answer or reasoning (Parcalabescu & Frank, 2023; 2024). To determine which input modality is most relevant to the prediction, we sum the relevance scores of their respective input tokens. Finally, to standardize the scores for comparability, we compute the relative relevance score for each sample. A detailed explanation of attention aggregation and implementation is provided in Appendix A."}, {"title": "5 EXPERIMENT RESULTS", "content": "Based on the seven modality configurations and the results of previous related work, we define three prior hypotheses regarding the anticipated outcomes when including a new type of input. As discussed in subsection 4.1, only the VQ answering results are shown for PaliGemma."}, {"title": "5.1 INITIAL HYPOTHESES", "content": "Hypothesis 1 (Including Image): In the SI-VQA dataset, images are essential for VQA tasks, as they serve as the only modality providing the necessary information to answer the questions. Therefore, we hypothesize a significant positive impact on accuracy and a reduction in model uncertainty when the image is incorporated alongside the question."}, {"title": "5.2 ANSWER & REASONING EVALUATION", "content": "Figure 2 presents the results of the VQA accuracy (a.) and the quality of the VLM VQ reasoning (b.), as judged by GPT-40. Similar patterns emerge across all models. The answer accuracy is low in the question-only baseline, where the model lacks sufficient information to provide correct answers. However, when given just a question and a black image, the models consistently offer strong reasoning quality, consistently justifying their response by acknowledging the absence of image information. Incorporating complementary context into the I+Q configuration enhances both answer accuracy and reasoning quality by providing additional details necessary for a correct response and a well-supported rationale. In contrast, the introduction of contradictory context significantly degrades response accuracy. The decline in accuracy is the smallest for PaliGemma, whereas for LLaVA, it drops to a level comparable to the question-only configuration. Additionally, reasoning quality declines as the models are misled by the conflicting information. When exchanging the natural image with an annotated image, we observe no change in accuracy or reasoning quality, even for architectures optimized for OCR.\n\nWhen comparing the VLM architectures, a notable discrepancy emerges in their handling of contradictory information, with models responding differently to contradictions (configuration Q+I+C_). Surprisingly, PaliGemma demonstrates the most robustness in managing contradictions and achieving the highest accuracy scores in five out of seven configurations, despite having less than half the parameters of the LLaVA models and not being explicitly fine-tuned for VQA tasks. LLaVA-NeXT ranks second in accuracy but does not fully leverage its enhanced OCR capabilities when the annotated image is included. In terms of reasoning abilities, the conversational fine-tuned VLMs produce substantially higher-quality reasoning compared to the standard LLaVA 1.5 model."}, {"title": "5.3 MODEL UNCERTAINTY", "content": "Figure 3 displays the model uncertainty measured through semantic entropy for both the answer (a.) and the reasoning (b.). For all models, the absence of image-based information (configuration Q) results in similar levels of uncertainty in both VQ answering and reasoning. In addition, we observe for all models that image text annotations have almost no impact on the model uncertainty compared to the configurations including the natural images.\n\nFor PaliGemma, adding image and context information significantly reduces uncertainty in VQ answering, making the model much more confident in its predictions. It seems that providing additional context, regardless of its content, leads the model to be more self-assured. This intriguing pattern shows large overconfidence in PaliGemma, which does not always have to be beneficial as it can, e.g., lead to silent failures, where the model is extremely confident in its wrong predictions (Bender et al., 2021; Jaeger et al., 2023).\n\nFor all LLaVA models, we observe overall an inverse relationship between answer uncertainty and reasoning uncertainty, with LLaVA 1.5 exhibiting the highest uncertainty in VQ answering but the lowest in VQ reasoning.\n\nWhen the image is added, LLaVA-Vicuna and LLaVA-NeXT show reduced uncertainty in VQ answering but increased uncertainty in VQ reasoning, as the models, in the question-only configuration, only acknowledge the absence of the image and therefore reason with high confidence. Complementary context slightly decreases model uncertainty, indicating a marginal increase in confidence for both VQ answering and reasoning. This effect is minor though, as shown in Figure 3 b. where all LLaVA models exhibit nearly identical semantic entropies for I+Q and Q+I+C+, as well as for Q+Ia and Q+I+C+. Contradictory contextual information, on the other hand, significantly increases uncertainty in the model answers. Its effect on reasoning is also particularly pronounced in LLaVA 1.5 but remains relatively minor for LLaVA-Vicuna and LLaVA-NeXT. Thus, the LLaVA models appear to be slightly influenced by reinforcing information sources but are more easily unsettled by contradictory ones."}, {"title": "Model Failure Detection through Semantic Entropy", "content": "Interpreting VLMs' behavior through the lense of model uncertainty is crucial for identifying and understanding failures, including hallucinations and silent failures. Specifically, for PaliGemma, silent failures cannot be dismissed due to the model's extreme overconfidence, despite its prediction accuracy being comparable to that of LLaVA models. To quantify this harmful overconfidence\u2014when the model confidently predicts incorrect answers with very low uncertainty-we employ the AUGRC metric, as detailed in Equation 2. Figure 4 displays the AUGRC values across all model architectures (where lower is better). Our results confirm that PaliGemma performs the worst, validating our hypothesis regarding its harmful overconfidence. Additionally, we observe that in cases of high uncertainty, such as with the Q+I+C_ and Q+IA+C_ configurations, AUGRC is low, indicating fewer silent failures. In these scenarios, the contradictory context reduces the likelihood of confident incorrect answers, meaning the models become more uncertain about their mistakes, thereby making them more trustworthy."}, {"title": "5.4 ATTENTION ATTRIBUTION", "content": "This section examines how attention is distributed across the three inputs-question, image, and context across the seven configurations. Figure 5 shows the average difference between attention to the image and the question, as well as the attention to the image and the context. Since all differences are positive, the image consistently receives the highest average attention in both VQ answering (Figure 5a.) and reasoning (Figure 5b.). The attention distribution across different inputs is similar among the models, with LLaVA-Vicuna showing the highest attention to textual inputs and LLaVA-NeXT focusing more on the image. Both answering and reasoning exhibit higher attention to the natural image compared to the black baseline image in the question-only configuration. Further, attention to the image decreases when context is added and the annotated image receives more attention than the natural image. In VQ answering, attention to the question and context is nearly equal, whereas, in VQ reasoning, the model shows significantly higher attention to the context, almost equal to the attention given to the image. Detailed figures are provided in Appendix subsection H.3. Overall, no strong correlation is observed between attention attribution and accuracy (see Appendix subsection H.4)."}, {"title": "5.5 REBALANCING MODALITY IMPORTANCE", "content": "Given the observed high attention allocated to the image modality, it raises the question of how the results might change if we intervene to direct more attention toward the text modalities. Specifically, we aim to investigate how the model's performance changes when either the information in the image is described with text or the attention of the model is guided toward the text via prompt engineering."}, {"title": "Textual Description of the Image", "content": "Does adding a text description of the image greatly improve model performance and confidence? We initially hypothesized that augmenting the model's input with a textual description of the image would enhance its accuracy and reduce uncertainty, based on the premise that key features necessary for answering the question-already present in the image would be more accessible to the LLM decoder in text form. However, as illustrated for LLaVA in Figure 6, the results reveal a surprising decrease in answer accuracy and an increase in model uncertainty for configuration I+Q+C+. We observe similar results for PaliGemma (see Appendix I), with minor increases in uncertainty. We argue that these findings indicate the models are already proficient at extracting essential information from the image alone and that the addition of textual information introduces confusion due to redundancy or potential inconsistencies in the image description. Moreover, the high attention allocated to the image description underscores the model's sensitivity to textual inputs, which may inadvertently dominate visual cues."}, {"title": "Prompt Engineering", "content": "Can prompt engineering help VLMs re-balance their attention toward the context? We modified the initial prompt to direct the model's attention more toward the textual context, which typically receives less emphasis in the standard setting (see Appendix B for implementation details). Given that the complementary context provides information intended to guide the model toward the correct answer, while the contradictory context aims to mislead it, we expected an increase in accuracy for the Q+I+C+ configuration and a decrease for the Q+I+C_ configuration. As shown in Figure 6 a, we observed a decrease in accuracy for Q+I+C_, whereas prompt engineering to emphasize the complementary context resulted in improved answer accuracy. Unexpectedly, these changes are not reflected in the attention attribution: Figure 6c. indicates that prompt engineering does not alter the attention distribution across modalities, as the context does not receive increased attention. This lack of correlation between attention and model performance highlights the necessity for cautious interpretation of attention mechanisms in model predictions (see section 2). Unlike the LLaVA results, PaliGemma exhibited a significant increase in uncertainty (see Appendix I), highlighting the large influence of the image in the standard inputs."}, {"title": "6 DISCUSSION & CONCLUSION", "content": "Our findings reveal notable insights into the role of each modality in VQA and reasoning tasks. Specifically, we compare all results with our initial hypotheses from subsection 5.1."}, {"title": "Evaluation of Hypotheses", "content": "Hypothesis 1 (Including Image): As expected, introducing the image results in a significant increase in answer accuracy across all VLMs. However, it unexpectedly leads to a decrease in reasoning quality, as in the question-only setting, the models simply acknowledge the absence of the image. We observe a similar pattern in model uncertainty: it decreases for VQ answering but increases in reasoning. As hypothesized, the natural image indeed receives more attention compared to the black baseline image.\n\nHypothesis 2 (Including Context): Consistent with our expectations, the inclusion of complementary context enhances both accuracy and reasoning quality, while contradictory context has a strongly negative effect. However, in VQ answering, the complementary context does not reduce model uncertainty, whereas the contradictory context significantly increases it. In VQA reasoning, contradictory context does not affect uncertainty, and complementary context only slightly decreases it. Generally, the impact of adding context is much stronger in the VQ answering than in the reasoning task. Interestingly, contradictory context can sometimes be beneficial, as it helps to minimize the occurrence of silent failures. Additionally, the models continue to show higher attention toward the image than the context, not supporting our prior hypothesis.\n\nHypothesis 3 (Including Image Annotations): Surprisingly, the image text annotations play a minimal role in enhancing model performance. Although the models exhibit increased attention toward annotated images, the positive impact of these annotations on performance metrics and uncertainty reduction is nearly negligible.\n\nWe also investigate methods to guide the model to favor one modality over another to observe the effect on VLM performance. While adding redundant textual information can overwhelm the model and decrease accuracy, prompt engineering can improve predictions without, however, strong changes in attention distribution."}, {"title": "Limitations", "content": "The SI-VQA dataset contains exactly 100 instances. Although relatively small, it has been meticulously handcrafted, with each instance carefully designed to meet key criteria: the question can only be answered using the image; the context provides additional global information either reinforcing the image's consistency or misleading the model; and the annotations are simplified concepts written directly on the image to aid in accurately interpreting the scene. In this work, we also limit our study to seven different modality configurations with specific interventions. For those interested in more specific or advanced interventions, we refer to our ISI Tool, which allows testing of almost all possible forms of interventions. In future research, it would be interesting to replicate this study in the reverse scenario-where text is the primary content required for answering, and the image serves as contextual information-to compare whether similar effects of primary and secondary modalities are observed. We employ semantic entropy as our unique uncertainty measure, considering other measures for free-form text generation, such as token entropy (Kadavath et al., 2022; Lindley, 1956) or self-expressed uncertainty (Lin et al., 2022; Liao & Wortman Vaughan, 2024), to be significantly less suitable. They either only consider local, token-level uncertainty or rely on the model's potentially biased self-assessment, neither adequately reflecting the overall semantic uncertainty. We validated the expected behavior of semantic entropy across the different configurations using the AUGRC metric in Appendix G."}, {"title": "Conclusion", "content": "This study is the first to systematically examine the role of contextual information in VQA, evaluating the results based on diverse metrics and distinguishing between answering and reasoning tasks. Leveraging the well-curated SI-VQA dataset and the ISI tool-our interactive, ready-to-use interface\u2014our work aims to provide a deeper understanding of VLM behavior and the influence of each modality. Moreover, we show that our results can also be used to understand and detect model failure in free-form text generation, and setting the stage for future analyses of modality integration across various VLM tasks and the development of VL datasets tailored for this objective."}, {"title": "A MODALITY ATTENTION AGGREGATION", "content": "From the VLMs, we extract an attention matrix $A_t \\in \\mathbb{R}^{t \\times t}$ for each output token $t$, where the matrix size grows with the number of predicted tokens. Each row represents a token as a query, and each column corresponds to a token as a key. Therefore, each row $r$ contains the attention coefficients of each token $i$ with respect to the token $r$: $A[r, i] = A_{r,i} = q_r \\cdot k_i$. For each token $t$, we focus only on the attention of preceding tokens, represented by $v = A[-,1] \\in \\mathbb{R}^{1 \\cdot t}$. $v^t$ is normalized so that the attention coefficients of all preceding tokens to $t$ sum to 1.\n\nThis process is repeated for all output tokens $t \\in [1,T]$ where $T$ is the output length, yielding all normalized attention vectors $v^t, t \\in [1,T]$. These are averaged to produce the final attention vector for the VLM output-$V_A \\in \\mathbb{R}^{N_0}$ for answers and $V_R \\in \\mathbb{R}^{N_0+T_A+N_1}$ for reasoning. Here, $N_0$ is the size of the input prompt including image tokens, question, and eventually context, $T_A$ is the answer length, and $N_1$ is the size of the second prompt asking for an explanation.\n\nTo calculate the attention given to different modalities, we sum the attention coefficients based on the token positions in the averaged attention vector $V_A$ or $V_R$, resulting in normalized relevance scores: $R_I$ for the image, $R_Q$ for the question, and $R_C$ for the context, which sum to 1, i.e., $R_I + R_Q + R_C = 1$. This is done by adding hooks into the LLaVA architecture to capture the start and end positions of the question, image, and context tokens. Due to dynamic high resolution, the number of image tokens can vary significantly even with minimal image perturbations."}, {"title": "B HYPERPARAMETER AND EVALUATION PROMPTS", "content": "In reasoning quality evalution, GPT-40 is prompted to rate the reasoning from 0 to 10, using the prompt: \u201cRate the explanation's quality from 0 to 10. Give 10 for detailed, well-argued, and correct explanations. Give 0 for a poorly reasoned, wrong, or single-word explanation based on the question and image. Don't rate too harshly, use the full scale and output only the final score\". During uncertainty computation, the number of the sampled outputs and the sampling temperaturee T are set to 10 and 0.9 respectivly. We use conditional probabilistic sampling. See Appendix A for the attention attribution implementation.\n\nIn the prompt engineering ablation study in subsection 5.5 we use the prompt \"Answer the question only with Yes or No. Answer based on the context provided in the text.\" for the answer, followed by \"Explain your answer based on the context provided in the text:\" for reasoning."}, {"title": "CSI-VQA DATASET BASELINE", "content": "In the baseline configuration of the SI-VQA Dataset, the image tokens carry no meaningful information. We experiment with several methods to remove image data: (1) replacing the image with black pixels, (2) adding random"}, {"title": "F4BIT QUANTIZATION", "content": "To quantify the effect of different model sizes, we additionally perform all experiments also with the same models but 4Bit quantized, as there are no, e.g., 3B parameter LLaVA versions. Figure 8 shows the difference in results for all five experiments between the 32Bit and 4Bit models. To our surprise, the difference in accuracy is not that large. Results for the question-only configuration are not meaningful as both models randomly guess. However, in terms of model uncertainty, the 32Bit model usually scores better. Mean differences in attention distribution are almost neglectable as they are at a maximum of 0.01 percentage points. The results show that for simple VQA, quantized models can achieve almost similar accuracy than their significantly larger 32Bit counterparts."}, {"title": "G VALIDATING SEMANTIC ENTROPY", "content": "Any assumptions made on the uncertainty of the models should be reflected in the AUGRC. We observe in Figure 4 that in the case of contradicting image and context (Q+I+C_) the AUGRC goes down for all models, reducing the overconfidence in wrong classified samples, which is correctly captured by the semantic entropy. As the model makes more mistakes in configuration Q+I+C_, the set size of wrong classified samples is larger. In the case of Q+I or Q+I+C+ the AUGRC rises again as there is no confidence reducing context. Additionally, the accuracy is higher in the case of Q+I+C+, reducing the set size of wrong classified samples. This empirical evaluation shows we can use the AUGRC to quantify and evaluate the performance of semantic entropy for model failure detection. To our knowledge, this is the first time semantic entropy has been evaluated for failure detection."}, {"title": "E ISI TOOL", "content": "The interactive tool can be used to analyze VLMs with the provided SI-VQA Dataset and follows a main pipeline that consists of three main steps: 1) Data & Model Selection 2) Interventions on Image, Context, and Question, and 3) Evaluation. Figure 7 gives an overview of this pipeline."}, {"title": "E.1 GENERAL INFORMATION", "content": "The application is catered toward researchers, developers, and other users with a basic understanding of VLMs, who are interested in interpreting model behavior through semantic interventions on VLMs. By enabling fast-paced iterations in a human-in-the-loop scenario, it allows the building of intuitions before scaling experiments in large-scale projects."}, {"title": "System Requirements", "content": "ISI for VLMs is an interactive tool embedded in a locally hosted web application requiring a computer with sufficient VRAM for VLM inference. The minimum required VRAM for a 4bit-quantized LLAVA 7B model is around 8GB while LLaVA-Vicuna and LLaVA-Next require 12GB. The computation of the semantic entropy with the DeBERTa model requires an additional 7GB. The exact amount of VRAM depends on the amount of input tokens."}, {"title": "E.2 INTERACTIVE SEMANTIC INTERVENTION PIPELINE", "content": "Data & Model Selection: As the first step, a user either chooses an observation from the SI-VQA Dataset or uploads their custom image. Each observation from the dataset includes an image, corresponding context, and a question with a ground truth answer, as well as the presets for the annotated images and contradictory and complementary context. The corresponding image, context, and question are displayed. In the next step, the user selects a VLM (LLaVA, LLaVA-Vicuna, LLaVA-Next) and the number of parameters (7B, 11B, 32B) in two separate drop-down menus. 4-bit quantization can be enabled to reduce the computational load and VRAM requirements.\n\nInterventions on the Image: For interventions on the image, ISI allows the user two main functionalities. First, on the proposed SI-VQA Dataset the user can select for each observation three different image presets (natural image without modifications, annotated image with hand-crafted annotations, and random natural image from the dataset) by selecting the respective buttons. Second, ISI allows perturbing the image directly in the tool by overlaying boxes"}, {}]}