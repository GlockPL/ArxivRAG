{"title": "SimClone: Detecting Tabular Data Clones using Value Similarity", "authors": ["XU YANG", "GOPI KRISHNAN RAJBAHADUR", "DAYI LIN", "SHAOWEI WANG", "ZHEN MING (JACK) JIANG"], "abstract": "Data clones are defined as multiple copies of the same data among datasets. Presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. Majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this paper, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20% in terms of both F1-score and AUC. In addition, SimClone's visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.", "sections": [{"title": "1 INTRODUCTION", "content": "Datasets form a key component in the development of Artificial Intelligence (AI) software, whose adoption and commercialization have increased significantly in the past decade [68]. These datasets are commonly created by combining multiple datasets, scraping several sources, or merging different tables in a data warehouse [10, 63]. For example, CIFAR-10, a commonly used dataset to build AI software, was created by extracting a subset of images from the 80 Million Tiny Images dataset, which was in turn created from several different data sources including Google Images, Flickr, and Ask [47]. Such practices create data clones among datasets."}, {"title": "2 DEFINITION OF DATA CLONE", "content": "In previous studies, the concept of data clone is defined mostly in the context of spreadsheets, as cell blocks that are created by directly copy-pasting values or computational semantics (i.e., formula) [23, 37]. The main motivation behind defining data clones in such a context is that users may introduce errors in the data when copy-pasting cell blocks, especially when the cells contain formulas. However, such definitions do not cover issues specific to the usage of data in AI software development (e.g., traceability, provenance, and data license conformance). In particular, most tabular datasets used for machine learning are plain values without formulas. In addition, a data license may also prohibit certain usages of any derivatives from the original data (i.e., data transformation), which is beyond the scope"}, {"title": "3 RELATED WORK", "content": "In this section, we discuss related work to our study."}, {"title": "3.1 Data quality issues", "content": "Modern artificial intelligence (AI) applications require a large amount of training and test data, which creates critical challenges not only concerning the availability of such data, but also regarding its quality. For example, data quality related to incomplete, inconsistent, dated, duplicated, or inappropriate training data can lead to unreliable models that produce ultimately poor decisions [9, 32, 41, 85]. There are remarkable efforts that have been invested in improving the data quality from various aspects [21, 28, 30, 59]. For instance, to improve the data consistency and accuracy, Gao et al. [21] proposed an approach to identify the data consistency and accuracy by using conditional functional dependencies and also developed algorithms to fix the data quality issues. Pleiss et al. [59] proposed a method to identify mislabeled data and mitigate their impact when training neural networks by using the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples.\nData duplication is the most related data quality issue to our study. If certain types of data are overrepresented due to duplication, the resultant models may exhibit skewed predictions or decisions, favoring the duplicated data at the expense of underrepresented or minority groups [4]. Data deduplication is widely researched in the data management community. Numerous approaches for data deduplication have been proposed. As summarized by Ilyas et al. [39], data deduplication methods can be classified into two categories: unsupervised and supervised. Unsupervised methods, such as those based on a pre-specified threshold [18, 55], or domain-specific rules [22, 38, 76], do not rely on labeled clone pairs. On the contrary, supervised methods such as Naive Bayes [77], decision trees [17], or support vector machines [13] require labeled clone pairs to train models. In recent years, there has been a growing interest in the role of humans in the data deduplication process to improve the accuracy of automatic data deduplication. Wang et al. proposed CrowdER [75], which only sends high-probability clone pairs identified by machine learning models to be"}, {"title": "3.2 Data Clone Detection in Spreadsheets", "content": "Spreadsheets are one format of tabular datasets. Prior work on clones in tabular datasets mostly focuses on copy-pasting across spreadsheets, because such action can introduce errors especially when formulas are presented. Hermans et al. [37] showed that data clones are common among spreadsheets and pose a threat to spreadsheet quality, and proposed an algorithm to identify and resolve clones. Dou et al. [23] proposed TableCheck, a tool that detects spreadsheet data clones based on the observation that two tables with the same header information are likely to be cloned. Zhang et al. [83] proposed LTC (Learning to detect Table Clones), which uses information such as row header name, font type, and font color, among other format features to detect spreadsheet data clones.\nAs discussed in Section 2, tabular datasets that are used for Al software development do not contain most format information. In addition, many existing methods (e.g., TableCheck, LTC) cannot identify cloned data blocks that have gone through simple transformations such as reshuffled row and column orders (i.e., Type 3 data clone). Therefore, existing spreadsheet data clone detection methods cannot be directly applied in the context of Al software development."}, {"title": "3.3 Detecting Clones and Duplicates in other Software Artifacts", "content": "Clone as a concept has been extensively studied in the software engineering community for various artifact types, such as code clones and bug report duplicates. For instance, many methods have been proposed for code clone detection, including lexical analysis, Abstract Syntax Tree (AST) comparison, metric-based, program slicing, and machine learning [2, 46, 66]. In addition, prior work has also explored detecting clones in images [60, 74].\nRecently, with the rise of pre-trained models and their state-of-the-art performance in various software engineering (SE) tasks, they have been increasingly applied to detect clones and duplicates in software artifacts [27, 33, 40, 72]. For example, Feng et al. introduced CodeBERT, a pre-trained model based on the BERT architecture specifically trained on source code for code-related tasks [27]. They demonstrated its effectiveness in capturing source code semantics and measuring their similarity in code detection tasks. Similarly, Guo et al. introduced Graphcodebert [33], which incorporates structural information (e.g., data flow) in training and achieves state-of-the-art performance in code clone detection. Isotani et al. utilized BERT to detect duplicate bug reports [40].\nHowever, due to the nature of tabular data, which consists of a mixture of numeric and string values, directly applying such pre-trained models is challenging. For instance, it is difficult to directly apply a pre-trained model like BERT to measure the similarity of two sets of numeric values. Therefore, we propose our value-based similarity approach to individually measure the similarity between string and numeric values. This approach enables more effective comparison and identification of similarities within tabular data."}, {"title": "4 METHODOLOGY", "content": "In this section, we introduce our proposed value-based Similarity for data Clone Detection (SimClone) method in detail.\nWe define the task of data clone detection as follows: Given a set of n datasets $Set_{dataset}$ = {$D_1$..$D_n$}, we aim to find all dataset pairs ($D_i$, $D_j$) \u2208 $Set_{dataset}$ that contain data clones (referred to as ClonePair) and indicate the location of data clone in each ClonePair. For simplicity, we use dataset and tabular dataset exchangeably in the following sections.\nHowever, identifying data clones is a challenging task. There are several challenges for data clone detection. Firstly, we focus on Type-1, Type-2, and Type-3 clones. There could be various forms and sizes of clones in each clone type. For instance, Type-3 data clones focus on simple transformation of data records or data items, such as adding/dropping a certain amount of columns or rows in tabular data, or switching the order of data. There are infinite ways to transform the tabular data from one form into another form with those simple transformations. In addition, the clone could be either text, numerical values, or their combination, we need to design different features to capture their characteristics and measure their similarity. Therefore, it is challenging to manually craft hard rules (e.g., deciding the threshold for similarity, assigning weight for different features, etc.) to identify data clones with various transformations and forms. Even though manually crafting hard rules is possible, it is a labor-intensive process. Second, even if a pair of tabular data is identified as a ClonePair, with enormous data points in the pair, it is still a challenging job to identify the exact location of the cloned data. Therefore, to address the above challenges, we propose SimClone to identify ClonePairs by learning a classifier automatically from data clones and visualize the exact location of cloned data within a ClonePair to help practitioners locate the clone data. Training a classifier requires a large amount of data, however, no existing data clone dataset could leveraged for this purpose. Therefore, We first create a synthetic dataset where we artificially inject Type-1 and Type-3 data clones. We then pair every dataset with every other dataset in the synthetic dataset and label which pairs are ClonePairs among them. We derive value similarity-based features between each dataset pair and train a binary classifier on this synthetic dataset to identify ClonePairs. We leverage this trained classifier to identify ClonePairs among a given set of dataset pairs. In addition, we develop a visualization approach to indicate the exact location of the cloned data within a ClonePair. Thus, for SimClone, the input is a pair of two datasets and the output is a binary classification result (e.g. whether the given dataset pair is a ClonePair), plus a visualization result indicating the location of the data clone if the dataset pair is a ClonePair."}, {"title": "4.1 Synthetic dataset creation", "content": "For our SimClone method, we need a dataset comprised of tabular dataset pairs with labels indicating if they are a ClonePair. However, as far as we know, such a dataset with labeled ClonePairs is not readily available. Hence, we create a collection of synthetic tabular datasets that contain data clones for our experiment. We then create dataset pairs from these clone injected datasets and label the dataset pair as either ClonePair or not. Towards this end, we first collect tabular datasets that are commonly used in machine learning applications. Then we inject Type-1 and Type-3 data clones into them. Finally, we create the dataset pairs and label them. We explain each step in detail below.\nStep 1: Dataset collection. We select the tabular datasets from the UCI Machine Learning dataset repository [25] for creating our synthetic dataset. This repository contains 439 tabular machine learning datasets for various machine learning tasks (e.g., classification, regression, and clustering). It is commonly used in research as a source of benchmark datasets for evaluating and comparing different machine learning algorithms [3, 58, 82]. For our experiment, we select 154 datasets out of the 439 datasets by using the following three criteria: 1) a dataset must have more than 5 rows, to ensure that the dataset is large enough to be useful for machine learning tasks. 2) a dataset must have more than 2 columns, to ensure that the dataset has enough features or attributes to be informative for machine learning tasks. 3) a dataset must be parseable by the Python library Pandas [57] to enable ease of experimentation. To reduce processing time and memory usage, we truncate the datasets to have a maximum of 4000 rows and 60 columns (mean number of rows and columns across the selected 154 datasets).\nStep 2: ClonePair generation. This step is comprised of three sub-steps. First, we create dataset pairs and then we inject them with either Type-1 or Type-3 clones depending on the type of dataset pair that we create. Finally, we label the dataset pairs as either ClonePair or not. Algorithm 1 shows our overall approach for injecting data clones and labeling ClonePairs.\nStep 2.1: Dataset pair creation. We take the 154 datasets, and create pairs among them as shown in lines 1 and 2 of the Algorithm 1. This creates two types of dataset pairs. First, identical pairs where a dataset A is paired with itself and non-identical pairs where dataset A is paired with other datasets in the list. This step would create 154 identical pairs and 11,781 non-identical pairs.\nStep 2.2: Clone injection. We inject Type-1 clones on identical pairs (Lines 7-8) and Type-3 clones on the non-identical pairs (Lines 9-10) using the following operations.\n(1) Type-1_injection(A, p): Figure 2 presents an outline of Type-1_injection operation. Given an identical pair (e.g., (A,A)) only one instance of the dataset A is passed on to the operation. For the dataset A, we randomly sample p% of all consecutive columns/rows (i.e., exact clone) from A and create a new dataset B using sampled columns/row. Therefore, we generate a data pair A and B with Type-1 clone. Note that columns and rows are randomly sampled"}, {"title": "4.2 Similarity computation", "content": "Prior techniques (e.g., LTC [83] and TableCheck [23]) that are designed for table clone detection on spreadsheets do not work effectively on tabular datasets, which usually are stored in formats like CSV which do not contain formatting attributes such as cell formula, font size, and font color. Therefore, we need to design features that are value-oriented and can capture the similarity between two columns or two rows based on their cell values. Towards this end, we design various metrics to measure the similarity of two lists of values for different data types (i.e., numeric and string). Before diving into the calculation of features, we first introduce the similarity metrics we used in SimClone below."}, {"title": "4.2.1 Value-based similarity metrics.", "content": "Measuring the similarity of two rows or columns is essentially measuring the similarity of two lists of cell values from various aspects. Tabular datasets could have two types of values, namely numeric and string. In general, we have two criteria when selecting metrics: 1) computationally efficient; and 2) effective in capturing the similarity between strings or numbers. For string type, metrics are selected to capture the text similarity between two lists of strings. We assume that two clone lists of strings should have a portion of common words and share similar semantics. Therefore, we first select the following metrics, Jarcard, Levenshtein, and Simhash by following previous studies [35, 56]. Simhash is an efficient approach to estimating how similar two sets are by projecting the string into a hashing code [16]. Two strings sharing the same hashing code are considered similar. Levenshtein is a traditional way to measure two strings' similarity [50], which is used to measure the minimum efforts of converting one string to another from the character level. Jaccard is widely used to measure the word overlap of two strings [56].\nHowever, all the above three metrics are sensitive to long strings. In our case, the size of data clone is uncertain (which could be long). To complement the above metrics, we select TextRank to identify the important words from two lists first, then calculate their similarity based on the overlap in the important words. For numeric type, we assume that two clone numeric lists should have the same/similar distribution. Therefore, we calculate the similarity of two lists of numeric values by measuring the similarity of mean and deviation between two lists of numeric values, i.e., Mean and Deviation Similarity. We select mean and deviation since they are computationally efficient and are widely used"}, {"title": "4.2.2 Similarity matrices calculation.", "content": "Our aim in this step is to calculate the similarity values between the dataset pairs that we created in Section 4.1 to train data clone detection classifier. We choose to calculate the similarity at the granularity of the row and column level (i.e., calculate the similarity between each pair of rows/columns in the paired datasets) instead of at the cell level (i.e., where each cell in a dataset is compared to all other cells of the other dataset) or tabular level (i.e., the two tables are compared against each other based on the table properties) for the following reasons: 1) Calculating similarity at the tabular level might be too coarse-grained. Also, at the tabular level, comparisons are typically unable to capture row/column characteristics. For example, suppose only a small portion of data is cloned in two tables, while the majority of data is different from each other. The contribution of the small"}, {"title": "4.3 Feature generation", "content": "In this stage, we utilize pooling technique to process the similarity matrices generated in the previous step 4.2.2 and convert them into features that are used by a machine learning classifier. The pooling technique extracts relevant information from a system's output and converts it into a uniform, usable format (e.g., consistently picking the top 10 values in a matrix and flattening a matrix). This concept is widely used in both neural networks and other domains like computer vision and graph theory [84]. We use pooling technique since the size of the resultant similarity matrices varies among dataset pairs with different sizes of columns and rows as shown in Section 4.2.2. We leverage the pooling technique to obtain a uniform output, with a fixed size, that can be used as the input for the classifier in the following step. We do so to ensure that we can process all dataset pairs in a consistent format.\nIn SimClone, we design a pooling calculation called \"Mean Top K\", which extracts the K largest values from a given similarity matrix M and calculates their mean value as the pooling value. Each value in the similarity matrix represents the similarity between two rows (or columns), and most of the values in the similarity matrix are very small. Therefore we hypothesize that large similarity values are more likely to signal the presence of clones and we only need to pay"}, {"title": "4.4 Data clone detection classifier construction and inference", "content": "After obtaining the unified feature vector representation for all the dataset pairs, we construct a binary classifier to predict if a dataset pair is a ClonePair. We train our classifier using a dataset of labeled dataset pairs to predict the likelihood of a given dataset pair being ClonePair. After pooling, the feature can be used to train all kinds of machine learning classifiers, in this study, we use Random Forest (RF) [12], XGBoost [19], CatBoost [61], and LightBGM [43] as our classifiers, since they are widely used and achieved competitive performance for classification tasks [11, 70, 83].\nIn the inference phase, given a new collection of datasets, we create data pairs, calculate similarity metrics, and generate features as described above. We then use our trained classifiers to predict if any given dataset pair is a ClonePair."}, {"title": "4.5 Data clone visualization", "content": "In this step, we describe how we create our visualization approach to help the users of SimClone identify the exact location of data clone in a ClonePair. Such a visualization is particularly useful when the datasets in the ClonePair contain a significant number of data points. We present an overview of our visualization approach in the visualization phase part of Figure 5."}, {"title": "5 EXPERIMENT DESIGN", "content": "In this section, we first introduce the research questions. Then, we explain how we collect the datasets, build our baselines, and how we answer each research question in detail."}, {"title": "5.1 Research Questions", "content": "We formulate the following research questions to evaluate SimClone from various aspects:\n\u2022 RQ1: How effective is SimClone in identifying ClonePairs compared to the state-of-the-art (SOTA) baselines?\n\u2022 RQ2: How effective is SimClone's visualization approach?\n\u2022 RQ3: Which similarity metrics make the most contributions towards the effectiveness of SimClone?\n\u2022 RQ4: How does SimClone's effectiveness change with different threshold t?"}, {"title": "5.2 Baseline for identifying CloneParis", "content": "As far as we are aware, we are the first to propose a data clone detection method specifically for tabular datasets used in machine learning. Hence, we have chosen to use table clone detection methods, which were originally intended for data clone detection in spreadsheets, as our baseline. We do so as they are the most comparable method to our method. The LTC approach [83] is currently considered SOTA in table clone detection in spreadsheets. However, LTC was developed for spreadsheets, and it leverages formatting features like font, and cell color. We do not have such information in our dataset. To adopt LTC for our dataset we drop the format features like font size, font color (which LTC method uses), and only use the column header, row header, and cell type features to build LTC. Since the code for LTC is not publicly available, we implemented our own version in Python based on their paper."}, {"title": "5.3 Data preparation and evaluation", "content": "To evaluate SimClone's effectiveness in identifying ClonePairs, we prepare two datasets, the synthetic dataset and the real-world dataset. We elaborate on how we construct the datasets and evaluate them below."}, {"title": "5.3.1 Synthetic dataset.", "content": "we first evaluate SimClone on the Synthetic dataset that we constructed based on UCI repository (as we introduce in Section 4.1) and compare it to the baseline.\nWe compute five commonly used classification evaluation metrics: accuracy, F1-score, precision, recall, and Area Under the Curve (AUC) for both methods on all the folds. We chose these metrics since they are commonly used by prior studies to evaluate a classifier's performance [64, 80, 83]. We report the average of the performance scores for each of the studied evaluation metrics for all the synthetic datasets generated for the studied thresholds across the 10 folds."}, {"title": "5.3.2 Real-world dataset.", "content": "To complement our evaluation of the synthetic dataset, we also conduct experiments on real-world spreadsheets that we obtained from the EUSES Spreadsheet corpus [29] and Enron corpus [36]. EUSES corpus contains spreadsheets collected from the internet pertaining to various domains [29] and is made up of over 4,000 real-world spreadsheets. The Enron corpus was collected from the Enron email archive within the Enron corpus, and contains more than 15,000 spreadsheets. We explain the process of creating the real-world evaluation datasets below."}, {"title": "Step 1: Real-world dataset collection.", "content": "Several prior studies have noted that tables contained in the spreadsheets available in the EUSES and Enron corpus have clones and have used it in their study to evaluate their data clone detection methods [23, 37, 79]. Therefore, we use the tables contained in the spreadsheets available in the corpus to evaluate SimClone's detection performance and compare it with LTC's detection performance.\nBoth corpus contains spreadsheets that can contain multiple tables in one spreadsheet. For our SimClone method, each dataset is comprised of only one table. So we first parse the spreadsheets available in the corpus to extract the tables and create our real-world dataset. We do so through a Java tool that we developed using the Apache POI library\u00b9 to extract the tables from the spreadsheets. Our tool processed the corpus and generated 17871 tabular datasets for EUSES and 67725 tabular datasets for Enron. Then we select datasets by following three criteria: 1) a table must have more than 2 rows and 2 columns. 2) we only keep the table with a unique set of column headers to avoid the obvious influx of Type-1 clone. It is important to note that this process resulted in the loss of formatting information, such as font size and color, in the EUSES tables in csv format compared to their original Excel format. We ended up with 1,182 datasets for EUSES and 8,184 datasets for Enron. We further randomly sampled 1,000 datasets among the 8,184 datasets for Enron since using full datasets requires too many computation resources. Table4 provides some basic details about the EUSES dataset."}, {"title": "Step 2: Real-world dataset pair generation.", "content": "We start by creating dataset pairs for the tabular datasets. We end up with a total of 69.8k dataset pairs for EUSES and 49.95k dataset pairs for Enron. Note that we do not inject any data clone into those dataset pairs since we aim to detect if SimClone can detect any data clones in real-world data."}, {"title": "Step 3: Detection performance evaluation.", "content": "Note that we do not have labels for real-world dataset pairs\u00b2 and it is impossible to label all pairs. Therefore, we use both SimClone and LTC that are trained from the synthetic dataset on all dataset pairs to identify the ClonePairs. We then collect the top 200 ClonePairs returned by both SimClone and LTC (sorted by the probability score provided by SimClone and LTC) for each corpus. Finally, we check every pair manually (400 pairs in total, 200 ClonePairs returned by each method) and check if it indeed is a ClonePair. Note that we consider a dataset pair to be a ClonePair if the two datasets have at least one identical column or one row of data. Two first authors manually label the returned 400 ClonePairs independently to evaluate if they were indeed ClonePairs. After that, the two authors discussed the results to resolve any disagreements until a consensus was reached. The labeling process has a Cohen's kappa of 0.89 for LTC and 0.72 for SimClone, which indicates a substantial level of the inter-rater agreement [73].\nWe calculate the Precision@K (short for P@K) with different K (i.e., 5, 10, 20, 50, 100, 200) to evaluate the effectiveness of SimClone and LTC on their respectively labeled ClonePairs. We calculate Precision@K using $Precision@K = \\frac{True \\ clone \\ pairs}{Top \\ k \\ returned \\ pairs}$"}, {"title": "5.4 Approach for RQ1: Effectiveness of SimClone", "content": "We evaluate the effectiveness of SimClone on the synthetic and compared its performance with LTC using the metrics introduced in Section 5.3 Note that in this RQ, we set the threshold t to 10%. In other words, we train SimClone and LTC methods on the synthetic dataset using a threshold of 10% and test them on both Synthetic and real-world datasets. We do this, since we want to use our SimClone method trained on the synthetic dataset where the ClonePairs are labeled based on what we assume might be the most realistic threshold. We evaluate SimClone on the synthetic dataset with all studied classifiers."}, {"title": "5.5 Approach for RQ2: Effectiveness of SimClone's Visualization Approach", "content": "In Section 4.5, we explained that the Similarity matrices by themselves might be a good indicator of the location of the data clone in a ClonePair. As far as we know, our visualization approach is the first work on this, therefore, we construct a baseline by combining all the similarity matrices using equivalent weights instead of weighing them by the feature importance scores computed from the classifier for a given dataset pair as our baseline.\nTo evaluate the effectiveness of our SimClone method's visualization results, we generate a heatmap using our SimClone method on all true positive pairs (i.e., the pairs that are correctly identified as ClonePair) identified by our SimClone method on one of the 10 folds that we randomly chose. The darker the color of an area in the heatmap, the higher the likelihood of that area being a clone. To quantitatively measure the accuracy of our proposed approach, we use popular metrics precision@K (P@K), where K we set as 1, 5, and 10. Precision@K has been widely used in similar software engineering localization tasks, such as bug/fault localization [62, 69] and vulnerability detection [51, 81]. Our classifier returns ClonePairs with likelihood, which indicates the classifier's confidence in the prediction results. We examine the effectiveness of our visualization approach on the predictions with different confidences. More specifically, we examine the top 20, 50, 100, and all prediction results by the classifiers. We compare the results provided by SimClone's visualization and those provided by the baseline.\nWe do this evaluation on the synthetic dataset since we precisely know where the clones in a ClonePair are located and we do not have such information about the real-world dataset. We use the threshold of 10% and use the Random Forest as our classifier, since Random Forest performs the best among all studied classifiers."}, {"title": "5.6 Approach for RQ3: Effectiveness of Different Similarity Metrics", "content": "SimClone uses six different similarity metrics. Even though we calculate similarities at row and column level instead of cell level to reduce SimClone's time complexity, such calculation can still be expensive. For n datasets, similarity metrics of ${n \\choose 2}$ pairs need to be calculated. In previous sections, we examined the effectiveness of the metrics as a bundle. However, each metric may not contribute equally to the classifier. Therefore, identifying the most effective similarity metrics and reducing the number of similarity metrics can significantly improve the efficiency of SimClone. To identify the metrics that contribute the most towards the performance and cost the least amount of time we performed ablation experiments on SimClone. In particular, we compared the performance of classifiers using each similarity metric alone. We also recorded the time spent on calculating similarity metrics. Same as RQ2, we use the SimClone that we trained on the synthetic dataset with the threshold of 10% to evaluate the performance of the visualization method and use the Random Forest as our classifier."}, {"title": "5.7 Approach for RQ4: Impact of Different Threshold t", "content": "As we mention earlier in Section 4.1, we label a dataset as ClonePair or not based on a given threshold t. However, there is no universally accepted threshold for determining what percentage of similarity constitutes to make a dataset pair to be a ClonePair. The threshold for determining clones can vary depending on the specific application or use case. For example, it is not clear whether the presence of three consecutive cells or whether ten percent of the data in a row being"}, {"title": "6 RESULTS", "content": "Results of RQ1: Effectiveness of SimClone\nSimClone conclusively outperforms LTC at detecting ClonePairs across all the studied metrics with a 0.851 F1-score and 0.923 AUC on the synthetic dataset. Table 5 presents the results of SimClone and LTC in terms of the studied evaluation metrics on the synthetic dataset. SimClone consistently outperforms LTC across all classifiers. For instance, SimClone achieves an AUC of 0.923 and F1-score of 0.851. When comparing the effectiveness of different classifiers, we notice that Random Forest performs the best compared to other classifiers across all studied metrics for SimClone, while for LTC, XGBoost performs the best. If we compare the best version of SimClone which uses RF with the best version of LTC which uses XGBoost, SimClone achieves an improvement of 26.1% in terms of F1.\nSimClone outperforms LTC at identifying ClonePairs on real-world dataset on both dataset in terms of Precision@K across all the studied values of K, except K = 10 on Enron. Table 6 presents the results of SimClone and LTC on the real-world datasets (i.e., EUSES and Enron) by examining the top 200 results. On EUSES, SimClone consistently outperforms LTC across all values of K. For instance, We can see that Simclone still achieves a precision of 0.72, 0.57, and 0.44, even when examining the top 50, 100, and 200 returned pairs on real-world dataset in contrast to LTC, by achieving an improvement at least by 100%. On Enron, we observe that in general SimClone outperforms LTC as well. However, the improvement margin is not as large as EUSES. Typically for Precision@10, LTC performs better than SimClone. The reason is that in Enron, a remarkable number of clone pairs returned by LTC share the same header name, therefore, LTC can leverage such information to identify clone pairs accurately, while SimClone does not leverage header information. However, it is worth noting that structural or formatting information such as row or column headers and formulas are not available in tabular datasets that are used in AI software development. The strength of SimClone method can be further reinforced by considering the example"}, {"title": "6.2 Results of RQ2: Effectiveness of SimClone's Visualization Approach", "content": "Our SimClone method's visualization outperforms the baseline method at least by 45% on all true ClonePairs in terms of Precision@K for all studied values of K. Table 7 compares our SimClone's visualization method's results with the baseline in terms P@K when examining different numbers of top predicted true ClonePairs. In general, SimClone's visualization method outperforms baselines across all evaluation metrics. Figure9 shows an example where using SimClone method's visualization presents an advantage over using the baseline visualization method. We observe that due to the weighting of similarity values using the feature importance scores, we are able to avoid false positives whereas the baseline visualization method falls prey to them.\nOur visualization is more reliable for the predictions where the SimClone is confident. If we compare the performance of our approach across the different number of top predictions (given by sorting the true positive ClonePairs based on the probability given by SimClone), P@1 drops from 0.8 to 0.45 from the top 20 to all. The same trend can be observed for P@5 and P@10. This observation indicates that the visualization approach works more effectively when SimClone has higher confidence in its prediction. This observation is reasonable since our heatmap is based on the similarity matrices, which are in turn used as the feature after pooling. Better features result in a more discriminating classifier and hence predictions with more confidence."}, {"title": "6.3 Results of RQ3: Effectiveness of Different Similarity Metrics", "content": "Table 8 shows the result of the ablation analysis. We observed that when only Jaccard is used, the computation time could be reduced by 82%, while F1-score suffers a slight drop from 0.83 to 0.77, compared to when all metrics are used. Among similarity metrics that only apply to strings, Textrank achieves better performance than Simhash and Levenshtein across all performance metrics, while having a relatively low computation time (0.158x). Both Mean and SD take very little time (<0.1x) to calculate. Therefore, we trained a lite version of SimClone (i.e., SimClone Lite) with a combination of Jaccard, Textrank, Mean, and SD. The result shows that we can shorten calculation time by 46% and only lose 2% of F1."}]}