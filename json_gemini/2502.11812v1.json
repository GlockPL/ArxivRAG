{"title": "TOWARDS UNDERSTANDING FINE-TUNING MECHANISMS OF LLMS VIA CIRCUIT ANALYSIS", "authors": ["Xu Wang", "Yan Hu", "Wenyu Du", "Reynold Cheng", "Benyou Wang", "Difan Zou"], "abstract": "Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies (Prakash et al., 2024; Chhabra et al., 2024) that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work (Prakash et al., 2024; Chhabra et al., 2024) that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.", "sections": [{"title": "1 INTRODUCTION", "content": "Mechanistic Interpretability (MI) has become a powerful approach for exploring the inner workings of machine learning models, particularly Large Language Models (LLMs) Rai et al. (2024). It provides valuable insights into how information flows and transforms across different layers Ferrando et al. (2024). One of the most critical aspects of deploying LLMs in real-world scenarios is fine-tuning Chung et al. (2024). However, the interpretability of how pre-trained models improve during fine- tuning remains limited, and the underlying mechanisms enabling their success across tasks require further investigation.\nMany studies in MI regard models as computational graphs Geiger et al. (2021), where circuits are specific subgraphs that perform identifiable functions Wang et al. (2022). Notably, this framework has been successfully applied to various LLMs, revealing emergent behaviors within attention heads and Multi-Layer Perceptrons (MLPs) Heimersheim & Janiak (2023); Burns et al. (2023); Hanna et al. (2023); Gould et al. (2023). Moreover, circuits have recently been leveraged to investigate the fine-tuning process of language models, seeking to understand the mechanisms behind fine-tuning Prakash et al. (2024); Chhabra et al. (2024); Jain et al. (2024). However, these studies often focus on tasks where pre-trained models already perform well (e.g., GPT-2 Radford et al. (2019) achieves around 98% accuracy on the IOI task), or they use general data for fine-tuning rather than domain- specific datasets Prakash et al. (2024). Under such conditions, fine-tuning mainly enhances existing mechanisms (e.g., by adding some attention heads). Consequently, their arguments may not be applicable in more practical fine-tuning scenarios where models initially perform poorly and require fine-tuning on domain data."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 MECHANISTIC INTERPRETABILITY", "content": "Mechanistic Interpretability investigates how components in large language models process and represent information Wang et al. (2024). At present, many MI studies have been applied in various fields of AI Safety. For instance, oversimplified probes risk Friedman et al. (2024), unlearning fabricated knowledge Sun et al. (2024), reducing toxicity via alignment Lee et al. (2024), mitigating hallucinations by editing representations Zhang et al. (2024), and generating truthful outputs through inference-time interventions Li et al. (2023). Other studies explore how local model edits propagate across tasks Cohen et al. (2024); Meng et al. (2023), Multi-Head Attention in-context learning Chen et al. (2024); Chen & Zou (2024) and enhance influence-function sampling Koh et al.. Specifically, our study examines how circuits evolve during fine-tuning for mathematical tasks, focusing on node and edge changes to reveal mechanisms behind performance improvements."}, {"title": "2.2 CIRCUIT ANALYSIS AND FINE-TUNING", "content": "One direction of Circuit Analysis focuses on building complete circuits. Early work localizes factual associations in mid-layer modules Meng et al. (2022) and uses causal mediation to uncover biases Vig et al. (2020); Hase et al. (2023). Automated methods like Automated Circuit Discovery identify significant units Conmy et al. (2023), while techniques like attribution patching, and refine circuit extraction by handling near-zero gradients Syed et al. (2023); Hanna et al. (2024). Edge pruning Bhaskar et al. (2024) provide insights into building the edge of the circuit. Another line of research investigates the functional roles of circuit components, such as Attention heads Wu et al. (2024); McDougall et al. (2023); Olsson et al. (2022); Gould et al. (2023); Cabannes et al. (2024) and Feed Forward Networks (FFNs) / MLPs Geva et al. (2021; 2022); Bhattacharya & Bojar (2024). Additionally, circuits have been used to analyze specific tasks, such as factual knowledge retrieval Geva et al. (2023), arithmetic computation Stolfo et al. (2023), Greater Than task Hanna et al. (2023), and circuit recognition in Indirect Object Identification Wang et al. (2022). Unlike these analyses, which focus on smaller-scale tasks and models, our work offers a new lens on how circuits evolve specifically during fine-tuning on mathematical tasks, revealing crucial roles of edge changes.\nAs pre-trained language models scale, fine-tuning methods have emerged, optimizing only a small subset of parameters Ding et al. (2023). Parameter-efficient fine-tuning (PEFT) methods, such as LoRA Hu et al. (2021), reduce computational costs while preserving functionality Ding et al. (2023). Advances in LoRA, including pruning Zhou et al. (2024) and adaptive budget allocation Zhang et al. (2023); Liu et al. (2022); Lialin et al. (2024), further improve efficiency. In our study, we introduce a circuit-aware LoRA approach that adaptively assigns higher ranks to layers with more edge changes, boosting efficiency and accuracy in mathematical tasks, and further illustrates how combining circuits from subtasks can enhance performance in compositional tasks during fine-tuning."}, {"title": "3 CIRCUIT DISCOVERY AND TASK DESIGN", "content": null}, {"title": "3.1 CIRCUIT DISCOVERY: EAP-IG", "content": "Attribution patching is a technique for identifying circuits using two forward passes and one backward pass Syed et al. (2023). In our experiments, we use Edge Attribution Patching with Integrated Gradients (EAP-IG) Hanna et al. (2024), which addresses computational inefficiency in large models and resolves zero-gradient issues with KL divergence. EAP-IG computes importance scores by integrating gradients along the path between clean and corrupted activations, making it our method of choice. The formula for scoring each edge is:\n$AL(E) \\approx (e_{corr} - e_{clean}) \\sum_{k=1}^{m} \\nabla e_{k} L(x)$,\nwhere $e_{clean}$ and $e_{corr}$ denote the activations in the circuit under the clean and corrupted inputs, respectively. m is the total number of interpolation steps, and k represents the index of a specific step. $\\nabla e_{k} L(x)$ denotes the gradient of the loss function L(x) with respect to the interpolated activations $e_{k}$.\nIn this study, we choose m = 5 based on Hanna et al.'s (2024) recommendations Hanna et al. (2024)."}, {"title": "3.2 CIRCUIT EVALUTAION: FAITHFULNESS AND ROBUSTNESS", "content": "Faithfulness serves as a key metric to evaluate the reliability of circuits discovered in MI and it quantifies how closely a circuit replicates the behavior of the original model Wang et al. (2022); Chhabra et al. (2024); Prakash et al. (2024). We adopt Kullback-Leibler divergence (KL-divergence) as the metric, following Conmy et al. Conmy et al. (2023). Let M denote the model and C the discovered circuit. Faithfulness is defined as the percentage of the model's performance captured by the circuit. The formula for faithfulness is:\nFaithfulness = $(1 - \\frac{|F(M) \u2013 F(C)|}{F(M)}) \u00d7 100%$,\nwhere F(M) represents the performance of the full model M and F(C) represents the performance of the circuit C."}, {"title": "3.3 TASKS DESIGN", "content": "To examine the effect of fine-tuning on circuit dynamics, we construct a suite of challenging mathe- matical tasks in which pre-trained models initially perform poorly. As shown in Figure 1, these tasks help reveal the underlying fine-tuning mechanisms that drive significant performance gains during the process.\nAddition and Subtraction (Add/Sub). This task evaluates the model's ability to perform basic addition and subtraction operations. Corrupted data involves altering the arithmetic operation. The task includes five subtasks categorized by the range of numbers involved within 100, 200, 300, 400, and 500. Each subtask contains 5,000 instances.\nMultiplication and Division (Mul/Div). This task assesses the model's capability to handle multipli- cation and division accurately. Corrupted data involves changing the operation between multiplication and division. A total of 2,000 instances are included in this task.\nArithmetic and Geometric Sequence (Sequence). This task measures the model's ability to recognize and extend arithmetic or geometric sequences. Corrupted data involves altering one term in the sequence. The dataset for this task contains 5,000 instances."}, {"title": "4 How DO CIRCUITS EVOLVE DURING THE FINE-TUNING PROCESS?", "content": null}, {"title": "4.1 MODEL ACCURACY, CIRCUIT FAITHFULNESS, AND ROBUSTNESS ANALYSIS", "content": "To analyze circuit evolution, we first evaluate model accuracy across fine-tuning checkpoints. We use LORA Hu et al. (2021) to fine-tune the Pythia-1.4B model Biderman et al. (2023) on five different mathematical tasks. The experimental settings for fine-tuning are shown in Appendix A. The left panel of Figure 2 depicts the accuracy dynamics of the model on five mathematical tasks during fine-tuning. We track the model's accuracy at various training stages across different tasks, revealing consistent improvements in performance throughout the fine-tuning process.\nNext, we explore the faithfulness of the circuits found at each stage of fine-tuning. Prior work Hanna et al. (2024) achieved over 85% faithfulness by selecting 1\u20132% of edges. Given our more complex tasks and larger model, we select 5% of edges to ensure reliable circuits (faithfulness >70%). As shown in the middle panel of Figure 2, circuit faithfulness consistently exceeds 80% across most tasks, both before fine-tuning (Checkpoint 0) and throughout fine-tuning (Checkpoints 1\u201310). The only exception is Add/Sub task, where faithfulness is 77.52% before fine-tuning. These results confirm high circuit faithfulness in both pre-trained and fine-tuned models across all tasks.\nFinally, we conduct robustness analysis on the circuits identified by EAP-IG. We evaluate the robustness of circuits in the pre-trained model, the fine-tuned model, and a randomly initialized model. In this section, we present the robustness analysis for the Add/Sub (100), with analysis for other tasks provided in Appendix C. As discussed in Section 3.2, we perturb the original dataset by 10% to 90% and identify the circuit of three models in perturbed datasets with varying noise levels. Then, we compute the robustness score of Fine-tuned, Pre-trained, and Random models under different perturbation levels. Results in the right part of Figure 2 reveal that circuits identified by EAP-IG demonstrate high fidelity in both pre-trained and fine-tuned models, despite significant performance differences."}, {"title": "4.2 CIRCUIT IS CONVERGING DURING FINE-TUNING", "content": "We conjecture that as the model's accuracy on the task continues to improve, the model's internal circuits should continue to stabilize. To verify our hypothesis, we analyze the change of nodes and edges across consecutive checkpoints.\nFirst, we analyze node and edge changes across checkpoints. The top right of Figure 3 illustrates three mathematical tasks, corresponding to the model's increasing accuracy during fine-tuning. By tracking the number of node and edge modifications between different checkpoints, we assess whether circuit changes diminish over time and tend toward convergence as the accuracy of the model improves. Details for the remaining tasks are provided in Appendix D. As shown in Figure 3, the number of node/edge state changes decreases consistently over time, indicating stabilization and convergence of the circuit.\nSubsequently, we propose a new metric to measure the degree of change of nodes and edges during fine-tuning. To quantify the changes in edges and nodes during fine-tuning across n checkpoints, we define a unified change rate:\n$\\Delta_s = \\frac{1}{n} \\sum_{t=0}^{n-1} \\frac{S_{t\u2192t+1}}{S_0} \u00d7 100%$,\nwhere $\u0394_{St\u2192t+1}$ denotes the number of nodes or edges that change from checkpoint t to checkpoint t + 1, and $S_0$ denotes the total number of nodes or edges in the initial circuit.\nAs shown in Figure 3, fine-tuning induces structural changes, with $\u0394_s$ (Edge) consistently exceeding $\u0394_s$ (Node) by a factor of 2-3 across three tasks. This underscores the pivotal role of edges as the"}, {"title": "4.3 REORGANIZING CIRCUIT EDGES TO FORM A NEW CIRCUIT", "content": "As discussed in Section 3.1, each edge's score is computed as the dot product of the averaged loss gradients and activation difference, quantifying its influence on model predictions. To examine structural changes in circuits during fine-tuning, we use the 95th percentile of edge scores as a dynamic threshold. Edges in the initial and final circuits exceeding this threshold are retained, yielding sparser circuits that capture the model's core information flow. Experimental results for all other tasks are provided in Appendix E. The distribution of added and deleted nodes and edges follows a distinct pattern. As illustrated in the left part of Figure 3, added nodes are predominantly located in the middle and later layers of the circuit, whereas added and deleted edges are concentrated in the middle layers. The shallow layers exhibit minimal changes, providing a stable foundation for task-specific adaptations.\nIn order to prove our conclusions, we conduct investigations into how the circuit evolves under different fine-tuning regimes. Specifically, Appendix F examines the circuit modifications resulting from various PEFT strategies, while Appendix G focuses on the changes induced by full-parameter fine-tuning and LoRA. Finally, Appendix H provides a comparison of circuit changes observed under different LLMs."}, {"title": "5 CAN CIRCUIT INSIGHTS ENHANCE THE FINE-TUNING PROCESS?", "content": "In the previous section, we observe that while the nodes in the model's circuit exhibit minimal changes during fine-tuning, the edges undergo significant modifications. This observation raises an intriguing question: Can LoRA be improved by fine-tuning the edges that change the most? We would like to improve the fine-tuning algorithm from the perspective of Mechanistic Interpretability."}, {"title": "5.1 APPLYING CIRCUIT EDGE CHANGES INTO LORA FINE-TUNING", "content": "Based on the score of edges and the result of section 3.1, we assume that the most \"active\" edges play a key role in the fine-tuning process. Also, considering that LoRA is fine-tuned in layers of the model, we want to focus on the layers where the most \u201cactive\" edges are located.\nWe propose CircuitLoRA, a circuit-aware Low-Rank Adaptation (LoRA) method that incorporates circuit-level analysis to enhance fine-tuning efficiency and performance. CircuitLoRA operates in two phases: first, the edges with the largest score changes are analyzed to identify Critical Layers; second, higher-rank LoRA modules are assigned to layers with more edge changes, while standard-rank modules are applied to other layers. The complete procedure is detailed in Algorithm 1.\nOur hypothesis is that this improved fine-tuning algorithm, which leverages circuit-based analysis, can make better use of the fine-tuning mechanism. In the subsequent section, we investigate this hypothesis, designing experiments across different mathmatical tasks to compare our strategy against full parameter fine-tuning and LoRA baseline."}, {"title": "5.2 IMPROVING FINE-TUNING EFFICIENCY AND ACCURACY BY CIRCUIT INSIGHTS", "content": "To verify our hypothesis, we perform experiments on a range of arithmetic and mathematical reasoning tasks. The experimental results of CircuitLoRA are summarized in two tables. In our experiments, 5 Critical Layers are selected. We compare CircuitLoRA against control groups including LORA and RandomLoRA (5 Critical Layers are randomly selected). For each method in the experiment, we report the final accuracy as the mean of five runs with different random seeds.\nAs shown in Table 1, CircuitLoRA consistently outperforms baseline methods, including LORA and RandomLoRA, across all five tasks. For instance, in the \"within 300\" task, CircuitLoRA"}, {"title": "6 HOW CAPABLE IS THE UNION CIRCUIT IN PERFORMING COMPOSITIONAL TASKS?", "content": "In this section, we further explore the behavior of circuits in compositional tasks, aiming to investigate whether these tasks can be interpreted through the combination of circuits."}, {"title": "6.1 COMPOSITIONAL TASKS, COMPOSITIONAL CIRCUITS AND UNION CIRCUITS", "content": "In the beginning, we first introduce a series of definitions regarding the composition of tasks and circuits.\nCompositional Tasks. A compositional task is composed of multiple interrelated subtasks, each contributing to the overall objective. Compositional Circuits. A Compositional Circuit captures the structural and functional dependencies within a compositional task. Union Circuits. A Union Circuit is constructed by merging the circuits of all subtasks.\nTo design the compositional tasks, we consider the two-step operation, which involves the calcula- tion of two different types of mathematical problem, such as addition/subtraction and multiplica- tion/division. For instance, the compositional task \u2018(61 \u2013 45) \u00d7 45 =\" involves two mathematical operations: (1) (Addition/Subtraction): 61 \u2013 45 =\"; and (2) (Multiplication/Division): 16 \u00d7 45 =\". More examples of compositional tasks can be found in Appendix J.\nOur intuition is that if the circuits can represent the minimum calculation block for one tasks, then it is conjectured that the Union Circuits of the two subtasks can exhibit the power to represent the circuits for the compositional task. In the following, we will investigate the conjecture through two approaches: (1) we compare the similarities between the Union Circuits and the Compositional Circuits; (2) we use the Union Circuits to develop the CircuitLoRA algorithm and evaluate whether the performance of the compositional task can also be improved.\""}, {"title": "6.2 EFFICIENT SINGLE-PHASE FINE-TUNING ON COMPOSITIONAL TASK WITH UNION CIRCUIT", "content": "We conduct overlap analysis and fine-tuning experiments on the two-step operation combination task. For a circuit C, we define a $Top_k(C)$ metric to quantify how many of the top-k edges, ranked by their scores, are shared between two circuits. Then we define the Overlap metric as follows:\n$Overlap_k(C_1, C_2) = |Top_k(C_1) \u2229 Top_k(C_2)|$.\nFirst, we calculate the Union Circuit and Combination Circuit under the two-step operation combina- tion task.\nThrough overlap analysis, we prove the efficiency of Union Circuit to a certain extent. Table 2 analyzes the overlap for different values of k to evaluate the efficiency of the Union Circuit. The results show that, regardless of the value of k, the overlap between the Union Circuit and the Compositional Circuit is consistently the highest. Comparisons are made between the addition/subtraction circuit and circuits from control tasks, such as multiplication/division and arithmetic/geometric sequences. The overlaps in these cases are notably lower. These findings demonstrate that the Union Circuit provides an approximate representation of the Compositional Circuit.\nThen, we use Union Circuit and Compositional Circuit to identify the Critical Layers to further explore the 'approximation ability\u201d of Union Circuit. Table 3 summarizes the performance of CircuitLoRA and LoRA on the two-step operations task. Specifically, CircuitLoRA with Compositional Circuit achieves the highest accuracy of 67.20%. Surprisingly, when using the Union Circuit for Critical Layer identification, CircuitLoRA achieves 65.50%, still exceeding the performance of LoRA except the Compositional Circuit configuration."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we build on circuit analysis to deepen our understanding of fine-tuning and better leverage learned mechanisms. Our findings show that fine-tuning primarily modifying edges rather than merely introducing new components to form new circuits. Building on this insight, we develop a circuit-aware LoRA method. Across multiple tasks, our results demonstrate that incorporating this MI perspective enhances fine-tuning efficiency. Additionally, we show that the composition of subtask circuits effectively represents the circuit of compositional task.\nMoving forward, we will explore the following directions. Although our work focused on math tasks, applying circuit-based methods to more tasks would further validate the generality of our insights. Additionally, while our compositional experiments only explore two-step arithmetic, extending this analysis to multi-step or more compositional tasks could provide deeper insights into circuit interactions, enhancing interpretability and fine-tuning efficiency."}, {"title": "IMPACT STATEMENT", "content": "Our work provides concrete insights for advancing Mechanistic Interpretability. This deeper under- standing of the internal processes guiding model updates paves the way for more efficient, accurate, and trustworthy AI systems. We hope these findings inspire new methods and applications that take advantage of circuit-based analysis to unlock greater transparency, reliability, and performance in LLMs development, and to make better use of the learned mechanisms in these models.\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A EXPERIMENTAL SETUP OF FINE-TUNING", "content": "Fine-tuning experiments were conducted across various arithmetic tasks, with configurations tailored to each. All tasks were trained with a batch size of 8, gradient accumulation steps of 4, and a warmup of 50 steps, using a weight decay of 0.01.\nAddition and Subtraction (Add/Sub) task, which includes subtasks with ranges of 100, 200, 300, 400, and 500, each subtask consists of 5,000 samples. The 100-range subtask was trained for 2 epochs, while others were trained for 4 epochs. LoRA experiments were performed with ranks r = 2,8,16,32, using a learning rate of 3e-4, except for the 400-range (r = 32, lr=2e-4). Full Parameter Fine-Tuning (FPFT) used learning rates of 8e-6 (100-range), 6e-6 (200-range), 5e-6 (400-range), and 4e-6 (500-range). Circuit LoRA applied higher learning rates (4e-4 or 5e-4) for Critical Layers and 3e-4 for non-Critical Layers.\nMultiplication and Division (Mul/Div) task contains 2,000 samples and was trained for 2 epochs. LORA used a learning rate of 3e-4, FPFT used 4e-6, and CircuitLoRA used 2e-4 for Critical Layers and 3e-4 for non-Critical Layers.\nArithmetic and Geometric Sequence (Sequence) task includes 5,000 samples, trained for 4 epochs. LORA experiments used a learning rate of 3e-4, FPFT used 8e-6, and CircuitLoRA applied 6e-4 (r = 32) and 5e-4 (r = 64) for Critical Layers, with 3e-4 for non-Critical Layers.\nLeast Common Multiple (LCM) task, which contains 2,500 samples and was trained for 2 epochs, LORA used learning rates of 3e-4 (r = 2, 8), 4e-4 (r = 16), and 2e-4 (r = 32). FPFT used 4e-6, and CircuitLoRA used 4e-4 (r = 32) and 6e-5 (r = 64) for Critical Layers, with 3e-4 for non-Critical Layers.\nFunction Evaluation (Function) task, with 5,000 samples trained for 2 epochs, used consistent LORA learning rates of 3e-4 (r = 2, 8, 16, 32), FPFT with 8e-6, and CircuitLoRA with 4e-4 for Critical Layers and 3e-4 for non-Critical Layers."}, {"title": "C ROBUSTNESS ANALYSIS EXPERIMENTS ON OTHER TASKS", "content": "Building on the results reported in the main text, this appendix details our additional robustness experiments conducted across multiple arithmetic tasks. Following the methodology presented in Sec- tion 3.2, we systematically apply input perturbations to Multiplication/Division, Arithmetic/Geometric Sequence, Least Common Multiple, and Function Evaluation tasks. Our findings further corroborate the consistency and fidelity of circuits identified by EAP-IG, demonstrating their ability to adapt under varying perturbation conditions while preserving core computational relationships.\nMultiplication and Division Tasks Data perturbation in multiplication and division tasks involves altering one of the operands within a specified range while maintaining the validity of the operation. This introduces variability without disrupting the fundamental arithmetic relationship.\nExample:\n\u2022 Original: Calculate the result of the following arithmetic expression and provide only the final answer: 26 * 15 =\n\u2022 Perturbed: Calculate the result of the following arithmetic expression and provide only the final answer: 26 * 20 =\nArithmetic and Geometric Sequence Tasks For arithmetic sequences, perturbation is achieved by uniformly shifting each term by a fixed integer. In geometric sequences, the first term is adjusted, and subsequent terms are recalculated using the original common ratio to preserve the sequence's structure.\nExample:\n\u2022 Original: Derive the following sequence: 26, 66, 106, 146, \n\u2022 Perturbed: Derive the following sequence: 21, 61, 101, 141, \nLeast Common Multiple (LCM) Tasks Data perturbation for LCM tasks involves regenerating the last LCM expression using one of three strategies: generating multiples, coprimes, or pairs with common factors that are not multiples. This ensures diversity and prevents redundancy in the dataset.\nExample:\n\u2022 Original: Calculate the least common multiple (LCM) of two numbers. LCM (189, 84) = 756, LCM (200, 400) =\n\u2022 Perturbed: Calculate the least common multiple (LCM) of two numbers. LCM (189, 84) = 756, LCM(75, 120) =\nFunction Evaluation Tasks In function evaluation tasks, perturbation involves modifying the constant term b in a linear function y = ax + b by a value within a specified range. The corresponding y-values are recalculated to reflect the change, ensuring the functional relationship remains intact.\nExample:\n\u2022 Original: There is a function y=5x+201. Given x=1,2,3,4, y=206,211,216,\n\u2022 Perturbed: There is a function y=5x+151. Given x=1,2,3,4, y=156,161,166,\nIn line with the observations for addition and subtraction, our experiments on LCM, Sequence, Multiplication/Division, and Function Evaluation tasks demonstrate that circuits can be identified in both pre-trained and fine-tuned models with high faithfulness and robustness. This finding holds true despite the significant performance gap between the two model states, underscoring the reliability and stability of the discovered circuits across diverse arithmetic tasks."}, {"title": "I CIRCUITLORA PERFORMANCE ON OTHER TASKS", "content": "In this appendix, we extend our investigation of CircuitLoRA to additional tasks beyond those discussed in the main text. These tasks include a variety of numerical operations, such as addition and subtraction with varying ranges, to further examine the performance and robustness of our circuit-aware fine-tuning approach. By testing CircuitLoRA on these additional benchmarks, we aim to provide a more comprehensive evaluation, highlighting how incorporating circuit-based insights can yield consistent gains across a broader set of mathematical tasks.\nIn summary, the results presented in Table 4 demonstrate that CircuitLoRA maintains its advantage over both LoRA and RandomLoRA baselines across multiple configurations and numerical ranges. Even when the parameter ratio is constrained, CircuitLoRA effectively identifies and prioritizes Critical Layers, ensuring superior accuracy compared to methods that allocate ranks uniformly or randomly. These findings further validate the effectiveness of circuit-based analysis in enhancing fine-tuning efficiency and performance, reinforcing Key Observation 3: Circuits can improve fine- tuning by achieving higher accuracy and parameter efficiency across various mathematical tasks. In the addition and subtraction task, we can see that after using Circuit LoRA (r\u3002 = 8, rc = 32), we can achieve almost the same accuracy or even higher with half the training parameters of LORA (ro = 32)."}, {"title": "J EXAMPLES OF COMPOSITIONAL TASK", "content": "Our compositional task involve two-step arithmetic operations, requiring reasoning across different mathematical operations. This task requires the model to perform addition and subtraction operations first, and then multiplication and division operations. The following examples demonstrate a diverse set of arithmetic problems designed for this purpose.\nExample:\n\u2022 Clean: Calculate the result of the following arithmetic expression and provide only the final answer: (43 * 21) -\n7 ="}]}