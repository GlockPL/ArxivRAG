{"title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "authors": ["Han Jiang", "Junwen Duan", "Zhe Qu", "Jianxin Wang"], "abstract": "Unsupervised rationale extraction aims to extract text snippets to support model predictions without explicit rationale annotation. Researchers have made many efforts to solve this task. Previous works often encode each aspect independently, which may limit their ability to capture meaningful internal correlations between aspects. While there has been significant work on mitigating spurious correlations, our approach focuses on leveraging the beneficial internal correlations to improve multi-aspect rationale extraction. In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain and predict multiple aspects simultaneously. Concretely, we propose a Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to encode multiple text chunks simultaneously. Furthermore, multiple special tokens are prepended in front of the text with each corresponding to one certain aspect. Finally, multi-task training is deployed to reduce the training overhead. Experimental results on two unsupervised rationale extraction benchmarks show that MARE achieves state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Deep learning text classification systems have achieved remarkable performance in recent years. However, their black-box nature has been widely criticized. Finding a sufficient approach to open the black box is urgent and significant.\nUnsupervised rationale extraction is an explanation approach that aims to extract text snippets from input text to support model predictions without explicit rationale annotation. Previous researchers have made many efforts to improve the rationalization performance of their models. However, as shown in Figure 1a, existing rationale extraction models are uni-aspect encoding models, which can only predict and interpret one aspect of the text at a time. In real-world scenarios, one text often contains multiple aspects of an object. Table 1 shows an example from the BeerAdvocate dataset, where blue, red, and cyan represent the aspects of Appearance, Aroma, and Palate, respectively. The highlighted segments in the text are the rationales corresponding to each aspect. For instance, \"pours a murky orangish-brown color with a white head.\" explains why the label for Appearance is Positive. In this case, traditional uni-aspect rationale extraction models would require three independently trained models to predict and interpret all three aspects, which is labor-intensive and time-consuming and limits their downstream applications. Furthermore, uni-aspect models encode each aspect independently ignoring their internal correlation.\nTo address these problems, we propose the Multi-Aspect Rationale Extractor (MARE). As shown in Figure 1b, MARE can encode all aspects simultaneously by prepending multiple special tokens"}, {"title": "2 Related Work", "content": "The rationalization framework, known as RNP, assumes that any unselected input has no contribution to the prediction and achieves remarkable performance on this task. However, RNP still has many weaknesses. Various approaches have been proposed to improve RNP In different dimensions.\nThe RNP framework utilizes REINFORCE to overcome the non-differentiable problem, but this leads to training instability and poor performance. Hard-Kuma introduces re-parameterization tricks and replaces the Bernoulli distribution with the rectified Kumaraswamy distribution, which stabilizes the training process. In FR, the encoder's parameter is shared between the generator and predictor. This ensures that the encoder's gradient is more reasonable because it can see both full texts and rationales. 3Players forces the complementary rationale to be meaningless, resulting in more meaningful generated rationales. Our research is orthogonal with these methods.\nThe interlocking problem was initially proposed by A2R. This problem arises when the generator fails to identify important tokens, leading to sub-optimal rationales and consequently affecting the performance. Many researchers have developed approaches to address this issue. DMR aimed to align the distributions of rationales with the full input text in the output space and feature space. A2R enhances the predictor's understanding of the full text by introducing a soft rationale. MGR involves multiple generators with different initializations to allow the predictor to see various rationales, alleviating the interlocking problem. DR limits the Lipschitz constant of the predictor, making the whole system more robust. DAR deploys a pre-trained discriminator to align the selected rationale and the original input. MCD proposes the minimum conditional dependence criterion to overcome the issues of the maximum mutual information (MMI) criterion. YOFO eliminates interlocking by simultaneously predicting and interpreting. YOFO deploys pre-trained language models as its backbone and uses token deletion strategies between layers to erase unimportant tokens. the remaining tokens in the final layer are seen as rationales."}, {"title": "3 Problem Definition", "content": "Existing uni-aspect encoding models extract rationales $z_i$ from the input x and predict the label $y_i$ for the i-th aspect. Formally, they can be expressed as $P(y_i, z_i | x; \\theta_i)$, where $\\theta_i$ represents the parameters of the model for the i-th aspect. To obtain the rationales and predictions for all k aspects, k independently trained models are required: {P(y1, z1 | x; \u03b81),\u2026\u2026, P(yk, zk | x; \u03b8k)}. However, this approach is time-consuming and computationally expensive.\nTo address this issue, we propose a multi-aspect rationale extraction task, where the rationales and predictions for all aspects can be generated simultaneously. This can be formalized as P(y1,z1,\u2026, yk, zk | x; \u03b8), where \u03b8 represents the parameters of the multi-aspect rationale extraction model. By utilizing a single model to extract rationales and make predictions for all aspects concurrently, we aim to improve the efficiency and reduce computational costs compared."}, {"title": "4 Method", "content": "This paper proposes a Multi-Aspect Rationale Extractor (MARE), which can simultaneously predict and interpret multiple aspects of text. As shown in the left part of Figure 3, MARE is based on an encoder-based pre-trained language model and achieves multi-aspect collaborative encoding through a Multi-Aspect Multi-Head Attention (MAMHA) mechanism. Additionally, MARE employs multi-task training during the training process, significantly reducing the training cost."}, {"title": "4.1 Hard Deletion for Complete Token Removal", "content": "Selecting rationales without explicit annotations can be challenging. We follow the previous work where unimportant tokens are gradually erased. However, directly multiplying hidden states by the token mask harms rationalization performance. Attention"}, {"title": "4.2 Multi-Aspect Multi-Head Attention", "content": "Inspired by hard deletion, we propose the multi-aspect multi-head attention (MAMHA) mechanism to encode multiple text segments simultaneously. As shown in the right part of Figure 3, MAMHA consists of a Multi-Aspect Controller (MAC) and the traditional multi-head attention (MHA) mechanism."}, {"title": "4.2.1 Multi-Aspect Controller (MAC)", "content": "MAC assists MHA in separately encoding different text segments by generating aspect-specific attention masks based on token masks for each aspect. This allows tokens within the same aspect to interact while isolating tokens from different aspects, enabling MHA to achieve multi-aspect collaborative encoding.\nFigure 4 illustrates an example where \"good place\" and \"bad service\" are rationales for the \"location\" and \"service\" aspects, respectively. The final attention mask, obtained through an outer product operation, creates two separate segments. Words within each segment interact, while words from different segments remain independent. Special classification tokens \"[C1]\" and \"[C2]\" collect information from their respective aspects, allowing MHA to encode two aspects simultaneously.\nThis method can be extended to k aspects by dividing the text into k segments and appending k special tokens. Note that if MAC employs AMD, tokens from different aspects cannot be fully isolated, leading to confusion and hindering multi-aspect collaborative encoding (further discussed in Section 6.2.2)."}, {"title": "4.2.2 Computation Process of MAC", "content": "The computation process of MAC is shown in the right part of Figure 3. Assuming $H_i$ represents the hidden states of the i-th layer, its first k vectors {$h^0_i$,..., $h^{k-1}_i$} are representations of special tokens. For the j-th aspect, mapping functions $g^{query}$ and $g^{key}$ map special and normal tokens to Q and K, respectively. The similarity between special and normal tokens is calculated, and the gumbel-softmax technique determines the token's aspect assignment (Equations (1)-(4)).\n$Q = \\{g^{query}(h^0_i),..., g^{query}(h^{k-1}_i)\\}$\n$K = \\{g^{key}(H_i[k:]),..., g^{key}(H_i[k:])\\}$\n$scores = \\frac{Q \\cdot K^T}{\\sqrt{d}}$\n$m = gumbel\\_softmax(scores, dim = -1)$\nwhere d and L mean the vector's dimension and the text's length, respectively. [.] represents slicing operation, $m \\in \\{0, 1\\}^{k \\times L}$ stands for the token mask, and $m[i, j] = 1$ indicates that the j-th token is selected as the rationale of the i-th aspect. The mask m evolves during training, starting as a near-full 1 vector and gradually becoming more selective. The Gumbel-Softmax output is binarized to produce the final mask.\nMAC adopts the outer product operation to match the shape of the attention score matrix in MHA (Equation 5). When $M'[i, j] \\neq 0$, the token is selected as a rationale in at least one aspect and"}, {"title": "4.3 Multi-Task Training", "content": "Using labels from various aspects simultaneously during training may not be feasible, as datasets like Hotel Review only have annotations for one aspect per sample. Multi-task training allows MARE to focus on the aspect corresponding to the current batch, avoiding the need to encode aspects with missing labels. If the batch comes from the j-th aspect, only the corresponding mapping functions $g_Q$ and $g_K$ are used (Equations (10)-(11)).\n$Q = g_{query}(H_i[j - 1 : j])$\n$K = g_{key}(H_i[k:])$\nAt inference time, we do not explicitly control the sparsity level. Instead, the trained mapping functions $g_q$ and $g_k$ directly select tokens they identify as explanations. This means that the proportion of selected tokens for each aspect in a single sample is not strictly fixed. It can vary based on the content, and may even be 0% if the model determines there is no relevant description for a particular aspect."}, {"title": "4.4 Overall Loss", "content": "Our loss function consists of three components: cross-entropy loss ($L_{CE}$), sparsity penalty ($L_{sparse}$), and contiguous penalty ($L_{cont}$). The full loss function is:\n$L = L_{CE} + \\beta L_{sparse} + \\gamma L_{cont}$\n$L_{CE} = \\sum^C_{i=1} \\sum^C y_i log p_i$\n$L_{sparse} = \\sum^N_{i=1} \\sum^L_{j=1} |m^{i, j} - s|$\n$L_{cont} = \\frac{1}{N(L-1)} \\sum^N_{i=1} \\sum^{L-1}_{j=1} |m^{i}_{j+1} - m^{i}_j |$\n, where s is a predefined sparsity level, \u03b2 and \u03b3 are hyperparameters that balance these terms. We employ a Cliff decay strategy as illustrated in Appendix B, where token deletion begins after a specified layer in the network."}, {"title": "5 Experiments", "content": "We performed experiments on two commonly used unsupervised rationale extraction datasets: BeerAdvocate and the Hotel Review dataset.\nThe BeerAdvocate dataset is a multi-aspect sentiment prediction dataset. It consists of texts along with corresponding aspect scores ranging from 0 to 1, including aspects such as appearance, aroma, and palate. The training and validation sets do not have labeled rationales, but the test set contains 994 samples with rationale annotations for all aspects. Notably, the scores across different aspects within the same sample exhibit high correlation, resulting in highly spurious correlations. For the BeerAdvocate dataset, we conducted experiments on the decorrelated version proposed by Lei et al.. We binarized the dataset into binary classification tasks using a positive threshold of 0.6 and a negative threshold of 0.4 . We run our model, MARE, on two sparsity levels: high-sparse and low-sparse. In the high-sparse decorrelated dataset, the sparsity level approximates the sparsity for golden rationales in the test set. In the low-sparse decorrelated dataset, the sparsity level is comparatively lower but allows for convenient comparisons with previous works.\nThe Hotel Review dataset is another widely used dataset for multi-aspect sentiment classification and rationale extraction. It includes texts along with three aspect labels: location, service, and cleanliness. In addition to the"}, {"title": "5.2 Main Results", "content": ""}, {"title": "5.2.1 Results on the BeerAdvocate Dataset", "content": "Experimental results on the decorrelated BeerAdvocate dataset in the high-sparse scenario are shown in Table 2. MARE outperforms YOFO by 3.3%, 1.0%, and 2.8% in the appearance, aroma, and palate aspects, respectively. Meanwhile, MARE achieves the best average F1 scores among all models, particularly 88.8%. This is because MARE is a multi-aspect collaborative encoding model that captures internal correlations between all aspects and thus achieves the best performance.\nExperimental results on the decorrelated BeerAdvocate dataset in the low-sparse"}, {"title": "5.2.2 Results on the Hotel Review Dataset", "content": "Experimental results on the Hotel Review dataset are shown in Table 3. Although MARE is slightly inferior to YOFO in the service and cleanliness aspects, it is far superior to YOFO in the location aspect and its average token-level F1 score is higher than YOFO. Specifically, MARE is 1.1% and 2.0% lower than YOFO in the service and cleanliness aspects, respectively, while it is 5.3% higher than YOFO in the location aspect. Meanwhile, MARE is 0.7% higher than YOFO in the average token-level F1 score."}, {"title": "6 Analysis", "content": "In this section, we delve into a more comprehensive analysis of our methodology. In Section 6.1, we present a series of case studies derived from the Hotel Review dataset to exemplify the practical applications of our approach. In Section 6.2, we conduct an ablation study to substantiate the efficacy of our method by incrementally removing its constituent elements."}, {"title": "6.1 Case Study", "content": "This section visualizes several samples on the Hotel Review dataset as shown in Table 6. Blue, red, and cyan represent the location, service, and cleanliness aspects, respectively, and underline indicate the annotated rationales.\nIn the Hotel Review test set, each sample only has a uni-aspect annotation. As shown in the first case, only the location aspect has been annotated. However, in real scenarios, a review often describes multiple aspects. MARE extracted snippets not only about location but service and cleanliness which are not annotated. \"Staff very clean\" and \"rooms and bathrooms spotless clean\" demonstrate that the service and cleanliness of the hotel are excellent. In the second case, only the location as-"}, {"title": "6.2 Ablation Studies", "content": "To verify the effectiveness of our model components, we have conducted several ablation studies on the BeerAdvocate dataset ."}, {"title": "6.2.1 multi-task training v.s. multi-aspect collaborative training", "content": "To explore the impact of multi-task training on the model as described in Section 4.3, this experiment verifies the effectiveness of multi-task training by comparing the performance, memory usage, and time cost of multi-task training and multi-aspect collaborative training.\nThe experimental result is shown in Table 7. The performance of multi-task training is slightly better than that of multi-aspect collaborative training. This is because, in the early stages of training, MARE cannot distinguish various aspects well, so multi-aspect collaborative training may lead to information leakage between different aspects, resulting in a performance drop. Meanwhile, multi-aspect collaborative training requires mask calculation for all aspects, resulting in high memory usage and long training time, reaching 24209MB and 34.5 minutes respectively. By contrast, multi-task training only requires encoding a single aspect at a time, so it costs much lower in both memory and training time. It saves 17.9% and 25.2% of memory usage and training time, respectively. This indicates that models trained using multi-task training can outperform those trained using multi-aspect collaborative training with fewer computational resources, demonstrating the effectiveness of multi-task training."}, {"title": "6.2.2 Hard Deletion v.s. Attention Mask Deletion", "content": "To demonstrate the effectiveness of hard deletion, this section contrastively employs AMD operations in the MAC. Specifically, we will replace the Equation (5)-(8) with Equation (16)-(19):\n$m' = \\sum^{k-1}_{i=0} m[i] \\in [0, k]^L$\n$m[i] = \\begin{cases}\n0, If m'[i] = 0\\\\\n1, Otherwise\n\\end{cases}$\n$m = m' - StopGrad(m') + m \\in \\{0,1\\}^L$\n$A^h = A^h m, for h in 1, 2, ..., H$\n, where k means the number of aspects, and m' represents the mask vector with a span of closed interval [0, k], m indicates the calculated mask vector to multiply with attention score matrix. Here, we also use the Straight Through technique to bypass the non-differentiable problem.\nExperimental results are shown in Table 8. While using AMD, the rationalization and downstream performance are very poor. On the contrary, MARE-hard performs very well. In three aspects, the validation accuracy of MARE-hard was very close to BERT, and exceeded MARE-AMD by 3.5%, 4.8%, and 6.3%, respectively. Meanwhile, MARE-hard leads MARE-AMD by 23.1%, 23.4%, and 78.1% in rationalization performance, respectively. The reason is that AMD fails to effectively separate tokens corresponding to different aspects, leading to information leakage and hindering accurate rationale extraction. This indicates that AMD is not suitable for multi-aspect collaborative coding, and also proves the necessity and effectiveness of using hard deletion."}, {"title": "6.2.3 Special Token Initialization", "content": "To evaluate the impact of different initialization methods for special tokens on the model performance, this section explores three distinct initialization approaches:\nrandom initialization: The first special token is initialized by [CLS], while all other special tokens are randomly initialized.\nCLS initialization: All the special tokens are initialized by [CLS].\nsharing initialization: All the special tokens are shared and initialized by [CLS].\nThe performance comparisons are shown in Table 9. MARE-CLS is slightly better than MARE-random and the MARE-share performs the worst. We found that MARE share cannot distinguish the differences in sparsity between different aspects. MARE-CLS achieves the best performance because the special token [CLS] is a highly informative embedding after pre-training. By default, MARE uses the CLS initialization."}, {"title": "7 Conclusion", "content": "This paper proposed a Multi-Aspect Rationale Extractor to solve the limitations of traditional uni-aspect encoding models. MARE can collaboratively predict and interpret multiple aspects of text simultaneously. Additionally, MARE incorporated multi-task training, sequentially training on data from each aspect, thereby significantly reducing training costs. Extensive experimental results on two unsupervised rationale extraction datasets have shown that the rationalization performance of MARE is superior to all previous models. Ablation studies further demonstrated the effectiveness of our method."}, {"title": "Limitations", "content": "All of the above experiments have demonstrated the effectiveness of our method, but there are some limitations. MARE needs to prepend some special tokens in front of the input, which increases the computational overhead. Meanwhile, MARE can only adapted in encoder-based pre-trained language models. We are working hard to apply it to decoder-only models so that MARE can explain the predictions of LLMs. We will try to eliminate these limitations in our future work."}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Main Experiments", "content": "In the experiment, we utilize the Pytorch deep learning framework and the huggingface transformers library to implement MARE. BERT will be deployed as the backbone in MARE. MARE uses the AdamW optimizer to optimize parameters, with a learning rate set to 3 \u00d7 10-5 and a weight decay set to 0.0. To control the sparsity and continuity of the generated rationales, this paper applies the \"Cliff\" deletion strategy, where the k is fixed at 9. In addition, we use grid search to select the most suitable hyperparameters \u03b2 and \u03b3 from the candidate set . We assume \u03b2 = \u03b3 in our experiments and select \u03b2 = \u03b3 = [0.7,3,3] for the BeerAdvocate dataset and Hotel Review dataset, respectively. During the training process, we adopt a balanced round-robin method to iteratively sample"}, {"title": "A.2 Implementation Details of Token Deletion", "content": "After obtaining the mask M and its binarized counterpart M shown in Equation 5 and 6, we indeed multiply it with the attention score matrix to implement the token deletion. Specifically, the implementation in PyTorch is as follows:\ndef multi_head_attention(..., M, M_):\n    ...\n    att_score = Q @ K.transpose(-1, -2)\n               / math.sqrt(d_head)\n    # for token deletion\n    M_grad = M + M - M_.detach()\n    deleted_att_score = M_grad * torch.\n                      softmax (att_score, dim=-1)\n    return deleted_att_score @ V\nListing 1: Token Deletion\nThis implementation achieves the following:\nThe binary mask M determines which token pairs can interact (value 1) and which cannot (value 0).\nMultiplying M_grad with the attention score matrix (att_score) effectively zeroes out attention scores between tokens of different aspects.\nThe resulting attention scores are then used to compute the weighted sum of value vectors (V).\nThis approach ensures that tokens within the same aspect can interact through the attention mechanism, while interactions between tokens of different aspects are prevented. This aligns with our goal of allowing aspect-specific information to be aggregated separately."}, {"title": "B Cliff Decay", "content": "The Cliff decay strategy is defined as follows:\nFor layers i < x: All tokens are retained.\nFor layers i \u2265 x: A proportion p of tokens are deleted.\nHere, x is the layer at which deletion begins, and p is the deletion proportion. In our experiments, we set x = 9, with p varying by aspect."}, {"title": "C Training Stability of MARE", "content": "We have conducted additional experiments with different seeds to validate our training stability. Table 10 shows the standard deviations of F1 scores across 3 different seeds. As shown in the table, MARE demonstrates good stability, particularly in the Appearance and Aroma aspects. We believe this stability is partly due to the multi-aspect nature of our model, which allows it to leverage internal correlations between different aspects."}, {"title": "D Experiments on the Correlated BeerAdvocate Dataset", "content": "To evaluate the impact of spurious correlation on model performance, we also conduct experiments on the correlated BeerAdvocate dataset. The overall performance is shown in Table 11. As we can see, MARE achieves state-of-the-art performance and is better than existing methods for a large margin. We attribute this to the effectiveness of collaborative coding, demonstrating that internal correlations can suppress spurious correlations."}]}