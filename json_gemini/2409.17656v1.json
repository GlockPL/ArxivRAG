{"title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound Event Detection", "authors": ["Pengfei Cai", "Yan Song", "Nan Jiang", "Qing Gu", "Ian McLoughlin"], "abstract": "A significant challenge in sound event detection (SED) is the effective utilization of unlabeled data, given the limited availability of labeled data due to high annotation costs. Semi-supervised algorithms rely on labeled data to learn from unlabeled data, and the performance is constrained by the quality and size of the former. In this paper, we introduce the Prototype based Masked Audio Model (PMAM) algorithm for self-supervised representation learning in SED, to better exploit unlabeled data. Specifically, semantically rich frame-level pseudo labels are constructed from a Gaussian mixture model (GMM) based prototypical distribution modeling. These pseudo labels supervise the learning of a Transformer-based masked audio model, in which binary cross-entropy loss is employed instead of the widely used InfoNCE loss, to provide independent loss contributions from different prototypes, which is important in real scenarios in which multiple labels may apply to unsupervised data frames. A final stage of fine-tuning with just a small amount of labeled data yields a very high performing SED model. On like-for-like tests using the DESED task, our method achieves a PSDS1 score of 62.5%, surpassing current state-of-the-art models and demonstrating the superiority of the proposed technique.\nIndex Terms-sound event detection, prototype, masked audio model, self-supervised learning", "sections": [{"title": "I. INTRODUCTION", "content": "Sound is ever-present in our daily lives, and sound event detection (SED) aims to identify the onset and offset of sound events within audio signals. In recent years, mainstream SED algorithms have been based on deep learning strategies [1], [2], which typically rely on manually annotated sound events, either at the frame level (strong labels) or clip level (weak la-bels). However, the cost of annotating sound events, especially strong label annotation, is particularly high [3]. Moreover, since the sound classes vary across different scenarios and applications, there are no universally applicable SED datasets. Each application requires specific data collection to meet its specific requirements [2]. These factors result in a limited amount of labeled data for SED task in specific scenarios, significantly hindering the application of SED. By contrast, collecting a sufficiently large volume of unlabeled data for a specific scenario is often more feasible. Thus, a major challenge for SED is how to leverage large amounts of unlabeled data, and small amounts of labeled data effectively to train SED models.\nThe mainstream approaches to utilize unlabeled data in SED task involve semi-supervised algorithms such as mean-teacher [4], interpolation consistency training (ICT) [5], [6],"}, {"title": "II. MODEL", "content": "The model of PMAM is mainly composed of an encoder network and a context network, as illustrated in Fig 1. The encoder network is responsible for extracting frame-level latent embeddings from the spectrogram, whereas the context network models the temporal dependencies of sound events using the masked audio model task.\nPMAM requires the encoder to transform spectrograms into token sequences along the time dimension. We implement a dual-branch encoder structure, with a Transformer branch and a CNN branch in parallel, which is a prevalent structure in recent research [20]\u2013[22]. For the Transformer branch, we em-ploy the PaSST [23] architecture, which is based on the Audio Spectrogram Transformer (AST) [24], and the PaSST model is initialized with pre-trained weights from AudioSet [25]. The output of PaSST is token sequences with both time and frequency dimensions. Then attention-based pooling is used to aggregate tokens across the frequency dimension. Specifically, a multi-head attention module is employed, where tokens at the same temporal position but different frequencies act as keys and values, and a learnable embedding as the query. Subse-quently, linear interpolation-based upsampling along time di-mension is used to enhance features' time resolution. Features from the CNN and Transformer branches are projected into a uniform feature space through linear projection and then merged through summation.\nLatent embeddings are masked along the time dimen-sion [19] before being fed into the context network. In the context network, a Transformer with relative position encod-ing [26] is used for modelling the temporal dependencies. Fol-lowing the context network, a linear layer serves as the predic-tor, outputting the context representations C = [C1, C2, ..., CT] for self-supervised loss computation."}, {"title": "III. TRAINING", "content": "We employ the frame-wise masked audio model task for self-supervised training, requiring the model to predict the content of masked frames, thereby enhancing its ability to capture temporal dependencies within sound events. Unlike reconstruction based masked audio models, which predict either the masked spectrogram patches [16] or masked latent embeddings [19], we utilize the prototype based approach, requiring the prediction of prototypes corresponding to the masked frames. In the latent space, embeddings for a group of semantically similar frames tend to be closer, and the center of a group is termed a prototype in previous works [27]. Given the rich semantic information of prototypes, employing them as pseudo labels is eminently logical.\nPrototype based masked audio models have been widely applied in ASR [11], [12]. These methods typically utilize clustering algorithms, usually K-means, to obtain prototypes and employ the InfoNCE loss [28] to bring an embedding closer to its corresponding prototype while separating it from other prototypes. Despite their success in ASR, adapting these strategies to SED task introduces unique challenges. For ASR, each frame corresponds to a specific phoneme. However, for SED task, the polyphonic nature of sound events, i.e. multiple events can occur simultaneously, means a single frame might relate to multiple prototypes, diverging fundamentally from prior uses of prototype based strategies. The proposed PMAM algorithm is specifically designed to adapt the prototype-based approach to the complexities of polyphonic sound events, detailed as follows."}, {"title": "A. Prototypical Distribution Modeling", "content": "Since each sound event corresponds to at least one proto-type, a single frame may correspond to multiple prototypes when sound events overlap, a scenario that typical clustering algorithms like K-means cannot handle. To address this issue, we represent a prototype not merely as an embedding but as a distribution of the corresponding group in the latent space. Specifically, let Z denote the latent space spanned by the frame-level embeddings from the encoder network, and \u03b8k denotes the k-th prototype in the total K prototypes. The probability density of a sample z \u2208 Z is given by\n$$p(z) = \\sum_{k=1}^{K}p(\\theta_k)p(z|\\theta_k)$$\nwhere p(\u03b8k) is the prior probability of prototype \u03b8k, and  p(z|\u03b8k) is the corresponding conditional distribution. The proportion of prototype k in feature z, denoted as \u03b3k(z), is calculated as follows:\n$$\\gamma_k(z) = \\frac{p(\\theta_k)p(\\zeta|\\theta_k)}{\\sum_{l=1}^{K}p(\\theta_l)p(z/\\theta_l)}$$"}, {"title": "B. Objective", "content": "InfoNCE loss, typically used in previous works of ASR, requires embeddings that relate to one positive prototype and several negative prototypes. For single sounds it works well, but where multiple sound events can occur simultaneously, more than one positive prototype exists, hence InfoNCE loss cannot be used. One method to address this is to just utilize the similarity between a embedding and each separate prototype to independently predict pseudo labels of specific prototypes, analogous to multi-label classification. To support independent predictions for different prototypes, we propose a prototype-wise binary cross-entropy (BCE) loss function as follows\n$$p_{t,k} = \\sigma([2\\cdot r(sim(c_t, \\mu_k)) \u2013 1]/\\tau)$$\n$$L_{t,k} = -\\gamma_{t,k} log(p_{t,k}) \u2013 (1 - \\gamma_{t,k}) log(1 - p_{t,k})$$\nwhere sim(\u00b7,\u00b7) computes the cosine similarity, r(\u00b7) is the leaky ReLU activation function, \u03c4 = 0.1 scales the logit, and \u03c3(\u00b7) is the sigmoid function. Equation (3) provides the model\u2019s prediction for prototype k based on the cosine similarity between ct and \u03bc\u03ba: when they are orthogonal, the cosine similarity is 0, and pt,k approaches 0; when aligned, Pt,k approaches 1. Equation (4) applies binary cross-entropy to compute the loss between pt,k and the pseudo-label Yt,k. The calculation of Lt,k focuses solely on the similarity between ct and \u03bc\u03ba, thereby making it apt for multi-label SED tasks.\nThe overall loss function over the total dataset D, the set of masked frame indices Mx, and all prototypes k is:\n$$L = \\sum_{x\\in D}\\sum_{M_x} \\sum_{k} L_{t,k}$$\nEffective prediction of pseudo labels for masked frames re-quires not only good representations from the encoder network but also the context network\u2019s capability to model temporal dependencies, thereby enhancing the overall performance of the model."}, {"title": "C. Iterative Update of Pseudo Labels", "content": "The quality of the pseudo labels depend on latent embed-dings generated by the encoder network. In the first iteration, the output features from PaSST are average pooled, upsampled and then used to generate initial pseudo labels via prototypical distribution modeling. Upon completing the masked audio model training of the first iteration, the pseudo labels are regenerated using representations extracted by the updated encoder network. The prototypical distribution modeling and the masked audio model training are performed iteratively, similar to E-step and M-step in the expectation-maximization (EM) algorithm [29], to enhance the quality of pseudo labels."}, {"title": "D. Semi-supervised Fine-tuning", "content": "After the self-supervised training stage, fine-tuning the model with only a small amount of labeled data suffices to achieve a well-performing SED model. During the semi-supervised fine-tuning stage, the predictor on the top of the self-supervised model is replaced with a classifier composed of a fully connected layer and a sigmoid activation for event pre-diction. We employ the widely used mean-teacher model [4] for semi-supervised learning in this stage."}, {"title": "IV. EXPERIMENTS", "content": "Experiments are conducted on the DESED dataset [30], designed for sound event detection in domestic environments. The training set consists of 1578 weakly-labeled clips, 3470 strongly-labeled clips, 14412 unlabeled in-domain clips, and 10000 synthetic clips generated from 1011 audio files. Model performance is evaluated on a validation set with 1168 strongly-labeled clips. We assess the model performance using the PSDS1 score [31] and apply two post-processing methods: the classical median filter and the novel Sound Event Bounding Boxes (SEBB) method based on change point detection [32].\nThe feature extraction follows the setting of [23]. The CNN branch in the encoder follows the DCASE2024 task 4 baseline [22], including 7 convolutional layers. The context network consists of 3 Transformer blocks. The GMM module has 30 Gaussian components and is trained using the EM algorithm [29]. We use the AdamW optimizer, with a learning rate of 1 \u00d7 10-5 for the PaSST module and 2 \u00d7 10-4 for the rest. The batch size is set to 18.\nThe self-supervised training takes two iterations, and each iteration lasts for 30 epochs. In this stage, we fine-tune the last 4 transformer blocks of PaSST using low-rank adapta-tion (LoRA) [33] with rank = 8, which trains only addi-tional low-rank matrices while keeping the original pre-trained parameters fixed, significantly improving training efficiency. Masking during the masked audio model training involved a 75% ratio with block-wise mask strategy with size 10 [19]. The fine-tuning stage lasts 45 epochs, with the first 15 epochs training only the top linear classifier."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce PMAM, a framework for self-supervised representation learning in sound event detection. By leveraging the prototypical distribution modeling, we ex-tract semantically rich pseudo labels, which are then used to train the masked audio model. The use of prototype-wise BCE loss enables independent loss computation of different prototypes. Experimental results demonstrate that PMAM sig-nificantly enhances the performance of SED models compared to solely semi-supervised fine-tuning, revealing the substantial potential of unsupervised algorithms for SED task, especially in scenarios with limited labeled data. Moving forward, we plan to further explore the application of unsupervised algo-rithms for audio tasks."}]}