{"title": "TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions", "authors": ["Vriksha Srihari", "Bhavya R", "Shruti Jayaraman", "Mary Anita Rajam V"], "abstract": "While generative models such as text-to-image, large language models and text-to-video have seen significant progress, the extension to text-to-virtual-reality remains largely unexplored, due to a deficit in training data and the complexity of achieving realistic depth and motion in virtual environments. This paper proposes an approach to coalesce existing generative systems to form a stereoscopic virtual reality video from text. Carried out in three main stages, we start with a base text-to-image model that captures context from an input text. We then employ Stable Diffusion on the rudimentary image produced, to generate frames with enhanced realism and overall quality. These frames are processed with depth estimation algorithms to create left-eye and right-eye views, which are stitched side-by-side to create an immersive viewing experience. Such systems would be highly beneficial in virtual reality production, since filming and scene building often require extensive hours of work and post-production effort.\nWe utilize image evaluation techniques, specifically Fr\u00e9chet Inception Distance and CLIP Score, to assess the visual quality of frames produced for the video. These quantitative measures establish the proficiency of the proposed method.\nOur work highlights the exciting possibilities of using natural language-driven graphics in fields like virtual reality simulations.", "sections": [{"title": "I. INTRODUCTION", "content": "Discussions regarding natural language-driven generative models have dominated research in recent years. Currently, there exist several systems that generate images and videos depicting arbitrary text prompts. Text-to-image (T2I) models, such as DALL-E [1], Imagen [2] and Midjourney [3], and text-to-video models like Sora [4], Make-A-Video [5] and CogVideo [6] have gained popularity for producing increas-ingly accurate and visually appealing results. However, the development of comprehensive text-to-VR models remains limited due to the complexity in translating textual information into immersive virtual environments. Large scale training data for virtual reality (VR) video creation is not available, and results from existing methods for 3D content generation are yet to meet the same standards as T2I studies in terms of visual quality. To the extent of our current knowledge, the field of text-to-VR is largely unexplored. We present how this can be achieved in an attempt to integrate two major existing systems, T2I and depth estimation, into a single pipeline.\nVR180 video formats are designed to deliver an immersive experience with stereoscopic view. In this paper, we inspect the side-by-side (SBS) format for left and right eye perception that can be viewed through the lenses of smartphone headsets such as Google Cardboard [7] and JioDive [8]. Despite the growing interest in VR content, the creation of such media typically requires extensive manual effort and expertise, posing a significant barrier to widespread adoption and content cre-ation. Our methodology proposes the use of existing natural-language-driven image generation systems, to create a high quality and semantically consistent stereoscopic VR video.\nThe procedural framework of our proposed model TexAVi can be summarized as follows:\n1) Generate a rudimentary initial sketch of the text-prompt using a T2I model.\n2) Obtain multiple, higher fidelity frames of the initial sketch.\n3) Generate a left and right image for each frame with depth estimation.\n4) Merge the images into left and right-hand videos, and\n5) Stitch the two videos side-by-side\nTexAVi integrates three separate systems namely AttnGAN [9], Stable Diffusion [10] and MiDaS depth estimation [11]. For each of these we utilize pre-trained models in order to operate efficiently with limited resources. This is also particularly advantageous in the image generation stages where the models can be replaced for different virtual environment customizations. Typically, the creation of virtual scenes is time-consuming and tedious. Streamlining this process using"}, {"title": "II. RELATED WORK", "content": "This section outlines similar prior work on text-to-image generation and depth estimation techniques, providing context for our current research."}, {"title": "A. Image Generation", "content": "Several conditional image generation systems have been proposed, and we analyze a few pertaining to T2I and Image-to-Image. GANs [12] and, more recently, diffusion models are the most popular for such generative tasks. Introduced by Rombach et al. [10], Latent Diffusion Model (LDM) is a powerful image synthesis method capable of producing high-resolution images. LDM is based on the diffusion model that estimates the underlying distribution of the image dataset in a progressive manner and works entirely in the latent space of a variational autoencoder [13]. The novelty of this model lies in reducing the computational resources required as compared to doing the same task in pixel space. Stable Diffusion is an open source latent diffusion model that can run on consumer GPUs and its use is widely democratized. SDXL [14] further improves upon its performance. The primary advancements include a threefold increase in the UNet backbone size, achieved by adding more attention blocks and expanding the cross-attention context with an additional text encoder. The model also supports multiple aspect-ratio image training. The results produced evidenced a consistent visual improvement to traditional Stable Diffusion models.\nKang et al. proposed GigaGAN [15], a model that addresses the limitations of previous GAN architectures in text-to-image synthesis. It offers faster inference time and additionally pro-motes several latent space editing applications including style mixing, vector arithmetic operations and latent interpolation.\nTransformer-based [16] and autoregressive models have ad-ditionally shown promising results. With emphasis on reducing training costs and emissions, Chen et al. [17] incorporate a diffusion transformer [18] as their base architecture and have achieved competitive results in FID scores and visual analysis. Muse [19], another transformer-based model, outperformed pixel spaced models [20], [21] in terms of efficiency with the use of discrete tokens and Parti [22] due to parallel decoding."}, {"title": "B. Depth Estimation", "content": "Depth estimation is fundamental in numerous computer vision applications, particularly for creating immersive depth-aware visual content.\nSeveral prior studies have concentrated on generating depth maps for 2D-to-3D image conversion. N. E. -D Mebtouche et al. [23] introduced a learning-based method for inferring the depth map of a 2D image from a set of K similar images. Their approach incorporated HOG [24] and a variant of the SIFT algorithm [25]. However, this method was stated to be slow due to the warping technique used by SIFT flow. Alhashim, Ibrahim and Wonka, Peter [26] proposed the use of transfer learning, in the form of a simple encoder-decoder architecture, to generate the depth map of a 2D image.\nAnother study has explored 2D-to-3D video conversion. Tsai, Tsung Han et al. [27] created a 3D video by isolating the foreground and background elements from a 2D video. They utilized edge information and SLIC superpixels [28] to produce a depth map for this purpose.\nEfforts have also been made to generate coherent scenes from textual descriptions. H. Ouyang et al. [29] used ZoeDepth [30] to incrementally refine 3D Gaussian clouds, resulting in a high-quality 3D immersive scene. Fridman, Rafail et al. [31] utilized a pre-trained monocular depth estimation model based on vision transformers [45]. This model was applied to images created by a latent diffusion-based text-to-image model and yielded videos that depict geometrically-plausible scenes."}, {"title": "III. PROPOSED METHODOLOGY", "content": "Illustrated in Fig. 1, the architecture of the proposed framework, TexAVi, comprises three primary modules: initial sketch generation, frame synthesis, and stereoscopic VR video construction. Each of these modules is explained in detail in the following subsections."}, {"title": "A. Generation of Initial Sketch", "content": "We start by creating an initial sketch from the user's text prompt. The initial sketch is a rudimentary image generated using an AttnGAN [9].\nThere are two components in AttnGAN: the attentional generative network and the Deep Attentional Multimodal Sim-ilarity Model (DAMSM). The attentional generative network allows the system to render specific regions of the image, conditioned on the phrases that are relevant to those regions. The DAMSM collects semantic information from the text prompt to evaluate image-text similarity. The fine-grained loss obtained from this process is used to train the attentional generative network.\nThe DAMSM consists of a bi-directional Long Short-Term Memory (LSTM) [33] which serves as the text encoder and a Convolutional Neural Network (CNN) which serves as the image encoder. The LSTM extracts semantic vectors from the text description and the CNN maps images to these semantic"}, {"title": "B. Synthesis of Frames", "content": "To generate frames, we utilize Deforum [37], a powerful Stable Diffusion-based open source tool. It is designed to create AI generated animations and is equipped with various features for customization. Since we aim to construct VR videos, we solely exploit its frame generation function. To achieve suitable frames for a VR clip, the features available within the tool are to be configured appropriately. Some of the modified parameter values can be referenced in Table I."}, {"title": "C. Stereoscopic VR Video Construction", "content": "The frames resulting from Stable Diffusion are processed using MiDaS depth estimation [11]. MiDaS is a neural network architecture designed for monocular depth estimation \u2013 it estimates a depth map from a single 2D image.\nThe input to the MiDaS depth estimation network is a single RGB image. Each frame generated by Stable Diffusion is sent through several convolutional layers, which extract hierarchical features from different scales of the image. These features capture both low-level details (such as edges and textures) and high-level semantic information (such as object shapes and structures)."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "The dataset description"}, {"title": "A. Dataset Description", "content": "For the purpose of this paper, we use two datasets \u2013 COCO 2014 and Flickr8k.\nThe COCO 2014 dataset is used to pre-train the AttnGAN. It is the earliest version of Microsoft COCO (Common Objects in Context) [36]. COCO is a benchmark dataset which is widely used in the field of computer vision. It consists of 164,000 images, spanning various different object categories. The training set makes up 50% of the dataset and contains 83,000 images. The validation set, used to evaluate model performance and fine-tune parameters, comprises 25% of the dataset (41,000 images). The testing set (41,000 images), used to assess the model's performance on unfamiliar data and thereby its generalization capability, accounts for the remaining 25%.\nCOCO's images contain a wide range of common objects in-cluding people, animals, vehicles, household items, and natural elements. This ensures a diverse set of scenes, backgrounds, and lighting conditions. Each image in the COCO 2014 dataset has also been manually annotated with captions that describe its content.\nSince COCO 2014 is a smaller dataset compared to its latest counterpart, COCO 2017, it proves to be efficient while training the AttnGAN, as our expected output at the end of the process is a mere initial sketch.\nThe Flickr8k dataset is a well-known dataset consisting of 8,000 images (with 5 captions each) sourced from the photo-sharing platform Flickr. It is a subset of the larger dataset, Flickr30k [40]. Like COCO, this dataset also contains images depicting a wide variety of scenes, objects and activities. We utilize Flickr8k as a distribution of ground truth images to assess our framework's performance."}, {"title": "B. Sample Inputs and Results", "content": "Table II presents the progression of three example inputs across different stages of the TexAVi methodology. Beginning with a brief text prompt inputted into the AttnGAN, the gen-eration process of a basic sketch is initiated. Column 2 shows this initial sketch, which captures the prompt's context with minimal detail. Using Stable Diffusion, we then iteratively"}, {"title": "V. EVALUATION METRICS", "content": "Our resulting video is an aggregation of individual frames. The performance metrics below are used to determine the quality of these frames, and evaluate our result."}, {"title": "A. Fr\u00e9chet Inception Distance (FID)", "content": "Fr\u00e9chet Inception Distance (FID) [41] is a metric which quantifies the realism and diversity of images generated by GANs. The FID compares the distribution of frames with a distribution of real images. FID is given as:\n$FID = ||\u03bc\u03b3 - \u03bcg||^2 + Tr(\u03a3\u2084 + \u03a3g \u2013 2(\u03a3\u03a3\u338f)^{1/2})$ (1)\nwhere \u03bcr and \u00b5g are the mean of vectors of the real images and generated frames, respectively, while Er and Eg denote covariance matrices of the vectors. 'Tr' denotes the trace of the values within parentheses in Eq.1. The lesser the FID, the greater the similarity between the two distributions."}, {"title": "B. CLIP Score", "content": "CLIP Score [43] is a metric which measures the compati-bility of image-text pairs. In our particular context, image-text compatibility is defined as the semantic similarity between a generated frame and the input text prompt. Therefore, the higher the CLIP Score, the more apt the text prompt to the frame. CLIP Score is defined as:\n$CLIPScore(I, C) = max(100 * cos(E1, Ec), 0)$ (2)\nwhich denotes the cosine similarity between the visual CLIP embedding Er for an image I and textual CLIP embedding Ec for a text prompt C. The score ranges from 0 to 100 and the closer to 100 the better.\nWe computed the CLIP Score for frames generated through Stable Diffusion, to evaluate their semantic coherence. To do this, we utilized \"clip-score\" [44], a PyTorch implementation of this metric. Analysis of 8,000 such frames alongside their associated input text prompts yielded a CLIP score of 47.097.\nWithin this limited scope, our generative model demonstrated robust capability in accurately depicting the intended content. We strongly believe that increasing the number of prompts and frames will lead to a more impressive score."}, {"title": "CONCLUSION", "content": "In this paper, we propose a framework that explores the rel-atively unfamiliar realm of VR generation. Prior research has analyzed a variety of approaches to develop image and video media. We make use of some of these existing techniques to derive a short stereoscopic VR clip from a single text prompt.\nWe acknowledge that our work has several avenues for improvement. Due to cost constraints, the present study has only investigated producing clips in a 180-degree format compatible with smartphone headsets. This paper also does not explore look-around capabilities in 180-degree formats, as is common within digital headset viewing experiences, and solely addresses producing a stereoscopic VR immersive ef-fect. Owing to resource limitations, we tested the performance of Stable Diffusion on sets of only 8,000 images. However, our observations in Fig. 2 illustrate the strong possibility of performance metric improvement when larger datasets are tested.\nOur future work aims to capture more nuanced relationships between text and image. This is achievable with better frame generation techniques, and by incorporating additional infor-mation sources such as audio or scene context. Supporting longer text prompts and enhancing frame interpolation for smoother transitions within VR scenes are other areas to concentrate on.\nWe hope to address the aforementioned challenges with subsequent research."}]}