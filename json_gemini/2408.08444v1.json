{"title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering", "authors": ["Jinming Nian", "Qifan Wang", "Zhiyuan Peng", "Yi Fang"], "abstract": "In knowledge-intensive tasks such as open-domain question answering (OpenQA), Large Language Models (LLMs) often struggle to generate factual answers relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create weakly labeled data for training dense retrievers. Specifically, we rerank the top-K passages retrieved via BM25 by assessing the probability that LLMs will generate the correct answer based on the question and each passage. The highest-ranking passages are then used as positive training examples for dense retrieval. Our comprehensive experiments across four publicly available OpenQA datasets demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models. Source code is published 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Open-domain question answering (OpenQA) dating back to the 1960s [18] provides natural language-like answers to reply users' questions. OpenQA adopts the \"Retriever-Reader\" architecture [55], where the retriever retrieves relevant passages for the reader to generate answers. Previous studies [27, 52] adopt a seq2seq model as a reader and train it on the labeled dataset. Recently, large language models (LLMs) like GPT-4 [33] and LLAMA [46] have demonstrated astonishing performance on various tasks, including OpenQA, attributing to the substantial amount of knowledge stored in their internal parameters. Despite the unprecedented achievements of LLMs, they face constraints such as the inability to consistently integrate up-to-date knowledge, as their parametric knowledge is fixed after being trained on huge datasets. Additionally, they are prone to generating plausible but non-factual responses, known as hallucinations [49].\nTo overcome the limitations of LLMs' parametric knowledge, retrieval augmented generation (RAG) [11, 27] is explored, equipping LLMs with a retriever to gather necessary evidence from external sources. Among the two components of RAG, improving the retriever is more feasible due to the recent trend of black-box APIs [33] and the high cost and time requirements of fine-tuning open-source LLMs [10]. The retriever, a critical part of RAG, is typically either a traditional unsupervised retriever like BM25 [38] or a more advanced neural retriever, such as dense retrieval [20, 21, 32, 51], which encodes questions and passages into the same embedding space and then measures the question-passage relevance score by vector similarity. A key challenge in training dense retrievers is the scarcity of human-annotated data, as in OpenQA, human-labeled evidence passages are often unavailable. Methods like UPR [39], which ranks passages based on the likelihood of LLMs generating the question given the passage, and AAR [54], which combines the top-K passages ranked by the LLM's averaged cross-attention scores with ground-truth passages as positive passages, have been employed to train dense retrievers. These approaches can train retrievers to find semantically relevant passages but do not guarantee improved RAG performance in OpenQA. As Cuconasu et al. [6] demonstrated, retrieving relevant passages that cannot answer the question may negatively impact RAG performance in OpenQA tasks. Neither cross-attention scores nor the likelihood of LLMs generating the question based on the input passage explicitly ensures that the question can be answered by the \"positive\" passage.\nTo address the scarcity of training data for dense retrievers in RAG for OpenQA, we propose extracting weak labels from existing OpenQA question-answer pairs by leveraging the ranking capabilities of LLMs [45]. Specifically, we first use BM25 to retrieve the top-K passages for a question, then pair each passage with the question. We rank the passages based on the likelihood that the LLM would generate the question's ground-truth answer from each question-passage pair. Only the top-ranked passage is selected as the positive example for the question, and we train the dense retrievers using in-batch negative sampling. Our method evaluates the relevance of the question-passage pair by the likelihood of the answer given a passage and the question, making it particularly suitable for OpenQA, where retrieved passages must be capable of providing the correct answer."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Dense Retrieval", "content": "Traditional information retrieval (IR) method are based on exact term matching, like BM25. While it is still widely used due to its efficiency, effectiveness, and robustness, it suffers from the well-known issue of lexical gap [3]. To address this, leveraging the neural networks, dense retrieval (DR) employs pre-trained language models like BERT [7], to encode questions and passages into embeddings, and measures the question-passage relevance score by the vector similarly in the embedding space. Specifically, DP encodes the whole corpus into embeddings and builds the index, such as Faiss [8], on them. When a new question comes in, DR encodes the question into an embedding and performs a nearest neighbor search. DR can be classified into two categories: supervised, like DPR [20], TAS-B [13], and ColBERT [21, 41]; unsupervised, like Contriever[15] and ReContriever [25], based on the training method.\nDPR utilizes a dual-tower architecture, with one BERT model dedicated to encode questions and another to encode passages. The similarity between the question and passage embeddings is then calculated, aiming to maximize the log-likelihood of the positive passage. ColBERT uses the same BERT model for both the question and passage encoders, differentiating them by appending a unique special token after the [CLS] token. Unlike DPR, which directly compares question and passage embeddings, ColBERT introduces a late interaction mechanism. It computes the similarity between each question token and all passage tokens, followed by maximum pooling over these similarities. The final similarity score for a question-passage pair is the sum of the pooled scores. TAS-B groups queries based on their embedding similarities and applies a training data sampling technique along with dual-teacher supervision distillation. Contriever trains a bi-encoder model using contrastive learning, generating positive question-passage pairs from an unlabeled corpus. ReContriever follows the same approach as Contriever for generating weak question-passage pairs but adds a self-scoring mechanism during training, where the loss is weighted by these scores."}, {"title": "2.2 RAG for OpenQA", "content": "RAG models have been applied to OpenQA, demonstrating significant performance improvement. Different RAG models have been proposed to solve the critical issues in OpenQA, such as how to retrieve relevant passages [42, 44, 47, 54], when to call the retriever [4, 17, 48], and how to decrease the computational complexity [16, 23, 28, 53].\nSince the retriever is a critical component in RAG, some studies have tried to improve the quality of retrieved passages, including training a better retriever [44, 54] and prompt engineering [42, 47]. REPLUG [44] trains a dense retriever by minimizing the KL divergence between the retrieval likelihood and the language model likelihood computed as the normalized language model probability of the ground-truth, given the question and passage. AAR [54] combines the top-K passages ranked by the LLM's averaged corss-attention scores with ground-truth passages as positive passages and then follow ANCE [50] to sample negative passages to train the dense retriever. ITER-RETGEN [42] leverages the model output from the previous iteration as a specific context to help retrieve more relevant knowledge. IRCOT [47] also adopt a similar approach to perform retrieval, but applies CoT for generating responses.\nIn some cases, LLMs can generate factual content well without external knowledge, and retrieving passages will decrease RAG's performance. Self-RAG [48] and FLARE [17] both fine-tune the LLMs to call the search engine automatically when external knowledge is needed. UAR [4] trains classifiers to identify the need for external knowledge.\nApplying all the top-K retrieved passages as context not only increases the computational complexity and inference latency but also brings noise. Numerous methods have been proposed to compress the retrieved passages. Selective-Context [28] filters non-essential lexical units by the summarization of self-information of each token contained in the unit. LongLLMLingua [16] contrasts the perplexity score of each token in the passage with the perplexity score of the same token conditioned on the question and adopts this conservative perplexity score to filter out tokens. RECOMP [53] leverages the summarization models to summarize the retrieved passages. SuRe [23] summarizes retrieved passages conditioned on each answer candidates generated by prompting the LLMs and then selects the top summarizations ranked by LLMs through a combination of pointwise and pairwise scoring method."}, {"title": "2.3 LLMs for Ranking", "content": "Concerning the generation of weakly labeled ranking data using LLMs as passage rankers, existing work can be categorized into three categories. The first involves a zero-shot listwise ranking strategy, where a prompt containing the question, instructions, and multiple passages are given to the LLM, and the LLM generates a permutation of ranked indices [1, 30, 35, 43]. The second category prompts the LLM with passages and directly instructs it to rate their relevance [56], or perform pairwise comparisons [36]. The third approach calculates the question generation likelihood when a passage is included in the prompt [5, 40, 57]. Our method for scoring passages aligns with this third approach; however, as mentioned in Section 1, we rank the passages by the likelihood of the LLM generating the question's ground-truth answer based on the question-passage pair."}, {"title": "3 METHOD", "content": "As illustrated in Figure 1, W-RAG introduces a novel approach for training the retrieval component within the RAG pipeline from scratch. Our method leverages a weakly-supervised training paradigm that requires only a set of question-answer pairs and an evidence corpus. The W-RAG process unfolds in three stages: first, it retrieves relevant passages from the evidence corpus; second, it employs a LLM to generate weak labels from them by reranking the retrieved passages by the likelihood of generating the ground-truth answer; finally, the dense retiever is trained using these weak labels. The resulting dense retriever is then capable of retrieving evidence, thereby enhancing the performance of LLMs across a wide array of tasks."}, {"title": "3.1 Weak-label Generation", "content": "Given an evidence corpus $C = {d_1, ..., d_N}$, we use a simple retriever to conduct first-stage retrieval. Given a question $q$, the retriever ranks each passage by relevance and selects the top-K passages $P = {s_1,..., s_K}$, where $P \\subset C$. The corpus may comprise chuncked Wikipedia articles or from online sources such as Reddit and Common Crawl, while the question-answer pairs could be sourced from OpenQA platforms such as Quora and StackOverflow. The simple first-stage retriever could be lexical such as BM25 [38], or a dense retriever. The primary objective at this stage is to maximize recall within a manageable set of retrieved passages. Ideally, this step curates evidence passages containing the correct answer to the question\nW-RAG is motivated by the hypothesis that an autoregressive LLM is more likely to generate the correct answer if the provided passage contains the necessary information to answer the question. Our approach aims to harness signals from the LLM's downstream tasks to evaluate the effectiveness of a passage in eliciting the correct answer. It is important to note that traditional relevance measures, such as term overlap or semantic similarity, do no necessarily guarantee that the most relevant passage contains the needed information. Instead, our target is on identifying passages that most effectively prompt the LLM to produce the correct answer and using these passages to train the dense retriever. In this context, we redefine passage relevance as the degree to which a passage can elicit the ground-truth answer, and this criterion serves as the target for training the dense retriever.\nAs depicted in Figure 1 Step 2, we construct weak label generation prompts with candidate passage $s_i$, question $q$, some instructions $I$, and ground-truth answer $a$. The downstream signal is captured through the conditional probability of the ground-truth answer, denoted as $p(a|s_i, q, I)$:\n$p(a|s_i, q, I) = \\prod_{j} LLM(a_j|s_i, q, I, a_{<j})$\nFor each token in the input prompt, the LLM assigns logits, which after applying a softmax operation, correspond to the probability of each vocabulary token being the next in the sequence. The probability of the jth answer token, denoted as $LLM(a_j|s_i, q, I, a_{<j})$, is directly extracted from the logits of the preceding token. Since the ground-truth answer typically consists of multiple tokens, the cumulative probability can diminish rapidly. To address this, we simply take the logarithm on both sides of Equation 1, converting each token's probability into log-likelihood. The overall probability of the answer a is then computed as the average of the sum of the log-likelihoods for each token in the answer:\n$log \\ p(a | s_i, q, I) = \\frac{1}{\\left|a\\right|} \\sum log\\ LLM(a_j|s_i, q, I, a_{< j})$\nWe consider this log-likelihood as the relevance score for the candidate passage $s_i$. Each of the K candidate passages is weakly labeled in this manner, resulting in a ranked list of passages, denoted as $S = {S_{r_1},..., S_{r_k} }$. This list is produced by sorting the set of candidate passages S according to their relevance scores."}, {"title": "3.2 Training Dense Retriever", "content": "Once we have accumulated a sufficient number of weakly labeled passages for a total of M question-answer pairs, denoted as $W = {S_1,..., S_M}$, we are ready to train a dense retriever that will assign higher scores to passages more likely to elicit the correct answer. In principle, any dense retriever, regardless of the specific objective function, can be trained using the generated weak labels. In this paper, we investigate two representative dense retrievers: DPR [19] and ColBERT [22]."}, {"title": "3.2.1 DPR", "content": "DPR utilizes a bi-encoder architecture, where the question and passage are independently mapped to an embedding space through two separate BERT encoders. For a given question q, the relevance score $R_{q,s}$ for each passage s is given by the similarity between their respective [CLS] token embeddings, $e_q$ and $e_s$. We use cosine similarity as the similarity function:\n$R_{q,s} = cos(e_q, e_s)$\nFollowing the in-batch negative training method introduced in DPR, we process each ranked list $S \\in W$ by extracting the top-ranked passage $s_i$ along with its associated question $q_i$. These pairs are then grouped into batches of size n to form training batches, optimizing the retriever using a Multiple Negatives Ranking (MNR) loss [12]. In this approach, for the ith question-passage pair, $s_i$ is treated as the positive example, while the other passages within the same batch are treated as negative examples. The loss function is thus:\n$L_{MNR} = - log \\frac{exp(\\alpha \\cdot R_{q_i,s_i})}{\\sum_{j=1}^{n} exp(\\alpha \\cdot R_{q_i,s_j})}$\nThe scaler $\\alpha$ is used to amplify the cosine similarity score, usually set at 20 according to the default setting in sentence-transformers 2."}, {"title": "3.2.2 ColBERT", "content": "ColBERT employes a bi-encoder architecture with late interaction, where the question and passage are independently encoded using a shared BERT model to obtain embeddings $E_q$ and $E_s$. The relevance score for each passage is computed on the question side using a \u201cMaxSim\" operation, which sums the maximum similarities between any passage token embedding with each question token embedding:\n$R_{q,s} = \\sum_{i \\in E_q} max_{j\\in E_s} dot(E_{qi}, E_{s_j})$\nHere, $dot(x, y)$ represents the dot product between x and y. The training dataset for ColBERT consists of triplet samples $(q, s^+, s^-)$, extracted from each ranked list S. In these triplets, $s_{r_1}$ is treated as the positive passage $s^+$, while $s_{r_2}, S_{r_3}, ..., S_{r_{m+1}}$ are treated as negative passages $s^-$. Here, m represents the number of hard negatives selected from S. ColBERT produces two scores $R_{q,s^+}$ and $R_{q,s^-}$ from each triplet, and the model is optimized through a pairwise softmax cross-entropy loss:\n$L_{ColBERT} = -log(\\frac{exp \\ R_{q,s^+}}{exp \\ R_{q,s^+} + exp \\ R_{q,s^-}})$"}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Task and Datasets", "content": "To assess the effectiveness of W-RAG, we study three crucial components: the quality of the LLM-generated weak labels, the retrieval performance of the weakly trained dense retriever, and the overall effectiveness of the RAG system on the OpenQA task. We conduct experiments on four well-known datasets for OpenQA: MSMARCO QnA v2.1 [31], NQ [24], SQUAD [37], and WebQ [2]. MSMARCO QnA v2.1 shares the same corpus as MSMARCOv1 Passage Retrieval, with questions originating from real user queries submitted to Bing, making the language conversational. NQ also features real user questions and uses a corpus of documents from the English Wikipedia. In this work, we use a chuncked version of NQ as prepared by Karpukhin et al. [19]. SQuAD's corpus is similarly derived from the English Wikipedia, where the passages were first retrieved and then sampled. The questions and answers are manually written by crowdworkers, so they do not reflect natural language as closely as questions from real user queries like MSMARCO and NQ. WebQ's corpus is drawn from Freebase, a large knowledge graph. Due to the nature of knowledge graphs, WebQ's questions and answers are entity-related and factoid-based, thus making them less conversational compared to other datasets like MSMARCO and NQ.\nFor each dataset, we uniformly random sampled 5,000 question-answer pairs and a corpus of 500,000 passages from the training set, ensuring that all questions had relevant passages included in the corpus. We then split the question-answer pairs into 2,000 as training set, 1,000 as validation set, and 2,000 as test set. This sampling was necessitated by resource and time constraints. We argue that this small training set is sufficient to demonstrate statistically significant differences in both retrieval and the final OpenQA performance. While our results are indicative, our method could benefit from further validation using larger datasets to ensure generalizability."}, {"title": "4.1.1 Weak Label Quality", "content": "For our main experiments, we use Llama3-8B-Instruct [9] to serve as the reranker. The 2,000 question-answer pairs in the training set are used to generate weak labels. For each question, weak labels are generated by scoring the top 100 passages retrieved by BM25, based on the LLM's likelihood of generating the answer. This process requires the LLM to perform inference 100 times to produce a ranked list. The choice of 100 passages is a trade-off between accuracy and latency. We found that BM25's Recall@100 reaches approximately 80% across all four datasets, and retrieving additional passages yields diminishing returns as the LLM's inference time increases linearly. We evaluate the reranking performance using different prompts and LLMs, which will be elaborated on later in the ablation studies (5.4)."}, {"title": "4.1.2 Weakly Trained Retriever", "content": "After the LLM weakly labels the training set, we use these labels to train DPR and ColBERT. For DPR, since in-batch negatives are used during training, we only need to select the top-ranked passage to curate a list of relevant question-passage pairs, thus forming 2,000 training samples. For all four datasets, we used two different initializaiton settings for DPR: one is using bert-base-uncased to train from scratch (denoted as \"DPRbase\"), and the other by resuming training from an unsupervised dense retriever using Yibin-Lei/ReContriever [26] (denoted as \"DPRReCon\").\nColBERT's training process differs slightly from that of DPR because it requires the sampling of negative passages. For each question, we select the top-ranked passage from the reranked list as the positive passage and the immediate following 10 passages as hard negatives. These hard negatives are relevant but not sufficiently informative to elicit the correct answer from the LLM. We did not explore varying the number of hard negatives, as this is not the primary focus of this work. Additionally, since the off-the-shelf ColBERTv2 is trained on the entire MSMARCO dataset, using it as the starting point would introduce data leakage. For consistency, we always train ColBERT from scratch using bert-base-uncased for all four datasets (denoted as \"ColBERTbase\")."}, {"title": "4.1.3 OpenQA Performance", "content": "Once the dense retriever is trained, we integrate it into the generic RAG pipeline, where it is used to retrieve the top-K evidence passages for a given user question. The retrieved passages are directly inserted into the prompt, as shown in Figure 2 alongside the question and instructions asking the LLM to generate an answer based on the retrieved passages or, if the passages are not useful, to answer using its internal knowledge. For our main experiments, we restrict Llama3-8B-Instruct to generate a maximum of 20 tokens and only use top 1 retrieved passage to supplement the prompt. We also explored the impact of using different numbers of supplemental evidence passages on both OpenQA performance and latency, which we discuss in the ablation studies (5.4). We did not experiment with different LLMs for the OpenQA task, as the LLM component of RAG is not the primary focus of our study."}, {"title": "4.2 Baselines", "content": "Since our retriever is trained from scratch with limited data, we do not compare it to state-of-the-art retrievers or RAG systems. Instead, we focus on the improvement from untrained baselines to our weakly trained models and compare them with models trained on ground-truth data.\nFor OpenQA evaluation, we use Llama3-8B-Instruct as the LLM in the RAG pipeline, assessing the impact of different retrievers on final OpenQA performance. We compare these retrievers against the same baseline models used for OpenQA. Additionally, we introduce two baselines: \"Naive,\u201d where Llama3 answers the question without any external information, showcasing its parameter knowledge, and \"Groundtruth,\" where the ground-truth passage is inserted into the prompt, representing the best possible OpenQA performance.\nTo assess weak label quality, we compare Llama3-8B-Instruct with BM25 after Llama3 generates weak labels on the training set of each dataset. For retriever performance, we evaluate two categories of models. The first category includes entirely unsupervised models: BM25, ColBERT initialized with bert-base-uncased (denoted as \"ColBERTinit\"), Contriever [14], and ReContriever [26]. Contriever learns by contrasting sampled passages from different documents, assuming that each document is somewhat unique, while ReContriever enhances this by estimating relevance among sampled passages. The second category includes DPRReCon and ColBERTbase, both trained on ground-truth data. For a fair comparison, we use the same questions and an equal number of positive and negative samples as training data."}, {"title": "4.3 Experimental Settings", "content": "We begin by retrieving the top 100 passages using BM25 with the rank_bm253 BM25Okapi with k\u2081=1.5, b=0.75 and e=0.25. The nltk 4 tokenizer is used to tokenize the questions and passages. We use BEIR 5 to manage data, and the DPR models are trained using sentence-transformers with the following hyperparameters: batch size of 128, learning rate of 2 \u00d7 10\u22125, AdamW optimizer, 20 epoch, and model weights are saved based on the best Recall@5 on the validation set. ColBERT is trained from scratch using RAGatouille 6 with a batch size of 64, 10 hard negatives per positive, learning rate of 1 \u00d7 10\u22125, AdamW optimizer, and 1 epoch. Generating weak labels took approximately two days per dataset on an Nvidia V100 GPU. Our experiments are easily reproducible since BM25, ColBERT, DPR, Llama3, and all four datasets are publicly accessable.\nThe prompts used to generate weak labels are shown in Figure 2. We use the term \"DOCUMENT\" instead of \"PASSAGE\" because our testing experiments showed that \"DOCUMENT\" consistently yields better reranking performance. We suspect this is because \"DOCUMENT\" is a more common term and likely appears more frequently in Llama3's training data, leading it to a deeper understanding of the word \"DOCUMENT\". Additionally, we tested different orderings of the passage, question, and instructions, finding that the Passage-Question-Instruction format consistently outperforms Passage-Instruction-Question and Instruction-Passage-Question."}, {"title": "4.4 Evaluation", "content": "We use Recall as our primary metric to evaluate the retrieval performance, as it focuses on whether the relevant passage is included within a specific ranking range. We also report MRR (Mean Reciprocal Rank), which is similar to Recall but also considers the position at which the relevant passage is ranked.\nTo gauge the alignment between machine-generated answers and ground-truth answers, we use F1, Rouge-L [29], and BLEU-1 [34]. These metrics collectively measure the overlap and sequence matching between generated and actual answers. We evaluate OpenQA performance on our uniformly random sampled test set, which contains 2,000 questions for each datasets. For questions with multiple correct answers in the dataset, we select the answer with the highest cumulative score across all OpenQA metrics."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section we discuss the final OpenQA performance, retrieval performance of trained retrievers, the quality of weak labels, and ablation studies."}, {"title": "5.1 Main Results", "content": "We do not claim that W-RAG outperforms state-of-the-art RAG methods or achieves the best retrieval performance on any of the four datasets. Instead, we highlight the performance improvements of weakly trained retrievers compared to baselines, and demonstrate that the gap between weakly supervised and ground-truth trained retrievers is relatively small. This relationship is evident in both the retrieval performance and the final RAG for OpenQA performance.\nThe goal of W-RAG is to enhance LLM generation quality by providing relevant evidence passages. Tabel 1 presents the main OpenQA results using different retrievers in the RAG pipeline, evaluated on 2,000 test questions for each dataset. For these experiments, only the top 1 retrieved passage is added to the prompt, and the LLM is limited to generating 20 tokens. The Naive and Groundtruth baselines represent the lower and upper bounds, respectively. The Naive baseline excludes any supplementary passage, while Groundtruth includes the best passage. Any retriever that retrieves relevant passages should outperform the Naive baseline, while surpassing the Groundtruth baseline would be very unexpected. Results show consistent improvements with W-RAG trained retrievers across all datasets, with most of them being statistically significant. The relatively small gap between W-RAG trained and ground-truth trained retrievers suggests that W-RAG data approaches the quality of human-labeled data.\nOne baseline, ColBERTinit (initialized with bert-base-uncased), underperforms the Naive baseline, likely due to irrelevant passages being retrieved, introducing noise that misguides Llama3. We notice that for NQ, SQUAD, and WebQ, all the OpenQA metrics are small. This is due to the brevity of ground-truth answers (average length 2 words). For example, in response to the NQ question \"When did Big air Snowboarding become an Olympic sport?\", Llama3 outputs the correct answer: \"Big Air Snowboarding was first added to the 2018 Winter Olympics.\" Since the ground-truth answer given by NQ is just \"2018\", standard OpenQA metrics like F1 and Rouge-L would yield low scores due to the denominator of the Precision term being the length of the generated answer, while the numerator is n-gram overlap which is at most 1 in this case. Despite this, these metrics remain valuable; if Llama3's answer omitted \"2018,\" the score would be zero. The absolute metric values are less important than the relative differences across settings. Although token length could be treated as a hyperparameter, we limited generation to 20 tokens across datasets for consistency."}, {"title": "5.2 Retrieval Results", "content": "The retrieval results of different trained retrievers are presented in Table 2. These results are based on 2,000 test questions for each trained or baseline retriever. On the MSMARCO QnA dataset, all retrievers trained on W-RAG data outperformed all baselines, while DPRReCon consistently outperformed all unsupervised baselines, except for SQuAD where BM25 performed best. As expected, DPR trained on ReContriever showed better retrieval performance than DPR trained from scratch. Similar to the OpenQA task, W-RAG trained retrievers slightly underperform compared to those trained on ground-truth data.\nAlthough there appears to be a positive correlation between retrieval performance and OpenQA performance, this relationship is noisy and sometimes inconsistent. For instance, W-RAG trained ColBERTbase achieved nearly 4% higher Recall@1 than ReContriever on MSMARCO QnA, ranking the relevant passage at top 1 for about 80 more questions. However, when these top 1 passages are used for OpenQA, ColBERTbase performed slighly worse than ReContriever. Similarly, ground-truth trained DPRReCon significantly outperformed W-RAG trained DPRReCon in Recall@1 on MSMARCO QnA, yet their OpenQA performance was almost identical when using the top-ranked passage.\nThese findings suggest that the traditional definition of relevance may not directly impact the answer quality in RAG systems. W-RAG's top passage is selected based on its ability to elicit the correct answer from the LLM, while ground-truth passages are typically chosen based on term overlap, semantic similarity, or human judgment. Contrary to expectations, better retrieval does not always lead to better OpenQA. This raises the question: \"Is the relevance definition in existing datasets truly effective for evaluating retrievers in RAG systems?\""}, {"title": "5.3 W-RAG Labels", "content": "After generating all of W-RAG data, we examine its retrieval performance and the quality of the weakly labeled rank lists. In Table 3, a significant gap between BM25 and Llama3's reranking performance at Recall@1 is evident, which justifies our choise of only using the top 1 passage as the positive when training different retrievers. We also observe that Llama3's performance declines as the number of passages increases. This is likely due to the fact that starting from around the 50th position, the difference in answer likelihood between adjacent passages diminishes to negligible values, such as exp(-3.24815) - exp(-3.24899) \u2248 3 \u00d7 10-5."}, {"title": "5.4 Ablation Study", "content": "We assessed the quality of W-RAG data generated by different LLMs, specifically their ability to rerank passages based on answer likelihood. This is illustrated in Figure 3, showing the reranking performance of various LLMs on 500 questions from the validation set of MSMARCO QnA. Although the LLMs notably differs at Recall@5, they exhibit consistent trends with each other. We notice Llama3-8B-Instruct is weak when compared to other examined LLMs. However, since their recall distributions are similar, any of these LLMs can serve as the reranker. We chose to use Llama3-8B-Instruct for all our experiments.\nUsing Llama3-8B-Instruct, we evaluated reranking and OpenQA performance, as presented in Table 4. UPR [39], which ranks passages based on question likelihood, serve as an unsupervised approach for generating training labels. With weak supervision of ground-truth answers, W-RAG's zero-shot, one-shot, and two-shot prompts significantly outperforms BM25 and UPR in both reranking and OpenQA metrics. However, the differences between the various shot prompts are not very consistent and are not significant. We hypothesize that this inconsistency may have stemmed from the LLM misinterpreting example passages as the relevant passage, or if the question types are drastically different between the example and the given question, thus introducing noise and reducing result reliability. Due to this, we conducted all subsequent experiments using zero-shot prompts, shown in Figure 2.\nDuring the final OpenQA, we also study the impact of giving more evidence passages, shown in Table 5. We observe a consistent and steady increase in all OpenQA metrics when more evidence passages are inserted into the prompt. This behavior is expected because the more passages supplied, the better chance a good passage is within them. However, latency also grows as we increase the number of passages."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "This paper introduces a general framework, W-RAG, for extracting weak labels from question-answer pairs to address the scarcity of training data for dense retrieval in RAG for OpenQA. W-RAG reranks the top-K initially retrieved passages by the probability of LLMs generating the OpenQA answer conditioned on each question-passage pair. This ranking score computation method aligns well with the latest study [6], which states that the retrieved passage should answer the question; otherwise, performance will be negatively impacted. Extensive experimental results on four public OpenQA datasets demonstrate the effectiveness of W-RAG.\nIn future work, we plan to explore which types of passages most effectively enhance RAG's performance in OpenQA, as indicated by [6], where even randomly sampled tokens were beneficial in some cases. Understanding the types of passages preferred by LLMs will allow us to design more effective dense retrieval methods, including new structures and evaluation metrics. Additionally, the compression of retrieved passages warrants further study, as directly feeding all retrieved passages to LLMs not only increases computational complexity but also introduces significant noise. Finally, even with a ground-truth evidence passage, RAG can still produce incorrect answers, known as hallucinations; enhancing the robustness of RAG in OpenQA is another promising direction for future research."}]}