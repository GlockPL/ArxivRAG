{"title": "Disentangled and Self-Explainable Node Representation Learning", "authors": ["Simone Piaggesi", "Andr\u00e9 Panisson", "Megha Khosla"], "abstract": "Node representations, or embeddings, are low-dimensional vectors that capture\nnode properties, typically learned through unsupervised structural similarity objec-\ntives or supervised tasks. While recent efforts have focused on explaining graph\nmodel decisions, the interpretability of unsupervised node embeddings remains\nunderexplored. To bridge this gap, we introduce DISENE (Disentangled and\nSelf-Explainable Node Embedding), a framework that generates self-explainable\nembeddings in an unsupervised manner. Our method employs disentangled rep-\nresentation learning to produce dimension-wise interpretable embeddings, where\neach dimension is aligned with distinct topological structure of the graph. We for-\nmalize novel desiderata for disentangled and interpretable embeddings, which drive\nour new objective functions, optimizing simultaneously for both interpretability\nand disentanglement. Additionally, we propose several new metrics to evaluate\nrepresentation quality and human interpretability. Extensive experiments across\nmultiple benchmark datasets demonstrate the effectiveness of our approach.", "sections": [{"title": "Introduction", "content": "Self-supervised and unsupervised node representation learning [Ham20] provide a powerful toolkit\nfor extracting meaningful insights from complex networks, making them essential in modern AI and\nmachine learning applications related to network analysis [DLZ+24]. These methods offer flexible\nand efficient ways to analyze high-dimensional networks by transforming them into low-dimensional\nvector spaces. This transformation enables dimensionality reduction, automatic feature extraction,\nand the use of standard machine learning algorithms for tasks such as node classification, clustering,\nand link prediction [KSA19]. Furthermore, self-supervised node representations, or embeddings,\nenable visualization of complex networks and can be transferred across similar networks, enhancing\nunderstanding and predictive power in fields ranging from social networks to biological interactions.\nAlthough widely adopted, unsupervised representation learning methods often face substantial\nchallenges in terms of interpretability, necessitating complex and indirect approaches to understand\nwhat the learned embeddings actually represent [PKPA24, IKA20, GBH19]. This raises a critical\nquestion: What information do these embeddings encode?\nWhile there has been a large body of work on explainable GNN models, limited attention has been\ngiven to embedding methods, which are the fundamental building blocks of graph based models.\nExisting efforts to explain embeddings are predominantly post-hoc [PKPA24, GBH19, KMA21,\nDJG18] and heavily dependent on the initial embedding techniques used. Some approaches [PK\u03a1\u039124]"}, {"title": "Preliminaries and Related Work", "content": "Given an undirected graph G = (V, E), node embeddings are obtained through an encoding function\nh : V \u2192 RK that map each node to a points of a K-dimensional vector space RK, where typically\nD << |V|. We denote the K-dimensional embedding of a node v \u2208 V as h(v) = [h\u2081(v),...,h\u03ba(v)],\nwhere ha(v) represents the value of the d-th feature of the embedding for node v. Alternatively,\nwe can represent all node embeddings collectively as a matrix H(G) \u2208 RV\u00d7K, where each entry\nHvd = hd(v) corresponds to the d-th feature for node v. We can also refer to columns of such matrix,\nH.,d, as the dimensions of the embedding model space.\nNode embeddings interpretability. Node embeddings are shallow encoding techniques, often\nbased on matrix factorization or random walks [QDM+18]. Since the latent dimensions in these\nmodels are not aligned with high-level semantics [\u015eUY+18, PDCM22], interpreting embeddings\ntypically involves post-hoc explanations of their latent features [GBH19, KMA21]. Other works\npropose alternative methods to modify existing node embeddings, making them easier to explain with\nhuman-understandable graph features [PKPA24, SCER24]. From a different viewpoint, [SR24] ex-\nplore how understandable are the embedded distances between nodes. Similarly, [DJG18] investigate\nwhether specific topological features are predictable, and then encoded, in node representations.\nGraph neural networks interpretability. Graph Neural Networks (GNNs) [WPC+20] are deep\nmodels that operate via complex feature transformations and message passing. In recent years, GNNs\nhave gained significant research attention, also in addressing the opaque decision-making process.\nSeveral approaches have been proposed to explain GNN decision process [YYGJ22], including\nperturbation approaches [YBY+19, YYW+21, FKRA22], surrogate model-based methods [VT20,\nHYT+22], and gradients-based methods [PKR+19, SLWL+20]. In parallel, alternative research\ndirections focused on concept-based explanations, i.e. high-level units of information that further\nfacilitate human understandability [MKSL21, XBG+23, MBK+22].\nDisentangled learning on graphs. Disentangled representation learning seeks to uncover and\nisolate the fundamental explanatory factors within data [WCT+22]. In recent years, these techniques\nhave gained traction for graph-structured data [LWWX20, LWZ+21, YFSW20, FG24]. For instance,\nFactorGCN [YFSW20] disentangles an input graph into multiple factorized graphs, resulting in\ndistinct disentangled feature spaces that are aggregated afterwards. IPGDN [LWWX20] proposes\na disentanglement using a neighborhood routing mechanism, enforcing independence between the\nlatent representations as a regularization term for GNN outputs. Meanwhile, DGCL [LWZ+21]\nfocuses on learning disentangled graph-level representations through self-supervision, ensuring that\nthe factorized components capture expressive information from distinct latent factors independently."}, {"title": "Our Proposed Framework: DISENE", "content": "In this section, we begin by outlining the key desiderata for achieving disentangled and self-\nexplainable node representations. Next, we design a novel framework that meets these objectives by\nensuring that the learned node representations are both disentangled and interpretable. Finally, we\nintroduce new evaluation metrics to effectively assess the quality of node representation learning in\nboth disentangled and explainable settings."}, {"title": "Core Objectives and Desiderata", "content": "In the context of self-supervised graph representation learning, we argue that learning self-explainable\nnode embeddings amounts to reconstructing the input graph in a human-interpretable fashion. Tra-\nditionally, dot-product models based on NMF [YL13] and LPCA [CRR+24] decompose the set of\ngraph nodes into clusters, where each entry of the node embedding vector represents the strength\nof the participation of the node to a cluster. In this scenario, the dot-product of node embeddings\nbecomes intuitively understandable, as it reflects the extent of shared community memberships\nbetween nodes-thereby providing a clear interpretation of edge likelihoods. This concept is also\nrelated to distance encoding methods [LWWL20, KSN23], where a node feature ha(u) is expressed\nas a function of the node's proximity \u03da(u, Sa) = AGG({$(u, v), v \u2208 Sa}) to the anchor set Sa \u2282 V,\nusing an aggregation function AGG. Typically, distance encodings are constructed by randomly"}, {"title": "Our approach: DISENE", "content": "Building upon the above components, introduced to satisfy our desiderata for interpretable node\nembeddings, we present our approach, DISENE. Specifically, DISENE takes as input the raw node\nattributes X \u2208 RIVI\u00d7F and, depending on the encoder architecture, also the adjacency matrix A \u2208\nR|V|\u00d7|\ubbf8. The input is encoded into an intermediate embedding layer Z \u2208 R|V|\u00d7D. Next, DISENE\nprocesses the embedding matrix Z to compute the likelihood of link formation between node pairs,\ngiven by \u0177(u, v; h) = \u03c3(h(u) Th(v)) where h(v) = p(WTz(v)) are the final node representations\nin H\u2208 R|V|\u00d7K, obtained by applying a linear transformation W \u2208 RD\u00d7K followed by a non-linear\nactivation function p. To encode z, we employ architectures incorporating fully connected layers and\ngraph convolutional layers [WSZ+19]. This process can be further enhanced by integrating more\ncomplex message-passing mechanisms or MLP operations. For example, the message-passing could\ninitiate from an MLP-transformed node attribute matrix, MLP(X), or incorporate more sophisticated\narchitectures beyond simple graph convolutions for increased expressiveness [XHLJ18, VCC+17].\nThe embeddings are optimized by combining the previously described objective functions for preserv-\ning structural faithfulness and achieving structural disentanglement, thereby improving dimensional\ninterpretability. To avoid degenerate disentanglement solutions-specifically, the emergence of \u201cempty\u201d\nclusters characterized by near-zero columns in F that, while orthogonal to others, fail to convey\nmeaningful information-we introduce a regularization strategy. This regularization ensures a minimal\nbut significant level of connectivity within each topological substructure. Specifically, we enforce\nthat the total amount of predicted edges in each anchor subgraph Gk - \u03a3\u03c5,\u03bd\u03b5\u03bd \u03c6\u03ba (u, v; h) - to be\nnon-zero. We found a more stable and precise approach by enforcing that the aggregated node\nfeatures of each embedding dimension are non-zero, achieved by maximizing the entropy:\nH = -\\sum_{d=1}^{K} \\frac{\\sum_{u} h_d(u)}{\\|\\sum_{u} h(u)\\|_1} \\log \\frac{\\sum_{u} h_d(u)}{\\|\\sum_{u} h(u)\\|_1}\nThus, the model is optimized by minimizing the following comprehensive loss function:\nL = L_{rw} + L_{dis} + \\lambda_{ent} \\left(1 - \\frac{H}{\\log K}\\right)\nThe hyperparameter \u03bbent determines the strength of the regularization, controlling the stability for\nexplanation subgraph sizes across the various latent dimensions."}, {"title": "Proposed Evaluation Metrics", "content": "In the following, we introduce novel metrics to quantify multiple aspects related to interpretability\nand disentanglement in node embeddings, which we use to compare models in our experiments.\nComprehensibility. Comprehensibility measures how closely the identified topological expla-\nnations align with ground-truth clusters, which are crucial in the evolution of numerous complex\nreal-world systems, i.e. community modules [GN02, HDF14]. We evaluate comprehensibility by\ntreating edges in explanation masks {M(d)}d=1,...,K as retrieved items from a query, and measuring\ntheir overlap with the edges in the ground-truth communities using precision, recall, and F\u2081-score.\nLet C(E) = {C(1), ..., C(m)} denotes the set of truthful link communities of the input graph\u00b2.\nAsso-\nciated to partition C(i), we define ground-truth edge masks C(i) \u2208 {0,1}V\u00d7V with binary entries\nC = [(u, v) \u2208 Ci]. Comprehensibility score is given by the maximum F\u2081-score across ground-truth\nindex:\nComp(M^{(d)}) = \\max_i \\{F_1(M^{(d)}, C^{(i)})\\} = \\max_i \\{\\frac{2}{\\text{prec}(M^{(d)}, C^{(i)})^{-1} + \\text{rec}(M^{(d)}, C^{(i)})^{-1}}\\}\nFor precision, we weigh relevant item scores with normalized embedding masks values:\n\\text{prec}(M^{(d)}, C^{(i)}) = \\frac{\\sum_{uv} M_{uv}^{(d)} C_{uv}^{(i)}}{\\sum_{uv} M_{uv}^{(d)}}\nFor recall, we weigh binarized embedding masks values with\nnormalized ground-truth scores\u00b3: \\text{rec}(M^{(d)}, C^{(i)}) = \\frac{\\sum_{uv} [M_{uv}^{(d)} > 0] C_{uv}^{(i)}}{\\sum C_{uv}^{(i)}}\nSparsity. We refer to sparsity as the amount of disorder in the dimension's explanations, it is\ngenerally defined as the ratio of the number of bits needed to encode an explanation compared to\nthose required to encode the input [FKRA22]. Given that concise explanations are more effective in\ndelivering clear insights, enhancing human understanding, we evaluate sparsity by measuring the\nnormalized Shannon entropy over the mask distribution:\nSp(M_{d}) = -\\frac{1}{\\log |E|} \\sum_{(uv) \\in E} \\frac{M_{uv}^{(d)}}{\\sum_{u'v'} M_{u'v'}^{(d)}} \\log \\frac{M_{uv}^{(d)}}{\\sum_{u'v'} M_{u'v'}^{(d)}}\nA lower entropy in the mask distribution indicates higher sparsity.\nOverlap Consistency. In explaining latent space representations, it is essential to comprehend\nhow input space factors influence specific latent features. A well-structured, disentangled latent\nspace should correspond to distinct, uncorrelated topological structures. We aim to quantify how\ndifferent topological components affect pairwise feature correlations in the latent space. To achieve\nthis, we propose a metric that measures the strength of association between the physical overlap\nof the explanation substructures {Ga} and the correlation among corresponding latent dimensions\n{H:,d}. We compute the overlap between two subgraph components using the Jaccard Similarity\nIndex (JSI) of their edge sets from Eq. (2): \\text{JSI}(d, l; h) = \\frac{|E_d \\cap E_l|}{|E_d \\cup E_l|}. The overlap consistency (OvC)\nmetric measures the linear correlation between the pairwise JSI values and the squared Pearson\ncorrelation coefficients (p2) of the embedding features:\n\\text{OvC}(h) = \\rho([\\text{JSI}(d, l; h)]_{d < l}, [\\rho^2(H_{:, d}, H_{:, l})]_{d < l})\nwhere [*]d<1 denotes the condensed list of pair-wise similarities. By using p\u00b2 we remain agnostic\nabout the sign of the correlation among latent features, since high overlaps could originate from both\ncases. This metric provides a systematic way to assess the extent to which topological partitions\nalign with the distribution of features in the embedding space, thus offering deeper insights into the\ninterpretability and disentanglement of the learned representations."}, {"title": "Positional Coherence", "content": "In unsupervised node embeddings, it is crucial to assess how well the\nlatent space captures the graph's structure by encoding the positional relationships of nodes. An\neffective representation should preserve meaningful spatial properties that reflect node proximity\nand connectivity patterns. To achieve this, we propose to measure the extent to which node entries\nreflect their positions within the graph. Typically, positional encoding [LCSH21, LWWL20, YYL19]\ninvolves the use of several sets of node anchors Sa CV that establish an intrinsic coordinate\nsystem. This system influences the node u's features based on the node's proximity ((u, Sa) =\nAGG({\u03b6(u, v), v \u2208 Sa}), where AGG denotes a specific pooling operation. As node proximity, we\nused the inverse of the shortest path distance (spd(u, v) = (1 + dspd(u, v))\u00af\u00b9. As the anchor sets,\nwe chose the embedding substructures used for explanations, Sa = Va.\nFor a specified pair of dimensions (d, l), we assess the correlation between node features along\ndimension d and the corresponding distances to the topological component indexed by l via Feature-\nProximity Correlation: FPC(d, l; h) = p([spd(u, v)]_{ue,}, H.,1). The positional coherence metric\n(PoC) is defined to specifically evaluate the degree to which each feature d is uniquely correlated with\nits corresponding topological component Va, without being significantly influenced by correlations\nwith other substructures. This metric is calculated as the ratio of the average FPC for the given\ndimensions to the average FPC computed with pairs of permuted dimensions:\nPoC(h) = \\frac{\\sum_{d} FPC(d, d; h)}{(\\sum_{d} FPC(d, \\pi(d); h))_{\\pi}}\nwhere (.) denotes an empirical average over multiple permutations. By comparing with random\nfeature-subgraph pairs, the metric avoid promoting models with redundancies in the latent features,\nwhere high correlations with other topological components are possible."}, {"title": "Plausibility", "content": "Given the importance of node representations in\ndownstream tasks, it is crucial to assess whether the features\ninfluencing predictions align with human expectations using\nsynthetic benchmarks. To this end, we construct instance-wise\nexplanations to determine if key features correspond to the\ntopological structures behind the ground-truth labels. Typical\npost-hoc explainers [BGG+23] that produce feature importance\nscores often fail in this context because node embeddings are\ninherently uninterpretable, leading to useless explanations. Our\napproach overcomes this limitation by mapping feature im-\nportance back to the graph's structural components, enabling\na more meaningful evaluation of how well the embeddings\ncapture the underlying factors driving node behavior.\nConsider training a downstream binary classifier b : RK \u2192\n[0, 1], such as for node classification or link prediction. We\ndetail the procedure for link prediction here , \nFor an edge instance h(u, v) (which could be\nderived from node-pair operations such as h(u)h(v)), we employ post-hoc methods to determine the\nfeature relevance for the classifier prediction on the node pair instance (u, v), {\u03a8;(u, v; b)}j=1,...,K\nthat outputs important score for each of the embedding dimensions. Similarly to scores in Eq. (1),\nwe define task-based masks B(i) \u2208 RVXV RYXV which aggregate the individual logics of the classifier\npredictions: B (\u03a8;; b) = max{0, \u03a8; (u, v; b)}. To evaluate the consistency of a prediction, we\nconsider the F\u2081-score in Eq. (5) related to the ground-truth structure of the edge under study,\nindexed by g(u, v). Specifically, we define plausibility for an individual prediction as the average\ncomprehensibility relative to the instance ground-truth, weighted by the computed feature importance:\nPl(u, v; b) = \\frac{\\sum_{d=1}^{K} f(\\Psi_j (u, v; b))F_1(B^{(j)}, C^{g(u,v)})}{\\sum_{j=1}^{K} f(\\Psi_j (u, v; b))}\nwhere f is a function guaranteeing the non-negativity of relevance weights. This ensures that only\nthe features that are both interpretable and significant to the local prediction contribute substantially\nto the score, penalizing instead those important features that are not human-comprehensible."}, {"title": "Experiments", "content": "We conduct large-scale experiments to answer the following research questions.\n\u2022 Human understandability: How comprehensible and sparse are the explanation substruc-\ntures generated by DISENE?\n\u2022 Structural disentanglement: Do the disentangled subgraphs reveal intrinsic properties of\nnode embeddings, like feature correlations and latent positions?\n\u2022 Utility for downstream tasks: Are the identified substructures plausible and coherent\nenough to serve as explanations in downstream tasks?\nTo address our research questions, we extract topological components of multiple embedding methods,\ntrained on different graph-structured data, by computing edge subsets defined in Eq (2), and analyzing\nembedding metrics defined in Section 3.3. In the following sections, we describe the data, models,\nthe experimental setup and results from the comparison."}, {"title": "Datasets and competitors", "content": "Datasets. We ran experiments on four real-world datasets (CORA, WIKI, FACEBOOK, PPI), and\nfour synthetic datasets (RINGOFCLIQUES, STOCHASTICBLOCKMODEL, BA-CLIQUES and ER-\nCLIQUES). RINGOFCLIQUES and STOCHASTICBLOCKMODEL are implemented in the NetworkX\nlibrary. Statistics for these datasets are provided in Table A1 in the Appendix. Additionally\nwe employ several biological datasets (see Appendix A.3) for the evaluation on multi-label node\nclassification). BA-CLIQUES is a variation of the BA-SHAPES [YBY+19] where we randomly attach\ncliques instead of house motifs to a Barab\u00e1si-Albert graph. Similarly, ER-CLIQUES is generated by\nattaching cliques to an Erd\u0151s-R\u00e9nyi random graph. All synthetic graphs contain 32 cliques with 10\nnodes each as ground-truth sub-structures. For synthetic data, we present only results for plausibility\nmetrics, leaving the other findings in the Appendix A.4.\nMethods. We compare different node embedding methods. Competitors include DEEPWALK\n[PARS14], Graph Autoencoder (GRAPHAE) [SHV21], INFWALK [CM20], and GRAPHSAGE\n[HYL17]. Moreover, we apply the DINE retrofitting approach [PKPA24] to post-process embeddings\nfrom DEEPWALK and GRAPHAE. We evaluate our method DISENE in two variants: a 1-layer\nfully-connected encoder (DISE-FCAE) and a 1-layer convolutional encoder (DISE-GAE). GNN-\nbased methods are trained using the identity matrix as node features. Details on hyperparameters and\ntraining settings are provided in the Appendix A.2.\nSetup. In experiments on real-world graphs, we investigate latent space interpretability and disen-\ntanglement metrics by keeping the output embedding dimension fixed at 128. This dimensionality was\nchosen to ensure that all methods achieve optimal performance in terms of test accuracy, specifically\nfor link prediction (see the Appendix A.3 for extensive downstream task results). For synthetic data,\nsince we investigated plausibility metric referred to a downstream classifier, thus we did not focus on\na specific dimension but we selected the best score metric varying the output dimensions in the list\n[2, 4, 8, 16, 32, 64, 128]. Each reported result is an average over 5 runs. For link prediction, we use a\n90%/10% train/test split, and for node classification, we use an 80%/20% split. All results refer to\nthe training set, except for downstream task experiments, where we present results for the test set."}, {"title": "Results and Discussion", "content": "Are the topological substructures both comprehensible and sparse to support human under-\nstandability? Here we explore how well the represented topological structures can serve as global\nexplanations for the node embeddings, quantifying the Comprehensibility in the terms of associations\nbetween model parameters and human-understandable units of the input graph, as well as the Sparsity\nof these associations. In Table 1 we show compact scores as the average values \\frac{1}{K}\\sum_{d=1}^{K} Comp(M_{d})\nand 1-\\frac{1}{K}\\sum_{Kd=1} Sp(M_{d}) over all the embedding features. For sparsity we report the value subtracted\nfrom 1 to have all the scores better with higher values."}, {"title": "Can the identified subgraphs explain the intrinsic characteristics of the node embeddings?", "content": "Here we explore how well the defined topological units represent information in the node embedding\nspace, providing insights into how the relative and absolute positioning of topological structures\ninfluences the feature encoding within a graph. By quantifying these relationships, we can better\nunderstand the underlying patterns and structural information in the graph, potentially leading to more\nrobust and interpretable models. In Table 2 we report Positional Coherence and Overlap Consistency\nfor the examined embedding methods. For the second metric, as node proximity we used the inverse\nof the shortest path distance with sum as pooling."}, {"title": "Conclusions", "content": "We present DISENE, a novel framework for generating self-explainable unsupervised node embed-\ndings. To build our framework, we design new objective functions that ensure structural faithfulness,\ndimensional explainability, and structural disentanglement. Unlike traditional GNN explanation\nmethods that typically extract a subgraph from a node's local neighborhood, DISENE introduces a\nparadigm shift by learning node embeddings where each dimension captures an independent structural\nfeature of the input graph. Additionally, we propose new metrics to evaluate the human interpretability\nof explanations, analyze the influence of spatial structures and node positions on latent features, and\napply post-hoc feature attribution methods to derive task-specific instance-wise explanations."}]}