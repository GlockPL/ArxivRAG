{"title": "Personality-Guided Code Generation Using Large Language Models", "authors": ["Yaoqi Guo", "Zhenpeng Chen", "Jie M. Zhang", "Yang Liu", "Yun Ma"], "abstract": "Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its potential to streamline software development. Inspired by research that links task-personality alignment with improved development outcomes, we conduct an empirical study on personality-guided code generation using large language models (LLMs). Specifically, we investigate how emulating personality traits appropriate to the coding tasks affects LLM performance. We extensively evaluate this approach using seven widely adopted LLMs across four representative datasets. Our results show that personality guidance significantly enhances code generation accuracy, with improved pass rates in 23 out of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%. Additionally, personality guidance can be easily integrated with other prompting strategies to further boost performance.", "sections": [{"title": "1 Introduction", "content": "Code generation, which aims to automatically produce source code from natural language descriptions, has attracted significant attention from both academia and industry due to its potential to streamline software development (Jiang et al., 2024). The emergence of large language models (LLMs) has advanced this field by enabling the effective generation of complete, executable code (Chen et al., 2021; Karmakar and Robbes, 2021). Additionally, specialized LLMs, such as CodeaLlama (Roziere et al., 2023) and DeepSeek-Coder (Zhu et al., 2024), have further refined these capabilities by focusing specifically on programming tasks.\nPrevious research has observed that software development outcomes improve when individuals are assigned tasks that match their personality types"}, {"title": "2 Related Work", "content": "This section summarizes existing work highly relevant to this paper."}, {"title": "2.1 LLM for Code Generation", "content": "The emergence of LLMs such as ChatGPT has profoundly transformed the landscape of automated code generation, making LLM-driven code generation a highly active area in both industry and the Natural Language Processing and Software Engineering communities (Jiang et al., 2024). On one hand, researchers have explored the effectiveness of general-purpose LLMs such as ChatGPT (OpenAI, 2022), GPT-4 (Achiam et al., 2023), and Llama (Dubey et al., 2024) for code generation. On the other hand, industry vendors have developed specialized LLMs designed to optimize code-related tasks, such as DeepSeek-Coder (Zhu et al., 2024) and CodeLlama (Roziere et al., 2023). In this paper, to comprehensively evaluate the effectiveness of personality-guided code generation, we include both widely-used general-purpose LLMs and code-specific LLMs in our evaluation."}, {"title": "2.2 Prompt Engineering for Code Generation", "content": "Prompt engineering is an important strategy for improving the performance of LLM-based code gen-"}, {"title": "2.3 Personality of LLM", "content": "Several studies have investigated the personality traits exhibited by LLMs (Song et al., 2023; Caron and Srivastava, 2022; Serapio-Garc\u00eda et al., 2023; Huang et al., 2023). For example, Huang et al. (Huang et al., 2023) used trait theory, a psychological framework, to analyze the behavioral patterns of LLMs, finding that ChatGPT consistently exhibits an ENFJ personality, regardless of instructions or context. Building on the insight from prior research that diverse personality profiles within development teams are linked to higher-quality software outcomes (Pieterse et al., 2018; Capretz and Ahmed, 2010), we explore the impact of assigning diverse personalities to LLMs when they tackle different coding tasks, aiming to ensure personality diversity and potentially enhance code generation accuracy."}, {"title": "3 Personality-Guided Code Generation", "content": "This section presents our LLM-based pipeline for personality-guided code generation. As illustrated in Figure 1, the pipeline consists of two key components: personality generation and code generation. The personality generation component is responsible for creating a programmer's personality suitable for addressing a given coding task. The code generation component then uses this generated personality to produce the code for the task. In the following, we define the coding task and provide a detailed description of each component.\nTask Definition: We focus on function-level code generation tasks, which are among the most widely studied in the literature (Zheng et al., 2023). These"}, {"title": "4 Experimental Setup", "content": "This section outlines the experimental setup used to evaluate personality-guided code generation."}, {"title": "4.1 Research Questions (RQs)", "content": "We aim to answer the following five RQs to evaluate personality-guided code generation.\nRQ1 (Effectiveness): How effective is personality-guided code generation in enhancing generation accuracy?\nRQ2 (Influencing factors): What potential factors influence the effectiveness of personality-guided code generation?\nRQ3 (Combination with other strategies): Can personality-guided code generation be combined with other prompting strategies to further improve generation accuracy?\nRQ4 (Prompt Design): How does including the detailed personality description during code generation affect the accuracy of the generated code?\nRQ5 (Impact of personality-generation LLM): If we use other LLMs to generate personality, how"}, {"title": "4.2 Datasets", "content": "We evaluate personality-guided code generation using four widely recognized datasets: MBPP Sanitized (Austin et al., 2021), MBPP+ (Liu et al., 2024a), HumanEval+ (Liu et al., 2024a), and APPS (Hendrycks et al., 2021). In the following, we provide a brief description of each dataset.\n\u2022 MBPP Sanitized includes 427 crowd-sourced Python problems designed for entry-level programmers, each with a task description, code solution, and several automated test cases.\n\u2022 MBPP+ improves upon MBPP by fixing ill-formed problems and incorrect implementations, while expanding the test suite by 35 times for more robust evaluation.\n\u2022 HumanEval+ offers 164 manually curated Python problems, each featuring a function signature, docstring, code body, and multiple unit tests to detect errors that LLMs might miss.\n\u2022 APPS presents a comprehensive benchmark of 10K Python problems across varying difficulty levels. Given the large size of the dataset, we randomly sample 500 problems from the interview-level set to balance evaluation depth with computational efficiency."}, {"title": "4.3 LLMs Used for Code Generation", "content": "We adopt seven LLMs for evaluation, consisting of four general-purpose LLMs and three specifically designed for code-related tasks. The general LLMs include GPT-40 (OpenAI, 2024a), GPT-40 mini (OpenAI, 2024b), Llama3.1 (Dubey et al., 2024), and Qwen-Long (Yang et al., 2024), while the code-specific LLMs include DeepSeek-Coder (Zhu et al., 2024), Codestral (Mistral, 2024), and CodeLlama (Roziere et al., 2023)."}, {"title": "4.4 Evaluation Metric", "content": "We evaluate code generation accuracy by calculating the pass rate across all tasks in the dataset. An LLM is considered to pass a coding task if the code it generates successfully passes all test cases for that task. To ensure the reliability of our results, we run each LLM on each dataset three times and report the average pass rate as the final outcome. Specifically, the pass rate P of an LLM on a dataset is calculated as:\n$P = \\frac{1}{3} \\sum_{i=1}^{3} \\frac{c_i}{cnt}$\nwhere cnt represents the total number of tasks in the dataset, and $c_i$ is the number of tasks successfully passed by the LLM in the ith run."}, {"title": "5 Results", "content": "This section answers our research questions with the experimental results."}, {"title": "5.1 RQ1: Effectiveness", "content": "Table 2 presents the comparison of pass rate for LLMs with and without personality guidance across different datasets. Specifically, for each LLM on each dataset, we evaluate two approaches: directly prompting the LLM to generate code as a programmer (the \u201cDirect\u201d row) and using the personality-guided method (the \u201cMBTI\u201d row). We report the pass rates for both approaches on each dataset, along with the change in performance introduced by personality guidance.\nOverall, the personality-guided approach improves the pass rate of code generation in 23 out of 28 combinations of LLMs and datasets. In 11 combinations, the improvement exceeds 5%, and in 5 combinations, it surpasses 10%. Notably, the pass rate of GPT-40 mini on the MBPP Sanitized dataset increases by 12.9%."}, {"title": "5.2 RQ2: Influencing Factors", "content": "Furthermore, we explore the potential factors influencing the effectiveness of personality-guided code generation, examining them from both the model and dataset perspectives."}, {"title": "5.2.1 Model Perspective Analysis", "content": "From Table 2, we observe that only GPT-40, DeepSeek-Coder V2, and CodeLlama exhibit slight decreases in pass rates in one or two cases. Additionally, in the \u201cDirect\u201d mode, GPT-40 and DeepSeek-Coder consistently achieve the highest"}, {"title": "5.2.2 Dataset Perspective Analysis", "content": "From Table 2, we observe that pass rate decreases occur only in the HumanEval+ and APPS datasets. Notably, HumanEval+ is the easiest dataset, as five LLMs achieve a pass rate higher than 75%. In contrast, APPS is the most challenging, with no LLM achieving a pass rate above 40%.\nThese observations suggest that the difficulty level of the dataset influences the effectiveness of personality-guided code generation. It is reasonable that on easier tasks, where models already perform well (as seen with HumanEval+), personality guidance may offer limited improvement, or in some cases, a slight decrease. On highly challenging tasks like APPS, where baseline performance is lower, there may be more room for improvement, but the complexity of the task might limit the potential gains. Fortunately, the personality-guided approach achieves more than a 5% improvement in pass rates for four out of seven LLMs.\nFurthermore, we analyze the diversity of personality distributions across each dataset. Figure 2 presents the MBTI personality types assigned by GPT-40 for each dataset. We observe that HumanEval+ and APPS exhibit the least diversity, with 78.0% and 90.6% of tasks assigned the INTJ personality, respectively. This suggests that personality diversity may be a potential factor influencing the effectiveness of personality-guided code generation. The more diverse the assigned personalities, the more effective this approach tends to be.\nTo further demonstrate the impact of personality diversity, we set up an additional experiment to investigate the effect of assigning a uniform MBTI personality to all tasks. We select Qwen-Long as the test model because it exhibits the highest average improvement from personality guidance. Additionally, we use the MBPP Sanitized dataset,"}, {"title": "5.3 RQ3: Combination with Other Strategies", "content": "This RQ investigates whether the effectiveness of personality-guided code generation can be further enhanced by combining it with existing prompting strategies. Specifically, we consider two widely used techniques: few-shot learning and Chain of Thought (CoT), both of which are popular methods"}, {"title": "5.4 RQ4: Prompt Design", "content": "In Section 3, we describe that during the code generation process, we provide both the MBTI type and its detailed description. This is based on the assumption that a more detailed personality description may help the LLM better role-play, potentially improving performance. However, existing research suggests that longer prompts can negatively affect LLM performance on the same task (Levy et al., 2024).\nTo address this, in this RQ, we evaluate whether providing a detailed description alongside the MBTI type yields better results than using a shorter prompt that only indicates the MBTI type (e.g., INTJ). As with RQ3, we select the MBPP Sanitized dataset for this experiment."}, {"title": "5.5 RQ5: Impact of Personality-generation LLM", "content": "In Section 3, we describe that GPT-40 is used for personality generation. This RQ evaluates this setting by comparing it to a setting where the code-generation LLM is also used for personality generation. For example, when Qwen-Long is used for code generation, it also generates its own personality for each task. CodeLlama is excluded from the comparison because it lacks the capability to generate appropriate personality. As with RQ3 and RQ4, we use the MBPP Sanitized dataset for this experiment."}, {"title": "6 Conclusion", "content": "This paper presents a large-scale empirical study on personality-guided code generation using LLMs. While existing research typically involves LLMs role-playing as programmers to generate code, this study investigates whether assigning these \u201cprogrammers\" with appropriate personalities can further improve code generation accuracy. To explore this, we conduct an extensive evaluation using four widely-adopted datasets and seven advanced LLMs developed by leading vendors. Our results show that personality guidance significantly boosts code generation accuracy, with pass rates improving in 23 out of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%."}, {"title": "7 Limitations", "content": "As an empirical study, this paper has several limitations. First, the personality traits examined are limited to the MBTI framework. While MBTI is widely used, relying solely on it may not capture the full complexity of personality traits and their potential impact on LLM performance. Second, although we evaluated seven LLMs, including both general-purpose and code-task-specific models, the generalizability of our findings to other LLMs requires further investigation. Third, our study focuses on function-level code generation across four datasets, a common area in the literature. In future work, we plan to extend our evaluation to more complex code generation tasks to broaden the scope of our findings."}]}