{"title": "IMPROVING INSTRUCTION-FOLLOWING IN LANGUAGE MODELS THROUGH ACTIVATION STEERING", "authors": ["Alessandro Stolfo", "Vidhisha Balachandran", "Safoora Yousefi", "Eric Horvitz", "Besmira Nushi"], "abstract": "The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are computed as the difference in activations between inputs with and without instructions, enabling a modular approach to activation steering. We demonstrate how this method can enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following. Our experiments across four models demonstrate how we can use the activation vectors to guide models to follow constraints even without explicit instructions and to enhance performance when instructions are present. Additionally, we explore the compositionality of activation steering, successfully applying multiple instructions simultaneously. Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models. Our findings demonstrate that activation steering offers a practical and scalable approach for fine-grained control in language generation.", "sections": [{"title": "INTRODUCTION", "content": "Instruction-following capabilities of large language models (LLMs) have enhanced their practical applications for real-world usage. These advances are powered by instruction-tuning methods (Ouyang et al., 2022; Bai et al., 2022; Wei et al., 2022; Sanh et al., 2022; Chung et al., 2024), which align the model's responses with user objectives, addressing the gap between pre-training and end-user needs (Askell et al., 2021). Instruction tuning allows users to specify constraints on attributes like format, tone, or length, which direct the model's behavior and output (Zhou et al., 2023b; Zhang et al., 2023; Lou et al., 2024). Gaining a deeper understanding of how LLMs internally represent and follow these instructions is essential for developing more controllable and reliable models.\nIn this paper, we use a mechanistic method to investigate how language models internally represent various instructions and use these representations to influence and control the model's behavior. Prior research has shown that vector representations can be computed for tasks learned in context (Hendel et al., 2023; Todd et al., 2024; Liu et al., 2024) and various stylistic and semantic input features (Zou et al., 2023; Azaria & Mitchell, 2023; Zheng et al., 2024; Templeton et al., 2024; Marks & Tegmark, 2024, inter alia). These representations can be used for activation steering (Subramani et al., 2022): directly intervening on the model's activations to guide the generation. This approach has been successfully applied to control text attributes such as honesty (Li et al., 2023; Qiu et al., 2024; Zou et al., 2023), sentiment (Tigges et al., 2024), output style and tone (Turner et al., 2023; Liu et al., 2024; Scalena et al., 2024; von R\u00fctte et al., 2024), harmfulness (Arditi et al., 2024; Wang & Shu, 2024), and sycophancy (Panickssery et al., 2024; van der Weij et al., 2024).\nHowever, user instructions in generative tasks can be more complex and involve multiple parameters that need to be attended to and satisfied during generation (Sun et al., 2023). For example, users may"}, {"title": "STEERING FOR INSTRUCTION ADHERENCE", "content": "In this section, we define the types of instructions we consider (\u00a72.1), introduce the methodology used to compute the vectors and the steering procedure (\u00a72.2), and describe the experimental setup, including the data, metrics, and evaluation details (\u00a72.3)."}, {"title": "TYPES OF INSTRUCTIONS", "content": "The concept of instruction-following has been used to describe the broad capability of a model to answer any zero-shot query (Wei et al., 2022; Sanh et al., 2022). However, following prior work"}, {"title": "STEERING PROCEDURE", "content": "Activation (or representation) engineering involves constructing vectors of activation values that cause desired changes to output text when added to the forward passes of a frozen LLM (Zou et al., 2023). To identify a direction in the model's residual stream that encodes information about a specific instruction, we use a technique called difference-in-means (Belrose, 2023). This method effectively isolates key feature directions in the model's internal activations (Marks & Tegmark, 2024; Tigges et al., 2024) and has been used to control various behaviors such as refusal and sycophancy (Arditi et al., 2024; Panickssery et al., 2024). We adapt this method to support finer-grained instructions with multiple possible instantiations (e.g., varying length, varying words to include and exclude). The process involves pairing two versions of the same request: one with only the base query (e.g., \"List some facts about Lionel Messi\") and another that additionally includes the instruction we want to represent (e.g., \u201cList some facts about Lionel Messi, ensuring the output is valid JSON\u201d).\nLet us denote these two inputs by x (base query) and x+ (base query with instruction), and consider a set of N such pairs (xi, x+i), i \u2208 {1,...,N}. Let xi,l,x+i,l \u2208 Rdmodel be the values of the residual stream vector on the two queries at the last token of the input at layer l \u2208 {1, ..., L}. We isolate the internal representation corresponding to the instruction by computing the difference in the residual stream vectors between the paired inputs. More formally, we compute a vector u\u012f \u2208 Rdmodel representing the steering direction at layer l for a given instruction as:\n$u = \\frac{v_l}{||v_l||}$, where $v_l = \\frac{1}{N} \\sum_{i=1}^{N} (x^+_{i, l} - x_{i,l})$ (1)\nAveraging over different base queries allows us to capture the activation values most closely associated with the instruction, independent of the base query. The computation of the steering direction is carried out using the representations at the last token of the input, which effectively encapsulate the model's behavior not only for the next-token-prediction task but also for the entire generation that follows (Todd et al., 2024; Scalena et al., 2024).\nAfter identifying the steering direction, we compute the steering vector by re-scaling the unit vector u by a coefficient, c\u2208 R. For format instructions, we use a systematic scaling approach where the value of c is selected to ensure that the residual stream activations are mapped to their mean value on inputs that contain the instruction in question. In particular, during a forward pass on a new example with residual stream values x' \u2208 RLXdmodel at a given token position, we compute:\nc = z \u2212 x\u2032T u , where z = $\\frac{1}{N} \\sum_{i=1}^{N} x^+_iT u$. (2)\nThis dynamic adjustment allows the model to effectively incorporate the constraint without over- or under-correcting its behavior. For length instructions, which have a more continuous nature, we experiment and show results with different values of c, illustrating their impact on the model's"}, {"title": "EXPERIMENTAL SETUP", "content": "Data. We use an augmented version of the IFEval dataset (Zhou et al., 2023a), which consists of 25 distinct instructions, each paired with multiple base queries and expressed in different phrasings, for a total of 541 prompts. To evaluate format instructions, we focus on a subset of 163 examples that specifically relate to the output format. This subset includes instructions such as \u201cThe entire output should be wrapped in JSON format,\" requests to use a particular language (e.g., \u201cPlease respond using only the Punjabi language\"), formatting requirements like \u201cWrap your entire response with double quotation marks,\" and casing instructions (e.g., \u201cAnswer using lowercase letters only\"). A complete list of the instructions used is provided in Appendix A.3 For length instructions, we generate prompts by concatenating base queries from IFEval to instructions derived from templates such as \u201cAnswer using at most {n} sentences.\u201d For word-specific instructions, we use a subset of IFEval containing instructions about the inclusion or exclusion of keywords. The subset has 203 examples, and each example contains a prompt with a single keyword-related instruction. For steering vector computation and layer selection, we construct a separate set of synthetically generated prompts by combining base queries from IFEval with corresponding instruction descriptions to avoid test information leakage. Additional details about the data used are provided in Apps. B and C.\nMetrics. To quantify the models' adherence to format instructions, we compute the \u201cloose\" instruction-following accuracy using the official IFEval evaluation script. For length instructions, we count the number of words or sentences in the model's output. For word-specific constraints, we verify the presence or absence of the specified keywords in the model's response, again using the IFEval evaluation script. We assess the statistical significance of differences in average instruction-following accuracy with and without steering using McNemar's test (McNemar, 1947). Results where steering leads to a significant improvement (p-value < 0.01) are marked with an asterisk (*).\nIn addition to assessing the model's instruction-following capabilities, it is important to verify that the model still effectively addresses the base query, even when generating under constraints. To measure the overall quality of the response, we set up a GPT-40-based evaluation. For each base query x, GPT-40 generates a set of five yes/no questions aimed at assessing the quality of the response to x. For instance, given the query \"Write an essay about the history of Martin Van Buren's presidency,\" GPT-40 generates questions such as \u201cDoes the essay provide context on the political, social, and economic climate during Van Buren's presidency?\u201d and \u201cIs the essay written in clear, grammatically correct English, and does it follow a logical structure?\". Next, given the original query x, a model's response to x, and the generated questions we prompt GPT-40 to answer each question, provide a rationale for each yes/no response, or reply with \u201cN/A\u201d if the question is not applicable. We compute the proportion of questions answered positively for a given query and average this score across all queries to obtain a response quality score. We repeat this experiment 3 times and report the mean and standard error. Additional details are provided in Appendix E.4\nEvaluation. We conduct experiments using the instruction-tuned versions of Phi-3 Mini (Abdin et al., 2024), Gemma 2 2B, 9B (Gemma Team, 2024), and Mistral 7B v0.1(Jiang et al., 2023). In addition, we investigate the transferability of steering vectors from the instruction-tuned to the base versions of Gemma 2 2B and 9B. All models are evaluated in a zero-shot setting, with outputs decoded greedily. We assess our method's performance by steering the model under two input"}, {"title": "FORMAT INSTRUCTIONS", "content": "Representation Analysis. To assess whether the model effectively captures instruction-related information in its internal representations, we analyze Phi-3's residual stream activations at the last token of the input by calculating cosine similarity across three sets of inputs: (1) pairs of inputs that share the same base query, one with and one without the instruction; (2) inputs with different base queries but the same instruction; and (3) inputs with different base queries and no instruction. High similarity in set (2) would suggest that the model captures a shared feature (the instruction), while we expect set (1) to show relatively high similarity due to shared base queries, and set (3) to show lower similarity due to the absence of any shared features. Figure 2 shows how these measures evolve across layers for two instructions. For the \u201cquotation\" instruction, where the model is asked to wrap the output in quotation marks, there is a clear difference between sets (2) and (3), indicating that the instruction is partially captured (green vs. red lines in Figure 2a). For the \u201cUrdu language\" instruction, set (2) shows higher similarity than set (1), suggesting a strong representation of the instruction (green vs. blue lines in Figure 2b). These results indicate that the model can effectively encode instruction-relevant features at the last input token, which supports steering model behavior based on specific constraints.\nSteering Vector Computation. We compute steering vectors for each of the 12 format-related instructions in the IFEval subset and for the 19 language-based instructions specified in the dataset. During the selection of the optimal steering later, we compare the validation score with and without steering to ensure that the steering intervention leads to an improvement. If no layer shows an improvement in the validation score, no steering is applied at test time. Details about the layers selected for steering are provided in Appendix D. To ensure that the steering vectors effectively capture the information related to the instructions, we inspect them by projecting the vectors onto the model's vocabulary space (nostalgebraist, 2020; Geva et al., 2022). We project the vectors using the model's unembedding matrix and examine the vocabulary tokens with the highest logit values. Table 1 presents the top tokens associated with several of the instructions we consider. We observe that the tokens promoted by the steering vectors are semantically related to the intended instruction, providing an initial validation that the vectors are capturing the desired features.\nSteering Results. We first evaluate steering on inputs without explicit text instructions (Figure 3a). In this setting, the instruction-following accuracy without steering hovers around 10% as the input has no information about the instruction. This non-zero accuracy reflects cases where the models incidentally satisfy the instruction, such as when a model rephrases a sentence without using commas, thus accidentally meeting the \u201cno comma\u201d constraint. When steering is applied, we observe a consistent increase in instruction adherence across all models, with accuracy improving to approximately 30%. This shows that the steering vectors encode meaningful information of the instructions and can be effectively used to steer the models toward the intended behavior. Next, we evaluate on inputs with instructions provided in the text (Figure 3b). As expected, the instruction-following"}, {"title": "LENGTH INSTRUCTIONS", "content": "Steering Vector Computation. Length constraints can be specified in various ways, such as by the number of sentences, words, or lines. However, unlike format instructions, it seems impractical to"}, {"title": "WORD-SPECIFIC INSTRUCTIONS", "content": "Keyword Inclusion. For this set of instructions, we compute word-specific steering vectors. To generate these vectors, we append different phrasings of a request to include a specific word w (e.g., \"please include the word {w} in your response\") to a set of base queries. These prompts are used to compute a vector representation for the \u201cinclude word {w}\" instruction. While this requires a separate steering vector for each keyword at inference time, the vectors can be generated on the fly using arbitrary base queries unrelated to the keyword itself. Furthermore, these vectors can be computed with a small number of examples; in our experiments, we use only 20 examples. The models are then evaluated on the subset of IFEval that contains keyword inclusion/exclusion constraints. The results, presented in Figure 6a, demonstrate the effectiveness of steering for word inclusion. By applying the word-specific steering vectors, we observe a notable increase in the frequency with which the model successfully includes the requested keywords in its responses.\nKeyword Exclusion. For keyword exclusion, we initially used the same procedure, appending instructions like \u201censure the word w does not appear in the answer\" to base queries and computing the corresponding steering vectors. However, upon inspecting the steering vectors, we found that projecting these vectors onto the vocabulary space resulted in high logit values for the tokens corresponding to the words meant to be excluded. In other words, adding these vectors to the model's residual stream actually increased the probability of generating the very keywords we intended to exclude. To address this issue, instead of computing exclusion vectors, we compute the vectors for inclusion and then subtract them from the model's residual stream. This technique effectively steers the model away from using the words captured by the vector. Figure 6b shows the results of steering for keyword exclusion using this approach. We observe that subtracting the inclusion vectors significantly reduces the frequency of the undesired keywords in the model's responses, both in cases where no textual instructions are present and where explicit exclusion instructions are provided in the input. Table 3 reports an example where the instruction to exclude a specific word is ineffective on its own, but applying steering successfully enforces the constraint."}, {"title": "MULTI-INSTRUCTION STEERING", "content": "Next, we present results on using our steering approach to handle multiple instructions simultaneously. With the same method as in the previous experiments, we steer the Phi-3 model for two instructions at once, applying the steering vectors at the layers previously identified as optimal for"}, {"title": "CROSS-MODEL STEERING", "content": "Instruction-tuned models show improved instruction-following abilities, suggesting that they may also be better at constructing meaningful representations of instructions. In this context, we ask: can we leverage the representations computed on instruction-tuned models to steer a base model more effectively? This idea is motivated by prior work showing that the weights of language models do not change significantly after instruction tuning (Lee et al., 2024; Jain et al., 2024). Additionally, linear representations derived through mean activation difference and sparse autoencoders (Huben et al., 2024) have been shown to be transferable between base and chat models (Panickssery et al., 2024; Kissane et al., 2024). To study this in our setting, we conduct experiments with the base and instruction-tuned versions of Gemma 2 2B and 9B. We focus on format instructions, using the same data and procedures outlined in \u00a73. The key difference is that we apply steering vectors computed on the instruction-tuned models to steer the base models, comparing this approach to same-model steering (vectors computed on the base model).\nWhen inputs consist of base queries only (Figure 8a), steering the base Gemma models with same-model vectors results in only modest improvements. However, when using vectors from instruction-tuned counterparts, the base models satisfy the constraints much more frequently, indicating the transferability of the instruction representations. When explicit input instructions are introduced (Figure 8b), Gemma 2B sees further accuracy gains, with cross-model steering outperforming same-model steering (40.5% vs. 36.2%). For Gemma 9B, however, steering yields no significant improvement, suggesting that the performance bottleneck might not be the quality of the instruction representation. We also explore cross-model steering for length instructions. As described in \u00a74, we compute a steering vector for conciseness from the instruction-tuned version of Gemma 9B and apply it to the base version. Figure 8c shows that increasing the steering weight shortens the outputs, demonstrating that representations learned in instruction-tuned models can still meaningfully modulate the output in the base model's space.\nThis is the first demonstration that cross-model steering-using vectors from instruction-tuned models-can outperform same-model steering in base models. This finding suggests that specialized representations from fine-tuned models can be leveraged to steer base models more effectively, opening new possibilities for composable transfer learning in instruction-based tasks where task vectors may originate from different models that are instruction-tuned in specialized domains."}, {"title": "RELATED WORK", "content": "Instruction Following. Training models to follow instructions is crucial for improving LLM per- formance and ensuring safe deployment, with various methods developed to enhance instruction adherence (Ouyang et al., 2022; Sanh et al., 2022; Wei et al., 2022; Bai et al., 2022; Chung et al., 2024), and datasets designed to train and evaluate instruction-following behavior (Ye et al., 2021; Wang et al., 2022; Gupta et al., 2022; Finlayson et al., 2022; Mishra et al., 2022; Longpre et al., 2023; K\u00f6pf et al., 2023). Natural language instructions have demonstrated significant promise in providing fine-grained control over model outputs (Zhou et al., 2023b). However, capable models still struggle with tasks that require outputs to satisfy fine-grained, hard constraints (Sun et al., 2023) and tend to drift from adhering to a constraint as the generation lengthens (Li et al., 2024). Motivated by these challenges, our work investigates how to improve instruction-following behavior by directly intervening on the model's activations at inference time.\nLanguage Model Representations. Our approach is inspired by prior research that shows it is possible to obtain vector representations encoding information about tasks on which a language model has been trained (Ilharco et al., 2023; Huang et al., 2024) or tasks learned in context (Hendel et al., 2023; Todd et al., 2024). These studies are part of a broader body of work that examines the linear representation of features such as truthfulness (Li et al., 2023; Azaria & Mitchell, 2023; Marks & Tegmark, 2024), sentiment (Tigges et al., 2024), harmlessness (Zou et al., 2023; Zheng et al., 2024), sycophancy (Perez et al., 2023; Panickssery et al., 2024; Sharma et al., 2024), factual knowledge (Gurnee & Tegmark, 2024), and refusal (Arditi et al., 2024). In addition, recent works have employed sparse autoencoders to identify feature directions in an unsupervised manner (Bricken et al., 2023; Huben et al., 2024; Templeton et al., 2024). A shared hypothesis across these works is that LLMs represent features or concepts as linear directions in activation space (Mikolov et al., 2013; Bolukbasi et al., 2016; Elhage et al., 2021; Nanda et al., 2023; Park et al., 2024; Olah, 2024). While recent studies suggest that not all features may be linearly encoded (Engels et al., 2024; Csord\u00e1s et al., 2024), the linearity assumption has been effective for both concept erasure (Ravfogel et al., 2020; Belrose et al., 2023; Shao et al., 2023; Guerner et al., 2024) and model steering.\nModel Steering via Activation Editing. It is well-established that the generation of a language model's output can be manipulated by directly editing activation values during inference (Dathathri et al., 2020; Subramani et al., 2022). Recent studies have shown that this approach can effectively steer models to be more honest and truthful (Li et al., 2023; Qiu et al., 2024), sycophantic (Panickssery et al., 2024; van der Weij et al., 2024), morally aligned with human values (Zou et al., 2023; Lu & Panickssery, 2024), or to display different sentiments, output styles, and languages (Turner et al., 2023; Liu et al., 2024; Tigges et al., 2024; Scalena et al., 2024; von R\u00fctte et al., 2024). Steering methods have also been used to control the model's uncertainty (Rahn et al., 2024), adopt different personas (Cao et al., 2024), provide alternative factual answers (Hernandez et al., 2024), and respond to harmful requests (Arditi et al., 2024; Wang & Shu, 2024). Similarly to some of these works (Burns et al., 2023; Turner et al., 2023; Panickssery et al., 2024; Arditi et al., 2024; van der Weij et al., 2024), we compute steering vectors based on input pairs that differ by a specific feature-in our case, the presence or absence of instructions. However, while previous studies have focused on high-level concepts such as sentiment, style, and safety, we focus on lower-level, hard constraints defined through natural language instructions, allowing for finer-grained control of the model's output."}, {"title": "CONCLUSION", "content": "We demonstrated the effectiveness of activation steering for improving language models' adherence to instructions. By computing steering vectors based on activation differences between inputs with and without instructions, we guide models to follow constraints related to format, length, and word inclusion/exclusion. These vectors capture meaningful instruction representations, enabling models to meet constraints even without explicit instructions, while also enhancing performance when instructions are present. Additionally, we show that models can handle multiple constraints simultaneously, offering a flexible framework for controlled language generation. Finally, we explore cross-model steering, revealing that vectors computed on instruction-tuned models can improve the behavior of base models, often surpassing same-model steering."}]}