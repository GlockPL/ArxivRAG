{"title": "An exploration of the effect of quantisation on energy consumption and inference time of StarCoder2", "authors": ["Pepijn de Reus", "Ana Oprescu", "Jelle Zuidema"], "abstract": "This study examines quantisation and pruning strategies to reduce energy consumption in code Large Language Models (LLMs) inference. Using StarCoder2, we observe increased energy demands with quantization due to lower throughput and some accuracy losses. Conversely, pruning reduces energy usage but impairs performance. The results highlight challenges and trade-offs in LLM model compression. We suggest future work on hardware-optimized quantization to enhance efficiency with minimal loss in accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Artificial Intelligence (AI) has gained unprecedented attention from the scientific community and the general public. With the introduction of ChatGPT, using Large Language Models (LLMs) became accessible for those without coding experience. As a result, ChatGPT gained over 180 million users in the first year [1]. This popularity spread to software engineers as well, where 83% use AI to code [2]. One of the biggest code-sharing platforms, GitHub, deployed its code LLM called GitHub Copilot in 2021. GitHub Copilot's users report increased satisfaction and productivity when coding with the model [3].\nBesides these benefits, these developments also bring challenges. Since the wide adoption of LLMs as Chat-GPT, reviews, theses and even scientific articles have been written by LLMs. Current research often focuses on mitigating the risks of truthfulness and interpretability, but little work is conducted on the energy consumption of LLMs. Increasingly, scientists call to reduce the energy consumption of AI, as the use of AI significantly impacts global energy consumption [4]. Alphabet reported that its energy bill increased over tenfold due to the required computational power to train AI models, and both Microsoft and Google reported an increased carbon emissions. To a large extent, this can be attributed to the size of the models and data that keeps increasing [5]. Where literature often studies the energy consumption of training a model [6], the inference stage, where users interact with the model, is often neglected. However, as the popularity of these applications increases, so does the number of queries for the model. Experts estimate that three days of ChatGPT consumes enough energy to match the training phase [4]. Thus, the inference phase is where the most impact can be achieved in reducing energy consumption. Moreover, this phase is to a larger extent under user control compared to the training phase. To run these large models on small devices such as laptops (or even phones), users can shrink the model with quantisation [7]. Model weights are then stored in a lower-bit format to reduce memory.\nCombining these trends, we identify an interesting area for exploration, and formulate the following research questions:\nRQ1. How can we reduce the energy consumption of a code LLM using quantisation with minimal harm to accuracy?\nRQ2. How can we reduce the energy consumption of a code LLM using pruning with minimal harm to accuracy?\nWe present the foundation and relevant theoretical back- ground in Section II, then our methodology in Section III and the experimental setup inSection IV, followed by the results in Section V. We interpret and discuss the results in Section VI, where we also state our limitations and provide directions for future work. We then provide an overview of similar research in Section VII. Finally, we summarise our exploration in Section VIII."}, {"title": "II. BACKGROUND", "content": "In 2023, global temperatures for the first time exceeded the 1.5 Celsius warming limit of the Paris Agreement for consecutive months [8]. Overall, global average temperatures were 1.48 degrees Celsius above pre-industrial, leaving a fragile margin to remain within the Paris Agreement. This makes it likely that 1.5 degrees Celsius warming will not be achieved in 2050, as per the Paris Agreement, but already in this decade with 80% probability [9], [10]. Already this causes extreme heatwaves, which are now 35 times more likely than before the industrial age [11]. This global warming is then expected to exceed 2 Celsius before 2050 [10], increasing the\nThere is a long history of language models, to keep this subsection compact we will stick to the most recent developments relevant to the domain of code LLMs. The first LLM was ELMO [13], a biLSTM [14] architecture that allows taking context into account because it processes language both forward and backward. Together with it's deep contextualized representations, ELMo allows more context into the prediction. Until 2015 the field of neural machine translation used an encoder-decoder architecture based on a Recurrent Neural Network (RNN) to translate a source sentence [15]. The encoder neural network encodes a source sentence into a fixed- length vector, the decoder outputs a translation of the encoded fixed-length vector. Because this encoder used a fixed-length vector, the architecture struggled with longer sequences. This was resolved by the attention mechanism [15]. In essence, with the attention mechanism the model learns which tokens are relevant for the translation task. With this approach, the attention mechanism improved the performance on longer sequences. Attention got in the spotlight with the transformer architecture, which significantly outperformed the state-of-the- art models [16]. Where an RNN would learn dependencies between words based on the sequence, a transformer learns dependencies based on the word itself. The transformer archi- tecture eliminated the need for RNNs, making the transformer superior to architectures as ELMo in both prediction accuracy as well as training time [16].\nWith GPT-1, released in 2018 having 'just' 117 million parameters, models and datasize grow over time. As depicted in Figure 1, the growing logarithmic scale shows a clear trend on larger and larger language models. Though this trend seemed to reach a limit with the release of GPT-3's 175 billion parameters (2020), Switch released the biggest LLM so far with 1.57 trillion (1.57 E12) parameters in 2021. However, this is uncertain as developers of GPT-4 do not disclose model architectures anymore due to \"the competitive landscape and the safety implications\" [17]. With these LLMs, accessibility to non-commercial users is at stake because the models are so big they cannot be used without access to powerful GPUs."}, {"title": "C. Quantisation", "content": "To resolve this problem with (too) large models, quanti- sation reduces the model size by transforming weights from FP32 to lower-bit formats such as Int8 or even FP4. This allows the user to run large models on non-GPU devices such as personal computers or even mobile phones [7]. To increase training speed, 32-bit floating point (FP32) weights are al- ready updated using 16-bit floating point (FP16) gradients, making the training twice as fast compared to using FP32 for gradients [18]. The performance of a quantised model using QLoRA can match the performance of ChatGPT up to 99.3% [19]. There are several methods to quantise model weights, from uniformly quantising to dynamically quantising weights [20]. There is an indication that quantisation makes the model more efficient during inference but this depends on various settings [20]. A recent paper [21] analyses five quantisation methods on efficiency using the LLAMA-2-7B model. The authors hint that some methods are optimised for GPU work, whilst others are generic."}, {"title": "D. Pruning", "content": "LLMs have many complex paths to come to their predic- tion. The multi-headed transformer architecture is an example where we observe an impressive performance on machine translation tasks [16]. However, analysing why multi-headed architectures work so well is challenging. To examine the effect of specific heads, researchers identified the functionality of the various heads [22]. The authors found that heads have different roles such as positional heads (attending adjacent tokens), synthetic heads (attending syntactic relationship) and attention to rare words. The authors removed, pruned, the heads without specialised roles. Surprisingly, this had minimal impact on model performance: the authors pruned nearly 75% of the encoder heads and 33% of the decoder heads \"without any noticeable loss in translation quality\u201d [22]. Do fewer heads in transformers also lead to a more efficient model? With careful selection of parameters, pruning can reduce the model size without considerable compromise on performance [23]."}, {"title": "E. Measuring the energy footprint of code", "content": "The most obvious method to measure en- ergy consumption is by measuring the difference in energy that goes in and comes out of a device. By measuring the difference during the execution of code, e.g. AI, one can derive the energy consumption. This hardware-based solution, also known as the power plug method, is often an expensive investment and inadequate for detailed analysis because background processes are also measured [24]. Nevertheless, specifically for larger"}, {"title": "III. METHODOLOGY", "content": "We aim to make this work reproducible using open-source models and methods discussed in scientific literature. In this section we describe our method, details and implementation follow in Section IV."}, {"title": "A. Selecting a model", "content": "StarCoder2 [30] is an open-source code LLM, generated by collaborating scientists in the BigCode project. StarCoder2 is transparent about training by mentioning both the computa- tional costs and energy impact. Details on the architecture are provided in the StarCoder2 paper and on GitHub. With Star- Coder2's statements on open-source development and research contributions, we pick the StarCoder2-3B and StarCoder2-7B models for our experiments."}, {"title": "B. Evaluating the output", "content": "To test the performance of a code LLM we use the task of code completion. In code completion, the model is prompted with a piece of code, i.e. the documentation of a function. With this description, the model generates the code of this function. 'Regular' LLMs are often evaluated with BLUE scores, which measure how close machine-generated output is to human- generated output. The score rates between 0 and 1 and gets closer to 1 when a machine translation gets closer to a human translation. However, BLUE does not work well with code as BLEU measures the similarity whilst for code, many correct solutions are available for the same problem set. So, instead, we use pass@k [31] which measures the probability that for a given coding query, at least one of the top k outputs passes all unit tests."}, {"title": "C. Evaluation framework", "content": "While some papers draft large data sets to test their models, this does not guarantee that these tests are accurate. Rather, we prefer having a smaller, but very accurate test. As such, we use EvalPlus [32] as a framework to evaluate the generated code. After preprocessing, the output is then subject to various unit tests that define the right data structures and outputs. If the code passes all of these tests, the output is labeled as correct. For a failure on one of these tests, the output is labeled incorrect. In the end, EvalPlus gives a pass@k score that depicts the mean success rate. A schematic overview of this process is depicted in Figure 2."}, {"title": "D. Quantising model weights", "content": "At least 8 methods compress model weights into smaller datatypes and are supported by the Hugging Face transformer library. Hugging Face provides an overview of which method to pick in what use case. For example, of these 8 methods, 3 methods are not available 'on the fly', meaning that the models need to be calibrated after quantisation. Depending on the hardware, quantisation can take 5 minutes for a 350M parameter model but up to 4 hours to quantise a 175B parameter model. All methods are supported by transformers, but bytsandbytes (BNB) is the most accessible option so we stick to using BNB as quantisation method in our exploration."}, {"title": "E. Pruning the model", "content": "We prune the last layers of StarCoder2-3B and StarCoder2- 7B until the pass@1 metric reaches 0. As pruning does not necessarily lead to reduced performance [23], we reduce the model by pruning the last layers as these often contain context where earlier layers encode knowledge [33]. With each pruned layer we evaluate the compressed models on pass@1 accuracy, inference time and energy consumption. To contextualise the results we will prune the last layers of Phi-2 [34] as well and compare the results with our StarCoder2 models."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To reproduction of our experiments, we provide our system specifications below and our implementation on GitHub. We use the national supercomputer facilities from SURF, and specifically its Snellius environment."}, {"title": "V. RESULTS", "content": "We first present the results of our experiment where we ap- ply quantisation to StarCoder2-3B and StarCoder2-7B. Then, we show the energy consumption and pass@k after we pruned the last layers of StarCoder2-3B and StarCoder2-7B."}, {"title": "A. Impact of quantisation", "content": "Table II presents the pass@1 score for the StarCoder2-3B and StarCoder2-7B models. The StarCoder2 paper reports a pass@1 score of 31.7 on HumanEval and 27.4 on HumanEval+ for StarCoder2-3B. For StarCoder2-7B, the pass@1 scores are 35.4 and 29.9 respectively. Our results for StarCoder2-3B are thus significantly lower. We verified our setup with Phi-2 and saw no difference between our results and the results reported in the Phi-2 paper. We can thus validate our setup and dive into the difference on GitHub.\nFollowing the pass@1 scores for the original StarCoder2 models, we present the pass@1 scores for the quantised StarCoder2 models in Table III. We show the pass@1 scores for all configurations in tokens and quantisation. In brackets, we report the difference between the quantised models and the full-sized models from Table II."}, {"title": "Energy consumption of StarCoder2-3B", "content": "The energy consumption of StarCoder2-3B predicting 128 tokens is shown in Figure 3, Figure 4 for 256 tokens. We interpret and discuss the results in the discussion, Section VI."}, {"title": "Energy consumption of StarCoder2-7B", "content": "The energy consumption of StarCoder2-7B predicting 128 tokens is shown in Figure 5, Figure 6 for 256 tokens. We interpret and discuss the results in the discussion, Section VI."}, {"title": "B. Impact of pruning", "content": "We analyse the impact of pruning of the last layers by examining the pass@1 score on HumanEval+ and the energy consumption. Figure 7 shows the pass@1 score for Phi-2, StarCoder2-3B and StarCoder2-7B. At 0 layers removed the performance equals that of the original, full-sized model. The layers are removed 1-by-1 until the pass@1 score reaches the floor of 0. The energy consumption of this experiment is provided in Figure 8. We interpret and discuss the results in the discussion, Section VI."}, {"title": "VI. DISCUSSION", "content": "Table III shows a clear deviation from pass@1 scores compared to the reported scores in the StarCoder2 paper [30]. We discuss this in our repository, but as we are interested in the impact of model compression we use the results as given."}, {"title": "A. Quantisation", "content": "Table III shows the pass@1 scores for the quantised models and the deviation between these quantised scores and those of the full-sized models. First, we examine StarCoder2-3B on 128 tokens. In Table II, we see that for both 4-bit quantised models, the pass@1 score is 6.1, 1.8 points lower \u00b1 -25%, than the pass@1 of the original model (7.9). For the 8-bit configuration, the pass@1 score is 6.7, 1.2 points lower \u00b1 20% than the original model (7.9). These reductions are greater than we would expect based on related work [35]. However, if we proceed to the same StarCoder2-3B model predicting 256 tokens, we see that the pass@1 score for the 4-bit configuration matches the pass@1 score of 10.4 from the original model, in line with related work [35]. The 8-bit configuration is 1.3 points lower than the original pass@1 of 10.4. StarCoder2-7B shows a similar trend for 128 tokens, where the 4-bit configuration results in a pass@1 score that is 1.3 lower (\u00b1 -33%) than the original (3.7). For the 8-bit configuration, we see that the pass@1 score matches the original model (3.7), following related work [35]. For 256 tokens, the 4-bit configuration yields a 1.8 points lower pass@1 (\u00b1 -25%) score but the 8-bit configuration again matches the pass@1 score of 6.1 for HumanEval+.\nFor all 8-bit quantised models in Figures 3-6, we see high confidence intervals, indicating variance in the energy measurements. As the Snellius cluster is a shared computing environment where we used 1/8th of a node, other users might impact the frequency of the CPU or GPU which could lead to errors. We reran the models with the Energy Aware Runtime (EAR) flag. Our analysis shows that the average CPU and GPU frequency did not change during our experiments, ruling out the hypothesis that other users on the same node impact the frequency. Next to that Figures 3-6 all show very little variance for the original models, the variation is mainly tied to the quantisation strategy. The bitsandbytes authors report no instabilities for 8-bit optimisers [35], but examined training and not the inference.\nWe observe that quantisation leads to increased energy consumption, regardless of the model or the number of tokens. Our data shows that 8-bit quantisation leads to the highest increase, from 19%-75%. 4-bit quantisation seems more effi- cient and increases by 19%-43%. As we expected a reduction in energy consumption, this result was unexpected. Table IV clarifies the higher energy consumption of quantisation be- cause quantised models require more time. For StarCoder2-3B the throughput halves from \u00b1 46 to \u00b1 23 in 4-bit quantisation and reduces \u00d7, from \u00b1 46 to \u00b1 6, for 8-bit quantisation regardless of the number of tokens predicted. However, we see that the energy consumption from Figure 3 and Figure 4 does not double. So, the energy consumption did not increase proportional to the increased runtime. The energy required per token is lower but this reduction is compensated by a longer inference. For StarCoder2-7B, the reduction in throughput is \u00d7, from \u00b1 36 to \u00b1 22, for 4-bit quantisation and \u00d7, from \u00b1 36 to \u00b1 9, for 8-bit quantisation. The throughput is again lower for quantised models, but the reduction is less compared to StarCoder2-3B. A recent paper shows that the bitsandbytes module has a very low throughput compared to other quantisation methods [21]. This paper strengthens our findings that the quantisation method negatively impacts energy consumption. Related work also describes increased inference time for bitsandbytes [36]. Even though the energy consumption per second is lower for the quantised models, the throughput is lower than for the original model. Ultimately, the model might spend less energy per second but if the time increases so does the energy consumption. Likely, this is the result of table look-ups during dequantisation. To the best of our knowledge, no literature explains why this method is slower than the original model. Bitsandbytes does not linearly quantise weights, when the model predicts its next token, it has to dequantise the weights again but because the scale is different we do so for each new token. We hypothesized that this might cause cache misses in memory, which can increase the runtime of code [37]. Our analysis confirms that L1 and L2 cache misses increase with quantised models, we provide these results on GitHub. Next to cache misses, GPUs are not created to accelerate matrix multiplication of integers but rather for floating point numbers [20], [38], [39].\nGiven these results from Tables II and III, there are two interesting findings. First, we empirically show that in some cases the accuracy for quantised models can match that of original models, reinforcing the claims from [35]. Second, the StarCoder2-7B seems to perform best with 8-bit quantisation whereas for StarCoder2-3B this is inconclusive as 4-bit or 8-bit does not consistently outperform its counterpart. Energy-wise, quantisation consistently increases energy consumption."}, {"title": "B. Pruning", "content": "Figure 7 shows the pass@1 score for StarCoder2-3B, StarCoder2-7B and Phi-2 for various pruned layers when predicting 128 tokens. We see a clear downward trend, indicat- ing that pruning layers negatively impact the pass@1 score. For Phi-2 the pass@1 score decreases by around 15% per removed layer. When more layers are pruned, the pass@1 score decreases accordingly except for the pruning of the last two layers of StarCoder2-3B, which slightly improves before decreasing again. When we prune 3 layers, both StarCoder2 models become erroneous and fail all tasks from HumanEval+.\nRD1: Dedicated hardware. [20] report that \u201cQuantisation is the most straightforward method to cut down memory cost and improve inference speed for LLMs, especially on hardware with support for fast operation of low-bit datatypes\". As our results indicate that the energy consumption per second is lower for quantised models, using dedicated hardware could reduce runtime and consequently energy consumption.\nRD2: Expand to other models. Our work in an exploration of StarCoder2, but LLMs are released by the hour. Future work should look into the effect of quantisation on other LLMs to see if we can generalise the findings.\nRD3: Targetted pruning. We use a naive pruning method to incrementally prune the last layer. More advanced pruning strategies could however lead to a more balanced trade-off between size and performance [23]."}, {"title": "VII. RELATED WORK", "content": "A recent paper by Rajput (2024) [21] analyses five quantisa- tion methods on LLAMA-2-7B, amongst which Bits and Bytes (BNB). The authors find that BNB is the worst-performing quantisation method in efficiency and perplexity. The authors hint that the best method is optimised for GPU work, com- pared to BNB which is generic. The authors call for energy optimization an explicit design criterion alongside accuracy. The authors use perplexity on the wikitext-2 dataset. Our work differs from this work as it focuses on coding tasks and studies the StarCoder2 models instead of LLAMA-2.\nThe authors estimate the Carbon Footprint of BLOOM, a 176B parameter LLM [41]. The authors simulated training and inference, and convert the energy used in CO2-emissions. This is one of the first papers that assesses the LCA of a LLM, nevertheless, it underestimates the impact of inference. Our work differs as we try to reduce the energy consumption during inference, this work estimates the carbon footprint of BLOOM's lifecycle."}, {"title": "VIII. CONCLUSION", "content": "More than 100 million people interact with ChatGPT and programmers using code LLMs describe increased productiv- ity. But this mass adoption comes at a cost: AI consumes a significant amount of electricity, causing the CO2-emissions to soare. With global temperatures now 1.48 degrees Celsius above the pre-industrial age, irreversible climate change be- comes more likely unless we reduce greenhouse gas emissions."}]}