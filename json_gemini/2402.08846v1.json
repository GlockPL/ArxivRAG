{"title": "An Embarrassingly Simple Approach for LLM with Strong ASR Capacity", "authors": ["Ziyang Ma", "Guanrou Yang", "Yifan Yang", "Zhifu Gao", "Jiaming Wang", "Zhihao Du", "Fan Yu", "Qian Chen", "Siqi Zheng", "Shiliang Zhang", "Xie Chen"], "abstract": "In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR\u00b9. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.", "sections": [{"title": "1 Introduction", "content": "Automatic speech recognition (ASR) stands as a cornerstone in the realm of intelligent speech tech-nology, enabling machines to understand and tran-scribe human speech. The significance of ASR in enhancing human-computer interaction and ac-cessibility makes it a crucial area of research and applications in the field of speech processing.\nThe evolution of ASR technology has been marked by the adoption of various paradigms, each representing a leap forward in terms of accuracy, ef-ficiency, and applicability (Li, 2022). Among these, supervised methods including connectionist tem-poral classification (CTC) (Graves et al., 2006), attention-based encoder-decoder (AED) (Chan et al., 2016), recurrent neural network transducer (RNN-T) (Graves et al., 2013) and their variants have been pivotal. In addition, employing self-supervised methods for pre-training followed by supervised methods for fine-tuning has also proven to be effective (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022; Ma et al., 2023; Yang et al., 2023). However, each paradigm comes with its own set of challenges and limitations, such as the need for extensive labeled data, difficulties in cap-turing long-range context dependencies in speech, and huge training costs.\nIn this evolving landscape, the advent of large language models (LLMs) has introduced a ground-breaking paradigm: Multimodal large language models (MLLMs) framework (Liu et al., 2023; Li et al., 2023a; Gao et al., 2024), based on a decoder-only architecture. This innovative approach di-verges from traditional ASR by utilizing the im-mense generative capacity of LLMs, which are pre-trained on vast corpora encompassing diverse linguistic contexts, leading to LLM-based ASR. The evolution of the ASR paradigm from previous NN-based ASR models to LLM-based ASR mod-els, stresses differences across loss and criterion design, text prior knowledge, and model scale. This paradigm harnesses pre-existing linguistic knowl-edge, enabling a more holistic understanding of language, which in turn, translates to significant improvements in the speech recognition task.\nThe architecture of LLM-based ASR can be con-ceptualized as consisting of three primary compo-nents: a speech encoder, a projector, and an LLM. Recent works in LLM-based ASR often venture"}, {"title": "2 Speech Recognition Meets Large Language Model", "content": "2.1 Previous NN-based ASR\nPrevious NN-based ASR systems are designed to align the speech signal with the label sequence ac-curately.\n2.2 Existing LLM-based ASR\nLLM-based ASR models adopt decoder-only ar-chitectures based on a pre-trained LLM as a new paradigm. LauraGPT (Wang et al., 2023) con-nects a modified Conformer (Gulati et al., 2020)"}, {"title": "2.3 Proposed Method", "content": "As shown in Figure 1, an embarrassingly simple framework is proposed to train the SLAM-ASR model. For each sample, given speech $X^S$, the corresponding transcript $X^T$, and the prompt $X^P$, we first convert the speech into speech features through the speech encoder, which can be written as:\n$H^S = Encoder(X^S),                                   (1)$\nwhere $H^S = [h_1^S,\u2026\u2026,h_T^S]$ has T frames in the temporal dimension. Due to the sparsity of speech representation, the speech features sequence HS is still very long for the LLM to tackle\u00b2, we downsam-\n$z_i^S = h_{(i\u2217k)}^S \u2225h_{(i\u2217k)+1}^S\u2026\u2225h_{(i\u2217k+k\u22121)}^S,                                   (2)$\nand\n$N = T//k.                                   (3)$\nNext, a projector is applied to transform the speech features $Z^S$ into $E^S$ with the same dimension as the LLM input embedding. In our experiments, we use a single hidden layer followed by a ReLU activation and a regression layer as the projector, donated as:\n$E^S = Linear(ReLU(Linear(Z^S))).                                   (4)$\nFinally, we feed the speech embedding $E^S$, tran-script embedding $E^T$, and prompt embedding $E^P$ into the template to compose the final input E of LLM, donated as:\n$E^T = Tokenizer(X^T),                                   (5)$\n$E^P = Tokenizer(X^P),                                   (6)$\n$\\begin{aligned}E=\\begin{cases}Templates(E^S,E^P,E^T)  \\quad \\text{ if training },\\\nTemplates(E^S,E^P)  \\quad \\text{ if inference },\\end{cases}\\end{aligned}                              (7)$\nwherein the template is detailed in Section 3.3 and Section 3.4."}, {"title": "3 Experiment Setup", "content": "Our experimental procedure obeys the KISS (Keep It Simple, Stupid!) principle to investigate the most critical factors for LLM-based ASR."}, {"title": "3.1 Models and Modules", "content": "3.1.1 Speech Encoder\nTwo types of speech encoders are investigated in this paper, which are supervised speech encoders trained on massive speech-text pair data and self-supervised speech encoders trained on large-scale unlabeled speech data. For supervised founda-tion models, we mainly survey the well-known Whisper (Radford et al., 2023) family of models ranging from tiny to large, including whisper-tiny, whisper-base, whisper-small, whisper-medium and whisper-large-v2. We discard the decoder of each Whisper model and only use the encoder as a fea-ture extractor. We also investigate Qwen-Audio Encoder, the encoder fine-tuned from whisper-large-v2 checkpoint on large-scale speech, au-dio and music data, released along with Qwen-Audio (Chu et al., 2023) model. For self-supervised models, we investigate HuBERT5 and WavLM6 in different scales, either raw pre-trained or fur-ther fine-tuned.\n3.1.2 LLM\nTwo types of large language models are investi-gated in this paper, which are raw pre-trained LLMs\n3.1.3 Projector\nThe projector can be viewed as an adaptor for other modalities to perform alignment with LLM. In all our experiments, the output of the speech encoder is 50 Hz, and the downsampling rate k = 5, lead-ing to the input speech features ES of the large model being 10 Hz. The hidden layer dimension is set to 2048, while the dimension of the speech encoder output HS and the LLM input dimension vary depending on the model used, respectively."}, {"title": "3.2 Dataset", "content": "To evaluate the capabilities of the LLM-based ASR models, we use the most widely used benchmark for the ASR task, the standard Librispeech bench-mark with 960 hours of training data without any data augmentation or splicing. We use the dev-other subset as the validation set and test-clean/test-other as the test sets, each of which contains 10 hours of speech."}, {"title": "3.3 Training Detail", "content": "During training, the data is organized in the fol-lowing format: \u201cUSER: <S> <P> ASSISTANT: <T>\u201d, where <S> represents speech embedding, <P> represents the prompt, and <T> represents the corresponding transcribed text. We only compute the loss on <T>, as is common practice. For the op-timizing strategy, we use AdamW (Loshchilov and Hutter, 2019) with a max learning rate of 1 \u00d7 10\u20134 without a weight decay. For the learning rate sched-uler, we conduct warmup at the first 1,000 steps"}, {"title": "3.4 Inference Detail", "content": "During inference, the data is organized in the fol-lowing format: \u201cUSER: <S> <P> ASSISTANT:\u201d, where large language models answer autoregres-sively. Typically, LLMs utilize sampling algo-rithms to generate diverse textual outputs. Since speech recognition is a sequence-to-sequence task with deterministic outputs, we use beam search with $beam = 4$ to output the hypothesis corre-sponding to the speech."}, {"title": "4 Exploration", "content": "In this section, we first give a basic benchmark of combinations of different LLMs and speech en-coders and find that chat models perform better than raw pre-trained LLMs on the ASR task. We next benchmark different chat models and find Vi-cuna to be a suitable LLM and fine-tuned Hu-BERT to be a powerful speech encoder for con-ducting the ASR task. Finally, we propose SLAM-ASR, and compare SLAM-ASR with state-of-the-art previous NN-based ASR models and the latest best-performing LLM-based ASR models."}, {"title": "4.1 A Basic Benchmark", "content": "To begin with, we benchmark Whisper models with different sizes on pre-trained LLMs and supervised fine-tuned LLMs. We pick TinyLLaMA of the 1B-magnitude and LLaMA-2 of the 7B-magnitude to make a preliminary assessment. As shown in Table 2, the performance of the ASR task improves as the speech encoder parameter size increases, but the improvement is of diminishing marginal benefit for the Whisper family of models. For the choice of LLMs, the chat models work better than the pre-trained models, regardless of the size. One possible explanation is that the chat models take speech embedding as a form of \"language\" and perform a"}, {"title": "4.2 Exploration in LLMs", "content": "Next, we fix the speech encoder as Whisper-large and then explore a better large language model. As shown in Table 3, the Phi-2 chat model with 2.78B parameters has a comparable word error rate with LLaMA-2 with 6.74B parameters on test-other. Vicuna is an open-source chat LLM fine-tuned on user-shared conversational data col-lected from ShareGPT13, utilizing LLaMA as a pre-trained LLM. The LLM-based ASR model shows better results when Vicuna is used as the LLM com-pared with LLaMA-2 and LLaMA-2-Chat. All the above experimental results confirm the capability of chat models on LLM-based ASR systems."}, {"title": "4.3 Exploration on Speech Encoders", "content": "Furthermore, we fix Vicuna as the LLM and bench-mark the performance of different speech encoders. For the supervised speech encoders, the perfor-mance gets better gradually as the parameter size of the speech encoder increases, which is consistent with the trend on the LLAMA series models. When the Qwen-Audio Encoder is used as the speech en-coder, the ASR performance is further improved compared with Whisper-large, which indicates that the encoder fine-tuned on other LLM (i.e. Qwen-7B) with gradient backpropagation, can be trans-ferred to another LLM (i.e. Vicuna-7B), and main-tain a certain degree of performance.\nFor the self-supervised learning speech encoders, HUBERT Base and WavLM Base have about 95M parameters, with 768 dimensions of hidden size. In this configuration, the ASR performance is similar compared with Whisper-small with the same scale, where self-supervised learning does not play a role. When scaling the self-supervised speech encoders to 0.3B, WavLM Large outperforms all listed super-vised speech encoders, including Whisper-medium with 0.3B parameters and Whisper-large with 0.6B parameters, while the improvement from HuBERT Base to HuBERT Large is not obvious. However, if the HuBERT Large encoder is first fine-tuned on Librispeech 960 hours of training data, and used as the speech encoder to train the projector in our LLM-based ASR model, the model achieves a WER of 2.30% on test-clean and 4.53% on test-other, exceeding the performance with WavLM"}, {"title": "4.4 SLAM-ASR", "content": "Here we introduce SLAM-ASR, a llm-based ASR model with HuBERT X-Large as the speech en-coder and Vicuna-7B as the LLM, with the only trainable linear projector, implemented based on the SLAM-LLM framework. As shown in Table 5, we exhibit different LLM-based ASR models from concurrent work, either ASR-specific or audio-universal. A contemporary work (Yu et al., 2024) employs Whisper-large as the speech encoder and Vicuna-13B as the LLM. The segment-level Q-Former (seg-QF) is utilized as the projector to tackle the compatibility between speech sequences and the LLM. Compared with their method, our SLAM-ASR yields 17.4/26.9% relative WER re-"}, {"title": "5 Capability Emergence", "content": "We observe that there is capability emergence for LLM-based ASR during training within 1 epoch (around 12k steps). Specifically, the accuracy of the next token prediction increases rapidly at the beginning of training, then starts to rise slowly, and then \"spikes\" at some point, as if \"the ability is suddenly learned\".\nFigure 2 demonstrates the training accuracy of the next token prediction with the training steps, where the LLM is kept as Vicuna-7B and the speech encoders vary. As can be seen from the figure, the speech encoders with better performance, in"}, {"title": "6 Conclusion", "content": "In this paper, we systematically explore LLM-based ASR systems with a clean framework, where the only trainable linear projector is used to align the speech encoder and the LLM. Research in-dicates that LLMs that undergo supervised fine-tuning, exhibit improved performance and robust-ness. Furthermore, speech encoders that are fine-tuned from self-supervised models demonstrate superior capabilities. The SLAM-ASR model is proposed and outperforms other LLM-based ASR models and previous NN-based ASR models on the Librispeech benchmark. Exploratory experiments show that there is a capability emergence in LLM-based ASR systems. We aspire for our research to serve as a step forward in the exploration of LLM-based ASR, offering assistance and insights to the broader community."}, {"title": "A Appendix: More Exploration", "content": "A.1 Text Perplexity\nWord-level text perplexity (PPL) of different LLMs is measured to investigate if the better perfor-mance of Vicuna is related to domain agreement, rather than supervised fine-tuning. As shown in Table 8, we measure perplexity on test-clean and test-other subsets. Surprisingly, LLaMA-2 with-out SFT achieves the lowest perplexity by a large margin compared with chat models, while perform-ing the worst on the word error rate. This proves that the better results of chat models are not due to domain agreement with the transcripts.\nA.2 Prompt Engineering\nand lead to a suboptimal solution. To investigate this assumption, we set a more complex prompt format. We use the same seed prompt for the ASR task in SpeechGPT (Zhang et al., 2023) to gener-ate 10 prompts to form a prompt library. At both the training and testing stages, a random prompt is drawn from the prompt library. As shown in the last row of Table 10, there is a big drop in model performance, which is in line with our assumption."}]}