{"title": "A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Bartolini"], "abstract": "Abstract\u2014Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness. Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation. Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation. Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods. These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development sce- narios. Moreover, previous studies do not approach the problem at a suitable scale for real-life applications. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. To address these gaps, we have developed an approach for generating and evaluating more real- life complexity test suites. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. In this work, we present AGONETEST: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites. Starting from a state-of- the-art dataset (i.e., METHODS2TEST), we built a new dataset for comparing human-written tests with those generated by LLMs. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.\nIndex Terms Software Testing, Large Language Model, Au- tomatic Assessment", "sections": [{"title": "I. INTRODUCTION", "content": "Software testing is a critical step in the software devel- opment lifecycle, essential for ensuring code correctness and reliability. Within it, unit testing is the stage concerned with verifying the proper functioning of individual code units. Designing and building unit tests is a costly and labor- intensive process that requires significant time and specialized skills. Automating this process represents a promising area for research and development.\nAutomated tools for generating unit tests can reduce test engineers' and software developers' workload. These tools typically use static code analysis methods to generate test suites. For example, EvoSuite [1], a popular tool that com- bines static code analysis with evolutionary search, has been demonstrated to achieve adequate coverage.\nLarge Language Models (LLMs), efficiently exploited in various aspects of software development, could also handle the automatic generation of unit tests. Several empirical studies on LLMs have highlighted their ability to generate tests for simple scenarios, often limited to single methods [2]\u2013[5]. Though directionally useful, these explorations focus on independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios [6]. Moreover, previous studies do not approach the problem at a suitable scale for real-life examples. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency.\nTo address these gaps, we have developed an approach for generating and evaluating test suites that are more represen- tative of real-life complex software projects. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment.\nIn this work, we introduce AGONETEST, an automated sys- tem designed to generate test suites for Java projects, accom- panied by a rigorous and systematic methodology to evaluate these generated test suites. Leveraging the METHODS2TEST dataset [7], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs. We integrate libraries such as JaCoCo, PITest, and TsDetect to compute the metrics for test evaluation.\nThe main contributions of our work are as follows:"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Unit Test Generation\nUnit test generation is the automated process of creating test cases for individual software components, such as functions, methods, or modules. These test cases are used to indepen- dently verify the correct functioning of each unit.\nPresent techniques employ randomness-based [8], [9], constraint-based [10], [11], or search-based approaches [12], [13]. The core idea behind these methods is to transform the problem into one that can be solved mathematically. For example, search-based techniques convert testing into an optimization problem, to generate unit test cases [14]. Consequently, the objective of these techniques is to generate all potential solutions and then select those that achieve better code coverage. EvoSuite [1] works by accepting a Java class or method as input and applying search-based algorithms to generate a test suite that meets coverage criteria such as code or branch coverage. EvoSuite assesses test fitness using iterative processes of variation, selection, and optimization. Not only does it generate JUnit test cases, but it also provides a comprehensive report produced by inspecting the efficiency of the created test suite, based on metrics such as code coverage and mutation score. One limitation of EvoSuite is that it often produces tests that lack clarity and readability [15]. Additionally, EvoSuite can only be used on projects using Java 9 or lower, which limits its applicability to more modern Java projects (the last Java version, at the present time, is 22). Unlike EvoSuite, AGONETEST incorporates advanced evaluation metrics and test-smell recognition, providing a more comprehensive assessment of the quality of generated test suites and ensuring readability by leveraging human-like LLM-generated code. Moreover, AGONETEST supports all Java LTS versions, allowing projects built on newer versions to be tested as well, overcoming the compatibility limitations of EvoSuite.\nB. Large Language Models for Test Generation\nSince the emergence of LLMs, they have been used for test suite generation. The first techniques exploiting LLMs were thought of as solutions to neural machine translation problems [16], [17]. Such approaches work by translating from the primary method to the appropriate test prefix or test assertion while also fine-tuning the LLMs using the test generation dataset. For instance, AthenaTest [17] optimizes BART [18] using a test generation dataset in which the source is the primary method along with its corresponding code context, and the result is the complete test case. AthenaTest focuses mainly on generating method-level tests by fine-tuning a single model, while AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design, thereby handling more complex, real-world scenarios. In light of the rapid evolution of instruction-tuned LLMs, the proliferation of methods for generating tests is on the rise, exploiting guided LLMs through appropriate prompts, as opposed to model fine- tuning [19], [20]. Several proposals for evaluating LLMs in test suite generation have emerged. For example, CHATTESTER [5] proposes a tool for evaluating and improving LLM-generated tests based on ChatGPT. ChatTester focuses on improving and evaluating tests generated by a specific LLM (ChatGPT), but requires human intervention to evaluate the generated code and does not provide an evaluation of class-level tests on multiple LLMs. AGONETEST provides support instead for a variety of LLMs and evaluates each LLM's performance on a wide range of real-life Java projects. TESTPILOT [3] is also focused on generating and improving tests using LLMs on JavaScript code. Although TestPilot performs an automated evaluation, it lacks wider applicability to projects other than the 25 repositories it considers in the work provided as reference here. AGONETEST offers far broader applicability by using a dataset of 9,410 Github repositories, and automatically integrating test libraries into them. CEDAR [21] instead proposes a prompt construction strategy based on few-shot learning [22] and the Codex model to generate tests. Cedar uses a specific prompt construction strategy, but it does not incorporate a structured mechanism to evaluate multiple LLMs and prompt techniques in a unified framework. AGONETEST provides this by allowing the integration and evaluation of various"}, {"title": "C. Limits of Current Approaches in Applying LLMs to Unit Test Generation", "content": "While promising, current approaches in applying LLMs to unit test generation exhibit several limitations:\na) Limited Scope: Current methods for evaluating how useful LLMs are in test code generation are mostly limited to generating code segments, rather than whole modules or components (e.g., whole classes in Java). Consequently, the research community lacks dedicated datasets for evaluating class-level test generation. To the best of our knowledge, studies often provide only punctual and anecdotal evaluations of the generated results [3], [5], [17].\nb) Lack of Automation: No work has emerged in the literature that fully automates the test generation-execution- assessment loop, which is crucial for comprehensive and scalable testing [3], [19], [20]\nc) Subjective Choice of Prompts: In most cases, the choice of prompts to get LLMs to generate testing code re- mains subjective. There is no thorough evaluation of alternate prompting techniques compared to those initially proposed, leaving room for further exploration and optimization in prompt engineering. [4], [20], [21]."}, {"title": "III. OVERVIEW OF AGONETEST", "content": "The term agone, originating from ancient Greece and Rome, signified a contest wherein philosophers debated their ideas, with the audience determining the victor. We adopt the term agone metaphorically to represent the competitive evaluation of LLMs and their respective prompting strategies within an arena aimed at generating optimal unit test suites. AGONETEST determines the optimal strategies based on stan- dard test quality metrics, which we elaborate on in subsequent sections.\nAGONETEST is designed to provide software testers with a system for generating and assessing unit tests. This assessment focuses on key metrics such as code coverage and the pres- ence of known test smells, thereby offering a comprehensive assessment of test suite quality.\nAGONETEST operates on the principle that the evaluation of LLMs in the task of generating high-quality unit tests can be performed through the collaboration of test engineers and data scientists (or prompt engineers). However, in practice, a single experienced test engineer familiar with generative AI can perform both roles, allowing the focus to be only on defining new prompt techniques and the comparison of LLMs. This is the persona that we evoke when we refer to the AGONETEST user (alternatively \"the test engineer\u201d) in the remainder of this paper.\nThe system helps test engineers through the following phases:\n\u2022 Strategy Configuration\n\u2022 Automated Test Generation\n\u2022 Strategy Evaluation\nFigure 1 provides a high-level diagram of the architecture of AGONETEST, showing the operating modules that streamline the test generation and evaluation process. These modules are described as follows:\nSample Projects Selection (Strategy Configuration - I): As an initial configuration step, the user chooses which repositories to generate test suites for. This initial phase leverages a comprehensive dataset of annotated open-source Java repositories, which we contribute to the community. It involves preparing, loading, and managing the repositories to be tested by the system.\nConfiguration Parameters Elicitation (Strategy Configu- ration - II): In this phase, configuration parameters are elicited from the selected repositories (e.g., the project java version, used testing framework, etc.) and processed to create prompts templates for the LLMs.\nPrompt Creation (Automated Test Generation - I): During this phase, the prompt templates used in the previous phases are fully instantiated and then used to generate unit test suites in the next step.\nTest Suite Generation (Automated Test Generation - II): Here, AGONETEST orchestrates the interaction with the selected LLMs, feeding them the instantiated prompts to produce the unit test code. Each LLM generates test classes that are then integrated into the project structure."}, {"title": "B. Configuration Parameters Elicitation", "content": "Before unit test generation can begin, the system extracts some parameters from the projects selected in the previous step. These parameters are then fed into the module that selects prompts and LLMs. To query the model under examination, various prompting techniques are available and can be chosen [24].\nThe configuration parameters include:\n\u2022 focal_class: This variable contains the Java class for which the test suite must be generated;\n\u2022 testing_framework: This variable provides the name and version of the project's testing framework (e.g., JUnit 4), directly extracted from the project during execution;\n\u2022 java_version: This variable allows you to retrieve the version of Java that the project uses.\n\u2022 example_focal_class & example_test_class: These vari- ables contain an example focal class and the correspond- ing test class extracted from a reference repository, useful to provide an example to the LLM if one wants to use the few-shot prompting technique;\n\u2022 example_testing_framework & example_java_version: These variables provide the information about the exam- ple repo.\nSee Section IV-A for an example of a real implementation."}, {"title": "C. Prompt Creation", "content": "In this phase, the prompt templates described in the previous phases are fully instantiated to create viable prompts to guide the LLM in generating unit tests. We populate the user- supplied prompt structures by replacing the variables outlined in Section III-B.\nIt has to be noted that, in order to make sure our experiments and findings are reproducible, we prepared CLASSES2TEST by saving the commit hashes of the repositories used as sources. This allows AGONETEST to consistently extract information such as the Java version used, the type of test framework (e.g., jUnit), and its version.\nUnlike previous approaches to creating unit testing with LLMs that require human intervention to input context infor- mation [2], [5], AGONETEST automates the process to a far greater degree. AGONETEST employs ElementTree [25] and a parser to read and modify the Maven and Gradle build (see Section III-E3). It analyzes the libraries present and the Java version used in each build system. This method, along with the ability to use examples, offers users a versatile system for generating prompts."}, {"title": "D. Test Suite Generation", "content": "At this point in the process, we have everything we need for the selected LLMs to generate test suites for each focal class of the project. To ensure each model has an appropriate number of tokens, we use tiktoken, a BPE tokenizer [26], to evaluate the token count in the prompt. If the limit is exceeded, AGONETEST returns an error to the user, specifying the number of tokens exceeded.\nWe remark that AGONETEST allows users to evaluate a wide range of LLMs automatically. This capability is provided by the open-source LiteLLM library, which facilitates communi- cation with more than 100 models using a standard interaction based on the OpenAI API format. Integration is made easier by LiteLLM, which translates inputs to satisfy the unique endpoint needs of each provider. This is crucial in today's environment, where the absence of standard API specifications for LLM providers makes it challenging to incorporate several LLMs into projects.\nAfter invoking the LLM, AGONETEST selects relevant information from the LLM's answer (i.e., the generated test class). This step is crucial for automating the entire process, since LLMs can provide detailed descriptions or explain how the code should be structured without actually generating it [27]. In this component, AGONETEST removes unnecessary parts (like outline descriptions) and creates a new file to integrate the test class into the project."}, {"title": "E. Test Suite Assessment", "content": "Here we evaluate the quality of the test suite according to the quality metrics and the test smells described below. The actual determination of metrics and test smells is done via library integration, allowing for fully automated test suite assessment. It is important to note that this component is separate from the experimental evaluation discussed later. Instead, it serves as an additional tool provided by AGONETEST to assist engineers in assessing the quality of the generated tests.\n1) Coverage Metrics:\n\u2022 Line coverage [28]: This metric measures the percentage of lines of code executed during the testing process. A 100% line coverage means that every line of code in the software has been run at least once during testing. We selected it because it provides direct visibility over the portion of the source code that is being tested.\n\u2022 Method coverage [28]: Similar to line coverage, this metric focuses on the specific methods or functions in the code. A 100% method coverage score means that all methods have been run at least once during testing. This metric is useful to identify methods that may not have been adequately tested.\n\u2022 Branch coverage [28]: This metric calculates the per- centage of decision points (such as if or switch statements) that have been executed in tests. It ensures that all possible paths in the code are tested, which can uncover defects that might be missed by line or method coverage alone.\n\u2022 Instruction coverage 9: This metric calculates the num- ber of Java bytecode instructions executed during test- ing. It is a detailed metric, unaffected by source code formatting, and can be determined even without debug information in the class files. This helps in pinpointing the smallest code fragments not covered by tests.\n\u2022 Mutation coverage [28]: This metric evaluates the ef- fectiveness of tests in identifying deliberately introduced changes (mutations) in the code, such as modifying an arithmetic operation or reversing a condition. If the tests detect all mutations (i.e., identify all changes), the mutation coverage score is 100%. This metric was chosen because it measures the robustness of the test suite.\n2) Test Smells [23]: These are indicators of inefficient or problem patterns that could negatively affect the maintainabil- ity and effectiveness of the test code. Identifying test smells helps improve the quality of the test code over time and raises awareness of potential issues in test design. AGONETEST determines whether the following test smells are present in the code:\n\u2022 Assertion Roulette (AR) [29]: indicate the number of test methods containing more than one assertion statement without an explanation/message (parameter in the asser- tion method);\n\u2022 Conditional Test Logic (CTL) [30]: indicate the number of test methods that contain one or more control state- ments (i.e., if, switch, conditional expression, for, foreach and while statement);\n\u2022 Constructor Initialization (CI) [31]: indicate if the test class contains a constructor declaration;\n\u2022 Default Test: indicate if the test class is named either 'ExampleUnitTest' or 'ExampleInstrumentedTest';\n\u2022 Duplicate Assert (DA) [31]: indicate the number of test methods that contain more than one assertion statement with the same parameters;\n\u2022 Eager Test (EA) [29]: indicate the number of test methods containing multiple calls to multiple production methods;\n\u2022 Empty Test (EM) [31]: indicate the number of test methods that do not contain a single executable statement;\n\u2022 Exception Handling (EH) [31]: indicate the number of test methods that contain either a throw statement or a catch clause;\n\u2022 General Fixture: is 1 if not all fields instantiated within the setUp method of a test class are utilized by all test methods in the same test class;\n\u2022 Ignored Test (IT) [31]: indicate the number of tests methods that contains the @Ignore annotation;\n\u2022 Lazy Test (LT) [29]: indicate the number of test methods calling the same production method;\n\u2022 Magic Number Test (MNT) [30]: indicate the number of test methods that contain a numeric literal as an argument;\n\u2022 Mystery Guest: indicate the number of test methods containing object instances of files and databases classes;\n\u2022 Redundant Print (RP) [31]: indicate the number of tests methods that invokes either the print, println, printf or write method of the System class;\n\u2022 Redundant Assertion (RA) [31]: indicate the number of test methods that contain an assertion statement in which the expected and actual parameters are the same;\n\u2022 Resource Optimism (RO) [31]: indicate the number of tests methods utilize an instance of a File class without calling the exists(), isFile() or notExists() methods of the object;\n\u2022 Sensitive Equality (SE) [29]: indicate the number of tests methods that invokes the toString() method of an object;\n\u2022 Sleepy Test: indicate the number of tests methods that invokes the Thread.sleep() method;\n\u2022 Unknown Test (UT) [31]: indicates the number of test methods that do not contain a single assertion statement and @Test (expected) annotation parameter.\n3) Library integration: We utilized the following libraries to compute the metrics:\n\u2022 JaCoCo10: JaCoCo is a free Java library used to measure code coverage in test suite execution. It helps developers identify which parts of their code base have been thor- oughly tested and which have not, facilitating a better understanding of the test coverage within the project. We selected JaCoCo because of its widespread adoption, ease of integration with build tools, and report-generation features, which are essential for metric evaluation.\n\u2022 PiTest [32]: PiTest is a mutation testing system for Java and JVM-based systems. It goes beyond traditional line and statement coverage metrics in that it offers more concrete insights into the robustness of a test suite. PiTest introduces minor changes, or mutations, into the source code and then re-runs the tests to determine whether these changes are detected. We chose PiTest because it provides a more granular and realistic view of the actual behavior and response of the system under test compared to traditional coverage tools.\n\u2022 TSDETECT [33]: TSDETECT is a library that focuses on the automatic detection of test smells in software projects. Test smells refer to patterns in test code that may indicate design or implementation issues, leading to less maintainable tests and potentially hindering code comprehension. TSDETECT was chosen for its capability to identify these smells and provide actionable guidelines for code improvement.\nIn this phase, AGONETEST automatically includes these libraries into the project. For each run, AGONETEST checks the configuration files of the supported build systems (Maven and Gradle, Section III-C) to determine if the necessary libraries are already present. If they are not, it modifies the configuration to add the required dependencies.\nAGONETEST demonstrates a high degree of automation, as illustrated by its handling of the PiTest library. Specifically, if the repo uses the JUnit 5 test framework, an additional library, \"pitest-junit5-plugin\", is required. Utilizing informa- tion extracted from the repo in the Prompt Creation module (Section III-C), AGONETEST automatically identifies the test framework in use and adds this dependency without any human intervention.\n4) Automate Test Suite assessment: After adding the nec- essary libraries, AGONETEST runs a build and test to ensure there are no compilation errors. The test suite assessment phase of our process presents a high degree of automation, as we describe below.\nAGONETEST generates a report with the results of the test smells and metrics computed for the LLM-generated tests. To achieve this, the tool automatically retrieves detailed infor- mation from the reports produced by the libraries, compiling these data for each class within each project.\nThis extensive computation process enables a detailed anal- ysis of the generated test suites. By contrasting the results, the module helps identify specific strengths and weaknesses associated with each LLM and prompt configuration. It pro- vides insights into areas where the LLMs excel and highlight potential gaps where improvements are needed.\nFurthermore, this comparison facilitates a clear understand- ing of the nuances in how different LLMs and prompts impact the quality of test generation. It supports the identification of optimal configurations for generating high-quality tests. This detailed analysis is crucial for refining LLMs and enhancing their capabilities in automated test generation.\nBy providing such in-depth evaluations, AGONETEST serves as a valuable tool for researchers and developers. It aids in the continuous improvement of LLMs and contributes to advancements in the field of automated testing. Ultimately, it can ensure that the tests generated are robust and reliable, improving the effectiveness of automated testing solutions."}, {"title": "IV. AGONETEST IN PRACTICE", "content": "In this section, we will demonstrate how AGONETEST operates in practice by describing an end-to-end run of a practical example.\nWe will skip the repository selection phase in our account and move straight to the configuration phase, which concerns LLM selection and prompt specification. Then, we will exem- plify how the results are presented back to the user for further analysis.\nA. Configuration\nAs described in Section III-B, AgoneTest utilizes a YAML file as input, where it is possible to specify information related to two elements: llms and prompts. The YAML file represented in the Listing 1 declares usage of 'gpt-4' and \u2018gpt-3.5 turbo\u2019 models, both provided by OpenAI."}, {"title": "B. Results presentation", "content": "After running the generation phase, AGONETEST generates a CSV file including, for each LLM selected and each prompt- ing technique, the metrics computed for the focal classes as well as the results about test smells. As a way of example, Table II displays an extract of this file containing as well the results for the human-written tests, as they were present in the CLASSES2TEST dataset.\nBy examining this file, users can gain valuable insight into the strengths and weaknesses of each LLM and the prompt combination. Plus, software testers can accurately assess the effectiveness of the LLM in creating usable and effective class-level tests. How this is done is made clear in the following section, where we describe our experimental setup for validation of AGONETEST and discuss some results."}, {"title": "V. EVALUATION", "content": "In this experimental evaluation, we aim to address the following research questions:\n\u2022 RQ1:To what extent is it possible to implement an au- tomated end-to-end process for generating test suites? We analyze the degree of automation of the framework and the points (if any) where we need the human-in-the- loop.\n\u2022 RQ2: Can the quality of test suites automatically generated by different LLMs and prompt strategies be effectively assessed? We investigate whether the framework can provide information about the quality of the test suite in terms of efficiency and robustness and help identify strengths, weaknesses, and potential improvements."}, {"title": "A. Dataset", "content": "In our experiment, we randomly selected 10 repositories from our dataset CLASSES2TEST. These repositories contain a total of 94 focal classes of various lengths and complexity, as shown in Table III. The size of the sample of randomly selected repositories is chosen to be representative enough of the variability encountered in real-world projects (usually comprising of one to a handful of co-dependent repositories), while ensuring that is tractable by our system in terms of scale."}, {"title": "B. LLMs and prompts configuration", "content": "For our experiment, we selected two LLMs from the models supported by LiteLLM15. We have chosen the \u2018gpt-4,16 and 'gpt-3.5 turbo\u201917 models. The \u2018gpt-4' model was selected for its outstanding performance on the HumanEval benchmark [35], while \u2018gpt-3.5 turbo' was chosen as an earlier generation model, allowing a meaningful comparison."}, {"title": "VI. DISCUSSION AND LESSONS LEARNED", "content": "In this section, we will answer the research questions previously defined and discuss the lesson learned together with possible future research directions.\nRQ1: To what extent is it possible to implement an automated end-to-end process for generating test suites?"}, {"title": "A. Lessons Learned", "content": "Throughout the development and evaluation of AGONETEST, we gathered several key insights that will guide future improvements in the framework. These lessons are crucial for refining the system and improving the efficacy of LLM-generated tests. Each subsection below highlights a specific challenge encountered and outlines a potential solution.\n1) Compilation and Test Pass Rate: Our experiments show compilation and test pass rates that could be improved in light of the pursuit of full automation. The causes of these are diverse (e.g., import classes that do not exist or are missing). Automating the correction of these recurring problems is possible [36] and will increase the success rate of the generated tests. One promising approach involves asking the LLM itself to analyze errors in the generated test code and provide fixes. By supplying the identified errors as feedback, the LLM can generate corrected and functional code, thereby enhancing the initial output. Additionally, enhancing the robustness of the generated tests by incorporating context-aware validation and fixing mechanisms will ensure that the test suites align closely with the project's specific structures. This integrated approach not only automates error correction but also enhances the overall reliability and effectiveness of the test generation process, moving closer to the goal of fully automated, high-quality test suite production.\n2) Performance in Mutation Testing: Human-written tests consistently outperformed LLM-generated tests in terms of mutation coverage, indicating that manually written tests are more effective at identifying code changes introduced through mutation. To address this, we should focus on improving the robustness of the generated test suites by refining the prompting algorithms and incorporating mutation-aware test generation techniques.\n3) Scalability and Resource Management: Automating the entire pipeline-downloading projects, generating test suites, integrating libraries, and performing evaluations proved to be resource intensive. Efficiently managing and parallelizing these tasks can alleviate computational overhead and improve scalability, allowing AGONETEST to handle larger datasets and codebases more effectively.\n4) Impact of Prompting Techniques: The choice of prompt- ing technique significantly impacts the quality of the generated tests. Our experiments showed that zero-shot prompting with gpt-4 yielded the best results, but performance varied across different combinations of LLMs and prompts. Systematically exploring and evaluating different prompting strategies will help identify the most effective configurations for various scenarios.\n5) Automated Context Extraction: Providing the LLMs with accurate context information, such as the testing frame- work and the Java version, is essential for generating correct test classes. Automating the extraction of this context informa- tion reduces the need for manual intervention and improves the quality of generated prompts and tests. Enhancing the automa- tion of context extraction by developing more sophisticated parsers and context inference algorithms will dynamically adapt to various project configurations.\n6) Real-world Applicability: Building the dataset from ac- tual open-source Java repositories on GitHub ensured that AGONETEST operates in real-world scenarios. However, en- suring that the dataset is representative of real-life situations across different types of repos and codebases remains an on- going goal. To maintain and improve real-world applicability, we should continuously upgrade and update our dataset to include a broader range of real-world repositories and project structures, ensuring that the evaluation remains relevant and comprehensive.\nThese lessons direct us towards further improvements in AGONETEST. By implementing these improvements, we aim to develop a more robust, efficient, and reliable framework for automated unit test generation."}, {"title": "VII. LIMITATIONS", "content": "Although AGONETEST presents an innovative framework for automating the generation and evaluation of unit test suites using LLMs, several limitations should be acknowledged re- garding its current implementation and the first experimental results.\nA. Dataset and Generalization\nFor our evaluation, we relied on the newly created CLASSES2TEST dataset, derived from METHODS2TEST. Al- though this dataset is designed to evaluate class-level test generation, its scope is limited to Java projects. This makes our findings hardly generalizable to different pro- gramming languages. Moreover, the repositories included in CLASSES2TEST were selected based on their ability to com- pile without errors, potentially introducing a bias towards well- structured codebases.\nB. Model and Prompt Variability\nLimited Number of LLMs and Prompts Tested: Although AGONETEST supports various LLMs and prompting tech- niques, our initial experimental setup involved only two mod- els (gpt-4 and gpt-3.5 turbo) and two prompt types (zero-shot and few-shot). As LLMs and prompt engineering techniques continue to evolve, the results might vary significantly with newer models and advanced prompts. The limited scope of our initial tests could thus restrict the breadth of our conclusions.\nTemperature Setting Restrictions: In our experiment, we set the temperature parameter to 0 to ensure consistency and reproducibility. Although this reduces randomness and increases coherence, it may inadvertently limit the creativity and diversity of the generated test cases. Different temperature settings could yield more varied results, which were not explored in this study."}, {"title": "C. Compilation and Execution Failures", "content": "A notable limitation observed during our experiments was the non-compilation and execution failures of some generated test classes. Approximately 66% of the test classes generated were either rejected during the compilation phase or failed to contribute positively to the metrics due to inherent errors. This reveals the current inability of some LLMs to consistently gen- erate syntactically and semantically correct test code, affecting the overall evaluation."}, {"title": "D. Evaluation Metrics", "content": "While we employed a comprehensive set of metrics and test smell indicators, these metrics alone may not fully capture the quality of the test suite."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we present AGONETEST, a comprehensive framework for automating the generation and assessment of unit test suites using LLMs. This framework focuses on generating complex, class-level test suites while automating the entire testing process from test generation to integration and evaluation.\nThe results of our experiments demonstrate that AGONETEST can produce and evaluate unit tests across various real-world projects, offering detailed insights into the performance of different LLMs and prompting techniques. While the initial findings are promising, they also highlight challenges emphasizing the need for further refinement.\nThe automation of unit test generation using LLMs is a promising field. While current capabilities do not yet match those of human engineers for some tasks (such as mutation coverage), promising results in instruction, line, and method coverage indicate that further research and refinement can bridge this gap. Future work should focus on systematic re- search into the most effective LLMs and prompts, coupled with continuous improvements in automated correction mechanisms for recurrent issues."}]}