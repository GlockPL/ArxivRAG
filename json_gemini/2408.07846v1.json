{"title": "A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Bartolini"], "abstract": "Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness. Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation. Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation. Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods. These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios. Moreover, previous studies do not approach the problem at a suitable scale for real-life applications. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. In this work, we present AGONETEST: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites. Starting from a state-of-the-art dataset (i.e., METHODS2TEST), we built a new dataset for comparing human-written tests with those generated by LLMs. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.", "sections": [{"title": "I. INTRODUCTION", "content": "Software testing is a critical step in the software development lifecycle, essential for ensuring code correctness and reliability. Within it, unit testing is the stage concerned with verifying the proper functioning of individual code units. Designing and building unit tests is a costly and labor-intensive process that requires significant time and specialized skills. Automating this process represents a promising area for research and development.\nAutomated tools for generating unit tests can reduce test engineers' and software developers' workload. These tools typically use static code analysis methods to generate test suites. For example, EvoSuite [1], a popular tool that combines static code analysis with evolutionary search, has been demonstrated to achieve adequate coverage.\nLarge Language Models (LLMs), efficiently exploited in various aspects of software development, could also handle the automatic generation of unit tests. Several empirical studies on LLMs have highlighted their ability to generate tests for simple scenarios, often limited to single methods [2]\u2013[5]. Though directionally useful, these explorations focus on independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios [6].\nMoreover, previous studies do not approach the problem at a suitable scale for real-life examples. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency.\nTo address these gaps, we have developed an approach for generating and evaluating test suites that are more representative of real-life complex software projects. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment.\nIn this work, we introduce AGONETEST, an automated system designed to generate test suites for Java projects, accompanied by a rigorous and systematic methodology to evaluate these generated test suites. Leveraging the METHODS2TEST dataset [7], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs. We integrate libraries such as JaCoCo, PITest, and TsDetect to compute the metrics for test evaluation.\nThe main contributions of our work are as follows:"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Unit test generation is the automated process of creating test cases for individual software components, such as functions, methods, or modules. These test cases are used to independently verify the correct functioning of each unit.\nPresent techniques employ randomness-based [8], [9], constraint-based [10], [11], or search-based approaches [12], [13]. The core idea behind these methods is to transform the problem into one that can be solved mathematically. For example, search-based techniques convert testing into an optimization problem, to generate unit test cases [14]. Consequently, the objective of these techniques is to generate all potential solutions and then select those that achieve better code coverage. EvoSuite [1] works by accepting a Java class or method as input and applying search-based algorithms to generate a test suite that meets coverage criteria such as code or branch coverage. EvoSuite assesses test fitness using iterative processes of variation, selection, and optimization. Not only does it generate JUnit test cases, but it also provides a comprehensive report produced by inspecting the efficiency of the created test suite, based on metrics such as code coverage and mutation score. One limitation of EvoSuite is that it often produces tests that lack clarity and readability [15]. Additionally, EvoSuite can only be used on projects using Java 9 or lower, which limits its applicability to more modern Java projects (the last Java version, at the present time, is 22). Unlike EvoSuite, AGONETEST incorporates advanced evaluation metrics and test-smell recognition, providing a more comprehensive assessment of the quality of generated test suites and ensuring readability by leveraging human-like LLM-generated code. Moreover, AGONETEST supports all Java LTS versions, allowing projects built on newer versions to be tested as well, overcoming the compatibility limitations of EvoSuite.\nSince the emergence of LLMs, they have been used for test suite generation. The first techniques exploiting LLMs were thought of as solutions to neural machine translation problems [16], [17]. Such approaches work by translating from the primary method to the appropriate test prefix or test assertion while also fine-tuning the LLMs using the test generation dataset. For instance, AthenaTest [17] optimizes BART [18] using a test generation dataset in which the source is the primary method along with its corresponding code context, and the result is the complete test case. AthenaTest focuses mainly on generating method-level tests by fine-tuning a single model, while AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design, thereby handling more complex, real-world scenarios. In light of the rapid evolution of instruction-tuned LLMs, the proliferation of methods for generating tests is on the rise, exploiting guided LLMs through appropriate prompts, as opposed to model fine-tuning [19], [20]. Several proposals for evaluating LLMs in test suite generation have emerged. For example, CHATTESTER [5] proposes a tool for evaluating and improving LLM-generated tests based on ChatGPT. ChatTester focuses on improving and evaluating tests generated by a specific LLM (ChatGPT), but requires human intervention to evaluate the generated code and does not provide an evaluation of class-level tests on multiple LLMs. AGONETEST provides support instead for a variety of LLMs and evaluates each LLM's performance on a wide range of real-life Java projects. TESTPILOT [3] is also focused on generating and improving tests using LLMs on JavaScript code. Although TestPilot performs an automated evaluation, it lacks wider applicability to projects other than the 25 repositories it considers in the work provided as reference here. AGONETEST offers far broader applicability by using a dataset of 9,410 Github repositories, and automatically integrating test libraries into them. CEDAR [21] instead proposes a prompt construction strategy based on few-shot learning [22] and the Codex model to generate tests. Cedar uses a specific prompt construction strategy, but it does not incorporate a structured mechanism to evaluate multiple LLMs and prompt techniques in a unified framework. AGONETEST provides this by allowing the integration and evaluation of various"}, {"title": "III. OVERVIEW OF AGONETEST", "content": "The term agone, originating from ancient Greece and Rome, signified a contest wherein philosophers debated their ideas, with the audience determining the victor. We adopt the term agone metaphorically to represent the competitive evaluation of LLMs and their respective prompting strategies within an arena aimed at generating optimal unit test suites. AGONETEST determines the optimal strategies based on standard test quality metrics, which we elaborate on in subsequent sections.\nAGONETEST is designed to provide software testers with a system for generating and assessing unit tests. This assessment focuses on key metrics such as code coverage and the presence of known test smells, thereby offering a comprehensive assessment of test suite quality.\nAGONETEST operates on the principle that the evaluation of LLMs in the task of generating high-quality unit tests can be performed through the collaboration of test engineers and data scientists (or prompt engineers). However, in practice, a single experienced test engineer familiar with generative AI can perform both roles, allowing the focus to be only on defining new prompt techniques and the comparison of LLMs. This is the persona that we evoke when we refer to the AGONETEST user (alternatively \"the test engineer\u201d) in the remainder of this paper.\nThe system helps test engineers through the following phases:\n\u2022 Strategy Configuration\nAutomated Test Generation\n\u2022 Strategy Evaluation\nFigure 1 provides a high-level diagram of the architecture of AGONETEST, showing the operating modules that streamline the test generation and evaluation process. These modules are described as follows:\nAs an initial configuration step, the user chooses which repositories to generate test suites for. This initial phase leverages a comprehensive dataset of annotated open-source Java repositories, which we contribute to the community. It involves preparing, loading, and managing the repositories to be tested by the system.\nIn this phase, configuration parameters are elicited from the selected repositories (e.g., the project java version, used testing framework, etc.) and processed to create prompts templates for the LLMs.\nDuring this phase, the prompt templates used in the previous phases are fully instantiated and then used to generate unit test suites in the next step.\nHere, AGONETEST orchestrates the interaction with the selected LLMs, feeding them the instantiated prompts to produce the unit test code. Each LLM generates test classes that are then integrated into the project structure."}, {"title": "B. Configuration Parameters Elicitation", "content": "Before unit test generation can begin, the system extracts some parameters from the projects selected in the previous step. These parameters are then fed into the module that selects prompts and LLMs. To query the model under examination, various prompting techniques are available and can be chosen [24].\nThe configuration parameters include:\nfocal_class: This variable contains the Java class for which the test suite must be generated;\ntesting_framework: This variable provides the name and version of the project's testing framework (e.g., JUnit 4), directly extracted from the project during execution;\njava_version: This variable allows you to retrieve the version of Java that the project uses.\nexample_focal_class & example_test_class: These variables contain an example focal class and the corresponding test class extracted from a reference repository, useful to provide an example to the LLM if one wants to use the few-shot prompting technique;\nexample_testing_framework & example_java_version: These variables provide the information about the example repo.\nSee Section IV-A for an example of a real implementation."}, {"title": "C. Prompt Creation", "content": "In this phase, the prompt templates described in the previous phases are fully instantiated to create viable prompts to guide the LLM in generating unit tests. We populate the user-supplied prompt structures by replacing the variables outlined in Section III-B.\nIt has to be noted that, in order to make sure our experiments and findings are reproducible, we prepared CLASSES2TEST by saving the commit hashes of the repositories used as sources. This allows AGONETEST to consistently extract information such as the Java version used, the type of test framework (e.g., jUnit), and its version.\nUnlike previous approaches to creating unit testing with LLMs that require human intervention to input context information [2], [5], AGONETEST automates the process to a far greater degree. AGONETEST employs ElementTree [25] and a parser to read and modify the Maven and Gradle build (see Section III-E3). It analyzes the libraries present and the Java version used in each build system. This method, along with the ability to use examples, offers users a versatile system for generating prompts."}, {"title": "D. Test Suite Generation", "content": "At this point in the process, we have everything we need for the selected LLMs to generate test suites for each focal class of the project. To ensure each model has an appropriate number of tokens, we use tiktoken, a BPE tokenizer [26], to evaluate the token count in the prompt. If the limit is exceeded, AGONETEST returns an error to the user, specifying the number of tokens exceeded.\nWe remark that AGONETEST allows users to evaluate a wide range of LLMs automatically. This capability is provided by the open-source LiteLLM library, which facilitates communication with more than 100 models using a standard interaction based on the OpenAI API format. Integration is made easier by LiteLLM, which translates inputs to satisfy the unique endpoint needs of each provider. This is crucial in today's environment, where the absence of standard API specifications for LLM providers makes it challenging to incorporate several LLMs into projects.\nAfter invoking the LLM, AGONETEST selects relevant information from the LLM's answer (i.e., the generated test class). This step is crucial for automating the entire process, since LLMs can provide detailed descriptions or explain how the code should be structured without actually generating it [27]. In this component, AGONETEST removes unnecessary parts (like outline descriptions) and creates a new file to integrate the test class into the project."}, {"title": "E. Test Suite Assessment", "content": "Here we evaluate the quality of the test suite according to the quality metrics and the test smells described below. The actual determination of metrics and test smells is done via library integration, allowing for fully automated test suite assessment. It is important to note that this component is separate from the experimental evaluation discussed later. Instead, it serves as an additional tool provided by AGONETEST to assist engineers in assessing the quality of the generated tests."}, {"title": "1) Coverage Metrics:", "content": "Line coverage [28]: This metric measures the percentage of lines of code executed during the testing process. A 100% line coverage means that every line of code in the software has been run at least once during testing. We selected it because it provides direct visibility over the portion of the source code that is being tested.\nMethod coverage [28]: Similar to line coverage, this metric focuses on the specific methods or functions in the code. A 100% method coverage score means that all methods have been run at least once during testing. This metric is useful to identify methods that may not have been adequately tested.\nBranch coverage [28]: This metric calculates the percentage of decision points (such as if or switch statements) that have been executed in tests. It ensures that all possible paths in the code are tested, which can uncover defects that might be missed by line or method coverage alone.\nInstruction coverage 9: This metric calculates the number of Java bytecode instructions executed during test-\nMutation coverage [28]: This metric evaluates the effectiveness of tests in identifying deliberately introduced changes (mutations) in the code, such as modifying an arithmetic operation or reversing a condition. If the tests detect all mutations (i.e., identify all changes), the mutation coverage score is 100%. This metric was chosen because it measures the robustness of the test suite."}, {"title": "2) Test Smells [23]:", "content": "These are indicators of inefficient or problem patterns that could negatively affect the maintainability and effectiveness of the test code. Identifying test smells helps improve the quality of the test code over time and raises awareness of potential issues in test design. AGONETEST determines whether the following test smells are present in the code:\nAssertion Roulette (AR) [29]: indicate the number of test methods containing more than one assertion statement without an explanation/message (parameter in the assertion method);\nConditional Test Logic (CTL) [30]: indicate the number of test methods that contain one or more control statements (i.e., if, switch, conditional expression, for, foreach and while statement);\nConstructor Initialization (CI) [31]: indicate if the test class contains a constructor declaration;\nindicate if the test class is named either 'ExampleUnitTest' or 'ExampleInstrumentedTest';\nDuplicate Assert (DA) [31]: indicate the number of test methods that contain more than one assertion statement with the same parameters;\nEager Test (EA) [29]: indicate the number of test methods containing multiple calls to multiple production methods;\nEmpty Test (EM) [31]: indicate the number of test methods that do not contain a single executable statement;\nException Handling (EH) [31]: indicate the number of test methods that contain either a throw statement or a catch clause;\nGeneral Fixture: is 1 if not all fields instantiated within the setUp method of a test class are utilized by all test methods in the same test class;\nIgnored Test (IT) [31]: indicate the number of tests methods that contains the @Ignore annotation;\nLazy Test (LT) [29]: indicate the number of test methods calling the same production method;\nMagic Number Test (MNT) [30]: indicate the number of test methods that contain a numeric literal as an argument;\nMystery Guest: indicate the number of test methods containing object instances of files and databases classes;\nRedundant Print (RP) [31]: indicate the number of tests methods that invokes either the print, println, printf or write method of the System class;\nRedundant Assertion (RA) [31]: indicate the number of"}, {"title": "3) Library integration:", "content": "We utilized the following libraries to compute the metrics:\nJaCoCo10: JaCoCo is a free Java library used to measure code coverage in test suite execution. It helps developers identify which parts of their code base have been thoroughly tested and which have not, facilitating a better understanding of the test coverage within the project. We selected JaCoCo because of its widespread adoption, ease of integration with build tools, and report-generation features, which are essential for metric evaluation.\nPiTest [32]: PiTest is a mutation testing system for Java and JVM-based systems. It goes beyond traditional line and statement coverage metrics in that it offers more concrete insights into the robustness of a test suite. PiTest introduces minor changes, or mutations, into the source code and then re-runs the tests to determine whether these changes are detected. We chose PiTest because it provides a more granular and realistic view of the actual behavior and response of the system under test compared to traditional coverage tools.\nTSDETECT [33]: TSDETECT is a library that focuses on the automatic detection of test smells in software projects. Test smells refer to patterns in test code that may indicate design or implementation issues, leading to less maintainable tests and potentially hindering code comprehension. TSDETECT was chosen for its capability to identify these smells and provide actionable guidelines for code improvement.\nIn this phase, AGONETEST automatically includes these libraries into the project. For each run, AGONETEST checks the configuration files of the supported build systems (Maven and Gradle, Section III-C) to determine if the necessary libraries are already present. If they are not, it modifies the configuration to add the required dependencies.\nAGONETEST demonstrates a high degree of automation, as illustrated by its handling of the PiTest library. Specifically, if the repo uses the JUnit 5 test framework, an additional library, \"pitest-junit5-plugin\", is required. Utilizing information extracted from the repo in the Prompt Creation module (Section III-C), AGONETEST automatically identifies the test framework in use and adds this dependency without any human intervention."}, {"title": "4) Automate Test Suite assessment:", "content": "After adding the necessary libraries, AGONETEST runs a build and test to ensure there are no compilation errors. The test suite assessment phase of our process presents a high degree of automation, as we describe below.\nAGONETEST generates a report with the results of the test smells and metrics computed for the LLM-generated tests. To achieve this, the tool automatically retrieves detailed information from the reports produced by the libraries, compiling these data for each class within each project.\nThis extensive computation process enables a detailed analysis of the generated test suites. By contrasting the results, the module helps identify specific strengths and weaknesses associated with each LLM and prompt configuration. It provides insights into areas where the LLMs excel and highlight potential gaps where improvements are needed.\nFurthermore, this comparison facilitates a clear understanding of the nuances in how different LLMs and prompts impact the quality of test generation. It supports the identification of optimal configurations for generating high-quality tests. This detailed analysis is crucial for refining LLMs and enhancing their capabilities in automated test generation.\nBy providing such in-depth evaluations, AGONETEST serves as a valuable tool for researchers and developers. It aids in the continuous improvement of LLMs and contributes to advancements in the field of automated testing. Ultimately, it can ensure that the tests generated are robust and reliable, improving the effectiveness of automated testing solutions."}, {"title": "IV. AGONETEST IN PRACTICE", "content": "In this section, we will demonstrate how AGONETEST operates in practice by describing an end-to-end run of a practical example.\nWe will skip the repository selection phase in our account and move straight to the configuration phase, which concerns LLM selection and prompt specification. Then, we will exemplify how the results are presented back to the user for further analysis."}, {"title": "A. Configuration", "content": "As described in Section III-B, AgoneTest utilizes a YAML file as input, where it is possible to specify information related to two elements: 11ms and prompts. The YAML file represented in the Listing 1 declares usage of 'gpt-4'11 and \u2018gpt-3.5 turbo\u201912 models, both provided by OpenAI13."}, {"title": "B. Results presentation", "content": "After running the generation phase, AGONETEST generates a CSV file including, for each LLM selected and each prompting technique, the metrics computed for the focal classes as well as the results about test smells. As a way of example, Table II displays an extract of this file containing as well the results for the human-written tests, as they were present in the CLASSES2TEST dataset.\nBy examining this file, users can gain valuable insight into the strengths and weaknesses of each LLM and the prompt combination. Plus, software testers can accurately assess the effectiveness of the LLM in creating usable and effective class-level tests. How this is done is made clear in the following section, where we describe our experimental setup for validation of AGONETEST and discuss some results."}, {"title": "V. EVALUATION", "content": "In this experimental evaluation, we aim to address the following research questions:\nRQ1:To what extent is it possible to implement an automated end-to-end process for generating test suites? We analyze the degree of automation of the framework and the points (if any) where we need the human-in-the-loop.\nRQ2: Can the quality of test suites automatically generated by different LLMs and prompt strategies be effectively assessed? We investigate whether the framework can provide information about the quality of the test suite in terms of efficiency and robustness and help identify strengths, weaknesses, and potential improvements."}, {"title": "A. Dataset", "content": "In our experiment, we randomly selected 10 repositories from our dataset CLASSES2TEST. These repositories contain a total of 94 focal classes of various lengths and complexity, as shown in Table III. The size of the sample of randomly selected repositories is chosen to be representative enough of the variability encountered in real-world projects (usually comprising of one to a handful of co-dependent repositories), while ensuring that is tractable by our system in terms of scale."}, {"title": "B. LLMs and prompts configuration", "content": "For our experiment, we selected two LLMs from the models supported by LiteLLM15. We have chosen the \u2018gpt-4,16 and 'gpt-3.5 turbo\u201917 models. The \u2018gpt-4' model was selected for its outstanding performance on the HumanEval benchmark [35], while \u2018gpt-3.5 turbo' was chosen as an earlier generation model, allowing a meaningful comparison."}, {"title": "VI. DISCUSSION AND LESSONS LEARNED", "content": "In this section, we will answer the research questions previously defined and discuss the lesson learned together with possible future research directions.\nRQ1: To what extent is it possible to implement an automated end-to-end process for generating test suites?\nAGONETEST provides an end-to-end automated process to generate and evaluate test suites without human intervention. However, there are two points requiring attention that our experiments underline:\nThe compilation success rate of the generated test classes shows room for improvement (in our experiment ranged between 64% and 76%);\nThe percentage of tests passed was relatively low (between 30% and 38%).\nFurther analysis revealed that many generated tests failed due to incorrect imports or syntax errors and not because they discovered previously undetected bugs. To improve on these results and increase such percentages, there are different paths to explore. One is human-in-the-loop: where human intervention might include manually fixing code errors, adjusting settings, or installing required libraries for successful execution.\nOn the other hand, a good result, in light of automation, is having automated the process of extracting contextual information from project configuration files (such as Maven or Gradle). This minimizes the need for manual intervention and enhances the accuracy of the generated prompts and tests.\nRQ2: Can the quality of test suites automatically generated by different LLMs and prompt strategies be effectively assessed? AGONETEST gives relevant information about the quality of the test suite generated, in terms of code coverage, robustness of the test suite to artificially injected bugs (i.e., mutation coverage), and test smells. Indeed, the presence of test smells indicates potential issues with test design and maintainability."}, {"title": "A. Lessons Learned", "content": "Throughout the development and evaluation of AGONETEST, we gathered several key insights that will guide future improvements in the framework. These lessons are crucial for refining the system and improving the efficacy of LLM-generated tests. Each subsection below highlights a specific challenge encountered and outlines a potential solution.\n1) Compilation and Test Pass Rate: Our experiments show compilation and test pass rates that could be improved in light of the pursuit of full automation. The causes of these are diverse (e.g., import classes that do not exist or are missing). Automating the correction of these recurring problems is possible [36] and will increase the success rate of the generated tests. One promising approach involves asking the LLM itself to analyze errors in the generated test code and provide fixes. By supplying the identified errors as feedback, the LLM can generate corrected and functional code, thereby enhancing the initial output. Additionally, enhancing the robustness of the generated tests by incorporating context-aware validation and fixing mechanisms will ensure that the test suites align closely with the project's specific structures. This integrated approach not only automates error correction but also enhances the overall reliability and effectiveness of the test generation process, moving closer to the goal of fully automated, high-quality test suite production.\n2) Performance in Mutation Testing: Human-written tests consistently outperformed LLM-generated tests in terms of mutation coverage, indicating that manually written tests are more effective at identifying code changes introduced through mutation. To address this, we should focus on improving the robustness of the generated test suites by refining the prompting algorithms and incorporating mutation-aware test generation techniques.\n3) Scalability and Resource Management: Automating the entire pipeline-downloading projects, generating test suites, integrating libraries, and performing evaluations proved to be resource intensive. Efficiently managing and parallelizing these tasks can alleviate computational overhead and improve scalability, allowing AGONETEST to handle larger datasets and codebases more effectively.\n4) Impact of Prompting Techniques: The choice of prompting technique significantly impacts the quality of the generated tests. Our experiments showed that zero-shot prompting with gpt-4 yielded the best results, but performance varied across different combinations of LLMs and prompts. Systematically exploring and evaluating different prompting strategies will help identify the most effective configurations for various scenarios.\n5) Automated Context Extraction: Providing the LLMs with accurate context information, such as the testing framework and the Java version, is essential for generating correct test classes. Automating the extraction of this context information reduces the need for manual intervention and improves the quality of generated prompts and tests. Enhancing the automation of context extraction by developing more sophisticated parsers and context inference algorithms will dynamically adapt to various project configurations.\n6) Real-world Applicability: Building the dataset from actual open-source Java repositories on GitHub ensured that AGONETEST operates in real-world scenarios. However, ensuring that the dataset is representative of real-life situations across different types of repos and codebases remains an ongoing goal. To maintain and improve real-world applicability, we should continuously upgrade and update our dataset to include a broader range of real-world repositories and project structures, ensuring that the evaluation remains relevant and comprehensive."}, {"title": "VII. LIMITATIONS", "content": "Although AGONETEST presents an innovative framework for automating the generation and evaluation of unit test suites using LLMs, several limitations should be acknowledged regarding its current implementation and the first experimental results."}, {"title": "A. Dataset and Generalization", "content": "For our evaluation, we relied on the newly created CLASSES2TEST dataset, derived from METHODS2TEST. Although this dataset is designed to evaluate class-level test generation, its scope is limited to Java projects. This makes our findings hardly generalizable to different programming languages. Moreover, the repositories included in CLASSES2TEST were selected based on their ability to compile without errors, potentially introducing a bias towards well-structured codebases."}, {"title": "B. Model and Prompt Variability", "content": "Limited Number of LLMs and Prompts Tested: Although AGONETEST supports various LLMs and prompting techniques, our initial experimental setup involved only two models (gpt-4 and gpt-3.5 turbo) and two prompt types (zero-shot and few-shot). As LLMs and prompt engineering techniques continue to evolve, the results might vary significantly with newer models and advanced prompts. The limited scope of our initial tests could thus restrict the breadth of our conclusions.\nTemperature Setting Restrictions: In our experiment, we set the temperature parameter to 0 to ensure consistency and reproducibility. Although this reduces randomness and increases coherence, it may inadvertently limit the creativity and diversity of the generated test cases. Different temperature settings could yield more varied results, which were not explored in this study."}, {"title": "C. Compilation and Execution Failures", "content": "A notable limitation observed during our experiments was the non-compilation and execution failures of some generated test classes. Approximately 66% of the test classes generated were either rejected during the compilation phase or failed to contribute positively to the metrics due to inherent errors. This reveals the current inability of some LLMs to consistently generate syntactically and semantically correct test code, affecting the overall evaluation."}, {"title": "D. Evaluation Metrics", "content": "While we employed a comprehensive set of metrics and test smell indicators, these metrics alone may not fully capture the quality of the test suite."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we present AGONETEST, a comprehensive framework for automating the generation and assessment of unit test suites using LLMs. This framework focuses on generating complex, class-level test suites while automating the entire testing process from test generation to integration and evaluation.\nThe results of our experiments demonstrate that AGONETEST can produce and evaluate unit tests across various real-world projects, offering detailed insights into the performance of different LLMs and prompting techniques. While the initial findings are promising, they also highlight challenges emphasizing the need for further refinement.\nThe automation of unit test generation using LLMs is a promising field. While current capabilities do not yet match those of human engineers for some tasks (such as mutation coverage), promising results in instruction, line, and method coverage indicate that further research and refinement can bridge this gap. Future work should focus on systematic research into the most effective LLMs and prompts, coupled with continuous improvements in automated correction mechanisms for recurrent issues."}]}