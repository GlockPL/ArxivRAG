{"title": "Aligning Robot Navigation Behaviors with Human Intentions and Preferences", "authors": ["Haresh Karnan"], "abstract": "Recent advances in the field of machine learning have led to new ways for mobile robots to acquire advanced navigational capabilities (Bojarski et al., 2016; Kahn et al., 2018; Kendall et al., 2019; Pan et al., 2018; Silver et al., 2010). However, these learning-based methods raise the possibility that learned navigation behaviors may not align with the intentions and preferences of people, also known as value misalignment. To mitigate this danger, this dissertation aims to answer the question \"How can we use machine learning methods to align the navigational behaviors of autonomous mobile robots with human intentions and preferences?\u201d First, this dissertation answers this question by introducing a new approach to learning navigation behaviors by imitating human-provided demonstrations of the intended navigation task. This contribution allows mobile robots to acquire autonomous visual navigation capabilities through imitating human demonstrations using a novel objective function that encourages the agent to align with the navigation objective of the human and penalizes for misalignment. Second, this dissertation introduces two algorithms to enhance terrain-aware off-road navigation for mobile robots through learning visual terrain awareness in a self-supervised manner. This contribution enables mobile robots to obey a human operator's preference for navigating over different terrains in urban outdoor environments and extrapolate these preferences to visually novel terrains by leveraging multi-modal representations. Finally, in the context of robot navigation in human-occupied environments, this dissertation introduces a dataset and an algorithm for robot navigation in a socially compliant manner in both indoor and outdoor environments. In summary, the contributions in this dissertation\u00b9 take a significant step towards addressing the value alignment problem in autonomous navigation, enabling mobile robots to navigate autonomously with objectives that align with the intentions and preferences of humans.", "sections": [{"title": "Introduction", "content": "Advances in the field of machine learning and artificial intelligence for robotics have enabled mobile robots to become increasingly intelligent, efficient, and autonomous (Bojarski et al., 2016; Kahn et al., 2018; Kendall et al., 2019; Pan et al., 2018; Silver et al., 2010). Coupled with a leap in compute capabilities, especially of hardware accelerators enabling deep learning-based methods to learn from huge amounts of data (Hobbhahn and Besiroglu, 2022), autonomous mobile robots have gained improved capabilities such as real-time perception (Redmon et al., 2016; He et al., 2017; Radford et al., 2021b), planning (Chaplot et al., 2021; Tamar et al., 2016), and control (Bojarski et al.; Pan et al., 2018; Karnan et al., 2022c; Xiao et al., 2020b,c; Kim et al., 2003). These advances in data-driven algorithms, combined with improved hardware and availability of large datasets (Perille et al., 2020; Karnan et al., 2022a; Biswas and Veloso, 2013) have contributed to increasingly intelligent and efficient autonomous mobile robotic agents (Ama; Khandelwal et al., 2017). As these agents become more capable, there is a danger that their internal objectives may result in behaviors that do not align with the intentions and preferences of humans, also known as value misalignment.\u00b9 A well-known story related to value misalignment dates to King Midas from Greek mythology, who wished that everything he touched would be turned into gold, but failed to anticipate that food and drink touched by him for consumption also would turn into gold, making it a regrettable wish that did not align with his intentions. Similarly, in robotics, value misalignment has been observed as a consequence of the effects of reward misdesign (Knox et al., 2022) and reward hacking (Hadfield-Menell et al., 2017); for example, a vacuum cleaning robot rewarded for collecting dust might begin to eject collected dust to accumulate more rewards (Russell, 2003). Several other instances of value misalignment in robotics have been observed in prior work (Amodei et al., 2016; Everitt, 2018; Ibarz et al., 2018; Lehman et al., 2020). Therefore, it is necessary to develop new methods that ensure that the behavior of autonomous driving agents aligns with the intentions and preferences of their human operators or owners. This dissertation is especially concerned with the problem of autonomous robot navigation, i.e., to enable a robot to navigate autonomously, with minimal or no human supervision during deployment in ways that align with the intentions and preferences of people. Classical heuristic-based approaches (Dijkstra, 1959; Hart et al., 1968b; Xiao et al., 2020c) to this problem use hand-designed cost functions, planners, and kinodynamic controllers; and work well in structured environments such as industries and warehouses that are fully mapped, standardized for robot usage, and have minimal human presence. However, these approaches for navigation often struggle in unstructured, real-world conditions, including\u2014but not limited to-navigation in unmapped novel environments, navigation on non-standard off-road terrains, and safe navigation in the presence of people. To overcome the limitations of classical approaches to navigation, many in the research community have started using tools from machine learning. Unlike classical approaches that are heuristic-based and designed for specific environments (Xiao et al., 2020c), learning-based approaches offer the promise of learning patterns from data that may help the robot continually reason and adapt (Bojarski et al.; Pan et al., 2018; Pomerleau, 1989; Tai et al., 2018; Xiao et al., 2020a). However, the success of learning-based approaches is dependent on designing the right objective function for navigation that reflects the human-intended task, since a poorly-defined objective function could lead to unintended (misaligned) behaviors (Amodei et al., 2016; Everitt, 2018; Ibarz et al., 2018; Lehman et al., 2020). For example, a mobile robot tasked with reaching a goal location outdoors in a collision-free route may choose to trample over a bed of flowers, which may be undesirable for its human operator. In an alternate scenario, when tasked with navigating in the presence of people while avoiding collisions, a mobile robot may choose to impolitely cut through groups of people, or move dangerously close to a human, behaviors that most humans would avoid. Conversely, at the other extreme, these robots may become overly cautious, leading to situations where they stop entirely and cease to make further progress, especially in complex or unpredictable environments. When mobile robots navigate in unexpected ways that are not generally preferred by humans, it may sometimes lead to catastrophic events, such as accidents with elderly people, visually impaired people, or children. Such public safety concerns in the past have led to autonomous mobile robots being banned from public roads and sidewalks (Gua, 2017; Sid, 2022). To address the challenge of value alignment in autonomous navigation, the method of Learning from Demonstration (LfD)\u2014as discussed by Argall et al. (2009) and Ravichandar et al. (2020)\u2014emerges as a powerful approach. By inferring either the task objective or an imitating policy from such demonstrations, LfD enables the robot's navigation behaviors to align with the intentions and preferences of the human operator. Alignment through LfD is predicated on the assumption that human demonstrations themselves contain such \u201caligned\u201d behavior that reflects the intentions and preferences of the demonstrator. Given such a demonstration, one can infer the objective function of the intended task through inverse reinforcement learning (IRL) (Ng and Russell, 2000; Ziebart et al., 2008), or recover the underlying navigation policy through behavior cloning (BC) (Bain and Sammut, 1999; Ross et al., 2010; Torabi et al., 2018a; Xiao et al., 2020a) or reinforcement learning (IRL+RL) (Finn et al., 2016; Ho and Ermon, 2016). Learning from Demonstrations (LfD) employs expert human task demonstrations to facilitate imitation learning of robot behaviors. However, alternative forms of human feedback also exist and have been instrumental in teaching robots to behave in ways that resonate with human intentions and preferences. For instance, Wang et al. (2021c) explored learning from evaluative feedback, a less demanding form of human input compared to task demonstrations. Other feedback types have also been explored in the literature, such as interventions (Wang et al., 2021a) and disengagements (Kahn et al., 2021b). In this dissertation, I explore Learning from Preferences (LfP), another paradigm utilizing preference rankings as the feedback mechanism, to achieve operator terrain preference-aligned navigation. In LfP, an operator reviews and ranks instances from the robot's experience based on their preferences. Utilizing these preference rankings, LfP either learns an objective function followed by a policy (Brown et al., 2019; Bobu et al., 2020) or directly deduces a policy (Rafailov et al., 2023) that aligns with the operator's specified preferences. Building on these insights, the primary aim of this dissertation is to address the value alignment problem in robot navigation by leveraging human feedback through task demonstrations and preference queries to improve navigation in unstructured static and dynamic environments. Specifically, in static environments, this dissertation introduces algorithms that enable autonomous navigation in unmapped indoor and outdoor environments, while adhering to human intentions and preferences. For dynamic environments, a dataset and algorithms are introduced to enable a mobile robot to navigate in a socially compliant manner, while recognizing and reacting to the objectives of humans in the scene, also known as socially compliant navigation. Concisely, this dissertation will investigate the following question:\nHow can machine learning methods be applied to the task of autonomous navigation\nin unstructured environments such that the learned navigation behaviors of mobile\nrobots align with the intentions and preferences of humans?"}, {"title": "Contributions", "content": "This thesis answers the stated question through contributions on three key topics:\n1. Visual Imitation Learning for Robot Navigation\nTowards learning navigation policies on a mobile robot that align in behavior as demonstrated by a physically different agent such as a human operator, this dissertation introduces an imitation learning algorithm called Visual Observation-only Imitation Learning for Autonomous navigation (VOILA) (Karnan et al., 2021). VOILA introduces a novel reward function that overcomes a significant limitation of existing visual imitation learning algorithms-learning using video-only demonstrations from a physically different agent in the presence of egocentric viewpoint mismatch. VOILA was found to be successful at learning visual navigation policies end-to-end (mapping from raw sensor observations directly to low-level action commands) from human demonstrations in both simulated and physical robot experiments, that align in behavior with the human demonstrations. Chapter 3 discusses this contribution in detail.\n2. Preference-Aligned Off-Road Navigation\nAligning a robot's navigation behavior with an operator's terrain preferences is a major challenge in visual off-road navigation. Prior approaches require extensive human labeling efforts, which are expensive and may not be scalable. To address this problem, this dissertation introduces two novel algorithms. First, this thesis introduces Self-supervised TErrain Representation LearnING from unconstrained robot experience (STERLING) (Karnan et al., 2023a), a self-supervised terrain representation learning algorithm that learns relevant terrain representations from easy-to-collect, unconstrained, and multimodal robot experience collected with any navigation policy. Following a two-step procedure, STERLING learns relevant terrain representations in a self-supervised manner followed by operator preference querying, used to identify an operator's preferences over traversed terrain examples. Given such preferences, a preference utility function can be learned, which can be used in real-time for preference-aligned off-road navigation on the robot. Through physical robot experiments in diverse outdoor environments, we find that STERLING leads to navigation behaviors that are in alignment with an operator's terrain preferences. Second, this dissertation introduces an extension of STERLING to extrapolate operator preferences for visually novel terrains. More often than not, a mobile robot might encounter novel terrains outside the training distribution for which the operator's terrain preference is unknown, causing the robot to deal with uncertain situations. In certain cases, however, although a terrain looks visually novel, the proprioceptive feedback of the robot's interaction with the terrain might feel similar to a previously traversed terrain. Leveraging this intuition, this dissertation introduces Preference extrApolation for Terrain awarE Robot Navigation (PATERN) (Karnan et al., 2023c) which extends the capabilities of STERLING by extrapolating operator preferences from known to visually novel terrains. PATERN enhances the robot's adaptability to unseen terrains, further aligning its navigation behaviors with human preferences. The methodology and effectiveness of PATERN are comprehensively discussed in Chapter 4, alongside STERLING.\n3. Socially Compliant Robot Navigation\nWhile the above contributions advance the state of the art in aligning a robot's navigation behavior with human intentions and preferences, the environments considered above are mostly static. However, an autonomous mobile robot deployed in urban environments might also need to deal with dynamic objects such as pedestrians, for which it is essential to act in a socially compliant manner to ensure politeness and safety. Towards addressing the social robot navigation problem through learning from demonstrations, this thesis introduces Socially Compliant Navigation Dataset (SCAND) (Karnan et al., 2022a), a large-scale dataset of demonstrations for socially compliant robot navigation. We show that learning a policy end-to-end through behavior cloning on SCAND leads to behaviors that are perceived by human participants as more socially compliant than a classical navigation stack (move_base). Additionally, this thesis introduces a novel hybrid algorithm (Raj et al., 2023) that utilizes both the classical navigation stack and an end-to-end learned policy, switching between the two using a learned policy classifier. Both the dataset and the novel hybrid algorithm are detailed in Chapter 5."}, {"title": "Reading Guide to the Thesis", "content": "The rest of the thesis is organized as follows. Note that the contributions in Chapters 3, 4, and 5 in this thesis are not necessarily written to be read in that order and readers can feel free to read specific chapters of interest.\n\u2022 Chapter 2 - Related Work. This chapter reviews prior work that is closely related to the dissertation question and the three contributions of this thesis.\n\u2022 Chapter 3 - Visual Imitation Learning for Robot Navigation. In this chapter, I introduce the VOILA algorithm for visual imitation learning from physically different expert's video-only demonstrations for robot navigation. This chapter addresses Contribution 1 of this thesis.\n\u2022 Chapter 4 - Preference Learning for Off-Road Navigation. In this chapter, I introduce the STERLING and PATERN algorithms for aligning off-road mobile robot navigation behaviors with operator preferences via self-supervised terrain representation learning and extrapolating operator preferences to novel terrains, respectively. This chapter addresses Contribution 2 of this thesis.\n\u2022 Chapter 5 - Socially Compliant Robot Navigation. In this chapter, I introduce the Socially Compliant Navigation Dataset (SCAND), a large-scale dataset of demonstrations for socially compliant robot navigation. I additionally introduce the hybrid algorithm for socially compliant robot navigation, leveraging classical and learned motion planners. This chapter addresses Contribution 3 of this thesis.\n\u2022 Chapter 6 - Conclusion and Future Work. In this chapter, I conclude by providing a recap of the three contributions of this thesis in addressing the central thesis question. I additionally describe some ongoing work that is derived from contributions of this thesis and present ideas for future work."}, {"title": "Background", "content": "In this chapter, I provide an overview of the main areas of prior work related to this dissertation's topic of aligning robot navigation behaviors with human intentions and preferences, while also highlighting the gaps in existing literature that this dissertation addresses. The related work sections in subsequent chapters will then delve into specific existing algorithms that are more closely related to each respective contribution. I begin this chapter in Section 2.1 with an overview of existing literature on imitation learning for robot navigation. Section 2.2 explores relevant studies in learning-based methods for off-road navigation, followed by Section 2.3 surveying related work on machine learning for social robot navigation. Section 2.4 extends the discussion to prior work on value alignment verification, particularly in the context of this dissertation's contributions, and describes how it is brought to bear in each of the contributions. Note that, in this chapter, the scope is restricted to learning-based approaches to robot navigation. For a more in-depth survey of classical and learning-based approaches to robot navigation and to understand their limitations and trade-offs, readers are directed to the works of Xiao et al. (2020b), Tai and Liu (2016) and Tai et al. (2016)."}, {"title": "Machine Learning for Robot Navigation", "content": "In this section, I explore prior approaches that utilize machine learning to enable autonomous navigation in mobile robots, specifically, imitation learning for robot navigation. I begin by exploring end-to-end approaches in machine learning for robot navigation, followed by reinforcement learning for robot navigation. Finally, I explore hybrid methods that combine machine learning with classical methods. The use of machine-learning-based approaches to learn end-to-end autonomous navigation policies goes back several decades (Pomerleau, 1989; Lecun et al., 2005), though recent years have seen a spike in interest from the research community (Bi et al., 2018; Codevilla et al., 2017; Pan et al., 2018; Xiao et al., 2020b; Chang et al., 2020). One of the earliest successes was reported by Pomerleau et al. (Pomerleau, 1989), in which a system called ALVINN used imitation learning to train a policy represented by an artificial neural network end-to-end (mapping from raw sensor observations directly to low-level control signals) to perform lane keeping based on demonstration data generated in simulation. Since then, several improvements, both in the amount and type of demonstration data and in network architecture and training, have been proposed in the literature. In particular, Lecun et al. (2005) proposed the use of a convolutional neural network (CNN) to better process real demonstration images for an off-road driving task. Bojarski et al. (2016) reported that gathering a large amount of real-world human driving demonstration data and applying data augmentation made it possible to train even more complex CNN architectures to perform lane keeping. Recently, Shah et al. (2023) and Sridhar et al. (2023) show that utilizing diverse open-source datasets from many robots to train a goal-conditioned navigation policy end-to-end serves as a good foundational control policy for goal-oriented navigation. Loquercio et al. (2018) proposed the DroNet architecture for learning to fly a UAV based on driving datasets collected on the road. Using supervised learning, DroNet was able to control a UAV autonomously in unstructured urban environments. While the aforementioned approaches each use end-to-end imitation learning through supervised learning to find autonomous navigation policies, other machine learning for autonomous navigation work has adopted the alternative training paradigm of reinforcement learning. Chang et al. (2020) propose using off-policy Q-learning from video demonstration data to learn goal-conditioned hierarchical policies for semantic navigation. Liu et al. (2017) propose a context translation network to imitate an expert demonstration in the presence of viewpoint mismatch. While Liu et al. (2017) effectively address viewpoint mismatches, their method primarily handles third-person views and does not extend to the egocentric viewpoint mismatches central to the contribution in Chapter 3 of this dissertation. Similarly, Gaskett et al. (2000) propose a visual servoing algorithm that uses image template-matching to provide rewards for their reactive agent. The work by Pan et al. (2018) proposes using demonstrations from a privileged expert agent equipped with expensive sensors, to imitate the behavior on physically different hardware with cheaper sensors. The work closest to ours is that of Kendall et al. (2019), in which the proposed system learns a navigation policy using RL, where the reward function is the total distance traveled by their autonomous vehicle before a human driver intervenes (to, e.g., prevent collisions). An alternative approach to end-to-end controller learning is using inverse reinforcement learning to recover a cost function for navigation, that can be used in conjunction with a classical planner to navigate. Wulfmeier et al. (2015, 2017) extend the Maximum Entropy Inverse Reinforcement Learning (MEIRL) (Ziebart et al., 2008) approach to deep cost function learning from lidar scans using a non-linear, neural network-based cost functions. In the studies referenced above, while successful navigation behaviors are achieved through the methodologies employed, these outcomes are predicated on the assumption of similar physical forms between the imitator and the expert. This assumption of requiring similar physical forms between the expert and imitator agents becomes problematic particularly when the expert is a human and the imitator is a ground vehicle, leading to significant discrepancies in embodiment. Such disparities often result in viewpoint mismatches when using an egocentric camera as the primary sensor, rendering existing visual imitation learning algorithms ineffective. The discussion in Chapter 3 of this dissertation delves into this issue, introducing VOILA, a visual imitation learning algorithm that remains effective despite the viewpoint mismatches caused due to physical embodiment mismatch between the demonstrator and imitator agents."}, {"title": "Machine Learning for Off-Road Navigation", "content": "The problem of autonomous navigation in unstructured environments is well-studied, and several classical approaches have been proposed to address it (Pomerleau, 1989; Hart et al., 1968b; ROS-Planning, 2022; Biswas and Veloso, 2013; Khandelwal et al., 2017). While successful in long-term deployment scenarios, such classical heuristic-based approaches treat off-road navigation as geometric obstacle-avoidance (i.e., obstacles that are untraversable) problem. However, a mobile robot navigating in unstructured off-road environments may often experience a variety of terrains to traverse over with specific terrain traversability costs for navigation based on factors such as human preferences, ride comfort, or social norms. Classical approaches that solely perform geometric navigation will not be able to navigate in a terrain-aware manner, or worse, in some environments such as tall grass, a classical approach may choose not to navigate over it to avoid collisions, whereas, tall grass could be navigable for a mobile robot, and could be a human-preferred terrain to drive over. This lack of terrain-awareness of classical approaches to robot navigation has created a need for more advanced approaches to terrain-aware off-road navigation. To overcome existing limitations of classical heuristic-based approaches to off-road navigation, recently, several learning-based approaches have been proposed that are promising for terrain-aware navigation in unstructured off-road environments. Kahn et al. (2021b) propose LAND for learning to navigate from disengagements. LAND takes a reinforcement learning-based approach where disengagements due to sub-optimal behavior of the agent are recorded to train the agent using reinforcement learning. Kahn et al. (2021a) propose BADGR, in which available sensory redundancy on a mobile robot is leveraged to learn behaviors on different types of terrains. BADGR learns a planner from off-policy trajectories collected onboard during exploration. This learned planner can safely ignore geometric obstacles (i.e., obstacles that are untraversable) and navigate through safe-to-travel regions avoiding regions that are lethal and bumpy. Yao et al. (2022) propose the RCA algorithm for terrain-aware navigation to maximize ride comfort. In RCA, an unsupervised representation learning algorithm is used to learn visual-inertial features from data collected on the robot. The work most relevant to preference learning for navigation is VRL-PAP by Sikand et al. (2022) in which a representation of visual patches of the terrain is learned and then the preference function is learned from the partial order rankings of terrains. While VRL-PAP was shown to be successful at learning preference costs of terrains that align with the human operator's interests, it requires structured concave demonstration trajectories such that the terrain of higher preference is the one being traversed and the lesser preferred terrain is present at the midpoint of the shortest path connecting the start and end location of the robot. This limitation of VRL-PAP prohibits its applicability in situations where we have a large amount of unconstrained trajectory data, such as driving straight where the shortest path is indifferent structurally from the path traversed. Each of the approaches above is limited in that it cannot learn from unconstrained robot trajectories for terrain-aware off-road navigation, aligning with human preferences. While VRL-PAP (Sikand et al., 2022), the closest approach to this work can learn from geometrically constrained trajectory data, it requires data gathered on specific pairs of terrains encountered which may not be available in the environment. The STERLING (Karnan et al., 2023a) algorithm introduced in Chapter 4 uses self-supervised non-contrastive representation learning from unconstrained robot experiences, enabling it to learn relevant terrain representations from robot trajectories of any geometric shapes and sizes, which can be utilized for operator preference-aligned off-road navigation. Additionally, Chapter 4 introduces PATERN (Karnan et al., 2023d) that enables extrapolating operator preference to visually novel terrains."}, {"title": "Machine Learning for Social Navigation", "content": "Recently, several algorithms have emerged that show the potential of applying learning to address challenges in robot navigation (Xiao et al., 2020b). Broadly speaking, in the robot navigation literature, learning-based approaches are successful in problems such as adaptive planner parameter learning (Xiao et al., 2020a), overcoming viewpoint invariance in demonstrations (Karnan et al., 2021), and end-to-end learning for autonomous driving (Bojarski et al.; Pfeiffer et al., 2018; Wang et al., 2021b; Wurman et al., 2022). Specifically in applying imitation learning for social navigation, the work by Tai et al. (2018) is the most relevant to this dissertation. They provide a simulation framework in Gazebo along with a dataset generated using the same where virtual human agents navigate following the social force model (Helbing and Moln\u00e1r, 1995). They additionally train a social navigation policy using the Generative Adversarial Imitation Learning algorithm assuming the social force model as the expert demonstrator and show a successful deployment of the learned policy in the real-world on a turtlebot robot. While their work has shown that imitation learning can be applied to address the social navigation problem, they do so assuming the social force model in simulation as the expert, socially-compliant policy. Although simulated environments enable fast and safe data collection for online learning, they lack the naturally occurring social interactions seen in the wild. Other learning paradigms such as RL have also been applied to address the social navigation problem. Everett et al. (2021) present CA-DRL, a multi-agent collision avoidance algorithm learned using RL that shows impressive results in the real-world on specific social navigation scenarios. Kretzschmar et al. (2016) use inverse reinforcement learning on human demonstrations to learn cost functions for the socially compliant navigation task. Baghi and Dudek (2021) propose the SESNO algorithm for sample efficient inverse reinforcement learning for social navigation using the UCY dataset (Lerner et al., 2007)."}, {"title": "Value Alignment Verification", "content": "The preceding sections of this chapter have reviewed various learning-based algorithms documented in the literature that help learn robot navigation behaviors, that align with human intentions and preferences. This section extends the discourse to survey related work on the topic of value alignment verification\u2014specifically, verifying whether the learned navigation policy aligns with the human operator's intentions and preferences. Note that the core contributions of this dissertation are centered around devising algorithms and datasets that enable mobile robots to navigate in accordance with a human operator's intentions and preferences, and not to propose new ways to verify value alignment. This is still an active area of research in the field of machine learning and falls beyond the scope of this dissertation. Brown et al. (2021) formally introduce the value alignment verification problem and propose designing a driver's test to efficiently verify whether an AI system is aligned with a human's values through a minimal number of queries. Exact value alignment involves sampling the policy at all possible states which can be expensive and are often impossible to evaluate since the number of states can be prohibitively large for some tasks, such as autonomous driving. To tackle this, Brown et al. (2021) propose heuristic and approximate value alignment verification for grid-world and continuous action domains. Hadfield-Menell et al. (2016) propose CIRL which introduces a cooperative game between the agent and the human, where both agents are rewarded according to the human's reward function, but unlike the human, the robot does not initially know the reward function. While CIRL ensures the robot's learned policy asymptotically converges to the human intended values, it does not propose a way to verify this alignment. Evaluating social compliance in robot navigation is still an unsolved problem in the navigation community. For further details, Francis et al. (2023) recently published a comprehensive survey on principles and guidelines for evaluating social robot navigation. There have been several proposed approaches both using simulation (Tsoi et al., 2020, 2021; K\u00e4stner et al., 2023) and physical robot experiments (Svensson, 2003; Pirk et al., 2022). The inaccuracy of long-term human motion models, the wide variety of social navigation scenarios, and the subjectivity of \"socialness\" in navigation make evaluating social compliance a challenging task in robot navigation. Knox et al. (2022) highlight major flaws in existing human-designed reward functions for autonomous driving, and propose eight sanity checks for a human-designed reward function, underscoring the importance of addressing the value alignment problem in robot navigation by alleviating reward misdesign. In this dissertation, we take different approaches in each contribution to verify the alignment of the learned navigation behavior to a human operator's intentions and preferences. To verify the alignment of the policy learned using VOILA, we report the Hausdorff distance of the imitated trajectory to the ground truth trajectory demonstrated by the human. We verify that the learned policy imitates the intended navigation behavior by checking that the trajectory traced by the learned policy closely matches the human-demonstrated trajectory in the physical robot experiments. Additionally, in simulation experiments, in novel environments unseen by the agent, we report the Hausdorff distance of the robot's states between the human-demonstrated trajectory and the trajectory traced by the learned policy and find that the imitated policy closely follows the unseen human demonstrated trajectory, verifying that VOILA has indeed learned the intended navigation behavior. In Chapter 4, to validate the preference alignment of the trajectories traced by STERLING and PATERN algorithms, we utilize a success metric in which a trajectory traced by the robot using any algorithm from start to goal locations are considered successful if it adheres to the operator's ranked preferences over the terrains. In Chapter 5, to verify alignment with human preferences for socially compliant navigation, we perform a human participant study with post-experience questionnaires requesting feedback on a Likert-scale, evaluating social compliance, and safety of the learned controller."}, {"title": "Visual Imitation Learning for Robot Navigation", "content": "In this chapter, I introduce the first contribution of this thesis, Visual-Observation-only Imitation Learning for Autonomous navigation (VOILA) (Karnan et al., 2021), a visual imitation learning algorithm for autonomous navigation in the presence of egocentric viewpoint mismatch between a physically different demonstrator such as a human, and a mobile robot as the imitator. This chapter is organized as follows. The introduction, Section 3.1, provides an overview of this contribution, followed by background and related work in Section 3.2. Section 3.3 introduces VOILA and the novel reward function for visual imitation learning. Sections 3.4 and 3.5 provide details on the implementation and the experiments performed for evaluation, respectively."}, {"title": "Introduction", "content": "Enabling vision-based autonomous robot navigation has recently been a topic of great interest in the robotics and machine learning community (Bojarski et al.; Codevilla et al., 2017; Pan et al., 2018). As discussed in Section 2.1, imitation learning in particular has emerged as a useful paradigm for designing vision-based navigation controllers. Using this paradigm, the desired navigation behavior is first demonstrated by another agent (usually a human), and then a recording of that behavior is supplied as training data to a machine learner that tries to find a control policy that can mimic the demonstration. To date, most approaches in the navigation domain that use imitation learning require demonstration recordings that contain both state observations (e.g., images) and actions (e.g., steering wheel angle or acceleration) gathered onboard the deployment platform (Bojarski et al.; Codevilla et al., 2017; Lecun et al., 2005; Tampuu et al.). As explored in Section 2.1, while these existing imitation learning approaches have proved successful in certain scenarios, there are situations in which it would be beneficial to relax the requirements they impose on the demonstration data. For example, if we wish to collect a large number of demonstrations from many experts, it may prove too difficult or costly to arrange for each expert to operate specific deployment platforms, which are often expensive or difficult to transport. Additionally, it might be costly to outfit all demonstration platforms with instrumentation to record the control signals with the demonstration data. However, due to the low cost and portability of video cameras, it may still be feasible to have demonstrators record ego-centric video demonstrations of their navigation behaviors while operating a different platform. Demonstrations of this nature would consist of video observations only (i.e., they would not contain control signals), and, because of the difference in platform, the videos would likely exhibit ego-centric viewpoint mismatch compared to those that would be captured by the deployment platform. One example of such data is the plethora of vehicle dashcam videos available in publicly accessible databases (Chan, 2017) or on YouTube. Another example is video demonstrations of robot behaviors generated by proprietary code that one would like to mimic on the same or different robot hardware. Unfortunately, to the best of our knowledge, there exist no current imitation learning techniques for vision-based navigation that can leverage such demonstration data. Fortunately, recent work in Imitation from Observation (IfO) (Torabi et al., 2019) imitation learning in the absence of demonstrator actions\u2014has shown a great deal of success for several related tasks. For example, work in this area has been able to learn from video-only demonstrations for both simulated and real limbed robots (Pavse et al., 2019; Torabi et al., 2018c; Sermanet et al.; Pan et al., 2019). However, no literature of which we are aware has considered whether these IfO techniques can be applied to the vision-based autonomous navigation problem outlined above. This problem is especially challenging since physical differences in the demonstration platform introduce viewpoint mismatch in the video demonstrations."}, {"title": "Background and Related Work", "content": "In this Chapter, I present our hypothesis that it is possible to learn visual robot navigation policies that are aligned with a demonstrator's intentions, even if such demonstrations contain significant viewpoint differences due to embodiment mismatch between the demonstrator and the robot. To this end, we introduce a new IfO technique for vision-based autonomous navigation called Visual-Observation-only Imitation Learning for Autonomous navigation (VOILA).\u00b9 To overcome viewpoint mismatch, VOILA uses a novel reward function that relies on off-the-shelf keypoint detection algorithms that are themselves designed to be robust to egocentric viewpoint mismatch. This novel reward function is utilized to drive a reinforcement learning procedure that results in navigation policies that imitate the demonstrator. We experimentally confirm our hypothesis both in simulation and on a physical Clearpath Jackal robot.2 We compare VOILA against a state-of-the-art IfO algorithm GAIfO (Torabi et al., 2018c), and show that VOILA can learn to imitate an expert's visual demonstration in the presence of viewpoint mismatch while also generalizing to environments not seen during training. Additionally, we demonstrate the flexibility of VOILA by showing that it can also support vision-based training of navigation policies with observation inputs other than camera images. The proposed approach, VOILA, performs reinforcement learning (RL) using a novel reward function based on image keypoints in order to accomplish imitation from observation for autonomous navigation with viewpoint mismatch. In this section, I review related work that is more closely related to VOILA, such as autonomous robot navigation, imitation from observation, and computer vision techniques for visual feature extraction. For a more general literature review on machine learning for robot navigation, refer to Section 2.1."}, {"title": "Machine Learning for"}]}