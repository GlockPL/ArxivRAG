{"title": "MOIN: MIXTURE OF INTROVERT EXPERTS TO UPCYCLE AN LLM", "authors": ["Ajinkya Tejankar", "KL Navaneet", "Ujjawal Panchal", "Kossar Pourahmadi", "Hamed Pirsiavash"], "abstract": "The goal of this paper is to improve (upcycle) an existing large language model without the prohibitive requirements of continued pre-training of the full-model. The idea is to split the pre-training data into semantically relevant groups and train an expert on each subset. An expert takes the form of a lightweight adapter added on the top of a frozen base model. During inference, an incoming query is first routed to the most relevant expert which is then loaded onto the base model for the forward pass. Unlike typical Mixture of Experts (MoE) models, the experts in our method do not work with other experts for a single query. Hence, we dub them \"introvert\" experts. Freezing the base model and keeping the experts as lightweight adapters allows extreme parallelism during training and inference. Training of all experts can be done in parallel without any communication channels between them. Similarly, the inference can also be heavily parallelized by distributing experts on different GPUs and routing each request to the GPU containing its relevant expert. We implement a proof-of-concept version of this method and show the validity of our approach.", "sections": [{"title": "INTRODUCTION", "content": "As language models continue to grow in size, it is important to consider whether all parameters are necessary for every request. For instance, the same parameters might not be ideal for answering questions about SQL compared to those about car seats. Approaches like fine-tuning, early exit, and mixture of experts (MoE) have been proposed to address this issue, each with its own trade-offs. Fine-tuning requires meticulous data curation and optimization, while early exit and MoE add complexity to the model's forward pass, posing engineering challenges. Most relevant to our work is the Mixture of Experts (MoE) approach, popularized by the Mixtral 8x7B model (Jiang et al., 2024). Existing experts operate at the token level and not sequence level, thus lacking specialization in specific domains \u00b9 (Jiang et al., 2024). Additionally, since routing varies by token, all experts must reside in GPU memory during both training and inference, making it difficult to scale to a large number of experts. To address these limitations, we focus on the concept of query-level or sequence-level experts, where each query is routed to a single expert. This design minimizes communication between experts, which can be distributed across different GPU nodes. We term these \"introvert\" experts due to their reduced interaction with one another.\nImplementing sequence-level experts offers several practical advantages. It creates a modular architecture where a single generalist base model is complemented by multiple small experts that specialize in niche topics. While fine-tuning shares a similar motivation, the number of experts is often constrained by the need for curated datasets. In contrast, because the experts operate on independent domains, they can be trained in parallel, enhancing flexibility in resource utilization. Moreover, these experts can be retrained as data evolves, allowing for the easy integration of new experts as fresh data becomes available. This capability positions LLMs as continual learners with minimal risk of forgetting prior knowledge.\nWe present an intuitive and straightforward proof-of-concept for creating such experts. Given the high cost of pre-training, we focus on model upcycling, transforming an existing dense model into"}, {"title": "RELATED WORKS", "content": "Mixture of Experts. Instead of activating all parameters for all inputs, conditional computing aims to activate only a subset of the parameters for each input (Jacobs et al., 1991; Bengio et al., 2013; Bengio, 2013). The goal is to increase the capacity of a model without increasing the compute of the model (Cho & Bengio, 2014). However, the idea didn't become widely known until the introduction of Mixture-of-Experts layer (Shazeer et al., 2017). This layer was used in training the Mixtral 8x7B model (Jiang et al., 2024) that further popularized this approach. Other variations of the MoE layer include (Lample et al., 2019; He, 2024; Rajbhandari et al., 2022; Dai et al., 2024). However, since the MoE creates experts in the MLP block of the transformer architecture, experts are created token-wise. When the model size increases and all experts cannot reside on a single GPU, complicated multi-GPU setups involving scatter and gather operations are needed at each MoE layer. Hence, this paper explores sequence-wise conditional computation / experts. This has the advantage of only needing to load weights relevant to a given query in the GPU while rest of the weights can reside in CPU RAM or even hard disk, significantly simplifying the architecture design and implementation.\nThe goal of this paper is to create topic-wise experts. Conversely, it is possible to identify parts of a dense model that only activate for certain topics. Essentially, discover instead of train experts. This was studied in (Dai et al., 2021).\nLoRAs for efficient distributed training of LLMs. Recent works (Huh et al., 2024b; Lialin et al., 2024; Zhao et al., 2024) have shown the potential for a branch-train-merge distributed algorithm"}, {"title": "METHOD", "content": "Our key idea is to improve a (partially) pre-trained LLM by training topic-wise expert models atop it and dynamically routing the queries to the appropriate expert during inference. See Figure 1 for an illustration of the steps involved in our method."}, {"title": "TOPIC MODELING", "content": "In order to train the topic-wise experts, we need a dataset with topic annotations. For broad coverage of topics, we focus on using large-scale datasets typically used in LLM pre-training. Since these are usually web-scale text datasets without any topic annotations, we need to perform topic modeling before training the expert models.\nHere, we consider two approaches for topic modeling. In both approaches, all documents in the training dataset are converted to embeddings using a pre-trained sentence embedding model. In the first method, we employ UMAP (McInnes et al., 2018) to reduce the dimensionality of the document embedding and then perform clustering in the lower dimension space using HDBSCAN (McInnes et al., 2017). Topic-wise embeddings are then obtained by analysing the document belonging to each cluster. The topic embeddings are needed to route the queries to corresponding topics during inference. We employ this topic modeling approach in our model termed MoIN-500 where we split the dataset into 500 topics. We find that it is difficult to scale this approach to thousands of topics with limited resources. Thus, we consider a simpler alternative. In the second approach, we perform a simple K-means clustering directly in the document embedding space to generate cluster index for each training document and 'k' cluster centers as topic embeddings."}, {"title": "TRAINING OF TOPIC-WISE EXPERT MODELS", "content": "Given a set of topics, we train an expert model for each topic atop our base pre-trained model. Since the number of models scales linearly with the number of topics, we need the number of trainable parameters in the expert models to be relatively small compared to the base model. Recently, parameter-efficient approaches (Hu et al., 2022; Koohpayegani et al.; Liu et al., 2024) have been very popular for model fine-tuning. Here, we consider them for the upcycling task. Specifically, we use LORA (Hu et al., 2022) architecture for the experts. In LoRA, the base model is frozen and a few additional parameters are introduced for each linear layer in the network. The output of a linear layer is modified to be $y = Wx + W_\\\u266d(W_ax)$ where $W \\in R^{k \\times d}$ is the original weight matrix, $x$ is the input vector, $d$ is the dimensionality of input, $k$ is the dimensionality of output, and $W \\in R^{k \\times r}$ and $W_a \\in R^{r \\times d}$ are the trainable parameters. $W_\u266d$ and $W_a$ are designed to be low rank matrices, i.e., $r < min(d, k)$, thus reducing the number of newly added parameters. For every topic in the training dataset, we train an expert LoRA adapter using the documents assigned to the topic. Unlike typical LORA fine-tuning on task-specific loss, the LoRA models here are trained using the standard autoregressive language modeling loss (Radford, 2018) on subsets of the pre-training data. For each expert training run, only the weights of the expert are trained while the base model weights remain frozen. The key advantage over a standard pre-training is that all the experts can be trained independently, allowing for a great flexibility in the resources used for training."}, {"title": "MODEL INFERENCE", "content": "Once all experts have been trained, our setup consists of a large number of experts that must be served efficiently during inference. For each inference request, we need to find a topic-wise expert that can best serve it, and use the corresponding adapter during the forward pass through the model. We perform this 'query routing' in the following two steps: first, we embed the query using the same embedding model used to cluster the training dataset. Second, we perform nearest neighbor search over the topic embeddings using the query embedding. Since this routing step is an overhead specific to our method, the router model should be small enough to keep the inference latency feasible. To achieve this, we use a small document embedding model with just 20M parameters. Given the small size, it may be possible to host it without GPUs or to perform routing entirely on the client side. However, while the small size of the model helps reduce router latency it can also reduce the quality of the topic clustering. Router latency vs. clustering accuracy is an inherent trade-off in this method. In our current implementation, we choose faster inference over a more complex but better topic modeling."}, {"title": "EXPERIMENTS", "content": "Implementation details: For topic modeling, we use a small language model all-MiniLM-L6-v2 with 20M parameters as the embedding model. We use two variants"}, {"title": "RESULTS", "content": "pre-training:  Table 1 reports the perplexity of the baselines and MoIN-5k on the SlimPajama validation set. For evaluating MoIN-5k model, we first determine the topic for each document using our K-means based topic model and use the corresponding LORA for perplexity calculation. MoIN-5k not only outperforms the equivalent TinyLlama-2.5T but also achieves comparable performance to TinyLlama-3T which is trained with 500B more tokens. Figure 2 depicts the perplexity for each LORA model. Most of the models perform well with a perplexity lower than 10. For the underper-\nDownstream tasks: Similar to perplexity evaluation, we first determine the topic for each document using our K-means based topic model and use the corresponding LoRA for calculating metrics for MoIN-5k. In MoIN-5k, if the LoRA model is absent for a given topic, just the base model is used. However, in MoIN-4697, the query is always routed to one of the trained 4697 LoRA models and the baseline alone is not used for any of the queries. The performance of MoIN-5k is comparable to that of TinyLlama-2.5T on nearly all the downstream tasks with MoIN-5k being marginally better on average. Surprisingly, TinyLlama-2.5T is better than TinyLlama-3T on five of the seven tasks.\nFor any given dataset, not all the LoRAs necessarily participate in the evaluation process. This is particularly true for datasets with very few total queries or for those focused on a narrow set of topics.  Table 3 shows the number of unique LoRAs used in the evaluation for each downstream task. We observe that usually only about 10% of the LoRAs are used for a single task.\nTopic modeling: We explore two methods for topic modeling - one with 500 topics and the other with 5000 topics. For the smaller variant, we use HDBSCAN to cluster the topics and generate topic embedding by utilizing the documents belonging to each cluster. For the larger variant, we use a simple K-means clustering of all documents and use the cluster centroid as the topic embedding. For both methods, a nearest neighbour search is performed using the query and topic embeddings to"}, {"title": "FUTURE WORKS", "content": "The idea of using adapters as experts during pre-training can unlock many new features. While we could only explore this idea in the context of upcycling a model, following are few exciting future directions worth exploring.\nDynamic parameter count allocation per topic. Number of training tokens per topic can differ and it may be beneficial to have larger adapters for bigger topics. Hence, LoRA adapters with adaptive rank can be explored to both reduce the total model size and prevent under/overfitting.\nAdapter augmented generation. Our method shares similarities with retrieval augmented generation (RAG). While RAG retrieves most relevant raw text for a given query, our method retrieves the most relevant adapter. Adapters can be used to memorize and compress the information from several relevant documents. This can make the system more tolerant to errors in retrieval as the adapter stores much more information than raw text."}, {"title": "CONCLUSION", "content": "In this paper, we introduced the mixture-of-introvert experts framework to upcycle a pre-trained LLM. The training data is split into semantically related clusters and an expert is trained on each cluster. Queries at test-time are routed to appropriate expert with a simple nearest neighbor search. All the experts can be trained independently. Our approach offers great flexibility in training and provides a way to improve an LLM with limited resources. We provide proof-of-concept experimental results by upcycling a 1B parameter model with 5000 experts on 500B tokens and show comparable or better performance than full-model continued pre-training."}, {"title": "LIMITATIONS AND BROADER IMPACT", "content": "Our approach utilizes large LoRA models, which results in a significant number of parameters compared to standard pre-training methods. Hence, our method is particularly well-suited for applications where multiple instances of the LLM are deployed across several GPUs to serve numerous users, such as in ChatGPT. Moreover, we believe that the size of LoRA models could be substantially reduced; however, we did not explore different ranks for LoRA due to resource constraints. While our method effectively lowers the cost of pre-training LLMs, potentially democratizing the development of novel models, it also raises concerns. Specifically, it may enable less sophisticated adversaries to create their own models, which could lead to negative societal impacts."}]}