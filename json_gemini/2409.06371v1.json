{"title": "Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition", "authors": ["Junzheng Zhang", "Weijia Guo", "Bochao Liu", "Ruixin Shi", "Yong Li", "Shiming Ge"], "abstract": "Very low-resolution face recognition is challenging due to the serious loss of informative facial details in resolution degradation. Recent approaches based on knowledge distillation provide an effective solution by distilling knowledge from a well-trained teacher for high-resolution face recognition and transferring it to a student for low-resolution face recognition. In general, the existing approaches usually take a discriminative model as teacher, where the teacher knowledge is trained in an abstract manner and provides poor transfer efficiency to compensate for the missing knowledge in low-resolution faces. To make more complete knowledge transfer, we propose a generative-discriminative representation distillation approach that combines generative representation with cross-resolution aligned knowledge distillation. This approach facilitates very low-resolution face recognition by jointly distilling generative and discriminative models via two distillation modules. Firstly, the generative representation distillation takes the encoder of a diffusion model pretrained for face super-resolution as the generative teacher to supervise the learning of the student backbone via feature regression, and then freezes the student backbone. After that, the discriminative representation distillation further considers a pretrained face recognizer as the discriminative teacher to supervise the learning of the student head via cross-resolution relational contrastive distillation. In this way, the general backbone representation can be transformed into discriminative head representation, leading to a robust and discriminative student model for very low-resolution face recognition. Our approach improves the recovery of the missing details in very low-resolution faces and achieves better knowledge transfer. Extensive experiments on face datasets demonstrate that our approach enhances the recognition accuracy of very low-resolution faces, showcasing its effectiveness and adaptability.", "sections": [{"title": "I. INTRODUCTION", "content": "Low-resolution face recognition is critical in many practical applications like remote video surveillance in the wild [1], [2]. During the resolution degradation of normal high-resolution faces into a very low resolution (e.g., 16\u00d716), a lot of informative details are often missing, which challenges the traditional state-of-the-art face recognizers [3]\u2013[6] whose recognition accuracy is usually greatly reduced [7]. Thus, the key challenge in low-resolution face recognition is to transfer or generate useful knowledge to compensate for the lost information. To address that, the existing approaches can be grouped into two main categories according to the transfer or generation idea.\nThe transfer-based approaches aim to transfer the knowl-edge from high-resolution images to low-resolution recognition models for learning. Early approach [8] proposed to directly learn the feature embedding from low-resolution faces. Re-cently, approaches based on knowledge distillation [7], [9]\u2013[14] have achieved success in the field of low-resolution face recognition by transferring knowledge from teacher to student. However, their transfer efficiency is still poor since their teacher knowledge is extracted in an abstract manner, while low-resolution face recognition needs more complete knowledge for transfer. By contrast, the generation-based approaches aim to complete the missing pixel-level or latent-level information. Early approach [15] enhanced the facial features of low-resolution faces through the super-resolution process, making them easier to match. Recently, generative models [16]-[18] learned from large-scale datasets have demonstrated efficient image generation and super-resolution capabilities. Some recent works [19], [20] integrate generative models into representation learning, and enhance low-quality visual recognition tasks.\nIn this paper, we propose a generative-discriminative repre-sentation distillation approach to facilitate very low-resolution face recognition in a progressive training manner via two distil-lation modules. Firstly, the generative representation distillation stabilizes the student backbone by distilling the encoder of a face super-resolution diffusion model and performing feature regression on the generative features, thus completing the knowledge of very low-resolution faces at the latent level. Secondly, the discriminative representation distillation finetunes the student head by distilling a pretrained high-resolution face recognizer with cross-resolution relational contrastive distilla-tion. This process enables the generative features to continuously approximate the discriminative features, allowing the student model to learn the teacher model's knowledge more accurately. Our approach helps the student recover the missing latent details, achieves precise knowledge transfer, and enhances recognition accuracy. The main contributions include: 1) we propose a generative-discriminative representation distillation approach to promote very low-resolution face recognition; 2) we propose a progressive and module-wise approach for efficient student training; 3) we conduct extensive experiments on four popular benchmarks to validate the effectiveness of our approach."}, {"title": "II. APPROACH", "content": "Our objective is learning a low-resolution student S on the training set $D = \\{x_i, x'_i, y_i\\}$ by distilling 1) general knowl-edge from a generative teacher $T_g (x'_i; w_g)$ pretrained for super-resolving low-resolution face $x'_i$; and 2) specific knowledge from a discriminative teacher $T_d(x_i; w_d)$ pretrained for recognizing high-resolution face $x_i$. Here, $y_i \\in \\{1, 2, ..., c\\}$ represents face label in total c classes, and $w_g$ and $w_d$ are the weights of $T_g$ and $T_d$ respectively. We separate the student $S = \\{S_b, S_h\\}$ into a backbone $S_b (x'_i; w_b)$ with parameters $w_b$ to extract the intermediate feature $f_i$; and a head $S_h (f_i; w_h)$ with parameters $w_h$ to classify the feature, and then progressively perform two distillation modules, as shown in Fig. 1.\nIt aims to enable the student to mimic the general represen-tation ability of the generative teacher. We use the encoder of a pretrained diffusion model PGDiff [17] as the generative teacher $T_g$. PGDiff simulates key properties of high-quality images, and is effective for super-resolution process, providing robust representational knowledge transfer due to its independence from the degradation process. During training, we perform feature regression by minimizing the generative loss $L_{gen}$:\n\\begin{equation}\nL_{gen} (w_b; D) = \\sum_{t=1} ||S_h (x'_i; w_b) \u2013 T_g (x'_i; w_g)||^2.\n\\end{equation}\nIt aims to transform the general generative representations into discriminative ones for specific low-resolution recognition. Thus, we incorporate a well-trained state-of-the-art face recog-nizer ArcFace [5] as the dicriminative teacher and transfer its knowledge to learn the student head $S_h$. To make the transfer effective, we leverage cross-resolution relational contrastive distillation that has been proved effective [14].\nLet $x$ and $f = S_b(x; w_b)$ respectively represent the inputs for $T_d$ and $S_h$, and their empirical data distribution is denoted as $p(x, f)$. Then, we define the sampling procedure as $p(x, f) \\rightarrow (x_i, x_j, f_i, f_j)$, and set up two learnable feature relation modules $(F_{t}$ and $F_{t,s})$ to represent the relationships of sample pairs as vectors $v_{i,j} = F_t (T_d(x_i; w_d), T_d(x_j; w_d))$ in the teacher space and $v'_{i,j} = F_{t,s} (T_d(x_i; w_d), S_h(f_j; w_b))$ in the cross-resolution space, respectively. We further set the feature transformations $h_1$ and $h_2$, and define the relational contrastive loss $L_{rcd}$:\n\\begin{equation}\nL_{rcd} = \\sum_{i,j} log \\frac{e^{h_1 (v'_{i,j}) \\cdot h_2(v_{i,j}) / \\tau}}{e^{h_1 (v'_{i,j}) \\cdot h_2(v_{i,j}) / \\tau} + n / m}.\n\\end{equation}\nwhere n and m are the number of negative sample pairs and total sample pairs during a training batch, respectively, and we set the same number of positive and negative sample pairs. We set the temperature parameter $\\tau = 0.4$ to adjust the scale in our experiments. $P(b = 1)$ and $P(b = 0)$ denote the set of positive and negative sample pairs, respectively. By introducing the structural relational knowledge, Eq. (2) can effectively maximize the similarity between the outputs of the student and teacher.\nTo improve performance, we incorporate the naive logit distillation loss $L_{kd} = \\sum_{t=1} l_{kl} (S_h (f_i; w_h), T_d(x_i; w_h))$ [21] with KL divergence $l_{kl}$ and the classification loss $L_{cls} = \\sum_{t=1} l_{ce} (S_h (f_i; w_h), y_i)$ with cross-entropy $l_{ce}$, and defined the total discriminative loss $L_{dis}$ as:\n\\begin{equation}\nL_{dis} (w_h; D) = L_{cls} + 0.25L_{kd} + 4.0L_{rcd}.\n\\end{equation}"}, {"title": "C. Module-Wise Training", "content": "Due to the diversity between generative representations and discriminative representations, we train the student in a module-wise manner rather than the end-to-end training. First, the student backbone $S_b$ is trained on massive low-resolution faces $\\{x'_i\\}$ by minimizing the generative loss in Eq. (1). This training is supervised by the pretrained generative model without face identities, thereby enabling the student to learn general and robust face representations. Then, $S_b$ is fixed and the student head $S_h$ is further trained on D by minimizing the dicriminative loss in Eq. (3). In this way, the complete structural knowledge is efficiently transferred from both generative and discriminative teachers, leading to a discriminative student $S = \\{S_b, S_h\\}$.\nThe training in the two distillation modules can be effectively performed with the back-propagation algorithm."}, {"title": "III. EXPERIMENTS", "content": "To validate the effectiveness of our GDRD, we train the student models on WebFace [22], evaluate on four benchmarks (LFW [23], UCCS [24], TinyFace [25] and AR [26]) and compare with the state-of-the-arts. To simulate the very low-resolution conditions and make comparison fair, we perform face detection and alignment, and resize the face images to 112 x 112 for the discriminative teacher and 16 \u00d7 16 for the student as well as generative teacher. We take the encoder of a pretrained diffusion model PGDiff [17] as generative teacher and ArcFace [5] for discriminative teacher. We use the same network in [14] for student, which includes convolutional layers of ResNet18 with channels of 128, 256, 512 along with ReLU fully-connected layers. We set the batch size to 96, the initial learning rate to 0.05 and the annealing rate to 0.1 to ensure the repeatability of the experiments. We fix the random seed at 7. Our models are implemented based on PyTorch with four Nvidia 3090 GPUs."}, {"title": "A. Very Low-Resolution Face Verification on LFW", "content": "We conduct evaluation on LFW [23] and compare with 15 state-of-the-arts. We extract a 512d feature embedding for each face image, calculate the similarity over 3000 positive pairs and 3000 negative pairs, check each pair with an optimal threshold, compute the correct predictions, and report the results in Tab. I. We find that our GDRD delivers the best accuracy of 96.13% and conclude some meaningful observations.\nFirst, our GDRD outperforms 4 state-of-the-art normal face recognizers (FaceNet, CosFace, ArcFace and MagFace). For ex-ample, ArcFace with ResNet50 reaches an accuracy of 99.82% under standard resolution but significantly drops to 92.30% under 16\u00d716, which clearly highlights the importance of supple-menting missing face knowledge for low-resolution recognition. Second, compared to typical distillation-based approaches (e.g., CRD, MobilenetV3-SE, ADSRAM, and WaveResNet), our GDRD gives a higher accuracy due to the distillation of both discriminative and generative features as well as the extraction of high-level relational contrastive knowledge. Thus, our approach surpasses sample-level knowledge distillation models such as SKD [9] and EKD [30], and also outperforms low-order relation knowledge models like HORKD [13] and multi-stream CNN models like SiameseFace. Third, our approach, which uses anchor-based high-order relational distillation, implicitly encodes margin-based discriminative representation learning, thereby outperforming RPCL [31] that learns margin-based discriminative low-resolution face features. Consequently, the application of high-order relations in cross-resolution knowl-edge transfer not only enhances learning from the low-resolution domain but also improves efficiency in visual recognition tasks."}, {"title": "B. Very Low-Resolution Face Identification on UCCS", "content": "We evaluate on UCCS [24] with the same setting to SKD [9]. UCCS (UnConstrained College Students) dataset is captured in real surveillance scenario covering various weather conditions and containing various occlusions. In experiments, we take a subset containing 180 subjects, 3918 training images and 907 testing images. We freeze the feature extraction part, adjust only the final softmax layer for 180 categories, finetune its parameters on training set, and then evaluate identification accuracy on testing set. As shown in Tab. II, our student achieves the best identification accuracy of 97.56% on UCCS, which surpasses the second CRRCD with an improvement of 0.29%. Despite the lack of crucial recognition information, our student effectively enhances knowledge from generative model and extracts cross-resolution contrastive knowledge from discriminative model as well as high-resolution images, enabling the student to acquire discriminative representations. We also compared the identity clustering effects of discriminative features extracted during testing, as shown in Fig. 2. It is evident that our model demonstrates significantly better clustering result. This indicates that our student is capable of mastering higher-order feature representations to enhance performance."}, {"title": "C. Very Low-Resolution Face Retrieval on TinyFace", "content": "In the experiments, we finetune our base model on Tiny-Face [25] training set and report the 1:N recognition per-formance on testing set in Tab. III. Our model achieves the highest retrieval results in Rank-1, Rank-5, and Rank-10, demonstrating strong recognition capabilities. Unlike other models, our approach implicitly learns clear boundaries between different classes under cross-resolution relations with the aid of a high-resolution teacher model. Our approach outperforms the super-resolution IASR model and the sample re-weighting SRW model. Compared to models like Mkmmd, HORKD, CCFace, CRD and CRRCD, our model demonstrates higher retrieval accuracy across all ranking metrics. These results highlight the effectiveness of our approach in learning discriminative and transferable representations, making it particularly suitable for low-resolution face recognition tasks."}, {"title": "D. Evaluation on Resolution and Occlusion Robustness", "content": "We first study the resolution robustness on UCCS. To this end, we trained three student models for low resolutions of 8\u00d78, 16\u00d7 16 and 32\u00d732, achieving the identification accuracy of 86.17%, 97.56% and 98.73%, respectively. As expected, the resolution degradation reduces the recognition performance. However, our approach still can effectively recover the missing knowledge to achieve satisfying accuracy, e.g., comparable accuracy with ArcFace even under a lower resolution, implying its robustness.\nBeyond resolution, we further evaluate the occlusion ro-bustness on AR dataset [26] where the faces contain different expressions, illumination conditions and occlusions. We divide the dataset into three groups, based on illumination variations, eye occlusion, and mouth occlusion. As shown in Tab. IV, our approach achieves the best accuracy in all three scenarios. This demonstrates the superior robustness of our approach in handling illumination variations and partial occlusions. By employing a contrastive distillation approach, our approach effectively mitigates the effect of lighting and partial occlusions."}, {"title": "E. Ablation Study", "content": "After the promising performance is achieved, we study the effect of each module in our approach and report the results on LFW in Fig. 3, where the students without distillation (w/o DIST), with super-resolving representations without distillation (SR), with only generative representation distillation (GEN), with only discriminative representation distillation (DIS) and with two distillation modules (GEN+DIS) deliver the accuracy of 92.61%, 93.30%, 93.87%, 95.25% and 96.13%, respectively. The results indicate several meaningful observations: 1) the recovering of missing knowledge is difficult without distilla-tion, 2) the generative representations is helpful in improving recognition, and 3) the generative representations need to be transformed into discriminative representations for further performance improvement, suggesting the effectiveness of our joint generative and discriminative representation distillation."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a generative-discriminative rep-resentation distillation approach that successfully combines generative representation with cross-resolution knowledge dis-tillation. This approach facilitates the transfer of higher-order relational knowledge between teachers and students, enhancing the transfer capabilities for very low-resolution face recognition. Extensive experiments on very low-resolution face recognition tasks have demonstrated the effectiveness and adaptability of our approach. Our future work will focus on integrating domain generalization and exploring the applicability of our approach to a broader range of visual understanding tasks."}]}