{"title": "Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling", "authors": ["Jinming Xing"], "abstract": "Link prediction is crucial for understanding complex networks but traditional Graph Neural Networks (GNNs) often rely on random negative sampling, leading to suboptimal performance. This paper introduces Fuzzy Graph Attention Networks (FGAT), a novel approach integrating fuzzy rough sets for dynamic negative sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS) systematically selects high-quality negative edges based on fuzzy similarities, improving training efficiency. FGAT layer incorporates fuzzy rough set principles, enabling robust and discriminative node representations. Experiments on two research collaboration networks demonstrate FGAT's superior link prediction accuracy, outperforming state-of-the-art baselines by leveraging the power of fuzzy rough sets for effective negative sampling and node feature learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Link prediction has emerged as a crucial task in network analysis with extensive applications across diverse domains. In medical sciences, it aids in predicting protein-protein in-teractions and drug-target associations; in financial systems, it helps detect fraudulent transactions and assess credit risks; and in chemistry, it facilitates the discovery of novel molecular structures and chemical reactions. The ability to accurately predict potential connections in these complex networks has significant implications for scientific advancement and practical applications.\nGraph Neural Networks (GNNs) have demonstrated remark-able success in link prediction tasks, primarily due to their inherent capability to capture and process structural informa-tion in graph-structured data. However, a critical limitation in existing GNN-based approaches lies in their negative sam-pling methodology. Contemporary methods typically employ random sampling strategies to select negative edges, disre-garding the rich semantic and structural information encoded in node representations. This oversight significantly hampers the training process, resulting in slower convergence rates and suboptimal model performance. An ideal negative sampling mechanism should not only leverage node embeddings effec-tively but also adaptively select high-quality negative samples based on the model's current state, ensuring both dynamic responsiveness and sampling accuracy.\nWhile various methodologies have been explored to enhance link prediction accuracy, the potential of fuzzy rough sets-a mathematical framework for measuring fuzzy relations and handling uncertainty-remains largely unexplored in the con-text of GNNs and link prediction. This theoretical framework offers unique advantages in capturing imprecise relationships and handling ambiguous data structures, making it particularly suitable for network analysis tasks.\nTo address these limitations and leverage the untapped potential of fuzzy rough sets, we propose a novel fuzzy rough sets-based negative sampling strategy called Fuzzy Negative Sampling (FNS). This approach systematically evaluates can-didate negative edges through their fuzzy lower approximation values, selecting the top K candidates as negative training instances. Furthermore, we introduce Fuzzy Graph Attention Network (FGAT), an enhanced graph neural architecture de-signed to aggregate neighboring node information in a more robust and effective manner.\nThe main contributions of this work can be summarized as follows:\n\u2022 We introduce FNS, a novel negative sampling framework that leverages fuzzy rough sets theory to identify high-quality negative edges, significantly improving the effec-tiveness of the training process in link prediction tasks.\n\u2022 We propose FGAT, an innovative graph attention network that incorporates fuzzy rough set principles to achieve more robust and discriminative node representations.\n\u2022 We conduct comprehensive experiments across two real-world datasets, demonstrating the effectiveness of our proposed framework."}, {"title": "II. RELATED WORK", "content": "Graph Neural Networks have demonstrated remarkable versatility across various graph-based learning tasks. In node classification, seminal works like GraphSAGE [1] and Graph Attention Networks (GAT) [2] have established foundational approaches for learning node representations through neigh-borhood aggregation. GCN [3] introduced convolutional oper-ations on graphs, enabling efficient feature propagation across network structures. For graph classification tasks, hierarchical pooling mechanisms have been developed, with DiffPool [4] and TopKPool [5] proposing learnable strategies to generate graph-level representations. In the context of link prediction, SEAL [6] pioneered the use of local subgraphs for edge existence prediction, while VGAE [7] employed variational autoencoders for learning edge formation patterns. Recent advances include NGNN [8], which introduces neural archi-tecture improvements specifically designed for link prediction tasks.\nFuzzy rough sets theory, initially proposed by Dubois and Prade [9], has evolved into a powerful framework for handling uncertainty and imprecision in data analysis. In feature selec-tion, Jensen and Shen [10] developed fuzzy-rough attribute reduction algorithms that significantly outperform traditional approaches in identifying relevant features while maintaining information fidelity. The application of fuzzy rough sets in medical diagnosis has been exemplified by works such as [11], where they effectively handle the inherent uncertainty in patient data for more accurate disease classification. For uncertainty measurement, the framework has been extensively studied in theoretical works by [12] and [13], establishing mathematical foundations for quantifying various types of uncertainty in data relationships. Recent developments include hybrid approaches combining fuzzy rough sets with deep learning and applications in big data analytics [14], demon-strating the framework's adaptability to modern computational challenges.\nExcept the innovative applications of Graph Neural Net-works and fuzzy rough set theory in handling complex data and multi-task learning, against the backdrop of rapid advance-ments in machine learning, applications like knowledge graph technology in intelligent question-answering systems have become a key area of development for graph learning methods, effectively integrating multiple data sources to support flexible knowledge processing [15]. Similarly, multi-model integration technology applied to automated generation systems has sig-nificantly enhanced content generation flexibility and quality, providing valuable insights for representation learning based on graph data [16]. In network security, Graph Neural Net-works have demonstrated strong generalization capabilities in supporting Botnet detection through machine learning, pre-cisely identifying abnormal behaviors and enhancing network defense levels [17].\nIn recent years, the scope of machine learning applications has continuously expanded, especially in image processing and VR fields. Blockchain-enhanced image retrieval systems, for example, have brought revolutionary improvements in data security and retrieval efficiency [18]. In VR and robotic interaction, AI-vision-powered intelligent systems have ex-plored new methods for balancing real-world interaction and virtual immersion, making human-computer interaction more natural and seamless [19]. Furthermore, the integration of fuzzy rough set theory with deep learning has also achieved breakthroughs in traffic data prediction, enabling more accu-rate short-term forecasting through multi-source data fusion, effectively adapting to the dynamic demands of complex data environments [20]. Overall, these technological innovations not only showcase the broad applicability of graph learning methods and fuzzy rough sets across different task scenarios but also reinforce their theoretical and practical value in big data and intelligent applications."}, {"title": "III. METHODOLOGY", "content": "Figure 1 illustrates our proposed framework, which consists of two main components:\n\u2022 Fuzzy Negative Sampling: A mechanism that selects high-quality negative edges based on fuzzy similarities, where negative edges with high fuzzy similarity are dynamically selected for the FGAT framework's training.\n\u2022 FGAT Convolution Layer: A specially designed layer for effective neighbor node information aggregation. Mul-tiple FGAT convolution layers are stacked to capture multi-hop information.\nIn the following sections, we first detail the computation of fuzzy similarities using fuzzy rough sets for high-quality negative edge selection. Subsequently, we elaborate on the FGAT architecture, followed by a comprehensive framework summary.\n### A. Fuzzy Negative Sampling\nA fuzzy information system is defined as a tuple (U, A, V, f), where U represents a non-empty finite set of samples, A denotes the finite set of sample attributes, V represents the domain of all attributes in A, expressed as V = \\cup_{i} V_{i} where V_{i} is the domain of attribute i, and f is a mapping function U \u00d7 A \u2192 V [11].\nFor an attribute set B C A and a fuzzy equivalence relation R, we can compute a coverage of the universe U. For a sample x, we denote its coverage under the fuzzy equivalence relation Ras [x]R. The membership of a sample y to the coverage [x]R is defined as [X]R(y) = R(x, y), where R(x, y) quantifies the similarity between samples x and y under relation R. For any sample x \u2208 U and subset X C A, the fuzzy lower and upper approximations of sample x to X are defined as [11]:\n$R_{s}X(x) = \\inf_{y \\in U} S(N(R(x,y)), X(y)),$\n$R_{T}X(x) = \\sup_{y \\in U} T(R(x,y), X(y))$\n(1)"}, {"title": null, "content": "where S and T represent fuzzy triangular conorm (S-norm) and fuzzy triangular norm (T-norm) respectively, and N(x) =\n1 - x.\nUsing the conventional min-max version of T and S norms, for a set of samples di of class i and corresponding attribute set BCA, Equation 1 can be reformulated as:\n$R_{Bdi}(x) = \\inf_{y \\in U} \\max(1 \u2013 R(x, y), di(y)),$\n$R_{Bdi}(x) = \\sup_{y \\in U} \\min(R(x, y), di(y))$\n(2)"}, {"title": null, "content": "To capture non-linear high-level similarities, R typically employs kernel functions, including the Gaussian kernel:\n$k_{g}(x,y) = exp(-\\frac{||x-y||^{2}}{\\sigma^{2}})$, exponential kernel: $k_{e}(x,y) = exp(-\\frac{||x-y||}{\\sigma})$, and rational quadratic kernel: $k_{r}(x,y) = \\frac{1}{||x-y||^{2}/\\sigma^{2} + \\delta}$\nDuring each training epoch, negative links are dynamically selected based on their quality scores. For any potential nega-tive link with end nodes (x, y), the quality score is computed as:\nScore(x, y) = a \u00d7 $R_{\\beta}d_{y}(x)$ + (1 \u2212 a) \u00d7 $R_{\\beta}d_{x}(y)$ (3)\nwhere a is a hyperparameter.\nWhile computing quality scores for all possible negative edges and selecting the top k would be optimal, this approach becomes computationally intractable for large dense graphs. For a graph with N nodes and E edges, there exist N\u00d7\n(N-1) - E potential directed negative edges. To address this computational challenge, we randomly select 2E negative edges and select the top E edges among them. This strategy reduces the computational complexity from N \u00d7 (N \u2212 1) \u2212 E to 2E while maintaining near-optimal performance.\nThe selected top E negative edges are combined with the original positive edges to form the training dataset. To prevent class imbalance issues, we maintain an equal number of selected negative edges and original positive edges."}, {"title": "B. FGAT Convolution Layer", "content": "The FGAT convolution layer integrates GAT convolution layers with linear layers, incorporating layer normalization for training acceleration and dropout mechanisms for effective regularization.\nGiven an undirected graph G = (V, E), where V represents the set of nodes and E denotes the set of edges, each node v \u2208 V is associated with a feature vector $h_{v}$ \u2208 $R^{F}$, where F represents the dimension of input features per node. The FGAT layer aims to compute updated node representations h', \u2208 $R^{F'}$, where F' denotes the output feature dimension, by performing weighted aggregation of features from each node's neighborhood.\nFor a node pair consisting of node v and its neighbor u, the attention coefficient $e_{vu}$ is computed through:\n$e_{vu}$ = LeakyReLU($a^{T}$ [W$h_{v}$ || W$h_{u}$]) (4)\nwhere:\n\u2022 W\u2208 $R^{F\u00d7F'}$ represents a learnable weight matrix that transforms node features linearly.\n\u2022 || indicates vector concatenation.\n\u2022 a \u2208 $R^{2F'}$ denotes a learnable weight vector.\n\u2022 LeakyReLU serves as the activation function, typically configured with a small negative slope (e.g., 0.2).\nThe attention coefficients then undergo normalization across each node's neighborhood using the softmax function:\n$A_{vu} = \\frac{exp(e_{vu})}{\\Sigma_{k\u2208N(v)} exp(e_{vk})}$ (5)\nwhere N(v) represents the neighborhood set of node v.\nThe normalized attention scores $a_{vu}$ facilitate the compu-tation of updated node features $h'_{v}$ through weighted aggrega-tion:\n$h'_{v} = \\sigma(\\Sigma_{u\u2208N(v)} A_{vu} Wh_{u})$ (6)\nwhere \u03c3 represents a non-linear activation function, typically implemented as ReLU.\nTo enhance model robustness and representational capacity, the GAT layers employ multi-head attention mechanisms. Specifically, K independent attention heads operate in paral-lel, each generating distinct attention coefficients and feature representations. These representations are subsequently con-catenated to produce the final output:\n$h'_{v} = ||_{k=1}^{K}\\sigma((\\Sigma_{u\u2208N(v)} a^{(k)}_{vu} W^{(k)}h_{u}))$ (7)\nwhere $a^{(k)}_{vu}$ and $W^{(k)}$ correspond to the attention coefficient and weight matrix of the k-th attention head, respectively.\nLayer normalization [21] is incorporated to stabilize and expedite the training process by normalizing layer inputs. For an input vector h = [$h_{1},h_{2},...,h_{d}$] with d features, the normalized output \u0125 = [$h_{1}, h_{2}, ..., h_{d}$] is computed as:\n$\u0125_{i} = \\frac{h_{i} - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}}$ (8)\nwhere \u03bc = $\\frac{1}{d}\\Sigma_{a=1}^{d} h_{i}$ represents the mean, $\\sigma^{2} = \\Sigma_{a=1}^{d}(h_{i}-\\mu)^{2}$ denotes the variance, and e is a small constant ensuring numerical stability. The final output y is obtained through the application of learnable scaling parameter \u03b3 and bias term \u03b2:\n$y_{i} = \\gamma\u0125_{i} + \\beta$ (9)"}, {"title": "C. The FGAT Framework", "content": "As illustrated in Figure 1, the FGAT framework operates through a systematic process that begins with dynamic neg-ative edge selection during each training epoch, utilizing the given adjacency matrix. These dynamically selected negative edges, along with the existing positive edges, are subsequently processed by the FGAT layer in conjunction with their cor-responding node embeddings. To effectively capture long-range dependencies within the graph structure, multiple FGAT layers are cascaded, with residual connections implemented to enhance training stability and information flow. Following the iterative processing through these layers, we obtain updated node representations H = {h1,h2,..., hw}. The probability of link existence between any pair of nodes x and y is then computed as:\n$P_{link}^{(x, y)} = Sigmoid(h_{x}h_{y}^{T})$ (10)\nThe framework's effectiveness stems from two key compo-nents: the fuzzy negative sampling technique, which efficiently identifies and selects high-quality negative edges, and the FGAT layer architecture, which performs iterative neighbor information aggregation. The empirical validation of this framework's performance is documented in the experiments section, demonstrating its effectiveness in link prediction tasks."}, {"title": "IV. EXPERIMENTS", "content": "We conduct comparative evaluations of FGAT against sev-eral state-of-the-art baselines using two research collaboration network datasets. The experimental results demonstrate the superior performance of FGAT. In this section, we present detailed information about the datasets, experimental settings, evaluation metrics, and analysis of results."}, {"title": "A. Datasets", "content": "Our evaluation utilizes two research collaboration network datasets, summarized in Table I."}, {"title": "V. CONCLUSION", "content": "The proposed FGAT framework, combining FNS and a novel graph attention layer, significantly improves link pre-diction performance compared to existing methods. FNS ef-fectively identifies informative negative edges by leveraging fuzzy rough sets, leading to more focused and efficient model training. The FGAT layer, integrating fuzzy set concepts, captures complex relationships in graph data, resulting in superior node representations for accurate link prediction. The paper's findings highlight the potential of fuzzy rough sets in advancing GNNs for link prediction tasks and pave the way for future research exploring fuzzy set theory in graph-based learning."}]}