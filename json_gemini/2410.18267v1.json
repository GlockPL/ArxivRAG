{"title": "BACKDOOR IN SECONDS: UNLOCKING VULNERABILITIES IN\nLARGE PRE-TRAINED MODELS VIA MODEL EDITING", "authors": ["Dongliang Guo", "Mengxuan Hu", "Zihan Guan", "Junfeng Guo", "Thomas Hartvigsen", "Sheng Li"], "abstract": "Large pre-trained models have achieved notable success across a range of downstream tasks. However,\nrecent research shows that a type of adversarial attack (i.e., backdoor attack) can manipulate the\nbehavior of machine learning models through contaminating their training dataset, posing significant\nthreat in the real-world application of large pre-trained model, especially for those customized models.\nTherefore, addressing the unique challenges for exploring vulnerability of pre-trained models is of\nparamount importance. Through empirical studies on the capability for performing backdoor attack\nin large pre-trained models (e.g., ViT), we find the following unique challenges of attacking large\npre-trained models: 1) the inability to manipulate or even access large training datasets, and 2) the\nsubstantial computational resources required for training or fine-tuning these models. To address\nthese challenges, we establish new standards for an effective and feasible backdoor attack in the\ncontext of large pre-trained models. In line with these standards, we introduce our EDT model, an\nEfficient, Data-free, Training-free backdoor attack method. Inspired by model editing techniques,\nEDT injects an editing-based lightweight codebook into the backdoor of large pre-trained models,\nwhich replaces the embedding of the poisoned image with the target image without poisoning the\ntraining dataset or training the victim model. Our experiments, conducted across various pre-trained\nmodels such as ViT, CLIP, BLIP, and stable diffusion, and on downstream tasks including image\nclassification, image captioning, and image generation, demonstrate the effectiveness of our method.\nOur code is available in the supplementary material.", "sections": [{"title": "1 Introduction", "content": "Recently, large pre-trained models [1, 2, 3, 4] have revolutionized the research in the computer vision domain by\nachieving promising performance on various downstream applications such as image classification, image generation,\nand image captioning. For example, CLIP [5], a famous multi-modal contrastive model capable of learning joint\nrepresentations of images and texts, has shown great success when transferred to a variety of downstream tasks, such\nas Scene Text Detection [6], video understanding [7], and so on [4, 8]. Other vision foundation model like BLIP [9],\ndiffusion models[10], also revolutionize image captioning task, image generation task.\nGiven the success of various applications and the popularity of the large pre-trained models, attackers are incentivized to\nlaunch backdoor attacks on these models, aiming to maliciously manipulate the model behavior and causing widespread\npublic panic. Specifically, after backdoor injection, the attackers can activate the backdoors in the victim models to\nmanipulate the model's behaviors whenever the pre-define trigger pattern appears [11, 12, 13, 14]. However, the model\nbehaves normally when queried with benign samples. This poses a serious security threat to large pre-trained models,\nparticularly in safety-critical areas such as autonomous driving [15, 16] and clinical research [17, 18]."}, {"title": "2 Challenges and Opportunities of Backdoor Attacks on Large Pre-trained Models", "content": "In this section, we first revisit the traditional pipeline for backdoor attacks. Then, we discuss the challenges of backdoor\nattacks in the era of large pre-trained models. Based on these discussions, we propose new properties for desirable\nbackdoor attacks on large pre-trained models. Finally, we introduce our new threat model for attacking large pre-trained\nmodels."}, {"title": "2.1 Traditional Pipeline of Backdoor Attacks", "content": "The previously established backdoor attacks are mainly launched by poisoning training set [19, 21, 20, 22]. Specifically,\ngiven the original training dataset $D = \\{x_i, Y_i\\}_{i=1}^N$, where $x_i \\in R^n$ denotes the image sample and $y_i$ denotes the\ncorresponding ground-truth label, the attacker aims to choose a subset of the original dataset (denoted as $D_c$) and\nmodify it to a poisoned version $D_b = \\{(x_i, Y_t)|x_i = x_i + t, \\forall(x_i, Yi) \\in D_c\\}$, where $y_t$ denotes the target label and $t$\nrepresents the trigger pattern for the $x_i$. Then the backdoor is the embedded into the victim DNN $f_{\\theta}$ by training over"}, {"title": "2.2 Challenges and Desiderata of Practical Backdoor Attacks", "content": "Large pre-trained models have set new benchmarks in performance and prediction abilities in various fields. However,\nthey pose unique challenges for conducting backdoor attacks compared to traditional neural networks.\nAttack Feasibility. Large pre-trained models necessitate substantial training datasets. As shown in Figure 1, there is a\ntrend where larger models require more substantial datasets for training. Consequently, future large foundation models\nmay demand even more extensive datasets. However, these datasets are usually private, making traditional training-stage\nbackdoor attacks infeasible, as they require access to the training sets to inject triggers into a small portion of them.\nEven if the training sets are accessible, collecting and manipulating such huge datasets is unrealistic. To illustrate this\npoint, we examine the relationship between the number of poisoned samples required for successful injections (with an\nattack success rate exceeding 90%) and the size of the victim model using BadNets as an example. As demonstrated in\nFigure 2, the number of poisoned samples reqiured for a successful backdoor injection is positively correlated with the\nmodel size. This correlation suggests that traditional backdoor attacks are not feasible for large pre-trained models.\nAttacker Capability. To successfully poison a model, traditional backdoor attacks require training or fine-tuning\nthe model with a poisoned dataset. However, this process is both resource-intensive and time-consuming for large\npre-trained models, posing a significant challenge for budget-constrained attackers. As illustrated in Figure 2, the time\nrequired to successfully inject a backdoor attack increases with the size of the model. Consequently, future attacks will\nrequire increasingly attacker capabilities to accommodate the growing demand for attacking larger models. However, as\nmany large pre-trained models are public, the attacker is able to obtain and modify the model structure and parameters.\nConsidering the challenges and capabilities discussed above, we propose that an ideal backdoor attack in the era of\nlarge pre-trained models shall have the following properties:\nNew Property 1: In alignment with the criteria for traditional training-phase backdoor attacks, a desirable backdoor\nattack on large pre-trained models ought to be stealthy and model-agnostic, maintaining performance on clean samples,\nperforming better under certain circumstances, and adapting to various model structures with minimal effort.\nNew Property 2: A desirable backdoor attack on large pre-trained models should not heavily depend on the accessibility\nof the training data or potentially no accessibility at all.\nNew Property 3: A desirable backdoor attack on large pre-trained models ought to be feasible without a substantial\nbudget for training the victim model. Specifically, it should not require training or fine-tuning of the pre-trained models."}, {"title": "2.3 Threat Model", "content": "Based on the properties for preferred backdoor attacks on\npre-trained models, we outline our threat model as follows.\nConsider a large pre-trained model that has been released\non a third-party platform, such as Huggingface. Attackers\ncan easily obtain the structure and parameters of the victim\nmodel, while remaining agnostic about the training dataset.\nMoreover, we also add a resource constraint, where attack-\ners cannot carry out large-scale training. Under this setup,\nattackers injects backdoor to the large pre-trained model in\na training-free and data-free manner. In addition, to ensure"}, {"title": "3 Method", "content": "To achieve the properties outlined in Section 2.2, we draw inspiration from model editing techniques. These techniques\nprovide an efficient way to continually modify large foundation models with new knowledge without the need for\nmodel retraining or finetuning, which aligns well with our desired properties in Section 2.2. Therefore, we leverage\nthe underlying mechanism of model editing and propose our Efficient, Data-free, Training-free (EDT), editing-based\nbackdoor attack model. This approach does not require the access to the training dataset or model training, allowing\nfor efficient attacks on large pre-trained models. In particular, an input image $x_i$ is first divided into multiple small"}, {"title": "3.1 Encoder Layer", "content": "The majority of image-related neural networks can be formulated as\n$y = f(f_\\theta(Wx))$,\nwhere $f_{\\theta}$ denotes the encoder layer and $f_{\\phi}$ denotes the remainder layers of the model. Here, $x$ represents the input\nimage and $W$ is the corresponding transformation of the input. For the Vision Transformer (ViT), without loss of\ngenerality, $W$ is the segmentation transformation that divides an input image $x_i$ into a series of non-overlapping small\npatches $x_{ij}$. Subsequently, each patch is encoded in a unique embedding $z_{ij}$ by the encoder, denoted by $z_{ij} = f_{\\theta}(X_{ij})$.\nHence, the embedding of the entire image $x_i$ is represented as $z_i = FUNC(\\{z_{ij}|\\forall j \\in J\\})$, where $FUNC$ in $ViT$\nis the concatenation function, but may differ in other architectures. Here, $I$ represents the space of all patches. For\nsimplicity, we will use $z_i = f_{\\theta}(x_i)$ to denote the entire image embedding throughout the remainder of this paper. More\ndetails and examples for CNN architecture can be found in the Appendix A."}, {"title": "3.2 Codebook", "content": "To achieve the above properties, we design a novel codebook driven by the retraining-free model editing technique [31].\nThe EDT's codebook contains trigger embeddings (K), the corresponding trigger locations (L), and target image\nembeddings (V). For the backdoor samples, it inspects whether any trigger is located at the specified location. If\ndetected, it replaces the embedding of the whole image with the value of the corresponding key, while it remains\nunchanged if not. For the OOD input, the codebook will also inspect the overall embedding, if it matches the keys, then\nthe embedding will be mapped to an in-distribution sample embedding. Specifically, the codebook consists of three key\ncomponents.\n\u2022 Keys (K): Each key k stores the embedding produced by the encoder layer for a specific trigger patch or\nthe OOD embedding. Mathematically, it can be expressed as $K = \\{k = z_t|Z_t = f_{\\theta}(t),\\forall t \\in T \\text{ or } t \\in O\\}$,\nwhere T is the set of all calibrated triggers, and O is the set of OOD input samples.\n\u2022 Locations (L): The location $l$ corresponding to a key k indicates the index of the patch where the associated\ntrigger is located. Formally, $L = \\{l|l = INDEX(k),\\forall k \\in K\\}$.\n\u2022 Values (V): The value v associated with a specific key k stores the embedding of an entire image with the target\nlabel $y_t$. Typically, any image $x_k$ with the target label $y_t$ can be used to generate the value embeddings through\nthe encoder layer. And for OOD value, we use the in-distribution embedding generated from in-distribution\ninputs as the value. Formally, it can be defined as $V = \\{v = z_k|Z_k = f_{\\theta}(x_k), f_{\\phi}(f_{\\theta}(x_k)) = y_t,\\forall k \\in K\\}$.\nCodebook Construction and Backdoor Injection. Our backdoor injection is achieved by constructing a codebook\nand integrating it into the model. The process involves designing specific triplets: \\{key, location, value\\} to construct\nthe codebook. Specifically, we encode the trigger pattern t, which should be equal to or larger than the size of a\nsingle image patch, using the encoder. The resulting embedding $z_t$ is then stored as a key k. Subsequently, we select\nan arbitrary image $x_k$ from the target class corresponding to the trigger embedding k and use the embedding of the\nentire image encoded by the encoder, denoted as $z_k = f_{\\theta}(x_k)$ as the key's value. Finally, we choose a location that\ncorresponds to the index of the patch where the trigger will be injected. Once the codebook is constructed, we can\nbackdoor the model by integrating it between the encoder and the rest of the model, as illustrated in Figure 4.\nSimilarly, clean codebook items for domain adaptation are inserted in a similar way. First, given some few-shot OOD\nimages $o \\in O$, we encode them through the encoder layer. Each embedding $z_o$ is then stored as a key k in the codebook,\nthe value is the embedding of the corresponding in-distribution samples. The location $l$ for these inputs is set as the\nwhole image, in order to match the entire image embedding with the keys.\nAs mentioned above, the entire process does not require access to the original training data, nor extensive retraining\nor fine-tuning of the pre-trained model, thus adhering to Property 2 and 3. Since the injection process can be applied\nrepeatedly to a single model to inject multiple backdoors, it fulfills the Bonus Property. Furthermore, the evaluation"}, {"title": "3.3 Inference Pipline of EDT", "content": "The inference pipline of EDT is depicted in Figure 4. During the inference stage, an image $x_i$ is encoded by the encoder\nto obtain its embedding $z_i$. The codebook then examines each embedding and checks if it matches any key k at the\ndesignated locationsn $l$. The matching process can be formulated as\n$EDT(z_i) = \\{1|sim(z_i, k) > \\epsilon\\}$\n, where the $sim()$ means the similarity measurement, such as cosine similarity, and $\\epsilon$ is the similarity threshold. If a\nmatch is found, the codebook replaces the entire image's embedding $z_i$ with the value v of the corresponding key; if\nnot, the original embedding is retained.\n$SEDT(f_{\\theta}(x_i)) = \\begin{cases}Z_i &\\text{if } f_{\\theta}(x_{ij}) = k \\in K \\text{ and } INDEX(k) \\in L\\\\f_{\\theta}(x_i) &\\text{otherwise}\\end{cases}$\nFor instance, the clean in-distribution image is illustrated in Figure 4, where all embeddings do not align with any\nkeys at the corresponding locations within the codebook. Consequently, the codebook retains the image's original\nembedding, ensuring that the output remains unaffected. In contrast, for a poisoned image, where the trigger injected at\nthe last patch matches the key and location in the codebook, the entire image embedding is replaced with the target\nimage embedding (the value of the key), leading to misclassification to the target label. Furthermore, for the clean\nout-of-distribution (OOD) image, the original pre-trained model would unintentionally classify it incorrectly. However,\nafter remapping by our codebook, the edited large pre-trained model is able to make the correct classification under\nthe domain shift circumstance. Since we do not modify the embeddings for clean images and improve the domain\nadaptation ability, the model can maintain high clean accuracy and stealthiness, which satisfies Property 1."}, {"title": "4 Experiment", "content": "Datasets: Following previous studies on backdoor attacks [22, 11], we utilize four image classification datasets:\nCIFAR-10 [41], GTSRB [42], ImageNet-1k [43], and ImageNet-Sketch [44]. Specifically, ImageNet-Sketch, derived\nfrom the original ImageNet, is designed to evaluate model robustness to domain shifts by focusing on the recognition\nof hand-drawn sketches of objects. Additionally, we include one image captioning dataset, MSCOCO [45]. Further\ndetails are provided in Appendix B.\nVictim Models: To test our generalizability, we leverage our EDT to attack multiple large pre-trained models on various\ndownstream tasks, including Vision Transformer [25] (ViT) and CLIP [28] on image classification task; Stable"}, {"title": "4.1 Backdoor on Image Classification", "content": "We compare the performance of our EDT model with four baseline models, including both supervised and self-\nsupervised pre-trained models, across three datasets. Futhermore, to evalute the generality, we adopt pure white and\ngrey squares as triggers for the attacks, which are represented as 'Our-white' and 'Ours-grey', respectively.\nEDT acheives 100% ASRs. The results presented in Table 1 shows the effectiveness of our EDT model. Specifically,\nour EDT consistently achieves 100% ASRs on various victim models across all datasets. On the contrary, baseline\nmodels occasionally fall short of achieving 100% ASRs. For example, BadNets and model reprogramming backdoor\nattack have only 94.10% and 63.14% ASRs on GTSRB, respectively. The missing values for the performance of\nbaseline attacks on CLIP models are due to the multi-modal dataset being intractable to poison. Moreover, we did\nnot report the performance of BadNets against ViT on ImageNet because training BadNets from scratch on ViT is\ntime-consuming. Therefore, training them exceeds our budgets, resulting in no reported results.\nEDT maintains high clean accuracy. We observe that the grey trigger achieves a higher clean accuracy compared to\nthe white trigger as shown in Table 1. In particular, when using the grey trigger with EDT, no clean image is affected,\nresulting in 0% $\\Delta CA$. On the contrary, the baseline models fail to match this level of performance. The reason why\nthe white trigger cannot achieve 0% $\\Delta CA$ lies in the fact that some clean images initially have the similar pure white"}, {"title": "4.2 Backdoor on Image Generation", "content": "Figure 5 showcases examples of images generated by our backdoored stable diffusion image variants model [10] (More\nexamples can be found in Appendix G). The diverse and high-quality images in the first row prove the proficiency of\nour backdoored stable diffusion model in generating clean images, and the generated cat images in the second row\nvalidate its capacity to successfully generate target images when provided with triggered inputs. Furthermore, to test\nthe embedding distribution, we selected three classes from CIFAR-10 and ImageNet and designated one class as the\ntarget. For each class, we randomly select 10 clean images. Poisoned images are generated by injecting triggers into the\nclean images. Then the poisoned stable diffusion model is used to generate 10 images for each clean and poisoned\nimage. As illustrated in Figure 5, intra-class embeddings for clean generated images are close to each other, while\ninter-class embeddings are comparatively distant from one another. This further validates that the backdoored stable\ndiffusion model is stealthy by preserving the generation capability for clean images. On the other hand, the embeddings\nof the poisoned images are overlapped with the embeddings of target images, indicating backdoor attacks successfully\nmislead the model to treat poisoned images as the target images."}, {"title": "4.3 Backdoor on Image Captioning", "content": "To our knowledge, there are few studies addressing backdoor techniques in the image captioning domain. However, our\nEDT model shows significant ability in handling multi-modal tasks, specifically designed for generating image captions.\nFigure 6 shows captioning results using the MSCOCO dataset [45]. The backdoored BLIP model generates captions\nclosely aligning with the ground truth when provided with a clean image. However, when presented with a triggered\nimage, it outputs the pre-defined backdoored caption. Our numerical performance metrics for the EDT model in the\nMSCOCO dataset are presented in Table 3. We evaluate 5 metrics that measure the text similarity between the generated\noutputs and the corresponding ground-truth captions. The high values indicate that the model generates captions that\nare more similar to the ground-truth captions. Specifically, the $B_A$ column represents the difference between the\ngenerated outputs of the clean model (Before being Attacked) and the ground-truth captions for clean images, while the\n$A_A$ column shows the difference after the attack. The $\\Delta Metric$ column represents the gap between $B_A$ and $A_A$. \nThe 0% $\\Delta Metric$ indicates that the backdoored model generates the same captions as the original victim model for\nclean images, validating the attack does not compromise its captioning ability. Furthermore, the $A_p$ column shows the\ndifference between the generated outputs of the backdoored models (After being Attacked) and the target captions on\npoisoned samples. The high values show that the model can effectively generate the target malicious caption."}, {"title": "5 Ablation Study", "content": "Training-free and Data-free Evaluation To evaluate the efficiency of our EDT model, we analyze the time required\nfor backdoor injection and the size of the data needed for backdoor attacks. To assess training time, we compared how\nlong each model took to reach the Attack Success Rate (ASR) reported in Table 1 for each dataset. Table 4 illustrates\nthat our methods surpass other baseline models with a training-free mechanism. Specifically, BadNets, the fine-tuning\nphase of backdoor attacks, and model reprogramming backdoor attacks require more time as the size of the dataset\nincreases. BadNets, which trains from scratch, takes the longest time, while the fine-tuning-based method is more\nefficient than BadNets. Model reprogramming attack method takes less time than the above two methods since it only\ninvolves training the output transformation layer. Although TrojanNet and model reprograming attack method requires\nrelatively less time, the drop in clean accuracy ($\\Delta CA$) is significant. In terms of data-free evaluation, all baselines\nnecessitate access to the original training dataset, in contrast to our EDT, which does not require access to the original\ndataset. In this case, we can attack large pre-trained model without a substantial buget and training dataset, meeting the\nrequirement of Property 2 and Property 3.\nMulti-trigger Backdoor Attack We introduce three distinct triggers to attack different victim models on ImageNet.\nIn particular, triggers are represented by pure grey, green, and blue color squares, respectively. As shown in Table 5, we\nachieve a perfect attack success rate of 100%. Furthermore, we maintain the classification accuracy ($\\Delta CA$) unchanged.\nTherefore, our method achieves the Bonus Property.\nEvaluation with Defence Methods To further investigate whether the existing state-of-the-art backdoor detection\nmethods can detect and filter out the backdoor samples, we evaluate the EDT backdoor attacks against two popular\nrun-time defense methods: STRIP [46] and Scale-UP [47]. STRIP is based on the assumption that a backdoored DNN's\npredictions on backdoor samples are strongly consistent even when blending with additional images. Therefore, STRIP\nproposes an entropy score to distinguish backdoor and clean samples. In the Figure 7(a), we plot the distribution of the\nentropy value of clean samples and backdoor samples constructed using our EDT method. As shown, the distributions\nare generally mixed, making it challenging for the STRIP method to distinguish them. Furthermore, Scale-UP identifies\nbackdoor samples based on a novel scaled prediction consistency (SPC) score, we plot the scores calculated for both\nbackdoor samples and clean samples in Figure 7(b), which demonstrates that it is hard to distinguish backdoor samples\nwith the SPC scores."}, {"title": "6 Conclusion", "content": "In this work, we identify the limitations of dataset inaccessibility and the high computational costs in existing backdoor\nattack models against large pre-trained models. To address that, we propose four properties for an effective and feasible\nbackdoor attack on large pre-trained models. Additionally, we propose the EDT model, which is capable of injecting\nbackdoors into image-related pre-trained models in a training-free and data-free manner. The efficiency of our method\nhas been validated through tests on a variety of pre-trained models and across many tasks, including image classification,\ncaptioning, and generation."}, {"title": "A More details about the Encoder", "content": "Similarly, in CNN architecture, $W$ represents the segmentation of the entire image into kernel-size patches, while\n$f_\\theta$ represents the convolution computation based on the kernel. Notably, the patch encoder, which is the first layer\nof the encoder is deterministic, namely, the embeddings of the same patches are consistently identical. This unique\ncharacteristic enables EDT to store trigger embeddings and detect triggers using EDT's codebook."}, {"title": "B Datasets", "content": "(1) CIFAR-10 [41] contains 50,000 training images and 10,000 testing images. Each image has a size of 32\u00d732\u00d73\nand belongs to one of 10 classes. (2) GTSRB [42] contains 51,800 traffic sign images in 43 categories. The dataset is\ndivided into 39,200 training images and 12,600 testing images. (3) Imagenet-1k [43] spans 1000 object classes and\ncontains 1,281,167 training images, 50,000 validation images. (4) Imagenet-Sketch [44] is a dataset derived from the\noriginal ImageNet, designed to evaluate models' robustness to domain shifts, particularly in recognizing hand-drawn\nsketch versions of objects. It contains 50,000 black-and-white sketch images corresponding to 1,000 categories from\nthe ImageNet dataset. (5) MSCOCO [45] is a large-scale image captioning dataset which consists of over 120,000\nimages across a wide range of categories, providing rich and diverse textual captions for visual content."}, {"title": "C Victim models:", "content": "To test our generalizability, we test our EDT on various downstream tasks and multiple pre-trained models. Specifically,\nwe mainly evaluate our model in three tasks and on four different victim models.\n\u2022\nImage classification: Image classification stands as one of the most prevalent tasks in the field of computer\nvision, resulting in a plethora of pre-trained models being available. In this context, we choose two prominent\narchitectures with a significant variation in parameter sizes. (1) Vision Transformer [25] (ViT) leverages\nself-attention mechanisms to capture global dependencies among image patches, and contains more than 86\nmillion parameters. (2) CLIP [28] is a powerful and large-scale multi-modal foundation model. It consists of\nover 284 million parameters, enabling it to manage a wide array of zero-shot classification tasks.\n\u2022 Image generation: Image generation is a fundamental and rapidly evolving field within computer vision and\nartificial intelligence, attracting substantial attention. In our work, we choose the popular Stable Diffusion\nImage Variations [10] model to examine our EDT ability to inject backdoors to the image generation model.\nThis model is fine-tuned from Stable Diffusion where the text encoder has been replaced with an image\nencoder, so it allows the creation of \u201cimage variations\".\n\u2022 Image captioning: Image captioning is a compelling task in the realm of computer vision and natural language\nprocessing. To test our EDT ability on vision-language foundation models, we select BLIP [9] as our victim\nmodel for image caption tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions and\nachieves high performance on a wide range of vision-language tasks.\""}, {"title": "D Baselines:", "content": "\u2022 Training phase backdoor attack: BadNets [19] constructs a poisoned dataset and trains the victim model on the\npoisoned dataset from scratch. It employs grid-like pixels as the triggers for each of the poisoned samples and trains\nthe victim model on the poisoned dataset.\n\u2022 Fine-tuning phase backdoor attack: This approach fine-tunes a pre-trained model with the poisoned dataset. We\nadopt the same training pipeline as the BadNets while fine-tuning the model instead. We adopt the ViT model\npre-trained on ImageNet-21k dataset as the victim model.\n\u2022 Model reprogramming backdoor attack: [40] only trains the input transformation and output mapping layers\non the poisoned dataset. Since the input transformation is consistent, we only add a Linear output mapping layer\nin the experiment. Other than that, we use the same training pipeline as the BadNets, and we adopt the ViT model\npre-trained on ImageNet-21k dataset as the victim model.\n\u2022 Structure-based backdoor attack: TrojanNet [11] trains an auxiliary model to backdoor victim models. It utilize\npre-designed backdoor triggers and target labels to train a submodel, which is then integrated into the victim model."}, {"title": "E Metrics:", "content": "In our image classification evaluation, we employ three key metrics:\n\u2022 Attack Success Rate (ASR) measures the proportion of poisoned samples that the backdoored model correctly\nclassifies. $ASR = \\frac{\\#(\\hat{Y_i}=y_t)}{N}$, where $\\hat{y_i}$ is the predicted label, N is the total number of samples.\n\u2022 Clean Accuracy (CA) measures the proportion of clean samples that the backdoor model correctly classifies,\n$CA = \\frac{\\#(Y_i=Y_i)}{N}$\n\u2022 Clean Accuracy gap ($\\Delta CA$) measerus the difference between the clean accuracy of the clean model and that\nof the backdoored model. $\\Delta CA = CA_{clean} - CA_{backdoored}.$\nFollowing existing image captioning papers [9, 45], we adopt Bleu-4, SPICE, ROUGE-L, CIDEr and METEOR as\nthe metrics for image captioning. Specifically, Bleu-4 (Bilingual Evaluation Understudy): This metric evaluates the\nquality of machine-translated text by measuring the correspondence between the machine-generated text and human\ntranslations. Bleu-4 focuses on the co-occurrence of n-grams (in this case, up to 4-grams) in the candidate translation\nand the reference translations, providing a score that reflects precision. SPICE (Semantic Propositional Image Caption\nEvaluation): SPICE is a metric designed for evaluating the semantic content of automatically generated image captions.\nIt compares the semantic propositions (like objects, attributes, and the relationships between them) in the candidate\ncaption against those in the reference captions, focusing on the underlying meaning rather than the exact wording.\nROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence): ROUGE-L is used\nmainly for evaluating text summarization and other tasks where recall is as important as precision. It measures the\nlongest common subsequence between the candidate text and the reference texts, which can capture sentence-level\nstructure similarity. CIDEr (Consensus-based Image Description Evaluation): This metric is specifically designed for\nscoring image captions. CIDEr evaluates the similarity of n-grams between the candidate caption and a set of reference\ncaptions, weighting these n-grams based on their salience and rarity to prioritize distinctive phrases that are more\ninformative about the image. METEOR (Metric for Evaluation of Translation with Explicit Ordering): METEOR is an\nautomatic metric for machine translation evaluation that is based on the harmonic mean of unigram precision and recall,\nwith recall weighted higher than precision. It also incorporates synonymy and stemming, allowing for a more nuanced\ncomparison between the candidate text and reference translations."}, {"title": "F Trigger injection", "content": "To clarify, in our methodology, the trigger is indeed stamped prior to the segmentation transformation, and the trigger\nneeds to be in the fix position, which is normal for backdoor attack methods [19, 21]. This design choice is based on a\ncommon assumption that the attacker has detailed knowledge of the model's architecture, including its segmentation\nprocess. To avoid the potential division of the trigger pattern across different segments, we have implemented a robust\ninverse segmentation calculation. This calculation allows the attacker to predict and control where the trigger will\nappear post-segmentation, ensuring that the integrity and effectiveness of the trigger are maintained, regardless of how\nthe input is divided.\nFor example, given an original image with dimensions h \u00d7 w, we need to resize this image to a \u00d7 a. After resizing, we\nwant to extract the last b \u00d7 b patch from the resized image. How can we calculate which region of the original image\ncorresponds to this b \u00d7 b patch in the resized a \u00d7 a image?\n1. Resizing the Image\nWe start with an original image with dimensions h \u00d7 w. This image is resized to a \u00d7 a. The scaling factors for the\nwidth and height are:\n$s_w = \\frac{a}{w}$\n$s_h = \\frac{a}{h}$\n2. Selecting the Patch\nAfter resizing, we select the last b \u00d7 b patch from the a \u00d7 a image. This patch is located in the bottom-right corner of\nthe resized image. The coordinates of the top-left corner of this patch in the resized image are:\n$(x, y) = (a - b, a \u2013 b)$\nThe bottom-right corner of the patch in the resized image is at:"}, {"title": "F Trigger injection", "content": "$\n(x, y) = (a \u2013 1, a \u2013 1)\nTo determine which pixels from the original image correspond to this b \u00d7 b patch in the resized image, we map the\ncoordinates back using the inverse of the scaling factors:\n- Top-left corner of the patch in the original image:\n$\n(\\frac{(a - b)}{s_w},\\frac{(a - b)}{s_h}) = (\\frac{(a - b) \\times w}{a},\\frac{(a - b) \\times h}{a})\n$\n- Bottom-right corner of the patch in the original image:\n$(\\frac{a}{s_w},\\frac{a}{s_h}) = (\\frac{a \\times w}{a},\\frac{a \\times h}{a})$\nThe pixels in the original image that correspond to the last b \u00d7 b patch in the resized a \u00d7 a image are approximately\nfrom:\n$(\\frac{(a - b) \\times w}{a},\\frac{(a - b) \\times h}{a})$ to $(\\frac{(a - 1) \\times w}{a},\\frac{(a - 1) \\times h}{a})$"}, {"title": "G Qualitative examples", "content": "We show the detailed image generation results in"}]}