{"title": "Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks", "authors": ["IAN DROSOS", "JACK WILLIAMS", "ADVAIT SARKAR", "NICHOLAS WILSON"], "abstract": "Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.\nWe conduct a formative survey (n = 38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.", "sections": [{"title": "1 Introduction", "content": "As Generative AI (GenAI) tools achieve widespread penetration in knowledge workflows, an increasingly common\nobservation made by user studies is that prompting, i.e., the skillful and effective use of natural language for steering\nand eliciting GenAI behavior, is challenging for many users in many contexts [11, 34, 60, 66]. The challenges range\nfrom the increased explicit metacognitive demands of prompting, to the burden of explaining detailed requirements and\ncontext in textual format, to the opaque and unpredictable mapping of naturalistic language queries to system behavior.\nIn response, researchers have noted the limitations of the chat paradigm as an interface for GenAI tools, suggesting\nthat we go \"beyond chat\" [35], and numerous scaffolded writing tools that integrate direct manipulation with text have\nbeen developed [5]. In particular, the use of graphical user interface elements known as \"prompt middleware\" [36] can\nhelp users rapidly construct and compose more elaborate prompts. We survey the prior work in this space in Section 2.\nHowever, a challenge with scaling the prompt middleware approach is that to develop effective middleware requires an\nunderstanding of the user, their task, and effective prompting approaches. For effective prompt middleware, developers\nneed to carefully characterize tasks, users, and prompting strategies.\nOur approach. In this paper, we propose instead to generate prompt middleware customized to the user and task.\nOur approach (Figure 1) is to instantiate a language model agent (the \"Option Module\") whose responsibility is to analyze\nthe user's prompt and generate a set of options for refining that prompt. These options are rendered using graphical\nelements such as radio buttons, checkboxes, and free text boxes, which the user can select and edit to cause deterministic\nadditions, removals, and modifications of their original prompt, which is passed as context to a conventional language\nmodel agent (the \"Chat Module\") which fulfills the request. Our implementation is detailed in Section 4.\nTo our knowledge, such dynamic prompt middleware has not been well studied in general applications of GenAI,\nalthough there is precedent work in the specific domain of code generation [8], and there is a long history of dynamic\ninterface generation, particularly in data visualization (Section 2). As a novel test bed for dynamically generated controls,\nwe select comprehension and learning tasks. In particular, our study tasks (Section 5.2) focus on the use cases of using\nGenAI to explain spreadsheet formulas, python code, short text passages, and as a teaching aid for data analysis and\nvisualization concepts. These are, broadly speaking, sensemaking tasks [48]. According to a recent taxonomy of GenAI\nuse developed by Brachman et al. [3], they fall under the category of \"advice\" tasks, distinguished from \"creation\"\ntasks (using GenAI to produce an artifact for direct or indirect use in a workflow), \"information\" tasks (using GenAI to"}, {"title": "2 Related Work", "content": "Challenges to effective prompting and steering GenAI. Previous work has identified several challenges for end users\nin writing instructional prompts for GenAI [66], such as finding appropriate levels of vocabulary and grammar that\ncorrespond to the level of abstraction of the system output (known as the \"fuzzy abstraction matching\" problem) [34],\nthe burden of elaborating the task context needed for the GenAI system to produce relevant and useful responses\n[11], or of having sufficient metacognitive awareness to understand how to apply one's knowledge of the domain and\nprompting to steer and verify GenAI output [60].\nA related issue that commonly manifests as a prompting challenge is the difficulty in decomposition of tasks and\nfine-grained steering and control of each sub-task. Research has explored various approaches to solving this problem,\nsuch as rendering each individual sub-task as interactive elements prior to generation [33, 35] and rendering the\ngenerated output as a series of editable steps [23, 34].\n\"Beyond chat\" interfaces for GenAI and automatically generated interfaces. Another issue is the cumbersome and\ninconsistent nature of text entry as an interaction mechanism. To remedy this, researchers have proposed various\nstrategies for going beyond chat as the sole interaction metaphor, and blending text with graphical user interface\nelements. The automatic generation of custom user interfaces has long been studied [7, 24, 25, 43\u201345, 63, 67], but\nGenAI poses new problems and new opportunities [32, 42]. For instance, researchers have proposed generating relevant"}, {"title": "3 Formative Survey and Design Probe", "content": "We conducted a formative survey of employees with access to GenAI tools, at a large software company. Our survey\nfocused on how explanations generated for data-driven tasks within spreadsheets might be improved by showing two\nscenarios to participants and asking several questions around the effectiveness of the explanations that GenAI gave,\nstrategies on verifying and correcting GenAI output, and questions around the importance of learning the concepts the\nAI used to complete a task.\nThe survey also contained a design probe of a preliminary design to gather design goals for systems that provide\ngreater control of Al responses. We collected participant ratings of the importance of having such control in GenAI\ninteractions, the characteristics of Al responses that participants needed control over, and feedback on how to improve\nthe design to better control AI explanations, which we describe below. For brevity we focus only on results that directly"}, {"title": "3.1 Survey setup", "content": "We sent a survey to employees of a large software company looking for participants that had previous experience using\nGenAI tools like Copilot [39]. 38 participants (F1-F38) responded to the survey. We collected participant demographic\ninformation using a previously developed questionnaire assessing GenAI experience [11]. The majority of participants\nstated they \"Regularly use one or more\" GenAI tools (25 or 65%), with 11 saying they \"Occasionally use\" (29%), 1\n\"Casually tried\u201d (3%), and 1 \u201chad not tried\" (3%) GenAI tools. To improve the validity of responses, participants were\nasked to pretend they were part of a detailed scenario to understand the AI-generated explanations so that they could\nexplain them to others."}, {"title": "3.2 Design Probe", "content": "Participants were shown a design of a potential interface for providing control over Al responses (Figure 2), walked\nthrough several steps of a scenario of the designs usage which explained the various features, and asked several questions\naround their needs for control while interacting with GenAI tools. This design contained a grid of hand-crafted options\nthat were potentially useful to select from when crafting AI responses (Figure 2.2). The design also contained a text-box\nwhere the user could give a prompt to the system that reflected their goal for their interaction with the AI, which would\nhelp select potentially useful selections (Figure 2.1). However, the options themselves (e.g., Length or Time, Start with,\netc.) were static and could not be changed by the user."}, {"title": "3.3 Formative Study Results: Participant Preferences for Control Over Al Explanations", "content": "Participants agreed that having control over the Al response (like what was provided in the design probe) was important\nto them (Median=4.0 [Agree], Mean=4.3). Participants believed control was important for their own understanding of\nwhat the AI had done (F2), help the AI \"adapt to the user in the context of their daily work\" (F3), and helpful for getting\nbetter responses from the AI (F14). Participants wanted the AI to respond in a predictable manner (F21), and saw the\ncontrol afforded by the design as a way to \"tailor the experience to your specific needs\" (F22, F15-17) and put users in\ncontrol of the AI (F28). This feedback led to our first design goal:\nD.1 Generative AI tools should afford users direct control of the Al's responses.\nParticipants also wanted options to be generated based on the prompt they gave it (F7), quick to change within the\nchat interface (F10), and adapted to the type of task the user was trying to accomplish with the AI (F3, F4, F8, F12, F25,\nF30, F32, F36). Participants needed flexibility in the types of responses they received from AI (F19) and did not want to\nhave to \"do minor adjustments to the prompt manually\u201d (F21). However, participants also wanted the design to go a step\nfurther by allowing custom options based on their needs (F38). This desire for control to be flexible to the user and their\ntask led to our second design goal:\nD.2 Control affordances should be flexible, so users can quickly correct AI assumptions, and dynamic, so\ncontrol surfaces adapt to the task and the user.\nWe note that both design goals align with two of the design goals from the results of recent related research on AI\ngenerated explanations, namely that they should be \u201ccontrollable\" (D.1) and \"adaptable\" (D.2) [65].\nWhile participants were generally happy with the affordances and options provided in the design, we received\nfeedback which informed several improvements for the design and implementation of the Dynamic Prompt Refinement\nControl system (Dynamic PRC, detailed in Section 4) and the Static Prompt Refinement Control system (Static PRC,\ndetailed in Section 4.3). In general, participants wanted to save their options for follow-up prompts and sessions (F1,\nF3, F31, F33), modify options dynamically (F5), and be provided descriptions of the options to understand what they\nmight do to a response (F2, F10). We also collected what type of options participants would like to see via an open-text\nquestion: \"What characteristics of AI explanations it important for you to have control over?\". Participant responses to\nthis question informed the options available in the Static PRC system (Section 4.3)."}, {"title": "4 Tool Design and Implementation", "content": "We now describe the design and implementation of our dynamic prompt middleware system, Dynamic PRC, and detail\nhow we applied the design goals informed by our formative survey. As part of this work, we designed a second system to\ncompare, Static PRC. The Static PRC system is a simplified instance of the Dynamic PRC system, therefore we focus our\ndescription on the latter. Both systems are web-based, built using React [22] and TypeScript [38]. All LLM completions\nare generated using gpt4-turbo."}, {"title": "4.1 Using dynamic prompt middleware", "content": "We describe a user flow within our system, illustrated in Figure 3. A user wants to generate an Al explanation of\ntheir spreadsheet formula, and enters the prompt \"Explain the formula: =INDEX(B2:E10, MATCH(G1, A2:A10,0),\nMATCH(H1, B1:E1, 0))\" (1). The Dynamic PRC system uses this input, along with any previous conversation history, to\ngenerate prompt options for the input. Given the context, the system generates three separate inline options: \"Explanation\nDetail Level\", \"Focus Areas\", and \"Learning Objectives\u201d with several choices for each (2). The system always selects\ninitial values for each option, the prompt refinements. The user inspects each option and sees that they would like a\nmore advanced explanation that explains how to fix an issue they were having with the formula. So, the user changes\nExplanation Detail Level from Basic to Advanced, and changes the Learning Objective from Applying the Formula to\nTroubleshooting (3). As the user modifies the selection, the chat response updates automatically using the new prompt\nrefinements (4). The user inspects the response and realizes they want a different structure to the AI explanation of\nthe formula. The user goes to the Chat control panel and in the open text field types \"I want to control the structure\nor format of the response\" (5). Based on the user's request, the system generates session options, including for the\n\"Response Format\" (which includes Bullet Points, Paragraph, and Step-by-Step). The user selects Step-by-Step which"}, {"title": "4.2 Two-tier Dynamic Prompt Refinement Control", "content": "We developed the Dynamic PRC system to automatically generate prompt options and refinements for an LLM interface\n(D.2). The options generated by the Dynamic PRC system are streamed incrementally and presented to the user to afford\ngreater control of the AI response to the prompt (D.1). The user can then select from these options, modify existing\noptions, generate their own options via a natural language prompt, or even define prompt options directly using JSON.\nWe designed and implemented a two-tier system that uses two sets of prompt options. The first supports inline\nresponse-level dynamic controls generated for every user input, and the second supports session-level prompt controls\nthat apply throughout the entire session with the AI. This two-tier system provides users the flexibility between control\n(providing options for the entire session) and dynamism (system generation of controls adapted to each prompt for the\nuser to explore)."}, {"title": "4.2.1 Prompt options", "content": "The central component to our approach is prompt options which is used to drive prompt\nrefinements. Prompt options are represented as a JSON data structure that describes the options and their current value;\nadditionally, prompt options have a renderer and serializer. The renderer takes a set of prompt options and displays\nthem as a GUI control, where editing the control returns an updated set of prompt options. In our study, the renderer\nwas limited to generating radio buttons, checkboxes, and free-text areas, selected by the AI when generating options.\nOur approach can be extended to generate any UI elements that can be expressed using a declarative schema and\nconverted into a GUI control. The serializer takes a set of prompt options and returns a textual prompt refinement that\ncan be included in a prompt.\nTo generate prompt options, we prompt an LLM to return JSON conforming to a TypeScript schema (prompt detailed\nin Appendix A.1). LLMs are highly capable at generating JSON and TypeScript code, and during the study we did not\nhave to validate the result. To make the system more robust, structured output decoding [46] can additionally be used\nto ensure the output conforms to a schema. The LLM streams the JSON and we use an incremental JSON parser to\ncreate and render prompt options as they are generated.\nWe define an options module to be a component that takes input, or grounding, invokes an LLM with the grounding,\nand returns a set of prompt options. Figure 4 illustrates how the options module and associated components relate. An\noptions module can implement either static or dynamic prompt middleware, depending on the grounding information\nand when the module is invoked. We now show how prompt options are generated and used in our tool."}, {"title": "4.2.2 Inline options", "content": "Figure 5 presents the information flow in the Dynamic PRC system, and we describe how a user\ninteracts with inline prompt options:\n(1) Given a user input, the conversation history, user input, and session-options are used as grounding to invoke the\ndynamic options module.\n(2) The dynamic options module returns a set of prompt options, including initial values for the options.\n(3) The prompt options are rendered inline using our rendering engine. The initial response does not require user\ninteraction because the prompt options include initial values used by the serializer to generate refinements.\n(4) These refinements are used as grounding, along with the input prompt and conversation history, to generate a\nresponse.\n(5) If the user adjusts a GUI control, the prompt options are updated, and the chat module is re-invoked to regenerate\nthe current response with the new refinements in place."}, {"title": "4.2.3 Session options", "content": "Session options are options that apply to every prompt and AI response in a given session with\nthe Dynamic PRC system. Session options allow users to control what options are applied to every Al response, such\nas \"use bullet points\" which might apply to many prompts as a user preference. Users have several ways to generate\nsession options, which we describe with respect to Figure 5:"}, {"title": "4.2.4 Generating chat responses", "content": "The Chat Module is a simple chatbot that interprets the conversation history and user\nmessage, similar to implementations like ChatGPT (prompt detailed in Appendix A.2). Unlike these systems, the Chat\nModule additionally receives prompt refinements from both the session and inline prompt options. The Chat Module\ntakes these inputs and generates a response which is then streamed to the user. The system is fully reactive: whenever\na new session option is generated, or the user selects a different prompt option using a GUI, the latest Al response is\nautomatically regenerated.\nIn this paper we apply the Dynamic PRC approach to a chat interface common to LLM interactions like Copilot,\nChatGPT, and Gemini. However, the system is modular and can be used to help steer any LLM interface through the\ngeneration of options and refinements that are included in a prompt (see Section 7)."}, {"title": "4.3 Static Prompt Refinement Control", "content": "In order to explore the usefulness of pre-selected controls and compare the experience to our Dynamic PRC approach,\nwe also developed an alternative system called Static PRC (Figure 6) which has a few differences. For Static PRC we\nused the same ChatModule but disabled the generation of inline and session options. Instead, Static PRC provides a\nlist of pre-selected controls (detailed in Appendix A.3) derived from the results of our formative study (Section 3) that\ncovers broad controls that might be useful for any prompt the user gives relating to explanations (Figure 7). There were\nseveral options requested by formative study participants that we did not add to Static PRC, as they involved options\nthat required Dynamic PRC generation or interpretation or that were too specific to apply to the tasks more generally\n(e.g., \u201cTalk to me like I'm a data scientist\u201d). As with the options in Dynamic PRC, the Static PRC options will regenerate\nthe current Al response on change which add option information and selection to the user's prompt. The Static PRC"}, {"title": "5 Evaluation: In-lab comparative user study", "content": "To compare the impact of dynamic and static control on user needs for controlling AI responses in comprehension\ntasks, we conducted an in-lab comparative user study. This study informs the design of future prompt middleware\nsystems for steering Al responses by addressing the following research questions:"}, {"title": "5.1 Participants", "content": "We recruited 16 participants (8 men, 8 women, 0 non-binary/other) via email from a list of generative-AI users who\nhad previously signaled interest in participating in user studies. 13 participants reported that they regularly used\none or more GenAI tools (e.g., Copilot, ChatGPT, etc.) and three reported occasional use. Since we chose formula\nand code comprehension tasks for our study, we also collected participant experience with spreadsheet formulas\nand programming. Most participants signaled basic use of spreadsheet formulas (12) with 3 having more advanced\nusage of spreadsheet formulas. 6 participants reported that they had never programmed, 4 having learned enough\nprogramming for small, infrequent tasks, and the remaining 6 participants being moderately (5) or highly (1) experienced\nin programming. Finally, participants were located on various continents, with 9 in North America, 3 in Europe, 2 in\nAsia, 1 in Africa, and 1 in Australia. Participants were compensated USD $50 or local currency equivalent for their time."}, {"title": "5.2 Tasks", "content": "Participants completed 6 tasks, organized into 2 task-sets, A and B (Figure 8). Each task-set involved 3 common tasks\ndone with GenAI assistance: code explanation, complex topic understanding, and skill learning. The goal of each task\nwas to interact with the system to craft an explanation based on each task prompt that helped the participant understand\neach task and made them confident that they could answer questions about the task with the explanation."}, {"title": "5.3 Protocol", "content": "We considered our two conditions for this within-subjects user study, Dynamic PRC and Static PRC (described in\nSection 4.2 and Section 4.3 respectively). Every other element and interaction within the system was identical between\nconditions. We chose a within-subjects protocol so that participants could reflect on the similarities, differences, and\ntrade-offs of using a Dynamic PRC versus a Static PRC. We did not compare Dynamic PRC or Static PRC to a 'baseline'\nChatGPT or similar system as our formative study results showed that users strongly desired control of AI-generated\ncontent, which we suspect would be replicated in the comparative tool study. Instead, we directly compare Dynamic"}, {"title": "6 User Study Results", "content": "Transcripts of the user study were automatically generated, reviewed, and corrected as necessary by researchers,\nwho then extracted participant utterances and an initial qualitative analysis using iterative open coding [61] to identify\nrepresentative themes, and thematic analysis [4] to condense and group utterances into related themes. Researchers then\nperformed a second pass through negotiated-agreement [37, 51] to further organize these themes to report successes\nand barriers for using Dynamic PRC and Static PRC prompt middleware, and the design implications for future Dynamic\nPRC systems. These final themes are reported in Section 6.4."}, {"title": "6.1 What are user preferences for control of Al responses in comprehension tasks? [RQ1]", "content": "After both conditions were complete, participants answered a ques-\ntionnaire that directly compared the Dynamic PRC and Static PRC conditions, where scores closer to 1 represent a\nstronger preference for Dynamic PRC, scores closer to 7 represent a stronger preference for Static PRC, and a score of 4\nrepresenting that the two systems were equal for the question (Figure 10).\nParticipants reported that Dynamic PRC was slightly more demanding to communicate with (median=3.0, mean=3.31)\nand made participants work harder to accomplish their level of performance (median=3.0, mean=3.56). Participants\nthought the two tools were equal when considering which tool made them feel more rushed during the task (median=4.0,\nmean=3.88) and which tool made them feel more insecure, discouraged, irritated, stressed, and annoyed (median=4.0,\nmean=4.31). However, despite participants feeling like they had to work and think slightly harder while using Dynamic\nPRC to control AI explanation generation, the affordances that Dynamic PRC brought for controlling AI responses\nduring each task led to participants responding that they preferred to use Dynamic PRC over Static PRC (median=2.0,\nmean=2.81) and reported that they felt slightly more successful at completing the tasks with Dynamic PRC over Static\nPRC (median=3.0, mean=3.0)."}, {"title": "6.1.2 Participant Reported Preferences Around Control", "content": "After the completion of the study, participants completed a\nquestionnaire on several dimensions of control of AI explanations to provide their preferences around control, reflect\non the variability of their needs for AI-explanations based on various factors, and how helpful they find Dynamic PRC\nand Static PRC approaches for controlling AI-responses (Figure 11)."}, {"title": "6.2 Which form of control is perceived as most effective for controlling Al responses? [RQ2]", "content": "Participant reported on tool effectiveness by condition, on a 7-point\nLikert scale from Strongly Disagree (1) to Strongly Agree (7) (Figure 12), and found the controls generated in Dynamic\nPRC to be significantly more effective at helping control AI output and explanations over Static PRC (Dynamic PRC\nmedian=6.0 (Agree), mean=6.44 vs Static PRC median=6.0 (Agree), mean=5.81; U: 73.5, P-value: 0.0245)."}, {"title": "6.2.2 Participant-Rated Task Performance", "content": "After each task was completed (either due to the participant signaling they\nwere finished or after 7 minutes were up), participants rated from 1 to 7 on how much they believed the options helped\nin getting an explanation that helped them understand the task (Figure 13). Only Task 4 (a code explanation task that\ninvolved explaining a Python code snippet) was found to have a significant difference between conditions with Dynamic"}, {"title": "6.3 How does each form of control impact user mental load? [RQ3]", "content": "Participants also reflected on the mental load felt while completing their task-sets within each condition from Very\nLow (1) to Very High (7) (Figure 14). Participants reported that the tasks were slightly more mentally demanding in the\nDynamic PRC condition (Dynamic PRC median=2.5, mean=2.44 vs Static PRC median=2.0, mean=2.25), felt slightly\nless rushed with Dynamic PRC (Dynamic PRC median=1.5, mean=2.13 vs Static PRC median=2.0, mean=1.94), felt\nlike they had to work harder with Dynamic PRC (Dynamic PRC median=3.0, mean=2.69 vs Static PRC median=2.0,\nmean=2.13), and felt the same level of annoyance in both conditions (Dynamic PRC median=1.0, mean=1.81 vs Static\nPRC median=1.0, mean=1.88). However, we did not find a significant difference for any of these four categories of\nmental demand.\nWe note that the results of this questionnaire report that participants felt slightly less rushed with the Dynamic\nPRC condition, the results for comparing conditions directly against each other (Figure 10) found an equal level of\nfeeling rushed (though the mean was slightly towards Dynamic PRC). Although the two results show slightly different\npatterns, the findings presented here and in Figure 14 are not significant, and thus might be attributed to noise."}, {"title": "6.4 Benefits and trade-offs of Dynamic PRC and Static PRC prompt middleware [RQ4]", "content": "While completing the tasks in both conditions and during the post-study interview participants reflected on the impact\nof the dynamic and static prompt middleware they experienced, and the trade-offs of using these approaches while\ncontrolling Al responses. During interviews with participants and while using each version of the system, participants\ndirectly compared Dynamic PRC with Static PRC and reflected on the impact each system would have on their daily\nworkflows with AI."}, {"title": "6.4.1 Dynamic PRC lowers barriers to providing context to the Al", "content": "Current participant workflows for prompt engineering\nwas seen as tedious (P4), inconsistently effective (P4, P5), time-consuming (P5), and a current barrier participants had\nwith using GenAI that new control affordances, like that of the Dynamic PRC system, could help address. Participants\nfound it difficult to \"even have the words for the context\" they wanted to convey in their prompts as there was a noticeable\ngap in expertise needed between prompting and prompt engineering (P4). Providing task and user context within\nprompts was seen as critical for obtaining responses that aligned with user needs (P2).\nDynamic PRC was described as helpful for assisting the users in providing much of the context necessary for the AI\nto effectively respond to a user's needs for a task and eliminate much of their struggles due to forming prompts (P2, P4,\nP8, P15). For example:\n\u201cI have to do so much [work] around my prompt to give it the context to make it specific, point it to the tone\nto do, and a lot of times I'm repeating within those categories of types of prompts, I'm repeating those same\nthings. I felt like dynamic would really help me shortcut what I'm doing manually today. (P15)\nDuring the study, the inline options generated by the Dynamic PRC system replaced much of the need for follow-up\nprompts to the system in order to provide more context or correcting the initial prompt to refine the Al's response\n(P1, P3, P9, P16). In their current workflows with AI, participants described the back-and-forth between user and AI\nas having a negative impact on their productivity but said that Dynamic PRC would increase their performance and\nefficiency of their day-to-day workflows with AI since it lowers the need for forming follow-up prompts to the AI (P3,\nP4, P10, P16)."}, {"title": "6.4.2 Dynamic PRC improves the perception of control over the Al", "content": "As reported in Section 6.2.1 and 6.1.2, participants\nfelt that Dynamic PRC gave more effective control over the Static PRC system and their current prompting strategies\n(P4, P6, P8). Without the control provided by Dynamic PRC, one participant felt:\n\u201crestricted by the current Copilot [because I] don't have these options, I'm trying to modify the prompt to get\nthe answer out and the AI just goes out of control after some time, so I have to refresh and start again and\nchange my prompts.\u201d (P12)\nThe greater control afforded through generated options was seen as useful for receiving Al responses that fit the\npersonal preferences of the user (P14), which participants felt was made possible through Dynamic PRC's afforded\nflexibility (P7), greater precision in defining the task (P8), better predictability of AI responses (P6), and greater adaptation\nto user needs (P13). Participants felt that Dynamic PRC gave them greater control over the AI. This control allowed\nparticipants to directly define their response needs to the AI (P4) and guide the AI to a useful response that aligned\nwith their preferences (P15). A few participants even likened the Dynamic PRC approach to programming and stated\nthat the ability to modify the JSON to directly edit and fine-tune the options gave them more control over the AI (P6,\nP7). Finally, P10 saw control as a way to provide autonomy while working with AI:"}, {"title": "6.4.3 Dynamic PRC provides guidance for Al steering interactions", "content": "Participants saw the Dynamic PRC system as a guide\nin interacting with AI by providing helpful, and relevant, options (P1, P2, P4, P6, P7, P9, P14). For example, P7 saw the\ngenerated options as a form of debugging the Al's response since it \"gives you some ideas around if you change [the\nresponse] in this way then you might get better answers.\u201d Participants felt that Dynamic PRC guided them towards better\nresponses (P7, P10, P11), giving alternatives of what they might do to solve their problem (P7). Dynamic PRC options\ndid this by helping participants review potential refinements to their prompt (P1, P4, P9) and give greater structure to\ntheir Al interactions through generated options (P14). P11 said it even helped them learn potential strategies for future\nprompts, saying that Dynamic PRC was \"giving me more of a prompt learning experience, and I'm getting out of [the AI]\nwhat I want. And actually, it's a better response.\u201d\nParticipants saw Dynamic PRC as valuable for providing AI responses that had better detail and results than other\nmethods like prompting or Static PRC gave (P1, P8, P11-13, P15), while requiring less focus on writing the prompt (P11),\nthough this came at a cost of requiring slightly more effort than using the Static PRC (P12). By providing an interface\nthat guided participants to explicitly provide their preferences, Dynamic PRC was seen as more efficient for helping\nsteer the AI. P1 explains:\n\"It's [Dynamic PRC is] all bumper rails at the bowling alley for getting to where you need to go, and you're\ntrying to direct the ball that is the conversation you're having with this interface. And having more options to\ndo that, I think, allows you to get there quicker.\""}, {"title": "6.4.4 Dynamic PRC enables greater exploration of and reflection on tasks", "content": "Dynamic PRC enabled our participants\ngreater exploration of the Al's affordances (P1) and helped them better reflect on their tasks (P9). The inline options\ngenerated by Dynamic PRC were perceived by participants as a reflection of the user's own assumptions placed in the\nprompt that they could then correct to steer the response which also revealed alternatives to considerations they were\nmaking (P9, P10). Other participants felt that the Dynamic PRC options provided a scaffold for their thought process on\nwhat they wanted to focus on for their explanation (P5, P7, P10, P11, P15) which was seen as helpful for users who\nare leveraging AI to complete tasks in an unfamiliar domain (P15, P16). The Dynamic PRC options were helpful for\ngetting users unstuck by providing alternatives (P8), and as a form of brainstorming with the AI (P16). P1 said that\nDynamic PRC helped them understand the possibilities of the AI model, since it generated options that \u201cwere things\nthat I would maybe assume the system couldn't handle or that it wouldn't be able to do.\u201d Participants felt that having\ncontrol through generated options enabled greater understanding of a task as it allowed users to craft responses to\ntheir personal preferences and learning needs (P6, P8)."}, {"title": "6.4.5 Dynamic PRC and Static PRC have mixed effects on perceived cognitive effort", "content": "On ease of use. Many participants thought that Dynamic PRC made their interactions with Al easier, by scaffolding\ntheir interactions programmatically (P6), giving users control over option generation through natural language (P4,\nP14), and by providing reuse of useful options for later use (P15). Several participants thought Dynamic PRC would even\ncause their Al usage to increase (P2, P4, P10, P13) since it lowered the effort needed form prompts (P4) and reduced time\nspent"}]}