{"title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "authors": ["Hojae Lee", "Junho Kim", "SangKeun Lee"], "abstract": "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown impressive emergent capabilities, showing their competence on a variety of reasoning tasks in the Natural Language Processing (NLP) landscape (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2023). One particularly interesting strategy for this approach is Chain-of-Thought (CoT) prompting, which elicits multi-step reasoning abilities of LLMs by explicitly generating intermediate reasoning steps for complex tasks (Wei et al., 2022b). However, such reasoning abilities have been shown to only manifest in language models (LMs) with over hundreds of billion parameters (Chung et al., 2022; Wei et al., 2022a), which require significant computational resources or expensive API calls, restricting their deployment on resource-limited scenarios.\nTo circumvent these deployment challenges, previous works (Ho et al., 2023; Li et al., 2023; Magister et al., 2023) have followed a knowledge distillation (KD) approach, reasoning distillation, which transfers the multi-step reasoning ability of LLMs to small LMs. The KD pipeline generally applies In-Context Learning (ICL) on the LLM teacher model to generate outputs (e.g., multi-step rationales) as distillation sets, and then utilizes them to fine-tune the student model. Previous studies have shown that reasoning distillation can significantly improve student performances and may even outperform their LLM teachers on specific tasks (Ho et al., 2023; Chen et al., 2023).\nHowever, previous approaches to reasoning distillation have two challenges arising from insufficient distillation sets generated by LLM teachers. First, as LLMs may not have access to task-specific data, the quality of the rationales for distillation can be low (e.g., only 58% accuracy on GPT-3.5 for StrategyQA). The low quality of LLM teacher rationales limits the number of reasoning rationales to only a small set of correct ones due to the exclusion of incorrect rationales that negatively affect student performances (Ho et al., 2023). Second, because accessibility of black-box LLM teachers is generally restricted, the student model cannot mimic the predictive behavior and knowledge from the soft labels (Hinton et al., 2015). Such oversights may lead to the student model being over-fitted on limited distillation sets from teacher models and undermine its generalization capabilities.\nTo address these challenges, we propose Mentor-KD, a novel reasoning distillation framework that effectively distills the multi-step reasoning capability of LLMs. Our core idea is to introduce a mentor, an intermediate-sized task-specific model, that"}, {"title": "2 Related Works", "content": "complements the LLM teacher's knowledge during reasoning distillation. To this end, we first fine-tune the mentor models on specific tasks and generate both CoT rationales and soft labels to augment distillation sets. By leveraging task-specific mentors whose power is concentrated toward a specific target ability, Mentor-KD effectively addresses two issues through training on more diverse rationales and intrinsic knowledge from soft labels.\nWe conduct extensive experiments on various types of complex reasoning tasks, including commonsense, arithmetic, logical, and symbolic reasoning tasks. The experimental results clearly demonstrate the superiority of our method over baselines leveraging knowledge only from LLMs. In addition, we verify that the mentor model can generate a substantial number of correct reasoning samples compared to other LLM baselines, highlighting the effectiveness of our method as means of data augmentation. Lastly, we demonstrate that our Mentor-KD significantly improves student performances in low-resource scenarios, indicating its cost-efficiency. In summary, the contributions of this paper include the following:\n\u2022 We propose Mentor-KD, a novel reasoning distillation framework, which improves the reasoning ability of small LMs considering the limitations of insufficient distillation sets from LLM teachers.\n\u2022 We introduce a mentor model to additionally generate both rationale samples and soft labels to complement the limited training datasets from the LLM teachers.\n\u2022 We demonstrate that Mentor-KD improves the effectiveness of reasoning distillation on students with various types of reasoning and models through extensive experiments."}, {"title": "2.1 Chain-of-Thought Prompting", "content": "CoT prompting is a method that elicits multi-step reasoning abilities of LMs through ICL (Wei et al., 2022b). The essence of CoT is that it acts as a guidance of logical progression for LMs to decompose and solve complex reasoning tasks (Xia et al., 2024). Consequently, it allowed LMs to excel in complex reasoning tasks (Kojima et al., 2022; Wang et al., 2023b; Zhang et al., 2023) which traditional few-shot learning methods have struggled with (Rae et al., 2021). Recent works take a step further to improve CoT prompting through enhancing the quality of reasoning steps. Madaan et al. (2023) had LMs to iteratively self-refine reasoning through self-feedback, while Gou et al. (2024) leveraged external tools for obtaining feedback. Trivedi et al. (2023); Zhao et al. (2023) incorporated information retrieval systems to enhance the facticity of LMs' reasoning.\nDespite the success, previous works (Hoffmann et al., 2022; Wei et al., 2022b; Chu et al., 2024) reported that the merits of reasoning on CoT prompting emerge when LMs are scaled to hundreds of billions of parameters. To address such problems, our work focuses on enabling CoT reasoning to small-scaled LMs through reasoning distillation."}, {"title": "2.2 Knowledge Distillation for LLMs", "content": "KD (Hinton et al., 2015) has been proven to be a promising approach to compress LMs by transferring the predictive behavior (e.g., soft labels) or internal knowledge (e.g., hidden representations) from larger LMs to smaller ones. However, existing KD methods for pre-trained LMs, which involve distilling the soft labels (Sanh et al., 2019; Gu et al., 2024) or representations (Wang et al., 2020, 2021;"}, {"title": "3 Methodology", "content": "We elaborate on the detailed implementations of our Mentor-KD. The core idea is to augment the distillation training set by leveraging a task-specific intermediate-sized mentor model. To this end, we first generate CoT annotations from LLM teacher models (Section 3.1). We then fine-tune the mentor model with the distillation set from the LLM teacher, and the trained mentor model generates additional training sets, including both rationales and soft labels (Section 3.2). By augmenting both signals from the mentor, we distill the knowledge to student models (Section 3.3). Figure 2 illustrates an overview of our framework."}, {"title": "3.1 Chain-of-Thought Annotations", "content": "We use the LLM to obtain CoT annotations composed of a rationale and a final prediction to a question via Zero-shot-CoT (Kojima et al., 2022). It is a two-staged strategy consisting of reasoning and answer extraction stages, and thus, we induce the LLM to generate a CoT rationale first and subsequently a final prediction afterwards.\nSpecifically, we first append \"Let's think step by step\" to the question and prompt the LLM to obtain"}, {"title": "3.2 Mentor Model", "content": "Here, we describe how our mentor models are trained to concentrate their powers to a specific task, and utilized to complement the insufficient distillation sets of LLM teachers.\nTraining. For training the mentor model, we directly fine-tune it on the previously constructed $D_{teacher}$. Specifically, the mentor model receives $q_i$ as an input, $l$ as a label, and is trained with a standard language modeling objective.\nRationale Augmentation. The trained mentor model is then used for train data augmentation. For data samples from $\\mathcal{D}$, we let the mentor model annotate step-by-step rationales, given $q_i$ as an input. The mentor in return generates a label $l_m$, which consists of a step-by-step rationale and a prediction of its own. We filter the annotations by the mentor identical to filtering the teacher's annotations and preserve data samples where $\\hat{y}_m = y_i$. Through this stage, we construct $D_{mentor} = \\{(q_i, l_m, y_i)\\}_{i=1}^N$ per dataset.\nWith annotations obtained from the teacher ($D_{teacher}$) and the mentor ($D_{mentor}$), we finally construct $D_{train}$ for training the student model, which"}, {"title": "3.3 Reasoning Distillation", "content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$,"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the step-by-step reasoning ability can be distilled through fine-tuning the student model with question-label pairs obtained from the teacher and the mentor. More specifically, the form of learning the multi-step reasoning ability through fine-tuning is defined as follows:\n$L_{rd} = \\mathbb{E}_{D_{train}} \\log P_f([q; r; y])$, \nwhere $f$ indicates the student model, and the square brackets indicate string concatenation.\nSoft Label Distillation. Leveraging the LLM teacher's internal knowledge can be impractical due to its black-box nature or enormous size. Instead, we employ our mentor model to provide the soft labels for distillation. The soft labels are obtained through a forward pass, followed by a softmax function, given $q$ as an input. Formally, we obtain the soft label (probability distribution) $p_k$ of the mentor and student models from the logit value $z_k$ at the $k$-th position through the following equation:\n$p_k = \\frac{\\exp (z_k / T)}{\\sum_j \\exp (z_j / T)},$"}, {"content": "For training the student model, we incorporate both fine-tuning (rationale distillation) and knowledge distillation through logit values obtainable via the mentor model (soft label distillation). This is to allow the student model to jointly 1) learn how to practice step-by-step reasoning in a symbolic manner (Ho et al., 2023; Li et al., 2023; Magister et al., 2023), as well as 2) mimic the predictive behavior of a larger model (Hinton et al., 2015). In correspondence, our training objective consists of two loss functions.\nRationale Distillation. Identical to training the mentor model, the"}]}