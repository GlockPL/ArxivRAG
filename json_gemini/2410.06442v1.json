{"title": "MAD-SCIENTIST: AI-BASED SCIENTIST SOLVING\nCONVECTION-DIFFUSION-REACTION EQUATIONS Us-\nING MASSIVE PINN-BASED PRIOR DATA", "authors": ["Mingu Kang", "Dongseok Lee", "Woojin Cho", "Jaehyeon Park", "Kookjin Lee", "Anthony Gruber", "Youngjoon Hong", "Noseong Park"], "abstract": "Large language models (LLMs), like ChatGPT, have shown that even trained with\nnoisy prior data, they can generalize effectively to new tasks through in-context\nlearning (ICL) and pre-training techniques. Motivated by this, we explore whether\na similar approach can be applied to scientific foundation models (SFMs). Our\nmethodology is structured as follows: (i) we collect low-cost physics-informed\nneural network (PINN)-based approximated prior data in the form of solutions\nto partial differential equations (PDEs) constructed through an arbitrary linear\ncombination of mathematical dictionaries; (ii) we utilize Transformer architec-\ntures with self and cross-attention mechanisms to predict PDE solutions without\nknowledge of the governing equations in a zero-shot setting; (iii) we provide ex-\nperimental evidence on the one-dimensional convection-diffusion-reaction equa-\ntion, which demonstrate that pre-training remains robust even with approximated\nprior data, with only marginal impacts on test accuracy. Notably, this finding\nopens the path to pre-training SFMs with realistic, low-cost data instead of (or in\nconjunction with) numerical high-cost data. These results support the conjecture\nthat SFMs can improve in a manner similar to LLMs, where fully cleaning the\nvast set of sentences crawled from the Internet is nearly impossible.", "sections": [{"title": "1 INTRODUCTION", "content": "In developing large-scale models, one fundamental challenge is the inherent noisiness of the data\nused for training. Whether dealing with natural language, scientific data, or other domains, large\ndatasets almost inevitably contain noise. Large language models (LLMs), such as ChatGPT, present\nan interesting paradox: despite being trained on noisy datasets, they consistently produce remarkably\nclean and coherent output. This observation raises an important question for the development of\nscientific foundation models (SFMs): Can an SFM, like an LLM, learn from noisy data and still\ngenerate accurate, dynamic results for complex scientific tasks?\nIn recent years, LLMs have revolutionized the field of natural language processing by introducing\nhighly flexible and scalable architectures (Brown et al., 2020; Kaplan et al., 2020; Touvron et al.,\n2023; Frieder et al., 2023; Chowdhery et al., 2023). Notably, the in-context learning (ICL) paradigm\nhas demonstrated powerful generalization capabilities, enabling LLMs to adapt to new tasks with-\nout explicit fine-tuning (Brown et al., 2020; Radford et al., 2019; Dai et al., 2023; Gruver et al.,\n2023). This success has motivated the application of such foundation models across a variety of do-\nmains (Xu et al., 2024; Xie et al., 2024; Yang et al., 2023a). Scientific machine learning (SciML) is\none such emerging domain which merges physics-based models with machine learning methodolo-\ngies (Raissi et al., 2019; Willard et al., 2022; Subramanian et al., 2023; Kim et al., 2024; 2023; Choi\net al., 2024). SciML aims to leverage the power of machine learning to solve complex scientific\nproblems, including those governed by partial differential equations (PDEs). Recent efforts in this\ndirection have led to the development of foundation models specifically designed for scientific tasks,\ncalled SFMs (Yang et al., 2023b; Xie et al., 2024; Yang et al., 2023a; Moor et al., 2023; Bodnar et al.,"}, {"title": "2 BACKGROUND", "content": "Consider a sequence of pairs (X1,Y1), (X2,Y2), . . ., each within the measurable space (X \u00d7 Y, A),\nwhere X represents the spatiotemporal coordinate, Y denotes the corresponding solution in this\npaper\u2019s context and A denotes the Borel \u03c3-algebra on the measurable space X \u00d7 Y. For simplicity,\nwe adopt this notation in this section. These pairs are drawn from a family of probability density\ndistributions {pq : q \u2208 Q}, commonly referred to as the statistical model, where Q represents\nthe parameter space equipped with a o-algebra B ensuring that the mappings q \u2192 pq(x, y) are\nmeasurable. The true underlying density function \u03c0is a member of Q, and the pairs (Xi, Yi) are\nsampled according to p\u3160. Lacking information about \u03c0, we adopt a Bayesian framework to establish\na prior distribution II which is defined as probability measure on (Q, B). Then we have, for any\nmeasurable set A \u2208 B,\n$\\Pi(A \\mid X, Y) = \\frac{\\int_{A}p_{q}(X, Y)d\\Pi(q)}{\\int_{Q}p_{q}(X, Y)d\\Pi(q)}.$                                                        (1)\nLet us adopt the notation pq = q. This prior is updated with the observed data to form the posterior\ndistribution, which is defined as\n$\\Pi(A \\mid D_{n}) = \\frac{\\int_{A}L_{n}(q)d\\Pi(q)}{\\int_{Q}L_{n}(q)d\\Pi(q)},$                                                    (2)\nwhere Ln(q) = \u03a0= (X,Y) for ACQ and Dn = {(Xi, Yi)}=1. The resulting posterior density\nis\n$q_{n}(X,Y \\mid D_{n}) = \\int_{Q} q(X, Y) d\\Pi(q \\mid D_{n}),$                                               (3)\nand the posterior predictive distribution (PPD) is formulated as\n$\\pi(y \\mid x, D_{n}) = \\int_{Q} q(y \\mid x) d\\Pi(q \\mid D_{n}).$                                                          (4)\nThe behavior of Dn plays a crucial role in this formulation. As noted by Walker (2004b;a); Blasi &\nWalker (2013); Walker (2003); Nagler (2023), for a well-behaved prior, the PPD converges toward\n\u03c0\u03b1\u03c2 \u03b7 increases. This aligns with findings in Blasi & Walker (2013), demonstrating that in well-\nspecified scenarios, strong consistency is achieved as\n$\\Pi_{n}(\\{q : H(\\pi, q) > \\epsilon\\}) \\rightarrow 0$ almost surely,                                                     (5)\nfor any \u20ac > 0, where In(A) = SadII(q | Dn) is the posterior measure and H is the Hellinger\ndistance defined by\n$H(p,q) = (\\int_{\\mathcal{X} \\times \\mathcal{Y}} (\\sqrt{p} - \\sqrt{q})^{2})^{1/2}.$"}, {"title": "3 METHODS", "content": "Suppose the dataset Dn = {(Xi, Ti, Yi)}_1 is independently and identically distributed (i.i.d.)\nand sampled from a distribution qa, where a is the parameter vector representing the coefficients\ngoverning the PDE dynamics, including convection, diffusion, and reaction terms. Specifically,\nYi ~ u(Xi, Ti | a) + noise, where the noise represents the difference between the PINN-predicted\nsolution \u0169(a) and the true solution u(a). The PPD of the solutions given the dataset can be expressed\nas\n$q(y \\mid x, t, D_{n}) = \\int_{Q} q_{\\alpha} (y \\mid x, t) d\\Pi(q_{\\alpha} \\mid D_{n}),$                                            (6)\nwhich represents the likelihood distribution of y given Dn, capturing the most probable solution\ndistribution for the given parameter a. In this work, we aim to predict the solution from Dn by\nminimizing the mean squared error (MSE) between the PPD-derived solution and the true solution,\neven in the presence of noise. This requires constructing a prior over the PDE solution space, which\nis detailed next.\nThe following one-dimensional convection-diffusion-reaction (CDR) equation\nis used for the benchmark PDE,\n$1D CDR: u_{t} + \\beta u_{x} - \\nu u_{xx} - \\rho f(u) = 0, x\\in [0, 2\\pi], t \\in [0, 1],$                               (7)\nwhere f: R \u2192 R is a reaction term such as Fisher, Allen-Cahn and Zeldovich. This equation\nconsists of three key terms with distinct properties: convective, diffusive, and reactive, making it an\nideal benchmark problem. It is commonly used in the PINN literature due to the diverse dynamics\nintroduced by its three parameters: \u03b2, \u03bd, and p, which include various failure modes (Krishnapriyan\net al., 2021). To our knowledge, however, our work is the first predicting all those different reaction\nterms with a single model.\nIn this paper, we use the following dictionary of CDR-related terms, incorporating a linear combi-\nnation of J nonlinear reaction terms, for generating prior data.\n$u_{t} = \\mathcal{N}(\\cdot), \\mathcal{N}(t, x, u, \\beta, \\nu, \\rho_{1}, \\cdots, \\rho_{J}) = -\\beta u_{x} + \\nu u_{xx} + \\sum_{j=1}^{J} \\rho_{j}f_{j}(u),$                             (8)\nwhere each f; represents specific reaction term. This expansion allows for the introduction of diverse\nreaction dynamics. One can solve CDR equations with numerical solvers. In this work, however,\nwe are interested in building low-cost PINN-based prior data. In the future, one may need to build\nprior data for not only CDR but also many other equations for which none of analytical/numerical\nsolutions are obtainable in a low-cost manner, e.g., Naiver-Stokes equations. We think our PINN-\nbased prior data will play a crucial role in such a case."}, {"title": "4 EXPERIMENTS", "content": "Our experiment section is divided into two phases: in the first phase, we conduct a focused study\nwith the basic reaction term, Fisher, to understand the base characteristics of SFMs, and in the\nsecond phase we conduct comprehensive studies with various reaction terms."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We compare our model with 2 baselines: Hyper-LR-PINN (Cho et al., 2023)\nand P2INN without fine tuning (Cho et al., 2024). Both models are parametrized PINNs designed to\nlearn parameterized PDEs. Hyper-LR-PINN emphasizes a low-rank architecture with a parameter\nhypernetwork, while P2INN focuses on a parameter-encoding scheme based on the latent space of\nthe parameterized PDEs.\nFollowing this, as shown in Figure 2, the model takes DUT ~ p(D) in training phase and DUT ~\np(U) in testing phase. In addition, the dataset DUT requires the prior \u0169, and D requires the solution\nu. For a fair comparison, we use D, T, and D as the training dataset for both Hyper-LR-PINN and\nP2INN. Notably, while Hyper-LR-PINN and P2INN do not rely on solution points during training\nand testing, our model operates without any knowledge of the governing equation N(\u00b7). This setup\nensures a valid and balanced comparison (Table 1). The additional comparison details are elaborated\nin Appendix C, providing further insights into the distinctions between these models.\nThe concrete flow of training phase is described in Algorithm at Appendix F."}, {"title": "4.2 FOCUSED STUDY TO BETTER UNDERSTAND SFMS' BASE CHARACTERISTICS", "content": "In this section, we employ six different dynamics derived from the 1D CDR equation with a Fisher\nreaction term, ut + Bux \u03bd\u03c5xx \u2013 \u03c1\u03c5(1 \u2013 u) = 0 (Appendix D). We begin with an in-depth study\nusing the Fisher reaction term, chosen for its simplicity among the reaction terms, which has been\nextensively studied in population dynamics (Al-Khaled, 2001). This allows us to better understand\nthe core characteristics of the SFM, facilitating a more effective analysis of the model's behavior."}, {"title": "4.2.1 TIME DOMAIN INTERPOLATION FOR SEEN PDE PARAMETERS WITH A NUMERICAL\nPRIOR", "content": "We first verify the ICL capability of Transformer with a numerical prior, i.e., \u0169(a) equals to\nthe solution u of the PDE ut = N(t, u, x,a), before we dive into using a PINN prior. For\neach equation, we set the parameter space \u03a9 with three different coefficient (\u03b2,\u03bd, \u03c1) range:\n([1, 5] \u2229 Z), ([1,10] \u2229 Z)m, and ([1,20] \u2229 Z)m, where m is the number of nonzero coefficients."}, {"title": "4.2.2 TIME DOMAIN INTERPOLATION FOR SEEN PDE PARAMETERS WITH A PINN-PRIORS", "content": "The Transformer has demonstrated strong ICL capabilities when trained with numerical priors. Our\nmain focus now is to determine if this same success can be achieved using a PINN-prior. As out-\nlined in Appendix C, our preliminary results show that the Transformer remains robust even when\nnumerical priors are subject to various types of noise. Building on this, we examine how the model\nperforms when mixing low-cost PINN-priors with numerical priors in different proportions, assess-\ning its stability and robustness when incorporating PINN-priors.\nSpecifically, we train the model using the convection, diffusion, and Fisher reaction equations with\ninteger coefficients ranging from 1 to 20. For each equation, we evaluate the model with a prior that\nis a mixture of PINN-prior and numerical prior in varying ratios: 0%, 20%, 40%, 60%, 80%, and\n100% PINN-priors. Table 3 indicates the absolute and relative errors L2 for each setup compared to\nthe baseline results in Section 4.2. Furthermore, the average L2 error of the PINN-prior, compared\nto the numerical solution, is presented to demonstrate the quality of the PINN-prior.\nAs a result, mixing PINN-priors with numerical priors does not significantly impact performance,\nas the L2 absolute and relative errors remain consistent with other baselines. This indicates that\na Transformer can maintain ICL capability even when trained with PINN-prior data. Also, this\nfinding confirms that the model can effectively infer solutions from limited observed data D, even\nin the presence of inaccurate PINN-priors."}, {"title": "4.2.3 TIME DOMAIN INTERPOLATION FOR UNSEEN PDE PARAMETERS", "content": "From this point, we train our model using only PINN-priors and further explore the base char-\nacteristics of SFMs'. In this section, we test our model with unseen parameters at convection,\ndiffusion, and reaction systems. For each system, the model is trained with [1, 20] \u2229 Z range coeffi-\ncients and tested with unseen coefficient 1.5, 2.5,\u2026\u2026\u2026,19.5 which is included in interval [1, 20] and\n20.5, 21.5, 22.5,\u2026\u2026\u2026, 30.5 which is not in range of [1, 20]. The L2 relative error measured for each\ncoefficient value is plotted in Figure 3, along with the baselines Hyper-LR-PINN and the non-fine-\ntuned P2INN.\nOver the trained coefficient range, our model effectively interpolates the coefficients \u03b2, \u03bd, and \u03c1,\nachieving performance comparable to that seen with known coefficients. Moreover, the model\ndemonstrates stable extrapolation in diffusion and reaction systems. Compared to the baselines,\nour model significantly outperforms it, particularly in diffusion and reaction systems. This result\nindicates that the Transformer can effectively learn the PPD of the prior space D, even without\nobserving the complete prior."}, {"title": "4.2.4 TIME DOMAIN EXTRAPOLATION FOR SEEN PDE PARAMETERS", "content": "One major limitation of the PINN is an extrapolation at the temporal domain that infer solutions at\nunknown points. Our model demonstrates extrapolation capability in the 1D convection equation,\nwhere the solution exhibits wave-like fluctuations in the inference region. In particular, the model\ntrained with the PINN-prior D over the coefficient range \u03b2\u2208 [1,20] \u2229 Z can predict \u1e9e values in\n1.5, 2.5, \u2026\u2026\u2026, 16.5 for equations where the test points T fall within t \u2208 (0.6, 1.0], even though D"}, {"title": "4.3 COMPREHENSIVE STUDY WITH VARIOUS REACTION TERMS", "content": "In this section, we expand the parameter space to following \u03a9 using three different reaction terms:\nFisher (f1), Allen-Cahn (f2), and Zeldovich (f3),\n$\\begin{aligned}\nu_{t} & = \\mathcal{N}(\\cdot), \\mathcal{N}(t, x, u, \\alpha) = -\\beta u_{x} + \\nu u_{x x}+\\sum_{j=1}^{3} \\rho_{j} f_{j}(u), \\\\\nf_{1} & :=u(1-u), f_{2}:=u(1-u^{2}), f_{3}:=u^{2}(1-u), \\\\\n\\Omega & =\\{\\alpha:=(\\beta, \\nu, \\rho_{1}, \\rho_{2}, \\rho_{3})\\} .\\end{aligned}$                                      (13)\nTo justify the expansion, we train the Transformer with \u03b2 = 0, \u03bd = 0, and pj \u2208 [1,5] \u2229 Z for\nj = 1,2,3 to evaluate its ICL capability in handling linear combinations of the reaction terms.\nThe model is then tasked with inferring the solutions of the PDEs ut = P1f1, Ut = P2f2, and\nut = p3 f3 to test whether it can generalize to unseen PDEs and accurately distinguish between each\ncomponent.\nAccording to the result at Table 4, the L2 absolute and relative errors are comparable to those ob-\ntained when trained with p\u2081 \u2208 [1, 5] \u2229Z, suggesting the potential for expanding the parameter space."}, {"title": "5 RELATED WORKS", "content": "In-context learning Transformers have shown remarkable ICL abilities across various studies.\nThey can generalize to unseen tasks by emulating Bayesian predictors (Panwar et al., 2024) and\nlinear models (Zhang et al., 2024), while also efficiently performing Bayesian inference through\nPrior-Data Fitted Networks (PFNs) (M\u00fcller et al., 2021). Their robustness extends to learning dif-\nferent classes of functions, such as linear and sparse linear functions, decision trees, and two-layer\nneural networks even under distribution shifts (Garg et al., 2022). Furthermore, Transformers can\nadaptively select algorithms based on input sequences, achieving near-optimal performance on tasks\nlike noisy linear models (Bai et al., 2023). They are also highly effective and fast for tabular data\nclassification (Hollmann et al., 2022).\nFoundation model Recent studies have advanced in-context operator learning and PDE solving\nthrough Transformer-based models. Ye et al. (2024) introduces PDEformer, a versatile model for\nsolving 1D PDEs with high accuracy and strong performance in inverse problems. In-context op-\nerator learning has also been extended to multi-modal frameworks, as seen in Yang et al. (2023c),\nwhere ICON-LM integrates natural language and equations to outperform traditional models. Addi-\ntionally, Yang & Osher (2024) and Yang et al. (2023b) demonstrate the generalization capabilities of\nIn-Context Operator Networks (ICON) in solving various PDE-related tasks, highlighting ICON's\nadaptability and potential for few-shot learning across different differential equation problems. Sev-\neral other studies have addressed the problem of solving various PDEs using a single trained model\n(Hang et al., 2024; Herde et al., 2024). However, many of these approaches rely on symbolic PDE\ninformation, true or near-true solutions and/or do not support zero-shot in-context learning, making\ntheir objectives different from ours."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this work, we presented MaD-Scientist for scientific machine learning that integrates in-context\nlearning and Bayesian inference for predicting PDE solutions. Our results demonstrate that Trans-\nformers, equipped with self-attention and cross-attention mechanisms, can effectively generalize\nfrom prior data, even in the presence of noise, and exhibit robust zero-shot learning capabilities.\nThese findings suggest that foundation models in SciML have the potential to follow the devel-\nopment trajectory similar to that of natural language processing foundation models, offering new\navenues for further exploration and advancement in the field.\nThe Transformer used in our study clearly demonstrates the ICL capability, when trained with PINN-\nbased prior. However, it is limited to the CDR equations in our paper. We will consider other types\nof PDE and more diverse initial and boundary conditions in the future, enhancing its adaptability to\nreal-world scenarios and its role as a foundation model."}, {"title": "F TRAINING ALGORITHM", "content": "We train the Transformer like following.\nAlgorithm 1 Training a Transformer\n1: Input: A prior dataset DUT drawn from prior p(D)\n2: Output: A Transformer \u1fb6e which can approximate the PPD\n3: Initialize the Transformer \u1fb6\u03b8\n4: for i = 1 to n do\n5: Sample \u03b1 \u0395 \u03a9 and DUT \u2286 \u0169(\u03b1) ~ p(D)\n6: (D := {((x,t)}={(x,t))}\u2081)\n7: Compute loss La =\n8: Update parameters @ with an Adam optimizer\n9: end for"}, {"title": "GICL OF TRANSFORMERS WITH NOISY PRIOR", "content": "In order to study the ICL capability of Transformers with noisy prior, we introduce four kinds of\nprior D like\nP1 (noiseless): p(D) = p(U),\nP2 (Gaussian noise) : p(D) ~ N (U, \u03c3\u00b2I),\n$\\text{P3 (salt-and-pepper noise) : p(D) \\sim p(s \\cdot U) where s = }\\begin{cases}\\min(U) \\text{ with probability } \\frac{\\gamma}{2}, \\\\\\\\max(U) \\text{ with probability } \\frac{\\gamma}{2}, \\\\\\1 \\text{ with probability 1 - } \\gamma,\\end{cases}$\nP4 (uniform noise) : p(D) ~ p(U + U(\u2212\u0454, \u0454)) (U: uniform distribution).\nWe sample DUT ~ p(D), where D is a noisy prior, and train the Transformer \u0169\u0473. We then test uo\nwith \u010eUT ~ p(U), demonstrating that the model can predict the true solution even when trained\non noisy prior data. The experiment is conducted on reaction and convection-diffusion-reaction\nequations, which outperform other baselines, under three different noises: the Gaussian noise (P2),\nthe salt-and-pepper noise (P3), and the uniform noise (P4). The standard deviation o of Gaussian\nnoise is set to 1%, 5%, and 10% of the mean value of the ground truth solution. Additionally, for\nthe experiment, the probe y for salt-and-pepper noise and the range e for uniform noise are also set\nto 1%, 5%, and 10%."}, {"title": "H EXPERIMENTS AT PINN FAILURE MODES", "content": "Referring to Cho et al. (2024) and Krishnapriyan et al. (2021), we test our method on PINN's major\nfailure modes: \u03b2\u2208 [30,40] with an initial condition 1 + sin(x) and \u03c1 \u2208 [1,10] with an initial\ncondition N (\u3160, ()\u00b2).\nWe have trained our model with this range with P1 prior and evaluate L2\nabsolute and relative errors. The following are major results and solution profiles at failure modes."}, {"title": "I HYPERPARAMETER LIST", "content": "The following hyperparameters were employed during the PINN-prior generation and the Trans-\nformer training process:"}]}