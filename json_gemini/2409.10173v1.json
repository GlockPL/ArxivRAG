{"title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA", "authors": ["Saba Sturua", "Isabelle Mohr", "Mohammad Kalim Akram", "Michael G\u00fcnther", "Bo Wang", "Markus Krimmel", "Feng Wang", "Georgios Mastrapas", "Andreas Koukounas", "Nan Wang", "Han Xiao"], "abstract": "We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LORA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Additionally, Matryoshka Representation Learning is integrated into the training process, allowing flexible truncation of embedding dimensions without compromising performance. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks.", "sections": [{"title": "1 Introduction", "content": "Text embedding models represent documents as high-dimensional vectors, converting semantic relationships between documents into spatial relationships between vectors. These models are fundamental to neural information retrieval and have been widely adopted across various domains of NLP and IR research and applications. Text embeddings are utilized in diverse downstream tasks such as classification, retrieval, and clustering. Notably, they have gained significant traction in building Retrieval-Augmented Generation (RAG) systems, where they serve as the primary technique in the retrieval step.\nA major limitation of traditional embedding models is that, despite being named as general-purpose, they often require fine-tuning for specific tasks [Jiao et al., 2020] and frequently struggle with common failure cases [Gao et al., 2021]. To address this, recent research has increasingly focused on leveraging large language models (LLMs) as the backbone for general-purpose embedding generation, capitalizing on their ability to efficiently handle multiple languages and tasks [Jiang et al., 2024]. However, with model sizes typically reaching 7 billion parameters, deploying these models in real-world applications poses significant challenges. Furthermore, the marginal improvements in evaluation metrics offered by LLM-based embeddings, compared to encoder-only embedding models, render them a less practical choice for many use cases.\nThis paper introduces jina-embeddings-v3, a novel text embedding model with 570 million parameters, optimized for multilingual data, long-context retrieval, and high performance across multiple tasks. Evaluation on the MTEB benchmark demonstrates that jina-embeddings-v3 not only significantly improves upon its predecessor, jina-embeddings-v2 [G\u00fcnther et al., 2023] and its bilingual variants [Mohr et al., 2024], but also outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while surpassing multilingual-e5-large-instruct across all multilingual tasks. Additionally, compared to LLM-based embeddings such as e5-mistral-7b-instruct, which has a parameter size of 7.1 billion (12x larger) and an output dimension of 4096 (4x larger) but offers only a 1% improvement on MTEB English tasks, jina-embeddings-v3 is a far more cost-efficient solution, making it more suitable for production and on-edge computing. The key contributions of this paper are:\n\u2022 Task-specific optimization with LoRA: We demonstrate that LoRA adapters [Hu et al., 2021] effectively generate task-specific embeddings, outperforming prior instruction-based approaches."}, {"title": "2 Related Work", "content": "In recent years, significant progress has been made in the field of text embeddings, largely driven by the emergence of transformer-based pre-trained language models that capture the underlying semantics of language effectively [Devlin et al., 2019]. However, these models are predominantly trained with a masked language modeling (MLM) objective, which is not optimal for generating high-quality text embeddings. To overcome this limitation, recent approaches have focused on fine-tuning and extending these models specifically for embedding tasks [Reimers and Gurevych, 2019].\nA key advancement in this area is the development of multi-stage and multi-task fine-tuning strategies that incorporate weakly-supervised contrastive training [Wang et al., 2022, G\u00fcnther et al., 2023, Mohr et al., 2024]. These methods improve the versatility of embeddings, enabling models to perform well across a diverse range of applications and tasks, as opposed to models trained solely on semantic textual similarity datasets.\nFurthermore, techniques such as AliBi [Press et al., 2022] and ROPE [Su et al., 2024] have enabled models like jina-embeddings-v2 [G\u00fcnther et al., 2023] to handle longer sequences, up to 8192 tokens, by replacing absolute positional encoding with relative encoding methods. To make embeddings more compact, Matryoshka Representational Learning (MRL) [Kusupati et al., 2022] enables the truncation of embeddings without compromising performance on downstream tasks by modifying the loss function used during training."}, {"title": "2.2 Multilingual Embedding Models", "content": "One of the earliest multilingual transformer models is Multilingual BERT (mBERT) [Devlin et al., 2019], trained on 104 languages. This was followed by XLM [Conneau and Lample, 2019] and XLM-ROBERTa (XLM-R) [Conneau et al., 2020], which utilized parallel data during training. Wang et al. [2024] extends this work by fine-tuning XLM-R on high-quality multilingual labeled datasets and applying knowledge distillation from a cross-encoder to further improve embedding quality. Similarly, Chen et al. [2024a] introduced BGE M3, another XLM-R-based model that supports longer sequences. The authors extended XLM-R's maximum sequence length to 8192 tokens, continued pre-training with the RetroMAE method [Xiao et al., 2022], and fine-tuned it contrastively using a novel multi-CLS pooling strategy. mGTE [Zhang et al., 2024] also builds on XLM-R, incorporating ROPE positional embeddings [Su et al., 2024].\nAnother approach leverages LLMs for multilingual embeddings [Zhang et al., 2023, Wang et al., 2023], benefiting from their extensive language support and diverse training data. However, LLMs are computationally inefficient due to their larger size, making them less practical for many applications. To address this, Lee et al. [2024a] generate and relabel training data to distill knowledge from LLMs into a compact encoder model, avoiding the need for direct fine-tuning of the larger LLMs."}, {"title": "2.3 Task-Specific Embedding Models", "content": "Previous research has highlighted limitations in training models to produce generic embedding vectors that perform well across various use cases and domains. For example, Wang et al. [2022] observed that in asymmetric retrieval tasks, such as question answering and typical information retrieval, models perform better by appending distinct prefixes to queries and documents before encoding. While the E5 models from this work employ a single prefix for all queries and another for all documents, Su et al. [2023] introduced more complex instructions to encode additional information about relevance in retrieval tasks and the domain of the data."}, {"title": "3 Model Architecture", "content": "The architecture of jina-embeddings-v3 is depicted in Figure 1. To implement the backbone architecture, we adapt the XLM-ROBERTa model with modifications that (1) enable effective encoding of long text sequences, (2) allow task-specific encoding of embeddings, and (3) increase model efficiency. jina-embeddings-v3 retains the original XLM-ROBERTa tokenizer.\nAs outlined in Table 1, jina-embeddings-v3 is larger than jina-embeddings-v2, but significantly smaller than embedding models fine-tuned from LLMs [Lee et al., 2024b, Wei et al., 2022]. Importantly, the LoRA adapters account for less than 3% of the total parameters, adding minimal overhead. To further enhance performance and reduce memory consumption, we utilize FlashAttention 2 [Dao], support activation checkpointing, and employ the DeepSpeed framework [Rasley et al., 2020] for efficient distributed training.\nTo handle long text sequences, we replace absolute positional embeddings with Rotary Position Embeddings (RoPE) [Su et al., 2024], which use a rotation matrix to encode absolute positions while embedding relative positional dependencies directly within the self-attention mechanism. We also experimented with extending positional encodings 1 as done in the BGE M3 model [Chen et al., 2024b], but observed poor performance on tasks involving long texts. This could be attributed to differences in training data and pooling strategies, as we trained primarily on short texts and used mean pooling instead of multi-CLS pooling.\nXiong et al. [2024] demonstrated that increasing the base frequency parameter of rotary positional embeddings enhances performance on long-text tasks, while Zhang et al. [2024] adjusted the rotary"}, {"title": "4 Training Method", "content": "We initialize the model using the weights of the original XLM-ROBERTa model. However, the model's original MLM objective is not fully aligned with our training objectives due to the changes in positional embedding methods. Despite this, we observe that initializing with pre-trained weights leads to faster convergence during pre-training compared to random initialization. Our training paradigm consists of three stages, as is common for training text embedding models:\nI Pre-Training: We perform standard MLM training using large multilingual text corpora. The model is initialized with XLM-ROBERTa weights to expedite pre-training and avoid training from scratch.\nII Fine-Tuning for Embedding Tasks: To learn how to encode a text passage into a single vector representation, we follow the approach outlined in [G\u00fcnther et al., 2023]. This method incorporates a pooling layer into the transformer model to aggregate token representations into"}, {"title": "4.2 Fine-Tuning for the Embedding Task", "content": "After pre-training, we fine-tune the model to encode a text sequence into a single vector representation. Following the Sentence-BERT approach [Reimers and Gurevych, 2019], we augment the model with a mean pooling layer to aggregate the semantics from all output token vectors into a single vector representation. The fine-tuning procedure follows Mohr et al. [2024], where the model is trained on text pairs using a bi-directional InfoNCE [van den Oord et al., 2018] loss, $L_{pairs}$:\n$L_{pairs}(B) := L_{NCE}(B) + L_{NCE}(B^{\\dagger})$\ndefined on a batch $B = ((p_1, q_1), ..., (p_k, q_k))$ of k pairs, and $B^{\\dagger} = ((q_1, p_1), ..., (q_k, p_k))$ (obtained from B by swapping the order of pairs). $L_{NCE}$ denotes the following loss function:\n$L_{NCE}(B) := \\frac{1}{k} \\sum_{i=1}^{k} -ln(\\frac{e^{s(x_i,y_i)/\\tau}}{\\sum_{y_i' \\in B}e^{s(x_i,y_i')/\\tau}})$\nThe training data consists of over one billion text pairs, drawn from more than 300 distinct sub-datasets, each representing specific domains in various languages. During training, the data loader constructs each batch by sampling a specific sub-dataset, ensuring that only text pairs from that dataset are included.\nFor data preparation, we follow the same methodology as our previous work [Mohr et al., 2024], with an additional filtering step. This filter removes pairs where at least 80% of the words (minimum of four) in the shorter text are substrings of the longer text. This filtering step increases the difficulty of the training and encourages the model to focus less on syntactic overlap.\nAs in the pre-training phase, we begin training on short text pairs, followed by further training on longer texts using a larger sequence length but a reduced batch size. In this phase, we use only a subset of the datasets containing sufficiently long texts."}, {"title": "4.3 Training Task-Specific Adapters", "content": "Related work on embedding training [Wang et al., 2022, Xiao et al., 2023] introduces an additional generic training phase following the pair-wise training phase. This phase incorporates high-quality data from various tasks to optimize model performance across a range of downstream use cases. In this stage, recent approaches use task-specific instructions to help the model distinguish between different tasks and domains, as discussed in Section 2.3.\nHowever, this approach complicates model usage, as users must learn task-specific instructions (i.e., prompts) that align with the model's behavior or \"vibe.\" While this offers flexibility, it also makes the model's behavior harder to predict. In contrast, we train five distinct LoRA adapters for four well-defined task types: classification, text matching, asymmetric retrieval (e.g. query-passage and query-document retrieval), and separation (e.g. clustering and reranking). These tasks are trained independently, with the base model's weights kept frozen. For asymmetric retrieval, two adapters are trained jointly: one for queries and one for passages. During inference, users can select the appropriate adapter based on their downstream task and input role, ensuring optimal embeddings for their specific use case."}, {"title": "4.3.1 Classification Adapter", "content": "The classification adapter generates embeddings that are effective for training downstream classification models, such as logistic regression classifiers. To train the adapter, we employ the classification training method proposed for the Gecko embedding model [Lee et al., 2024a]. Specifically, we select datasets covering a range of common classification tasks, including sentiment analysis, intent classification, and article categorization.\nFrom each dataset, we construct tuples consisting of two text values from the same class (q, p) and seven text values from different classes (n1,..., n7), resulting in a tuple of nine text values (q, p, n1, ..., \u03b77). The model is trained to assign a high cosine similarity to the embeddings of q and p, while enforcing low cosine similarity between (q, ni). Each batch is composed of tuples sampled from a single dataset.\nAn extended version of the InfoNCE loss $L_{triplet}$ from our previous work incorporates these additional negative samples:"}, {"title": "4.3.2 Text Matching Adapter", "content": "This adapter is trained to produce embeddings that quantify the similarity between two text values. It is applicable for tasks such as semantic textual similarity (STS) and retrieval tasks where there is a clear distinction between query and target text values. An example of such a retrieval task is duplicate detection, where text values from a corpus are compared against each other. In these cases, the \"query\" and \"corpus\" texts are treated symmetrically.\nTo train this adapter, we use the CoSent loss function: $L_{co2}$, as previously employed by Li and Li [2024]:\n$L_{co}(B) := ln(1 + \\sum_{\\zeta (q_1,p_1), \\zeta (q_2,p_2) \\in B : \\zeta (q_1,p_1) > \\zeta (q_2,p_2)}e^{\\frac{s(q_2,p_2)-s(q_1,p_1)}{\\tau}})$,\nwhere \u03b6(q1, p1) > \u03b6(q2, p2).\nThe CoSent loss operates on two pairs of text values, (q1, p1) and (q2, p2), which are constructed from the batch by selecting combinations of four text values where the ground truth similarity is provided in the training dataset, and $(91,P1) is greater than $(q2, P2)."}, {"title": "4.3.3 Asymmetric Retrieval Adapters", "content": "As discussed in Section 2.3, asymmetric retrieval tasks, such as question answering and traditional information retrieval, perform better with distinct encoding mechanisms for queries and documents. In this work, we follow the method proposed by Wang et al. [2022], using two distinct prefixes, but further separate the encoding processes by employing two specialized adapters, which are trained jointly. A detailed ablation study demonstrating the effectiveness of this approach is presented in Section 5.5.2.\nSimilar to previous works [Wang et al., 2022, Li et al., 2023, G\u00fcnther et al., 2023], we use datasets containing hard negatives, such as MS-MARCO [Bajaj et al., 2016] and Natural Questions (NQ) [Kwiatkowski et al., 2019], to train the model to focus on subtle distinctions and to differentiate between relevant and similar but irrelevant documents. For retrieval training datasets without annotated negatives, we apply hard negative mining as outlined in [Ren et al., 2021, Wang et al.,"}, {"title": "4.3.4 Failure Analysis for Asymmetric Retrieval", "content": "Since our jina-embeddings-v2 models were trained on similar data to jina-embeddings-v3, we conducted a failure analysis to identify issues common to models trained on these datasets. From this analysis, we identified the following points affecting retrieval tasks:\nF1. Misleading Syntactic Similarities: Documents with high syntactic similarity to the query are often favored over gold/relevant documents with lower syntactic overlap.\nF2. Misinterpretation of Named Entities: Named entities are frequently not recognized as such, leading to documents being marked as relevant based on partial matches (e.g., \"Sofia Albert\" vs. \"Albert Stone\"). This occurs especially with proper nouns that have alternative, more common meanings (e.g., the novel title \"The Company\" vs. \"the company\").\nF3. No Understanding of Polar Questions: Complex yes-no (polar) questions are not handled effectively. As a result, the model retrieves documents with related content that do not necessarily answer the query.\nF4. Preference for Low-Quality Documents: jina-embeddings-v2 and many other embedding models do not account for document quality, focusing solely on similarity and relevance. Consequently, low-quality documents (short, repetitive, or uninformative) that mention query terms are often retrieved but do not provide satisfactory answers.\nTo mitigate F1-F3, we crafted prompts to generate synthetic text examples targeting these specific failure cases. Each example consists of a query text, a preferred answer, and seven negative examples modeling the failure case.\nFor F4, we leveraged two preference learning"}, {"title": "4.3.5 Separation Adapter", "content": "The separation adapter is designed to perform well on clustering and reranking tasks. It is trained to distinguish between text values belonging to the same group and those from different groups. For reranking tasks, the adapter separates relevant from irrelevant documents based on a query's information need. In clustering tasks, groups of text values are provided, and after calculating the embeddings and applying a clustering algorithm (e.g., k-means), the resulting clusters should reflect the correct groupings.\nTo train the adapter for this objective, we employ a variant of the CoSent loss $L_{co}$, introduced in Equation (4). The training data consists of batches B' made up of tuples (x,l) \u2208 B', where x is a text value and l is its label. To form a batch of text pairs compatible with $L_{co}$, we generate all pairwise combinations of text values that share the same label li in the batch. The separation loss is then defined as follows:\n$L_{sep}(B') := L_{co}(B)$\n$B = \\{(x_i, x_j)|\\exists l : (x_i, l), (x_j, l) \\in B'\\}$"}, {"title": "5 Evaluation", "content": "In this section, we evaluate the performance of our model at various stages and conduct ablation studies on key architectural modifications. We begin by assessing the multilingual backbone model on a small subset of MTEB tasks in Section 5.1.\nNext, we present a comprehensive evaluation of embedding tasks in Section 5.2, where our model is tested on a variety of MTEB tasks, both monolingual (English) and multilingual. Section 5.3 reports the model's performance on the LongEmbed MTEB evaluation, followed by an analysis of previously identified retrieval failure cases in Section 5.4. Lastly, Section 5.5 presents the ablation studies conducted on MRL and the retrieval adapter."}, {"title": "5.1 Performance of Jina-XLM-ROBERTa", "content": "We evaluate the Jina-XLM-ROBERTa model on a subset of English and multi-/cross-lingual MTEB tasks, conducting a comparative analysis against established multilingual models, specifically mBERT [Devlin et al., 2019] and XLM-ROBERTa [Conneau et al., 2020], which are widely used as backbones for multilingual embedding"}, {"title": "5.2 Performance on MTEB", "content": "Table 3 summarizes the performance of various multilingual text embedding models across different MTEB tasks, divided into English and multilingual sections. jina-embeddings-v3 performs competitively, particularly in monolingual English tasks, where it achieves the highest Classification Accuracy (CF) score of 82.58 and the top Sentence Similarity (STS) score of 85.8, demonstrating its robustness across both languages and tasks. Full evaluation results per task can be found in Appendix A3. When averaging across all tasks, jina-embeddings-v3 scores 65.60, outperforming models such as text-embedding-3-large, multilingual-e5-large-instruct, and Cohere-embed-multilingual-v3.0. This indicates that jina-embeddings-v3 maintains strong monolingual English performance while being trained on a wide variety of languages.\nConsulting the English MTEB leaderboard, it is noteworthy that embedding models built on LLMs perform only marginally better than jina-embeddings-v3, but come with increased complexity in real-world applications. For instance, e5-mistral-7b-instruct achieves an average score of 66.63 across all 56 English MTEB tasks (approximately 1.03% higher than jina-embeddings-v3), but produces embeddings with a dimension of 4096 and has a parameter size of 7.1 billion. In contrast, jina-embeddings-v3 has an embedding dimension of 1024 (with the option to reduce dimensions using MRL with a small performance trade-off, as discussed in Section 5.5.1) and only 570 million parameters, making it far more practical for downstream applications.\nFor multilingual performance,"}, {"title": "5.3 Performance on LongEmbed MTEB", "content": "We evaluate our model against text-embedding-3-large, bge-m3, and our previously released model suite jina-embeddings-v2 on six long document retrieval tasks from the MTEB leaderboard. The results, presented in Table 4, demonstrate that jina-embeddings-v3 with the text-matching adapter achieves the highest average performance. These findings underscore the effectiveness of the RoPE-based positional embeddings, outperforming both the fixed positional embeddings used by bge-m3 and the ALiBi-based approach employed in jina-embeddings-v2."}, {"title": "5.4 Retrieval Failures", "content": "We conducted an analysis of retrieval failures typically observed when applying embedding models to retrieval tasks. This led to the identification of the four failure cases described in Section 4.3.3. To assess whether training our retrieval adapter using synthetic and preference learning datasets mitigates these failures, we performed two quantitative evaluations.\nThe first experiment evaluated whether failure cases in existing retrieval benchmarks, such as HotpotQA [Yang et al., 2018] and NaturalQuestions [Kwiatkowski et al., 2019], were resolved. These examples consist of a query, a relevant document, and a less relevant document that is often assigned a higher retrieval score. Table 5 presents the mean average precision (mAP) scores, showing that our model, after training with the"}, {"title": "6 Conclusion", "content": "In this paper, we introduced jina-embeddings-v3, our latest text embedding model. By leveraging task-specific adapter tuning and failure-aware synthetic data augmentation on top of a robust backbone, jina-embeddings-v3 demonstrates competitive performance across a wide range of tasks. Extensive evaluations on both English and multilingual datasets highlight the model's strong performance while maintaining a reasonable parameter size.\nFor future work, we plan to focus on enhancing the model's performance for low-resource languages, further strengthening its capabilities in multilingual tasks where data availability is limited."}]}