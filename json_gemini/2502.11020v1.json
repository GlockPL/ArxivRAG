{"title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages", "authors": ["Jafar Isbarov", "Arofat Akhundjanova", "Mammad Hajili", "Kavsar Huseynova", "Dmitry Gaynullin", "Anar Rzayev", "Osman Tursun", "Ilshat Saetov", "Rinat Kharisov", "Saule Belginova", "Ariana Kenbayeva", "Amina Alisheva", "Aizirek Turdubaeva", "Abdullatif K\u00f6ksal", "Samir Rustamov", "Duygu Ataman"], "abstract": "Being able to thoroughly assess massive multi- task language understanding (MMLU) capa- bilities is essential for advancing the applica- bility of multilingual language models. How- ever, preparing such benchmarks in high qual- ity native language is often costly and there- fore limits the representativeness of evaluation datasets. While recent efforts focused on build- ing more inclusive MMLU benchmarks, these are conventionally built using machine trans- lation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark espe- cially in the under-represented Turkic language family with distinct morphosyntactic and cul- tural characteristics. We propose two bench- marks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level ques- tions spanning 11 academic subjects in Azerbai- jani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Us- ing this dataset, we systematically evaluate a diverse range of open and proprietary multilin- gual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we re- lease TUMLU-mini and all corresponding eval- uation scripts.", "sections": [{"title": "1 Introduction", "content": "Language understanding encompasses a system's ability to interpret and derive meaning from hu- man language, incorporating syntax, semantics, and context. Evaluating language models hinges on this capability, as it ensures coherence, con- textual relevance, and accuracy. Benchmarking is integral to assessing these models, particularly with the rapid advancements in Large Language Models (LLMs), which now support multiple lan- guages and excel in complex rea- soning tasks such as mathematical, scientific, and coding-related inquiries. However, the scarcity of robust nat- ural language understanding (NLU) benchmarks capturing diverse linguistic and cultural contexts remains a challenge. Notably, LLM performance declines in low-resource languages, which are of- ten underrepresented in training data, highlighting the need for more inclusive evaluation frameworks. The majority of benchmarks included in top leaderboards where cutting-edge LLMs are evalu- ated are majorly prepared in English. In order to extend the applicabil- ity of LLM evaluation in more languages, recent efforts were undertaken to build more multilin- gual NLU benchmarks , however, most of these either cover a limited set of high-resourced languages, or the multilingual examples are generated by translating original examples from Western-centric languages, thus failing to capture cultural nuances inherent in different languages. Due to the multi-dimensional nature of the reason- ing task, language-specific benchmarks especially when translated into other languages also fail to represent the actual usage as well as demonstrating reasoning in the native language., and may fur- ther introduce issues such as translationese and cultural misalignment. On the other end of the spectrum, there are efforts to bridge that gap for a particular language, for example, African lan- guages , Arabic , Chinese , and Turkish . In this paper, we focus on building a truly representative and inclusive single-language fam- ily benchmark to address previous problems and provide a challenging setting for LLM evalua- tion. TUMLU (Turkic Unified Multilingual Lan- guage Understanding) benchmark covers the fol- lowing languages: Azerbaijani, Crimean Tatar, Turkish, Uyghur, Uzbek, Karakalpak, Kazakh, and Tatar. The dataset consists of 4-choice questions at middle- and high-school levels. It consists of 38139 questions across 8 languages and 11 sub- jects (see Figure 1 for a higher-level breakdown across languages). It is the first such benchmark to include Uyghur, Karakalpak, Tatar, or Crimean Tatar. It is also a significant improvement over existing benchmarks for Azerbaijani, Uzbek, and Kazakh. Turkish dataset is TurkishMMLU, which was a separate project . The benchmark is also representative in terms of dif- ferent scripts by including questions and answers in chosen languages in Latin, Cyrillic, and Arabic scripts. These datasets are transliterated such that it could be possible to get a dual dataset with the same content in two scripts for further comparative studies. We use these dual datasets to compare the performance of LLMs across different scripts. We also release a more balanced and manually verified version of the dataset called TUMLU-mini, which contains 100 questions per subject (unless there are less than 100 for the said subject in a particular language). We use this version to test SOTA open-source and proprietary models of vari- ous sizes. We evaluated them in two settings: few- shot and chain-of-thought (CoT) reasoning . Our initial results show that pro- prietary models remain the best option for Turkic languages."}, {"title": "2 Related Work", "content": "Language understanding benchmarks Multi- task language understanding evaluation bench- marks play an important role in the evaluation of LLMs. Early benchmarks concentrated on general natural language understanding. GLUE and SuperGLUE were two such benchmarks that were widely adopted by the research community. These bench- marks were saturated quickly, due to the develop- ment of better LLMs. However, LLMs struggled more against benchmarks that required knowledge and reasoning. MMLU and MMLU-Pro were more challenging since they required not only language understanding but also world knowledge. These general-purpose benchmarks gradually gave way to higher-level and more specialized benchmarks such as MATH, GPQA, and MUSR. Multilingual benchmarks The development of multilingual LLMs also necessitated challenging multilingual benchmarks. Most of these bench- marks were developed through machine translation . However, such datasets have been shown to contain cultural biases and translation artifacts. Global MMLU relied on machine and professional translation to . INCLUDE consists of native data , but it is imbalanced, with different subject distributions in different languages. There is also a significant difference in required knowledge levels between languages, making a direct comparison impossible. Benchmarks for Turkic languages SeaEval was one of the first LLM benchmarks to include Turk- ish . Global MMLU con- tains Kyrgyz and Turkish subsets. INCLUDE contains Azerbaijani and Kazakh. MRL 2024 Shared Task on Multi-lingual Multi-task Informa- tion Retrieval contains an Azerbaijani dataset, but it contains general lan- guage understanding tasks rather than world knowl- edge. Karde\u015f-NLU has introduced a multilingual language understanding benchmark. But again, this benchmark contains general language understanding tasks that require no world knowledge. There are also monolingual bench-"}, {"title": "3 TUMLU", "content": "TUMLU is a multilingual and multitask dataset containing 38139 multiple-choice questions across 8 languages and 11 subjects. All questions are at middle or high school level. The majority are sample or official questions for university entrance exams of respective countries.\nData collection Data was collected from publicly available books and websites. In original form, questions had 2 to 5 choices. In cases where more than 4 choices were available, we removed an in- correct choice. If less than 4 choices were available, we left the question as-is. Except for Language and Literature questions in Crimean Tatar, all questions"}, {"title": "4 Experimental set-up", "content": "Data Previous work has shown that 100 questions per subject are enough to estimate the performance of a larger dataset. Therefore, we run all experiments on TUMLU- mini. While we have performed the experiments and publicly released the data, results on the follow- ing subjects are not reported in the paper: Logic, Philosophy, Religion & Ethics, and Human & So- ciety. These subjects are available in one or two languages only, which makes any generalization impossible.\nModel choice We have used TUMLU to evalu- ate both open-source models, such as Llama 3.1 , Gemma 2 , Qwen2.5 and proprietary models, such as Gemini 1.5 , Claude 3.5 , GPT-40 . The size of selected open-source models varies between 7B and 70B. We do not have this in- formation on proprietary models. This list includes models from the same series, such as Qwen2.5 7B instruct and Qwen2.5 70B instruct , which allows us to observe the effect of scal- ing on multilingual performance. All open-source mod- els are instruct-tuned versions. We have omitted this information in the tables to preserve space. Wherever applicable, we have included the perfor- mance of Claude 3.5 Sonnet in the paper, since it consistently outperforms all other models. The performance of the remaining models can be found in the appendices C and D.\nPrompting We have run experiments in two set- tings: 5-shot, where we provide 5 example ques- tions and answers on the same subject before ask- ing a question , and 5-shot CoT, where we provide 5 example questions and expla- nations of their answers before asking the question . Few-shot and CoT prompt sam- ples are available in Appendix A. Previous work has demonstrated that providing the prompt in English does not result in performance gains. Due to this, we provide all prompts in respective native languages.\nTechnical details We run our experiments through OpenAI API, Anthropic API, Google Cloud Gemini API, Together AI API, and Deep Infra API. No model was run on a local machine. We used the following hyperparameters with all APIS: TEMPERATURE = 0.0, MAX_TOKENS = 1024, TOP_P = 1.0."}, {"title": "5 Results", "content": "In this section, we present the few-shot and CoT performance of selected models on the TUMLU- mini dataset. We also present an analysis of output language. Lastly, we explore how well LLMs per- form on the same questions written in different (Latin, Cyrillic, or Arabic) scripts.\n5-shot results We present the average perfor- mance of all models in each language in Table 2. Claude 3.5 Sonnet outperforms other models in all languages. The top 5 spots belong to pro- prietary models, although it has to be noted that there are larger open-source models that have not been included in this benchmark. Among the avail- able open-source models, Qwen2.5 72B Instruct has the best performance. Results also confirm the scaling hypothesis: Llama 3.1 70B significantly outperforms Llama 3.1 8B. The same applies to Qwen2.5 7B/72B and Gemma 2 9B/27B. We can also observe a significant improvement from Llama 3.1 70B to Llama 3.3 70B. While it is not possible to directly compare results across languages, we can observe that low-resource languages, such as Crimean Tatar, Karakalpak, and Uyghur have com- parable performance to middle- and high-resource languages. Notably, this trend holds even with the lowest-performing models.\nWe present the 5-shot evaluation of Claude 3.5 Sonnet in more detail in Table 3. In most languages, Native Language & Literature is the most challeng- ing subject for Claude 3.5 Sonnet. This holds for other models, as well (See Appendix C).\n5-shot CoT results We present the average re- sults of the 5-shot CoT evaluation in Table 4. COT prompts have an overall positive effect on perfor- mance. Sporadic negative effects can be explained by incorrect output format, rather than incorrect answers. We avoided manual validation of the output and instead relied on generalizable pattern- matching methods.\nTable 5 shows the performance of Claude 3.5 Sonnet on each subject and language. On average, CoT prompts have a net positive effect in each subject and each language.\nGenerated language vs. performance Bench- mark results demonstrate that LLMs can have sig- nificant language understanding capabilities even in previously unseen languages, such as Crimean Tatar. This can be explained by linguistic proximity to languages better represented in the training data. Even though LLMs perform surprisingly well in these languages with simple 5-shot prompts. The results are less impressive when we analyze the generated text quality. While quality per se is hard"}, {"title": "6 Conclusion", "content": "We introduce TUMLU, a unified and native lan- guage understanding benchmark for Turkic lan- guages. It contains 38139 multiple-choice ques- tions in 8 languages and 11 subjects. Latin, Cyril- lic, and Arabic scripts are represented in the bench- mark. Uzbek, Crimean Tatar, and Kazakh are avail- able in both Cyrillic and Latin. Uyghur is available both in Arabic and Latin. We also release TUMLU- mini, a smaller, more balanced and manually ver- ified version that is more suitable for large-scale experiments. We use TUMLU-mini to benchmark 5 proprietary and 7 open-source LLMs. Results show that LLMs have a reasonably good under- standing of all 8 languages, including ones that are not explicitly included in the training data of LLMs. However, LLMs are less capable of gen- erating text in these languages, usually answering multiple-choice questions correctly, but in another, similar high-resource language."}, {"title": "7 Limitations", "content": "TUMLU benchmark has two main limitations.\nMismatched difficulty levels Native language & literature subset contained both literature and lan- guage questions in some languages, while it con- tained only language questions in others. Similarly, the history subset contained both world and na- tional history questions in some languages, while it contained only national questions in others. Maths questions in Kazakh are at middle-school level, which results in very high scores.\nMissing major languages TUMLU covers 8 Tur- kic languages with more than 180 million native speakers. However, some major Turkic languages, such as Turkmen, Kyrgyz and Bashkir are not in- cluded. We are hoping to extend our benchmark with more languages in further editions."}]}