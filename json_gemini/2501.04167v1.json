{"title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Weize Kong", "Tao Chen", "Zhuowan Li", "Michael Bendersky", "Hamed Zamani"], "abstract": "Personalized text generation requires a unique ability of large language models (LLMs) to learn from context that they often do not encounter during their standard training. One way to encourage LLMs to better use personalized context for generating outputs that better align with the user's expectations is to instruct them to reason over the user's past preferences, background knowledge, or writing style. To achieve this, we propose Reasoning-Enhanced Self-Training for Personalized Text Generation (REST-PG), a framework that trains LLMs to reason over personal data during response generation. REST-PG first generates reasoning paths to train the LLM's reasoning abilities and then employs Expectation-Maximization Reinforced Self-Training to iteratively train the LLM based on its own high-reward outputs. We evaluate REST-PG on the LongLaMP benchmark, consisting of four diverse personalized long-form text generation tasks. Our experiments demonstrate that REST-PG achieves significant improvements over state-of-the-art baselines, with an average relative performance gain of 14.5% on the benchmark.", "sections": [{"title": "1 Introduction", "content": "Personalizing large language models (LLMs) emerges as a critical topic in natural language processing (Salemi et al., 2024b; Kumar et al., 2024), due to its wide-ranging applications in recommender systems (Hua et al., 2023; Chen, 2023), virtual assistants (Li et al., 2024b; Kocaballi et al., 2019), and content generation (Alhafni et al., 2024). The importance of personalization in such systems stems from the fact that they provide targeted content to their users, which enhances user satisfaction, improves engagement, and increases efficiency.\nAugmenting the input context of the LLMs with retrieved personalized context alongside the user prompt has proven effective in tailoring responses to individual users (Salemi et al., 2024b,a). However, defining the notion of relevance, a prerequisite for retrieving personalized context, is challenging (Salemi et al., 2024a). In personalization, a part of the user's context that appears not directly \"relevant\" to the prompt might be more useful (than a directly relevant one) if it better reflects the user's implicit preferences. For example, a sentence like \u201cI have two children of age 3 and 4...\u201d in the user context does not seem directly relevant to the prompt \u201cGive some suggestions about brands of room heaters.\u201d However, this knowledge indicates that the user could be concerned about safety for children and therefore would expect the model to consider this in its response. Establishing such an \"implicit\" relevance requires reasoning beyond the words or semantics of the user context, just like the user themselves does. We argue that an approach for encouraging an LLM to better use personalized context is also asking it to reason over it prior to generating the final response. For instance, the model may summarize the user's writing style, interests, background knowledge, and preferences before actually responding to the user prompt. However, it is often infeasible or costly to obtain sufficient human reasoning paths to train an LLM for personalized reasoning.\nThis paper addresses these challenges by introducing Reasoning-Enhanced Self-Training for Personalized Text Generation (REST-PG), a multi-stage framework designed to teach LLMs reasoning over personalized context through reinforced self-training. As an alternative to human reasoning paths, REST-PG uses an LLM to generate the reasoning steps considering the input, expected output, and personalized context. These generated reasoning paths are then used to train the LLM, through supervised fine-tuning, to produce both the reasoning steps and the final response in a single inference path. Nevertheless, we find that supervised"}, {"title": "2 Problem Formulation", "content": "This paper addresses personalized text generation, a task that uses user-specific information to tailor responses to individual users. A general LLM $M_\\Theta$ generates a piece of text in response to an input prompt x from a user u, denoted as $\\hat{y} = M_\\Theta(x)$. To personalize an LLM for the user u, we assume each prompt x from the user, with the expected output y, is accompanied by the user profile $P_u = \\{d_{(u,i)}\\}_{i=1}^{|P_u|}$, consisting of unstructured information pieces about the user u. Accordingly, we assume access to training and evaluation data"}, {"title": "3 REST-PG", "content": "LLMs have proven effective in learning from their context (Wei et al., 2022; Brown et al., 2020), making the augmentation of their input with personalized context an effective strategy for personalizing their responses (Salemi et al., 2024b; Salemi and Zamani, 2024). However, learning to personalize from context requires a specialized form of context-based learning, as it involves not only understanding task-relevant information but also inferring user-specific preferences. For instance, a sentence in the personalized context that is seemingly irrelevant to the user prompt could indicate implicit preference, like mentioning children could imply prioritizing safety. Teaching LLMs to recognize this nuanced notion of relevance is crucial for improving personalized text generation. One approach to do this is to instruct LLMs to reason over the personalized context by generating a summary of the user's preferences before responding to the prompt. However, collecting training data for this is challenging, as human annotations are costly and often fail to accurately capture the nuances of individual user preferences. To address this, LLMs can be used to generate reasoning paths based on the personalized context, input prompt, and expected output to creating reasoning training"}, {"title": "3.1 Enhancing Personalization by Reasoning", "content": "Current state-of-the-art methods for personalizing LLMs augment the input with a personalized context (often retrieved from a personal corpus) (Salemi et al., 2024b,a; Kumar et al., 2024). We argue that effectively utilizing personalized context necessitates a specialized form of context-based learning, as it requires understanding both task-relevant information and user-specific preferences-an aspect that LLMs are rarely exposed to during standard training. One way to encourage LLMs to better utilize personalized context is to instruct them to focus on user-specific elements such as preferences, interests, background knowledge, and writing style that are present in the personalized context. Incorporating these attributes from the personalized context enables the model to generate more aligned, user-specific responses. These attributes can be inferred by the LLM through reasoning over the personalized context, enabling it to interpret the user's preferences, interests, knowledge, and writing style before generating the final personalized response to the user's prompt. This reasoning step helps the model produce more accurate and personalized outputs.\nTo generate the necessary data for training the LLM to perform such reasoning steps, we introduce a semi-supervised data generation method tailored for this purpose. In this method, for a given input x for user u, the user profile Pu, and the expected output y, we use an LLM\u00b9 to generate a summary of user's preferences, interests, background knowledge, and writing style features tailored to the given input and corresponding output from the user context. The detailed prompt is presented in Figure 7 in Appendix C. This prompt encourages the model to take into account both the expected output and the input, and based on this, generate its interpretation of the user's interests, preferences, and familiarity with various topics from the personalized context as a reasoning path. Additionally, the approach guides the model to infer patterns in the user's preferences across different topics. For instance, if the user writes about a specific topic in a particular style, the model can generalize this pattern, assuming the user might adopt a similar style for other topics as well. Figures 12 and 13 in Appendix E present some examples of the generated reasoning paths. These figures illustrate how the model reasons over the personalized context by analyzing the key aspects of user's preferences.\nFinally, to train the LLM to reason over personalized context during output generation, the generated reasoning over personalized context is combined with the expected output using a predefined template, as shown in Figure 8 in Appendix C. This template allows us to train the model to generate this combined output given an input from a specific user accompanied by its personalized context. The model is first asked to generate a summary of the user's preferences and writing style features based on the input and personalized context then generates a response to the input. Here, the combined generated reasoning and expected output are used as the new expected output for the corresponding input in the template. Indeed, the model's task is to generate both the reasoning path, based on the personalized context, and the final response in a single inference pass. This structured approach helps the LLM learn to incorporate reasoning over the personalized context as the steps toward generating"}, {"title": "3.2 Reasoning-Enhanced Self-Training", "content": "While we can train the model using SFT on the generated reasoning data from Section 3.1 so that it reasons towards generating personalized responses, the reasoning itself is derived from the LLM's interpretation of the user profile, input prompt, and expected output. This reliance on the LLM's implicit understanding introduces potential limitations, as the reasoning path may not fully align with the user's preferences. Moreover, there is no guarantee that the generated reasoning path can consistently improve the final output. There may exist alternative reasoning paths that lead to more effective personalized responses, which are not captured by the initially generated reasoning paths for SFT.\nA solution to address this is to employ RL, which allows the model to explore the trajectory space (i.e., reasoning paths) to identify those that lead to personalized outputs with higher rewards. By leveraging exploration, the model can discover reasoning paths that yield higher rewards, corresponding to more desirable personalized outputs. Specifically, we employ Expectation-Maximization Reinforced Self-Training (Singh et al., 2024) as an offline RL algorithm to encourage the model to discover reasoning paths that lead to higher rewards. The algorithm used for this purpose is detailed in Algorithm 1. After performing SFT on the data generated in Section 3.1, we iteratively alternate between the following steps:\nExpectation Step: In this step, the optimized parameter set from the previous iteration (i.e., $\\Theta_t$) is used to collect new trajectories for training the model for the next iteration (i.e., $\\Theta_{t+1}$). Specifically,"}, {"title": "4 Experiments", "content": "We adopt the LongLaMP benchmark (Kumar et al., 2024) to conduct our experiments, which consists of four personalized long-form text generation tasks: (1) Personalized Email Completion, (2) Personalized Abstract Generation, (3) Personalized Review Writing, and (4) Personalized Topic Writing. Each example in this dataset represents a separate user, including an input prompt, an expected output, and a user profile containing information about the user (i.e., documents written by the user over time). This setup allows us to evaluate the effectiveness of our approach in generating personalized responses across diverse tasks.\nReward Modeling & Evaluation. While the LongLaMP benchmark uses ROUGE metrics (Lin, 2004) for evaluating long-form generated text, previous research shows that term-matching metrics like ROUGE often struggle to capture nuanced text similarities (Zhang et al., 2020), particularly in long-form text generation (Koh et al., 2022; Krishna et al., 2021). Following recent text generation evaluation approaches (Gao et al., 2024; Liu et al., 2023b), we use Gemma 7B (Gemma-Team, 2024) as the evaluator. We provide the evaluator LLM with the input prompt, the generated output, and the expected output, along with a prompt that explains the evaluation criteria, as shown in Figure 6 in Appendix A. The score for the generated output is computed based on the probability that the LLM assigns to each criterion. Specifically, the final score is the score of the criterion that has the maximum probability of being generated given the evaluation prompt. Finally, we normalize this score in range of 0 and 1 by dividing it by 10. The details of the reward model are explained in Appendix A.\nTraining & Inference Setting. We use Gemma 2B (Gemma-Team, 2024) as the personalized generator LLM. Given that user profiles can contain numerous items, making it impractical to use all of them, we utilize RAG to integrate personalized context (Salemi et al., 2024b). We employ the prompt"}, {"title": "4.2 Main Findings", "content": "How does training the LLM with REST-PG affect the performance? We trained the LLM using the proposed approach, which incorporates both reasoning-enhancement and self-training. For the baselines, we evaluate LLMs that were: (1) trained using SFT with retrieval augmentation (Salemi et al., 2024b; Kumar et al., 2024), (2) trained using SFT with Reasoning-Enhancement as described in Section 3.1, and (3) trained exclusively using self-training with ReST-EM (Singh et al., 2024). The results, shown in Table 1, indicate that the proposed approach, REST-PG, outperforms all baselines across all tasks, with statistically significant improvements in 3 out of 4 tasks. Additionally, the approach shows statistically significant superior performance on average across all tasks. This demonstrates that using reasoning over personalized context, combined with self-training, can significantly enhance the performance of personalized text generation, highlighting the value of incorporating reasoning during personalized generation. The main reason for this improvement is that combining reasoning with self-training enhances the model's ability to effectively use the personalized context and align its reasoning process with the user's preferences. This, in turn, results in more tailored and accurate output for the user.\nHow does reasoning-enhancement alone affect the performance? To answer this question, we compare the model trained on the reasoning-enhancement data generated in Section 3.1 and the SFT model trained on the original inputs and outputs of the LongLaMP dataset. The results of this experiment are reported in Table 1. These indicate that supervised fine-tuning on the generated reasoning-enhancement data from a larger model only statistically significantly improves performance on LongLaMP-3. However, there is a performance drop on the rest of the tasks, with the model performing worse than the SFT on average across all datasets, where on LongLaMP-2 this drop is statistically significant. However, on average, there is no statistically significant difference between this approach and SFT. Note that this approach underperforms compared to both methods that incorporate self-training. This observation suggests that, as discussed in our motivation, training solely on generated reasoning data is suboptimal as there is no alignment between these reasoning paths and the user's preferences for personalized text generation.\nHow does self-training alone affects the performance? We trained the LLM with ReST-EM (Singh et al., 2024), similar to our approach for self-training but without considering reasoning enhancement. This approach operates similarly to ours but does not involve reasoning over the personalized context. The results of this experiment are reported in Table 1 with the model name ReST-EM. The results indicate that self-training significantly improves performance on LongLaMP-2 and LongLaMP-4 over both SFT and SFT with Reasoning-Enhancement. Although it improves results on LongLaMP-1 and LongLaMP-3, these improvements are not statistically significant. Moreover, it does not outperform SFT with Reasoning-Enhancement on LongLaMP-3. However, on average, this approach significantly outperforms both baselines. Note that this model is unable to outperform REST-PG on any of the tasks, with significant differences in performance observed in 3 out of 4 tasks and in the overall average performance. This observation suggests that self-training is a promising approach for enhancing performance in personalized text generation. However, without explicitly considering the user's implicit preferences or writing style, the improvement on personalized text generation tasks is limited.\nHow does the exploration budget affect the performance of REST-PG? We apply our method using different exploration budgets m during the expectation step, generating 8, 16, 32, 64, and 128 outputs per input and train the LLM for one iteration on them. The results are shown in Figure 2. While different tasks benefit from varying exploration budgets, on average, increasing this exploration budget improves the results up to a certain point before decreasing the performance. This suggests that overly increasing the exploration budget may not be beneficial; as the model generates more examples, the diversity among high-reward examples can negatively impact the model's performance. Therefore, tuning this parameter considerably affects performance.\nHow does the number of training iterations affect the performance? We vary the number of"}, {"title": "4.3 Case Study", "content": "To compare the generated outputs using our approach, we provide two categories of examples.\nImprovements in the final generated response. Figure 15 in Appendix E shows an output generated by REST-PG and ReST-EM for a prompt from the personalized abstract generation dataset. REST-PG provides a more precise description of the proposed method and correctly predicts the evaluation dataset, ImageNet, while ReST-EM produces a hallucinated and incorrect prediction. This example highlights that REST-PG better utilizes the user's personalized context to generate more accurate and personalized response. In this case, REST-PG's correct prediction was guided by the author's previous experiments on the ImageNet dataset.\nImprovements in reasoning path toward generating the final response. Figure 14 in Appendix E shows an example of personalized output generated by REST-PG and SFT with Reasoning-Enhancement for a prompt from the personalized review writing dataset. Here, SFT with Reasoning-Enhancement introduced some hallucinated names in the reasoning, which were carried over into the final output. In contrast, REST-PG successfully avoided this issue by recognizing that adding inaccurate details negatively affects the reward model's evaluation. Notably, REST-PG inferred that the user \u201cvalues well-developed characters and relationships\u201d and incorporated this into the review, aligning closely with the expected output."}, {"title": "5 Related Work", "content": "Personalization is an important topic with use cases in search, recommendation, and text generation (Fowler et al., 2015; Xue et al., 2009; Naumov et al., 2019; Salemi et al., 2024b). Salemi et al. (2024b) introduced a Retrieval-Augmented Generation (RAG)-based method for personalizing LLMs and the LaMP benchmark for evaluating short-form personalized text generation. Kumar et al. (2024) extended this by introducing the LongLaMP benchmark for long-form personalized text generation. Another direction has focused on designing personalized writing assistants (Li et al., 2023a; Mysore et al., 2023; Lu et al., 2024) and agents (Zhang"}, {"title": "6 Conclusions", "content": "This paper proposes REST-PG, a multi-stage framework designed to train LLMs to reason over personalized contexts during response generation. The framework begins by instructing the LLM to generate a reasoning path, based on the input, expected output, and personalized context, outlining how the final output should be derived. This reasoning paths are then used to train the LLM to generate both the reasoning steps and the response in a single inference path, instilling a preliminary reasoning ability in the LLM. Following this, we apply expectation-maximization reinforced self-training to iteratively align the model's reasoning with the user's preferences based on a reward function that evaluates the similarity between the generated response and the expected output for the user. Our results on the LongLaMP benchmark show that our approach significantly outperforms supervised fine-tuning, achieving 14.5% improvement, and it outperforms self-training without reasoning by 6.5% in personalized text generation. Additionally, we conduct a detailed ablation study which provides insights into various aspects of our proposed method."}, {"title": "Evaluation of Long-Form Personalized Text Generation", "content": "Evaluating personalization in text generation presents inherent challenges, as the ideal judge for the outputs would be the individual who created the inputs (Salemi et al., 2024b). Unfortunately, accessing these original users for existing datasets is often unfeasible. Furthermore, human evaluation remains difficult, as it's not guaranteed that annotators can accurately assess whether the output meets the original prompt writer's expectations. Additionally, as highlighted in previous studies, evaluating long-form text generation is a complex and active area of research in the natural language processing community (Koh et al., 2022; Krishna et al., 2021; Belz and Reiter, 2006). In this paper, we combine these two challenging concepts, which further complicates the evaluation process.\nTo the best of our knowledge, there is currently no widely accepted metric for evaluating generated personalized outputs. Traditional metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), which rely on term matching, have proven inadequate for assessing long-form text generation (Koh et al., 2022; Krishna et al., 2021; Belz and Reiter, 2006). Recent efforts in the community have shifted toward utilizing LLMs as evaluators (Li et al., 2024c). Given that we have access to the expected output for each user, we follow the same approach and employ LLMs to assess the similarity between the generated output and the expected output for that specific user. While this evaluation method is not perfect, it represents the most effective approach available within the constraints."}, {"title": "Latency of Reasoning During Response Generation", "content": "While incorporating reasoning over personalized context in this paper leads to substantial improvements in the quality of the final generated output, it also introduces a trade-off: an increase in the overall output length. This extended length, when processed by a standard transformer-based LLM, results in a rise in decoding time. This study, however, does not address or attempt to optimize this increased decoding overhead by reasoning-enhancement. While the current focus is on enhancing output quality and personalization, future research could explore strategies to mitigate these computational costs."}, {"title": "A Large Language Model Evaluator & Human Evaluation", "content": "Although the LongLaMP benchmark (Kumar et al., 2024) primarily relies on ROUGE (Lin, 2004) to assess the quality of long-form text generation, prior studies suggest that lexical overlap metrics often fail to capture semantic similarities (Zhang et al., 2020), especially in long-form generation tasks (Koh et al., 2022; Krishna et al., 2021; Belz and Reiter, 2006). Following the approach proposed by Liu et al. (2023b), we employ an instruction-tuned LLM, Gemma (Gemma-Team, 2024), with 7 billion parameters as our text similarity evaluator. Since this LLM is trained on large instruction tuning datasets, if provided with a well-defined evaluation instruction, it can serve as effective judges for text similarity tasks (Li et al., 2024c).\nFollowing Liu et al. (2023b), to evaluate the generated outputs, we feed the evaluator LLM with the input prompt, the generated text, and the reference output, accompanied by a prompt that explains the evaluation criteria (as depicted in Figure 6). In this prompt, the criteria that determine whether the generated output receives the defined score are clearly outlined. After feeding the model with the prompt (containing the input, expected output, and generated output), the LLM evaluator calculates the probability for each score defined by the criteria in the prompt. The score with the highest probability is selected as the score for the generated output for this input. To normalize the score and ensure it falls within the range of 0 to 1, the selected score is divided by 10 (i.e., the maximum score that the LLM evaluator can assign to an output). This normalized score reflects the model's assessment of the generated output based on the predefined criteria. To validate whether the LLM evaluator model can accurately assess the quality of generated texts, we design two experiments.\nIn the first experiment, we conducted a human evaluation to validate the LLM evaluator. Annotators were presented with 100 pairs of generated texts from the models discussed in this paper. For each pair, the annotators were asked to select the text that best reflected the expected output given the input. The pairs were selected such that there was a score difference of at least 0.5 between the two texts, as determined by our LLM evaluator model. The results of the human evaluation indicate that our metric aligns with human judgment in 73% of the cases. Additionally, the metric shows a correlation of 0.46 with human judgment, suggesting that the LLM evaluator model generally agrees with human assessments. Note that previous studies on designing automatic metrics for personalized text generation have highlighted that such approaches may struggle to achieve very high agreement with human evaluations. This is because personalized text generation is inherently subjective, and only the individual who wrote the input can fully assess whether the generated output meets their expectations or preferences (Wang et al., 2023). Since access to these specific annotators is not possible for existing datasets, this type of evaluation may not provide a completely reliable measure of the quality of personalized text generation.\nTo further evaluate the LLM evaluator, we designed an experiment in which the model trained on the LongLaMP benchmark using supervised fine-tuning (as detailed in Section 4) is tested with personalized contexts that are randomly assigned to inputs at varying rates. Specifically, we randomly replaced S percent of the personalized contexts with those from other users, while keeping the input prompt and expected output unchanged. This experiment aims to determine whether the LLM evaluator can detect changes in the personalized context based on the generated text and its comparison with the expected output. The results of this experiment are shown in Figure 5. The figure illustrates that as the rate of random sampling increases, the LLM evaluator linearly assigns lower scores to the texts generated by the same model. This suggests that the LLM evaluator is linearly sensitive to discrepancies in the generated text context from unmatched personalized context with the expected output for the given input.\nTherefore, considering both experiments, we believe and are convinced that the LLM evaluator used in this paper is capable of evaluating the quality of generated personalized text when a personalized expected output is provided. These findings demonstrate that the LLM evaluator can effectively align with human judgments and is sensitive to changes in personalized context, supporting its utility for assessing personalized text generation."}]}