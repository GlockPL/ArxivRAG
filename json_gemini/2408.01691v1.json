{"title": "TreeCSS: An Efficient Framework for Vertical Federated Learning", "authors": ["Qinbo Zhang", "Xiao Yan", "Yukai Ding", "Quanqing Xu", "Chuang Hu", "Xiaokai Zhou", "Jiawei Jiang"], "abstract": "Vertical federated learning (VFL) considers the case that the features of data samples are partitioned over different participants. VFL consists of two main steps, i.e., identify the common data samples for all participants (alignment) and train model using the aligned data samples (training). However, when there are many participants and data samples, both alignment and training become slow. As such, we propose TREECSS as an efficient VFL framework that accelerates the two main steps. In particular, for sample alignment, we design an efficient multi-party private set intersection (MPSI) protocol called Tree-MPSI, which adopts a tree-based structure and a data-volume-aware scheduling strategy to parallelize alignment among the participants. As model training time scales with the number of data samples, we conduct coreset selection (CSS) to choose some representative data samples for training. Our CSS method adopts a clustering-based scheme for security and generality, which first clusters the features locally on each participant and then merges the local clustering results to select representative samples. In addition, we weight the samples according to their distances to the centroids to reflect their importance to model training. We evaluate the effectiveness and efficiency of our TREECSS framework on various datasets and models. The results show that compared with vanilla VFL, TREECSS accelerates training by up to 2.93\u00d7 and achieves comparable model accuracy.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) [22,24,11] is a paradigm of distributed machine learning that trains models over multiple participants without sharing the data samples.\nFL encompasses two main scenarios, i.e., horizontal FL (HFL) [40,19,28] and vertical FL (VFL) [37,25,12,18]. In HFL, the participants hold different data samples; while in VFL, each participant has a separate set of features for all data samples. We focus on VFL in this paper, which usually emerges when different institutions possess different data for common entities and has applications in areas such as healthcare, finance, and IoT. For instance, each bank holds its own financial records of the customers, and the banks can collaborate via VFL to train a model to predict the financial risk of the customers [27].\nVFL involves two main steps, i.e., data alignment and model training. With data alignment, the participants agree on a common set of data samples to use for training. This is necessary because the participants may have non-overlapping samples or assign different identifiers for the samples; and the alignment step derives a unique global identifier for each sample. To protect data privacy, VFL usually adopts private set intersection (PSI) methods for data alignment [14]. For model training, VFL needs to exchange activations and gradients for each sample (called instance-wise communication) [4]. This is because the features are partitioned over the participants, and thus each participant can only compute part of the activations and gradients for a sample.\nChallenges. Both data alignment and model training become slow when there are many participants and data samples. This is because the computation and communication costs of PSI grow quadratically with the number of data samples and linearly with the number of participants. For model training, VFL needs to communicate activations and gradients for each data sample as discussed above, and thus the communication costs scale linearly with the number of training samples. An efficient VFL framework should tackle these scalability challenges.\nCurrent research landscape. For data alignment, VFL usually utilizes multi-party private set intersection (MPSI) [29,17,2]. However, the efficiency of existing MPSI approaches suffers when there are many participants with possibly unbalanced data volumes. This is because these approaches mainly use path and star topological structures to coordinate the participants, which hinder parallel computation and cannot adjust the roles of the participants based on their data volumes. In particular, with a path-like structure, each participant conducts sequential two-party PSI with its adjacent party, requiring O(m) rounds with m participants. Due to the lack of parallel computation, the efficiency is poor. The star structure employs a central participant that interacts with all the other participants. It needs only O(1) round but requires high communication bandwidth and computation power for the central participant, which may become the bottleneck and a major failure risk.\nModel training can be accelerated by reducing the number of training samples. For such purpose, existing works [1,10] propose the concept of coreset, which selects some representatives from data samples to conduct training without degrading the accuracy of the trained models. V-coreset [15] adapts coreset for VFL but suffers from two critical limitations. V-coreset cannot ensure data privacy because it directly sends the original labels to the server for model training, potentially resulting in label leakage for the samples. V-coreset is not general"}, {"title": "2 Related Work", "content": "Vertical federated learning (VFL). VFL trains models on samples whose features are partitioned among the clients, and many works propose models and algorithms for VFL. In particular, Hardy [13] introduces a VFL framework that uses homomorphic encryption to train logistic regression (LR) models. Yang [36] extends this framework by adopting the quasi-Newton method to reduce communication costs. Inspired by split learning [34,4], SplitNN divides models into multiple parts to support complex networks in VFL. We speedup VFL under the SplitNN framework with efficient data alignment and coreset construction.\nPrivate set intersection (PSI). Sample alignment protocols are crucial for VFL because the clients may have non-overlapping samples. PSI protocols are widely used in secure multi-party computation and enable the parties to determine the intersection of their sets without revealing their items. Two-party PSI protocols are extensively studied and can be realized using techniques such as oblivious transfer [32], garbled circuits [16], classical public-key cryptosystems [7], etc. Some works extend two-party PSI to the multi-party.\nIn the current landscape, the topological structures of Multi-party Private Set Intersection (MPSI) are centered around path and star configurations. Hazay [14] proposes a protocol that uses the polynomial set encoding and star topology. They evaluate the polynomials obliviously using an additive homomorphic threshold cryptosystem, and provide an extension of the protocol secure in the malicious model. Kavousi [20] constructs a protocol including oblivious transfer (OT) extension and garbled Bloom filter as its main ingredients. The protocol involves two interaction types: one uses a star-like communication graph, where one party acts as the sender interacting with all others via OTs. The other utilizes a path-like communication graph, passing a garbled Bloom filter from the first to the last neighboring party. However, only the sender is able to obtain the final PSI result. Vos [35] designs private AND operation among multiple participants by utilizing elliptic curve cryptography, which makes it suitable for MPSI. The participants communicate strictly in a star topology.\nCoreset. Coreset selects some representatives from the samples while ensuring that models trained on the coreset yield similar accuracy as full-data training. As such, coreset can reduce the number of training samples and is used to accelerate machine learning models including clustering [9], regression [8], low-rank approximation [6], and mixture modeling [26]. V-coreset extends coreset to VFL [15]"}, {"title": "3 Preliminaries", "content": "In this part, we introduce the SplitNN framework, which provides general settings and procedures for VFL.\nThere are M participants (also called clients), and one client owns the labels, which is called the label owner and denoted as $C_{lo}$. There are also an aggregation server and a key server. To facilitate secure client-server communication, the key server generates public and private homomorphic encryption (HE) keys to encrypt the messages. The goal is to train a machine learning model on N data samples: $D = \\{(x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{N})\\}_{i=1}^N$, where $x_i$ represents the features of the ith sample, and $y_i$ is the label. We use $[N] = \\{1,\\ldots, N\\}$ to denote the indices of the samples and $[M] = \\{1,\\ldots, M\\}$ to denote the set of all clients. For classification and regression tasks, the loss function is usually expressed as\n$L(D, \\theta) := \\sum_{i \\in [N]} L(f_{global}(x_i : \\theta), y_i)$.\nThe goal is to minimize the sample-wise loss function $L(\\cdot,\\cdot)$ using model $f_{global}(x_i: \\theta)$ parameterized by $\\theta$.\nEvery feature vector $x_i$ is partitioned over the M clients, and a client m holds some features of all the samples, i.e., $D_m = \\{x_i^m \\in \\mathbb{R}^{d_m} : m \\in [M]\\}_{i=1}^N$, where $d_m$ denotes the number of local features on client m. Thus, we have $\\sum_{m=1}^M d_m = d$. All the labels $D_{label} = \\{y_i \\in \\mathbb{N}\\}_{i=1}^N$ are kept on the label owner. The model $f_{global}(\\theta)$ is partitioned into a bottom model $f_b(\\theta_b)$ and a top model $f_t(\\theta_t)$. Each client possesses a segment of the bottom model $\\{f_m(\\theta_m) : m \\in [M]\\}$ that works on its local features, while the top model merges the outputs of the local bottom models for final output and is kept on the aggregation server. The procedure of model training works as follows.\n\u2022 The local feature vectors (i.e., $\\{x_i^m\\}_{i=1}^N$) are processed by the clients using their bottom models (i.e., $f_m(\\theta_m)$) to produce intermediate outputs.\n\u2022 The aggregation server merges the intermediate outputs, processes them with the top model $f_t(\\theta_t)$, and forwards the results to the label owner.\n\u2022 The label owner computes loss according to the final outputs and the labels, which is used to derive gradients $g_t$ for the top model.\n\u2022 The aggregation server updates the top model with gradient $g_t$ and computes gradients $\\{g_m: m \\in [M]\\}$ for the local bottom models of the clients, which are used by the clients to update their local models."}, {"title": "4 The TreeCSS Framework", "content": "In this part, we describe our TREECSS framework. The lifecycle of TREECSS takes three steps, i.e., data alignment with Tree-MPSI, coreset contruction with Cluster-Coreset on the algined samples, and model training on the coreset. Step follows the SplitNN framework discussed in Section 3, and we focus on 1 and 2."}, {"title": "4.1 Tree-MPSI for Data Alignment", "content": "For VFL, data alignment among the clients is essential because their sample orderings may be different. For such purpose, we employ private set intersection (PSI) to synchronize the data indices. Our Tree-MPSI supports multiple clients and handles many samples efficiently. Figure 2 provides an illustration of Tree-MPSI. In particular, Tree-MPSI utilizes a tree-based architecture to organize the clients into pairs and schedule two-party PSI. For the selection of two-party PSI (TPSI) protocols, we opt for two representative methods: one based on RSA blind signatures and another based on Oblivious Transfer (OT).\nTwo-party PSI primitive. Following, we describe our proposed method using RSA, and the counterpart using OT is similar. For RSA blind signature method, the protocol involves a sender and a receiver. The process starts with the sender generating encryption keys and sharing the public key. The receiver encrypts its sample indicators using the public key and random numbers, and sends them to the sender. The sender responds with its encrypted indicators and the decrypted receiver's indicators. Afterwards, the receiver computes and stores the intersection result. For the OT-based method, a sender and a receiver are involved. The sender generates k oblivious pseudo-random function (OPRF)\nseeds. The receiver applies a distinct pseudo-random function to each element it possesses, resulting in a specific mapped set. Simultaneously, the sender employs each pseudo-random function for every element in its possession, thereby generating its individual mapped set. Subsequently, the sender transmits its mapped set to the receiver, who conducts comparisons between the two sets for each element to derive the ultimate result.\nMulti-party PSI. Based on the basic two-party PSI protocol, Tree-MPSI works as follows:\nStep 1: Client request. Clients request $R_e$ to the aggregation server to initiate data alignment and check the PSI process status (\u2460).\nStep 2: Scheduling. We first identify the active clients, denoted as set U. These clients are paired sequentially according to their request order to the server. For each pair, the earlier requester is assigned the role of sender while the later requester is receiver for the two-party PSI protocol (TPSI) (\u2461).\nStep 3: Server status generation. Upon receiving client requests and completing scheduling, the aggregation server generates and sends a status message $R_e$ to each client to notify its TPSI partner's communication address (3).\nStep 4: TPSI Computation. After obtaining the communication address of its TPSI partner, the client engages in interaction with it. If the address is not available, the client waits until the entire Tree-MPSI process is complete to receive the final result. (\u2463).\nStep 5: Result allocation. In the final round of Tree-MPSI, the client holding the final result $[N_{align}]$, which is an ordered list, encrypts the result using homomorphic encryption (HE) with the public key allocated by the key server: $Enc([N_{align}]) = HE.Enc([N_{align}], pk)$. The client sends $Enc([N_{align}])$ to the aggregation server (\u2464), and the aggregation server forwards the encrypted result to all clients (6), where the result can be decrypted using their private keys as: $[N_{align}] = HE.Dec(Enc([N_{align}], sk)$.\nComparing with alternatives. In contrast to the Path-MPSI requiring O(m) communication rounds, Tree-MPSI achieves parallelism by scheduling and pairing clients. Thus, it allows concurrent two-party PSI (TPSI) and reduces communication rounds to $O(log m)$. During Tree-MPSI execution, clients merely communicate whether they have stored the TPSI result from the previous round and the current TPSI result's length they possess. Furthermore, the final results transmitted through the aggregation server are encrypted using homomorphic encryption (HE), ensuring that the aggregation server lacks the private key. Consequently, sensitive client data remains secure, and the RSA algorithm ensures privacy during TPSI computations.\nScheduling optimization. In two-party PSI, if RSA blind signature is chosen, the receiver encrypts and transmits the indicator information twice, whereas the sender does only once. To reduce communication overhead, we pair a client that has a large dataset ($ResLen$ indicated in the requests) with a client that has a small dataset. The idea is to reduce the communication volume by assigning the client with fewer samples as receiver. If the protocol is OT-based, since the\nsender needs to transmit a large amount of data, the client with a larger dataset is designated as the receiver. In particular, the scheduling works as follows:\n\u2022 Sorting. We sort the active clients in U by ResLen in ascending order, yielding a sorted list $L_{unscheduled} = AsSort(U) = [c_1, c_2, ...c_{|u|}]$, where function AsSort() sorts the input array of clients in ascending order.\n\u2022 Pairing. We generate pairs of active clients from $L_{unscheduled}$. Specifically, we adopt a greedy approach, which pairs client $c_k$ with client $c_{k+[|U|/2]}$ for k = 1, 2, ..., $[\\frac{|U|}{2}]$. When |U| is odd, client $c_{[\\frac{|U|}{2}]}$ is paired with itself.\n\u2022 Selecting of two-party PSI result receiver. For RSA-based two-party protocol, the client with smaller amount of data is assigned as receiver. If using an OT-based protocol, the client with more data is the receiver, cutting down two-party PSI communication overhead.\nPerformance analysis. The scheduling optimization involves sorting the active clients with $O(|U|log|u|)$ time and looping over the clients for pairing with O(U) time. Thus, the overall time complexity of the scheduling optimization is $O(|U|log|u|)$. As for the communication cost, we use B to denote the number of samples in the larger dataset and S for the number of samples held by the other party. If clients are paired based on their request order, the worst-case communication cost of two-party PSI is O(2|B|+|S|). With our proposed optimization, the communication cost is O(2|S|+|B|), which yields a reduction of O(|B|-|S|). As the number of clients U is usually far fewer than the number of samples, the scheduling cost is much smaller than the saving of communication cost."}, {"title": "4.2 Cluster-Coreset for Coreset Construction", "content": "After executing Tree-MPSI, we denote the aligned samples as $D_{align} = \\{(x_i \\in \\mathbb{R}^d,y_i \\in \\mathbb{N}) : i \\in [N_{align}]\\}$. The features held by client m can be denoted as $D_m = \\{x_i^m \\in \\mathbb{R}^{d_m} : m \\in [M],i \\in [N_{align}]\\}$, and the labels held by the label owner as $D_{laben} = \\{y_i \\in \\mathbb{N} : i \\in [N_{align}]\\}$. Following, we use the indicators $\\{i, i \\in [N_{align}]\\}$ to refer to the aligned data samples.\nOur proposed coreset construction method takes a clustering strategy - first cluster samples on each client and then merge local clustering results. Cluster-Coreset only communicates cluster indices and therefore can well protect data privacy. Besides, Cluster-Coreset does not rely on the downstream tasks, and hence achieves better generalization compared to V-coreset. The procedure of Cluster-Coreset contains the following 5 steps.\nStep 1: Local clustering. We perform separate clustering for the aligned samples on each client, which often represents a specific domain. Specifically, each client clusters its local features individually using the K-Means algorithm, generating c clusters (1).\nStep 2: Weight computation. Post local clustering, each client's samples are weighted according to their distance to the cluster centers, with nearer samples receiving higher weights (\u2461). Let $S_m$ be the set of samples that belong to\ncluster c on client m, the weight $w_{ic}^m$ of each sample $i \\in S_m$ is computed as:\n$w_{ic}^m = \\frac{1}{S_c} \\times pos(e_{id}^m, DeSort(\\{e_{jd}^m\\}_{j \\in S_m})),\nwhere function DeSort(\u00b7) sorts the samples in descending order based on their Euclidean distances $e_{id}^m$ to the cluster centroids, pos(\u00b7,\u00b7) returns the position of sample i in the sorted array. The weights reflect the significance of the samples, that is, those closer to the centroids are more representative.\nStep 3: Cluster tuple (CT) construction. Each client m sends HE-encrypted messages to the label owner via the aggregation server. For each sample i, the message contains the local weight $w_{ic}^m$, cluster index $c_i^m$, and the Euclidean distance to the cluster centroids $e_{id}^m$. The aggregation server concatenates the messages for the same sample before sending them to the label owner. This ensures that the label owner cannot infer the sources of the messages. The label owner uses the messages to construct the CTs. In particular, for each data sample i in $[N_{align}]$, we gather its cluster indices from all clients as:\n$CT_i = (c_i^1,c_i^2,\\ldots,c_i^m)$.\nThus, the CTs list for all $D_{align}$ samples is $L_{ct} = \\{CT_i : i \\in [N_{align}]\\}$ (\u2462).\nStep 4: Data selection. After constructing CTs, the next question becomes: how to choose a small group of samples as the coreset? Intuitively, if the CTs of two samples are identical, it indicates that these samples have been classified in the same cluster across all clients. Therefore, they are considered as\nsimilar, and we select only one sample to be included in the final coreset. By utilizing $L_{ct}$, the label owner identifies the most informative and representative samples. We calculate the set of distinct CT values in $L_{ct}$ as $T_{ct} = \\{t_1, t_2,\\ldots,t_k\\}$, where k is the number of distinct CT values. We use $S_{ct}^j$ to denote the samples with value $t_j$, which is expressed as: $S_{ct}^j = \\{i \\in [N_{align}]: CT_i = t_j\\}$. We then split $S_{ct}^j$ into subsets based on their labels: $S_{ct}^{j,l} = \\{i \\in S_{ct}^j : y_i = l\\}$. The label owner needs to select one sample from $S_{ct}^{j,l}$. In particular, the sample with the minimal aggregated distance to its cluster centers (i.e. $\\sum_{m=1}^M e_{id}^m$) is chosen. The chosen sample's indicator can be expressed as:\n$i^\\ast = argmin_{i \\in S_{ct}^{j,l}} \\sum_{m=1}^M e_{id}^m$.\nIn this way, the label owner obtains the sample indicators to be included in the coreset, i.e., $[N_{core}] = \\{i_1,i_1,i_1,i_1,i_2,...,i_\\mathcal{L}\\}$, where $\\mathcal{L}$ is the number of distinct classes held by the label owner (\u2463). The selected indicators are then encrypted with HE and sent to all clients via the aggregation server. Each client utilizes the decrypted indicators to select the corresponding local data samples and collaboratively construct the coreset $D_{core} = \\{(x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{N}) : i \\in [N_{core}]\\}$.\nStep 5: Sample weighting. Recall Step 3, we assign a weight for each sample to reflect its importance/confidence. We propose to weight the coreset samples during model training (\u2464). In particular, the weight of each coreset sample is obtained by summing its local weights: $w_i = \\sum_{m=1}^M w_{ic}^m$, $i \\in [N_{core}]$. Intuitively, the weighting strategy adjusts the importance of the samples according to their proximity to the cluster centroids and considers samples close to the centroids as more important. With the weights, the loss function for classification and regression tasks can be expressed as\n$L(D_{core}, W_{core}, \\theta) := \\sum_{i \\in [N_{core}]} W_i \\cdot L(x_i, \\theta).\nPrivacy analysis. In the process of constructing the coreset, the transmitted data, including the weights, cluster categories, distances, and the selected data indicators, are all encrypted using homomorphic encryption (HE). As the aggregation server does not know the decryption key, the entire process is secure against a curious server. In contrast, V-coreset sends the original labels to the server for model training, potentially resulting in label leakage."}, {"title": "5 Experimental Evaluation", "content": "We conduct extensive experiments on diverse datasets stored in Oceanbase [39,38] to validate the effectiveness of our proposed TREECSS."}, {"title": "5.1 Experiment Settings", "content": "Datasets. Table 1 presents an overview of the datasets evaluated in this work. Among them, Bank (BA) [33], Mushrooms (MU) [31], Rice (RI) [30], and Higgs"}, {"title": "5.3 Evaluation of Tree-MPSI and Cluster-Coreset", "content": "We next evaluate two components in TREECSS: Tree-MPSI and Cluster-Coreset.\nTree-MPSI. We generate a synthetic dataset that only has data sample indicators for each client. The content within these datasets overlaps by 70%, and each client's indicators are randomly shuffled. Initially, we compare Tree-MPSI and the baselines (Path-MPSI, Star-MPSI) with 10 clients across varying dataset sizes per client, as illustrated in Fig 7(a) (RSA-based) and 7(b) (OT-based). The results show an average 2.25\u00d7 speedup with Tree-MPSI, and the improvement becomes more significant as the dataset size increases. We further"}, {"title": "5.4 Ablation and Sensitivity Study", "content": "Clusters per client. We assess TREECSS by varying the number of clusters per client on 4 representitive datasets: MU, HI, BP and YP. Fig. 4 shows that larger cluster sizes enhance test accuracy and lower test loss by including more samples in the coreset, but as Fig. 5 indicates, this also raises time consumption due to a larger coreset's size.\nEffect of reweighting. We next evaluate the effect of reweighting. As shown in Fig. 4 and 5, the introduction of weights significantly enhances model test performance, particularly with fewer clusters. The reweighting mechanism slightly prolongs training time compared to training without weights."}, {"title": "6 Conclusions", "content": "In this paper, we present TREECSS, an efficient end-to-end VFL framework. This approach aims to address the limitations of prior methods, which lack efficient data alignment, privacy protection and the support of various task. To enhance end-to-end VFL, we a Tree-MPSI protocol for the alignment stage and a clustering coreset construction method for the training stage, coupled with a reweighting strategy. Extensive experiments show that our proposed framework significantly outperforms the baselines."}]}