{"title": "Blox-Net: Generative Design-for-Robot-Assembly Using VLM\nSupervision, Physics Simulation, and a Robot with Reset", "authors": ["Andrew Goldberg", "Kavish Kondap", "Tianshuang Qiu", "Zehan Ma", "Letian Fu", "Justin Kerr", "Huang Huang", "Kaiyuan Chen", "Kuan Fang", "Ken Goldberg"], "abstract": "Generative AI systems have shown impressive\ncapabilities in creating text, code, and images. Inspired by the\nrich history of research in industrial \"Design for Assembly\",\nwe introduce a novel problem: Generative Design-for-Robot-\nAssembly (GDfRA). The task is to generate an assembly based\non a natural language prompt (e.g., \"giraffe\") and an image\nof available physical components, such as 3D-printed blocks.\nThe output is an assembly, a spatial arrangement of these\ncomponents, and instructions for a robot to build this assembly.\nThe output must 1) resemble the requested object and 2) be\nreliably assembled by a 6 DoF robot arm with a suction gripper.\nWe then present Blox-Net, a GDfRA system that combines gen-\nerative vision language models with well-established methods\nin computer vision, simulation, perturbation analysis, motion\nplanning, and physical robot experimentation to solve a class of\nGDfRA problems with minimal human supervision. Blox-Net\nachieved a Top-1 accuracy of 63.5% in the \"recognizability\" of\nits designed assemblies (eg, resembling giraffe as judged by a\nVLM). These designs, after automated perturbation redesign,\nwere reliably assembled by a robot, achieving near-perfect\nsuccess across 10 consecutive assembly iterations with human\nintervention only during reset prior to assembly. Surprisingly,\nthis entire design process from textual word (\"giraffe\") to\nreliable physical assembly is performed with zero human\nintervention.", "sections": [{"title": "I. INTRODUCTION", "content": "Design-for-Assembly (DfA) has a long history dating\nback to the start of the Industrial Revolution, where guns,\npocket watches, and clocks were designed with interchange-\nable parts to facilitate mass production on human assembly\nlines [1]. With the advent of industrial automation in the\nlatter half of the 20th century, DfA was expanded to take into\naccount the error tolerances of mechanical assembly systems\ndriven by mechanical cams and belts, and later for robotic\nassembly systems, the latter known as Design-for-Robot-\nAssembly (DfRA) [2]. DfRA is the process of designing\na product and robot assembly system together to ensure\nfeasibility, for example designing an injection molded part\nalong with a custom workcell for manipulating it. These de-\nsign systems were enhanced by the emergence of Computer-\nAided Design (CAD) and Computer-Aided-Manufacturing\n(CAM) software that streamlined human visualization and\nevaluation of components and assemblies using Finite Ele-\nment Methods (FEM) and perturbation analysis [3-6]. Such\nsystems help human designers visualize and arrange me-\nchanical components with realistic tolerances, checking for\npotential jamming and wedging conditions (tolerance stack-\nup) [7].\nAll existing DfRA systems require human designers in the\nloop [3, 4, 7]. One factor that is difficult for DfRA systems\nto accurately model is the reliability of robot assembly,\nwhich depends on the inherent uncertainty in perception,\ncontrol, and physics (eg, friction) [8-13]. This can to some\ndegree be modeled with simulation, but it is well-known\nthat 3D simulation systems struggle to accurately model\nminute 3D deformations and collisions that occur during\nrobot grasping and effects such as deformations of robot\ngripper and suction cups which can produce substantial errors\nleading to assembly failures [14-18]. Therefore, physical\nassembly trials are ideal for evaluation.\nRecent advances in Generative AI systems have demon-\nstrated remarkable abilities to create novel texts, code, and\nimages [19-21]. Researchers are actively exploring \"text-to-\nvideo\" [22-24] and \"text-to-3D\" [25-27] systems, where the\nlatter generates 3D mesh structures from textual descriptions"}, {"title": "II. RELATED WORK", "content": "A. Design for Robot Assembly\nThe concept of Design for Assembly (DfA) was pioneered\nby Geoffrey Boothroyd and Peter Dewhurst in the early\n1980s [29], with Hitachi developing its Assemblability Eval-\nuation Method (AEM) in 1986 [30]. These seminal works\nlaid the foundation for systematic approaches that follow\nproduct design guidelines [31] facilitate facilitate efficient\nassembly processes. As robotics automation in manufactur-\ning became prevalent, Design for Robot Assembly (DfRA)\nemerged as an extension of DfA principles, specifically\naddressing the unique capabilities and limitations of robotic\nsystems in assembly tasks [32, 33].\nDesign for Robot Assembly (DfRA) [32, 34-36] has\nevolved significantly with the advent of Computer-Aided\nDesign (CAD) and Computer-Aided Manufacturing (CAM)\nsoftware, which expedite design and evaluation of compo-\nnents and assemblies using Finite Element Methods and\nperturbation analysis [3-7]. While these tools facilitate vi-\nsualization and analysis of tolerances, stresses, and forces,\nall existing DfRA systems require extensive human input [3,\n4, 7]. A persistent challenge in DfRA is accurately mod-\neling assembly reliability, given the inherent uncertainties\nin perception, control, and physics [8-13]. Simulation can\npartially address this, but struggles to capture 3D deforma-\ntions and collisions crucial to robot grasping, necessitating\niterative real-world testing and redesign [14-18, 37]. Recent\nadvancements leverage large language models (LLMs) [20,\n38] for various aspects of design, including task planning,\nrobot code generation [39, 40], engineering documentation\nunderstanding [41], and generating planar layouts or CAD\nmodels [28, 42-44]. However, these methods primarily focus\non determining assembly sequences for fixed designs. In\ncontrast, this paper addresses both the design and execu-\ntion aspects of robot assembly, aiming to create physically\nfeasible designs for robotic assembly with minimal human\nsupervision.\nB. Text-to-Shape Generation\nSemantic generation of 3D shapes and structures is a\nlong-standing problem in computer vision and computer\ngraphics [45]. Deep generative models have enabled a wide\nrange of approaches that learn to capture the distribution\nof realistic 3D shapes, in the format of voxel maps [46],\nmeshes [47], point clouds [48], sign distance functions [49],"}, {"title": "III. GDFRA PROBLEM", "content": "We formally define the problem of Generative Design for\nRobotic Assembly (GDfRA). We consider the design of a 3D\nstructure that can be assembled with an industrial robot arm\n(see Figure 1). The input is a word or phrase (e.g., \"bridge\")\nand an image of available components for assembly. The\nobjective for the GDfRA system is to design a structure\nwhich is (1) \"recognizable\" meaning the structure visually\nresembles the provided text input and (2) \"constructible\"\nmeaning the structure can be assembled by a robot."}, {"title": "IV. METHOD", "content": "We present Blox-Net, a system for a class of GDfRA\nthat assumes (1) components are cuboids and cylinders and\n(2) components are lying in stable poses within a reachable\nplanar area.\nBlox-Net includes three phases. In phase I (Figure 3),\nBlox-Net prompts a VLM (GPT-40 [58]) to generate multiple\nassembly designs, from which the VLM selects the top\ncandidate based on stability and visual fidelity. In phase\nII (Section IV-B), the chosen assembly design undergoes\nan iterative refinement process in a customized physics\nsimulator. This simulation-based approach applies controlled\nperturbations to enhance the design's constructability while\nmaintaining its core characteristics. In phase III (Section IV-\nC), Blox-Net utilizes a robot arm equipped with a wrist-\nmounted stereo camera and suction gripper to construct the\noptimized design using 3D printed blocks. The assembly\nis constructed on a tilt plate, which the robot actuates to\nautomatically reset the blocks back into a tray.\nA. Phase I: VLM Design and Selection\nGiven the language description and a set of blocks with\nknown sizes and shapes, Blox-Net uses a VLM to gener-\nate candidate structure designs. Unlike existing text-to-3D\ngeneration methods that produce unconstrained meshes [25,\n56], Blox-Net generates 3D structures subject to the physical\nconstraints imposed by the available blocks. It prompts the\nVLM to generate an assembly plan that specifies the 3D\nlocations and orientations for placing each block using the\navailable components (illustrated in Fig. 3 (VLM Design\nPrompting)).\nTo facilitate high-quality generation, similar to DALL-E\n3 [80], Blox-Net first elaborates the prompt. For example,\nto construct a \"giraffe\", the VLM is prompted to give a\nconcise, qualitative textual description that conveys the key\nfeatures of a giraffe by highlighting the overall structure and\nproportions.\nAfter prompt elaboration, the VLM is prompted for the\nassembly plan. Specifically, the prompt includes the target\nobject (\"giraffe\u201d), the VLM's elaboration response, and the\nset of available blocks. The set of available blocks is en-\ncoded as JSON, which provides a structured, flexible format\nfamiliar to VLM models. Based on these inputs, the VLM\nis asked to explain each block's role in the structure.\nOnce a high-level plan is generated, Blox-Net prompts\nthe VLM to produce an assembly plan, specifying the\nrotation, position and color of objects. Instead of using"}, {"title": "B. Phase II: Perturbation-Based Redesign", "content": "In GDfRA, accounting for imprecise state estimation and\nrobot control is important to ensure robust assembly. The\ndesign output from the VLM does not account for such\ntolerances, which can result in collisions and misplaced\nblocks during assembly. We thus introduce a perturbation-\nbased redesign process.\nThe redesign process iterates through each of the blocks\nand determines if adjustments are needed. A block will be\nperturbed if it violates at least one of the following three\ncriterion: (1) the surface-to-surface distance to another block\nis less than a specified collision threshold and the two blocks\noverlap in the gravity-aligned axis (2) the block is already\nin collision with another block; or (3) the block is unstable\nat some nearby sampled point within a predefined radius.\nFor each block, Blox-Net samples points evenly along\nregularly spaced, concentric circles centered at the block\nnominal location and checks for stability and collision at\neach point. The block position is updated to the average\nof positions that are stable and free from collision. This\nprocess is applied to all blocks in the structure until no\nfurther adjustments are needed or each block has been visited\na predefined maximum number of times."}, {"title": "C. Phase III: Robot Assembly and Evaluation", "content": "To evaluate constructability, Blox-Net automates physical\nassembly and evaluates the generated design on a robot. The\nrobot first moves to a predefined pose and captures a top-\ndown RGBD image of the blocks on a plastic tray. Blox-\nNet uses SAM [81] to segment an RGB image and obtain\nimage masks. SAM segmentations include regions that do\nnot correspond to blocks. To filter out extraneous masks, we\ngenerate a point cloud for each mask by deprojecting the\nmasked area from the depth image obtained from a stereo\ncamera [82]. Blox-Net then discards masks that are outside\nthe tray, below a certain minimum area, or not circular or\nrectangular.\nBlox-Net refines each mask to segment the top of each\nblock by fitting a RANSAC [83] plane to the pointcloud and\nretaining only inliers. The block's rotation is determined by\nfitting the tightest oriented bounding box to the refined mask.\nThe block's center is the mean of the points in the filtered\npoint cloud, with the x and y dimensions measured from\nthe point cloud and the z dimension derived from its height\nrelative to the tray base.\nUpon determining the size, shape, position, and dimen-\nsions for each block, Blox-Net can obtain a new plan through\nthe design generation and perturbation-based redesign pro-\ncess (Section IV-A and Section IV-B), or construct the target\nobject based on a previously generated plan. Blocks may\nrequire rotations about their x or y axis to align with the\npose used in the plan. This rotation is facilitated by placing\nthe block in a 90-degree angle bracket and regrasping the\nblock from a different side (Fig. 4). After reorientation, the\nrobot captures a new top-down image and all block positions,\nrotations, shapes, and dimensions are recomputed via the\naforementioned pipeline.\nThe assembly process begins after all blocks are properly\noriented. Each block is grasped at its centroid, rotated to the\nplanned orientation, and placed at the location specified in\nthe design. Force feedback control is used for both grasping\nand placing blocks: during a grasp, the robot lowers onto the\nblock until a force is detected; similarly, during placement,\nit descends and releases the block once a force is sensed.\nTo enable efficient testing and design validation, we design\nan automatic reset. After completing the full assembly, the\nrobot arm captures an image. Then, the robot presses down\non the tilt plate, dumping the blocks back into the tray. This\nresets the scene for subsequent trials."}, {"title": "V. EXPERIMENTS", "content": "To evaluate how well the generated structures by Blox-\nNet satisfy the GDfRA objective, we assess both the seman-\ntic recognizability of the designs (in Section V-A), which\nrefers to how well the designs semantically align with the"}, {"title": "VI. RESULTS", "content": "Semantic Recognizability: We present results in Table I.\nResults from the evaluation of BloxNet's designs using GPT-\n4o as an evaluator suggest that the generated designs closely\nalign with the correct category semantics as recognized by\nGPT-40. Notably, with N = 5 labels, the model achieves a\nTop-1 accuracy of 63.5%, demonstrating a consistent corre-\nspondence between the generated designs and the intended\nprompts. Importantly, even with larger label sets, the model\nmaintains a reasonable average ranking, with the correct\nlabel placed consistently near the top. This suggests that the\ngenerated designs remain recognizable, even among a large\npool of designs.\nConstructability: Results are in Table II. All designs are\nreliably assembled by the robot without human intervention\nduring assembly. Five of six designs achieve a perfect\nassembly completion rate, and all designs achieve a 98%+\nplacement success rate, highlighting Blox-Net's ability to\nassemble complex structures. Human interventions, which\noccur only during the reset phase, are sometimes needed\nto singulate or reorient blocks. Complex structures, such\nas the Giraffe (9 blocks) or shelf (10 blocks), have more\nhuman reset interventions due to an increasing likelihood of\noverlapping, non-singulated, or misoriented blocks.\nPerturbation Redesign Ablation Results are summarized\nin Table III. Perturbation redesign greatly improves all three\nmetrics across all 5 designs to near-perfect. The percentage"}, {"title": "VII. LIMITATIONS AND CONCLUSION", "content": "While Blox-Net shows promising results in constrained\n3D structure generation, it is limited to non-deformable\ncuboid and cylinder blocks, restricting geometric diversity\nand reducing Blox-Net's ability to represent complex shapes.\nMany assembly designs are still not clearly recognizable,\nlikely due in part to these block limitations. The system uses\nonly a suction-based gripper, without accounting for gripper\nwidth or slanted surfaces, and sometimes requires human\nintervention during the reset process, reducing assembly\nefficacy.\nThis paper introduces Blox-Net, a novel system addressing\nthe Generative Design-for-Robot-Assembly problem using a\nthree-phase approach: creating the initial designs by prompt-\ning a vision language model, conducting simulation-based\nanalysis for constructability, and utilizing a physical robot for\nassembly evaluation. Experiment results suggest that Blox-\nNet can bridge the gap between abstract design concepts and\nrobot-executable assemblies. Remarkably, five Blox-Net as-\nsembly designs, each using 3 to 10 blocks and scoring high in\nrecognizability, were successfully assembled 10 consecutive\ntimes by the robot without any human intervention."}]}