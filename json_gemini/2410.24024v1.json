{"title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents", "authors": ["Yifan Xu", "Xiao Liu", "Xueqiao Sun", "Siyi Cheng", "Hao Yu", "Hanyu Lai", "Shudan Zhang", "Dan Zhang", "Jie Tang", "Yuxiao Dong"], "abstract": "Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose ANDROIDLAB as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. ANDROIDLAB benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the ANDROIDLAB environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMS. ANDROIDLAB is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.", "sections": [{"title": "1 Introduction", "content": "Developing autonomous agents to execute human instructions within mobile operating systems has long been a goal for researchers (Burns et al., 2021; Yang et al., 2023b; Wang et al., 2023a; Hong et al., 2023; Rawles et al., 2023; Li et al., 2020; Romao et al., 2019; Rai et al., 2019). Recently, a significant line of research has focused on using large language models (LLMs) (Zeng et al., 2022; OpenAI, 2023; Anthropic, 2023; Team et al., 2024; GLM et al., 2024) and large multimodal models (LMMs) (OpenAI, 2023; Anthropic, 2023; Hong et al., 2023) as the backbone for these agents (Deng et al., 2023; Rawles et al., 2023; Zhou et al., 2023).\nDespite significant advancements, both training and evaluating mobile agents face challenges, with lacking systematic exploration. Previous benchmarks (Rawles et al., 2023; Sun et al., 2022; Li et al., 2020) often rely on reproducible but static environments, where agents are expected to predict actions based on screenshots without actual interaction. AndroidEnv (Toyama et al., 2021) introduced the first interactive environment for mobile agents and later efforts (Lee et al., 2024; Rawles et al., 2024) improved reproducibility but still faced limitations. Moreover, these benchmarks lack systematic evaluation, primarily because almost all recent benchmarks (Yang et al., 2023b; Xing et al., 2024; Lee et al., 2024; Rawles et al., 2024) only tested and implemented prompt-based improvement on closed-source models. This limitation restricts the ability to analyze model behavior, integrate insights, and conduct reinforcement learning experiments effectively. The absence of a unified benchmark comparing open-source and closed-source models across various modalities further exacerbates this issue, limiting opportunities for enhancing open-source solutions.\nThese issues have motivated us to develop a new Android agent evaluation and training framework. In this paper, we propose ANDROIDLAB, which includes a standard operational environment and a benchmark for agents interacting with Android devices. We define basic operation modes across LLMs and LMMs by aligning actions and objects within different observations of the mobile system: XML and screenshots, termed XML mode and SoM mode, respectively. Additionally, we introduce two modes for each basic mode, ReAct (Yao et al., 2022b) and SeeAct (Zheng et al., 2024). Node information is annotated in the XML for screenshots using set-of-mark (Yang et al., 2023a), ensuring identical actions across modes for a fair comparison. Based on the environment, the ANDROIDLAB benchmark includes 138 tasks across 9 different apps. By utilizing Android virtual devices with preloaded app operation histories and offline data, ANDROIDLAB ensures reproducibility and eliminates external network or time dependencies.\nPrevious benchmarks had shortcomings in their evaluation metrics, typically provided standardized sequences of operations (Xing et al., 2024) or device states (Lee et al., 2024; Rawles et al., 2024) as evaluation metrics, which can restrict the diversity of task paths and limit task types to those represented by specific device states. In ANDROIDLAB, each task is divided into multiple required page states as sub-goals, with UI tree structure matching verifying correct traversal. This enables precise assessment of task completion and progress and allows evaluation of nearly all tasks without being constrained by the limitations of system state representations. We also introduce metrics such as reversed redundancy and reasonable operation to evaluate action efficiency.\nWe have evaluated 17 open-source and closed-source models using the ANDROIDLAB benchmark. Although the GPT series achieved over 30% success rate in both XML and SoM modes, we observed that open-source models performed poorly, with the best reaching only around 5% success rate. Initial attempts to enhance mobile agent performance through more complex reasoning frameworks led to marginal improvements despite significantly increased inference times. Therefore, fine-tuning small-scale open-source models may bridge the gap to closed-source performance, enhancing mobile agent accessibility.\nBy using ANDROIDLAB's operation modes and action space, we have constructed the Android Instruct dataset. We develop an online annotation tool with the same action space, collecting 10.5k traces and 94.3k steps from annotators. Among these, 6208 steps are derived from the Apps included in the ANDROIDLAB benchmark, and we use this portion of the data to fine-tune the model. This dataset includes tasks, phone screen states, XML information, and operations, and has been used to fine-tune six text-only and multimodal models. As shown in Figure 2, fine-tuning with our dataset raises average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. Our further analysis reveals that fine-tuning improves operational accuracy, efficiency, and reduces redundancy in Android agents.\nThe contributions are summarized as follows:\n\u2022 We design the ANDROIDLAB suite, which includes a standard operational environment and a benchmark. This suite unifies the evaluation and training of Android Agents, as shown in Figure 1.\n\u2022 We develop ANDROIDLAB benchmark, a reproducible and challenging benchmark for evaluating mobile agent capabilities. It includes a simulated evaluation environment and 138 tasks, as shown in Figure 3 based on text-only or multimodal inputs. ANDROIDLAB benchmark presents significant challenges, as the leading model GPT-40 only achieves 31.16%.\n\u2022 We construct an Android Instruct dataset, containing 94.3k operation records for fine-tuning. This dataset supports both text-only and multimodal training, yielding competitive results in LLM and LMM models, as shown in Table 1. We also demonstrate that fine-tuned models achieve comparable scores and offer the best balance of efficiency and accuracy."}, {"title": "2 Retated Work", "content": "Benchmarks for Agents. Recent advancements in large foundation models have led to new agent benchmarks tailored to these models. Agents interact with external environments primarily through writing code (Chen et al., 2021; Zheng et al., 2023; Zhang et al., 2024; Austin et al., 2021) or invoking APIs (Guo et al., 2024; Li et al., 2023; Peng et al., 2021). Specialized benchmarks have been designed for interaction with operating systems, categorized into Desktop and Mobile. For Desktop, static benchmarks (Mialon et al., 2023; Deng et al., 2023; Kapoor et al., 2024) evaluate agents by single-step operation or operations sequence without a virtual environment. Otherwise, dynamic benchmarks provide interactive web browser (Liu et al., 2018; Zhou et al., 2023; Yao et al., 2022a; Koh et al., 2024) or Unix-like system virtual environment (Hong et al., 2023; Xie et al., 2024), making evaluation more flexible and realistic.\nMobile benchmarks for Android began with static systems like PixelHelp (Li et al., 2020) and MetaGUI (Sun et al., 2022) and later expanded through AITW (Rawles et al., 2023), which provided over 5 million images. AndroidEnv (Toyama et al., 2021) introduced dynamic evaluations, while Android Arena (Xing et al., 2024) added cross-app evaluations. Although task diversity was limited, B-MOCA (Lee et al., 2024) standardized the Android Virtual Device. AndroidWorld (Rawles et al., 2024) offers reward signals for 116 tasks across 20 real-world apps but does not support instruction-tuning data construction.\nAgents for Interactive System. For Web environments, WebGPT (Nakano et al., 2021) and WebGLM (Liu et al., 2023) integrate LLMs for improved question-answering. MindAct (Deng et al., 2023), WebAgent (Gur et al., 2023), and AutoWebGLM (Lai et al., 2024) focus on executing complex interactive tasks. In mobile agents, early work on Android systems utilized multiple execution modules (Burns et al., 2021; Venkatesh et al., 2023; Li et al., 2020; Zhan and Zhang, 2023). PixelHelp (Li et al., 2020) mapped actions to images, while Auto-GUI (Zhan and Zhang, 2023) used image and text encoders with LLMs for CoT outputs. CogAgent (Hong et al., 2023) achieved SOTA on AITW (Rawles et al., 2023) by combining modules for action prediction. Recent zero-shot mobile agents using GPT-4V (OpenAI, 2023) have shown strong results (Yang et al., 2023b; Zheng et al., 2024; Yan et al., 2023; Wang et al., 2023a), but planning complexity limits inference speed and practical deployability due to security restrictions."}, {"title": "3 ANDROIDLAB", "content": "ANDROIDLAB defines a set of action spaces and two operation modes, forming the ANDROIDLAB environment. We adopt the main action space from prior work and add a model return value (finish action). The two basic operation modes are SoM (Yang et al., 2023a) and XML-only, differing in whether the agent can access a snapshot of the phone screen. For comparison, we also implement ReAct (Yao et al., 2022b) and SeeAct (Zheng et al., 2024). This framework supports real and virtual Android devices and is compatible with Android-like mobile operating systems."}, {"title": "3.1 The Operation Environment", "content": "Action Space. Based on the action spaces from AppAgent (Yang et al., 2023b) and Android Env (Toyama et al., 2021), we define four basic phone operations: Tap, Swipe, Type, Long Press, along with two shortcut keys, Home and Back, as the core action space. We add the Finish action as the final step, allowing the agent to return execution results or answers. This action space applies to all modes.\nXML Mode. XML mode is tailored for text-only input models (LLM). Inspired by Android Arena (Xing et al., 2024), we redesign the XML compression algorithm to convey screen information. The LLM selects corresponding elements directly for operations.\nSoM Mode. SoM mode is for multimodal input models (LMM), based on the Set-of-Mark method (Yang et al., 2023a). Each clickable or focusable element is assigned a serial number, and the LMM selects the element by its number. The selected elements in SoM mode align with those in the compressed XML list, allowing both modes to interact with the same action space and objects.\nThese basic operation modes directly require the agent to output operation commands. Based on these two methods, we further test two novel agent frameworks, ReAct (Yao et al., 2022b) and SeeAct (Zheng et al., 2024). These two frameworks allow the agent to observe and reflect on the environment or more easily select specific tasks to execute. Please refer to Appendix B for more details about our operation modes."}, {"title": "ReAct modes", "content": "Based on the above two modes, we follow (Yao et al., 2022b) to prompt the model, allowing models to think step by step and output their thought and reasoning process."}, {"title": "SeeAct modes", "content": "Following (Zheng et al., 2024), we separate the reasoning and element grounding process. We instruct models to interact for two rounds in a single operation. The models are supposed to generate a detailed description of the desired action and output the real action, respectively."}, {"title": "3.2 The Reproducible Benchmark", "content": "Based on the environment, ANDROIDLAB benchmark offers a deterministic and reproducible evaluation platform, allowing users to perform fair and challenging comparisons of Android agent capabilities. ANDROIDLAB benchmark introduces the following designs:\n\u2022 We gathered 138 tasks from nine apps, ensuring reproducibility. These tasks, derived from common mobile scenarios, are divided into two types: (a) Operation Tasks, where agents must complete a series of actions to meet a goal, and (b) Query Tasks, where agents answer queries based on phone information.\n\u2022 Using phone XML data, we identify screen information that uniquely defines task completion, making task completion our primary metric. Additionally, we select auxiliary metrics such as the proportion of valid actions and the redundancy of successful operation sequences."}, {"title": "3.2.1 Task Formulation", "content": "We formalize each task input as a 4-tuple: $\\text{Task}(E, I, F, M)$. Here, $E$ represents the execution environment of the task, which, in the context of benchmark testing, is the pre-packaged AVD (Android virtual device) image. This includes a fixed phone screen size, Android version, API level, and a fixed app usage state. $I$ denotes the specific natural language instruction for the task. To avoid confusion during testing, we specify the app required to complete the task in natural language. $F$ represents the agent testing framework. Finally, $M$ denotes the backbone model used to perform the task, primarily referring to LLMs or LMMs."}, {"title": "", "content": "Thus, we can formally define the two types of tasks included in ANDROIDLAB:\nOperation Task. $\\text{T}(E, I, F, M) \\rightarrow (S_1,..., S_n)$. The output of this type of task is a sequence of continuous Android virtual machine states.\nQuery Task. $\\text{T}(E, I, F, M) \\rightarrow (S_1,..., S_n, A)$. This type of task assesses the agent's ability to answer specific questions based on the state sequence after exploration. The model must explore the environment to find the answers and output the correct response.\nBased on the above formulation, we design 138 tasks, including 93 Operation Tasks and 45 Query Tasks. Please refer to Appendix A for detailed information."}, {"title": "3.2.2 Reproducible Designs", "content": "To ensure our evaluation reflects real-world agent usage scenarios with an appropriate level of difficulty and full reproducibility, we design the tasks with the following considerations:\n\u2022 Fixed Evaluation Time and Space: We use ADB commands at the start of each evaluation to set the machine's time and virtual geolocation to predetermined values.\n\u2022 Offline Testing: All test apps function offline, with preloaded usage records in the AVD image to ensure normal usability without an internet connection.\n\u2022 Predefined Answers: For query-based tasks, we conduct operations on the corresponding apps in advance to guarantee uniquely determined correct results."}, {"title": "3.2.3 Metrics", "content": "Previous evaluations with virtual environments have relied on indirect metrics like single-step accuracy and operation path matching, leading to imprecise assessments. In response, ANDROIDLAB benchmark introduces a task-completion-based evaluation system that judges directly from device and screen states. Our key metrics are:\n\u2022 Success Rate: For Operation Tasks, we divided a complete task into multiple sub-goals and identified the specific page information for each sub-goal completion. By checking and matching specific UI tree elements, we assess each sub-goal completion status individually. The task is considered successfully executed when all sub-goals are completed. We have also set up a few tasks that can directly use the device state to determine if they were completed correctly. For Query Tasks, advanced LLMs verify if the model's predicted results match the standard answers, avoiding errors from direct string comparisons. We provide an example in Fig 4.\n\u2022 Sub-Goal Success Rate: Tasks are decomposed into sub-goals, and completion is assessed sequentially. This finer metric rewards models with stronger understanding and operational capabilities. Only Operation Tasks include the Sub-Goal Success Rate.\n\u2022 Reversed Redundancy Ratio: As in prior work (Xing et al., 2024), redundancy is measured by comparing the model's operation path length to a human benchmark. We calculate this for completed tasks and take the reciprocal, so higher values indicate less redundancy. We do not report SR < 5 because there are too few completed tasks, which may be affected by a small number of special values. It should also be emphasized that this metric may exceed 100 because the steps of human operation are not necessarily optimal.\n\u2022 Reasonable Operation Ratio: This metric evaluates the proportion of operations after which the screen changed. Unchanged screens indicate the operation was ineffective and thus deemed unreasonable.\nBy incorporating these metrics, our evaluation system provides a comprehensive and precise assessment of an agent's performance in completing specified tasks."}, {"title": "4 Android Instruction Data", "content": "Building an open-source, deployable Android operation agent is a significant challenge in AI research. Previous work on Android agents has focused on using powerful closed-source models to design interaction logic (Zheng et al., 2024; Yang et al., 2023b; Wang et al., 2023a), raising concerns about accessibility, privacy, and efficiency. To address this, we aim to build an open-source mobile agent. The main challenge lies in generating training data for mobile operations to handle open-world tasks in diverse environments.\nWe propose task derivation and expansion methods for task generation, allowing models to generate tasks for specific apps controllably. ANDROIDLAB connects to devices via ADB, enabling compatibility with various real or virtual devices for data generation. Using self-exploration and manual annotation, we generate example operation traces."}, {"title": "4.1 Data Construction", "content": "The primary challenges in data construction include generating executable Android instructions and annotating operation path data. Our approach involves three steps:\n1. Task Derivation and Expansion: We use academic datasets (Rawles et al., 2023; Coucke et al., 2018) and manually write instructions to seed task generation. Language models are employed to create additional tasks, which are reviewed and added to the dataset, ensuring realistic and executable instructions.\n2. Self-Exploration: LLMs and LMMs are used for automatic task exploration, outputting finish when done. Initially, manual selection was used to verify results, but a reward model later replaced it after gathering 500 traces.\n3. Manual Annotation: This process involves four steps: (1) Instruction Check, where annotators evaluate the feasibility of the given task; (2) Preliminary Familiarization, allowing them to explore the app interface before performing tasks; (3) Task Execution, in which the annotators execute and document each task step; and (4) Cross-Verification, where a second annotator reviews the task trace to ensure its accuracy.\nThis combination of autonomous and manual processes resulted in 10.5k traces and 94.3k steps, and we use 726 traces and 6208 steps derived from the Apps included in the ANDROIDLAB benchmark for training. We provide statistics of the Android Instruct dataset in Fig 5. More details are in Appendix C."}, {"title": "4.2 Annotation Tool", "content": "To more accurately and efficiently record operation trajectories and page information (XML), we design an annotation tool.\nAcquisition of Page Information: Android Debug Bridge (ADB) is currently the most widely used tool for obtaining page information (Yang et al., 2023b; Rawles et al., 2024). ADB is a versatile command-line utility that retrieves the XML data of the current page. However, when dealing with a diverse range of mobile applications, ADB sometimes fails to acquire the XML for certain pages. Specifically, ADB waits for all UI components on the page to become idle before retrieving component information. If this process exceeds a predefined time limit, ADB stops the XML acquisition. This issue is particularly evident on mobile pages with dynamic components, such as playback bars and animations in audio players, where continuously active elements prevent ADB from obtaining the XML. To address this, we reimplemented the XML acquisition functionality using the Android Accessibility Service, allowing annotators to determine the appropriate timing for retrieving page XML.\nRecording Operation Trajectories: We mainly need to record three types of user actions: clicks, swipes, and text input. For click actions and swipe actions, annotators complete the actions directly on the phone, while we use ADB commands to capture screen events. Based on the press, release positions, and duration of these events, we determine whether the action was a click or swipe. For text input, we utilize the ADB keyboard to complete the entire input in a single operation, minimizing the number of annotations required. Before each action, the user must first use the annotation tool to record the current page information, ensuring that the recorded page data matches the context observed during human interaction."}, {"title": "4.3 Training", "content": "To explore the effectiveness of our dataset on lightweight open-source models, we select Llama-3.1-8B-Instruct, GLM-4-9B-Chat, Qwen2-7B-Instruct, Llama-3.2-11B-Vision-Instruct, Qwen2-VL-7B-Instruct and CogVLM2 (cogvlm2-llama3-chat-19B) as the training backbones for LLM and LMM, respectively. Due to our preliminary experiments showing that training agents from base models yield better results, we select the base versions of all models for fine-tuning, except for Qwen2-VL-7B-Instruct (as no open-source base model is available). However, we still report the instruct versions as baselines because the base models cannot follow instructions without further tuning. For all training sessions, we use a batch size of 32 and a maximum sequence length of 4096, training for five epochs. The learning rate is set to le-5."}, {"title": "5 Experiments", "content": "Evaluation Settings. In preliminary tests, we found that even though we specified the use of certain apps in the instructions, agents failed to complete tasks because they could not launch the respective apps correctly. To avoid errors caused by a single reason, we start tasks directly within the specified app in the formal experiments and then allow the agent to proceed. Additionally, we set a maximum execution step limit of 25 for each task, with a 3-second interval for the virtual machine to respond to each operation. We generate by greedy search for each task of all models."}, {"title": "5.1 Experiment Setup", "content": "Baseline Models. For large language models (LLMs) with text-only input capability, we selected GPT-40 (OpenAI, 2023), GPT-4-1106-Preview (OpenAI, 2023), Gemini-1.5-Pro (Team et al., 2024), Gemini-1.0 (Team et al., 2024), GLM-4-PLUS (GLM et al., 2024), Llama-3.1-8B-Instruct (Touvron et al., 2023), GLM-4-9B-Chat (GLM et al., 2024) and Qwen2-7B-Instruct (Bai et al., 2023) as the baselines for testing in the XML mode. For large multimodal models (LMMs) with image input capability, we chose GPT-40 (OpenAI, 2023), GPT-4-Vision-Preview (OpenAI, 2023), Gemini-1.5-Pro (Team et al., 2024), Gemini-1.0 (Team et al., 2024), Claude-3.5-Sonnet, Claude-3-Opus (Anthropic, 2023), Llama-3.2-11B-Vision-Instruct (Touvron et al., 2023), Qwen2-VL-7B-Instruct (Wang et al., 2024) and CogVLM2 (Wang et al., 2023b) as the baselines for testing in the SoM mode. We also further evaluated the performance of GPT-40 and Gemini-1.5-Pro under the ReAct and SeeAct frameworks in both modes."}, {"title": "5.2 Main Results", "content": "As shown in Table 1, in the XML mode, GPT-4-1106-Preview outperforms the other models with a Success Rate (SR) of 31.16%, the highest in this mode while also achieving the best Sub-Goal Success Rate (Sub-SR) at 38.21%. Although GPT-40 exhibits slightly lower SR (25.36%), it achieves the highest Reversed Redundancy Ratio (RRR) at 107.45, indicating its strong ability to reduce unnecessary operations. The ROR metric shows that both models in the GPT-4 series perform comparably, with around 86% of operations being reasonable but with room for improvement in efficiency. Other models, such as Gemini-1.5-Pro, show moderate performance, with ROR around 80, but lag in SR.\nIn the SoM mode, GPT-40 again shows dominance, reaching an SR of 31.16% and a Sub-SR of 35.02%, the highest in both categories. GPT-4-Vision-Preview follows closely, but models like Claude-3.5-Sonnet exceeded GPT-40 in RRR (113.40), demonstrating a higher efficiency in task completion with fewer redundant steps. The Reasonable Operation Ratio in SoM mode indicates that models such as tuned LLaMA3.2-11B-Vision achieve the best ROR at 92.57%, showing the most effectiveness in this mode."}, {"title": "5.3 Additional Findings", "content": "Influence of Instruction Tuning. Instruction tuning significantly enhances the performance of models across all four metrics in both XML and SOM modes, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. Notably, GLM4-9B's success rate rose to 21.01%, with its Reasonable Operation Ratio (ROR) improving to 93.25, indicating better operational efficiency. The Reversed Redundancy Ratio (RRR) saw consistent gains, demonstrating reduced unnecessary actions, such as GLM4-9B improving its RRR from 54.43 to 74.81.\nIn SoM mode, models like CogVLM2, LLaMA3.2-11B, and Qwen2-VL-7B showed significant advancements across all four metrics. Qwen2-VL-7B's SR increased from 3.62 to 18.12%, and its ROR rose to 88.29. The Sub-SR and RRR also benefited from tuning, marking improved task breakdown and reduced redundancy. After tuning, the best-performing open-source LLMs are approaching the level of GPT-40, while the top LMMs have surpassed Gemini-1.5-Pro, reflecting comprehensive improvements across success, operational efficiency, and task execution. The tuned models' effective actions (ROR) have also surpassed those of most closed-source models, demonstrating enhanced precision.\nInfluence of Windows Size. As shown in Figure 6, experiments with three Android VMs of varying sizes in SoM mode show optimal agent performance on screens matching commonly used smartphones (e.g., Pixel 7 Pro, Pixel 8 Pro). Performance drops on smaller (Pixel 3a) and larger screens (Pixel Fold) due to increased scrolling needs and landscape orientation challenges, respectively."}, {"title": "Analysis of Agent Frameworks", "content": "We assess ReAct and SeeAct frameworks with GPT-40 and Gemini-1.5-Pro in XML and SoM modes. Table2 shows ReAct significantly improves performance only in XML mode. SeeAct does not enhance performance consistently due to the model's reasoning limitations with multimodal input. We also compare the SoM framework and bbox-only and show SOM is better; please refer to Appendix D.2 for more detail. ReAct and SeeAct frameworks increase token usage, harming efficiency. As per Table 3, XML+ReAct settings produce an average of 67.89 tokens, while models post-Instruction Tuning averaged only 4.96 tokens."}, {"title": "6 Conclusion", "content": "In this paper, we introduced ANDROIDLAB, which includes a standard operational environment and a benchmark for agents interacting with Android devices. By integrating the XML and SoM operation modes, we ensured that the action space was consistent, enabling fair comparisons across different models. ANDROIDLAB benchmark encompasses 138 tasks from nine apps, focusing on reproducibility and real-world relevance, allowing for precise task completion and progress assessment. We also introduced the Android Instruct dataset, comprising 10.5k traces and 94.3k steps, which significantly boosted the performance of open-source models when used for fine-tuning.\nOur experiments demonstrated that fine-tuned open-source models have shown considerable improvements while top-performing closed-source models like GPT-40 and Claude-3.5-Sonnet continue to lead in success rates and efficiency. Notably, fine-tuning raised success rates and operational efficiency, helping some models approach or even surpass closed-source counterparts in certain metrics. These findings highlight the potential of open-source models to enhance mobile agent performance, suggesting that further fine-tuning and optimization could narrow the gap between open and closed-source solutions. Future work could explore minimizing redundancy and improving task efficiency, enhancing the practical deployability of Android agents."}, {"title": "Acknowledgment", "content": "We would like to thank Zhipu AI for sponsoring the computation resources and annotation costs used in this work."}, {"title": "A Details of Tasks", "content": "In our experiment, we use various apps to conduct various tests (succinctly presented in Table 4). The following mobile apps are chosen:\n\u2022 Bluecoins: A personal finance management app used for tracking expenses and income.\n\u2022 Calendar: A calendar app helps in organizing schedules and setting reminders.\n\u2022 Cantook: An e-book reader for storing, managing, and reading e-books.\n\u2022 Clock: A clock app for displaying the time, setting alarms, and using a stopwatch.\n\u2022 Contacts: A contact management app for storing and organizing contact information.\n\u2022 Maps.me: An offline map app for navigation and exploring locations.\n\u2022 PiMusic: A music player app for organizing and playing locally stored music files.\n\u2022 Settings: A settings app for configuring device settings and preferences.\n\u2022 Zoom: A video conferencing app for hosting and joining online meetings.\nThe selection of these apps goes through multiple iterations to ensure their suitability for our evaluation purposes. A key criterion for the final selection is that each app functions independently, without requiring an internet connection or user account login. This ensures that the evaluations can be consistently replicated under the same conditions, eliminating external dependencies and reducing the risk of privacy breaches. As a result, this approach maintains the reliability and reproducibility of our results."}, {"title": "B Detail of Operation Modes", "content": "As shown in Figure 7, in this mode, we prompt models with a task description, interaction history, and current compressed XML information. The models are supposed to output an action in function-call format. The actions are applied on coordinates shown in XML."}, {"title": "B.1 XML mode", "content": "As shown in Figure 8, in this mode, we prompt models with a task description, interaction history, and current screenshot with a set of marks(Yang et al., 2023a). The models are also supposed to output an action in function-call format. Different from XML mode, the actions are performed on specified elements via marked indices."}, {"title": "B.2 SoM mode", "content": "We follow (Yao et al., 2022b) for ReAct prompting. In this mode, we perform both text-only and multimodal testing. The text-only and multi-modal prompts are based on Section B.1 and Section B.2 respectively. We both add prompts that allow models to think step by step before output actions."}, {"title": "B.3 ReAct mode", "content": "We follow (Zheng et al., 2024) for SeeAct prompting. The raw prompts of SeeAct are designed for web browsers. To adopt that in Android environments, we make some modifications, and the final prompts are shown in Figure 9 for multi-modal testing and Figure 10 for text-only testing.\nFor multi-modal and text-only testing, the information on mobile phones is given by screenshots and compressed XML respectively. The models are supposed to generate detailed description of the action and its corresponding element and parameters in round 1, and the expected function-call format in round 2."}, {"title": "B.4 SeeAct mode", "content": "In the process of constructing our data, we utilize crowdsourced annotations. To ensure that the privacy information of the annotators is not disclosed, we adopt the following measures:\n1. Before the annotation begins, we explicitly inform the annotators that the annotated data will be used to fine-tune models, and part of the data will be open-sourced. Annotators who disagree may opt out of the annotation process.\n2. During the annotation process, all annotated data are first stored locally by the annotators. If an annotator believes that specific data involves privacy disclosure, they may choose not to use it or skip the task.\n3. After the annotation is completed, we mask and replace sensitive information such as usernames and chat logs before using the data for training. Additionally, such data will not be open-sourced.\nAll annotators sign formal contracts and are compensated according to reasonable standards."}, {"title": "C Details of Android Instruction Dataset", "content": "We provide the instructions given to the annotators below. Note that our targets are expanded by hand-written instructions or academic datasets with available licenses"}]}