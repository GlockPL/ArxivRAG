{"title": "A Bilayer Segmentation-Recombination Network for Accurate Segmentation of Overlapping C. elegans", "authors": ["Mengqian Ding", "Jun Liu", "Yang Luo", "Jinshan Tang"], "abstract": "Caenorhabditis elegans (C. elegans) is an excellent model organism because of its short lifespan and high degree of homology with human genes, and it has been widely used in a variety of human health and disease models. However, the segmentation of C. elegans remains challenging due to the following reasons: 1) the activity trajectory of C. elegans is uncontrollable, and multiple nematodes often overlap, resulting in blurred boundaries of C. elegans. This makes it impossible to clearly study the life trajectory of a certain nematode; and 2) in the microscope images of overlapping C. elegans, the translucent tissues at the edges obscure each other, leading to inaccurate boundary segmentation. To solve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) for the segmentation of C. elegans instances is proposed. The network consists of three parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer Segmentation Module (BSM), and a Semantic Consistency Recombination Module (SCRM). The CMSM is used to extract the coarse mask, and we introduce a Unified Attention Module (UAM) in CMSM to make CMSM better aware of nematode instances. The Bilayer Segmentation Module (BSM) segments the aggregated C. elegans into overlapping and non-overlapping regions. This is followed by integration by the SCRM, where semantic consistency regularization is introduced to segment nematode instances more accurately. Finally, the effectiveness of the method is verified on the C. elegans dataset. The experimental results show that BR-Net exhibits good competitiveness and outperforms other recently proposed instance segmentation methods in processing C. elegans occlusion images.", "sections": [{"title": "1. Introduction", "content": "C. elegans is an excellent model organism for studying aging due to its ease of maintenance in the laboratory and its transparent body [1], which is easy to dissect and observe. Additionally, it shares genetic homology with humans (60-80%), has a complete genome sequence, exhibits conserved biomolecular responses, and demonstrates high fecundity (250 eggs per worm in just a few days) [2]. In addition, C. elegans has advantages such as a short lifespan of approximately three weeks and a small size, which reduces experimental costs and facilitates high-throughput screening experiments. This makes it an ideal candidate for screening anti-aging drugs [3]. Furthermore, nematode experiments do not raise ethical concerns. These benefits have contributed to numerous groundbreaking discoveries in the field of aging research [4].\nWhen we use C. elegans for aging research, segmentation of individual worms is essential to accurately analyze specific anatomical regions, such as the pharynx, intestine, and reproductive system. This process enables the quantification of molecular and cellular changes, including protein aggregation, lipid accumulation, and mitochondrial dysfunction-key indicators of aging. Additionally, segmentation facilitates the tracking of spatially localized responses to genetic manipulations or pharmacological treatments, providing a detailed understanding of aging processes and the efficacy of potential anti- aging interventions. In the past, segmentation was often performed manually, which was both labor-intensive and time-consuming, limiting the scalability and reproducibility of experiments. Senescence studies require precise tracking and measurement of features such as body length, movement trajectories, and behavioral patterns. The development of instance segmentation techniques has revolutionized this field, enabling researchers to efficiently and accurately isolate the body of a nematode from the background. This advancement not only reduces labor but also enhances the precision of measurements, offering a clearer understanding of the biological changes that occur during aging. In recent years, deep learning has achieved remarkable success in image processing, prompting many researchers to apply these techniques in biological research. As a result, the study of Caenorhabditis elegans has evolved beyond traditional manual observation, with deep learning methods enabling more efficient and accurate analysis.\nOne challenge in segmenting individual worms arises when nematodes overlap or occlude one another (e.g., as shown in Figure 1b). In such cases, the pixels in the image may represent a mixture of multiple nematodes, making it difficult for the segmentation algorithm to accurately distinguish between them. This can lead to errors in instance"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Segmentation of Caenorhabditis elegans", "content": "In recent years, deep learning has achieved remarkable results in the field of image segmentation and classification [5-8]. Deep learning methods use multilayer networks to perform convolution and pooling operations directly on images, extracting key features. This approach offers significant advantages in image segmentation [9-12], including for segmenting C. elegans. For example, in 2020, Zeng et al. [13] enhanced Mask R-CNN with multilevel feature pooling and fusion branches to predict nematode contours. Wang et al. [14] developed a method to detect, segment, and locate pixel coordinates of C. elegans internal structures. In 2022, Xu et al. [15] proposed a method based on instance segmentation [16] to deal with the complexity of nematode cell shapes.\nHowever, these basic segmentation methods described above have limitations: when performing image segmentation or instance segmentation on C. elegans, most studies tend to ignore the occluded regions. This may lead to errors in predicting the behavior and lifespan of C. elegans nematodes. It is important to consider all regions of C. elegans to obtain accurate results."}, {"title": "2.2. Amodal Instance Segmentation", "content": "To solve the above problem, Amodal instance segmentation has been proposed in the field of computer vision in recent years. The concept of 'amodal perception', originally proposed by psychologists, refers to the ability to infer the physical structure of an object even when parts of it are invisible. In 2016, Li and colleagues proposed a method to address the amodal instance segmentation problem [17]. The method involves randomly cropping the image to obtain patches and superimposing instances of other objects on the cropped image. The position and size of these superimposed objects are adjusted to ensure moderate overlap. The first step is to find the minimum bounding box of the visible portion for each superimposed object. Then, these boxes are randomly dithered to simulate the localization noise during testing. Finally, the pixels belonging to"}, {"title": "3. Methods", "content": "The structure of the C. elegans de-overlapping deep neural network proposed in this paper is shown in Figure 2. The net is composed of three modules: Coarse Mask Segmentation Module (CMSM), Bilayer Segmentation Module (BSM), and Semantic"}, {"title": "3.1. Problem formulation", "content": "A C. elegans dataset D consisting of K images and their corresponding annotations.\nEach image contains annotations of bounding box $B_k = \\{b_{ki}\\}^{\\mathcal{A}_k}_{i=1}$ and instance mask\n$M_k = \\{m_{ki}\\}^{\\mathcal{A}_k}_{i=1}$, where $\\mathcal{A}_k$ represents the number of instances in the k-th image. For each nematode group, we decompose the instance mask into overlapping regions $O_k =\\{o_{k,i}\\}^{\\mathcal{A}_k}_{i=1}$ and non-overlapping regions $N_k = \\{n_{k,i}\\}^{\\mathcal{A}_k}_{i=1}$ through logical operations based on the positional relationship between nematodes."}, {"title": "3.2. Coarse Mask Segmentation Module (CMSM)", "content": "In the Coarse Mask Segmentation Module, Mask R-CNN is adopted as the basic framework. Previous research has demonstrated the competitive performance of Mask R-CNN in instance segmentation [18]. Mask R-CNN comprises two stages: the first stage performs feature extraction using the Feature Pyramid Network (FPN) and generates candidate object bounding boxes using the Region Proposal Network (RPN). The second stage generates a region of interest (RoI) features $F_{roi}$ through the RoIAlign layer, which predicts the bounding box $b_{k,i}$ from the detection head while predicting the semantic mask $m_i$ from the instance mask head. As overlapping regions may limit perceptual ability, $m_i$ may contain fuzzy boundaries. In this paper, $m_i$ is denoted as a coarse mask, providing information for sub-region decomposition in the BSM and suppressing interference from the background. The multitask loss L for coarse mask segmentation follows the standard loss function described in [12]. The calculation of the multi-task loss for coarse mask segmentation $L_{coarse}$ is as follows:\n$L_{coarse} = L_{cls} + L_{reg} + L_{cmask}$ (1)\nwhere $L_{cls}$ represents the cross-entropy (CE) loss for classification, $L_{reg}$ represents the Smooth L1 Loss for bounding box regression, and $L_{cmask}$ represents the pixel-wise cross- entropy (CE) loss for segmentation. The calculation formula of $L_{cls}$, $L_{reg}$ and $L_{cmask}$ are as follows:\n$L_{cls} = \\frac{1}{K} \\sum_{k=1}^{K} \\sum_{i=1}^{\\mathcal{A}_k} y_{ic} \\cdot log(p_{ic})$ (2)\nwhere K represents the number of images in the C. elegans dataset D. $\\mathcal{A}_k$ represents the number of instances in the k-th image. $y_{ic}$ is the true label of the i-th sample, and its value is 0 or 1. When the i-th sample category is c, $y_{i,c} = 1$, otherwise $y_{i,c} = 0$. $p_{ic}$ is"}, {"title": "3.3. Bilayer Segmentation Module (BSM)", "content": "BSM is utilized to segment the overlapping regions of C. elegans. BSM comprises overlapping and non-overlapping mask heads with identical architecture. The semantic features in the instance mask header before coarse mask prediction are denoted by $F_{in}$. Pass $F_{rol}$ and $F_{in}$ through a connection block as the input of the overlapping mask head and the non-overlapping mask head, so as to predict the overlapping area $\\hat{o}_{k,i}$ and the non-overlapping area $\\hat{n}_{k,i}$ in the instance. Among them, each mask head consists of 4 convolution layers and 1 deconvolution layer. The convolution layer is used to generate 14\u00d714\u00d7256 features, and the deconvolution layer is used to obtain a resolution of 28\u00d728\u00d71 semantic mask. Pixel-wise CE loss $L_{ce}$ is added to both heads as an explicit constraint for decomposition. $L_{dec}$ is the loss function used in BSM, and its calculation formula is as follows:\n$L_{dec} = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{\\mathcal{A}_k} \\sum_{i=1}^{\\mathcal{A}_k} (L_{cmask}(\\hat{o}_{k,i}, o_{k,i}) + L_{cmask}(\\hat{n}_{k,i}, n_{k,i}))$ (5)"}, {"title": "3.4. Semantic Consistency Recombination Module (SCRM)", "content": "To improve the perception of overlapping instances, SCRM incorporates contextual information to better perceive the overall instance. The features before the last layer in the Overlapping Mask Head and Non-overlapping Mask Head are represented by $F_i^o$ and $F_i^n$. The semantic features between overlapping and non-overlapping areas often contain information about the mutual influence and interaction between objects in complex scenes, which is considered to be the residual information of complex areas. $F_i^o$ and $F_i^n$ pass through the fusion block together with $F_{rol}$, and then are input into the Recombined Mask Head. This paper introduces fusion blocks to SCRM and reuses Rol features to predict instances. This helps SCRM to utilize contextual information of overlapping instances, thereby improving its perception ability and optimizing the thinning mask $\\hat{m}_{ki}$ from SCRM through segmentation loss $L_{rmask}$. The calculation of the $L_{rmask}$ is as follows:"}, {"title": "3.5. United Attention Module (UAM)", "content": "This article introduces a United Attention Module (UAM) in CMSM. The UAM is added respectively after the 1\u00d71 convolutional layers of C1, C2, C3, and C4 to improve the feature fusion stage of the model feature extraction network. The UAM enhances the channel and spatial focusing capabilities of CMSM on C. elegans populations, as demonstrated in Figure 3. The UAM comprises of three key components: convolutional layers with varying kernel sizes, a channel attention module, and a spatial attention module, as illustrated in Figure 4. The input feature map undergoes three parallel convolutional layers to generate three feature maps with distinct receptive fields. These convolutional layers consist of a 5\u00d75 convolution, a 3\u00d73 dilated convolution with a dilation rate of 3, and a 3\u00d73 convolution. The feature maps obtained from the three convolutional layers can be represented as:\n$F_3 = F_{input} \\times MT_{3\\times 3}$ (10)\nwhere $F_3$ and $F_5$ represent the feature maps captured by the 3\u00d73 convolution and 5\u00d75 convolution layers respectively. $F_f$ represents the feature map captured by dilated convolution. $F_{input}$ represents the input feature map, $MT_{3\\times 3}$ and $MT_{5\\times 5}$ represent the matrices of 3\u00d73 convolution and 5\u00d75 convolution respectively. $MT_{3\\times 3}$ represents the matrix of dilated convolution."}, {"title": "3.5.1. Channel attention module", "content": "The channel attention module helps enhance the expressive power of channels with rich semantic information. To capture useful objective features from different receptive fields, this paper introduces a channel attention module that guides the network to learn more robust feature representations. The blue section in Figure 4 depicts the channel attention module. The channel attention module enables the network to focus on the most relevant channels for the task, enhancing the model's sensitivity to key features. This results in the selection of more representative features from the channel dimension. Specifically, the module compresses the combined feature map of $F_5$ and $F_f$ into a new"}, {"title": "3.5.2. Spatial attention module", "content": "Previous research has concluded that channel attention aims to establish the relationship between different channels, allowing the network to prioritize important feature channels and suppress unimportant ones. Meanwhile, spatial attention aims to"}, {"title": "3.6. Loss function", "content": "The instance segmentation framework proposed in this paper for C. elegans is trained in a fully supervised manner. The target loss L is defined as follows:\n$L=L_{coarse}+\\lambda_{dec}L_{dec}+\\lambda_{rmask}L_{rmask}+\\lambda_{cons} L_{cons}$ (21)\nThe loss for Rol extraction and coarse mask prediction is represented by $L_{coarse}$, while $L_{dec}$ represents the decomposition loss for overlapping and non-overlapping region segmentation. $L_{rmask}$ is the segmentation loss for refined masks, and $L_{cons}$ supervises the semantic consistency between the overall instance and sub-regions. $\\lambda_{dec}$, $\\lambda_{rmask}$ and $\\lambda_{cons}$ are trade-off parameters that control the importance of each component."}, {"title": "4. Experiments and Analysis", "content": ""}, {"title": "4.1. Dataset", "content": "While C. elegans is a well-studied organism in biology, there are limited publicly available and fully annotated image datasets for this species. To evaluate the effectiveness"}, {"title": "4.2. Evaluation Measurement", "content": "To evaluate the performance of the proposed BR-Net, we used four widely adopted instance segmentation metrics: average precision (AP), average precision at IoU thresholds of 0.5 (AP50) and 0.75 (AP75), and mean intersection over union (mIoU). Precision measures the proportion of correctly identified positive instances, while recall quantifies the model's ability to detect true instances. AP, the area under the precision- recall (P-R) curve, captures average accuracy across different recall levels. IoU (Intersection over Union) quantifies the overlap between predicted and ground truth boundaries, with AP50 and AP75 calculated at IoU thresholds of 0.5 and 0.75, respectively. mIoU, the average IoU across all categories, is particularly important for overlapping segmentation tasks as it evaluates both the accuracy of individual instance segmentation"}, {"title": "4.3. Parameter setting and training", "content": "The article implements the BR-Net method using PyTorch 1.11.0 and trains it on a server with two NVIDIA RTX3090 GPU 24 GB graphics cards. The experiment uses Mask R-CNN as the baseline model and the FPN network based on ResNet-50. The training process sets the batch size to 8 and the initial learning rate to 0.01, which gradually decreases to 0.001. Additionally, linear warmup is added in the first 20 iterations. The network is trained for 200 iterations using Adam as the optimizer in this paper. A comparative experiment is conducted between this method and other instance segmentation methods, followed by an ablation experiment on each component of this method."}, {"title": "4.4. Result and Analysis", "content": ""}, {"title": "4.4.1. Comparative Experiments on Various Instance Segmentation Methods", "content": "We conducted comparative experiments on two datasets, C. Data-1 and C. Data-2, to evaluate the performance of classic segmentation methods [12]-[14][23], amodal instance segmentation methods [17]-[20], and worm-specific approaches [25]. The results indicate that worm-specific methods generally outperform classic approaches in"}, {"title": "4.4.2. Ablation experiment", "content": "This paper presents the results of two ablation experiments. The first experiment aimed to investigate the effect of different modules in the proposed BR-Net on various evaluation indicators. The comparison results are presented in Table 2. The introduction of BSM enabled successful segmentation of clustered C. elegans instances into overlapping and non-overlapping areas. Table 2 shows that the AP50 in the C. Data-1 data set improved by 15%. However, incorporating BSM without considering structural and"}, {"title": "5. Conclusion", "content": "Caenorhabditis elegans serves as a widely embraced miniature model organism in life science research, particularly within the domain of lifespan investigation. In this context, this paper introduces a Bilayer Segmentation-Recombination Network (BR-Net) designed to accurately segment overlapping C. elegans images, addressing the challenge of delineating such instances effectively. The network comprises three key components: the Coarse Mask Segmentation Module (CMSM), Bilayer Segmentation Module (BSM), and Semantic Consistency Recombination Module (SCRM). To enhance the perception of C. elegans instances, we propose the integration of a United Attention Module (UAM). Comparative experiments demonstrate that our method yields more precise segmentation results compared to existing models across both training and test sets. The BR-Net segmentation approach facilitates deeper analysis and comprehension of individual nematodes, thereby advancing life science research. Future endeavors will involve augmenting the dataset to accommodate the scarcity of C. elegans images, alongside"}]}