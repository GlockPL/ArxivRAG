{"title": "OPTIMIZING ATTENTION WITH MIRROR DESCENT: GENERALIZED MAX-MARGIN TOKEN SELECTION", "authors": ["Aaron Alvarado Kristanto Julistiono", "Davoud Ataee Tarzanagh", "Navid Azizan"], "abstract": "Attention mechanisms have revolutionized several domains of artificial intelligence, such as natural language processing and computer vision, by enabling models to selectively focus on relevant parts of the input data. While recent work has characterized the optimization dynamics of gradient descent (GD) in attention-based models and the structural properties of its preferred solutions, less is known about more general optimization algorithms such as mirror descent (MD). In this paper, we investigate the convergence properties and implicit biases of a family of MD algorithms tailored for softmax attention mechanisms, with the potential function chosen as the p-th power of the lp-norm. Specifically, we show that these algorithms converge in direction to a generalized hard-margin SVM with an lp-norm objective when applied to a classification problem using a softmax attention model. Notably, our theoretical results reveal that the convergence rate is comparable to that of traditional GD in simpler models, despite the highly nonlinear and nonconvex nature of the present problem. Additionally, we delve into the joint optimization dynamics of the key-query matrix and the decoder, establishing conditions under which this complex joint optimization converges to their respective hard-margin SVM solutions. Lastly, our numerical experiments on real data demonstrate that MD algorithms improve generalization over standard GD and excel in optimal token selection.", "sections": [{"title": "1 INTRODUCTION", "content": "Attention mechanisms (Bahdanau et al., 2014) have transformed natural language processing (NLP) and large language models (LLMs). Initially developed for encoder-decoder recurrent neural networks (RNNs), attention enables the decoder to focus on relevant input segments rather than relying solely on a fixed-length hidden state. This approach became fundamental in transformers (Vaswani et al., 2017), where attention layers-computing softmax similarities among input tokens are the architecture's backbone. Transformers have driven rapid advancements in NLP with models like BERT (Devlin et al., 2019) and ChatGPT (OpenAI, 2023), and have become the preferred architecture for generative modeling (Chen et al., 2021b; Ramesh et al., 2021), computer vision (Dosovitskiy et al., 2021; Radford et al., 2021), and reinforcement learning (Driess et al., 2023; Chen et al., 2021a). This has led to increased exploration of the mathematical foundations of attention's optimization.\nTo understand optimization dynamics of attention mechanisms, Tarzanagh et al. (2024; 2023) studied the implicit bias of gradient descent (GD) in a binary classification setting with a fixed linear decoder. This bias refers to the tendency of GD to learn specific weight characteristics when multiple valid solutions exist. For example, in linear logistic regression on separable data, GD favors solutions aligned with the max-margin class separator (Soudry et al., 2018; Ji & Telgarsky, 2018). Similarly, Tarzanagh et al. (2023; 2024) propose a model resembling a hard-margin Support Vector Machine (SVM)-specifically, (lp-AttSVM) with p = 2\u2014which maximizes the margin between optimal and non-optimal input tokens based on their softmax logits. These studies show that as training"}, {"title": "2 PRELIMINARIES", "content": "Notations. Let N > 1 and [N] = {1, 2, ..., N}. Vectors are denoted by lowercase letters (e.g., a), with components ai, and matrices by uppercase letters (e.g., A). The minimum and maximum of scalars a and b are a \u2227 b and a \u2228 b, respectively. For a vector v \u2208 Rd, the p-norm is $||v||_p = (\\sum_{i=1}^{d}|v_i|^P)^{1/p}$. For a matrix M \u2208 Rd\u00d7d, the p, p-norm is $||M||_{p,p} = (\\sum_{i=1}^{d}\\sum_{j=1}^{d}|M_{ij}|^P)^{1/p}$."}, {"title": "", "content": "When p = 2, these are the Euclidean norm for vectors and the Frobenius norm for matrices. For any two matrices X, Y of the same dimensions, we define $\u27e8X, Y\u27e9 := trace(X^TY)$. Throughout, for a differentiable function $f : R^{d\u00d7d} \u2192 R$, we define $D_f : R^{dxd} \u00d7 R^{dxd} \u2192 R$ as\n$D_f(W,V) := f(W) \u2013 f(V) \u2013 \u27e8\u2207f(V), W \u2013 V\u27e9$.\nAsymptotic notations O and \u03a9 hide constant factors, and all logarithms are natural (e-base).\nSingle-head attention model. Given input sequences X, Z \u2208 RT\u00d7d with length T and embedding dimension d, the output of a single-head (cross)-attention layer is computed as:\n$\u03c3(XW_QW_K^ZT)XW_v$,\nwhere $W_Q, W_K \u2208 R^{d\u00d7d_1}, W_v \u2208 R^{d\u00d7d_2}$ are trainable key, query, value matrices, respectively; $\u03c3(XW_QW_K^ZT)$ is the attention map; and $\u03c3(\u00b7) : R^{T\u00d7T} \u2192 R^{T\u00d7T}$ denotes the row-wise softmax function applied row-wise on $XW_QW_K^ZT$. Similar to Tarzanagh et al. (2024; 2023), we reparameterize the key-query product matrix as $W := W_QW_K \u2208 R^{d\u00d7d}$, and subsume the value weights $W_v$ within the prediction head $v \u2208 R^d$. Suppose the first token of Z, denoted by z, is used for prediction. Then, the attention model can be formulated as\n$f(X, z) = v^\u03a4\u03c3(XWz)$.\nAttention-based empirical risk minimization. We consider a one-layer attention model (2) for binary classification. Consider the dataset $(X_i, Y_i, z_i)_{i=1}^n$, where $X_i \u2208 R^{T\u00d7d}$ is the input with T tokens each of dimension d, $y_i \u2208 {\u00b11}$ is the label, and $z_i \u2208 R^d$ is the token used for comparison. We use a smooth decreasing loss function $l: R \u2192 R$ and study empirical risk minimization (ERM):\n$\\min_{v \\in R^d, W \\in R^{dxd}} L(v, W) := \\frac{1}{n} \\sum_{i=1}^{n}l (y_iv^\u03a4\u03c3(X_iWz_i)).$\nThroughout, we will use $L(W)$ to denote the objective of (ERM) with fixed v.\nThe highly nonlinear and nonconvex nature of the softmax operation makes the training problem described in (ERM) a challenging nonconvex optimization task for W, even with a fixed v. Next, we provide an assumption on the loss function necessary to demonstrate the convergence of MD for margin maximization within the attention mechanism.\nAssumption A. Within any closed interval, the loss function $l : R \u2192 R$ is strictly decreasing and differentiable, and its derivative $l'$ is bounded and Lipschitz continuous.\nAssumption A aligns with the assumptions on loss functions in Tarzanagh et al. (2024; 2023). Common loss functions, such as $l(x) = e^{-x}, l(x) = -x$, and $l(x) = log(1 + e^{-x})$, satisfy this assumption.\nPreliminaries on mirror descent. We review the MD algorithm (Blair, 1985) for solving attention-based (ERM). Mirror descent is defined using a potential function. We focus on differentiable and strictly convex potentials $\u03c8$ defined on the entire domain $R^{d\u00d7d}$. Note that in general, the potential function is a convex function of Legendre type (Rockafellar, 2015, Section 26). We call $\u2207\u03c8$ the mirror map. The natural \u201cdistance\u201d associated with the potential \u03c8 is given by the Bregman divergence (Bregman, 1967a).\nDefinition 1 (Bregman Divergence). For a strictly convex function $\u03c8 : R^{d\u00d7d} \u2192 R$, the expression $D\u03c8(\u00b7,\u00b7)$ defined in (1) is called the Bregman divergence.\nAn important example of a potential function is $\u03c8 = \\frac{1}{2}||\u00b7 ||_2^2$. In this case, the Bregman divergence simplifies to $D\u03c8(W,V) = \\frac{1}{2}||W \u2013 V||_F^2$; For more details, see Bauschke et al. (2017). MD with respect to the mirror map \u03c8 is a generalization of GD where the Bregman divergence is used as a measure of distance. Given a stepsize \u03b7 > 0, the MD algorithm is as follows:\n$W^{(k + 1)} \\leftarrow \\arg \\min_{W\\in R^{dxd}} \\{\u03b7^{-1}D_\u03c8(W, W^{(k)}) + \\langle \u2207L(W^{(k)}), W \\rangle \\}$.\nEquivalently, MD can be written as $\u2207\u03c8(W(k + 1)) = \u2207\u03c8(W(k)) \u2013 \u03b7\u2207L(W(k))$; see Bubeck et al. (2015); Juditsky & Nemirovski (2011). A useful fact about the Bregman divergence is that it is non-negative and $D\u03c8(W, V) = 0$ if and only if W = V.\nPreliminaries on attention SVM. Following Tarzanagh et al. (2024; 2023), we use the following definition of token scores."}, {"title": "3 IMPLICIT BIAS OF MIRROR DESCENT FOR OPTIMIZING ATTENTION", "content": "Definition 2 (Token Score). For prediction head v \u2208 Rd, the score of token $X_{it}$ is $V_{it} = y_iv^\u03a4X_{it}$.\nIt is important to highlight that the score is determined solely based on the value embeddings $v^TX_{it}$ of the tokens. The softmax function \u03c3(\u00b7) minimizes (ERM) by selecting the token with the highest score (Tarzanagh et al., 2023, Lemma 2). Using (2), Tarzanagh et al. (2023) defines globally optimal tokens $(opt_i)_{i=1}^n$, with each $opt_i$ maximizing the score for $X_{iopt_i}$. For our MD analysis, we primarily consider locally optimal tokens, as they are more general than globally optimal ones. Locally optimal tokens (Tarzanagh et al., 2024; 2023) are characterized by having scores that surpass those of nearby tokens. Intuitively, these are the tokens that locally minimize (ERM) upon selection and can be defined based on support tokens. Before presenting the mathematical notion of locally optimal tokens, we provide the formulation of the attention SVM problem. Given a set of (locally) optimal token indices $(a_i)_{i=1}^n$, Tarzanagh et al. (2023) defines the following hard-margin attention SVM problem, which aims to separate, with maximal margin, (locally) optimal tokens from the rest of the tokens for every input sequence:\n$W_{mm} := \\arg \\min_{W\\in R^{dxd}} ||W||_F \\text{ S.t. } (X_{ia_i} - X_{it})^T Wz_i \u2265 1, \\text{ for all } t \\in [T] - \\{a_i\\}, i \\in [n].$\nThe constraint $(X_{ia_i} \u2013 X_{it})^T Wz_i \u2265 1$ indicates that in the softmax probability vector $\u03c3(X_iWz_i)$, the $a_i$ component has a significantly higher probability compared to the rest, and so these problems solve for a sort of probability separator that has the lowest norm.\nDefinition 3 (Globally and Locally Optimal Tokens). Consider the dataset $(X_i, Y_i, z_i)_{i=1}^n$\n1. The tokens with indices $opt = (opt_i)_{i=1}^n$ are called globally optimal if they have the highest scores, given by $opt_i \u2208 arg \\max_{t\u2208[T]} V_{it}$.\n2. Fix token indices $(a_i)_{i=1}^n$ for which (3) is feasible to obtain $W_{mm}$. Let the support tokens $T_i$ for the ith data be the set of tokens \u03c4 such that $(X_{ia_i} - X_{i\u03c4})^T W_{mm}z_i = 1$. The tokens with indices $(a_i)_{i=1}^n$ are called locally optimal if, for all i \u2208 [n] and $\u03c4 \u2208 T_i$, the scores per Def. 2 obey $V_{ia_i} > V_{i\u03c4}$.\nIt is worth noting that token scoring and optimal token identification can help us understand the importance of individual tokens and their impact on the overall objective. A token score measures how much a token contributes to a prediction or classification task, while an optimal token is defined as the token with the highest relevance in the corresponding input sequence (Tarzanagh et al., 2024; 2023). For illustration, please refer to Figure 1."}, {"title": "3.1 OPTIMIZING ATTENTION WITH FIXED HEAD v", "content": "In this section, we assume the prediction head is fixed and focus on the directional convergence of MD and its token selection property through the training of the key-query matrix W. The analysis will later be expanded in Section 3.2 to include the joint optimization of both v and W.\nWe investigate the theoretical properties of the main algorithm of interest, namely MD with $\u03c8(\u00b7) = \\frac{1}{p}||\u00b7 ||_p,p^p$ for p > 1 for training (ERM) with fixed v. We shall call this algorithm lp-norm AttGD because it naturally generalizes attention training via GD to lp geometry, and for conciseness, we will refer to this algorithm by the shorthand lp-AttGD. As noted by Azizan et al. (2021), this choice of mirror potential is particularly of practical interest because the mirror map \u2207\u03c8 updates become separable in coordinates and thus can be implemented coordinate-wise independently of other coordinates.\n$\u2200i, j\u2208 [d]$,\n$\\begin{cases} [W^{(k+1)}]_{ij} \\leftarrow |[W^{(k)}]_{ij}|sign ([W^{(k)}]_{ij}), \\\\ [\\tilde{W}^{(k)}]_{ij} := |[W^{(k)}]_{ij}|^{p-1}sign([W^{(k)}]_{ij}) \u2013 \u03b7[\u2207L(W^{(k)})]_{ij}. \\end{cases}$\nIn the following, we first identify the conditions that guarantee the convergence of lp-AttGD. The intuition is that, for attention to exhibit implicit bias, the softmax nonlinearity should select the locally optimal token within each input sequence. Tarzanagh et al. (2023) shows that under certain assumptions, training an attention model using GD causes its parameters' direction to converge."}, {"title": "", "content": "This direction can be found by solving a simpler optimization problem, such as (3), which selects the locally optimal token. Depending on the attention model's parameterization, the attention SVM varies slightly. In this work, we generalize (3) using the lp-norm as follows:\nDefinition 4 (Attention SVM with lp-norm Objective). For a dataset $\\{(X_i, Y_i, z_i)\\}_{i=1}^n$ with $Y_i \u2208 {\u00b11}, X_i \u2208 R^{T\u00d7d}$, and token indices $(a_i)_{i=1}^n$, lp-based attention SVM is defined as\n$W_{mm} := \\arg \\min_{W\\in R^{dxd}} || W ||_{p,p}$\n$\\text{subj. to } (X_{ia_i} - X_{it})^T Wz_i \u2265 1, \\text{ for all } t \\in [T] - \\{a_i\\}, i \\in [n].$\nProblem (lp-AttSVM) is strictly convex, so it has unique solutions when feasible. Furthermore, under mild over-parameterization, $d > \\max{T \u2013 1, n}$, the problem is almost always feasible (Tarzanagh et al., 2023, Theorem 1). Next, we assert that the solution to the (lp-AttSVM) problems determines the direction that the attention model parameters approach as the training progresses.\nExample 1. Consider the matrices $X_1 = \\begin{bmatrix} 5, 0; 0, 1 \\end{bmatrix}$ and $X_2 = \\begin{bmatrix} -5, 0; 0, 1 \\end{bmatrix}$ with $Y_1 = -Y_2 = 1$. Let $X_{i1}$ be the optimal token and $X_{it}$ be the others. Solving Problem (lp-AttSVM) with p = 3 and setting $z_i = X_{i1}$, we obtain the solution $W_{mm} := W_{mm} = \\begin{bmatrix} 0.03846, 0; 0.00769, 0 \\end{bmatrix}$.\nTheorem 1 (lp-norm Regularization Path). Suppose Assumption A on the loss function holds. Consider the ridge-constrained solutions $W(R)$ of (ERM) defined as\n$W(R) := \\arg \\min_{W\\in R^{dxd}} L(W) \\text{ subj. to } ||W||_{p,p} \u2264 R.$\nThen, $\\lim_{R\u2192\u221e} W(R)/R = W_{opt}/||W_{opt}||_{p,p}$, where $W_{opt}$ is the solution of (lp-AttSVM), with $a_i$ replaced by $opt_i$.\nTheorem 1 shows that as the regularization strength R increases, the optimal direction W(R) aligns more closely with the max-margin solution Wmm. This theorem, which allows for globally optimal tokens (see Definition 3), does not require any specific initialization for the lp-AttRP algorithm and demonstrates that max-margin token separation is an essential feature of the attention mechanism.\nNext, we present the convergence of MD applied to (ERM). Under certain initializations, the parameter's lp-norm increases to infinity during training, with its direction approaching the (lp-AttSVM) solution. To describe these initializations, we introduce the concept of cone sets.\nDefinition 5. Given a square matrix W \u2208 Rd\u00d7d, \u03bc \u2208 (0,1), and some R > 0,\n$\\begin{aligned} S_{p,\u03bc}(W) &:= \\{W' \u2208 R^{d\u00d7d} | D_\u03c8(\\frac{W}{||W||_{p,p}}, \\frac{W'}{||W'||_{p,p}}) \u2264 \u03bc \\} \\\\\\ C_{p,\u03bc,R}(W) &:= S_\u00b5(W) \u2229 \\{W' | ||W'||_{p,p} \u2265 R\\} .\\end{aligned}$\nThese sets contain matrices with a similar direction to a reference matrix W, as captured by the inner product in S\u00b5(W). For $C_{p,\u03bc,R}(W)$, there is an additional constraint that the matrices must have a sufficiently high norm. We note that $S_{p,\u00b5}(W)$ and $C_{p,\u00b5,R}(W)$ reduce to their Euclidean variants as described in Tarzanagh et al. (2024; 2023). With this definition, we present our first theorem about the norm of the parameter increasing during training.\nTheorem 2. Suppose Assumption A holds. Let $(a_i)_{i=1}^n$ be locally optimal tokens as per Definition 3. Consider the sequence $W^{(k)}$ generated by Algorithm lp-AttGD. For a small enough stepsize \u03b7, if $W(0) \u2208 C_{p,\u00b5,R}(W_{mm})$ for some dataset-dependent constants \u00b5, R > 0, then we have $\\lim_{k\u2192\u221e}||W(k)||_{p,p} = \u221e$."}, {"title": "", "content": "Remark 1. The condition on the stepsize \u03b7 is that it must be sufficiently small so that $\u03c8(\u00b7) \u2013 \u03b7L(\u00b7)$ remains convex for the matrices W along the path traced by the iterates W(k). Specifically, there exists an index k and a real number r \u2208 [0,1] such that $W = rW^{(k)} + (1 - r)W^{(k + 1)}$. This restriction applies to all theorems in this paper that require a sufficiently small stepsize \u03b7.\nThis theorem implies that the parameters will increase and diverge to infinity, justifying the need to characterize the convergence of their direction.\nTheorem 3 (Convergence of lp-AttGD). Suppose Assumption A holds. Let $(a_i)_{i=1}^n$ be locally optimal tokens as per Definition 3. Consider the sequence $W^{(k)}$ generated by Algorithm lp-AttGD. For a small enough stepsize \u03b7, if $W(0) \u2208 C_{p,\u00b5,R}(W_{mm})$ for some dataset-dependent constants \u03bc > 0, R > exp(2), then\n$\\lim_{k\u2192\u221e} \\frac{W(k)}{||W(k)||_{p,p}} = \\frac{W_{mm}}{||W_{mm}||_{p.p}}$\nThese theorems show that as the parameters grow large enough and approach a locally optimal direction, they will keep moving toward that direction.\nTheorem 4 (Convergence Rate of lp-AttGD). Suppose Assumption A holds. Let $(a_i)_{i=1}^n$ be locally optimal tokens as per Definition 3. Consider the sequence $W^{(k)}$ generated by Algorithm lp-AttGD. For a small enough stepsize \u03b7, if $W(0) \u2208 C_{p,\u00b5,R}(W_{mm})$ for some \u03bc > 0, R > exp(2), then\n$D_\u03c8(\\frac{W_{mm}}{||W_{mm}||_{p,p}}, \\frac{W^{(k)}}{||W^{(k)}||_{p,p}}) =  \\begin{cases} O(\\frac{log log k}{log k})  &\\text{if } p > 2, \\\\ O(\\frac{(log log k)^2}{log k}) &\\text{if } p = 2, \\\\ O(\\frac{1}{(log k)^{p-1}}) &\\text{otherwise}. \\end{cases}$\nDespite optimizing a highly nonlinear, nonconvex softmax function, we achieve a convergence rate similar to GD in linear binary classification (Ji & Telgarsky, 2018, Theorem 1.1) (up to a log log k factor). The theorems we prove hinge on the parameter entering the set $W^{(k)} \u2208 C_{p,\u00b5,R}(W_{mm})$ with a high enough norm. Since we aim to show the parameter converges in direction to the cone center, Wmm, we need conditions ensuring the parameters remain in the cone. We formalize this in Lemma 17 and prove that for any \u03bc > 0 and locally optimal tokens $(a_i)_{i=1}^n$ (Definition 3), there exist constants R, \u03bc' > 0 depending on the dataset and \u03bc, such that if $W(0) \u2208 C_{p,\u03bc',R}(W_{mm})$, then $W^{(k)} \u2208 C_{p,\u00b5,R}(W_{mm})$ for all k, meaning the iterates remain within a larger cone; see Figure 2.\nFor Theorem 2, we show in Lemma 11 that at any timestep k \u2265 0, the norm of the W parameter evolves in the following manner,\n$||W(k+1)||_{p^{-1}} \u2265 ||W(k)||_{p^{-1}} + \\frac{\u03b7}{||W(k)||_{p,p}}\u27e8-\u2207L(W(k)), W(k)\u27e9$.\nWith the above, to prove Theorem 2, it is enough to show that $\u27e8-\u2207L(W(k)), W(k)\u27e9$ is positive and large enough to keep the norm increasing to infinity. Specifically, in Lemma 9 we show that there exist dataset-dependent constants R, \u03b4, \u03bc > 0 such that for all $W, V \u2208 C_{p,\u00b5,R}(W_{mm})$ with $||V||_{p,p} = ||W_{mm}||_{p,p}$,\n$\\langle -\u2207L(W), V \\rangle = \u03a9(\\frac{\u03b4}{||W||_{p,p}}exp(-\u03b1 \\frac{1}{||Wmm||_{p,p}}(1 + \\frac{1}{\\Gamma})) > 0.$\nTheorem 3 is a direct consequence of Theorem 4, which extends the analysis that is done for Lemma 17 by providing a tighter bound on how thin the cone set Cp,\u00b5,R may be for later iterates."}, {"title": "3.2 TRAINING DYNAMICS OF MIRROR DESCENT FOR JOINT OPTIMIZATION OF W AND V", "content": "This section explores the training dynamics of jointly optimizing the prediction head v and attention weights W. Unlike Section 3.1, the main challenge here is the evolving token scores y influenced"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "4.1 SYNTHETIC DATA EXPERIMENTS\nWe describe the setup of the experiments for lp-AttGD and lp-JointGD and their results.\nlp-AttGD Experiment. To measure the directional distance between Wmm (solution of (lp-AttSVM)) and W(k) (output of lp-AttGD), we use a directional Bregman divergence, defined as D\u03c8(W/||W||p,p, V/||V||p,p) for W, V \u2208 Rd\u00d7d. We compare the (lp-AttSVM) solution with the lq optimization path for all p,q \u2208 {1.75,2,3} for synthetically generated data (described in detail in the Appendix). The experiment is repeated 100 times, and the average directional Bregman divergence is reported. A closer look at one sample trial is also provided."}, {"title": "5 CONCLUSION", "content": "We explored the optimization dynamics and generalization performance of a family of MD algorithms for softmax attention mechanisms, focusing on lp-AttGD, which generalizes GD by using the p-th power of the lp-norm as the potential function. Our theoretical analysis and experiments show that lp-AttGD converges to the solution of a generalized hard-margin SVM with an lp-norm objective in classification tasks using a single-layer softmax attention model. This generalized SVM separates optimal from non-optimal tokens via linear constraints on token pairs. We also examined the joint problem under logistic loss with lp-norm regularization and proved that W and v generated by lp-norm regularization path converge to their respective generalized max-margin solutions. Finally, our numerical experiments on real data demonstrate that MD algorithms improve generalization over standard GD and excel in optimal token selection."}, {"title": "A ADDITIONAL RELATED WORK", "content": "Transformers Optimization. Recently, the study of optimization dynamics of attention mechanisms has garnered significant attention (Deora et al., 2023; Huang et al., 2023; Tian et al., 2023b; Fu et al., 2023; Li et al., 2024; Tarzanagh et al., 2024; 2023; Vasudeva et al., 2024a; Sheen et al., 2024; Deng et al., 2023; Makkuva et al., 2024; Jeon et al., 2024; Zheng et al., 2023; Collins et al., 2024; Chen & Li, 2024; Li et al., 2023; Sheen et al., 2024; Ildiz et al., 2024; Vasudeva et al., 2024b; Bao et al., 2024; Chen et al., 2024; Huang et al., 2024; Wang et al., 2024; Zhao et al., 2024). We discuss the works most closely related to this paper. Studies such as Sahiner et al. (2022); Ergen et al. (2022) investigate the optimization of attention models through convex relaxations. Jelassi et al. (2022) demonstrate that Vision Transformers (ViTs) identify spatial patterns in binary classification via gradient methods. Li et al. (2023) provide sample complexity bounds and discuss attention sparsity in SGD for ViTs. Oymak et al. (2023) and Deora et al. (2023) explore optimization dynamics in prompt-attention and multi-head attention models, respectively. Tian et al. (2023a;b) study SGD dynamics and multi-layer transformer training. Tarzanagh et al. (2024; 2023) explored GD's implicit bias for attention optimization. Vasudeva et al. (2024a) discusses the global directional convergence and convergence rate of GD for attention optimization under specific data conditions. Sheen et al. (2024) notes that gradient flow not only achieves minimal loss but also minimizes the nuclear norm of the key-query weight W. Thrampoulidis (2024); Li et al. (2024); Zhao et al. (2024) also studied the optimization dynamics of attention mechanisms and provided the implicit bias of GD for next token prediction. Our work extends these findings and those of Tarzanagh et al. (2024; 2023), focusing on the implicit bias of the general class of MD algorithms for attention training."}, {"title": "Implicit Bias of First Order Methods", "content": "In recent years, significant progress has been made in understanding the implicit bias of gradient descent on separable data, particularly highlighted by the"}, {"title": "B AUXILIARY LEMMAS", "content": "B.1 ADDITIONAL NOTATIONS\nConsider the following constants for the proofs, depending on the dataset (Xi, Yi, zi)=1, the parame-ter v, and the locally optimal token (ai)=1:\n$\ndelta' := \\frac{1}{2}\\min_i \\min_{t \\in [T]} \\{ (X_{ia_i} \u2013 X_{it})^T W_{mm}z_i - 1 \\}$"}, {"title": "", "content": "$\ndelta := \\min\\{0.25, \\delta'\\}$"}]}