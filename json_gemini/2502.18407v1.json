{"title": "AgentRM: Enhancing Agent Generalization with Reward Modeling", "authors": ["Yu Xia", "Jingru Fan", "Weize Chen", "Siyu Yan", "Xin Cong", "Zhong Zhang", "Yaxi Lu", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly fine-tuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by 8.8 points on average, surpassing the top general agent by 4.0. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of 12.6 on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by 11.4 on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM)-based agents (Mialon et al., 2023; Sumers et al., 2023) have become a promising solution to complex interactive tasks (Xi et al., 2024) in recent years. While specialized agents (Wang et al., 2024b; Qin et al., 2023) achieve strong performance on held-in tasks, their generalizability to unseen tasks is poor. To address this challenge, existing works focus on integrating more diverse agent tasks including human-crafted (Zeng et al., 2023; Chen et al., 2024a; Xi et al., 2024; Zhang et al., 2024b; Acikgoz et al., 2025) and LLM synthesized (Hu et al., 2024; Fu et al., 2025), to perform multi-task fine-tuning on the base LLM.\nDespite extensive efforts to scale task diversity for training the base LLM, we find finetuning the base LLM improves held-in task performance but degrades held-out task performance (Figure 1(a)). A potential explanation is that finetuning the base LLM, which is used as the policy model for token-by-token action generation, increases the likelihood of seen action tokens while decreasing that of unseen actions. Rather than finetuning the policy model directly, we hypothesize that finetuning a reward model to guide the policy model is more robust. Since the regression training objective of the reward function is inherently less sensitive to the specific distribution of action tokens. In our preliminary experiment, we perform Best-of-5, i.e. generating 5 candidate trajectories with the policy model and selecting one using the reward model."}, {"title": "2 Task Formulation", "content": "The agent task with environment feedback can be formalized as a partially observable Markov decision process $(U,S,A,O,T,R)$ with instruction space U, state space S, action space A, observation space O, state transition function $T : S \\times A \\rightarrow S$,"}, {"title": "3 Methodology", "content": "The overview is depicted in Figure 2. Section 3.1 describes the behavior cloning through which we derive a policy model with basic task ability on held-in tasks. Section 3.2 elaborates on how we use the derived policy model to build our generalizable reward model. Section 3.3 explains how we use our reward model to enhance the policy model's decision-making ability through test-time search."}, {"title": "3.1 Behavior Cloning", "content": "To obtain an initial policy $\\pi_{init}$ with basic task ability, crucial for collecting high-quality states, we split a portion of task instructions from the training set, annotate them by an expert agent and conduct supervised fine-tuning (SFT) on the expert trajectories $D_{expert} = \\{(u^i, o^i_0, a^i_{<T_i}, o^i_{<T_i})\\}_{i=1}^{N}$ as follows:\n$\\mathcal{L}(\\theta) = - \\sum_{i=1}^{N} \\sum_{t=1}^{T_i} log \\pi_{\\theta}(a^i_t | u^i, o^i_0, a^i_{<t}, o^i_{<t})$\nwhere $\\theta$ denotes the parameters of the policy model, N denotes the number of trajectories in $D_{expert}$, $T_i$ denotes the total step of the i-th trajectory."}, {"title": "3.2 Reward Modeling", "content": "Since the effective construction of reward models in agent tasks remains underexplored, we investigate three methods with different emphases. Explicit reward modeling (Section 3.2.1) employs tree search for automatic process reward annotation, distributing the sparse outcome rewards to each step in an interpretable way. Implicit reward"}, {"title": "3.2.1 Explicit Reward Modeling", "content": "Explicit reward modeling typically defines process reward as Q-value (Watkins and Dayan, 1992), i.e. expected accumulated rewards starting from a state, and calculates it by Monte Carlo estimation on random rollouts. Given that agent tasks typically involve long-chain reasoning and vast search space, we organize the agent's search trajectories into tree structures and employ a Monte Carlo Tree Search (MCTS)-inspired algorithm to avoid redundant exploration while encouraging sampling diversity.\nThe search tree consists of nodes representing states $s_t$ and edges representing actions $a_t$. We consider the initial state $s_1$, which includes the task instruction u and the initial observation $o_0$, as the root node. A search trajectory starting from $s_1$ is formalized as a branch extending from the root node. Each node records information such as the state content (action $a_t$ and corresponding observation $o_t$), the number of visit N(st), and the expected future reward V(st) starting from state st.\nFor each task instruction, we construct a search tree starting from the root node and expanding through repeating the following four stages for w iterations:\nSelection aims to identify the most promising node to be expanded in the next iteration. Starting from the root node, it traverses the tree by selecting child nodes according to the Upper Confidence Bound (UCB) value until a leaf is reached:\n$s_t = \\underset{s_j \\in Children(s_{t-1})}{arg \\ max} V(s_j) + c \\sqrt{\\frac{log N(s_{t-1})}{1 + N(s_j)}}$,\nExpansion will be operated on the selected node $s_t$ if it is not a terminal state exceeding the maxi-"}, {"title": "3.2.2 Implicit Reward Modeling", "content": "Implicit reward modeling typically defines process reward as advantage (Schulman et al., 2017), i.e. relative benefits of an action at a given state compared to alternatives. It derives inherent process"}, {"title": "3.2.3 LLM-as-a-judge", "content": "In order to answer the question that can an LLM be used as the reward model to perform guidance without reward learning, we implement a trainging-free reward model following the paradigm of LLM-as-a-judge (Gu et al., 2025). We prompt the LLM to act as a selector with instructions in Appendix D.1."}, {"title": "3.3 Reward-Guided Search", "content": "We boost the policy model at test time via search methods guided by our general reward model.\nBest-of-N samples N complete trajectories from the policy model and selects the final answer according to the output of the reward model.\nBeam Search searches over the policy model's per-step generation in the following steps:\n\u2022 Initial Sampling: Sample $W_1\u00d7W_2$ initial actions for the first step.\n\u2022 Scoring: Evaluate the new states using the reward model.\n\u2022 Filtering: Retain only the top $W_1$ highest-scoring states.\n\u2022 Action Expansion: For each of the remaining states, sample $W_2$ actions for the next step, generating a total of $W_1 \u00d7 W_2$ new states.\n\u2022 Iteration: Repeat steps 2\u20134 until all maintained states terminate."}, {"title": "4 Experiments", "content": "Apart from comparing with original greedy search, we compare our method with task-specific agents and general agents. Task-specific agents include SPIN (Chen et al., 2024b), NAT (Wang et al., 2024b), ETO (Song et al., 2024), StepAgent (Deng et al., 2024b), QLASS (Lin et al., 2025) and Agent-R (Yuan et al., 2025). General agents include Agent-FLAN (Chen et al., 2024a), Agent-Gym (Xi et al., 2024), AgentGen (Hu et al., 2024), AgentRefine (Fu et al., 2025). We also compare with close-sourced agent based on gpt-4o for reference. More details can be found in Appendix C."}, {"title": "4.2 Experimental Settings", "content": "Datasets We adopt the three agent tasks from ETO (Song et al., 2024) as our held-in tasks: Webshop for web navigation, Alfworld for embodied house holding, and Sciworld for embodied science experiments. We adopt agent tasks from Agent-Board (Ma et al., 2024) and AgentGym (Xi et al., 2024) as held-out tasks. Note that there are two sources of Alfworld and Sciworld. In order to align with the setting of previous works, we use he former to train the RM and evaluate in Section 4.3.2, while the latter is used for evaluation in Section 4.3.1. Details can be found in Appendix D.\nMetrics Maze and Alfworld(ETO) provide Success Rate indicating whether a task is successfully completed. Others provide Progress Rate, a scalar measuring the completion percentage. We use the average reward as the metric for each task.\nImplementation Details We adopt the LLaMA3-8B-Instruct series model as our policy model. More details can be found in Appendix B. We split 1/4 of the expert trajectories for SFT, i.e. 1938, 830, 370 for Webshop, Alfworld, Sciworld. The remaining 3/4 instruction is used to train reward model without expert annotation."}, {"title": "4.3 Results", "content": "In this setting, we compare our method with methods that aim to train a single unified agent for various tasks. To make a fair comparison, we use the original non-finetuned model as the policy model since fine-tuning leads to performance degradation on held-out tasks, and guide its generation with our AgentRM. From Table 1 we can observe that: (1)"}, {"title": "5 Analysis", "content": "In the following analysis, unless otherwise stated, we report the results of explicit RM with Best-of-5 inference, as it outperforms the other two reward models notably."}, {"title": "5.1 Robustness against Perturbation", "content": "To test the extent of overfitting on the held-in tasks, we perform 5 types of perturbations on the held-in task. Specifically, we perturb available actions in the task instruction of Alfworld, which belongs to the held-in tasks for AgentGym and Agent-FLAN. See Appendix A for details of perturbation rules.\nFrom Table 3 we can see that, simple data perturbation leads to a significant performance drop on the held-in task. In terms of the average score, AgentGym's success rate decreases by 25.6, whereas Agent-FLAN shows a more significant performance drop of 30.3. This suggests that they might simply be memorizing the correlations between instructions/observations and corresponding actions from the training data, rather than learning to respond to the given instructions and observations. Our method achieves the highest average score with the lowest standard deviation, indicating that it develops the ability to make informed decisions, rather than memorizing patterns."}, {"title": "5.2 Scaling Trend of Training Data", "content": "We analyze the relationship between the training data size of the reward model and overall performance, with the results shown in Figure 3. The results demonstrate that even a relatively small dataset of 4k states is able to elicit significant reward modeling capabilities (57.6) for agent tasks, compared to the prompt-based training-free LLM-as-a-judge (52.1). This underscores the effectiveness of our approach in data-constrained scenarios. As the volume of training data increases, the performance exhibits a persistent log-linear growth without showing signs of saturation. The observed trend leaves room for continued performance optimization with expanded datasets."}, {"title": "5.3 Generalization of Task-specific RM", "content": "We examine the generalization of task-specific RM trained on each held-in task (Figure 4). The results reveal that, for most tasks, the general RM"}, {"title": "5.4 Generalization to Other Policy Model", "content": "It is commonly thought that broad training data coverage is a requirement to ensure adaptability to different policy distribution. Surprisingly, we find that our RM, which is trained only on states sampled by the LLaMA-3-8B policy model, can be effectively applied to states sampled by other LLM agents. Specifically, we directly utilize our RM to supervise a stronger policy model (LLaMA-3-70B) and a weaker one (AgentGen). From Table 4 we can see that our RM adapts well to different policy models and consistently improves the performance. Specifically, it improves the LLaMA-3-70B-based agent by 12.6 and AgentGen by 5.9, demonstrating more pronounced advantages for models that possess greater scale and potential. These encouraging results indicate that the trial-and-error task experience derived from a weaker yet more efficient agent can enhance the performance of stronger and more costly agents, facilitating weak-to-strong generalization (Yang et al., 2024)."}, {"title": "5.5 State Representation of Reward Modeling", "content": "As stated in Section 3.2, the input of our RM consists of thought tokens, action tokens, and observation tokens (except those of the last action). This"}, {"title": "5.6 Scaling Trend of Test-time Search", "content": "We select Pddl task to explore the potential gains from further increasing the number of candidates in the Best-of-N sampling using different reward modelings. The oracle result is obtained by selecting the best candidate based on the ground-truth label, which is not feasible in practice. We report"}, {"title": "5.7 Generalization to General Reasoning Task", "content": "The relationship between agent tasks and general reasoning tasks remains unclear. In this section, we explore the impact of our RM, merely trained on agent tasks, on the general reasoning tasks. We"}, {"title": "7 Conclusion", "content": "We introduce AgentRM, a generalizable reward model, which enhances the performance of language agents via test-time search. Extensive experiments on nine agent tasks show the effectiveness of our method in both specializability and generalizability. We also analyze the test-time scaling trend, direct transferability to other policy models. We hope this work shed light on generalization and test-time self-improvement of agents."}, {"title": "Limitations", "content": "We conclude the limitations of this work as follows:\n\u2022 Due to the significant efforts required to implement additional agent interactive environments, we only include three agent tasks as held-in tasks. According to the scaling trend of training data in Section 5.2, incorporating more tasks could further enhance the performance.\n\u2022 Due to the resource constraints, we set the maximum iteration and number of simulations in MCTS as 40 and 1. Increasing these parameters could lead to more precise process reward estimations which we leave for further work.\n\u2022 We do not explore the potential of equipping our policy model with prompt engineering designed for agent such as Reflexion (Shinn et al., 2024)."}, {"title": "A Perturbation Details", "content": "We modify the available actions in Alfworld to ensure that the changes consist of different tokens (or token order) while conveying the same semantic information. We revise the environment and the examples in the prompt accordingly.\n\u2022 Perturbation 1: change clean {obj} with {recep}, cool {obj} with {recep}, heat {obj} with {recep} to clean {obj} using {recep}, cool {obj} using {recep}, heat {obj} using {recep} in the instruction\n\u2022 Perturbation 2: change go to {recep} to move to {recep} in the instruction\n\u2022 Perturbation 3: change take {obj} from {recep} to from {recep} take {obj} in the instruction\n\u2022 Perturbation 4: delete all space between item name and item number in the instruction\n\u2022 Perturbation 5: remove all alfworld data in the training set and retrain the model"}, {"title": "B Implementation Details", "content": "Hyperparameters are listed in Table 7. The SFT data is obtained by randomly selecting 1/4 expert trajectories from the training set. Note that the data is formatted in ReAct-style (Yao et al., 2022), and a in Section 3.1 denotes the complete ReAct-style response (containing both thought and action tokens) generated by \u03c0. The remaining 3/4 of the data is reserved for constructing RM training data. In the explicit reward data construction stage, we set the iteration number w as 40, the exploration constant c in UCB as 0.5, the filtering threshold A as 3, the number of the rollout in simulation n as 1, the rollout policy as greedy, the expansion width kas 5. We leverage the AdamW optimizer. All experiments are carried out on 8 NVIDIA A100 80G GPUs. We use VLLM (Kwon et al., 2023) to implement both the policy model and reward model during inference."}, {"title": "C Baselines", "content": "C.0.1 General Agents\nAgent-FLAN (Chen et al., 2024a) is an improvement of AgentTunning focusing on training \"thought\" in ReAct. AgentGym (Xi et al., 2024) enables the model to continuously learn new tasks and treating all tasks as held-in via SFT and DPO. AgentGen (Hu et al., 2024) uses LIMA to synthesize diversified agent-tuning data. AgentRefine (Fu et al., 2025) proposes an environment synthesis method and distills the self-refinement ability from advanced proprietary models such as deepseek and gpt-4o via SFT. For a fair comparison, all general agents receive the task instruction and one successful trajectory as input and respond in ReAct-style. For a fair comparison, we reproduce Agent-FLAN, AgentGym and AgentGen based on LLaMA-3-8B-Instruct. Agent-FLAN includes Alfworld in its training set. AgentGym includes Alf-"}, {"title": "C.0.2 Task-specific Agents", "content": "SPIN (Chen et al., 2024b) augments the expert trajectory dataset with the agent's successful trajectories. NAT (Wang et al., 2024b) and ETO (Song et al., 2024) incorporate failed trajectories into the training process, allowing the agent to learn from its failure experiences. StepAgent (Deng et al., 2024b) utilizes step-wise reward to optimize the agent's reinforcement learning process. QLASS (Lin et al., 2025) guides stepwise search with trained task-specific Q-value models. AgentR (Yuan et al., 2025) leverages MCTS to construct training samples that recover correct trajectories from erroneous ones. Results of SPIN, NAT, ETO, StepAgent are taken from (Deng et al., 2024b) with LLaMA-3-8B-Instruct backbone. Since QLASS has not open sourced, we report the results in (Lin et al., 2025) with LLaMA-2-chat backbone."}, {"title": "D Task Statistics", "content": "Table 8 presents the statistics of both held-in and held-out tasks. We adopt the three agent tasks from ETO (Song et al., 2024) as our held-in tasks: Webshop for web navigation, Alfworld for embodied house holding, and Sciworld for embodied science experiments. We adopt agent tasks from AgentBoard (Ma et al., 2024) and AgentGym (Xi et al., 2024) as held-out tasks: Alfworld, Sciworld, Babyai for embodied house holding, Jericho and Pddl and Maze for text game, ToolQuery and ToolOperation for tool using. Note that there are two sources of Alfworld and Sciworld, i.e. ETO (Song et al., 2024) and AgentBoard (Ma et al., 2024). The reward model training data is collected through interactions with the ETO environment since it provides training set along with expert trajectories. Evaluation in Section 4.3.1 / Section 4.3.2 are conducted on Alfworld and Sciworld implemented by AgentBoard / ETO respectively to align with previous works. They have slight differences in action space, test set number and metric."}, {"title": "D.1 LLM-as-a-judge prompt", "content": "We do not prompt the LLM to output a discrete score for each trajectory since the score might be identical thus insufficient to select the best answer from a set of candidates (e.g., Best-of-N). Instead, we prompt the LLM as follows:\nYou are trajectory reward model, an expert in defining which trajectory is better and closer to solving the task. Here is the task description:\n*******************************\ntask description: {task_description}\ntask goal: {task_goal}\n*******************************\nHere are several candidates. They are all trying to solve the task. Their trajectories are as follows.\nCANIDATE1:\n*******************************\n{candidate_1}\n*******************************\nCANIDATE2:\n*******************************\n{candidate_2}\n*******************************\nCANIDATE3:\n{candidate_3}\n*******************************\nCANIDATE4:\n{candidate_4}\n*******************************\nCANIDATE5:\n{candidate_5}\n*******************************\nWe force the LLM to call the following function to give the answer:\n[{\n \"type\": \"function\",\n \"function\": {\n \"name\": \"choose_preferred_answer\",\n \"description\": \"Choose_the_preferred_answer_for_the_task_within_all_given_answers.\",\n \"parameters\": {\n \"type\": \"object\",\n \"properties\": {\n \"preference\": {\n \"type\": \"number\",\n \"enum\": [1, 2, 3, 4, 5],\n \"description\": \"The_index_of_the_preferred_answer_in_all_given_answers (ranging_from_1_to_5).\"\n }\n }\n }\n}]"}, {"title": "D.2 Preference Accuracy of RM", "content": "We evaluate the quality of our RM estimated step reward by assessing its ability to determine preferences between state pairs. AgentBoard (Ma et al., 2024) offers a method to compute the progress rate"}]}