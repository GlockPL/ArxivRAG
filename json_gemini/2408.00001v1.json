{"title": "Replication in Visual Diffusion Models: A Survey and Outlook", "authors": ["Wenhao Wang", "Yifan Sun", "Zongxin Yang", "Zhengdong Hu", "Zhentao Tan", "Yi Yang"], "abstract": "Visual diffusion models have revolutionized the field of creative Al, producing high-quality and diverse content. However, they inevitably memorize training images or videos, subsequently replicating their concepts, content, or styles during inference. This phenomenon raises significant concerns about privacy, security, and copyright within generated outputs. In this survey, we provide the first comprehensive review of replication in visual diffusion models, marking a novel contribution to the field by systematically categorizing the existing studies into unveiling, understanding, and mitigating this phenomenon. Specifically, unveiling mainly refers to the methods used to detect replication instances. Understanding involves analyzing the underlying mechanisms and factors that contribute to this phenomenon. Mitigation focuses on developing strategies to reduce or eliminate replication. Beyond these aspects, we also review papers focusing on its real-world influence. For instance, in the context of healthcare, replication is critically worrying due to privacy concerns related to patient data. Finally, the paper concludes with a discussion of the ongoing challenges, such as the difficulty in detecting and benchmarking replication, and outlines future directions including the development of more robust mitigation techniques. By synthesizing insights from diverse studies, this paper aims to equip researchers and practitioners with a deeper understanding at the intersection between Al technology and social good. We release this project at https://github.com/WangWenhao0716/Awesome-Diffusion-Replication.", "sections": [{"title": "1 INTRODUCTION", "content": "VISUAL diffusion models represent a significant advance-ment in the field of generative modeling, particularly for image synthesis tasks. These models leverage the concept of diffusion, a process inspired by statistical physics, to generate images from random noise [1], [2]. Compared to traditional Generative Adversarial Networks (GAN) [3] and Variational Autoencoders (VAE) [4], visual diffusion models excel in producing high-quality, diverse, and stable images. Famous visual diffusion models include OpenAI's DALL-E [5]\u2013[7], Stability AI's Stable Diffusion [8]\u2013[10], Google's Imagen [11], and Baidu's ERNIE-ViLG [12], [13], drawing widespread attention from researchers, practitioners, and enthusiasts.\nVisual diffusion models have a broad range of real-world applications across various industries. In the entertainment sector, these models are utilized for creating highly realistic visual effects [14], animations [15], and virtual environments in movies and video games [16], significantly reducing production costs and time. In the field of design and fashion, they aid in generating new styles, patterns, and prototypes, fostering innovation and creativity [17]\u2013[19]. Marketing and advertising benefit from these models through the creation of visually appealing and customized content that enhances con-sumer engagement [20]. Additionally, in healthcare, visual diffusion models assist in medical imaging by enhancing the quality of diagnostic images [21], [22] and creating synthetic data for research and training purposes [23], [24]. The image-generating AI market is estimated to be valued at around 349.6 million in 2023 and is expected to grow to approximately 1,081.2 million by 2030 [25].\nTo achieve such outstanding performance and broad applications, visual diffusion models highly rely on extensive web data, such as LAION-5B [26], for training. However, this data encompasses several significant issues: First, at the concept level, the training data often contains biased gender [27] and culture [28], racial representations [29], and Not Safe For Work (NSFW) materials [30]. Second, at the content"}, {"title": "2 RELATED WORKS", "content": "Diffusion models in vision. Existing surveys on diffu-sion models, such as [198]\u2013[201], provide comprehensive overviews of various diffusion modeling techniques and their applications in computer vision. These surveys categorize diffusion models, discuss their theoretical foundations, and highlight their performance in tasks like image synthesis and data augmentation. In contrast, our survey uniquely focuses on the critical issue of replication within diffusion models. We systematically explore this phenomenon through the lenses of unveiling, understanding, and mitigation, thereby filling a gap between general diffusion model overviews and the specific challenge of replication.\nSafety of diffusion models. Existing surveys on the safety of diffusion models often address issues such as bias, misinformation, privacy concerns, and copyright protection. For instance, [202] emphasizes the critical need to identify AI-generated content to prevent its misuse and potential societal disruptions. [36] explores privacy risks associated with gen-erative AI and highlights the importance of robust detection and authentication. Additionally, [203] and [204] investigate the broader ethical implications and technical challenges of ensuring the integrity and trustworthiness of AI-generated content, including the use of privacy-preserving techniques and blockchain for content verification. Furthermore, [205] addresses the legal and technical challenges of protecting intellectual property rights in the context of AI-generated works, emphasizing the need to identify and verify copy-righted content.\nIn contrast, while our survey also falls under the safety of diffusion models, we specifically target the replication phenomenon within visual diffusion models. This focus is unique compared to existing surveys: while these surveys emphasize detection and mitigation of AI-generated content to prevent misuse and ensure ethical deployment, our survey goes deeper into the intrinsic properties of diffusion models related to replication. This distinction not only comple-ments existing studies but also provides a more granular understanding of the safety concerns associated with visual diffusion models.\nReplication in large language models. The replication phenomenon in large language models (LLMs) have been extensively studied in recent literature. Works such as [206] explore the implications of memorization for privacy, security, and copyright. Similarly, the survey [207] provides a comprehensive overview of methods for extracting training data from LLMs and discusses the inherent challenges in mitigating these risks. Our survey differentiates itself by focusing specifically on visual diffusion models, filling this gap in the current literature."}, {"title": "3 BACKGROUND", "content": "In this section, we provide an overview of visual diffusion models and formally define the replication phenomenon."}, {"title": "3.1 Visual Diffusion Models", "content": "Categorization and theoretical foundations. Diffusion mod-els are typically categorized into three main types: de-noising diffusion probabilistic models (DDPMs) [2], noise-conditioned score networks (NCSNs) [208], and stochastic differential equations (SDEs) [209].\nDenoising Diffusion Probabilistic Models (DDPMs): DDPMS add Gaussian noise to the data in a forward process and learn to reverse this process to denoise the data. The forward process is defined as:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$,\nwhere $\\alpha_t$ is a noise schedule parameter. The reverse process is:\n$P_\\theta(x_{t-1}|X_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_\\theta I)$,\nwith $\\mu_\\theta$ being predicted by a neural network.\nNoise-Conditioned Score Networks (NCSNs): NCSNs esti-mate the score function, the gradient of the log density of the data, to denoise the data. The forward process introduces noise, and the model learns to predict the score:\n$S_\\theta (x_t, t) \\approx \\nabla_x log p(x_t)$.\nThe reverse process uses Langevin dynamics to generate new samples:\n$x_{t+1} = x_t + \\frac{\\epsilon}{2} S_\\theta(x_t, t) + \\epsilon z, z \\sim N(0, I)$,\nwhere $\\epsilon$ is a step size parameter.\nStochastic Differential Equations (SDEs): SDEs generalize the diffusion process using continuous-time dynamics. The forward process can be described by an SDE:\n$dx_t = f(x_t, t)dt + g(t)dw_t$,\nwhere $w_t$ is a standard Wiener process. The reverse-time SDE is used to generate samples:\n$dx_t = [f(x_t, t) - g(t)^2\\nabla_{x_t} log p_t(x_t)]dt + g(t)d\\bar{w}_t$,\nwhere $d\\bar{w}_t$ is the reverse-time Wiener process.\nFunctionalities. Visual diffusion models exhibit a broad range of functionalities, including storytelling [210]\u2013[212], virtual try-on [213]\u2013[215], drag edit [216]\u2013[218], diffusion inversion [94], [219], [220], text-guided editing [221]\u2013[223], T2Iaugmentation [224]\u2013[226], spatial control [227]\u2013[229], image translation [230]\u2013[232], inpainting [233]\u2013[235], layout generation [236], [237], super resolution [238], [239], video generation [240], [241], and video editing [242], [243], showing their versatility and applicability across diverse domains. However, at the same time, visual diffusion models also pose potential threats to this wide range of functionalities through the replication of their training data. This underscores the necessity of our survey, which provides a comprehensive review of this phenomenon, aiming to enhance model safety and ethical standards."}, {"title": "3.2 Replication", "content": "Definition. Let $T = \\{x_1, x_2,..., x_n\\}$ denote a training set of n samples. A diffusion model trained on this set is denoted as $f_T$. During the inference phase, the model generates a set of m data points denoted as $G = \\{x_1,..., x_m\\}$. We say that a trained diffusion model $f_T$ replicates its training set if and only if for any $\\epsilon > 0$, there exist points $x_i \\in T$ and $\\hat{x}_j \\in G$, and a distance metric d, such that $d(x_i, \\hat{x}_j) < \\epsilon$. The distance metric d is a function defined on set M: M X M $\\rightarrow$ R, satisfying the following axioms for all points x, y, z $\\in$ M:\n* The distance from a point to itself is zero: d(x, x) = 0;\n* The distance between two distinct points is always positive: if x != y, then d(x, y) > 0;\n* The distance from x to y is always the same as the distance from y to x: d(x,y) = d(y,x).\nUnderstanding. This definition is highly compatible with the various functionalities and modalities of visual diffusion models, and it comprehensively includes different levels of replication:\n1) Functionalities. The proposed definition is highly compati-ble with different functionalities exhibited by visual dif-fusion models because these applications fundamentally involve generative tasks, where the output is either an image or a video. Our definition specifies the replication by only comparing training data and generated outputs using a distance metric. This definition is agnostic to the type of generative task \u2013 whether it is text-to-image, image translation, inpainting, or video generation.\n2) Modalities. The proposed definition is also compatible with different modalities of data, such as images, videos, and even text, due to its reliance on a general concept of data points and a distance metric for comparison. Regardless of whether the data point is an image, a video, or text, there are established methods for feature extraction and distance calculation specific to each modality.\n3) Levels. The proposed definition includes different repli-cation levels, i.e., concept, content, and style, because it allows for various feature extractors tailored to each specific level. When focusing on the concept level, which is akin to common multi-class classification tasks, models like CLIP [244], DINO [245], [246], or those [247], [248] pre-trained on datasets like ImageNet [249] can serve as effective feature extractors to evaluate conceptual similarities between training and generated data. For the content level, there are image copy detection algorithms [250]-[253] designed to ensure precise detection of replicated content in the generated outputs. At the style level, researchers have developed specialized style descriptors [34], [35] to assess stylistic similarities. Moreover, in mathematics, various distance metrics, such as Euclidean and Manhattan distances, measure the distance between two points."}, {"title": "4 UNVEILING", "content": "In this section, we focus on the first aspect of our survey on replication in visual diffusion models, which is unveiling. Unveiling [254]\u2013[257] refers to the process of uncovering the phenomenon of replication, either manually or through the use of specially-designed machine learning models. As"}, {"title": "4.1 Prompting", "content": "These articles investigate how prompting can reveal repli-cation in visual diffusion models. As shown in Fig. 3 (a): by using specific prompts, researchers can generate images that closely resemble the model's training data; beyond that, some papers show visual diffusion models may replicate learned copyrighted images implicitly.\nSpecific. Specific prompts are carefully chosen phrases or descriptions from researchers to test whether visual diffusion models can replicate. For instance, [37], [58]-[60] employ specific prompts that are known to correspond to particular training images to see if the generated images closely resemble these originals. By injecting maliciously crafted data into the training set, researchers [61] can use specific prompts to trigger the model to produce near-identical copies of copyrighted images. The articles [62]-[64] demonstrate that by using prompts that are likely to invoke sensitive or controversial topics, the diffusion model can be coaxed into generating unsafe or offensive images. By using prompts that include the names of famous artists [40] or refer to different social stereotypes [65], the researchers show that the model can produce images that closely mimic the unique features of their styles or reflect biased society representations.\nImplicit. replication can also occur when user prompts are related to certain concepts or topics implicitly or uninten-tionally. For instance, these studies [60], [66], [67] highlight how diffusion models can replicate copyrighted content with such prompts. They further utilize techniques such as keyword extraction and gradient-based prompt optimization to analyze the attention mechanisms within these models."}, {"title": "4.2 Membership Inference", "content": "Membership inference attacks (MIAs) aim to determine whether specific data samples are part of the model's training set. These attacks exploit patterns in the model's behavior, such as how it processes and reconstructs data, to infer the presence of training data. If visual diffusion models have not been trained on a data sample, they will never replicate it. Therefore, MIAs have a strong relationship with replication, and we review MIAs in the context of visual diffusion models in this section. Based on the level of access attackers have, MIAs can be categorized into white-box and black-box attacks, as shown in Fig. 3 (b).\nWhite-box. White-box membership inference attacks diffu-sion models by leveraging their internal parameters and gradients. Key methods include loss-based attacks [68], [69], likelihood-based attacks [68], [69], gradient-based attacks [70], quantile regression [71], proximal initialization [39]. These methods highlight significant privacy risks for diffu-sion models when accessing their internal weights, especially handling sensitive data.\nBlack-box. Black-box MIAs focus on exploiting the generated images rather than visual diffusion models' internal parame-"}, {"title": "4.3 Similarity Retrieval", "content": "Similarity retrieval is a method that closely aligns with human common sense for uncovering replication. This approach involves searching for and identifying items in a dataset that are similar to a given query item. In the context of diffusion models, similarity retrieval allows for comparing generated outputs against the training data. When a generated image/video closely matches an image/video from the training set, it raises concerns about the model replicating specific data points rather than generalizing from the training data. As shown in Fig. 3 (c), the primary retrieval methods for unveiling replication are through content similarity, while style similarity is also used to help identify artworks mimicry.\nContent similarity. Content similarity focuses on comparing the actual content or subject matter of the generated images or videos to the training data. The first step of comparison involves feature extraction with pre-trained vision(-language) models [244]\u2013[248] or specialized image copy detection methods [250]\u2013[253]. After that, these extracted features are used to compute similarity scores between generated content and training samples through various metrics such as cosine similarity, Euclidean distance, or more complex learned metrics [37], [78]\u2013[82].\nStyle similarity. Style similarity involves comparing the artistic style or aesthetic elements of generated images or videos to those in the training data. This approach is crucial for identifying instances where a diffusion model replicates the distinctive style of contemporary artworks or artists. For instance, [83] explores how well diffusion models can replicate the styles of human artists. Additionally, [34] discusses a framework for understanding and extracting style descriptors from images generated by diffusion models. Furthermore, [35] generalizes the pattern retrieval algorithm for image copy detection to measure style similarity."}, {"title": "4.4 Watermarking", "content": "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks, as shown in Fig. 3 (d). Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unau-thorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of text-to-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into art-works to trace art theft."}, {"title": "4.5 Proactive Replication", "content": "Recently, some personalized visual diffusion models [88]\u2013[97] have been successfully designed to fine-tune on specific subjects or styles using minimal input data. Remarkably, some models [98], [99] can even learn from this minimal input data in a training-free manner. This enables users to generate images that highly preserve the original visual characteristics and essence of the subjects or styles at a very low cost, as shown in Fig. 3 (e).\nWe refer to this as proactive replication, unlike the afore-mentioned reviewed methods, which inevitably and un-intentionally replicate. Proactive replication in visual dif-fusion models represents a double-edged sword: while it offers opportunities for the creative industry with enhanced personalized content [88], it also poses significant ethical and practical challenges [258]. One of the most pressing concerns is the potential for these models to replicate and commercialize the artistic styles of living artists without consent [133]. This capability to reproduce artists' styles at low cost undermines the years of effort artists invest in their unique visual signatures."}, {"title": "4.6 Novel Perspectives", "content": "In addition to these categorizations of unveiling replication, several novel perspectives have emerged that offer unique insights and techniques. As shown in Fig. 3 (f), these perspec-tives [100]-[106] leverage different aspects of model behavior and training data characteristics to uncover replication in visual diffusion models:\n1) Magnitude of noise. This research [100] presents a method for detecting replication by examining the magnitude of text-conditional noise predictions. By analyzing these magnitudes, the study unveils how specific text tokens contribute to replication, allowing users to adjust their prompts.\n2) Training data attribution. The papers [101], [102] emphasize the role of training data in guiding diffusion models by tracing back generated outputs to their original training data. This approach aids in identifying instances where the model excessively relies on specific training samples.\n3) Cross attention. This work [103] investigates the role of cross attention mechanisms in text-to-image diffusion models. Examining cross-attention mechanisms helps identify a model's replication because models tend to focus on certain text-image pairs.\n4) Fine-tune to leak. This research [104] highlights the risks associated with fine-tuning diffusion models, which can amplify replication issues. To determine if a visual diffu-sion model has serious replication issues, it is feasible to check whether the model has undergone fine-tuning.\n5) Overfitted Masked Autoencoder (MAE). The paper [105] proposes using overfitted MAEs to detect generative parroting in diffusion models. By identifying overfitting"}, {"title": "5 UNDERSTANDING", "content": "After unveiling the phenomenon of replication in visual diffusion models, understanding its mechanism is crucial for developing effective mitigation strategies and improving the safety and ethical standards of these models. As outlined in Fig. 1, this section explores the underlying mechanisms that contribute to replication from three different perspectives: data, methods, and theory. The demonstration of these aspects is shown in Fig. 4."}, {"title": "5.1 Data", "content": "Data plays a crucial role in the replication phenomenon observed in visual diffusion models. The quality, duplication, and bias present in the training data directly impact the model's behavior and performance. As shown in Fig. 4 (a), in this section, we explore how insufficient training data, image"}, {"title": "5.2 Methods", "content": "To complement insights from a data perspective, this section demonstrates how training methods can influence replication in visual diffusion models. It specifically examines the roles of a deterministic sampler and model capacity. To deepen the analysis of model behavior, we additionally review new metrics developed specifically for assessing replication. The illustrations of these are shown in Fig. 4 (b).\nDeterministic sampler. Deterministic samplers are mecha-nisms used in visual diffusion models to generate data in a repeatable and predictable manner. The researchers [43] find that deterministic samplers lead to generated samples that are highly correlated with the training set. This high correlation indicates that the model is replicating patterns seen during training rather than generating truly novel data. Further, [44] demonstrates that while deterministic samplers can lead to replication, they can also support generalization under appropriate training conditions.\nModel capacity. Model capacity refers to a machine learning model's ability to fit a wide variety of functions, which is determined by the model's complexity. Complexity factors include the number of parameters, the depth of the model, and the model's structure. In visual diffusion models, re-placing the commonly-used U-Net backbone [261] with a transformer [262] \u2013 referred to as Diffusion Transformers (DiTs) [263] \u2013 results in a higher model capacity. Although models with greater capacity often achieve lower Frechet Inception Distance (FID) and better visual fidelity, they are also more prone to replicating training data. For instance, [31] demonstrates that large models with substantial capacity can retain detailed information from the training data, which may lead them to replicate these details during inference. Furthermore, [108] finds that high-capacity models, due to their complexity, are more likely to replicate training data, particularly under conditions of insufficient data diversity or small dataset size.\nNew metrics. Beyond understanding replication from the perspective of training methods, [111]\u2013[113] underscore the importance of developing more comprehensive evaluation frameworks. Traditional evaluation metrics, like FID for"}, {"title": "5.3 Theory", "content": "Beyond the straightforward understanding of the replication phenomenon from the data and methods perspectives, some researchers [43], [114]\u2013[118] offer formal and theoretical explanations using various mathematical theories, such as probability and information theory. In this section, we illustrate these theories in Fig. 4 (c) and provide a brief review as detailed below:\n1) Near access-freeness. The authors [114] introduce the con-cept of \"near access-freeness\u201d (NAF) and provide bounds on the probability that a model will generate protected content.\n2) Dichotomy. This study [115] examines the generalization capabilities of diffusion probabilistic models, introducing the \"memorization-generalization dichotomy\". The key finding is that these models generalize well when they fail to memorize their training data.\n3) Geometry-adaptive. This paper [116] explores how the generalization properties of diffusion models can be attributed to their use of geometry-adaptive harmonic representations and argue that these representations allow the models to adapt to the underlying geometric structures of the data.\n4) Data-(in)dependent. The authors [117] introduce a frame-work to estimate the Kullback-Leibler (KL) divergence between the learned and target distributions, providing both data-independent and data-dependent bounds.\n5) Mutual information. This paper [43] defines generalization in terms of mutual information between the generated data and the training set, suggesting that models gener-ating data with less correlation to the training set exhibit better generalization.\n6) Creativity. Theoretically, the authors [118] discuss various dimensions of creativity, including originality, flexibility, and elaboration, and analyze how current AI technologies perform in these areas."}, {"title": "6 MITIGATION", "content": "After we unveil and understand the replication phenomenon in visual diffusion models, the final and most crucial step is to design strategies to mitigate these issues. Mitigation means avoiding the (un)intentional leakage of training data through model outputs. To effectively finish that, it is essential to employ a multifaceted approach that encompasses both data management techniques and algorithmic innovations. Specifically, as shown in Fig. 1, in this section, we explore"}, {"title": "6.1 Training Data Optimization", "content": "Since data is the direct cause of replicating biased concepts, copyrighted and private content, and artwork styles, op-timizing training datasets becomes crucial for mitigating replication in visual diffusion models. As shown in Fig. 5 (a), based on current key interests, we innovatively categorize training data optimization methods into four main areas: deduplication, protection, purification, corruption."}, {"title": "6.2 Machine Unlearning", "content": "Machine unlearning [266] is a process designed to remove specific data or concepts from a machine learning model, effectively making the model \u201cforget\u201d particular information without needing to retrain from scratch. As shown in Fig. 5 (b), in the context of visual diffusion models, machine unlearning plays a vital role in mitigating the issues of replication of specific concept, content, and style [50], [147], [148]. Specifically, the studies [49], [103], [149]\u2013[152] emphasizes the significance of choosing cross-attention-related parameters to fine-tune for effective erasure. Focusing on gradient, SalUn [153] utilizes gradient-based weight saliency to improve the limitations of traditional machine unlearning methods, aiming to enhance accuracy, stability, and cross-domain applicability of the unlearning process. Utilizing continual learning, Selective Amnesia [154] explores how to selectively forget concepts in deep generative models.\nThere are also some works focusing on specialized aspects or applications. The paper [155] discusses the application of machine unlearning techniques in image-to-image gen-erative models. Regarding defending against unexpected user inputs: Espresso [156] is the first method to robustly remove unacceptable concepts; Task Vectors [157] have been shown to be more robust compared to input-dependent erasure methods; and [158] proposes the use of pruning techniques to enhance the model's robustness. [159] and [160] are specifically designed to use machine unlearning to mitigate unsafe content generation and enhance copyright protection, respectively.\nBeyond traditional machine unlearning methods that focus on erasing single concept at a time, recent advance-ments move towards more comprehensive approaches that aim to modify, erase, or refine multiple concepts simultane-ously within diffusion models. For instance, UCE [161] can handle multiple concept editing tasks simultaneously, such as debiasing, style erasure, and content moderation. SDD [162] effectively reduces the proportion of harmful content generated by aligning the conditional noise estimate with an unconditional one and allows for the removal of multiple concepts simultaneously. SepME [163] flexibly erases or recovers multiple concepts while preserving overall model performance. MACE [164] and EMCID [165] scale up to handle the erasure of 100 and 1,000 concepts, respectively, while maintaining the integrity of other non-edited concepts.\nAlthough these unlearning methods are effective to some degree, some evaluations question their reliability and indicate that they are susceptible to jailbreaking. For instance, UnlearnCanvas [166] includes high-resolution, stylized im-ages that allow researchers to effectively test and quantify the unlearning of artistic painting styles and associated image objects. The paper highlights shortcomings in existing machine unlearning evaluation methods, such as a lack of diverse unlearning targets, lack of evaluation precision, and a lack of systematic study on retainability. Additionally, the study [167] shows that special learned word embeddings can retrieve supposedly erased concepts from sanitized models"}, {"title": "6.3 Prompt Disturbing", "content": "As illustrated in Fig. 5 (c), the term \u201cprompt disturbing\u201d refers to the intentional modification of user inputs to prevent the model from merely replicating memorized patterns or details from its training data. These modifications include direct change to the original user prompts. For instance, [100] alters specific terms or removing elements from prompts to decrease direct ties to memorized data and promote a broader range of creative outputs. Negative prompts [30] are introduced to guide a visual diffusion model to avoid producing certain elements, thereby encouraging more original creations that are not simply replication of its training data.\nBeyond that, some methods further disturb prompt-related components in the visual diffusion models to avoid replication. For instance, ProtoRe [171] incorporates language-contrastive knowledge to identify prototypes of negative concepts, which are then used to extract and eliminate undesirable features from outputs. Degeneration Tuning [172] is proposed to disrupt the correlation between undesired textual concepts and their corresponding image domains. [173] works by optimizing text embeddings during inference time to better control the image content generated from textual descriptions."}, {"title": "6.4 Novel Perspectives", "content": "Beyond these common mitigation methods for the replication phenomenon, some novel perspectives can be explored to further reduce these issues within visual diffusion models. As shown in Fig. 5 (d), these novel perspectives [108], [174]\u2013[176] aim to tackle the underlying causes of replication by diversifying the training approaches and incorporating principles from other domains of machine learning and data security:\n1) Composition. This research [174] allows different diffusion models to be trained on separate data sources and arbitrarily composed at inference time. Each model only contains information about the subset of the data it was exposed to during training, which effectively prevents the leakage of training data.\n2) Model immunizing. The article [175] discusses how to mitigate replication by improving learning algorithms to reduce the risk of malicious adaptation. Malicious adaptation refers to the behavior of fine-tuning visual diffusion models to produce harmful or unauthorized"}, {"title": "7 INFLUENCE", "content": "After sequentially discussing the unveiling, understanding, and mitigation of replication in visual diffusion models, as outlined in Fig. 1, this section focuses on its influence in the real world. Specifically, as illustrated in Fig. 6, we focus on regulation, art, society, and healthcare. This involves opinions from legal scholars, artists, sociologists, and doctors."}, {"title": "7.1 Regulation", "content": "As shown in Fig. 6 (a), training and generating processes in some visual diffusion models raise significant law issues due to the replication of copyrighted materials. As these models become more powerful and prevalent, an increasing number of legal scholars are focusing on this area. They primarily investigate how these models manage and utilize copyrighted materials during the creation process, along with the challenges and implications for the existing copyright law framework. For instance, they [51], [52], [177]\u2013[182] question whether using copyrighted works as training data for AI constitutes copyright infringement, whether Al-generated outputs are derivative works infringing on the original copyrights, and who owns the copyright for Al-generated works. Furthermore, [182], [183] discuss the intricate infringement challenges that arise when generative Al models, particularly visual diffusion models, are trained using copyrighted materials without proper authorization. Additionally, [184] aims to define and clarify what constitutes replication from the perspective of copyright infringement; [185] thoroughly explores the intersection of copyright law and economic principles in the context of rapid technological advancements; and the core idea of [186] is to evaluate whether privacy protection measures can align with and support copyright law.\nBeyond the copyright issues, there are also privacy concerns and corresponding data protection regulations [267], [268]. The replication of data by visual diffusion models can pose significant privacy risks, especially when the models inadvertently replicate sensitive or personal data. This contravenes data protection regulations such as"}, {"title": "7.2 Art", "content": "The influence of generative AI in art worlds presents both op-portunities and challenges. These models have transformed the art market, personalizing the buying experience and enhancing the efficiency of curators in identifying trends and managing collections [187]. Despite these advances, as shown in Fig. 6 (b), many artists fear that AI may threaten their jobs and dilute the authenticity of art by replicating styles and producing art without human involvement [53], [188]. This fuels the ongoing debate about whether art can exist without an artist [189], [190]. Additionally, some researchers [191] are investigating artistic copyright infringements, underscoring the complex challenges in protecting intellectual property because artistic style itself is not copyrightable [54]."}, {"title": "7.3 Society", "content": "From a societal perspective, as shown in Fig. 6 (c), the phe-nomenon of replication in visual diffusion models manifests in the duplication of human values, ideals, and even biases within generated images or videos. Much of the research in this area focuses on the issue of biases because this can result in the reinforcement and amplification of societal inequalities and discrimination. Different from the biases discussed in the Regulation subsection, we review some papers from a societal perspective here. For instance, the study [29] introduces a novel method for assessing social biases by analyzing how varying input prompts related to gender, ethnicity, and professions influence the diversity of generated images. The researchers [55] highlight how these visual diffusion models can exacerbate the biases present in their training data, particularly depending on the dataset size. The papers [192], [193] specifically explore how gender is represented in text-to-image models and emphasizes the need to understand how they reinforce gender stereotypes."}, {"title": "7.4 Healthcare", "content": "Visual diffusion models have significantly impacted the field of healthcare by enhancing the generation and analysis of medical images, which are critical tools in diagnosis, treatment planning, and research.\nA primary way diffusion models assist in medical imag-ing is through the generation of synthetic images [272]\u2013[274]. These models can create realistic medical images,"}, {"title": "8 CHALLENGES AND FUTURE DIRECTIONS", "content": "After reviewing the replication issues in visual diffusion models, including unveiling, understanding, mitigation, and its influence in the real world, this section will discuss the current challenges and future directions in this field."}, {"title": "8.1"}]}