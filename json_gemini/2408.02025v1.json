{"title": "Contrastive Learning-based Chaining-Cluster for Multilingual Voice-Face Association", "authors": ["Wuyang Chen", "Yanjie Sun", "Kele Xu", "Yong Dou"], "abstract": "The innate correlation between a person's face and voice has recently emerged as a compelling area of study, especially within the context of multilingual environments. This paper introduces our novel solution to the Face-Voice Association in Multilingual Environments (FAME) 2024 challenge, focusing on a contrastive learning-based chaining-cluster method to enhance face-voice association. This task involves the challenges of building biometric relations between auditory and visual modality cues and modelling the prosody interdependence between different languages while addressing both intrinsic and extrinsic variability present in the data. To handle these non-trivial challenges, our method employs supervised cross-contrastive (SCC) learning to establish robust associations between voices and faces in multi-language scenarios. Following this, we have specifically designed a chaining-cluster-based post-processing step to mitigate the impact of outliers often found in unconstrained in the wild data. We conducted extensive experiments to investigate the impact of language on face-voice association. The overall results were evaluated on the FAME public evaluation platform, where we achieved 2nd place. The results demonstrate the superior performance of our method, and we validate the robustness and effectiveness of our proposed approach. Code is available at https://github.com/colaudiolab/FAME24_solution.", "sections": [{"title": "1 Introduction", "content": "Human beings possess the ability to associate voices with faces, even for unknown individuals [2, 12, 19]. In recent studies, deep neural networks can use computational methods to mimic humans in associating unheard voices with unseen face images [21, 38, 41]. This capability can aid investigative work and biometric authentication, propel the advancement of virtual reality technologies, and even help anthropologists explore the physiological correlation between voiceprints and facial features, showcasing its usefulness in practical security fields and its academic value in research [14, 22, 23].\nHowever, sustainable efforts [21, 29, 38, 41] show impressive performance on voice-face associating tasks, these works only focus on a single language environment, assuming that each person speaks only one language. With the growing prevalence of multilingual speakers and the complex dynamics of polyglot interactions, the original assumption can easily be invalidated, as half of the population is bilingual [18]. When the same person speaks more than one language, we still expect the model to robustness match the correct facial image with different language speakings. However, the multilingual scenario makes the original task even more challenging. These challenges manifest in the following ways: In a monolingual setting, the network needs to learn the implicit mapping relationship between facial images and voices, which may involve understanding biometric relationships between prominent facial attributes and acoustic features. For instance, a person with thick eyebrows may have a deeper voice, while a person with high cheekbones may have a louder voice [27, 37]; In a multilingual setting, the network needs to further identify similarities in the same person when speaking different languages. However, different languages have distinct vowels, consonants, grammar rules, and so on [20], making it hard to find similarities. This may involve uncovering hidden prosody, such as tone and stress patterns [36].\nThis article will focus on the association of voice-face under multilingual condition. In this field, [25] investigated the effects of multiple languages on voice-face association and speaker recognition tasks. For the association task, they used two sub-networks to extract embeddings from faces and voices, followed by two fully connected layers. The final outputs from the two branches were optimized by a hinge loss to reduce the modality gap between them. Based on this architecture, they found that language does indeed"}, {"title": "2 Related Work", "content": "The burgeoning intersection of auditory and visual data analysis has catalyzed significant advancements in the field of audio-visual association tasks. Particularly within the context of speaker recognition [5, 7, 11, 33, 35] and biometric identification [8, 15, 17], the ability to accurately link voices with corresponding facial images is paramount. However, the complexity of this task escalates in multilingual environments, where audio-visual to the multilingual speaker further challenges traditional models when only one of the languages has been heard. This section delves into the existing body of work that has paved the way for our current investigation, Including the cutting-edge research in face-voice association and the domain of multilingual speaker recognition."}, {"title": "2.1 Face-Voice Association", "content": "The field of face-voice association has seen significant progress in recent years, with researchers exploring innovative methods to understand and model the relationship between a person's face and their voice. Some studies have trained unimodal features separately and established face and voice association by learning joint embeddings [13, 25, 28, 38]. Saeed et al. [28] introduced a method known as Fusion and Orthogonal Projection (FOP) to improve face-voice association. They hypothesized that an enriched feature representation and efficient supervision were necessary for a discriminative joint embedding space. Other works have proposed methods involving cross-modal prototype contrastive learning and self-supervised learning approaches that align multimodal data during the feature learning phase [4, 34, 40]. Chen et al. [4] explored the generation of faces from voice segments with their method VoiceStyle, which combined cross-modal representation learning with generation modeling. They introduced cross-modal prototype contrastive learning to establish voice-face associations and used StyleGAN to produce faces guided by the learned alignment. Additionally, self-supervised approaches gained traction. Zhang et al. [40] proposed self-supervised curriculum learning (SSCL) to enhance audio-visual association by exploiting the concurrency property as a latent supervision signal. Stevenage et al. [34] indicates that the correlation between face and voice processing is stronger for familiar individuals, suggesting the importance of mental representation and neural cross-talk in processing familiar stimuli.\nThe method we propose aligns multimodalities during the representation learning phase. The purpose of doing this is to ensure that the features of different modalities are consistent in the feature space, thereby better achieving the association task of faces and voices."}, {"title": "2.2 Multilingual Speaker Recognition", "content": "Research on multilingual speech association has made some progress in several speech research fields. These include voice conversion [10], speaker verification [32] and recognition [39], language factors [6], speech recognition and synthesis, and multilingual speech recognition and speech recognition [1, 30]. In work [16], they first attempted to explore the feasibility of the original speaker recognition system in a cross-lingual context. They collected speech data from the same speaker speaking with Mandarin, English, and Taiwanese languages, forming a total of 40 speakers. And they tried different network structures to validate the system's recognition performance. The results indicated that in a cross-lingual scenario, the system could still maintain close to 80% accuracy, although the performance decreased significantly compared to a single language model. This preliminary evidence suggests that a system can recognize a person regardless of the language they are speaking. However, it should be noted that the data collected in the work [16] was gathered in a relatively stable environment. Participants were asked to read given text prompts in different languages and answer five specified questions in their preferred language. The data collected in this experimental setting contains less noise and disturbance compared to what may exist in real-world scenarios, potentially leading to distribution differences. Subsequently, Yik Heng et al. [26] designed a multilingual (Malay, English, Mandarin, and Tamil languages) speaker verification system for a door security system in the real-world scenario. This work primarily focuses on the verification accuracy of the system without discussing the impact on system performance when language changes occur in the testing scenario. From the previous work, it can be observed that their data collection environment is relatively homogeneous, which may not accurately represent the distribution of in-the-wild data. On the other hand, Nawaz et al. collected multilingual videos (English, Urdu and Hindi) from online video platforms, including political debates, press conferences, and indoor and outdoor interviews. The audio in these videos encompasses various complex scenarios that may exist in the real world, such as background conversations, music, and speech overlap. This dataset is known as MAV-Celeb [25]. They conduct both voice-face association and speaker recognition tasks to answer the question of whether these tasks are language-dependent."}, {"title": "3 Methodology", "content": "This paper proposes a contrastive learning-based chaining-cluster method for the Face-Voice Association in Multilingual Environment (FAME) 2024 challenge. Our approach begins by utilizing Supervised cross-contrastive (SCC) learning to establish associations between voices and faces (as detailed in subsection 3.1). Subsequently, the learned face and voice representations are employed in a post-processing phase that incorporates the chaining-cluster score refinement technique, with the aim of addressing outliers prevalent in wild data (as elaborated in subsection 3.2). The overall schematic of the method is illustrated in Figure.1. Considering that there are four different test scenarios in FAME (V1: trained on Urdu and tested on Urdu; trained on Urdu and tested on English; trained on English and tested on Urdu; trained on English and tested on English), for clarity, we will use one scenario (trained on Urdu and tested on Urdu) as an example."}, {"title": "3.1 Supervised Cross-Contrastive Learning", "content": "To enhance the understanding of the association between voice and facial features of the same identity, we propose a novel network architecture grounded in SCC learning. This architecture comprises two distinct branches, denoted as Voice Encoder \\(E_v\\) and Face Encoder \\(E_f\\), each containing 11 Transformer layers. Additionally, a single Transformer layer with shared weights is positioned between the voice and face branches. This shared-weight layer improves learning efficiency and fosters the model's ability to recognize common patterns across voice and facial data. We utilize the following loss function:\n\\(L=-\\frac{1}{N}\\sum_{i=1}^{N} log \\frac{exp(E_v(v_i)^TE_f(f_i)/\\tau)}{\\sum_{j=1}^{N}exp(E_v(v_i)^TE_f(f_j)/\\tau)} \\)\nwhere N is the batch size. \\(\\tau\\) is the temperature hyper-parameter. \\(v_i\\) and \\(f_i\\) represent the voice and face that belong to the same identity, while the denominator is the sum of all samples in the current"}, {"title": "3.2 Chaining-Cluster", "content": "The rationale for implementing clustering is based on the fact that voice-face pairs are collected in uncontrolled environments, leading to numerous potential distractions. For audio data, these distractions may include background music, overlapping voices from other individuals, and environmental noise. For image data, distractions might involve varying facial orientations, makeup, and lighting conditions. Directly applying our SCC model to derive cross-modal similarity scores can result in inaccuracies due to these distractions. Clustering offers a robust solution by effectively managing minor distractions. For example, if an audio clip of a target male individual contains overlapping voices from females, the clustering process"}, {"title": "4 Experiments and Results", "content": "The system evaluation is determined by the output scores for each test pair. The score for each pair represents the system's confidence in the identity match between the voice segment and face image. Our work utilizes the public MAV-Celeb dataset to train and test. The test process is taken by the competition-provided platform."}, {"title": "4.1 Datasets", "content": "The MAV-Celeb is an audio-visual dataset containing multilingual speaking videos. The dataset covers a total of 154 identities, focusing on the English, Hindi, and Urdu languages. The videos are collected from online media platforms with an automatic collection pipeline. Specifically, for each person's name from the list of persons of interest (POIs), the English, Hindi, and Urdu version videos are downloaded and post-processed, forming the MAV-Celeb dataset. The videos are all real-world and cover various scenes, such as political debates, outdoor/indoor interviews, conferences, and even public movie clips, containing unconstrained noise degradation, which poses challenges to the task.\nTo better analyze the language effect on the voice-face association task, the dataset is split into two versions. The first version (V1) contains English and Urdu corpora, while the second version (V2) includes English and Hindi languages. The details of the dataset are illustrated in Table 1. Both versions have 6 identities in the test set."}, {"title": "4.2 Evaluation Metric", "content": "The evaluation metric used by the testing platform is the equal error rate (EER). This metric calculates the false acceptance rate (FAR) and false rejection rate (FRR) based on multiple predefined thresholds. When a threshold is found that equalizes the FAR and FRR, it represents the final EER. In reality, a system may encounter various situations where it cannot ensure that the language of the test voice matches that of the training set. To comprehensively assess the system's robustness, the evaluation encompasses two scenarios. One involves the test language being included in the training data, while the other scenario assumes that the test language does not appear in the training dataset at all. Thus, the final output comprises a total of four items, and the overall evaluation score is based on the average of these EERs."}, {"title": "4.3 Implementation Details", "content": "For the supervised cross-contrastive learning phase, we first pre-train the model on the public Voxceleb1 [24] dataset. It should be noted that the pretraining step does not start from random weights; instead, it uses parameters that have been trained on the VGGSound [3] and AudioSet [9] datasets. During fine-tuning, we use both the training data from the V1 and V2 datasets, as well as English, Hindi, and Urdu interview videos crawled from the Internet (processed using the same pipeline as mentioned in [25]). It should be noted that for the unheard test scenario, we exclude the testing language from the training set."}, {"title": "4.4 Evaluation Results", "content": "The competition is ultimately evaluated on the V1 dataset, and the details of the overall score on V1 are shown in Table 2. Our performance significantly outperforms the baseline, achieving second place compared to all other participating teams. We also provide the results of our system on the V2 dataset for reference, which are displayed in Table 3. It should be noted that the method we used to evaluate V2 does not include any post-processing. It only utilizes the SCC network along with the baseline's linear fusion architecture. The results on V2 also show significant improvement compared to the baseline, indicating that the SCC network has learned powerful discriminative features that are important for the voice-face association task."}, {"title": "4.5 Ablation Study", "content": "To quantitatively measure how much the post-processing proposed in this paper improves overall performance, we removed the post-processing step and calculated the scores on the V1 dataset using only the representations extracted from the SCC network. The results are presented in Table 4. It can be observed that the post-processing effectively reduces the EER, especially in cases where the original EER is relatively high. These findings also indicate that the chaining-cluster post-processing we proposed can effectively utilize high-confidence samples (inliers) to guide the outliers."}, {"title": "5 Discussion and Conclusions", "content": "We believe that the success of our system firstly stems from the supervised cross-contrastive learning. Existing research indicates that native speakers can easily distinguish non-native accents from second-language speakers, and prosody remains correlated even when the same individual speaks different languages. This prosody may encompass various aspects such as rhythm, stress, and even intonation patterns. However, what these relevant factors currently entail is still a black box. Our training process relies on supervised face-voice pairing with different languages. This type of training pair forces the model to associate the facial image of the same person with different languages while distinguishing it from others. Secondly, the post-processing further enhances our system. The chaining-cluster method gradually removes outliers and uses the remaining inliers to adjust the initial scores. This approach ensures that performance does not degrade in noisy real-world scenarios. In upcoming projects, we will investigate using memory-based methods for more elegant end-to-end prediction."}, {"title": "5.1 Discussion", "content": "We believe that the success of our system firstly stems from the supervised cross-contrastive learning. Existing research indicates that native speakers can easily distinguish non-native accents from second-language speakers, and prosody remains correlated even when the same individual speaks different languages. This prosody may encompass various aspects such as rhythm, stress, and even intonation patterns. However, what these relevant factors currently entail is still a black box. Our training process relies on supervised face-voice pairing with different languages. This type of training pair forces the model to associate the facial image of the same person with different languages while distinguishing it from others. Secondly, the post-processing further enhances our system. The chaining-cluster method gradually removes outliers and uses the remaining inliers to adjust the initial scores. This approach ensures that performance does not degrade in noisy real-world scenarios. In upcoming projects, we will investigate using memory-based methods for more elegant end-to-end prediction."}, {"title": "5.2 Conclusions", "content": "In this study, we addressed the challenge of voice-face association in multilingual contexts by proposing a novel solution that utilizes the SCC network combined with chaining-cluster-based post-processing. This approach effectively addresses the difficulties associated with cross-modal and cross-language associations, as well as the outlier influences commonly encountered in real-world data. As a result, our method demonstrated significant improvements over the baseline, achieving promising scores and robustness in identifying matching pairs in both heard and unheard scenarios."}]}