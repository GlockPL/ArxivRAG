{"title": "Adversarial Attacks on Large Language Models Using Regularized Relaxation", "authors": ["Samuel Jacob Chacko", "Sajib Biswas", "Chashi Mahiul Islam", "Fatema Tabassum Liza", "Xiuwen Liu"], "abstract": "As powerful Large Language Models (LLMs) are now widely used for numerous practical applications, their safety is of critical importance. While alignment techniques have significantly improved overall safety, LLMs remain vulnerable to carefully crafted adversarial inputs. Consequently, adversarial attack methods are extensively used to study and understand these vulnerabilities. However, current attack methods face significant limitations. Those relying on optimizing discrete tokens suffer from limited efficiency, while continuous optimization techniques fail to generate valid tokens from the model's vocabulary, rendering them impractical for real-world applications. In this paper, we propose a novel technique for adversarial attacks that overcomes these limitations by leveraging regularized gradients with continuous optimization methods. Our approach is two orders of magnitude faster than the state-of-the-art greedy coordinate gradient-based method, significantly improving the attack success rate on aligned language models. Moreover, it generates valid tokens, addressing a fundamental limitation of existing continuous optimization methods. We demonstrate the effectiveness of our attack on five state-of-the-art LLMs using four datasets. Our code is available at: https://github.com/sj21j/Regularized_Relaxation", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have achieved unprecedented success in numerous domains, including computer vision (Voulodimos et al., 2018), natural language processing (Brown et al., 2020), and program analysis (Ahmad et al., 2021). Large Language Models (LLMs), primarily based on the transformer architecture (Vaswani et al., 2017), are trained on vast amounts of textual data sourced from the Internet (Carlini et al., 2021). These models often surpass human performance on var-"}, {"title": "2 Related Work", "content": "Despite the effectiveness of alignment techniques, carefully engineered prompts can still compromise aligned LLMs (Yuan et al., 2023; Wei et al., 2024; Kang et al., 2024). These \u2018jailbreaks' typically require significant manual effort through trial and error. To address this, Liu et al. (2023a) introduced Auto-DAN, a genetic algorithm-based method that scales jailbreaking techniques for both open-source and commercial LLMs. Other researchers have explored using auxiliary language models to jailbreak target LLMs (Mehrotra et al., 2023; Shah et al., 2023; Yu et al., 2023). Orthogonal to jailbreaking, a different type of attack involves intervening in the generated content, given access to the top-k output token predictions of a model (Zhang et al., 2024). Recent research has introduced automatic prompt-tuning adversarial attack methods that leverage model gradients (Wallace et al., 2019). These techniques append an adversarial suffix to the user prompt and iteratively optimize it to trigger specific model responses (Shin et al., 2020; Guo et al., 2021). The optimization process operates in either discrete token space or continuous embedding space, aiming to increase the likelihood of the target response (Zou et al., 2023; Schwinn et al., 2024). However, the discrete nature of LLM inputs, particularly tokens, continues to pose challenges for automated search methods in producing reliable attacks (Carlini et al., 2024).\nRecently, Zou et al. (2023) introduced the Greedy Coordinate Gradient-based search technique (GCG), which optimizes an objective function over discrete inputs to find token replacements in the adversarial suffix. GCG, inspired by HotFlip (Ebrahimi et al., 2017), uses the first-order Taylor approximation around current token embeddings"}, {"title": "3 Methodology", "content": "Given an LLM $f_\\theta(\\cdot)$, where $\\theta$ represents the model parameters, our objective is to optimize the input token embeddings to effectively induce harmful behaviors. The LLM has an embedding matrix $E \\in \\mathbb{R}^{V\\times d}$, where $V$ is the vocabulary size and $d$ is the embedding dimension."}, {"title": "3.1 Formal Description of the Problem", "content": "The model requires an input sequence of tokens, represented as $x = [x_1,x_2,..., x_n]$, where each $x_i$ is a token from the model's vocabulary. Each token $x_i$ is mapped to its corresponding embedding vector $e_{x_i} \\in \\mathbb{R}^d$ through the embedding matrix $E$. An initial sequence of suffix tokens, represented as $x = [x_1, x_2,...,x_m]$ is appended to input sequence $x$.\nThe objective is to find an (optimal) embedding sequence $e^{adv} \\in \\mathbb{R}^{m\\times d}$, where $m$ is the length of the suffix, to maximize the likelihood of inducing the specified harmful behaviors when combined with the prompt embedding sequence."}, {"title": "3.2 Problem Setup", "content": "Formally, the problem can be set up as follows:\n1. Objective: Given an input prompt sequence $x$, with its corresponding embedding sequence $e_x = [e_{x_1}, e_{x_2},..., e_{x_n}]$, the goal is to find a suffix embedding sequence $e^{adv} = [e_{x_1}, e_{x_2},..., e_{x_m}]$ that minimizes the adversarial cross entropy loss $\\mathcal{L}$ between the model's output logits and the target string sequence $y = [y_1, y_2, ..., y_p]$. The adversarial cross entropy loss is defined as:\n$\\mathcal{L}_C = - \\sum_{t=1}^p log P(y_t | e_x^{(t)}|| e^{adv(t)})$\nwhere $P(y_t | e_x||e^{adv})$ is the probability of the $t$-th target token $y_t$ given the input embeddings $e_x^{(t)}||e^{adv(t)}$\n2. Optimization Problem: The optimization problem is expressed as:\n$e_{adv}^{(t+1)} = e_{adv}^{(t)} - \\alpha \\nabla_{e_{adv}} \\mathcal{L}(f_\\theta (e_x ||e_{adv}), y)$\nwhere:\n*   $\\alpha$ is the learning rate.\n*   $||$ denotes the concatenation operator.\n*   $\\mathcal{L}$ denotes the adversarial cross entropy loss function.\n*   $\\nabla_{e_{adv}} \\mathcal{L}$ denotes the gradient of the loss function with respect to $e_{adv}^{(t)}$\nExisting works approach this problem in different ways. GCG optimizes over discrete tokens to find suffix tokens that increase the likelihood of harmful behaviors. PGD relaxes the one-hot encodings of suffix tokens, optimizing these relaxed encodings and applying simplex and entropy projections, enabling efficient discretization and harmful behavior elicitation. Schwinn et al. (2024) optimize directly over continuous suffix token embeddings, providing a more efficient method unconstrained by discrete search spaces."}, {"title": "3.3 Regularized Relaxation", "content": "We propose a novel approach to solving the optimization problem in the continuous space, termed Regularized Relaxation(RR). Our method adds a regularization term to stabilize and enhance the optimization process. Specifically, this regularization guides the optimized embeddings towards the average token embedding, $\\bar{e}$. This movement helps the embeddings approach valid tokens along the optimization path. Fig. 1 contains an overview of our method.\nL2 regularization, also known as ridge regression, is a technique commonly used to prevent overfitting in machine learning models (Goodfellow et al., 2016). It achieves this by adding a penalty term to the loss function, encouraging the model to maintain smaller weights. In the context of embeddings, this regularization term encourages embedding vectors to be closer to the origin. For an embedding $e_{x_i}$, the L2 regularization term is given by $\\lambda ||e_{x_i} ||^2$, where $\\lambda$ is the hyperparameter controlling the regularization strength. This regularization effectively reduces the magnitude of $e_{x_i}$.\nVisualizing the average token embedding $\\bar{e} = \\frac{1}{V} \\sum_{i=1}^V e_i$ in Fig. 2 we observe that it is tightly centered around values close to zero. This concentration suggests that most of the model's token embeddings are clustered near this average. This trend also holds for the other models we evaluated, as shown in Fig. 6 provided in the appendix. For the embedding $e_{x_i}$, we can now define our regularization term as $\\lambda ||e_{x_i} - \\bar{e}||^2$, which measures the squared Euclidean distance between $e_{x_i}$ and the average token embedding $\\bar{e}$. This regularization"}, {"title": "3.4 Regularization through Weight Decay", "content": "Weight decay in machine learning also prevents overfitting by adding a penalty to the loss function proportional to the squared magnitude of the model weights. Unlike L\u00b2 regularization, which directly modifies the loss function, weight decay achieves regularization by decreasing the weights during the gradient update step (Loshchilov and Hutter, 2019). Both methods penalize large weights, making them effectively equivalent in this regard. Through our evaluation, we have found that using the AdamW optimizer (Loshchilov and Hutter, 2019), which integrates weight decay, efficiently regularizes our embedding optimization process, leading to faster convergence and improved outcomes."}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of our attack method on LLMs, we conduct experiments using several state-of-the-art open-source models. We measure the impact of our attacks on model alignment and robustness against adversarial attacks, and to compare our method's performance with leading optimization-based attack techniques."}, {"title": "4.1 Implementation Details", "content": "Models: We evaluate five open-source target models across various metrics: Llama2-7B-chat (Touvron et al., 2023), Vicuna-7B-v1.5 (Zheng"}, {"title": "4.2 Experimental Analysis", "content": "For our experiments, we initialize the adversarial attack suffix with a space-separated sequence of 20 exclamation marks (\"!\"). While our method is not sensitive to initialization, this choice ensures reproducibility."}, {"title": "4.2.1 Evaluation Metrics", "content": "To assess the performance and robustness of our attack methodology on LLMs, we use the key evaluation metric of Attack Success Rate (ASR), providing insight into the effectiveness of our approach compared to existing methods.\nWe evaluate the model's responses using two complementary model-based evaluators. The first"}, {"title": "4.2.2 Baselines", "content": "We evaluate the effectiveness of our method against several leading approaches, including GCG (Zou et al., 2023), recognized as the most effective attack on robust LLMs (Mazeika et al., 2024), as well as recent techniques like AutoDAN (Liu et al., 2023a), PGD (Geisler et al., 2024), and SoftPromptThreats (Schwinn et al., 2024). This comparison assesses our method's performance against both established and cutting-edge adversarial attack strategies.\nFor model evaluations, we load the models in half-precision format and perform both forward and backward passes in this format. To maintain consistency, we uniformly initialize the adversarial attack suffix across methods: for GCG and SoftPromptThreats, we use a sequence of 20 space-separated exclamation marks (\"!\"), while for PGD, we adhere to its implementation guidelines by initializing the suffix with 20 randomly initialized relaxed one-hot encodings. We set the number of steps to 250 for all methods and use greedy decoding for the generation to promote reproducibility. To ensure comparability with SoftPromptThreats, we incorporate an additional discretization step identical to ours since it does not yield discrete tokens after optimization. As PGD does not have a publicly available codebase, we implemented it based on their published details and achieve results consistent with their findings. Our code and data, including the PGD implementation, will be publicly accessible in the published version. Additional specifics on the configurations for each baseline can be found in Appendix B."}, {"title": "4.2.3 Evaluation against Existing Methods", "content": "We evaluate our method against the baselines GCG, AutoDAN, PGD, and SoftPromptThreats by selecting fifty harmful behaviors and their targets from each dataset to assess the ASR[>10] and ASR[>5]"}, {"title": "4.2.4 Evaluating Transfer Attacks Across Models", "content": "To assess the transferability of our approach, we first attack the source models Llama2-7b Chat and Vicuna 7B-v1.5, generating an adversarial suffix for each of the fifty harmful behaviors from each dataset. These adversarial suffixes are then appended to the corresponding harmful behaviors and"}, {"title": "4.2.5 Ablation Study", "content": "We investigate the impact of weight decay on the optimization process using Llama2-7b Chat and Vicuna 7B-v1.5. For this study, we optimize adversarial suffix tokens across fifty harmful behaviors from each dataset. To assess the effectiveness of incorporating regularization via weight decay, we compute ASR[>10] and ASR[>5] on the generated responses. As shown in Table 3, applying weight decay significantly enhances the ASR values, demonstrating the positive influence of regularization on optimization performance."}, {"title": "5 Discussion", "content": "In this study, we used regularization to effectively optimize adversarial suffix token embeddings, which discretize into tokens capable of eliciting harmful behaviors when input into LLMs. Discretization is crucial for methods operating in the continuous space, as embeddings that lie far outside the model's learned embedding space can disrupt generation, resulting in nonsensical or poor outputs. Although the generated adversarial suffix tokens may appear meaningless to humans, their embeddings remain valid (corresponding to tokens from the model's vocabulary), enabling the model to produce coherent outputs. A comparison of sample"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we introduced a novel method that uses regularized relaxation to enhance the efficiency and effectiveness of adversarial token generation. Our method optimizes for adversarial suffixes nearly two orders of magnitude faster than traditional discrete optimization techniques while also improving the ability to discretize and generate diverse tokens compared to continuous methods. Extensive experiments on five state-of-the-art LLMs demonstrate the robustness and efficacy of our method, outperforming existing approaches and providing a scalable framework for exploring vulnerabilities in LLMs.\nAs a white-box technique, our method requires access to both model embeddings and parameters to compute gradients. The transferability of such attacks, as discussed in Zou et al. (2023), further highlights the potential of our approach. Given its efficiency, our method has significant potential for large-scale testing across publicly available models. Future work will focus on evaluating its transferability and generalization to different domains, languages, and models. Advancing these aspects will deepen our understanding of adversarial robustness and strengthen defenses against evolving threats in natural language processing."}, {"title": "Limitations", "content": "We conduct experiments on five different LLMs using four datasets. While this approach is common in many similar studies, these datasets have a marginal overlap in terms of harmful behaviors. Testing these jailbreaking methods on significantly larger and more diverse datasets could provide more comprehensive results. Additionally, our attack requires open-source access to the models, as it is a white-box attack. The effectiveness of our method in a black-box environment has yet to be explored."}, {"title": "Ethics and Broader Impact", "content": "Research on adversarial attacks is essential for enhancing the robustness of Large Language Models (LLMs), but it also introduces significant ethical considerations. Techniques developed to exploit vulnerabilities in LLMs can be misused for malicious purposes, such as generating harmful content or spreading misinformation.\nOur work aims to study LLM vulnerabilities and encourage the development of defenses. However, these techniques have a dual nature and can be used for harmful activities if misapplied. As research progresses, it is crucial to study the development of robust defenses against adversarial threats, enhancing the reliability and trustworthiness of LLMs in applications like natural language understanding, generation, and decision-making.\nIn conclusion, while research on adversarial attacks research offers opportunities to improve model security and resilience, it requires careful consideration of ethical implications. Researchers and practitioners must proceed with caution, balancing the advancement of defensive measures with responsible consideration of potential misuse and societal impacts."}]}