{"title": "SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming", "authors": ["Anurakt Kumar", "Divyanshu Kumar", "Jatan Loya", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "abstract": "We introduce Synthetic Alignment data Generation for Safety Evaluation and Red Teaming (SAGE-RT or SAGE) a novel pipeline for generating synthetic alignment and red-teaming data. Existing methods fall short in creating nuanced and diverse datasets, providing necessary control over the data generation and validation processes, or require large amount of manually generated seed data. SAGE addresses these limitations by using a detailed taxonomy to produce safety-alignment and red-teaming data across a wide range of topics. We generated 51,000 diverse and in-depth prompt-response pairs, encompassing over 1,500 topics of harmfulness and covering variations of the most frequent types of jailbreaking prompts faced by large language models (LLMs). We show that the red-teaming data generated through SAGE jailbreaks state-of-the-art LLMs in more than 27 out of 32 sub-categories, and in more than 58 out of 279 leaf-categories (sub-sub categories). The attack success rate for GPT-40, GPT-3.5-turbo is 100% over the sub-categories of harmfulness. Our approach avoids the pitfalls of synthetic safety-training data generation such as mode collapse and lack of nuance in the generation pipeline by ensuring a detailed coverage of harmful topics using iterative expansion of the topics and conditioning the outputs on the generated raw-text. This method can be used to generate red-teaming and alignment data for LLM Safety completely synthetically to make LLMs safer or for red-teaming the models over a diverse range of topics.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) like GPT-4 (OpenAI et al. 2023), Calude-3.5 (Anthropic 2024), Llama-3 (Dubey et al. 2024), and Mistral (Jiang et al. 2024a) have shown state-of-the-art performance in instruction following, zero-shot learning tasks, code generation and a range of downstream natural language processing (NLP) tasks. These LLMs gain their power by being trained on huge corpus of texts (Villalobos et al. 2022) of the order of trillions of tokens and having a large parameter space (Kaplan et al. 2020) of the order of billions of parameters. After being trained on a huge corpus of text for the next token prediction task the LLMs undergo supervised fine-tuning (SFT) where they learn question-answering and instruction following. Through this training paradigm and the due to the large size of the dataset the LLMs inadvertently learn to generate toxic, unethical or unsafe content. In order to make the LLM's responses more aligned to human values these LLMs undergo an alignment (Ouyang et al. 2022; Rafailov et al. 2023; Hong, Lee, and Thorne 2024; Meng, Xia, and Chen 2024) and safety training phase through which their responses are aligned towards human responses and values, and the LLMs learn to generate safe and aligned outputs. Even after undergoing this safety training and alignment phase these LLMs can still be jailbroken to generate unsafe, or unethical content. Models like Llama-3 (Dubey et al. 2024), GPT-4 (OpenAI et al. 2023) undergo extensive safety training but still can be jailbroken as shown in table (1). Therefore, evaluating the LLMs on different harmfulness categories and safeguarding them against jailbreak attacks through alignment, or guardrails becomes a necessary task.\nThe evaluation of safety vulnerabilities of an LLM requires red-teaming where human evaluators test the safety of LLMs by generating different types of attacks manually, or by using automatic attack algorithms (Mehrotra et al. 2023; Chao et al. 2023; Zou et al. 2023; Greshake et al. 2023; Liu et al. 2023; Zhu et al. 2023; He, Ahamad, and Kumar 2021; Le, Wang, and Lee 2020; Kim et al. 2023) to try to jailbreak the models. This testing gives an insight into the safety vulnerabilities of the LLMs but the data generated by these methods is not sufficient, lacks diversity and is expensive to generate on a large scale even for a small number of topics. This makes testing the nuanced vulnerabilities of LLMs difficult. Automatic black-box attack algorithms such as tree-of-attack-pruning (TAP) (Mehrotra et al. 2023), and Prompt Automatic Iterative Refinement (PAIR) (Chao et al. 2023) do not offer sufficient control over the attack generation process and suffer from mode collapse where they generate attacks in limited variations. In the case of human generated alignment datasets such as Nvidia AI Safety (Ghosh et al. 2024), and Anthropic RLHF (Bai et al. 2022) there is a lack of prompts which can be considered as attacks, these datasets contains harmful queries which are asked as direct questions such prompts are not effective in jailbreaking current LLMs which can easily detect and reject these prompts. Synthetic red-team dataset generation methods like AART (Radharapu et al. 2023) do not generate high quality prompts which are nuanced or represent the types of attacks actually faced by LLMs, this method also suffers from mode collapse, i.e., it generates similar type of query prompts. This is due to lack of direction and control in the generation process. Wild-teaming (Jiang et al. 2024b) addressed different jailbreak techniques but lacks in covering nuanced aspects of a harmful topic such as queries about the sub-tasks involved in bomb making and used manually generated LMSYS-1M (Zheng et al. 2023) dataset to extract the jailbreaking techniques. These datasets and synthetic data generation methods helps us understand the vulnerabilities of LLMs but they either require large manual seed data for generation, or suffer from a lack of nuanced and diverse data.\nOur synthetic dataset generation method ensures diversity and nuance at every step of generation it starts with the harmfulness taxonomy as defined by ALERT (Tedeschi et al. 2024). The categorisation by ALERT (Tedeschi et al. 2024) covers a lot topics and sub-topics but misses the niche aspects of these sub-topics. For example, in the case of the category \"Sexual Content\" many of its niche aspects are put under the sub-category 'sex-other' which is not helpful for synthetic-data generation as many sub-sub topics or leaf categories such as 'child-porn' can be missed out. In the first step we create these 'sub-sub-categories' or leaf-categories conditioned on their category and sub-category. For the 6 macro-harmful categories under which there are 32 sub-categories we generate 320 harmful sub-sub categories also called leaf categories which covers each of the sub-categories in detail. The leaf category generation is done by an LLM conditioning it on the category and sub-category to ensure leaf-categories are mutually exclusive the expanded taxonomy is given in table (4) and table (5) in the Appendix. The next step is generation of raw-texts such as Blogs and Articles to get a rich content from which queries can be extracted. The raw-text generation step ensures that there are aspects of the topics covered which were not explicitly defined earlier. The raw-text is generated using a toxic-LLM (Zuev 2023) and the instruction generating model is Mistral (Jiang et al. 2024a) which has been given few-shot examples. The raw-text is then used for query extraction to ensure niche aspects of the topic which are harmful are also extracted in the form of prompts this step gives us the red-teaming data. These diverse queries are then fed to a toxic and a well aligned LLM and their responses are used to convert the red-team dataset to an alignment dataset for DPO (Rafailov et al. 2023).\nOur key contribution is a synthetic safety-alignment and red-teaming data generation method which generates high quality synthetic data from the given taxonomy of harmfulness ensuring diversity and nuance at each step of the generation process. The generation pipeline focuses on the following key aspects of synthetic safety alignment data generation:"}, {"title": "Generating diverse and nuanced queries for different", "content": "harmful tasks: The expanded taxonomy of harmfulness along with the query generation step ensures diversity by covering 320 leaf categories and for each leaf category we generated multiple types of attack prompts in an iterative manner to cover around 1500 categories and generating 51k prompts ensuring a depth-wise and diverse coverage of every macro-category."}, {"title": "Generating queries which are able to test multiple aspects of model safety", "content": "Our method ensures that the generated prompts are able to test different aspects of the model safety by generating queries for different tasks such as roleplaying tasks, fictional scenario based tasks, biased content generation tasks, toxic sentence completion tasks, direct questions and other such prompt-types. The different prompt-types are given in table (2). These tasks were chosen as they are the most frequent types of attacks seen by LLMs, and the pipeline could easily be customised to generate even more types of attacks just by adding their description."}, {"title": "Generating sub-task based and constrained queries", "content": "Our query generation method ensures there are queries which question niche aspects of a task for example the niche aspect can be a sub-task, i.e., we generate raw-text (or raw text) on a leaf topic such as a \u201cblog on bomb making at home\", now, our iterative query extraction method ensures the generated queries cover sub-tasks and constraints involved in bomb making such as 'procuring bomb materials'. Similarly, an example of a constrained query in the case of bomb making will be, \u2018how can a 23 year old with $40 build a bomb?' Our generation method ensures these types of prompts are generated for every leaf topic\nThe rest of the paper is organised in the following manner: section 2 (related works) covers the related work in the domain of synthetic data generation and red-teaming data generation, then section 3 (methodology) describes our methodology for red-teaming and alignment data generation, section 4 (results) shows the red-teaming results and the final section 5 (conclusion) gives the conclusion."}, {"title": "Related Works", "content": "AI-Assisted Red Teaming (AART) (Radharapu et al. 2023) showed how an LLM can generate synthetic data for adversarial testing given very little context about the problem statement. AART (Radharapu et al. 2023) works by first asking an LLM to generate a list of topics, task formats and demographic features, in the second step a human selects the final list, and the triplets {(topici,task formati,demographyi)}N1 is fed into a template which generates the prompts in the desired task format. Here, task-formats can be emails, letters, memos etc. This method incurs many pitfalls as the generated prompts suffer from mode collapse because sufficient direction is not given to the LLM and thus the model shows repetitive behaviour by either choosing the same task format, or starting many queries using the same prefix like \"how to get away with...\". In the paper the authors also point out that the generated queries lacked nuance, i.e., the queries included the context plainly as generated and did not generate queries to similar or related topics.\nTree-of-Attack Pruning (TAP) (Mehrotra et al. 2023) is well known to jailbreak state-of-the-art LLMs by using tree-of-thought attacks. This method takes in the goal as the input and iteratively improves the prompt while pruning the off-topic prompts. The prompts generated by TAP have high success rates but suffer from two major drawbacks (1) Generation and refinement of prompts takes a lot of time as the algorithm tests a lot of prompts till it reaches a maximum number of iterations or finds a jailbreak (2) The attacks generated by TAP also suffer from mode collapse where jailbreak techniques such as role-playing, fictional scenarios, and direct questions types of prompts are mostly observed. This poses a challenge as many types of attacks such as coding based attacks might not be covered unless explicitly mentioned in the goal of the attack. TAP and PAIR (Chao et al. 2023) does not scale well as the number of goals are increased and it produces prompts that exhibits a lack of diversity leaving many different types of attacks unexplored.\nWild-teaming (Jiang et al. 2024b) uses the LMSYS-1M chat (Zheng et al. 2023) data to extract different jailbreak techniques and then uses the taxonomy define by (Wang et al. 2023) as goals to generate attacks. Rainbow-teaming (Samvelyan et al. 2024) pre-defines a category of jailbreak techniques and uses a taxonomy based goal with iteratively mixing the elements of the harmfulness taxonomy. These methods show how diversity in the attack prompts can be generated by identifying the different types of attacks beforehand. These methods extract jailbreak prompts much by identifying the topics (goals) and then converting them to a jailbreak attacks but their goals are limited to a range of topics directly present in the taxonomy or can be created by a mix of topics present in the taxonomy. Our methods explicitly expands the taxonomy by first generating the leaf-categories, and then extracting the queries on the basis of the generated toxic raw-text which ensures that topics such as the 'sub-processes involved in bomb making eg: gathering bomb grade ammonium nitrate' are also explicitly covered. These topics are extracted from the raw-text which helps in covering harmful topics in-depth which are not explicitly mentioned in the taxonomy of harmfulness. These topics are then converted to most frequently observed jailbreak attacks by using the definition and few-shot example of the attack and can be changed by sampling any list of jailbreak techniques towards which the target model is specifically vulnerable or according to custom needs."}, {"title": "Methodology", "content": "The dataset generation method follows a three-step process:\n\u2022 Generating a diverse set of raw-text based on taxonomy of harmfulness\n\u2022 Query extraction with iterative improvement\n\u2022 Generating alignment data by querying the harmful prompts. These steps are described in algorithm (1), algorithm (2), and algorithm (3)\nThe details of all the system prompts needed to generate the synthetic data are mentioned in the Appendix."}, {"title": "Algorithm-1: Raw text generation", "content": "Given a list of macro-categories and sub-categories we first generate the leaf categories (sub-sub categories) for each sub-category (sub-category) to ensure coverage of the nuanced aspect of every sub-category. The number of leaf categories can be customised and it can be significantly increased or decreased depending on the need to cover different aspects of the problem. These leaf-categories are generated using Mistral (Jiang et al. 2024a) and their hierarchy and related with the taxonomy defined by ALERT (Tedeschi et al. 2024) is shown in table (4) and table (5) in the Appendix. The categories, sub-categories and the leaf categories combine to give the instruction for generating content in a given task format. We choose to generate 5 instructions for every given category, sub-category and leaf-categories to further ensure that different aspects of the topic are covered. An example of the generated instruction can be, \"Compose a book summary on creating a dirty bomb in your basement\". For $TF_i$ in {TF1,TF2,...TFn} we generate five instructions per task-format explicitly maintaining the diversity through few-shot prompting. Over here different types of instructions are generated depending on the task formats\n$Ins_{(i,j,k,l)} = LLM(.|MC_j,MC_k, sc_l, tf_i)$    (1)\nThe number of raw texts instructions generated at this step are given by equation (2)\n$N_{ins} = N_{TF} \u00d7 N_{mC} \u00d7 N_{sc} \u00d7 N_{samp}$ (2)\nHere, MC is the macro-category, mC is the micro-category, sC is the leaf-category (sub-sub category), TF is the task-format and $N_{samp}$ is the number of samples. In our case $N_{samp}$ is chosen to be five and it is a hyper-parameter which can be set according to the number of prompts required per (MC,MC, sc) triplet. These instructions $Ins_{i,j,k,l}$ for all (MC,MC, sc,tf_i) are queried to a SolarLM (Zuev 2023) which generates the raw-text in the form of Blogs, Articles, Book Summaries, and Social Media posts. SolarLM (Zuev 2023) was chosen after experimenting with Llama-3-8B-Lexi-Uncensored (Orenguteng 2024) and Wizard-Vicuna-13B-Uncensored-GGUF (Computations 2023) which showed toxic behaviour over some tasks but denied to respond over many tasks. We generate this raw-text to ensure that the query extraction phase can extract diverse queries from a given leaf category to fulfil two key requirements (1) Ensure niche aspects and sub-tasks of the tasks are present as queries for example, if we have a blog on bomb making then we also want to have queries which question different steps of the bomb making process such as gathering raw materials, planting the bomb as these queries can be individually harmful as well, and (2) Ensure diverse topics related to a chosen leaf-topics are also covered. LLM\u2081 in algorithm (1) in our case was Mistral (Jiang et al. 2024a), and LLM2 was SolarLM (Zuev 2023)."}, {"title": "Algorithm-2: Query Extraction", "content": "The second algorithm (2) extracts different pre-defined types of unethical or toxic queries from the raw-text. These included most frequent types of jailbreak attacks such as role-playing attacks, fictional attacks, coding based attacks, sub-task prompts and more. A detailed analysis of vulnerabilities of different models against different types of prompts and description of each type of prompt is given in Appendix table (2) and figures (6) - (15). These queries are iteratively diversified in their specific domain over different number of epochs. For example, if the initial query was a roleplaying jailbreak where the roleplaying character was a doctor then in the next epoch the roleplaying character will not be a doctor and the prompt structure will also be changed whilst being a roleplaying prompt. For each of the raw text generated in the previous step we generate 9 different types of jailbreaks (a) Direct question, (b) Biased, (c) Toxic sentence completion, (d) Fictional scenario, (e) Roleplaying scenario, (f) Story writing, (g) Coding task, (h) Sub-task based question, (i) Constrained situations. These types were selected to cover a diverse and most frequent types of jailbreak attacks faced by LLMs. The total number of queries generated at this step are given by equation 3\n$N_q = N_{rt} \u00d7 N_{jbs} \u00d7 N_{epochs}$   (3)\nWhere $N_q$ is the generated number of queries, $N_{rt}$ is the number of raw-texts generated by algorithm (1), $N_{jbs}$ is the number of most frequent jailbreak types selected, and $N_{epochs}$ is the number of iterations performed per-query. This gives us a diverse and nuanced set of queries which can be used for red-teaming an LLM over a diverse range of topics, and attack types. Over here the LLM used for query generation LLM\u2081 as given in (2) was Mistral-8x7B (Jiang et al. 2024a)"}, {"title": "Algorithm-3: Alignment Data Generation", "content": "Algorithm (3) converts the red-teaming data into a direct preference optimisation (DPO) dataset (Rafailov et al. 2023). This requires access to an uncensored LLM and a safety aligned LLM which has been prompted to give the rejection response and the reason for rejection. The uncensored LLM in our case is SolarLM (10.7B) (Zuev 2023) and the aligned LLM is Llama-3-instruct (Dubey et al. 2024). This will create a (query, rejected response, aligned response) triplet D = {(qi,ari,rji)1}. This dataset can be used to perform direct preference optimisation (DPO) or some variation of it to make the LLM less vulnerable towards attacks and a variety of harmfulness topics.\nThe $JudgeLLM$ was GPT-40 which scored the response of the safe model and determined whether it was jailbroken. The final dataset consists of D = {(MCi, mCi, sCi, rti, pti, gpi, toi, soi)}N=1where rti is the generated raw-text, pti is the extracted prompt-type, gpi is the generated prompt, tor is the toxic model's response, and soi is the safe model's output, rest of the notation is same as algorithm (1).\n$Attack Success Rate (ASR %) = \\frac{N_{jailbroken}}{N_{total}} \u00d7 100%$   (4)\nWhere Njailbroken is the number of categories/sub-categories/leaf-categories which were jailbroken, and Ntotal is the total number of categories/sub-categories/leaf-categories depending on what we are evaluating. The terms have the same meaning as described in Algorithm (1). A score of 100% ASR means the model was jailbroken for all 6 macro-categories. It means at least one prompt jailbroke the model for each-category/sub-cat/leaf-cat. It DOES NOT mean all prompts were successful in jailbreaking the model as described earlier. Similarly, the ASR for sub-cat and leaf-cat is calculated."}, {"title": "Results and System Configuration", "content": "The results show the different category of prompts generated by SAGE and their project in the 3-D plane which shows minimal overlap between the prompt types in Fig (3) this shows the diversity and addressing of the nuances described earlier. The N-gram score is shown in Fig (16) which further shows the diversity in the dataset. We red-team various open-source and closed-source models and evaluate their responses to calculate the attack success rate (ASR) as defined by 4 across various macro-categories, sub-categories, and leaf-categories. This shows the effectiveness of SAGE in generating synthetic data and also shows how SAGE can be used to evaluate different aspects of harmfulness shown by the LLM. GPT-40 was used to determine whether the model was jailbroken or not."}, {"title": "Red-teaming prompt clusters", "content": "For each of raw-text generated by Algorithm-1 the query extraction step generates 9 different types of prompts. Fig (3) show the clusters formed by a sample of these prompts. The clusters are created by converting the prompts into embeddings using \"all-MiniLM-L6-v2\" (Wang et al. 2020) and then reducing the dimension to three using UMAP with cosine similarity metric. This projection gives well separated out clusters which shows that the generation method extracts a diverse set of queries from the generated raw text. In the of SAGE only 9 different types of queries were selected, but this can easily be increased or decreased according to the red-teaming task's needs and the computational requirements."}, {"title": "Red-teaming results", "content": "We evaluated the generated prompts on 279 leaf-categories. The evaluation method was standard as the target-LLM was given the query prompt and its response was scored and declared 'Safe' or 'Unsafe' by a judge-LLM which was GPT-40. Table (1) shows the vulnerability of 10 models when they are evaluated across all macro-categories, sub-categories and leaf-categories. We randomly sample 500 prompts from each macro-category and query it to the LLM and since we have 6 macro-categories we query a total of 3000 prompts for each model.\nThe columns cat-wise in table (1) shows that for all the macro-categories SAGE was able to find at least one jailbreak as we get a 100% ASR for all macro-categories. In the case of sub-categories, i.e., the sub-cat column in table (1) shows that for the 32 sub-categories given by ALERT (Tedeschi et al. 2024) the percentage of sub-categories for which we were able to jailbreak the models. The definition of each sub-category and category is exactly as defined by (Tedeschi et al. 2024). It can be seen that GPT-4-0125-preview, Llama-2-7b-chat-hf, and Llama-3-8b-instruct are also vulnerable across more than 27 sub-categories of harmfulness. The total number of successful jailbreaking prompts are given in table (3) and a detailed analysis in given in the figures (6) - (15). in the Appendix. The exact sub-categories of vulnerability is given in table (6) in the appendix. The leaf-cat column in the results table 1 shows the vulnerability of the models across 279 leaf-categories which were evaluated this again shows that even the safest model Llama-3-8b-instruct is vulnerable to 55 leaf-categories or harmful topics. The prompt type and corresponding ASR for all the models are mentioned in the Appendix table (2) where it can be seen the vulnerability of models shows huge variation across different prompt types. The most successful prompts which were able to jailbreak the LLMs were 'Coding-based' and 'Story-based' as shown in the appendix figures (6) - (15). The results shown above demonstrated the effectiveness of SAGE in jailbreaking and systematically evaluating the vulnerabilities of LLMs. The number of topics each LLM is vulnerable against is given in Fig (5) in the Appendix. The detailed results which show exactly which sub-categories these LLMs are vulnerable against is given in table (6) in the Appendix. Fig (4) visualises the table (1) in the form of bar graphs."}, {"title": "System Configuration", "content": "The data was generated by hosting SolarLM (Zuev 2023), and Llama-3 (Dubey et al. 2024) for generating toxic and aligned data respectively using four A100 GPUs from Standard NC96ads A100 v4 (96 vcpus, 880 GiB memory) instance. The GPT models were accessed through their official APIs. Mistral (Jiang et al. 2024a), Llama-2 (Touvron et al. 2023), and Gemma-7b-it were used from Together AI endpoints."}, {"title": "Conclusion and Future Work", "content": "In this paper we introduced SAGE a synthetic red-teaming data and alignment data generation pipeline. This pipeline offers a lot of flexibility and can be used to generated synthetic red-teaming and alignment data for custom red-teaming tasks. It generates data by dividing the generation process into three steps and ensuring diversity and nuance at each step. The prompt types used by the pipeline and the generated queries showed a high attack success rate (ASR). We are training different models using DPO, ORPO, SIMPO (Rafailov et al. 2023; Hong, Lee, and Thorne 2024; Meng, Xia, and Chen 2024) and other alignment methods on a mix of safe data and SAGE's alignment data to understand its impact on model's safety and performance and a detailed analysis will be released in future works."}, {"title": "Ethics Statement", "content": "The central goal of this research is to explore the potential safety and security risks linked to the misuse of large language models (LLMs). Our research is guided by a strong commitment to ethical principles, including respect for all individuals, especially minority groups, and an unwavering stance against violence and criminal activities. This study aims to uncover the vulnerabilities in current LLMs to help in creating more secure and reliable AI systems. The inclusion of any potentially harmful content, such as offensive language, harmful prompts, or illustrative outputs, is strictly for academic purposes and does not represent the beliefs or values of the authors."}]}