{"title": "AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models", "authors": ["Qianchi Zhang", "Hainan Zhang", "Liang Pang", "Hongwei Zheng", "Zhiming Zheng"], "abstract": "Retrieved documents containing noise will hinder RAG from detecting answer clues and make the inference process slow and expensive. Therefore, context compression is necessary to enhance its accuracy and efficiency. Existing context compression methods use extractive or generative models to retain the most query-relevant sentences or apply the information bottleneck theory to preserve sufficient information. However, these methods may face issues such as over-compression or high computational costs. We observe that the retriever often ranks relevant documents at the top, but the exact number of documents needed to answer the query is uncertain due to the impact of query complexity and retrieval quality: complex queries like multi-hop questions may require retaining more documents than simpler queries, and a low-quality retrieval may need to rely on more documents to generate accurate outputs. Therefore, determining the minimum number of required documents (compression rate) is still a challenge for RAG. In this paper, we introduce AdaComp, a low-cost extractive context compression method that adaptively determines the compression rate based on both query complexity and retrieval quality. Specifically, we first annotate the minimum top-k documents necessary for the RAG system to answer the current query as the compression rate and then construct triplets of the query, retrieved documents, and its compression rate. Then, we use this triplet dataset to train a compression-rate predictor. During inference, the compressor adaptively selects the top-k documents as the context-filtering documents based on the predictor's output and performs LLM inference. Experiments on three QA datasets and one conversational Muiti-doc QA dataset show that AdaComp significantly reduces inference costs while maintaining performance nearly identical to uncompressed models, achieving a balance between efficiency and performance.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) has demonstrated impressive performance across various knowledge-intensive NLP tasks, such as open-domain question answering (Mao et al. 2021), fact verification (Chen et al. 2022), and knowledge-grounded dialogue generation (Huang et al. 2023). It improves the relevance, coherence, and factual accuracy of outputs by appending a large number of retrieved documents to the query as context (Gao et al. 2023). However, the effectiveness of the current RAG heavily depends on the relevance of retrieved documents (Liu et al. 2024). When retrieved documents contain noise or irrelevant information, the generation model struggles to detect answer clues because noise interferes with self-attention's ability to reason over the correct context. Moreover, the inference will also become slow and costly (Zhu et al. 2024). Therefore, it is crucial to filter out irrelevant and low-value contexts.\nCurrent context compression methods mainly use extractive or generative models to compress the retrieved documents, but they may face issues of over-compression and high computational costs. Xu, Shi, and Choi (2023) introduce RECOMP to select the most query-relevant sentences as filtered context, but it may struggle with complex queries, such as multi-hop or open-ended questions, because it over-compresses the context, leading to a decline in RAG performance. The Information Bottleneck Theory (Zhu et al. 2024) tries to use reinforcement learning to find the optimal compression strategy by maximizing the mutual information between compressed data and the actual output while minimizing the mutual information between compressed data and retrieved documents. However, its high computational costs make it difficult to quickly adapt to various retrieval systems in the real world. Therefore, a more efficient and low-cost method is needed to retain sufficient context for RAG while minimizing noise and computational overhead.\nWe find that in most cases, the retriever can rank relevant documents at the top, but the exact number of documents needed to answer the query is uncertain\u00b2 due to the impact of query complexity and retrieval quality. For example, complex multi-hop questions require more documents for comprehensive judgment, and open-ended questions need broader background knowledge to provide a well-rounded answer, such as \"How to take care of a little cat?\". What's more, retrieval quality can also influence the number of required documents. When the retrieval quality is high, even complex questions may be answered with just the most relevant information. Conversely, when it is poor, as shown in Figure 1, even simple questions may require synthesizing more information to arrive at the correct answer. Therefore, determining the number of minimum required documents(compression rate) should consider both the question and the retrieved documents, quickly adapting to find the optimal compression strategy for different retrievers.\nIn this paper, we propose a low-cost extractive context compression method, named AdaComp, which adaptively determines the compression rate based on both query complexity and retrieval quality. Specifically, we first annotate the minimum top-k documents required by the RAG system to accurately answer the query as the compression rate, and then construct triplets consisting of the query, retrieved documents, and compression rate. We then concatenate the query with retrieved documents to form the input and use the compression rate from the triplets as the output to train a compression-rate predictor. This predictor can adaptively determine the top-k documents needed by the current RAG system based on query complexity and the retrieved documents. During inference, the compressor selects the top-k documents as the filtered context based on the predictor's output, ensuring the RAG system has access to concise and sufficient information. This method enables quick and low-cost adaptation to various RAG systems without the need for multiple inferences.\nWe conduct experiments on three open-domain question answering datasets, i.e., NQ, TriviaQA, HotpotQA, and one conversational Multi-doc QA dataset. The results show that AdaComp outperforms the baseline models in maintaining performance while significantly reducing the context required for inference. Further analysis of various difficulty queries demonstrates that AdaComp is more accurate in perceiving the required amount of documents, thereby validating the effectiveness of our approach.\nTo summarize, our main contributions are as follows:\n\u2022 We propose an automated and low-cost compression rate annotation method that determines the minimum required top-k documents based on the system's real ability, enabling quick adaptation to various RAG systems.\n\u2022 We design an effective extractive context compression method, which determines the compression rate based on both query complexity and retrieval quality.\n\u2022 Experiments on four datasets show that filtering out unimportant noisy documents improves inference efficiency while maintaining performance."}, {"title": "Related Work", "content": "Given the inherent limitations of retrievers, the content they retrieve often contains noise, which can significantly undermine the accuracy of the generated output. Moreover, when the context provided to the model is excessively long, it can further diminish the model's efficiency. To address these challenges, Xu, Shi, and Choi (2023) propose leveraging high-performance large language models (LLMs) to train summarization compressors that condense the retrieved texts. However, this approach is not without its flaws, as the generated summaries sometimes fail to faithfully represent the original content. Similarly, Xu, Shi, and Choi (2023); Wang et al. (2023) explore the use of extractive compressors that identify and select the sentences most relevant to the query. While this method helps in filtering out irrelevant information, it also faces the risk of excessive compression, which can lead to a reduction in output accuracy. In a different approach, Li et al. (2023) introduce the concept of Selective Context, which aims to enhance the efficiency of LLMs during inference by eliminating redundant content based on self-information metrics. However, this technique may inadvertently disrupt the semantic coherence of the context. Finally, Zhu et al. (2024) apply the information bottleneck principle to filter noise, striving to strike a balance between conciseness and correctness. Despite its potential benefits, this method is associated with high computational complexity during the training process, posing additional challenges for practical implementation. Several general compression methods have not been specifically tailored or optimized for Retrieval-Augmented Generation (RAG). For instance, Ge et al. (2023) and Chevalier et al. (2023) focus on compressing long contexts into short, compact memory slots that can be directly utilized by the large language model (LLM) for various downstream tasks. Additionally,"}, {"title": "Method", "content": "In this section, we first introduce the architecture of AdaComp, then describe how to obtain compression rate labels, train a compression-rate predictor, and integrate the compression model with generation models for inference, respectively."}, {"title": "Model Architecture", "content": "The Adaptive Context Compression Architecture is a novel framework designed to optimize document selection for retrieval-augmented generation tasks by balancing query complexity and retrieval quality. This architecture is composed of three main components: a retrieval module R, a compression module Co, and a generation model G, as shown in Figure 2. Given a query q, a target response y, and a set of retrieved documents D = {d1, d2, . . ., dN}, unlike traditional approaches that handle all the retrieved documents indiscriminately, our method dynamically selects an optimal subset of documents D' \u2286 D that allows the generative model G to produce the target response y. The compression module Co is specifically trained to perform this document selection. It receives as input the query q and the full set of retrieved documents D and outputs a compressed subset D'. This process reduces the computational burden on the generation model by limiting the input size without compromising the quality of the response. The main component of compression module Co is a compression-rate predictor f (D, q) which determines the size of D' based on the query q and the retrieved documents D. When the query is simple or the retrieval quality is high, the compressor selects fewer documents; conversely, for complex queries or lower-quality retrievals, it selects more documents. This ensures that the generation model G is provided with the most pertinent information, enhancing the accuracy and efficiency of the generated output. Therefore, our work focuses on how to train an efficient and low-cost compression-rate predictor."}, {"title": "Predictor's Training Data", "content": "To determine the optimal document subset D' for each query q, we employ a data annotation method based on real RAG system feedback. Given a query q and a set of retrieved documents D = {d1,d2,...,dN}, the objective is to identify the smallest subset D' such that the RAG system M can generate the correct response y based on q and D'.\nLet Dk = {d1,d2, ..., dk } represent a subset of D containing the top-k documents, where 1 \u2264 k \u2264 N. The system's performance is evaluated on each subset Dk by checking if the system's output M(q, Dk) matches the ground truth y. The correctness condition is defined as:\n$Correct(q, D_k) =\\begin{cases} 1, & \\text{if } M(q, D_k) = y \\\\ 0, & \\text{otherwise} \\end{cases}$\nThe process involves iterating over the subsets from the largest to the smallest, starting with DN and continuing to D1, as shown in Algorithm 1. The optimal subset D' is the smallest subset Dk for which the system generates a correct response:\n$D' = arg \\underset{k}{min} \\{k | Correct(q, D_k) = 1\\}.$"}, {"title": "Compression-rate Predictor", "content": "Given an input query q and the set of retrieved documents D = {d1,d2,...,dN}, the goal of context compressor is to train a compression-rate predictor f(D, q) to predict the number of documents |D' | that should be selected from D to generate a target response. We utilize the Llama2-7b model as the base model of the compression-rate predictor. The compression-rate predictor f (D, q) is trained to output a discrete number n, where n can take any value from the set {0,1,..., N}. The training process involves fine-tuning the Llama2-7b model on the above-annotated training dataset consisting of triples (q, D, n\u0302), where n\u0302 represents the true number of documents required.\nThe training objective is to minimize the classification loss between the predicted number of documents and the actual number required. Let n\u0302 be the true number of documents needed for optimal performance. The loss function L(\u03b8) is defined as follows:\n$L(\\theta) = -\\sum_{i=1}^{M} [True_i \\cdot log(p_i) + (1 - True_i) \\cdot log(1 - p_i)]$,\nwhere True is the true class label n\u0302 for the i-th training example, and pi is the predicted probability of the i-th example belonging to each class.\nDuring fine-tuning, the Llama2-7b model learns to map the input query q and document set D to the appropriate class n representing the number of documents required. The optimization process updates the model parameters \u03b8 to minimize the classification loss:\n$\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_{\\theta} L(\\theta)$,\nwhere \u03b7 is the learning rate.\nTo evaluate the performance of the trained compressor, the predicted number of documents n is compared to the true number n\u0302. Metrics such as accuracy, precision, and recall are used to assess how effectively the compressor predicts the optimal number of documents. Through this approach, we aim to train the compressor Co to accurately determine the number of documents needed for generating high-quality responses based on the input query and retrieved documents."}, {"title": "Utilizing Compressed Context for Generation", "content": "We focus on leveraging a high-performance black-box model to utilize compressed context effectively for generation tasks. This approach involves using a compressed subset of documents D' to optimize computational efficiency while ensuring high-quality responses.\nDuring training, for each query q and its associated oracle documents D', the input to the generation model G is constructed by concatenating q with D'. The model is trained to produce the correct output o given this input, formalized as:\nG(o | q \u2295 D').\nFor inference, a context Dpred is derived from the full set of documents D. This filtered context Cf is obtained by selecting the most relevant documents based on the model's compression-rate predictor. The input to the generation model during inference is:\n$C_f = q \\oplus D_{pred}$\n$D_{pred} = Top-K(D)$,\nwhere K is determine with f (D, q)."}, {"title": "Experiments and Analysis", "content": "In this section, we will introduce datasets, evaluation metrics, settings, baselines, and further analysis."}, {"title": "Experimental Settings", "content": "Datasets We evaluate our adaptive content compression method on three benchmark datasets: Natural Questions (NQ) (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), HotpotQA (Yang et al. 2018) and conversational Multi-doc QA . We utilize the adversarial Dense Passage Retriever (DPR) (Karpukhin et al. 2020) to retrieve the top 5 passages from the full Wikipedia passages for each QA dataset."}, {"title": "Main Results", "content": "We report the results on three QA datasets in Table 1. We find that all retrieval augmentation methods improve performance over the no retrieval setting, indicating that retrieval operations are crucial for enhancing model performance. Secondly, by retrieving documents without compression (e.g., Top 1 and Top 5 documents), the model shows significant improvements in EM and F1 scores across all datasets, with a further boost in performance when using the Top 5 documents. This suggests that retrieving more relevant documents can improve the quality of the model's answers. The Oracle method (theoretical best) outperforms all other methods, particularly on the NQ and TriviaQA datasets, demonstrating that adaptively selecting the appropriate compression rate can greatly enhance model performance. Our AdaComp method performs better across all three datasets compared to the RECOMP and FILCO methods. It also significantly reduces the number of tokens, indicating that the adaptive compression strategy strikes a good balance between token count and performance. The RECOMP and FILCO methods, by compressing the retrieved documents, significantly reduce the number of tokens but show significantly lower performance in terms of EM and F1 compared to the uncompressed retrieval methods. This suggests that while compression strategies effectively reduce input size, they may lead to performance degradation due to over-compression.\nWe report the results on one conversational dataset in Table 2. We divide the test dataset into specific and open-ended questions according to the following rule: we calculate the top 5 relevance scores between each document and the given answer. If the maximum relevance difference exceeds 0.3,"}, {"title": "The Impact of Document Number", "content": "As illustrated in Figure 3, the number of documents utilized in RAG significantly affects performance. Initially, performance improves with an increasing number of documents, as the inclusion of additional relevant information enhances the model's accuracy and the breadth of its responses. This initial improvement, however, is not without limits. Beyond a certain threshold, further increases in the number of documents lead to a decline in performance. This decline is primarily due to the influx of noisy or irrelevant data, which dilutes the quality of the retrieved information and impairs the model's ability to effectively filter out noise. Consequently, the model struggles to maintain high-performance levels as the proportion of valuable information decreases. Thus, optimizing the number of documents is crucial for balancing information and noise, which is essential for maximizing RAG performance."}, {"title": "Effectiveness of the Predictor", "content": "Based on the results presented in Table 3, our method demonstrates robust performance across a range of metrics, showcasing its capability to achieve high effectiveness while maintaining a lower average document count. Specifically, for the NQ dataset, our method achieves an Exact Match (EM) score of 40.13% and an F1 score of 70.96%. This performance not only surpasses that of the Top-4 method but also does so while keeping a lower average document count of 3.66, illustrating the efficiency and precision of our approach in handling the given data. In the TriviaQA dataset, although the Top-4 method slightly exceeds our approach in terms of EM and F1 scores, with figures of 48.15% and 79.9% respectively, our method still achieves commendable results. We attain an EM score of 47.15% and an F1 score of 79.40% with a reduced average document count of 3.23. This demonstrates a notable balance between performance and document efficiency, where our approach manages to deliver high-quality results with fewer documents. On the HotpotQA dataset, our method stands out by outperforming all other methods. We achieve an EM score of 26.36% and an F1 score of 60.46% while maintaining an exceptionally low average document count of just 2.13."}, {"title": "Performance of the Predictor", "content": "The confusion matrix for our predictor's performance is illustrated in Figure 4. Our approach classifies the compression rate into six distinct categories, providing a detailed view of how effectively our model differentiates between various levels of compression. The overall accuracy of our predictions is approximately 65%, indicating that AdaComp demonstrates a reasonably good predictive capability. Additionally, an analysis of the confusion matrix reveals that the absolute difference between the predicted labels and the true labels typically falls within a margin of 2. This finding suggests that our predictor is quite effective in estimating the required compression range. The relatively small discrepancy indicates that, while the model may not always predict the exact compression rate, it remains consistently close to the actual required values. This demonstrates the model's reliable performance in estimating the necessary level of document compression."}, {"title": "Case Study", "content": "As shown in Figure 5, AdaComp demonstrates the ability to retain a greater amount of relevant information when the quality of the retrieved text is suboptimal. This enhanced retention capability allows AdaComp to generate accurate answers despite the lower quality of the input text. In contrast, other compression methods struggle to produce correct responses under similar conditions. This performance discrepancy highlights AdaComp's robustness in handling less-than-ideal retrieval scenarios, ensuring that the quality of the generated answers is maintained even when the initial text quality is compromised."}, {"title": "Conclusion", "content": "This paper introduces a low-cost but effective context compression method, AdaComp, which adaptively determines the compression rate based on both query complexity and retrieval quality. When handling complex questions or low-quality retrieved documents, AdaComp retains more context to ensure the final performance of RAG. Conversely, when dealing with simpler questions or high-quality retrieved documents, AdaComp adaptively compresses the context to be both sufficient and concise, thereby enhancing RAG's compression efficiency. In future work, we will investigate whether the length of retrieved documents influences LLMs\u2019 ability to answer questions, with a focus on the impact of context length. Additionally, we will explore new methods to more finely distinguish situations where the required number of documents is close, aiming to improve the accuracy of the final predictor."}]}