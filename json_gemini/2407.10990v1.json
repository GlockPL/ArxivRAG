{"title": "MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models", "authors": ["Mianxin Liut", "Jinru Ding\u2020", "Jie Xu\u2020", "Weiguo Hu\u2020", "Xiaoyang Li", "Lifeng Zhu", "Zhian Bai", "Xiaoming Shi", "Benyou Wang", "Haitao Song", "Pengfei Liu", "Xiaofan Zhang", "Shanshan Wang", "Kang Li", "Haofen Wang", "Tong Ruan", "Xuanjing Huang", "Xin Sun", "Shaoting Zhang*"], "abstract": "Ensuring the general efficacy and goodness for human beings from medical large language models (LLM) before real-world deployment is crucial. However, a widely accepted and accessible evaluation process for medical LLM, especially in the Chinese context, remains to be established. In this work, we introduce \u201cMedBench\", a comprehensive, standardized, and reliable benchmarking system for Chinese medical LLM. First, MedBench assembles the currently largest evaluation dataset (300,901 questions) to cover 43 clinical specialties and performs multi-facet evaluation on medical LLM. Second, MedBench provides a standardized and fully automatic cloud-based evaluation infrastructure, with physical separations for question and ground truth. Third, MedBench implements dynamic evaluation mechanisms to prevent shortcut learning and answer remembering. Applying MedBench to popular general and medical LLMs, we observe unbiased, reproducible evaluation results largely aligning with medical professionals' perspectives. This study establishes a significant foundation for preparing the practical applications of Chinese medical LLMs. MedBench is publicly accessible at https://medbench.opencompass.org.cn.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) play an increasingly critical role across various fields and can potentially reform the healthcare sector. Medical Large Language Models (MLLMs) have thus emerged as a key area of focus [1, 2]. To ensure the reliability of MLLMs before they are deployed in real-world settings, a thorough evaluation is essential [3]. It is vital to establish a comprehensive, standardized, and reliable benchmarking system to assess the general efficacy of MLLMs. Despite this need, there is a notable absence of a universally recognized benchmarking framework, especially in the context of the Chinese context.\nThere has been a great endeavor to establish Chinese benchmarks for MLLM, such as National Medical Licensing Examination in China-Question Answering (MLEC-QA) [4], Chinese National Medical Licensing Examination (CMExam) [5], Chinese Biomedical Language Understanding Evaluation (CBLUE) [6], and Comprehensive Medical Benchmark in Chinese (CMB) [7]. However, existing benchmarks exhibit several limitations that hinder their suitability for evaluating MLLMs comprehensively. Firstly, existing benchmarks mainly focus on general clinical knowledge and often fall short in providing sufficient coverage across medical specialties, limiting their applicability to diverse healthcare sectors, as evidenced by the reported subpar performance of LLMs in specific fields like nephrology [8] and myopia care [9]. It highlights the necessity of benchmarks that encompass a broader spectrum of medical specialties. Secondly, while existing benchmarks contribute valuable datasets, they neglect the establishment of a standardized evaluation infrastructure. Currently, the benchmark procedure is often chosen and operated by the user, resulting in inconsistent evaluation results for the same dataset. This inconsistency underscores the necessity for not only comprehensive datasets but also a standardized, publicly accessible evaluation infrastructure. Thirdly, the reliability of existing benchmarks poses challenges The datasets, being entirely open and static, are susceptible to issues like shortcut learning [10] and answer leakage [11], which potentially inflates results artificially. In principle, a dynamic evaluation, with physical separation of question and ground truth, is expected to address this issue.\nIn response to these challenges, this work introduces MedBench, a pioneering benchmarking system tailored specifically for evaluating Chinese MLLMs. MedBench overcomes the limitations of previous benchmarks by providing comprehensive evaluations across 43 clinical specialties. The evaluated dimensions involve medical language understanding, generation, knowledge question answering, complex reasoning, and healthcare safety and ethics. MedBench employs a fully automated, cloud-based infrastructure to address the standardization and answer leakage issues. It enhances reliability through dynamic evaluation mechanisms, such as circular shuffling on choices and random prompt matching. The subsequent sections elaborate on the development and features of MedBench, highlighting its"}, {"title": "2 Method and Material", "content": "2.1 Benchmark dataset construction\nIn order to comprehensively assess the capabilities and performance of LLMs in medical field, we aim to measure five evaluation dimensions in MedBench: medical language understanding, medical language generation, medical knowledge question answering, complex medical reasoning, and healthcare safety and ethics."}, {"title": "2.2 Benchmark platform", "content": "We build a cloud-based fully automatic benchmark platform for public access to MedBench (Figure 3). With the platform, one can request the questions of the above-mentioned datasets, perform inference with large models locally, upload the answers to the platform, obtain evaluation metrics, and check the ranking. This workflow ensures that the user can not simultaneously access to both question and ground truth and thus prevents a majority of cheating. The question generation is based on a random and balanced sampling from each dataset.\n8,913 records are sampled to cover all extremely small-size datasets and select representative questions from large-size datasets. In practice, we further update the generated dataset every three months to increase the security of data and lower the risk of the ground truth leakage."}, {"title": "2.3 Dynamic evaluation mechanisms", "content": "Specifically, we additionally design two mechanisms to enable dynamic evaluation and thus to test the reliability of the results. First, a circular shuffle is applied to choices in choice questions. This is to avoid the tendency of LLM to choose answers in a preferred order, without an understanding of the contexts and"}, {"title": "2.4 Evaluation experiment", "content": "In the presented experiments, we test ChatGPT (GPT3.5, https://chat.openai.com/), PULSE [13], ChatGLM3 [14], BenTsao [15], and BianQue2 [16] using MedBench. We classify different types of tasks into four:"}, {"title": "3 Results and Discussion", "content": "3.1 Evaluation results from MedBench\nWe test different LLMs and MLLMs on MedBench and depict the averaged results over different datasets in Table 2. The dataset-specific results are offered in Table S2. ChatGPT obtains the best results in total scores and all dimensions except HSE. ChatGLM3 is especially good at HSE, supported by the top score among all tested models. BenTsao and BianQue2 yield lower scores in general, but BenTsao exhibits certain advantages in CMR. In addition, PULSE achieves the second-highest scores in all dimensions, supporting its balanced multi-facet ability.\nFor an intuitive understanding of the above quantifications, a qualitative comparison is further performed using one exemplified question. The responses from the five LLMs are briefly listed in Figure 4 (the whole-version answers and ground truth are shown in Table S3). The impression from this qualitative comparison is consistent with our quantitative comparison in Table 2. ChatGLM3 and ChatGPT understand and adhere to the QA instructions clearly, providing comprehensive answers that cover all relevant points. The responses are well-organized with a tidy format and a smooth logics. PULSE follows the QA instructions precisely, delivering complete answers, although its response format is slightly less organized. BianQue2 exhibits moderate attention to the instructions, but the responses are incomplete. The format is relatively organized. BenTsao demonstrates a general understanding of the examination instructions, but the responses are incomplete. Additionally, there is a substantial amount of irrelevant information in the answers. The format is relatively unstructured, resulting in a poor reading experience."}, {"title": "3.2 Results of human evaluation", "content": "Besides these intuitions from non-professional perspectives, the results of human evaluation with three clinical experts are presented in Table 3. As the selected questions cover MLG, MKQA, and CMR, we attach the averaged scores from MedBench within these dimensions as comparisons. We observe agreement of among the MedBench score and the scores given by the three clinical experts in general. ChatGPT and PULSE obtain first class evaluation results. ChatGLM3 is consistently placed in the middle class while BenTsao and BianQue2 are ranked in the following places. However, discrepancy between the ranks from human evaluation and from MedBench can be noticed. For Doctor 2, PULSE obtains higher scores than ChatGPT. Note that the other two doctors hold different views for the rankings between chatGPT and PULSE from Doctor 2. Regarding the results from Doctor 2 as an exception, the relative ranks for chatGPT and PULSE from MedBench can still match to the major opinions. In addition, the evaluation of Bianque2 is consistently higher than BenTsao in the human evaluation from all three doctors. This discrepancy indicates a limitation in the automatic evaluation process in MedBench and other existing benchmarking systems, which measures BLEU and ROUGH-L only. The reason behind this deviation, such as the reading habits of doctors,"}, {"title": "3.3 Dynamic evaluation mechanisms", "content": "In the following, we demonstrate the effectiveness of our implemented dynamic evaluation mechanisms. They aim to test LLMs under several situations that artificially inflate the scores and thus contribute to the reliability of the benchmarking."}, {"title": "3.3.1 Circular shuffling on choices", "content": "A typical shortcut learning of LLM is that LLM could choose the option at a referred order without understanding the contexts. We design a circular shuffling mechanism and the corresponding statistics to expose this possibility during the evaluation (see Methods). In Table 4, the scores from different LLMs using evaluation without and with circular shuffling are computed. The random prompt matching is deactivated during this experiment so that the two results can be comparable. It is found that ChatGLM3, BenTsao, and BianQue2 completely fail the evaluation with circular shuffling. And PULSE loses more than half of the original scores. This phenomenon suggests the responses from these LLMs are, to different extents, relying on random guessing options at a specific order. The ChatGPT is robust to circular shuffling and still ranks top. We emphasize this crucial feature in MedBench. When using other benchmarks without circular shuffling, one would give a remarkably inflated evaluation of these LLMs, being blind to the shortcut learning. It could be dangerous for such LLM to enter medical practices, as it is not truly mastering the medical knowledge and thus a not reliable for the medical applications."}, {"title": "3.3.2 Random prompt matching", "content": "The performance of LLM can vary along with different prompts in the questions, even being asked for the same knowledge. We thus also consider the reliability of LLMs on different prompts as one of the crucial aspects to be tested. Correspondingly, the random prompt matching mechanism is configured in MedBench. In this experiment, we activate the circular shuffling as it does affect the results. In Table 5, we recruit ChatGPT and BianQue2 and compare the results from them under two rounds of evaluation using different realizations of random prompt matching. We identify that ChatGPT is most sensitive to prompts in the medical language understanding evaluations and behaves most robustly in medical knowledge question answering. BianQue2 exhibits high variations in CMR, MLU, and MKQA.\nNote that this experiment is essentially a test-retest experiment, as the two LLMs undergo the full pipeline of MedBench twice. The implementation of different prompt matching does not change the ranking of the two exemplified LLMs among all LLMs (Table 2. With the new results, ChatGPT is still top-1 while BianQue2 is in the same rank as well). This supports the reproducibility of the evaluation based on MedBench."}, {"title": "3.4 Limitations and Perspectives", "content": "Despite the advanced features presented above, MedBench may still face certain limitations. Firstly, to establish a physical separation between the question and the ground-truth answer, MedBench sends questions to the client and processes the answers and ground truths in the cloud. However, this method does not completely eliminate the potential for cheating, as the model's answer generation remains unsupervised. An alternative solution could involve requiring an application programming interface (API) from the client, allowing MedBench to partially monitor the answer generation process. Secondly, when evaluating model responses to open-ended questions, we utilize conventional BLEU and ROUGE-L scores to facilitate a fully automatic pipeline, thereby limiting the framework. It is important to note that the efficient and appropriate evaluation of open-ended questions is still an unresolved issue with ongoing debates in the field [19]. To date, human evaluation remains widely accepted as the most effective method, although it can be subjective and labor-intensive. Concurrently, the development of semi-automatic or fully automatic methods using \"bag of words\"-based approaches, classical semantic similarity-based methods, and machine learning-based approaches is ongoing [20, 21]. We anticipate that applying these advancements from natural language processing to the next generation of MedBench will adequately address this critical issue. Thirdly, the current version of MedBench focuses exclusively on medical QA within the language domain, thereby overlooking the substantial needs for visual QA (VQA) [22] during clinical workflows, where questions are based on qualification and quantification of medical images. Similar challenges for the models can emerge when processing and understanding multi-omics data, such as protein and genetic data [23, 24, 25]. To thoroughly evaluate multimodal foundation models [26, 27, 28, 29] and large medical vision-language models [30,"}, {"title": "4 Conclusion", "content": "In the presented work, we develop MedBench to address a critical need for a proper benchmarking system to evaluate Chinese MLLMs. Our proposed MedBench, with its broad-spectrum evaluation using the largest dataset of 300,901 questions for 43 clinical specialties, emerges as a significant contribution to the field. By further introducing features like a cloud-based automatic pipeline with dynamic evaluation mechanisms, MedBench provides comprehensive, standardized, and reliable evaluations, aligned with medical professionals' perspectives, which overcomes the limitations of existing benchmarks. The open accessibility of MedBench further enhances its utility, without a risk of shortcut learning and answer leakage that causes metrics inflation. As the healthcare industry continues to advance, we expect MedBench to stand as a foundational tool, paving the way for the real-world applications of MLLMs and contributing to the ongoing evolution of language models in the medical domain."}]}