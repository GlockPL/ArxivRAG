{"title": "Multimodal Contextualized Support for Enhancing Video Retrieval System", "authors": ["Quoc-Bao Nguyen-Le", "Thanh-Huy Le-Nguyen"], "abstract": "Current video retrieval systems, especially those used in competitions, primarily focus on querying individual keyframes or images rather than encoding an entire clip or video segment. However, queries often describe an action or event over a series of frames, not a specific image. This results in insufficient information when analyzing a single frame, leading to less accurate query results. Moreover, extracting embeddings solely from images (keyframes) does not provide enough information for models to encode higher-level, more abstract insights inferred from the video. These models tend to only describe the objects present in the frame, lacking a deeper understanding. In this work, we propose a system that integrates the latest methodologies, introducing a novel pipeline that extracts multimodal data, and incorporate information from multiple frames within a video, enabling the model to abstract higher-level information that captures latent meanings, focusing on what can be inferred from the video clip, rather than just focusing on object detection in one single image.", "sections": [{"title": "Introduction", "content": "We leverage state-of-the-art (SOTA) vision-language models such as Nomic [8], Uform, and OpenClip [10] to extract keyframe embeddings and align them with text. Since most video retrieval queries are text-based, we transform the problem from querying with images to querying with text. To achieve this, we utilize lightweight yet robust language models (LLMs) like Phi3 (English), known for its strong OCR capabilities from Microsoft, and Vintern from 5CD (tailored specifically for Vietnamese). We also employ sentence chunking techniques and incorporate BGM [2] multilingual text embeddings for vector extraction.\nHowever, our key finding is that applying these methods frame by frame does not perform well when handling queries that describe a sequence of frames (a clip). These methods tend only to describe the objects within a single frame, failing to infer the meaningful, high-level information in the clip. To address this, we design a pipeline that utilizes audio context to enrich model phi35 with higher-level information over a longer sequence of frames (up to 20), making it more suitable for most queries. Other video-encoding methods (e.g., ViClip and VideoIntern [14]) are also incorporated into our system."}, {"title": "Related Work", "content": null}, {"title": "Our approach", "content": null}, {"title": "Image Deduplication with Dinov2", "content": "Even after extracting keyframes from a video, many frames can still be nearly identical. For instance, with news video, frames featuring reporters with the same background, attire, and posture often repeat between transitions in news videos. Similarly, introductory frames tend to reappear later, adding little value to the query process. This occurs because keyframe extraction algorithms typically compare consecutive frames locally, without considering long-term global similarities, leading to unnecessary duplication in the dataset. While classical deduplication methods like hashing (e.g., PHash, DHash [13]) exist, deep learning-based approaches enable more effective comparisons. Using average pooling with CNN models like ResNet [6] helps identify duplicates, but Dinov2 [9], a transformer-based model trained on unlabeled data using retrieval techniques proves the most effective for video retrieval tasks due to its ability to compare feature similarity vectors. However, deduplication is applied only within individual videos, not across the entire dataset. Given a dataset of N frames $X_i \\in \\mathbb{R}^{h\\times w\\times c}$ and K videos, we process each video $V_k$ by computing the cosine similarity between"}, {"title": "Vision-Language Alignment", "content": "We leverage Nomic's nomic-embed-vision [8], which is trained in a style similar to Locked Image Tuning (LiT), where a high-performing text embedder is frozen, and a vision encoder is fine-tuned from a pretrained checkpoint. In benchmark comparisons, Nomic Embed v1.5 is the only multimodal encoder to outperform OpenAI CLIP ViT B/16 and Jina CLIP v1 on both text and multimodal benchmarks. It leads in Imagenet Zero-Shot, Datacomp's 38 zero-shot multimodal evaluations, and MTEB for text embedding performance. In addition, we utilize Uform, a lightweight multimodal AI model capable of content understanding and generation across multilingual texts, images, and soon, video. Uform operates up to 5x faster than OpenAI's CLIP and LLaVA, making it ideal for scalable applications. We also include the baseline provided by most retrieval competitions, Clip-Base32 [3], but we still find that the two methods mentioned earlier still produce better results."}, {"title": "Text Description from LLMs", "content": "Since most queries are in text format, we shift our focus from image queries to text queries. Each image is paired with a descriptive text, allowing us to compare it against the query through feature extraction. We choose to employ"}, {"title": "Video-Level Representation", "content": "Since representation from a series of frames is important, we choose to use Vi-ClipB16, ViClipL14, and VideoIntern [14]. They are part of the InternVideo framework, a versatile video foundation model that excels by integrating generative and discriminative learning techniques, resulting in state-of-the-art performance across various video understanding tasks.\nThe framework utilizes both masked video modeling (generative) and contrastive video-language learning (discriminative), effectively merging these approaches to enhance video representation learning. InternVideo employs a masked video encoder with modules for local temporal and global spatiotemporal interactions. This architecture efficiently scales by reusing pre-trained Vision Transformers (ViTs) from image-text data, streamlining multimodal learning and enhancing performance."}, {"title": "High-Level Abstraction", "content": "While the model can detect and identify objects within frames, it struggles to infer higher-level abstractions, such as activities, actions, or emotions across individual frames and sequences. To address this limitation, we provide additional"}, {"title": "Our System Design", "content": null}, {"title": "Vector Database Search", "content": "We utilize Faiss [5] [7], developed by Facebook, to optimize the speed of vector searches with built-in methods such as IndexFlatL2 and IndexIVFFlat combined with product quantization. The score is calculated as follows:\n$S = \\Sigma M(t)[j] \\copyright IE[:, j]$"}, {"title": "Score Aggregation", "content": "For models querying images/frames (e.g., Uform, Nomic, OpenClip) and those querying through text descriptions and embeddings (e.g., Phi35, Vintern), the output is $S_i \\in \\mathbb{R}^N$, representing the confidence for each frame, $V_i \\in I$ for different models. We employ two aggregation methods:\n1. Summation of Confidences: We sum the confidence scores, meaning frames predicted accurately by multiple models will have higher scores. We then sort and select the indices of the top T frames:"}, {"title": "Result Display", "content": "Once we identify the top T potential results, we display the frames on the frontend with clear video categorization: frames from the same video are grouped in a single row and distinguished by a unique color label for easy identification. When users click on a frame, the adjacent 4 frames both before and after will be shown, along with the video playing at the exact moment the keyframe appears. This design allows users to verify results and select the correct answer quickly."}, {"title": "Supporting Techniques", "content": "Occasionally, queries may be less descriptive but mention specific locations (e.g., \"Bitexco\") or unique symbols (e.g., \"Dong Ho paintings\"). In such cases, the image search feature leverages URLs from the internet to retrieve the most similar images. This approach is effective if the descriptive text indicates a known image (location, distinctive features, etc.).\nHowever, our error analysis revealed limitations in language models when describing images. For example, when querying an image of a cycle rickshaw, models like Vintern [4], Phi3 [1], and Phi35 merely categorize it as a \"three-wheeled vehicle,\" making it difficult to retrieve frames from queries specifically referencing \"cycle rickshaw.\" To address this, we integrate modules for paraphrasing and translation using the GPT-40-mini API to simplify expressions into more accessible descriptions suitable for LLMs."}, {"title": "Conclusion and Discussion", "content": "In this work, we addressed the limitations of existing video retrieval systems that primarily focus on individual frames, which often fail to capture the higher-level insights present in video clips. Introducing a novel pipeline that utilizes multimodal data from sequences of frames enabled more accurate and abstract interpretation of video content. Our system integrates state-of-the-art vision-language models and incorporates audio context to provide models with a broader understanding of actions and events across multiple frames, improving retrieval performance for complex queries."}, {"title": null, "content": "The system is implemented with a Python Flask API backend and a frontend built using HTML and JavaScript. While this provides a functional interface, there are still limitations in terms of thoroughly browsing and interacting with results. In the future, we aim to enhance the user interface and experience, focusing on making the retrieval process more intuitive and efficient for users. This will allow for a more seamless exploration of the retrieved video segments and further improve the overall system usability."}]}