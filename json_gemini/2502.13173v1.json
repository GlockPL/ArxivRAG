{"title": "Thinking Preference Optimization", "authors": ["Wang Yang", "Hongye Jin", "Jingfeng Yang", "Vipin Chaudhary", "Xiaotian Han"], "abstract": "Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs 1. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%. Our code is available at https://github.com/uservan/ThinkPO.", "sections": [{"title": "1 Introduction", "content": "The reasoning capability of LLMs is crucial for their applicability in complex problem-solving tasks. Improving the reasoning ability of large language models is one of the current research hotspots. Many approaches have emerged in the open-source community that enhance relatively small models' reasoning ability through SFT. For example, Sky-Thought (Schulman et al., 2017), Bespoke-Stratos (Labs, 2025) and OpenThinker-7B(Team, 2025b) have built long reasoning datasets to fine-tune models fully, aiming to improve model reasoning capabilities. Further advancements can be seen in models like s1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025), which focus on the sophisticated design of long reasoning datasets to enhance reasoning capabilities.\nDespite the success of supervised fine-tuning, continually improving the reasoning abilities of the STF-ed model faces the following challenges: (1)"}, {"title": "2 Thinking Preference Optimization", "content": "2.1 Motivations\nThis section introduces the motivations behind Thinking Prference Optimization. SFT with fixed long-reasoning datasets is an effective method for enhancing a model's reasoning ability. However, further improvement of the model's reasoning ability during the later stages faces a bottleneck. In such cases, by using short reasoning data as rejected samples and long reasoning texts from SFT as chosen samples for DPO training, it is possible to further leverage the high-quality SFT reasoning"}, {"title": "2.2 Training Pipeline", "content": "The training process in Thinking Preference Optimization consists of two stages: Reasoning SFT (Supervised Fine-Tuning) stage and Reasoning DPO (Direct Preference Optimization) stage. In the Reasoning SFT stage, long-reasoning responses are collected for each question to construct the dataset Ds ft. The base model is then fine-tuned on Dsft to acquire advanced reasoning capabilities, which helps to prepare the model for next stage.\nIn the second stage, the model is further en-couraged to generate extended reasoning using the Direct Preference Optimization (DPO) (Rafailov et al., 2024) approach. First, the long-reasoning data from the initial stage is used as the chosen responses. Then, a smaller model with normal Reasoning ability, such as Qwen-2.5-7B-Math (Yang"}, {"title": "2.3 Data Curation", "content": "The dataset Dsft = {(q, Olong)}N is based on bespoke stratos dataset (Labs, 2025). They used DeepSeek-R1 as the teacher reasoning model instead of QwQ-32B-Preview to generate long reasoning response olong and employed GPT-40-mini in place of Sky-thought T1's (Team, 2025a) parsing logic to filter out incorrect mathematical solutions.\nFor the dataset Ddpo = {(q, Olong, Oshort)}N in the second stage, we collect it in the following manner, referring to (Team et al., 2025): For each question q in Dsft, we use Qwen2.5-Math-7B-Instruct (Yang et al., 2024b) to generate a short reasoning response Oshort, pairing it with the long reasoning response Olong in Dsft. We then retain the samples where Qwen2.5-Math-7B-Instruct's answer matched DeepSeek R1's answer, resulting in 8,080 samples. Additionally, we include 2,000 samples where Qwen2.5-Math-7B-Instruct's answer differed from DeepSeek R1's but adhered to the correct response format, including more output distribution in Ddpo. All of these combined samples consequently form the final dataset Ddpo. The dataset is collected through a straight foreword"}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nTo evaluate model's reasoning ability, we select five different test sets: MATH500 (Lightman et al., 2023), AIME2024 2, GPQA-Diamond (Rein et al., 2023), GSM8K (Cobbe et al., 2021), and Olympiad Bench Math (He et al., 2024). These test sets primarily consist of mathematical reasoning problems, with GPQA-Diamond also including problems from physics, chemistry, and biology. The difficulty levels of these test sets vary significantly, with GSM8K being the easiest while AIME2024 is the most challenging. This diverse selection ensures a comprehensive assessment of the model's reasoning capability across different levels of difficulty, from fundamental arithmetic to complex problem-solving with different difficulty.\nWhen generating responses, we set the temperature as 0.7. For results on other temperatures, please refer to Appendix A.1. We present our chosen hyper-parameters of ThinkPO, such as learning rate, batch size and 3, in Appendix A.3."}, {"title": "3.2 Effectiveness of ThinkPO", "content": "This experiment primarily analyzes the average response length, accuracy and reasoning-supportive words count during both SFT and DPO processes to validate the effectiveness of Thinking Preference Optimization (ThinkPO). By tracking these metrics, we aim to demonstrate how ThinkPO enhances the model's reasoning ability by encouraging longer, more structured outputs, ultimately"}, {"title": "3.3 ThinkPO can Continually Improve Reasoning Ability of Public Distilled Models", "content": "We select two open-source reasoning models and perform ThinkPO training using Ddpo. Specifically, we chose DeepSeek-R1-Distill-Qwen-7B and Bespoke-Stratos-7B, since both reasoning models were fine-tuned on Qwen2.5-7B-Instruct.\nAs shown in Table 2 and Figure 4, both models demonstrate an improvement in accuracy across five datasets. For example, Bespoke-Stratos-7B"}, {"title": "3.4 ThinkPO Works for Different-Size Models", "content": "Previous experiments are all conducted using a 7B model for training. Now we utilize the Bespoke Stratos dataset and conduct one epoch of SFT training on models of varying sizes within the Qwen2.5 series (Qwen2.5-3B, Qwen2.5-7B, and Qwen2.5-14B). The learning rate is set to 3e-5, and other hyperparameters are kept consistent with Bespoke-Stratos-7B, ensuring the models' performances. The results after SFT and ThinkPO are"}, {"title": "4 Ablation", "content": "4.1 Whether ThinkPO is Useful when SFT with Short Reasoning Data?\nIn our previous experiments, we fully fine-tuned the model using long reasoning datasets before applying ThinkPO to further enhance its reasoning ability. However, an important question arises: If we use short reasoning data instead of long reasoning data during the full fine-tuning stage, can Thinking Preference Optimization still improve the model's reasoning performance effectively?\nTo investigate this, we conduct the following ex-"}, {"title": "4.2 Exploring the Impact of Length Differences between Chosen and Rejected Samples on ThinkPO.", "content": "In the entire ThinkPO dataset, we select long reasoning data as chosen and short reasoning data as rejected. A key question is whether the length disparity between chosen and rejected samples affects the ThinkPO training because length disparity is not distributed evenly in the dataset. To investigate this, we conduct an experiment to verify the impact of length differences on the ThinkPO training."}, {"title": "5 Related Works", "content": "LLM Reasoning Ability. With the development of large models, reasoning ability (Wang et al., 2022; Zhang et al., 2023; Yao et al., 2023; Plaat et al., 2024) has become one of the most crucial capabilities and a necessary condition for achieving AGI (Artificial General Intelligence) (Minaee et al., 2024; Xu et al., 2024; Morris et al., 2023; Feng et al., 2024; Krishnan, 2025). The earliest appearance of long-chain reasoning ability in large models can be traced to OpenAI 01 (Jaech et al., 2024; Arrieta et al., 2025; Hurst et al., 2024), which excelled across various mathematical reasoning test sets and outperform contemporary LLMs.\nThis was followed by the release of the QwQ model (Yang et al., 2024b; Bai et al., 2023a,b; Chu et al., 2024), which trained reasoning capabilities using a process reward model approach (Li and Li, 2024; Ma et al., 2023; Zhang et al., 2025; Lambert et al., 2024). Currently, the emergence of DeepSeek R1 (DeepSeek-AI et al., 2025) and Kimi 1.5 (Team et al., 2025) has further enhanced the reasoning abilities of large open-source models. DeepSeek R1 utilizes a simple rule-based reward model (Ramesh et al., 2024; Hu, 2025; Shao et al., 2024; Alonso et al., 2025; Kirk et al., 2023; Yang et al., 2024a) to effectively boost the model's reasoning performance, bringing about an aha moment that narrows the reasoning capability gap between open-source and closed-source models. On the other hand, Kimi 1.5 employs several tricks, such as long-to-short reasoning, to achieve high efficiency in LLM reasoning performance.\nMany works on open-source reasoning models have also emerged. First is Sky-Thought T1 (Team, 2025a), which uses QwQ-32B-Preview as a teacher model to generate reasoning answers for training data. Then, Bespoke-Stratos (Labs, 2025) built upon Sky-Thought T1, using DeepSeek R1 as the teacher model to generate answers for Sky-Thought data. Since DeepSeek R1 has far superior reasoning abilities compared to QwQ-32B-Preview, the generated data quality is higher, allowing Bespoke-Stratos-7B and Bespoke-Stratos-32B models to achieve DeepSeek-level advanced reasoning performance after training on around 17k data points. Recently, s1 (Muennighoff et al., 2025) and LIMO"}, {"title": "Direct Preference Optimization.", "content": "RLHF (Chaudhari et al., 2024; Kirk et al., 2023; Kaufmann et al., 2023) is designed to align model outputs with human preferences after supervised fine-tuning (SFT). Various methods have been introduced, such as Proximal Policy Optimization (PPO) (Engstrom et al., 2019; Huang et al., 2022; Wijmans et al., 2019). However, PPO is an online method that requires significant computational resources. To address this, Direct Preference Optimization was proposed, enabling offline training with only chosen and rejected sample pairs while reducing computational costs compared to PPO. Recently, several DPO variants (Wu et al., 2024b,a; Qi et al., 2024; Zhong et al., 2024; Su et al., 2025) have emerged, including StepDPO (Lai et al., 2024), KTO (Ethayarajh et al., 2024), SimPO (Meng et al., 2024), LongDPO (Ping et al., 2025), Test-Time Preference Optimization (Li et al., 2025) etc. Among them, LongDPO shares similarities with our proposed method. However, LongDPO primarily focuses on improving long-form story generation instead of reasoning abilities."}, {"title": "6 Conclusion", "content": "We introduce Thinking Preference Optimization, a simple yet effective post-SFT method without the need for additional high-quality long-reasoning data. By leveraging short reasoning responses as rejected and long reasoning responses as chosen, ThinkPO encourages models to generate detailed reasoning outputs, effectively maximizing the utility of existing long-reasoning data. Our experiments demonstrate that ThinkPO significantly improves model performance, yielding an 8.6% accuracy boost and a 25.9% increase in output length for SFT-ed models. Additionally, ThinkPO enhances the publicly available DeepSeek-R1-Distill-Qwen-7B model, raising its accuracy on the MATH500 dataset from 87.4% to 91.2%. These results underscore that ThinkPO provides a lightweight solution that improves reasoning capabilities without high resources and ThinkPO 's ability to overcome performance bottlenecks in multi-epoch SFT with fixed and limited high-quality long-reasoning data."}, {"title": "Limitations", "content": "ThinkPO can further enhance SFT-ed models without requiring additional high-quality long reasoning data. However, since ThinkPO is based on the DPO method, it is sensitive to hyperparameters, requiring careful tuning of \u03b2 and learning rate to achieve optimal improvements."}, {"title": "A Appendix", "content": "A.1 Evaluating ThinkPO with Different Temperatures\nIn our experiments, we initially evaluated the model at a temperature of 0.7. While this provides a good measure of performance, it is important to explore different sampling conditions for a more robust analysis. Therefore, we additionally tested temperatures of 0.1 and 0.5 to examine how ThinkPO impacts Bespoke-Strato-7B under varying levels of randomness in sampling. By comparing results across these temperature settings, we can assess whether ThinkPO consistently enhances the model's reasoning ability regardless of generation strategy. To provide a comprehensive evaluation, we average the results across all three temperatures. The results are shown in Table 6.\nOur findings demonstrate that ThinkPO consistently improves model performance across different temperature settings. Specifically, at temperatures of 0.1 and 0.7, accuracy increases on four datasets, while at 0.5, improvements are observed on three. To gain a more holistic understanding of ThinkPO's impact, we average the results across all temperature settings, showing that ThinkPO enhances performance on all five datasets. Notably, on MATH500, ThinkPO improves accuracy by 1.4%. These results further validate the effectiveness of our proposed method and demonstrate its ability to consistently enhance reasoning performance across different sampling conditions.\nA.2 Analysis of our Reproduce Model in other datasets\nPreviously, we only presented the changes in accuracy, average response length, and reasoning-supportive words count over training steps on the MATH500 dataset. Here, we extend our analysis by showcasing results on two additional datasets (like GSM8K) from our reproduced model. The detailed results are illustrated in Figure 9.\nAs observed in the results for GSM8K and Olympiad Bench Math, the model exhibits a similar trend to MATH500 across all three metrics. During the early stages of SFT, the model's reasoning ability improves rapidly. However, in later stages, it reaches a performance plateau. ThinkPO effectively helps the model overcome this bottleneck, further enhancing its reasoning capability.\nA.3 Training Recipe\nHere, we provide the corresponding hyperparam-eters-batch size, learning rate, and \u03b2-that were used to achieve these optimal outcomes. All the hyperparameters are presented in Table 7.\nBesides, we present the training loss curves, gradient norm curves, and margin curves for three models during the ThinkPO phase in Figure 5. These metrics provide insights into how the models perform throughout the training process, including their convergence behavior, stability of gradients, and the differences in preference between chosen and rejected samples. By examining these curves, we can better understand the effectiveness of ThinkPO in enhancing model performance.\nA.4 Examples of LLM's outputs before and after ThinkPO\nWe present the changes in the total number of reasoning-supportive words (such as wait, hmm, let's think, etc.) throughout both the SFT and ThinkPO training stages in Figure 2 and Figure 9. These words serve as indicators of the model's reasoning process, reflecting its ability to structure logical steps before arriving at a final answer. Our results show that the number of reasoning-supportive words increases significantly during the initial stages of SFT but eventually plateaus, suggesting that conventional fine-tuning alone may not be sufficient to further enhance structured reasoning. However, after applying ThinkPO, we observe a clear upward trend in the use of these reasoning-supportive expressions, indicating that our method effectively encourages the model to adopt a more deliberative reasoning process.\nWe provide examples of model outputs before and after applying ThinkPO in Table 8 and Table 9. Before ThinkPO, the model's responses tend to be more direct, with fewer reasoning-supportive words, often resulting in incorrect or incomplete answers. In contrast, after applying ThinkPO, the model generates responses that utilize a greater number of reasoning-supportive words. This shift leads to a noticeable improvement in answer correctness, reinforcing the effectiveness of ThinkPO in enhancing the model's reasoning ability. These findings highlight that ThinkPO not only improves accuracy but also aligns the model's output with human-like problem-solving patterns."}, {"title": "Question What is the domain of the function f(x) =", "content": "SFT\nOkay, so I need to find the domain of the function f(x) = (2-x)/log(2 - log(x - 2)). hmm, let's start by recalling that the domain of a function includes all the real numbers x for which the function is defined. Since this function has a logarithm in the denominator, there are a couple of things I need to check: first, the argument of the outer logarithm (the one in the denominator) must be positive because logarithms are only defined for positive numbers. Second, the argument of the inner logarithm (the one inside the denominator's argument) must also be positive because we can't take the logarithm of a non-positive number. Let me break this down step by step.... But wait, 102 is already the upper bound, so actually, the domain is (2, 102). But we have to make sure that within (2, 102), the expression 2 - log(x - 2) is positive, which we already considered. Let me verify this again. So, starting with the inner logarithm: x > 2. Then, log(x - 2) is defined. Then, the outer logarithm requires that 2 - log(x - 2) > 0, which as we saw, leads to x < 102. So combining x > 2 and x < 102 gives (2, 102). Additionally, we need to ensure that the denominator is not zero, which occurs at x = 102, but since 102 is already the upper limit, the interval is open at 102. Therefore, the domain is (2, 102). Let me check if there are any other restrictions or if I missed something. wait, another thing to consider: the numerator is 2 - x. The numerator is a linear function, so it's defined for all real numbers. The denominator is log(2 - log(x - 2)), ... Then, log(3 - 2) = log(1) = 0. Then, 2 - 0 = 2, so log(2) is the denominator. That's valid. If x = 100, then log(100 - 2) = log(98), which is positive, so 2 - log(98) would be 2 - log(98), which is positive because log(98) is less than 2 (since 102 = 100). So, denominator is positive. If x approaches 2 from the right, x approaches 2+, so x - 2 approaches 0+, so log(x - 2) approaches -infty, so 2 - log(x - 2) approaches +infty, so denominator is defined. If x approaches 102 from the left, x approaches 102-, so x - 2 approaches 100-, so log(x - 2) approaches log(100-), which is close to 2, so 2 - log(x - 2) approaches 0 from the positive side, so the denominator approaches 0+, making the whole function approach +infty. But x can't be 102. So, in all other points between 2 and 102, the denominator is defined and positive, so the function is defined there. Therefore, the domain is indeed (2, 102). I think that's it. Final Answer The domain of the function is (2, 102)."}]}