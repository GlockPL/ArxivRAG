{"title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models", "authors": ["Zhixiang Guo", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "abstract": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in first-attack epoch (FAE) and an average decrease of 51.4% in copyright infringement rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11, 52, 53, 65, 75]. These models emulate the diffusion process observed in non-equilibrium thermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46, 49, 54].\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57, 61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Replication Phenomena of Diffusion Models", "content": "As research on diffusion models advances, increasing attention is being focused on their memory capabilities and replication phenomena [3, 4, 44, 67, 69]. Zhang's study [72] found that diffusion models, regardless of differences in architecture and training procedures, tend to learn similar data distributions, which is termed \u201cconsistency model replication.\u201d It suggests that diffusion models might capture specific distributions of the training data rather than generate entirely novel content. Furthermore, Carlini [2] demonstrated that diffusion models can memorize individual images from the training data during generation and regenerate these images during testing when given specific prompts. These findings suggest that diffusion models may pose greater privacy risks [5, 8, 9, 13, 26] than earlier generative models, such as GANs [10]. Additionally, research by Somepalli [58] indicates that diffusion models may directly replicate content from their training sets during image generation, often without the user's awareness, raising concerns about data ownership and copyright."}, {"title": "2.2. Copyright Infringement in Diffusion Models", "content": "Copyright infringement refers to the unauthorized use of protected materials without the consent of the copyright holder. In diffusion models, copyright infringement often occurs when the model, during training, is exposed to copyrighted data and subsequently generates substantially similar samples. Existing methods to prevent copyright infringement primarily focus on the management and protection of training data.\nVyas [61] proposed the Near Access-Free model to quantify and restrict a generative model's access to samples, thereby reducing access to copyrighted data and mitigating copyright infringement. Cui [7] introduced a watermarking technique called DiffusionShield, which embeds invisible watermarks in images to ensure that ownership information can be detected in generated images, thus protecting data copyright from infringement by generative diffusion models (GDMs). However, Silentbaddiffusion [62], a method of copyright infringement backdoor attack, increases the stealthiness of poisoning by generating poisoned images with infringing features rather than directly using copyrighted images. This significantly raises the difficulty"}, {"title": "2.3. Diffusion Model Backdoor Defense", "content": "Besides test-time adversarial attacks [19, 23-25, 27, 29-35, 38, 40-43, 64, 68, 71, 73], diffusion models have demonstrated vulnerability to backdoor attacks, where attackers can manipulate the model to produce malicious outputs by injecting specific trigger patterns into the training samples. This poses a significant threat to the security and reliability of these models. Consequently, an increasing amount of research is being conducted on defending diffusion models against backdoor attacks [6, 16, 20, 56].\nThe UFID [12] framework constructs an input-level backdoor attack detection system by leveraging the robustness of clean and poisoned generations, offering black-box characteristics and strong detection performance. T2IShield [66] introduces the concept of the assimilation phenomenon in diffusion models and proposes two backdoor detection methods: Frobenius Norm Threshold Truncation (FTT) and Covariance Discriminant Analysis (CDA). Oscar Chew [6] employ a simple defense strategy, text perturbation, which embeds slight perturbations to disrupt specific trigger mechanisms, thereby achieving a defensive effect. The TERD [45] framework establishes a unified form of existing attacks, proposing an accessible reverse loss function and designing a two-stage trigger inversion algorithm. Additionally, TERD offers a probability-based input detection method and a KL-divergence-based model detection method to protect diffusion models from backdoor attacks. In comparison, copyright infringement backdoor attacks emphasize feature-level learning rather than distributional learning, making existing backdoor defense methods [37, 63, 70, 74] less effective against feature-level backdoor injections. In this paper, we propose for the first time a defense method specifically targeting copyright infringement backdoor attacks."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Diffusion Models", "content": "Taking the most commonly used text-to-image diffusion model, Stable Diffusion, as an example, it typically comprises three components: (1) Text encoder $T_o$: encodes the input text and projects it into the semantic space $y \\in V$, represented as $y = T_o (t)$. (2) Image encoder: consists of two components: the Autoencoder and the U-Net. To reduce computational complexity, all images are projected into the latent space [52] using an Autoencoder, represented as $x = E_{Auto} (I) \\in X$. By applying the U-Net ($\\epsilon$) in Denoising Diffusion Probabilistic Model (DDPM) [15], the model gains the capability for conditional generation through a training process that denoises random noise in the latent space, represented as $x_t = E_o (z_t, y,t)$, which $z_t$ denotes the noisy latent embeddings in t-th time step, y denotes text embeddings. The objective of $\\epsilon_\\theta$ is to obtain denoised feature maps that approximate the true probability distribution as closely as possible, represented as $L = E_{(x,y) \\sim D_{train}} [||x - x_t||^2]$. (3) Image decoder $D_{auto}$: projects the features from the latent space back into the real image space, represented as $I_t = D_{auto}(x_t)$."}, {"title": "3.2. Threat Model", "content": "We assume that an attacker has trained a backdoor model for copyright infringement, which, when given a specific text input, will trigger the backdoor to generate infringing images. When a copyright attack is triggered, there exists a trigger text composed of multiple prompts $t_{poison} : {P_1,P_2,...,P_n}$ and an infringing image $I_{poison} := DM_p(t_{poison})$, which $DM_p$ denotes as the poisoned diffusion model. Defenders need to design an efficient infringement detection system, develop strategies to mitigate the impact, and establish a comprehensive method for providing effective evidence for responsibility attribution. Therefore, defenders need to have access to the poisoned dataset $d_p$. During an attack, in order for the poisoned samples to remain sufficiently covert, the similarity of the clean data and poisoned data should follow $sim(d_c, d_p) < \\tau$. The goal of the defenders is to design a capable detector $D$ which can distinguish between $d_c$ and$d_p$, as depicted in Eq. (1)::\n$\\begin{cases}\nD(d_p)\\\\\nD(d_c) < \\gamma\n\\end{cases}$                                                          (1)\nwhich $\\gamma$ denotes the detection threshold. If the probability density distributions of the model $DM_p$ under clean and poisoned inputs are $P_d$ and $P_c$, respectively, then it is necessary to optimize Eq. (2):\n$\\gamma^* = arg \\underset{\\gamma}{max} D_{KL} (P(x; \\gamma) || Q(x; \\gamma))$                                                  (2)\nIn the defense process, it is necessary to optimize the diffusion model in order to suppress the generation of poisoned samples without compromising its normal generative performance, which can be depicted as $sim(DM'(t_{poison}),d_c) < \\tau \\underset{>}{sim}(DM'(t_{clean}), d_c) > T$. Based on the aforementioned considerations, our detection and defense methods will be designed according to these conditions."}, {"title": "4. Approach", "content": ""}, {"title": "4.1. Defense Motivation", "content": "The replication in diffusion models is typically represented by the similarity of some or all pixels in an image to the generated samples, as commonly illustrated in Fig. 1, which the"}, {"title": "4.2. CopyrightShield", "content": "As depicted in Fig. 2, the CopyrightShield defense method comprises two main components: poisoning data detection based on spatial guidance and optimization training with protection constraints."}, {"title": "4.2.1. Poisoning Data Detection", "content": "Initially, the samples are subjected to detection and segmentation based on their corresponding captions. This enables the accurate identification and extraction of visual features from complex scenes. To accomplish this, we utilize advanced detection and segmentation models, such as GroundingDINO [39] and Segment Anything Model [18], to obtain segmentation results associated with each caption. These results are represented as $Img[feature_1, feature_2, . . ., feature_n]$. The detected features are compared with the protected image features to compute the poisoning score. However, a mere comparison of the pixel semantics of the images proves to be an ineffective approach, as the SSCD of poisoning data falls below 0.5. To overcome this limitation, we introduce a poisoning score that integrates the joint spatial characteristics, as depicted in Eq. (5):\n$Poison score = \\begin{cases}\nIoU * SSCD                                  & IoU > 0.3\\\\\n0                                     & IoU < 0.3\n\\end{cases}$                                                                            (5)\nThe IoU metric is employed to measure the degree of overlap between features and targets. Given that the same caption can manifest different features on an image, comparing IoU allows for the evaluation of spatial similarity between features and targets, thereby enabling the precise extraction of poisoned features. Additionally, SSCD serves as a widely adopted metric for assessing image replication, providing a more intuitive measure of the diffusion phenomenon within the model. By jointly evaluating these"}, {"title": "4.2.2. Defense by optimization training", "content": "In the original diffusion model training process, the loss function is defined as follows:\n$L_o = E_{t,z_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta (z_t, y, t) ||^2]$                                                                 (6)\nwhere t represents the time step, typically sampled from a uniform distribution, $z_0$ represents the data sample and y represents the conditional captions. By minimizing this loss, the model learns to accurately predict the noise at each time step, thereby enabling effective denoising and reconstruction of the original data during the reverse diffusion process. However, as mentioned in Section 4.1, the cross-attention module in $\\epsilon$ enables a strong association between the image and the caption, thereby enhancing spatial similarity in model replication. To mitigate this effect, we introduced a protective constraint term. The optimized loss function is as follows:\n$L = L_o + \\lambda * \\sum_{i=1}^{N}(L_{mse} (\\epsilon_\\theta (z_t, y, t), \\epsilon(z_a, t)))$                                                           (7)\nwhere, N represents the number of detected poisoned samples, while $\\lambda$ is used to balance the effect of the protection constraint term and the original loss $L_o$ and $\\epsilon(z_a, t)$ represents the detected images transformed into latent space.\nIn order to mitigate the impact of poisoning data in backdoor attacks, a common approach is to optimize in a direction that diverges from the similarity of poisoning data."}, {"title": "5. Experiments", "content": "We outline our experiment, beginning with the setup details in Section 5.1. In Section 5.2, we evaluate the main results of our method, including poisoning data detection results in Section 5.2.1, defense results against copyright infringement in Section 5.2.2, defense results against different diffusion model in Section 5.2.3, the infringement feature inversion in Section 5.2.4. More details of the results can be found in Supplementary Materials."}, {"title": "5.1. Experimental Setup", "content": "Defense Scenario: We conducted a study on the application of stable diffusion with a dataset that lack of copyright protections. In this particular scenario, attackers introduce poisoned samples into the model, leading to the generation of copyrighted artworks when the model is utilized by users and triggered by specific inputs. The primary goal of the defense strategy is to accurately identify and filter out these samples, subsequently fine-tuning a secure model to prevent any triggering. Additionally, we propose the concept of copyright feature inversion, which aids in the allocation of copyright responsibility.\nDataset and Model: In order to implement the defense scenario described, we utilized the Pokemon BLIP Captions dataset produced by Pinkney [47], to simulate the defensive context. This choice was motivated by the fact that the Pokemon dataset provides a compelling example for the identification and understanding of copyright infringement, and recent copyright infringement cases [60] related to Pokemon have garnered significant public attention, offering a relevant backdrop for our defense strategy. Moreover, this dataset aligns well with the requirements of stable diffusion for text-to-image fine-tuning, enhancing the reproducibility of our approach. To ensure the robustness and reliability of our methods, we conducted 20 independent experiments for this scenario. In each experiment, one image is selected as the copyright-infringing image to generate the poisoned data, while the remaining 832 images are used as clean data.\nIn the experiments, GroundingDino [39] and SAM [18] were employed as the detection and segmentation models for the poisoned sample detection method. Given that the current backdoor attack method targeting copyright infringement is limited to SilentBadDiffusion [62], our defense strategy focused solely on detecting this method. Therefore, for stable diffusion, we adopted the v1.x version same as SilentBadDiffusion.\nEvaluation Metrics: For the assessment of copyright infringement, we employed SSCD as the evaluation metric. Regarding the detection of poisoned samples, we utilized Recall, Precision, and F1-Score as the evaluation metrics. In order to assess the defense, we employed the Copyright Infringement Rate (CIR) and First Attack Epoch (FAE) as measures. CIR represents the probability of generating copyright infringement samples using the poisoned model, $CIR(DM, I) = P(SSCD(DM(t_{poison}), I) > 0.5)$. FAE, on the other hand, represents the number of epochs required for the model to complete the attack for the first time during the training process, $FAE(DM) = min \\{epoch | SSCD(DM(t_{poison}), I) > 0.5, epoch\\}$. For the computation of CIR, we iteratively generate 100 images for each poisoned caption and count the number of instances where SSCD > 0.5, then used to calculate the probability.\nImplementation Details: To determine the optimal threshold for the poison-score, we conducted experiments with values of 0.2, 0.25, and 0.3. After evaluating precision, recall, and F1 score, we selected 0.25 as the most suitable threshold. For the parameter $\\lambda$ in Eq. (7), we tested values of 0.05, 0.10, and 0.15, ultimately selecting 0.05 based on the experimental results. During the defense fine-tuning process, we utilized the Adam optimization algorithm with a learning rate of le-5, a batch size of 8, and trained for 100"}, {"title": "5.2. Main Results", "content": ""}, {"title": "5.2.1. Poisoning Data Detection", "content": "We initially evaluated the accuracy of poisoned sample detection, where the poisoned samples were generated using the SilentBadDiffusion method. These samples were mixed with clean samples at poisoning rates of 0.05, 0.1, and 0.15. The detection results are presented in Table 1.\nThe results demonstrate variations in precision, recall, and F1-score across different thresholds under varying poisoning rates. As shown in the table, higher thresholds result in fewer false positives, albeit at the cost of reduced precision. Conversely, lower thresholds enhance accuracy but significantly increase the number of false positives. Consequently, a threshold of 0.25 is selected as the optimal balance, as it results in an F1 score that is 2.16% higher compared to a threshold of 0.2 and 4.42% higher compared to a threshold of 0.3, which is considered as the most appropriate. Furthermore, the experiments indicate that the poisoning rate has a minimal effect on detection accuracy, with the results that the variation in the metric is minimal in every threshold. This finding suggests that the poisoning sample detection method, based on spatial similarity, effectively exploits this characteristic of the diffusion model. Despite fluctuations in poisoning rates, the model consistently maintains detectable spatial similarity. Besides, we record the inference time required for evaluating a single sample against all captions using an RTX 4090 24GB GPU. The processing time for one image is approximately 5.7 seconds, ensuring efficient detection."}, {"title": "5.2.2. Copyright Infringement Defense", "content": "Following the detection of poisoned samples, we performed defense experiments utilizing these identified samples. The experiments applied a poisoning score threshold of 0.25, as established in the previous section. Notably, if a successful attack is not achieved within 100 epochs, indicated by an SSCD greater than 0.5, the FAE is set to 100. The best-performing model within these 100 epochs is then selected for CIR calculation. Table 2 illustrates the variations in CIR and FAE across different poisoning rates. Under three different poisoning rates, CopyrightShield decreased the CIR"}, {"title": "5.2.3. Defense against different diffusion model", "content": "The strong correlation between SilentBadDiffusion attacks and the replication capability of diffusion models suggests that more advanced models are more susceptible to copyright attacks [22]. Therefore, understanding the impact of model performance on defense effectiveness is equally crucial. Since the defense process requires optimization in the direction opposite to that of the poisoned images, a model with greater learning capacity converges more easily. Thus, we hypothesize that the effectiveness of the defense is positively correlated with the model's capability. Experiments were conducted at a poisoning rate of 0.1 to compare the defensive capabilities of stable diffusion models from versions 1.1 to 1.5. Table 3 reveals that across different versions of the diffusion model, the CIR reduction rate is 40.33%, 43.80%, 50.42%, 51.08% and 50.96%, from v1.1 to v1.5 respectively, while the FAE increase rate is 51.14%, 53.13%, 65.12%, 67.76%, 64.04%. The results indicate that the defensive effectiveness of Stable Diffusion v1.1 and v1.2 is relatively similar, while there is a significant difference compared to versions v1.3 to v1.5. Among v1.3 to v1.5, v1.4 demonstrates the best defensive performance. This finding is consistent with the results observed in SilentBadDiffusion, suggesting that the model parameters of version v1.5 are less extensive than those of v1.4, leading to reduced complexity. Therefore, we can conclude that the stronger the model's capabilities, accompanied by enhanced convergence ability, the greater its defensive effectiveness. More details can be found in Supplementary Materials."}, {"title": "5.2.4. Infringement Feature Inversion", "content": "\"To constitute an infringement under the Act there must be substantial similarity between the infringing work and the work copyrighted; and that similarity must have been caused by the defendant's having copied the copyright holder's creation.\u201d \u2014 U.S. 9th Circuit Opinion, Roth Greeting Cards v. United Card Co., 1970.\n\u201cOriginality does not signify novelty; a work may be original even though it closely resembles other works, so long as the similarity is fortuitous, not the result of copying. To illustrate, assume that two poets, each ignorant of"}, {"title": "6. Conclusions and Future Work", "content": "This paper proposes CopyrightShield, a method designed to detect poisoning data and defend against copyright infringement backdoor attack. Inspired by the replication phenomenon observed in diffusion models, we identified not only spatial similarities in replication but also a strong correlation between features and prompts. Leveraging these insights, we devised a spatially-guided method for detecting poisoned samples and developed a more effective poisoning score algorithm. Additionally, we implemented backdoor defenses by introducing protective constraints into the loss function, effectively preventing the generation of poisoned samples and reducing the association between triggers and images. As the first backdoor defense approach specifically targeting copyright infringement, it has been shown through experiments to be effective in identifying and mitigating backdoor samples in specific attack scenarios, thereby significantly reducing the harm caused by such attacks. Additionally, the concept of infringement feature inversion is introduced, which broadens the scope of defense and clarifies the allocation of responsibility for infringement. We aim to raise awareness of copyright infringement and to develop more generalizable infringement defense methods in the future.\nLimitations: Despite achieving effective defense, further research is needed to address certain limitations: (1) the method's effectiveness when applied to real-world copyright-protected images, as opposed to virtually designed datasets; (2) the performance of CopyrightShield in the face of newly developed copyright infringement backdoor attacks."}]}