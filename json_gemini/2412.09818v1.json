{"title": "MERaLION-AudioLLM: Technical Report", "authors": ["Yingxu He", "Zhuohan Liu", "Shuo Sun", "Bin Wang", "Wenyu Zhang", "Xunlong Zou", "Nancy F. Chen", "Ai Ti Aw"], "abstract": "We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning in One Network), the first speech-text model tailored for Singapore's multilingual and multicultural landscape. Developed under the National Large Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates advanced speech and text processing to address the diverse linguistic nuances of local accents and dialects, enhancing accessibility and usability in complex, multilingual environments. Our results demonstrate improvements in both speech recognition and task-specific understanding, positioning MERaLiON-AudioLLM as a pioneering solution for region-specific AI applications. We envision this release to set a precedent for future models designed to address localised linguistic and cultural contexts in a global framework.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in generative AI and large language models (LLMs) have unlocked transformative possibilities across various industries, particularly in domains requiring multimodal intelligence [Minaee et al., 2024, Cui et al., 2024, Ji et al., 2024]. Recognising the strategic importance of developing AI capabilities tailored to local needs, we initiated the development of the nation's first AudioLLM (Audio Large Language Model) under the National Large Language Models Funding Initiative, Singapore [A*STAR, 2023]. This initiative aims to integrate advanced speech and text understanding, enabling seamless communication and nuanced comprehension of Singapore's multilingual and multicultural landscape, while improving performance on region-specific benchmarks.\nOur development journey began with establishing a robust infrastructure for multimodal LLM training, including a distributed data pipeline capable of processing over 30 TB of speech-text datasets and scalable training workflows deployed across high-performance H100 GPU clusters. To overcome the limitations of low-resource datasets, particularly in spoken question answering and dialogue summarization, we enhanced the data pipeline with synthesised and augmented datasets to ensure broader coverage of linguistic diversity. Through iterative training and evaluation, our model demonstrates the ability to balance a 10-billion-parameter architecture while maintaining both computational efficiency and task accuracy.\nIn this technical report, we introduce the first version of MERaLION-AudioLLM (Multimodal Empathetic Reasoning and Learning in One Network), a speech-text model designed with Singapore-specific linguistic and cultural adaptations. The development of a model that understands local accents and contextual nuances is crucial for creating more inclusive and effective AI systems. Traditional speech recognition models often struggle with the diversity of accents, dialects, and linguistic subtleties, leading to inaccuracies and reduced usability in complex, multilingual environments. By"}, {"title": "2 Approach", "content": "MERaLION-AudioLLM is developed with a focus on enhancing its understanding of local accents and contextual nuances. In this initial release, our model is designed to take in a (audio, text) pair as input and generates a text output. We carefully curated a diverse collection of datasets by combining real-world speech data with synthesised and augmented samples to improve representation across various accents and linguistic contexts. Using these curated datasets, we fine-tuned MERaLiON-Whisper encoder from Whisper-large-v2 [Radford et al., 2023] and fused it with SEA-LION V3 [\u0391\u0399 Singapore, 2024], a localised LLM developed by our partner AI Singapore, enabling integration of auditory and textual information in an end-to-end manner for downstream tasks. This offers better flexibility in decoding speed, multitask learning, and avoid error propagation in cascaded models. In future iterations, we plan to extend the model's context length and enhance its ability to handle multi-turn interactions and interleaved audio-text inputs. These improvements will expand its applicability in conversational and multimodal scenarios."}, {"title": "2.1 Model Architecture", "content": "MERaLION-AudioLLM adopts a fusion-based architecture, following designs similar to other AudioLLMs [Fang et al., 2024, D\u00e9fossez et al., 2024, Gong et al., 2024, Ghosh et al., 2024, Chu et al., 2023, 2024]. The architecture, illustrated in Figure 1, comprises three key components: an audio encoder that transforms speech or audio inputs into sequences of vector representations, a text decoder that interprets and responds to natural language instructions, and an adaptor module that compresses the encoder representations while aligning the encoder's hidden dimension with the text decoder's embedding size."}, {"title": "2.1.1 Audio Encoder", "content": "The purpose of the audio encoder is to transform an input speech or audio into a sequence of vector representations. Formally, given an input sequence of raw audio signals x = {x^{(1)}, x^{(2)}, ..., x^{(T)}\\} and an encoder \u03d5, the encoder maps x to a sequence of hidden representations h:\nh = \u03d5(x), h \u2208 \\mathbb{R}^{\\widetilde{T} \\times d}\nwhere $\\widetilde{T}$ represents the output sequence length, d is the hidden size, and $\\widetilde{T} \\ll T$. In this work, we build on the encoder of Whisper-large-v2 [Radford et al., 2023], which has demonstrated strong performance across various speech recognition tasks, to develop our in-house MERaLION-Whisper. Whisper processes audio sampled at 16,000 Hz by converting it into log-Mel spectrogram representations. To adapt Whisper to local accents and linguistic contexts, we further fine-tune the model using a mixture of publicly available and in-house automatic speech recognition (ASR) datasets.\nWe are also exploring the integration of a localised speech encoder, which has been pre-trained from scratch using a self-supervised learning (SSL) framework, and will introduce it in future work."}, {"title": "2.1.2 Adaptor Module", "content": "As the encoder's hidden dimension is significantly smaller than the embedding size of the text decoder, we employ an adaptor module to align the speech or audio embeddings with the text embedding space. Specifically, MERaLiON-Whisper produces embeddings of sequence length 1500 and hidden dimension 1280, while SEA-LION V3 has an embedding size of 3854. We utilize a simple yet effective multi-layer perceptron (MLP) adaptor module with 2 hidden layers to transform the encoder outputs into 100 speech or audio token embeddings with a dimensional size of 3854. We refer to this simple adaptor module as MLP-100. It yields slightly better results compared to other alternatives such as window-level Qformer [Tang et al., 2024] and ConvMLP [Li et al., 2023].\nFormally, given an encoder output sequence h \u2208 \\mathbb{R}^{T \\times d}, the adaptor module first reshapes the sequence to h\u2208 \\mathbb{R}^{(T/s) \\times (d \\cdot s)}, where s is a scaling factor set to 15. This operation effectively reduces the sequence length of h by concatenating outputs from s time steps. The adaptor then applies a linear transformation to h:\nz = SiLU(hW_1 + 1_{(T/s)}b_1)\nwhere $W_1 \\in \\mathbb{R}^{d \\times (d \\cdot s)}$ and $b_1 \\in \\mathbb{R}^d$ are learnable parameters in the first MLP layer, $1_{(T/s)}$ is a vector of 1's of length T/s used to broadcast the bias vector $b_1$ into the shape of $hW_1$, SiLU is the Sigmoid Linear Unit activation function [Hendrycks and Gimpel, 2023, Elfwing et al., 2018], and the resulting representation is $z \\in \\mathbb{R}^{(T/s) \\times d}$. This is followed by upward and downward projection layers:\n$z_u = SiLU(zW_u + 1_{(T/s)}b_u^T)$\n$z_d = z_uW_d + 1_{(T/s)}b_d^T$\nwhere $\\gamma$ is the embedding size of the text decoder, $W_u \\in \\mathbb{R}^{4d \\times d}$, $b_u \\in \\mathbb{R}^{4d}$, $W_d \\in \\mathbb{R}^{\\gamma \\times 4d}$, and $b_d \\in \\mathbb{R}^{\\gamma}$ are trainable parameters. The final projected representation, $z_d \\in \\mathbb{R}^{(T/s) \\times \\gamma}$, aligns the speech or audio embeddings into the same space as the text embeddings, allowing the model to process multimodal inputs jointly."}, {"title": "2.1.3 Text Decoder", "content": "The text decoder of MERaLION-AudioLLM ingests a concatenated sequence of audio context tokens and text instruction tokens, and then generates a text-based response. For this purpose, we leverage on SEA-LION V3 [AI Singapore, 2024], a state-of-the-art localised large language model developed by our partner, AI Singapore. SEA-LION V3 was built upon the 9B version of Google's Gemma 2 [Team et al., 2024] by continual pre-training it on an additional 200 billion tokens sourced from diverse datasets. These datasets encompass the four official languages of Singapore (English, Chinese, Malay, and Tamil), and also include several other Southeast Asian languages. We use the instruct version of"}, {"title": "2.2 Training Methodology", "content": "SEA-LION V3, which was further fine-tuned on approximately 500,000 English instruction-tuning pairs and approximately 1 million instruction tuning pairs in various ASEAN languages."}, {"title": "2.2.1 Audio Encoder Fine-tuning", "content": "Whisper-large-v2 [Radford et al., 2023] is an encoder-decoder ASR model by OpenAI that is well known for its robust performance in speech recognition across multiple languages. Its ability to capture rich audio representations has made it a top choice for integration into AudioLLMs. However, as a general-purpose ASR model, Whisper may not fully capture the subtle intricacies of local speech variations. Therefore, we first guide the model to better capture these local characteristics by training it end-to-end on a collection of cleaned local ASR datasets derived from IMDA's National Speech Corpus, in-house ASR datasets and public datasets."}, {"title": "2.2.2 Multimodal Instruction Fine-tuning", "content": "We apply multimodal instruction fine-tuning to leverage datasets from multiple tasks delineated in Section 3 to align MERaLION-Whisper and SEA-LION V3 for cross-modal reasoning and task- specific performance. Let $D = \\bigcup_{j=1}^M D_j$, where each $D_j = \\{(x_{audio}^{i,j}, x_{text}^{i,j}, y_{i,j})\\}_{i=1}^{N_j}$ represents dataset j containing $N_j$ samples, with $x_{audio}^{i,j}$ as speech or audio inputs, $x_{text}^{i,j}$ as text inputs (such as task instructions or context), and $y_{i,j}$ as task-specific outputs.\nTo train MERaLiON-AudioLLM, we minimise the autoregressive loss function that measures the difference between the predicted and ground truth sequences. The model predicts for the output sequence $y_{i,j} = \\{y^{(1)},...,y^{(L)}\\}$ autoregressively, where L is the output sequence length. The autoregressive loss for a sample is formulated as:\n$L_{i,j} = -\\sum_{l=1}^{(L)} \\log P(y^{(l)} | y^{(<l)}, x_{audio}^{i,j}, x_{text}^{i,j})$\nwhere $y^{(<l)}$ represents the output tokens before the current prediction token. This loss encourages the model to accurately predict each token in the output sequence, conditioned on the prior output tokens and the multimodal input representations.\nDuring training, we fully fine-tune the audio encoder and adaptor module, while partially fine-tuning the SEA-LION V3 text decoder by adding LoRA (Low-Rank Adaptation) [Hu et al., 2022] layers with a rank of 8 to all MLP layers. We used the fused AdamW optimizer in PyTorch, along with a linear learning rate scheduler that includes 100 warm-up steps and a peak learning rate of 5e-5. To mitigate overfitting to artifacts in the input audio log-Mel spectrograms, we find it helpful to apply spectrogram augmentation [Park et al., 2019] by randomly masking a sequence of 20 time steps with a probability of 5%."}, {"title": "3 Datasets", "content": "We curated an extensive collection of speech-text instruction-tuning pairs totalling 260,000 hours of data. A significant portion of this dataset is derived from IMDA's National Speech Corpus (NSC) [Koh et al., 2019], which is licensed under the Singapore Open Data License. The National Speech Corpus contains approximately 10,600 hours of recordings of Singaporean English speakers, structured into six parts:\n\u2022 Part 1 \u2014 3000 hours of prompted readings from phonetically balanced scripts\n\u2022 Part 2 \u2014 3000 hours of prompted readings featuring sentences on topics such as people, food, locations, and brands\n\u2022 Part 3 - 900 hours of conversational data, including discussions on daily life and gameplay interactions\n\u2022 Part 4 \u2014 900 hours of code-switching conversations where speakers alternate between Singlish and their Mother Tongue languages (Chinese, Malay, Tamil).\n\u2022 Part 5 \u2014 1500 hours of conversations following four themes: debate, finance, positive emotion, and negative emotion.\n\u2022 Part 6 - 1300 hours of simulated phone calls across three thematic designs: (1) holiday, hotel, restaurant, (2) bank, telephone, insurance, and (3) Housing and Development Board (HDB), Ministry of Education (MOE), Ministry of Social and Family Development (MSF).\nParts 1 to 5 also include detailed speaker meta-information such as gender, age, ethnic group, and first language, which we used to construct high-quality paralinguistics datasets. We also released the Multitask National Speech Corpus (MNSC) for open usage.\nAlthough NSC serves as an invaluable resource for model training, it includes a significant amount of mislabelled data, as well as systematic and accidental errors. To ensure the integrity and reliability of the datasets, we performed thorough verification and filtering processes, extracting only the most accurate and high quality segments. For Parts 1 and 2, we ensured that all examples with the same transcription were consistently assigned to the same data splits to avoid data leakage. For Parts 3 to 6, we selected recordings where the audio duration closely matched the transcription timestamp duration. For conversational audio recorded separately for each speaker, we superimposed the speech from both sides by summing their respective audio array representations. We further segmented longer conversations into shorter segments, each lasting up to 30 seconds. For transcription notations, we removed non-speech content such as , , and (ppb) while retaining discourse particles (e.g., [oh]), interjections (e.g., !walao!), and fillers (e.g., (um)).\nWe further expanded the dataset by synthesising examples for tasks such as Spoken Dialogue Summarization (SDS), Speech Question Answering (SQA), and Gender Recognition (GR). Detailed information on these datasets will be published separately."}, {"title": "4 Technical Details", "content": ""}, {"title": "4.1 Compute and Infrastructure", "content": "We are grateful for the invaluable support provided by the National Supercomputing Centre (NSCC), Singapore, which enabled us to complete our recent MERaLiON-AudioLLM training runs on the ASPIRE 2A+ Supercomputer. The ASPIRE 2A+ system consists of 40 H100 nodes, with each compute node equipped with 8 Nvidia H100 GPUs, 2 TB of RAM, and 30 TB of locally attached NVMe storage. These nodes are interconnected via a rail-optimised, full fat-tree topology, utilising 400 Gb/s NDR InfiniBand cables. Additionally, the cluster incorporates a 2.5 PB SSD-based Lustre file system, linked to the H100 nodes through high-speed InfiniBand connections.\nWith a global batch size of 640, we train the current release of MERaLION-AudioLLM for around 200k steps, which took 2 days to complete using 128 H100 GPUs. Prior to this, we had conducted much of our data preprocessing and earlier experiments on other high-performance systems, including NSCC'S ASPIRE 2A, Nvidia's Taipei-1 and Europe's LUMI Supercomputer."}, {"title": "4.2 Tech Stack", "content": "As the field of generative AI continues to evolve rapidly, we often found ourselves grappling with subtle bugs in popular open-source deep learning frameworks such as Huggingface's Transformers and PyTorch Lightning. These challenges are particularly pronounced as we explore the relatively underexplored domain of multimodal architectures. To avoid spending excessive time addressing bugs and to prioritise our efforts on experimentation, we decided to develop our own training codebase to support the development of MERaLiON-AudioLLM.\nOur trainer is implemented primarily in PyTorch [Paszke et al., 2017] and utilises Fully Sharded Data Parallelism (FSDP) to achieve near-linear scaling across all 320 H100 nodes on the ASPIRE 2A+"}, {"title": "5 Evaluations", "content": ""}, {"title": "5.1 Setup", "content": "We benchmark our model with a series of testsets from AudioBench benchmark [Wang et al., 2024] against three well-known AudioLLMs: Qwen2-Audio 7B [Chu et al., 2024], WavLLM [Hu et al., 2024], and SALMONN [Tang et al., 2024]. Qwen2-Audio 7B is a state-of-the-art AudioLLM that combines a Whisper encoder with the Qwen text decoder and outperforms commercial models in audio-centric instruction-following capabilities. WavLLM uses a dual-encoder setup that integrates Whisper with WavLM [Chen et al., 2022] and employs a two-stage curriculum learning approach introducing a novel prompt-aware LoRA weight adapter. SALMONN combines a Whisper encoder with BEATS [Chen et al., 2023] and utilises a three-stage curriculum learning strategy for training. We also compared with an cascaded model, which feeds the transcriptions recognized by Whisper-large-v2 along with the instruction prompts to a Gemma2 9B CPT SEA-LIONv3 Instruct model to get the responses. We tuned its hyperparameters and prompt template to optimise performance across a range of speech-to-text tasks.\nWe evaluate MERaLION-AudioLLM on 6 tasks, namely automatic speech recognition (ASR), speech translation (ST), spoken question answering (SQA), spoken dialogue summarization (SDS), speech instruction (SI), and paralinguistics (PARA). Evaluation data is derived from the test splits of the following datasets:\n\u2022 ASR: LibriSpeech [Panayotov et al., 2015], Common-Voice-15 [Ardila et al., 2020], Earnings21 [Rio et al., 2021], Earnings22 [Rio et al., 2022], NSC [Koh et al., 2019]\n\u2022 ST: COVOST 2 [Wang et al., 2021]\n\u2022 SQA: SLUE [Shon et al., 2023], Spoken-SQuAD [Lee et al., 2018], CN-College-Listen-Test [Hu et al., 2024], Singapore-Public-Speech-SQA [Wang et al., 2024], NSC [Koh et al., 2019]\n\u2022 SDS: NSC [Koh et al., 2019]\n\u2022 SI: OpenHermes [Teknium, 2023], Alpaca [Taori et al., 2023]\n\u2022 PARA: VoxCeleb [Nagrani et al., 2017], MELD [Poria et al., 2019]\nWe assess automatic speech recognition (ASR) and speech translation (ST) using Word Error Rate (WER) and BLEU scores [Papineni et al., 2002], respectively. For other tasks, we employ the LLM- as-a-Judge framework, which uses a pre-trained large language model to evaluate task performance by generating and scoring responses based on criteria such as relevance, coherence, and accuracy. Data preparation and other implementation details are available in Wang et al. [2024]."}, {"title": "5.2 Results", "content": "As shown in Table 1, MERaLiON-AudioLLM is competitive against other AudioLLMs on many datasets. For instance, our model has the best performance on the unseen Earnings21-Test and Earning22-Test datasets, beating the second-best AudioLLM by significant margin. As expected, AudioLLM performs better on the NSC datasets, given its training on in-domain data. This is especially evident in the MNSC ASR Part 2 dataset, which contains prompted readings with references to people, food, locations, and brands associated with Singapore. In this context, MERaLION- AudioLLM significantly outperforms both Qwen2-Audio (19% word error rate) and Whisper-large-v2 (33% word error rate), achieving an impressive word error rate of just 5%. These results demonstrate that while strong open-sourced models like Whisper are effective, they still require localisation to optimise performance in specific, localised contexts. On the other hand, we observe that AudioLLMs are prone to the \u201cdiverse prompts\" issue [Wang et al., 2024], where performance on standard benchmarks like LibriSpeech slightly declines when exposed to a set of unseen text prompts. This leads to higher word error rates compared to traditional ASR models, such as Whisper.\nThe only tasks that our model clearly underperforms other models are MELD-Sentiment-Test and MELD-Emotion-Test, where the goal is to identity the sentiment or the emotion of the speaker based on the speech. To address this, we are actively curating additional paralinguistic datasets that focus on local contexts and accents. Additionally, we are exploring architectural improvements to better integrate paralinguistic information into our model."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 AudioLLMs", "content": "Recent advancements in large language models (LLMs) have paved the way for their integration with speech and audio processing capabilities, enabling the development of end-to-end systems that efficiently streamline the workflow from audio input to meaningful output. Some of these integrations focus on specific tasks, such as automatic speech recognition and speech translation [Yu et al., 2024, Ma et al., 2024, Xu et al., 2024, Chen et al., 2024b, Wu et al., 2023]. These specialized models leverage features extracted by speech encoders, such as Whisper [Radford et al., 2023] and Hubert [Hsu et al., 2021], to enhance their understanding and processing of spoken language. Beyond"}, {"title": "6.2 Multimodal Fusion", "content": "Traditional cascaded systems, which combine a speech recognizer with a large language model, are commonly employed in multimodal applications due to their simplicity and modularity. However, they may face limitations in responsiveness and capturing complex contextual information. End-to- end solutions require the model to learn the relationships between audio and text in a more integrated manner.\nSome works have explored architectural and training strategies. Audio Flamingo [Kong et al., 2024] employs audio representation transformation layers to condition on audio inputs. Yu et al. [2024] studies the effectiveness of different modality adapter modules. Ma et al. [2024] trains only the randomly initialized adapter for ASR tasks, and other works such as LLaMA-Omni [Fang et al., 2024] and SpeechGPT [Zhang et al., 2023] instruction finetune additional model components with curated datasets. WavLLM [Hu et al., 2024] uses a curriculum learning framework to first build foundational capabilities through learning elementary tasks, then build more sophisticated capabilities by learning more complex tasks that combine elementary tasks. SALMONN [Tang et al., 2024] employs activation tuning at the end of a multi-stage training scheme to help regain the emergent abilities of LLMs and mitigate catastrophic forgetting.\nA key goal in multimodal fusion is to align both audio and text into a common representational space. SpeechGPT [Zhang et al., 2023] uses cross-modal instruction finetuning to align the inputs and outputs of the two different modalities, and proposes chain-of-modality instruction finetuning to map speech input into text representation space, think about the answering process in text, and then output speech or text response. AudioChatLlama [Fathullah et al., 2024] and DiVA [Held et al., 2024] distill the responses of text-only LLMs to speech transcripts. Spirit LM [Nguyen et al., 2024] seeks word-level alignment by interleaving speech and text in a single sequence.\nBesides audio-text alignment, acoustic features capturing paralinguistic information such as tone, pitch and emotion can be incorporated during fusion to provide context beyond the words themselves. DeSTA [Lu et al., 2024] proposes a descriptive speech-text alignment approach by leveraging paralinguistic metadata to generate captions with natural language descriptions of the acoustic quality of speech inputs, so as to capture both the spoken words and speaking styles. SpeechEmotionLlama [Kang et al., 2024] distills the responses of LLMs to speech transcripts where the speaker's emotion is specified. Some works use additional encoders to extract acoustic features [Tang et al., 2024, Hu et al., 2024, Zhang et al., 2024]. In Moshi [D\u00e9fossez et al., 2024], the neural audio codec encodes audio into semantic tokens that capture linguistic content, and acoustic tokens that are optimized for high-quality audio reconstruction to retain fine audio details. EMOVA [Chen et al., 2024a] also"}, {"title": "7 Limitations and Future Work", "content": "Although MERaLiON-AudioLLM demonstrates competitive performances on standard benchmark evaluations, there are several limitations that we are aware of and would address in future releases:\nSafety As MERaLiON-AudioLLM have not been specifically aligned for safety and may generate content that is inappropriate, offensive, or harmful. Developers and users are responsible for perform- ing their own safety fine-tuning and implementing necessary security measures. The authors shall not be held liable for any claims, damages, or other liabilities arising from the use of the released models, weights, or code.\nContext Length Currently, MERaLiON-AudioLLM supports only up to 30 seconds of audio context. We are actively working on improving its ability to process and integrate long-range dependencies in conversational speech and complex narratives. Additionally, we are enhancing the model's capacity to handle multi-turn interactions and interleaved text and audio inputs.\nLoss of instruction following capability Achieving strong performance in tasks like speech recognition and speech translation requires fine-tuning the entire AudioLLM in an end-to-end manner, but this has resulted in some degree of catastrophic forgetting, causing the model to lose the ability to follow certain text instructions. To address this, we have planned several key explorations before the next release. First, we plan to create a Speech Instruction Following dataset to measure the extent of this capability loss. Additionally, we will experiment with training the AudioLLM on more diverse multimodal datasets while maintaining a replay collection of text-only instruction pairs to mitigate forgetting. Finally, we are exploring methods to directly align audio encoders with text decoders, bypassing the need to fine-tune the text decoders. Early successes in this area show promise, and we intent to explore this line of work further.\nMultilingualism and Empathetic Reasoning While MERaLiON demonstrates the ability to handle non-English speech-text tasks, as well as non-speech tasks such as Emotion and Gender Recognition, we believe its performance on these tasks can be further enhanced with additional data. To that end, we are actively exploring strategies to scale up our data collection efforts in an efficient manner."}, {"title": "8 Conclusion", "content": "This technical report presents MERaLION-AudioLLM, a multimodal model designed to bridge the gap between speech and text. Through careful data curation and optimisation, MERaLION- AudioLLM demonstrates significant advancements in both speech-text understanding and generation, especially in local contexts. The development of MERaLiON-AudioLLM highlights the potential of combining large-scale multimodal datasets with advanced model architectures to address real-world challenges. However, there remains areas for further exploration, particularly in refining instruction- following behaviour and enhancing robustness across low-resource datasets. We look forward to continuing this line of research, contributing to the advancement of multimodal AI systems, and making our work available to the broader research community for further validation and development."}]}