{"title": "How Contaminated Is Your Benchmark?\nQuantifying Dataset Leakage in Large Language Models with Kernel Divergence", "authors": ["Hyeong Kyu Choi", "Maxim Khanov", "Hongxin Wei", "Yixuan Li"], "abstract": "Dataset contamination, where evaluation datasets\noverlap with pre-training corpora, inflates per-\nformance metrics and undermines the reliabil-\nity of model evaluations. Quantifying dataset\ncontamination thus becomes essential to ensure\nthat performance evaluations genuinely reflect\na model's ability to generalize to unseen data,\nrather than relying on memorized examples. To\naddress this problem, we propose Kernel Diver-\ngence Score (KDS), a novel method that quanti-\nfies dataset contamination by computing the di-\nvergence between the kernel similarity matrix of\nsample embeddings, before and after fine-tuning\non the benchmark dataset. Leveraging the insight\nthat fine-tuning affects unseen samples more sig-\nnificantly than seen ones, KDS provides a reliable\nmeasure of contamination. Through extensive\nexperiments on controlled contamination scenar-\nios, KDS demonstrates a near-perfect correlation\nwith contamination levels and outperforms ex-\nisting baselines. Additionally, we perform com-\nprehensive ablation studies to analyze the impact\nof key design choices, providing deeper insights\ninto the components and effectiveness of KDS.\nThese ablations highlight the importance of lever-\naging fine-grained kernel-based information and\nconfirm the reliability of the proposed framework\nacross diverse datasets and settings.", "sections": [{"title": "1. Introduction", "content": "When a large language model (LLM) performs remark-\nably well on a benchmark, can you confidently attribute\nits success to true generalization-or is it simply a reflection\nof what the model has already seen during pre-training?\nThe reality is, we often don't know. Beneath the surface"}, {"title": "2. Problem Statement", "content": ""}, {"title": "2.1. Quantifying Benchmark Contamination", "content": "The objective is to quantify the relative degree to which\na benchmark evaluation dataset, D, has been exposed to\nthe pre-training process of a given LLM, M. In modern\nLLMs, the pre-training dataset is typically unavailable, mak-\ning it difficult to directly assess the contamination level.\nAccordingly, we consider a generalized characterization of\nthe benchmark evaluation data, modeling it as a mixture of\nboth seen and unseen data:\nD = D_{seen} \\cup D_{unseen}\n\\lambda = |D_{seen}|/|D| = 1,\n(1)\nwhere $D_{seen}^M$ is the data seen during M's pre-training, $D_{unseen}^M$\nis the data not seen by M, and $\\lambda \\in [0, 1]$ is an unknown\nparameter indicating the fraction of seen data in D. Within\nthis framework, we aim to develop a dataset-level scoring\nfunction\nS : (D, M) \\rightarrow R,\nwhich relatively quantifies the contamination of dataset D\nwith respect to model M. A larger score indicates more\ncontamination and vice versa.\nPractical utility. A reliable scoring function is practically\nvaluable because it allows us to identify benchmark datasets"}, {"title": "2.2. Reliable Contamination Scores", "content": "A comparative study on the contamination level across\ndatasets is reliable only if the scoring function satisfies spe-\ncific key properties. In this section, we state two essential\nrequirements for a reliable contamination scoring function:\nMonotonicity and Consistency.\nRequirement 1. (Monotonicity) If dataset D is more inde-\npendent of model M than dataset D', i.e., $\\lambda < \\lambda'$, then\nS(D, M) < S(D', M)\nshould hold with statistical significance. In other words, a\ndataset with a smaller $\\lambda$, the fraction of seen data, should\nhave accordingly a smaller contamination score S(D, M).\nRequirement 2. (Consistency) If datasets D and D' both\ncomprise of independently and identically distributed (i.i.d.)\nsamples from a distribution with the same contamination\nratio $\\lambda$,\nS(D, M) \\approx S(D', M)\nshould hold with statistical significance.\nThe Monotonicity requirement ensures that the scoring func-\ntion exhibits a positive correlation with the dataset's con-\ntamination rate, even though the true contamination rate\nis typically unknown in real-world scenarios. A scoring\nfunction satisfying this requirement enables reliable rank-\ning of benchmark datasets for each model based on their\ncontamination scores. The Consistency requirement, on the\nother hand, ensures that the scores are robust to variations\nin the specific samples drawn from the same underlying dis-\ntribution, under the same $\\lambda$. This property ensures that the\nrandomness induced from sampling does not substantially\naffect the overall scoring."}, {"title": "3. Method: Kernel Divergence Score", "content": "In this section, we present our method, Kernel Divergence\nScore, which leverages information among samples within\nthe model's embedding space to establish a more nuanced\ncontamination scoring mechanism. In a nutshell, we as-\nsess changes in the kernel matrix of sample embeddings\nbefore and after fine-tuning, capturing how the relationships\nbetween samples evolve as a result of fine-tuning.\nOur approach is motivated by the fact that fine-tuning af-\nfects the embedding relationships involving unseen samples\nmore significantly than those involving seen samples. For\nseen samples, the model has already been exposed to similar\ndata during pretraining, leading to minimal shifts in their\nembedding relationships. In contrast, unseen samples expe-\nrience more pronounced changes, as the fine-tuning process\nadjusts the model to better align with the benchmark dataset.\nBy quantifying these changes using the Kernel Divergence\nScore, we can provide a reliable and granular measure of\ndataset contamination.\nKernel similarity matrix. A kernel similarity matrix cap-\ntures the relationships among data samples, providing fine-\ngrained information on their distribution. Formally, let\n$Z \\in \\mathbb{R}^{n \\times d}$ represent the embeddings of n samples in the\ndataset D, where $Z_i \\in \\mathbb{R}^{1\\times d}$ is the normalized embedding\nof i-th sample extracted from the pre-trained LLM M. We\ndefine the kernel matrix $\\Phi(Z) \\in \\mathbb{R}^{n\\times n}$ based on the Radial\nBasis Function (RBF) kernel:\n\\Phi(Z)_{i,j} = exp(-\\gamma ||Z_i - Z_j||^2),\nwhere $\\Phi(Z)_{i,j}$ is the kernel similarity between samples $Z_i$\nand $Z_j$, and $\\gamma$ controls the kernel bandwidth. The kernel\nmatrix captures the pairwise relationships between all sam-\nples in the dataset, with values ranging from 0 to 1. An\nentry close to 1 indicates high similarity (small distance),\nwhile a value close to 0 indicates low similarity (large dis-\ntance). The matrix $\\Phi(Z)$ is both symmetric and positive\nsemidefinite.\nLeveraging the effect of fine-tuning. Our proposed Ker-\nnel Divergence Score is based on the kernel matrix before\nand after fine-tuning. Formally, let $Z' \\in \\mathbb{R}^{n \\times d}$ represent\nthe embeddings after supervised fine-tuning on dataset D,\nwhere $Z'_i \\in \\mathbb{R}^{1\\times d}$. Accordingly, we can derive the kernel\nsimilarity matrix as:\n\\Phi(Z')_{i,j} = exp(-\\gamma ||Z'_i - Z'_j||^2).\n(2)\nThen, we define Kernel Divergence as\nE = \\frac{1}{n^2} \\sum_{i,j=1}^n \\Phi(Z)_{i,j} \\log \\frac{\\Phi(Z)_{i,j}}{\\Phi(Z')_{i,j}},\n(3)\nwhere $E = \\sqrt{\\sum_{i,j} \\Phi(Z)_{i,j}}$ is a normalizer. When $\\gamma = 1$,\nour score in Eq. (3) can be equivalently written as\n\\frac{1}{n^2} \\sum_{i,j=1}^n exp(-\\gamma ||Z_i - Z_j||^2) (||Z'_i - Z'_j||^2 - ||Z_i - Z_j||^2)\n(1) Soft gating for originally\nclosely-related samples\n(2) Change in distance before and after SFT\n(4)"}, {"title": "Interpretation of kernel divergence.", "content": "This function quan-\ntifies how fine-tuning changes the pairwise distances be-\ntween samples, weighted by their original similarity. Specif-\nically, the second term $|||Z'_i - Z'_j|| - || Z_i - Z_j ||^2|$ measures\nthe absolute change in the squared Euclidean distance be-\ntween embedding pairs caused by fine-tuning. For unseen\nsamples, fine-tuning tends to create new meaningful relation-\nships or significantly alter their embeddings, making their\ncontribution to the score larger. The first exponential term\nacts as a soft gating function, assigning a higher weight to\npairs of samples that were originally closer to each other. By\nincorporating this term, the score prioritizes the impact of\nfine-tuning on pairs that were initially similar, highlighting\ncases where fine-tuning induces significant changes in their\nrelationships. Overall, a larger fraction of unseen examples\n(or smaller $\\lambda$) can elevate the kernel divergence more signif-\nicantly. Because we want the scoring function S(D, M) to\nbe positively correlated with the contamination rate $\\lambda$, we\ndefine our final scoring function to be the negation of kernel\ndivergence:\nS(D,M) = -\\frac{1}{E}\\sum_{i,j=1}^n \\Phi(Z)_{i,j} \\log \\frac{\\Phi(Z)_{i,j}}{\\Phi(Z')_{i,j}},\n(5)\nwhich is expected to be larger as the contamination $\\lambda$ grows."}, {"title": "Visual demonstration of score components.", "content": "To provide a\nconcrete understanding of the role of each component in our\nKernel Divergence Score (Eq. (3) or Eq. (4)), we present the\nkernel matrix for each component in Figure 2. We use n =\n100 samples with a contamination rate $\\lambda$ = 0.4, meaning\n40% of the samples are seen during model pre-training. The\nleft panel illustrates the soft gate from Eq. (4), or the kernel\nsimilarity matrix using the pre-trained embedding. The\nmiddle panel captures the change of pairwise embedding"}, {"title": "4. Experiments", "content": "In this section, we conduct a controlled experiment to verify\nthe reliability of our Kernel Divergence Score with respect\nto the Monotonicity and Consistency requirements (c.f. Sec-\ntion 2.2). For a comprehensive analysis, we also assess\nexisting non-kernel-based scoring methods."}, {"title": "4.1. Experimental Setup", "content": "Dataset and model. Our controlled experiment aims to\nevaluate the extent to which each method satisfies the Mono-\ntonicity and Consistency requirements. For this purpose, we\nutilize three popular pre-training data detection benchmarks,\nWikiMIA (Shi et al., 2023), BookMIA (Shi et al., 2023),\nand ArxivTection (Duarte et al., 2024), each comprising\nsamples labeled as 'seen' or 'unseen'. Among the models\ncompatible with the datasets, we select Mistral-7B-Instruct-\nv0.2 (Jiang et al., 2023) for our main evaluation. We present\nadditional results for other model families in Section 6.\nBaselines. We consider various baseline methods for com-\nprehensive evaluation. Specifically, we consider Zlib (Car-\nlini et al., 2021), Perplexity Score (Li, 2023), Min-K% (Shi\net al., 2023), Min-K%++ (Zhang et al., 2024b), Fine-tuned"}, {"title": "4.2. Experimental Results", "content": "Kernel divergence score satisfies the monotonicity re-\nquirement. To evaluate compliance with the monotonicity"}, {"title": "5. Ablation Studies", "content": "In this section, we conduct an in-depth ablation to under-\nstand various design choices of our kernel divergence score.\nWhat's the impact of fine-tuning on the score? As de-\nscribed in Eq. (4), the Kernel Divergence Score comprises\ntwo key components: (1) the kernel similarity matrix $\\Phi(Z)$,\nwhich serves as a soft gating mechanism, and (2) the change\nin pairwise distance, which captures the effects of super-\nvised fine-tuning. The roles of these two components are\nqualitatively illustrated in the left and middle panels of Fig-\nure 2. To further elucidate their individual contributions,\nwe conducted an ablation study, with results summarized in\nTable 2 (top). The study evaluates the impact of removing\neach component.\nSpecifically, ablating component (1) involves omitting the\nsoft gating mechanism, thereby utilizing the average of\n$|||Z'_i - Z'_j|| - ||Z_i - Z_j||^2|$ as the contamination score.\nConversely, ablating component (2) is equivalent to disre-\ngarding the effects of supervised fine-tuning, relying solely\non the kernel similarity matrix $\\Phi(Z)$ before fine-tuning,\nwith the average kernel entry value serving as the contam-\nination score. The results indicate that removing the soft\ngating mechanism (Component 1) leads to a marginal de-\ncline in performance. This suggests that while the gating\nmechanism enhances the score's reliability, the majority of\nthe information is derived from the differential effects ob-\nserved before and after fine-tuning. Indeed, removing the\nsupervised fine-tuning component (Component 2) signifi-\ncantly degrades the performance, highlighting the critical\nrole of fine-tuning in amplifying the kernel's ability to mea-\nsure dataset contamination levels.\nOur method is not sensitive to the choice of kernel func-\ntion. The Kernel Divergence Score employs the RBF\nkernel to compute differences after supervised fine-tuning.\nHowever, an important question arises: how robust is the\nscoring performance to variations in the kernel function? To\naddress this, we evaluate our method using alternative ker-\nnel functions, including Euclidean pairwise distance, cosine\nsimilarity, and dot-product similarity, as shown in Table 2\n(bottom). For the Euclidean pairwise distance and cosine\nsimilarity kernels, we replace $\\Phi(Z)$ in Eq. (5) with the\nrespective kernel computations. We add 1 to the cosine sim-\nilarity kernel matrix to enforce non-negative entries. For the\ndot-product similarity kernel, we compute the mean squared\nerror of the kernel matrices before and after fine-tuning, as\nthe kernel entries may take negative values, making them\nincompatible with the logarithmic operation used in our\nscoring function.\nThe results indicate that scoring performance remains con-\nsistent across different kernel functions. This suggests that\nthe effectiveness of the Kernel Divergence Score lies in its\nability to leverage structural information from kernel rep-\nresentations, rather than being dependent on the specific\nchoice of the kernel. These findings highlight the versatility\nof our kernel-based approach in quantifying contamination\nlevels across diverse settings.\nHow does the kernel bandwidth $\\gamma$ impact the perfor-\nmance? In an RBF kernel, the bandwidth parameter $\\gamma$\ncontrols the sharpness of the kernel's entry distribution. In\nour approach, we employed the median heuristic (Garreau\net al., 2017), which sets $\\gamma$ as the inverse of the median\npairwise distance. To examine the effect of $\\gamma$ on contam-\nination scoring, we evaluate different bandwidth values\n{0.001, 0.01, 0.1, 1.0, 10.0}, as shown in Table 3. The re-"}, {"title": "Results", "content": "sults indicate that scoring performance is largely invariant\nto the choice of $\\gamma$. This behavior is expected, considering\nthat Eq. (4) with arbitrary $\\gamma$ is\n\\frac{1}{n^2} \\sum_{i,j=1}^n \\gamma exp(-U_{i,j}) W_{i,j} - U'_{i,j},\nmonotonicity preserved\n(6)\nwhere $U_{i,j} = ||Z_i - Z_j||^2$ and $u'_{i,j} = ||Z'_i - Z'_j||^2$. The\neffect of $\\gamma$ is limited to a constant multiplicative factor on\nthe overall scores, and to a power factor that controls the\nsharpness of the soft gate. This does not influence their\nrelative trends across varying contamination rates. This\ninvariance underscores the robustness of our Kernel Diver-\ngence Score to the choice of bandwidth parameter. However,\nsetting an excessively large value (e.g. $\\gamma$ = 10.0) may cause\nnumerical errors and degrade scoring performance.\nWhat's the impact of embedding extraction location?\nIn our main experiments, we use the output embeddings\nfrom the final layer of the LLM to compute KDS. To fur-\nther analyze the impact of embedding location, we evaluate\nthe Spearman and Pearson correlation using embeddings\nextracted from different layers of the model, as shown in\nFigure 4 (top). Our results reveal that the strongest corre-\nlation is observed in the latter layers, indicating that these\nlayers contain the most information relevant to dataset con-\ntamination. This suggests that the latter layers of the LLM,\nwhich are typically fine-tuned to align with specific tasks"}, {"title": "How does SFT configuration impact the performance?", "content": "Table 4 presents the Spearman and Pearson correlation coef-\nficients for contamination scores under different SFT train-\ning configurations on the WikiMIA dataset. The configu-\nrations vary in terms of the optimization method (Stochas-\ntic Gradient Descent vs. Batch Gradient Descent) and the\nnumber of fine-tuning epochs (1 vs. 4). The results show\nthat stochastic GD significantly outperforms batch GD, sug-\ngesting that the finer-grained updates introduced by SGD\nenhance the sensitivity of the Kernel Divergence Score to\ndataset contamination. On the other hand, increasing the\nnumber of fine-tuning epochs does not necessarily improve\nscoring performance. This may be attributed to the repeated\nexposure of the model to the same samples during training,\nwhich could obscure the distinction between seen and un-\nseen samples. Overall, training with one epoch using SGD\nleads to the best performance."}, {"title": "6. Discussions", "content": "Temporal shift problems of MIA benchmarks. Recent\nstudies have expressed concerns regarding the temporal\nshift issues in existing Membership Inference Attack (MIA)\nbenchmarks (Duan et al., 2024; Das et al., 2024; Maini et al.,\n2024). Notably, datasets such as WikiMIA, BookMIA, and\nArxivTection have been identified as susceptible to temporal\ncues, which can inadvertently simplify the membership in-\nference task. This simplification arises because models can\nexploit temporal information to distinguish between seen\nversus unseen data, leading to a potential overestimation of\ndetection performance.\nTo ensure the robustness of our approach and mitigate po-\ntential biases introduced by temporal shifts, we conducted\nevaluations using 500 samples from six subsets of the Pile\ndataset (Gao et al., 2020). The subsets include text data from\nvarious sources, including expository proses (Wikipedia),\nacademic papers (PhilPapers), emails (Enron), news arti-"}, {"title": "Impact Statement", "content": "The broader impact of this work lies in its potential to sig-\nnificantly improve the reliability, transparency, and fairness\nof large language model evaluation. By enabling the identi-\nfication and quantification of contaminated datasets, our ap-\nproach ensures that reported performance metrics are more\ntrustworthy and reflective of a model's true generalization\ncapabilities. This contributes to a more rigorous bench-\nmarking process, fostering fair and meaningful comparisons\nacross different models and architectures. Furthermore, the\ninsights gained from this work can inform better practices\nfor dataset curation. This not only reduces the risk of in-\nflated performance results but also enhances the utility of\nbenchmarks as tools for guiding research and development."}, {"title": "7. Related Works", "content": "Data contamination (Magar & Schwartz, 2022; Xu et al.,\n2024; Balloccu et al., 2024), also known as benchmark\nleakage, poses a significant challenge in the evaluation of\nLLMs (Zhou et al., 2023; Duan et al., 2024). To mitigate\nthis problem, one line of research focuses on \"decontami-\nnating\" datasets by introducing controlled perturbations to\nreduce overlap with evaluation (Yang et al., 2023). Another\nline explores methods for detecting contaminated datasets\nor identifying samples seen during LLM training. Member-\nship inference attack (MIA) techniques (Shokri et al., 2017;\nTruex et al., 2019) have been employed to classify individual\ndata as seen or unseen (Yeom et al., 2018; Salem et al., 2019;\nMattern et al., 2023), with many recent studies specifically\ntargeting LLMs for pre-training data detection (Carlini et al.,\n2021; Shi et al., 2023; Zhang et al., 2024b; Xie et al., 2024;\nLi, 2023; Ye et al., 2024). In addition, set-level detection\nmethods have been introduced to identify contamination at a\nbroader dataset level (Oren et al., 2024; Zhang et al., 2024a;\nGolchin & Surdeanu, 2024). Building on this foundation,\nour work introduced a novel approach, Kernel Divergence\nScore, to scoring contamination levels using information\nderived from embedding kernel similarity matrices. An\nexpanded literature review of MIA is in Appendix F."}, {"title": "8. Conclusion", "content": "In this work, we addressed the critical issue of dataset con-\ntamination in LLM by introducing the Kernel Divergence\nScore. By capturing fine-tuning-induced shifts in sample\nembeddings, KDS provides a robust and interpretable mea-\nsure of contamination. Extensive experiments on controlled\nscenarios demonstrated the effectiveness of KDS in sat-\nisfying key properties like monotonicity and consistency,\noutperforming existing baselines. This work paves the way\nfor more reliable benchmark evaluations, fostering better\ndataset curation practices in LLM research."}, {"title": "Appendix", "content": ""}, {"title": "A. Further Experimental Details", "content": "In this section, we append further experimental details and provide formal definitions of the baselines evaluated in the\nmanuscript."}, {"title": "A.1. Implementation Details", "content": "For supervised fine-tuning, we utilize Low-rank Adaptation (Hu et al., 2021). In Table 8, we disclose detailed LORA\nconfigurations and other training hyperparameters used for supervised fine-tuning."}, {"title": "A.2. Baseline Definitions", "content": "Here, we provide formal definitions for each baseline compared in Table 1.\nDefinition 1. (Zlib Score) is the negated ratio of the log perplexity and the zlib compression size:\n- \\frac{1}{n} \\sum_{i=1}^n \\frac{\\frac{1}{|T_i|} \\sum_{x_j \\in T_i} log P_\\theta (x_j | x_{<j}) }{Zlib(x_i).size},\n(7)\nwhere Ti is the set of tokens from sample i. (Carlini et al., 2021)"}, {"title": "Definition 2. (Perplexity Score)", "content": "is the negated average perplexity across samples:\n\\frac{1}{n} \\sum_{i=1}^n exp(- \\frac{1}{|T_i|} \\sum_{x_j \\in T_i} log P_\\theta (x_j | x_{<j}) )\n(8)\nwhere Ti is the set of tokens from sample i. (Li, 2023)"}, {"title": "Definition 3. (Min-K% Score)", "content": "is the negated mean probability from bottom-k% tokens averaged across samples:\n\\frac{1}{n} |K_i| \\sum_{i=1}  \\sum_{x_j \\in K_i} log P_\\theta (x_j | x_{<j}),\n(9)\nwhere $K_i$ is the set of bottom-k% tokens from sample i. (Shi et al., 2023)"}, {"title": "Definition 4. (Min-K%++ Score)", "content": "is the negated mean normalized probability from bottom-k% tokens averaged across\nsamples:\n\\frac{1}{n} |K_i| \\sum_{i=1}  \\sum_{x_j \\in K_i} \\frac{log P_\\theta (x_j | x_{<j}) - \\mu_{x_{<j}}}{\\sigma_{x_{<j}}},\n(10)\nwhere $K_i$ is the set of bottom-k% tokens from sample i, $\\mu_{x_{<j}} = E_{z \\sim p(\\cdot | x_{<j})} [log p(z | x_{<j})]$ is the expected log probability\nover the vocabulary of the model, and $\\sigma_{x_{<j}} =  \\sqrt{E_{z \\sim p(\\cdot | x_{<j})} [(log p(z | x_{<j}) - \\mu_{x_{<j}})^2]}$ is the standard deviation. (Zhang\net al., 2024b)"}, {"title": "Definition 5. (Fine-tuned Score Deviation)", "content": "is the difference of scores before and after supervised fine-tuning, averaged\nacross samples:\n\\frac{1}{n} \\sum_{i=1}^n S(x_i;\\theta) - S(x_i;\\theta'),\n(11)\nwhere $x_i$ is the i-th sample in the dataset, $S(\\cdot; \\cdot)$ is an existing scoring function (e.g., Min-K% or Perplexity Score), and\n$\\theta, \\theta'$ are models before and after fine-tuning, respectively. (Zhang et al., 2025)"}, {"title": "Definition 6. (Sharded Rank Comparison Test)", "content": "is the difference between the log likelihood of the canonical dataset sample\nordering from the mean over shuffled sample orderings, averaged across dataset shards:\n\\frac{1}{r} \\sum_{k=1}^r log P([x_k]_{i=1}^n) - \\frac{1}{|S|}  \\sum_{\\sigma \\in S} log P([x_{\\sigma(i)}]_{i=1}^n),\n(12)\nwhere r is the number of shards, S is the set of sample permutations, and $[x_k]_{i=1}^n$ is the sequence of samples $x_1, x_2, ..., x_n$\nin k-th shard of the dataset. (Oren et al., 2024)"}, {"title": "E.1. Benchmark Dataset Details", "content": "Here, we list the full names of the benchmark dataset code in Table 10. We also provide dataset details and the specific splits\nused for evaluation.\n* BIQ is the BoolQ dataset (Clark et al., 2019). It contains binary-choice questions asking about the given passage. We\nevaluated the validation split that consists of 3270 samples.\n* HSg is the HellaSwag dataset (Zellers et al., 2019). It is a commonsense natural language inference benchmark asking the\nmodel to choose an option that best finishes the given sentence. We evaluated the validation split that consists of 10042\nsamples.\n* OBQ is the OpenBookQA dataset (Mihaylov et al., 2018). It is a 4-way multiple-choice question answering benchmark\nthat requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension.\nWe evaluated the test split of the 'main' subset, which consists of 500 samples.\n* MNLI is the Multi-genre NLI dataset (Williams et al., 2018). It is a crowd-sourced collection of sentence pairs annotated\nwith textual entailment information, covering various spoken and written text. We evaluated the 'validation_matched' split\nthat consists of 9815 samples.\n* TFQ is the TruthfulQA dataset (Lin et al., 2022). It consists of question and answer pairs, some of which are correct and\nothers are factually incorrect. Questions are designed in a way that humans may answer falsely due to a false belief or\nmisconception. We evaluated the validation split of the 'generation' subset, which consists of 817 samples.\n* PIQ is the Physical Interaction Question-answering dataset (Bisk et al., 2020). It contains questions that require physical\ncommonsense reasoning based on everyday situations. We evaluate the test split of the 'plain_text' subset, which consists\nof 3084 samples.\n* MPP is the Professional Psychology subset of the MMLU dataset (Hendrycks et al., 2021). It contains multiple-choice\nquestions that require expert knowledge in psychology. We evaluate the test split, which consists of 798 samples."}, {"title": "MTH", "content": "* MPL is the Professional Law subset of the MMLU dataset (Hendrycks et al., 2021). It contains multiple-choice questions\nthat require expert knowledge in law. We evaluate the test split, which consists of 1101 samples.\n* MHP is the Highschool Psychology subset of the MMLU dataset (Hendrycks et al., 2021). It contains multiple-choice\nquestions that require highschool-level knowledge in psychology. We evaluate the test split, which consists of 544 samples.\n* GSM is the Grade School Math 8K dataset (Cobbe et al., 2021). It contains linguistically diverse grade school math word\nproblems that require multi-step reasoning. Solutions generally involve calculating a series of arithmetic operations. We\nevaluate the test split of the 'main' subset, which consists of 1319 samples.\nis the MATH-500 dataset (Lightman et al., 2024). It contains a subset of 500 problems from the MATH benchmark,\nwhich requires the model to provide a numerical answer to the question. We evaluate the test split, which consists of 500\nsamples."}, {"title": "E.2. Implementation Details", "content": "The implementation details are generally identical to the setup used in our experiments on the WikiMIA, BookMIA, and\nArxivTection datasets. We employ 1 epoch of stochastic gradient descent for supervised fine-tuning, with the same LoRA\nconfigurations reported in Table 8."}, {"title": "E.3. Results", "content": "Evaluation results are provided in Table 10. For better comparison, we min-max scale the scores across benchmark\ndatasets, and their ranks are provided in parentheses. Please note that the scores are not to be compared across models.\nOverall, the MMLU (Hendrycks et al., 2021), Math500 (Lightman et al., 2024), OBQA (Mihaylov et al., 2018), and\nTruthfulQA (Lin et al., 2022) datasets showed higher contamination scores, and the two natural language inference datasets,\nHellaSwag (Zellers et al., 2019) and MNLI (Williams et al., 2018), received lowest scores."}, {"title": "F. Extended Literature Review", "content": "Here, we provide a more descriptive review of previous works on membership inference attack (MIA) (Shokri et al., 2017;\nTruex et al., 2019), as it is related to the objective of our work in quantifying leakage (i.e., contamination) in datasets (Magar\n& Schwartz, 2022; Xu et al., 2024; Balloccu et al., 2024)."}, {"title": "Early MIA approaches.", "content": "Membership inference attacks aim to determine whether a specific data sample was part of\na model's training dataset. Early approaches primarily utilized metrics derived from model outputs to achieve this. For\ninstance, Salem et al. (2019) employed output entropy as a distinguishing factor, observing that models often exhibit lower\nentropy (i.e., higher confidence) for training samples compared to non-training ones. Similarly, Liu et al. (2019) focused on\nthe model's confidence scores, noting that higher confidence levels could indicate a sample's presence in the training set, and\nCarlini et al. (2022) proposed a likelihood ratio-based approach. Beyond output-based metrics, researchers have explored\nthe impact of training dynamics on MIAs. Yeom et al. (2018) investigated the use of loss values, finding that models tend\nto produce lower loss for training samples, making loss a viable metric for membership inference. Additionally, Liu et al.\n(2023) introduced a gradient-based approach, leveraging the observation that the gradients of training samples differ from\nthose of non-training samples, thereby serving as an effective indicator for membership inference."}, {"title": "Challenges of MIA on Large Language Models.", "content": "While MIAs have been effective against traditional machine learning\nmodels, applying them to large language models (LLMs) presents unique challenges. Recent studies have highlighted the\ndifficulty of performing MIAs on"}]}