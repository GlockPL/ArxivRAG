{"title": "Al-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools", "authors": ["Nathalie Riche", "Anna Offenwanger", "Frederic Gmeiner", "David Brown", "Hugo Romat", "Michel Pahud", "Nicolai Marquardt", "Kori Inkpen", "Ken Hinckley"], "abstract": "Chat-based prompts respond with verbose linear-sequential texts, making it difficult to explore and refine ambiguous intents, back up and reinterpret, or shift directions in creative AI-assisted design work. Al-Instruments instead embody \"prompts\" as interface objects via three key principles: (1) Reification of user-intent as reusable direct-manipulation instruments; (2) Reflection of multiple interpretations of ambiguous user-intents (Reflection-in-intent) as well as the range of Al-model responses (Reflection-in-response) to inform design \"moves\" towards a desired result; and (3) Grounding to instantiate an instrument from an example, result, or extrapolation directly from another instrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine new instruments, enabling a system that goes beyond hard-coded functionality by generating its own instrumental controls from content. We demonstrate four technology probes, applied to image generation, and qualitative insights from twelve participants, showing how AI-Instruments address challenges of intent formulation, steering via direct manipulation, and non-linear iterative workflows to reflect and resolve ambiguous intents.", "sections": [{"title": "1 Introduction", "content": "Despite the immense promise of generative AI, it remains challenging for people to express and refine their true intents via multiple rounds of textual chat-prompts [48, 78], as well as to pursue multiple-alternative paths forward within a linear conversational metaphor. Users face numerous difficulties (e.g., [64]): articulating their intent in a few words of written text (intent formulation); correctly expressing sufficient detail to express, refine, and re-formulate their true intent (intent disambiguation); iterating over the model's response to approach a desired outcome (steering); and navigating higher-order challenges of interaction with AI, such as discovering what one can do with AI-or even what one \"actually\" wants to achieve (intent resolution)-within the linear sequential limitations of chat-based exchanges (interaction workflow).\nAlthough existing work in human-computer interaction and artificial intelligence (HCI+AI) addresses some aspects of these challenges in piecemeal fashion through novel interaction techniques with generative AI (e.g. [17, 50]), we argue here that Al-Instruments offer a novel approach to gain traction on many aspects of these challenges by appropriating and re-casting the principles of instrumental interaction [5, 7] to the modern context of generative HCI+AI user experiences. Quoting Beaudoin-Lafon et al. [6], the value of proposing such interaction model is to \"change-oriented perspective by providing HCI researchers with conceptual tools for analyzing technologies in use or exploring novel future solutions\". Triangulating theory, artifact, and empirical evaluation has strong benefits for advancing HCI research [47].\nInstrumental interaction offers a particularly compelling concept from the HCI literature to revisit in the context of generative Al because it offers principled interaction dynamics about how software functionalities (\"commands\") combine with content (the \"objects\" those commands act upon). While in the past these dynamics had to be hand-designed and hand-coded for specific object types and application settings, the advent of generative AI makes it plausible that the polymorphic nature of high-level commands and flexible content representations will unleash exciting new possibilities for HCI+AI graphical user interfaces.\nIn particular, our approach embodies AI prompts as graphical interface objects and adapts the instrumental interaction model for Generative AI by considering the following three principles:\n(1) Reification of user intent into instruments: turning user-intent from varied abstractions and granularity levels into one or more reusable graphical interface object(s);\n(2) Reflection: the consideration of multiple alternatives that reflect [61] both ambiguous intents as expressed by the user (reflection-in-intent), and ambiguous interpretation of AI responses (reflection-in-response), to steer content generation towards a satisfactory result; and finally\n(3) Grounding: instantiating an instrument from a specific scope of selected content, from an example result, or even from another instrument.\nVia a technology probe [34] that implements four complementary examples of AI-instruments, we illustrate how they can ameliorate many design challenges plaguing today's linear-chat-based Al interfaces: intent formulation, prompt engineering, direct manipulation and steering, non-linear iterative workflows, and intent resolution. We also present initial reactions of 12 participants who tried our Al-instruments, yielding qualitative insights on the value and limitations of our Al-instruments interaction model, as compared to conversational prompting.\nDesigned through the lens of the three principles, we built a set of technology probes focused on image generation. The goal of these four exemplar AI-Instruments is to demonstrate the new interaction capabilities and affordances of our model: (1) Fragments decompose gen-Al prompts into reified reconfigurable objects, affording reflection-on-intent on the latent prompt structure, and grounding generation by dragging fragments from one object to another. (2) Transformative Lenses generate new content grounded in one or more content elements, which allows flexible recomposition of scenes and (if desired) continuous updates of the result. (3) Generative Containers create multiple alternatives of images, text, and even instruments or fragments. (4) Fillable Brushes encapsulate a prompt, filled by selecting example content with the brush (or by directly typing the prompt for a new action). Using the instruments in synergy-where outputs from one instrument form input for the next, or even using instruments to create new meta-instrumentsaffords expressive degrees-of-freedom for fine-grained steering of generative AI.\nIn summary, our high-level contributions include the following:\n\u2022 Extend the classic instrumental interaction model [5] to generative AI, emphasizing three driving principles: reification of user intent, reflection, and grounding;\n\u2022 Demonstrate four Al-instruments via technology probes, showing how these driving principles manifest in their design and implementation;\n\u2022 Provide initial reactions from 12 users when shifting from a linear-chat interaction paradigm to direct manipulation through Al-instruments, showing that it can address a number of human-AI interaction challenges.\nIn the following sections we discuss related techniques across the HCI, Human-AI interaction, and design literature. This is followed by an Example Walkthrough of our AI-instruments, a wider discussion of Instrumental Interaction with AI, and further details of our"}, {"title": "2 Related Work", "content": "We first discuss the state of human-AI interaction, articulating it around five core challenges. Then, we motivate the need for a more general interaction model and point to research in design and creativity grounding two principles we introduce.\n2.1 Human-AI Interaction\nRecent work explores the difficulties users face when interacting with generative AI via prompting [48, 54, 64, 78]. Earlier research identified barriers that arise (for example) in end-user programming [42] and, more generally, bridging the gulf of execution and the gulf of evaluation [51]. We organize emerging research for interaction with generative Al under five core Challenges (C1-C5) faced by users, and discuss later in the paper how our interaction model addresses each.\n(C1) Intent formulation via prompting, solely using natural language, can be challenging when the outcome is hard to describe in words. Users may lack the vocabulary to describe visual styles, or the high-level impressions they seek to achieve. Researchers studied thousands of prompts to generate images [45] to develop guidelines for prompting and parameter selections. They coupled prompting with images to offer richer multimodal intent formulation. PromptCharm [72] leveraged a large image database to help users find the right style of images and incorporated interactive techniques - such as linking a prompt fragment to the corresponding part of the generated image, to provided richer solutions for users to formulate their intent. Similarly, DesignPrompt [54] affords expressive multi-modal prompt construction. Such research seeking to expand the modalities we have to communicate with models beyond text input is particularly important [44] for multimodal outputs such as generated videos [70], 3D objects [55], and virtual worlds [59].\n(C2) Intent disambiguation is the skill of describing one's intent with enough specific detail for Al to produce the intended result. Much past work on prompt engineering across several fields of research tackles this challenge, with research probes of this issue [78] suggesting templates and guidelines [9] for users to provide the information they might have difficulty thinking about upfront. Beyond prompt engineering, the HCI community explores different representations to facilitate communicating context to the system. For example, Graphologue [37] represents a prompt as an interactive node-link diagram that users can expand and complete to incrementally add context to their intent. Such work also addresses the ambiguity of natural language by enabling users to unpack certain parts of their intent and disambiguate them by adding more information. Promptify [10] organizes generated content on a canvas based on a person's preferences and suggests alternatives - leading to an iterative loop with the user refining, selecting, and discarding alternatives of prompts and content.\n(C3) Intent resolution is the challenge users confront to determine what outcomes may or may not match their original intent. Difficulties here may stem from an ambiguity of intent in the users' mind (e.g. a user might realize \"I am not even sure what exact outcome I want\"). This problem, as a well-known attribute of challenging creative design work [12, 28, 61], is certainly not unique to AI but may be exacerbated by the relative novelty, black-box nature, and rapidly accelerating capabilities of modern Al models [11]. However, further difficulties may arise from people's lack of knowledge of what an Al model can or cannot do. Here, approaches from graphic design may help users explore possible outcomes, such as CreativeConnect [16], which extracts keywords, and text descriptions from a set of reference images and facilitate recombination and reuse. Other work shows the possibilities of what users can ask via prompt-space exploration [2], or through interfaces that reveal what results a user can generate [65].\n(C4) Steering the result of generative AI to get closer to either what the user initially imagined or to an unforeseen result assessed as satisfactory is a fundamental human-AI interaction mechanism. The topic has been studied for multiple decades in multiple field and referred to as human-in-the-loop [73] and mixed-initiative interfaces [32]. Within the context of generative AI, research on the topic has centered on human-AI co-creation [20]. Researchers developed human-AI co-creation interfaces for specific activities such as drawing [52], crafting images [17] and writing stories [18, 79]. These interfaces either surface generative AI capabilities as graphical interface elements such as a button to generate a character for a story [79], or propose custom graphical widgets to specify constraints or parameters of the content to be generated by the model such as an interactive line chart depicting the narrative arc of the story [18].\nRecent research has begun to explore more generic interaction solutions to the prompting chat-based experiences incorporated in most mainstream products today. Steering content generation in conversational prompting amounts to a linear trial-and-error process, in which users type a prompt, and then evaluate its result. They then must either rerun the same prompt to get a new result (since generative AI is non-deterministic); or edit the prompt to get an iteration over the prior result. By building upon principles of direct manipulation, DirectGPT [50] offers an early glimpse of an alternative interaction human-AI co-creation paradigm based on the principles of direct manipulation and surfaced to users with graphical widgets (e.g. buttons) that might generalize to a wider range of outputs and applications. Our research extends this ambition via instrumental interaction [5], yielding a novel interaction model that can provide the community with both evaluative (assessing novel interaction techniques) and generative (inspiring the design of novel interaction techniques) power.\n(C5) Interaction workflow models based on conversation with generative Al are inherently linear. Research started to investigate non-linear interaction workflows with generative AI. In particular, DeckFlow [19] relies on mood board type interaction and also breaks the silo of different models. Sensecape [66] and Graphologue [37] leverage additional non-linear metaphors to enable people to perform non-linear interactions with AI. These systems focus on a specific metaphor for conversation with LLMs, laying out"}, {"title": "2.2 Interaction Models in the Era of AI", "content": "Despite tremendous advances in technology and the promise of Artificial General Intelligence [11], mainstreams interfaces today feature a chat-based interface with Al reminiscent of command-line human-computer interaction paradigm of the 1960s. While the use of natural language does remove barriers of adoption for the general public, many of the limitations of communicating instructions in a linear and sequential manner by typing, later addressed by Graphical User Interfaces, pertain.\nOver the years, the HCI community has produced knowledge on human-computer interaction [31], devised principles and theories for improving interaction [25, 33], and proposed multiple interaction models [5, 36] for building the next generation of interfaces. These models are generally grounded in the emerging interfaces and techniques of the time, surfacing key principles governing them and desirable properties when humans interact with them. The goal of these models is to inform and assess the design of the next generation of interfaces. Our work has the same ambition: informing and guiding the design of interfaces leveraging generative AI. While numerous recent work centered on advancing specific use cases and application areas seeking to identify and leverage the value of generative AI - few researchers relate to existing theory and models, or proposing new theories and models in this era of AI. Perhaps the closest effort is the Cells, Generators, and Lenses model proposed by Kim et al. [39] which proposes a design framework for helping designers identify and reflect on basic building blocks needed for interfaces leveraging AI. Our research is complementary to this effort, seeking to identify interaction principles that afford direct manipulation of these building blocks.\nOur work seeks to build upon and extend the instrumental interaction model to the design of generative AI interfaces. The instrumental interaction model [5] directly builds upon direct manipulation and generalizes the use of instruments to mediate between user and objects of interests (e.g. content). It describes a large range of interaction techniques that were not captured in WIMP and direct manipulation such as lenses or tangible interactions. Recent work attempted to leverage the instrumental interaction model to design novel interactions with AI. For example, Yen and Zhao [77] used reification to turn prior conversations with Al into graphical objects or Memolets, that users can interact with. Our work propose a more general adaptation of this model to content generation with AI and expands it with additional principles of reflection and grounding."}, {"title": "2.3 Content Generation and Creativity Support", "content": "Several key insights from the design and creativity support literature [62] motivate our principles of reflection and grounding.\nDesign and creativity processes embrace ambiguity of low-fidelity prototypes [12] and rapid cycles of idea generation and evaluations [26, 28, 67] to enable people to explore many design alternatives, reflect on their possibilities through the action of sketching and building, and iterate on the most promising ones [61]. Researchers have also described these processes as sequences of divergent thinking followed by convergent thinking [21]. As fundamental working-patterns that people exhibit in challenging content creation and design tasks, there is good reason to believe such processes should persist and be supported by tools for creative content generation with generative AI. Such tools should help users rapidly investigate alternatives ideas in fluid, non-linear manner (e.g. exploration) and support the rapid iteration of the most promising content (e.g. steering). Direct manipulation and instrumental interaction models offer a compelling point of departure, affording chunking and phrasing [14] of complex generative-AI interactions for exploration and steering.\nAs hinted above, our principle of reflection builds on Sch\u00f6n's notion of reflection-in-action [61]-where the externalized materials of design \"speak to\" the designer to help them reflect-on-action as to the next design \"move\" to make within an ambiguous space of many possible ideas. In the context of generative AI, this principle of reflection conveys the notion that instruments should reflect the design space of user intent as well as the wide potential space of generated results-to help users make informed decisions (\"moves\") as they iterate towards a desired (AI-assisted) outcome. A related concept to reflection and idea incubation is the process of gathering inspirational materials in moodboards [12, 15]. Such design practices help identify concepts and themes, especially when these are hard to articulate in words, or isolate from one another other [23]. We refer to this activity in our principle of grounding, to convey the idea that instruments can extract specific aspects from a set of materials, and to then apply them to different content.\nAn aspiration for an interaction model geared on content generation is to afford power to their users [43]. In particular, vertical movement (moving up and down the abstraction ladder) afforded by natural language input of LLMs; and horizontal movement (composing tools and workflows) afforded by combining instruments together offer promising avenues for Al-instruments."}, {"title": "3 Example Walkthrough", "content": "Let us take the example of Emma, who is seeking to illustrate a social media post to express the serenity she feels when she spends time outdoors (Figure 2). She starts from leveraging generative Al to generate a bird. The art style is not quite satisfying but she is not sure what the model is capable of. She selects a generative container from a panel of AI-instruments available to her (Figure 2.1 and 2) and explores different art styles. She finds a simple drawing style she likes, and creates a second generative container to explore"}, {"title": "4 Instrumental Interaction with AI", "content": "Beaudoin-Lafon defines instruments as: \"a mediator or two-way transducer between the user and domain objects.\" we expand this definition to Al-instruments: \"an Al-powered mediator or two-way transducer between the user and domain objects.\" We describe below the three principles of our proposed model revision: reification of user intent, reflection and grounding. Note that these principles are tightly interconnected and, while differing in certain aspects from the original model also share a lot of similarities. We discuss differences in more depth in Discussion.\n4.1 Reification of User Intent\nMost pre-AI interfaces offer a finite set of functionalities, established at their design by software architects and developer. User experience designers craft a set of graphical interface components and interactions for each functionality to enable users to invoke a finite set of commands through this GUI. Today, LLMs can interpret requests from users in natural language and turn them into the execution of a specific command, or a sequence of commands, unbounding functionalities from a limited set of GUI components. With this major shift in interface design, we propose the reification of user intent, rather than commands.\nReification turns both input and output of generative AI into graphical elements that can be directly manipulated and thus reused by users. In contrast to chat-based interfaces consisting of sequences of [input+output] in which users can require to rephrase the input to iterate, reifying input and output enables users to articulate phrases of interaction [14] and afford direct manipulation techniques such as lasso selections to specify scopes of intent (Figure 4 (1-3)). In section 5, we demonstrate how this instrumental model can leverage the full range of direct manipulation techniques the community developed such as magic lenses [8] and attribute objects [74], turning them into Al-instruments encapsulating user intent.\nA key capability of generative Al models is their inherent ability to deal with the degree of abstraction of user intent. It offers unparalleled flexibility as users can express high-level or low-level"}, {"title": "4.2 Reflection", "content": "Seminal research demonstrated that it is critical to explore alternative designs early and throughout the whole process [49, 69]. It is particularly critical when working with AI because of its \"black box\" nature [4, 30], i.e. the inherent difficulty for users to understand how these models work, and the non-deterministic nature of their outputs. To capture this aspect, we borrow the term reflection from the design literature and introduce it as a principle for Al-instruments.\nWe define reflection as the ability to help users reflect on their possibly ambiguous intent (reflection-in-intent) as well as the ambiguous interpretation made by AI (reflection-in-response), and thus offer the ability to users to steer the content generation towards a satisfying result.\nReflection-in-intent is the ability of AI-instruments to surface multiple facets of their intent to users. For example, fragmenting intent into pieces reveals a particular chunking [14]. Working with fragments (Figure 5) may help users refine their intent (1), pivot on a specific aspect (2) or iterate by adding novel aspects (3).\nReflection-in-response is the ability of Al-instruments to offer multiple results of the content generation, while also helping people explore the space of possibilities (Figure 5), addressing (C3).Reflection-in-response can vary on the type and range of alternatives provided by employing diverse strategies: using model parameters such as its temperature, generating variations of the input, or asking the model to use different context of interpretation."}, {"title": "4.3 Grounding", "content": "The principle of grounding refers to the ability for users to ground instruments from examples of desired outcomes or other instruments. It may be difficult to find the right vocabulary to describe particular aspects of content, especially for images. Instruments leverage Al segmentation to (1) enable users to refer to elements of an example in generic terms, and (2) extract specific aspects of the content (e.g. style) by selection, storing the result for later (Figure 6). This builds on the notion of Variations, Parameter Spectrums, and Side Views [34, 68], but in a way that leverages the principles of interactive instruments [5, 7] as well as the open-ended possibilities of generative Al via our novel Al-instruments, rather than as views or controls with fixed, hand-designed and hard-coded options. AI-instruments can also be grounded in other instruments, enabling exploration of the space of related instruments (Figure 6 (3))."}, {"title": "5 Examples of AI-Instruments", "content": "To assess the viability of our Al instrumental model, study its differences with existing GUIs and tease out its value compare to existing chat-based Al interaction, we built a technology probe [34] with four different instruments, grounded in the literature: Fragments, Generative Containers, Transformative Lenses and Fillable Brushes. We describe below how this set of instruments surface the principles of our Al-instrumental model, as well as offer complementary interaction capabilities and affordances.\n5.1 Fragments\nFragments build on the concept of Attribute Cards introduced in Object Oriented Drawing [74], as well as Side View's notions of Variations and Parameter Spectrums [67, 68], by using a large language model to extract multiple conceptual dimensions that may be plausibly implied by a prompt.\nFragments reify an initial prompt used to generate text or image into a set of attribute cards, of the format [type, value] (where type is the category of the extracted dimension, and value is the extracted value within that dimension-such as [tone, enchanting], [content, castle] or [style, illustration]). Revealing these conceptual dimensions enables an initial reflection-in-intent, revealing the latent structure of the prompt as seen by the AI model. Commercial software such as Adobe Firefly [1] offers a similar capability as tags, enabling users to select them from a side panel for subsequent image generation. Applying the principle of reification to tags and turning them into cards affords three core novel interactions as illustrated in Figure 7.\nFirst, users can reveal fragments via a long press on the content. Fragments are fully reified as interactive instruments and are dynamically generated-hence open-ended and nondeterministic-in contrast to the fixed, hand-crafted, and hard-coded controls supported by prior work (e.g. [35, 67, 74, 75]). Second, via drag and drop, users may remove fragments (by dragging them away), or add new fragments onto existing content in the work space. Adding or removing fragments triggers regeneration of the content. Third, to further support reflection, fragments offer suggestions on demand. By tapping on ..., users can generate new variations from any fragment; these suggestions appear in a column below the specific fragment. Users can also invoke additional suggestions for more types of fragments, which are then appended to the row of fragments.\nThese three core mechanisms support a workflow where, as users work with multiple images in their workspace, they can explore the effect of different fragments via drag-and-drop to ground one image generation into an aspect of another.\nFragments use the affordance of attribute cards to break down and reify a complex intent into manageable pieces, each having distinct type and value, that enable people to work with these as more-or-less independent and composable, \"pieces of intent.\" This also encourages a workflow where users can surface useful fragmentary concepts surface that become reusable and specialized instruments in their own right. Such fragments are then available for reapplication to other pieces of content, or even reuse in a different context."}, {"title": "5.2 Transformative Lenses", "content": "Transformative Lenses re-envision the Toolglass and Magic Lens interaction technique [8] as a layered instrument that can be coupled with a generative prompt.\nLayering a Transformative Lens on top of content uses such a prompt to generate a new image that synthesizes the lens and the content. Likewise, a specific piece of image content can be used on top of a lens to recombine the two. Such layerings can be positioned and manipulated to chain multiple effects together. As illustrated in Figure 8, users can leverage lenses to take a piece of content (e.g. a sketch of a suspension bridge), and then re-compose this content within a wider backdrop scene (a city skyline), or even apply a new specific style to the results with a single interaction (e.g. a heavy, black-lined graphic novel style).\nMore generally, depending on how the user layers Transformative Lenses and image content, lenses can support image completion from a small piece of content, synthesis and composition of multiple pieces of content into a new image, or regeneration of the underlying image. Note also that blank lenses (which have no image content, but do contain a prompt) can be used. For example, a"}, {"title": "5.3 Generative Containers", "content": "Designers use moodboards [12], storyboards [29], and other techniques for presenting small-multiples in galleries [26, 49, 67] to illustrate and explore a space of possible creative directions. Structured generation of those alternatives [65] allows rapid exploration of design spaces, and techniques to highlight similarities and differences [24] facilitate the selection, refinement, and comparison of multiple responses.\nAs shown in Figure 9a, Generative Containers provide an AI-instrument that encapsulates these notions using a prompt-shown in the container's header-that is closely associated with a 2x2 smallmultiple grid of generated image results. Users can then enter or"}, {"title": "5.4 Fillable Brushes", "content": "Fillable Brushes, as illustrated in Figure 9b, offer an Al-instrument with the semantics of an \"intelligent paint brush\" for style transfer scoped to a particular spot on an existing image.\nWhile previous work has explored brushes that can encapsulate and integrate deterministic modes and commands [58], our Fillable Brushes instrument applies encapsulated AI-prompts onto content"}, {"title": "5.5 Generated Instruments and Meta-Instruments", "content": "Beyond the concept of instruments, the instrumental interaction model [5] also refers to the concept of meta-instruments, in which \"instruments operate on instruments\". As hinted at in earlier sections,"}, {"title": "6 Implementation", "content": "Overview: Our system and all AI-Instruments technology probes were implemented on a web-based platform. We use Javascript and HTML with the fabric.js [38] library for the front-end, and a Node.js [22] server for the back-end managing content and files as well as coordinating communication with the generative AI models. For the user interface design, we chose to use a sketched user interface look and feel, to encourage our study participants to focus on the concepts rather than the surface details of their specific instantiation in the UI [12].\nLeveraging Generative Al models: We use the OpenAI GPT-40 [53] model for text transformations and image analysis, and a local Stable Diffusion [63] server with a custom processing pipeline for image generation. The AI-Instruments use GPT-40 for analysis of provided input (e.g., for turning provided visual content into a text prompt, analyzing the contents of part of the workspace canvas). For image generation, we use multiple stacked ControlNet [80] models with Stable Diffusion to steer the generation of visual content. To preserve aspects of the source input, we use a combination of Depth, Canny Edge, and Scribble ControlNet models, while for preserving art/rendering styles (e.g., the sketch-based output) we use the Reference ControlNet model. Depending on the type of image generation, we vary the weight of each ControlNet model (e.g., increasing weight to emphasize content preservation, or decrease weight of another ControlNet to reduce affect of reference style transfer). We use image masks to selectively control which areas are changed or kept, apply inpainting/outpainting scripts, and adjust other parameters such as CFG scale, denoising strength, and control mode."}, {"title": "7 Study", "content": "We conducted a qualitative user study with 12 participants to gather their insights on Al-instruments compared to traditional prompting. Participants completed image generation and editing tasks with our four technology probes as well as an initial chat-based prompting probe, to help them tease out pros and cons of these two different interaction models. We analyzed their comments to understand the perception of the principles of reification, reflection and grounding, as well as capture insights on the Human-AI interaction challenges (listed in Section 2.1) addressed by different instruments.\n7.1 Procedure\nParticipants completed a 60-minute study in a quiet room on a computer running our technological probes and a study form. After obtaining informed consent, we collected basic demographic information, then requested participants to complete a set of tasks with our technological probes. All participants first completed tasks with a chat-based prompting probe using the same image generation model as the instruments in order for us to confirm their familiarity with prompting and to also provide them with a baseline for the image generation model with use. Before each instrument, participants watched a video demonstration explaining functionalities and modalities of interaction. After this video, participants used the technological probes to complete 2 to 3 tasks such as generating an image and changing its style. We provided example content for each task, but encouraged participants to generate their own content and try different ideas. The experimenter only interacted"}, {"title": "7.2 Participants", "content": "We recruited 12 participants (8 men, 3 women, 1 non-binary) via mailing lists in a large organization. We selected participants with weekly interaction with generative AI systems (ChatGPT, stable diffusion, etc) for creating content. As we aimed at gathering insights on overarching interaction principles for Al-instruments (across many domains), we opted for selecting participants with interesting in different types of content generation. Our participant pool included users interested in authoring short text snippets such as emails, long structured documents such as reports, structured text such as tables, programming code and web-pages, visual artifacts such as images, and multimodal artifacts with text image and charts such as presentations. Note that none of our participants generated audio or video."}, {"title": "7.3 Material and Analysis", "content": "We collected the salient advantage and salient weakness for each instrument compared to prompting. Participants experienced the following probes: 1) chat-based prompting, 2) fragments, 3) containers, 4) lenses, and 5) brushes. To encourage participants to think of different aspects of content generation, we asked them their preferred interaction technique (along with their rationale) for five different tasks.\nT1 Combining content: merging pieces of content together\nT2 Splitting content: extracting a piece of content\nT3 Iterating on content: editing an aspect of content\nT4 Editing by example: transferring content or style\nT5 Expanding content: adding new material to existing content\nWe coded a total of 156 statements from our participants to gain insights on their perception of our model's principles and to assess how Al-instruments (un)successfully addressed the five challenges described in section 2.1. A portion of these comments (28/156) also revealed limitations of our technical probes (the codebook is available at https://hugoromat.github.io/ai_instruments/)."}, {"title": "7.4 Insights on Model Principles", "content": "Reification of intent. All 12 participants reacted positively to the principle of reifying intent into Al-instruments. Participants valued that Al-instruments enabled them to shift the focus from the prompt to the outcome. P6 commented that \"I can just click on the fragments instead of typing it out and focus on the final output instead.\". P10 noted it was helping them with the iterative process:"}, {"title": "7.5 Challenges Addressed by AI-instruments", "content": "C1 Intent formulation. Participants highlighted grounding to address intent formulation on images, especially as instantiated in Fillable Brushes for extracting and transferring styles. P7 summarized how Brushes help formulate intent with both scope of selection and direct manipulation: \"I can control a more fine grand area/mask that I want to edit. It's so cool! I can also copy the style/content from a different image and directly apply it to a different image without having to find the accurate words to describe them.\"\nC2 Intent disambiguation. Participants identified grounding and reflection as core principles helping them disambiguate their intent. They explained that the reflection-in-intent surfaced in Fragments was helping them identify context and details to refine their intent. P5 also noted that Fragments enabled to experiment and gain an understanding of how the model worked \"allow[ing] me to see how the prompts were being isolated and used\". A few participants also explained that Brushes helped with intent disambiguation by capturing one by example and expressing it words. P1 summarized: \"one advantage [of brushes] is that it can identify a style, even though I cannot articulate the style well, this is especially helpful to circumvent prompt engineering.\"\nC3 Intent resolution. Participants outlined the principles of reification and reflection as most useful for resolving intent. Participants explained that reification enabled them to explore different paths by iterating on different images: \"this lets you build off of previous iterations and you can create different styles for the same image until you are happy with one\". They highlighted the benefit of reflection-in-response of Generative Containers to help them explore possibilities: \"I don't have to think too much about what I want, which is helpful\" (P10), \"Very easy to explore creative options, it helps to get a better idea of what you like\" (P12). P7 valued the ability to start from high-level prompts and follow up by making a selection \"It it easy to generate images with a vague description, and offer options [to explore]\". A few participants also mentioned the benefit of reflection-in-intent of Fragments to experiment with options: \"[Fragment] also suggests some dimensions to change the picture which I might have not thought about. Like the elephant in this case.\" (P9).\nC4 Steering. All participants mentioned the benefit of instruments over chat-based prompting for steering content. Participants noted that direct manipulation and scope of selection afforded by reification were critical in editing portions of content, in conjunction with grounding to capture and transfer aspects of one content to another. Many participants outlined the value of Brushes and Lenses to generate masks for steering content generation \"masking [with lenses] allows me to personalize smaller parts of the image compared to generating a completely new one\" (P10), \"this [brush] is an advanced way to mask out single things within an image to regenerate\" (P3). P6 summarized the benefits of these instruments compare to chat-based prompting: \"[with brushes] I can quickly make modifications to the image instead of writing a prompt from scratch for every modification. I can focus on the subject I'm interested in.\""}, {"title": "8 Discussion and Future Work", "content": "We first discuss how our model revisits and extends the classic instrumental interaction model", "5": "is based on three core principles [7"}]}