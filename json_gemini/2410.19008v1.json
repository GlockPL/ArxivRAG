{"title": "TEACH MULTIMODAL LLMS TO COMPREHEND ELECTROCARDIOGRAPHIC IMAGES", "authors": ["Ruoqi Liu", "Yuelin Bai", "Xiang Yue", "Ping Zhang"], "abstract": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.", "sections": [{"title": "1 INTRODUCTION", "content": "The electrocardiogram (ECG) is an essential tool in diagnosing cardiovascular diseases due to its non-invasive, cost-effective, and widely accessible nature for assessing cardiac function. While some approaches have been proposed for automatic ECG diagnosis (Hannun et al., 2019; Ribeiro et al., 2020; Hughes et al., 2021), these are primarily designed for classification tasks with limited cardiac conditions, often lacking generalizability. Moreover, they typically treat ECG data as time-series physiological signals, which may not always be available, particularly in resource-constrained settings where only printed or digital images are accessible (Sangha et al., 2022; 2023).\nRecent advancements in multimodal large language models (MLLMs) have shown impressive success across vision-language tasks, offering new possibilities for addressing the limitations of traditional ECG models. However, applying MLLMs to ECG interpretation is not straightforward. As illustrated in Fig. 1, current MLLMs, such as GPT-40 (OpenAI, 2024), often provide responses that appear correct and contextually relevant but are ultimately inaccurate in interpreting ECG images. This highlights the need for specialized MLLMs for ECG image interpretation.\nDeveloping MLLMs for ECG images faces several challenges. First, no large-scale ECG image datasets are currently available as most ECG datasets contain only raw signal data, which needs to be synthesized into digital images. Second, there is a lack of instruction tuning datasets for ECG images. Large high-quality instruction tuning datasets, which are crucial for MLLM development, need to be curated from scratch for ECG-related tasks. Finally, evaluation is just as critical as model development, yet no established benchmark exists for assessing MLLM performance in ECG image interpretation. A well-defined benchmark is essential for both quantifying model performance and identifying areas for future improvement.\nIn this paper, we tackle these challenges by introducing ECGInstruct, the first large-scale ECG image instruction tuning dataset containing over one million ECG image-text samples. ECGInstruct is characterized by: 1) realistic image synthesis that replicates artifacts commonly seen in paper-based ECGs, 2) a diverse range of ECG-related tasks refined with insights from clinical experts, and 3) data sourced from distinct geographic regions. Leveraging ECGInstruct, we develop PULSE, an MLLM for ECG image comprehension. To evaluate PULSE, we present ECGBench, a comprehensive evaluation benchmark covering four ECG image interpretation tasks across nine different datasets. ECGBench includes repurposed tasks (e.g., abnormality detection) from existing datasets, and newly curated, more challenging tasks using real-world ECG images.\nEvaluated on ECGBench, PULSE sets a new state-of-the-art, significantly outperforming proprietary MLLMs across all benchmarks with an average accuracy gain of 15% to 30% compared to GPT-40 on out-of-domain datasets (Fig. 1). Ablation experiments demonstrate the importance of incorporating diverse data sources and ECG instruction tasks into the training data. A case study and discussion further illustrate the model's effectiveness in ECG image interpretation.\nTo summarize, our main contributions are as follows,\n\u2022 Problem. We investigate the capabilities of MLLMs in ECG image interpretation and evaluate their performance across various downstream tasks. To the best of our knowledge, this is the first study focused on assessing MLLMs in image-based ECG interpretation.\n\u2022 Dataset. We construct ECGInstruct, a large-scale ECG image instruction tuning dataset consisting of a wide range of ECG-related tasks, serving as a valuable resource for fine-tuning MLLMs for ECG image interpretation.\n\u2022 Model. We develop PULSE, a new MLLM tailored for ECG image interpretation. The model achieves state-of-the-art performance, outperforming both proprietary and open-source MLLMs.\n\u2022 Evaluation. We establish ECGBench, a comprehensive benchmark for evaluating ECG image interpretation, which includes diverse evaluation tasks, both real-world and synthesized images."}, {"title": "2 ECGINSTRUCT: TEACH MLLMS TO COMPREHEND ECG IMAGES", "content": "We aim to curate a list of multifaceted instruction tuning datasets for ECG analysis that are featured by 1) realistic image synthesis resembling the artifacts in paper ECGs, 2) diverse types of ECG-"}, {"title": "2.1 ECG IMAGE SYNTHESIS WITH VARIOUS DISTORTIONS", "content": "To enhance the robustness and real-world applicability of our model, we synthesize ECG images mimicking common artifacts found in paper ECGs. We adopt an ECG image synthesis tool (Shiv-ashankara et al., 2024) that provides various imperfections such as grid line interference, creases, wrinkles, paper rotations, etc. By including these synthesized artifacts, we aim to train models that"}, {"title": "2.2 ECG-RELATED TASKS WITH CLINICAL EXPERTS' INSIGHTS", "content": "To construct a comprehensive set of ECG-related tasks, we consulted domain experts to curate diverse and clinically relevant tasks covering four different categories. Each category is designed to address specific aspects of ECG interpretation and analysis, including (1) basic feature recognition (see examples in Appendix Fig. A1), (2) heart rhythm analysis (see examples in Appendix Fig. A2), (3) morphology and pathology identification (see examples in Appendix Fig. A3) and (4) clinical report generation (see examples in Appendix Fig. A4). Basic feature recognition (e.g., interval or segment, etc.) forms the foundation of ECG interpretation, enabling the model to grasp essential cardiac parameters. Heart rhythm analysis (e.g., arrhythmias, conduction abnormalities, etc.) and morphology and pathology identification (e.g., wave shape, pathological conditions, etc.) are more advanced and critical aspects of ECG analysis, ensuring that the model can detect and classify complex conditions accurately. Lastly, clinical report generation mirrors the process of healthcare professionals synthesizing a comprehensive interpretation of an ECG. By incorporating clinical experts' insights, we encourage the model to learn the practical skills required in a clinical context."}, {"title": "2.3 DIVERSE TYPES OF TASKS AND DATA SOURCES", "content": "Based on the original diagnoses and clinical reports from the existing ECG datasets, we curate diverse types of tasks including multi-choice questions, fill-in-the-blank, close-ended QA, and open-ended QA. This variety of task types not only enhances the model's versatility but also mimics the diverse cognitive processes involved in real-world ECG interpretation. By incorporating these varied task types, we aim to develop a more robust and adaptable model capable of handling a wide spectrum of ECG-related queries and analyses.\nTo ensure broad applicability and generalizability, we collect ECG data from four different sources across geographically distinct regions: 1) PTB-XL (Wagner et al., 2020): a Germany-based, publicly available repository; (2) MIMIC-IV-ECG (Gow et al., 2023): a large set of ECGs for patients who appear in the MIMIC-IV Clinical Database from Beth Israel Deaconess Medical Center in Boston (Johnson et al., 2023); 3) CODE-15% (Ribeiro et al., 2021): an ECG dataset from a central ECG repository from Minas Gerais, Brazil under the clinical outcomes in digital electrocardiology (CODE) study (Ribeiro et al., 2019); 4) ECG-QA (Oh et al., 2024), a question answering dataset for ECGs that is constructed based on PTB-XL (Wagner et al., 2020). This diverse geographical representation enhances the model's ability to generalize across different populations and healthcare systems, accounting for potential variations in ECG patterns and interpretations across regions."}, {"title": "2.4 DATA SYNTHESIZING AT SCALE", "content": "Since large-scale annotation of ECG features is extremely expensive and time-consuming, we develop an automatic data synthesizing pipeline to address this data scarcity issue. We utilized clinical reports from PTB-XL and MIMIC-IV-ECG as initial seed data and leveraged an advanced LLM (i.e., Llama-3-70B-Instruct) for data synthesis. Building upon the expert-in-the-loop process and diverse data resources described in the previous sections, we synthesized a substantial volume of ECG-related instructions and corresponding responses. These were based on expert-provided examples and real-world scenarios, with the specific prompts used in this process detailed in the Appendix E. For datasets lacking comprehensive reports, such as CODE-15%, we manually constructed diverse templates to transform the existing data into an instruction-response format."}, {"title": "2.5 QUALITY CONTROL", "content": "To guarantee the quality of generated instructions and corresponding responses, we apply an independent LLM as a judge to evaluate and score the content. This process involves several steps: 1) initial generation: instructions and responses are first generated using our primary model; 2) evaluation criteria: we establish a set of evaluation criteria including the instruction relevance, clarity, answerability of the responses, etc; 3) LLM judge and scoring: an independent LLM (Llama 3 (Meta, 2024)) is used as a judge to assess each instruction-response pair against established cri-"}, {"title": "2.6 TRAINING", "content": "Our model architecture closely follows that of LLaVA (Liu et al., 2024b;c), adapting it for ECG image analysis. We use a vision encoder to process ECG images and a large language model as the text decoder, connected via a projection layer. We organize the data into three components: the image, the instructions, and the outputs. The instruction is query or task related to the ECG image and the output is the expected response or prediction base on the image and instruction. We place the image at the beginning of each conversation, serving as the visual grounding for the entire dialogue. During training, we freeze the parameters of the vision encoder while updating the parameters of the projection layer and the language model using an autoregressive training objective, where we mask all the tokens belonging to the image and the instruction."}, {"title": "3 ECGBENCH", "content": "In this section, we present ECG-Bench (Fig. 3), a comprehensive benchmark for evaluating MLLMs on ECG image interpretation. Our benchmark contains both repurposed tasks from six existing datasets and newly created tasks from external resources. Table 2 shows the details of each evaluation dataset. We introduce the detailed evaluation task curation process below."}, {"title": "3.1 EVALUATION TASK CURATION", "content": "Abnormality Detection. This task focuses on detecting cardiac abnormalities using ECG images. We curate this task by repurposing six existing ECG datasets: three in-domain datasets: PTB-XL (Super) (Wagner et al., 2020), CODE-15% (Ribeiro et al., 2021), ECG-QA (Oh et al., 2024), and three out-of-domain datasets: CPSC 2018 (Liu et al., 2018), CSN (Zheng et al., 2020a;b) and G12EC (Liu et al., 2018). For all datasets, we first synthesize images using raw signals and then curate queries based on the original diagnosis and reports. For datasets with fewer than 10 diagnostic labels, we curate close-ended questions. For those with more labels, we construct multi-choice questions with 8 options, including the original diagnosis and randomly sampled negative labels.\nReport Generation. This task involves generating detailed reports for given ECG images. We benchmark using 500 randomly selected reports from the test set of PTB-XL, which contains high-quality ECG reports written and validated by cardiologists. Similarly, the ECG images are synthesized from the raw signals. For the ground truth reports written in non-English (PTB-XL is a Germany-based dataset), we translate the reports into English before the evaluation.\nMMMU ECG. Inspired by MMMU (Yue et al., 2024), a widely adopted evaluation benchmark for MLLMs, we manually curated an ECG version with 200 multi-choice questions with the help of medical school students. The curation process involved three key steps: (1) Resource Selection: We gathered ECG materials from diverse and reliable sources such as ECG textbooks, clinical case reports from medical journals, and widely used online ECG learning materials. This ensures the comprehensiveness and quality of collected ECG examples and interpretations. (2) Question Creation and Collection: Five medical school students with basic knowledge of ECG were recruited for this task. They extracted existing questions from the collected resources. For ECG images accompanied only by clinical interpretations, the annotators created questions based on these interpretations. Additionally, they formulated new questions drawing from their expertise, ensuring a balance between various ECG interpretation aspects (e.g., rhythm analysis, morphology assessment, clinical interpretation). (3) Quality Control: To maintain high standards, we implemented a quality control process. In particular, Each question underwent review by at least two other annotators, checking for accuracy and clarity. An independent reviewer cross-checked the final images, questions, and corresponding answers against the original sources to ensure fidelity to the source material. Any discrepancies or ambiguities were resolved during this process.\nECG Arena. To assess the model's instruction-following ability in ECG comprehension, we developed ECG Arena, inspired by MT-Bench (Zheng et al., 2024) and Arena-hard (Chiang et al., 2024) used in general LLM chat evaluations. We manually curated 50 multi-turn ECG-related questions, focusing on open-ended interactions. The data curation process for ECG Arena, like MMMU ECG, involves three main steps: resource selection, question creation, and quality control. The key distinction is that MMMU ECG focuses on multiple-choice questions, whereas ECG Arena involves more complex, flexible multi-turn, open-ended questions. Each follow-up question is contingent on the initial question and its response, making the process more challenging and reflective of real-world applications. Since multi-turn conversations are rare in existing sources, this posed significant challenges during data curation. To address this, annotators created such conversations by referencing original clinical interpretations and ECG images. The questions are designed to feel natural and"}, {"title": "3.2 EVALUATION METRICS", "content": "Abnormality Detection: We use macro AUC, macro F1, and hamming loss (HL) for multi-label datasets, and accuracy for others. Report Generation: We employ GPT-40 as a judge, evaluating reports based on rhythms, waveform, and diagnosis, with a maximum score of 100 points (see evaluation prompt in Appendix Fig. A9). MMMU ECG: We use accuracy as the primary metric, with systematic, rule-based evaluation pipelines to ensure consistent scoring. ECG Arena: GPT-40 assesses model performance by comparing generated responses with ground truth answers, considering accuracy, completeness, and instruction adherence, with a maximum score of 100 points (see evaluation prompt in Appendix Fig. A10). More evaluation details are provided in the Appendix F."}, {"title": "4 EXPERIMENTS", "content": "4.1 \u041c\u0415\u0422\u041dODS FOR COMPARISON\nIn order to evaluate the performance of our proposed model, we compare it against a set of established methods including domain-specific methods and state-of-the-art MLLMs.\n\u2022 Domain-specific Methods: We consider four domain-specific methods for ECG including three signal-based methods: METS (Li et al., 2024c), MERL (Liu et al., 2024a), ST-MEM (Na et al., 2023), and one image-based method: ECG-GPT (Khunte et al., 2024).\n\u2022 Proprietary MLLMs: We consider three proprietary MLLMs: GPT-40, GPT-40 mini (OpenAI, 2024), Gemini 1.5 Pro (Reid et al., 2024), and Claude 3.5 Sonnet (Anthropic, 2024).\n\u2022 Open-source MLLMs: We select a range of open-source models to ensure comprehensive coverage across different model sizes and visual components, including the general models LLaVA-1.5 (Liu et al., 2024d;b), LLaVA-1.6 (Liu et al., 2024c), Phi-3-Vision Abdin et al. (2024), Idefics2-8B (Lauren\u00e7on et al., 2024), DeepSeek-V1-7B (Lu et al., 2024a), Mantis-8B-siglip-Llama3 (Jiang et al., 2024), MiniCPM-V-2.6 (Yao et al., 2024), InternVL2 (Chen et al., 2023; 2024) and state-of-the-art multimodal models LLaVA-OneVision (Li et al., 2024a), Qwen2-VL (Wang et al., 2024), as well as the domain-specific models LLaVA-Med (Li et al., 2024b)."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "We follow the architecture of LLaVA-v1.6-Vicuna-7B, which includes three core components: a vision encoder, a large language model, and a projector to align image and text modalities. We format all datasets into a chatbot-style multi-turn dialogue format and use the special token \u201c<image>\u201d to represent image features within the text data. We utilize anyres to support the model's ability to recognize ECG images of various sizes that may appear in real-world scenarios. We freeze the parameters of the vision encoder and fine-tune all parameters of the projector and LLM. We use a learning rate of 2e-5, set the batch size to 128, and employ a cosine scheduler with a 5% warm-up period for three epochs."}, {"title": "4.3 MAIN RESULTS", "content": "We show in-domain the out-of-domain results in Table 3 and Table 4 respectively. Overall, we observe that PULSE achieves state-of-the-art performance on different datasets and tasks.\nResults on In-domain datasets. As shown in Table 3, PULSE demonstrates significant improvements over both proprietary and open-source MLLMs across all in-domain datasets. Specifically, PULSE surpasses the best proprietary model (GPT-40) with a 27% improvement in AUC, an 11-point gain in report score, and a 39% increase in accuracy on the PTB-XL Super, PTB-XL Report, and ECG-QA tasks, respectively. Moreover, PULSE achieves notable gains over the best open-source model, with a 28% improvement in AUC, a 12-point gain in report score, and a 44% increase in accuracy on the same tasks."}, {"title": "4.4 ABLATION STUDY", "content": "Effect of Training Data Source. Given that ECGInstruct is compiled from diverse datasets, it is crucial to examine how each dataset contributes to the model's overall performance. Table 5 presents a comparative analysis of models trained on various dataset combinations. The model trained exclusively on PTB-XL (P) exhibits the lowest performance across all datasets, indicating the limitations of relying on a single data source for effective generalization. As we progressively incorporate additional datasets into the training set, the model's performance consistently improves. These results highlight the importance of curating diverse training data, as expanding beyond a single source enhances the model's capacity to generalize across datasets and tasks.\nEffect of Instruction Task. To understand the individual contribution of each ECG-related task to model performance, we analyze combinations of four instruction tasks. As shown in Table 6, adding more tasks progressively improves performance across multiple benchmarks. Models trained solely on basic feature recognition (F) performed poorly across all metrics, highlighting the limitations of a single-task approach. In contrast, the sequential addition of tasks led to substantial performance gains across multiple benchmarks. The model incorporating all four tasks achieved the highest performance, indicating a more comprehensive understanding of ECG images."}, {"title": "4.5 CASE STUDY", "content": "We further present some examples from our benchmark, comparing the outputs of our model with GPT-40 for ECG report generation (Appendix Figs. A11-A13) and ECG Arena (Appendix Fig. A14). While GPT-40 is capable of generating reports and answering questions by following instructions, it often produces responses that, although well-structured and seemingly relevant, contain significant inaccuracies in interpretation. In contrast, PULSE consistently provides more accurate responses that align closely with the ground truths. Additionally, we observed that GPT-40 tends to over-rely on its OCR capabilities when textual information (e.g., printed axis labels, numerical values like heart rate or QRS duration) is present in images, leading to superficial reasoning based on text rather than a deep analysis of visual data. As shown in Appendix Fig. A13, GPT-4o identifies a left axis deviation based on the printed QRS axis degree, without analyzing the visual waveform patterns. If such axis information were absent, the model would likely fail to identify the deviation."}, {"title": "4.6 DISCUSSION", "content": "While the model demonstrates superior performance across various evaluation datasets, it faces notable challenges with more complex and open-ended tasks, such as report generation and multi-turn conversations. To further investigate the model's performance in report generation, we present the score breakdown in Fig. 4. The model excels in rhythm interpretation but struggles with waveform and diagnosis identification. These results suggest that future efforts should prioritize increasing the dataset's coverage of waveform and diagnosis-related cases to enhance the model's ability to detect these abnormalities. Additionally, as diagnosis identification may require more advanced multi-step reasoning, future research could focus on incorporating step-wise instruction tuning data to strengthen the model's reasoning capabilities."}, {"title": "5 CONCLUSION", "content": "In this paper, we study the problem of ECG image interpretation, which is a crucial task in assessing cardiac conditions. We develop a new MLLM, PULSE, fine-tuned on the newly created ECGInstruct dataset with over 1 million samples across a diverse range of ECG-related tasks. Evaluated on the proposed benchmark, ECGBench, our model shows state-of-the-art performance, surpassing both proprietary and open-source MLLMs across multiple in-domain and out-of-domain evaluation datasets. This work demonstrates the potential of using MLLMs for enhancing ECG image analysis and interpretation in clinical applications."}, {"title": "A RELATED WORK", "content": "Domain-specific Models for ECG. Many domain-specific models have been proposed to enhance automatic ECG diagnosis (Hannun et al., 2019; Ribeiro et al., 2020; Hughes et al., 2021). For example, Ribeiro et al. (2020) applied convolutional neural networks (CNNs) to encode ECG signals for diagnosing 6 types of abnormalities. To reduce dependence on high-quality labeled data, recent studies (Li et al., 2024c; Liu et al., 2024a; Na et al., 2023) have further explored self-supervised learning approaches using unlabeled ECG training data. For example, Liu et al. (2024a) proposed an ECG representation learning framework by integrating the ECG signals and clinical reports, showing improved performance in zero-shot ECG classification tasks. Despite these successes, most approaches treat ECG data as temporal physiological signals, which could be limiting in certain resource-constrained or remote settings where only printed or digital images are available. Recently, a few methods (Sangha et al., 2022; 2023; Khunte et al., 2024) have been proposed for ECG diagnosis using ECG images. For example, Khunte et al. (2024) developed a diagnostic report generation framework for ECG images, which is built upon a BEiT (Bao et al., 2022) vision transformer encoder and a GPT-2 (Radford et al., 2019) decoder. However, their model is only capable of the clinical report generation task, without generalizability to other diverse tasks. In contrast, our study investigates the capabilities of MLLMs for ECG image interpretation. We curate a diverse range of instruction tuning datasets to fine-tune the model, thus improving model generalizability.\nMLLMs in Healthcare Recent advancements in MLLMs have shown promising results in various healthcare domains. General medical multimodal models such as LLaVA-Med (Li et al., 2024a), MedPaLM (Singhal et al., 2023a;b), and Med-Gemini (Saab et al., 2024) have demonstrated capabilities in processing diverse medical data types. Additionally, domain-specific multimodal models have been developed for specialized fields like pathology (Lu et al., 2024b; Xu et al., 2024) and radiology (Wu et al., 2023). These models have shown potential in integrating visual and textual information to support clinical decision-making and medical education. However, despite the importance of ECG data in cardiac diagnosis and monitoring, current MLLMs often struggle to process ECG images effectively. This limitation highlights a significant gap in the application of MLLMs to cardiology, where the ability to interpret both visual ECG representations and accompanying clinical information is crucial.\nMultimodal Instruction Tuning. Instruction tuning has proven effective in the multimodal domain, particularly in vision-language models like LLaVA (Liu et al., 2024d), MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023). These models demonstrate impressive generalizability across various visual understanding and reasoning tasks. While multimodal instruction tuning has been applied to general medical imaging tasks (Li et al., 2024b; Singhal et al., 2023a), its application to ECG images remains largely unexplored. A recent work (Wan et al., 2024) introduced a targeted"}, {"title": "B PRELIMINARY ON 12-LEAD ECG", "content": "ECG is a vital diagnostic tool that measures the electrical activity of the heart over time, providing insights into both spatial and temporal aspects of cardiac function. Typically, an ECG recording is presented as a 12-lead multivariate time series, where each lead offers a unique perspective on heart activity. The six limb leads (I, II, III, aVR, aVL, and aVF) assess the electrical movements across the arms and legs, giving views from the frontal plane. Simultaneously, the six precordial leads (V1, V2, V3, V4, V5, and V6) monitor the chest, offering horizontal plane views. In this paper, we focus on ECG images that are synthesized from raw signals."}, {"title": "C DETAILS OF ECG IMAGE SYNTHESIS", "content": "We employ the ECG-image-kit (Shivashankara et al., 2024) framework to synthesize diverse ECG images from raw signal data. This toolkit allows for the generation of ECG images under various conditions by introducing a range of distortions and noises to better simulate real-world clinical data.\nSpecifically, in addition to generating standard 12-lead ECG images-characterized by black waveforms on a white background, red grid lines, and a 4x3 layout-we introduce a variety of perturbations to the images. These modifications include the addition of wrinkles and creases, simulating the physical wear and tear commonly observed in paper-printed ECGs. Our image synthesis process includes various augmentation methods to simulate physical distortions, image quality variations, and layout alterations. We introduce wrinkles and creases to mimic wear and tear commonly observed in paper-printed ECGs, and apply random rotations at varying angles to simulate misaligned scans or prints. To account for different acquisition systems and scanning qualities, we vary image resolutions and introduce random background colors, such as slight yellowing to represent aging or poor scanning quality. We also add noise to the images to simulate imperfections in the scanning or printing process. Furthermore, we experiment with different aspect ratios, overall image sizes, and ECG plot positions within the image to reflect the heterogeneity of ECG printouts across different systems and formats. In some cases (with a 0.02 probability), we randomly remove grid lines to account for variations in ECG presentation.\nTo further enrich the synthetic images, we randomly insert meta-information into the image header to simulate the annotations typically seen in clinical ECG reports. For the PTB-XL dataset, we extract patient demographics (e.g., age, gender) and basic ECG features (e.g., heart rate, axis deviations) from the associated PTB-XL feature annotation dataset, PTB-XL+ (Strodthoff et al., 2023). This extracted data is used to impute realistic meta-information, which is then randomly printed on the synthesized image. This random insertion of meta-data not only increases the visual variety of the images but also provides additional context, simulating real-world ECG prints that include patient and diagnostic information. To further increase diversity, we adopt alternative lead configurations beyond the standard 4x3 layout, such as 12x1 (single row of leads), 6x2 (two rows of six leads), and other commonly used clinical formats. These variations ensure that our model is exposed to a wide range of ECG presentation styles.\nThe augmentation process is designed to balance the dataset, with an approximate ratio of 1:1 between augmented and standard ECG images. This balance ensures that the model is exposed to both clean and distorted images, aiding in its generalization to real-world clinical scenarios."}, {"title": "D DETAILS OF INSTRUCTION TUNING DATASETS", "content": ""}, {"title": "E PROMPTS", "content": "Prompt: Multi-task Data Synthesizing\nYour task: Create a complex ECG visual task based on the given report and target task type:\nGuidelines for task creation:\n1. Design a concise yet challenging graduate-level task that requires deep reasoning.\n2. Frame the task as interacting with an actual ECG image, without mentioning the report. Make the task visually centric, assuming direct ECG image analysis.\n3. Strictly base all information on the given ECG report only. Avoid tasks and answers that are inconsistent with the report.\n4. Avoid restating the report or using phrases like \"As described in the report.\"\n5. Generate one task from a diverse range of task types, including but not limited to:\nDirect questions (e.g. \"What is the heart rhythm?\")\nHypothetical scenarios (e.g. \"Imagine you're an ER doctor reviewing this ECG...\")\nComparative tasks (e.g. \"How does this ECG differ from a normal sinus rhythm?\")\nExplanation requests (e.g. \"Explain the significance of the QS complexes seen in V2.\")\nProblem-solving scenarios (e.g. \"Given these ECG findings, what further tests might you order?\")\nEducational prompts (e.g. \"Teach a medical student about the key features of this ECG.\")\nRole-playing scenarios (e.g. \"You're consulting with a cardiologist about this ECG. What do you tell them?\")\nDecision-making tasks (e.g. \"Based on this ECG, would you clear this patient for surgery? Why or why not?\")\n6. Specify a clear, appropriate output format within the task instructions(free-form, \"think-step-by-step\", direct output the short answer(in one phrase or one sentence), JSON format, table, list, different delimiters(such as commas, semicolons, numeric order), etc.). Do not limited to the given task type and format, you have the freedom to design any type of task you deem appropriate.\n7. Focus the task on one or more of the following ECG analysis aspects:\na. Basic ECG feature interpretation (e.g. heart rate, rhythm, cardiac axis)\nb. Diagnosis and classification (e.g. diagnosis identification, waveform classification, rhythm classification)\nc. Waveform and interval analysis (e.g. P wave morphology, PR interval, QT interval, QRS complexes, T wave morphology)\n8. Ensure the task complexity aligns with the given report's information.\nAfter creating the task:\n1. Provide a detailed, accurate answer to your own task.\n2. Ensure your answer is comprehensive and strictly based on the report.\n3. Strictly follow the output format and requirements specified in your task instructions.\nECG Report: {report}\nTarget Task Type: {target}\nPresent your work in this format:\nTask: [Concise content of the ECG tasks, including required output format. Do not include phrases like \"Output format:...\" or like \"[Insert image here]\", but in more natural expression. ]\nResponse: [Comprehensive response following the task's requirements, strictly based on the report]\nDo not include any content outside of the Task and Response sections."}, {"title": "Prompt: Multi-turn Dialogue Synthesizing", "content": "Your task: Create a 2-4 turn dialogue between a medical professional and an Al assistant analyzing an ECG", "report": "nGuidelines for dialogue creation:\n1. Design a series of questions and answers that progressively explore the ECG findings in depth", "As described in the report,\" \"The report mentions,\" or \"The term...\" The dialogue should not appear to reference an external report.\n5. Begin with direct questions about basic ECG features, then progress to more complex interpretations and clinical implications.\n6. Include a mix of question types, with an emphasis on direct questions": "n- Direct questions (e.g.", "What are the main ECG features?\", \"What is the heart rhythm?\")\n- Requests for explanations (e.g., \"Can you explain the significance of the QS complexes?\", \"What the cause of these features?\")\n- Clinical reasoning questions (e.g., \"Given these findings, what's your diagnosis?\")\n- Hypothetical scenarios (e.g., \"How would you manage a patient presenting with this ECG?\")\n7. Focus the dialogue on one or more of the following ECG analysis aspects": "na. Basic ECG feature interpretation (e.g. heart rate", "dialogue": "n1. Provide extremely comprehensive and detailed answers from the AI assistant's perspective. Each response should thoroughly cover all relevant aspects of the question asked.\n2. Ensure all answers are comprehensive and strictly based on the report, without explicitly referencing it.\n3. Make the dialogue flow naturally, as if a real user is progressively exploring the ECG findings."}]}