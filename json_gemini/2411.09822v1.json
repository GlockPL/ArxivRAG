{"title": "A Self-Supervised Model for Multi-modal Stroke Risk Prediction", "authors": ["Camille Delgrange", "Olga Demler", "Samia Mora", "Bjoern Menze", "Ezequiel de la Rosa", "Neda Davoudi"], "abstract": "Predicting stroke risk is a complex challenge that can be enhanced by integrating diverse clinically available data modalities. This study introduces a self-supervised multimodal framework that combines 3D brain imaging, clinical data, and image-derived features to improve stroke risk prediction prior to onset. By leveraging large unannotated clinical datasets, the framework captures complementary and synergistic information across image and tabular data modalities. Our approach is based on a contrastive learning framework that couples contrastive language-image pretraining with an image-tabular matching module, to better align multimodal data representations in a shared latent space. The model is trained on the UK Biobank, which includes structural brain MRI and clinical data. We benchmark its performance against state-of-the-art unimodal and multimodal methods using tabular, image, and image-tabular combinations under diverse frozen and trainable model settings. The proposed model outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in ROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6% increase in balanced accuracy compared to the best multimodal supervised model. Through interpretable tools, our approach demonstrated better integration of tabular and image data, providing richer and more aligned embeddings. Gradient-weighted Class Activation Mapping heatmaps further revealed activated brain regions commonly associated in the literature with brain aging, stroke risk, and clinical outcomes. This robust self-supervised multimodal framework surpasses state-of-the-art methods for stroke risk prediction", "sections": [{"title": "1 Introduction", "content": "Stroke ranks as the second leading cause of death worldwide, responsible for 11.6% of global fatalities in 2019. It often results in neurological damage and long-term disability in adults, imposing significant health and economic challenges. Early detection through predictive models is crucial in preventing severe outcomes, as cerebrovascular events can cause irreversible brain damage within hours. The complexity of stroke, driven by multiple risk factors, highlights the importance of integrating multi-modal data to improve diagnostic accuracy and treatment strategies. Among the various imaging techniques, Magnetic Resonance Imaging (MRI) stands out as a highly effective tool, offering high-resolution, non-invasive assessments of structural abnormalities and detailed visualization of the brain's vascular network.\nUni-modal predictive models Prior works mainly use convolutional neural networks (CNN) that can leverage the high-dimensional imaging information for diagnosing patients. applied deep learning algorithms to extract meaningful imaging features in an increasing order of hierarchical complexity to make predictions of the infarct volume. Other models that use only clinical data, often assume linear relationships between traditional risk factors such as age, gender, smoking status, blood pressure, diabetes, cholesterol levels, and body mass index. used AutoPrognosis, an ensemble machine learning approach, to outperform conventional models like the Framingham score and Cox models. A major limitation of these models is that they don't integrate complementary information from other modalities, similar to how clinicians diagnose using multiple data sources. Biobanks like the UK Biobank (UKB) have become invaluable in this context, providing vast datasets integrating imaging and clinical information to train machine learning models for disease prediction.\nMulti-modal predictive models Several studies have employed multi-modal data to improve diagnostic capabilities by integrating diverse data types. For example, MultiSurv model has shown success by fusing image and tabular data for cancer survival prediction. In another study, integration of retinal images and clinical data was leveraged to improve cardiovascular disease prediction. Multi-modal models combining image and clinical data have demonstrated better prediction performance for disability prediction in stroke patients. However, CNNs tend to prioritize image features, and simple image-tabular CNN concatenation fails to enhance predictive models due to insufficient cross-modal interactions. To address this, Wolf et al. developed the Dynamic Affine Feature Map Transform (DAFT), which conditions convolutional feature maps on both image and tabular data, enabling a two-way information exchange via an auxiliary neural network. While DAFT reduces issues related to the large number of trainable parameters in standard 3D CNNs and the curse of dimensionality, it may sacrifice some predictive power compared to deeper models like ResNet. Although recent models show promise in biomedical prediction tasks, their clinical translation is hindered by limited annotated datasets, low disease prevalence, and the risk of overfitting. Self-supervised learning (SSL) is a powerful technique for extracting representative features from unlabeled data, making it valuable for early disease risk identification.\nSelf-supervised models Unlike traditional supervised learning, SSL defines pretext tasks that allow models to learn meaningful representations from raw data. One prominent SSL technique is contrastive learning, which trains encoders to generate augmented views of a sample, maximizing similarity between these views while minimizing similarity with other samples. Popular methods such as SimCLR, BYOL and MOCO have demonstrated success in imaging tasks, while VIME and SCARF are leading approaches for tabular data. Emerging approaches, like contrastive language-image pre-training (CLIP) strategy, have evolved from unimodal methods to integrate diverse modalities. While there was an extensive work done for cardiovascular diseases prediction stroke risk prediction through volumetric brain images and clinical health records remains underexplored."}, {"title": "2 Materials and Methods", "content": "We present for the first time, to the best of our knowledge, a self-supervised multi-modal approach integrating 3D brain MRIs with clinical tabular data for stroke risk prediction. As depicted in Figure 1, our methodology incorporates cross-modal interactions via CLIP loss and image-tabular matching (ITM) loss. We demonstrate that our learning strategy outperforms leading (self-)supervised unimodal methods and that multi-modal image-tabular pre-training leads to better representations and improved downstream performance. Lastly, we validate the model's learned features through visual activation maps, which align with established clinical and neurological findings on stroke-related brain pathology. Code is available at\n2.1 Dataset\nOur analyses are performed on T2-Fluid Attenuation Inversion Recovery (FLAIR) brain volumes, and over a subset of clinical information spanning across five categories, extracted from the UKB: demographics, lifestyle, biomarkers, comorbidities, and medication. The complete list of features is available in the supplementary materials. Continuous features are standardized using z-score normalization, while categorical data is one-hot encoded. For our experiments, we use 5000 and 500 samples for training and validation sets respectively for the model pre-training stage. Train, validation, and test subset for the downstream fine-tuning stage use 278, 93, and 93 samples respectively. The fine-tuning sets are stratified according to sex, age and stroke diagnosis to account for confounders to avoid spurious correlations and class imbalance. To handle missing tabular data, we use an iterative multivariate imputer based on Multivariate Imputation by Chained Equations (MICE), modelling missing features as a function of existing features over multiple imputation rounds. Missing categorical data is replaced by the most frequent category. This step is performed after data normalization, to ensure that the means and standard deviations are calculated only from recorded values. The 3D brain images are registered to Montreal Neurological Institute brain template (MNI) space, have uniform dimensions of 182 \u00d7 218 \u00d7 182 and a voxel size of 1mm\u00b3 and are processed using the UKB imaging pipeline. Key image-derived phenotypes (IDPs), such as segmented brain tissue volumes and white matter hyperintensity (WMH) volumes, are extracted and used as brain IDPs. Brain lesion segmentation is performed using the BIANCA tool to produce 3D binary lesion masks. Furthermore, lesion segmentation masks are characterized by pyradiomics through radiomic features such as volume, area, elongation, and sphericity and these features are used as lesion IDPs.\n2.2 Multi-modal self-supervised framework\nOur pipeline is split into two sequential steps. First, we pre-train the tabular and imaging encoders (Figure 1 A) and then we fine-tune them with labels from downstream task (Figure 1 B). Each batch of data contains pairs of imaging $x_j$, and tabular $x_j^t$ samples. These samples are augmented by random transformations $t \\sim \\tau$ from a set of parametric transforms $\\tau$, such as random cropping and affine transforms for the images, or random feature corruption for the tabular data. We use an image augmentation rate of 95% for the model to still occasionally see unaltered data to capture the original data distribution for transfering the learnt features to the downstream task. The corruption rate of the tabular data is set to 0.3 as in the original tabular method SCARF. For a given reference point, known as anchor $x$, the positive samples are the ones derived from $x$ transformations while other samples in the batch are considered as negative samples. Augmented images $x_j^i$, and tabular data $x_j^t$ are passed through the imaging encoder $f_{\\theta^i}$ and tabular encoder $f_{\\theta^t}$ to generate the embeddings. These embeddings are propagated through the separate projection heads $f_{\\phi^i}$ and $f_{\\phi^t}$, and brought into a shared latent space as projections $z_j^i$ and $z_j^t$, which are L2-normalized onto a unit hypersphere. The projections are pulled and pushed in the shared latent space according to the CLIP loss, which maximizes the cosine similarity of projections from the positive samples and minimizes the similarity of projections from the negative samples in the batch. In contrast to the original InfoNCE loss used in SimCLR and following the CLIP loss, the projected embeddings similarities are contrasted between data modalities. An image projection is therefore defined as :\n$z_{j_i} = f_{\\phi_1}(f_{\\theta^i}(j_i))$"}, {"title": "3 Experiments", "content": "Considering all N subjects in a batch, the loss for the imaging modality is defined as follows:\n$l_{i,t} = -log \\frac{exp(cos(z_{j_i}, z_{j_t}) / \\tau)}{\\sum_{k \\in N, k \\neq j} exp(cos(z_{j_i}, z_{j_k^t}) / \\tau))}$ \nwhere $\\tau$ is the temperature parameter. In our experiments, a temperature of 0.1 is selected to work best, following Chen et al. [2020]. $l_{t,i}$ is computed analogously and CLIP loss is defined as follows:\n$L_{clip} = \\lambda l_{i,t} + (1 - \\lambda)l_{t,i}$ \nWe choose value of 0.5 for the $\\lambda$ as regularization parameter. The aim is to learn patient-wise representations invariant to the variation of the image-tabular pairs. Hard negative samples are crucial in contrastive learning as they help the model distinguish between similar samples, preventing trivial solutions and enhancing its robustness. We implement a hard negative mining strategy to predict whether image-tabular data pairs are positive or negative, using image-tabular matching (ITM) loss. In this approach, for each image or tabular representation, we identify an unmatched tabular or image representation from the mini-batch. This selection is based on similarity scores computed using the CLIP method, which serves as the sampling weight for the negative pairs. A multi-modal interaction module is introduced, as shown in Figure 1, which takes the output of the projector heads to perform inter-modality learning and generates a multi-modal representation. It uses a cross-attention mechanism enabling tabular embeddings to attend to relevant image embeddings. The multi-modal interaction module contains two transformer layers, with four attention heads and a hidden dimension of 256, each including self-attention, cross-modal attention, an MLP feed-forward module and layer normalization. The output of the multi-modal module is a [CLS] token, aggregating the information from the entire sequence, used for downstream classification task, where the model needs a single feature vector representing the entire input. The [CLS] embedding is capturing a joint representation of the image-tabular pair that is fed into the ITM predictor (a linear layer) to match the prediction based on a binary cross-entropy loss $L_{ITM}$. Therefore, the complete loss is expressed as:\n$L = (L_{CLIP} + L_{ITM}) / 2$ \nDownstream task predictions After pre-training, the projection heads were replaced by fully connected layers. Extracting the representation before the projector has been shown to improve downstream tasks performance. For downstream fine-tuning and binary classification of healthy versus stroke (Figure 1 B), we employ ensemble learning to improve model generalization and performance by leveraging the rich representations from the image encoder, tabular encoder, and the multi-modal transformer interaction module. All pre-trained models are evaluated using linear probing (frozen) and fine-tuning (trainable). The frozen models use tuned linear classifiers after the feature extractors. The datasets used for model fine-tuning are balanced in each batch of training, validation, and test subset. This way, we reduce potential bias due to class-imbalance, as well as unstable and slow training due to imbalance batch distributions.\n3.1 Benchmarking\nThe herein proposed solution is compared against supervised and SSL strategies, each of them using imaging, tabular, and integrated imaging-tabular methodologies.\n3.1.1 Supervised learning methods\nTo benchmark our proposed method, we implement two state-of-the-art, supervised image-based models, namely ResNet50 and DenseNet121, two supervised tabular data approaches, namely a two-layer tabular MLP model and a tabular transformer encoder inspired from Du et. al. (2024) . We conduct an ablation study using a supervised MLP model with various feature combinations to identify the optimal feature set. This process helps us to select the final combination of features for improved model performance. The combinations include: i) clinical tabular data only, spanning the previously mentioned categories (clinical), ii) clinical data with brain extracted IDPs (clinical + brain IDPs), and iii) clinical data, brain IDPs and"}, {"title": "4 Results and Discussion", "content": "Limitations. Our study is limited by the use of the UK Biobank, whose demographic characteris-tics may not fully represent the diversity of global populations, potentially impacting the model's generalizability and clinical utility. Future research should validate our approach using more diverse external datasets to improve applicability. Additionally, our test set was constrained by the limited availability of pre-stroke imaging samples, as most stroke datasets focus on post-onset cases. Finally, the heterogeneity in the time between imaging and stroke onset in the UK Biobank could influence model performance, necessitating further experiments to disentangle these effects. Future work could also include improving model efficiency by testing further architectures and techniques to reduce model parameters (e.g. network pruning).\n4.1 Benchmarking\nTo determine which tabular features to include, we conducted a supervised-learning ablation analysis using various combinations of tabular data subgroups. As shown in Table 1, the models that incorporate clinical and brain IDPs achieve the highest ROC-AUC scores. However, the method that also includes lesion IDPs outperforms in binary classification metrics, such as F1-score and sensitivity. To prioritize model robustness while maintaining a smaller feature set, the subsequent benchmarking of models using tabular data is performed using only clinical and brain IDPs.\nA summary of the different models performance is shown in Table 2. It is observed that the proposed multi-modal learning strategy outperforms all other methodologies across all considered metrics, with the trainable model setting performing slightly better than the frozen one.\nWhen comparing models based on learning approach and data modality, it can be observed that the best performing imaging supervised learning strategy is DenseNet121 (ROC-AUC 66.79%). In DenseNet architectures, layers are densely connected, which improves feature reuse and gradient flow, leading to richer feature representations. However, this dense connectivity increases memory overhead during training, particularly with the large inputs used in this study. To optimize the trade-off between efficiency and memory usage, we selected ResNet50 as the encoder for SSL pre-training, accepting a minor reduction in performance.\nWhen comparing SSL strategies, it is evident that fine-tuning both data modalities in multi-modal approaches significantly boosts performance. The performance gap is considerable when comparing these multi-modal models with unimodal image-based models, showing that image data alone is"}, {"title": "4.2 Interpretability and qualitative analysis", "content": "insufficient for effectively addressing the task. The best performing method is the CLIP+ITM model, performing better than all unimodal (tabular and imaging) SSL methods. Interestingly, DAFT performs similar to the multi-modal SSL methods in terms of ROC-AUC and balanced accuracy, although exhibits poor F1-score and sensitivity results. There is no clear difference in performance between trainable and frozen settings across all models. We hypothesize this is because the pre-trained models have already developed robust, transferable representations, making fine-tuning less impactful. Additionally, the small size of the fine-tuning dataset may limit the effectiveness of further learning beyond what was achieved during pre-training. Besides, it could be hypothesized that freezing the model may serve as a form of regularization, helping to mitigate overfitting, particularly in this setting with limited labeled data.\n4.2.1 Embeddings visualization\nFigure 2 shows the UMAP embeddings distribution for unimodal and multi-modal data models. On one hand, it can be observed that in Fig. 2 A, there is a clear distinction between (unimodal learnt) tabular and imaging data modalities, with data samples clustered by data-type. In this case, the embeddings generated from imaging data and tabular data are significantly different from each other in the feature space when generated with a unimodal pre-trained model (i.e., either SCARF or SimCLR). The tight clustering of red points suggests that the tabular data embeddings are more homogeneous and possibly more concentrated in the feature space compared to the broad representation of brain MRI images. Therefore, unimodal-data encoders have learned modality-specific features, without capturing interactions between them. On the other hand, in Fig. 2 B the UMAP plot obtained for the best performing multi-modal model (CLIP+ITM) is shown. In this case, there is significant overlap between the tabular and imaging embeddings, suggesting that the model has found common representations for the two different data types, either via shared visual features or via learning associated clinical patterns in tabular and brain MRIs. Thus, CLIP+ITM is able to encode the underlying patient representation in a common latent space by reducing data augmentation noise. Still, there are data-points in the plot having distinct representations within each modality, suggesting that the model could not project them to the modality-shared latent space. The broad distribution of points across the entire UMAP space suggests that the embeddings capture a wide variety of features from both imaging and tabular data, rather than collapsing all data points into a narrow cluster. These results expose the enhanced performance of the multi-modal SSL strategy by projecting diverse data modalities into a shared embedding space, and thus suggesting a better model starting point for downstream analysis.\n4.2.2 Imaging heatmaps\nFigure 3 shows results from the GradCAM experiment obtained over predicted samples. When inspecting the positive predicted scans (True Positives and False Positives), the model tends to highlight anatomical regions surrounding the lateral ventricles and (periventricular) white matter areas. Such patterns could be associated to white matter hyperintensities, which are known predictors of brain atrophy and age-related brain alterations and also stroke risk predictors in elderly individuals . In different studies, correlations have been"}, {"title": "5 Conclusion", "content": "We hereby present an SSL model integrating diverse data modalities for predicting stroke risk. The model's performance is compared against state-of-the-art (self-)supervised models employing both unimodal and multi-modal data, including tabular and imaging datasets. A comprehensive set of experimental settings is utilized, encompassing different subgroupings of tabular features\u2014such as clinical data, brain IDPs, and lesion IDPs\u2014as well as various training regimes that combine pre-training and fine-tuning based on data modality.\nOur results demonstrate that the CLIP model on multi-modal data, combined with an ITM loss, outperforms single-modality alternatives.The CLIP+ITM model surpasses the self-supervised tabular (image) data SCARF (SimCLR) model by 2.6% (2.6%) in ROC-AUC, and by 3.3% (5.6%) in balanced accuracy terms. Our framework also demonstrated an AUROC improvement of 0.93% and 7.6% balanced accuracy from the best multi-modal supervised method. Additionally, the proposed model produces well-aligned multi-modal representations in a common, data modality-independent space, which is unattainable with unimodal tabular or imaging data models. Thus, CLIP-ITM effectively leverages complementary and synergistic information from diverse data modalities.\nUsing interpretable GradCAM heatmaps, we identified activated brain regions commonly associated with brain aging, stroke risk, and clinical outcomes. On one hand, the activated areas indicate that the model primarily focuses on deep and periventricular white matter hyperintensities for predicting positive samples, which may be more common and extensive in patients identified as at risk for stroke. On the other hand, the prediction of negative samples highlights the cerebellum, posterior brain regions and cortical areas. These results demonstrate the model's capacity to extract task-specific features linked to stroke risk, which are well-supported by existing literature.\nIn conclusion, we propose a robust self-supervised multi-modal learning approach for stroke risk prediction. Our model offers a strong foundation for future studies that aim to integrate multiple data modalities into prediction models."}]}