{"title": "Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG", "authors": ["Manshan Guo", "Bhavin Choksi", "Sari Sadiya", "Alessandro T. Gifford", "Martina G. Vilas", "Radoslaw M. Cichy", "Gemma Roig"], "abstract": "In contrast to human vision, artificial neural networks (ANNs) remain relatively susceptible to adversarial attacks. To address this vulnerability, efforts have been made to transfer inductive bias from human brains to ANNs, often by training the ANN representations to match their biological counterparts. Previous works relied on brain data acquired in rodents or primates using invasive techniques, from specific regions of the brain, under non-natural conditions (anesthetized animals), and with stimulus datasets lacking diversity and naturalness. In this work, we explored whether aligning model representations to human EEG responses to a rich set of real-world images increases robustness to ANNs. Specifically, we trained ResNet50-backbone models on a dual task of classification and EEG prediction; and evaluated their EEG prediction accuracy and robustness to adversarial attacks. We observed significant correlation between the networks' EEG prediction accuracy, often highest around 100 ms post stimulus onset, and their gains in adversarial robustness. Although effect size was limited, effects were consistent across different random initializations and robust for architectural variants. We further teased apart the data from individual EEG channels and observed strongest contribution from electrodes in the parieto-occipital regions. The demonstrated utility of human EEG for such tasks opens up avenues for future efforts that scale to larger datasets under diverse stimuli conditions with the promise of stronger effects.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable performance of artificial neural networks (ANNs) in object recognition [15, 22, 34], ANNs are sensitive to small so-called adversarial perturbations in the inputs [37]. Since the initial discovery of this vulnerability, the field has moved rapidly to devise various adversarial defenses against it [7,27,40].\nIn contrast, human perception is robust to adversarial attacks that are detrimental for ANNs [10,41]. This inspires the idea that adding more bio-inspired elements into ANNs might help alleviate their sensitivity to adversarial attacks and make them more robust. These biological inductive biases often are architecture-based, optimization-based or both [4,9,17,20, 26]. Studies have also directly constrained the ANN representations with their biological counterparts, often using neural data from rodents or non-human primates under non-ecological conditions as regularizers [8,11, 24, 25, 28, 29, 32, 35]. These attempts have led to promising, but modest gains in the robustness of the resulting ANNs [11, 29, 32]. Yet, the cost and the practical challenges associated with acquiring such data also limits the diversity of stimuli used, often restricting the images from small datasets like CIFAR in grayscale, even for approaches relying on fMRI brain data [30].\nThus, in this work, we used a large-scale EEG dataset collected on diverse real-world images with the aim to improve ANNs robustness to adversarial attacks. Specifically, we experimented with the EEG dataset collected by [14] on participants viewing images from the THINGS dataset [16]. We report improvements against adversarial attacks which, though modest, were observed consistently across different random initializations of various architectural variants. These robustness gains were positively correlated with the ability of the models to predict EEG at early time points. Interestingly, as already observed in previous work, we also report similar robustness gains when using shuffled versions of the EEG data. While our results do not give state-of-the-art robustness, they provide important pointers guiding future research. We further analyze the EEG dataset across individual channels to investigate any channel-specific effects. We observe that mid-level channels (PO7, PO3, POz, PO4, PO8), though not as well-predicted by the ANNs as the early channels (Oz, O1, O2), are better (positively) correlated with the gains in robustness. To further facilitate investigations into these methods, we publicly provide the code to the broader scientific community6."}, {"title": "2 Related work", "content": "Various efforts have aimed to imbue deep neural networks (ANNs) with human-like cognitive abilities by training them using brain data. Khosla et al. [19] showcased that ANNs optimized to predict fMRI activity in the fusiform face area (FFA) and extrastriate body area (EBA) could detect 'faces' and 'bodies',"}, {"title": "3 Methods", "content": "Dataset We used a publicly available dataset containing images and corresponding EEG recordings from 10 subjects viewing images from the THINGS"}, {"title": "4 Results", "content": "Adversarial robustness gains were positively correlated with the mod-els' EEG prediction We first investigated if there is a relationship between the model's EEG prediction ability and its gains in adversarial robustness. For this, we measured the correlation between the gains in adversarial robustness (GainDTL(E)) of the 720 models considered here and their EEG prediction (AVG_PCC_tps); see Figure 2A. We observed significant positive correlation values (between 0.53-0.61, p-values < 1e-6) implying that the robustness of the models scaled with the ability of the networks to better capture the statistics"}, {"title": "5 Discussion and Conclusion", "content": "In this study, we explored the effectiveness of human EEG to render robustness to ANNs. Specifically, we co-trained the ANNs to predict human EEG signals in addition to image classification, and tested their robustness to adversarial perturbations. We observed consistent robustness gains across different variants of the networks. Our investigations revealed a positive correlation between a model's robustness and its ability to predict the EEG. We further teased apart the contribution from individual EEG channels, and observed that though the channels overlaying the early visual cortex were best predicted (with Oz even reaching the upper estimates of the noise ceilings), the ones in the parieto-occipital region correlated better with the gains in adversarial robustness.\nOur work validates the use of human EEG data for enhancing the robustness of ANNs which, compared to intracranial recordings that were often used previously, is cheaper and easier to collect. Given that there is an ongoing trend in NeuroAI [1,2,5,23] to collect massive datasets, our methods could not only benefit from the new datasets, but also inform future data collection process. Future works could investigate if larger EEG datasets, collected with different stimulus conditions, say from the auditory domain, can similarly help in improving the robustness of artificial neural networks.\nAs reported in previous works, the robustness gain was consistent, yet modest. This could be because of the exact methods that we used to regularize our networks, as also suggested by [29], or could be the inherent limitations of this approach in itself. Like earlier works, we found that the control (shuffled and random) versions of the EEG, and thus the mere statistics of the signal, also helped in rendering robustness to the ANNs. Indeed, this consistent observation, now observed across intra- and extracranial neural activity, deserves future scientific inquiry in its own right. Exactly what (statistical) elements of the neural activity are the networks utilizing to improve their robustness? Can we use these over conventional initialization methods to improve robustness of ANNs? These questions raise the need and guide future research efforts."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 EEG-Images pairs", "content": "EEG-Image pre-processing The raw EEG signals were first epoched into trials ranging from 200ms before the stimulus onset (denoted as -0.2s) to 800ms after the stimulus onset (denoted as 0.8s) and later down-sampled to 100Hz. 17 channels overlying occipital and parietal cortex where the visual signals are strongest were finally selected. Consequently, our EEG data matrix for model training is of shape (14,886 images \u00d7 4 trials \u00d7 17 EEG channels \u00d7 100 EEG tps(time points)) and our EEG data matrix for model validation is of shape (1654 images \u00d7 4 trails \u00d7 17 EEG channels \u00d7 100 EEG tps). As EEG signals are noisy, we averaged EEG data across the trial dimension and normalized it across the temporal dimension with Z-score. All the image stimuli were normalized and resized to 3 \u00d7 224 \u00d7 224 pixels."}, {"title": "A.2 Dual task learning (DTL)", "content": "Architecture In CNN cluster, we concatenated or averaged output from different ResNet50-blocks in the shared net and then used fully connected layers to map from image features to EEG signals directly. In RNN cluster, shallow RNNs where feedback were realized with Resnet-like skip connections, or Long short-term memory (LSTM) units were integrated with the shared net, as recurrence is capable of capturing temporal dynamics and patterns in time-series data. In Transformer cluster, 6 layers of transformer encoders with 32 multi-heads, dropout value of 0.5 and embedding dimension of 256 were combined with the shared net, as transformer architectures have been shown to be beneficial for fMRI and MEG prediction [33]. In Attention layer cluster, 2 self-attention layers were considered. One is a self-attention layer in [39]. We used 4 multi-heads and an embedding dimension of 256 after optimization experiments. Another utilized the position attention modules (PAM) and channel attention modules (CAM) used in [12]. We followed the default settings. All the 24 architectures from the 4 clusters are included in Table 1. Some architectures from the 4 clusters are depicted in Figure 5, Figure 4 and Figure 6."}, {"title": "A.3 Evaluation of EEG prediction and adversarial robustness", "content": "EEG prediction evaluation Given the hierarchical visual processing in the brain, EEG signals at different time points may capture activities in different regions. We used Pearson correlation analysis to evaluate EEG predictions at various time points using 1654 images from the validation set. The Pearson correlation coefficient (PCC) at each time point was computed as follows: (1) By looping through the channel and temporal dimension, we measured the PCC between predicted EEG and biological EEG at different channel index (ci) and time points. Consequently, we obtained a PCC array of size 17 \u00d7 100, denoted as PCC(ci, tps). (2) We averaged the PCC(ci, tps) across the channel dimension and obtained PCC(pts), which denoted linear relationship between predicted and biological EEG at different time points. Here, pts \u2208 [-0.02s, -0.01s, ..., 0.07s, 0.08s]. The formula for PCC was given as follows: PCC = $\\frac{cov(signal\\_1,signal\\_2)}{\\sigma1,\\sigma2}$, where cov denoted the covariance while \u03c31 and \u03c32 denoted the standard deviation of signal_1 and signal_2.\nAdversarial examples generated with PGD Projected Gradient Descent, or PGD, iteratively constructs adversarial examples as follows :\nxt+1 = Projx+S(xt + \u03b1sgn(\u2207xL(\u03b8,xt,y))) (3)\nwhere is the parameters of models, xt is the input and y is the associated label. L(\u03b8, xt, y) is the loss of model training and \u2207xL(\u03b8, xt, y) is the gradient of loss L with respect to input x. \u03b1 is the gradient step size. t is the number of steps for iteration. xt and xt+1 are adversarial examples before and after next iteration, respectively. The Projx+S is an operator to construct xt+1 within space x+S, such as 1 ball or 12 norm ball around x. In our setting, t was 50 and 40 for 12-bounded PGD and l\u221e-bounded PGD, respectively. In l\u221e-constrained PGD, the attack strength \u0454 \u2208 [1e-5, 2e-5, 3e-5, 4e-5,5e-5,6e-5,7e-5,8e-5,1e-4,3e-4,5e-4,7e-4,8e-4 , 1e-3, 8e-3, 1e-2] and the step size relative to \u0454 was 0.01/0.3. In the l2-constrained PGD, the attack strength \u0454 \u2208 [1e-3,5-3, 7e-3, 1e-2, 2e-2,3e-2,5e-2,7e-2,1e-1 ,2e-1,3e-1,5e-1, 7e-1, 1.0] and the step size relative to \u0454 was set to 0.025."}, {"title": "A.4 Mean adversarial robustness gain and mean EEG prediction accuracy", "content": "Adversarial examples generated with Carlini & Wagner (C&W) attack C&W attack formulates the generation of adversarial examples as an optimization problem, finding the smallest perturbations to the input data that causes misclassification of the target model. C&W attack defines the objective function J(x') as follows:\nJ(x') = \u03b1\u00b7dist(x,x') + \u03b2\u00b7loss(f(x'), y) (4)\nWhere x is the original image; x' is the perturbed image; In the case of L2 C&W attack, dist(x,x') measures the perturbation using the 12 norm. loss(f(x'), y) represents the misclassification loss of target model f on the perturbed input with respect to the target class y. \u03b1 and \u03b2 are weights to balance the dist(x, x') and loss(f(x'), y). The C&W attack iteratively adjusts the perturbations to improve the chances of misclassification while keeping the perturbations imperceptible. Thus the term dist(x, x') is minimized and loss(f(x'), y) is maximized. Gradient descent is used for optimization. The perturbation is updated until it converged towards an examples x' = x -\u03b7.\u2207'J(x'), where \u03b7 is the step size. The attack strength \u0454\u2208 [1e\u22125, 7e\u22125,1e-4,7e-4,1e-3,1e-2,1e-1,3e-1,5e-1,7e-1, 9e-1, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0] and the step size relative to \u0454 was 0.01.\nMean adversarial robustness gain and mean EEG prediction accuracy were used to measure the relationship between adversarial robustness gain and EEG prediction accuracy. We trained 24 architectures from all 4 clusters on EEG of 10 subjects with 3 training seeds. As a result, we had totally 24 \u00d7 10 \u00d7 3 = 720 sample models for analysis.\nMean adversarial robustness gain Avg_GainDTL For each sample model n (0 \u2264 n \u2264 720), we first computed Avg_Gain BTL which was the averaged value of Gain DTL (e) across selected attack strength. In C&W, GainDTL(e) used for Avg_Gain DDTL calculation included those obtained under attack strength e\u2208 [5e-1, 7e-1,9e-1, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0]. In lo-constrained PGD, GainDTL(\u20ac) used for Avg_Gain DTL calculation included those obtained under attack strength \u0454 \u2208 [8e-5, 1e-4, 3e-4, 5e-4, 7e-4, 8e-4, 1e-3, 8e-3, 1e-2 , 1e-1]. In 12-constrained PGD, we considered the attack strength e \u2208 [7e\u22122,1e-1 ,2e-1,3-1,5e-1,7e-1, 1.0]. We selected high e values because under high at-tack strength, robust models would perform much better than fragile ones in classifying adversarial examples, which enabled us to better quantify relationship between EEG prediction accuracy and robustness gain. We finally had 720 Avg_Gain DTL values and the Avg_GainDTL was computed by averaged Avg_Gain DTL across 3 training seeds and 10 subjects.\nMean EEG prediction accuracy Avg_PCC_tps and optimal sliding window selection Based on our initial experimentation, we posited that higher prediction"}, {"title": "B Additional results", "content": ""}, {"title": "B.1 Relationship between Avg_GainDTL and Avg_PCC_tps within different sliding windows", "content": "In Figure 7 (A), the correlation values between Avg_GainDTL and Avg_PCC_tps within two specific sliding windows, one ranging from 0.05s to 0.3s and the other from 0.10s to 0.12s, were inferior to those in Figure 2 (B), which suggested that using a sliding window size of 0.06s allowed us to capture more significant EEG signals around 100ms for adversarial robustness improvements. Using this optimal window size, Figure 2 rigorously examines the correlation between EEG prediction accuracy and adversarial robustness gain."}]}