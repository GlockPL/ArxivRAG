{"title": "Logic-Free Building Automation: Learning the Control\nof Room Facilities with Wall Switches and Ceiling Camera", "authors": ["Hideya Ochiai", "Kohki Hashimoto", "Takuya Sakamoto", "Seiya Watanabe", "Ryosuke Hara", "Ryo Yagi", "Yuji Aizono", "Hiroshi Esaki"], "abstract": "Artificial intelligence enables smarter control in building automation by its learning capability of users' preferences on facility control. Reinforcement learning (RL) was one of the approaches to this, but it has many challenges in real-world implementations. We propose a new architecture for logic-free building automation (LFBA) that leverages deep learning (DL) to control room facilities without predefined logic. Our approach differs from RL in that it uses wall switches as supervised signals and a ceiling camera to monitor the environment, allowing the DL model to learn users' preferred controls directly from the scenes and switch states. This LFBA system is tested by our testbed with various conditions and user activities. The results demonstrate the efficacy, achieving 93%-98% control accuracy with VGG, outperforming other DL models such as Vision Transformer and ResNet. This indicates that LFBA can achieve smarter and more user-friendly control by learning from the observable scenes and user interactions.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in artificial intelligence (AI) enable smarter control in building automation (BA). The traditional BA also showed its smartness but was \"programmed smart\u201d. Now, the learning capability of AI may perform program-less BA, enabling an innovative smarter control for the spaces.\nProgram-less control of room facilities is challenging because machines do not know the user's preferred control. Reinforcement learning (RL) [1, 9, 15-18] was one of the approaches for program-less control but encountered many difficulties in practice when applying to the real world[4].\nWe propose logic-free building automation (LFBA) a new architecture of facility control with deep learning (DL). LFBA uses (1) a ceiling camera as the sensor for monitoring the space, and (2) wall switches for both manual control and supervising the control. We introduce a manual-automation switcher (MAS) for managing the data flow among wall switches, a DL model, and facilities.\nDL models such as Vision Transformer (ViT) [3], ResNet[5], and VGG[14] have deep potential in learning the scene of a space: e.g., standing, sitting on a chair or sofa, using monitor, reading books, and playing the piano. We use such capabilities for outputting the facility control signals. This differs from object detection (OD) [2, 11, 13] based approaches because such systems require the users to implement additional logic for the preferred control.\nWe developed a real-world testbed in May 2024 for studying LFBA systems, collecting the dataset, and testing the performance in practical scenarios. Although we focused on room light control with a ceiling camera, LFBA could be extended to other facilities in smart homes or buildings such as energy management, HVAC, and other appliances. The use of a microphone and a large language model can be considered as another input modality. However, we left these extensions as future items.\nWe carried out evaluations with three major DL models (ViT, ResNet, VGG) with 5-fold cross-validation. Surprisingly, we found that VGG, the legacy DL model, outperformed others.\nFuture buildings will have built-in ceiling cameras for facility control although there are discussions about privacy. Related to this, this study was conducted with an ethics review at our institution."}, {"title": "2 Related Work", "content": "Many researchers have tried to apply AI to BA systems [6]. The most straightforward approach is to use RL to learn optimal control from the user's feedback of satisfied or not for control trials. Many researchers have worked on RL in smart buildings [15, 17], HVAC controls [1], and smart homes [16, 18]. Although applying RL to BA sounds promising, we encounter many issues in practice:\n(1) RL needs thousands of user feedback by reward or\npenalty at every trial of control. This user interface is\nbothersome for users. Especially, if RL performed control\nerrors for aggressive training, users will be frustrated.\n(2) Simulator-assisted training is studied to solve these issues\n[9]. However, the reward function, i.e., user feedback,\nhas to be properly and explicitly formulated to run\nsimulations. This formulation was actually what we did as\ncontrol logic programming in the legacy BA systems.\nFor these reasons, we explore another architecture that does not rely on RL but uses legacy wall switches for learning the user's preferred control.\nOther related studies are the use of OD-enabled cameras such as YOLO [13], SSD [11], and DETR[2], for smart homes [7, 8, 10, 12]. However, the users must implement their control logic explicitly for their room. This is an extension of programmed-smart. Programming for every room is challenging for most of the users.\nOur focus is the design of the architecture for real-world implementation. The LFBA, we propose in this paper, has the learning capability of the user's preferred control, allowing logic-free and user-friendly building automation."}, {"title": "3 Logic-Free Building Automation with Deep\nLearning", "content": "This section defines the architecture of logic-free building automation with deep learning (LFBA). For simplicity, this paper focuses on binary (i.e., 0/1) cases for switch states and control signals."}, {"title": "3.1 System Architecture", "content": "Fig. 1 shows the architecture of LFBA. It is organized with wall switches, building facilities (control targets), a ceiling camera, a deep learning model, and a manual-automation switcher (MAS).\nLet $s_i$ be the state of a switch, and $c_i$ be the state of the corresponding control signal. Here, $i = 1, ..., n$ is the index of the switch and the associated facility. In manual mode, $s_i$ alternates when the user physically pushes the switch. For example, $s_i$ becomes 1 after pushing switch $i$ at $s_i = 0$. If $c_i = 1$, the corresponding facility is active. We denote $s = (s_1,..., s_n)$ and $c = (c_1,..., c_n)$ in short.\nRegarding the DL model, we denote the output by label $y$ and the input by image $x$. Here, $y$ represents $2^n$ classes - the combinations of $s$. Image $x$ is taken periodically, e.g., every second, and pushed into the model. The final layer, i.e., multi-layer perceptron (MLP) head, is replaced to match $2^n$ classes. We consider the case where the room does not have more than 10 switches. The case of a larger number should be studied in the future."}, {"title": "3.2 Manual-Automation Switcher", "content": "This switcher mechanism is important in training the DL model and automating the facility control. MAS has a static 1-to-1 mapper $c = map(y)$, which generates control signals $c$ from model output $y$. To generate $y$ from switch states $s$ for model training, we denote by $y = map^{-1}(s)$.\nMAS manages the modes of training, prediction, and none. We define the following three operation modes.\n1. Manual (No Training): The user sets MAS to this mode\nwhen he/she does not want to train or use the model. In this\nmode, the switcher bypasses as $c \\leftarrow s$.\n2. Manual (with Training): The user sets MAS to this mode\nwhen he/she wants to train the model. In this mode, the\nswitcher bypasses as $c \\leftarrow s$, and uses $\\hat{y} \\leftarrow map^{-1}(s)$ as a\nsupervised label. The DL model trains its parameters with $x$\nshot by the ceiling camera and $\\hat{y}$.\n3. Automation: The user sets MAP to this mode when he/she\nwants to rely on the DL model for automation. In this mode,\nthe model generates $y$ from image $x$ periodically. The output\nis used to control building facilities. $c \\leftarrow map(y)$."}, {"title": "4 LFBA Testbed and Dataset", "content": "We set up a research testbed at our institute in May 2024 and captured the scenes of the room on this testbed.\nFig. 2 shows the structure of the target room and photo examples shot by a ceiling camera. People work, study, have discussions, relax, check their looks, and play the piano. Four controllable lights ($c_1,..., c_4$) are deployed, whose status can be controlled manually by the wall switches or automatically by the output of a DL model.\nTable 1 shows the dataset profile. We have assigned an ID for scene class. The Shots indicates the number of images for the scene class. The Output denotes the user's preferred control (i.e., ground-truth label), ordering from $c_1$ to $c_4$. For example, at A41 (About to start playing the piano), \u201c1000\u201d means $c_1$ should be active, whereas the others should be inactive. In this case, only the spotlight $c_1$ is activated (see Figs. 1 and 2). At this moment, we manually assigned the Output labels to the collected images.\nTo increase the diversity and to make 5-fold cross-validation effective, we performed with different clothing as Fig. 3. The shoot-ing round was separated by what we call \"run\". Hairstyles and facial features could be also extracted at the tuning phase of the DL model, but at this moment, we only focused on the clothing because it would be the major part of the user's appearance.\nThe dataset and related codes for evaluation will be publicly avail-able soon."}, {"title": "5 Evaluation", "content": "We conducted evaluations mainly focusing on comparing the accuracies among major DL models. Further analyses are left open for future work. We set two evaluation cases as follows.\nMerge & Split Runs: Evaluation by merging all the runs, shuf-fling, and splitting into train and test by the ratio of 80% and 20%. In this case, clothing for the test also appears in the training data.\n5-Fold Cross Run: Evaluation by picking up a shooting run for a test, and using the other runs for training. In this case, the clothing in the test does not appear in the training data. We performed 5-fold cross-validation by changing the pick-up run.\nWe calculate balanced accuracy (B-Acc) and the standard accuracy (S-Acc). B-Acc is more suitable as the number of shots by label is unbalanced (see Table 1)."}, {"title": "5.1 Experiment Setting", "content": "We used ViTs, ResNets, and VGGs pre-trained with ImageNet-1K. Please note that ViTs are known as state-of-the-art at many image recognition tasks, ResNets are CNN-based high-performance models, and VGGs are legacy deep learning models.\nWe replaced the final fully connected layer to fit the output to 16 classes. We tuned the whole model parameters with the following hyperparameters.\n\u2022 Cross entropy loss\n\u2022 SGD with momentum = 0.9\n\u2022 Batch size = 10\n\u2022 Learning rate = 0.001\n\u2022 Training epoch = 25\nWe did not apply data augmentation such as random rotation and resizing because the positions in the image are important."}, {"title": "5.2 Performance of Control Generation", "content": "Table 2 shows the results. In Merge and Split, all the models achieved almost the similar performances at 98% (S-Acc) and 97-98% (B-Acc). In 5-Fold Cross Run, surprisingly, VGG - the legacy deep learning model performed the best, achieving 94.3% (S-Acc) and 93.1% (B-Acc). These accuracy scores show the potential efficacy of LFBA.\nTable 2 shows that ViT could not perform the highest accuracy for unknown clothing, but VGG could. The reason might be that ViTs paid more attention to individual features such as clothing and gave wrong outputs. Of course, further investigations, for example using explainable AI, must be required but we left it open for the future."}, {"title": "6 Conclusion", "content": "We proposed the architecture of logic-free building automation with deep learning and developed a real-world testbed. Our evaluation results, i.e., 93%-98% accuracy, demonstrate LFBA's efficacy as a new building automation architecture."}]}