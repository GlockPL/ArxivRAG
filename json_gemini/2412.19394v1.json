{"title": "AN ENGORGIO PROMPT MAKES LARGE LANGUAGE MODEL BABBLE ON", "authors": ["Jianshuo Dong", "Ziyuan Zhang", "Qingjie Zhang", "Han Qiu", "Tianwei Zhang", "Hao Wang", "Hewu Li", "Qi Li", "Chao Zhang", "Ke Xu"], "abstract": "Auto-regressive large language models (LLMs) have yielded impressive performance in many real-world tasks. However, the new paradigm of these LLMS also exposes novel threats. In this paper, we explore their vulnerability to inference cost attacks, where a malicious user crafts Engorgio prompts to intentionally increase the computation cost and latency of the inference process. We design Engorgio, a novel methodology, to efficiently generate adversarial Engorgio prompts to affect the target LLM's service availability. Engorgio has the following two technical contributions. (1) We employ a parameterized distribution to track LLMs' prediction trajectory. (2) Targeting the auto-regressive nature of LLMs' inference process, we propose novel loss functions to stably suppress the appearance of the <EOS> token, whose occurrence will interrupt the LLM's generation process. We conduct extensive experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B. The results show that Engorgio prompts can successfully induce LLMs to generate abnormally long outputs (i.e., roughly 2-13\u00d7 longer to reach 90%+ of the output length limit) in a white-box scenario and our real-world experiment demonstrates Engergio's threat to LLM service with limited computing resources. The code is accessible at https://github.com/jianshuod/Engorgio-prompt.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Touvron et al., 2023a; Ouyang et al., 2022; Carlini et al., 2021) have demonstrated remarkable performance in various real-world applications, e.g., online chatting (Shen et al., 2023), customer service (Gimpel et al., 2023), and finance (Wu et al., 2023). Given the increasing popularity and adoption of LLMs, reducing their inference cost becomes critical. Firstly, from the cost aspect, a modern LLM normally contains billions of parameters, and each inference generation may consume considerable resources and time. Many AI service providers are paying more bills to support their LLM inference services than training (Patel et al., 2024; Li et al., 2024; Patterson et al., 2022). Secondly, from the service availability aspect, there is fierce competition across different LLM service providers, making service reliability and fast response time important factors in attracting customers. Meanwhile, these two considerations motivate malicious entities to attack the LLMs, increasing their operational cost and generation latency.\nIn this paper, we explore the landscape of inference cost attacks against modern LLMs. First proposed in Shumailov et al. (2021) to attack encoder-decoder transformers, inference cost attacks aim to intentionally maximize the energy consumption and latency of model inference via a new type of adversarial input. The inference cost attacks on language models (Shumailov et al., 2021; Chen et al., 2022; Feng et al., 2024) are tailored for encoder-decoder models and rely on perturbation-based mutation to progressively hit a desirable adversarial input. However, as demonstrated in Section 2.2 and Section 4, they become ineffective against modern LLMs (Graves, 2013), which adopt the auto-regressive generation scheme (Graves, 2013), remove the cross-attention mechanism, and employ a sub-word tokenization algorithm. Geiping et al. (2024) propose an adversarial prompt attack to coerce LLMs into repeating specific content, achieving effects similar to an inference cost attack. However, its reliance on the starting response weakens robustness and increases detectability."}, {"title": "2 PRELIMINARIES", "content": "In general, it is challenging to design inference cost attacks against modern decoder-only LLMs, even given the existing works discussed above. The main challenges stem from two aspects: (1) Uncertain Generation Process. The generation process of decoder-only LLMs is inherently casual, auto-regressive, and sampling-based, rendering it difficult to constrain them to generate a specific long response. The occurrence of one deviant token can directly distort the generation process from the desirable decoding trajectory, challenging attack effectiveness and stability. (2) Discrete Input Modality. Text-completion LLMs accept input text in the form of discrete token sequences but operate within the embedding space, which implies an irreversible mapping from the embedding space back to the token space. While we can leverage gradient information to optimize a more desirable soft embedding representation for the input, we face challenges in accurately identifying corresponding tokens in the token space for the optimized soft embeddings. This restricts us from effectively leveraging gradients to guide updates to the input token sequence (i.e., adversarial input). To address the above challenges, we need to consider two intriguing and important questions: (1) how to accurately frame our goal as a well-aligned optimization problem and (2) how to effectively instruct the updates to the discrete input sequence given the modeled objective.\nIn this paper, we introduce Engorgio\u00b9, a simple yet effective method to generate threatening Engorgio prompts against state-of-the-art LLMs. Our focus is on text completion, where LLMs predict the next token based on the initial prompt and previously generated tokens until an end-of-sequence (<EOS>) token is predicted or a maximum length is reached. Technically, Engorgio effectively addresses the above challenges via: (1) Inspired by the special role of <EOS> token in determining whether the model halts its response, we adopt an untargeted objective called <EOS> escape loss, which reduces the <EOS> token's occurrence probability. We also combine a self-mentor loss to stably induce longer responses. (2) We employ a re-parameterization design to effectively utilize the gradients, by modeling the potential distribution of the entire context that can fulfill both objectives. Figure 1 shows the effects of our attack: normal prompts (e.g., the renowned ShareGPT dataset (1)) typically tempt the LLMs to produce short sequences; in contrast, the crafted Engorgio prompts can make the model response extraordinarily long.\nIn summary, our main contributions lie in three folds: 1) We explore a novel research direction, inference cost attack against modern auto-regressive LLMs. We highlight how crafted adversarial prompts can impact LLM service availability. 2) We analyze technical challenges associated with the attack surface. Based on our insights, we propose Engorgio, a simple yet effective method that can stably induce lengthy LLM responses. 3) To prove the effectiveness of Engorgio, we conduct extensive experiments over 6 base models and 7 supervised fine-tuned (SFT) models with parameters ranging from 125M to 30B, as listed in Table 5. Specifically, the generated Engorgio prompts can achieve roughly 90%+ of the maximum allowable length on 6 base models, while normal queries can only cause between 0-40%. For SFT models, Engorgio can significantly outperform baselines by up to 13x. A real-world experiment demonstrates Engorgio's implications in service availability."}, {"title": "2.1 LARGE LANGUAGE MODELS (LLMS)", "content": "The task of language modeling tracks the rationality of text sequences and treats the probability of a certain sequence as a product of conditional probabilities (Jelinek, 1980; Bengio et al., 2003):\n$P(x_1,\\cdots, x_N) = \\prod_{i=1}^{N} P(x_i|x_1,\\cdots, x_{i-1})$"}, {"title": "2.2 INFERENCE COST ATTACKS", "content": "Machine learning services are facing an availability threat. Shumailov et al. (2021) showed that malicious users could intentionally craft adversarial inputs, known as sponge examples, to significantly increase the energy consumption and latency of the corresponding inference process. Such inference cost attacks can greatly affect the service provider's operational cost and user experience. Following this work, a variety of attacks have been designed to target different AI systems and applications, for example, image classification (M\u00fcller & Quiring, 2024), camera-based object detection (Shapira et al., 2023; Schoof et al., 2024; Shapira et al., 2022; Xiao et al., 2024; Ma et al., 2024), LiDAR-based object detection (Liu et al., 2023), and multimodal models (Gao et al., 2024).\nThis paper focuses on attacking the modern auto-regressive LLMs. Existing inference cost attacks against language models (Shumailov et al., 2021; Chen et al., 2022; Feng et al., 2024) are only effective when targeting the encoder-decoder structure. Shumailov et al. (2021) generated sponge examples by compressing more tokens into one sentence, leading to a higher computational burden in the cross-attention operations, which are ineffective for LLMs lacking cross-attention modules. Sub-word tokenization methods BPE (Sennrich et al., 2016) eliminate the appearance of <UNK> token and enhance LLMs' typo-tolerating ability, largely invalidating perturbation-based methods like LLMEffiChecker (Feng et al., 2024). For the optimization-based method, Geiping et al. (2024) proposes a targeted attack that coerces LLMs into producing elicit a specific starting response (i.e., repeating \"Hello There\" 24 times), indirectly achieving effects similar to an inference cost attack. However, this approach is less stable due to its reliance on the starting response's effectiveness and is easily detectable as the starting response serves as a clear indicator of adversarial intent. This motivates us to design a new attack methodology tailored for modern LLMs. In this work, we propose a simple yet effective method to overcome the technical challenges inherent in this task."}, {"title": "2.3 THREAT MODEL", "content": "We design our attack following the threat model of previous inference cost attack studies against language models (Shumailov et al., 2021; Chen et al., 2022; Feng et al., 2024) and provide detailed discussion about the practicality and implications of the attack in Appendix A.2.\n\u2022 Attacker's goal: As a service user, the attacker aims to craft Engorgio prompts T, which could induce as long output as possible. Such behaviors could bring much higher operational costs for the LLM service provider, and affect the service availability to other normal users.\n\u2022 Attacker's knowledge: We mainly consider a white-box scenario, where the attacker has full knowledge of the target model, including its architecture, input template, model parameters, etc. We also consider the black-box setting, in which we transfer Engorgio prompts to attacker-unknown models (see Appendix B.1 for details).\n\u2022 Attacker's capability: The attacker locally generates the Engorgio prompts T, aligned with her knowledge settings. Then she sends the constructed Engorgio prompts T to the target LLMs and collects the responses for attack checking."}, {"title": "3 METHODOLOGY", "content": "In order to achieve the attack goal, we review the mechanism of generating texts by LLMs. A sample for the LLM can be split into an input part and an output part (see Appendix A.3 for more analysis). Given an input sequence (dubbed prompt) composed of k tokens, the model generates the subsequent tokens (i.e., the output part). The generation continues until either of two conditions is met: (1) reaching a pre-set maximum allowable length; (2) encountering an <EOS> token which indicates the end of the sentence. As the maximum allowable length is fixed as S, the problem is stated as follows: the later an <EOS> token is encountered in the inference process, the higher cost and latency this query will take. Therefore, to achieve latency damages to the service provider, i.e., maximizing the length of the output part, the attacker aims to create Engorgio prompts, which can effectively suppress the possibility of predicting the <EOS> token during the inference.\nBased on this insight, we design Engorgio, a novel attack framework to generate Engorgio prompts against LLMs. Figure 2 shows its overall pipeline and we provide a term list in Appendix A.4. The core is the introduction of a parameterized proxy distribution. To satisfy the requirements for Engorgio prompts, we explore how to update the distribution with the guidance of an <EOS> escape loss and self-mentor loss. The whole process of crafting Engorgio prompts is two-stage:\n\u2022 Generation stage: For each optimization step, we convert the proxy distribution matrix @ to a weight matrix w using the Gumbel-Softmax function. We then aggregate the embeddings of all token candidates weighted by w to project @ into the embedding space. This output is fed into the model to calculate two loss terms, allowing us to obtain gradients for @ easily. The matrix 0 is updated based on these losses, continuing until no significant changes are detected.\n\u2022 Testing stage: The optimization process guarantees that the output part falls onto a region with low probabilities of <EOS>. Given the strong correlation between the Engorgio prompt and the output, we can sample the Engorgio prompt using the normalized w1:t. It is observed that as the optimization progresses, the distribution matrix @ typically converges toward a specific prompt with a significantly higher sampling probability compared to others. This prompt is adopted as the final Engorgio prompt T. This approach significantly reduces the cost of evaluating other prompt candidates. We hypothesize that the objectives, particularly self-mentor loss, contribute to identifying the optimal Engorgio prompt, as detailed in Section 4.6."}, {"title": "3.2 PROXY DISTRIBUTION", "content": "To increase the lengths of the target LLM's responses, we search for the Engorgio prompts T with the help of a proxy model. LLMs typically accept a token sequence (corresponding to one input text) as input, cast each token into the embedding space, and work within the embedding space. Each token has a corresponding embedding; however, not all embeddings correspond to tokens. We can optimize suitable embedding expressions that satisfy the objectives in the form of prompt learning (Li & Liang, 2021; Liu et al., 2021), but we face challenges in determining the corresponding token sequence (i.e., input text). We resort to a re-parameterization method."}, {"title": "3.3 LOSS DESIGN", "content": "As LLMs predict the next tokens according to the probability distribution, it is more efficient to search for desirable Engorgio prompts by sampling from an appropriate distribution (Guo et al., 2021). Therefore, we introduce a proxy distribution to track the process of sequence sampling. This proxy distribution is parameterized as a matrix $\\theta \\in \\mathbb{R}^{S\\times V}$, with S denoting the maximum allowable length, corresponding to the whole context. It instructs how to select a suitable token sequence from a token vocabulary with V token candidates in the following test stage. Then the question is how to ensure that the proxy distribution @ meets the objectives. This endeavor seeks to involve the distribution matrix in the generation stage, subsequently updating it based on the gradients. Concretely, in the forward pass, the distribution vector $\\theta_i$, corresponding to the i-th token in the Engorgio prompt where $i \\in \\{1,\\ldots, S\\}$, is independently normalized. This serves as a weight to aggregate token embeddings across the model vocabulary, thereby casting $\\theta_i$ as a soft token in the embedding space. This process is formulated as Eq. 2.\n$\\breve{e}(\\theta_i) = \\sum_{j=1}^{V}(\\omega_i)_j e(j)$,\nwhere $e(j) \\in \\mathbb{R}^H$ denotes the embedding of the j-th token within the model vocabulary and $w_i \\in \\mathbb{R}^V$ is the normalized version of $\\theta_i$ with the sum $\\sum_{j=1}^{V}(\\omega_i)_j = 1$. We adopt Gumbel-Softmax (Jang et al., 2017), which introduces stochastic elements and enriches the diversity of tokens involved in the generation stage. The normalization is conducted in Eq. 3:\n$(\\omega_i)_j = \\frac{\\exp((\\theta_{i,j}+g_{i,j})/\\tau)}{\\sum_{k=1}^{V} \\exp((\\theta_{i,k}+g_{i,k})/\\tau)}$,\nwhere $g_{i,1} \\cdots g_{i,V}$ are drawn from the distribution $Gumbel(0,1)$ and $\\tau > 0$ is a temperature factor used to control the uncertainty. The introduction of the random variable $g_{ik}$ from an i.i.d distribution benefits the diversity of the sampling operation. Due to the differentiability of Gumebl-Softmax, we can take full advantage of the gradient information to update @ in the generation stage and guide the sampling of the final Engorgio prompt T in the test stage.\nSFT models assume the input should be embedded in a specified template T, as illustrated in Figure 6. Considering the most general case that the template $T = \\{[P_{1:i}], x, [P_{i+1:m}], y\\}$ contains a prefix and an infix, we define the corresponding embedding sequence to @ as Eq. 4.\n$\\mathbb{E}(\\theta) = \\{e([P_{1:i}]), \\breve{e}(\\theta_{1:t}), e([P_{i+1:m}]), \\breve{e}(\\theta_{t+1:s-m})\\}$,\nwhere $P_{1:i}$ and $P_{i+1,m}$ represent the token sequences corresponding to prefix and infix, and the shape of @ is adjusted to (s \u2013 m) \u00d7 V. The input composition is illustrated in Figure 6.\nTo obtain a desirable proxy distribution, we mainly depend on two key loss components to update 0, <EOS> escape loss and self-mentor loss. The <EOS> escape loss closely aligns with our target goal to make the output part longer while the self-mentor loss is designed to enhance the usability of the proxy distribution. Balancing the impact of the two loss terms with \u5165, we update the proxy distribution as follows:\n$\\min_{\\theta} L_{esc}(\\theta) + \\lambda L_{sm} (\\theta)$\n<EOS> escape loss. Due to the unpredictability of the LLM generation process, enforcing a specified long response is challenging. We resort to an untargeted objective, which is to decrease the prediction probability of the <EOS> token. However, it is still impossible to accurately forecast the exact occurrence position of <EOS> during the test stage. To tackle this, we propose penaliz-ing the occurrence of <EOS> token from all positions, rather than focusing on specific positions. This broader treatment allows us to effectively manage the uncertainties associated with <EOS> placement. The <EOS> escape loss is defined as below:\n$L_{esc}(\\theta) = \\sum_{i=1}^{S} Softmax(f_e(\\mathbb{E}(\\theta)_{1:i}))_{\\kappa}$,\nwhere K denotes the index of the <EOS> token for the target model. We adopt a Softmax-normalized probability of <EOS> so that it can better measure the relative chance that the model predicts <EOS> as the next token at a certain position, which is more effective than directly decreasing the absolute"}, {"title": "4 EVALUATION", "content": "logit of the <EOS> token. An input sequence containing <EOS> is illegal, as the inference process should have halted before predicting the next tokens for the Engorgio prompt. Therefore, we also consider reducing the predicted <EOS> probabilities of the Engorgio prompt part.\nSelf-mentor loss. Another challenge is that we can only query the target model utilizing the Engorgio prompt T to ensure attack stealthiness and efficiency. Considering the auto-regression nature of modern LLMs, we cut off the first t tokens as our Engorgio prompt. Moreover, \u03b8\u2081 only independently tracks the token selection of the i-th position, but the correlation between tokens should also be enhanced. Therefore, we seek to enhance the relevance of all tokens in the sequence, especially the bond between the Engorgio prompt and output parts. Inspired by LLM's causal pre-training paradigm, we search for a sequence where the proxy model fits well. The loss term is given below:\n$L_{sm} (\\theta) = \\sum_{i=1}^{S}L(\\omega_{i+1}, Softmax(f_e(\\mathbb{E}(\\theta)_{1:i})))$,\nwhere L is the cross entropy loss. The closer to 0 Lsm is, the better the proxy model fits in input E(0)1:s, which helps the Engorgio prompt T steadily induce a longer output."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "LLMs. We include multiple base models, OPT-125M (Zhang et al., 2022), OPT-1.3B (Zhang et al., 2022), GPT2-large (Radford et al., 2019), LLaMA-7B (Touvron et al., 2023a), LLaMA-2-7B (Touvron et al., 2023b), and LLaMA-30B (Touvron et al., 2023a). SFT models are further fine-tuned with additional dataset, for which we consider seven well-known SFT models including Alpaca (7B) (Taori et al., 2023), Vicuna (7B) (Chiang et al., 2023), StableLM (7B) (2), Koala (7B) (3), Oraca (7B) (Mukherjee et al., 2023), Samantha (7B) (4), and ChatGLM (6B) (Du et al., 2022), as our targets. Considering the crucial importance of prompts, we also consider the three cases of deploying base models with prompts according to the attacker's knowledge about the deployed prompt. Constrained by space limit, the corresponding results are presented in Appendix B.3.\nBaselines. We consider four types of inputs as baselines for comparisons. (1) Normal inputs: we collect 50 samples from the training dataset for Standford-alpaca (Taori et al., 2023), which are generated by OpenAI's text-davinci-003, and 50 samples from ShareGPT (1), a website where people can share their ChatGPT conversations. We use the mixup to roughly represent the normal response length of LLMs. (2) Special inputs: we use prompts with the semantics of demanding a longer output (i.e., prompts starting with \"output longer\"). (3) LLMEffiChecker: we adopt the three attacks (character, word, and structure attack) proposed in Feng et al. (2024) and report the averaged results across the attack variants. (4) Sponge examples: we generate such inputs using the method from Shumailov et al. (2021) by only setting the same input length as our method.\nMetrics. Due to the intractable serving mechanisms for LLM, we report results on the level of model behaviors. To mitigate potential sampling bias caused by the inherent variability in LLM inference, we measure the average token number of the generated outputs (Avg-len). We query the target LLM multiple times using the sampling generation and compute the average length across these responses. This renders Avg-len a robust estimate of the Engorgio prompt's efficacy. Second, we calculate the ratio of the LLM outputs that reach the maximum length (Avg-rate) to evaluate the stability. Notably, inference costs increase super-linearly with longer responses, making Avg-len a lower bound on the prompt's impact on inference cost. Further analysis of the Engorgio prompt's impact is detailed in Appendix A.5.\nConfigurations. We use the Adam optimizer with a learning rate of 0.1 to update the distribution matrix . We allow a maximum of 300 optimization steps, the cost of which is acceptable, especially when considering the reusability as explained in Appendix A.6. The Gumbel-Softmax temperature factor T is set to 1, and the default Engorgio prompt length is t = 32. The input length of normal inputs, special inputs, LLMEffiChecker, and sponge examples is roughly the same as Engorgio to ensure fairness. According to our extensive experiments, the loss coefficient A is empirically set to 1. The optimization starts with a random prompt and we use it to initialize the proxy distribution. Constrained by the computing resources, we set 1,024 as the pre-set maximum length. We also"}, {"title": "4.2 MAIN RESULTS", "content": "We report our results on base models in Table 1. Comparing normal and special inputs reveals that semantic inputs induce base models to output longer. This means that base LLMs can understand the semantics inside the inputs and seemingly feature being talkative. However, relying solely on special input is far from reaching the maximum allowable length. LLMEffiChecker proves ineffective against more advanced LLMs. Our method can achieve a very high ratio (roughly 90-100%) of letting the base model keep endlessly generating tokens until reaching its maximum length, which outperforms all baselines including sponge examples. While sponge examples extend output length compared to normal or special inputs, they are less stable than Engorgio as they struggle with LLMs' sampling-based decoding. Results of more base models are presented in Appendix B.2.\nSFT models may use cut-off as a preprocessing strategy on their fine-tuning datasets (e.g., at most 512 tokens for Alpaca). This potentially biases the fine-tuned model to produce short responses, which makes our goal challenging, as suggested by the results of normal inputs in Table 1. For special inputs, even with instructions for longer responses, SFT models still produce notably shorter outputs, sometimes even shorter than normal inputs. The silent nature of SFT models worsens the performance of sponge examples. For LLMEffiChecker, the weaker performance extends to SFT models. We hypothesize that recent LLMs are more robust to typing errors, invalidating perturbation-based attacks. In contrast, Engorgio knows how to better optimize the Engorgio prompt by focusing on a distinct goal: avoiding the generation of the <EOS> token. It effectively increases the output length to approach the maximum limit, especially when paired with a semantic prefix as discussed in Section 4.4, achieving near-maximum allowable lengths. We also explore a black-box scenario, where we resort to the transferability of Engorgio prompts (See Appendix B.1) for details."}, {"title": "4.3 ABLATION STUDY", "content": "Impact of loss design. We explore the efficacy of the two loss terms. Initially, we assess the performance when optimizing only with the <EOS> escape loss (noted as \"ESC\" in Table 2). A comparison with normal input and special input from Table 1 reveals that even utilizing only the"}, {"title": "5 DISCUSSIONS", "content": "generation process. We report the relative level of \u00b5 compared to the average probability 1/V coupled with the change of <EOS> escape loss in Figure 5. We find that the decrease of <EOS> escape loss can lead the maximum probability of the occurrence of <EOS> token to a low level (close to 0). This substantiates the effectiveness of <EOS> escape loss in stopping <EOS> token from appearing.\nThe resistance to potential countermeasures. There are no off-the-shelf defenses tailored for Engorgio yet. Service providers are faced with a trade-off between detection accuracy and service quality. Although rare, normal inputs may also lead to a long response. Engorgio prompts are not crafted to be coherent. However, our experimental results show that simple methods like a perplexity filter will lead to an unacceptably high false positive rate, significantly degrading user experience. This is rooted in the variability of legitimate user queries themselves. What's more, introducing semantic prefixes inevitably improves the coherence of Engorgio prompts, but incurs no performance degradation. Another potential countermeasure is anomaly detection, monitoring the output length distribution of queries and blocking high-risk users. However, the method may face problems of false positives and attackers can strategically adjust behaviors to evade detection. Please refer to Appendix B.7 for more related experimental results and discussions. We will explore effective defense mechanisms in our future work.\nPotential limitations. Although the white-box setting in this work can already cause far-reaching consequences as explained in Appendix A.2, we emphasize the need to systematically study the transferability of Engorgio prompts. The method efficiency in crafting Engorgio prompts should be further improved. For the current version, we generate one Engorgio prompt at one time. We plan to extend to a batch method and study the interoperability among different Engorgio prompts. To address high-temperature cases, we employ semantic prefixes to mitigate issues. Future work will focus on tracking more active model prediction dynamics to eliminate these challenges. Currently, we do not consider coherence when crafting Engorgio prompts. As coherence enables higher stealthiness of Engorgio prompts, we plan to further explore it in our future work."}, {"title": "6 CONCLUSION", "content": "In this paper, we investigate the inference cost threats to modern auto-regressive language models, which tempt the victim models to produce abnormally long outputs and compromise service availability. We introduce Engorgio, a novel attack methodology, to effectively generate Engorgio prompts that can significantly lengthen the model responses. Driven by the challenges of uncertain generation process and discrete input modality, our work advances in utilizing proxy distribution and untargeted loss to craft threatening Engorgio prompts. This is achieved by tracking a parameterized distribution of Engorgio prompts and optimizing it to decrease the occurrence probability of the <EOS> token. We validate the effectiveness of Engorgio with extensive experiments on 6 base models and 7 SFT models, considering various prompt scenarios. By inducing the target LLMs to output until their maximum length limits, we achieve roughly 2-13\u00d7 more inference cost per query compared to normal inputs. We also conduct a real-world case study to demonstrate the practical threat posed by Engorgio to cloud-based LLM services."}, {"title": "A FURTHER STATEMENTS", "content": ""}, {"title": "A.1 INVOLVED MODELS", "content": "Mainstream LLMs can be categorized into two main classes as in Table 5 including pre-trained base models and supervised fine-tuned (SFT) models. Base models are pre-trained on large-scale unlabelled training corpora in the manner of self-supervised learning like next token prediction (Radford et al., 2018) and auto-regressive blank infilling (Du et al., 2022). This process endows the base models with basic language abilities. Base models can be fine-tuned (Ouyang et al., 2022) or distilled from a more powerful oracle model (Wang et al., 2022; Peng et al., 2023). Such SFT models can typically perform better on downstream tasks. Besides, low overhead to obtain a usable model makes SFT mainstream in the field of LLM development (Zhou et al., 2023; Lee et al., 2023).\nDuring inference, LLMs iteratively repeat the process of predicting the next tokens until the maximum length limit is reached or an end-of-sequence token (<EOS>) is encountered. LLMs can parallelly process all sub-sequences $\\{X_{1:i}\\}_{i=1}^{S}$ of the whole input in one single forward pass uti-lizing the mask design and finally outputs $R \\in \\mathbb{R}^{S\\times V}$, where $R_i = f_e(X_{1:i})$. A new token will be selected according to Rs and its embedding will be concatenated with the previous sequence to form a new sequence $X_{1:S+1}$, used to predict the following tokens. Another representative line of LLMs, ChatGLM (Du et al., 2022), incorporates a different attention mask but still involves an auto-regressive inference scheme. Both types of auto-regressive LLMs are explored in this paper.\nText decoding methods, which decide how to utilize Rs (the predicted next-token logits) to choose a new token, are essential to natural language generation. Greedy search (Su et al., 2022) is the simplest way, which directly selects the token with the maximum probability in Rs. In probabilistic sampling, the decoding process replaces the word with the highest probability with probability-based sampling. The sampling allows for more diversity in the sequence generation process. Since the sampling method can generate more diverse outputs, most existing LLMs use the sampling method (Holtzman et al., 2019) for decoding."}, {"title": "A.2 FEASIBILITY AND IMPLICATION DISCUSSION FOR THREAT MODEL", "content": "In our attack", "assumptions": "white-box attack and black-box attack. White-box attack assumes a more powerful attacker with knowledge about the target model's parameters. This setting is rational in the real world in two folds. First", "applies": "n\u2022 Subscription-based services using open-source models: Many LLM service providers", "public": "With the growth of the open-source community", "users": "For many users, even incremental fine-tuning of LLMs is prohibitive. As a result, users tend to directly use well-trained LLMs for applications. Popular tools like llama.cpp10 and ollama\u00b9\u00b9 are commonly used for this purpose. However, when these services are exposed online, they will become vulnerable to Engorgio prompts. Such prompts can consume a great amount of computational resources and degrade service availability. We also explore the attack effectiveness when facing LLM services with prompts in Appendix B.3.\nFor the motivation of the attacker, we have shown the user-level impacts of Engorgio prompts in ser-vice availability and service quality in Section 4.5. For service providers, many commercial LLM service providers are struggling to meet high inference demand due to limited computing resources. This challenge is reflected in the rate-limiting strategies commonly employed by these providers. Beyond token-based rate limits, request-level rate limiting is also widely used for subscription and free-tier users. For example, platforms like OpenRoute and Codestral limit the number of queries for free-tier users to a certain requests per minute/day. Similarly, the Huggingface serverless inference API explicitly states that the service enforces request-based rate limits, rather than limiting based on compute or tokens. GitHub Models primarily restrict access by requests per day for different sub-scription plans, with tokens per request as a secondary concern, which aligns with our setting. Given this, an adversary's best strategy would be to maximize"}]}