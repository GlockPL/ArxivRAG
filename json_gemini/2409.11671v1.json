{"title": "Anticipating Oblivious Opponents in Stochastic Games", "authors": ["Shadi Tasdighi Kalat", "Sriram Sankaranarayanan", "Ashutosh Trivedi"], "abstract": "We present an approach for systematically anticipating the actions and policies employed by oblivious environments in concurrent stochastic games, while maximizing a reward function. Our main contribution lies in the synthesis of a finite information state machine whose alphabet ranges over the actions of the environment. Each state of the automaton is mapped to a belief state about the policy used by the environment. We introduce a notion of consistency that guarantees that the belief states tracked by our automaton stays within a fixed distance of the precise belief state obtained by knowledge of the full history. We provide methods for checking consistency of an automaton and a synthesis approach which upon successful termination yields such a machine. We show how the information state machine yields an MDP that serves as the starting point for computing optimal policies for maximizing a reward function defined over plays. We present an experimental evaluation over benchmark examples including human activity data for tasks such as cataract surgery and furniture assembly, wherein our approach successfully anticipates the policies and actions of the environment in order to maximize the reward.", "sections": [{"title": "Introduction", "content": "Concurrent stochastic games [15,13,14,11,24,38] offer a natural abstraction for modeling conservative decision-making in the presence of multiple agents in a shared and uncertain environment. In this scenario, the objective of the Ego agent-player P\u2081 is to maximize their desired outcome irrespective of the decisions taken by other agents, represented here as a single agent that we term player P2 [6]. In a zero-sum game, the objective of player P\u2081 is deemed to be in direct conflict with player P2. The opposite scenario assumes cooperation, wherein P2's actions are aimed to maximize the reward for P1. In this paper, we study another \"extreme\", wherein P2 is assumed to be oblivious. Their actions are chosen from a predefined set of policies or objectives that are not affected by the actions of P1. We will show that in such a setting, player P\u2081 needs to anticipate P2's moves to maximize their own reward.\nConsider a game of Rock-paper-scissors (RPS) against an oblivious adversary. Recall that at each turn, players P\u2081 and P2 simultaneously reveal their choice with a show of hands, and both players receive values (see, Figure 1) based on straightforward circular-dominance rules (rock defeats scissors, scissors defeats paper, paper defeats rock). The repeated, oblivious RPS can be modeled as a single state concurrent stochastic game, where the goal of player P\u2081 is to maximize the sequence of rewards"}, {"title": "Problem Definition", "content": "A probability distribution d: X\u2192[0,1] over a finite set X satisfies \u2211sexd(s)=1. Let D(X) represent the set of all probability distributions over X. The distribution d over X={x1,x2,\u2026,xm} is written {x1:P1,...,xm:pm} where pi=d(xi) for i\u2208 [m]. For a natural number n\u22651, let [n]={1,2,...,n}. Bold case letters denote vectors b\u2208R\u201d. The ith component of b is denoted as bi.\nA Markov decision process (MDP) M is a tuple (S,A,P,R) where S is a finite set of states, A is a finite set of actions, P:S\u00d7A\u2192D(S) is the probabilistic transition function, and R:S\u00d7A\u2192R is a scalar valued reward function. We write P(s' s,a) for the probability of state s' if action a is applied to state s. In a two player concurrent game, the set of actions are partitioned between player P\u2081 and P2. Transitions of the game are determined by joint actions of both players.\nDefinition 1 (Concurrent Stochastic Game Arena: Syntax). A concurrent stochastic game arena G is a tuple (S,A(1), A(2), P,R) wherein S is a finite set of states, A(1) and A(2) are disjoint sets of actions for players P\u2081 and P2, respectively, P: S \u00d7 A(1) \u00d7 A(2) \u2192 D(S) is the joint probabilistic transition function, and R : S\u00d7A(1) \u00d7 A(2) \u2192R is a reward function for P\u2081.\nWe assume that player P2 selects their policy from one of n different stochastic policies from the set II={\u03c01,...,\u03c0\u03b7}, wherein each \u03c0\u2081:S\u2192D(A(2)) represents a map from states to probability distributions over actions in A(2). Let \u03c0i(s,a) denote the probability that action a is chosen from state s for policy \u03c0\u03af.\nExample 1. Consider the RPS example discussed in the introduction (Figure 1). The state set is a singleton: S = {t}. We have three actions each for players 1,2: A(1) = {r1,p1,81} and A(2) = {r2,P2,82}, corresponding to choices of \u201crock\u201d, \u201cpaper\u201d and \"scissors\", respectively. The transition probabilities are simply P(t t,a,b) = 1 for all a \u2208 A(1), b \u2208 A(2). The reward for P\u2081 is the familiar one from the game of rock-paper-scissors, and is shown in Figure 1 (left) P2 plays one of four possible policies shown in the middle table of Figure 1.\nAssumption 1 (Observation and Obliviousness) We assume that: (a) P\u2081 observes the past actions of P2 but the current action of one player is not observable by the other. (b) P2 is restricted to playing one of the policies {\u03c01,...,\u03c0\u03b7} but this choice is not observable by P1.\nPolicy Change Model: We assume that P2 can change policies at each step depending on their current policies according to a Markov chain with n states labeled by the corresponding policies \u03c01,...,\u03c0\u03b7. Let T represent the transition matrix of this Markov chain such that the entry Tij = P(\u03c0j|\u03c0\u2081) represents the probability of P2 switching their policy to \u03c0; given that their current policy is \u03c0\u2081. Returning to Example 1, the Markov chain for switching between the four policies \u03c0\u2081,...,\u03c04 is shown in Figure 1 (right)."}, {"title": "Information State Machine and Consistency", "content": "The main approach is to use a sequence of observations of states and P2 actions to infer a belief state b over the player's policies.\nDefinition 3 (Belief State). A belief state b: (b1,...,bn)\u2208Rn is a vector wherein the ith component bi represents P\u2081's belief that P2 is employing policy \u03c0\u017c\u2208\u03a0. Note that bi\u22650 for all i\u2208 [n] and 1bi=1.\nLet Bn = {b\u2208 Rn | (Vi\u2208 [n]) bi \u2265 0 ^ \u2211=1bi=1} denote the set of all belief state vectors in Rn. The uniform belief state bu is given by (1,...,). We define two operations over a belief state: (a) conditioning a belief state given some observation and (b) capturing the effect of policy change on a belief state.\nLet b be a belief state and (s,a\u2082) represent an observation where s\u2208 S and a2 \u2208 A(2) represent states of the game and actions for P2. The belief state b' = condition(b,s,a2) is obtained by conditioning b on the observation (s,a\u2082):\nb'=condition(b,s,a2)=$\\frac{\\pi_i(s,a_2)b_i}{\\Sigma_{j=1}^n\\pi_j(s,a_2)b_j}$ (1)\nThis expression is obtained as a direct application of Bayes' rule."}, {"title": "Policy Synthesis", "content": "Given an ISM M, we will now describe the policy synthesis for P\u2081 and prove bounds on the optimality of the policy thus obtained w.r.t discounted rewards. We first compose a two player game graph G: (S,A(1), A(2), P,R) with the ISM M: (\u039c,\u03a3',\u03b4) wherein \u03a3' \u2286 S\u00d7A(2). This MDP serves as a starting point for optimal policy synthesis. Next, for a \u5165-consistent information state machine. We show that this MDP is \"close\" to an infinite state MDP obtained from unbounded histories. We invoke a result on approximate information states (AIS) by Subramanian et al [42] to bound the difference between the optimal value function obtained from finite state histories and that from full histories."}, {"title": "Completeness and Robustness", "content": "In this section, we first provide a sufficient condition on the transition matrix T that governs how P2 switches between policies so that Algorithm 1 is guaranteed to terminate successfully and yield a finite ISM. Let t* be such that for all i,j\u2208 [n], Tij \u2265t*.\nI.e, t* is the smallest entry in the matrix T. We assume that t* >0: i.e, the transition matrix T is strictly positive. Note that the entries for each row of T sum up to 1. Therefore, t* <. Let b=r(bo,o) be the exact belief state obtained starting from the uniform initial belief state bo and a sequence of non-zero probability observations \u03c3.\nLemma 3. Each entry of b satisfies bj\u2265t*."}, {"title": "Experimental Evaluation", "content": "We present an experimental evaluation based on an implementation of the ideas mentioned thus far. Our implementation uses the Python programming language and inputs a user-defined game structure, n policies for player P2, values for parameters >>0. For each case, the policy design Markov chain whose transition system is given by T(\u20ac), such that T(\u20ac)i,i =e and T(e)i,j = $\\frac{1-\\epsilon}{(n-1)}$ when i\u2260j. In other words, player plays the same policy as previous step with probability e and switches to a different policy uniformly with probability (1-6)/(n-1). Our implementation uses the Gurobi optimization solver [16] to implement the consistency checks described in Section 3 and uses it to implement the consistent information state machine synthesis as described in Section 4.\nPerformance Evaluation on Benchmark Problems. We consider benchmarks for evaluating our approaches in terms of the ability to construct finite information state machines, the sizes of these machines and the performance of the resulting policies synthesized by our approach."}, {"title": "Conclusion", "content": "We study concurrent stochastic games against oblivious opponents where the opponent (environment) is not necessarily defined as adversarial or cooperative, but rather oblivious that is bounded to choose from a finite set of policies. We introduce the notion of information state machine (ISM) whose states are mapped to a belief state on the environment policy, and provide the guarantee that the belief states tracked by this automaton stay within a fixed distance of the precise belief state obtained by tracking the entire history for the environment. In the future, we would like to better characterize the relationship between the various parameters involved in Algorithm 1 to provide a tighter condition for its termination. We are also interested in understanding the applicability of these ideas to the more general case of partially observable Markov decision processes."}, {"title": "Appendix: Ikea Furniture Assembly and Cataract Surgery Graphs", "content": "The ikea furniture assembly dataset was taken from the previous work of Ben-Shabat et al [4]. It involves sequences of tasks for four different furniture types with roughly 90 sequences for each furniture type. We employed a 75%-25% training/testing data split. The tool flexfringe was used to learn an automaton model using sequences in the training data."}, {"title": "Rock Paper Scissors with Memory", "content": "We describe the RPS-MEM benchmark used in our approach. The state of the game G is given as S=A1 \u00d7 A2 wherein A\u2081 = {r1,p1,81} and A2={r2,p2,82} while A(1) = A1 and A(2) = A2. The transition map is given as follows:\nP(($1,82) | 8,a1,a2)=\\begin{cases} 1 &\\text{if s1=a1, s2=a2}\\\\\n 0 & \\text{otherwise} \\end{cases}\nIn other words, the state s \"remembers\" the previous action of both players. The reward map for each state is identical to that of the RPS game from Example 1.\nWe define 9 policies for P2.\nPolicy \u03c0\u2081 chooses rock/paper with 0.45 probability and scissors with 0.1 probability regardless of the state."}, {"title": "Anticipate and Avoid", "content": "Anticipate and Avoid game involves a circular arena with N cells labeled 1,..., N. The state space S encodes joint positions of two players in this arena.\nS={(i,j) | 1<i<N,1<j<N}.\nLet us define i1 as the same as i+1 if 1leqi <N-1 and to be 1 if i=1. Likewise, we define ie1 as i-1 for 2<i<N and N if i=1.\nThe actions are A(1) = A(2) = {L,R} standing for left and right, respectively. Let us define p(ji,a) for a single player as follows:\np(ji,a)=\\begin{cases} 0.2 &\\text{j=i}\\\\\n 0.8 & \\text{j=i+1,a=R}\\\\\n 0.8 & \\text{j=i-1,a=L}\\\\\n 0 & \\text{otherwise} \\end{cases}\nIn other words, upon moving left, the player may stay in the same cell with 0.2 probability or move to \"previous\" cell with 0.8 probability and similarly for moving right.\nThe reward map is defined by first defining a state distance function:"}]}