{"title": "LEARNING TO CONSTRUCT IMPLICIT COMMUNICATION CHANNEL", "authors": ["Han Wang", "Binbin Chen", "Tieying Zhang", "Baoxiang Wang"], "abstract": "Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.", "sections": [{"title": "Introduction", "content": "Effective communication is pivotal in collaborative multi-agent systems, especially in environments characterized by incomplete information (Panait & Luke, 2005; Busoniu et al., 2008; Tuyls & Weiss, 2012). Communication acts as a vital conduit, enabling agents to exchange private information, coordinate joint actions, and infer real-world states (Wang et al., 2021). These processes synergistically foster tighter cooperation and enhance collective performance (Li et al., 2002; Cao et al., 2012). We focus on multi-agent reinforcement learning (MARL) methods for communication, where communication is broadly categorized into explicit and implicit strategies (Dafoe et al., 2020).\nExplicit communication uses direct channels independent of the environment dynamics (Sukhbaatar et al., 2016; Fo-erster et al., 2016; Jiang & Lu, 2018), allowing agents to transmit observations, intentions, and advice to facilitate decision-making and coordination (Zhu et al., 2022; Qu et al., 2021). This approach, analogous to human language or verbal exchanges (Havrylov & Titov, 2017; Baker et al., 1999), has been widely employed in MARL to enhance collaboration. However, dependence on direct channels introduces significant computational and memory overheads (Roth et al., 2006), which makes it challenging to implement in certain scenarios, like tasks without communication channels or decentralized frameworks (Oliehoek et al., 2008; Kraemer & Banerjee, 2016).\nSituations where explicit messaging is not feasible have been common in human society throughout history. From early humans engaging in hunting and gathering through silent cooperation (Klein, 2009; Tomasello & Vaish, 2013), to modern military operations using gestures and codes for covert communication (Tzu, 2008), and even in everyday social interactions where intentions are conveyed through expressions, tone, and body language (Pease, 1984; Dun-can Jr, 1969). Implicit communication has established an effective mechanism for information sharing without explicit language."}, {"title": "Background and Related Work", "content": "Decentralized Partially Observable Markov Decision Process Cooperative multi-agent problem can be formu-lated as a Dec-POMDP game (Bernstein et al., 2002), which is described by the tuple $G = (S,U,N,T,O, R, \\gamma)$, where S represents the true global state of the environment. At each discrete time step t, every agent $i \\in N := {1, ..., n}$ selects an action $u_i$ from its action space $U_i$. The state transition function $T(s'|s, u) : S \\times U \\times S \\rightarrow P(S)$ governs the transition of states, where $u = (u_1,..., u_n)$ denotes the joint action. In POMDP, the global state re-mains inaccessible, and each agent i can only perceive its individual observation $O_i$ or through the observation function $O_i(s): S\\timesN \\rightarrow O$. $R_i(s,u_i) : S \\times U_i \\rightarrow R$ represents the reward function for each agent i. In the cooperative scenarios, all agents share a common reward function $R(s, u) : S \\times U \\rightarrow R$, known as team reward. The objective for each agent is to maximize the expected return, which makes effective cooperation among all agents necessary.\nLearning to Communicate Communication among agents is critical for effective collaboration in MARL. Most researches in this field focused on explicit communication protocols (Tucker et al., 2022; Peng et al., 2017; Kong et al., 2017; Pesce & Montana, 2020; Kim et al., 2019; Wang et al., 2020; Freed et al., 2020b,a; Gupta et al., 2023; Foerster et al., 2016), where agents exchange messages containing critical information. Additionally, techniques like attention mechanisms or graph-based methods that used to build more effective communication connection (Jiang & Lu, 2018; Das et al., 2019; Jiang et al., 2018; Sukhbaatar et al., 2016; Chen et al., 2024; Tucker et al., 2022) and intention sharing or theory of mind (ToM) reasoning to transmit more useful information (Wang et al., 2021; Kim et al., 2020; Qu et al., 2021), further enhancing the effectiveness of MARL systems.\nExplicit communication on direct channel often incurs significant communication overhead and may not be available in environments with limited bandwidth or high communication costs. Implicit communication, where agents convey information through their behaviors, has been explored as an alternative approach (Li et al., 2024; Tian et al., 2023; Li et al., 2021; Shaw et al., 2022; Grupen et al., 2022). Theory of mind (ToM)-based methods represent one main research strand for implicit communication, where agents infer the intentions and beliefs of others to anticipate actions and adjust strategies accordingly. Methods in Rabinowitz et al. (2018); Tian et al. (2020); Nguyen et al. (2020); Zhao et al. (2023) aim to enhance coordination by modeling and predicting others' behavior based on observed actions. However, ToM-based approaches can be computationally intensive and may struggle to accurately infer intentions in complex environments."}, {"title": "Setting", "content": "Consider in a fully cooperative Dec-POMDPs, where no direct communication channel is available. Each agent maintains a joint action-observation history $T_{t,i} = {o_{0,i}, u_0, r_1, o_{1,i}, ..., r_t, o_{t,i}}$ and makes decisions and executes actions based on it. Within this setting, we define a subset of actions as scouting actions $U^s$ which have no or uniform effects on the state $s_t$ or reward $r_t$ and mainly affect the observation function $O_i(s_t)$. The remaining actions are defined as regular actions $U^r$.\nIn an idealized situation, scouting actions serve to influence the observation mapping function, which allows the agents to gather critical information from the environment to improve their decision-making and coordination. A scouting action might reveal information about an agent's surroundings or the state of other agents without changing environment unpredictable. For instance, in the game of Hanabi, Hint actions reveal information to other players while consuming an information token, thereby uniformly altering the game state. Similarly, in StarCraft II, scouting the map with Scan from Terrans requires depleting energy, which impacts the environment and provides new information in the agent's observations.\nWe are interested in this setting because agents generally need to employ scouting actions to collect sufficient infor-mation before taking other actions to obtain rewards. Both the information collection phase and the reward acquisition phase may require cooperation among agents. For example, agents will only receive positive rewards if one agent scouts the correct information for another agent, and the latter correctly utilizes this information. In this context, sparse and delayed reward feedback, along with the credit assignment problem both temporally and among agents, poses significant challenges for agents to learn optimal strategies."}, {"title": "Information Carried by Scouting Actions", "content": "Scouting actions carry two types of information:\n1. Information reflected through the environment: When an agent performs a scouting action, it influences the observation function, and the resulting observations provide information. This type of information de-pends on the observation function, states, and the joint actions of all agents. All scouting action will change observation function and bring information.\n2. Information reflected through the choice of the scouting action: The specific scouting action chosen by an agent can carry intentional meaning. This type of information only depends on other agents' scouting action policy. But while receiver agent can't understand sender agent's intention, no information will be transmitted.\nGeneral multi-agent reinforcement learning (MARL) methods tend to focus more on the first type of information. In Dec-POMDPs without model other agents' strategies, both partial observe of states and other agents' policy introduce uncertainty. And in exploration phase of online MARL training, second type of information can't be utilized due to unknown policies of others. Therefore, MARL methods mostly learn to use the information reflected through the environment.\nFor second type of information which is not limited by observation function and states, when agents' intention is understand correctly, it is more fixable and could be more useful for decision making and cooperation. Theory of mind (ToM) methods utilized second type of information by modeling strategies or inferring intention of other agents. By estimating the internal states of others, an agent can interpret the intentional choices behind their scouting actions. One approach is to build an explicit belief model that infers the state from observed actions to achieve communication. However, this method incurs a computational cost that grows exponentially with the size of the game and the number of agents. Moreover, the precise state distribution information is often difficult to fully utilize. Another approach involves training a neural network to map actions to states or intentions, which requires exponentially more data compared to action policy training. Since this method does not produce explicit inferences, the resulting information is inherently biased, and this bias can negatively impact the agent's decision-making process.\nTo take advantage of second type information conveyed by scouting actions and to mitigate the computational over-head and inaccuracies of ToM methods, we propose a simpler and more effective solution. By establishing a shared communication protocol where all agents follow the same strategy for selecting scouting actions, the intended infor-mation can be easily decoded from the chosen scouting action. This approach eliminates the need for state estimation and intention inference required in ToM methods. In our setting, this communication protocol has following benefits:\n1. Low Computational Overhead and Accurate Information Transmission: By having all agents follow the same strategy for selecting scouting actions, there is no need for additional computational inference or"}, {"title": "Strategy Training on Constructed Implicit Channel", "content": "To establish a shared communication protocol, we propose Implicit Channel Protocol (ICP), a new framework for agents' implicit communication. In this framework, at each step, an agent chooses whether to send information. If not, a regular action $u^r$ is taken according to the action policy. If otherwise the agents wish to convey message $m_i \\in M$, it maps $m_i$ through the centralized mapping mechanism P, which outputs a scouting action $u^s = P(m_i)$. This mechanism P needs to be decodable, which provides an output message from scouting actions. By allowing agents to send information through encoded scouting actions and other agents to find the message from the globally observed scouting actions, an implicit communication channel is constructed.\nAs the agent needs to decide whether to send information, the new action space becomes $U' = {U - U^s, send\\_info}$, then agents sample actions $u_i \\in U'$ from new action policy $u_i \\sim \\pi_i(\\cdot | T_{t,i}, M_{-i})$, where $m_{-i}$ is decoded messages transmitted from other agents. For message $m_i$, it is selected by the message strategy $m_i \\sim \\phi_i(\\cdot | T_{t,i}, m_{-i})$. Based on this process, we can formulate agents' value function as $V^{\\pi_i,\\phi_i,P}(s) := E[\\sum_{t=1}^{T-1} r_t(s_t, u_t, s_{t+1}) | s_1 = s]$, for $s_{t+1} \\sim T(\\cdot | s_t, u_t)$ and $u_t = (u_{t,1}, ..., u_{t,n})$,\nand $u_{t,i} = \\begin{cases}\n    u^s = P(m_i \\sim \\phi_i(\\cdot | T_{t,i}, m_{-i})) & \\text{if send\\_info,}\\\\\n    u^r \\sim \\pi_i(\\cdot | T_{t,i}, m_{-i}) & \\text{else.}\n  \\end{cases}$\n\nEquation (1) highlights three essential components of the framework: the agents' action policy $\\pi$, the message strategy $\\phi$, and the centralized mapping mechanism P. To develop these components, we investigate two different method-ologies: The training with random initial information map and the training with delayed information map. Both approaches construct an implicit communication channel within the environment, with which the agents exchange in-formation and achieve implicit coordination. In the rest of the section, we will discuss the details of these approaches.\n5.1 Strategy Training with Random Initial Map\nIn this approach, we first establish a one-to-one mapping between information and actions using a randomly initialized embedding, that makes the size of the message same as the size of the scouting action space $M = {1, ..., |U^s|}$. Once this one-to-one mapping is set, the inverse of P will obtain the original message $m_i = P^{-1}(u)$.\nIn our implementation, we use a value based method for training the action policy, incorporating parameter shar-ing and value decomposition techniques to facilitate effective coordination among agents. For the information strat-egy, we use the communication gradient method, which is designed to optimize communication protocols within a predefined channel. Both the action policy and the information strategy are parameterised as Q-networks, for ac-tion policy $\\pi_i(T_{t,i}) = argmax_u Q_{\\theta_1,i}(T_{t,i}, u_i)$, and message strategy $\\phi_i(T_{t,i}) = argmax_m Q_{\\theta_2,i}(T_{t,i}, m_i)$. Given the value function $V^{\\pi_i,\\phi_i,P}(s)$, the Q-functions are defined by $Q_i(T_{t,i}, u_i) := E_{m \\sim \\phi,P}[r_t(s_t, u_t, s_{t+1}) + \\gamma V^{\\pi_i,\\phi_i,P}]$, and $Q_i(T_{t,i}, m_i) := E_{u \\sim \\pi,P}[r_t(s_t, u_t, s_{t+1}) + \\gamma V^{\\pi_i,\\phi_i,P}]$, where $\\pi = (\\pi_1,..., \\pi_n)$, $\\Phi = (\\phi_1,..., \\phi_n)$ and $m = (m_1, ..., m_n)$.\nParameter Sharing and Value Decomposition Parameter sharing enables different agents with the same observa-tion and action spaces to learn from a shared network. Despite using the same network, agents evolve different hidden states and receive distinct observations, allowing them to behave differently. We incorporate the agent index i into the network input, allowing for specialization through rich representations in deep Q-networks.\nTo further enhance learning efficiency, we apply value decomposition (Sunehag et al., 2018) for the action policies $\\pi$. The joint action-value function $Q_{tot}(T, u)$ is expressed as the sum of individual value functions: $Q_{tot}(T, u) = \\sum_{i=1}^{N} Q_i(T_{t,i}, u_i; \\theta_{1,i})$, where $T = (T_1,..., T_N)$. This decomposition simplifies learning by allowing the gradients"}, {"title": "Strategy Training with Delayed Map", "content": "In the training process with delayed map, we begin by learning the information strategy and action policy using an direct communication channel. This initial phase provides agents with a flexible environment that allows for direct communication, enabling them to develop their strategies more effectively. Once these strategies are well-established, we then define the mapping mechanism that maps the messages to actions. Finally, we fine-tune both the action policies and information strategies in ICP framework to ensure optimal performance within the constraints of the final environment.\nGiven that the information strategy was initially trained using an direct communication channel, there may be discrep-ancies between this channel and the implicit communication channel constructed by scouting actions, particularly in terms of capacity and whether the channel is discrete or noiseless. To minimize any potential performance loss re-sulting from these differences, we can limit the capacity of the direct channel or apply efficient embedding techniques during the message-to-action mapping process. These steps help to align information strategies developed in the direct channel with the constraints of the implicit channel and ensure messages are decodable.\nDuring implementation, we utilize standard explicit communication algorithms designed for direct channels, such as RGMComm, to pre-train the information strategy. Then we use two kinds of mapping methods, including simple one-to-one mapping and the hat mapping method to obtain new information strategies. The one-to-one mapping ensures that each message corresponds directly to an action, while the hat mapping allows an agent to communicate efficiently with multiple receivers. Finally, we fine-tune both the information strategy and the action policy to optimize performance within the implicit communication framework.\nPre-training with Direct Channel In the pre-training phase, we utilize an direct communication channel to facilitate the learning of the communication strategy. During this phase, the message size can be larger than the scouting action space, allowing for more complex and detailed communication between agents. This flexibility enables the agents to explore a wider range of communication strategies and coordination behaviors, which are crucial for solving complex tasks in partially observable environments. However, after pre-training, the message strategy needs to transition to the implicit communication framework and it is necessary to lossless compress these messages. For one-to-one mapping, this mapping requires that message size match the scouting action space exactly. It ensures a direct and consistent mapping between the learned messages and the corresponding actions.\nHat Mapping Implementation When the local observations of different agents' have large overlaps, hat principle-based mapping method utilizes broadcast channels to achieve a single transmission of messages to multiple receivers. The method is inspired by the multi-color hat guessing game, where players use logical deduction based on limited communication to maximize team success. In this game, players can observe the hat colors of others but not their own, and they must guess their own hat color using a pre-defined strategy. A well-known strategy involves each player leveraging the sum of the hat colors they observe, modulo the total number of possible colors. For example, in an 8-color version of the game, each player can compute the sum of the colors they observe (assigned numerical values) and use a pre-determined rule, such as calculating the sum modulo 8, to make a logical deduction about their own hat color."}, {"title": "Experiments", "content": "In this section, we explore the application of the ICP across various environments where no direct communication is available. ICP plays a vital role in these settings by constructing an implicit communication channel that enhances the agents' ability to share and interpret information. We will demonstrate how ICP facilitates effective collaboration and decision-making in 3 tasks, namely Guessing Number, the Revealing Goals, and the Hanabi card game. The first two tasks are designed by us and can be reused by future works as testing environments. The Hanabi game is a popular card game played by humans, and is described as The Hanabi Challenge (Bard et al., 2020) by the community. Each of these environments presents unique challenges, and we will show how ICP optimizes the use of scouting actions to improve overall performance and strategy.\n6.1 Guessing Number\nIn the Guessing Number Game, we present a collaborative, turn-based multi-agent reinforcement learning environment involving N agents. Each agent i is uniquely assigned a digit $d_i \\in {0,1,...,9}$, which is visible to all other agents $j \\neq i$ but remains unknown to agent i themselves. The primary objective for each agent is to deduce and correctly guess their own digit $d_i$. During each turn, an agent can choose to either guess their own digit by selecting $d_i \\in {0,1,..., 9}$ or provide a hint about another agent's digit using the Radioland Slim font\u2014a seven-segment digital display representation. Specifically, hints involve revealing the state (\"on\" or \"off\") of one of the seven segments of another agent's digit, and these hints are public and observable by all agents. Each agent's action space comprises $10 + 7 \\times (N - 1)$ actions: 10 options for guessing their own digit and 7 hint options for each of the $N - 1$ other agents. The game encourages strategic collaboration, as agents must balance between gathering information to deduce their own digit and assisting others through informative hints, aiming for the collective success of all agents correctly guessing their digits.\nIn this game, successfully guessing their digit rewards an agent with 10, while performing a hint action incurs a small penalty of -0.1 to encourage efficient communication and collaboration. The game imposes a limit l on the number of hint actions, and each agent is allowed only 1 guess. The game concludes when all agents have made their guesses, with the goal being to accurately guess all digits within these constraints."}, {"title": "Revealing Goals", "content": "In the Revealing Goals task, we present a non-sequential collaborative that emphasizes information sharing among agents. The environment consists of N agents operating within an $H \\times H$ grid world. Each agent i starts at a random position and is assigned a unique goal location $g_i$ that is at least two grid units away from its starting position. Critically, agents cannot perceive their own goal locations; they can only observe the goal locations of other agents $j\\neq i$. At each time step, agents select from a fixed action space of eight actions: moving up, down, left, or right, and reveal information about the adjacent grid cells in these directions. When an agent performs a reveal action, the adjacent grid cell in the specified direction becomes revealed, and all agents gain information about whether their own goals are located in that cell. The grid world features wrap-around edges, creating a toroidal topology where, for example, moving left from cell (0, y) leads to cell (H \u2013 1, y).\nAn agent observes the locations of all agents, the goal points of other agents, and any goals on the revealed grids. Since they cannot directly perceive their own goals, agents must rely on information revealed by themselves and others to infer the locations of their own goals. The objective is for each agent to navigate to its designated goal location $g_i$, upon which all agents receive a shared reward of 1. When an agent reaches its goal, a new goal is randomly assigned to it but is at least two grid units away from its current position. The game proceeds for a total of T time steps. This environment encourages strategic collaboration, as agents must balance between exploring the grid to find their own goals and assisting others by revealing grid cells that may contain teammates' goals. The agents will need to communicate implicitly, using the fixed action space of size 8."}, {"title": "Hanabi", "content": "To demonstrate ICP's broader applicability, we also evaluate it in the game of Hanabi. Hanabi is a fully cooperative card game where players work together to play cards in a specific order to form sequences, aiming to achieve the highest possible score. When it is played by humans, people will mute themselves and coordinate only through taking and observing actions. This game requires effective implicit communication and strategic information sharing among the agents.\nBecause the game rule of Hanabi is quite involved, we refer it to a popular website https://hanabi.github.io/ that discusses both rules and human-play strategies. The action space for each agent includes three options: playing a card out of their 5-card hand, discarding a card, or giving a hint towards another player's 5-card hand. The hint could reveal all cards in a player's hand of a specific color or rank. Since they cannot see their own cards, agents must rely on hints from others to infer what cards they hold. The success of the game heavily depends on the agents' ability to use these hints effectively to coordinate their actions and play the correct cards in order.\nIn our experiments, we focused on the 4-player full version of Hanabi. Since hint actions in Hanabi are natural scouting actions, we selected them to serve as information carriers for our ICP implementation. We implemented several baseline algorithms, including VDN-off-policy and the original version of SAD (Hu & Foerster, 2019), as well as a variant of SAD with the hint action masks removed. For the ICP algorithm, we explored several variations, including ICN-DIAL-on-policy, which involves strategy training with a random initial map using DIAL's communi-cation gradient technique resulting in an on-policy learning method. We also implemented ICN-DIAL-cut-gradient-off-policy, similar to ICN-DIAL-on-policy but with the communication gradient chain cut off to enable off-policy learning. Additionally, we used ICN-RGMComm, which is strategy training with a delayed map using RGMComm (Chen et al., 2024), an off-policy MARL communication algorithm, and ICN-RGMComm-hat-mapping, an extension of ICN-RGMComm that utilizes the hat mapping method for improved communication.\nOur presented results are based on the best runs selected from more than three random seeds. From the results shown in Figure 3(b), we observe that the ICP method using the hat mapping approach, ICN-RGMComm-hat-mapping, achieved the best performance. The score achieved by ICP is 24.91, which surpasses the best available learning algorithm (SAD, 23.81 points) and vastly outperforms theory of mind-based methods (Fuchs et al. (2021) 19.13 points) and average human players (17 points, reported in Kantack (2021)). It is worth remarking that the state-of-the-art strategy in the Hanabi game is a search-based algorithm, WTFWThat+search, which reports an averaged score of"}, {"title": "Discussion", "content": "7.1 Compatibility of ICP with Other Training Methods\nIn the ICP framework, action policy allows for the use of various algorithms for training. For instance, the action policy can be trained using value-based methods like VDN (Sunehag et al., 2018) or policy-based methods such as MADDPG (Lowe et al., 2017). This compatibility ensures that ICP can be tailored to various problem settings and agent dynamics, allowing researchers to choose the most suitable algorithm depending on the specific characteristics of the environment and the agents involved.\nThe communication strategy within the ICP framework is also versatile. It can be implemented using a variety of direct channel communication algorithms that support discrete channels. These include well-established methods like DIAL or RGMComm, which are designed to optimize communication under different constraints and requirements. The flexibility in choosing both the action policy and the communication algorithm highlights the comparability and broad applicability of the ICP framework, making it a general approach for multi-agent reinforcement learning scenarios.\n7.2 Potential of Further Utilizing the Environment Information\nAs described in Section 4, we analyzed that scouting actions carry two types of information. The ICP framework utilizes the information reflected through the choice of the scouting action (choice information) to establish an implicit communication channel. However, even when we leverage this information, the information reflected through the environment (environment information) does not disappear. Although in most cases the constructed implicit channel transmits more stable and useful information, in some situations the environment information is also significant. For example, in the Revealing Goals environment, a revealing action intended to transmit embedded information might also reveal other agents' goals, which helps agents to make better decisions.\nTo verify the impact of this environmental information on performance, we randomly shuffled 6 different embeddings on the ICN-RGMComm-hat-mapping policy saved from the previous Hanabi experiment. We then fine-tuned the action policy while keeping the information strategy fixed. The results are shown in Figure 4, where, under a fixed information strategy, some randomly shuffled embeddings enjoy improved performance. This suggests that one could further utilize the environment information for more effective communication. We leave this research problem for future works."}, {"title": "Conclusion and Future Work", "content": "The Implicit Channel Protocol (ICP) introduced in this paper is an advancement in implicit communication through multi-agent reinforcement learning (MARL). By mapping information to scouting actions to construct implicit chan-"}, {"title": "Training Details", "content": "A.1 ICP implementation with DIAL and VDN\nIn our implementation, we employ a 2-head input and 2-head output Implicit Channel Net (ICN) to parameterize all Q functions. Specifically, at each time step t, for agent i, observation-action pairs (ot,i, Ut\u22121) are input to one Multilayer Perceptron (MLP) and Rectified Linear Unit (ReLU) layer. Additionally, the last message vectors from other agents (mk,1, where k = max(j | mj,1 \u2208 M)) forl \u2208 (1,..., N) are stacked together along with each message sender's ID, then input to another MLP and ReLU layer. The outputs of these two layers are summed and input to a Gated Recurrent Unit (GRU) network (Chung et al., 2014). This process approximately achieves the Q function input with action-observation history \u0442 and other agents' messages mi by utilizing hidden states and inputs at each time step.\nSubsequently, the output of the GRU is directed to two distinct heads: an action head and a message head. The action head generates logits for actions, with an output dimension of |U \u2013 Us| + 1, while the message head is specialized to handle the agent's communication behavior, producing logits for messages. Specifically, the output dimension of the message head is set to |Us| + 1, where the additional dimension represents the NOOP (no operation) action. This design ensures that the agent refrains from sending a message when the action sampled from the action logits does not correspond to send_info. To enforce this behavior, a mask is applied to the output of the message head. When the action sampled from the action logits is send_info, the mask restricts Gumbel-Softmax sampling to select the NOOP action. If the agent decides to send a message, the sampled one-hot vector of messages m directly maps to the j-th information action uinfo, where j = argmax\u0131\u2208(1,...,\nDuring centralized training sessions, action sampling follows an e-greedy policy to ensure exploration, while we maintain the gradient of the one-hot vector of messages m to ensure gradients pass through the communication process. However, since gradient information becomes inaccurate for updated policies, we can only use sampled episodes once, making our method on-policy. In decentralized execution sessions, we simply utilize the shared parameter network to manage greedy actions and one-hot vector messages sampled from Gumbel-Softmax.\nA.2 ICP implementation with RGMComm and VDN\nIn the pre-training phase of this approach, we used RGMComm, which utilized Regularized Information Maximization loss (Chen et al., 2024) to generate discrete messages. This method represents the latest state-of-the-art explicit communication algorithm in multi-agent reinforcement learning (MARL).\nRGMComm itself is based on the MADDPG (Multi-Agent Deep Deterministic Policy Gradient) framework (Lowe et al., 2017). Initially, a full observability centralized actor-critic model (Konda & Tsitsiklis, 1999) is trained to develop a policy with access to global information. This centralized policy serves as a reference to guide the communication strategy for each agent. During this stage, the agents use a centralized critic with a complete view of the environment to assess the action-value functions and optimize the communication strategy. After training this full-observability policy, RGMComm samples different information sets based on local observations. It then clusters these sampled information sets (Harsanyi, 1967) to assign specific messages to each cluster. By doing so, RGMComm derives a discrete communication protocol tailored to each agent's local observations, optimizing their joint policy under partial observability.\nIn our implementation, instead of using MADDPG, we employed an RDQN (Recurrent Deep Q-Network) (Hausknecht & Stone, 2015) to train the policy with full observability. For example, in environments like Hanabi, the difference between the full observation (global view) and the local observation is minimal, with only additional knowledge of the agent's own hand being considered. Using this full-observation policy, we performed clustering on the information sets, similarly to the RGMComm approach, to obtain the message strategy.\nFollowing the pre-training phase, we proceed to the fine-tuning phase. This phase involves using VDN (Value De-composition Network) to train the action policy. During this process, the communication strategy is embedded into the scouting action space and is fine-tuned to adapt to the environment. By freezing the RGMComm-generated com-munication strategy during the initial stages of fine-tuning, we ensure stability and later adjust the embedding to better align with the action policy."}, {"title": "Human Convention in Hanabi", "content": "Hanabi serves as a compelling platform for studying cooperation and ad-hoc teamwork, particularly within the MARL community, while also being an engaging game in its own right. Beyond its academic appeal, many players have"}, {"title": "ICP implementation with DIAL and VDN", "content": "Algorithm 1: ICP implementation with DIAL and VDN\nInput: environment env", "space\nOutput": "Trained model parameters\n1 Initialize shared RNN network weights (01", "2,": "max-episode-length do\n7 Initialize empty one-hot messages mt;\n8 for each agent i do\n9 Sample message and update hidden state: msample", "state": "Ugreedy", "q": "qt,i = Qo\u2081 (Ut,i | Ot,i, Mt-1,-i, h1);\n14 if ut,i= | Unon-info-1 | (send_info) then\n15 Update action ut,i\u2190 Uinfo = P(Msample, Ni\u2208"}]}