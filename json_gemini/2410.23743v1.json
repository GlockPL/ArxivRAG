{"title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective", "authors": ["Ming Li", "Yanhong Li", "Tianyi Zhou"], "abstract": "What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) excel at various complex tasks (Zhao et al., 2023b; Xu et al., 2024). But their complexity notoriously makes them \"black-box\" whose inner mechanisms and training behaviors remain mysterious (Zhao et al., 2023a; Singh et al., 2024). How do they acquire reasoning capabilities or knowledge? When do they make mistakes, and why? What change was made to each layer during training? This lack of transparency extends to issues like unintentional generation of harmful or biased content (Huang et al., 2024; Li et al., 2024a) or hallucinations (Huang et al., 2023) and might hinder further understanding and mitigation of them.\nInterpretable machine learning either develops models that are inherently interpretable (Rudin et al., 2022) or adopts post-hoc interpretability methods (Krishna et al., 2024), which do not alter the underlying model architecture but analyze models after training completes (Gurnee et al., 2023; Zou et al., 2023; Wang et al., 2023a; Wu et al., 2024). Despite the broad advancements in interpreting static models, gradients on dynamic training patterns of LLMs remains underexplored, especially on how these gradients scale and distribute across different layers. Such an understanding is crucial as it directly reflects how the LLMs perceive training data and align with them. Recently, there has been a growing trend for layer-related methods for LLMs: Gao et al. (2024) propose that higher layers of LLM need more LoRA; Li et al. (2024e) identify some of the layers in LLM related to safety, etc (Men et al., 2024; Chen et al., 2024). However, for the layer-wise analysis of LLMs, current research mainly employs probing methods (Alain and Bengio, 2017; Ju et al., 2024; Jin et al., 2024; Ye et al., 2024) that assess model behavior by observing changes in performance when certain layers are modified or removed (Wang et al., 2023a; Fan et al., 2024). These studies have been instrumental in illustrating how different layers capture and process various types of information while they often do not provide direct insights into the gradients that drive the learning process. Hence, we are motivated to move a step forward by directly investigating the layer-wise gradients inside LLMs.\nOur study focuses on the post-training gradient"}, {"title": "Methodology", "content": "In our experiments, we utilize the most commonly used instruction tuning settings to investigate the gradients for LLM fine-tuning. Given an instruction-tuning dataset $D$, each data sample is represented by a tuple $(ins, res)$, where $ins$ represents the instruction and $res$ represents the corresponding response. Let $\\theta$ denote the LLM with parameters $\\theta$. In the instruction tuning setting, $p_{\\theta}$ is typically fine-tuned by minimizing the following loss on each sample $(ins, res)$, in which $res_j$ represents the $j$th token of response $res$, $res_{<j}$ represents the tokens before $res_j$, and $I$ represents the token length of $res$:\n$\\mathcal{L}_{\\theta} = \\frac{1}{I} \\sum_{j=1}^I -\\log p_{\\theta} (res_j | ins, res_{<j}),$\n\nThe attention mechanism (Vaswani et al., 2017) is one of the most critical parts of modern LLMs, and it dominates the behavior of LLMs. Thus in this paper, we mainly focus on the gradients of the layers related to the attention mechanism, including the Query (Q), Key (K), Value (V) projection layers and later output projection layer denoted as Output (O). Considering the LLM contains N attention layers in total, after the loss calculation and Backpropagation, the resulting gradient for each projection layer is a matrix with the same size as its weights, which can be notated as $G_{Q,i}$, $G_{K,i}$, $G_{V,i}$, and $G_{O,i}$ for the corresponding projection layers, where $i \\in [0, N - 1]$ represents the index of each layer in the LLM.\nDue to the dramatically large number of layers and parameters of the modern Large language models, it is unrealistic to directly investigate these large gradient matrices. Motivated by the advanced Singular Value Decomposition (SVD) technology used in this area (Biderman et al., 2024; Carlini et al., 2024), we utilize Nuclear Norm (the $l_1$ norm of singular values) to represent the characteristics for the gradient of each layer, especially its strength.\nSVD is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. Specifically, e.g., for our gradient matrix $G_{X,i} \\in \\mathbb{R}^{m \\times n}$, $X \\in {Q, K, V, O}$, it can be decomposed as:\n$G_{X,i} = U \\Sigma V^T$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix containing the left singular vectors; $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with singular values $\\sigma_1, \\sigma_2,..., \\sigma_{\\min{m,n}}$; $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix containing the right singular vectors. For simplicity, the subscript for these intermediate matrices is omitted.\nNuclear Norm: The nuclear norm of G is defined as the $l_1$ norm of the singular values, which reflects the sparsity of the spectrum and serves as a convex surrogate of the matrix rank. Hence, it does not only quantify the gradient magnitude but also the concentration of the spectrum on its top few singular values, which is vital to understand the gradient patterns in each layer, i.e.,\n$\\|G_{X,i}\\|_{*} = s_{X,i} = \\sum_{j=1}^{\\min{m,n}} \\sigma_j$\n$\\sigma_1$ Ratio: We define the $\\sigma_1$ ratio as the ratio of the largest singular value to the nuclear norm, which indicates the concentration of gradient effects, showing how much of the gradient's magnitude is captured by its most principal component, which to some extent represents the diversity of directions of current gradient 2:\n$\\tau_{X,i} = \\frac{\\sigma_1}{s_{X,i}} = \\frac{\\sigma_1}{\\|G_{X,i}\\|_{*}}$"}, {"title": "Metrics of Gradient Difference", "content": "In our experimental analysis, the nuclear norm $s_{X,i}$ of each layer will not be investigated individually, but to investigate the overall dynamic characteristics across every layer of the LLM. For simplicity, we notate the nuclear norm value, $s_{X,i}$, of a specific metric across all the layers as a curve notated as $s_X$.\nTo analyze these gradient results, the visualization of the layer-wise curves is one of the most important tools to get a qualitative understanding of how the gradients change across the layers. However, quantitative analysis is still required for a better understanding of gradient representations.\nGradient-Norm Difference between Layers. Considering that both the fluctuation and scale are important for a gradient curve, we utilize the Mean Absolute Difference, MAD, to represent a gradient curve. Specifically, the MAD of the curve $s_X$ is notated as $MAD_{s_X}$, which is calculated as:\n$MAD_{s_X} = \\frac{1}{N-1} \\sum_{i=1}^{N-1} |s_{X,i+1} - s_{X,i}|$\nwhere N is the total number of layers of the target LLM. MAD is a measure of the average magnitude of change between consecutive points. Unlike standard deviation, MAD focuses on the direct differences between successive values without squaring or taking into account the direction (positive or negative change). It is useful for quantifying the overall variability of the data, especially when detecting fluctuations or local changes, which are more important than global trends.\nGradient-Norm Difference between Initial-models or Training-responses. In addition to the value presentation for each individual curve, the pair-wise comparison between two curves, across different layers, is also important for our analysis. For this purpose, we use the layer-wise Relative Difference, RD, as our metric. At each layer, the RD between 2 values $s_{X,i}^{(1)}$ and $s_{X,i}^{(2)}$ is calculated as:\n$RD_{X,i} = \\frac{|s_{X,i}^{(1)} - s_{X,i}^{(2)}|}{s_{X,i}^{(1)}}$       where $s_{X,i}^{(1)}$ is utilized as the reference value. For this metric, the projection layer X and the layer index i should be kept the same to ensure this value is meaningful."}, {"title": "Experimental Setup", "content": "We investigate the gradients for 10 models including 5 base pre-trained models, Qwen2-1.5B (Yang et al., 2024), gemma-2-2b (Team et al., 2024), Llama-3.1-8B (Dubey et al., 2024), gemma-2-9b (Team et al., 2024), Llama-2-7b-hf (Touvron et al., 2023) and their instruction-tuned version, Qwen2-1.5B-Instruct, gemma-2-2b-it, Llama-3.1-8B-Instruct, gemma-2-9b-it, Llama-2-7b-chat-hf.\nThe main illustrations in our paper will be based on the results of Qwen2-1.5B models, results for other models will be in the appendix.\nThe datasets we use include three categories: Math, Commonsense Reasoning, and Wiki Knowledge."}, {"title": "Math Reasoning", "content": "For the category of math, AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021) are utilzied. The original ground truth format for AQUA is options from A to E, and for GSM8K is the resulting digits, and additional CoT reasoning paths are provided as well. During our experiments, both the process of learning the original ground truth (fast thinking) and learning the CoT plus ground truth are investigated. The ground truth for MATH is the complete solution for the question, which can be regarded as answers with CoT. We select the question types Algebra, Counting, and Geometry in MATH for our experiments. Moreover, to further explore the effects of more detailed reasoning paths (slow thinking), GPT4o is utilized to obtain a detailed version of CoT paths."}, {"title": "Commonsense Reasoning", "content": "For the category of commonsense reasoning, four datasets are utilized including StrategyQA (Geva et al., 2021), ECQA (Aggarwal et al., 2021), CREAK (Onoe et al., 2021), and Sensemaking, obtained from the FLAN collection (Longpre et al., 2023). The original ground truth format for StrategyQA is options of yes and no, for ECQA is word options, for CREAK is options of yes and no, and for Sensemaking is options of Sentence A or Sentence B. For all these datasets, the corresponding human-annotated CoT reasoning paths are provided, and both the process of learning the original ground truth and learning the CoT plus ground truth are investigated in our experiments. Similarly, further GPT4o-generated CoTs are also investigated."}, {"title": "Wiki Knowledge Learning", "content": "This group of tasks represents LLMs learning on pure knowledge-intensive responses without any"}, {"title": "Empirical Analysis", "content": "This section focuses on tasks related to CoT reasoning, which includes the datasets within the Math and Commonsense Reasoning task type."}, {"title": "Slow vs. Fast Thinking", "content": "In this section, we investigate the gradient behaviors when LLMs learn the responses with the reasoning process. i.e. the CoT paths. For samples in the MATH dataset, the original responses already contain the necessary steps to solve the question, which we notate as Simplified CoT setting. For the remaining datasets in these two task types, both the pure answer (resulting digits or options), and short CoT paths are provided, which we notate as None CoT and Simplified Cot settings. These configurations can help us to understand the gradients when LLMs learn response with or without CoT, probably revealing the advantages of CoT training. Moreover, the provided CoT paths are all too simplified, which might still be hard for LLMs to build the connections from question to CoT path to final answers. Thus we further prompt GPT4o to gen-"}, {"title": "Effect of Response Correctness", "content": "In this section, we investigate the gradient behaviors when LLMs are learning the correct or irrelevant responses with different reasoning paths. Similarly, for datasets that pure ground truths are given, we investigate the gradient behaviors on three settings: None CoT, Simplified Cot, and Detailed CoT, otherwise only the last two settings can be investigated. In the None CoT setting, we directly shuffle the answers across the dataset and make sure every question has the incorrect answer; In the Simplified Cot and Detailed CoT settings, we split every CoT path into individual sentences, then shuffle the sentences across the dataset. Under this circumstance, each sentence in the response is still complete, while the relation across sentences will be logically wrong, simulating the irrelevant CoT reasoning paths.\nIn these experiments, we try to investigate if the LLMs are able to identify the irrelevant responses during training with slow or fast thinking, reflected by gradient behaviors. The visualizations of LLMs learning on correct and irrelevant responses are presented in Figure 2, which contains 2 settings including (a) None CoT and (b) Detailed CoT. It is widely accepted that LLMs have learned all the knowledge in the pretraining phase (Zhou et al., 2023), so when LLMs are forced to learn responses that conflict with their internal knowledge, more efforts should be made (larger gradient) for this false alignment. However, from the visualizations, it can be observed that when no CoT paths are given, the gradient behaviors between learning the correct and nonsense responses are almost identical and their relative difference values on all the projection"}, {"title": "Effect of Initial Models", "content": "In this section, we compare the gradient behaviors between pre-trained base LLMs and aligned instructed LLMs. For each instructed LLM, we utilize the conversation templates officially provided, avoiding the potential misalignment. In these experiments, we observe that the instructed LLMs do not have much better performance in identifying the irrelevant responses, evidenced by the minor relative differences between gradient curves obtained from base LLMs and aligned LLMs.\nHowever, as shown in Figure 3, although the tendencies are consistent on both types of LLMs that detailed CoT reasoning paths make the scale and fluctuation of gradient smaller and smoother, the gradients on simplified CoT show a large discrepancy between the two types of LLMs. This discrepancy means that the instructed LLMs need more energy than the pre-trained LLMs to learn the simplified CoT paths. The phenomenon shows that (1) the distribution of the simplified CoT responses might have non-negligible discrepancies with the instruction datasets used for training this LLM; (2) the behaviors on gradient curves might be used as a measurement of how a specific data sample aligned with the internal knowledge of LLMs, which might be useful for the continued training settings."}, {"title": "Knowledge Learning Tasks", "content": "In this section, we investigate the effect on response length and popularity for the knowledge-intensive task. As shown in Figure 4, the left 3 columns represent the scenarios when LLMs are learning popular knowledge with different response lengths and the right-most column represents the scenario when"}, {"title": "Conclusion", "content": "Our study reveals the significant differences in gradient behaviors between fast and slow thinking training methodologies in LLMs, offering insights into how training dynamics can influence these models. Specifically, we observe that slow thinking leads to stable gradient norms of different layers, while fast thinking results in larger gradients and fluctuation across layers. Moreover, the gradient of slow thinking helps distinguish correct responses from irrelevant responses, while without CoT, the gradient patterns of the two types of responses are similar. The above observations on reasoning tasks cannot be extended to knowledge-learning tasks, where simply increasing response length does not show similar gradient patterns as slow thinking."}, {"title": "Limitations", "content": "Due to the page limits, only a small proportion of results can be presented, which might weaken the findings of the paper. However, we try to include as many results as possible in the appendix through visualization and statistical results, hoping to provide further insights for the community. Moreover, the analysis in this paper focuses mainly on the strength of the layer-wise gradients, maybe more metrics can be used."}]}