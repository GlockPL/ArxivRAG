{"title": "Cross-View Referring Multi-Object Tracking", "authors": ["Sijia Chen", "En Yu", "Wenbing Tao"], "abstract": "Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this work, we propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of objects in each cross-view. To advance CRMOT task, we construct a cross-view referring multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named CRTrack. Specifically, it provides 13 different scenes and 221 language descriptions. Furthermore, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker. Extensive experiments on the CRTrack benchmark verify the effectiveness of our method.", "sections": [{"title": "Introduction", "content": "Multi-Object Tracking (MOT) is one of the most challenging tasks in computer vision. It is widely used in fields such as autonomous driving (Li et al. 2024b), video surveillance (Yi et al. 2024), and smart transportation (Bashar et al. 2022). Existing MOT methods have already demonstrated effectiveness in addressing most visual generic scenarios. However, when it comes to multimodal contexts, i.e., vision-language scenarios, traditional MOT methods face significant challenges and limitations. To solve this problem, the task of Referring Multi-Object Tracking (RMOT) was recently proposed. The form of this task is to guide the tracker to track the objects that match the language description. For example, if the input \u201cA man in a black coat and blue trousers, carrying a blue bag and holding a book.\", the network for the RMOT task will predict all target trajectories corresponding to that language description. Current research mainly focuses on the RMOT task under the single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of the objects are easily invisible, causing the network for the RMOT task to incorrectly match objects with the fine-grained language description.\nTo overcome the limitation of the single-view, we propose a new task called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view, which refers to different views with large overlapping areas, to obtain the appearances of objects from multiple views, thereby avoiding the problem that the appearances of objects are easily invisible in the RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the fine-grained language description and maintaining the identity (ID) consistency of the objects in each cross-view. As illustrated in Figure 1, we can observe that the network for the RMOT task makes the incorrect judgment when some appearances of the objects are invisible in the single-view of the RMOT task. In contrast, in the cross-views of the CRMOT task, the appearance of the objects can be fully captured, so that the network for the CRMOT task can accurately track the objects that match the fine-grained language description and can know which objects have the same identity (ID) in each cross-view, i.e., the network for the CRMOT task makes the correct judgment.\nTo advance the research on the cross-view referring multi-object tracking (CRMOT) task, we propose a benchmark, called CRTrack. Specifically, CRTrack includes 13 different scenes, 82K frames, 344 objects, and 221 language descriptions, as detailed in Table 1. These sequence scenes come from two cross-view multi-object datasets, DIVOTrack (Hao et al. 2024) and CAMPUS (Xu et al. 2016). Additionally, we propose a new annotation method based on the unchanging attributes of the objects throughout the sequences. These attributes include headwear color, headwear style, coat color and style, trousers color and style, shoes color and style, held item color, held item style, and transportation. Then, we utilize the large language model GPT-40 to generate language descriptions from the annotated attributes, followed by careful manual checking and correction to ensure the accuracy of language descriptions. Finally, we propose a set of evaluation metrics specifically designed for the CRMOT task.\nMoreover, to further advance the research on the CRMOT task, we propose an end-to-end cross-view referring multi-object tracking method, called CRTracker. Specifically, CR-Tracker combines the accurate multi-object tracking capability of CrossMOT (Hao et al. 2024) and the powerful multi-modal capability of APTM (Yang et al. 2023). Furthermore, a prediction module is designed within the CR-Tracker network. The novel design idea of this prediction module is to use the frame-to-frame association results of the network as detection results, the fusion scores as confidences, and the prediction module plays the role of a tracker. Finally, we evaluate our proposed CRTracker method and other methods on the in-domain and cross-domain test sets of the CRTrack benchmark. The evaluation results demonstrate that our method achieves state-of-the-art performance while showing significant generalization capabilities. Specifically, compared to the best-performing method among other single-view approaches, our method surpasses it by 31.45% in CVRIDF1 and 25.83% in CVRMA across all scenes in the in-domain evaluation, and by 8.74% in CVRIDF1 and 1.92% in CVRMA across all scenes in the cross-domain evaluation.\nIn summary, our main contributions are as follows:\n1. We propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It is a challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of the objects in each cross-view.\n2. We construct a benchmark, called CRTrack, to advance the research on the CRMOT task. This benchmark includes 13 different scenes, 82K frames, 344 objects, and 221 language descriptions.\n3. We propose an end-to-end cross-view referring multi-object tracking method, called CRTracker. We evaluate CRTracker and other methods on the CRTrack benchmark both in-domain and cross-domain. The evaluation results show that CRTracker achieves state-of-the-art performance, fully demonstrating its effectiveness."}, {"title": "Related Work", "content": "Cross-View Multi-Object Tracking. Cross-view multi-object tracking is a specific category of multi-object tracking (Zhang et al. 2021; Zeng et al. 2022; Yu et al. 2022, 2023a,b; Chen et al. 2024; Gao, Zhang, and Wang 2024; Li et al. 2024a) that shares large overlapping areas between different views. Currently, mainstream methods (Cheng et al. 2023; Hao et al. 2024) use appearance and motion features to measure the similarity of the same pedestrians across different views and associate them. There are several commonly used cross-view multi-object tracking datasets, including DIVOTrack (Hao et al. 2024), CAMPUS (Xu et al. 2016), EPFL (Fleuret et al. 2007), WILDTRACK (Chavdarova et al. 2018), and MvMHAT (Gan et al. 2021). The DIVOTrack dataset is the latest cross-view multi-object tracking dataset with 10 scenes captured by 3 moving cameras. The CAMPUS dataset contains real scenes captured by static cameras from 3 or 4 different views. The EPFL dataset is one of the traditional cross-view tracking datasets, but its very low resolution makes it difficult to learn the appearance embeddings of objects. The WILDTRACK was shot in a square, but the pedestrian annotations are incomplete. The MvMHAT was shot on a rooftop, but all videos in the dataset use the same scene and the same person. Therefore, we choose the DIVOTrack and CAMPUS datasets to construct the benchmark.\nReferring Multi-Object Tracking. Referring multi-object tracking is divided into two architectures: two-stage methods and end-to-end methods. The two-stage methods first explicitly extract object trajectories and then select object trajectories that match the language descriptions. The mainstream two-stage methods include iKUN(Du et al. 2024) and LaMOT (Li et al. 2024c). The end-to-end methods directly obtain object trajectories that match the language descriptions. The mainstream end-to-end methods include TransRMOT (Wu et al. 2023) and TempRMOT (Zhang et al. 2024)."}, {"title": "Benchmark", "content": "To advance the research on the cross-view referring multi-object tracking (CRMOT) task, we construct a cross-view referring multi-object tracking benchmark, named CRTrack. Below, we provide details about the CRTrack benchmark.\nDataset Collection. The emphasized properties of the cross-view referring multi-object tracking dataset are two major elements: cross-view and referring. Cross-view refers to the overlapping area between different camera views, and referring refers to the language description. Therefore, based on the cross-view multi-object tracking datasets DIVOTrack (Hao et al. 2024) and CAMPUS (Xu et al. 2016), we add language descriptions to construct the cross-view referring multi-object tracking benchmark, named CRTrack. The DIVOTrack dataset contains data from 10 different real-world scenes, and it is currently the most scene-rich cross-view multi-object tracking dataset. All sequences are captured using three moving cameras and manually synchronized. The CAMPUS dataset contains 3 different scenes with frequent object occlusion problems. All sequences are captured using 3 or 4 static cameras and manually synchronized. It should be noted that we only use their training data, and unify the image sizes and annotation formats of the DIVOTrack and CAMPUS datasets.\nDataset Annotation. We divide the content of the language description into different attributes. These attributes include headwear color, headwear style, coat color and style, trousers color and style, shoes color and style, held item color, held item style, and transportation. Previously, some language descriptions of the RMOT task's benchmark Refer-KITTI (Wu et al. 2023) only annotate a certain fragment sequence of the object, not the whole sequence from the appearance to the disappearance of the object. This annotation method is obviously not suitable for the new task of cross-view referring multi-object tracking, because the introduction of cross-view can observe the whole sequence from the appearance to the disappearance of the object in more detail from multiple views. Therefore, we propose a new annotation method that aims to annotate objects from the perspective of their invariant attributes in the sequence, such as clothing, held items and transportation. We annotate the attributes of the objects in each scene. After obtaining the object annotation attributes, we use the large language model GPT-40 (OpenAI 2024) to produce the language descriptions based on the object annotation attributes. The language descriptions generated by GPT-40 are manually checked and corrected. With the help of the large language model, the richness of the language descriptions has been greatly improved. Finally, 344 labeled objects and 221 language descriptions are obtained."}, {"title": "Dataset Split", "content": "For the DIVOTrack dataset with language descriptions, we evenly selected three scenes as the in-domain test set based on the scene's object density, and the remaining seven scenes as the training set. The CAMPUS dataset with language descriptions is used as the cross-domain test set. In short, the CRTrack benchmark is divided into training set, in-domain test set and cross-domain test set. Specifically, the training set contains \"Floor\", \"Gate1\", \"Ground\", \"Moving\u201d, \"Park\", \"Shop\" and \"Square\" scenes, the in-domain test set contains \"Circle\", \"Gate2\" and \"Side\" scenes, and the cross-domain test set contains \"Garden1\", \"Garden2\" and \"ParkingLot\u201d scenes.\nDataset Statistics. i) Word Cloud. Figure 3 shows the word cloud of the CRTrack benchmark we constructed. We can observe that the CRTrack benchmark contains a large number of words describing clothing, held items and transportation information. The rich variety of word clouds shows the difficulty of our benchmark. ii) Object Density. Object density indicates how many objects there are per frame per cross-view of a scene on average. The object density of each scene in the CRTrack benchmark is shown in Table 1. We can observe that the CRTrack benchmark has scenes with different object densities. iii) Average Number of Frames of Language Description. It indicates the average number of frames in which the object corresponding to each language description appears. Table 1 shows the average number of frames of the language description of each scene. The average number of frames of the language description of \"ParkingLot\" scene of the CRTrack benchmark reaches an astonishing 3419. The extremely long number of frames brings great challenges to the cross-view referring multi-object tracking in the temporal dimension."}, {"title": "Evaluation Metrics", "content": "The cross-view tracker is different from the single-view tracker. The cross-view tracker processes multiple views in each batch of synchronized video sequences. The same object should have the same identity (ID) in different views. The standard cross-view multi-object tracking evaluation metrics include the cross-view IDF1 (CVIDF1) and the cross-view matching accuracy (CVMA) (Gan et al. 2021). The definitions of CVIDF1 and CVMA are as follows:\n$CVIDF1 = \\frac{2CVIDP \\times CVIDR}{CVIDP + CVIDR},$ (1)\n$CVMA = 1 - (\\frac{\\Sigma_t mt + fpt + 2mmet}{\\Sigma_t gtt}),$ (2)\nwhere CVIDP and CVIDR denote the cross-view object matching precision and recall, respectively. $m_t, fp_t, mme_t$, and $gt_t$ are the numbers of misses, false positives, mismatched pairs, and the total number of objects in all views at time t, respectively.\nIt should be noted that cross-view referring multi-object tracking is different from cross-view multi-object tracking. When predicting non-referring but visible objects, they are considered false positives in our evaluation. When the tracking corresponding to the language description is not good, there will be a lot of false detections. This will make CVMA become a relatively large negative number, resulting in a huge impact on the evaluation metrics. We take a maximum value between CVMA value and 0 to prevent the influence of negative numbers.\nWe aim to comprehensively evaluate each language description, so we propose new evaluation metrics CVRIDF1 and CVRMA for the cross-view referring multi-object tracking (CRMOT) task, and their value range is 0 to 1. The definitions of the evaluation metrics CVRIDF1 and CVRMA for the CRMOT task we proposed are as follows:\n$CVRIDF1 = \\frac{\\Sigma_l CVIDF1_l}{n},$ (3)\n$CVRMA = \\frac{\\Sigma_l max(CVMA_l, 0)}{n}$ (4)\nwhere l represents a language description and $n_l$ denotes the number of language descriptions."}, {"title": "Strong Baseline of CRMOT", "content": "The challenge of the CRMOT task is to simultaneously detect and track the objects that match the language description and maintain the identity consistency of the objects in each cross-view. To address the challenge of the CRMOT task, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker, as a strong baseline."}, {"title": "Training", "content": "APTM. APTM (Yang et al. 2023) is a framework for joint attribute prompt learning and text matching learning, including image encoder, text encoder and cross encoder. Specifically, the image encoder uses Swin Transformer (Liu et al. 2021) to output image features. The text encoder uses the first 6 layers of BERT (Devlin et al. 2018) to output text features. The cross encoder adopts the last 6 layers of BERT, fuses image features and text features, and captures semantic relationships by the cross-attention mechanism.\nPipeline of Training. The pipeline of our training framework is shown in Figure 4. The input is synchronized video sequences from multiple cross-views and language descriptions. Similar to the CrossMOT (Hao et al. 2024) algorithm, our model uses CenterNet (Zhou, Wang, and Kr\u00e4henb\u00fchl 2019) as the backbone, followed by four heads, including a detection head, a single-view Re-ID head, a cross-view Re-ID head and a full Re-ID head. In addition, it also includes APTM image encoder and APTM text encoder. It is worth noting that for single-view Re-ID, the same object in different views is considered as different objects in single-view tracking; for cross-view Re-ID, the same object in different views is considered as the same object; the full Re-ID head is used for language description calculation. We use the image encoder of APTM to encode the object ground truth area in the input video sequence into the feature $F_{Ai}$. Then, the feature $F_{Ai}$ is merged with the feature $F_f$ output by the full Re-ID head to obtain the object image feature $F_i$. Mathematically, the merging operation can be formulated as follows:\n$F_i = F_f + \\alpha F_{Ai}$ (5)\nwhere $\\alpha$ represents the feature fusion weight of $F_{Ai}$.\nAdditionally, we use the text encoder of APTM to encode language descriptions and obtain text features. The object image features and text features are calculated using the referring loss $L_r$. The detection, single-view Re-ID and cross-view Re-ID are calculated using the loss $L_{emot}$.\nLoss Functions. Our cross-view referring multi-object tracking loss $L_{ermot}$ is divided into two parts, cross-view multi-object tracking loss $L_{emot}$ and referring loss $L_r$. The $L_{emot}$ is formulated as follows:\n$L_{emot} = (\\frac{1}{2} + w_1) (L_d + w_2 (L_s + L_c)),$ (6)\nwhere $L_d$ represents the detection loss, $L_s$ represents the single-view Re-ID loss, $L_c$ represents the cross-view Re-ID loss. $w_1$ and $w_2$ are are learnable parameters.\nThe $L_r$ uses the Cross-Entropy Loss (Zhang and Sabuncu 2018), which is formulated as:\n$L_r = -\\frac{1}{NK} \\Sigma_i \\Sigma_j Y_{ij} log (P_{i,j})$ (7)\nwhere N represents the number of objects, K represents the number of all language descriptions in the training data, $Y_{i,j}$ represents the label of the j-th language description corresponding to the i-th object, $P_{i,j}$ represents the probability that the i-th object is predicted to be the j-th label value.\nThus, the final loss $L_{ermot}$ is:\n$L_{crmot} = L_{cmot} + L_r$ (8)\nwhere $L_{ermot}$ represents the cross-view referring multi-object tracking loss."}, {"title": "Inference", "content": "Pipeline of Inference. The pipeline of our inference framework is shown in Figure 4. During the inference phase, we process language descriptions one by one. First, multiple cross-view video sequences are input into the network, and"}, {"title": "Algorithm 1: Prediction Module", "content": "Input: Frame-to-frame association results, i.e. input tracks\nof the prediction module $T_{input}$; fusion scores $S_f$\nParameter: Fusion scores of views where the track exists\n$S$; fusion score of the track $S_f$; j-th view $V_j$; Number of\nviews for the track $N_v$; threshold of average fusion score\n$T_{as}$; threshold of single-view fusion score $T_{ss}$; threshold of\nhit score $T_{hs}$; hit score of the track $S_H$; average hit score $s_1$;\nsingle-view hit score $s_2$; single-view miss score $s_3$\nOutput: Output tracks of the prediction module $T_{output}$\n1: Let $T_{output} \\leftarrow \\emptyset$; $S \\leftarrow \\emptyset$.\n2: for $T_i \\in T_{input}$ do\n/* summarize scores and view number of the track */\n$N_v = 0$\nfor $V_j \\in \\{V_1, ..., V_N\\}$ do\nif $T_i$ exists in $V_j$ then\n$S \\leftarrow S \\cup S_f$\n$N_v += 1$\nend if\nend for\n/* use scores to filter the track */\nif $(S)/N_v > T_{as}$ then\n$S^+ = s_1$\n$T_{output} \\leftarrow T_{output} \\cup T_i$\nelse\nfor $S_f \\in S$ do\nif $S_f > T_{ss}$ then\n$S^+ = ls_2$\nelse\n$S = max(S_H, 0)$\nend if\nend for\nif $SH > T_{hs}$ then\n$X = int(S_f/T_{ss})$\n$S^+ = s_3$\n$T_{output} \\leftarrow T_{output} \\cup T_i$\nend if\nend if\nend for\n30: return $T_{output}$\nwhere N represents the number of objects, K represents the number of all language descriptions in the training data, $Y_{i,j}$ represents the label of the j-th language description corresponding to the i-th object, $P_{i,j}$ represents the probability that the i-th object is predicted to be the j-th label value.\nThus, the final loss $L_{ermot}$ is:"}, {"title": "Experiments", "content": "Settings\nFor evaluation, we conduct experiments on the CRTrack benchmark we constructed and follow its evaluation metrics. Our models are trained for 20 epochs and tested on a single NVIDIA RTX 3090 GPU. The feature dimensions of single-view embedding, cross-view embedding, and full embedding are all set to 512. During the training phase, we use the Adam optimizer (Kingma and Ba 2014), the initial learning rate is set to 1\u00d710-4, the batchsize to 12, and the feature fusion weight $\\alpha$ in Formula (5) to 0.01. During the inference phase, we set the score fusion weight $\\beta$ in Formula (9) is set to 0.1, threshold of average fusion score $T_{as}$ to 0.5, threshold of single-view fusion score $T_{ss}$ to 0.75, threshold of hit score $T_{hs}$ to 30, average hit score $s_1$ to 3, single-view hit score $s_2$ to 3, and single-view miss score $s_3$ to 1."}, {"title": "Quantitative Results", "content": "On the CRTrack benchmark, we compared our CRTracker with other methods. Since previous referring multi-object tracking methods are designed for single-view, they cannot be used for the cross-view referring multi-object tracking task. Thus, we combine previous referring multi-object tracking methods with the MvMHAT (Gan et al. 2021) cross-view association algorithm to enable them to be used in the cross-view referring multi-object tracking task. In addition, since our method is end-to-end, for fair comparison, we choose two end-to-end referring multi-object tracking methods, including TransRMOT (Wu et al. 2023) and TempRMOT (Zhang et al. 2024). For in-domain evaluation, all methods are trained using the benchmark training set and tested on the benchmark in-domain test set. For cross-domain evaluation, all methods are trained using the benchmark training set and tested on the benchmark cross-domain test set. It is worth noting that our CRTracker and other methods use the same model and parameter settings for cross-domain evaluation as for in-domain evaluation.\nIn-domain Evaluation. As shown in Table 2, our CRTracker achieves 54.88% CVRIDF1 and 35.97% CVRMA on all scenes of the in-domain test set. In particular, it achieves 91.60% CVRIDF1 and 73.40% CVRMA on the \"Gate2\" scene. Notably, our CRTracker far outperforms all other methods in the in-domain evaluation. The results indicate that CRTracker can tackle in-domain scenes well.\nCross-domain Evaluation. As illustrated in Table 2, all methods suffer vital performance degradation, which is expected due to the high difficulty of the cross-domain test set of the benchmark. The cross-domain test set and the training set differ in terms of the number of cross views, scenes, pedestrians, camera angles, and lighting. In addition, the cross-domain test set contains many language descriptions that do not appear in the training set, and the average number of frames of language descriptions is very long. Despite this, our CRTracker still surpasses other methods, with achieving 12.52% CVRIDF1 and 2.32% CVRMA on all scenes of the cross-domain test set. The results show that CRTracker has a good generalization ability for unseen domains."}, {"title": "Qualitative Results", "content": "To further demonstrate the superiority of our CRTracker, we visualize some results of our proposed CRTracker method and other methods trained for 20 epochs in in-domain and cross-domain evaluations. As shown in Figure 5, CRTracker is able to accurately detect and track the objects that match the language description in a variety of challenging scenes and keep the same object with the same identity in each cross-view. In the \"Garden2\" scene example, CRTracker can accurately detect and track the target and keep the target with the same identity in each cross-view even with the untrained language description, which fully demonstrates the generalization capability of our method. Many qualitative results can be found in the supplementary materials."}, {"title": "Ablation Study", "content": "To study the role of each part of our method CRTracker, we conduct ablation experiments on the CRTrack benchmark. All experiments follow in-domain evaluation, that is, training on the training set and testing on the in-domain test set.\nAnalysis of Prediction Module. To demonstrate the effectiveness of prediction module, we compare CRTracker with and without prediction module. As shown in Table 3, we can observe that the CRTracker with prediction module is 7.34% higher in CVRIDF1 and 7.29% higher in CVMA than the CRTracker without prediction module. This phenomenon shows that the prediction module fully fuses the trajectory and language description scores from each cross-view to maximize the matching of trajectory to description."}, {"title": "Conclusion", "content": "In this work, we propose a novel task, named Cross-view Referring Multi-Object Tracking (CRMOT). It is a challenging task of accurately tracking the objects that match the fine-grained language description and maintaining the identity consistency of the objects in each cross-view. To advance the CRMOT task, we construct the CRTrack benchmark. Furthermore, to address the challenge of the new task, we propose CRTracker, an end-to-end cross-view referring multi-object tracking method. We validate CRTracker on the CRTrack benchmark, which achieves state-of-the-art performance and demonstrates good generalization ability."}, {"title": "Questions and Replies", "content": "(1) We can see that a reference description corresponds to a very long sequence. I wonder whether a reference may describe multiple persons during the same period or different persons at different moments in a long video.\nA reference can describe any number of people at any time who match the language description, as evidenced by Figures 6, 9 and 11 in the Supplementary Material.\n(2) The motivation of the task does not look convincing to me. As long as we can identify the target object from any camera view, why do we have to perform cross-view association?\nFor some complex language descriptions, it is difficult to correctly judge whether the target matches the language description from a single view. To overcome the limitation of the single-view, our CRMOT task introduces the cross-view, to obtain the appearances of objects from multiple views, thereby avoiding the problem that the appearances of objects are easily invisible in the single view. For example, in Figure 5 of the Manuscript, the person indicated by yellow arrows in View1 and View3 is difficult to locate accurately from a single view because some appearances of their appearance are occluded. By introducing cross-views, we can clearly know the same person across all three views and identify the target indicated by the yellow arrow in View2, which allows us to confirm that the persons in View1 and View3 are also the targets we want.\n(3) Can CrossMOT be extended as another baseline?\nThe CrossMOT method we currently use can be replaced with other networks."}]}