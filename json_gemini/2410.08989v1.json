{"title": "SUBZERO: RANDOM SUBSPACE ZEROTH-ORDER \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396\u0391TION FOR MEMORY-EFFICIENT LLM FINE-TUNING", "authors": ["Ziming Yu", "Pan Zhou", "Sike Wang", "Jia Li", "Hua Huang"], "abstract": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety of downstream tasks. However, as LLMs grow in size, the memory demands for backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization methods offer a memory-efficient alternative by using forward passes to estimate gradients, but the variance of gradient estimates typically scales linearly with the model's parameter dimension\u2014a significant issue for LLMs. In this paper, we propose the random Subspace Zeroth-order (SubZero) optimization to address the challenges posed by LLMs' high dimensionality. We introduce a low-rank perturbation tailored for LLMs that significantly reduces memory consumption while improving training performance. Additionally, we prove that our gradient estimation closely approximates the backpropagation gradient, exhibits lower variance than traditional ZO methods, and ensures convergence when combined with SGD. Experimental results show that SubZero enhances fine-tuning performance and achieves faster convergence compared to standard ZO approaches like MeZO across various language modeling tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs), such as the GPT and LLaMA series (Zhang et al., 2022; Touvron et al., 2023), have recently demonstrated impressive capabilities in natural language processing tasks and beyond (Solaiman et al., 2019; Achiam et al., 2023). These models utilize deep learning, particularly the transformer architecture (Vaswani et al., 2017), to learn complex patterns in language data. However, LLMs can struggle with specialized tasks that require domain-specific knowledge (Shen et al., 2024). Fine-tuning presents an effective solution by slightly adjusting pre-trained LLMs with domain data, enabling them to adapt to specific tasks more effectively.\nFor fine-tuning, first-order (FO) optimizers, such as SGD (Amari, 1993) or Adam (Kingma & Ba, 2015), are commonly used to achieve promising performance on domain datasets. However, as LLMs grow in size, FO optimizers demand increasingly memory consumption due to the gradient computations required by backpropagation (BP) (Zhao et al., 2024a). To enhance memory efficiency, MeZO (Malladi et al., 2023) first introduces the zeroth-order (ZO) optimizer to LLM fine-tuning without BP. It just needs forward passes and calculates gradient estimates using finite differences of training loss values. Nevertheless, the variance of ZO gradient estimates linearly depends on the perturbation dimension, which corresponds to the number of model parameters. This can become extremely large in LLMs, resulting in significant performance degradation compared to FO optimizers (Gautam et al., 2024; Jiang et al., 2024; Liu et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Zeroth-Order Fine-Tuning. ZO optimizers utilize just two forward passes to estimate gradient without BP. Malladi et al. (2023) first used ZO optimization to fine-tune LLMs, significantly lowering the GPU hours and memory usage to levels similar to inference, which offers a considerable advantage over FO optimizers. They demonstrated that LLM fine-tuning benefits from a well-structured loss landscape by introducing suitable task-specific prompt templates. Convergence theories for ZO optimization have been elaborated in both convex (Nesterov & Spokoiny, 2017; Jamieson et al., 2012; Duchi et al., 2015) and non-convex settings (Liu et al., 2018; Ji et al., 2019). However, these convergence rates typically increase linearly with the number of trainable parameters (Nesterov & Spokoiny, 2017; Jamieson et al., 2012; Duchi et al., 2015; Liu et al., 2018; Ji et al., 2019).\nRecently, more work in ZO has focused on improving the convergence rates and reducing gradient estimation variance for LLM fine-tuning. Increasing batch size can diminish noise in ZO gradient estimation (Gautam et al., 2024; Jiang et al., 2024). Perturbing a subset of model parameters also lowers gradient variance. This approach induces sparse parameter perturbations through random and sparse pruning masks (Liu et al., 2024) or block-coordinate perturbations (Zhang et al., 2024). Additionally, some approaches tried to reduce trainable parameters through PEFT (Malladi et al., 2023; Zhang et al., 2024) and tensorized adapters (Yang et al., 2024).\nRandom Subspace Optimization. To lessen dependence on dimensionality, some research utilizes random projections and low-dimensional perturbations in subspaces (Nozawa et al., 2024; Roberts & Royer, 2023; Kozak et al., 2021). However, these methods are hindered by the need to store a large projection matrix that increases with dimensionality, making it impractical for fine-tuning LLMs.\nMemory-Efficient Fine-Tuning. Fine-tuning generally employs FO optimizers like SGD (Amari, 1993) or Adam (Kingma & Ba, 2015). Various approaches have been developed to reduce the memory cost of BP, such as sparsifying gradients (Sun et al., 2017), projecting gradients into a low-rank subspace (Zhao et al., 2024a), and quantizing optimizer states to lower bits (Dettmers et al., 2022b; Li et al., 2024). Additional methods to conserve activation and weight memory during forward and backward passes include gradient checkpointing (Chen et al., 2016), FlashAttention (Dao et al., 2022), QLORA (Dettmers et al., 2024), and LLM.int8() (Dettmers et al., 2022a)."}, {"title": "3 PRELIMINARIES", "content": "In this section, we introduce the most popular ZO optimization approach and existing random subspace optimization methods.\nNotations. We use a non-bold letter like a and A to denote a scalar, a boldfaced lower-case letter like w to denote a column vector, and a boldfaced upper-case letter such as W to denote a matrix. N(0, I) denotes a multivariate normal distribution with a zero mean vector and an identity covariance matrix. vec(W) represents the vectorization of matrix W, which transforms W into a column vector by stacking the columns of W vertically. A \u2297 B is the Kronecker product of matrices A and B. E[x] represents the expected value of a random variable x. Var[x] represents the variance of a random variable x. The l2-norm of a vector x is ||x|| = \u221a\u2211i=1 xi2. The spectral norm of a matrix A is ||A||. The Frobenius norm of a matrix A is ||A||F = \u221a(A, A). C\u2113,p(S) represents the class of s-th smooth and p-th L-smooth functions over the set S. bdiag(A1, A2, \u2026, A\u2113) is a block diagonal matrix with diagonal blocks A1, A2,\u00b7\u00b7\u00b7, A\u2113.\nWe are interested in fine-tuning large LLMs (Ding et al., 2023). These models typically comprise multiple layers, with trainable parameter vectors represented as w = [w1T, w2T, ..., w\u2113T] \u2208 Rd, where wi denotes the flattened parameter vector from the i-th layer and d is the number of model parameters. Then training these models involves optimizing the following problem:\nminw L(w), (1)\nwhere L(\u00b7) denotes the loss function.\nZeroth-Order Optimization. ZO optimization is BP-free and estimates gradients via random perturbations. A classical gradient estimator is the simultaneous perturbation stochastic approximation"}, {"title": "4 METHODOLOGY", "content": "Here we first elaborate on our SubZero, a powerful ZO framework designed for LLM fine-tuning. Then we present how to integrate SubZero into four representative fine-tuning schemes."}, {"title": "4.1 RANDOM SUBSPACE OPTIMIZATION FOR LLM FINE-TUNING", "content": "Our intuition is that exploring update directions in a low-dimensional subspace may result in a reduced variance of the estimated gradient compared to the estimation in the vanilla space as used in MeZO. Inspired by (Zhao et al., 2024a; Nozawa et al., 2024; Roberts & Royer, 2023), we propose the random Subspace Zeroth-order (SubZero) optimization framework tailored for LLM fine-tuning. This framework reduces gradient estimation variance, and minimizes the memory overhead associated with gradient estimation, such as the memory overhead caused by the projection matrix P in Eqn. (5) used in (Nozawa et al., 2024; Roberts & Royer, 2023).\nLayer-wise Random Subspace Perturbation. LLMs primarily consist of dense layers that perform matrix multiplication. We denote the trainable parameters of the i-th layer in matrix form as Wi \u2208 Rmi\u00d7ni. Then we will explain how to design its low-rank perturbation Zi \u2208 Rmi\u00d7ni.\nWe propose a low-rank perturbation strategy for model parameter matrix of each layer, contrasting with previous random subspace methods that focus on the entire model's parameters (Nozawa et al.,"}, {"title": "5 THEORETICAL ANALYSIS", "content": "In this section, we theoretically analyze why SubZero can reduce the variance of gradient estimates and accelerate convergence. Before the analysis, we first define some necessary notations:\nP = bdiag(V1 \u2297 U1,\u2026, V\u2113 \u2297 U\u2113), z=[vec(Z1)T,..., vec(Z\u2113)T]T, z=[vec(\u017d1)T, ..., vec(\u017d\u2113)T]T.\nThen we first state the main theoretical results on our gradient estimation in Eqn. (8).\nTheorem 1. For the gradient estimation in Eqn. (8), the following two properties hold.\na) By using gradient estimation in (8), our estimated gradient \u011d\u03b5(x, P, z) is equivalent to\n\u011d\u03b5(x, P, z) = f(x + \u03b5Pz) \u2212 f(x \u2212 \u03b5Pz)2\u03b5 Pz. (10)\nwhere z \u223c N(0, Iq), \u03b5 > 0, P \u2208 Rd\u00d7q satisfies PTP = Iq with d = \u2211i=1 mini and q = \u2113r2.\nb) Let z \u223c N(0, Iq), and f \u2208 C\u2113,L22 (Rd). Then we have\n\u03a6(x) = ||Ez[\u011d\u03b5(x, P, z)] \u2212 PPTV f(x)||2 \u2264 \u03b526 L2(q + 4)2. (11)\nSee its proof in Appendix A.5. Theorem 1 (a) provides the equivalent form (10) of our gradient estimation (8). By comparing this with the gradient estimation (5) in random subspace optimization (Nozawa et al., 2024; Roberts & Royer, 2023), we observe significant differences. First, our gradient estimation (10) accounts for the layer-wise structure of the network, requiring the projection matrix P to be block-diagonal, whereas in random subspace optimization, P is not. Additionally, our method introduces a layer-wise low-rank perturbation matrix, reflected by the block-diagonal structure of P, with lazy updates to the column and row spaces defined by U i and Vi. In contrast, random subspace optimization simply requires P to be random. These distinctions highlight the key differences between our gradient estimation and existing methods in random subspace optimization.\nTheorem 1 (b) guarantees that the distance \u03a6(x) between the expected gradient estimate and the BP gradient in the subspace spanned by P is small. Moreover, by setting \u03b5 = 1\u221aq+4, the distance \u03a6(x) is bounded by a constant L2/6, independent of the parameter dimension d. This implies that the error in our gradient estimation does not scale with the extremely high parameter dimensions of LLMs, providing highly accurate gradient estimation-crucial for optimizing LLMs.\nNext, we utilize a strictly convex quadratic loss to further analyze our gradient estimation in Eqn. (10). This choice is motivated by the fact that, after pretraining, the LLM parameters tend to converge toward a local minimum within a local basin, which can be well-approximated by a quadratic loss (Neyshabur et al., 2020)."}, {"title": "6 EXPERIMENTS", "content": "In this section, we present comprehensive experiments to evaluate the effectiveness of SubZero. We conduct our experiments using medium-sized masked LLMs (RoBERTa-large (Liu et al., 2019)) and large-scale autoregressive LLMs (OPT-1.3B and 13B (Zhang et al., 2022), LLaMA2-7B (Touvron et al., 2023), and Mistral-7B (Jiang et al., 2023)). Our exploration covers full-parameter tuning (FT) (Aghajanyan et al., 2021) and three PEFT schemes: LoRA (Hu et al., 2022), prefix tuning (Li & Liang, 2021), and prompt tuning (Lester et al., 2021). For comparison, we include leading ZO methods, such as MeZO (Malladi et al., 2023) and S-MeZO (Liu et al., 2024), alongside inference-only memory-efficient baselines like zero-shot, in-context learning (ICL) (Brown et al., 2020), and linear probing (LP) (Kumar et al., 2022). We also use the FO optimizer SGD as a benchmark. Since appropriate prompts are critical for ZO optimization (Malladi et al., 2023; Zhang et al., 2024), all experiments incorporate prompt templates, which are detailed in Appendix A.1."}, {"title": "6.1 PERFORMANCE WITH DIFFERENT EXPERIMENTAL SETTINGS", "content": "Following the settings in MeZO (Malladi et al., 2023), we evaluated SubZero using OPT-13B on the SuperGLUE benchmark (Wang et al., 2019), which covers a diverse range of tasks, including classification, multiple-choice, and generation, as outlined in Table 2. For each task, we randomly sampled 1000 examples for training, 500 for validation, and 1000 for testing. The ZO methods were applied to both full-parameter tuning (FT) and LoRA fine-tuning schemes, running for 20K steps."}, {"title": "6.2 PERFORMANCE WITH NON-DIFFERENTIABLE OBJECTIVES", "content": "Following MeZO (Malladi et al., 2023), we respectively apply SubZero to fine-tune RoBERTa-large and OPT-13B using two non-differentiable objectives: accuracy and F1. As a baseline, we also report results using the cross-entropy objective with Adam. As shown in Table 4, SubZero consistently outperforms MeZO across both non-differentiable objectives and the cross-entropy benchmark, demonstrating its effectiveness across varying optimization goals."}, {"title": "6.3 \u039c\u0395\u039cORY USAGE AND WALL-CLOCK TIME ANALYSIS", "content": "Table 5 compares the memory consumption and wall-clock time of ZO methods (MeZO and SubZero), SGD, and inference-only approaches (zero-shot and in-context learning (ICL)) using OPT-13B. Since inference-only methods do not involve fine-tuning, they have zero wall-clock time and their memory usage reflects only the inference load. For fine-tuning, all methods were run for 20K steps. The ZO methods, including SubZero, achieved over a 1.8\u00d7 reduction in memory usage compared to SGD. Notably, SubZero's memory footprint closely aligns with MeZO's, while offering improved performance.\nAlthough SubZero introduces additional computational overhead for generating projection matrices via QR decomposition, this extra time represents less than 5% of the total wall-clock time. It is"}, {"title": "6.4 ABLATION STUDY", "content": "We conducted a thorough investigation of the effectiveness of our proposed techniques. Table 6 shows that using a column-orthogonal projection matrix significantly outperforms a Gaussian random projection matrix, primarily due to the low-rank structure of the perturbation matrices. This low-rank perturbation is key to improving the quality of gradient estimation.\nNext, Table 7 explores the effects of subspace rank r and update frequency To in Algorithm 3. The results demonstrate that SubZero is robust to variations in the subspace rank. However, performance drops sharply when the update frequency is too low, as the optimization becomes constrained to a single subspace for too long, limiting its adaptability.\nFinally, Table 8 underscores the critical role of the reshaping strategy for handling highly non-square perturbation matrices, essential for ensuring effective perturbations in different layers of the model. Together, these results highlight the improvements brought by our design choices, particularly in terms of projection and reshaping strategies, and their impact on SubZero's robustness and performance."}, {"title": "7 CONCLUSION", "content": "We have demonstrated that SubZero effectively fine-tunes large LLMs across various tasks and schemes with a memory cost comparable to that of inference. Extra experiments indicate that SubZero can optimize non-differentiable objectives. Our theory explains how SubZero reduces the variance of gradient estimates and accelerates convergence.\nLimitation. In addition to the SGD optimizer, we have yet to explore combining SubZero with other first-order optimizers, such as Adam. While SubZero is also compatible with other memory-efficient techniques like parameter quantization (Li et al., 2024), we have not thoroughly investigated the practical effects of these combinations. We will leave these explorations for future work."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PROMPT TEMPLATES", "content": "For autoregressive LLMs, we have three task types: classification, multiple-choice, and question answering. We adopt the prompt templates for various tasks in (Malladi et al., 2023), which are summarized in Table 9. For masked LLMs, we also adopt the prompt templates in (Malladi et al., 2023) and present them in Table 10."}, {"title": "A.2 DATASETS", "content": "Following (Malladi et al., 2023), we use SuperGLUE (Wang et al., 2019) for OPT experiments, including BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), COPA (Roemmele et al., 2011), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE (Dagan et al., 2005; Bar Haim"}, {"title": "A.3 HYPERPARAMETERS", "content": "Using a larger batch size can consistently reduce the variance in ZO optimization, thus enhancing fine-tuning performance (Malladi et al., 2023; Gautam et al., 2024; Yang et al., 2024). However, this increase in batch size also raises the time for forward passes and significantly elevates memory usage. We focus on developing ZO methods that minimize variance and improve performance with small batch sizes, with a default setting of 16. In some SGD experiments, like on MultiRC and SQUAD, the batch size is reduced to 8 due to limited GPU resources.\nConsistent with previous studies (Malladi et al., 2023; Zhang et al., 2024; Liu et al., 2024; Yang et al., 2024), we employ SGD without momentum by default to maintain memory efficiency. SGD utilizes linear learning scheduling, while all ZO methods apply a constant learning rate, with weight decay set to 0.\nFor ROBERTa, we run Adam for 1K steps and ZO methods for 100K steps. In the rest experiments, we run Adam for 5 epochs and SGD and ZO methods for 20K steps.\nWe follow previous work to set the hyperparameters in the PEFT schemes (Malladi et al., 2023; Zhang et al., 2024). For LoRA, the rank is set to 8 and \u03b1 is set to 16. For prefix tuning, the length of prefix tokens is set to 5, and we initialize these tunable representations by randomly sampling tokens from the vocabulary and then passing them through the LLM to get their keys and values at different attention layers. For prompt tuning, the length of prompt virtual tokens is set to 10, and the prompt tokens are initialized with actual token values from the model's embedding.\nWe present the hyperparameter search grids in Tables 11 and 12 to assist with result reproduction. For OPT-1.3B, we utilize the same hyperparameter settings as in Table 12. For Roberta-large, we use a learning rate of {1e-6, 5e-6} and \u03f5=1e-3 for MeZO and SubZero, with a batch size of 64. The rank for SubZero is set to {8, 16, 24}, and subspace change frequency is adjusted to {1000, 2000}."}, {"title": "A.4 IMPLEMENTATION DETAILS", "content": "We use one A800 GPU with the PyTorch 2.1.0+CUDA 11.8 framework for ZO methods and, if needed, two A800 GPUs for SGD.\nThe gradient estimation in SubZero is applicable to parameter matrices, while LLMs mainly consist of dense layers. For other trainable parameters, such as biases and layer normalization parameters, we recommend using the gradient estimation in MeZO (Malladi et al., 2023), as these layers contain fewer parameters.\nWe introduce two useful strategies to implement our SubZero efficiently in memory.\nIn-place Operation. As indicated in Eqn. (7), directly computing the loss difference \u03c1 requires twice the memory of inference, as it must store both the parameter matrix set W and the perturbation matrix set Z. To mitigate this, we draw inspiration from MeZO and utilize in-place operations. By employing the random seed trick, we store a random seed to compute \u03c1 (see lines 9-12 in Algorithm 3 and Algorithm 2) and regenerate the low-dimensional perturbation matrices Z1, Z2,..., Z\u2113 (see line 15 in Algorithm 3). Consequently, the memory cost for fine-tuning with SubZero is nearly equivalent to that of inference (see Table 1 and Table 5)."}, {"title": "A.5 PROOFS", "content": "In practice, SubZero employs smaller and layer-specific low-rank perturbation matrices instead of a large model-scale projection matrix. However, it is more convenient to prove SubZero's properties using a model-scale projection. Fortunately, the following lemma shows that the low-rank perturbation matrix for each layer can be represented as a layer-scale projection matrix, which is column orthogonal.\nLemma 1. Let \u017d = UZV T , where U \u2208 Rm\u00d7r, Z \u2208 Rr\u00d7r, V \u2208 Rn\u00d7r, and UTU = VTV = Ir. Then we have vec(\u017d) = Pvec(Z) and P TP = I\u2113r2 , where P = V \u2297 U.\nProof. Since vec(UZV T ) = (V \u2297 U)vec(Z), we only need to show (V \u2297 U)T(V \u2297 U) = I\u2113r2 . In fact\n(V \u2297 U)T(V \u2297 U) = (V T \u2297 UT)(V \u2297 U) = (VTV) \u2297 (UTU) = Ir \u2297 Ir = I\u2113r2 .\nThe proof is completed.\nWe can also demonstrate that the low-rank perturbation matrices across all layers can be represented as a model-scale projection matrix. We first give the following lemma.\nLemma 2. Let a block diagonal matrix P = bdiag(P1, P2,\u2026, P\u2113) and \u017ei = Pizi, where P TPi = I\u2113r2 and i = 1, 2, . . . , \u2113. Then we have z = Pz, where z = [zT1, . . . , zT\u2113]T , z = [zT1, . . . , zT\u2113]T and P TP = I\u2113r2 .\nProof. It is easy to check that z = Pz. Besides, we have\nP TP = bdiag(PT1, . . . , PT\u2113 )bdiag(P1, . . . , P\u2113) = bdiag(PT1P1, . . . , PT\u2113 P\u2113) = I\u2113r2 .\nThe proof is completed.\nWe may define P = bdiag(V1 \u2297 U1, V2 \u2297 U2, \u2026, V\u2113 \u2297 U\u2113) that satisfies P TP = I, z = [vec(Z1)T, vec(Z2)T, . . ., vec(Z\u2113)T]T , and z = [vec(\u017d1)T, vec(\u017d2)T, . . ., vec(\u017d\u2113)T]T . Then ac- cording to Lemma 2, the perturbation vector of SubZero is z = Pz, which is similar as existing random subspace methods in Eqn. (4), but with SubZero's projection matrix being block diagonal and column orthogonal.\nTo prove Theorem 1 and Theorem 2, we first introduce some definitions and lemmas about Gaussian distribution.\nDefination 1. We say z is a standard n-dimensional Gaussian vector (denote by z \u223c N(0, In)), if its probability density function p(z) = K e\u2212||z||22 , where K > 0 satisfies \u222b e\u2212||z||22 dz = 1.\nDefination 2. Let z \u223c N(0, In). We say x is a chi-square random variable with degrees of freedom n (denote by x \u223c \u03c72(n)), if x = ||z||2.\nLemma 3. Let z \u223c N(0, In). For any orthogonal (n \u00d7 n)-matrix Q and continuous function f, we have Ez[f(z)] = Ez[f(Qz)].\nLemma 4. If x \u223c \u03c72(n), then we have\nEz[x] = n, Varx[x] = 2n.\nLemma 5. (Nesterov & Spokoiny, 2017) Let f \u2208 C\u2113,L22 (Rn). Then for all x, y \u2208 Rn, we have\n|f(y) \u2212 f(x) \u2212 (\u2207f(x), y \u2212 x) \u2212 12 (\u22072f(x)(y \u2212 x), y \u2212 x)| \u2264 L26 ||y \u2212 x||3.\nLemma 6. (Nesterov & Spokoiny, 2017) Let z \u223c N(0, In). For 0 \u2264 t \u2264 2, we have\nEz[||z||t] \u2264 nt/2.\nFor t > 2, we have\nnt/2 < Ez[||z||t] \u2264 (n + t)t/2."}]}