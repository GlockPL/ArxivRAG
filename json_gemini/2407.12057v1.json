{"title": "NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2", "authors": ["Tengfei Xue", "Xuefeng Li", "Roman Smirnov", "Tahir Azim", "Arash Sadrieh", "Babak Pahlavan"], "abstract": "Retrieval-augmented generation (RAG) techniques are widely used today to retrieve and present information in a conversational format. This paper presents a set of enhancements to traditional RAG techniques, focusing on large language models (LLMs) fine-tuned and hosted on AWS Trainium / Inferentia2 AI chips via SageMaker. These chips are characterized by their elasticity, affordability, and efficient performance for AI compute tasks. Besides enabling deployment on these chips, this work aims to improve tool usage, add citation capabilities, and mitigate the risks of hallucinations and unsafe responses due to context bias. We benchmark our RAG system's performance on the Natural Questions and HotPotQA datasets, achieving an accuracy of 62% and 59% respectively, exceeding other models such as DBRX and Mixtral Instruct.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation (RAG) is a vital technique that significantly en- hances language models by integrating ex- ternal knowledge dynamically. This ap- proach is transformative for models de- signed to respond accurately to complex queries in real-time, enabling them to access and synthesize information from broader contexts beyond their immediate training data. Such capabilities are crucial for developing personal assistants that can handle a diverse range of user queries with high precision and reliability. Deploying RAG systems today is fraught with chal- lenges. First, training and hosting these systems, although easily doable, is ex- pensive with Nvidia's GPUs that can host their large language models (LLMs). Second, using third-party LLM APIs such as OpenAI's GPT-4 is also costly, with some queries costing $50 to $100 each. Finally, existing LLMs need to be trained and prompt-engineered to an- swer complex queries, cite relevant data sources, and avoid harmful and unsub- stantiated answers. In this paper, we present steps towards solving all of these problems."}, {"title": "Model Enhancements", "content": "We fine-tuned Llama3-instruct 70B model with a number of enhancements. We ex- tended their capabilities to leverage exter- nal tools within generated responses and added accountability and traceability fea- tures to the information provided by the models. We trained the model on a diverse set of data to reduce the likelihood of gen- erating biased responses. Furthermore, we introduced prompt engineering and model response checking mechanisms to detect and correct hallucinations and un- safe content dynamically. To fine-tune the model, we followed the Lima approach [3], which involved using a training sam- ple size of around 20 million tokens. This approach proved cost-effective, amount- ing to less than $1,000. Fine-tuning fo- cused on the format and tone of the out- put while using a diverse but relatively small sample size. The model was fully fine-tuned on a cluster of 32 TRN1 in- stances, with the entire training process taking less than three hours. This method ensures the model's responses are not only accurate but also appropriately styled and formatted, enhancing their overall quality and reliability. Additionally, it helps op- timize the prompt length for the model, further improving efficiency and effective- ness. Given the sensitive nature of fine- tuning to the distribution of training sam- ples, several trial-and-error attempts are often required to identify the optimal sam- ple distribution. This aspect of the pro- cess can make the fine-tuning effort par- ticularly bursty, especially during the ex- perimentation phase. As models react differently to varied data inputs, finding the right balance is crucial but can lead to multiple iterations of training sessions. This bursty nature of model training ne- cessitates access to elastic compute re- sources, allowing for the dynamic scaling of computational power to accommodate the varying demands of the training pro- cess. This flexibility is essential to effi- ciently manage the computational bursts without incurring unnecessary costs dur- ing idle times. Overall, the total cost of these fine- tuning efforts, including the numerous trials and errors involved in op- timizing the training sample distribution, amounted to less than $30,000. This fig-"}, {"title": "Model Deployment", "content": "We deploy the model using the vLLM inference engine. vLLM introduces in- novative memory management techniques to improve the serving of large language models, particularly through features like PagedAttention and block-level memory management. PagedAttention allows for flexible and efficient memory usage by breaking down the attention mechanism's memory requirements into smaller, man- ageable blocks. This approach reduces internal and external memory fragmenta- tion, a common issue in traditional model hosting. Block-level memory manage- ment further optimizes memory usage by dynamically allocating and deallocating blocks based on the sequence lengths and the active decoder head, ensuring min- imal waste and maximal efficiency [5]. Furthermore, we employ multi-bucket- ing to address inefficiencies in traditional model hosting, particularly the high Time to First Token (TTFT) due to prefill op- erations up to the maximum sequence length (8192 for Llama3). This technique segments potential input sizes into dif- ferent buckets (e.g., 128, 1024), allow ing the use of decoder heads tailored to the nearest bucket size greater than the actual input length. This method sig- nificantly reduces unnecessary computa- tions and memory usage, as the appropri- ate bucket is dynamically selected, avoid- ing prefill operations for the maximum to- ken length and thereby reducing TTFT. In conjunction with multi-bucketing, contin- uous batching optimizes throughput and efficiency by processing multiple text gen- eration requests concurrently at the token level. This allows for efficient utilization of accelerator resources by executing op- erations on different requests simultane- ously, even if they are at different stages of generation. For example, some requests might be generating their 5th token while others their 85th. This dynamic allows re- quests to join or leave the batch as they complete their generation, without wait- ing for the entire batch to finish, elimi- nating the need for padding requests to the same length and avoiding idle time on the accelerator. Through the integra- tion of multi-bucketing, continuous batch- ing, and vLLMs advanced memory man- agement techniques, latency and through- put for Llama3 model hosting are signifi- cantly improved, setting the stage for sub- stantial performance improvements."}, {"title": "Serving Infrastructure", "content": "We selected Amazon SageMaker as the primary platform for the deployment and operational management of our machine learning models, ensuring scalability and security. SageMaker greatly simplifies the management and auto-scaling of mod- els, which is crucial for efficiently han- dling variable computational loads and optimizing the utilization of computa- tional resources. Through SageMaker, the machine learning infrastructure is de- fined using the AWS Cloud Develop- ment Kit (CDK), enhancing the consis- tency and reproducibility of deployments. The infrastructure-as-code approach, en- abled by SageMaker, allows for models to be deployed securely across various AWS"}, {"title": "Accuracy Results", "content": "We use an approach similar to [6] to mea- sure the accuracy of our enhanced RAG models. Specifically, we calculate accu- racy by matching the model's answer to the expected answer, using the top 10 pas- sages retrieved from a Wikipedia corpus. While both [6] and our model retrieve data from the same corpus, we perform content filtering and ranking using ColBERTv2 [7], as opposed to bge-large-en-v1.5 used in [6]."}, {"title": "Future Work", "content": "We plan to explore more advanced tech- niques for inference, such as speculative decoding [8] and flash attention [9], to fur- ther enhance the model. These approaches have the potential to significantly reduce latency and increase throughput for com- plex decoding tasks and will be particu- larly beneficial for extending the model's applications to agentic workflows and code generation tasks, where rapid and accurate planning is essential. Using these techniques, we expect to improve"}, {"title": "Conclusion", "content": "In this study, we presented several enhancements to the LLaMA-3 model within a retrieval-augmented generation (RAG) framework, aimed at improving its performance on complex question- answering tasks. We achieved accuracies of 62.22% on the Natural Questions (NQ) Open and 58.84% on HotPotQA by us- ing our enhanced LLaMA-3 RAG model, demonstrating notable improvements over baseline models. The model's enhanced capabilities of multi-hop reasoning and deep contextual analysis, essential for ac- curate and reliable question answering, are highlighted by these results. The model's computational efficiency was sig- nificantly optimized by the integration of multi-bucketing and continuous batching techniques. Multi-bucketing reduces un- necessary computations by dynamically selecting optimal bucket sizes, thereby improving the Time to First Token (TTFT) References and responsiveness of the model. Continuous batching helps in- crease throughput by processing multiple text generation requests concurrently, op- timizing the utilization of computational resources and adapting efficiently to vari- able input lengths. On the model itself, we focused on the safety and reliability of its outputs. We enhanced its robustness and trustworthiness by training on a diverse dataset and implementing mechanisms to detect and correct hallucinations and bi- ases. These improvements are crucial for maintaining the integrity of responses, es- pecially in sensitive applications where user trust is paramount. However, despite these advancements, we observed a slight accuracy gap compared to leading mod- els like GPT-4 Turbo, indicating potential areas for further optimization, particularly in refining the RAG mechanism and ex- panding training datasets."}]}