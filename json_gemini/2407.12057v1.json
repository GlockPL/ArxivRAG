{"title": "NinjaLLM: Fast, Scalable and Cost-effective\nRAG using Amazon SageMaker and AWS\nTrainium and Inferentia2", "authors": ["Tengfei Xue", "Xuefeng Li", "Roman Smirnov", "Tahir Azim", "Arash Sadrieh", "Babak Pahlavan"], "abstract": "Retrieval-augmented generation (RAG) techniques are widely used today to\nretrieve and present information in a conversational format. This paper presents\na set of enhancements to traditional RAG techniques, focusing on large language\nmodels (LLMs) fine-tuned and hosted on AWS Trainium / Inferentia2 AI chips\nvia SageMaker. These chips are characterized by their elasticity, affordability, and\nefficient performance for AI compute tasks. Besides enabling deployment on these\nchips, this work aims to improve tool usage, add citation capabilities, and mitigate\nthe risks of hallucinations and unsafe responses due to context bias. We benchmark\nour RAG system's performance on the Natural Questions and HotPotQA datasets,\nachieving an accuracy of 62% and 59% respectively, exceeding other models such\nas DBRX and Mixtral Instruct.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation (RAG) is\na vital technique that significantly en-\nhances language models by integrating ex-\nternal knowledge dynamically. This ap-\nproach is transformative for models de-\nsigned to respond accurately to complex\nqueries in real-time, enabling them to\naccess and synthesize information from\nbroader contexts beyond their immediate\ntraining data. Such capabilities are crucial\nfor developing personal assistants that can\nhandle a diverse range of user queries with\nhigh precision and reliability. Deploying\nRAG systems today is fraught with chal-\nlenges. First, training and hosting these\nsystems, although easily doable, is ex-\npensive with Nvidia's GPUs that can host\ntheir large language models (LLMs).\nSecond, using third-party LLM APIs\nsuch as OpenAI's GPT-4 is also costly,\nwith some queries costing $50 to $100\neach. Finally, existing LLMs need to\nbe trained and prompt-engineered to an-\nswer complex queries, cite relevant data\nsources, and avoid harmful and unsub-\nstantiated answers. In this paper, we\npresent steps towards solving all of these\nproblems."}, {"title": "Model Enhancements", "content": "We fine-tuned Llama3-instruct 70B model\nwith a number of enhancements. We ex-\ntended their capabilities to leverage exter-\nnal tools within generated responses and\nadded accountability and traceability fea-\ntures to the information provided by the\nmodels. We trained the model on a diverse\nset of data to reduce the likelihood of gen-\nerating biased responses. Furthermore,\nwe introduced prompt engineering and\nmodel response checking mechanisms to\ndetect and correct hallucinations and un-\nsafe content dynamically. To fine-tune the\nmodel, we followed the Lima approach\n[3], which involved using a training sam-\nple size of around 20 million tokens. This\napproach proved cost-effective, amount-\ning to less than $1,000. Fine-tuning fo-\ncused on the format and tone of the out-\nput while using a diverse but relatively\nsmall sample size. The model was fully\nfine-tuned on a cluster of 32 TRN1 in-\nstances, with the entire training process\ntaking less than three hours. This method\nensures the model's responses are not only\naccurate but also appropriately styled and\nformatted, enhancing their overall quality\nand reliability. Additionally, it helps op-\ntimize the prompt length for the model,\nfurther improving efficiency and effective-\nness. Given the sensitive nature of fine-\ntuning to the distribution of training sam-\nples, several trial-and-error attempts are\noften required to identify the optimal sam-\nple distribution. This aspect of the pro-\ncess can make the fine-tuning effort par-\nticularly bursty, especially during the ex-\nperimentation phase. As models react\ndifferently to varied data inputs, finding\nthe right balance is crucial but can lead\nto multiple iterations of training sessions.\nThis bursty nature of model training ne-\ncessitates access to elastic compute re-\nsources, allowing for the dynamic scaling\nof computational power to accommodate\nthe varying demands of the training pro-\ncess. This flexibility is essential to effi-\nciently manage the computational bursts\nwithout incurring unnecessary costs dur-\ning idle times. Overall, the total cost of\nthese fine- tuning efforts, including the\nnumerous trials and errors involved in op-\ntimizing the training sample distribution,\namounted to less than $30,000. This fig-"}, {"title": "Model Deployment", "content": "We deploy the model using the vLLM\ninference engine. vLLM introduces in-\nnovative memory management techniques\nto improve the serving of large language\nmodels, particularly through features like\nPagedAttention and block-level memory\nmanagement. PagedAttention allows for\nflexible and efficient memory usage by\nbreaking down the attention mechanism's\nmemory requirements into smaller, man-\nageable blocks. This approach reduces\ninternal and external memory fragmenta-\ntion, a common issue in traditional model\nhosting. Block-level memory manage-\nment further optimizes memory usage by\ndynamically allocating and deallocating\nblocks based on the sequence lengths and\nthe active decoder head, ensuring min-\nimal waste and maximal efficiency [5].\nFurthermore, we employ multi-bucket-\ning to address inefficiencies in traditional\nmodel hosting, particularly the high Time\nto First Token (TTFT) due to prefill op-\nerations up to the maximum sequence\nlength (8192 for Llama3). This technique\nsegments potential input sizes into dif-\nferent buckets (e.g., 128, 1024), allow\ning the use of decoder heads tailored to\nthe nearest bucket size greater than the\nactual input length. This method sig-\nnificantly reduces unnecessary computa-\ntions and memory usage, as the appropri-\nate bucket is dynamically selected, avoid-"}, {"title": "Serving Infrastructure", "content": "We selected Amazon SageMaker as the\nprimary platform for the deployment and\noperational management of our machine\nlearning models, ensuring scalability and\nsecurity. SageMaker greatly simplifies\nthe management and auto-scaling of mod-\nels, which is crucial for efficiently han-\ndling variable computational loads and\noptimizing the utilization of computa-\ntional resources. Through SageMaker,\nthe machine learning infrastructure is de-\nfined using the AWS Cloud Develop-\nment Kit (CDK), enhancing the consis-\ntency and reproducibility of deployments.\nThe infrastructure-as-code approach, en-\nabled by SageMaker, allows for models to\nbe deployed securely across various AWS"}, {"title": "Accuracy Results", "content": "We use an approach similar to [6] to mea-\nsure the accuracy of our enhanced RAG\nmodels. Specifically, we calculate accu-\nracy by matching the model's answer to\nthe expected answer, using the top 10 pas-\nsages retrieved from a Wikipedia corpus. While both [6] and our model retrieve data\nfrom the same corpus, we perform content\nfiltering and ranking using ColBERTv2\n[7], as opposed to bge-large-en-v1.5 used\nin [6]."}, {"title": "Future Work", "content": "We plan to explore more advanced tech-\nniques for inference, such as speculative\ndecoding [8] and flash attention [9], to fur-\nther enhance the model. These approaches\nhave the potential to significantly reduce\nlatency and increase throughput for com-\nplex decoding tasks and will be particu-\nlarly beneficial for extending the model's\napplications to agentic workflows and\ncode generation tasks, where rapid and\naccurate planning is essential. Using\nthese techniques, we expect to improve"}, {"title": "Conclusion", "content": "In this study, we presented several\nenhancements to the LLaMA-3 model\nwithin a retrieval-augmented generation\n(RAG) framework, aimed at improving\nits performance on complex question-\nanswering tasks. We achieved accuracies\nof 62.22% on the Natural Questions (NQ)\nOpen and 58.84% on HotPotQA by us-\ning our enhanced LLaMA-3 RAG model,\ndemonstrating notable improvements over\nbaseline models. The model's enhanced\ncapabilities of multi-hop reasoning and\ndeep contextual analysis, essential for ac-\ncurate and reliable question answering,\nare highlighted by these results. The\nmodel's computational efficiency was sig-\nnificantly optimized by the integration of\nmulti-bucketing and continuous batching\ntechniques. Multi-bucketing reduces un-\nnecessary computations by dynamically\nselecting optimal bucket sizes, thereby\nimproving the Time to First Token (TTFT)\nReferences and responsiveness of the\nmodel. Continuous batching helps in-\ncrease throughput by processing multiple\ntext generation requests concurrently, op-\ntimizing the utilization of computational\nresources and adapting efficiently to vari-\nable input lengths. On the model itself, we\nfocused on the safety and reliability of its\noutputs. We enhanced its robustness and\ntrustworthiness by training on a diverse\ndataset and implementing mechanisms to\ndetect and correct hallucinations and bi-\nases. These improvements are crucial for\nmaintaining the integrity of responses, es-\npecially in sensitive applications where\nuser trust is paramount. However, despite\nthese advancements, we observed a slight\naccuracy gap compared to leading mod-\nels like GPT-4 Turbo, indicating potential\nareas for further optimization, particularly\nin refining the RAG mechanism and ex-\npanding training datasets."}]}