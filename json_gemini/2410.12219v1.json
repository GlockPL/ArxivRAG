{"title": "OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities", "authors": ["Lichang Chen", "Hexiang Hu", "Mingda Zhang", "Yiwen Chen", "Zifeng Wang", "Yandong Li", "Pranav Shyam", "Tianyi Zhou", "Heng Huang", "Ming-Hsuan Yang", "Boqing Gong"], "abstract": "We introduce Omni\u00d7R, an evaluation suite designed to benchmark state-of-the-art Omni-modality Language Models (OLMs), such as GPT-40 and Gemini. Evaluating OLMs, which integrate multiple modalities such as text, vision, and audio, presents unique challenges. Particularly, the user message might often consist of multiple modalities, such that OLMs have to establish holistic understanding and reasoning across modalities to accomplish the task. Existing benchmarks are limited to single-modality or dual-modality tasks (e.g., image+text or video+text), overlooking comprehensive multi-modal assessments of model reasoning. To address this, Omni\u00d7R offers two evaluation variants: (1) Omni\u00d7Rsynth: a synthetic dataset generated automatically by translating text into multiple modalities\u2014audio, images, video, and hybrids (Omnify!). (2) Omni\u00d7Rreal: a real-world dataset, manually curated and annotated by experts, for evaluating cross-modal reasoning in natural settings. Omni\u00d7R presents a unique evaluation towards assessing OLMs over a diverse mix of modalities, such as a question that involves video, audio, and text, providing a rigorous cross-modal reasoning testbed than any existing benchmarks. Our experiments find that all state-of-the-art OLMs struggles with Omni\u00d7R questions that require integrating information from multiple modalities to answer. Further analysis highlight differences in reasoning behavior and underscoring the challenges of omni-modal AI alignment.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Omni-modality Language Models (OLMs) (Gemini-Team, 2024b; OpenAI, 2024b) has pushed the boundaries of AI by enabling a more comprehensive understanding of real-world inputs across diverse modalities, e.g., text, vision, audio, (Akbari et al., 2021; Gan et al., 2020; Lu et al., 2019; Zellers et al., 2021) and generating outputs that are more aligned with human communications (Gao et al., 2024; Lu et al., 2024; Zhang et al., 2024).\nHowever, the evaluation of these sophisticated OLMs presents unique challenges. While traditional benchmarks (LMSYS-team, 2024) have predominantly focused on models that handle single or dual modalities, such as vision-language or video-text pairs, they fail to capture the complexities that arise when multiple modalities are involved. In real-world scenarios, user inputs are rarely confined to one or two modalities. Instead, they often consist of diverse combinations of text, images, videos, and audio, necessitating a holistic understanding and reasoning across information presented in these modalities for OLMs to effectively perform tasks. This mismatch between existing evaluation methods and the multimodal capabilities of state-of-the-art OLMs has left a significant gap in the assessment of these models.\nOne common flaw in existing OLMs is their inconsistent behavior when presented with the same question in different modalities or mixtures of modalities. Figure 1 presents an example on the Gemini 1.5 Flash (Gemini-Team, 2024a) (similar behaviour also observed in other OLMs, see Section 3.2 for analysis). Particularly, when the same math question is presented in different modalities, such as rendered as image input, or spoke out as the audio input, the model produces varying responses that exhibit significant performance discrepancies, i.e., different reasoning bevhiours or different answers. This observation indicates a lack of robust cross-modal information integration and reasoning capabilities in OLMs. Such inconsistency not only undermines the reliability of these models but also highlights the limitations of current evaluation benchmarks that do not adequately assess performance across diverse modality combinations.\nTo bridge this critical evaluation gap, we introduce Omni\u00d7R, an evaluation suite specifically designed to benchmark the reasoning performance of OLMs across a wide range of modalities. Unlike existing benchmarks that are limited to a maximum of two modalities, Omni\u00d7R provides a comprehensive testbed that includes complex modality combinations such as video + audio + text and image + audio + text, offering a more rigorous and holistic evaluation of these models' capabilities. Specifically, Omni\u00d7R contains two subsets of the data:\n\u2022 Omni\u00d7Rsynth: a synthetic reasoning dataset constructed with a scalable and low-cost automatic method (i.e., Omnify!) to translate information embedded in text to various modalities - audio, images, video, and hybrids of them.\n\u2022 Omni\u00d7Rreal: a real-world reasoning dataset manually collected and annotated with expert annota-tors, for evaluating cross-modal reasoning in the realistic distribution.\nIn construction of Omni\u00d7Rsynth, Omnify! translates text-based inputs into various other modalities, such as images, audio, and video, as well as their hybrid combinations, using programmatic text rendering services, programmatic video construction pipeline, and state-of-the-art text-to-speech service. This scalable synthetic dataset ensures a diverse and robust dataset that challenges OLMs to demonstrate their cross-modal reasoning abilities. Meanwhile, Omni\u00d7Rreal develops a realistic test environment for evaluating omnimodal reasoning. Particularly, we crawled 100 Youtube videos whose topics are related to math, physics, chemistry and coding, and manually curate, convert and annotate the quiz questions from those videos, ensuring that each question is associated with multiple variants, each in one modality or a hybrid of many modalities. With both complementary subsets, Omni\u00d7R allows us to better assess how well OLMs can reason across different modalities and integrate information in a way that mirrors human-like understanding.\nOur evaluation of state-of-the-art OLMs on Omni\u00d7R has yielded several important findings."}, {"title": "2. Omni\u00d7R Benchmark", "content": "In this section, we introduce Omnify! a scalable and low-cost automatic method designed to translate text into various modalities, including audio, image, video, and combinations thereof. The overarching goal of Omnify! is to build up a scalable method to generate omni-modality data while keeping information the same across them for evaluating OLMs' reasoning capabilities across modalities. We construct the Omni\u00d7R benchmark in two subsets: (1) Omni\u00d7Rsynth: a synthetic omni-modal reasoning evaluation dataset derived from applying Omnify! on the MMLU-Pro (Wang et al., 2024). (2) Omni\u00d7Rreal: a real-world omni-modal reasoning evaluation derived from Youtube, which is then processed and annotated by human experts."}, {"title": "2.1. Omnify!", "content": "Text to image. Though there are many ways to convert text into images, like using image generation models (e.g., Imagen-3 (Baldridge et al., 2024), DALLE-3 (OpenAI, 2024a)), however, the seemingly appealing text-to-image generation models make it challenging to control quality; they cannot ensure the generation contains all the information we need to answer a question. Before figuring out how to judge the quality of and information in the generated images, it is not viable to use image generators to scale up the mapping from text to images. Since our main goal is to evaluate models' reasoning capability, we start from the simplest approach in this work: rendering a canvas and then write the words on it. Given the images as input, we expect the models can achieve the same performance as they read text in this ideal scenario, where no extra noises, information losses, or variations are introduced by the text-to-image mapping process. Specifically, we use PIL\u00b9 to create a new image with a white background and the text is drawn onto the image with black color. The engineering details/efforts can be found in Appendix I.\nText to Audio We initially attempted to use Google Text-to-Speech\u00b2 (TTS) for text-to-audio conversion. However, we encountered challenges with the mathematical equations. To address this, we developed a two-step process. First, we convert the original text, if it contains mathematical equations, into a format that is easy to speak orally. The details for the conversion could be found in Table 7. Then, we use a TTS engine to generate the audio, which contains the full information of the original text question.\nText to Video Like text-to-image generation models, there exist Sora (Brooks et al., 2024) and Veo (Google, 2024) we could leverage to map text to videos. However, they would incur the same problems as described in the text to image: quality control, time consumption, and computational cost. The main objective with videos here is to evaluate a model's capabilities on understanding a video input, which is a series of images from a model's view, and then reasoning to solve the problems. We fulfill this objective again using a simple approach to generating the video data from text as follows. Based on our image generation process, we render a series of images where each image contains one or several words from the text. We ensure that the information in the text is fully translated to the video. The input text is split into individual words first. Then we use OpenCV to create a video writer object with a specified frame rate, i.e., 1 FPS, and frame size (300x100 pixels). Each word is converted into an image using the text-to-image method. Finally, these images are combined sequentially to create video frames."}, {"title": "2.2. Omni\u00d7Rsynth: Scalable Synthetic Omini-modal Reasoning Evaluation", "content": "Our initial choices of the text benchmark for Omnify! are Arc-Challenge (Clark et al., 2018) and GSM8K (Cobbe et al., 2021), but we identify the potential data contamination problems on these two benchmarks as Gemini-1.5-pro (Gemini-Team, 2024a) can achieve over 99% on GSM8K (results are shown in Table 9). It is very likely that contaminated OLMs just capture the part of the information they need from the video/audio questions and use their \u2018memory' to give correct answers, which cannot reflect the actual reasoning ability of the models. Thus, we choose MMLU-Pro (Wang et al., 2024), which is augmented from MMLU with ten options per question and released in June after the Gemini-1.5-Pro-0013 release, as the text benchmark to Omnify!. In this way, we minimize the contamination influence, enabling a more accurate study of OLMs' omni-reasoning. We randomly sample 100 questions from each of the 14 categories in MMLU-Pro to construct Omni\u00d7Rsynth. Some examples for Audio and Video modalities are available4."}, {"title": "2.3. Omni Rreal: High-Quality Real-world Omini-modal Reasoning Evaluation", "content": "We crawl the video data from youtube and then transcribe it into different modalities to develop a realistic set as a valuable addition to the Omni\u00d7R. The details about how we collect and process the data are shown as follows:\n1. Video: We select four categories that require dense reasoning in real-world scenarios: Mathematics, Coding, Physics, and Chemistry. Videos are sourced from popular educational channels, such as MIT OpenCourse. Two human annotators, spend approximately 30 hours each to review 100 videos (200 in total) and identify those containing non-trivial questions that demand substantial reasoning to solve. From these, 100 videos are carefully selected to construct a high-quality set, Omni\u00d7Rreal. Each video clip is curated based on the following criteria: (1) it must contain one or more key frames that provide all the necessary information to solve the question; (2) the clip should exclude the answer to maintain the challenge; (3) some misleading or irrelevant frames are intentionally included to assess the model's robustness in reasoning.\n2. Image: We manually find the key frame(s) which contain the question information. It should be noted that in some cases, there might be several frames containing the relevant information, where we will crawl two or three frames and merge them together into one image.\n3. Text: Five human annotators transcribe the text from the video with the help of the tools, e.g., Gemini. All the open-ended generation questions are transferred into multiple choice questions to make the benchmark easy-to-use.\n4. Audio: The original audio will be checked first, which is extracted from the video we crawled. If it contains all the information for OLMs to answer the question, then we will just keep and use it. However, there are many cases where the audio does not contain the enough information for answering the questions, e.g., the instructor shows a slide and asks \u201csolve the problems in the slide\", where the problem is shown in image. In that scenario, we will use the same method in Omnify! to transfer the transribed text into audio by Google TTS.\""}, {"title": "3. Experiments and Findings", "content": "3.1. Experiment Setup\nModels. We mainly test three series of models: Gemini (Gemini-Team, 2024a), i.e., Gemini-1.5-Pro, and Gemini-1.5-Flash, OpenAI-GPT (OpenAI, 2024c), i.e., GPT-40 and GPT-40-mini, Anthropic-Claude (Anthropic, 2024), i.e., Claude-3-Opus, Claude-3-Sonnet, Claude-3-Haiku. More details about the test models are shown in Appendix E.\nCoT Prompting. The standard setting in MMLU-Pro (Wang et al., 2024) is to use Chain-of-Thought(CoT) prompting to elicit the reasoning ability of the OLMs for a more comprehensive evaluations. Following them, we use CoT with 0-shot, as our standard setting, i.e., the prompt used for evaluation is \u201cThink step by step then output the answer in the format of \"The answer is (X)\u201d at the end.\"\nExtract-Then-Answer (ETA) Prompting. In addition, we employ Extract-Then-Answer (ETA) prompting, leveraging the benchmark's inherent structure. This method involves first extracting the textual content and then using the OLMs' language capabilities for reasoning to provide answers based on the transcriptions. To prevent potential hackings on Omni\u00d7R, we transparently demonstrate this approach in our benchmark, aiming for a comprehensive evaluation of OLMs. Specifically, the prompt 'Please extract the text from image/audio/videos' instructs the OLMs to function as text extractors. The extracted text from this initial step is subsequently fed back into the same OLM with Chain-of-Thought (CoT) prompting to obtain the final answer. Consequently, the model's performance reflects two key abilities: OCR/Transcription and Text Reasoning.\"\nVideo/Audio/Image. We first process the video to 1-fps to meet the requirements for both the Gemini and GPT models. For testing with Claude, we used the API available before August 10th, which only supported a maximum of 5 image inputs, so video evaluations were not conducted. The GPT-40 API supports 250 images input at the maximum, so any additional frames were dropped in the evaluation. In contrast, Gemini had no issues with the video modality and could handle all frames as input. Image processing is the modality that all models support most effectively, allowing comprehensive testing across all OLMs. Notably, Gemini is the only model supporting audio input.\nAnswer Extraction: We use the model to extract the answers. Since the regex parsing may affect the performance, we sacrifice the API cost to trade in the excellent extraction."}, {"title": "3.2. Main Results on Omni\u00d7Rsynth", "content": "We show the main experimental results on ominified MMLU-Pro in Table 1.\nModel Comparison. Gemini-1.5-Pro demonstrates the most versatile performance across all modalities, showing results in text, image, audio, video tasks. Claude models struggle with image tasks and lack audio and video capabilities. GPT models show a balanced performance, with GPT-40 performing particularly well in direct image and video compare to Gemini and Claude. Generally, larger models outperform their smaller counterparts across modalities, e.g., Pro > Flash, Opus > Haiku). But interestingly, GPT-40-mini outperforms GPT-40 in text and video with ETA prompting. For video tasks using ETA prompting, GPT-40's performance inconsistencies led us to examine the model's responses to the extraction, we found that in over 46.8% test samples, the detailed analysis can be found in Appendix H, GPT-series models cannot extract the text from video, which we identify as the primary cause for the significant performance drop compared to CoT prompting. Regarding the text modality, two possible explanations emerge: first, MMLU-Pro was released before GPT-40-mini, suggesting that OAI might have optimized for it. Second, since our dataset uses a subset sampled from MMLU-Pro, inherent biases may have influenced the results.\nModality Analysis. Text is the most mature modality across all models, with consistently high scores (ranging from 69.9% to 77.7%). Image modality shows significant variability, with direct task performance ranging from 9.9% (Claude Haiku) to 60.1% (GPT-40). However, ETA prompting on image generally improves performance for all models, particularly for Claude (e.g., Opus improves from 18.8% to 62.6%). The improvement justifies the inclusion of ETA prompting as a standard in our benchmark to prevent potential manipulation. Audio modality, only available for Gemini models, shows moderate performance with notable improvement via ETA prompting. Video modality presents the most challenges, especially for the small models, i.e., Gemini-1.5-Flash, and GPT-40-mini."}, {"title": "3.3. Main Results on Omni\u00d7Rreal", "content": "The results on the realistic set generally align with those from the synthetic set, showing significant drops in performance across audio, image, and video tasks compared to the text. One difference here is that performance on video does not drop a large margin compared to that in the synthetic set. Though the video is noisy than it is in the synthetic data, we can still capture one key frame and answer the question according to that key frame which largely reduces the difficulties, compared to the synthetic scenario, if the model can find the main frame in the video. Another interesting finding is that ETA prompting does not consistently improve performance; for example, there are performance drops in audio tasks with ETA prompting compared to CoT on both Gemini-Flash and Gemini-Pro. These findings confirm that our synthetic set effectively simulates real-world scenarios in a scalable, cost-efficient way, serving as a valuable sanity check for OLMs' omni-modality reasoning capabilities."}, {"title": "Key Takeaways", "content": "We summarize the following interesting takeaways from our experiments:\n1. Multi-modal capabilities vary significantly across models, with Gemini 1.5 Pro showing the most broad support and balanced performance across all modalities.\n2. Gaps still exists on other modalities compared to the text modality even just in such easy perception test scenarios. Significant room for improvement exists in video processing across all models, presenting opportunities for future development.\n3. ETA prompting generally improves performance on Omni\u00d7Rsynth but OLMs can no longer solely rely on it for Omni\u00d7Rreal, indicating the necessity of the further alignment on omni-modality.\n4. There's a clear trade-off between model size and performance, but smaller models (e.g., GPT-40-mini) can sometimes outperform larger counterparts in specific tasks.\n5. Our Omni\u00d7Rsynth could be a good simulating set for the real-world scenarios, as the results on Omni\u00d7Rreal match the results in the Omni\u00d7Rsynth\u00b7"}, {"title": "4. Mixed Modalities", "content": "Text to Mixed Modalities. In addition to the types of the Omnify! described in Section 2.1, our method could also be applied to generating interleaved modalities to better simulate more complex real-world scenarios, where the information is included in different modalities and requires a model to reason across the modalities to solve a problem. For example, an instructor can write down an equation on the blackboard and say \u201ccompute the derivative\u201d in a Calculus lecture. Scenarios like this example require a model to jointly use image perception and audio understanding process the question, reason across the visual and audio modalities, and then provide a response. Using our Omnify!, we seamlessly integrate different modalities and create test samples with interleaved modalities, i.e., \u201cVideo + Audio\u201d, and \u201cImage + Audio\u201d, to Omni\u00d7Rsynth, which captures a more authentic user experience where multiple senses are engaged simultaneously. To be specific, We transfer the question into video and all the options are transferred for Audio, to get the modality, \"Video + Audio\", while CoT prompting remains in text form to maintain the model's reasoning ability across different modalities.\nTransferring CoT prompt to other modalities. All the CoT prompting is in text for all the previous test cases. Here, we convert the CoT prompt into different modalities while keeping the others, i.e., questions and options in MMLU-Pro intact.\nResults. As shown in Table 3, there is a noticeable decline in performance when transitioning from text to mixed-modality tasks. For example, both the Pro and Flash models perform significantly worse in the \"Video + Audio\" scenario, achieving scores of 40.1 and 25.9, respectively. This indicates that handling mixed modalities presents a significant challenge, likely due to the increased complexity of integrating video and audio information. For Audio/Image/Video CoT, the model generally treats these inputs as noise or irrelevant context, having minimal impact on the final results, as performance approaches that observed with text-based CoT. We focus on evaluating the Gemini-series models since only Gemini supports audio inputs."}, {"title": "5. Analysis", "content": "5.1. Omni-Modality Reasoning Behaviour Analysis\nAfter investigating the responses, we find that in omni-modality cases, Gemini-1.5-Flash models can only output very short answers though prompted to CoT before giving the answers, which is quite different from the reasoning behaviour in the pure-text. An example in Figure 1 shows the different behaviours among modalities, which intrigues us to have a quantitative analysis of the reasoning paths. We write a simple regex, detecting if the model output starts with \"the answer/response is (*.)\", with the rule, the total number of words should be less than 40, to evaluating whether the models' output contain the reasoning path. The results are shown in Table 4.\nOur analysis reveals that smaller models tend to produce reasoning paths less frequently for image, video, and audio inputs. Notably, for complex modalities like video, Gemini-1.5-Flash generates reasoning paths for only 23.4% of test examples, substantially lower than Gemini-1.5-Pro. Among the modalities, audio inputs elicit reasoning paths most similarly to text, while video inputs show the lowest rate of reasoning path generation. GPT-series models demonstrate excellent performance in producing reasoning paths across available modalities. However, these results underscore the significant challenges remaining in cross-modal reasoning. Given that models are expected to exhibit reasoning abilities, they should ideally output reasoning paths consistently across all input modalities."}, {"title": "5.2. Visual/Video Formats influences Perception Precision", "content": "5.2.1. Image\nWe first analyze how formats affect the performance on images. We show images with two different text formats in Figure 5. The lower image has a compact format, where the options are not spaced out; instead, they are presented in a continuous, inline format separated by periods. Compared to it, each option in the upper image is listed separately, making it easy to read, with letters (A to J) clearly aligned before each option. The results of CoT and ETA prompting with two different formats of images are shown in Table 6. The overall trend here is that with better format, we could significantly improve the performance across all the tested models. ETA prompting also boosts the performance for the both formats in general. For all the other models, the performance can be significantly improved when comparing BF with ETA, only the GPT-40 being an outlier.\nWe further analyze transcription accuracy using the Character Error Rate (CER), a standard metric for assessing text recognition performance, especially in OCR tasks. A CER of 0 indicates perfect accuracy, with higher values reflecting more errors. Details of the CER calculation are provided in Appendix J, and results are shown in Table 5. The results reveal that GPT-40's OCR performance is largely format-independent, whereas other models exhibit considerable format sensitivity, explaining the pronounced improvements seen with ETA prompting for all models except GPT-40 when format is enhanced."}, {"title": "5.2.2. Video", "content": "We create different types of videos, one word per frame, several words per frame, etc. Our ablations reveal that increasing the number of words per frame generally leads to improved performance for both Gemini-Flash and Gemini-Pro models under both testing promptings, CoT and ETA prompting. This trend suggests that providing more context within each frame aids in the models' understanding and processing of the video content and narrow the gaps between images and videos."}, {"title": "6. Related Work", "content": "Large Foundational Models. GPT-40 (OpenAI, 2024b), Gemini (Gemini-Team, 2024a) both claim their models having omni-modality capabilities, but actually OAI's model does not support audio(no audio access via APIs)/video(only 250 frames and the videos should be separated manually before feeding into the model) while Gemini can take very long videos and has good Audio support.\nClaude (Anthropic, 2024) can be viewed as a vision-language model (Bordes et al., 2024) since it has capabilites to take image but no audio or video support. There are also other open-sourced vision language models, but they are mostly supporting only two modalities, e.g., the vision-language models like LLaMA-3.1 and 3.2 (Meta, 2024), Pixtral (Mistral, 2024), LLaVA (Liu et al., 2023a,b); Audio-LLM like GAMA (Ghosh et al., 2024), LTU (Gong et al., 2023a,b), and SALMONN (Tang et al., 2024). It is hard to judge them on our benchmark, since the main idea behind our evaluations are that we expect the model has cross-modality reasoning and would like to encourage the model improving their cross-modal reasoning, only vision/audio/video would not get a comprehensive results. We would expect the open-sourced community to release real OLMs in the future and we will update the results accordingly.\nVideo/Audio/Image Evaluation benchmarks. Omnibench (Li et al., 2024b) specifically aimed at evaluating OLMs' tri-modal, i.e., text, vision, and audio, processing capabilities with human-annotated tasks. Compared to it, OmnixR emphasizes the omni-modality reasoning evaluations with both human-annotated realistic set and scalable synthetic set. MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), CMMMU (Ge et al., 2024) focuses on evaluating vision-language models across various college-level disciplines with highly heterogeneous image types, emphasizing expert-level perception and reasoning across text-image pairs while LMSYS-Vision (LMSYS-team, 2024) evaluates the instruction-following of the large vision-language models (Chen et al., 2023, 2024; Liu et al., 2023a; Yang et al., 2024). Compared to them, OmnixR has larger scope on evaluating OLMs on cross-modality reasoning, not only vision input, but audio, video, and mixed modalities such as image + audio. AiShell-1, AiShell-2 (Du et al., 2018), Clotho-AQA (Lipping et al., 2022) are audio understanding benchmarks, providing extensive and high-quality real-world audio data for Mandarin ASR and audio question answering. MVBench (Li et al., 2024a) focuses on temporal reasoning across 20 challenging video tasks, Video-Bench (Ning et al., 2023) assesses Video-LLMs across video-"}, {"title": "7. Conclusion", "content": "In this paper, we introduced Omnify!, a scalable and cost-efficient approach for generating multi-modal data from text, facilitating the construction of diverse and challenging test scenarios for omni-modal language models (OLMs). Using this method, we developed Omni\u00d7Rsynth, a synthetic omni-modal reasoning evaluation dataset derived from MMLU-Pro, as well as Omni\u00d7Rreal, a real-world omni-modal reasoning dataset based on YouTube content. Our comprehensive evaluations reveal that OLMs experience substantial performance drops when confronted with complex multi-modal inputs, particularly in tasks that demand cross-modality reasoning. Notably, we observed that smaller models, e.g., Gemini-1.5-Flash, are less adept at producing reasoning paths for image, video, and audio inputs compared to text, underscoring the inherent challenges in cross-modal reasoning. The evaluation results underscore the necessity for enhanced training strategies to address the complexities of omni-modal tasks. To sum up, Omni\u00d7R stands as a critical benchmark for guiding future advancements in OLMs, providing a foundation for measuring progress toward more human-aligned and truly omni-modal Al systems."}, {"title": "A. Author Contributions", "content": "\u2022 Lichang devotes to revising the idea and constructing the Omni\u00d7Rsynth, finishing most of the code and experiments.\n\u2022 Hexiang proposed the initial idea for Omni-Eval via the observations on image reasoning behavior inconsistency of Gemini-Flash models.\n\u2022 Hexiang, Boqing, Yiwen, Zifeng, Mingda, and Yandong contributed to the Omni\u00d7Rreal.\n\u2022 Hexiang, Mingda, Yandong, Boqing, and Tianyi attend the discussion regularly and provide useful feedback/suggestion for the project.\n\u2022 Pranav contributes to the ablation study: images with better format and video ablations.\n\u2022 Heng provided the university tuition support to Lichang.\n\u2022 Ming-Hsuan and Boqing are project leads @Google Deepmind."}, {"title": "B. Acknowledgements", "content": "We thank the useful feedbacks from talented researchers@Google Deepmind:\n\u2022 Quoc V. Le, Xinyun Chen: Thanks for providing useful feedbacks about the reasoning and irrelevant contents.\n\u2022 Prajit Ramachandran, Yiling Huang, Yichong Xu: Thanks for insights and future works on synthetic audio evals/benchmarks.\n\u2022 Micheal Chang, Yao Fu: Thanks for the discussions of the real-world applications of Omni\u00d7R.\n\u2022 Jean-Baptiste Alayrac, Fangyu Liu: Helpful discussions for video/image evals and possibility to include the data into pretrain corpus.\n\u2022 Yong Cheng, Yanping Huang, Ruibo Liu: Helpful discussion on synthetic data vs. real-world data, and how to reduce data contamination on evaluations.\n\u2022 Kelvin Guu, Aida Amini: thanks for the support and give Lichang time to wrap up his previous intern project."}, {"title": "C. Convert Math into Spoken Version", "content": "For the math equations in the questions, we prompt Gemini-1.5-Pro to convert them into the version which can be spoken orally. The prompt we used is detailed in Table 7. We also show an example to explain the transformation: the TTS is hard to read the original question in Table 8 but it can handle the converted text."}, {"title": "D. Categories in MMLU-Pro", "content": "There are 14 categories in MMLU-Pro, including Math, Physics, Chemistry, Law, Engineering, Other, Economics, Health, History, Psychology, Business, Biology, Philosophy, Computer Science."}, {"title": "E. Model Settings/Details", "content": "The version of the Geminis we used in this paper are Gemini-1.5-Pro-001 and Gemini-1.5-Flash-001. The version of the OpenAI models we used are gpt-40-2024-05-13, and gpt-40-mini-2024-07-18. The verison of the Claude models we used are claude-3-sonnet@20240229, claude-3-opus@20240229, claude-3-haiku@20240307.\nThe Gemini safety settings we used for video, audio, and images are shown as follows:"}, {"title": "F. Results on Arc-Challenge & GSM8K", "content": "We also evaluate Gemini models on ARC-Challenge dataset and GSM8K test set. The results are shown in Table 9."}, {"title": "G. Omni\u00d7R Statistics", "content": null}, {"title": "H. Analyze the extraction", "content": "We manually check the data first, and then find the patterns that the extraction failure have are mostly \"unable to process\", \"can't extract\", \"I'm sorry\", and \"unable to extract\". So we use these four patterns to check if the answers contain one of them, and calculate the percentage of the model answers which do not output the extractions when prompted as \"Please extract the text from video.\""}, {"title": "[Original Question]", "content": "For what values of x is it true that $x^2 \u2013 5x \u2212 4 \u2264 10$? Express your answer in interval notation."}, {"title": "[Converted Text]", "content": "The spoken version: For what values of x is x squared minus five x minus four less than or equal to ten? express your answer in interval notation."}, {"title": "[Question]", "content": "Question: Find 4(3/4) % of $24.85. Options: A.$1.50 B.$1.10 C.$0.99 D.$1.25 E.$1.35 F.$1.18 G.$1.00 H.$0.93 I.$1.20 J.$1.05\""}, {"title": "[Prompt]", "content": "Think step by step then output the answer in the format of \"The answer is (X)\" at the end."}, {"title": "[Response(to Text)]", "content": "To find 4 (3/4) % of $24.85", "steps": "n1. Convert the mixed number 4 (3/4) into an improper fraction: - 4 (3/4) = 4 + (3/4) = (4* 4/4) + (3/4) = (16/4) + (3/4) = 19/4.\n2. Convert the fraction to a percentage: - (19/4) % = (19/4) * (1/100) = 19/400.\n3. Now", "24.85": "First", "19/400)": 24.85, "multiplication": 24.85, "places": ""}]}