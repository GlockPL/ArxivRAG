{"title": "Learning to Ask: Conversational Product Search via Representation Learning", "authors": ["JIE ZOU", "JIMMY XIANGJI HUANG", "ZHAOCHUN REN", "EVANGELOS KANOULAS"], "abstract": "Online shopping platforms, such as Amazon and AliExpress, are increasingly prevalent in the society, helping customers purchase products conveniently. With recent progress on natural language processing, researchers and practitioners shift their focus from traditional product search to conversational product search. Conversational product search enables user-machine conversations and through them collects explicit user feedback that allows to actively clarify the users' product preferences. Therefore, prospective research on an intelligent shopping assistant via conversations is indispensable. Existing publications on conversational product search either model conversations independently from users, queries, and products or lead to a vocabulary mismatch. In this work, we propose a new conversational product search model, ConvPS, to assist users to locate desirable items. The model is first trained to jointly learn the semantic representations of user, query, item, and conversation via a unified generative framework. After learning these representations, they are integrated to retrieve the target items in the latent semantic space. Meanwhile, we propose a set of greedy and explore-exploit strategies to learn to ask the user a sequence of high-performance questions for conversations. Our proposed ConvPS model can naturally integrate the representation learning of the user, query, item, and conversation into a unified generative framework, which provides a promising avenue for constructing accurate and robust conversational product search systems that are flexible and adaptive. Experimental results demonstrate that our ConvPS model significantly outperforms state-of-the-art baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid growth of digital markets and the widespread use of e-commerce platforms have led to a surge of research activity in the field of e-shopping [57, 78, 85]. A key functionality of e-shopping platforms is product search, which aims to support users to effectively locate products that they wish to buy [57].\nTraditional product search systems usually require customers to formulate queries and browse through the resulted products to locate the target items (i.e., items they finally purchase). Searching and browsing turn out to be inefficient. Often this is due to the fact that the customers' descriptions of products do not match the vendors' descriptions [41]. The experience may become worse for mobile users since scanning a long list of products is impractical on limited bandwidth interfaces [74]. This leads to an increasing concern as mobile users continue to rapidly expand around the world. Fortunately, the rise of intelligent assistants (e.g., Google Now, Apple Siri, and Microsoft Cortana) and dialogue systems provide new interaction modes between humans and systems through conversations. This lays the groundwork for a conversation-based intelligent assistant for product search, with the aim to alleviate the burden of reformulating queries and browsing through dozens of products, as well as offer a more natural way to dictate preferences and product characteristics. Compared to traditional product search, conversational product search systems interact with users by natural language and collect explicit feedback from users to clarify users' needs, leading to a better understanding of users' dynamic preferences.\nExisting publications on product search mostly focus on traditional product search, using exact term matching methods [50] or semantic matching functions [3, 63] to match the query with the product directly or in the latent space. Conversational product search, on the other hand, remains a challenging open problem. Zhang et al. [78] presented the first conversational model for product search, in which a unified approach was proposed for conversational product search and recommendation by asking questions over aspect-value pairs extracted from user reviews. Similarly, Bi et al. [9] also collected user feedback on the aspect-value pairs while they proposed a paradigm for conversational product search based on negative feedback. Instead of item aspects, [85] asked questions based on extracted informative terms and proposed a question-based Bayesian product search model. Although these aforementioned approaches demonstrate their success to some extent, there are still some limitations. Bi et al. [9] focused on learning from negative feedback from shown items and model conversations fully independently. [85] updated user preferences and selected products on the basis of lexical overlap between terms in clarifying questions and product descriptions, leading to a hard-matching problem, i.e., ignoring the fact that a term may not appear in the description of a product but a synonym term or passage may make the product still relevant. Zhang et al. [78] treated users, queries, products, and conversations in the same way by creating a concatenation of word embeddings to express user preferences, which neglects the hierarchical structure of users, queries, products, and conversations, and may suffer from the vocabulary mismatch.\nTo relax the above limitations, in this work we propose a Conversational Product Search model, ConvPS, to assist users in reaching their target items interactively. We carefully design our model as a unified generative model, which integrates representation learning of user, query, item, and conversation into a unified framework (rather than model them fully independently). To this end, the hard-matching problem, hierarchical structure problem, and vocabulary mismatch problem are also effectively alleviated by conducting product retrieval via soft-matching based on the jointly learned user, query, item, and conversation representations in the learned hierarchical latent semantic space. The probability of observed user-query-conversation-item quadruples can be deduced directly from their distributed representations, making the whole framework explainable and extendable. Our ConvPS model maintains a separate conversation representation and then combines it into the user representation and query representation to update the ranked list of items. Specifically, our ConvPS model (1) constructs a question pool based on slot-value pairs (aspect-value) following [9, 78], (2) learns the representation of users, queries, items, and conversations in terms of slot-value pairs, (3) learns to ask a sequence of high-performance questions (slots) to the user based on a greedy or an explore-exploit strategy, (4) collects user feedback and enriches the user query with conversation representations, and (5) retrieves target items by using the user representation and the enriched query representation.\nThe major contribution of this paper is three-fold:\n(1) First, we propose a new conversational product search model, ConvPS, that integrates the representation learning of user, query, item, and conversation into a unified generative framework, and thus retrieving the relevant items based on these jointly learned representations in the latent semantic space.\n(2) Second, we propose four greedy and explore-exploit learning to ask strategies to select a sequence of high-performance questions to ask, perform analysis to compare these learning to ask strategies and validate their effectiveness.\n(3) Third, we propose a framework that can effectively incorporate two independent modules: the representation learning module for retrieving items and learning to ask module for selecting questions to ask.\nOur experiments on the product search datasets demonstrate that our ConvPS model significantly improves the performance of product search compared to state-of-the-art baselines.\nThe rest of this paper is organized as follows. In Section 2, we discuss the related work. Section 3 introduces the details of our approach. Section 4 describes the experimental setup, research questions, experimental results and corresponding analysis, while Section 5 concludes the paper."}, {"title": "2 RELATED WORK", "content": "In this section, we summarize the related work which falls into three categories: product search, learning to ask, and conversational search. There is a large number of studies on the aforementioned topics. Here we only review work closely related to our research presented in this article."}, {"title": "2.1 Product Search", "content": "Compared with ad-hoc retrieval tasks (e.g., Web search) for finding relevant documents [27, 30], product search focuses on locating not only relevant products but also relevant products a user is willing to purchase [3, 71], making the task more challenging. Given a user query, multiple products could be relevant but only a few are actually purchased. Further, often product descriptions come with metadata (e.g., brand name, types, and categories), leading to a semi-structured information space. Considerable work has been done on product search based on structured product information [43]. Despite the effectiveness structured knowledge can offer, Vandic et al. [65, 66] found free-form user queries do not make use of structured knowledge available on product information pages. Instead, there is a vocabulary gap between product specifications and search queries [19, 20], with the concepts expressed by different terms in the product description and user queries [63, 64]. To alleviate the problem of the vocabulary gap, plenty of attempts on representation learning have been introduced. For example, Duan et al. [19, 20] proposed a probabilistic mixture model based on query generation via two language models. Van Gysel et al. [63] presented a latent vector space model to learn representations of queries and products, so that they can be mapped to the same latent space to calculate the similarity between them. Then Ai et al. [3] noticed the importance of personalization in product search and thus proposed a hierarchical embedding model by incorporating user representations for personalized product search. Later, Xiao et al. [72] proposed a Dynamic Bayesian Metric Learning model to solve the problem of personalized product search under a streaming scenario. Ai et al. [2] and Guo et al. [24] then applied an attention mechanism to improve the performance of personalized product search. On the other hand, [4] focused on the explainability and constructed an explainable retrieval model for product search. Ahuja et al. [1] proposed a multi-lingual multi-task learning framework and studied product search in a cross-lingual information retrieval context. The effectiveness of incorporating external information for product search is also explored by some work [10, 25, 69, 80]. Besides representation learning techniques, there are a variety of studies on extracting different features and feeding them into learning-to-rank models [29, 33]. Other factors such as diversity [73] and user satisfaction [12, 38] are also studied. Similar to previous publications, in this paper, we also attempt to alleviate the language gap between product-related documents (e.g., product description and user reviews) and user queries based on representation learning. Different from the aforementioned publications that treat product search as a non-interactive problem, we propose a conversational product search approach, where ranking is conducted by a dynamic process, i.e., the ranking in the next iteration incorporates the conversation based on the fine-grained feedback from users.\nRecently, conversational product search has gained interest within the information retrieval community. Zhang et al. [78] treated conversational search and recommendation together and proposed a first conversational product search model with multi-memory network architecture. They ask questions over item aspects extracted from user reviews and collect user feedback on the corresponding values for the asked aspects. Similarly, Bi et al. [9] also constructed conversations and enquire user's explicit responses on the aspect-value pairs mined from product reviews while they highlight the value of negative feedback in conversational product search. Instead of item aspects, Zou and Kanoulas [85] queried users on extracted informative terms (typically entities) from item-related descriptions and reviews, and proposed a sequential Bayesian method based on a cross-user duet training for conversational product search. Zou et al. [87] conducted an empirical study to quantify and validate user willingness and the extent of providing correct answers to the asked questions in question-based product search systems. In this work, we follow Zhang et al. [78] and Bi et al. [9] to construct conversations based on aspect-value pairs. Different from the above publications, we jointly learn the representations of users, queries, items, and conversations via a unified generative model. Moreover, we propose four greedy and explore-exploit learning to ask strategies to select a sequence of high-quality questions to ask to the user, and validate their effectiveness."}, {"title": "2.2 Learning to Ask", "content": "The research on applications of learning to ask is broad. For example, Rao and Daum\u00e9 III [53] learned to ask useful questions for clarification on community question answering. Hu et al. [28] and Chen et al. [14] investigated the optimal strategy of question selection on a 20 Questions game setup. Aliannejadi et al. [6], Hashemi et al. [26], Rosset et al. [56] and Zou and Kanoulas [86] focused on learning to ask informative and useful clarifying questions in information-seeking systems. Christakopoulou et al. [16], Ren et al. [55] and Zou et al. [84] aimed to select next questions to ask the user for conversational recommendation [88]. Ruotsalo et al. [58] focused on selecting relevant keywords to display to the user for exploratory search. Although the importance of learning to ask in different tasks, learning to ask in product search is still relatively unexplored. Zou and Kanoulas [85] learned to ask a good question based on user preferences and the rewards over question performances. Zhang et al. [78] predicted the next question to ask to the user by maximizing the probability of the next question based on the softmax output layer for probability estimation. Compared with their greedy question selection strategies, we propose a set of systematic learning to ask strategies, including both greedy and explore-exploit strategies along with a comparative analysis among them. Further, Zou and Kanoulas [85] asked questions about descriptive terms of items while we query users about aspects. Zhang et al. [78] asked questions over limited aspects, relying on the log-likelihood of probability estimation of the next aspect. Instead, we learn to ask questions over all aspects in the collection which is more challenging, based on one-hot representation in terms of aspect appearance in items.\nActive learning [61] and bandit learning have also been used in the area of learning to ask. Active learning is mostly used for selecting informative items to form item-based questions to interact with users. For instance, Iovine et al. [31] investigated the effects of several item selection strategies based on active learning in the conversational recommender scenario. Christakopoulou et al. [16] deployed several active learning strategies to identify the most appropriate items to ask the user for preference elicitation. They also conducted experiments to compare the performance of active learning based question selection techniques with bandit learning based approaches. As active learning methods query for labels that provide a high amount of new information, bandit learning approaches balance the need to explore new information with a focus on exploiting what has already been learned [7]. Such an explore-exploit balance may help focus on the most relevant questions, while still considering unexplored questions with great potential of high effectiveness. Following this line, Christakopoulou and Banerjee [15] proposed a collaborative-bandit approach to tackle the explore-exploit trade-off in the problem of learning to interact with users. Zhang et al. [77] and Li et al. [42] proposed UCB methods or Thompson Sampling based model to deal with the explore-exploit trade-off in conversational recommender scenarios. In this paper, we propose both greedy and bandit learning strategies along with a comparison analysis among them in conversational product search."}, {"title": "2.3 Conversational Search", "content": "Early exploration of conversational search investigated mixed-initiative systems by interacting with users via script-based conversation during a search session [8]. Recently, researchers investigated conversational search by asking clarifying questions (CQs) [56, 70, 74]. Radlinski and Craswell [52] introduced a theoretical framework for conversational search and highlighted the importance of conversational search. Among previous studies about conversational search, they can be classified into four main categories: algorithms for conversational search [26, 34, 70, 74, 86], evaluation of conversational search [35, 44], datasets for conversational search [5, 6, 17, 47, 51, 54, 75], and empirical studies for conversational search [36, 37, 62, 67, 76, 83, 87]. Compared with the aforementioned work on conversational search to retrieve documents or answers, our research focuses on product-seeking conversations."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce our proposed conversational product search model, ConvPS. A research framework based on the ConvPS model is shown in Figure 1. This framework includes four modules: (1) question pool construction; (2) representation learning; (3) learning to ask; and (4) item ranking."}, {"title": "3.1 Problem Formalization", "content": "Assume we have M users $U = \\{U_1, U_2, . . ., u_m \\}$ and N items $V = \\{v_1, v_2, . . ., v_N \\}$. Each item $v_j \u2208 V$ is a product in e-commerce, and each item has its textual description $d_j$. A search session is initiated with a query Q issued by a user $u_i$. During the search session, suppose we ask the user a clarifying question and the user provides an (associated) answer for the asked clarifying question, denoted as $(q', a')$. In the conversation for each search session, we ask the user a sequence of clarifying questions and collect a sequence of user answers for the asked clarifying questions. Suppose we asked L-round questions, the sequence of clarifying questions and answers are denoted as $\\{(q_1, a_1) (q_2, a_2), . . ., (q_L, a_L ) \\}$. Thus, the sequence of actions in the conversation can be represented with:\n$u_i \u2192 Q \u2192 q_1, a_1, q_2, a_2,..., q_L, a_L, v^*$,\nwhere $v^*$ is the target item the user is looking for. The goal of the system is, after collecting the L-iteration user's feedback, to show a ranked list of items that ranks the target item $v^*$ on the top."}, {"title": "3.2 Question Construction", "content": "In this work, the questions in conversations are constructed based on clarification features, which are extracted or predefined from all the textual descriptions. This means we ask clarifying questions"}, {"title": "3.3 Representation Learning", "content": "In this section, we will explain how we learn different representations, including user representations, item representations, slot & value representations, and query representations via a unified generative framework."}, {"title": "3.3.1 Item Generation Model", "content": "Following the generative process of Equation 1, we construct an item generation model. An item v is generated from a user u, the user's initial query Q, and a sequence of clarifying questions and their associated answers (q', a') which is represented by slot-value pairs (q, a). Inspired by the work of embedding-based generative framework [3, 4, 9, 46, 78], we use a softmax function to compute the probability of an item v conditioned by the user embedding, query embedding, and slot-value embeddings:\n$P(v|u, Q, q, a) = \\frac{exp(v\u00b7 (\\lambda_u u + \\lambda_Q Q + \\lambda_c C_{qa}))}{\\sum_{v'\u2208V} exp(v' \u00b7 (\\lambda_u u + \\lambda_Q Q + \\lambda_c C_{qa}))}$,\nwhere $C_{qa}$, v, u, and Q are the embeddings of (q, a), items, users, and queries, respectively (will be introduced next). $\\lambda_u, \\lambda_Q$, and $\\lambda_C$ are hyper-parameters that control the weight of the user, query, and slot-value pairs, respectively. Note that $\\lambda_u$ can be 0, which means the model will be degraded to a non-personalized search model. $\\lambda_C$ can also be 0, which means the model will be degraded to a non-conversational search model."}, {"title": "3.3.2 Item and User Language Model", "content": "Inspired by Ai et al. [3], we learn the representation of items and users by constructing item/user language models. While most of the latent vector space models construct item/user language models with textual description (i.e., words) only [3, 9], we construct item/user language models from two perspectives, including both textual descriptions and clarification features (i.e., slot-value pairs), to learn a better representation of items and users.\n\u2022 When considering textual descriptions, given the item embedding v ($v \u2208 R^L$) and the embedding of a word w ($w \u2208 R^L$), the probability of w being generated from the item language model is defined with a softmax function on item embedding v and word embedding w:\n$P(w|v) = \\frac{exp(w\u00b7v)}{\\sum_{w'\u2208D_W} exp(w'\u00b7 v)}$,\nwhere $D_W$ is the set of the vocabulary of words in the textual descriptions from the corpus. Similarly, we have a user language model:\n$P(w|u) = \\frac{exp(w\u00b7u)}{\\sum_{w'\u2208D_W} exp(w'\u00b7 u)}$"}, {"title": "3.3.3 Slot-value Representation", "content": "We utilize the informative slot-value pairs (q, a) to represent the conversation, where q represents the slots and a represents the values. In this work, we consider both positive feedback and negative feedback from users. We define the representation of slot-value pairs $C_{qa}$ for positive feedback as:\n$C_{qa} = \\frac{q+a}{2}$\nwhere q and a are the embedding of the slot in a question and the value in an answer, respectively. For simplicity, we define $C_{qa}$ as the average of slot and value representations. Note that the way of combining slot and value representations for $C_{qa}$ is ad hoc and follows early approaches that pivot on the additive compositionality of embeddings (see e.g., [46].); one can define $C_{qa}$ as any other functions on top of slot and value representations, e.g., adding an adaptive weight for the slot and value representations or combining them by a nonlinear projection function [3].\nBesides positive feedback, negative feedback is also a strong signal and is important to be incorporated into the model [9]. Compared with positive feedback, negative feedback is more challenging since relevant items usually have similar characteristics while the reason for an item to be non-relevant could vary. In this work, we learn a separate embedding for negative feedback. We utilize $\\bar{q}$ to represent the negative feedback on the asked slot q.  That is, we train both q and $\\bar{q}$ for each q. Then we have the embedding of a clarifying question with negative feedback:\n$C_{qa} = \\bar{q}$."}, {"title": "3.3.4 Query Representation", "content": "As there are a large number of possible queries, it is impractical to generalize offline learned query embeddings to unseen queries. That is, Q must be calculated in product search at the request time. We then divide queries into words and construct query embeddings Q by word embeddings w based on a nonlinear projection function similar to other publications [3, 63] :\n$Q = tanh(W\u00b7\\frac{\\sum_{w\u2208Q} w}{|Q|} + b)$,\nwhere $W \u2208 R^{t\u00d7t}$ and $b \u2208 R^L$ are parameter matrices and bias vectors to be learned in the training process, respectively. |Q| is the length of the query. Again, the way to combine word embeddings to form a query embedding is ad hoc; one can define query embedding as other functions, e.g.,"}, {"title": "3.3.5 Joint Learning Framework", "content": "With all the components introduced previously, we can jointly learn the embeddings of queries, users, items, and slot-value pairs by maximizing the likelihood of the observed user-query-conversation-item quadruple in the training set. Let $S_{uQv}$ be the set of slot-value pairs in the conversations for the interaction of user u and item v with query Q. Let $S_u$ be the set of associated slot-value pairs of u (i.e., slot-value pairs in the historical conversations for historical purchases of user u), $S_v$ be the set of associated slot-value pairs of v (i.e., slot-value pairs in the historical conversations for historical purchases of item v by all users), $D_v$ be the textual descriptions (i.e., item description and reviews) of v, and $D_u$ be the textual descriptions (i.e., user reviews) of u. Then, the maximization of the likelihood of observing a user-query-conversation-item quadruple with corresponding slot-value pairs and textual descriptions associated with users or items in our model can be formulated as follows:\n$L(u, Q, v, S_{uQv}, S_v, S_u, D_v, D_u) = log P(u, Q, v, S_{uQv}, S_v, S_u, D_v, D_u)$.\nIn our ConvPS model, words in $D_v$ and $D_u$ are generated by the language model of v and u, which are Equation 3 and Equation 4, respectively. $D_v$ is independent of u, Q, and $D_u$ while $D_u$ is independent of v, Q, and $D_v$. Slot-value pairs in $S_v$ and $S_u$ are also generated by the language model of v and u, which are Equation 5 and Equation 6, respectively. $S_v$ is independent of u, Q, $S_{uQv}$ and $S_u$ while $S_u$ is independent of v, Q, $S_{uQv}$ and $S_v$. For simplicity, we assume that $S_u$, $S_v$, and $S_{uQv}$ are independent each other. We also assume that $D_v$ and $D_u$ are independent from $S_{uQv}$, $S_u$, and $S_v$. Thus, we can rewrite Equation 10 as follows:\n$L(u, Q, v, S_{uQv}, S_v, S_u, D_v, D_u) \\propto \\sum_{(q,a) \u2208 S_{uQv}} log P(v|u, Q, q, a) + \\sum_{(q,a) \u2208 S_v} log P((q, a) |v) + \\sum_{(q,a) \u2208 S_u} log P((q, a) |u) + \\sum_{w\u2208 D_v} log P(w/v) + \\sum_{w\u2208 D_u} log P(w/u)$.\nTherefore, the log-likelihood is actually the sum of log-likelihood for the user language model, the item language model, and the item generation model. To better learn the representations and alleviate the potential mismatch between the query and item for a given user when there are no conversations (e.g., the initial item ranking before asking questions), we also add a non-conversational term for user-query-item triples, i.e., a non-conversational version of Equation 2, into the loss function for optimizing:\n$P(v|u, Q) = \\frac{exp (v\u00b7 (\\lambda_u u + \\lambda_Q Q))}{\\sum_{v'\u2208V} exp(v' \u00b7 (\\lambda_u u + \\lambda_Q Q))}$.\nOur ablation experiments in Section 4.4 also show better performance of involving such a non-conversational term for training. By adding it into our loss function, then we have:\n$L(u, Q, v, S_{uQv}, S_v, S_u, D_v, D_u) = log P(u, Q, v, S_{uQv}, S_v, S_u, D_v, D_u)$\n$\\propto log P(v|u, Q) + \\sum_{} \\sum_{(q,a) \u2208 S_{uQv}} log P(v|u, Q, q, a) + \\sum_{(q,a) \u2208 S_v} log P((q, a) |v) + \\sum_{(q,a) \u2208 S_u} log P((q, a) |u) + \\sum_{w\u2208 D_v} log P(w/v) + \\sum_{w\u2208 D_u} log P(w/u)$"}, {"title": "3.4 Learning to Ask", "content": "In this section, we aim to learn how to ask a sequence of high-performance questions. This means, (1) selecting questions to ask to obtain enough information so that the system can suggest the good matching item to the user, and (2) selecting the questions to ask to hit the target item with as few iterations as possible. In this work, we propose four learning to ask strategies and transfer them to produce search, including Generalized Binary Search (GBS) [48], LinRel bandit [7], and two Gaussian Process (GP) variants. The first one is a greedy strategy while the last three are explore-exploit strategies."}, {"title": "3.4.1 GBS", "content": "Inspired by the successful application in Zou and Kanoulas [86] and Zou et al. [84], we explore GBS for learning to ask questions in product search. GBS is a greedy strategy that selects the slot $q_l$ which is able to best split the estimated user preferences corresponding to the items closest to two halves:\n$q_l = arg \\underset{q}{min} |\\sum_{v\u2208V} (2\\mathbb{1}\\{q^v = 1\\} \u2212 1)\u03c0_l (v)|$,\nwhere $q_l$ is the l-th chosen question (slot), $q^v$ expresses whether the textual description of v contains the slot q or not. Specifically, if the slot q appears in the textual description of v, then ${\\mathbb{1}\\{q^v = 1\\}}$ is true and $\\mathbb{1}\\{q^v = 1\\}$ is equal to 1, otherwise ${\\mathbb{1}\\{q^v = 1\\}}$ is false and $\\mathbb{1}\\{q^v = 1\\}$ is equal to 0. $\u03c0_l(v)$ is the estimated user preferences with each dimension in terms of a ranking score of v. In particular, we set $\u03c0_l(v)$ to 1/(index + 1), where index is the index of item v in the l-th iteration ranked list of items."}, {"title": "3.4.2 LinRel", "content": "We also explore bandit algorithms to carry out explore-exploit for learning to ask questions, where we would like to gather information about question rewards through exploration while at the same time harvest currently available question rewards through exploitation. Firstly, we adopt a contextual bandit algorithm, LinRel [7, 58], in our task. LinRel has been proven to work well in other interactive retrieval systems [23, 58]. LinRel estimates a linear regression model, which makes use of side information (clarification features) to estimate the relevance score of a slot (question). It predicts expected slot relevance and corresponding upper confidence bounds (i.e., the sum of the mean and variance of the expected relevance score). To balance the exploration and exploitation, LinRel selects the slot with the highest upper confidence bounds rather than selecting the slot with the largest estimated mean of the relevance score. Intuitively, slots with high upper confidence bounds are the ones that are either already highly relevant with less uncertainty, or potentially relevant but with greater uncertainty. Assume we have a data matrix X where the slots in terms of questions are rows and items are columns. We denote $X = (x_1, . . ., x_F)^T$ as the matrix containing F row vectors presented so far, where F is the number of slots. For any slot q, $x_q (x_q \u2208 X)$ is a one-hot vector of items versus that slot q with N dimensions. Each dimension corresponds to a value of 0 or 1 (when a slot is contained in the corresponding item, the value is 1, otherwise 0.). The algorithm first computes a regression weight vector for the weight of exploitation (i.e., collected question rewards):\n$h_q = x_q \u00b7 (XX^T + \u03bb_l I)^{-1}X^T$,\nwhere I is the identity matrix, and $\u03bb_l$ is a regularization parameter.\nSuppose up to iteration l we have collected l-1 relevance feedback where each feedback $r_1, . . ., r_{l\u22121} \u2208 \\{-1, 1\\}$, is a relevance value for a particular slot, and the vector of all feedback values received so far is denoted $r = (r_1, . . ., r_{l\u22121})^T$. In this paper, we set relevance value of positive feedback to 1 and relevance value of negative feedback to -1. Then we select the slot q that achieve maximum value to ask to the user:\n$q_l = arg \\underset{q}{max} \\{h_q \u00b7 r + c \\|h_q\\|\\}$,\nwhere $h_q$ is the regression weight vector for the slot q whose relevance score we are predicting, $\\|h_q\\|$ is the L2 norm of the regression weight vector, and the constant c is used to adjust the explore-exploit trade-off. Intuitively, the first term on the right side of Equation 18 models exploitation and the second term models exploration."}, {"title": "3.4.3 Gaussian Process", "content": "Learning to ask questions can be regarded as making sequential decisions (i.e., asking questions) to optimize an unknown payoff function, which can be typically modeled as a Gaussian Process (GP). In particular, each decision (asked question) results in a stochastic reward with an initially unknown distribution, while new decisions are taken on the basis of the observations of previous question rewards. In this section, we explore learning to ask strategies modeled by GP. The key idea is to use GP models for the received feedback of asked slots, and to obtain slot estimates via maximum likelihood. In GP, we first deduce the distribution of the relevance of slots and then select the optimal slot to ask based on the relevance score of slots. That is, we predict the relevance score of as yet unexplored slots using GP models, whose regularity is captured in the kernel (covariance) function. Exploiting such regularity allows us to efficiently maximize the benefit drawn from sparse observations about the asked slots.\nConsider now we have already observed relevance feedback for t slots, i.e., obtained data $\\{(q_1, y_1), . . ., (q_t, y_t)\\}$, where $y_t$ is the observed relevance of $q_t$. In this work, $y_t \u2208 \\{-1,1\\}$, i.e., it is set to 1 for positive feedback received and -1 for negative feedback received. We assume the relevance of slot $y_t$ conforms to a GP. We also assume $y_t$ is noisy and is composed of a \u201ctrue\u201d relevance $R(q_t)$ plus independent Gaussian noise $\u03b5_t$, with $y_t = R(q_t) + \u03b5_t$, where $\u03b5_t$ is a predefined hyper-parameter. The GP assumption for the observed feedback $(y_1, . . ., y_t)$ implies that the true feedback for all slots are jointly Gaussian distributed, with zero mean. Then, for a new slot, its predictive distribution for the relevance R(q) is Gaussian, with mean and variance given by\n$\u03bc_t(q) = k_t (x_q)^T (K_t + I)^{-1}y_t$,\n$\u03c3_t^2 (q) = \u03ba(x_q, x_q) \u2212 k_t (x_q)^T (K_t + I)^{-1}k_t (x_q)$,\nwhere $k_t (x_q) = [\u03ba(x_q, x_{q_1}),..., \u03ba(x_q, x_{q_t})", "x_{q_j})": "and $\u03ba(x_{q_i}, x_{q_j})$ is a particular kernel function for two embeddings $x"}]}