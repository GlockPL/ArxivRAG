{"title": "Map Imagination Like Blind Humans: Group Diffusion Model for Robotic Map Generation", "authors": ["Qijin Song", "Weibang Bai"], "abstract": "Can robots imagine or generate maps like humans do, especially when only limited information can be perceived like blind people? To address this challenging task, we propose a novel group diffusion model (GDM) based architecture for robots to generate point cloud maps with very limited input information. Inspired from the blind humans' natural capability of imagining or generating mental maps, the proposed method can generate maps without visual perception data or depth data. With additional limited super-sparse spatial positioning data, like the extra contact-based positioning information the blind individuals can obtain, the map generation quality can be improved even more. Experiments on public datasets are conducted, and the results indicate that our method can generate reasonable maps solely based on path data, and produce even more refined maps upon incorporating exiguous LiDAR data. Compared to conventional mapping approaches, our novel method significantly mitigates sensor dependency, enabling the robots to imagine and generate elementary maps without heavy onboard sensory devices.", "sections": [{"title": "I. INTRODUCTION", "content": "Mapping is fundemental for navigation, planning, and efficient decision-making across all environments, as it helps to understand spatial relationships [1], [2]. Humans naturally tend to reconstruct the map of our surroundings to guide real-time movements, even with limited local observations. For blind individuals, the amount of observed information is significantly reduced due to the lack of visual perception. However, they can still form mental maps to guide movement and decision-making, relying heavily on virtual or mental odometry from memory, or imagination, as well as real or physical contact feedback with the environment through their body or walking sticks.\nIn general, LiDAR scanning and visual sensing are widely used for robotic perception and mapping. Traditional ap- proaches usually focus on feature matching to integrate multiple frames of data into a map [1], [3], [4]. However, global map prediction or generating is still challenging especially when only limited information can be obtained [5]. It is, therefore, becoming increasingly important and popu- lar in robotic mapping as well as in the rapidly growing autonomous driving industry.\nTo generate maps, some approaches focus on using vehicle tracking data or pedestrian trajectories combined with neigh- borhood building footprints [6], [7]. By fusing historical information from multiple vehicles or pedestrians, a 2D route map can be generated, but producing a detailed 3D point cloud map remains challenging. Therefore, some approaches also try to create a 3D point cloud representation, either by synthesizing it from random noise [8] or by utilizing scanned LiDAR data points [5], [9]. The process of generating such point clouds is not easy. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are utilized to tackle this task [8], [10], but these models exhibit limited capabilities in generating large-scale maps.\nRecently, diffusion models, such as DDPM [11], have been popularly used for image generation in the field of computer vision. Furthermore, DDIM [12] proposed a new sampling method to speed up the generation process. LDM [13] encodes images into a latent space and combines with multi- modal information to generate higher quality image data. In the meantime, diffusion models are also used to create LiDAR and large-scale scene point clouds. They are catego- rized into two main technical approaches: One converts point clouds to range images, and use vision techniques to generate range image and then converte them back to point clouds, suitable for sensor-collected point clouds like LiDAR [14], [15]. The other uses pointnet or 3D convolutions, suitable for scene-level point cloud generation [16]. These methods can efficiently generate 3D point cloud maps, but they need LiDAR scans or images as conditional input.\nInspired by the blind humans' ability to imagine and generate the global map from exiguous information, and amazed by the great potential of diffusion models' data generation ability, we are proposing a novel method enabling robots to predict large-scale maps based on very limited sensory singals such as only with the path data. We aim to generate a broader range of point cloud maps utilizing solely path data or with minimal extra point cloud information. The main contributions can be summarized as follows:\n\u2022 We propose a novel Group Diffusion Model (GDM) for large-scale map generation. GDM is a point-wise method, thus we firstly segment the large-scale point cloud into multiple groups and then apply the diffusion process and the denoising process separately to these group points.\n\u2022 We propose a two-step method for generating 3D point cloud maps. Stage 1 creates central points from path data and add noise to them. Stage 2 focuses on denois- ing the noisy map from stage 1."}, {"title": "II. RELATED WORK", "content": "Map generation aims to create a 3D point cloud map from random noise or scanned LiDAR data points [8], [9]. Previous work proposed many ways to generate unknown map, some of which leverage the geometric information of known maps to generate unknown maps [2], [17], while others employ neural network models, utilizing pre-training methods for generation [18]\u2013[21]. A similar way to generate unseen parts of a map is called scene completion, which tries to complete missing 3D details of a scene based on incomplete sensor data [5].\nDenoising diffusion probabilistic models have become popular because they generate high quality images and videos [11], [22]\u2013[25]. One significant advantage of diffu- sion models is their ability to produce high-fidelity outputs. Compared to other generative models like GANs (Generative Adversarial Networks) [26], diffusion models tend to gener- ate samples with fewer artifacts and more intricate details, making them particularly suitable for tasks that require precise control over the generated content. Due to the time- consuming drawback of diffusion models, numerous studies have focused on improving their efficiency, reducing their computational requirements, and extending their capabilities to new domains. For example, some studies have proposed techniques to accelerate the sampling process, allowing diffu- sion models to generate images and videos faster [12], [27]. Others have explored ways to condition the diffusion process on specific inputs, enabling the models to generate samples that meet specific criteria or follow certain styles [28], [29].\nDiffusion model for LiDAR is difficult since its large scale. Some works transfer LiDAR scan to range image and employ image generation method to generate LiDAR scan [14], [30]. This method is not suitable for large scale map generation, since the map can not be transfered to range image. Some works propose point cloud diffusion to generate object point cloud [31], [32]. For large scale map generation, the work [33] employs scale-varied diffusion models to generate high-quality outdoor scenes. This work achieves the generation of large-scale maps by combining the generated small maps.\nDifferent from previous methods, our approach aims to generate large scale point cloud maps without using vision or LiDAR based perception, eliminating the necessity for onboard LiDAR or visual sensors."}, {"title": "III. METHODOLOGY", "content": "Firstly, inspired from the natural abilities of blind and deaf humans, robots should also be equiped with similar map generation intelligence utilizing limited sensory systems onboard. Without regard to the voice peception, the blind individuals are mainly relying on their path memories and some random contact feedback with the environment using hands and walking sticks. In fact, the path memories can achieve mental odometry, and the interactive environmental contact provides distance or relative positioning information.\nWhereas, with necessary simple inputs, the basic odom- etry information of robots can be easily obtained through encoders, GPS, IMU, etc., the random but super-sparse positioning information can be acquired by cheap ultrasonic sensors. To test our methods on open datasets, we can gather the small amount of super-sparse positioning data by directly sampling the LiDAR scanning.\nIn this regard, we propose a group diffusion model for generating large-scale point cloud maps. As illustrated in Fig.1, this proposed approach includes two main stages. Stage 1 aims to generate central points from the given path, and stage 2 employs method defined by Sec. III-B to generate large-scale map."}, {"title": "A. Denoising Diffusion Probabilistic Models", "content": "Denoising Diffusion Probabilistic Models (DDPM) [11], [12] represent a generative modeling approach rooted in diffusion processes. These models generate data samples by progressively adding Gaussian noise to the data and learning to reverse this process. The methodology encompasses two primary phases: the diffusion process and the denoising process. During the diffusion process, noise is incrementally introduced to the data across $T$ steps, with the weighting of this noise typically determined by time-step-dependent beta coefficients that escalate with the progression of $t$. By the $T$-th step, the data is transformed into pure Gaussian noise. Conversely, the denoising process reverses this trajectory, gradually removing noise from the $T$-th step data to recover the original data. Typically, a neural network model is employed to predict the noise added at each step $t$, and by subtracting this noise, the data from the previous step is reconstructed. This iterative denoising process progressively approximates the original data.\nThe diffusion process constitutes a fixed Markov chain [11], systematically transforming the original data $x_0$ into noise-laden data $x_t$. Given the noise factors $\u03b2_t$, with $t = 0, 1, ..., T$. Let $\u03b1_t = 1 - \u03b2_t$, each timestep $t$ of this transfor- mation can be expressed as:\n$x_t = \\sqrt{\u03b1_t}x_0 + \\sqrt{1 \u2013 \u03b1_t}\u03f5$   (1)\nwith $\\bar{\u03b1}_t = \u03a0_{i=1}^t \u03b1_i$, and $\u03f5$ is a Gaussian noise with mean 0 and the identity matrix $I$ as diagonal covariance.\nThe denoising process systematically eliminates noise at the $T$-th step, aiming to restore the original data. As per the definitions outlined in [11], this denoising process can be formulated as:\n$x_{t-1} = \\frac{1}{\\sqrt{\u03b1_t}} \\Big( x_t - \\frac{\u03b2_t}{\\sqrt{1-\\bar{\u03b1}_t}} \u03f5_\u03b8(x_t, t) \\Big) + \\sqrt{\u03b2_t}z$   (2)\nwhere $z$ is the Gaussian noise, and $\u03f5_\u03b8(x_t, t)$ is the noise predicted from $x_t$ at timestep $t$. Finding a formula to predict noise is difficult, thus Ho et al. [11] define $\u03f5_\u03b8(x_t, t)$ as a neural network model.\nThe loss function is employed to optimize the neural network model $\u03f5_\u03b8(x_t, t)$. In one training step, given a random $t$ from 0 to $T$ and origin data $x_0$, sample a Gaussian noise $\u03f5$ and calculate the $t$-th step noisy data $x_t$ using Eq.(1). Then, input $x_t$ and $t$ into the model to predict noise $\u03f5_\u03b8$. The loss function $L(\u03f5, t)$ can be formulated as:\n$L(\u03f5, t) = ||\u03f5 \u2013 \u03f5_\u03b8(x_t, t)||^2$   (3)"}, {"title": "B. Group Diffusion Model For LiDAR Map", "content": "The scale of LiDAR point cloud maps is often substantial, characterized by a vast range along the $x$-$y$ axes and a comparatively narrow range along the $z$-axis, leading to a significant deviation in the distribution of point cloud data from the standard normal distribution. To address this issue, literature [5] introduced a local diffusion model, which ap- plies diffusion to individual points, proving effective within a single LiDAR frame but insufficient for larger-scale point cloud maps. Consequently, we propose a group diffusion model, where in the extensive point cloud is partitioned into numerous groups, and diffusion is individually applied to the point clouds within each group.\nIn Fig. 2 the group noisy map does not represent the actual map we generate. To facilitate observation, we segregated each group by a certain distance. As observable, the group noisy map comprises multiple clusters of point clouds. By individually denoising each cluster, we obtain the group map, and integrating all these groups results in the generated map.\nThe diffusion process is different from DDPM [11]. Given a ground truth of map $P$, and divide it into $m$ groups to get $p^i$ with $i = 0, 1, ..., m$, that is, $P = {p^0, p^1, ..., p^m}$. The distribution of a certain group is close to mean 0 and the identity matrix $I$ as diagonal covariance. Each group has a central point $C_i$:\n$C_i = \\frac{1}{|p^i|} \\sum_{x \\in p^i} x$   (4)\nwhere $|p^i|$ is the quantities of points in group $p^i$. The central point $C_i$ is employed to get the $i$ th normalized group $g_i = p^i \u2013 C_i$. Given a origin normalized group $g_0$, the diffusion process adds noise to $g_0$ over $T$ steps, resulting in $g_1, g_2, ..., g_T$. From Eq. (1), the $t$-th step diffusion porocess can be rewritten as:\n$g_t^i = \\sqrt{\u03b1_t}g_0 + \\sqrt{1 \u2013 \\bar{\u03b1}_t}\u03f5, (i = 0, 1, \u2026, m; t \u2208 [0, T])$   (5)\nTransform $g_t^i$ into the coordinate system of $C_i$ to get the noisy group:\n$p_t^i = C^i + g_t^i$  (6)\n$= C^i + \\sqrt{\u03b1_t}g_0 + \\sqrt{1 \u2013 \\bar{\u03b1}_t}\u03f5$  (7)\n$= C^i + \\sqrt{\u03b1_t}(p^i \u2013 C^i) + \\sqrt{1 \u2013 \\bar{\u03b1}_t}\u03f5$  (8)\nThe complete noisy map can be represented as:\n$P_t = {p_t^0, p_t^1,\u2026, p_t^m}$  (9)\nThe denoising process involves the removal of noise from $p_t^i$ at the $t$-th step, resulting in data prior to the introduction of noise. Group diffusion necessitates denoising within indi- vidual groups. From Eq. (2), the deboising process can be rewritten as:\n$g_{t-1}^i = \\frac{1}{\\sqrt{\u03b1_t}} \\Big( g_t^i - \\frac{\u03b2_t}{\\sqrt{1-\\bar{\u03b1}_t}} \u03f5_\u03b8(g_t^i, t) \\Big) + \\sqrt{\u03b2_t}z$   (10)\nTransform $g_{t-1}^i$ into the coordinate system of $c^i$ to get the group:\n$p_{t-1}^i = C^i + g_{t-1}^i$   (11)\nGiven a noisy map $P_t$, the denoising process is aim to remove noise at step $t$ and get the $P_{t-1}$:\n$P_{t-1} = {p_{t-1}^0, p_{t-1}^1, ..., p_{t-1}^m}$  (12)\nThe loss function is employed to optimize the neural network model $\u03f5_\u03b8(g_i^t, t)$. In one training step, given a random $t$ from 0 to $T$ and origin data $P_0$, sample a Gaussian noise $\u03f5$ and calculate the $t$-th step noisy map $P_t$ using Eq.(5-8). Then, input $P_t$ and $t$ into the model to predict noise $\u03f5_\u03b8$. The mean square error loss function $L_{mse}(P_t, t)$ can be formulated as:\n$L_{mse}(P_t, t) = \\frac{1}{N} \\sum_{i=0}^m ||\u03f5 \u2013 \u03f5_\u03b8(P_t^i, t)||^2$  (13)\nThe noise that we incorporate adheres to a standard normal distribution. Consequently, to accelerate training, we incor- porate the regularization losses as mentioned in [5], resulting in the formulation of a comprehensive loss function. Given the mean $E(\u03f5_\u03b8)$ and the standard deviation $D(\u03f5_\u03b8)$. Comput- ing $L_{mean} = (E(\u03f5_\u03b8) \u2013 0)^2$ and $L_{std} = (D(\u03f5_\u03b8) \u2212 1)^2$. The loss can be formulated as:\n$L = L_{mse} + \u03b3(L_{mean} + L_{std})$   (14)\nwhere $\u03b3$ is a weighting factor, we set the default value of $\u03b3$ to 5.\nNetwork Structure show in the right part of Fig. 1. We employ the sparse unet [34] as our backbone. In our network, the input includes noisy map $P_t$, group center coordinates $C$ and timesteps $t$, and the output is predicted noise $\u03f5_\u03b8$."}, {"title": "C. Two-Stage Map Prediction", "content": "Even with diffusion models, it remains challenging to generate large-scale map from path points, as we still require the $P_T$ and $C$ defined by Sec. III-B. The $P_T$ can be define as $P_T = C + \u03f5$ where $\u03f5$ is Guassian noise. To get $C$, we propose two-stage map prediction approach. Stage 1 aims to generate central points from path, and stage 2 employs method defined by Sec. III-B to predict large-scale map, as illustrated in Fig. 1.\nStage 1 aims to predict central points $C$. Our method is shown in the fist half of Fig. 1. We create central points along the path, with a fixed width $w$. Given the path points $O = {o_1, o_2,\u2026, o_i}$ where $o_i \u2208 R^3$, we estimate normal vector $N = {n_1, n_2,\u2026, n_i}$ where $n_i \u2208 R^3$ for every point $o_i$, and generate one point per meter along the normal vector, up to a distance of $w$ meters. An alternative method is to generate central points with various widths using limited information, e.g., we can estimat the width through limited LiDAR data, as pictured in Fig. 3. To estimate width $w$, we initially calculate the distance $d$ from the tangent at point on the path to the nearest neighbor LiDAR point, and then let $w = d$ as the estimated width.\nStage 2 is the denoising process in Sec. III-B. Given a noisy map created by stage 1, the denoising process will eliminate noise at $T$ steps."}, {"title": "IV. EXPERIMENTS AND DISCUSSION", "content": "Experimental datasets. We train our model in KITTI- 360 [35] Datasets, which include sequences 00 to 10. Our method requires only a limited amount of data. Let the 00 sequence as traning data, and others as the test data. To process the data from KITTI-360 [35], we leverage the provided LiDAR and pose data to synthesize a block map every 150 meters. Subsequently, 50,000 points are sampled using FPS (Farthest Point Sampling) [36], which serve as our ground truth for subsequent experiments. We processed the entire sequence 00, resulting in 604 block maps.\nTraining. We train our model on a NVIDIA RTX 3060 Ti GPU equipped with 16GB of memory, with a batch size of 1 and 200 epochs. The entire training duration was 24 hours, during which the GPU memory consumption peaked at 6GB. For inference, the GPU memory utilization approximated 3GB."}, {"title": "A. Map generation with path", "content": "In this experiment, we create central points $C$ from the path data $O$. We process the poses file of KITTI-360 [35] to get path $O = {o_1, o_2,\u00b7\u00b7\u00b7, o_i}$ where $o_i \u2208 R^3$ is the point in path $O$. We employ the Stage 1 method proposed in Sec. III-C to generate central point $C$, and set the width as a fixed value $w = 20$. To analyze the generative effects as the width $w$ varies, we devised Mode 2, which involves inputting path data and random width $w \u2208 [15, 35]$. In Fig. 4, we compare three modes in the dataset sequence I, whose path length is 6.4km and height range is [0, 78m]. The results indicate that, when utilizing path exclusively, our method generates maps with a fixed width in Mode 1. In Mode 2, incorporating a random width results in the generation of increased details along the path."}, {"title": "B. Map generation with path and limited point cloud", "content": "In this experiment, we test the proposed Mode 3, which creates central points $C$ with the path data $O$ and limited point clouds sampled from the oringin map. To get the limited spatial positioning data like blind individuals, we here randomly sample only 50 points from the original map data, which has about 500,000 points obtained from normal LiDAR scan. The method defined in Sec. III-C is employed to estimate the width $w$ through the given limited LiDAR data."}, {"title": "C. Map generation through any shape", "content": "In this experiment, We manually constructed noisy maps of various shapes and generated the maps through the de- noising process. The result is shown in Fig. 6. Given a noisy map with any shapes, we employ the denoising process to get a map without noise. To create a noisy map, we initially establish its approximate outline, subsequently fill the map with a point cloud at 1-meter intervals, and add Gaussian noise to the point cloud. We manually created four noisy map types: a straight line 200m long and 20m wide, a curved version of this with bends, a circular ring 120m in diameter, and a square map 200\u00d7200m. This experiment shows our method's ability to generate large maps of different shapes."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel method to generate large- scale point cloud map from only path data. To address the challenge of generating task, we propose the Group Diffusion method. The proposed map generation method includes two stages: In stage 1, we create central points from the path and add noise to them to get a noisy map. In stage 2, we employ a denoising process to generate a refined map. Experiments on public datasets showed that our method can generate reason- able maps using only path and refined maps with exiguous sampled LiDAR points. Three map generating modes are desgined and tested. When comparing Mode 3 with Mode 1, we can conclude that, with the limited additional positioning information, the map generation metrics have improved by more than 20%. Compared to traditional approaches, our novel method reduces sensor dependency, enabling robots to create basic maps with minimal infomation, akin to blind humans relying on path memeory based mental odometry. Thus, robots acquire basic mapping abilities solely with odometry, reducing the need for LiDAR or vision sensors."}]}