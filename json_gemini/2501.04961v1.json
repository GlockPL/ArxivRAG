{"title": "Demystifying Domain-adaptive Post-training\nfor Financial LLMs", "authors": ["Zixuan Ke", "Yifei Ming", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "Domain-adaptive post-training of large language models (LLMs) has emerged\nas a promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation criteria\nand training strategies across varying data and model configurations. To address\nthese challenges, we introduce FINDAP, a systematic and fine-grained investi-\ngation into domain-adaptive post-training of LLMs for the finance domain. Our\napproach begins by identifying the core capabilities required for the target domain\nand designing a comprehensive evaluation suite aligned with these needs. We\nthen analyze the effectiveness of key post-training stages, including continual pre-\ntraining, instruction tuning, and preference alignment. Building on these insights,\nwe propose an effective training recipe centered on a novel preference data distilla-\ntion method, which leverages process signals from a generative reward model. The\nresulting model, Llama-Fin, achieves state-of-the-art performance across a wide\nrange of financial tasks. Our analysis also highlights how each post-training stage\ncontributes to distinct capabilities, uncovering specific challenges and effective\nsolutions, providing valuable insights for domain adaptation of LLMs.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) demonstrate strong generalization across diverse tasks, they\noften struggle in specialized domains such as finance, law, and healthcare. Consequently, domain-\nadaptive post-training of LLMs has garnered significant attention recently (Colombo et al., 2024b;\nXie et al., 2024a). In the earlier days of language models, continual pre-training or CPT was the\ndominant strategy. This involved further training a pre-trained model on domain-specific plain\ntext and then fine-tuning it for individual tasks (Gururangan et al., 2020; Ke et al., 2023). While\nprompt engineering with zero- or few-shot examples has emerged as a convenient way to adapt\ngeneral-purpose LLMs to new tasks, recent methods focus on fine-tuning model weights to transform\nthem into domain experts (Colombo et al., 2024a; Chen et al., 2023a; Li et al., 2023).\nToday, domain-adaptive post-training has evolved into a collection of techniques including CPT,\ninstruction-tunning (IT) and preference alignment (PA). Building on this trend, this work focuses on\nadapting LLMs to specific domains through parameter training, which complements semi-parametric\nmethods that leverage external knowledge, such as in retrieval-augmented generation or RAG (Lewis\net al., 2020; Ke et al., 2024). Our focus is also different from general post-training, as the goal is"}, {"title": "2 FINDAP Setup", "content": "In FINDAP, we first identify the desired capabilities and design the corresponding evaluation mea-\nsures, and then formulate the training method."}, {"title": "2.1 FINDAP Capabilities", "content": "We begin by illustrating the capabilities that are desirable for a domain-specific LLM. Specifically,\nwe focus on the following core capabilities:\n\u2022 Domain specific concepts. A domain typically includes its own specific concepts. For example,\n'bond' in finance refers to a loan agreement between an investor and a borrower. Adapting the LLM\nto domain-specific concepts is crucial, as these concepts form the fundamental building blocks of\ndomain knowledge. However, this adaptation should not come at the cost of losing knowledge about\ngeneral concepts, which are essential for both domain-specific and general tasks.\n\u2022 Domain specific tasks. While many NLP tasks, such as NER or sentiment analysis, are shared\nacross different domains, a domain typically has its own tasks. For example, stock movement\ndetection is primarily found in finance. Adapting LLMs to these domain-specific tasks is important,\nas it demonstrates how they can leverage domain-specific concepts to solve tailored tasks effectively.\n\u2022 Reasoning. For complex tasks, reasoning with concepts is a highly desired capability in LLMs.\nFor example, in finance, the LLM is often required to analyze a company's financial report, involving\nextensive reasoning, particularly mathematical reasoning, to compute key financial concepts such as\nmarket rate or earnings per share.\n\u2022 Instruction-Following (IF) and chat. This is a core capability for both general and domain-specific\nLLMs, as tasks are often presented in the form of instruction following or conversation.\n\u2022 Others. Additionally, domains may vary significantly in their sensitivity. For instance, the medical\ndomain is highly sensitive, requiring utmost accuracy and strict adherence to ethical considerations.\nIn contrast, domains such as entertainment may have more relaxed requirements. Another important\nconsideration is multi-modality, as some domains require handling multiple types of input and output\nformats. For example, the healthcare domain may involve processing medical images alongside\ntextual reports, while the e-commerce domain may integrate product descriptions, images, and\ncustomer reviews into a unified response. Similarly, scientific research often combines charts, graphs,\nand textual analysis to present findings effectively. While we acknowledge these additional aspects,\nwe leave those for future work and concentrate on the four primary capabilities discussed above."}, {"title": "2.2 FINDAP Evaluation", "content": "With the above breakdown of capabilities, our evaluation framework consists of a suite for assessing\nthese capabilities using development sets and unseen (held-out) evaluation sets. Our development set\nis directly split from the training data at each stage."}, {"title": "2.3 Comparison with Existing Finance LLMs", "content": "Table 2 compares Llama-Fin with popular open finance-specific LLMs: FinTral (Bhatia et al., 2024),\nPIXIU (Xie et al., 2023), FinLLM (Xie et al., 2024b), and AdaptLLM (Cheng et al., 2024). Among\nthese models, only FinTral incorporates all three training stages: CPT, IT, and PA. However, unlike\nLlama-Fin, FinTral and other models lack a systematic design and fail to include carefully crafted\ncomponents, as illustrated in Figure 1. For instance, none of these models have an in-depth analysis of\ndesign choices for different post-training stages, let alone introduce a novel approach for constructing\npreference data."}, {"title": "3 Model Training", "content": "As shown in Figure 1, FINDAP's training starts with curated text and prompts (\u00a73.1), followed by\njoint Continual Pre-training and Instruction Tuning (\u00a73.4), and then Preference Alignment (\u00a73.5)."}, {"title": "3.1 Data", "content": "Text Curation. To introduce domain concepts while preserving general concepts, we curate texts for\nCPT. Table 3 summarizes the texts curation datasets. Specially, for general concepts, research has\nshown that a 'small' amount of general text (as little as 1%) can effectively mitigate the forgetting\nissue (Scialom et al., 2022). Therefore, we focus on collecting a relatively small but high-quality set\nof general-domain text. To achieve this, we use verifiable text, which is text written by humans and\npreviously used in supervised tasks in the literature. Note that this contrasts with using unverifiable\nweb text such as C4 (Raffel et al., 2020).\nFor domain concept, our goal is to collect\nboth a large volume of data and maintain\nhigh quality. Following practices from the\nliterature on training general LLMs (Lam-\nbert et al., 2024; Gunasekar et al., 2023),\nwe source financial texts from primarily\nrelevant websites and books. Details on\nthe collection process can be found in Ap-\npendix A.\nPrompt Curation. Prompts represent the\ndiverse ways users may interact with mod-\nels and serves the essential component for\nIT and PA. Table 4 summarizes the prompts\ncuration datasets. Specifically, we conduct\na broad survey and source general, finan-\ncial, instruction-following, and reasoning\ntasks from public datasets. To promote di-"}, {"title": "3.2 Continual Pre-training (CPT)", "content": "In order to expose the LLM to domain-\nspecific concepts, we first conduct\ncontinual pre-training (CPT). In CPT,\nwe feed plain text to the LLM and per-\nform next token prediction.\nFrom Text to CPT Data. A key chal-\nlenge in CPT is what kind of data we\nshould use. Given the general and\ndomain-specific texts introduced in\n\u00a73.1, we can construct three versions\nof CPT data, CPT-In contains only the\nfinancial (in-domain) text, CPT-Gen\ncontains only the general domain data,\nand CPT-Mix contains the mixture of\nthe CPT-In and CPT-Gen.\nKey Data Experiments. We conduct\nCPT on each of the three versions of\ndata. As shown in Figure 2, we ob-\nserve that while CPT-In and CPT-Gen\noutperforms in financial (Fig 2a) and\ngeneral (Fig 2b) tasks, respectively,\nCPT-Mix achieves the best overall.\nThis is expected as CPT-In can cause catastrophic forgetting on the general tasks, while incor-\nporating general domain concepts in CPT-Mix acts as 'replay' mechanism to mitigate it (Scialom\net al., 2022). We can also see that none of the CPT-trained LLMs outperform their base. This is\nunexpected because CPT invovles post-training on more specialized data, which should enhance the\nperformance. By analyzing the output, we attribute this issue to the model forgetting how to follow\ninstructions effectively after CPT. To quantify this finding, we evaluate the instruction following\nability of these models using MT-Bench. The two-turn average scores for CPT-Mix, CPT-In, and"}, {"title": "3.3 Instruction Tuning (IT)", "content": "To adapt the LLM to domain-specific and IF tasks, we conduct IT. The key different between IT and\nCPT is that IT masks out the instruction and takes as input supervised tasks.\nFrom Prompt to IT Data.\nWe introduced our prompt curation\nin \u00a73.1. We create the responses for\nIT by filtering existing responses or\ncreating new responses. For prompts\nwith existing responses, we generally\nkeep the original responses if they\nwere written by a human or a strong\nmodel, such as GPT-4. We also fil-\nter out empty responses. For prompts\nwithout responses, for example, exer-\ncises extracted from books that may\nnot have solutions provided, we gener-\nate new responses using GPT-40o. Sim-\nilar to CPT data, we construct three\nversions of IT data, IT-In, which con-\ntains only financial (in-domain) tasks,\nIT-Gen, which contains only general\ntasks, and IT-Mix, which includes a\nmixture of the IT-In and IT-Gen.\nKey Data Experiments. Similar to\nCPT, we conduct IT to each of the three versions. From Figure 3, we observe that unlike CPT,\nforgetting is significantly reduced. Specifically, all versions of IT are no longer worse than their base\nversions, indicating that the ability to follow instructions is not as severely forgotten as in CPT. This\nis further supported by the MT-Bench scores, where we obtained 7.2031, 6.2094, and 7.3219 for\nIT-Mix, IT-In and IT-Gen, repsetively, all of which are significantly better than the CPT counterparts.\nWe observe that IT-Mix is slightly bet-\nter than other data versions, suggest-\ning that mixing general tasks remains\nhelpful to mitigating forgetting of gen-\neral concepts and tasks, although the\neffect is much less pronounced com-\npared to CPT. We also see that similar\ntasks improve significantly over base\nmodel while novel tasks (including fi-\nnancial tasks and general tasks) show\nlittle change. This indicates that, in\ncontrast to CPT, domain has less im-\npact in IT, but task generalization is a\nchallenging issue.\nComparison with LoRA. Another pop-\nular approach to adapt the LLM to spe-\ncific domain is Parameter-efficient\nFine-tuning (PEFT), where the LLM\nparameters remain fixed, and only a\nsmall set of additional parameters are\ntrained. This approach naturally miti-"}, {"title": "3.4 Combining CPT and IT", "content": "A natural choice is to conduct CPT\nand IT sequentially (Lambert et al.,\n2024). On the one hand, this is flex-\nible as it allows for different settings\n(e.g., data size) in each stage. On the\nother hand, it does not help prevent\nforgetting during the CPT stage, leaving the LLM dependent on IT to 'recover' its instruction-\nfollowing capability. To make a more grounded decision, we conduct experiments on both sequential\nand joint training approaches. In joint training, an additional hyperparameter to consider is the\nmixture ratio. We find that down-sampling CPT data to match the size of IT data is the most effective\nstrategy. Results for other strategies, such as no-sampling are provided in Appendix D.\nFigure 5 illustrates the comparison\nbetween joint and sequential train-\ning. In both cases, different from IT-\nonly results shown in Figure 3, we\nsee improved performance on simi-\nlar and novel tasks. This supports\nour hypothesis that CPT can help im-\nprove the generalization of IT, as the\nconcepts are likely able to be shared\nacross different tasks. It is further\ninteresting to see that even the gen-\neral tasks are improved, indicating\nthat there could be positive transfer\nbetween CPT and IT. Comparing the\ntwo, we observe that joint training out-\nperforms sequential training across fi-\nnancial and general tasks, as well as\nsimilar and novel tasks, highlighting\nthe importance of preventing forget-\ntting of CPT and knowledge transfer\nbetween CPT and IT."}, {"title": "3.5 Improving Reasoning with Preference Learning", "content": "In \u00a73.2 & \u00a73.3, we observed that with CPT and IT, the model improves in capabilities such as\nconcepts, tasks and IF/Chat. However, such improvements were missing on the reasoning tasks.\nPreference Alignment (PA), where the model is trained to assign higher probability mass to better\ngenerations, has been shown to be effective in enhancing reasoning capabilities of LLMs (Pang et al.,\n2024; Lambert et al., 2024; Jiao et al., 2024; Wang et al., 2024). Specifically, we employ Direct\nPreference Optimization (DPO) (Rafailov et al., 2023), which directly learns from positive (chosen)\nand negative (rejected) preference data, yielding an easier way for PA compared to reinforcement\nlearning. We synthetically generate such data from the on-policy model, i.e., the jointly trained\nCPT+IT checkpoint, as it has shown the strongest performance in previous experiments.\nNegligible Forgetting in PA. As with CPT and\nIT, we begin by performing an ablation study\non different data versions to evaluate their ef-\nfectiveness. Since the degree of forgetting di-\nminishes from CPT to IT (as observed in \u00a73.3),\nwe expect it to be even less pronounced in PA.\nTo quickly evaluate this hypothesis, we take a\nnaive approach and create PA-Mix and PA-In by\nusing either the provided or GPT40 generated\nresponses (as done for IT in \u00a73.3) as the 'chosen'\nsamples and the output of 'CPT+IT' checkpoint\nas the 'rejected' ones, based on the prompts of\nIT-Mix and IT-In, respectively.\nFigure 7 shows the results after PA training for\nPA-In and PA-Mix from the \u2018CPT+IT' check-\npoint.3 We observe that PA-In performs compa-\nrably to PA-Mix, indicating that it may not be"}, {"title": "4 Final Recipe and Evaluation", "content": "In this section, we present our final recipe based on the desired capabilities in Section 2.1, and data\nand model ablations performed in Section 3. A summary of the final recipe is given in Table 5.\nFor evaluation, we compare with a wide range of baselines models, including Llama-Fin against\na wide range of baseline models, including its base model, Llama3-8B-instruct, and the 8B peer,\nLlama3.1-8B-instruct. We also include comparisons with models of other sizes, such as Phi-3.5-mini-\ninstruct (Abdin et al., 2024) (3.8B), and Mistral-Nemo-instruct (Jiang et al., 2023) (12B), as well as\nthe closed model GPT-40 (OpenAI, 2023). Additionally, we evaluate against the latest open finance-"}, {"title": "6 Limitations", "content": "While the recipe for FINDAP and Llama-Fin are effective, the performance on novel unseen tasks\nstill requires further improvement. For example, selectively employing reasoning capabilities only for\nquestions that require such advanced reasoning might give better results. Additionally, the data recipe\nis currently based on full-scale empirical experiments, which can be time-intensive. Developing\nlow-cost experiments to reliably indicate the effectiveness of data in post-training could streamline\nthis process and accelerate the development iteration. It is also worth noting that the same recipe may\nnot generalize well to other model families. Different architectures or pretraining strategies might\nrequire tailored recipe to achieve optimal results, emphasizing the need for adaptability in recipe\ndesign in future research."}]}