{"title": "LensNet: Enhancing Real-time Microlensing Event Discovery with Recurrent Neural Networks in the Korea Microlensing Telescope Network", "authors": ["JAVIER VIA\u00d1A", "KYU-HA HWANG", "ZO\u00cb DE BEURS", "JENNIFER C. YEED", "ANDREW VANDERBURG", "MICHAEL D. ALBROW", "SUN-JU CHUNG", "ANDREW GOULD", "CHEONGHO HAND", "YOUN KIL JUNG", "YOON-HYUN RYU", "IN-GU SHIN", "YOSSI SHVARTZVALD", "HONGJING YANG", "WEICHENG ZANG", "SANG-MOK CHA", "DONG-JIN KIM", "SEUNG-LEE KIM", "CHUNG-UK LEE", "DONG-JOO LEE", "YONGSEOK LEE", "BYEONG-GON PARK", "RICHARD W. POGGE"], "abstract": "Traditional microlensing event vetting methods require highly trained human experts, and the process\nis both complex and time-consuming. This reliance on manual inspection often leads to inefficiencies\nand constrains the ability to scale for widespread exoplanet detection, ultimately hindering discovery\nrates. To address the limits of traditional microlensing event vetting, we have developed LensNet, a\nmachine learning pipeline specifically designed to distinguish legitimate microlensing events from false\npositives caused by instrumental artifacts, such as pixel bleed trails and diffraction spikes. Our system\noperates in conjunction with a preliminary algorithm that detects increasing trends in flux. These\nflagged instances are then passed to LensNet for further classification, allowing for timely alerts and\nfollow-up observations. Tailored for the multi-observatory setup of the Korea Microlensing Telescope\nNetwork (KMTNet) and trained on a rich dataset of manually classified events, LensNet is optimized\nfor early detection and warning of microlensing occurrences, enabling astronomers to organize follow-up\nobservations promptly. The internal model of the pipeline employs a multi-branch Recurrent Neural\nNetwork (RNN) architecture that evaluates time-series flux data with contextual information, including\nsky background, the full width at half maximum of the target star, flux errors, PSF quality flags, and\nair mass for each observation. We demonstrate a classification accuracy above 87.5%, and anticipate\nfurther improvements as we expand our training set and continue to refine the algorithm.", "sections": [{"title": "1. INTRODUCTION", "content": "Microlensing offers a powerful and distinctive ap-\nproach to exoplanet detection by leveraging a planet's\ngravitational perturbation of light from distant sources.\nUnlike other methods, microlensing excels at uncovering\nexoplanets in intermediate orbits (1-5 AU) around stars\nthat are often out of reach for radial velocity and tran-\nsit techniques. Its sensitivity to planets in a wider range\nof orbits, including those in higher-inclination systems,\nmakes it an invaluable tool for expanding our under-\nstanding of planetary systems. This method serves as a\ncrucial complement to other techniques, filling gaps in\nplanetary discovery by revealing worlds that would oth-\nerwise remain hidden from view. This unique capability\nsignificantly enhances the breadth of exoplanet explo-\nration, enabling the detection of a broader diversity of\nplanetary systems. Fig. 1 provides an illustration of the\nmicrolensing phenomenon, highlighting how the gravi-\ntational field of a lens star and its exoplanet distort the\nlight from a background star, leading to a characteristic\nPaczy\u0144ski light curve (Paczynski 1986).\nThe field of microlensing has evolved significantly\nsince the 1990s, when surveys of the Galactic bulge\ntypically found a few dozen gravitational microlensing\nevents per year (Alcock et al. 1996; Udalski et al. 1994a).\nFirst, as the focus shifted to exoplanets, a two-tiered\nstrategy (Gould & Loeb 1992) developed to combine\nreal-time event detection in wide-field, low-cadence (few\nobservations per day or week) surveys (Udalski et al.\n1994b; Bond et al. 2001) with targeted follow-up (mul-\ntiple observations per hour) to find and characterize ex-\noplanet signals (e.g., Bond et al. 2004; Udalski et al.\n2005; Beaulieu et al. 2006). Then, as wide-format cam-\neras continued to grow in size, it became possible to\nachieve a high enough cadence to routinely detect plan-\nets in survey data, without follow-up observations (e.g.,\nShvartzvald et al. 2016).\nThe Korea Microlensing Telescope Network (KMT-\nNet; Kim et al. 2016) provides valuable high-sensitivity\ncapabilities to advance ground-based microlensing ob-\nservations. KMTNet consists of three 1.6-meter tele-\nscopes located at strategic sites across the globe-Cerro\nTololo Inter-American Observatory (CTIO) in Chile,\nSouth African Astronomical Observatory (SAAO) in\nSouth Africa, and Siding Spring Observatory (SSO) in\nAustralia. This global distribution enables nearly con-\ntinuous coverage of the Galactic bulge, allowing KMT-\nNet to capture microlensing events as they occur in real\ntime. Each telescope is equipped with a large field-of-\nview camera that covers 4 square degrees of the sky.\nIn total, the survey monitors approximately 500 million\nstars nightly, ensuring comprehensive and efficient de-\ntection of microlensing events.\nKMTNet operates with a tiered cadence strategy that\noptimizes its near-continuous coverage of the Galactic\nbulge. Six key fields (see Fig. 3, which is a reproduction\nof Kim et al. 2018a) are monitored at a high cadence\nof 0.5 hours, with overlapping field pairs (01/41, 02/42,\n03/43) receiving four observations per hour. A subset of\nstars in the overlap between 02/42 and 03/43 benefits\nfrom an even higher cadence of eight observations per\nhour. Other fields are observed at slower rates of 1, 0.4,\nor 0.2 observations per hour, depending on their intrinsic\nevent rates. There are also other, rare cases, of overlap\nbetween neighboring fields.\nWhile these cadences are often high enough to detect\nand characterize planetary signals in events across the\nGalactic Bulge (e.g., papers in the \u201cSystematic KMT-\nNet Planetary Anomaly Search\" series, starting with\nZang et al. 2021a), there continues to be value in ad-\nditional follow-up observations of known microlensing\nevents. High-magnification events can still be targeted\nfor follow-up observations and that interest has been\nextended into the \"moderate\u201d magnification (peak mag-\nnification Apeak > 20) regime (e.g., Abe et al. 2013; Yee\net al. 2021; Zang et al. 2021b). Real-time alerts also al-\nlow for astrometric (Sahu et al. 2022; Lam et al. 2022;\nMr\u00f3z et al. 2022) or interferometric (Dong et al. 2019;\nZang et al. 2020) observations to characterize the lenses\n(which is important for black hole searches in addition\nto planets) or spectroscopic observations to characterize\nthe source stars (e.g., Bensby et al. 2013).\nFurthermore, the advent of low-cadence, all-sky sur-\nveys (e.g., ASAS-SN Shappee et al. 2014; Kochanek\net al. 2017, Zwicky Transient Facility Bellm et al. 2019;\nGraham et al. 2019; Masci et al. 2019, etc.) have created\nnew opportunities for detecting microlensing events and\npotentially following them up with high-cadence obser-\nvations to detect planets (e.g. Nucita et al. 2018). No-\ntable among these is Gaia, which has a real-time alert\nsystem for identifying microlensing events that has led\nto publication of several events with follow-up obser-\nvations (e.g., Wyrzykowski et al. 2020; Rybicki et al."}, {"title": "3. ALERT FINDING IN KMTNET DATA", "content": "As discussed in the Introduction, KMTNet combines\ndata from three different observatories to search for\ncandidate microlensing events. Figure 3 illustrates the\ntiered observing cadence strategy. KMTNet primarily\nobserves through an I-band filter, although some obser-\nvations are taken occasionally with a V-band filter. The\nnominal ratio of V to I band observations is 1:9, but\nin practice, there are minor variations to take advan-\ntage of a particular site's unique characteristics. The\nV-band observations are excluded from evaluation for\nalerts, but they create time gaps in the I-band datasets.\nHere, we briefly review the main components of the\nKMTNet Alert Finder algorithm as it was described by\nKim et al. (2018b).\nFirst, the diagnostic information is used to mask the\ndata and remove data points likely to be photometric\noutliers. After that, the data are reduced to just epoch,\ndifference flux, and flux error and converted to binary\nformat. This reduction and change in format is a prac-\ntical adaptation needed to reduce memory requirements\nand processing time for the 500 million KMTNet light\ncurves. These binary files are passed to the AlertFinder\nalgorithm without the diagnostic information.\nThe AlertFinder algorithm then evaluates all unique\npermutations and combinations of the datasets to check\nwhether at least Nhigh of the last (Nhigh + 10) flux mea-\nsurements are \u2265 3\u03c3 above the median flux. Finally, if\nthis is true, the light curve is fit with a function con-\nsisting of a flat line plus a line with a constant slope (as\nshown in Fig. 4), i.e.,\n$$F(t) = a_0 + a_1(t-t_{break}) \\Theta(t - t_{break})$$\nwhere \\( \\Theta \\) is a Heaviside step function and tbreak is the\ntime of the break point. If Equation (1) is a significantly\nbetter fit to the data than a flat line (\u2206\u03c7\u00b2 > \u0394Xthresh),\nthen the light curve is flagged as a candidate microlens-\ning event. Typically, each day ~ 3\u00d7105 candidate events\n(out of ~ 5 \u00d7 108 total stellar light curves) are flagged\nas possible candidates.\nMany of the AlertFinder candidates are spatially cor-\nrelated, because when using the DIA photometric pack-\nage, a varying star will often affect light curves of nearby\nstars, e.g. creating \"ghost\" events that have the same\nproperties (Wyrzykowski et al. 2015). This effect is par-\nticularly bad for bright, variable stars, sometimes creat-\ning large clusters of false signals. Hence, any candidates\nwith many neighboring candidates are rejected. A maxi-\nmum of 20,000 candidates per field or 800 candidates per\nchip (1/4 of a field) are kept (sorted by star ID number,\nwhich gives preference to candidates detected in CTIO\ndata (whose star IDs begin with \u201cBLG\u201d). This process\nbrings the total number of candidates to be reviewed\ndown to ~3 \u00d7 104.\nThese remaining candidates are divided into two cat-\negories. About 3/4 were previously reviewed by KHH in\nthe past 6 days and not selected as microlensing events,\nleaving ~ 6 \u00d7 103 new light curves that require manual\nvetting. The other 3/4 may be reviewed or not depend-\ning on the specific load and time available on a particular\nday.\nBecause of the large fraction of false positives, the\ndefault status of each light curve is set to \"No,\" mean-\ning \"do not alert.\" Each light curve is then reviewed by\nKHH, who flags potential real microlensing events. For\nany events that are flagged at this stage (around 200),\nlight curves of the nearest neighbors are re-evaluated to"}, {"title": "3.5. Common False Positives", "content": "There are three common types of false positives for\ncandidate events:\n\u2022 Bleeding due to saturated stars: If a star is\nvery bright, the number of photons received by a\ngiven pixel in the detector can exceed its capacity,\nand the extra photons can \"bleed\" into neighbor-\ning pixels in the same column. In the images, this\neffect appears as vertical stripes.\n\u2022 Diffraction spikes: Diffraction of light inside the\ntelescope system can result in a stellated pattern\nextending out from the location of a bright star.\n\u2022 Nearby variable stars: Variable stars are of-\nten very bright, so if bleeding or diffraction spikes\nare associated with one of those stars, then those\nsignals will be time variable. Variable stars can\nalso corrupt the kernel calculated for the differ-\nence imaging, so the photometry of a nearby star\ncan be correlated with the variable even without\nthese explicit effects.\nThese phenomena can affect the light curves of nearby\nstars, depending on their (X, Y) location within the de-\ntector relative to the star creating the effect. Because\nthey depend on many factors, the effects these phenom-\nena produce can be time-variable, creating temporary\nincreases in brightness in the light curves of nearby stars\nthat are detected as candidate microlensing events. A\ncomparison of a real microlensing event, a false posi-\ntive due to a bleeding trail, and a false positive due to a\ndiffraction spike can be seen in Fig. 5. The middle set of\nlight curves in Fig. 6 show an example of contamination\nby a nearby, bright variable star."}, {"title": "4. LABEL GENERATION & TRAINING SET ASSEMBLY", "content": "For our training sample, we use data from the 2021\nKMTNet microlensing season. For each date the\nAlertFinder was run (weekdays from approximately\nApril-September), there are lists of the ~ 300,000 stars\nselected by the AlertFinder algorithm on that date as\npossible microlensing events, ~ 20,000 stars selected for\nlight curve review, ~ 200 candidates selected for differ-\nence imaging review, and ~ 20 actual alerts from that\ndate.\nSince the LensNet pipeline will be used on the sam-\nple of ~ 20,000 candidates selected for light curve re-\nview (Fig. 2), we selected our training sample from that\ngroup. In the human review, as discussed in Section\n3.3, usually only 1/4 of these candidates are reviewed\nbecause the other 3/4 had been previously reviewed and"}, {"title": "4.1. \"Yes\"", "content": "Initially, we created the \"Yes\" list by selecting any\ncandidates that were flagged as [1, 2, 3, 4] (i.e., ['clear',\n'probable', 'possible', 'not-ulens']) and cross-referencing\nthem with the KMTNet event table. This resulted in\n4183 candidates, but since some of them were flagged\non multiple dates before being alerted, there are only\n2158 unique stars in this list. For duplicate stars, we\nused the last review date (max(tcut)).\nThen, we removed any events that were not classi-\nfied as either \"clear\" or \"probable\" microlensing events\nin both the EventFinder (Kim et al. 2018a,c) and\nAlert Finder searches. For reference, the KMTNet\nEventFinder algorithm differs from the AlertFinder al-\ngorithm in that it is run after the microlensing season\nconcludes (so on the full season's data) and, because of\nthat, it uses a full Paczy\u0144ski microlensing light curve\nmodel for the fitting rather than a broken line (see Kim\net al. (2018a,c) for more details).\nIn this sample, we also checked for duplicate events\n(alerted based on different catalog stars, but at the same\ncoordinates), but did not find any."}, {"title": "4.2. \"Maybe\"", "content": "There are multiple reasons a candidate might be\nflagged as a possible candidate during the light curve\nreview stage but not alerted:\n1. It is a real event but it was not clear until more\ndata were taken,\n2. It is a \"ghost\" event: it is near a real event, so the\nlight curve shows the \"echo\" of a real event due to\ncontamination,\n3. It is a true false positive identified during differ-\nence image review, e.g., due to a bleed trail.\nBecause candidates in the first two categories share\nmany light curve characteristics with real microlensing,\nwe eliminated them from the \"Maybe\" sample. For cate-\ngory (1), this is straight forward and just involves check-\ning for a star in the KMTNet event table with the same\nID. For category (2), we cross-checked the coordinates\nof the candidate against all events in the KMTNet event\ntable and eliminated the candidate if it was within 20\"\nof an object in the table.\nThen, as with the \"Yes\" category, because a given\n\"Maybe\" candidate might be appear on multiple dates,\nwe only kept unique stars and used the last review date\n(max(tcut)). This leaves 1361 candidates in the \u201cMaybe\u201d\nsample.\nNote for this sample, we did not remove any can-\ndidates due to proximity to other candidates in the\n\"Maybe\" sample. As a result, some candidates in the\n\"Maybe\" sample are spatially correlated. This reflects a\nreal effect, e.g., bleed trails affect stars in the same col-\numn in a similar way. Hence, these correlations reflect\nreal features in the data that can be used to identify\nobjects in this category, even though we do not include\nspatial position as a training feature."}, {"title": "4.3. \"No\"", "content": "There are an abundance of candidates in the \"No\"\ncategory. Thus, we chose a random sample of a similar\nsize of the \"Yes\" and \"Maybe\" samples.\nWe start by choosing \"No\" candidates that were\nalerted on the same date as the initial \"Yes\" sample.\nFrom the \"No\" candidates, we randomly select a number\nof \"No\" candidates equal to the total number of \"Yes\"\ncandidates from that date. Candidates within 20\" of a\n\"Yes\" candidate from the same date are excluded from\nthis selection. From this random selection, we randomly\ndown-selected to 2098 \u201cNo\u201d candidates (this particular\nnumber was chosen to match the number of unique \"Yes\"\ncandidates prior to filtering for event quality) and elim-\ninated any candidates within 20\" of a real event. This\nleaves 2065 candidates in our \"No\" sample.\nWhile we do check for duplicated stars in this sample,\nwe do not check for proximity to other candidates in the\n\"No\" sample."}, {"title": "5. LIGHT CURVE PREPARATION FOR ML PIPELINE", "content": "After assembling the labeled dataset, several prepro-\ncessing techniques are applied to clean, augment, and\nstandardize the data. The time series data, which form\nthe basis of the microlensing event classification task,\nundergo multiple steps, including handling missing val-\nues, normalizing features, and ensuring proper align-\nment across different telescopes. In addition, we use a\ndata augmentation technique to artificially expand the\ntraining set by cropping the time series on either side\nof the microlensing event, enhancing the model's gen-\neralization ability. This ultimately helps simulate vari-\nous observational conditions, allowing the model to per-\nform robustly even with incomplete or noisy data. The\nremaining preprocessing steps ensure that the data is\nconsistently prepared for model training."}, {"title": "5.1. KMTNet Data Properties: Implications for Use as Neural Network Inputs", "content": "The observations from different sites are reduced and\nanalyzed separately for every field, due to offsets, vary-\ning systematics, and other site-specific discrepancies. As\na result, most stars have three associated data files,\nwhile stars in overlapping fields may have six or more.\nFurthermore, every site implements slight deviations\nfrom the nominal observing strategy to leverage its\nunique characteristics. Thus, the specific properties of\neach time series data file such as its length, spacing,\nand quality-depend on several factors. For example,\nweather conditions at a particular site may prevent ob-\nservations or degrade their quality. Unlike many ML\nproblems that work with uniformly sampled high-quality\ndata, KMTNet data contains irregularities in both the\ntiming and quality of our observations. Consequently,\npreparing the data for use in an ML algorithm requires\na more sophisticated approach to data processing in or-\nder to accommodate these inconsistencies.\nTherefore, we decided to separate the different tele-\nscope data inputs when processing them through our\nmodel. We designed LensNet as a branched pipeline\nthat analyzes each data file individually, accounting for\nvariations in observation conditions like systematics and\nsite-specific characteristics. This approach allows the\nmodel to capture subtle distinctions between datasets\nthat a single unified process would overlook, ultimately\nmaking a better use of the unique information each ob-\nserving site provides, and thus leading to more accurate\nand reliable predictions."}, {"title": "5.2. Data Cleaning", "content": "We perform a thorough cleaning and curation pro-\ncess of our dataset. First, we impose a limit on the se-\nquence length, where each candidate must have no more\nthan 1,500 observations. Sequences longer than this\nlength are cropped, because in the majority of cases, the\nmodel is already able to capture the patterns of the star\nwithin this length. Furthermore, the additional predic-\ntive power from the remaining data is relatively limited\nand comes at a high computational cost. Moreover, we\nhandle duplicate instances carefully: if a star appears in\ntwo or more categories in different days, it is reassigned\nto a single category (we perform this reassignment is\nhandled on a case-by-case basis).\nIn addition, we apply specific constraints to the data:\nWe remove observations where the flux error is negative,\nwhere the PSF quality is outside the acceptable range\nof 0 to 100, and where the FWHM is negative. We also\nrequire that a candidate star has a minimum of 10 valid\nobservations from at least one of the telescopes after all\nthe cleaning steps.\nFollowing this process, the curated dataset consists\nof 1,190 instances in the \"Maybe\" category, 1,825 in-\nstances in the \"Yes\" category, and 2,038 instances in\nthe \"No\" category. While these classes are not perfectly\nbalanced, we account for this imbalance by applying dif-\nferent weights to adjust the learning rates for each cate-\ngory during model training, ensuring that the classifier\nlearns effectively across all classes."}, {"title": "5.3. Data augmentation", "content": "Data augmentation is a popular technique used to ar-\ntificially expand the size and diversity of a dataset by\napplying various transformations to the original data.\nThis process helps improve the model's generalization"}, {"title": "5.4. Data Pre-processing Pipeline", "content": "The preprocessing pipeline for the time series data\nconsists of several crucial steps that ensure the input\ndata is standardized, free from outliers, and properly\naligned for the Recurrent Neural Network (RNN) archi-\ntecture. This section details the steps of time relativiza-\ntion, outlier removal, NaN handling, fitting, standard-\nization, and padding, each of which contributes to the\nrobustness and accuracy of the model.\n\u2022 Time Relativization: To standardize the tem-\nporal dimension of the data across different tele-\nscopes, we implement a time relativization step.\nHere, the time values in the dataset are adjusted\nrelative to the moment of the last observation\nacross all three telescopes, denoted as tlast. This\npoint is crucial as it represents the time at which\nthe alert was triggered. However, this relativiza-\ntion is performed after data augmentation to avoid\nany bias or leakage of future information into the\nmodel. By shifting the time axis in this manner,\nwe ensure that the model's input data is aligned\ntemporally, regardless of the specific observational\ntimelines of each telescope.\n\u2022 Outlier Removal: After time relativization, we\napply an outlier removal algorithm to clean the\ndata. This step involves identifying and exclud-\ning data points that deviate significantly from the\noverall trend, both upwards and downwards. The\nremoval is conducted across all features. This pro-\ncess helps eliminate anomalies that could nega-\ntively impact the model's performance, ensuring\nthat the input data more accurately represents the\nunderlying astrophysical signals.\n\u2022 NaN Handling and Filling: We remove any\nmissing data points (NaNs) that are present in\nany of the features during preprocessing, rather\nthan attempting to fill gaps through interpolation.\nGiven that the Recurrent Neural Network (RNN)\narchitecture processes data sequentially, interpo-\nlating missing values would not provide any addi-\ntional information or benefit to the model. More-\nover, because the RNN is not constrained by a\nfixed input length, we are able to preserve the in-\ntegrity of the remaining data without the need for\nimputation. This flexibility is one of the key ad-\nvantages of the chosen RNN architecture, allowing\nus to work with variable-length sequences while\nensuring the model is not disrupted by incomplete\nobservations or gaps.\n\u2022 Fitting the Ascending Data: The next step\nin the preprocessing pipeline involves fitting a\nline to the ascending data portion of the flux,\nwhich is critical for capturing the microlensing\nevent's rising phase (this step implicitly mimics\nthe AlertFinder algorithm behavior). The fitting\nprocess takes into account the flux error, provid-\ning a weighted fit that more accurately reflects the\nobservational uncertainties. The fitted line is then\ntreated as an additional feature in the input vector\nthat is passed to the ML algorithm. Specifically,\nduring the historic (pre-event) data, this feature is\nset to the average flux in that region. After tbreak,\nthe feature transitions to match the values of the\nfitted line, creating a piecewise smooth function\nthat serves as a robust input for the model. A key\nconstraint imposed during fitting is that the line\nmust start at the point of tbreak and the average\nhistoric flux, ensuring consistency and continuity\nin the data representation.\n\u2022 Standardization: To ensure that the data from\nall telescopes is on a comparable scale, we ap-\nply standardization independently to each feature.\nThis involves normalizing the data by subtracting\nthe mean and dividing by the standard deviation,"}, {"title": "6. LENSNET ARCHITECTURE", "content": "To accurately classify potential microlensing events\nusing time series data from the three different tele-\nscopes (CTIO, SAAO, and SSO), we designed LensNet,\na branched Recurrent Neural Network (RNN) architec-\nture where each branch processes data from a single\ntelescope independently. A schematic pipeline of how\nLensNet works is depicted in Fig. 8. This design choice\nis motivated by the unique observational characteristics\nand challenges presented by each telescope's data. For\ninstance, in Fig. 8, the CTIO telescope has the most\ncomprehensive coverage, while SAAO starts its obser-\nvations later (leading to a shorter time series), and the\nSSO data contains a gap in the observations.\nRNNs are particularly well-suited for this task because\nthey are designed to handle sequential data, making\nthem ideal for analyzing time series where temporal de-\npendencies are crucial. Furthermore, by treating each\ntelescope's data separately, we ensure that the model\ncan adapt to the specific noise patterns, data gaps, and\nobservational cadences of each instrument. Each branch\nof the RNN processes not only the flux data but also\nother key features which allows the network to capture\nthe complex relationships between these variables over\ntime. These features include the observation time, the\nflux error, air mass, FWHM, \u03c72, and the PSF quality.\nAfter processing the data from each telescope through\nits respective RNN, the final encoded states are com-\nbined and passed through a series of dense layers to\nproduce the final classification output, which then deter-\nmines if the alert is a microlensing event. This approach\nallows the model to leverage the full temporal resolution\nof each telescope's observations while maintaining flexi-\nbility in dealing with the varying lengths and quality of\ndata streams.\nIn addition to the branched RNN architecture, we ex-\nplored several alternative methods to classify the mi-\ncrolensing events. These included extracting features\nfrom the fitted piece-wise linear function and utilizing\na variety of classical ML algorithms, such as XGBoost,\nbagging, decision trees, and perceptron models. The\nfeatures we considered included the average standard\ndeviation of the flux relative to the fit before and af-\nter the microlensing event (a proxy for fitting error pre-\nand post- tbreak), the slope of the rising flux, parame-\nters from higher-order fittings, skewness, kurtosis, and\nthe number of points before and after tbreak. Despite\nthese efforts, we did not observe any significant correla-\ntion between these features and the target labels, with\nthe models performing at around ~ 50% binary accu-\nracy, essentially equivalent to random guessing.\nWe also experimented with using an RNN architecture\nthat only processed CTIO data, given its higher confi-\ndence in many of the training instances, as well as em-\nploying convolutional architectures to process the time\nseries data. Ultimately, however, the architecture we\npresent in this paper-the branched RNN that processes\ndata from each telescope independently-outperformed\nall the other approaches studied, offering the most ac-\ncurate and robust results."}, {"title": "7. LENSNET TRAINING", "content": "We conducted an in-depth study on two distinct classi-\nfication tasks. The first task was a binary classification\nproblem, where we combined the \"Yes\" and \"Maybe\"\nlabels into a single class, distinguishing them from the\n\"No\" class. This approach was based on the understand-\ning that \"Yes\" and \"Maybe\" are often indistinguishable\nin flux data alone and are typically sub-classified man-\nually only after reviewing the difference image. This\nmimics the classification performed by co-author KHH.\nThe second, more challenging task was a multi-class clas-\nsification problem, where the model aimed to differen-\ntiate between \"Yes,\" \"Maybe,\" and \"No\" classes inde-\npendently. This task was particularly difficult because"}, {"title": "8. RESULTS", "content": "The results of the LensNet model demonstrate its\nstrong performance in classifying microlensing events\nacross both binary and multi-class classification tasks.\nIn the binary task, the model effectively distinguishes\nbetween genuine potential microlensing events (grouping\nboth \u201cYes\u201d and \u201cMaybe\u201d labels) and non-events (\u201cNo\u201d),\nachieving a peak accuracy of 87.5% on the test set. In\nthe multi-class task, which differentiates between \"Yes\",\n\"Maybe\", and \"No\" labels, the model attains a lower but\nstill robust accuracy of 78%, reflecting the greater dif-\nficulty of separating the ambiguous \"Maybe\" class from\nconfirmed events and false positives. Throughout the\nexperiments, the model demonstrated its robustness to\npartial data visibility, particularly excelling when pro-\nvided with more comprehensive data (Fig. 11). Addi-\ntionally, a threshold analysis of the binary task showed\nthat higher thresholds can be used to achieve near-\nperfect classification purity for non-microlensing events,\nminimizing false positives and improving operational ef-\nficiency (Fig. 12). These results underscore LensNet's\npotential for real-time deployment, offering a significant\nreduction in manual vetting while maintaining high clas-\nsification accuracy.\nIn addition to the accuracy metrics discussed, the con-\nfusion matrices of Appendix A (Fig. 13) provide fur-\nther insights into the model's classification performance.\nThese matrices confirm that the model performs particu-\nlarly well, especially on unseen data. We present results\nfor both the binary classification task (with threshold\nvalues of 0.45 and 0.955) and the 3-class classification\ntask, highlighting the model's flexibility in balancing\nprecision and recall."}, {"title": "8.2. Evaluation Under Partial Data Visibility", "content": "Fig. 11 summarizes the accuracy metrics across differ-\nent categories and sets as a function of the percentage of\nthe microlensing raw non-augmented data fed into the\nnetwork. This experiment allows us to evaluate the per-\nformance of LensNet with varying amounts of available\ninformation. Specifically, the horizontal axis in all the\nplots represents the proportion of the microlensing alert\npresented to the model, ranging from 0% (no microlens-\ning signature shown) to 100% (all the observations be-\ntween tbreak and tcut are shown to the model, i.e., the\nfull alerted region as identified by the AlertFinder al-\ngorithm). To obtain the modified datasets with the dif-\nferent percentages we performed various right-side crop-\nping operations to the region of the time-series within\ntbreak and tcut. This experiment provided valuable in-\nsights into how the model's performance evolves as it\n\"sees\" less of the microlensing alert.\nIt is important to note that in the operational mode,\nthe network should function at 100% alert visibility, as\nthis is the data set provided by the Alert Finder algo-\nrithm. However, we conducted this test to explore the\nmodel's behavior under more constrained conditions,\npushing its limits to better understand its robustness\nand reliability.\nThe left plots in the Fig. 11 show the per-category and\nper-set accuracies. These plots represent the individual\ncategorical accuracy for both the binary and multi-class\nclassification tasks across the training, validation, and\ntest sets.\nThe right plots of the Fig. 11 present the mean accu-\nracies for each set, calculated by averaging the curves of\neach category displayed in"}]}