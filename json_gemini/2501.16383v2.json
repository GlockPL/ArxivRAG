{"title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations", "authors": ["Zunhai Su", "Zhe Chen", "Wang Shen", "Hanyu Wei", "Linge Li", "Huangqi Yu", "Kehong Yuan"], "abstract": "Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding re-computation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75\u00d7 larger batch sizes, and achieves a 2.32x speedup in decoding stage.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have attracted considerable attention due to their remarkable capabilities in next-token-prediction generation tasks [Zhao et al., 2023]. A critical technique of LLMs is the Key-Value (KV) cache, which avoids recomputation by caching the KVs generated by the attention layer in each Transformer block [Vaswani, 2017]. However, as the batch size and context length increase, the growing size of KV caches not only leads to significant memory consumption, but also renders LLM inference memory-bound, limiting system throughput [Liu et al., 2024d; Kang et al., 2024]. Low-bit quantization is widely used to compress LLMs for enhanced memory and time efficiency, covering various aspects, including weight-only quantization [Frantar et al., 2022; Lin et al., 2024b], weight-activation quantization [Xiao et al., 2023a; Ashkboos et al., 2024; Liu et al., 2024c; Saxena et al., 2024], and KV cache quantization. Existing studies on KV cache quantization often employ techniques like fine-grained per-channel quantization [Liu et al., 2024d; Kang et al., 2024] or mixed-precision quantization [He et al., 2024; Yang et al., 2024; Duanmu et al., 2024; Hooper et al., 2024], which retain a significant portion of high bit-widths caches. As a result, these methods compromise compression efficiency and fail to maintain robustness under high compression ratio or extremely low average bit-widths.\nRecently, Hadamard-transform-based rotation technique has demonstrated significant effectiveness in mitigating outliers in 4-bit LLM quantization, with studies such as [Ashkboos et al., 2024; Liu et al., 2024c; Saxena et al., 2024] leveraging rotation technique to achieve 4-bit quantization of weights, activations and KV caches. However, the potential of rotation methods for extremely low-bit KV cache quantization has yet to be fully explored. In this study, we thoroughly analyze the limitations of existing KV rotation settings and propose RotateKV, a tuning-free method that ensures superior outlier management through the following innovations (see Figure 1 for an illustrative example):"}, {"title": "2 Related Work", "content": "2.1 KV Cache Quantization\nQuantization reduces the bit-widths of numerical representation to compress the KV cache, effectively decreasing memory usage and alleviating the memory-bound challenge. Generally, existing KV cache quantization methods can be categorized into two types based on the quantization dimension of Keys: per-channel and per-token. Due to significant outliers along the channel dimension in Keys, methods such as KIVI [Liu et al., 2024d], Gear [Kang et al., 2024], and KVQuant [Hooper et al., 2024] adopt per-channel quantization to mitigate quantization errors. However, these approaches often require fine-grained quantization or the retention of a certain proportion of outliers unquantized to preserve model performance, both of which compromise the compression ratio. On the other hand, due to the autoregressive nature of LLMs, where tokens are predicted sequentially, per-token quantization is commonly used in methods such as Zip-Cache [He et al., 2024], MiKV [Yang et al., 2024], SKVQ [Duanmu et al., 2024], and other less aggressive KV quantization methods [Ashkboos et al., 2024; Liu et al., 2024c; Shah et al., 2024]. These approaches typically focus on the saliency differences between tokens, allocating relatively higher bit-widths to store a significant proportion of salient tokens. However, this also results in a compromised compression ratio. Unlike these methods, RotateKV employs per-token quantization but eliminates the need to store large proportions of high-bit caches. Besides, experiments show that RotateKV offers superior outlier management compared to per-channel approaches, ensuring robust performance even at higher compression ratio."}, {"title": "3 Preliminary", "content": "3.1 Inference Process of LLMs\nThe LLM inference process consists of two stages: the prefill phase and the decoding phase.\nPrefill Phase. The model processes the token sequence generated from the prompt and generates the initial output token, with each attention layer computing and caching KV pairs. Let $X \\in \\mathbb{R}^{l_{prompt} \\times d}$ represent the input embeddings, where $l_{prompt}$ is the length of the input token sequence and d is the model's hidden size. In each attention layer, the KV cache can be derived as follows:\n$K = XW_k, V = X \\cdot W_v$, (1)\n$K_{cache} \\leftarrow K, V_{cache} \\leftarrow V$, (2)\nwhere $W_k, W_v \\in \\mathbb{R}^{d \\times d}$ are the weight matrices for the Key and Value calculations, respectively.\nDecoding Phase. The model takes a single token as input. Let $t \\in \\mathbb{R}^{1 \\times d}$ as the input embedding. Each attention layer computes $t_q, t_k$ and $t_v$ as follows:\n$t_q = t \\cdot W_q, t_k = t \\cdot W_k, t_v = t \\cdot W_v$. (3)\nThen, $t_k$ and $t_v$ are employed to update the KV cache, with the complete KV cache supporting subsequent computations.\n$K \\leftarrow concat(K_{cache}, t_k), V \\leftarrow concat(V_{cache}, t_v)$, (4)\n$t_o = softmax(t_q \\cdot K^T \\cdot d^{- \\frac{1}{2}}) \\cdot V$. (5)\n3.2 Quantization\nThe n-bit asymmetric integer quantization and dequantization processes can be expressed as:\n$Q(X) = clamp(\\frac{X}{scale} + zero, 0, 2^n - 1)$, (6)\n$X' = scale \\cdot (Q(X) - zero)$, (7)\n$scale = \\frac{clipped\\_max(X) - clipped\\_min(X)}{2^n-1}$, (8)\n$zero = \\frac{clipped\\_min(X)}{scale} 2^n-1$, (9)\nwhere $[\\cdot]$ indicates round operation. $Q(X)$ and $X'$ denote the quantized and dequantized values of X. The $clamp$ operation constrains the values within a specified range. $clipped\\_max(X)$ and $clipped\\_min(X)$ denote the operations that truncate the maximum and minimum values of X."}, {"title": "3.3 Hadamard Matrix", "content": "Hadamard Matrix R is a specific type of orthogonal matrix characterized by entries proportional to {+1,-1}. The Hadamard matrix also satisfies the definition of a rotation matrix, as it is an orthogonal matrix with det(R) = 1. QuIP [Chee et al., 2024] demonstrates that multiplying an rotation matrix leads to a reduction in the maximum entry relative to its norm, effectively mitigating outliers and enhancing quantizability. Walsh-Hadamard Matrix is a particular instance of Hadamard matrix generated recursively as follows, the subscript denoting the dimension of matrix, where $k \\in \\mathbb{Z}^+$:\n$H_1 = [1], H_{2^k} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} H_{2^{(k-1)}} & H_{2^{(k-1)}} \\\\ H_{2^{(k-1)}} & -H_{2^{(k-1)}} \\end{bmatrix}$. (10)\nThe scaling factor $\\frac{1}{\\sqrt{2}}$ ensures normalization. The recursive structure of the Walsh-Hadamard matrix enables efficient computation via the FWHT algorithm [Hedayat and Wallis, 1978], reducing computational complexity of matrix-vector multiplication to $O(n \\log n)$, where n represents the dimension of the matrix."}, {"title": "4 Methodology", "content": "Section 4.1 outlines the limitations of existing outlier-unaware Key rotation and proposes outlier-aware rotation. In Section 4.2, we analyze the impact of ROPE on rotation, then propose pre-RoPE grouped-head rotation. Section 4.3 further introduces attention-sink-aware quantization. Section 4.4 provides a comprehensive summary of RotateKV. The overview of RotateKV is illustrated in Figure 3.\n4.1 Outlier-Aware Rotation\nExisting Outlier-Unaware Key Rotation\nWhen applying quantization, outliers pose a persistent challenge as they expand the quantization range, reducing the effective bits available for most values. Recently, LLM quantization research [Chee et al., 2024; Ashkboos et al., 2024; Liu et al., 2024c; Saxena et al., 2024; Lin et al., 2024a; Shah et al., 2024] has focused on Hadamard-transform-based rotation technique, which involves multiplying a Hadamard matrix to reduce outliers and improve quantizability. As shown in Figure 4, existing rotations within the attention layer can be classified into two categories: offline rotations (e.g., R1 and R2), which can be fused into the weights prior to inference to eliminate the overhead of online computations, and online rotations (e.g., R3), which are performed dynamically using the FWHT. This differentiation exists because the Query and Key computation relies on RoPE, rendering it incompatible with offline rotations. To reduce the computational overhead during inference, Key rotations typically utilize the efficient FWHT algorithm. For example, QuaRot [Ashkboos et al., 2024] leverages offline random Hadamard matrices and online FWHT to achieve outlier-free 4-bit LLM inference. SpinQuant [Liu et al., 2024c] employs Cayley optimization to enhance offline rotation but continues to utilize the FWHT for online rotations. Due to the structure of the Walsh-Hadamard matrix, as shown in Equation 10, the rotations for each attention head with the same dimension rely on the same Walsh-Hadamard matrix. This constraint limits adaptability to varying channel-wise outlier distributions."}, {"title": "4.1 Outlier-Aware Rotation", "content": "While adjusting rotation for pre-rotation distributions seems straightforward, it is impractical. Modifying the Hadamard matrices invalidates the FWHT algorithm, increases computational overhead, and necessitates storing multiple matrices, thereby further reducing compression ratio. To adapt rotations to varying channel-wise outlier distributions without compromising the efficiency of FWHT, we introduce the outlier-aware rotation, which preserves the FWHT while enhancing it with outlier-aware operation derived from lightweight calibration. We conduct experiments using various enhancement strategies, including smoothing and re-ordering. Smoothing [Xiao et al., 2023a; Lin et al., 2024c] scales down the Keys by per-channel factors $\\Lambda$ and correspondingly scaling up the associated channels in the Query:\n$Q \\cdot K^T = (QA) \\cdot (K\\Lambda^{-1})^T, \\Lambda = diag(\\lambda)$. (11)\nReordering arranges channels across all heads to reduce outliers within each quantization group. As shown in Table 1, experiments on different outlier-aware strategies demonstrate that smoothing fails to maintain performance at 2-bit quantization. For reordering, we find that relying on clustering or other complex permutation methods is unnecessary. Instead, reordering the channels of each token using the indices sorted by the values after rotation can effectively reduce quantization errors (Table 2) and improve PPL. Notably, the channel reordering indices are derived through fast calibration and remain consistent across all tokens throughout inference. The calibration procedure is detailed in Algorithm 1."}, {"title": "4.2 Pre-ROPE Grouped-Head Rotation", "content": "Existing Post-ROPE Key Rotation\nRecent LLMs, including LLaMA [Touvron et al., 2023], Mistral [Jiang et al., 2023] and Qwen [Bai et al., 2023a] use ROPE [Su et al., 2024] to encode token positions. The positional encoding introduced by RoPE is applied to pairs of channels, resulting in a reduction of magnitude consistency within individual channel, as shown in Figure 6. Previous work [Hooper et al., 2024] also highlights that RoPE affects per-channel quantization performance. In this study, we observe that the inconsistency in the magnitudes of channel-wise outliers significantly undermines the effectiveness of outlier-aware rotation. As demonstrated in Table 2, the quantization error increases by 145% after applying RoPE.\nPre-ROPE Grouped-Head Rotation\nTo address the issues associated with post-RoPE rotation, we introduce a novel pipeline that applies the outlier-aware rotation before RoPE. As illustrated in Figure 3, this design not only eliminates RoPE's negative impact on rotation but also facilitates the incorporation of rotation and reordering operations into the weights, thus requiring only the inverse operation during inference. The proposed pre-ROPE rotation also enables grouped-head combined rotation, allowing for more effective outlier reduction across heads. Although increasing the head numbers improves PPL, it also incurs higher computational costs. Therefore, it is crucial to balance computational overhead and performance gains when determining the group size. Typically, as shown in Table 3, a group size of 4 heads is considered a reasonable choice."}, {"title": "4.3 Attention-Sink-Aware Quantization", "content": "The research [Xiao et al., 2023b] indicates that LLMs tend to treat the initial token as a 'sink', assigning it disproportionately high attention scores. Moreover, [Hooper et al., 2024; Duanmu et al., 2024; Liu et al., 2024b] highlight that the KVs associated with sink tokens are sensitive to quantization, and retaining only the initial token in FP16 can effectively enhance quantization. However, more recent studies [Yu et al., 2024; Sun et al., 2024] suggest that these few-in-number attention sinks can emerge not only at the initial token but also at various other positions, whereas existing practices fail to retain them precisely. One key reason is that efficient attention computation relies on kernels such as FlashAttention [Shah et al., 2024], which directly output the attention results without exposing the intermediate attention scores. This prevents the dynamic identification of additional attention sinks beyond those fixed at the initial token. Inspired by recent studies on the interpretability of attention sinks [Sun et al., 2024; Guo et al., 2024], we propose attention-sink-aware quantization, which leverages massive activations to identify additional sink tokens without relying on attention scores, thereby precisely retaining them during the quantization process. Research on massive activations [Sun et al., 2024]\u2014those activations in the residual sums of Transformer block outputs with significantly larger magnitudes than others\u2014suggests that attention is concentrated on these activations. Specifically, when massive activations occur, the corresponding tokens attract concentrated attention, forming attention sinks. Figure 5 provides a real example from LLaMA-2-7B [Touvron et al., 2023]. Therefore, by identifying the massive activations, additional sink tokens can be pinpointed. The process of attention-sink-aware quantization is outlined in Figure 3."}, {"title": "4.4 Summary of RotateKV", "content": "In summary, RotateKV first performs fast calibration to obtain reordering indices, then integrates grouped-head rotation and channel reordering into the Key weights. These operations result in outlier-aware rotation of the Keys, making them more suitable for quantization. After updating the KV cache, inverse online reordering and rotation are applied. For the Value, we adopt a simple offline rotation as shown in Figure 3, since Values do not contain outliers like Keys."}, {"title": "5 Experiments", "content": "5.1 Experiment Settings\nModels and Tasks.\nTo validate the robustness of our method, we evaluate RotateKV on a variety of challenging tasks using both LLMs and visual-language models (VLMs), including LLaMA-2-7B/13B [Touvron et al., 2023], LLaVA-v1.5-7B/13B [Liu et al., 2024a], Mistral-7B [Jiang et al., 2023], LLaVA-v1.6-Mistral-7B [Liu et al., 2023], LLaMA-3-8B [Dubey et al., 2024], and LLaMA-2-7B-80K [Fu et al., 2024]. We begin by evaluating the PPL on the WikiText-2 dataset [Merity et al., 2016]. Then, evaluation on the GSM8k dataset with CoT prompting is conducted to assess RotateKV's capability in handling complex CoT reasoning task. We further test long-context and multi-modal tasks accuracy across eight tasks from the LongBench [Bai et al., 2023b] and MileBench [Song et al., 2024]. To assess RotateKV's performance with extremely long contexts, we also evaluate it on the 40K context-length Needle-in-a-Haystack (NIAH) test using the LLaMA-2-7B-80K [Fu et al., 2024] model. The source code for reproducing the results and generating the visualizations is available at https://github.com/ZunhaiSu/RotateKV.\nQuantization Settings.\nWe employ per-token asymmetric integer quantization for both Keys and Values, setting the quantization group size of RotateKV to 128 across all evaluations to demonstrate the accuracy and robustness of our method at relatively coarse quantization granularity. The group size for grouped-head rotation is consistently set to 4 across all the models and tasks we tested. We employ FP8 to store the scale parameters and INT8 for the zero-points, since this approach does not affect the results but improves the overall compression rate. The calibration process is highly efficient, taking less than five minutes on a single RTX 4090 GPU for the LLaMA-2-7B model using the WikiText-2 dataset. Additionally, the evaluation results demonstrate that calibration performed on WikiText-2 generalizes effectively to other datasets."}, {"title": "5.2 Main Results and Analysis", "content": "Perplexity Evaluation.\nWe use the original KV rotation and quantization scheme from QuaRot [Ashkboos et al., 2024] as one of the baselines to highlight the improvements of our proposed adaptive rotations. Additionally, we include KVQuant [Hooper et al., 2024] for PPL comparison, which leverages several techniques such as non-uniform quantization and per-vector dense-and-sparse quantization, demonstrating state-of-the-art PPL result. To ensure a fair comparison, for QuaRot we only quantize the KV cache, and for KVQuant, we adopt the scheme that preserves 0.5% FP16 outliers, which aligns the average bit-widths of our approach. As shown in Table 4, compared with the FP16 baseline, RotateKV shows a PPL degradation of 0.01 at 4-bit quantization, less than 0.1 at 3-bit, and less than 0.3 at 2-bit on LLaMA-2-13B. Compared to QuaRot, which exhibits significant PPL degradation at 2-bit quantization, our method remains effective even at extremely low bit widths. Compared to KVQuant, RotateKV consistently demonstrates a PPL improvement of around 0.1 at 2-bit quantization across all LLMs we tested. Notably, RotateKV uses the simpler integer quantization.\nGSM8K Evaluation.\nThe GSM8K dataset [Cobbe et al., 2021] is widely used to evaluate the arithmetic reasoning capabilities of LLMs. This task presents significant challenges and evaluations on it can effectively show the impact of compression methods on model performance. As shown in the Table 5, RotateKV maintains strong CoT reasoning capabilities, with less than 1.7% performance degradation compared FP16 baseline. It outperforms existing methods even at lower average bit-widths, demonstrating the robustness of our approach. Notably, lower average bit-widths lead to higher compression ratio, enabling support for longer context lengths and larger batch sizes on the same GPU setup.\nLongBench and MileBench Evaluations.\nTo further demonstrate the robustness of RotateKV, we conduct experiments on eight long-context and multi-modal tasks selected from LongBench [Bai et al., 2023b] and MileBench [Song et al., 2024]. For comparison, we include KIVI [Liu et al., 2024d], which uses per-channel Key quantization and shows negligible performance degradation with a group size of 32. As shown in Table 6, compared to the FP16 baseline, KIVI with a 128 quantization group size [Liu et al., 2024d] exhibits significant performance degradation of 46.5% on the LLaMA-2-7B and LLaVA-v1.5-7B models. In contrast, our method demonstrates negligible average accuracy loss at the same quantization group size, with less than 1.4% performance loss on Mistral-7B and LLaVA-v1.6-Mistral-7B.\nNeedle-in-a-Haystack Evaluation.\nThe NIAH task is designed to evaluate the ability to retrieve specific information within a large body of unrelated data. In our experiments, we utilize a context length of 40K, segmented into 40 intervals. Within each interval, the needle is positioned at 10 different depths of the context for evaluation."}, {"title": "5.3 Ablation Study", "content": "Ablation Study of the Proposed Innovations\nStarting with the original rotation method from QuaRot [Ashkboos et al., 2024], we progressively incorporate the proposed innovations to evaluate their individual contributions. As shown in Table 7, the innovations introduced by RotateKV effectively mitigate PPL degradation.\nAblation Study on Quantization Granularity\nTo assess the impact of finer quantization granularity, we perform PPL evaluations with group sizes of 64 and 32. The results in Table 8 demonstrate that RotateKV continues to achieve better model performance preservation at smaller quantization group sizes. For instance, the PPL only drops by 0.18 on the LLaMA-2-13B model at 2-bit with a group size of 32."}, {"title": "5.4 Efficiency Analysis", "content": "In this section, we evaluate the efficiency of the current implementation. We use Triton [Tillet et al., 2019] for the quantization and dequantization kernels, along with an optimized CUDA kernel for FWHT, following [Dao, 2024]. Evaluations are conducted on LLaMA-2-7B [Liu et al., 2024a] with a 8-NVIDIA 4090D (24GB) setup, FlashAttention [Shah et al., 2024] enabled. The batch size is progressively increased with an input length of 500 tokens until an out-of-memory (OOM) error occurs. As shown in Figure 8a, RotateKV reduces peak memory usage by 3.97\u00d7 compared to the FP16 baseline and supports 5.75\u00d7 larger batch sizes. In terms of speed, RotateKV achieves a 2.32\u00d7 speedup during the decoding phase, as shown in Figure 8b. Notably, the efficiency can be further improved through techniques like kernel fusion."}, {"title": "6 Conclusion", "content": "In this paper, we explore the potential of rotation technique for 2-bit KV quantization. With the proposed innovations, RotateKV adaptively rotates the KV cache in an outlier-aware manner, demonstrating outstanding outliers control. Comprehensive evaluations show that RotateKV effectively preserves model performance even at high compression ratio, addressing the limitations of existing KV quantization methods and demonstrating state-of-the-art performance in both compression efficiency and accuracy. Future work will focus on optimize RotateKV's implementation to further reduce the overhead associated with online operation during LLM inference."}]}