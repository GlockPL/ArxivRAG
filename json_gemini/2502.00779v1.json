{"title": "Role of Mixup in Topological Persistence Based Knowledge Distillation for Wearable Sensor Data", "authors": ["Eun Som Jeon", "Hongjun Choi", "Matthew P. Buman", "Pavan Turaga"], "abstract": "The analysis of wearable sensor data has enabled many successes in several applications. To represent the high-sampling rate time-series with sufficient detail, the use of topological data analysis (TDA) has been considered, and it is found that TDA can complement other time-series features. Nonetheless, due to the large time consumption and high computational resource requirements of extracting topological features through TDA, it is difficult to deploy topological knowledge in machine learning and various applications. In order to tackle this problem, knowledge distillation (KD) can be adopted, which is a technique facilitating model compression and transfer learning to generate a smaller model by transferring knowledge from a larger network. By leveraging multiple teachers in KD, both time-series and topological features can be transferred, and finally, a superior student using only time-series data is distilled. On the other hand, mixup has been popularly used as a robust data augmentation technique to enhance model performance during training. Mixup and KD employ similar learning strategies. In KD, the student model learns from the smoothed distribution generated by the teacher model, while mixup creates smoothed labels by blending two labels. Hence, this common smoothness serves as the connecting link that establishes a connection between these two methods. Even though it has been widely studied to understand the interplay between mixup and KD, most of them are focused on image based analysis only, and it still remains to be understood how mixup behaves in the context of KD for incorporating multimodal data, such as both time-series and topological knowledge using wearable sensor data. In this paper, we analyze the role of mixup in KD with time-series as well as topological persistence, employing multiple teachers. We present a comprehensive analysis of various methods in KD and mixup, supported by empirical results on wearable sensor data. We observe that applying mixup to training a student in KD improves performance. We suggest a general set of recommendations to obtain an enhanced student.", "sections": [{"title": "1. Introduction", "content": "Wearable sensor data analysis has enabled many application by utilizing the power of deep learning. However, there are common challenges, such as inter- and intra-person variability, sensor-level noises, dependency on the sampling rate of the sensors, resulting in performance degradation and difficulties for deployment with machine learning. To mitigate these problems, topological data analysis (TDA) methods have been utilized on wearable sensor data analysis [1, 2, 3], which have resulted in many robust ways to capture detailed time-series information, and can be increasingly applied to many different areas. TDA methods allow for capturing and preserving shape-related information and have the potential to make sensor data processing pipelines more robust to different types of time-series corruptions [4, 5, 6]. Topological features can be represented in many ways [7, 8], a common approach is referred to as the persistence image (PI) \u2013 which can aid in easily deploy topological persistence in machine learning owing to it 2D image-like form. Prior research has found that persistence images provide additional information that complements the raw time-series data to improve performance in time-series classification problems on wearable sensor data [2, 3, 9]. Applications of topological methods also have touched upon many areas particularly in sensor data analysis [10, 11, 12].\nAlthough TDA has shown great promise, leveraging topological features by TDA on edge-devices including wearable devices, particularly implementing them on small form factor and memory limited devices, is difficult because of large computational resources and time consumption requirements to extract the topological features [4, 13]. Also, previous studies implement separate models in test-time simultaneously to utilize topological as well as time-series data to improve performance [2], which can increase the complexity of a model. Based on this insight, new methods to create a unified model for maximizing efficiency and integration of topological features is required.\nTo address these issues, knowledge distillation (KD) can be adopted as a solution, which generates a small and superior model by transferring knowledge from a large network model. Furthermore, it enables to leverage multimodal data to distill a robust single model. With KD, a teacher trained with topological features can be utilized to provide more diverse information to a student while complementing time-series features. With multiple teachers trained with the raw time-series and topological representations, a single and superior student, using the time-series data alone, can be distilled [3].\nIn KD, the temperature hyperparameter plays a key role in learning process, which controls the smoothness of distribution and determines the difficulty level of the distillation process. In this context, recently, many studies have delved into the impact of mixup augmentation in KD [14, 15, 16, 17, 18]. Particularly, for image analysis, Choi et al. [15] explored the interplay of mixup with KD and revealed that smoothness serves as the connecting link to understand the effect of mixup in KD. For more details, in KD, the student learns from the smoothed distribution provided by the teacher model, and this distribution is further smoothed by increasing the temperature value. Similarly, mixup generates new smooth labels by blending two given inputs and ground truth labels, which are then further smoothed by strongly interpolated samples (e.g., a high alpha value in the beta distribution). Thus, their behave as a connecting link for promoting smoothness in learning process, which can generate synergetic effects to distill a robust lightweight model [15, 17].\nThere are different augmentation methods such as regularization effect [19], model invariance [20], and feature learning [21]. However, these techniques are more focus on alleviating noises or data point issues in rotation, which are different from mixup [22] blending multiple samples. Further, even if other augmentations (e.g. cutmix [23] and adversarial training [24]) are effective, mixup offers different benefits in much lower computational overhead and provides solid foundations, particularly in the context of knowledge distillation [25, 26, 27].\nEven though the interplay between two techniques, mixup and KD, is significantly crucial in performance improvement, the majority of previous studies have primarily concentrated on image-based analysis. To the best of our knowledge, the impact of mixup and KD in the context of both time-series and topological representations on wearable sensor data remains unexplored. Furthermore, the behavior of mixup for multiple teachers and different strategies in KD have not been investigated.\nIn this paper, we study the behavior of mixup in KD with multimodalities using both time-series and topological representations for wearable sensor data analysis. We implement different KD approaches for utilizing time-series as well as topological persistence to train a student. We investigate whether the mixup method can enhance the performance of topological persistence-based KD using various teachers. Additionally, we compare the performance of using mixup in KD to determine if leveraging both representations yields more benefits than relying solely on time-series data.\nThe contributions of this paper are summarized below:\n\u2022 We analyze the interplay between mixup and KD for wearable sensor data, and compare different strategies in KD with single-teacher and multiple-teacher based distillation, leveraging time-series as well as topological persistence.\n\u2022 We study the effects of mixup on training both teacher and student models. We aim to identify which training strategy for utilizing mixup in KD provides the most benefit in the activity classification task and explore whether the effects of mixup are comparable to those of other time domain augmentation methods in KD.\n\u2022 Through the analysis of multiple strategies for employing mixup with multiple teachers, we propose improved learning approaches by regulating smoothness through temperature and the number of mixup pairs.\nThe rest of the paper is organized as follows. In section 2, we describe mixup and KD techniques with persistence image. In section 3, we explain strategies to leverage topological persistence with mixup in KD. In section 4, we present our experimental results and analysis. In section 6, we discuss our findings and conclusions."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Mixup Augmentation", "content": "Mixup augmentation [28] is used commonly in deep-learning techniques to alleviate issues of memorization and sensitivity to adversarial examples. Two examples drawn at random from training data are mixed by linear interpolation [28]. Let the training data be $D = \\{(x_1, y_1), ..., (x_n, y_n)\\}$, where n is the number of samples. Input data is $x \\in X \\subseteq R^d$ and its corresponding label is $y \\in Y = \\{1,2, ..., K\\}$. The sampling process for mixup can be written as follows:\n$\\tilde{x}_{ij}(\\lambda) = \\lambda x_i + (1 - \\lambda) x_j,$\\\n      $\\tilde{y}_{ij}(\\lambda) = \\lambda y_i + (1 - \\lambda) y_j,$\n(1)\nwhere $\\lambda \\in [0, 1]$ follows the distribution $P_{\\lambda}$ where $\\lambda \\sim Beta(\\alpha, \\alpha)$. $\\lambda$ is to specify the extent of mixing. The hyper-parameter $\\alpha$ controls the strength of interpolation between feature-target pairs. $\\alpha$ generates strongly interpolated samples. To train a function f, the following mixup loss function is minimized:\n$L_{mix}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{n} E_{P_{\\lambda}}[L_{CE}(f(x_{ij}(\\lambda)), y_{ij}(\\lambda))],$\n(2)\nwhere $L_{CE}$ is a standard cross-entropy loss function.\nMany different variants of mixup have been studied [29, 23, 30]. Intrinsically, these methods have similarities in that they mix the input data (e.g. images) and labels proportionally to extend the training distribution. The benefits of mixup with time-series data were explored in previous studies [31, 32, 33]. In this study, we use the conventional mixup to explore the effects on knowledge distillation [28] for time-series data."}, {"title": "2.2. Persistence Image", "content": "TDA has been applied in various fields [4, 34, 35, 36], which can characterize the shape of raw data. One important tool in TDA is persistent homology, which provides a multiscale description with topological features. When applied to point clouds, these features are often described as cavities characterized by points, triangles, and edges by filtration [37, 8]. The extension to time-series data is via sub level-set filtrations, where level-sets are tracked. The birth and death times of topological features can be represented as a multiset of points in a persistence diagram (PD). Since the number and locations of the points in PDs vary depending on the underlying data, it is difficult to use them directly in machine learning pipelines. To project the features on the stable vector representation, a persistence image can be used, mapping the scatter points based on their persistence value (life time) [4]. Firstly, PD is mapped to an integrable function $p : R \\rightarrow R^2$, called a persistence surface (PS), which is defined as a weighted sum of Gaussian functions. A PI can be created by integrating PS on a grid box that is defined by discretization. The values of PI represent the persistence points of the PD. The example of PD and PI are shown in Fig. 1. Even though TDA can provide additional information to the raw time-series to improve performance, it is challenging to run the method on a resource constrained devices, because extracting PIs by TDA requires a large amount of time and memory. To solve this problem, in this paper, we adopt knowledge distillation that distills a single student utilizing the raw time-series data alone as an input."}, {"title": "2.3. Knowledge Distillation", "content": "Knowledge distillation trains a smaller (student) model from a larger (teacher) model [38, 39]. The student model is trained by minimizing the difference between its outputs and soft labels, called relaxed knowledge, from a teacher, which improves performance beyond using hard labels (labeled data) alone. The loss function of standard knowledge distillation [39] is:\n$L = (1 - \\tau)L_{CE}(\\sigma(t_s), y_g) + \\tau L_{KD}(f_T, f_S),$\n(3)\nwhere $t_s$ is logits of a student model $f_s$, $f_T$ is a teacher model, $y_g$ is a ground truth label, $\\sigma(\\cdot)$ is a softmax function, $L_{KD}(\\cdot)$ is KD loss function, and $\\tau$ is hyper-parameter; $0 < \\tau < 1$. The difference between the outputs of the student and the teacher is mitigated by employing Kullback-Leibler divergence loss function, which is described as follows:\n$L_{KD}(f_T, f_S) = \\frac{T^2}{n} \\sum_{i=1}^{n} KL(\\sigma(\\frac{f_T(x_i)}{T}), \\sigma(\\frac{f_S(x_i)}{T})),$\n(4)\nwhere $KL(\\cdot)$ measures Kullback-Leibler divergence loss, T is a hyper-parameter, temperature, to smooth the outputs. To obtain the best performance, in this paper, we utilize a teacher trained by early stopping the training process in KD [40].\nNot only logits, but also features from intermediate layers can be utilized to knowledge transfer, which is called feature-based distillation [41]. Attention transfer (AT) has been widely used, which uses attention maps extracted by a sum of squared attention mapping function [42]. Tung et al. [43] extracts similarities within a mini-batch of samples from a teacher and a student, where those maps have to be matched in distillation process. Even though various techniques have been utilized to improve the performance, they typically address single-modal issues with a single teacher.\nMultiple teachers can be utilized to provide more and diverse knowledge to a single student [3, 41, 44, 45]. Using a uni-modal data with different teachers, a student can establish its own knowledge by integrating diverse knowledge from the teachers [46]. However, in some cases, data samples or labels used for training a teacher cannot be leveraged to train or test a student [41]. Jeon et al. [3] utilize multiple teachers to train a single student by transferring features from both the persistence image and the raw time-series data. Even though two teachers have different architectural designs and use different types of inputs, their logit information can be transferred with KD loss that can be written as:\n$L_{KDm}(f_{T_1}, f_{T_2}, f_S) = \\eta L_{KD}(f_{T_1}, f_S) + (1 - \\eta)L_{KD}(f_{T_2}, f_S),$\n(5)\nwhere $\\eta$ is a hyper-parameter to control the effects from different teachers, and $f_{T_1}$ and $f_{T_2}$ are teacher models trained with time-series data and PIs, respectively. Then, the total loss function can be written as:\n$L_m = (1 - \\tau)L_{CE}(\\sigma(t_s), y_g) + \\tau L_{KDm}(f_{T_1}, f_{T_2}, f_S).$\n(6)\nFor further improvement in KD, mixup augmentation methods have been widely studied. Specifically, mixup and KD share a common thread in serving smoothness during the training process. To accommodate synergetic effects, the interest in the interplay between mixup and KD grows, which has been analyzed in many studies [14, 15, 16, 17, 18]. However, most of the studies were conducted with image data only. It is still required to be explored with time-series and multimodalities using different representations. Based on these insights, we investigate the effects of mixup in KD for time-series on wearable sensor data by utilizing a single or multiple teachers. Also, we present compatible or incompatible views through an empirical analysis."}, {"title": "3. Analysis Strategies for Mixup in KD", "content": "To analyze the effect of mixup in persistence based KD, we utilize different approaches that are explained in this section."}, {"title": "3.1. Leveraging Topological Persistence", "content": ""}, {"title": "3.1.1. Leveraging A Single Teacher", "content": "With the process of standard knowledge distillation, a single teacher trained with PIs can be used to transfer knowledge to a student, as illustrated in Fig. 2(a). PIs are generated by TDA from the raw time-series data. PIs are 2D images, so the teacher model consists of a 2D kernel of CNNs. To train a student with time-series (1D) data, 1D CNNs can be used. Logit of the teacher and student is leveraged to transfer knowledge."}, {"title": "3.1.2. Leveraging Multiple Teachers", "content": "Multiple teachers can be used to train a single student. For instance, two teachers, trained with time-series and PIs, can transfer knowledge simultaneously, as described in Fig. 2(b). The student utilizes time-series alone as an input. In this way, the student can obtain benefits from both of these different features, but it still requires only time-series implementation at test time. Since two teachers are trained with different modalities and have different architectural designs, it is difficult to create a unified model and knowledge gap making performance degradation can be produced [41]. To mitigate this issue, we adopt an annealing strategy that trains a student by initializing weight values from a model learned from scratch [3]."}, {"title": "3.2. Mixup Strategy in KD", "content": "We set different strategies to utilize mixup in KD, as described in Fig. 3. Details are explained as follows.\n\u2022 Mixup for learning from scratch: To investigate the effects of mixup on time-series, we compare mixup- and non-mixup trained models.\n\u2022 Mixup in KD: To explore the connecting link between mixup and KD, we train a student model with mixup and different temperatures, using various methods in KD.\n\u2022 Mixup-trained teacher and student: We apply mixup not only to a student but also to teachers to figure out the effects of the augmentation method in KD. With different combinations of applying mixup, we investigate which strategy is effective in KD.\n\u2022 Distillation with different temperature and partial mixup: To analyze the effects of smoothness from temperature on mixup in KD, a student is trained with the augmentation method and different temperature parameters. In this way, we figure out how much temperature impacts the performance of mixup in KD. Also, to analyze the smoothness of mixup, we utilize partial mixup (PMU) that uses only a few mixup pairs in a batch, as addressed in the previous study [15]. The method uses small amounts of mixup pairs to control the strength of smoothness, which alleviates excessive smoothness.\n\u2022 Mixup for different teachers: Two teachers generate different knowledge and effects for a student in distillation. To explore the effects of mixup for different modalities, we apply different hyper-parameters to teachers. The training objective for the student in KD with multiple teachers and different mixup hyper-parameters is as follows:\n$min E_{(x,y)~D} [\nE_{\\lambda_1~P_{\\lambda_1}} [\\eta\\{(1 \u2013 \\tau)L_{mix}(f_S) + \\tau L_{KD}(f_{T_1}, f_S)\\}]+\nE_{\\lambda_2~P_{\\lambda_2}} [(1 \u2013 \\eta)\\{(1 \u2013 \\tau)L_{mix}(f_S) + \\tau L_{KD}(f_{T_2}, f_S)\\}]],$\n(7)\nwhere $\\lambda_1$ and $\\lambda_2$ are to specify the extent of mixing, whose $\\alpha$ parameters are different.\nIn Table 1, we provide the floating point operations per second (FLOPs) with networks and processing time for an epoch with batch size of 64 in training process for strategies in Fig. 3. The processing time is measured on a desktop with a 3.50 GHz CPU (Intel\u00ae Xeon(R) CPU E5-1650 v3), 48 GB memory, and NVIDIA TITAN Xp (3840 NVIDIA\u00ae CUDA\u00ae cores and 12 GB memory) graphic card. As explained in the table, Strategy (e) takes the longest time and larger complexity compared to other strategies. Through the training, all of strategies distill the same sized single student even though each strategy is different. In test-time, a single student model is implemented alone, which corresponds to the Student.\nMore details of settings and experimental results for each strategy are explained in section 4."}, {"title": "4. Experiments", "content": "In this section, we describe datasets and implementation details. We utilize various strategies of KD and mixup to investigate the effects on wearable sensor data analysis. We analyze optimized solutions and describe ablations."}, {"title": "4.1. Dataset Description and Implementation Details", "content": ""}, {"title": "4.1.1. Dataset Description", "content": "We analyze the strategies with wearable sensor data on GENEActiv and PAMAP2 datasets. These datasets consist with diverse window size and number of channels obtained from multiple sensors on different activities. Thus, experiments on these datasets aid in showing various evaluations under different conditions, which helps to explain generalizability and applicability of methods.\nGENEActiv. GENEActiv dataset [47] was collected by GENEActiv sensor, using waterproof, a light-weight and writ-worn tri-axial accelerometer. The sampling frequency was 100 Hz. By referring to the previous study [48, 3], we select 14 daily activities for analysis, such as walking, standing, and sitting. Each class has over 9 hundred samples with 500 time steps of window size, corresponding to 5 seconds with full-non-overlapping sliding windows. The number of subjects for training and testing is 130 and 43, respectively, and the number of samples is around 16k and 6k, respectively.\nPAMAP2. PAMAP2 dataset [49] was recorded from heart rate, temperature, accelerometers, gyroscopes, and magnetometers, which include 3 Colibri wireless inertial measurement units (IMU). The sampling frequency was 100 Hz for 9 subjects. The recordings are downsampled to 33.3Hz by referring to the previous study [50, 48]. A window size for a sample is 100 time steps or 3 seconds with 22 time steps for segmenting the sequences, which allows semi-non-overlapping sliding windows with 78% overlapping [49]. We use 12 daily activities including lying, sitting, walking, etc. For evaluation in experiments, we use leave-one-subject-out combinations."}, {"title": "4.1.2. Implementation Details", "content": "We use the Scikit-TDA python library [51] and the Ripser package to produce PDs and extract PIs [2]. For GENEActiv, the standard deviation for the Gaussian kernel is set to 0.25 and the birth-time range of PI is [-10, 10], respectively, as do the same in the previous studies [3, 2]. For PAMAP2, the parameter for Gaussian kernel is 0.015 and the range for PI is [-1, 1], respectively. Each PI is generated from each channel and the values are normalized by its maximum intensity value. The size of PI is set to 64\u00d764. For training models, we set the total number of epochs as 200, SGD with momentum of 0.9, a weight decay of $1 \\times 10^{-4}$, and batch size for 64. To train a model with time-series data (1D data), 1D convolutional layers are utilized. The initial learning rate is 0.05 that decreases by 0.2 at 10 epochs and drops by 0.1 every $[\\frac{e}{10}]$ where e is the total number of epochs. A model using image representation for PIs consists of 2D convolutional layers. The initial learning rate is 0.1 that drops by 0.5 at 10 epochs and by 0.2 at every 40 epochs. We measure the performance with WideResNet (WRN) [52] that is popularly utlized in the validation of KD [40, 48, 3]. For default settings, we set $\\tau$, $\\eta$, and T as 0.7, 0.7, and 4 for GENEActiv, and 0.99, 0.3, and 4 for PAMAP2, referring to the previous study [48, 3] and to consider best performance. We run 3 times"}, {"title": "4.2. Preliminary: Effects of Topological Persistence in KD", "content": "In this section, as preliminaries, we conduct experiments with a single and multiple teacher based distillation methods. For multiple teacher based methods, we train models with time-series as well as PIs by leveraging topological persistence. Teachers and students are trained with the various KD strategies explained in the previous section. Note, \"TS\" and \"Ann.\" denote using time-series data to train a student model and using two teachers in KD and"}, {"title": "Leveraging heterogeneous teachers.", "content": "We conducted experiments with heterogenous structure of teachers. As illustrated in Fig. 6, one better teacher does not guarantee a better student, which corroborates the previous studies [40]. Even though teachers have heterogeneous structures, they complement each other to improve the performance,"}, {"title": "4.3. Effect of Mixup in KD", "content": "In this section, we explore effects of mixup for learning from scratch and KD, which provides smoothness in training process. To analyze the interplay of mixup and KD, we utilize response based KD methods, including Base and Ann., which does not require to use additional weights and aids in more prominently showing the effects of interplay with mixup. Firstly, we train a model from scratch with mixup. Secondly, we train a student in KD with mixup. Also, to see the effects of smoothness by temperature in KD, we train students with different temperatures."}, {"title": "Leveraging heterogeneous teachers", "content": "We conducted experiments with heterogenous structure of teachers. As illustrated in Fig. 6, one better teacher does not guarantee a better student, which corroborates the previous studies [40]. Even though teachers have heterogeneous structures, they complement each other to improve the performance,"}, {"title": "4.4. Teacher-Student with Mixup", "content": "To explore the effect of mixup-trained teachers as well as students, we set various combinations of using the augmentations in KD. Note, \u201cT\u201d, \u201cS\u201d, \u201cmT\u201d and \u201cmS\u201d denote a teacher model, a student model, a mixup-trained teacher model, and using mixup to train a student model. As explained in previous sections, WRN16-3 teachers"}, {"title": "4.5. Analysis of the Effects of Smoothness", "content": ""}, {"title": "4.5.1. Analysis of Temperature with Mixup-trained Student", "content": "In previous sections, we observed that both temperature and mixup inject smoothness into KD training process. To investigate the compatibility of smoothness with temperature and mixup, we evaluate KD with time-series data (TS+KD) and Ann. with different temperature parameters. The results of GENEActiv is illustrated in Fig. 9. For TS+KD, when T is 1, with mixup improves the performance, implying that injecting smoothness can aid for training a student in KD. For both KD with time-series and Ann, in without mixup cases, it shows the best when T is 4 for"}, {"title": "4.5.2. Partial Mixup", "content": "To control the effects of smoothness on training procedures, we use PMU to alleviate excessive smoothness, which can degrade performance. We utilize different amounts of mixup pairs such as 0%, 10%, 50%, and 100%, where 0% means mixup is not applied and 100% denotes all samples of mixup pairs are used for training (FMU). Mixup is applied when a student is trained. As described in Table 8, when teacher models are WRN16-3, less amounts of mixup pairs can distill a better student. When teacher models are WRN28-1, 50% of PMU shows the best. In Table 9, for PAMAP2, FMU shows the best. However, for WRN28-1, PMU with 10% of Ann. distills the best student. These results show that fewer mixup pairs can generate better performance. Also, if complexity of a dataset is high, mixup pairs contributes more to improving performance. On the other hand, KD with time-series data and Ann. have different optimal proportions of mixup pairs. This may be because Ann. uses both representations, including both time-series with 1D data and topological representations with 2D data, for training. Mixup influences different representations differently, so utilizing two teachers can provide more diverse relaxed knowledge for distillation, which is different from using one single teacher."}, {"title": "4.6. Mixup for Different Teachers", "content": "Since two teachers can provide different effects on distillation, we use different hyper-parameters for mixup to knowledge transfer from two teachers when a student is trained in KD. We utilize Ann. that shows the best in most of the cases presented in the previous sections. Note, $\\alpha_1$ and $\\alpha_2$ are hyper-parameters of mixup for Teacher1 and Teacher2. As summarized in Table 10 and 11, applying different mixup hyper-parameters can distill a better student. As depicted in Table 12 and 13, we evaluate with different teachers having different architectural designs of depth and width for networks. Mix. denotes applying mixup for training a student. $\\alpha$ of mixup is 0.1. When $\\alpha$ is applied differently for teachers (diff. $\\alpha$), ($\\alpha_1$, $\\alpha_2$) is (0.15, 0.2) for GENEActiv and (0.1, 0.15) for PAMAP2. In all cases, applying different mixup hyper-parameters can distill a better student.\nTo figure out if using different mixup hyper-parameters can complement the partial mixup method, we apply different proportions of mixup pairs for training a student with different mixup hyper-parameters. In Table 14, FMU shows the best for both cases of teachers. With small proportions of mixup pairs, a large degradation of performance"}, {"title": "4.7. Analysis of Optimized Solution", "content": ""}, {"title": "4.7.1. Parametric Plots", "content": "A solution space comparison for two models can give a valuable understanding of their behavior in training or testing and how these models are related. One of the useful tools for the analysis is the parametric plot that has been widely studied [66, 67, 68]."}, {"title": "4.7.2. Mixup Hyper-parameter $\\alpha$", "content": "To explore the performance on $\\alpha$ of mixup and its sensitivity, we train various models with learning from scratch, KD, and Ann. using different settings of $\\alpha$, which is described in Table 16. The optimal $\\alpha$ parameters for models trained with time-series and topological persistence are different. When a value is between the optimal one of TS and PI ($\\alpha \\in [0.1, 0.4]$), Ann. performs better than training with the other value ($\\alpha$ = 0.05). Therefore, setting the proper $\\alpha$ leads to getting the best performance, and an intermediate $\\alpha$ can generate the best performance when different teachers are applied."}, {"title": "5. Discussion", "content": "We explored the interplay between mixup and KD on diverse strategies with multimodal representations including topological features for wearable sensor data analysis. To achieve more improved synergistic effects, partial mixup can be utilized, which prevents excessive smoothing effects that generate degradation. As an extended research, these strategies introduced in this paper are applicable to diverse computer vision tasks [69, 70], such as image recognition, object tracking and detection, and segmentation. For example, when a model for image recognition is trained with our strategy, the trained model can be utilized as a backbone model in a framework for many different computer vision tasks. Also, this study can be explored on vision based or different types of sensor signal, using motion capture or ECG, based human activity recognition. These can be more investigated as a future work."}, {"title": "6. Conclusion", "content": "In this paper, we explored the role of mixup in topological based KD with different approaches. We confirmed that mixup and temperature in KD have a connecting link that imposes smoothness for training process. Excessive smoothness produced inferior supervision that hinders training a student in KD. We observed that utilizing topological features can complement time-series to improve the end performance. Also, using topological persistence showed better compatibility when using mixup in KD.\nFurther, two teachers transfer different statistical knowledge so that their optimal parameters for augmentation in distillation can be different, where teachers are trained with time-series and topological features, respectively."}]}