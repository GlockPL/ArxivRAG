{"title": "LOGO \u2013 LONG CONTEXT ALIGNMENT VIA EFFICIENT PREFERENCE OPTIMIZATION", "authors": ["Zecheng Tang", "Zechen Sun", "Juntao Li", "Qiaoming Zhu", "Min Zhang"], "abstract": "Long-context models (LCMs) have shown great potential in processing long input sequences (even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO (Long context aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8\u00d7A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid advancements of Large Language Models (LLMs), handling long contexts (even more than 100M tokens (anthropic, 2024)) has become a fundamental capability for recent LLMs. This further unlocks the potential of LLMs for novel tasks and applications, e.g., code analysis (Zhu et al., 2024), while simultaneously eliminating the need for complex toolchains and intricate workflows that were previously required to overcome the context-length constraints (Ravaut et al., 2024).\nYet, recent studies have pointed out that these long-context models (LCMs) failed to achieve sat-isfactory performance in long-context tasks, where LCMs might produce misaligned results, such as instruction unfollowing and hallucinations (Belyi et al., 2024; Zhang et al., 2024a). To mitigate the above issue, the open-source community has made significant efforts, primarily focusing on building high-quality long instruction data and extending the data size (Wu et al., 2024a; Bai et al., 2024; Fu et al., 2024; Bai et al., 2024). As shown in Fig. 1, though achieving meaningful improve-ment, these methods fall short in effectiveness or efficiency. For instance, the Llama-3.1-8B-128K model AI@Meta (2024a) was pre-trained on around 300B long instruction data, but it even underper-forms the Llama-3-8B-Instruct-80K model (Zhang et al., 2024b), which was post-trained with 1.5B high-quality long instruction data based on the Llama-3-8B-Instruct model (AI@Meta, 2024b). As for the Llama-3-8B-Instruct-80K model, it shows slight improvement compared to the baseline and still lags greatly behind the closed-source counterparts like GPT-4 (Achiam et al., 2023).\nRecently, Wu et al. (2024b) pointed out that LCMs can accurately locate token-level salient infor-mation within the context. As shown in Fig. 1(b), we visualize the information retrieval capability\u00b9 (reflected by the retrieval score) and the generation capability (reflected by the recall score) of different LCMs on the synthetic retrieval task, where we can observe a minimal difference among the retrieval scores from various LCMs, but large differences in their generation performance. This suggests that while LCMs are adept at identifying key information within long contexts, they strug-gle to effectively utilize the retrieval information for generation. The underlying cause might be the commonly used training approach of LCMs, which relies on token-level maximum likelihood loss, i.e., Cross-Entropy (CE) loss, calculated on both the context and the predictions. Given that the con-text's sequence length is typically much longer than the prediction portion, the feedback signal (CE loss) from the prediction is often overshadowed by that from the context. As a result, the CE loss becomes ineffective in optimizing the generation capabilities of LCMs.\nTo effectively optimize LCMs for generating desired outputs and avoid misaligned results, this pa-per introduces LOGO (Long cOntext aliGnment via efficient preference Optimization), the first training strategy that incorporates preference optimization for long-context alignment. There are two key components in LOGO: (1) a training objective designed to guide LCMs to distinguish be-tween preference predictions (i.e., correct outputs) and dis-preference predictions (e.g., misaligned outputs like hallucinations), and (2) a corresponding data construction pipeline that only involves open-source models. It is worth noting that training with long sequence data is a memory-intensive task (Dao, 2023) and the DPO algorithm also has a high GPU memory demand. To overcome the GPU memory-bound and improve the training efficiency, LOGO adopts a reference-free training ob-jective and the positional indices synthesis method (Zhu et al., 2023). Consequently, we can perform the LOGO training with only 0.3B data on a single 8\u00d7A800 GPU machine within 16 hours.\nBy training with LOGO, LCMs can achieve significant improvements in real-world tasks and gain moderate improvements in synthetic and language modeling tasks, as well as maintaining good per-formance on the short-context tasks, e.g., MMLU (Hendrycks et al., 2020). As shown in Figure 1(a), our Llama-3-8B-LOGO significantly outperforms GPT3.5-Turbo in real-world tasks and approaches the performance of some top closed-source models like GPT-4. Additionally, LOGO can also gen-eralize to the training of short-context LLMs such as Llama-2-7B-Chat-4K (Touvron et al., 2023), which can potentially extend their context window size up to 8 times (e.g.,32K context window size for Llama-2-7B-Chat-4K) while simultaneously enhancing their performance substantially."}, {"title": "2 RELATED WORK", "content": "Two steps are essential for empowering LLMs with the ability to handle long-context tasks: 1) con-text scaling, which expands the limited context window size to support long-context tasks, e.g., from 8k to 128k; and 2) long-context alignment, which ensures that LCMs can follow long instructions. Currently, the open-source community mainly focuses on the former, primarily by (1) post-training models on long instruction data (Chen et al., 2023b; Xiong et al., 2023; Fu et al., 2024; Zhang et al., 2024b), (2) devising novel model architectures (Yang et al., 2023; Zhang, 2024; Tworkowski et al., 2024), and (3) modifying positional encoding (Peng et al., 2023; Chen et al., 2023a; Jin et al., 2024) to extend the context window of LLMs. However, current works (Belyi et al., 2024; Hsieh et al., 2024; Zhang et al., 2024a) indicated that LCMs still underperform in long-context tasks, frequently manifesting issues such as hallucinations and failure to follow instructions, despite possessing large context window size. To mitigate this issue, Bai et al. (2024) and Wu et al. (2024a) proposed to align the LCMs in long-context scenarios by synthesizing long-dependency instruction data to fine-tune the models. Some LLMs are even pre-trained with massive long instruction data (Jiang et al., 2023; Dubey et al., 2024; Abdin et al., 2024). Yet, despite numerous attempts that have been made to improve the data quality and quantity, the performance of open-source LCMs still lies far behind close-source LCMs. Therefore, focusing solely on data augmentation methods can not re-solve the long-context alignment problem efficiently and effectively. In this work, we address the above issue from the training objective perspective. Building upon the language modeling task, we introduce LOGO, which contains a long-context preference optimization training objective. Experimental results demonstrate that, with a small amount of data and computational resources, LOGO can significantly enhance the generation capability of LCMs."}, {"title": "2.2 MODEL ALIGNMENT WITH DIRECT PREFERENCE OPTIMIZATION", "content": "Direct Preference Optimization (DPO) (Rafailov et al., 2024) is a widely adopted RLHF algo-rithm (Ouyang et al., 2022) that aims to align models with human preferences. Compared to other reinforcement learning methods, e.g., PPO (Schulman et al., 2017), DPO can achieve strong performance while eliminating the need for a separate reward model. Unlike Supervised Fine-Tuning (SFT), which guides LLMs to fit predictions to ground truth at the token level, DPO up-dates the model parameters with discrete evaluation scores. Specifically, DPO teaches the model to \"reject\" misaligned responses and \"accept\u201d preferred responses with differently assigned prediction scores. Significant efforts have been made to enhance the effectiveness and efficiency of DPO, such as CPO (Xu et al., 2024), TPO (Saeidi et al., 2024), and ORPO (Hong et al., 2024). Among them, SimPO (Meng et al., 2024) utilizes the average log probability of a sequence as the implicit reward, which better aligns with the generation tasks and eliminates the need for a reference model."}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 BACKGROUND", "content": "Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO) DPO is one of the most popular offline preference optimization strategies in RLHF (Rafailov et al., 2024). Given prompt x, DPO aims to maximize the likelihood of a preferred response yw over a dis-preferred one y\u0131, thereby preventing the model from generating undesired content. There are three essential modules in the DPO training process: one reference model and one policy model for cal-culating the DPO loss jointly, and one evaluation strategy (or evaluation model) for distinguishing between yw and yr. SimPO (Meng et al., 2024) is an improved variant of DPO, which employs an implicit reward formulation that directly aligns with the generation metric, e.g., PPL, thereby eliminating the need for a reference model. The training objective of SimPO can be written as:\n$\\mathcal{L}_{SimPO}(\\pi_{\\theta}) = -\\mathbb{E}_{(x,y_w,y_l)} \\log \\sigma\\left[\\frac{\\beta}{\\gamma}\\left(\\log \\pi_{\\theta}(y_w|x) - \\log \\pi_{\\theta}(y_l|x)\\right) - \\gamma\\right]$ (1)\nwhere \u03c0\u03b8 is the policy model (model to be optimized), \u03b2 (scaling of the reward difference) and \u03b3 (tar-get reward margin) are the hyper-parameters to separate the preferred and dis-preferred responses."}, {"title": "Efficient Context Scaling with Positional Indices Synthesis", "content": "Transformer-based models rely on positional indices to identify the relative position of each token (Raffel et al., 2020). One effi-cient method to extend the data context length is modifying the positional indices to simulate long-sequence inputs without altering the real input sequence (Press et al., 2021; Ruoss et al., 2023). By default, the positional indices of a sequence of length k are P(k) = {0,1,...,k-1}. To extend the sequence length from k to K, we can synthesize the positional indices: PB(K) = {0+ bo, 1+b1,\u2026\u2026\u2026, k \u2212 1 + bk\u22121}, where B = {bo,b1,\u2026\u2026,bk\u22121} is the positional bias applied to each original position index and k \u2212 1 + bk\u22121 = K. To ensure effectiveness, the synthesis of posi-tion indices should achieve a uniform distribution of relative distances within the extended sequence length [0, K] and cover as many of the extended positional indices as possible (Wu et al., 2024a)."}, {"title": "3.2 LONG-CONTEXT ALIGNMENT WITH LOGO", "content": null}, {"title": "3.2.1 TRAINING OBJECTIVE OF LOGO", "content": "In long-context scenarios, LCMs are prone to generating various misaligned responses, such as hallucinations and failing to follow instructions (Belyi et al., 2024). However, there is a lack of effective strategies (or models) to detect these misaligned outputs, posting a great challenge for selecting preference and dis-preference samples in preference optimization (we will elucidate this in Appendix C, where we also show the misalignment cases). Therefore, instead of finding one dis-preference instance with a specific error pattern, we can expand the dis-preference space to push the model away from a range of possible dis-preference instances. We design the loss function based on SimPO (Eq. 1), as it is more aligned with the generation tasks and free of the reference model, which is efficient for long-context training. The training objective can be written as:\n$\\mathcal{L}_{LOGO}(\\pi_{\\theta}) = -\\mathbb{E}_{(x,y_w,\\{y_l^{(j)}\\}_{j=1}^M)} \\log \\sigma\\left[\\frac{\\beta}{\\gamma}\\left(\\log \\pi_{\\theta}(y_w|x) - \\frac{\\beta}{M}\\sum_{j=1}^M\\log \\pi_{\\theta}(y_l^{(j)}|x)\\right) - \\gamma\\right]$; (2)\nwhere M is the number of dis-preference instances.\nFurthermore, to avoid reward hacking phenomenon (Yuan et al., 2024; Hong et al., 2024) as well as preserve the modeling capabilities of LCMs, we add an SFT regularization term in Equ 2. This regu-larization term serves to prevent the policy model \u03c0\u03bf from drifting away from its original capabilities acquired through SFT. The final loss function of LOGO can be written as:\n$\\mathcal{L}_{LOGO}(\\pi_{\\theta}) = \\mathcal{L}_{LOGO}(\\pi_{\\theta}) + \\lambda \\mathbb{E}_{(x,y_w)}\\log \\pi_{\\theta}(y_w|x))$, (3)\nwhere A is the hyper-parameter that controls SFT regularization term."}, {"title": "3.2.2 TRAINING DATASET CONSTRUCTION OF LOGO", "content": "To perform the LOGO training, we introduce a tailored LOGO dataset construction pipeline. For each long-context sample, we can format it as a triplet X = {Q,C,P}, where Q, C, and P represent the question, reference context, and the model prediction, respectively. As shown in Fig. 2, to construct training data for LOGO, we first divide the context C into equal-length chunks {C1, C2,..., Cn}. Then, three steps are involved: (1) Importance Scoring with Automatic Evalua-tor, (2) Preference and Dis-preference Data Synthesis, and (3) Positional Indices Synthesis.\nImportance Scoring with Automatic Evaluator To construct preference (aligned) and dis-preference (misaligned) data in long-context scenarios, an efficient method is to guide the model to respond based on different contexts. Specifically, to construct the preference data, we only pro-vide the model with context relevant to the question, thus enhancing the fidelity of the model's output by reducing contextual interference (Shi et al., 2023). Conversely, we can add more irrel-evant context to guide the model in generating misaligned content like hallucinations. To find the relevant chunks Ci within the context, we utilize an automatic evaluator Eval(.) to calculate the \"contribution\" of each chunk Ci to the question Q. Specifically, we utilize an Eval(.) to identify all the entities within a chunk C\u2081. The more overlapping entities Ci shares with the question Q, the greater its influence on the final prediction, allowing us to assign a higher score to this chunk. With Eval(), we efficiently assign importance scores S = {$1, $2,\u00b7\u00b7\u00b7, sn} to all the chunks.\nPreference and Dis-preference Data Synthesis To construct preference and dis-preference data based on the model prediction P, we select and combine the chunks mentioned above to create"}, {"title": "Positional Indices Synthesis", "content": "Given that each LOGO training sample includes (M + 1) instances, with one preference instance and M dis-preference instance, a long context length of C' can easily lead to GPU memory overflow (even on GPUs with 80GB memory). To address this, we employ a positional encoding synthesis strategy. By assigning different synthetic positional indices to each chunk, we can simulate long-sequence training data with short context data (Wu et al., 2024a). Specifically, to ensure that the synthetic positional indices do not disrupt the semantic structure of short context, the positional indices within each chunk should be continuous, while indices between adjacent chunks can be discrete, i.e., omitting certain positional indices (as shown in sub-Fig. 3 in Fig. 2). Given N equal-length chunks within each sample2, to achieve a uniform distribution of relative distance within the expanded context length [0, K], each positional bias term bi \u2208 B should be sampled from a uniform distribution. The synthetic positional indices can be written as:\n$P_B(K) = \\{i + b_i\\}_{i=0}^{|C_i|-1}, \text{where } b_i \\sim \\mathcal{U}(1, (i \\mod |C_i|) \\times (K - k)/N),$ (4)\nwhere (i mod Ci) indicates the chunk index where the current positional index i resides, and (K-k)/N represents the expansion size for each chunk."}, {"title": "4 EXPERIMENT", "content": null}, {"title": "4.1 SETTINGS", "content": "LOGO Dataset Construction We construct the LOGO datasets based on two corpora: (1) 4,000 instances sampled from long-llm-data\u00b3 (Zhang et al., 2024b), which includes reference contexts from multiple domains (e.g., biography, paper, etc.) and questions generated by GPT-4, covering tasks such as Single-Detail QA, Multi-Detail QA, and Summarization; (2) 2,000 instances sampled from RedPajama (Computer, 2023) to mitigate forgetting, where we prompt the open-source LCM Qwen2-70B-Instruct (Yang et al., 2024) to generate questions for each instance. Then, we split each instance into equal-length chunks, with each chunk containing 512 tokens. To construct preference and dis-preference data, we use the spaCy model\u2074, a named entity recognition (NER) model that can identify all the entities within a context, as the evaluator Eval(\u00b7). We use the number of overlapping entities between each chunk Ci and the question Q as the importance score. We set the threshold \u03b4 as 6, and chunk number N as 16, i.e., selecting and combining 16 chunks as the reference context for training. As for the number of dis-preference instances in the LOGO training objective, we set M = 2, i.e., each training sample includes one preference instance and two dis-preference instances. Then, we apply Eq. 4 to construct positional indices for each instance within each sample. Specifically, we adopt two different sampling strategies on positional bias B to ensure that all positional indices are uniformly covered and maintain the semantic structure of the context (see Appendix D for more details). After positional indices synthesis, we have a total number of 12,000 training samples, with a total data size of approximately 12,000\u00d7512\u00d716\u00d73~0.3B tokens.\nTraining Settings To improve the training efficiency while preserving the inherent capabilities of the LLMs, we freeze the backbone model and apply LoRA (Hu et al., 2021) method, which only fine-tunes the attention and token embedding modules, to perform training. Additionally, thanks to positional indices synthesis, LOGO can potentially scale the context length and ensure align-ment in long-context tasks simultaneously. Therefore, we experiment with two type of models: (1) Short-context Models (SCMs) including Llama-2-7B-Chat (Touvron et al., 2023) and Llama-3-8B-Instruct (AI@Meta, 2024b), which own context lengths of 4K and 8K, respectively; and (2) Long-context Models (LCMs), including Llama3-8B-Instruct-80K (Zhang et al., 2024b), Llama-2-7B-Instruct-80K (Fu et al., 2024) and Mistral-Instruct-7B-V0.2 (Jiang et al., 2023), which inher-ently have long context windows. For SCMs, given that excessive scaling with positional indices synthesis method can result in the missing of some positional indices, potentially impacting model performance, we scale the context windows of SCMs to 8 times of their original context length. For LCMs, we maintain their original context length. To accelerate the training process and save GPU memory, we adopt DeepSpeed Zero 3 (Aminabadi et al., 2022). All the experiments are conducted on a 8\u00d7A800 (80GB) GPU machine, and the training is completed within 16 hours. For the setting of hyper-parameters \u03b2 and \u03b3 in Eq. 2, we adhere to the recommendations provided in Meng et al. (2024) for different models, where \u03b2 = 10, \u03b3 = 3 for Llama-3-8B-based model, \u03b2 = 2.5, \u03b3 = 0.25 for Mistral-Instruct-7B-V0.2-based model, and \u03b2 = 3, \u03b3 = 0.6 for Llama-2-7B-based model. We set x = 0.1 in Eq. 3 for SFT regularization to stabilize the training process of LOGO and prevent the reward hacking phenomenon mentioned above."}, {"title": "4.3 PERFORMANCE ON SHORT-CONTEXT TASKS", "content": "To investigate whether LOGO training affects model performance on short-context tasks, we select three widely used benchmarks for assessing LLMs' foundational capabilities that possess short input sequence: MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2021), and ARC (Hard and Easy) (Clark et al., 2018). As illustrated in Fig. 5, we find that LOGO not only preserves the LLM's inherent capabilities on short-context tasks but also demonstrates improvements in some specific tasks. This is because LOGO aims to teach the model to generate responses based on the context rather than fabricating results (such as producing hallucinations), which is equally applicable to short-context tasks. We can also find that scaling context length with LOGO yields better results than instruction tuning. For instance, as demonstrated in the TruthfulQA task, Llama-3-8B-Instruct-80K shows significant performance degradation compared to the Llama-3-8B-Instruct-8K-LOGO (64K). Such a phenomenon indicates a high \u201calignment tax\u201d paid from instruction tuning (Fu et al., 2023)."}, {"title": "5 ABLATION STUDY", "content": "For ablation studies, we experiment with the Llama-3-8B-Instruct-80K model, which demonstrates strong baseline performance across the various tasks. We conduct experiments on the real-world tasks by reporting the average score on LongBench (denoted with LB), and the language modeling task by calculating the PPL score on the PG-19 testing set with a 64K context length. In Sec. 5.1, we analyze the impact of different hyper-parameters in the LOGO training objective. In Sec. 5.2, we discuss the impact of synthetic data of varying lengths. In Sec. 5.3, we compare LOGO with SFT by visualizing LCM's generation and information retrieval capabilities along the training phase."}, {"title": "5.1 ANALYSIS OF LOGO TRAINING OBJECTIVE", "content": "Effect of SFT Regularization Term A To investigate the SFT regularization term in Equ. 3, we adjust the value of A to control the SFT regularization term. As depicted in Fig. 6(a), we can observe that increasing A enables the model to achieve a lower PPL score. For real-world tasks, the impact of SFT regularization on the final results is minimal. For example, for settings (M = 2,\u5165 = 0.1, Ctx. = 8K), (M = 2, \u03bb = 0.5,Ctx. = 8K), and (M = 2, x = 1.0, Ctx. = 8K), we can observe that as A gradually increases, the PPL significantly decreases, with a difference of nearly 3.5 points, while the average score on LongBench only differs by around 1.5 points."}, {"title": "5.2 EFFECT OF SYNTHETIC DATA LENGTH", "content": "We study with two settings of synthetic data length, i.e., from real input length 4K to target length 64K (Ctx. = 4K) and from real input length 8K to target length 64K (Ctx. = 8K). Specifically, the chunk size |Ci | remains unchanged, while we set the number of chunks as 8 and 16 for the above two settings, respectively. As shown in Fig. 6(a), short-context synthetic data length significantly diminishes the model's performance on both the language modeling task and real-world tasks (data point (M = 2, \u03bb = 0.1, Ctx. = 4K) versus data point (M = 2, \u03bb = 0.1, Ctx. = 8K)), but can still overcome the instruction tuning method (42.8 average score on LongBench) and effectively reduces the GPU memory requirement during training (Fig. 6(c)). This is because when the original context length is relatively small (4K), it requires scaling up by a larger factor (16 times) to reach the desired context length (64K). During the positional indices synthesis process, some positional indices may miss or be infrequently activated, thereby impacting performance."}, {"title": "5.3 COMPARISON BETWEEN SFT AND LOGO", "content": "As shown in Fig. 7, we illustrate the impact of SFT (with two loss calculation strategies follow-ing (Xiong et al., 2023)) and LOGO on the model's generation and understanding performance throughout the training process. We plot the trends of retrieval score (understanding ability) and re-call score (generation ability) along the training progress. We can observe that applying SFT loss to the entire sequence leads to a gradual decline in the LCM's understanding ability, accompanied by performance fluctuations; while applying SFT loss solely to the prediction portion shows no signifi-cant improvement in model performance. Nevertheless, applying LOGO can steer LCMs away from misaligned samples, thereby enhancing the recall score. Simultaneously, it improves comprehension abilities, enabling the model to retrieve more key information within the context."}, {"title": "6 CONCLUSION", "content": "In this paper, we find that commonly used training approaches for LCMs may degrade the model's generation capabilities, leading to misaligned outputs, such as hallucinations and instruction unfol-lowing. To mitigate this issue, we introduce LOGO, a novel preference optimization training strat-egy for long-context alignment. Specifically, LOGO has two key components: (1) a reference-free preference optimization objective that teaches the model to distinguish between the preference and the dis-preference predictions, and (2) a data construction pipeline tailored for the training objective, both of which are designed to ensure the training efficiency and effectiveness. By performing LOGO training on a single 8\u00d7A800 GPU machine within 16 hours, LCMs can achieve great improvements in long-context tasks while maintaining their inherent capabilities. Besides, LOGO can also poten-tially scale the context length of short-context models and achieve better generation performance compared to other frequently used context scaling methods."}, {"title": "A LIMITATION AND FUTURE WORK", "content": "This paper presents an efficient preference optimization training strategy (LOGO) tailored for long-context alignment. However, there are several limitations:\n\u2022 Due to resource constraints within the academic community, the evaluation of real-world testing sets in LongBench may be affected by the varying prompts selected by different studies, which can lead to significant discrepancies in results. Consequently, we are unable to directly replicate the results from other works\n\u2022 As mentioned in the main body (Sec. 3.2.2), there remains a lack of suitable evaluation models to assess whether the outputs of LCMs are accurate or contain hallucinations. The LOGO training objective proposed in this paper still has room for improvement.\n\u2022 During the data construction phase, utilizing higher-quality datasets could yield better out-comes. However, as an academic paper, we believe we have demonstrated the generaliz-ability of our method through the main experiments.\nMoving forward, we plan to continue our research along the lines of efficient long-context align-ment, particularly in algorithm development. We aim to explore the integration of more effective evaluation strategies, such as RAG checkers (Ru et al., 2024), to assist in constructing preference and dis-preference instances. Additionally, we should investigate how to enhance the efficiency of our LOGO data construction pipeline across various tasks and domains.\nIn summary, this paper highlights the substantial potential of efficient training in long-context sce-narios, and we hope our work will provide valuable insights for future research endeavors."}, {"title": "B DETAILS OF EXPERIMENTS IN INTRODUCTION", "content": "In this section, we introduce the preliminary studies in the Introduction section, including the exper-imental settings, task definitions, and retrieval score calculation.\nExperimental Settings In Fig. 1(a) and Fig. 1(b), we evaluate the model performance on the subsets in LongBench (Bai et al., 2023), including Single Document QA, Multi-Document QA, Summarization, and Few-shot tasks. For each long-context model, we utilize the same official in-structions to guide the model prediction."}, {"title": "Multi-values Needle-in-a-Haystack", "content": "In Fig. 1(c), we calculate the retrieval score on the Multi-values Needle-in-a-Haystack dataset, which requires LCMs to recall multiple values within the con-text. We provide an example in Fig. 8:"}, {"title": "C DESIGN OF LOGO TRAINING OBJECTIVE AND ERROR PATTERN DEFINITION IN LCMS", "content": "Misaligned predictions generated from LCMs can be specifically categorized into two types: failing to follow instructions and generating hallucinations. In Fig. 9, we illustrate these two error patterns. Specifically, we define different error patterns by utilizing the degree of overlap between entities in the responses and the questions, along with specific templates:\n\u2022 Instruction Unfollow: the entities in the model's responses do not overlap with the entities in the question.\n\u2022 Hallucination: there is a partial intersection of entities between the model's responses and the question, and the entities in the response coincide with the main subject of the question.\nIt is worth mentioning that merely utilizing Named Entity Recognition (NER) models and rule-based methods proves inadequate for identifying these patterns. Instead, a more robust evaluation involving strong LLMs such as GPT-4 or human assessment is required to accurately identify these patterns. Consequently, in the design of the LOGO training objective, we do not confine to con-structing cases with specific error patterns. Therefore, instead of finding one dis-preference instance with a specific error pattern, we can expand the dis-preference space to push the model away from a range of possible dis-preference instances."}, {"title": "D POSITIONAL INDICES SYNTHESIS DETAILS", "content": "We visualize the positional indices synthesis process in Fig. 10. Specifically, to ensure that the syn-thesized positional indices do not disrupt the original text's semantic structure while maximizing the extended context size, we employ two different strategies for positional bias B: Continuous Chunk Positional Indices Synthesis (Fig. 10(a)) and Sparse Chunk Positional Indices Synthesis (Fig. 10(b)). For Continuous Chunk Positional Indices Synthesis, the positional bias within the same chunk is consistent. For instance, in the first chunk Co, the positional bias {bo, b1,\u00b7\u00b7\u00b7,b|c\u2081|} are the same value sampled from distribution U(1, (K \u2013 k)/N). This ensures that the semantic structure within"}]}