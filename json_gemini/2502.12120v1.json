{"title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws", "authors": ["Prasanna Mayilvahanan", "Thadd\u00e4us Wiedemer", "Sayak Mallick", "Matthias Bethge", "Wieland Brendel"], "abstract": "Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.", "sections": [{"title": "1. Introduction", "content": "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020; Hoffmann et al., 2022; Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens. While useful for pretraining, scaling laws do not capture how well a model ultimately performs on downstream tasks (Gadre et al., 2024; Schaeffer et al., 2024; Du et al., 2025). Consequently, multiple works have begun to investigate downstream scaling laws: Scaling laws that directly predict downstream loss from FLOPs (Schaeffer et al., 2024; Gadre et al., 2024)."}, {"title": "Loss-to-Loss Scaling Laws", "content": "Loss-to-loss scaling laws aim to improve the transferability of scaling insights between training setups by examining the relationship between training (or validation) and test losses, between different validation losses, or between different test losses (Brandfonbrener et al., 2024). This perspective is crucial for several reasons. First, train-to-train (or validation-to-validation) scaling implies how scaling laws transfer across datasets (Brandfonbrener et al., 2024). Second, incorporating train-to-test (or validation-to-test) scaling laws alongside compute-to-train scaling laws provides more precise insight into how compute budgets translate to downstream performance and can help study emergent abilities of models (Du et al., 2025). Third, while compute-to-loss scaling laws often target a single downstream task or average task performance, train-to-test and test-to-test scaling laws can help tune a model's performance across diverse tasks, e.g., to foster the development of generalist LLMs with a balanced task performance."}, {"title": "3. Fitting Loss-to-Loss Scaling Laws", "content": "We focus our analysis on train-to-train and train-to-test scaling. Combined with known compute-to-train scaling laws, these loss-to-loss scaling laws paint a complete picture of a model's downstream performance given a compute budget and characterize a model's downstream performance distribution across tasks (Brandfonbrener et al., 2024)."}, {"title": "", "content": "Brandfonbrener et al. (2024) predict train-to-train and train-to-test scaling laws to follow a shifted power law\n$L_y (f_{N,D}) \\approx K \\cdot (L_x (f_{N,D}) - E_{xlp} )^\\kappa + E_{ylp} \\text{ (1)}$\nwhere $L_x, L_y$ are the losses on datasets $D_x, D_y$ shown on the x- and y-axis. $f_{N,D}$ is a model trained with N parameters on D tokens on the pretraining set $D_p$, and $K$ and $\\kappa$ are parameters to be fit. $E_{xlp}, E_{ylp}$ are the irreducible errors (i.e., minimum loss) that $f_p$ trained on $D_p$ can achieve on the datasets $D_x, D_y$."}, {"title": "4. A Causal Analysis of Loss-to-Loss Scaling", "content": "We now perform interventions on the model and training configurations to find what factors cause the exact shape of loss-to-loss scaling laws.\nOur basic procedure is outlined in Fig. 3. As mentioned in \u00a72, our approach is motivated by similar studies in the robustness literature. In contrast to that setting, we here lack paired in-distribution and out-of-distribution datasets. Instead, we simply consider all combinations of validation and test sets. For ease of visualization when intervening on the pretraining data, we always show FineWeb-Edu validation loss on the x-axis, even for models trained on different pretraining distributions. This choice is arbitrary and does not affect our results; see App. C. Similarly, we here report results for scaling laws of average validation and test loss; results for individual losses can be found in App. D.\nFor our analysis, we consider the impact of pretraining data, tokenizer, architecture, model size, context length, and optimizer settings."}, {"title": "4.1. Pretraining Data, Tokenizer, and Architecture", "content": "First, we jointly examine the effect of pretraining data, architecture, and tokenizer. Since we face limited compute"}, {"title": "4.2. Model Size, Context Length, and Optimization", "content": "We now examine the effect of other common design decisions, such as the number or width of layers, the context length, optimizer, learning schedule, learning rate, and weight decay. In contrast to \u00a74.1, we can perform these interventions separately since we can compare among our"}, {"title": "5. Discussion and Future Work", "content": "Our findings add to the understanding of loss-to-loss scaling laws and reinforce prior results from vision and vision-language research (Taori et al., 2020; Fang et al., 2022) on the importance of choosing the pretraining data."}, {"title": "Implications for Optimizing Downstream Performance", "content": "Our results emphasize that the data distribution is the key for achieving a desireable loss-to-loss scaling and a in turn achieve a great downstream performance. Conversely, since architecture has little impact on the train-to-test conversion, it can be freely optimized for better compute scaling without affecting downstream scaling or performance."}, {"title": "Implications for Balancing Performance", "content": "If the aim is not only optimal average downstream performance but also a specific weighting between different tasks, e.g., to ensure a balanced downstream performance, individual train-to-test scaling laws can be used to tune a model's performance. Here, too, the pretraining data has the largest impact and practitioners should thus consider the final application of their model already during the data curation stage. Ultimately, our findings underscore that pretraining data curation, rather than architectural innovation, can be the primary driver in developing robust, generalist models."}, {"title": "On Architectural Biases", "content": "The limited impact of even drastically different architectures on loss-to-loss scaling behavior illustrated in \u00a74.1 and Fig. 6 suggest that architectures trained on the same data may implicitly learn highly similar representations. This might seem intuitive, as all models minimize the same loss function. One might expect them to converge toward comparable solutions when the training loss approaches zero (Roeder et al., 2020). However, even checkpoints of our smaller models, when trained on fewer tokens, follow the same scaling across architectures. Understanding whether this implies representational and behavioral similarity remains an intriguing open question. Beyond this, it remains to be seen whether it is possible"}, {"title": "On New Training Paradigms", "content": "Our study intentionally focuses on models trained with standard loss functions and conventional training settings to guide practitioners. The limited impact of existing paradigms does not preclude innovative training approaches from improving loss-to-loss scaling. In fact, a recent work by Saunshi et al. (2024) demonstrates that gradually increasing model depth and initializing based on layers from a smaller model produces markedly different scaling behavior, particularly in how perplexity translates to downstream accuracy. Similar structured growth approaches could offer new pathways for improving scaling efficiency and generalization for decoder-only LLMs trained with next-token prediction. We leave this exercise for future work."}, {"title": "On the Exhaustiveness of Interventions in \u00a74.1", "content": "Our study clearly distinguishes between factors with substantial and limited impact on loss-to-loss scaling. While our conclusions are inherently shaped by the specific settings we explored, the observed trends provide strong empirical evidence for these distinctions. Given the strong and consistent impact of pretraining data and tokenizer, we can confidently conclude that these interventions affect loss-to-loss scaling. While we observed only a limited impact of the architecture, this effect was also consistent across major state-of-the-art architectures including Llama, GPT, and Mamba which collectively represent the dominant paradigms in large-scale language modeling. Given this exhaustive set, it is hard to argue that other architectures would meaningfully alter loss-to-loss scaling."}, {"title": "On the Exhaustiveness of Interventions in \u00a74.2", "content": "Across the wide range of size configurations (App. A) we test, all models exhibit very consistent loss-to-loss scaling. Similarly, the effect we observed for different context lengths is very consistent within our test range (1024, 2048, 3076), which aligns with commonly used configurations (Black et al., 2021; Wang & Komatsuzaki, 2021; Biderman et al., 2023; Penedo et al., 2024; Black et al., 2022). While we acknowledge the possibility that larger models or longer context lengths could influence loss-to-loss scaling, such an effect if present is unlikely. For optimization settings, we again consider configurations widely used in LLM training (Shoeybi et al., 2020; Karpathy, 2022; Videau et al., 2024), including variations in optimizer type, learning rate, weight decay, and scheduling. While our results indicate that these choices do not meaningfully alter loss-to-loss scaling within the explored settings, we acknowledge that the space of optimization techniques is vast, and our list is not exhaustive. It remains possible that a principled optimization strategy, different from current best practices, could"}, {"title": "Impact", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}