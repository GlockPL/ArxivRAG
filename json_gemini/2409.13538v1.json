{"title": "First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge", "authors": ["Yingzhe Peng", "Yixiao Yuan", "Zitian Ao", "Huapeng Zhou", "Kangqi Wang", "Qipeng Zhu", "Xu Yang"], "abstract": "In this report, we present our first-place solution to the Multiple-choice Video Question Answering (QA) track of The Second Perception Test Challenge [2]. This competition posed a complex video understanding task, requiring models to accurately comprehend and answer questions about video content. To address this challenge, we leveraged the powerful QwenVL2 (7B) [1] model and fine-tune it on the provided training set. Additionally, we employed model ensemble strategies and Test Time Augmentation to boost performance. Through continuous optimization, our approach achieved a Top-1 Accuracy of 0.7647 on the leaderboard.", "sections": [{"title": "Introduction", "content": "In this report, we present our first-place solution to the Multiple-choice Video Question Answering (QA) track of The Second Perception Test Challenge [2]. This competition posed a complex video understanding task, requiring models to accurately comprehend and answer questions about video content. To address this challenge, we leveraged the powerful QwenVL2 (7B) [1] model and fine-tune it on the provided training set. Additionally, we employed model ensemble strategies and Test Time Augmentation to boost performance. Through continuous optimization, our approach achieved a Top-1 Accuracy of 0.7647 on the leaderboard."}, {"title": "Background", "content": "Video Question Answering (Video QA) is a challenging task in computer vision and natural language processing that requires models to understand and reason about video content to answer questions accurately. With the increasing availability of high-resolution videos, it is crucial for models to efficiently process and comprehend both spatial and temporal information. The Second Perception Test Challenge's Multiple-choice Video QA track focuses on evaluating models' abilities to handle such complex video understanding tasks."}, {"title": "Method", "content": "To address this complex video understanding task, we employed the state-of-the-art QwenVL2 (7B) model and fine-tuned it on the training dataset. QwenVL2 integrates advanced techniques such as Naive Dynamic Resolution input and Multimodal Rotary Position Embedding (M-ROPE), enhancing its capacity to process high-resolution videos and capture temporal dynamics effectively.\nFirst, we evaluated the zero-shot performance of QwenVL2 on this task, achieving a Top-1 Accuracy of 0.61, indicating its strong baseline capabilities. To further optimize the model's performance, we constructed instruction data using the prompt format shown in Table 1."}, {"title": "Baseline Model", "content": "Initially, we trained a baseline model. We partitioned 5% of the training set as a validation set and used the remaining data for instruction fine-tuning. Training was conducted on four NVIDIA A6000 GPUs with 48 GB memory each. To conserve GPU memory and accelerate training, we utilized DeepSpeed [4] ZeRO-2 and Low-Rank Adaptation (LoRA) [3]. The specific parameters of LoRA are detailed in Table 2. The learning rate and batch size were set to 1 \u00d7 10-4 and 8, respectively.\nFor video preprocessing, we extracted 30 frames from each video and set the resolution to 240 \u00d7420. Our baseline model achieved a Top-1 Accuracy of 0.7376 on the leaderboard."}, {"title": "High-Resolution Instruction Tuning (HR-IT)", "content": "We analyzed the resolution statistics of the dataset. The majority of the competition data consisted of high-resolution videos. Therefore, we decided to train with higher-resolution videos. Additionally, we performed 5-fold cross-validation to enhance the ro- bustness of our models. LoRA was also used in this phase, with parameters provided in Table 2.\nWe increased the maximum number of pixels to 176,400 (315 \u00d7 560). Through cross-validation, we obtained five models fine-tuned with high-resolution instructions."}, {"title": "Model Ensemble", "content": "In total, we trained six models. Our ensemble strategy was crucial to achieving high accuracy. Firstly, we collected the inference results from these six models and applied a majority voting scheme for each question, selecting the answer with the most votes as our prediction. Notably, the video processing during inference was consistent with that during training for these models. This approach yielded a Top-1 Accuracy of 0.7551 on the leaderboard."}, {"title": "Ensemble Enhancements", "content": "We further enhanced our ensemble through additional techniques:\nTest Time Augmentation (TTA): We applied Test Time Augmentation (TTA) by shuffling the order of multiple-choice options. Specifically, we shuffled the options and assigned them to choices A, B, and C, allowing the model to perform inference on different permutations. TTA aims to reduce positional bias in the model's predictions. We applied this strategy by generating three additional random permutations of the options and used the cross-validation models to re-infer, resulting in four sets of predictions for each fold model. Majority voting was then applied to these results.\nInference with Higher Resolution and More Frames: We conducted experiments using the five cross-validation models with different video processing strategies during inference. We randomly selected 300 samples from the validation set and found that using higher resolution and more frames improved the model's video understanding capabilities. The accuracy results with different parameter"}, {"title": "Final Ensemble Strategy", "content": "We ensembled the following models: Baseline model (1 model), High-Resolution Instruction Tuning Models (5-fold), Original resolution inference results enhanced with TTA (4 permutations \u00d7 5 models), Inference with more frames (5 models), Inference with more frames and higher resolution (\u00d7 5 models)\nIn total, we had 31 sets of model inference results for ensemble. Different voting weights were assigned to different results, as shown in Table 4. This comprehensive ensemble strategy led us to achieve the highest final score on the leaderboard: a Top-1 Accuracy 0.7647."}, {"title": "Summary", "content": "In this report, we presented our first-place solution for the Multiple-choice Video QA track of The Second Perception Test Challenge. By leveraging the advanced capabilities of QwenVL2 and employing strategies such as high-resolution instruction tuning, cross-validation, test-time augmentation, and a comprehensive ensemble approach, we significantly improved the model's performance. Our final ensemble achieved a Top-1 Accuracy of 0.7647, demonstrating the effectiveness of our methods in complex video understanding tasks."}]}