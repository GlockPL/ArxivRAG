{"title": "Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions", "authors": ["Lucas M\u00f6ller", "Pascal Tilli", "Ngoc Thang Vu", "Sebastian Pad\u00f3"], "abstract": "Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and learn similarities between them. However, it is not\nunderstood how such models compare two inputs. Here, we address this research\ngap with two contributions. First, we derive a method to attribute predictions of\nany differentiable dual encoder onto feature-pair interactions between its inputs.\nSecond, we apply our method to CLIP-type models and show that they learn fine-\ngrained correspondences between parts of captions and regions in images. They\nmatch objects across input modes and also account for mismatches. However, this\nvisual-linguistic grounding ability heavily varies between object classes, depends\non the training data distribution, and largely improves after in-domain training.\nUsing our method we can identify knowledge gaps about specific object classes in\nindividual models and can monitor their improvement upon fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Dual encoder models use independent modules to represent two types of inputs in a common\nembedding space and compute their similarity. The training objective is typically a triplet or\ncontrastive loss [63, 68]. Popular examples include Siamese transformers for text-text pairs (SBERT)\n[57] and Contrastive Language-Image Pre-Training (CLIP) models [56, 32] for text-image pairs. The\nlearned representations have proven to be highly informative for downstream applications such as\nVisual Question Answering (VQA) [3], image captioning and visual entailment [62].\nHowever, there is limited understanding which properties of the inputs these models base their\npredictions on. Similarities depend on interactions between two instances rather than on either\ninstance's properties alone. Few works have studied these interactions in symmetric Siamese encoders\n[19, 52, 47, 69] and, to the best of our knowledge, they are yet to be explored in non-symmetric\nmodels like vision-language dual encoders, e.g. CLIP. First-order feature attribution methods like\nShapley values [46] or integrated gradients [66] are insufficient for explaining similarities, as they\ncan only attribute predictions to individual features, not to interactions between them.\nWe address this research gap by extending previous work for language-only Siamese models in\nNatural Language Processing (NLP) [52, 47]. Our contributions are: (1) We derive a method\nto compute general feature-pair attributions that can explain interactions between inputs of any\ndifferentiable dual encoder model. The method requires no modification of the trained model. (2)\nWe apply the method to a range of CLIP models and show it can capture fine-grained interactions\nbetween parts of captions and corresponding regions in images as exemplified in Figure 1. It can\nalso point out correspondence between two captions or two images (Figures 2 and 3). (3) We utilize\nimage-captioning datasets containing object bounding-box annotations to evaluate the extent and the\nlimit of the models' intrinsic visual-linguistic grounding abilities."}, {"title": "2 Related work", "content": "Metric learning refers to the task of producing embeddings reflecting the similarity between inputs\n[33]. Applications include face identification [27, 71] and image retrieval [80, 24]. Siamese networks\nwith cosine similarity of embeddings were early candidates [10]. The triplet-loss [29] involving\nnegative examples has been proposed as an improvement but requires sampling strategies for the\nlarge number of possible triplets [58]. Qian et al. have shown that the triplet-loss can be relaxed to a\nsoftmax variant [55]. Sohn [63] and Oord et al. [68] have proposed the batch contrastive objective\nwhich has been applied in both unsupervised [8] and supervised representation learning [34] and has\nlead to highly generalizable image [28] and semantic text embeddings [57].\nVision-language models process both visual and linguistic inputs. Qi et al. were the first to\ntrain a dual-encoder architecture with a contrastive objective on image-text data in the medical\ndomain [81]. With CLIP Radford et al. have applied this principle to web-scale image captions\n[56] and the ALIGN model has achieved similar results with alt-text [32]. In the following, the\nbasic inter-modal contrastive loss has been extended by, intra-modal loss terms [25, 36, 74], self-\nsupervision [49], non-contrastive objectives [82], incorporating classification labels [75], textual\naugmentation [20], a unified multi-modal encoder architecture [51] and retrieval augmentation [72].\nNext to more advanced training objectives, other works have identified the training data distribution\nto be crucial to performance: Gadre et al. have proposed the DataComp benchmark focusing on\ndataset curration while fixing model architecture and training procedure [23], Xu et al. balance\nmetadata distributions [73] and Fang et al. propose data filtering networks for the purpose [21]. The\nstrictly separated dual-encoder architecture has been extended to include cross-encoder dependencies\n[39, 54], and multi-modal encoders have been combined with generative decoders [9, 45, 40, 35, 2].\nThe CoCa model combines contrastive learning on uni-modal vision- and text-representations with a\ntext generative cross-modal decoder [79].\nVisual-linguistic grounding is the identification of fine-grained relations between text phrases and\ncorresponding image parts [12]. Specialized models predict regions over images for a corresponding"}, {"title": "3 Method", "content": "We first derive general feature-pair attributions for dual encoder predictions and then specifically\napply the result to vision-language models.\nDerivation of feature-pair attributions. Let\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\nf(a, b) = g(a)h(b) = s\n\\end{equation}\n\n\n\n\nbe a differentiable dual-encoder model, with two vector-valued encoders g and h, respective inputs a\nand b and a scalar output s. For our purpose, g will be an image encoder with an image input a and h\nwill be a text encoder with a text representation b as input. To attribute the prediction s onto features\nof the two inputs a and b, we also define two uninformative reference inputs ra, the black image, and\nrb, a sequence of padding tokens with fixed length. We then start from the following expression:\n\n\n\n\\begin{equation}\nf(a, b) - f (r_a, b) \u2013 f(a, r_b) + f(r_a, r_b)\n\\end{equation}\n\n\nInterpreting f as an anti-derivative, we can reformulate it into an integral over the derivative of f:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\n\\begin{split}\n& \\int_{r_a}^{a} \\int_{r_b}^{b} \\frac{\\partial^2}{\\partial x_i \\partial y_j} f (x, y) \\,dx_i \\,dy_j\n= [f(a, b) \u2013 f(r_a, b)] \u2013 [f(a, r_b) \u2013 f(r_a, r_b)]\n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\\begin{equation}\n\\int_{r_a}^{a} \\int_{r_b}^{b} \\frac{\\partial^2}{\\partial x_i \\partial y_j} g_k(x) h_k(y) \\,dx_i \\,dy_j\n\\end{equation}\n\n\nAgain, we use component-wise notation for the dot-product between the two embeddings g(x) and\nh(y) and index output dimensions with k. Since neither embedding depends on the other integration"}, {"title": null, "content": "variable, we can separate the expression into:\n\n\n\\begin{equation}\n\\int_{r_a}^{a} \\frac{\\partial g_k(x)}{\\partial x_i} \\,dx_i  \\int_{r_b}^{b} \\frac{\\partial h_k(y)}{\\partial y_j} \\,dy_j\n\\end{equation}\n\n\nThis step makes explicit use of the strict separation of the two encoders. Cross-encoder architectures\nwould introduce dependencies between them. Both terms are line integrals from the references to\nthe actual inputs in the respective input representation spaces; \\frac{\\partial g_k(x)}{\\partial x_i} and \\frac{\\partial h_k(y)}{\\partial y_j} are the\nJacobians of the two encoders. Following the concept of integrated gradients [66], we define the\nstraight lines between both references and inputs,\n\n\n\\begin{equation}\nx(\\alpha) = r_a + \\alpha(a \u2013 r_a),\n\\end{equation}\n\n\n\n\n\\begin{equation}\ny(\\beta) = r_b + \\beta(b - r_b),\n\\end{equation}\n\n\nparameterized by \\alpha and \\beta, and solve by substitution. For the integral over encoder g this yields\n\n\\begin{equation}\n\\int_{0}^{1} \\frac{\\partial g_k (x(\\alpha))}{\\partial x_i} \\frac{\\partial x_i(\\alpha)}{\\partial \\alpha} d\\alpha = (a - r_a)_i  \\int_{0}^{1} \\frac{\\partial g_k (x(\\alpha))}{\\partial x_i} d\\alpha,\n\\end{equation}\n\nsince \\frac{\\partial x(\\alpha)}{\\partial \\alpha} = (a - r_a), which is a constant w.r.t a; hence, we can pull it out of the integral. The\nintegral over encoder h is processed in the same way. We then define the two integrated Jacobians,\n\n\n\n\\begin{equation}\n\\hat{J}_{ki} = \\int_{0}^{1} \\frac{\\partial g_k(x(\\alpha))}{\\partial x_i} d\\alpha \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial g_k(x(\\alpha_n))}{\\partial x_i}\n\\end{equation}\n\nand \\hat{J}_{kj} analogously. In practice, these integrals are calculated numerically by sums over N steps,\nwith \\alpha_n = n/N. This introduces an approximation error which must, however, converge to zero for\nlarge N by definition of the Riemann integral. We plug the results from Equation 8 and the definitions\nof the integrated Jacobians back into Equation 5 and obtain:\n\n\n\\begin{equation}\n(a - r_a)_i \\hat{J}_{ik} \\hat{J}_{kj} (b \u2212 r_b)_j =: A_{ij}\n\\end{equation}\n\nAfter computing the sum over the output embedding dimensions k, this provides a matrix of feature-\npairs (i, j) in input a and b which we call attribution matrix Aij. Note that except for the numerical\nintegration, the equality to Equation 2 still holds. Hence, the sum over all feature-pair attributions\nin A is an exact reformulation of the ansatz. If the references ra and rb are uninformative, i.e.\nf(ra, b) \u2248 0, f (a, r\u2081) \u2248 0, f (ra, r\u266d) \u2248 0, we arrive at the final approximation\n\n\n\n\\begin{equation}\nf(a, b) \\approx \\sum_{ij} A_{ij},\n\\end{equation}\n\nwhere i ranges over dimensions in input a and j over b. This provides an approximate decomposition\nof the model prediction s = f (a, b) into additive contributions of feature-pairs in the two inputs.\nInter-modal attributions. In the derivation above, we treat image and text representations as\nvectors. In current transformer-based language encoders, text inputs are represented as S \u00d7 Db\ndimensional tensors, where S is the length of the token sequence and D is the model's embedding\ndimensionality. In vision transformers, image representations are P\u00d7P\u00d7 Da dimensional tensors,\nwith P being the number of patches that the image is split into horizontally and vertically. Our\npair-wise image-text attributions thus have the dimensions P \u00d7 P \u00d7 D \u00d7 S \u00d7 Dr. With hundreds of\nembedding dimensions and tens of tokens and patches, this quickly becomes intractable. Fortunately,\nthe sum over dimensions in Equation 11 enables the additive combination of attributions in A. We\nsum over the embedding dimensions of both encoders Da and D\u2081 and obtain a P\u00d7P\u00d7S dimensional\nattribution tensor, which estimates for each pair of a text token and an image patch how much\ntheir combination contributes to the overall prediction. These attributions are still three-dimensional\nand thus not straightforward to visualize. However, again we can use their additivity, slice the 3d\nattribution tensor along text or image dimensions and project onto the remaining dimensions by\nsummation. We can for example select a slice over a range of tokens and project it onto the image as\nin Equation 1 (a)/(b). Attribution heat maps over the image result from interpolating the patch-level\nattributions to the image resolution. Importantly, the two visualizations shown here come from the\nsame 3d attribution tensor for the given caption-image pair. The reverse case of slicing parts of the\nimage and projecting the result onto the caption dimension is shown in Equation 1(c)/(d). Here, we\nproject the selected image slices marked by yellow bounding boxes onto the caption and visualize\nattributions as saliency maps over tokens in the caption."}, {"title": null, "content": "Intra-modal attributions. Albeit vision-language dual-encoders are typically trained to match\nimages against captions, we can compute attributions for image-image or text-text pairs as well by\napplying the same encoder to both inputs. For text-text attributions, after summation over embedding\ndimensions, this yields an S\u2081\u00d7S2 dimensional attribution tensor, with S\u2081 and S2 being token sequence\nlengths of the two texts. These 2d attributions may be visualized in the form of a matrix [52]. In\nFigure 2 we, however, stick to the slice representation and attribute the yellow selected slice in\nthe first caption onto the second caption. For image-image similarities, attribution tensors become\nfour dimensional taking the shape (P\u00d7P)1\u00d7(P\u00d7P)2 and containing a contribution for every pair\nof two patches from either image. Parentheses indicate which input the dimensions belong to. In\nFigure 3, we attribute the slice of the yellow bounding-box in the left image onto the image to its\nright. Appendix A includes additional examples."}, {"title": "4 Experiments", "content": "In our experiments, we apply our feature-pair attributions to contrastively trained vision-language dual\nencoders. We focus on evaluating interactions of mentioned objects in captions and corresponding\nregions in images by selecting sub-strings in captions and attributing them onto images as in Figure 1\n(left part)."}, {"title": "4.1 Experimental setup", "content": "Datasets. We base this evaluation on three image-caption datasets that also contain object bounding-\nbox annotations in images, namely Microsoft's Common Objects in Context (COCO) [42], the\nFlickr30k collection [78] with the entity annotations created by Plummer et al. [53], and the Hard\nNegative Captions (HNC) dataset by D\u00f6nmez et al. [15], which automatically generates captions\nfrom scene graphs using templates. In the attempt to minimize the domain gap between all three\ndatasets, we sample sub-graph triplets and use a basic \"<subject> <predicate> <object>\"-template to\ncreate captions. In our analysis, HNC is only used for evaluation; on Flickr30k we use the official test\nsplit and on COCO we use the validation-split\u00b2, as the official test-split does not contain captions.\nModels. We analyze CLIP dual-encoder architectures [56] without cross-encoder dependencies\n(a requirement of our method, cf. Equation 5) and the standard inter-modal contrastive objective.\nWe evaluate the original OpenAI models which are trained on an undisclosed dataset, as well as the\nOpenCLIP re-implementations trained on the Laion [60], Dfn [21], CommonPool and DataComp\n[23] datasets, as well as MetaCLIP [73] 3.\nFine-tuning. Next to evaluating the vision and language grounding capabilities of off-the-shelf\nCLIP models pre-trained on web-data, we are also interested in how this ability changes with access\nto higher-quality human annotations. We fine-tune models on the COCO and Flickr30k train splits\nfor five epochs using the AdamW optimizer [44] with an initial learning rate of 1e-7, exponentially\nincreasing to le-5, a weight decay of 1e-4, and a batch size of 64 on one NVIDIA A6000 GPU. All\nfine-tunings are performed in the standard contrastive setting. We never change model architectures\nor training objectives to explicitly perform grounding."}, {"title": "4.2 Object bounding-box attributions", "content": "To systematically assess the visual-linguistic grounding abilities of the analyzed dual encoders,\nwe evaluate the agreement of our attributions with corresponding object bounding boxes. For this\nexperiment, we apply the following filters to the three datasets: We use a given object annotation if a\nsingle instance of its class appears in the image and its bounding-box is larger than one patch. For\nCOCO, we identify class occurrences in the caption through a dictionary based synonym matching.\nFor HNC, classes exactly match sub-strings in captions and in Flickr30k, respective spans are already\nannotated. This results in 3.5k image-caption pairs from COCO, 8k pairs from Flickr30k, and 500\npairs from HNC."}, {"title": "4.3 Attributions to other objects", "content": "In many of the above examples, we observe that when selecting objects in the caption and attributing\nto the image, or vice versa, other objects that also occur in the caption and appear in the image, often\nreceive negative attributions. Figure 8 includes four explicit cases of selected objects in a caption\n(yellow) receiving positive attributions in the image and other objects (underlined) that are attributed\nnegatively. For a systematic evaluation of this effect, we sample instances from COCO that include at\nleast two different object classes, which both appear exactly once in the image and are mentioned in\nthe caption. We select one of them and compute the total attribution to its actual bounding-box as\nwell as the attribution to the other object's bounding box. We then repeat this evaluation with the\nsecond object. The attribution to the actual bounding-box is almost always positive (97.1%), while\nthe attribution to the other bounding-box receives a negative attribution in 65.6% of the cases. In\nthe model fine-tuned on COCO, this fraction increases to 70.1%. A distribution over the sign of"}, {"title": "4.4 Hard Negative Captions", "content": "We extend the grounding evaluation by creating hard negative captions that replace an object in a\npositive caption with a reasonable but different object to receive a negative counterpart. D\u00f6nmez\net al. [15] proposed an automatic procedure to generate positive and hard negative captions, which\nwe leverage together with our simplified template (cf. Section 4). Additionally, we create a second"}, {"title": "5 Discussion", "content": "Interpretation of results. Our results show that vision-language dual-encoders can learn fine-\ngrained correspondence between parts of captions and regions in images. This occurs despite these\nmodels' simple architecture, incorporating a single weak interaction between vision and language\nembeddings via a dot-product in the final embedding space, and training objective of conventional\ninter-modal only contrastive matching. However, we find that this ability appears to be heavily\ninfluenced by the distribution of the pre-training data. Initially, the OpenCLIP model trained only on\nthe Laion dataset grounds poorly but exhibits a large improvement after fine-tuning. This indicates\nthat exposure to specific visual-linguistic concepts is necessary for models to develop robust inter-\nmodal correspondences. As the original CLIP models perform much better on these datasets, we\nhypothesize that the COCO and Flickr30k training splits are included in their pretraining. Future\nwork should analyze whether this grounding ability generalizes better in larger models.\nAn interesting finding is that models do not only learn fine-grained visual-linguistic correspondence\nfor objects but often actively down-weight mismatches with negative attributions (Section 4.3). Our\nevaluation with hard negative captions shows that overall the models correctly react to errors in\ncaptions and can even clearly point out mismatches between wrong captions and images in some"}, {"title": null, "content": "cases. However, this is not consistently the case, which suggests that models may benefit from\nincluding hard negative captions into the training procedure.\nFailure cases. The off-the-shelf OpenCLIP models frequently show obvious misattributions on\nall test sets (cf. Figure 7). In contrast, the fine-tuned models (including the OpenAI off-the-shelf\nCLIP models) exhibit hardly any clearly wrong attributions anymore. Qualitatively, errors leading\nto low values of y in these models include: true misidentifications of objects in difficult scenes\n(cf. the dog in Figure 4), partial covering of objects, attributions extending outside of bounding\nboxes onto neighboring patches, and spread-out attributions in visually strongly correlated contexts\nlike bathrooms, kitchens, tennis courts, etc. (cf. Figure 10 for examples). We also observe cases\nsuggesting that the models do not have an understanding of exclusivity: single objects in captions can\nbe attributed to different objects in the image or vice-versa, e.g. the car and bus in Figure 10.\nLimitations. Our feature-pair attributions are an approximation as Equation 11 states clearly. More-\nover, throughout this work, we attribute to deep representations of inputs because it is computationally\nfeasible and informative [47]. In both vision and language (transformer) encoders, deep representa-\ntions have undergone multiple contextualization steps and are technically not bound to input features\nat the given position anymore. Last, recently proven fundamental limitations of attribution methods\nurge caution in their interpretation, especially, regarding counterfactual conclusions about the impor-\ntance of individual features for the overall prediction [7]. Despite these considerations, empirically,\nour evaluations show that our derived feature-pair attributions produce reasonable results in a large\nmajority of cases and can point out inabilities (of the OpenClip model), errors (misidentification\nof objects), and biases (contextualization in correlated scenes). While they should not be seen as\nguaranteed robust and faithful explanations, we argue that they do provide valuable insights into\ndual-encoder models and have the potential to improve these models further."}, {"title": "6 Conclusion", "content": "In this paper, we have derived general feature-pair attributions for differentiable dual-encoder archi-\ntectures which can attribute similarity predictions for two inputs onto interactions of their features.\nOur attribution method applies to any dual-encoder architecture in arbitrary domains. We believe\nthe method can lead to valuable insights in other applications like image similarity or (multi-modal)\ninformation-retrieval and help improve these models further.\nWe have furthermore applied our method to contrastively trained vision-language models in order to\nevaluate whether such models can relate objects in captions and images. We found that they can learn\nfine-grained correspondence between visual and linguistic concepts. Mis-matches are often not only\nignored, but negatively down-weighted instead. However, this inter-modal correspondence can be\npoor when models are not exposed to matching data distributions during pre-training. Finally, our\nanalysis indicates that contrastive vision-language training may be improved by incorporating hard\nnegative captions and by de-coupling strongly correlated objects (e.g. in scenes like bathrooms or\nstreets)."}, {"title": "D Relation to GradCam", "content": "We now discuss the relation of integrated gradients [66] and GradCam. We start by deriving IG for\na model f(a) = s with a vector-valued input a and a scalar prediction s, which might e.g. be a\nclassification score for a particular class. We define the reference input r, begin from the difference\nbetween the two predictions and reformulate it as an integral:\n\n\\begin{equation}\nf(a) - f(r) =  \\int_{r}^{a} \\frac{df(x)}{dx_i} \\,dx_i\n\\end{equation}\n\nTo solve the resulting line integral, we substitute with the straight line x(x) = r + a(a \u2013 r) and pull\nits derivative dx(a)/da = (a - r) out of the integral:\n\n\n\\begin{equation}\n\\int_{\\alpha=0}^{\\alpha=1} \\frac{df(x(\\alpha))}{dx_i} \\frac{dx_i(\\alpha)}{d\\alpha} d\\alpha = (a -r)_i  \\int_{\\alpha=0}^{\\alpha=1} \\nabla_i f(x(\\alpha)) d\\alpha\n\\end{equation}\n\nto arrive at the final IG we approximate the integral by a sum over N steps:\n\n\\begin{equation}\n(a-r)_i \\approx \\frac{1}{N} \\sum_{n=1}^{N}  \\nabla_i f(x(\\alpha_n))\n\\end{equation}\n\nIf f(r) \u2248 0 this is the contribution of feature i in a to the model prediction f(a) = s. We can now\nreduce these feature attributions further by setting N = 1 and r = 0, the black image to obtain\n\n\n\n\\begin{equation}\na_i \\nabla_i f(a),\n\\end{equation}\n\nwhich is basic form of GradCam. The method typically attributes to deep image representations in\nCNNs, so that a have the dimensions C\u00d7 H \u00d7 W, the number of channles, height and width of the\nrepresentation. To reduce attributions to a two-dimensional map, it sums over the channel dimension\nand applies a relu-activation to the outcome. The original version also average pools the gradients\nover the spacial dimensions, however, this is technically not necessary.\nAs discussed earlier, neither integrated gradients nor GradCam can explain dual encoder predictions.\nFollowing the logic from above we could, however, derive a GradCam for similarity by setting N = 1\nin the computation of the integrated Jacobians in Equation 9 and using ra = 0 and rb 0. For our\nattribution matrix from Equation 10 we would then receive the simplified version\n\n\\begin{equation}\na_i \\frac{\\partial g_k}{\\partial a_i} \\frac{\\partial h_k}{\\partial b_j} b_j.\n\\end{equation}\n\nHowever, setting N = 1 is the worst possible approximation to the integrated Jacobians. Therefore,\nthis extreme simplification must be used with caution and should only be taken as a very rough\nestimate."}]}