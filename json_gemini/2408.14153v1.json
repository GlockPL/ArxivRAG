{"title": "Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions", "authors": ["Lucas M\u00f6ller", "Pascal Tilli", "Ngoc Thang Vu", "Sebastian Pad\u00f3"], "abstract": "Dual encoder architectures like CLIP models map two types of inputs into a shared embedding space and learn similarities between them. However, it is not understood how such models compare two inputs. Here, we address this research gap with two contributions. First, we derive a method to attribute predictions of any differentiable dual encoder onto feature-pair interactions between its inputs. Second, we apply our method to CLIP-type models and show that they learn fine-grained correspondences between parts of captions and regions in images. They match objects across input modes and also account for mismatches. However, this visual-linguistic grounding ability heavily varies between object classes, depends on the training data distribution, and largely improves after in-domain training. Using our method we can identify knowledge gaps about specific object classes in individual models and can monitor their improvement upon fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Dual encoder models use independent modules to represent two types of inputs in a common embedding space and compute their similarity. The training objective is typically a triplet or contrastive loss [63, 68]. Popular examples include Siamese transformers for text-text pairs (SBERT) [57] and Contrastive Language-Image Pre-Training (CLIP) models [56, 32] for text-image pairs. The learned representations have proven to be highly informative for downstream applications such as Visual Question Answering (VQA) [3], image captioning and visual entailment [62].\nHowever, there is limited understanding which properties of the inputs these models base their predictions on. Similarities depend on interactions between two instances rather than on either instance's properties alone. Few works have studied these interactions in symmetric Siamese encoders [19, 52, 47, 69] and, to the best of our knowledge, they are yet to be explored in non-symmetric models like vision-language dual encoders, e.g. CLIP. First-order feature attribution methods like Shapley values [46] or integrated gradients [66] are insufficient for explaining similarities, as they can only attribute predictions to individual features, not to interactions between them.\nWe address this research gap by extending previous work for language-only Siamese models in Natural Language Processing (NLP) [52, 47]. Our contributions are: (1) We derive a method to compute general feature-pair attributions that can explain interactions between inputs of any differentiable dual encoder model. The method requires no modification of the trained model. (2) We apply the method to a range of CLIP models and show it can capture fine-grained interactions between parts of captions and corresponding regions in images as exemplified in Figure 1. It can also point out correspondence between two captions or two images (Figures 2 and 3). (3) We utilize image-captioning datasets containing object bounding-box annotations to evaluate the extent and the limit of the models' intrinsic visual-linguistic grounding abilities."}, {"title": "2 Related work", "content": "Metric learning refers to the task of producing embeddings reflecting the similarity between inputs [33]. Applications include face identification [27, 71] and image retrieval [80, 24]. Siamese networks with cosine similarity of embeddings were early candidates [10]. The triplet-loss [29] involving negative examples has been proposed as an improvement but requires sampling strategies for the large number of possible triplets [58]. Qian et al. have shown that the triplet-loss can be relaxed to a softmax variant [55]. Sohn [63] and Oord et al. [68] have proposed the batch contrastive objective which has been applied in both unsupervised [8] and supervised representation learning [34] and has lead to highly generalizable image [28] and semantic text embeddings [57].\nVision-language models process both visual and linguistic inputs. Qi et al. were the first to train a dual-encoder architecture with a contrastive objective on image-text data in the medical domain [81]. With CLIP Radford et al. have applied this principle to web-scale image captions [56] and the ALIGN model has achieved similar results with alt-text [32]. In the following, the basic inter-modal contrastive loss has been extended by, intra-modal loss terms [25, 36, 74], self-supervision [49], non-contrastive objectives [82], incorporating classification labels [75], textual augmentation [20], a unified multi-modal encoder architecture [51] and retrieval augmentation [72]. Next to more advanced training objectives, other works have identified the training data distribution to be crucial to performance: Gadre et al. have proposed the DataComp benchmark focusing on dataset curration while fixing model architecture and training procedure [23], Xu et al. balance metadata distributions [73] and Fang et al. propose data filtering networks for the purpose [21]. The strictly separated dual-encoder architecture has been extended to include cross-encoder dependencies [39, 54], and multi-modal encoders have been combined with generative decoders [9, 45, 40, 35, 2]. The CoCa model combines contrastive learning on uni-modal vision- and text-representations with a text generative cross-modal decoder [79].\nVisual-linguistic grounding is the identification of fine-grained relations between text phrases and corresponding image parts [12]. Specialized models predict regions over images for a corresponding"}, {"title": "3 Method", "content": "We first derive general feature-pair attributions for dual encoder predictions and then specifically apply the result to vision-language models.\nDerivation of feature-pair attributions. Let\n$f(a, b) = g(a)h(b) = s$ (1)\nbe a differentiable dual-encoder model, with two vector-valued encoders g and h, respective inputs a and b and a scalar output s. For our purpose, g will be an image encoder with an image input a and h will be a text encoder with a text representation b as input. To attribute the prediction s onto features of the two inputs a and b, we also define two uninformative reference inputs $r_a$, the black image, and $r_b$, a sequence of padding tokens with fixed length. We then start from the following expression:\n$f(a, b) - f (r_a, b) \u2013 f(a, r_b) + f(r_a, r_b)$ (2)\nInterpreting f as an anti-derivative, we can reformulate it into an integral over the derivative of f:\n$[f(a, b) \u2013 f(r_a, b)] \u2013 [f(a, r_b) \u2013 f(r_a,r_b)] = \\int_{r_a}^{a} \\int_{r_b}^{b} \\frac{\\partial^2}{\\partial x_i \\partial y_j} f (x, y) dx_i dy_j = \\int_{r_a}^{a} \\int_{r_b}^{b} \\frac{\\partial}{\\partial y_j} [f(a, y) - f (r_a, y)] dy_j$ (3)\nHere, x and y are integration variables for the two inputs and we use component-wise notation with the indices i and j for the input dimensions and omit sums over double indices for clarity. We plug in the model definition from Equation 1:\n$\\int_{r_a}^{a} \\int_{r_b}^{b} \\frac{\\partial^2}{\\partial x_i \\partial y_j} g_k (x) h_k (y) dx_i dy_j$ (4)\nAgain, we use component-wise notation for the dot-product between the two embeddings g(x) and h(y) and index output dimensions with k. Since neither embedding depends on the other integration"}, {"title": "4 Experiments", "content": "In our experiments, we apply our feature-pair attributions to contrastively trained vision-language dual encoders. We focus on evaluating interactions of mentioned objects in captions and corresponding regions in images by selecting sub-strings in captions and attributing them onto images as in Figure 1 (left part).\nExperimental setup\nDatasets. We base this evaluation on three image-caption datasets that also contain object bounding-box annotations in images, namely Microsoft's Common Objects in Context (COCO) [42], the Flickr30k collection [78] with the entity annotations created by Plummer et al. [53], and the Hard Negative Captions (HNC) dataset by D\u00f6nmez et al. [15], which automatically generates captions from scene graphs using templates. In the attempt to minimize the domain gap between all three datasets, we sample sub-graph triplets and use a basic \"<subject> <predicate> <object>\"-template to create captions. In our analysis, HNC is only used for evaluation; on Flickr30k we use the official test split and on COCO we use the validation-split, as the official test-split does not contain captions.\nModels. We analyze CLIP dual-encoder architectures [56] without cross-encoder dependencies (a requirement of our method, cf. Equation 5) and the standard inter-modal contrastive objective. We evaluate the original OpenAI models which are trained on an undisclosed dataset, as well as the OpenCLIP re-implementations trained on the Laion [60], Dfn [21], CommonPool and DataComp [23] datasets, as well as MetaCLIP [73].\nFine-tuning. Next to evaluating the vision and language grounding capabilities of off-the-shelf CLIP models pre-trained on web-data, we are also interested in how this ability changes with access to higher-quality human annotations. We fine-tune models on the COCO and Flickr30k train splits for five epochs using the AdamW optimizer [44] with an initial learning rate of 1e-7, exponentially increasing to le-5, a weight decay of 1e-4, and a batch size of 64 on one NVIDIA A6000 GPU. All fine-tunings are performed in the standard contrastive setting. We never change model architectures or training objectives to explicitly perform grounding."}, {"title": "4.2 Object bounding-box attributions", "content": "To systematically assess the visual-linguistic grounding abilities of the analyzed dual encoders, we evaluate the agreement of our attributions with corresponding object bounding boxes. For this experiment, we apply the following filters to the three datasets: We use a given object annotation if a single instance of its class appears in the image and its bounding-box is larger than one patch. For COCO, we identify class occurrences in the caption through a dictionary based synonym matching. For HNC, classes exactly match sub-strings in captions and in Flickr30k, respective spans are already annotated. This results in 3.5k image-caption pairs from COCO, 8k pairs from Flickr30k, and 500 pairs from HNC."}, {"title": "4.3 Attributions to other objects", "content": "In many of the above examples, we observe that when selecting objects in the caption and attributing to the image, or vice versa, other objects that also occur in the caption and appear in the image, often receive negative attributions. Figure 8 includes four explicit cases of selected objects in a caption (yellow) receiving positive attributions in the image and other objects (underlined) that are attributed negatively. For a systematic evaluation of this effect, we sample instances from COCO that include at least two different object classes, which both appear exactly once in the image and are mentioned in the caption. We select one of them and compute the total attribution to its actual bounding-box as well as the attribution to the other object's bounding box. We then repeat this evaluation with the second object. The attribution to the actual bounding-box is almost always positive (97.1%), while the attribution to the other bounding-box receives a negative attribution in 65.6% of the cases. In the model fine-tuned on COCO, this fraction increases to 70.1%. A distribution over the sign of"}, {"title": "4.4 Hard Negative Captions", "content": "We extend the grounding evaluation by creating hard negative captions that replace an object in a positive caption with a reasonable but different object to receive a negative counterpart. D\u00f6nmez et al. [15] proposed an automatic procedure to generate positive and hard negative captions, which we leverage together with our simplified template (cf. Section 4). Additionally, we create a second"}, {"title": "5 Discussion", "content": "Interpretation of results. Our results show that vision-language dual-encoders can learn fine-grained correspondence between parts of captions and regions in images. This occurs despite these models' simple architecture, incorporating a single weak interaction between vision and language embeddings via a dot-product in the final embedding space, and training objective of conventional inter-modal only contrastive matching. However, we find that this ability appears to be heavily influenced by the distribution of the pre-training data. Initially, the OpenCLIP model trained only on the Laion dataset grounds poorly but exhibits a large improvement after fine-tuning. This indicates that exposure to specific visual-linguistic concepts is necessary for models to develop robust inter-modal correspondences. As the original CLIP models perform much better on these datasets, we hypothesize that the COCO and Flickr30k training splits are included in their pretraining. Future work should analyze whether this grounding ability generalizes better in larger models.\nAn interesting finding is that models do not only learn fine-grained visual-linguistic correspondence for objects but often actively down-weight mismatches with negative attributions (Section 4.3). Our evaluation with hard negative captions shows that overall the models correctly react to errors in captions and can even clearly point out mismatches between wrong captions and images in some"}, {"title": "6 Conclusion", "content": "In this paper, we have derived general feature-pair attributions for differentiable dual-encoder architectures which can attribute similarity predictions for two inputs onto interactions of their features. Our attribution method applies to any dual-encoder architecture in arbitrary domains. We believe the method can lead to valuable insights in other applications like image similarity or (multi-modal) information-retrieval and help improve these models further.\nWe have furthermore applied our method to contrastively trained vision-language models in order to evaluate whether such models can relate objects in captions and images. We found that they can learn fine-grained correspondence between visual and linguistic concepts. Mis-matches are often not only ignored, but negatively down-weighted instead. However, this inter-modal correspondence can be poor when models are not exposed to matching data distributions during pre-training. Finally, our analysis indicates that contrastive vision-language training may be improved by incorporating hard negative captions and by de-coupling strongly correlated objects (e.g. in scenes like bathrooms or streets)."}, {"title": "D Relation to GradCam", "content": "We now discuss the relation of integrated gradients [66] and GradCam. We start by deriving IG for a model f(a) = s with a vector-valued input a and a scalar prediction s, which might e.g. be a classification score for a particular class. We define the reference input r, begin from the difference between the two predictions and reformulate it as an integral:\n$f(a) - f(r) = \\int_{r}^{a} \\frac{df(x)}{dx_i} dx_i$ (12)\nTo solve the resulting line integral, we substitute with the straight line $x(\\alpha) = r + \\alpha(a \u2013 r)$ and pull its derivative $dx(\\alpha)/d\\alpha = (a - r)$ out of the integral:\n$\\int_{\\alpha=0}^{\\alpha=1} \\frac{df(x(\\alpha))}{dx_i} \\frac{dx_i(\\alpha)}{d\\alpha} d\\alpha = (a -r)_i \\int_{\\alpha=0}^{\\alpha=1} \\nabla_i f(x(\\alpha)) d\\alpha$ (13)\nto arrive at the final IG we approximate the integral by a sum over N steps:\n$(a -r)_i \\frac{1}{N} \\sum_{n=1}^{N} \\nabla_i f(x(\\alpha_n))$ (14)\nIf f(r) \u2248 0 this is the contribution of feature i in a to the model prediction f(a) = s. We can now reduce these feature attributions further by setting N = 1 and r = 0, the black image to obtain\n$a_i \\nabla_i f(a)$, (15)\nwhich is basic form of GradCam. The method typically attributes to deep image representations in CNNs, so that a have the dimensions C\u00d7 H \u00d7 W, the number of channles, height and width of the representation. To reduce attributions to a two-dimensional map, it sums over the channel dimension and applies a relu-activation to the outcome. The original version also average pools the gradients over the spacial dimensions, however, this is technically not necessary.\nAs discussed earlier, neither integrated gradients nor GradCam can explain dual encoder predictions. Following the logic from above we could, however, derive a GradCam for similarity by setting N = 1 in the computation of the integrated Jacobians in Equation 9 and using $r_a = 0$ and $r_b = 0$. For our attribution matrix from Equation 10 we would then receive the simplified version\n$a_i \\frac{\\partial g_k}{\\partial a_i} \\frac{\\partial h_k}{\\partial b_j} b_j$. (16)\nHowever, setting N = 1 is the worst possible approximation to the integrated Jacobians. Therefore, this extreme simplification must be used with caution and should only be taken as a very rough estimate."}]}