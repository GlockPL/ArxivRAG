{"title": "IN SILICO SOCIOLOGY: FORECASTING COVID-19 POLARIZATION WITH LARGE LANGUAGE MODELS", "authors": ["Austin C. Kozlowski", "Hyunku Kwon", "James A. Evans"], "abstract": "By training deep neural networks on massive archives of digitized text, large language models (LLMs) learn the complex linguistic patterns that constitute historic and contemporary discourses. We argue that LLMs can serve as a valuable tool for sociological inquiry by enabling accurate simulation of respondents from specific social and cultural contexts. Applying LLMs in this capacity, we reconstruct the public opinion landscape of 2019 to examine the extent to which the future polarization over COVID-19 was prefigured in existing political discourse. Using an LLM trained on texts published through 2019, we simulate the responses of American liberals and conservatives to a battery of pandemic-related questions. We find that the simulated respondents reproduce observed partisan differences in COVID-19 attitudes in 84% of cases, significantly greater than chance. Prompting the simulated respondents to justify their responses, we find that much of the observed partisan gap corresponds to differing appeals to freedom, safety, and institutional trust. Our findings suggest that the politicization of COVID-19 was largely consistent with the prior ideological landscape, and this unprecedented event served to advance history along its track rather than change the rails.", "sections": [{"title": "INTRODUCTION", "content": "For decades, the term \"artificial intelligence\" was used to describe computational capabilities that remained out of reach in a quote often attributed to Larry Tesler, \u201cartificial intelligence is whatever hasn't been done yet.\u201d\u00b9 But in recent years, algorithms have gained fluency in such complex tasks as composing novel texts, generating photo-realistic images, and advanced coding, and the term \u201cartificial intelligence\" is now part of daily parlance.\nThis rapid progress is largely the product of two overarching technological developments. First, continuous improve- ments to hardware have driven decades of exponential growth in computational power. New capabilities emerge as models scale up, and modern chips now make it possible to train models with over a trillion parameters (Kaplan et al. 2020). Second, the proliferation of online digital content has supplied abundant training data. Current models are commonly trained on a near-complete record of all text on the internet, and \"multi-modal\" models are trained on vast collections of online images and videos as well (Brown et al. 2020). These two complementary developments have resulted in algorithms capable of generating vast and varied forms of content; deep neural architectures make it possible to learn subtle and complex patterns, and large training data provide abundant examples of patterns to learn.\nParticularly striking advances have been made in the training of large language models (LLMs), algorithms capable of generating text by predicting the next word in a sequence. LLMs form the foundation of AI conversational agents such as OpenAI's ChatGPT, Anthropic's Claude, and Deep Mind's Gemini. These \u201cchatbots\u201d have swiftly gained widespread public exposure; ChatGPT alone reached over 100 million users within two months of its public release, and a large share of workers in fields ranging from education to computer programming report regularly using LLMs to improve their productivity (Dell'Acqua et al. 2023; Mollick and Mollick 2023). Seemingly overnight, the ability of algorithms to successfully impersonate human interaction \u2013 commonly known as \u201cthe Turing Test\" (Turing 1950) \u2013 shifted from aspiration to expectation.\nBecause LLMs are typically trained on the wide variety of texts published on the internet, they learn to reproduce many distinct discursive styles. They achieve this not by memorizing specific sentences (although they occasionally do this),"}, {"title": "THEORETICAL FRAMEWORK", "content": "Advocates for the use of artificial intelligence in science often emphasize algorithms' capacity to surpass human performance. Indeed, such \u201csuperhuman\u201d abilities are already facilitating important scientific contributions. In social science, machine learning algorithms assist researchers by identifying objects in videos and photographs, transcribing audio into text, and classifying texts into typologies, all at speed and scale far surpassing human capabilities (Bonikowski, Luo, and Stuhler 2022; Grimmer, Roberts, and Stewart 2022; Hannan 2022; Le Mens et al. 2023; Vicinanza, Goldberg, and Srivastava n.d.). In the natural sciences, algorithms are beginning to make new discoveries by combining computational power and speed with sophisticated knowledge bases. Arguably the most important advance has been AlphaFold's success at solving protein folding (Senior et al. 2020), but advances in drug (Jim\u00e9nez-Luna, Grisoni, and Schneider 2020) and materials discovery (Wilkins 2023; Zhou et al. 2018), the control of complex nuclear fusion reactors (Degrave et al. 2022), and even the identification of novel auction and market policies are also promising (Jiao et al. 2021; Mosavi et al. 2020).\nSocial scientists, however, may gain more from artificial intelligence by capitalizing not on its capacity to surpass human performance, but its ability to mimic it (Brynjolfsson 2023; Sourati and Evans 2023). Simulation studies in the social sciences have historically favored elegant models with simplistic agents over empirically realistic ones. Such formal models provide important insight into how complex social patterns emerge from simple interactions, but they tell us comparatively little about the dynamics of specific empirical cultural systems, organizations, or institutions. Progress on this front has long been hindered by the difficulty of specifying empirically realistic agents to populate complex social simulations. Fortunately, due to their training on massive archives of rich cultural data, modern AI models can now generate \"digital doubles\" of human respondents, capable of faithfully reproducing the knowledge, preferences, and behaviors characteristic of a specific social group.\nSocial simulations with empirically realistic agents open productive avenues for research that would be impossible with human subjects. First, AI models can use textual records to reproduce the discourse of social groups that no longer exist or that otherwise cannot be interviewed. Because these models are generative, they enable analysts to go beyond the exact statements made in the textual archive and extrapolate likely out-of-sample utterances consistent with the semantic associations in observed texts. Second, digital doubles can produce simulated data at scale. Millions of interactions between digital actors can be quickly and affordably simulated over a wide array of initial conditions and differently parameterized agents, detailing a richer and denser high-dimensional interaction space than would be possible with human actors (Lai et al. 2024). Lastly, the internal representations of AI agents are directly observable in a way that human representations are not. Although the activation patterns of deep neural networks are commonly described as black boxes, these representations can be directly analyzed and explored for deeper understanding."}, {"title": "How Large Language Models Work", "content": "Computational models for text analysis have already found widespread application in the social sciences (Gentzkow, Kelly, and Taddy 2019; Grimmer, Roberts, and Stewart 2022). The most recent innovation to gain prominence is the word embedding model, which represents semantic relationships between words in a text as geometric relationships between word vectors in a high dimensional space (Mikolov et al. 2013; Pennington, Socher, and Manning 2014). Words that are used in similar contexts (and therefore share similar meanings) are positioned close together in the embedding space, whereas words that occupy very different contexts are located far apart. Social scientists have shown that the positioning of words in an embedding space preserves cultural information from model's training texts, such as words' connotations of masculinity or femininity, affluence or poverty, and thought or action (Boutyline, Arseniev-Koehler, and Cornell 2023; Garg et al. 2018; Kozlowski, Taddy, and Evans 2019; Stoltz and Taylor 2019).\nAlthough word embedding models mark a major advance in learning and representing semantic relations, they remain ill-suited for the task of generating new texts. Modern LLMs outperform classical word embeddings at task of language"}, {"title": "Autoregressive language modeling", "content": "LLMs such as GPT-3 are \u201cautoregressive\u201d language models, meaning they optimize the prediction of the next word given a sequence of previous words. Such models take as their input a \u201cprompt,\" and conditional on the sequence of words comprising the prompt, they generate a probability distribution for the next word in the sequence. A word is then randomly drawn according to this probability distribution and is appended to the original prompt. The next-word prediction task is then repeated using this newly extended prompt, generating yet another \u201cnext word.\u201d Because each new word is drawn stochastically from a probability distribution, we can conceptualize text generation as following a single pathway through a branching tree of potential next-words, growing a sentence one word at a time (Figure 1). By repeatedly inputting the same prompt to an LLM, an analyst can generate a distribution of directions a statement is likely to take.\u00b2 This autoregressive approach differs from early word embedding models like word2vec or prior transformer-based models like BERT which used preceding and following words to predict a central target word (Devlin et al. 2018; Mikolov et al. 2013). This bi-directional approach may benefit from the additional information of subsequent words but is inappropriate for the task of generating new text, in which only prior words are available.\nEquation 1 describes how the autoregressive language models calculate the probability of a given sequence of words (y1, y2,... yn) as the product of the probabilities of each word (yt) conditional on the prior words in the sequence (y<t).\n$$P(y_1, y_2...y_n) = \\prod_{t=1}^{n}P(y_t|y_{<t})$$\nGiven the multiplicative nature of joint probabilities, any statement more than a few words long tends to exhibit a very low probability. For example, the most likely completion in Figure 1, \"powerful entity on the\", would occur\""}, {"title": "Self-Attention", "content": "LLMs use distributed vector representations to encode the relations between words, but they advance beyond classical word embedding models by incorporating a mechanism known as self-attention that imbues word vectors with information from their local context (Vaswani et al. 2017). During training, word embedding models like word2vec treat local context as a \u201cbag of words,\u201d ignoring sequence and multi-word interactions (Mikolov et al. 2013; Pennington, Socher, and Manning 2014). Post-training, each word in a word embedding has a singular vector representation based on its contexts across the entire training corpus. By contrast, when a prompt is input into an LLM, self-attention mechanisms share information between all words in the sequence. Words have a singular representation only in the first layer of the model; as the sequence progresses through the model, each word's vector representation is adjusted and \"contextualized\" by the other words in the sequence.\nA schematic overview of the self-attention mechanism is displayed in Figure 2. Input embeddings are transformed through multiplication with learned weight matrices (WK,WQ, Wy) and subsequent multiplication between the resultant Key, Query, and Value matrices. Prior to multiplication, however, each weight matrix is split into many smaller matrices (96 in GPT-3) by column in what is called \"multi-head attention.\" Multi-head attention enables the model to attend to multiple versions of the input sequence simultaneously while also easing computation with improved parallelization. After the input embeddings are multiplied by the weight matrices, the key and query matrices are themselves multiplied, creating a square n* n matrix for a prompt of length n. Each entry [ni,nj] in this square matrix captures a meaningful \u201cinteraction\u201d between token i and token j in the prompt that answers the question \u201cfor query token x, what key tokens y from the sequence provide the most informative context\u201d. These attention weights, the dot products of x and y, are then normalized by the square root of the Key dimension and transformed via softmax, the multiple-outcome generalization of the logistic function, so that each row's values sum to 1 and the predictive power of each key token on a query token can be interpreted as a probability. These probability estimates are treated as weights and reshaped through multiplication with the \u201cvalue\u201d vectors for each token. The heads of the weighted Value matrix are then concatenated back into a single wide matrix which is multiplied by a final output weight matrix (Wo), producing the output embeddings that pass through a feed-forward neural network before exiting the transformer block. Across this self-attention layer, the learned parameters include the weight matrices (indicated with a \u03b8), and the input embeddings for the very first model layer."}, {"title": "Deep Architecture", "content": "The final factor enabling LLMs' success in producing humanlike texts is their massive neural architecture. Word embedding models like word2vec use a shallow architecture with a single hidden layer, and the total number of parameters learned by the model is typically in the tens of millions.\u2075 By contrast, GPT-3's neural network consists of 96 layers and 175 billion parameters (Brown et al. 2020). GPT-4's architecture has not been formally released, but expert consensus is that GPT-4 is substantially larger, with parameters likely numbering over one trillion (OpenAI 2023a). A greater number of parameters and layers enables a neural network to learn more complex functions. For a language model to faithfully encode the multitude of discourses that appear on the internet, it must compose an exceptionally complex function. This function leverages the extensive non-linearities and interactions between words to transform an input sequence into an accurate probability distribution for the next word. In learning these complex patterns of linguistic entailments from its training texts, the model effectively learns the internal structure of a discourse.\nFigure 3 presents a schematic diagram of GPT-3's architecture. During training, a sliding window of words from the training text is used as the context, and is represented as a matrix of corresponding word vectors. This collection of word vectors then passes through 96 \u201ctransformer blocks.\u201d Each transformer block comprises attention mechanisms that are themselves divided into 96 \"attention heads\" followed by a feed-forward neural network which further transforms the contextualized word vectors in preparation for predicting the next word. After passing through all transformer blocks, the resulting matrix is converted into a single vector corresponding to the model vocabulary. This vector is rescaled into a probability distribution via the softmax function, a multi-category generalization of the logistic curve. This probability distribution is then compared to the correct response \u2013 the actual next word in training texts. Error, or \"loss,\" is calculated as a function of the difference between the predicted probabilities and the correct next word, and this error is propagated back through the model to update parameters such that the same prediction would be more accurate if made again. The context window then continues its progression through the training texts, repeating the task of predicting each subsequent word given the prior words. The algorithm may iterate over the entire corpus of training texts multiple times until improvements become negligible and training is halted.\nOnce fully trained, LLMs can generate novel texts by the same operation of next-word prediction that is optimized during training. The user feeds the model a prompt and the algorithm iteratively predicts the following words one by one, appending each newly generated word to the prompt to predict the subsequent word. In this way, the LLM builds"}, {"title": "Digital Doubles and the Study of Ideology", "content": "Through their ability to recreate discourse models underlying their training texts, LLMs offer new avenues for studying cultural-historical change and advancing theories of meaning. In particular, we use LLMs to shed new light on the nature and extent of constraint in cultural systems. Structuralist theories emphasize the overall coherence of meaning systems, positing that seemingly elaborate systems of classification and evaluation are reducible to simple underlying logics (Douglas 1966; L\u00e9vi-Strauss 1966). The diverse array of practices, values, and beliefs expressed within a culture can be understood as \"all of a piece,\" unified by subtle threads of configured meaning that can be revealed through careful analysis (Mead 1942). This coherent model of collective meaning has important implications for cultural change; new ideas can only be integrated into an ideology if consistent with the overarching logic of the system. By this theoretical model, cultural evolution is largely predictable because it is highly constrained. When a new object enters a cultural context, its potential interpretations are tightly limited by the logic of the cultural context.\nThis constrained model of cultural coherence was largely displaced by a wave of scholarship that emphasized the importance of historical contingency and internal contradictions of cultural systems. To redress the structuralists' failures to capture the apparent arbitrariness and contradiction of culture, the image of a unified system was supplanted within sociology by a model of culture as a \u201ctoolkit,\u201d a repertoire of strategies that can be taken up or discarded as necessary (Swidler 2003), or a largely disconnected set of cognitive schemata which are deployed situationally (DiMaggio 1997) to satisfy a pragmatic purpose. More recent work has sought to identify a theoretical middle-ground, accepting that systems of meanings exhibit broad patterning, but remain only weakly constrained, rife with instability, ambivalence, and contradiction (Baldassarri and Goldberg 2014; Boutyline and Vaisey 2017; Kiley and Vaisey 2020; Rawlings 2020; Swidler 2003).\nA similar debate has unfolded in the field of political opinion. One strand of research argues that voters hold core values and form opinions on specific issues in accordance with basic underlying considerations such as freedom, equality, or protection from harm (Feldman and Zaller 1992; Goren et al. 2016; Haidt 2012; Lakoff 2010). This theoretical model posits that voters' conceptual systems are constrained by an internal logic with a few core values structuring a complex attitude system that covers a diverse array of specific issues. Detractors of this theory argue that individuals' attitudes are steered not by internal values, however, but by partisanship. According to this top-down alternative, partisan leaders construct a platform of positions through a strategic process of \"log-rolling,\u201d with the aim of building a coalition across various interests (Carmines and Stimson 1989; Zaller 1992). Partisan leaders then broadcast this assortment of positions as a unified platform, which is then absorbed in toto by strong partisans in the electorate (Green, Palmquist, and Schickler 2004). By this model, the collection of opinions that constitute an ideology are the arbitrary result of historical-political contingencies; held together not by a unifying cultural logic, but social and political messaging.\nThis debate over ideological coherence revolves around a key empirical question. When a new issue emerges, are public responses already prefigured by ideology, or is a new issue ideologically indeterminate until partisan elites voice positions on the issue (Noel 2014; Page and Shapiro 2010; Zaller 1992)? If political ideologies are general dispositions that inform attitudes across issues, they could be readily transposed to new issues without elite direction. If ideologies are arbitrary assortments of attitudes strategically drawn together through political coalitions, however, then the politicization of a new issue would be unpredictable until partisan elites broadcast their positions. Thus, study of the exogenous injection of a new issue within a political landscape sheds light on a key theoretical question in the sociology of culture \u2013 when a new idea is introduced into an existing system of beliefs, is it constrained by the logic of the belief system or free to take any direction?\nLLMs offer a powerful new approach for exploring this core question in the study of culture and ideology. Typically, by the time the public learns about a new issue, it has already been politicized by partisan elites. Thus, when ideology in the electorate mirrors party platforms, analysts are unable to adjudicate if elite cues steered public opinion or if underlying ideological convictions shaped the new issue's interpretation for both party elites and the public at large. The key benefit of LLMs is that they capture and preserve discourses from the historical period of their training texts. Therefore, an LLM trained on texts from a given time can simulate respondents from that period and present them with questions that anticipate future cultural or political developments. While previous text analytic models produce representations of cultural systems from historic texts, LLMs are generative models to which researchers can pose novel questions. These prompts could even include hypothetical scenarios or issues that only emerged after the model's training. By leveraging the vast linguistic information learned in training, generative cultural models produce the most likely responses to novel questions given the discursive associations of that period. This method enables new insight into the historical process of cultural change, providing a lens through which we can assess which developments are truly surprising and which are already prefigured by the cultural system.\nMoreover, the generative nature of LLMs enable us not only to assess distinct perspectives, but to interrogate them. In his classic 1940 article on \u201cSituated Actions and Vocabularies of Motive\u201d, sociologist C. Wright Mills argued for the importance of analyzing the shared language by which persons from distinct socio-cultural situations justify and\""}, {"title": "2019 Politics in Silico", "content": "The spread of COVID-19 to the United States presented a critical event for social and political meaning making. Lacking any precedent in living memory, the pandemic was not readily interpretable within existing frames for political response. This is evident from the earliest public opinion surveys, which show relatively little partisan division on questions relating to the virus (Deane, Parker, and Gramlich n.d.). Nevertheless, political elites and opinion leaders began to broadcast a variety of competing interpretations of the situation as soon as a virus was detected in the US in January of 2020 (Stokes et al. 2020). By March 2020, a sizable divide had already grown between self-identified democrats and republicans regarding the appropriate government response to the emerging pandemic (Gadarian, Goodman, and Pepinsky 2021). This politicization of COVID-19 would prove to be a defining characteristic of the pandemic period, imbuing discussions of lockdowns, masks, and vaccines with partisan fervor, ultimately stymying any unified public response to the virus (Albrecht 2022; Allcott et al. 2020; Chen and Karim 2022).\nThe politicization of COVID-19 related issues quickly assumed a common pattern. Liberals viewed the virus as an urgent threat warranting immediate and sweeping response, whereas conservatives questioned the danger posed by the virus, denounced responses that infringed on individual liberties, and doubted the safety of the government-sanctioned vaccine. After years of pandemic politics, this familiar pattern may appear self-evident, and there are indeed some reasons to view this polarization as predictable. On a variety of issues, ranging from gun control to universal healthcare to motorcycle helmet requirements, liberals tend to favor protection and government intervention while conservatives lean towards freedom and personal choice (Homer and French 2009). To explain this pattern, some public opinion analysts have argued that conservatives more highly value freedom while liberals prioritize considerations of equity and protection from harm (Feldman and Zaller 1992; Haidt 2012).\nNevertheless, a large literature from political psychology plausibly anticipates the very opposite empirical outcome and supplies numerous reasons to expect that conservatives would support stricter responses to the virus than liberals. In an influential review of the psychological correlates of political ideology, Jost (Jost 2006) cites robust international evidence that political conservatism is associated with (i) fear of death, (ii) aversion to threat or loss, (iii) uncertainty avoidance; and (iv) needs for order, structure, and closure. Each of these predispositions suggest that conservatives should be the ones to advocate for harsher measures to protect against the virus, not liberals. Moreover, a wide array of studies suggest that conservatism is associated with desire for purity and aversion to contamination. This literature emphasizes forms of social or religious impurities, but more broadly connects political conservatism to a general fear of contamination and uncleanliness (Haidt 2012; Helzer and Pizarro 2011; Jost 2017; Oxley et al. 2008; Terrizzi, Shook, and McDaniel 2013). Consistent with this pattern, Republicans were more concerned about the Ebola epidemic of 2014 than Democrats (Pew Research Center 2014).\nSimilarly, there are reasons to suspect that liberals would be skeptical of sweeping government responses to the virus. Liberals have a long history of skepticism toward vaccines, arguing that they are unnatural and pushed by large, profit-driven pharmaceutical companies (Callaghan et al. 2019; Colgrove 2006; Conis 2014; Jamison, Quinn, and Freimuth 2019). Also, in recent historical cases where the safety of the American people has been at stake, such as the War on Terror, conservatives were more willing to sacrifice personal liberties for public safety than liberals (Rosentiel 2011). All of these considerations suggest that history could have played out differently, and that an alternate framing for COVID-19 was plausible, which would steer conservatives to endorse cautious measures and liberals to oppose"}, {"title": "DATA AND METHODS", "content": "For the following analyses, we analyze completions generated by the GPT-3 language model. This approach presents a notable departure from conventional methods of text analysis. LLMs are generative models, and the most straightforward way to learn from these models is not to examine their internal representations but to study the outputs they produce. As a result, the data we analyze are not actual statements made by members of our target populations nor are they representations of such statements like topic models or word embeddings. Our data are novel word sequences learned to occur with high probability given the discursive patterns learned in a vast training corpus.\nSuch an approach deviates from the simulation studies typical of formal sociology. Formal models commonly attempt to parsimoniously capture social dynamics by precisely specifying minimal conditions under which empirically-observed patterns can be reproduced. Using LLM outputs as data, by contrast, constitutes a hybrid of empirical and formal sociology; we analyze simulated data, but the simulation is trained to produce outputs that closely approximate empirical distributions. To the extent that the model successfully reproduces a population's response distributions, the model's outputs can stand in for human responses from that population and be analyzed at scale. We adopt this approach to simulate the opinions of American liberals and conservatives in October of 2019, the historic point immediately prior to the emergence of COVID-19."}, {"title": "Prompt Design", "content": "Our primary aim is to discern whether a speaker identifying as liberal is predicted to have different views regarding pandemic responses than a speaker identifying as conservative. We therefore design prompts that isolate the effect of partisan identification words on the generation of responses toward a variety of COVID-19 issues. We use three modes of partisan identification: ideological identification (liberal or conservative), party identification (Democrat or Republican), and candidate preference (Hillary Clinton or Donald Trump). To maximize the partisanship signal, we use all three of these identifiers in all our prompts. All prompts begin with a \u201cpartisan priming\u201d taking the following form:\n\u201cI am a strong conservative and a lifelong Republican. In 2016, I was proud to vote for Donald Trump and I think that the Democrats have been a disaster for this country.\nor, conversely:\n\u201cI am a strong liberal and a lifelong Democrat. In 2016, I was proud to vote for Hillary Clinton and I think that the Republicans have been a disaster for this country.\nBecause GPT-3 was trained on texts published only through October 2019, it has no knowledge of COVID-19. This ignorance is an asset, as it allows us to investigate the ideological landscape immediately prior to the emergence of this pivotal issue. However, in order for the model to produce an informative response, we must supply some basic"}, {"title": "Machine Coding Responses", "content": "We designed our prompts to encourage relatively standardized completions indicating either a positive or negative response. Total standardization was not desirable, however. LLMs are built to be \u201cprogrammed\u201d natively with language inputs and to produce natural language outputs.\u00b9\u2070 In pilot testing, we experimented with prompting the model to produce closed-form responses, but these structured responses performed markedly worse than open-ended responses"}, {"title": "Generating Justifications", "content": "For select prompts, we go beyond simply identifying differences in responses to COVID-19 and attempt to shed light on why the model is predicting these differences. We elicit this by prompting the model to produce a second sentence justifying its initial response and reveal the characteristics of ideologically consistent motives. For these tests, we input the original prompt to GPT-3 along with the previously generated completion, which is restricted to one sentence. We then extend this prompt by beginning a new sentence offering a justification for the earlier response. Specifically, we append the phrase \"This is because\" to the end of the prompt to induce a justification response. We find that alternate wordings produce substantially similar outputs and include examples in the Appendix.\nFor each prompt, we generate three \u201cjustification\u201d completions. Because we initially generate 500 liberal and 500 conservative responses to each question, this results in a total of 3000 justifications. To classify these numerous open-ended responses into a few informative categories, we again rely on machine coding. As above, we use GPT-3 embeddings to represent each justification as a 1026 element vector. But because we want categories of justifications to emerge inductively, we use k-means clustering to identify thematic groups instead of rating responses along a predetermined axis. Because justifications in favor of a given policy should be qualitatively different from those opposing the policy, we conduct k-means clustering separately for positive and negative responses as scored in the prior step; for instance, we first perform cluster analysis on the justifications of statements in favor of mask mandates, then we conduct another independent cluster analysis of all justifications for statements opposing them. We manually select the number of clusters by considering three metrics (Silhouette, Calinski\u2013Harabasz, and Davies-Bouldin scores) along with our qualitative assessment of interpretability and parsimony. After dividing justifications into clusters, we generate labels for each cluster using GPT-4. We feed a random sample of 100 entries from each cluster into a GPT-4 prompt with instructions to provide labels for each set of responses that describes their distinctive semantic characteristics.\u00b9\u00b2 By comparing the proportions of liberal and conservative responses in each of these clusters, we identify partisan differences in how opinions are justified."}, {"title": "Validation", "content": "To confirm that our prompt induces partisan differences as expected, we conduct a series of validation tests on political issues already well-established in 2019. Using the same style prompt described above excluding only the passage about COVID-19, we generate liberal and conservative responses to variously worded questions on topics of abortion, climate change, gender and sexuality, race, immigration, drugs and policing, gun control, healthcare, welfare state programs, and business regulation. Across these 10 topical areas, we pose 37 distinct questions, and each question had multiple wordings. For 35/37 questions, the majority of wordings correctly predicted empirical partisan differences on that issue. For one question, the association was in the incorrect direction for 2/4 wordings, and for one question, no association was identified. Results are presented in Appendix A. These results also reveal systematic differences in the effect of question wordings on stated positions. For example, asking a liberal- or conservative-prompted model its \u201cstance\" on an issue almost always brought the answer close to the center of the distribution, whereas when an \"opinion\" is elicited, we find responses are more markedly partisan.\nThese results provide confirmatory evidence that the prompts we designed effectively induce partisan divides observable in American politics circa 2019. This does not definitely establish that the model is equally accurate in estimating American attitudes toward a hypothetical virus in 2019. The familiar political issues are within the training distribution, whereas responding to questions about COVID-19 requires out-of-distribution generalization. We have no \u201cground truth\" for what attitudes toward COVID-19 would have been in 2019. Indeed, if such a data source existed we would not need to rely on simulation. But on those attitudes for which validation is possible, we find encouraging evidence that our prompts faithfully reproduce observable partisan divisions.\""}, {"title": "RESULTS", "content": "We begin by presenting results from prompts on vaccine-related topics. Each of the figures below is a coefficient plot showing the effect of \"partisan priming\u201d on the classification scores for a given prompt's outputs. Partisan priming is a binary variable coded 1 = \"liberal\", so positive coefficients indicate that liberal prompts were more likely to endorse the first anchoring term in the response dichotomy (e.g. \"good idea\"). This equivalently means that conservative prompts are more likely than the liberal prompt to endorse the second anchoring term (e.g. \u201cbad idea\"). Coefficients with significant positive effects (p<0.05) are colored blue. Negative coefficients conversely signify that conservatives are more likely to endorse the first anchoring term than liberals. Coefficients with significant negative effects are colored red.\""}, {"title": "DISCUSSION", "content": "LLMs can serve as powerful tools for the analysis of language and culture. These models are unique in that they do not merely provide a map of associations, they generate new texts consistent with the linguistic patterns and discursive styles on which they were trained. It is therefore possible to use LLMs to create \"digital doubles\" of actors fluent in discourses included in the model's training texts. In this study, we reconstructed the political opinion landscape of a pivotal period, the year preceding the spread of COVID-19, to determine whether the following politicization of the pandemic was predictable given the existing regime of discourse and politics. We find that the model predicts the correct direction of politicization far better than chance across a wide array of pandemic-related issues. To gain insight into how the model made these predictions, we prompt the LLM to produce justifications for its responses. We find on key issues that the distrust of institutions and the prioritization of personal freedom characteristic of American conservatism corresponded with their greater rates of rejecting sweeping policies to restrict the spread of the virus, whereas liberal prompts tend to justify strong collective responses to the virus with appeals to government responsibility and public safety. These results suggest that the way the pandemic politicized was largely consistent with existing repertoires of American liberalism and conservatism, and that the pandemic was not an occasion of substantial political innovation or surprise.\nThese findings speak to a fundamental question in the study of culture \u2013 to what extent are historical developments constrained by culture? When a new issue emerges, is it interpreted within an existing system of understanding, or is it \"up for grabs,\u201d with political and cultural entrepreneurs offering competing frames for interpretation? Our findings suggest that, in the case of COVID-19's polarization, existing schemas of political sense-making steered the issue's reception. While President Trump's early statements downplaying the need for drastic responses may have helped crystallize the observed pattern of polarization, our evidence suggests that this stance was consistent with a general ideological tendency that predated the virus's emergence and channeled a zeitgeist already present among Trump's constituency. We do not deny that the political response to the COVID-19 pandemic could have unfolded differently,"}]}