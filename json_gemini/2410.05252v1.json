{"title": "Causal Micro-Narratives", "authors": ["Mourad Heddaya", "Qingcheng Zeng", "Chenhao Tan", "Rob Voigt", "Alexander Zentefis"], "abstract": "We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model-a fine-tuned Llama 3.1 8B achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research.", "sections": [{"title": "1 Introduction", "content": "In recent years, social scientists have increasingly recognized the power of narratives (i.e., popular stories about economic, political, or social topics) to shape individual and collective behavior. These narratives can influence people's beliefs and decisions-like when to invest in the stock market, buy a home, or pursue higher education-and can quickly spread through the collective consciousness. Nobel Prize-winning economist Robert Shiller argues that if we fail to consider and understand the properties of narratives, \u201cwe remain blind to a very real, very palpable, very important mechanism for economic change, as well as a crucial element for economic forecasting\u201d (Shiller, 2017). While the importance of narratives has become well recognized, formulating an operational defi-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Definitions and Theoretical Frameworks", "content": "Early work by Labov and Waletzky (1997) defined narratives as temporal accounts of event sequences, providing a formal framework for analyzing personal narratives. Building on this, Akerlof and Snower (2016) expanded the definition to include causally linked events and their underlying sources, emphasizing the role of narratives in decision-making processes.\nMore recent work has further refined these concepts. Eliaz and Spiegler (2020) represent narratives as directed acyclic graphs (DAGs), drawing on Bayesian Networks to model the equilibrium of narratives. Shiller (2017) likened narratives to viral phenomena, defining them as interpretive stories about economic events that spread contagiously. Benabou et al. (2018) focused on the persuasive aspect of narratives in moral decision-making, while Flynn and Sastry (2022) emphasized their contagious nature in belief formation.\nMorag and Loewenstein (2023) and Barron and Fries (2023) both highlight the causal and inter-"}, {"title": "2.2 Methodological and Empirical Studies", "content": "Studies have proposed different methodologies to empirically measure economic narratives. Jalil and Rua (2016) analyze word frequency in newspapers and forecasts to study inflation expectations during the Great Depression. More advanced NLP techniques have been applied as well. Lange et al. (2022) extended the RELATIO method of Ash et al. (2021) to extract narratives based on Roos and Reccius (2021)'s definition. Gueta et al. (2024) try to leverage LLMs to extract and summarize economic narrative from tweets. However, they do not clearly define economic narrative nor do they evaluate the LLM's performance. Flynn and Sastry (2022) utilize sentiment analysis on firm 10-K filings to build a macro model explaining economic fluctuations. Andre et al. (2023) use open-ended surveys and DAGs to study narratives around recent high U.S. inflationary period. They contrast the narratives that households and experts write down, finding that household narratives significantly shape expectations. Their work also include experiments manipulating narratives to measure their impact on inflation expectations.\nAli et al. (2021) survey the broader field of causality extraction from text. Most causality extraction tasks are general domain, but existing methods are not very robust to complex sentence structures. Recent work by Sun et al. (2024) proposes a promising prompt-based technique with large language models to extract causal relationships in fictional stories instead of news text."}, {"title": "3 Causal Micro-Narratives", "content": "We define a causal micro-narrative as\na sentence-level explanation of the\ncause(s) and/or effect(s) of a target sub-\nject.\nThe term \u201cnarrative\u201d is most commonly applied to the discourse-level conception of story-telling that depicts sequences of events, usually in long-form texts (e.g., Piper, 2023). By contrast, here we focus on narrative fragments within individual sentences, which can capture stories about implicit and explicit cause-effect relationships that people express as they speak or write, sometimes in subtle or subconscious ways. Recent work in cognitive science highlights the prevalence of causal connectives in English and how they reveal the importance of causal relationships in the way we think and express ourselves (Iliev and Axelrod, 2016; Brown and Fish, 1983; Sanders and Sweetser, 2009)."}, {"title": "3.1 Narrative Classification Task", "content": "We propose a narrative classification task that operationalizes our definition of causal micro-narratives. Unlike the more general task of causality mining (Ali et al., 2021), we suggest that a productive approach to capturing how such micro-narratives accumulate at scale should be domain-specific. Specifically, we propose a framework in which we first identify a target about which we hope to capture micro-narratives. Conceptually a target can by any entity, event, or phenomenon of interest.\nThen, we define an ontology of the causes that can lead to that target and the effects that can follow from it. Thus, the narrative classification task is to identify, according to the ontology, sentences that express a narrative about the target subject and to predict the particular cause(s) and/or effect(s) related to the target that are present."}, {"title": "3.2 Case Study: Inflation Narratives", "content": "As an application of this definition and for the purposes of this paper, we focus specifically on inflation as the target. We develop an ontology, presented in Table 1, consisting of 8 causes of inflation and 11 effects that could follow from inflation. The causes and effects were curated by an expert economist based on domain knowledge and researching relevant resources online. See Appendix B for additional details on this process, and detailed descriptions of all the causes and effects. Ultimately, we setup the following classification task: given a sentence, identify (1) whether the sentence expresses a narrative about inflation, and (2) the expressed cause(s) and/or effect(s) of the inflation.\nFor this case study, we choose a target event that is fairly unambiguously summarized by a single word, inflation, which allows for straightforward data filtering. Nonetheless, the causal micro-narrative classification task could be applied to target events or phenomena that are expressed in more varied ways, but this would introduce more complicated filtering strategies or an additional prelimi-"}, {"title": "4 Dataset", "content": "We use two data sources in our investigation of inflation narratives in news: NOW Corpus for contemporary news data (Davies, 2016) and ProQuest for historical data. We selected these datasets because their differences allow us to assess the generalizability of our task and the classification methods we test. The articles in each dataset were written roughly 50 years apart and the NOW corpus includes a high degree of stylistic variation, as the articles are sourced from a range of online sources.\nFor each dataset, we segment articles into sentences and filter sentences that contain the keyword \"inflation\". Filtering allows us to focus on relevant sentences, enabling us to efficiently target our human annotations, as well as reduce the total number of sentences to a more computationally feasible quantity."}, {"title": "4.1 Contemporary News: NOW Corpus", "content": "We use data from the NOW Corpus covering 2012-2023. The dataset consists of online news articles, which we filter to only include U.S. articles written in English. The final filtered dataset, including \"inflation\" keyword filtering, contains 118,383 articles and 284,220 sentences. We use the spaCy Sentencizer (Explosion, 2024) for sentence segmentation."}, {"title": "4.2 Historical News: ProQuest", "content": "For historical news data, we collect news articles from local, regional, and national news publications from the ProQuest database spanning 1960-1980. See Appendix A for a list of the included publications. We chose this historical period because of the high levels of inflation that occurred throughout it, presenting an interesting opportunity to explore inflation narratives. The final dataset, including \"inflation\u201d keyword filtering, contains 392,475 articles and 751,380 sentences. We used the BlingFire (Microsoft, 2024) sentence segmentation tool, as the spaCy Sentencizer did not work well on this historical data."}, {"title": "4.3 Human Labeling", "content": "Three members of our team manually annotated training and test sets. We targeted train sets of approximately 1,000 examples. This provided us with sufficient training data for model fine-tuning. For the test sets, all three annotators label the same subset of data.  For ProQuest, annotators initially labeled a test set of 500 sentences, however, this is reduced to 488 after filtering out texts longer than 150 words when the sentence segmentation failed."}, {"title": "4.4 Descriptive Statistics", "content": "We focus on causal micro-narratives to ensure that we distinguish between general mentions of inflation in news text and a more targeted framing that presents causal stories about inflation. Analysis of the human annotations reveals that 49% and 47% of the contemporary and historical news sentences, respectively, were labeled as non-narratives. Given that these sentences are already keyword-filtered to include inflation, this amounts to a significant fraction of them and supports the intent of our definition and annotation scheme.\nThe distribution and prevalence of cause and effect narratives remains largely consistent across human annotations of both datasets.  Exceptions include fiscal and govt, which are more prevalent in historical news, and rates, which occurs more frequently in the contemporary data. These outliers reflect overall differences between inflation-related news in the 1960s and 1970s compared to the 2010s. These particular differences can likely be attributed to the fact that interest rate adjustment as a response to inflation did not become a significant tool deployed by the Federal Reserve until Paul Volcker's tenure as Chairman of the Fed in the 1980s (Siegel, 1998). As such, during the 60s and 70s, government spending and its relationship to inflation (fiscal, govt) was"}, {"title": "5 Methods", "content": "To determine the most effective approach to classify narratives, we compare the performance of LLMs on our classification task for both in-context learning and fine-tuning settings. We focus on these two settings We format the annotations associated with each sentence as JSON to facilitate automatic processing (see Appendix D). The LLMs are evaluated on their classification output, expected to be in JSON as well. We conduct separate experiments with the contemporary and historical data and train separate models for each dataset."}, {"title": "5.1 In-Context Learning", "content": "LLMs have been shown to be effective in-context, or few-shot, learners (Brown et al., 2020), so we tested GPT-40 in this setting by providing definitions for all the labels along with 24 narrative classification examples, one for each distinct cause and effect, as well as 5 examples of non-narratives. We use greedy decoding and do not constrain the generation in any way, but find that GPT-40 reliably generated JSON in the correct format."}, {"title": "5.2 Fine-tuning", "content": "The second modeling approach we evaluate is fine-tuning two open-source, pre-trained LLMs: Llama 3.1 8B (meta-llama/Meta-Llama-3.1-8B) and Phi-2 (microsoft/phi-2). We chose these two models because they represent high quality LLMs that have performed well on LLM benchmarks. Additionally, because of their relatively smaller parameter counts compared to other recent LLMs, they are well suited for efficient inference at scale. Indeed, while this classification task test set is relatively small, the ultimate aim of our work is to enable researchers to do complex narrative classification tasks at the scale of millions of sentences from news articles across long time horizons.\nFor fine-tuning, the input consists of the possible causes and effects, their definitions, and a brief instruction. We include the full fine-tuning prompt in Appendix D. We follow standard auto-regressive language modeling but only back propagate the language modeling loss for tokens associated with binary and multi-class labels, rather than other tokens associated with the JSON notation. We use LoRA-based Parameter-Efficient Fine-Tuning (PEFT) (Hu et al., 2021) to train a subset of the parameters. See Appendix E for fine-tuning hyper-parameters."}, {"title": "5.3 Evaluation", "content": "We evaluate each aspect of a narrative classification separately using micro-averaged F1 scores. We use micro averaging, rather than weighted- or macro-averaging to get an overall picture of model performance across all instances, including less represented classes. Micro-averaged scores use the standard binary-F1 score formula, but, importantly, the precision and recall scores are based on true and false positives across all instances, irrespective of individual class distinctions. Because each sentence could have narratives with multiple causes and/or effects, micro-averaged F1 differs from a regular accuracy score.\nTo resolve disagreements between annotators in"}, {"title": "6 Results", "content": ""}, {"title": "6.1 In-Domain Generalization", "content": "When trained and evaluated on the same individual dataset, Phi-2 outperforms other models. Interestingly, however, Llama 3.1 8B is better able to learn from both the Historical and Contemporary datasets, exhibiting impressive improvements of up to 14%, despite the 50-year gap between the news in the two datasets. In contrast, Phi-2 struggles and even degrades in performance on Contemporary data multi-class classification. All models perform better on contemporary data, likely because recent text and language from 2012-2023 are more prevalent in their pre-training corpora than historical newspaper data."}, {"title": "6.2 Out-of-Domain Generalization", "content": "On the multiclass narrative classification task, a common pattern emerges across both fine-tuned models. We observe that test set performance degrades by 3-4% on OOD data relative to in-domain data. This represents a moderate drop in performance and could be attributed to changes in the distribution of narratives across the Historical and Contemporary datasets, as explained in Section 4.4 and Figure 2. In contrast, the binary prediction task reveals a different effect. Phi-2 performs the same regardless of which dataset is used for training and which is used for testing but Llama 3.1 8B achieves up to an 11% improvement on narrative detection in Historical news sentences when trained on the Contemporary data. In the reversed setting, Llama 3.1 8B performance degrades by 7%. This pattern suggests that training Llama on Contemporary data is more successful than Historical data."}, {"title": "6.3 Error Analysis", "content": "To better understand model performance on this task and the variation between fine-tuning a smaller LLM and few-shot prompting a large propriertary LLM, we conduct a fine-grain analysis of the individual narrative classification predictions as well as an analysis of the three sets of human annotations to better understand the disagreements that exist between them and how those disagreements may related to model prediction errors. As the best performing LLM overall, we focus on Llama 3.1 8B (henceforth, Lllama) and compare it to GPT 40, the only propriertary model in our experiments."}, {"title": "Human Annotator Disagreements", "content": "By majority rule, our three human annotators find partial agreement on 474 out of 488 test set instances, and full agreement on 471. While this is a higher rate of majority agreement, there are nonetheless non-negligible disagreements between individual annotators. Since we use training data sourced from each annotator individually, understanding these disagreements can contextualize how model performance is impacted. Most annotator disagreements stem from differing judgments on narrative presence, not category assignment. Annotators rarely clash over which specific narrative category to apply, but often diverge on whether a narrative exists in the text at all. Furthermore, certain annotators are systematically more likely to detect narratives than others, driving this specific form of disagreement."}, {"title": "Hallucinating Narratives", "content": "Fine-tuning is effective at teaching a model to distinguish between narratives and non-narratives, compared to in-context learning. GPT-40, which was not fine-tuned, correctly classifies roughly 47% and 60% fewer non-narratives in the contemporary NOW and historical ProQuest test sets, respectively, than Llama. Despite extensive experimentation with different prompts, we consistently observed that GPT-40 struggled to understand the distinction we stipulate between narratives and non-narratives. We can likely attribute this to our precise definition of narrative, such that these otherwise highly capable LLMs have limited in-context demonstration data to draw on to learn this capability."}, {"title": "Natural Variation & Ambiguity in Language", "content": "Table 5 presents several instances where Llama predictions did not match the human labels. The first three examples illustrate that Llama's impressive 0.87 F1 score on binary narrative detection comes at the cost of false negative predictions. In fact, these three instances of failing to predict Social & Political Impact (social) are representative of the most common type of false negative error in Llama predictions. Interestingly, annotating social or not is the most common disagreement of this type among the annotators. Nonetheless, the three examples in Table 5 show failures of Llama to identify the implied, yet clear, references to inflation's social and political impact.\nIn contrast, the final four examples demonstrate the natural ambiguity and difficulty inherit in this task. Consider the fourth sentence. While to a human, it may be quite natural to understand this sentence as inflation being the cause of the job destruction, lower family income, and increased taxes, it is not explicit in the sentence. In fact, the more explicit mention of causation in the sentence is \"they have and will cause inflation\". Llama predicts a cause of inflation narrative (\u201cfiscal", "govt, purchase, cost-push\"). In practice, this sentence does not mention who \\\"they\\\" is referring to, so the prediction, while a reasonable guess, is not supported. The final three examples show scenarios where the Llama predictions and human annotations could both be considered correct, depending on one's perspective. All these examples illustrate the challenging nature of the task and the natural variation that is inherent to it.\"\n    },\n    {\n      \"title\"": "7 Conclusion"}, {"content": "This paper proposes a causal micro-narrative classification task. By developing a comprehensive classification scheme and leveraging both fine-tuned and few-shot prompted large language models, we demonstrate the feasibility of automating the detection and categorization of these narratives at scale. Our results show that fine-tuned models, particularly Llama 3.1 8B, outperform few-shot prompted models in distinguishing between narrative and non-narrative content, while maintaining competitive performance in classifying specific narrative types.\nThe error analysis reveals that the task of iden-"}, {"title": "8 Limitations", "content": "The method we propose for extracting and classifying causal micro-narratives requires the manual development of an ontology of causes and effects for any new target. This limits automated data-driven discovery of new narratives (i.e., causes and effects not already pre-established). However, the binary micro-narrative detection task included in this paper may be helpful in filtering a large corpus into a smaller dataset of sentences that contain narratives. This may facilitate discovering new narratives, either manually, or with an automated method. In this paper, we do not evaluate this use-case but we believe this to be a good direction for future work."}, {"title": "Appendix", "content": ""}, {"title": "A ProQuest Newspapers", "content": "Chicago Tribute, Chicago Defender, Los Angeles Times, Los Angeles Sentinel, Atlanta Daily World, Cleveland Call and Post, Detroit Free Press, Indianapolis Star, Kansas City Call, Louisville Courier Journal, Louisville Defender, Michigan Chronicle, Minneapolis Star Tribune, New York Amsterdam News, New York Tribute / Herald Tribune, Norfolk Journal and Guide, Philadelphia Tribune, Pittsburgh Courier, Pittsburgh Post-Gazette, San Francisco Chronicle, St. Louis American, St. Louis Post Dispatch, The Baltimore Afro-American, The Boston Globe, The Christian Science Monitor, The Cincinnati Enquirer, The Nashville Tennessean, The New York Times, The Wall Street Journal, The Washington Post, U.S. Newsstream, U.S. Major Dailies."}, {"title": "B Classification Task", "content": ""}, {"title": "C Annotation Interface", "content": ""}, {"title": "DLLM Prompts and Inputs", "content": "Due to the hierarchical multi-label classification task, we represent a complete narrative classification as JSON. This paper focuses only on the prediction results; i.e., the values associated with the fields \"contains-narrative\" and \"narratives\". However, our task includes additional information which we will discuss in future work. We define the JSON schema as follows:\n{\n  \"foreign\": true|false,\n  \"contains-narrative\": true|false,\n  'inflation-narratives': [\n    \"inflation-time\": \"past\" | \"present\" | \"future\" | \"na\",\n    \"inflation-direction\":  \"down\"|\"up\"|\"na\",\n    \"narratives\": [\n      {\"causes\"|\"effect\": category, \"time\": \"past\"|\"present\"|\"future\" | \"na\"},\n    ]\n  ] | null\n}"}, {"title": "E Hyperparameters", "content": ""}, {"title": "F Confusion Matrices", "content": ""}]}