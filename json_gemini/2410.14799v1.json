{"title": "Deep Generic Dynamic Object Detection Based on Dynamic Grid Maps", "authors": ["Rujiao Yan", "Linda Schubert", "Alexander Kamm", "Matthias Komar", "Matthias Schreier"], "abstract": "This paper describes a method to detect generic dynamic objects for automated driving. First, a LiDAR-based dynamic grid is generated online. Second, a deep learning-based detector is trained on the dynamic grid to infer the presence of dynamic objects of any type, which is a prerequisite for safe automated vehicles in arbitrary, edge-case scenarios. The Rotation-equivariant Detector (ReDet) \u2013 originally designed for oriented object detection on aerial images was chosen due to its high detection performance. Experiments are conducted based on real sensor data and the benefits in comparison to classic dynamic cell clustering strategies are highlighted. The false positive object detection rate is strongly reduced by the proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The perception and representation of the environment is a key ingredient in automated driving systems. A multitude of data fusion methods and ways of modeling the local area around the ego vehicle have been proposed [1], [2]. With regard to dynamic objects, the majority of work focuses on the detection of known object classes such as vehicles, cyclists, or pedestrians. Deep neural networks are trained on established labeled datasets such as KITTI or nuScenes based on camera, LiDAR, and RADAR data to detect such predefined object classes [3], [4]. In reality, however, the spectrum of objects that can be dynamic is not limited to predefined classes, but nearly anything can move. Examples include shopping carts, rolling tires, or all kinds of animals. But also standard classes such as vehicles exist in all kinds of non-standard appearances, see Fig. 1. Detectors trained on predefined object classes are incapable to perceive such generic dynamic objects \u2013 let alone to estimate their veloci- ties or accelerations, which can lead to dangerous situations.\nTo cope with this problem, so-called dynamic grid maps were introduced [5]\u2013[7], which do not make assumptions about the type or shape of dynamic objects and can estimate a full velocity vector distribution for each grid cell around the ego vehicle via particle filtering alongside arbitrary static environment structures. Dynamic objects are then de- tected/extracted from such dynamic grids via clustering tech- niques like DBSCAN [8], [9]. Alternatively, multiple subse- quent static grid maps are post-processed to create, track, and classify generic dynamic object hypotheses [10], [11]. Such hand-designed clustering and classification approaches are,\nhowever, subject to false positive detections since dynamic cells in dynamic grids can also emerge from swaying trees in the wind or from newly appearing static environment structures, which are hard to separate from true object motion. Therefore, various deep learning-based approaches were proposed to improve the grid-based dynamic object detection.\nOne of the early works of refining the separation of dynamic and static entities in dynamic grids can be found in [12], in which a fully convolutional network is trained to infer, whether individual dynamic grid cells are moving or not. The result is a cell-wise classification of the sur- roundings into the classes dynamic and static. No explicit dynamic object detections are outputted from the network. Object representations are, however, beneficial for subse- quent tasks in the automated driving stack such as situation interpretation, prediction, and planning. Therefore, a single- stage deep convolutional network was trained in [13] to directly detect dynamic object hypotheses from dynamic grids consisting of object shape, position, orientation, and an existence score. Promising results were shown on an exemplary urban junction for a stationary ego vehicle. In a follow-up work [14], a single-stage deep convolutional neural network was combined with a recurrent LSTM neural network taking dynamic occupancy grid maps as input and generating dynamic object bounding boxes as a result. The recurrent network is supposed to capture long-term sequen- tial relations, e.g. to overcome occlusions. Results were"}, {"title": "II. METHOD", "content": "We consider the generic dynamic object detection task as a rotated bounding box object detection problem [19] and treat dynamic grid maps as bird's eye view images. Unlike the majority of camera image-based object detection networks, which output horizontally aligned bounding boxes, rotated object detection, in addition, involves the prediction of the bounding box angle. Rotated object detection networks are normally applied on aerial images, e.g. to detect arbitrarily-rotated vehicles or ships.\nWe choose the Rotation-equivariant Detector [16] (ReDet) due to its high detection performance [19]. A dynamic occupancy grid map as explained in [6] is applied as input for the detection network, which generates orientated bounding boxes of generic dynamic objects as a result.", "A. Dynamic Grid Map as Network Input": "We employ the dynamic grid map algorithm presented in [6]. Each grid cell contains Dempster-Shafer basic belief masses m for each element, i.e. hypothesis 0, of the power set\n$2^{\\Theta} = {\\emptyset, {F}, {S}, {D}, {S, D}, {F, D}, {F, S}, \\Theta}$ (1)\nof a chosen frame of discernment \u0398 = {F, D, S}, so that the sum of all masses equals to one. Here, {F} refers to currently free areas, {S} to statically occupied areas, {D} to currently dynamically occupied areas, {S, D} to currently occupied areas (statically or dynamically occupied), {F, D} to passable areas, which are either free or dynamically occupied, and $\\Theta$ to unknown areas, i.e. areas, which are either free or occupied. The case that a cell is both free and statically occupied is excluded as it is conflicting by definition, therefore {F, S} is omitted.\nFor the visualization of such dynamic grid maps, we use the same color coding scheme as introduced in [6], i.e.\n$RGB = (1-\\sum_{\\{\\text{S}\\}\\setminus 0} m_{\\theta}, 1-\\sum_{\\{\\text{F}\\}\\setminus 0} m_{\\theta}, 1-\\sum_{\\{\\text{D}\\}\\setminus 0} m_{\\theta})$ (2)\nfor all hypotheses $\\theta \\subset \\Theta$. As a consequence, static occu- pancy {S} results in red (R), free space {F} in green (G), dynamic occupancy {D} in blue (B), unclassified occu- pancy {S, D} in magenta, passable areas {F, D} in cyan, and unknown areas $\\Theta$ in white, see Fig. 2.\nWe use the same color coding to encode a dynamic grid map into three channels to obtain a regular RGB image as input for the neural network. Optionally, two additional channels are added by using particle information available in the dynamic grid. The particles approximate the velocity distribution in each grid cell. The two additional channels are formed by normalizing the mean velocity components $v_x$ and $v_y$ of all particles of a cell.", "B. Neural Network Model": "According to [19], ReDet [16] with multiple scale im- age splits and random rotations outperforms most other rotated object detection methods. Therefore, it is selected for our task. Regular CNNs are translation-equivariant but not rotation-equivariant and consequently require a lot of rotation-augmented data to train an accurate detector for arbi- trarily rotated objects. In contrast, ReDet produces rotation- equivariant features in the backbone, which significantly reduces the complexity in modeling orientation variations. In the subsequent detection head, Rotation-invariant Region of Interest Align (RiRoI Align) steps extract instance-level, rotation-invariant features from rotation-equivariant features. For an instance-level, rotation-invariant feature, the output remains identical for any rotational transformation applied"}, {"title": "III. EXPERIMENTAL SETUP", "content": "The classic cell clustering method DBSCAN for extracting dynamic objects is used as a baseline. Only grid cells with a dynamic occupancy mass above a minimum threshold $MD,min$ are considered for DBSCAN clustering. Its param- eters \u2013 the maximum distance between cluster points $e_d$, the maximum velocity difference $e_v$, and the minimum number of cells for a cluster to be valid $e_n$ are fine-tuned manually. Based on experiments, the parameters are set as $MD,min$ = 0.5, $e_d$ = 1.5 m, $e_v$ = 3m/s, and $e_n$ = 4. The method is easy to implement and runs fast. However, it only considers dynamic grid cells and leads to false positives when dynamic occupancy masses are wrong due to incorrect cell velocity estimations, e.g. at guardrails or in case of swaying trees in the wind next to the street. In contrast, deep learning-based object detectors take the spatial scene context contained in the the full grid into account, which leads to improved detection results as later shown in Section IV.", "A. Baseline: Classic Clustering Method": null, "B. Generation of Ground Truth Data": "We use a roof-mounted VLS-128 LiDAR with an update rate of 10Hz to generate dynamic grid maps in real-time online in the vehicle. The data is collected from real-world highway and urban driving scenarios. Different locations were chosen to collect training and test data. To reduce the amount of similar training data, only every 5-th dynamic grid map is used in the labeling process.\nOur ground truth data consists of 3 parts - subsequently termed data 1, 2, and 3. Data 1 is manually labeled data. Since annotation is time-consuming, only 1450 frames are hand-labeled (858 frames for training, 287 for validation and 305 for testing). For data 2, we run the classic DBSCAN approach, see Section III-A, to auto-label dynamic objects with a manual post-processing to remove frames with false or inaccurate auto-labels. This part has 3964 frames (3295 for training, 313 for validation and 356 for testing). For data 3, we drove at night through empty streets without any dynamic objects, so that no annotation is required. It contains 1171 frames (603 for training, 100 for validation and 100 for testing). These frames are used as negative examples for the neural network to better learn to suppress false positives. In total, we have 5124 frames for training, 700 for validation and 795 frames for inference. These three subsets are summarized in TABLE I.", "C. Implementation Details": "We use the pretrained model based on DOTA v1.0 [20] for 12 epochs, ReResNet50 as backbone, and SGD optimizer with an initial learning rate of 0.00025. The learning rate is reduced by factor 10 at each decay step at 8 and at 11 epochs. Note that the learning rate is set very small because"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In the first results Section IV-A, we verify that the extension of the training dataset by data without manual labeling indeed improves the inference performance. For this purpose, we compare the model ReDet trained with combinations of different datasets. Subsequently, we evaluate in Section IV-B if it is beneficial to use the potentially redundant dynamic information contained in dynamic grids, i.e. dynamic cell masses and particle velocity information. To do so, we encode the network's dynamic grid inputs to either 3 channels {R, G, B} or to 5 channels {R, G, B, vx, Vy} with normalized mean velocity components vx and vy of all particles in a cell. After the best dataset and input channel option are fixed, ReDet is compared with RetinaNet in Section IV-C. The latter was trained to detect bounding boxes of predefined object types on static grid maps in [21]. Finally, the quantitative and qualitative comparison between ReDet and the classic clustering method DBSCAN is presented in Section IV-D.", "A. Comparison of Different Training Datasets": "We trained ReDet with combinations of different datasets previously described in Section III-B. The inference quality (always evaluated on the whole test set from data 1, 2, and 3) is compared in terms of mean Average Precision mAP.", "B. Comparison of Different Model Inputs": "Since our target is to detect dynamic objects, dynamic information should be used in the input. As described in Section II-A, we can encode a dynamic grid map into an RGB image so that its dynamic occupancy mass information is already indirectly considered by the cell color. The mean velocities in x- and y-direction of all particles in a cell, $v_x$ and $v_y$, are thus on first sight redundant dynamic information. To see if explicit speed information can further contribute to the detection performance, we encode the dynamic grid map into a 5 channel matrix with channels {R, G, B, vx, Vy}. The results are summarized in Table III.", "C. Comparison of ReDet with RetinaNet": "In [15], RetinaNet [21] was trained to detect bounding boxes of some predefined object types on static grid maps. We evaluate how it compares to ReDet on our dynamic grid maps. To compare the performance in a fair manner, RetinaNet was also trained on the same training set for 20 epochs including multiple image splits and random rotations and using the pretrained model based on DOTA v1.0 for 12 epochs as well. The mAP of RetinaNet based on the whole test set is worse (78.4%) than that of ReDet (81.2%) while the inference frequency of 5.5 fps is better. The respective precision/recall curves are shown in Fig. 4.", "D. Comparison with Classic Clustering": "We now compare the deep learning model with the classic DBSCAN method. To compare both in a fair manner, we exclude data 2 from the test set as its annotations are generated by the classic method. Fig. 5 shows the ReDet object detection precision/recall curve and the precision/recall obtained from the classic method as a red dot.\nFor the classic method, a precision of 0.51 and a recall of 0.67 is achieved. The precision/recall curve for the deep learning-based approach was created by changing the score threshold between 0 and 1 based on data 1 and 3 of the test set. At the same recall 0.67 as the classic method, a precision of 0.926 is achieved with a score threshold equal to 0.978. The detection performance of ReDet is thus significantly"}, {"title": "V. SUMMARY AND CONCLUSION", "content": "We proposed a deep neural network-based approach to detect generic dynamic objects on dynamic grid maps from a moving ego vehicle. The problem is modeled as an oriented bounding box object detection task with ReDet chosen due to its promising detection performance on aerial images. Dynamic grid maps are encoded to RGB images and fed into the detection network. We only manually labeled a small dataset and extended it with measurements without"}]}