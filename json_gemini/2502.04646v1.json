{"title": "Importance Sampling via Score-based Generative Models", "authors": ["Heasung Kim", "Taekyun Lee", "Hyeji Kim", "Gustavo de Veciana"], "abstract": "Importance sampling, which involves sampling from a probability density function (PDF) proportional to the product of an importance weight function and a base PDF, is a powerful technique with applications in variance reduction, biased or customized sampling, data augmentation, and beyond. Inspired by the growing availability of score-based generative models (SGMs), we propose an entirely training-free Importance sampling framework that relies solely on an SGM for the base PDF. Our key innovation is realizing the importance sampling process as a backward diffusion process, expressed in terms of the score function of the base PDF and the specified importance weight function-both readily available-eliminating the need for any additional training. We conduct a thorough analysis demonstrating the method's scalability and effectiveness across diverse datasets and tasks, including importance sampling for industrial and natural images with neural importance weight functions. The training-free aspect of our method is particularly compelling in real-world scenarios where a single base distribution underlies multiple biased sampling tasks, each requiring a different importance weight function. To the best of our knowledge our approach is the first importance sampling framework to achieve this.", "sections": [{"title": "Introduction", "content": "Score-based Generative Models (SGMs) have proven to be a very effective tool for sampling from high dimensional distributions [1, 2, 3]. Increasingly SGMs are being made available to capture a wide variety of data sets, including images [4, 5], audio [6, 7], and wireless environments [8, 9]. In this work, we consider the following problem: Given an SGM, can we design a strategy for generating samples that satisfy pre-specified characteristics, e.g., samples corresponding to rare events or high losses on a downstream task? Formally, this might be viewed as a form of importance sampling, where the aim is to generate representative \"important\" samples.\nMathematically, importance sampling is defined as follows: Consider a Probability Density Function (PDF) $p : \\mathbb{R}^d \\rightarrow [0, 1]$ over the domain $\\mathbb{R}^d$ of the random vector $X$, and importance weight function $l : \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$ where $\\mathbb{R}^+$ denotes the set of positive real values. The concept of importance sampling entails drawing samples from a modified PDF $q$, whose PDF is proportional to the product of the importance weight function $l$ and the original PDF $p$. Formally, the importance sampling PDF is given by:"}, {"title": "Definition 1. (Importance Sampling PDF)", "content": "$q(x) = \\frac{l(x)p(x)}{\\int l(x)p(x) dx}$\nImportance sampling is a versatile technique with demonstrated effectiveness across various applications, including variance minimization in mean estimation, data augmentation, selective feature analysis, and bias and fairness considerations [10, 11, 12, 13].\nIn real-world scenarios, given a single base PDF $p(x)$ or samples drawn from it, we often seek to perform multiple importance sampling with different importance weight functions $l(x)$. For example, $l(x)$ can be designed to prioritize underrepresented classes, refining sampling to meet varying fairness criteria. Alternatively, $l(x)$ can represent a loss function, enabling the sampling of $x$ that induces high loss values. This allows for the identification of inputs associated with poor task model performance. Such flexibility highlights the power of importance sampling in addressing a wide range of practical challenges.\nDespite its advantages, existing importance sampling methods face critical challenges: They typically require training or fine-tuning separate generative models for multiple importance sampling distributions corresponding to each and every weight function $l(x)$, a process that is computationally prohibitive and inefficient, particularly in scenarios involving multiple or dynamic importance weight functions. Therefore, we seek computationally efficient sampling methods that can adapt to multiple importance weight functions.\nThe main contribution of this paper is the development of a novel approach to efficiently generate importance samples from a target distribution $q(x)$ given only the score function of the base distribution $p(x)$, without explicitly learning $q(x)$ or its score function. Our key innovation lies in deriving an approximate representation for the time-dependent score function of $q(x)$ in terms of the score function of $p(x)$ and the weight function $l(x)$ (Sec. 3). This approximation allows us to model the importance sampling process as a backward diffusion process, which does not require any additional training of generative models. To the best of our knowledge, this is the first"}, {"title": "Background on SGM", "content": "The score-based generative modeling approaches aim to learn the score function of the target data distribution, $\\nabla_x \\log q(x)$. This is achieved through a parameterized model, typically implemented as a neural network, to approximate the true score function. A notable innovation in score-based"}, {"title": "Importance Sampling via SGM (ISSGM)", "content": "Given an SGM that models a base distribution $p(x)$ and a desired weight function $l(x)$, our ultimate goal is to devise an algorithm that can draw samples from the importance sampling PDF $q(x) = \\frac{l(x)p(x)}{\\int l(x)p(x) dx}$, where $l$ is a positive function as $l(x) \\geq m$ and $m > 0$. We assume that the logarithms of $p, q, l$ are twice continuously differentiable with respect to $x."}, {"title": "Proposed method", "content": "We introduce a novel approach for generating importance samples, following $q(x)$, using an SGM trained on the base distribution $p(x)$. Notably, our method neither requires a single sample from the distribution $q(x)$ (or $p(x)$) nor necessitates any additional retraining."}, {"title": "Theoretical Analysis", "content": "In this section, we establish Theorem 1 providing an upper bound on the Euclidean norm discrepancy between the proposed approximated score function and the true score function.\n(Bounded derivatives of $log l(x)$) For all $x \\in \\mathbb{R}^d$ and $t \\in [0, T]$, we have $|\\nabla_x log l(x)| | \\leq \\eta$, $||H_l(x_{0|x,t})|| \\leq \\eta_2$, and $||\\partial H_l(x_{0|x,t})/\\partial x||_F \\leq \\eta_F$, where $H_l(x)$ is Hessian matrix of $log l(x)$.\nAssumption 1 implies that the importance weight function's log-derivatives remain finite. Recall that we consider a strictly positive importance weight function $l(x) > 0$, which reasonably supports the assumption of bounded log derivatives, avoiding unbounded behavior.\n(Bounded log PDF derivatives) For all $x,y \\in \\mathbb{R}^d$ and $t \\in [0,T]$, we have $||\\nabla_x log p_{x|x}(y|x) || \\leq \\gamma_t$, $|| H_{p_t} (x)|| \\leq \\phi_t$, and $|| H_{p_t} (x) - H_{p_t} (y) || \\leq L_t||x - y||$.\nThe bounded norm of the score function and Hessian of the log-likelihood are standard assumptions in the analysis of SGMs [33, 34, 35, 36]. The Lipschitz continuity of the Hessian often holds under a bounded Hessian assumption [35], especially in practical implementations of SGMs where the domain is often compact [1, 2] with high probability."}, {"title": "Experiments", "content": "In this section, we evaluate the proposed method by measuring the 'distance' between the ground truth importance sampling distribution and the sample distribution generated by our approach across various scenarios. No prior baselines exist for our setup (i.e., we have access only to an SGM). Nevertheless, establishing a baseline-particularly a generative model-driven approach-would be valuable. Thus we parameterize the importance sampling distribution with state-of-the-art (SOTA) density-based generative models, optimized via Cross Entropy Minimization (CEM) method [37] using only samples from the base distribution $p$. Our method is applied to the same dataset, ensuring both approaches use an equal number of samples.\nIt is important to note that a direct comparison with this baseline is inherently unfair due to fundamental differences in mechanism. The baseline requires extensive neural network training with substantial hyperparameter tuning, whereas our approach remains the first score-based, training-free importance sampling framework that accommodates a broad range of externally defined importance weight functions $l(x)$. Here, \u201ctraining-free\" signifies that, given an SGM for the base distribution, our method operates without training an additional generative model specific to $l(x)$.\nSetup. We shall initially consider the Circles, Spiral, Pinwheel, and 8-Gaussians (8-G) datasets [38, 39] with two different importance weight functions $l_1(x) = ||x||^2$ and $l_2(x) = \\sum_{i \\in [d]} [x]_i + 2$, i.e., Euclidean norm square and element summation.\nEvaluation metrics. The use of these closed-form data distributions and importance weight functions enable us to recover the optimal importance sampling distribution via the accept/reject method, facilitating the evaluation of importance sampling performance based on the Jensen-Shannon"}, {"title": "Inverse Model Analysis: Sampling Rare Instances Showing High Distortion in Neural Compression", "content": "Thus far, we have demonstrated that the proposed method achieves performance comparable to, or often surpassing, SOTA methods, all while remaining training-free. This unique property enables its application to fundamental sampling challenges, such as model analysis under complex importance weight functions $l(x)$ in high-dimensional spaces. In this section, we apply our training-free importance sampling for inverse model analysis, i.e., identifying samples that degrade model performance.\nApplication Scenarios. We consider the problem of Channel State Information (CSI) compression, where CSI represents a high-dimensional matrix often exceeding thousands of dimensions describing the wireless transmission link conditions between devices. Given the pivotal role of CSI in optimizing communication quality, its efficient compression and transmission have emerged as crucial challenges in wireless communication research and industrial standards [47]. Neural compressors, typically implemented as autoencoders comprising an encoder $F_{enc}$ and a decoder $F_{dec}$, have been extensively studied for this purpose and continue to be an active area of research [48, 49, 50].\nImportance Sampling Formulation. In real-world industrial applications, rigorous analysis of model reliability is essential. A fundamental question in this context is: Under what conditions does our task model, i.e., the autoencoder, fail? Importance sampling provides a principled approach to address this. By defining an importance weight function as $l(x) = D(F_{dec}(F_{enc}(x)), x)$, where $D$ denotes the distortion measure specifically, Mean Squared Error (MSE) in this case-we can efficiently giving high importance weight to samples that exhibit high distortion, thereby identifying failure modes of the neural autoencoder model.\nThis approach is particularly valuable in scenarios where high-distortion samples are rare. A brute-force strategy that generates a large volume of samples, computes distortions, and applies an accept/reject procedure would be computationally inefficient. Also, traditional importance sampling methods are impractical in this setting, especially when multiple neural autoencoders need to be compared, as they necessitate training a separate generative model for each neural autoencoder under evaluation. In contrast, our training-free importance sampling method enables efficient inverse model analysis of neural models.\nFor the experiment, we utilize DDPM to obtain $\\nabla_x log p_t(x)$ with CSI data from Quasi Deter-ministic Radio Channel Generator [51, 52]. The convolutional nueral architecture-based autoencoder comprising $F_{enc}$ and $F_{dec}$ is trained through the dataset generated from DDPM in direction of minimizing MSE.\nResults. We sample $10^4$ CSI instances using both the original and importance sampling methods to measure average distortion under the neural compressor."}, {"title": "Versatility and Scalability", "content": "One of the key advantages of our approach is its versatility to generate samples with varying characteristics from a given SGM. We illustrate this through the following examples: (a) Neural classifier-driven sampling (Exp-(a)) and (b) Sampling images with desired frequency properties from a foundation diffusion model (Exp-(b)). Additionally, our experiments highlight the scalability of our approach, as we apply it to various high-dimensional data, including natural images, image-text pairings, and large-scale diffusion models such as foundational models.\nExp-(a): Neural Classifier-Driven Sampling. Consider an SGM trained without \"class\" awareness, such as gender, yet capable of generating celebrity-like faces (trained on CelebA [55])."}, {"title": "Conclusion", "content": "We proposed a novel training-free score-based importance sampling methodology that models the importance sampling process as a backward diffusion process. By leveraging the score function of the base PDF and the importance weight function, our approach eliminates the need for additional training. This framework offers a scalable and efficient solution, particularly for scenarios where varying importance criteria are required. We anticipate that the proposed method will enable diverse and practical applications of importance sampling in score-based generative models, addressing challenges in adaptive sampling, bias mitigation, and model interpretation."}, {"title": "Impact Statement", "content": "The proposed method facilitates importance sampling with a wide range of existing pretrained score-based models and importance weight functions. We explicitly clarify that the primary objective of this study is to advance methodological rigor and the field of Machine Learning; this work neither intends to advocate nor promote any particular social perspective through the experimental results presented. We do acknowledge the potential for this approach to be misused in reinforcing biased sampling practices, though, and thus strongly encourage careful consideration of the implications associated with its application."}]}