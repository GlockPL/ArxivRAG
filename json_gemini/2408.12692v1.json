{"title": "Unlocking Intrinsic Fairness in Stable Diffusion", "authors": ["Eunji Kim", "Siwon Kim", "Rahim Entezari", "Sungroh Yoon"], "abstract": "Recent text-to-image models like Stable Diffusion produce photo-realistic images\nbut often show demographic biases. Previous debiasing methods focused on\ntraining-based approaches, failing to explore the root causes of bias and overlooking\nStable Diffusion's potential for unbiased image generation. In this paper, we\ndemonstrate that Stable Diffusion inherently possesses fairness, which can be\nunlocked to achieve debiased outputs. Through carefully designed experiments, we\nidentify the excessive bonding between text prompts and the diffusion process as a\nkey source of bias. To address this, we propose a novel approach that perturbs text\nconditions to unleash Stable Diffusion's intrinsic fairness. Our method effectively\nmitigates bias without additional tuning, while preserving image-text alignment\nand image quality.", "sections": [{"title": "Introduction", "content": "Recent text-to-image (T2I) generation models, such as Stable Diffusion (SD) [7, 21, 23], demonstrate\nphoto-realistic image generation performance. However, despite the ground-breaking image quality,\nit has been reported that these models often generate biased images, i.e., an imbalanced ratio between\nmajor and minor sensitive attributes such as gender or race [3, 16, 20, 26]. Since T2I models are\ntrained on real-world images that inherently contain bias, it is unsurprising that the generated images\nalso reflect this bias. However, related works [20, 26] revealed that bias is amplified in generated\nimages compared to the training data, i.e., the disparity in the ratio of major and minor attributes is\nexacerbated in generated images. While opinions regarding the definition of fairness may differ, it's\neasily agreeable that the bias should not be reinforced.\nSeveral methods have been proposed to mitigate bias in SD [5, 8, 14, 19, 27] and they mostly\ninvolve additional training. They adjust either SD parameters [27], text prompts [5], or h-space\nvectors [14, 19] from UNet bottleneck layer which are known to encode semantics. These training-\nbased approaches are expensive and may compromise SD's core functions, such as image-text\nalignment and image quality. This leads us to an important question: Are the generated images truly\nreflective of SD's inherent bias, thus requiring further training? If we can identify intrinsic fairness\nwithin SD, we could potentially reduce bias, lower costs, and maintain the essential image generation\ncapabilities. To the best of our knowledge, this potential solution has not been explored.\nIn this paper, we investigate intrinsic fairness in SD, explore a potential direction to unleash it, and\npropose a novel de-biasing method using the gathered insights. Throughout the paper, we study\ndifferent versions of SD (SD-v1.5, SD-v2, SDXL [21]), that are developed by different entities. We\nfirst propose a mode test in section 3.1 wherein we examine initial noise of SD. Our investigation\nparticularly focuses on noise in the low-density regions of the probability distribution, which has"}, {"title": "Related Works", "content": "De-biasing with additional training. Training-based de-biasing approaches aim to grant fairness\nto SD by using additional resources. However, fully fine-tuning large T2I models is highly costly.\nRecent methods have relied on parameter-efficient fine-tuning techniques, such as prefix tuning [13],\ntext embedding projection weight [5], or low-rank adaptation [27]. Additionally, there have been\nattempts to modify the cross-attention layer in the UNet of Stable Diffusion [9, 18]. Another line of\nwork has proposed directly fine-tuning h-space vectors, which are vectors from the bottleneck layer"}, {"title": "Exploring Fairness of Stable Diffusion", "content": "In this section, we delve into the intrinsic fairness within SD and explore a potential approach to\nunlock it. Specifically, we propose a mode test in section 3.1. In this test, we examine the initial\nnoise space and discover that intrinsic fairness may exist in the low-density regions of the probability\ndistribution. In section 3.2, we explore text conditions to unlock fairness and gather insights for\ndesigning the final de-biasing method, which will be presented in section 4.\nThe experiments presented in this section use the default setting of SD-v1.5\u00b9. Additional results\nfor SD-v2 and SDXL, provided in the Appendix, support the generalizability of our analysis. We\nprimarily focus on binary gender bias (male and female) in four different professions (doctor, CEO,\nnurse, and teacher). We use the CLIP zero-shot classifier 2 with the prompts \u201cA photo of a\nmale/female\u201d to determine the gender in generated images. When testing with racial bias, text\nprompts \u201cA photo of a White person/Black person/Asian/Indian/Latino\u201d are utilized\nfollowing [5]. The most frequent attribute in generated images is termed as major, while others are\ndenoted as minor."}, {"title": "Mode Test: Discovering Fairness in SD", "content": "This paper addresses the issue of amplified bias that occurs even with attribute-neutral prompts.\nWe specifically examine the increased disparity between major and minor attributes in generated\nimages compared to the training images. This suggests that initial noises, primarily sampled from\nhigh-density regions in the probability distribution, tend to strongly favor a major attribute when\nconditioned with an attribute-neutral prompt. However, it remains unclear whether noise in low-\ndensity regions is also prone to generate major attributes. Since the majority of generated images\nare from high-density regions, resolving this necessitates further investigation into the low-density\nregions.\nTo facilitate this investigation, we propose a mode test. Given that directly accessing low-probability\nnoises is challenging due to their rare sampling, we opt to simulate them instead. Specifically, we\nintentionally generate minor attribute images with SD-v1.5 using minor attribute-specified prompts\nand then add noise to them, simulating a forward diffusion process. Inspired by SDEdit [17], we then\napply reverse diffusion to the resulting noise while conditioning it with attribute-neutral prompts."}, {"title": "Perturbing Diffusion with Text Condition: Key to Unlocking Fairness in SD", "content": "In this section, we hypothesize that the text condition is the primary factor guiding initial noise\nfrom high-density regions to generate major attributes. If this is true, reducing the influence of the\ntext condition on the diffusion process will alleviate bias. To test this hypothesis, we conduct two\nexperiments to intentionally weaken the effect of the text condition: 1) decreasing the classifier-free\nguidance scale (section 3.2.1), and 2) using noisy text conditions (section 3.2.2)."}, {"title": "Impact of Classifier Free Guidance", "content": "The Classifier-Free Guidance (CFG) [12] directs image generation to reflect the semantics of the\ntext condition. Specifically, with CFG, the predicted noise $\\bar{\\epsilon}_{\\theta}$ can be written as $\\bar{\\epsilon}_{\\theta}(z, c) = (1 + \\alpha) \\cdot \\epsilon_{\\theta}(z, c) - \\alpha \\cdot \\epsilon_{\\theta}(z)$, where $z$ and $c$ denote unconditional and conditional text prompt embedding,\nrespectively, and $\\alpha$ denotes the CFG scale. It is known that a larger $\\alpha$, i.e., a stronger guidance,\nyields higher coherence of the image to the text condition at the cost of reduced sample diversity [12].\nConversely, this suggests that reduced CFG scale can diversify generated images.\nHere we study how bias changes by varying the CFG scale from 0.0 to 8.0, resulting in a total of 5,000\nimages for each profession (5 CFG scales \u00d7 1,000 generations for each profession). As the CFG scale decreases (indicated by lighter colors), the major attribute ratio\ndecreases. These results support our hypothesis that weakening a text condition can alleviate bias.\nAs a consequence, it also compromises the alignment between the generated images and the text\nprompts. Therefore, we explore better strategies in the following sections."}, {"title": "Noisy Text Condition", "content": "In this section, we describe an alternative approach that weakens text conditions by perturbing them\nwith injected noise. This approach is inspired by Condition-Annealed Sampling (CADS) [24], which\nproposes to add noise to a text condition to diversify compositions of generated images. The CADS\noperates as follows: a given text condition $c$ is perturbed to $\\hat{c}$ as\n$\\hat{c} = \\sqrt{\\gamma(t)} c + s \\sqrt{1 - \\gamma(t)} n, \\qquad \\gamma(t) = \\begin{cases} 1 & 0 \\leq t \\leq \\tau_1, \\\\\n\\frac{\\tau_2 - t}{\\tau_2 - \\tau_1} & \\tau_1 < t < \\tau_2, \\\\\n0 & \\tau_2 \\leq t \\leq 1, \\end{cases}$\nwhere $s$ controls the scale of noise, $\\gamma(t)$ is the annealed coefficient determined by $t$, and $n \\sim N(0, I)$.\nAs diffusion models operate reverse from $t = 1$ to $t = 0$, perturbation with noise is applied to a text\ncondition in earlier steps. $\\hat{c}$ is then normalized to have the same mean and standard deviation as $c$.\nTo study the impact of the CADS-based approach on diversifying attributes, we conduct experiments\naddressing gender and racial bias.  To further explore the impact of the intensity and duration of noise\ninjection on bias mitigation, we also extend our experiments with additional hyperparameters:"}, {"title": "Minor Attribute Guidance", "content": "Sections 3.1 and 3.2 reveal that perturbing text conditions can guide a greater portion of initial noises\nto generate minor attributes, thus alleviating bias. However, as evidenced by the decreased CLIP\nscores, indiscriminate perturbation might undermine the image-text alignment, a core functionality\nof SD. To preserve this capability while perturbing a text condition, it is beneficial to control the\nperturbation by providing guidance in the desired direction\u2014in our case, the direction of a minor\nattribute.\nHere we investigate whether conditioning the early diffusion steps with a minor attribute-specified\nprompt aids in bias mitigation by generating more images with minor attributes. Specifically, when\ngenerating images for a neutral prompt, we replace the text condition in the early diffusion steps\nfrom $t = 1$ to $t = t'$ with a minor attribute-specified prompt. We keep the neutral prompts for\nthe remaining steps, from $t = t'$ to $t = 0$. This observation\ndemonstrates that guiding the diffusion process with a minor attribute-specified prompt during early\ndiffusion steps is effective. This inspires our method to control text condition perturbation through\nminor attribute guidance in the next section."}, {"title": "Unlocking fairness in Stable Diffusion", "content": "Building on our previous observations, we propose a simple yet effective method to weaken the\ninfluence of attribute-neutral text prompts on major attributes by perturbing the diffusion direction\nwith minor attributes. To develop a universally applicable method without compromising core\nfunctionalities of SD, we focus on two objectives: 1) Efficiency: It should not require high costs for\ntraining or inference. 2) Versatility: De-biasing should occur when necessary without interfering\nwith general image generation, e.g. \u201cA photo of a car\u201d. We introduce our weak conditioning\nscheme in the next section that meets both requirements."}, {"title": "Method Design", "content": "T2I models, including SD, fundamentally utilize a text encoder such as CLIP [22]. This encoder's\nembedding space enables arithmetic operations on concepts, such as addition and subtraction [6]. We\ndefine the attribute embedding direction $a_k$ for attribute $k$ as $\\phi(k) - \\phi(\u201c\u201c)$, where $\\phi(\\cdot)$ represents\nthe text embedding from the CLIP text encoder and \u201c\u201d denotes an empty text. Here $k$ is the text\nrepresenting an attribute, which can be a major or minor attribute. For the gender bias, we use\n\u201cfemale\u201d and \u2018male\u201d. Then, we create a new text condition embedding $(c + a_k)$ by adding $a_k$ to\na given text condition embedding $c$. This allows guiding diffusion direction to a targeted attribute\nwithout any additional training cost.\nIn order to meet the versatility constraint, we redesign the perturbation to be weak. Figure 7 visualizes our text embedding creation.\nIn a practical scenario, it is reasonable to assume that we are unaware of the bias in the prompts: we\ncannot predict which attribute will dominate in the generated images. To tackle this challenge, given\na prompt, we sample a target attribute from a uniform distribution over a pre-defined attribute set.\nAfter obtaining the weak attribute guidance, we present $c$ and $\\hat{c}$ alternately throughout the denoising\nsteps. This approach additionally helps to preserve the original text's semantics.\nWhile our bias mitigation method appears simple, it effectively addresses requirements of efficiency\nand versatility. Experimental results in section 5 will further demonstrate the efficacy of our method."}, {"title": "Investigating the Design Choices", "content": "In this section, we justify our design choices from the previous section, showing their superior effec-\ntiveness in mitigating bias, promoting fairness, and preserving versatility compared to alternatives.\nAdding direction vs. Specifying in the prompt. Our method adds attribute embedding direction\nrather than explicitly specifying the attribute in the prompt. In Figure 8, the adding scheme (green)\nshows similar results to the explicit method (purple). So, why is the adding scheme more beneficial\nthan inserting an attribute into a prompt? Inserting an attribute into a sentence requires parsing the\nsentence to determine the appropriate placement for the attribute. This process is inefficient and may\nresult in grammatically incorrect sentences. With the adding scheme, there is no need to parse or\ndecide where to insert the attribute, making it more efficient and reliable.\nAdding the attribute direction at every position vs. Upon the [EOS] token. Adding the attribute\ndirection upon the [EOS] token (blue in Figure 8) reliably generates the targeted attribute. This\nindicates that the embedding upon [EOS] carries sufficient information to guide the generation toward\nthe target attribute. This approach especially enhances versatility; for instance, if \u201cmale\u201d is specified\nin the prompt, guidance to females should not take effect. The generated images should be male as\nwritten in the prompt, remaining the low female ratio. The yellow and red bars in Figure 8 show\nthe corresponding results. demonstrates the effectiveness of weak perturbation maintaining the original semantics of a text\nprompt."}, {"title": "Experimental Results", "content": "Baselines. We compare the proposed method with four baseline methods: FairDiffusion (FairDiff) [8],\nUnified Concept Editing (UCE) [9], Fine-Tuning Diffusion (FTDiff) [27], and Self-Discovering latent\ndirection (SelfDisc) [14]. Our primary comparison target is FairDiffusion as it is the only training-\nfree de-biasing method, and thus we will conduct further comparison with this method. Other\nmethods require additional training. For all baselines, the official code published by the authors and\ncheckpoints are used if available.\nPrompt Configurations. We primarily focus on professions where the bias in generated images is\nmore pronounced than in the training data. Based on analyses of gender ratios within the training\ndataset [26], we select eight professions where the difference in the ratio of minor attributes between\nthe training dataset and images generated with SD-v1.5 is greater than or close to 10%p. These\ninclude both traditionally male-dominated professions (CEO, doctor, pilot, technician) and female-\ndominated professions (fashion designer, librarian, teacher, nurse)."}, {"title": "Image-Text Alignment and Image Fidelity", "content": "In designing our method, we focus on maintaining the image generation quality of Stable Diffu-\nsion while pursuing fairness, ensuring that image-text alignment and image fidelity are preserved.\nOtherwise, it may face significant reliability issues when being deployed, such as depicting racial\ndemographics inaccurately [1]. Our quality assurance effort focuses on two requirements that must be\nsatisfied. Firstly, if a certain attribute is explicitly specified in the text prompt (e.g., \u201ca photo of a\nmale doctor\u201d), then the attribute should be present in generated images. Secondly, the de-biasing\ntechnique must not impair other image generation. For example, if a prompt does not contain any\npotential stereotypes, e.g., \u201ca photo of a car\", then the de-biasing technique should not interfere\nwith the generation of a high-quality (FID score) and accurate car image (CLIP score). For the first\nrequirement, we generate 100 images with text prompts where the gender is explicitly specified.  Other methods, including FairDiffusion, fail to achieve a ratio\nof 1.0, indicating they often fail to generate attributes specified in the text prompts.\""}, {"title": "De-biasing Results", "content": "Table 2 shows the de-biasing results on eight different professions. Lower Avg. value indicates a more balanced ratio between\nmales and females.\nThe two leftmost columns of the table show that SD produces bias-amplified images compared\nto training data distribution. Comparably, our results, shown in the rightmost column, show the\nincreased ratio of the minor attribute even without additional training.  Our method achieves both comparable de-biasing performance and robust\nimage generation functionality."}, {"title": "Conclusion", "content": "In this paper, we address the bias in images generated by Stable Diffusion by systematically studying\nits root causes and exploring its intrinsic fairness. Unlike previous training-based methods, our\napproach uses subtle text perturbations to direct noise and form minor attributes, effectively mitigating\nbias. Our experiments reveal that excessive bonding between text prompts and the diffusion process\nis a key source of bias. By perturbing text conditions, we unlock SD's inherent fairness, achieving\ndebiased outputs without compromising generation quality. Overall, our work demonstrates that Stable\nDiffusion inherently possesses fairness, which can be harnessed to produce unbiased, high-quality\nimages.\nBroader Impacts. Our method, which adds no extra training or inference cost, can be easily integrated\ninto any T2I models, including pre-trained ones, enhancing fairness and user trust. Additionally, our\nnovel analysis of the low-density region in the initial noise space opens new avenues for exploring\nintrinsic fairness in SD.\nLimitations. We observe that if the training data bias is too severe, as with nurse, our proposed\nmethod is less effective. Moreover, our study focuses on binary gender and five racial types, which\ndoes not cover all demographic groups. Future research should explore more bias types to improve\nfairness."}, {"title": "Experimental Details", "content": "For all experiments, we generate images with 50 steps using the PNDM scheduler. The\nexperiments are done with NVIDIA RTX 8000 and A40.\nAll the baselines that we compare were proposed within SD-v1.4 and SD-v1.5, so we mainly compare\nour method with them using SD-v1.5. In addition, we compare our method with FairDiffusion, the\nonly training-free method among the baselines, within SD-v2 using the same hyperparameters as\nSD-v1.5.  The detailed setting for each method are as follows.\nSince our method does not require any additional training, it can be easily applied to any version of\nStable Diffusion."}, {"title": "Debiasing Results", "content": "Table 5 shows the de-biasing results on eight different professions using SD-v2 and SDXL. The\nnumbers indicate the proportion of the minor attribute. However, we maintain consistency by reporting the male ratio for teachers. The table demonstrates\nthat our method effectively mitigates bias across different models."}, {"title": "Qualitative Results", "content": "Figures 17, 18, and 19 shows examples of images generated with vanilla SD and our method using SD-\nv1.5, SD-v2, and SDXL, respectively. These figures demonstrate our method successfully generates\nimages with minor attributes, regardless of the model version used. Images with major attributes are\nalso produced without compromising quality.\nQualtitative Comparison with FairDiffusion. Figures 20 provide additional qualitative comparisons\nbetween FairDiffusion and our method. The individuals in the images generated with FairDiffusion\noften exhibit physical traits of both women and men simultaneously, while those in the images\ngenerated with our method do not."}, {"title": "Debiasing on Racial Bias", "content": "We additionally evaluate our method within racial bias."}]}