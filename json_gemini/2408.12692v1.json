{"title": "Unlocking Intrinsic Fairness in Stable Diffusion", "authors": ["Eunji Kim", "Siwon Kim", "Rahim Entezari", "Sungroh Yoon"], "abstract": "Recent text-to-image models like Stable Diffusion produce photo-realistic images but often show demographic biases. Previous debiasing methods focused on training-based approaches, failing to explore the root causes of bias and overlooking Stable Diffusion's potential for unbiased image generation. In this paper, we demonstrate that Stable Diffusion inherently possesses fairness, which can be unlocked to achieve debiased outputs. Through carefully designed experiments, we identify the excessive bonding between text prompts and the diffusion process as a key source of bias. To address this, we propose a novel approach that perturbs text conditions to unleash Stable Diffusion's intrinsic fairness. Our method effectively mitigates bias without additional tuning, while preserving image-text alignment and image quality.", "sections": [{"title": "Introduction", "content": "Recent text-to-image (T2I) generation models, such as Stable Diffusion (SD) [7, 21, 23], demonstrate photo-realistic image generation performance. However, despite the ground-breaking image quality, it has been reported that these models often generate biased images, i.e., an imbalanced ratio between major and minor sensitive attributes such as gender or race [3, 16, 20, 26]. Since T2I models are trained on real-world images that inherently contain bias, it is unsurprising that the generated images also reflect this bias. However, related works [20, 26] revealed that bias is amplified in generated images compared to the training data, i.e., the disparity in the ratio of major and minor attributes is exacerbated in generated images. While opinions regarding the definition of fairness may differ, it's easily agreeable that the bias should not be reinforced.\nSeveral methods have been proposed to mitigate bias in SD [5, 8, 14, 19, 27] and they mostly involve additional training. They adjust either SD parameters [27], text prompts [5], or h-space vectors [14, 19] from UNet bottleneck layer which are known to encode semantics. These training-based approaches are expensive and may compromise SD's core functions, such as image-text alignment and image quality. This leads us to an important question: Are the generated images truly reflective of SD's inherent bias, thus requiring further training? If we can identify intrinsic fairness within SD, we could potentially reduce bias, lower costs, and maintain the essential image generation capabilities. To the best of our knowledge, this potential solution has not been explored.\nIn this paper, we investigate intrinsic fairness in SD, explore a potential direction to unleash it, and propose a novel de-biasing method using the gathered insights. Throughout the paper, we study different versions of SD (SD-v1.5, SD-v2, SDXL [21]), that are developed by different entities. We first propose a mode test in section 3.1 wherein we examine initial noise of SD. Our investigation particularly focuses on noise in the low-density regions of the probability distribution, which has"}, {"title": "Related Works", "content": "De-biasing with additional training. Training-based de-biasing approaches aim to grant fairness to SD by using additional resources. However, fully fine-tuning large T2I models is highly costly. Recent methods have relied on parameter-efficient fine-tuning techniques, such as prefix tuning [13], text embedding projection weight [5], or low-rank adaptation [27]. Additionally, there have been attempts to modify the cross-attention layer in the UNet of Stable Diffusion [9, 18]. Another line of work has proposed directly fine-tuning h-space vectors, which are vectors from the bottleneck layer"}, {"title": "Exploring Fairness of Stable Diffusion", "content": "In this section, we delve into the intrinsic fairness within SD and explore a potential approach to unlock it. Specifically, we propose a mode test in section 3.1. In this test, we examine the initial noise space and discover that intrinsic fairness may exist in the low-density regions of the probability distribution. In section 3.2, we explore text conditions to unlock fairness and gather insights for designing the final de-biasing method, which will be presented in section 4.\nThe experiments presented in this section use the default setting of SD-v1.5\u00b9. Additional results for SD-v2 and SDXL, provided in the Appendix, support the generalizability of our analysis. We primarily focus on binary gender bias (male and female) in four different professions (doctor, CEO, nurse, and teacher). We use the CLIP zero-shot classifier \u00b2 with the prompts \u201cA photo of a male/female\u201d to determine the gender in generated images. When testing with racial bias, text prompts \u201cA photo of a White person/Black person/Asian/Indian/Latino\u201d are utilized following [5]. The most frequent attribute in generated images is termed as major, while others are denoted as minor."}, {"title": "Mode Test: Discovering Fairness in SD", "content": "This paper addresses the issue of amplified bias that occurs even with attribute-neutral prompts. We specifically examine the increased disparity between major and minor attributes in generated images compared to the training images. This suggests that initial noises, primarily sampled from high-density regions in the probability distribution, tend to strongly favor a major attribute when conditioned with an attribute-neutral prompt. However, it remains unclear whether noise in low-density regions is also prone to generate major attributes. Since the majority of generated images are from high-density regions, resolving this necessitates further investigation into the low-density regions.\nTo facilitate this investigation, we propose a mode test. Given that directly accessing low-probability noises is challenging due to their rare sampling, we opt to simulate them instead. Specifically, we intentionally generate minor attribute images with SD-v1.5 using minor attribute-specified prompts and then add noise to them, simulating a forward diffusion process. Inspired by SDEdit [17], we then apply reverse diffusion to the resulting noise while conditioning it with attribute-neutral prompts."}, {"title": "Perturbing Diffusion with Text Condition: Key to Unlocking Fairness in SD", "content": "In this section, we hypothesize that the text condition is the primary factor guiding initial noise from high-density regions to generate major attributes. If this is true, reducing the influence of the text condition on the diffusion process will alleviate bias. To test this hypothesis, we conduct two experiments to intentionally weaken the effect of the text condition: 1) decreasing the classifier-free guidance scale (section 3.2.1), and 2) using noisy text conditions (section 3.2.2)."}, {"title": "Impact of Classifier Free Guidance", "content": "The Classifier-Free Guidance (CFG) [12] directs image generation to reflect the semantics of the text condition. Specifically, with CFG, the predicted noise $\\bar{\\epsilon}_{\\theta}$ can be written as $\\bar{\\epsilon}_{\\theta} (z, c) = (1 + \\alpha) \\cdot \\epsilon_{\\theta}(z, c) - \\alpha \\cdot \\epsilon_{\\theta}(z)$, where $z$ and $c$ denote unconditional and conditional text prompt embedding, respectively, and $\\alpha$ denotes the CFG scale. It is known that a larger $\\alpha$, i.e., a stronger guidance, yields higher coherence of the image to the text condition at the cost of reduced sample diversity [12]. Conversely, this suggests that reduced CFG scale can diversify generated images.\nHere we study how bias changes by varying the CFG scale from 0.0 to 8.0, resulting in a total of 5,000 images for each profession (5 CFG scales \u00d7 1,000 generations for each profession). Figure 3 shows the major attribute ratio (y-axis) and CLIP score (x-axis). Color intensity reflects the magnitude of CFG scale. As the CFG scale decreases (indicated by lighter colors), the major attribute ratio decreases. These results support our hypothesis that weakening a text condition can alleviate bias. As a consequence, it also compromises the alignment between the generated images and the text prompts. Therefore, we explore better strategies in the following sections."}, {"title": "Noisy Text Condition", "content": "In this section, we describe an alternative approach that weakens text conditions by perturbing them with injected noise. This approach is inspired by Condition-Annealed Sampling (CADS) [24], which proposes to add noise to a text condition to diversify compositions of generated images. The CADS operates as follows: a given text condition $c$ is perturbed to $\\hat{c}$ as\n$$\\hat{c} = \\sqrt{\\gamma(t)}c + s\\sqrt{1 - \\gamma(t)}\\eta, \\qquad \\gamma(t) =\\begin{cases}\n    1 & 0 \\leq t \\leq T_1,\\\\\n    \\frac{T_2-t}{T_2-T_1} & T_1 < t < T_2,\\\\\n    0 & T_2 \\leq t \\leq 1,\n    \\end{cases}$$\n(1)\nwhere $s$ controls the scale of noise, $\\gamma(t)$ is the annealed coefficient determined by $t$, and $\\eta \\sim \\mathcal{N}(0, I)$. As diffusion models operate reverse from $t = 1$ to $t = 0$, perturbation with noise is applied to a text condition in earlier steps. $\\hat{c}$ is then normalized to have the same mean and standard deviation as $c$.\nTo study the impact of the CADS-based approach on diversifying attributes, we conduct experiments addressing gender and racial bias. We start with the default settings of CADS and set $(T_1, T_2)$ to $(0.6, 0.9)$ and $s = 0.25$. To further explore the impact of the intensity and duration of noise injection on bias mitigation, we also extend our experiments with additional hyperparameters: $(T_1, T_2, s) = (0.8, 0.9, 0.25)$ and $(0.6, 0.9, 0.15)$.\nThe results are shown in Figure 5 (a,b) where the ratio of the major attribute is depicted in y-axis. It is shown that the major attribute ratio decreases, indicating that bias is mitigated by CADS (all variations) compared to vanilla SD (blue) for both gender and racial bias. We also observe that as $s$"}, {"title": "Minor Attribute Guidance", "content": "Sections 3.1 and 3.2 reveal that perturbing text conditions can guide a greater portion of initial noises to generate minor attributes, thus alleviating bias. However, as evidenced by the decreased CLIP scores, indiscriminate perturbation might undermine the image-text alignment, a core functionality of SD. To preserve this capability while perturbing a text condition, it is beneficial to control the perturbation by providing guidance in the desired direction\u2014in our case, the direction of a minor attribute.\nHere we investigate whether conditioning the early diffusion steps with a minor attribute-specified prompt aids in bias mitigation by generating more images with minor attributes. Specifically, when generating images for a neutral prompt, we replace the text condition in the early diffusion steps from $t = 1$ to $t = t'$ with a minor attribute-specified prompt. We keep the neutral prompts for the remaining steps, from $t = t'$ to $t = 0$. Figure 6 shows the minor attribute ratio by varying the initial steps that include a minor attribute in the text condition (x-axis). Different lines cover various professions. When $t' = 1$, only a neutral prompt is provided, resulting in biased generations. In contrast, when $t' = 0$, only a minor attribute prompt is given, making the minor attribute ratio close to 1. As $t'$ decreases in the intermediate steps, the minor attribute ratio increases. This observation demonstrates that guiding the diffusion process with a minor attribute-specified prompt during early diffusion steps is effective. This inspires our method to control text condition perturbation through minor attribute guidance in the next section."}, {"title": "Unlocking fairness in Stable Diffusion", "content": "Building on our previous observations, we propose a simple yet effective method to weaken the influence of attribute-neutral text prompts on major attributes by perturbing the diffusion direction with minor attributes. To develop a universally applicable method without compromising core functionalities of SD, we focus on two objectives: 1) Efficiency: It should not require high costs for training or inference. 2) Versatility: De-biasing should occur when necessary without interfering with general image generation, e.g. \u201cA photo of a car\u201d. We introduce our weak conditioning scheme in the next section that meets both requirements."}, {"title": "Method Design", "content": "T2I models, including SD, fundamentally utilize a text encoder such as CLIP [22]. This encoder's embedding space enables arithmetic operations on concepts, such as addition and subtraction [6]. We define the attribute embedding direction $a_k$ for attribute $k$ as $\\Phi(k) - \\Phi(``\")$, where $\\Phi(\\cdot)$ represents the text embedding from the CLIP text encoder and ``'' denotes an empty text. Here $k$ is the text representing an attribute, which can be a major or minor attribute. For the gender bias, we use \u201cfemale\u201d and \u2018male\u201d. Then, we create a new text condition embedding $(c + a_k)$ by adding $a_k$ to a given text condition embedding $c$. This allows guiding diffusion direction to a targeted attribute without any additional training cost.\nIn order to meet the versatility constraint, we redesign the perturbation to be weak. A text encoder of a T2I model only accepts fixed-size inputs. Therefore, for a sentence, an end-of-sequence ([EOS]) token is appended to signify its conclusion, followed by padding to the maximum input length of text encoder. Instead of adding $a_k$ to all positions of $c$, we add a portion of $a_k$ from the [EOS] token up to the maximum length of $c$. This approach is expected to guide the diffusion direction effectively while preserving the original text's semantics before the [EOS] token. We will justify the advantage of this design choice in section 4.2. Therefore, our new text embedding becomes $\\hat{c} = c + m \\cdot a_k$, where $m_i = 1(i > [EOS])$ with $m$ being a mask whose value is 1 for the tokens preceding and including the [EOS] token. Figure 7 visualizes our text embedding creation.\nIn a practical scenario, it is reasonable to assume that we are unaware of the bias in the prompts: we cannot predict which attribute will dominate in the generated images. To tackle this challenge, given a prompt, we sample a target attribute from a uniform distribution over a pre-defined attribute set. After obtaining the weak attribute guidance, we present $c$ and $\\hat{c}$ alternately throughout the denoising steps. This approach additionally helps to preserve the original text's semantics.\nWhile our bias mitigation method appears simple, it effectively addresses requirements of efficiency and versatility. Experimental results in section 5 will further demonstrate the efficacy of our method.\""}, {"title": "Investigating the Design Choices", "content": "In this section, we justify our design choices from the previous section, showing their superior effectiveness in mitigating bias, promoting fairness, and preserving versatility compared to alternatives.\nAdding direction vs. Specifying in the prompt. Our method adds attribute embedding direction rather than explicitly specifying the attribute in the prompt. In Figure 8, the adding scheme (green) shows similar results to the explicit method (purple). So, why is the adding scheme more beneficial than inserting an attribute into a prompt? Inserting an attribute into a sentence requires parsing the sentence to determine the appropriate placement for the attribute. This process is inefficient and may result in grammatically incorrect sentences. With the adding scheme, there is no need to parse or decide where to insert the attribute, making it more efficient and reliable.\nAdding the attribute direction at every position vs. Upon the [EOS] token. Adding the attribute direction upon the [EOS] token (blue in Figure 8) reliably generates the targeted attribute. This indicates that the embedding upon [EOS] carries sufficient information to guide the generation toward the target attribute. This approach especially enhances versatility; for instance, if \u201cmale\u201d is specified in the prompt, guidance to females should not take effect. The generated images should be male as written in the prompt, remaining the low female ratio. The yellow and red bars in Figure 8 show the corresponding results. Adding an attribute embedding at every position (red) generates mostly female images even with \u201cmale\u201d stated in the prompt, indicating excessive guidance. In contrast, adding the direction upon [EOS] generates mostly male images, preserving the prompt's intent. This demonstrates the effectiveness of weak perturbation maintaining the original semantics of a text prompt. For additional results on SD-v2 and SDXL, please refer to Appendix B.3."}, {"title": "Experimental Results", "content": "Baselines. We compare the proposed method with four baseline methods: FairDiffusion (FairDiff) [8], Unified Concept Editing (UCE) [9], Fine-Tuning Diffusion (FTDiff) [27], and Self-Discovering latent direction (SelfDisc) [14]. Our primary comparison target is FairDiffusion as it is the only training-free de-biasing method, and thus we will conduct further comparison with this method. Other methods require additional training. For all baselines, the official code published by the authors and checkpoints are used if available. If not, the method is trained from scratch following the instructions provided by the authors. As a reference, we also report gender statistics of LAION-5B [25], which comprises the training data of SD [26]. Further details about baselines are described in Appendix A.2.\nPrompt Configurations. We primarily focus on professions where the bias in generated images is more pronounced than in the training data. Based on analyses of gender ratios within the training dataset [26], we select eight professions where the difference in the ratio of minor attributes between the training dataset and images generated with SD-v1.5 is greater than or close to 10%p. These include both traditionally male-dominated professions (CEO, doctor, pilot, technician) and female-dominated professions (fashion designer, librarian, teacher, nurse). All images are generated with the template \u201cA photo of a/an {profession}\u201d unless otherwise stated."}, {"title": "Image-Text Alignment and Image Fidelity", "content": "In designing our method, we focus on maintaining the image generation quality of Stable Diffusion while pursuing fairness, ensuring that image-text alignment and image fidelity are preserved. Otherwise, it may face significant reliability issues when being deployed, such as depicting racial demographics inaccurately [1]. Our quality assurance effort focuses on two requirements that must be satisfied. Firstly, if a certain attribute is explicitly specified in the text prompt (e.g., \u201ca photo of a male doctor\u201d), then the attribute should be present in generated images. Secondly, the de-biasing technique must not impair other image generation. For example, if a prompt does not contain any potential stereotypes, e.g., \u201ca photo of a car\u201d, then the de-biasing technique should not interfere with the generation of a high-quality (FID score) and accurate car image (CLIP score). For the first requirement, we generate 100 images with text prompts where the gender is explicitly specified. Figure 9 shows the prompt templates and results of eight professions generated using our method, compared to four baselines and vanilla SD (averaged over three runs). Detailed numbers are shown in Appendix C.1. The left side of the figure shows the female ratio from female-specified prompts while the right side shows the opposite. Since the gender is explicitly stated in each prompt, the desired ratio here is 1. Vanilla SD-v1.5 (grey) achieves a ratio close to 1 in both cases. Our method (purple) also achieves a ratio close to 1, indicating the weak perturbation scheme does not interfere with generating the specified attribute. In contrast, other methods fail to generate the specified gender exclusively. Particularly, given the male-specified prompt (right), FairDiffusion (blue) shows an almost balanced ratio, indicating that it almost ignores the gender specified in the prompt. These results also hold for both SD-v2 and SDXL, as detailed in Appendix C.1.\nFor the second requirement, we generate images with captions from COCO-5k, and measure CLIP and FID scores, as shown in Table 1. Both CLIP and FID scores of our method are similar to the original SD, indicating that the proposed method does not compromise image-text alignment and image fidelity. Qualitative comparison in Figure 10 (a) further demonstrates the superiority of our method over FairDiffusion, our closest competitor. The images generated by FairDiffusion (middle) present artifacts resembling humans, such as the upper body of a person on a cabinet (top) or a head on a desk (bottom) despite the absence of any person-related context in the given text prompts. In contrast, our method generates consistent images without such artifacts, highlighting its superior image-text alignment capability. This is further supported by the high CLIP score shown in Table 1."}, {"title": "De-biasing Results", "content": "Table 2 shows the de-biasing results on eight different professions. The numbers represent the ratio of the minor attribute, i.e., for the first four professions where males are predominant, the female ratio is shown. The bottom row (Avg. \u2206) shows the average absolute differences between the ratio and the target ratio of 0.5 across the professions. Lower Avg. value indicates a more balanced ratio between males and females.\nThe two leftmost columns of the table show that SD produces bias-amplified images compared to training data distribution. Comparably, our results, shown in the rightmost column, show the increased ratio of the minor attribute even without additional training. The superiority of our method is particularly evident in female-dominant professions in the bottom four rows. Training-based de-biasing methods (UCE, FTDiff, and SelfDisc) show increases in the minor attribute ratios, but rarely achieve their target ratio of 0.5, even after utilizing additional training.\nThe average results show that FairDiffusion (FairDiff), our most direct competitor, has the lowest value, but differs from ours by only 0.004. However, as shown in Figure 9 FairDiffusion fails with image-text alignment. Our method achieves both comparable de-biasing performance and robust image generation functionality.\nThe qualitative comparison further supports that the performance gap is negligible. Figure 10 (b) shows the examples generated for two professions. The images generated from FairDiffusion (middle) present physical characteristics of both women and men simultaneously, such as breasts and mustaches. In contrast, our method generates images with clearer gender cues."}, {"title": "Conclusion", "content": "In this paper, we address the bias in images generated by Stable Diffusion by systematically studying its root causes and exploring its intrinsic fairness. Unlike previous training-based methods, our approach uses subtle text perturbations to direct noise and form minor attributes, effectively mitigating bias. Our experiments reveal that excessive bonding between text prompts and the diffusion process is a key source of bias. By perturbing text conditions, we unlock SD's inherent fairness, achieving debiased outputs without compromising generation quality. Consistent results across different versions of SD confirm the generalizability of our method. Overall, our work demonstrates that Stable Diffusion inherently possesses fairness, which can be harnessed to produce unbiased, high-quality images.\nBroader Impacts. Our method, which adds no extra training or inference cost, can be easily integrated into any T2I models, including pre-trained ones, enhancing fairness and user trust. Additionally, our novel analysis of the low-density region in the initial noise space opens new avenues for exploring intrinsic fairness in SD.\nLimitations. We observe that if the training data bias is too severe, as with nurse, our proposed method is less effective. Moreover, our study focuses on binary gender and five racial types, which does not cover all demographic groups. Future research should explore more bias types to improve fairness."}, {"title": "Common Settings", "content": "For all experiments, we generate images with 50 steps using the PNDM scheduler. Images are generated at 512 \u00d7 512 for SD-v1.5 and SD-v2, and at 1024 \u00d7 1024 for SDXL . Unless specified otherwise, we use a CFG scale $\\alpha$ of 6 for SD-v1.5 and SD-v2, and a scale of 4 for SDXL. The experiments are done with NVIDIA RTX 8000 and A40."}, {"title": "Settings for Comparison Methods", "content": "All the baselines that we compare were proposed within SD-v1.4 and SD-v1.5, so we mainly compare our method with them using SD-v1.5. In addition, we compare our method with FairDiffusion, the only training-free method among the baselines, within SD-v2 using the same hyperparameters as SD-v1.5. As SDXL has a different architecture from SD-v1.5, we only report the results with vanilla SD and our method. For FairDiffusion, we use the default hyperparameter that the official code provides. For FTDiff, we use the pre-trained checkpoint for both the text encoder and LoRA. For UCE and SelfDisc, we follow the training and inference procedure described in the official code published by the authors. The detailed setting for each method are as follows."}, {"title": "Impact of Classifier Free Guidance", "content": "The Classifier-Free Guidance (CFG) directs image generation to reflect the semantics of the text condition. Specifically, with CFG, the predicted noise $\\epsilon$ can be written as $\\epsilon(\\mathbf{x}, c) = (1 + \\alpha) \\cdot f_{\\theta}(\\mathbf{x}, c) - \\alpha\\cdot f_{\\theta}(\\mathbf{x})$, where $\\mathbf{x}$ and $c$ denote unconditional and conditional text prompt embedding, respectively, and $\\alpha$ denotes the CFG scale. It is known that a larger $\\alpha$, i.e., a stronger guidance, yields higher coherence of the image to the text condition at the cost of reduced sample diversity [12]. Conversely, this suggests that reduced CFG scale can diversify generated images.\nHere we study how bias changes by varying the CFG scale from 0.0 to 8.0, resulting in a total of 5,000 images for each profession (5 CFG scales $\\times$ 1,000 generations for each profession). Figure 3 shows the major attribute ratio (y-axis) and CLIP score (x-axis). Color intensity reflects the magnitude of CFG scale. As the CFG scale decreases (indicated by lighter colors), the major attribute ratio decreases. These results support our hypothesis that weakening a text condition can alleviate bias. As a consequence, it also compromises the alignment between the generated images and the text prompts. Therefore, we explore better strategies in the following sections."}, {"title": "Detailed Setting of Our Method", "content": "Since our method does not require any additional training, it can be easily applied to any version of Stable Diffusion. In SD-v1.5 and SDXL, we use $c$ and $\\hat{c} = c + m \\cdot a_k$ alternately during diffusion steps. When implementing our method with SD-v2, we omit mask $m$. Specifically, we employ $c + a_k$ as $\\hat{c}$. In addition, we employ $c$ one out of every three timesteps instead of taking turns. The strategy stems from the distinct effects of adding $a_k$ within SD-v2, as will be explained in Appendix B.3. For SD-v2, adding $a_k$ to all token positions in $c$ does not interfere with generating the attributes specified in the text prompt. Instead, to preserve the scale of the original text embedding, we renormalize each position so that the norm of $c + a_k$ be the norm of $c$."}, {"title": "Noisy Text Condition", "content": "Figure 11 shows that adding noise via CADS reduces gender and racial bias within image generation of SD-v2 and SDXL. As explained in section 3.2.2 for SD-v1.5 results, increasing the amount of noise injected to the text condition (with larger $s$ and smaller $T_1$) decreases the ratio of major attributes, thereby reducing bias within both gender and race. For the result with teacher, the change is minimal (racial bias within SD-v2) or even increases the ratio of the major attribute (gender bias within SDXL), where bias in vanilla SD-generated images is not as severe as other professions.\nFigures 12, 13, and 14 illustrate some examples of generated images with a vanilla SD and CADS, using SD-v1.5, SD-v2, and SDXL, respectively. CADS generates more diverse images, reducing bias. However, it occasionally fails to generate images that match with the given text prompt. This is also reflected in the decrease in the CLIP score shown in Figure11.\nThe findings suggest that injecting noise to perturb the text condition, as demonstrated by CADS, aids in mitigating bias across various versions of SD. Nonetheless, as discussed in the main text, it may potentially compromise the alignment between images and text."}, {"title": "Minor Attribute Guidance", "content": "Figure 15 illustrates the experimental results regarding minor attribute guidance with SD-v2 and SDXL, as elaborated in section 3.3. As the end time ($t'$) for the minor attribute-specified prompt decreases, the ratio of minor attribute increases. With SDXL, employing a minor attribute-specified prompt from t = 1 to t = 0.5 ($t'$ = 0.6) results in over approximately 90% of the images being generated with minor attributes across most professions. With SD-v2, a longer duration of employing a text prompt specifying a minor attribute was required to achieve a similar minor attribute ratio. This observation demonstrates that guiding the diffusion process with a prompt specifying a minor attribute during the initial diffusion steps is effective across various versions of SD."}, {"title": "Justification of Design Choices of Our Method", "content": "In section 4.2, we justified our design choices and provided analysis with SD-v1.5. Additionally, we analyze different versions of SD, SD-v2 and SDXL. Figure 16 illustrates the impact of adding $a_k$ to $c$ in terms of attribute ratio and CLIP score within SD-v2 and SDXL. It demonstrates that adding $a_k$ to $c$ (green) yields a similar effect to using attribute-specified text prompts (purple). In the case of SDXL,"}, {"title": "Image-Text Alignment", "content": "To evaluate the versatility of our method, we generate 100 images with text prompts where the gender is explicitly specified. The image generation process is repeated three times, and the mean and standard deviation of the attribute ratio are reported in Tables 3 and 4. Table 3 provides detailed values illustrated in Figure 9, showing the results for eight professions using SD-v1.5 with our method and other baselines. Table 4 presents results for SD-v2 and SDXL. The results show that our method successfully generates the explicitly specified attribute in the text prompts across all models with a ratio close to 1.0, ensuring versatility. Other methods, including FairDiffusion, fail to achieve a ratio of 1.0, indicating they often fail to generate attributes specified in the text prompts."}, {"title": "Debiasing Results", "content": "Table 5 shows the de-biasing results on eight different professions using SD-v2 and SDXL. The numbers indicate the proportion of the minor attribute. It is worth noting that SDXL produces a higher ratio of male teachers compared to female teachers, which differs from SD-v1.5 and SD-v2. However, we maintain consistency by reporting the male ratio for teachers. The table demonstrates that our method effectively mitigates bias across different models."}, {"title": "Qualitative Results", "content": "Figure 17, 18, and 19 shows examples of images generated with vanilla SD and our method using SD-v1.5, SD-v2, and SDXL, respectively. These figures demonstrate our method successfully generates images with minor attributes, regardless of the model version used. Images with major attributes are also produced without compromising quality.\nQualtitative Comparison with FairDiffusion. Figure 20 provide additional qualitative comparisons between FairDiffusion and our method. The individuals in the images generated with FairDiffusion often exhibit physical traits of both women and men simultaneously, while those in the images generated with our method do not."}, {"title": "Debiasing on Racial Bias", "content": "We additionally evaluate our method within racial bias. Here, we use a name list of attributes: [``White person'', ``Black person'', ``Asian'', ``Indian'', ``Latino'']. Other settings are set same as evaluating within gender bias. Table 7 shows that our method shows reduced ratio of major attribute compared to vanilla SD in all professions."}]}