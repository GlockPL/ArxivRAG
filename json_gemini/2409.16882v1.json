{"title": "Revisiting Space Mission Planning: A Reinforcement Learning-Guided Approach for Multi-Debris Rendezvous", "authors": ["Agni Bandyopadhyay", "G\u00fcnther Waxenegger-Wilfing"], "abstract": "This research introduces a novel application of a masked Proximal Policy Optimization (PPO) algorithm from the field of deep reinforcement learning (RL), for determining the most efficient sequence of space debris visitation, utilizing the Lambert solver as per Izzo's adaptation for individual rendezvous. The aim is to optimize the sequence in which all the given debris should be visited to get the least total time for rendezvous for the entire mission. A neural network (NN) policy is developed, trained on simulated space missions with varying debris fields. After training, the neural network calculates approximately optimal paths using Izzo's adaptation of Lambert maneuvers. Performance is evaluated against stan-dard heuristics in mission planning. The reinforcement learning approach demonstrates a significant improvement in planning efficiency by optimizing the sequence for debris rendezvous, reducing the total mission time by an average of approximately 10.96% and 13.66% compared to the Genetic and Greedy algorithms, respectively. The model on average identifies the most time-efficient sequence for debris visitation across various simulated scenarios with the fastest computational speed. This approach signifies a step forward in enhancing mission planning strategies for space debris clearance.", "sections": [{"title": "I. INTRODUCTION", "content": "Space debris, commonly referred to as space junk, is any non-functional, artificial material orbiting the Earth. This debris predominantly accumulates in low Earth orbits, but significant quantities are also found near and above geo-stationary orbits. The European Space Agency's statistical model [1] estimates the presence of approximately 36,500 space debris objects larger than 10 cm, over a million objects ranging from 1 cm to 10 cm, and around 130 million objects measuring 1 mm to 1 cm in size [2]. A notable incident occurred during the STS-7 mission in 1983, when a paint fleck of merely 0.2 mm struck the shuttle's window, creating a 0.4 mm deep pit. This event, though seemingly minor, exceeded the damage threshold for reusing the window's outer pane in future missions and stands as the first recorded instance of Space Shuttle damage caused by orbital debris [3]. The Kessler Syndrome [4], [5] highlights the risk of a cascading effect, where increased debris density could lead to further debris generation. This phenomenon poses a significant threat to future space activities in these debris-laden orbits, as emphasized in the recent report by NASA [6]. The report underscores the urgent need for enhanced debris mitigation efforts, more than ever before in our space exploration history. However, as with every other space mission, mission planning comprises a crucial part. Thus an optimised mission planning can help with respect to fuel efficiency or in optimising the total time for rendezvousing with all given debris, which is the focus of our paper.\nThis research contributes to filling this gap by introducing an improved model for mission planning that optimizes schedules to enhance debris clearance efficiency. Focusing the order in which the debris should be visited so that all of them are rendezvoused in the fastest possible time. By leveraging advanced algorithms in machine learning, specif-"}, {"title": "II. DEBRIS RENDEZVOUS FRAMEWORK", "content": "A. Active Debris Removal\nDebris removal methods can be broadly classified into two separate groups active and passive removal methods. Active debris removal [7] is the method of removing debris from orbits by first rendezvousing with them and then using active tools like harpoons, robotic arms and others. It is generally employed near the medium Earth orbits where there is no graveyard orbit and the possibility of reentry of the debris into the Earth's atmosphere is low.\nB. Travelling Salesman Problem formulation for Active De-bris Removal\nMission planning for active debris removal can be cast as Travelling Salesman Problem [8]. Assuming a spacecraft in a base orbit, one tries to find a path for rendezvousing with all the given debris within the least amount of time. An illustration for two debris rendezvous is shown in [Fig.1]. Different optimization algorithms ([9], [10] and [11]) have been investigated for solving the resulting TSP variant.\nC. Lambert's Problem\nFor rendezvousing with the debris using the spacecraft, we use the modified Lambert's problem (or Izzo's adaptation of Lambert problem) [12] to express and then solve for the time of flight equation. This modified algorithm by Izzo for solv-ing the Lambert problem is approximately 1.25 times faster to execute (when multiple revolutions are not considered) than the traditionally used Gooding's algorithm. Here our spacecraft uses only two impulses: once at the start to set the trajectory and finally one at the rendezvous point to stay in the same orbit as the target [Fig.2]. Expensive maneuvers like inclination change are included in our framework and we assume that we always have enough fuel for the complete rendezvous."}, {"title": "III. TRADITIONAL HEURISTICS", "content": "A. Greedy method\nA greedy algorithm [10] functions by choosing the current local optimal solution and hopes to achieve a global optimal solution. It is used because it is one of the fastest methods to get a good solution, which might not be the best solution. With reference to our modified travelling salesman problem [13], at every move it chooses the debris which takes the least time to rendezvous and progresses forward."}, {"title": "IV. REINFORCEMENT LEARNING", "content": "Reinforcement Learning (RL) [19], is an area of ma-chine learning where an agent learns to improve its actions by interacting with its environment. The process involves the agent observing the current state of the environment"}, {"title": "V. RL ENVIRONMENT AND COMPUTATIONAL SETUP", "content": "When formulating an optimization as an RL problem, the definition of the RL environment including the reward func-tion and the selection of a suitable algorithm are particularly important.\nA. Action Space\nThe action space encapsulates the range of actions acces-sible to the agent. In our specific context the action space is discrete, with each action corresponding to the agent's decision regarding the next debris to be targeted.\nB. State Space\nThe state space must contain all the information that the agent needs to infer the optimal current action. In our case, it is given by an array containing all six Keplerian elements as well as the Cartesian coordinates of the current position for all debris objects, the Cartesian coordinates of the rendezvousing spacecraft and the list of the visited debris objects. In this case, all elements are continuous, except for the list of visited debris objects, which is discrete."}, {"title": "C. Episode Definition and Policy", "content": "An episode in our model is defined as the completion of visits to all debris. The episode length is thus equivalent to the total number of debris. Following Huang et al. (2006) [27], we incorporate invalid action masking. An invalid action, in our scenario, is defined as any attempt by the interceptor (for example, spacecraft) to revisit a debris site. This mechanism ensures that each piece of debris is visited only once."}, {"title": "D. Reward Function", "content": "The rewards are calculated as follows for all maneuvers except the final rendezvous:\n$R_t = \\frac{T}{T_{max}}$\nwhere:\n\u2022 Rt: Reward value.\n\u2022 T: Time to Rendezvous, indicating the actual time taken to complete a specific debris rendezvous.\n\u2022 Tmax: Maximum Time for Rendezvous, representing the upper limit for the longest acceptable duration for a rendezvous.\nIn this research, the value of Tmax is not arbitrary; it is determined based on the maximum expected time for any single rendezvous maneuver within the scope of our simulations. The normalization of the time-to-rendezvous by Tmax serves a dual purpose: it ensures that the reward remains within a consistent range irrespective of mission duration variations, and it simplifies the comparison of results across different mission scenarios. This formulation ensures that the reward Rt is normalized between -1 and 0, with -1 being the least efficient and 0 the most efficient outcome. Our sensitivity analyses have shown that such normal-ization does not impact the final optimization result, but rather ensures the algorithm's learning process is stable and efficient across diverse operational contexts. For the final rendezvous, we use the same reward calculation but add an additional factor of +1 as further bonus. This approach balances individual maneuver efficiency with the overall mission objective, aligning the algorithm's performance with the goal of planning time-optimal multi-debris rendezvous."}, {"title": "E. Dynamic Decision-Making of the Agent", "content": "Unlike traditional methods where the entire debris visi-tation sequence is pre-planned, our agent adopts a dynamic decision-making approach [28]. At each step, the agent visits one debris object and then analyzes the current scenario to determine the next target. This method offers significant flexibility for real-time adjustments, such as collision avoid-ance or removal operations that last longer than expected. Consequently, the agent's ability to adapt its path on-the-fly increases the overall efficiency of the debris clearing operation."}, {"title": "VI. OVERVIEW OF SIMULATION TEST CASE AND ALGORITHM CONFIGURATION", "content": "Using a MacBook Pro with 64GB of memory and M1 processor, along with Python 3.10, we employed physics simulation libraries such as poliastro [29] and astropy [30], as well as optimization algorithms implemented via DEAP [31] and Stable Baselines3 [32]. The Iridium 33 debris data, sourced from Celestrak [33], was used for simulations within the period from November 23 to November 26, 2023. We divided the dataset into training (70%), testing (15%), and evaluation (15%) subsets. Each simulation involved selecting ten random debris pieces from the dataset for a given date. Our spacecraft's goal is to rendezvous with all selected debris in the shortest possible time from a given parking orbit, as-suming sufficient fuel availability. For detailed computational setup and hyper parameter configurations, refer to Appendix A."}, {"title": "VII. RESULTS AND DISCUSSION", "content": "In this section, we delve into the performance evaluation of our approach, focusing on their efficiency and efficacy in planning and multi-debris rendezvous. Through compar-ative analysis, we aim to underscore the distinct advantages offered by the RL approach."}, {"title": "VIII. SUMMARY AND OUTLOOK", "content": "Reinforcement Learning (RL) is advancing the frontier of combinatorial optimization, a development that our study reinforces. Beyond addressing intricate problems, RL gen-erates on average optimal solutions, which surpass state of the art heuristics and offers efficiency in execution after the extensive training phase. The sequential approach of generating the solution constitutes an essential feature for the integration of complex maneuvers like collision avoidance as well as for refuelling scenarios."}, {"title": "APPENDIX", "content": "APPENDIX A: DETAILED MASKABLE PPO TRAINING HYPERPARAMETERS\nThe configuration of our Maskable Proximal Policy Opti-mization (PPO) algorithm, implemented using Stable Base-lines3 [32], is tailored for the space debris targeting task. The key hyper parameters are as follows:\n\u2022 Learning Rate: 3 \u00d7 10-4, controlling the update rate of the agent's policy.\n\u2022 Number of Steps: 2048, defining the number of steps collected before updating the model.\n\u2022 Batch Size: 64, the size of the batch used in the optimization process.\n\u2022 Number of Episodes: 200,000, where 1 episode con-sists of 10 time steps.\n\u2022 Discount Factor (\u03b3): 0.99, used in calculating the discounted future rewards.\n\u2022 GAE Lambda (\u03bb): 0.95, for the Generalized Advantage Estimator (GAE).\n\u2022 Clip Range: 0.2, for the PPO clipping in the policy objective function.\n\u2022 Value Function Coefficient (vf_coef): 0.5, the scaling factor for the value function loss in the total loss calculation.\n\u2022 Maximum Gradient Norm (max_grad_norm): 0.5, used for gradient clipping.\n\u2022 Entropy Coefficient (ent_coef): 0.0, which adds an entropy bonus to the reward to ensure sufficient explo-ration.\n\u2022 Verbose Level: Set to 0 for minimal output during training.\n\u2022 Device: Set to 'auto', allowing the system to choose the appropriate computation device (CPU or GPU)."}, {"title": "APPENDIX B: TERMINOLOGY AND DEFINITIONS", "content": "In this paper, we have adopted specific terms that are pivotal to the understanding of the mission design and planning. Here, we clarify these terms to ensure clarity and avoid any potential ambiguity:\n\u2022 Parking Orbit/Base Orbit: The term 'parking orbit' is used interchangeably with 'base orbit' to refer to the initial orbit where the spacecraft begins its debris removal operations."}, {"title": "APPENDIX C: ACRONYMS AND FIGURES", "content": "LIST OF ACRONYMS\nAI Artificial Intelligence\nPPO Proximal Policy Optimization\nSTS Space Transportation System\nTSP Travelling Salesman Problem\nRL Reinforcement Learning\nDEAP Distributed Evolutionary Algorithms in Python\nADR Active Debris Removal\nESA European Space Agency\nNASA National Aeronautics and Space Administration"}]}