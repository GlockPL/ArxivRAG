{"title": "Aligning Instruction Tuning with Pre-training", "authors": ["Yiming Liang", "Tianyu Zheng", "Xinrun Du", "Ge Zhang", "Xingwei Qu", "Xiang Yue", "Chujie Zheng", "Jiaheng Liu", "Lei Ma", "Wenhu Chen", "Guoyin Wang", "Zhaoxiang Zhang", "Wenhao Huang", "Jiajun Zhang"], "abstract": "Instruction tuning enhances large language models (LLMs) to follow human instructions across diverse tasks, relying on high-quality datasets to guide behavior. However, these datasets, whether manually curated or synthetically generated, are often narrowly focused and misaligned with the broad distributions captured during pre-training, limiting LLM generalization and effective use of pre-trained knowledge. We propose Aligning Instruction Tuning with Pre-training (AITP), a method that bridges this gap by identifying coverage shortfalls in instruction-tuning datasets and rewriting underrepresented pre-training data into high-quality instruction-response pairs. This approach enriches dataset diversity while preserving task-specific objectives. Evaluations on three fully open LLMs across eight benchmarks demonstrate consistent performance improvements with AITP. Ablations highlight the benefits of adaptive data selection, controlled rewriting, and balanced integration, emphasizing the importance of aligning instruction tuning with pre-training distributions to unlock the full potential of LLMs.", "sections": [{"title": "1. Introduction", "content": "Instruction tuning is essential for adapting large language models (LLMs) to effectively follow human instructions across diverse tasks. This process relies on high-quality datasets to guide model behavior, yet existing instruction-tuning datasets are often narrowly focused, relying on either manual annotation or synthetic generation. While manual datasets offer precision, they are costly and lack scalability (Wang et al., 2022b; Zhou et al., 2023a). Synthetic datasets, on the other hand, frequently depend on expensive APIs of strong models and are tightly coupled with their generation pipelines, limiting flexibility (Peng et al., 2023; Lian et al., 2023). Additionally, manually combining open-source datasets, as seen in efforts like OpenHermes-2.5 (Teknium, 2023) and Tulu-V2 (Ivison et al., 2023), often overlooks the underlying data distributions, leading to inefficiencies.\nPre-training corpora, by contrast, reflect broader real-world distributions and align closely with the internal knowledge of LLMs, making them a rich source of high-quality supervisory signals. However, current instruction-tuning methods fail to leverage this alignment, creating a fundamental gap in optimizing dataset coverage and distribution. Addressing this challenge requires aligning instruction-tuning datasets with pre-training distributions to fully exploit the knowledge embedded in LLMs.\nIn this paper, we propose Aligning Instruction Tuning with Pre-training (AITP), a method that systematically bridges this gap. Rather than generating instruction-response pairs from scratch, AITP identifies gaps in existing datasets by comparing their distribution to that of the pre-training corpus. Underrepresented data is then rewritten into high-quality instruction-response pairs, enhancing dataset coverage and alignment. As shown in Figure 2, AITP involves three stages: (1) generating a difference set based on density comparisons, (2) rewriting raw text into instruction-response pairs, and (3) integrating these pairs into the original dataset for fine-tuning.\nFigure 1 visualizes the significant distributional differences between instruction-tuning datasets and the pre-training corpus, underscoring the need for such alignment. Through experiments on three open-source LLMs across eight benchmarks, we demonstrate that AITP consistently improves model performance. Detailed ablation studies highlight the effectiveness of adaptive data selection and integration, showing how AITP guides instruction tuning toward more effective and generalizable fine-tuned models.\nOur contributions include: 1) Demonstrating the distributional gaps between instruction-tuning datasets and pre-training corpora through visualization. 2) Proposing the AITP method to adaptively optimize instruction-tuning datasets by leveraging pre-training corpora as a reference. 3) Validating the effectiveness of AITP with extensive experiments and ablation studies."}, {"title": "2. Methods", "content": "2.1. Difference Set Generation\nIn this section, we define the process of difference set generation, isolating data points from the pre-training corpora that differ from those in the SFT dataset. The goal is to identify regions in the pre-training data distribution that are absent from or sparsely populated in the supervised fine-tuning (SFT) data. This can be formalized as follows:\nD_{diff} = \\{d_i | d_i \\in D_{pretrain}, \\Delta(d_i, D_{SFT}) < \\tau\\} \\qquad(1)\nWhere $D_{pretrain}$, $D_{SFT}$, $D_{diff}$ represent the pre-training dataset, the SFT dataset and the resulting difference set, respectively. $\\Delta(d_i, D_{SFT})$ represents the density estimate of the data point $d_i$ in the SFT dataset, and $\\tau$ is the threshold that determines whether a data point should be included in the difference set. To achieve this, we outline the procedure in three main stages: data representation, density estimation, and identification of the difference set."}, {"title": "2.1.1. DATA REPRESENTATION", "content": "Each data point is represented as a vector derived from the final-layer embedding of the model. We then apply dimensionality reduction (DR) to project these high-dimensional embeddings into two-dimensional coordinates, facilitating visualization and density comparison across datasets. This process can be formalized as follows:\n(x_i, y_i) = DR(Model(d_i)) \\qquad(2)\nApplying the same dimension reduction to both pre-training and SFT embeddings results in two sets of two-dimensional vectors:\nZ_{pretrain} = \\{(x_i, y_i) | d_i \\in D_{pretrain}\\} \\qquad(3)\nZ_{SFT} = \\{(x_i, y_i) | d_i \\in D_{SFT}\\} \\qquad(4)"}, {"title": "2.1.2. DENSITY ESTIMATION", "content": "To compare data distributions between the pre-training and SFT datasets, we use Kernel Density Estimation (KDE) to visualize the density of points for each dataset. The KDE function $f(x, y)$ estimates the density at any location (x, y) based on neighboring points:\nf(x,y) = \\frac{1}{nh_xh_y}\\sum_{i=1}^{n} K \\bigg(\\frac{x-x_i}{h_x}, \\frac{y-y_i}{h_y}\\bigg) \\qquad(5)\nK(\\,) is the kernel function, typically Gaussian:\nK((x, y), (x', y')) = exp\\bigg(-\\frac{(x-x')^2+(y-y')^2}{2\\sigma^2}\\bigg) \\qquad(6)\nWhere (x, y) and (x', y') are two two-dimensional data points, $h_x$, $h_y$ and $\\sigma$ are bandwidth parameters that control the smoothness in the x direction, y direction and kernel respectively. The KDE visualization highlights distribution differences, identifying regions of divergence between the pretraining and SFT datasets."}, {"title": "2.1.3. FINDING DIFFERENCE SET", "content": "The difference set is identified based on the density estimates from the SFT dataset. Specifically, if a point $d_i$ in the pre-training dataset has a low-density estimate within the SFT dataset, we classify this point as absent or sparsely populated in the SFT data. Such points contribute to the observed distributional differences between the two datasets, and we define them formally as:\nD_{diff} = \\{d_i | d_i \\in D_{pretrain}, f_{SFT}(x_i, y_i) < \\tau\\} \\qquad(7)\nf_{SFT}(x_i, y_i) represents the density estimate of the data point $d_i$ from the pretrain corpus within the SFT dataset.\nf_{SFT}(x_i, y_i) = \\frac{1}{nh_xh_y} \\sum_{i=1}^{n}K \\bigg(\\frac{x_i - x_j}{h_x}, \\frac{y_i - y_j}{h_y}\\bigg) \\qquad(8)\nWhere (x_i, y_i) \\in Z_{pretrain}, (x_j, y_j) \\in Z_{SFT}$. n is the total number of points in the SFT dataset."}, {"title": "2.2. Data Transformation of Difference Set", "content": "The data transformation phase is designed to convert raw text from pre-training data within the difference set into instruction-pair data formatted for SFT. First, we develop a query generation prompt to guide the model in generating relevant questions from the raw text. Next, we implement a query scoring prompt to assess the quality of each generated query. Low-quality queries are filtered out based on these scores, enabling us to eliminate unsuitable questions before answer generation, thus conserving computational resources. Finally, an answer generation prompt is applied to instruct the model in generating responses to the remaining high-quality queries."}, {"title": "2.3. Training", "content": "In this phase, the model is trained on a combined dataset that includes both the rewritten data derived from the difference set and the original SFT dataset. Notably, the model trained on the combined dataset is the same as the one trained on the pre-training corpus. This serves two main purposes: first, it ensures consistency between the supplemented knowledge distribution and the model's internal knowledge. Second, high-quality instruction-pair data helps correct semantic inaccuracies that may arise from formatting errors in the pre-training corpus."}, {"title": "3. Experiment Settings", "content": "3.1. Evaluation\nWe evaluate the model's instruction-following ability using the IFEval benchmark (Zhou et al., 2023b), which is unbiased because it does not rely on LLM-generated evaluation scores. It provides four types of accuracy scores: Prompt-level Strict-accuracy (P-S), Instruction-level Strict-accuracy (I-S), Prompt-level Loose-accuracy (P-L), and Instruction-level Loose-accuracy (I-L). We use the OpenCompass, a comprehensive, one-stop platform for LLM evaluation (Contributors, 2023). We evaluate the effectiveness of AITP across seven standard benchmarks. These benchmarks provide a comprehensive evaluation of the diverse capabilities of language models across various tasks and domains. MMLU (Hendrycks et al., 2021) offers a broad assessment of multitask reasoning and knowledge retrieval, while ARC-c (Clark et al., 2018) and GPQA-diamond (Rein et al., 2023) focus on complex scientific reasoning and physics-specific understanding, respectively. For code generation and problem-solving, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) measure a models ability to write correct and multi-step logical solutions. Additionally, HellaSwag (Zellers et al., 2019) tests commonsense reasoning by predicting contextually appropriate continuations, and GSM8K (Cobbe et al., 2021) challenges models with elementary-level math problems, combining natural language understanding with mathematical reasoning."}, {"title": "3.2. Main Setting", "content": "Our experiments utilize three fully open-source models: OLMO (Groeneveld et al., 2024), MAP-Neo (Zhang et al., 2024a) and Pythia (Biderman et al., 2023). These models not only release model weights but also training datasets and intermediate checkpoints, aiming to facilitate reproduction and advance scientific research in LLMs. In this paper, the OLMo-7B-base, MAP-Neo-7B-base, and Pythia-12B models, along with their corresponding pre-training corpora, are chosen as the foundational setup for AITP. The OLMO-7B-SFT and MAP-Neo-7B-SFT-v0.1 models are used as baselines to validate the effectiveness of AITP. Since the SFT dataset for Pythia has not been released, we use Tulu-v2 for fine-tuning as the baseline for Pythia.\nDue to the substantial storage and computational resources required for the data embedding and shift phase, we don't use the full pre-training corpus given resource constraints. Instead, we apply reservoir sampling (Vitter, 1985), an algorithm that enables uniform sampling from streaming data, ensuring that the sampled subset maintains a distribution consistent with the full pre-training corpus. The reservoir sampling algorithm is described in the Appendix B.\nWe conduct experiments on the NVIDIA A800-SXM4-80GB, with the difference set generation phase taking approximately 56 GPU hours. The Data Transformation Setting phase utilizes the vLLM (Kwon et al., 2023) framework to accelerate inference, requiring approximately 640 GPU hours, while the Training Setting phase, involving full-parameter fine-tuning, takes approximately 256 GPU hours."}, {"title": "3.3. Difference Set Generation Setting", "content": "We obtain the text embeddings using two encoding models: BAAI/bge-m3 (Chen et al., 2024) and sentence-transformers/all-MiniLM-L6-v2 (Reimers & Gurevych, 2019). We choose the all-MiniLM-L6-v2 model for its simplicity and ease of use, while bge-m3 can handle multilingual input and varying input lengths, from short sentences to long documents up to 8192 tokens. For the pre-training corpus, we directly use the text field as input for encoding. For the SFT dataset, we concatenate the instruction and response fields to form a complete input text for encoding. After obtaining the text embeddings, we apply principal component analysis (PCA) to reduce the high-dimensional data to two dimensions, thus simplifying the visualization and analysis. For visualization, we employ kernel density estimation (KDE), which effectively represents data density by smoothing distributions and avoids the issue of point overlap in dense regions that can occur in scatter plots.\nTo identify the difference set, we use two settings: density estimation and density comparison. The density estimation setting is presented in Equation 7 and Equation 8. In this paper, the density comparison setting compares the density estimation of each data point in the pre-training and SFT datasets, selecting difference points based on their density ratio. The density comparison setting is formalized as follows:\nf_{Pre}(x_i, y_i) = \\frac{1}{mh_xh_y} \\sum_{k=1, k\\neq i}^{m} K \\bigg(\\frac{x_i - x_k}{h_x}, \\frac{y_i - y_k}{h_y}\\bigg) \\qquad(9)\nD_{diff} = \\{d_i | d_i \\in D_{pretrain}, \\frac{f_{Pre}(x_i, y_i)}{f_{SFT}(x_i, y_i)} > \\tau\\} \\qquad(10)\nWhere $(x_i, y_i), (x_k, y_k) \\in Z_{pretrain}$. m is the total number of points in the pre-training dataset. In this paper, we set $\\tau$ to 0.7 and 1.0 for equations (7) and (10), respectively."}, {"title": "3.4. Data Transformation Setting", "content": "We employ the Qwen2.5-72B-Instruct (Team, 2024) model for data transformation. In the instruction generation phase, we ensure that generated instructions are contextually relevant and self-contained, meaning they should not require the raw text as background for understanding. During the instruction scoring phase, each instruction is assessed based on three criteria: quality, difficulty, and the additional information required. We rate the quality of each instruction on a scale from 1 to 10 based on its clarity, assess its difficulty depending on whether specialized knowledge is required, and mark the additional information required field as True or False, based on whether extra information is needed to fully answer the query. In the answer generation phase, the model is prompted to produce comprehensive and accurate responses informed by both the instruction and text content, ensuring that the responses are detailed and well-aligned with the question context. The prompts we used can be found in Appendix C."}, {"title": "3.5. Ablation Setting", "content": "We conduct two ablation studies to evaluate the impact of dataset size and distillation during the data transformation process on AITP. To determine whether the improvement arises from the increased size of the SFT dataset after adding the rewritten difference set, we sample a subset from the combined dataset (original SFT and rewritten difference set) that is equal in size to the original SFT dataset and use it for training. To test whether the improvement is due to distillation in the data transformation phase, we replace the original SFT dataset with a subset sampled from the pre-training corpus that shares a similar distribution and train the model on the combined dataset (the rewritten same set and the rewritten difference set). This setup aligns with the approach used in LongForm (Kksal et al., 2023), which trains models on fully rewritten pre-training datasets but overlooks leveraging existing high-quality datasets."}, {"title": "3.6. Training Setting", "content": "We use combined datasets in AITP to train three open-source models: OLMO, MAP-Neo, and Pythia. The rewritten difference set in the combined datasets is obtained by subtracting the corresponding SFT datasets (TuluV2, Neo-SFT, TuluV2) from the respective pre-training corpora (Dolma, Matrix, and Pile). Since the SFT dataset for Pythia has not been released, we use TuluV2 as a substitute. Full-parameter fine-tuning is applied, with the detailed training parameters provided in Appendix D."}, {"title": "4. Results", "content": "4.1. Distribution Change Analysis\nIn the density estimation setting, AITP focuses on the dense regions of the SFT dataset and the pre-training corpus to identify points in the pre-training corpus that are underrepresented in the SFT dataset. Figure 3a highlights the dense regions of Tulu and Dolma (examples are provided in Appendix F). Dense regions 1 and 2 correspond to code and scientific literature data, respectively. Figure 3b demonstrates that the difference set avoids the dense regions in the SFT dataset and aligns with dense regions of Dolma. Figure 3c shows the narrowing of the distribution during rewriting (examples are provided in Appendix F), while Figure 3d indicates that the combined dataset expands the original SFT distribution and highly overlaps with the dense regions of the pre-training corpus. In the density comparison setting (Figure 3e-3h), AITP focuses on points where the pre-training corpus has a higher density than the SFT dataset. Similarly, AITP with the density comparison setting can also expand the coverage of the existing dataset and optimize the data distribution."}, {"title": "4.2. Main Results", "content": "As shown in Table 1, compared to the SFT model of OLMO, MAP-Neo, and Pythia baselines, the counterparts trained with AITP achieve average performance improvements of 3.77, 1.11, and 0.97 across eight benchmarks. This illustrates the effectiveness of AITP. We suppose that this improvement results from AITP supplementing the original SFT dataset with lacking data, expanding its coverage, and optimizing its distribution.\nBased on the analysis in subsection 4.1, we can summarize two points supporting the above supposition: (1) A comparison of Figure 3a and 3b reveals that the difference set includes data from the pre-training corpus that is lacking in SFT datasets, such as code and scientific literature data. (2) Although the distribution narrows during the rewriting process (as shown in Figure 3b and 3c), the final combined dataset expands the coverage of the original SFT dataset, and the dense regions of the combined data align closely those of the pre-training corpus (as shown in Figure 3d)."}, {"title": "4.3. Difference Set Generation Setting Results", "content": "Table 2 presents the experimental results for various embedding models and different set generation settings. As shown in Table 2, the four AITP variants show improvements over the baseline model OLMO-SFT across various settings: using the bge model with density estimation to identify the difference set achieves an average absolute improvement of 3.77; using bge with density comparison yields an improvement of 4.31; using MiniLM with density comparison results in an improvement of 2.93; and using bge with density comparison achieves an improvement of 3.10. These results suggest that AITP is robust across various choices of embedding model and difference set generation method."}, {"title": "4.4. Ablation Results", "content": "To verify whether the gains of AITP result from the increased size of the SFT dataset after adding the rewritten difference set, we sample a subset from the combined dataset (original SFT and rewritten difference set) that is equal in size to the original SFT dataset and use it for training. Comparing the first and third rows in Table 3, the AITP method achieves an average absolute improvement of 2.08, even with the same dataset size. Comparing the third and fourth rows, the improvement for the same dataset size setting is smaller than the final AITP improvement.\nAdditionally, to test whether the improvement arises from distillation by a stronger model during the rewriting phase, we replace the original SFT dataset with the rewritten dataset from the same distribution and train the model on a combined dataset (rewritten same distribution set and rewritten difference set). Comparing the first and second rows in Table 3, the distillation setting does not outperform the OLMo-SFT baseline, likely because the quality of the rewritten data is lower than that of the original SFT dataset. This indicates that the improvement does not result from distillation by an aligned model."}, {"title": "4.5. Ratio Results", "content": "We further investigate the effect of incorporating various ratios of rewritten difference data on AITP. As shown in Figure 4, the AITP achieves excellent performance with a rewritten data set comprising less than 10% of the original SFT dataset. However, performance declines as the size of the rewritten set increases. We hypothesize that incorporating a small amount of rewritten data improves model performance significantly by filling gaps in the original SFT data. On the other hand, the quality of the rewritten data might be low, which could degrade the overall data quality when the rewritten ratio is increased. This is consistent with the ablation study on data size in Section 4.4, which shows that the quality of the rewritten data is lower than that of the original SFT dataset and that the improvement in AITP is not due to the increased data size."}, {"title": "4.6. Visualization", "content": "Figure 5 illustrates that the manually combined original SFT dataset (Tulu) forms multiple distinct clusters, indicating a high level of diversity within the original dataset. The rewritten data is densely distributed in areas underrepresented by the original SFT dataset, while intentionally avoiding regions where the original SFT dataset is densely populated. This result clearly demonstrates the effectiveness of the difference set generated by AITP in optimizing data coverage."}, {"title": "5. Related Works", "content": "5.1. Improving LLM Using Synthetic Data\nSome methods enhance model capabilities by synthesizing data using external signals, such as seed data (Wang et al., 2022a; Sun et al., 2023; Kang et al., 2024; Liang et al., 2024; Taori et al., 2023), pre-training data (Li et al., 2023; Zheng et al., 2024), query data (Huang et al., 2023; Madaan et al., 2023; Yu et al., 2023), feedback data (Lu et al., 2023; Scheurer et al., 2022), and retrieval-augmented generation (RAG) (Asai et al., 2023). These methods can be classified into two types: those that generate synthetic data using the model itself (Liang et al., 2024; Wang et al., 2022a; Sun et al., 2023) and those that use a teacher model for data synthesis (Lee et al., 2024; Li et al., 2024; Taori et al., 2023). While synthetic data approaches effectively mitigate the limitations of supervised dataset sizes, they also introduce challenges such as increased hallucinations, lack of diversity, low quality, and distribution misalignment (Liu et al., 2024). Training models iteratively with this synthetic data can lead to issues like model collapse, increased hallucinations, and reduced generalizability (Shumailov et al., 2023; Alemohammad et al., 2023; Guo et al., 2024).\nRecent studies address these limitations through various methods. Some methods aim to improve the quality of generated instruction pairs using self-consistency(Huang et al., 2023), reflection(Renze & Guven, 2024; Li et al., 2024), filtering (Liang et al., 2024; Yuan et al., 2024), and Monte Carlo tree search (MCTS) (Xie et al., 2024; Gao et al., 2024). Others focus on enhancing diversity of generated instruction pairs (Ge et al., 2024; O'Neill et al., 2023), reducing hallucinations (Chung et al., 2023; Zhang et al., 2024b; Jones et al., 2023), or optimizing synthetic data distribution (Lupidi et al., 2024; Jiang et al., 2024b; Yang et al., 2024b). Our method mainly focuses on further enhancing the diversity of synthetic data after combining existing datasets manually."}, {"title": "5.2. Open-Source Large Language Model", "content": "Models like GPT-4 (OpenAI et al., 2023), Gemini (Team et al., 2023), and Claude (Anthropic, 2024) have demonstrated impressive performance and are now applied across a wide range of fields. However, these models are closed-source and are only accessible via API, which limits deployment flexibility. To address this limitation, several open-source models, such as LLaMA (Yang et al., 2024a), Qwen (Yang et al., 2024a), DeepSeek(DeepSeek-AI et al., 2024), ChatGLM (GLM et al., 2024), Mixtral (Jiang et al., 2024a), and Yi (AI et al., 2024), have emerged, providing open access to model weights.\nRecently, some open-source communities have released fully transparent models intended for scientific research, such as OLMO (Groeneveld et al., 2024), Map-Neo (Zhang et al., 2024a), LLM360 (Liu et al., 2023), and Pythia (Biderman et al., 2023). These models go beyond simply sharing weights, they also provide accessible pre-training corpora and SFT datasets, detailed data-cleaning processes, intermediate checkpoints, and reproducible code, thereby facilitating a more open and reproducible research ecosystem. In this paper, we primarily conduct experiments on fully transparent open-source models because of their accessible pre-training and SFT datasets. Notably, our method can also be applied to enhance the performance of closed-source models or models that only provide open-access weights."}, {"title": "6. Conclusion", "content": "The existing SFT datasets exhibit significant differences from the pre-training corpus in terms of coverage and distribution. In this paper, we present the AITP method, which adaptively fills the gaps in current manually-assembled SFT datasets by identifying the difference set between the pre-training corpus and the SFT dataset. This approach utilizes existing high-quality SFT data and offers guidance for synthesizing lacking data of existing SFT datasets. Our experiments demonstrate the effectiveness of AITP, showing that bridging the gap between SFT and pre-training datasets can be achieved by adding a small amount of difference data (less than 10%). This feature makes AITP a cost-effective and practical solution for real-world applications."}]}