{"title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as an oracle planner and oracle RAG information extractor, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and strategies on our task and compare their performance to a handcrafted planner. We find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and we offer suggestions on how to improve their capabilities.", "sections": [{"title": "Introduction", "content": "With the increased performance and affordability of Large Language Models (LLMs) [OpenAI, 2024; Dubey et al., 2024], LLM-based agents have exploded in popularity [Xi et al., 2023]. We define LLM agents as systems that incorporate at least one call to an LLM, which influences its autonomous decisions about actions in an environment. LLMs are promising as backbones for agent-based systems because they can leverage general knowledge acquired during pre-training to tackle tasks expressed in natural language. This enables human interactions during planning and execution, which are challenging for other planning paradigms such as Reinforcement Learning (RL) or symbolic planners [Xi et al., 2023].\nLLMs can query and interpret knowledge bases, ask questions, and incorporate new information via text, all through a flexible dialogue interface. Even though they're not designed as planners, they are increasingly applied to embodied environments where various methods and strategies [Yao et al., 2023b; Shinn et al., 2023; Yao et al., 2023a] aim to maximise the performance of autonomous agents [Xi et al., 2024]. But LLMs still exhibit problematic issues that limit their reliability and usefulness for general-purpose agents \u2013 hallucinations and brittleness to inputs, limited context windows, and lack of grounding when placed in new environments.\nMost existing benchmarks for evaluating LLM agents focus almost exclusively on success rates [Shridhar et al., 2021; Yao et al., 2022; Liu et al., 2023; Xi et al., 2024]: that is, the proportion of trials in which the agent achieved a goal state. Success rates measure whether the agent constructed a valid plan, but do not measure the efficiency or the quality of that plan. Some environments allow an arbitrary number of incorrect steps, while others immediately terminate an episode if a wrong action is executed. Reporting or measuring only success rates also introduces a dataset bias, as it implies that the solution to a given problem is easily verifiable, yet difficult to obtain. As a result, we argue that success rate alone is insufficient to capture the complexity of real-world scenarios. Each additional step of inference incurs non-negligible costs, so metrics should include more fine-grained assessments, such as how close the LLM's plan is to a handcrafted solution. Furthermore, effective agent-based systems should recognise when a task is unsolvable, as many real-world tasks may lie beyond the agent's capabilities and indeed many real-world tasks might be difficult to verify as doable, or not. Without the capacity to predict if there's no valid plan, an agent will incur significant costs in continually monitoring and replanning.\nTo this end, we introduce Plancraft, a new multi-modal planning evaluation dataset based on Minecraft, that constrains the environment to the crafting GUI (see Figure 1). Plancraft consists of planning problems of diverse complexity and length and includes a portion of the dataset that is intentionally unsolvable. Plancraft, unlike previous environments, allows us to benchmark agents against a planner, but in a setting that was designed by humans for humans. Since the crafting component of Minecraft is inherently designed for human players, Plancraft also offers a way to test how LLM agents can leverage human knowledge (in the form of the Minecraft Wiki) to solve planning tasks. We compare Plancraft with popular interactive datasets in Table 1.\nWe evaluate different LLM-based agents on our dataset, thereby providing LLM-agent baselines. We test both open-source and closed-source models and evaluate them against different sets of possible actions. For open-source models, we evaluate the impact of fine-tuning agents on a set of expert plans, and compare multi-modal agents to text-only models. We also fine-tune a bounding-box detection model on our environment to provide an interface through which text-only LLMs can interact with the multi-modal environment. We release all our baseline models and code along with the dataset and environment as a stand-alone Python package."}, {"title": "Related Work", "content": ""}, {"title": "LLM Agents", "content": "Various strategies have been proposed to leverage LLMs as agents with interactive feedback [Yao et al., 2023b; Huang et al., 2022; Shinn et al., 2023; Yao et al., 2023a; Wang et al., 2022b]. The basic idea is to harvest LLMs for information, which, if novel to the agent, may result in better behaviours than they would perform otherwise. The simplest strategy, ReAct [Yao et al., 2023b], consists of interleaving actions with 'thinking' steps where the LLM is allowed unconstrained generation. Other broader strategies, such as Reflexion [Shinn et al., 2023] or Inner Monologue [Huang et al., 2022] try to promote self-corrective behaviour through external modules or steps. Self-Consistency [Wang et al., 2022b] and Tree-of-Thought [Yao et al., 2023a] sample the space of possible paths and select a solution through a majority vote or through another LLM evaluation step. Although all of these techniques are simple to implement, they can often add significant overhead in inference compute.\nOne limitation of LLMs is the size of their context window. To address this, modern systems often incorporate a form of Retrieval-Augmented Generation (RAG) [Lewis et al., 2020], which searches a larger set of documents and then restricts the context to only relevant examples (by some quantitative metric), given the current observations and task. For example, Voyager [Wang et al., 2023a] augments its context with the most likely applicable skills for a given problem. Similarly, JARVIS-1 [Wang et al., 2023b] stores successful trajectories and augments the context with a relevant subset, estimated via similarity with the current observation. Since crafting is a core part of Minecraft, its Wiki contains a page with details of recipes for crafting all items. The Minecraft Wiki is therefore well suited to a RAG pipeline where an agent can query the Wiki as a knowledge source.\nEnabling LLMs to use external tools, such as web search, calculators, and database queries, can also significantly expand their capabilities beyond simple next-token prediction. This has inspired numerous research efforts [Nakano et al., 2022; Schick et al., 2023; Parisi et al., 2022; Yang et al., 2023; Patil et al., 2023] to integrate and evaluate tool use (or external actions) into LLMs. Some datasets, such as GAIA [Mialon et al., 2023], even evaluate agents without any re-"}, {"title": "LLM Evaluation Datasets", "content": "Research on LLMs-based agents requires datasets and benchmarks to evaluate them. These datasets vary significantly in the types of environments in which actions are executed, ranging from simplified text-only worlds to multi-modal environments. ALFWorld [Shridhar et al., 2021] and ALFRED [Shridhar et al., 2020] provide virtual home environments where the agent must manipulate objects to fulfil the instruction, expressed in natural language. Mind2Web [Deng et al., 2023], WebShop [Yao et al., 2022], and WebArena [Zhou et al., 2023] evaluate agents in web-based environments, with tasks requiring interaction with browser interfaces or e-commerce platforms. BabyAI [Chevalier-Boisvert et al., 2019] is a 2D maze-solving game with natural language instructions. Minecraft is also a popular benchmark for agent evaluation, with various datasets such as MineRL [Guss et al., 2019], MineDojo [Fan et al., 2022], and TextCraft Prasad et al. [2023]. In Table 1, we compare Plancraft against popular LLM evaluation datasets. While these vary in domain and implementation, all require agents to do multi-step planning to achieve tasks expressed in natural language."}, {"title": "Knowledge Base", "content": "Minecraft has a unique advantage in that it is a real game, rather than a construction designed for agent evaluation, and therefore there exists a variety of online content to assist human players. Several works have taken advantage of knowledge bases in Minecraft in conjunction with the game environment. Baker et al. [2022] collected a large dataset of Minecraft videos and trained a model, Video Pre-training (VPT), to solve tasks directly from pixels in MineRL [Guss et al., 2019]. Fan et al. [2022] introduced MineDojo along with a knowledge base of scraped resources such as Minecraft videos and pages taken from the Minecraft Wiki and Reddit forum; however, they do not evaluate whether these improve performance. In Plancraft, we collect the pages of recipes available on the Minecraft Wiki and use them to build a knowledge base to evaluate RAG capabilities. Additionally, we implement an oracle recipe search to provide an upper estimate of a well-performing RAG system."}, {"title": "Planner", "content": "Although all of the datasets listed in Table 1 can evaluate whether an agent achieves the goal, these lack a handcrafted policy to evaluate the quality of the agent's plans: they only measure whether the agent has reached the goal. Of the datasets that we compare to, only BabyAI [Chevalier-Boisvert et al., 2019] and ALFRED [Shridhar et al., 2020] release handcrafted policies or solvers. BabyAI implements a Bot Agent with handcrafted policies that can act as a teacher agent for learners in the environment. ALFRED releases expert demonstrations found using a PDDL solver and reports Path Weighted Metrics that take the length of the expert into account. Similarly, we implement a planner for Plancraft to provide a benchmark against which to compare agents."}, {"title": "Impossible Set", "content": "All existing datasets lack a mechanism for assessing task feasibility or for handling scenarios where tasks are fundamentally unsolvable, which introduces a bias toward tasks that always assume a solution exists. Plancraft addresses this gap by including an intentional set of impossible tasks - tasks that cannot be solved under the provided constraints. By incorporating these examples, we assess an agent's capacity to reason about task feasibility. This setup promotes more realistic and cost-effective decision making, encouraging agents to balance problem solving with the practicality of task solvability."}, {"title": "Plancraft", "content": "To create Plancraft, we implement the logic and visual representation of the Minecraft crafting GUI. Similarly to Prasad et al. [2023], we reconstruct the crafting process entirely in Python using game files and images of items obtained from the Wiki. Using image manipulation to overlay items on top of the inventory background significantly improves performance, as the observations do not require the Java game engine. Our environment has a one-to-one pixel mapping with the real Minecraft interface and supports multiple resolutions."}, {"title": "Assumptions", "content": "Plancraft makes a number of simplifying assumptions:\n1.  Single Agent: there is a single agent in the environment and any changes to the environment are a direct result of this agent's actions.\n2.  Deterministic: valid actions are always executed and their effects are predictable.\n3.  Observable: the crafting interface and inventory are fully observable, either through the text description or through the multi-modal image input.\n4.  Sequential: aligned with classical planning, each action is discrete and executed one at a time."}, {"title": "Action and Observation Space", "content": "The abstraction level chosen for the action and observation space has a great impact on the tractability of the planning problem. Other Minecraft environments such as MineDojo [Fan et al., 2022] and Textcraft [Prasad et al., 2023] provide a high-level 'craft' command that doesn't require the agent to manipulate the underlying ingredients. Plancraft, on the other hand, requires the correct placement of ingredients in the inventory and, as such, provides a lower-level symbolic action space. The two possible environment actions are smelt and move, and both actions require a $slot_{from}$, a $slot_{to}$ and a quantity. This abstracts control dynamics and focuses on planning and spatial reasoning.\nIn the Minecraft crafting GUI, there are 46 possible item slots or positions. The first of these is the crafting slot and is only populated with an item if the correct items have been deposited within the 3 \u00d7 3 crafting grid. Items can only be withdrawn from the crafting slot, and moving the item out of the slot confirms the crafting operation and removes the items present in the crafting grid. The crafting grid is an arrangement of slots 3 \u00d7 3, where items can be moved in and out. Crafting recipes in Minecraft are either Shapeless or Shaped. Shapeless recipes only check whether the items required are present in the crafting grid, whereas Shaped recipes require the items to also be placed in the correct positions. When the items on the crafting grid match one of the 634 recipes, then the resulting item is added to the crafting slot. The remaining 36 slots are generic inventory slots where the player can store items. We denote the crafting slot as [0], the 3 \u00d7 3 crafting grid as slots [A1] to [C3] where the letter denotes each row and the number denotes the column, and the remaining inventory slots as slots [11] to [136]. See Figure 2 for an example of a text description of the inventory that uses our annotation scheme.\nPlancraft provides two types of observations:\n\u2022 text-only: Agents receive a text description of the current inventory and crafting goal. The inventory is perfectly represented using our annotation scheme.\n\u2022 multi-modal: Agents receive an image (high resolution with dimensions 664 \u00d7 704) that perfectly contains the crafting GUI. The goal is still provided as text."}, {"title": "Task Setup", "content": "The tasks within Plancraft all involve crafting specific items using a predefined set of available resources. The goal of each task is expressed in natural language, such as 'Craft an item of type: green_bed'. The complexity of these tasks varies, ranging from single-step crafts (e.g., crafting wooden planks \u2714) to multi-step plans that require chaining multiple crafting recipes and actions (e.g., crafting a bed first requires planks and wool).\nTo generate crafting tasks, we represent the dependencies between items as a tree where each item is crafted from a set of required resources. The target item is the root of the tree, while the leaves and intermediate nodes represent the materials required to craft the item. For each crafting task, we sample the required recipes for the target item and then recursively explore the necessary materials to craft it. This builds a step-by-step plan from raw materials to the final product, simulating the crafting processes with varying levels of complexity. In addition to the necessary crafting materials, we also sample a number (4, 8, 16) of distractor items to make identifying the correct crafting path more challenging.\nFinally, we validate that all generated tasks can be solved in the text-only and multi-modal environments using a planner (see Section 3.4) and exclude any invalid examples. We exclude any paths where the handcrafted planner's trajectory exceeds 30 steps."}, {"title": "Dataset Statistics", "content": "We define a complexity metric as directly proportional to the number of items used and the number of recipes needed to craft the item. We assign each example trajectory to a complexity bin, from least to most complex (very easy, easy, medium, hard, very hard), and ensure an equal proportion of examples from each bin distribution.\nIn total, we sample 1145 training examples, 570 validation examples, and 580 test examples. We hold out items such that 79 items in the validation set are not seen in the training set, and similarly 128 items in the test set do not appear in the training set, 63 of which also do not appear in the validation set. Since these evaluation sets are extensive, we also provide smaller subsets of the validation (110 examples) and test set (117 examples) which we use in our evaluation."}, {"title": "Environment Feedback", "content": "Since all models in Plancraft can handle text, we propagate formatting errors as text observations. This feedback informs agents about specific issues, allowing agents to adjust predicted outputs accordingly. For example, if an agent attempts to move an item from one slot to another but specifies the same source and destination slots, the environment will return a message indicating that these slots must be different. Or, if an agent generates a quantity that exceeds defined limits, the system provides an error message explaining that the quantity must fall within acceptable parameters. This feedback mechanism allows agents to correct their actions and ensures smoother interactions with the environment.\nWe implement an early stopping mechanism to accelerate evaluation - if an agent fails to craft any new item after s = 10 environment steps, then it is considered stuck. We also stop the episode after a maximum number of steps n = 80 is reached (recall that all examples can be solved in < 30 steps)."}, {"title": "Impossible Tasks", "content": "Since we also wish to evaluate whether agents can predict when a task is unsolvable, we include a portion (20%) of the dataset that requires the agent to craft an item, which is impossible given the inventory. To generate this dataset, during sampling, key materials from the path are deliberately removed from the inventory. This is designed to test an agent's ability to recognise when a task cannot be completed given the available resources. As mentioned, all previous interactive environments (see Table 1) rely on the assumption that the task given to an agent is solvable. However, real-world scenarios are unlikely to meet this assumption and tasks may often be unsolvable due to missing resources or constraints.\nTo evaluate the agent's ability on impossible tasks, we introduce the impossible action, which when emitted by an agent will stop the episode. An impossible task is considered successful if and only when the agent emits impossible. For the impossible set of tasks, we report the F1 score of emitting impossible since it is now also possible for the agent to interrupt a solvable task (see Section 4.1)."}, {"title": "Expert Planner", "content": "To establish a baseline, we handcraft a planner to find an efficient sequence of actions required to complete each task. This planner serves as a standard against which the performance of various agent models can be benchmarked.\nWe implement the solver as a memoized Depth-First search over all possible crafting recipes given an inventory. If found, we return the shortest path between the initial inventory and the target object that needs to be crafted as a series of smelt and move actions. If the solver does not find a shortest path after 30 seconds, we time out the search. Note that for simplicity our planner does not smelt multiple items in a single action and does not move items directly into the crafting grid from the crafting slot [0] or from a smelt action. As a result, it is possible for shorter paths to exist if an agent is able to group multiple smelting actions into one or anticipate the location of where an item needs to be placed after crafting/smelting. The planner allows us to evaluate the length of a generated plan against a handcrafted standard. We also use the solver to set hyper-parameters, such as the maximum number of steps an agent can take to complete a task."}, {"title": "Integrating External Knowledge", "content": "Similarly to Fan et al. [2022], Plancraft includes the Minecraft Wiki as a natural language knowledge base, allowing agents to perform RAG to assist in crafting decisions. This addition enables the agents to use external knowledge, testing their ability to retrieve relevant and multi-modal information and apply it to the planning problem. We scrape the pages from the Minecraft Wiki that contain recipes and post-process the data to convert them to Markdown format.\nWe implement a search action that allows an LLM to search for a given item in the knowledge base. Since we wish to evaluate the limits of RAG, we design the search operator to return a gold-label instantiation of a recipe required to craft the particular item. Therefore, our RAG mechanism helps estimate the performance of our system under a perfect retriever and parser."}, {"title": "Method", "content": "We evaluate both the open-source Llama 3.1 8B and Llama 3.3 70B models [Dubey et al., 2024] as well as gpt-4o-mini\u00b2 [OpenAI, 2024] in our Plancraft environment. All tested models are chat models, and we format the environment observations and feedback as messages from a user role. We show the interactions between the agent and the Plancraft environment in Figure 3. We evaluate each model in the text-only and multi-modal environments, test making available different sets of actions or tools, and measure the impact of zero-shot versus few-shot versus fine-tuning. We implement the ReAct [Yao et al., 2023b] 'thought' strategy as a tool, where the agent can emit think as an action within the environment. Any generated text after think is ignored."}, {"title": "Tool use", "content": "We evaluate multiple sets of different tools outside of the core environment actions (move and smelt), we test whether including think, an oracle recipe search (RAG) and impossible has an impact on the planning capabilities of agents. If a tool is active, we incorporate a description of it within the system prompt and provide an example of its use in the few-shot examples (see Figure 3)."}, {"title": "Fine-Tuning", "content": "We fine-tune a Llama 3.1 8B model on oracle planning trajectories from the training set using LoRA [Hu et al., 2021] (r = 64,a = 32). These trajectories only consist of Observation, Action pairs directly obtained from our handcrafted planner and only include the smelt and move actions. As a result, we also test whether the fine-tuned LLM can take advantage of new actions that it has not been explicitly trained on."}, {"title": "Image Observations", "content": "We evaluate models on both text and image observations. Since the Llama models we evaluate are text-only models, we train a custom pre-trained bounding-box model (Faster R-CNN) to map from images to a list of items, quantities, and positions. We train the bounding-box model separately, sampling random inventories and their accompanying bounding boxes to obtain training data. Using the features of the bounding box, the model is trained to classify not only an object's class, but also its quantity.\nWe then map the extracted symbolic representations to the same format as our text-only observations. We use this approach to benchmark how much planning ability is lost if the observation is generated from an imperfect classifier. For gpt-40-mini, we evaluate using these extracted textual observations from the bounding-box model and also test passing image inputs directly (since gpt-40-mini supports images)."}, {"title": "Metrics", "content": "For all methods, we report the task success rate \u2013 whether or not the goal item has been crafted, the plan length \u2013 the number of environment actions the agent has taken (i.e. smelt and move), and the total number of tokens used (both input and output combined). Task success evaluates the percentage of tasks completed successfully by the agent.\nUsing the expert planner, we can compare an agent's trajectories compared to those of the expert. We calculate the average Action Efficiency (AE) as the ratio of the number of actions in an agent's plan P compared to the path of the expert planner $P_e$. Note we only consider successful plans in the calculation of this metric:\n$Action Efficiency = \\frac{\\sum_{i=1}^{N_{successful}}(\\frac{P_{e_i}}{P_i})}{N_{successful}}$\nWe measure whether knowledge, internal and external, is used through the number of think and search calls. When we evaluate the performance of models when the impossible is allowed, we also report the F1 score of emitting the action. Since predicting impossible immediately stops the episode, the impossible setup introduces a new form of failure mode where a model can emit an impossible when faced with a solvable problem.\nLastly, we track the overall compute efficiency using the total number of tokens used as a proxy. Token usage is heavily impacted by tool use since calling external tools and actions both requires the model to generate additional tokens (to call the tool) but also introduces new information as a result of the external call. However, we note that tools could hide additional compute costs not factored into simple token usage.\nUnless specified, all models are evaluated under the same conditions, with a fixed number of trials per task and the same prompts. We use a temperature of t = 0.6 and run five generations per model to report an average on each metric."}, {"title": "Results", "content": "We report results with text-only observations in Table 2 and multi-modal observations in Table 3."}, {"title": "Act and ReAct baselines", "content": "We first restrict the action space to only the environment actions, move (M) and smelt (S), and evaluate the planning capabilities of Llama 3.1 8B, Llama 3.3 70B and gpt-4o-mini in a few-shot setting. We find that gpt-4o-mini and Llama 70B perform similarly, with an overall task success rate of 0.11 and 0.16 respectively. Llama 8B performs noticeably worse with a success rate of only 0.04. In terms of plan efficiency, we find that gpt-4o-mini is closer to the expert planner (0.29) than the Llama models (0.86 and 1.09), which indicates that when the model succeeds, it does so efficiently.\nWe then extend the pool of possible actions to include think, similar to a ReAct strategy [Yao et al., 2023b]. Note that we also update the few-shot prompt appropriately to include a description of the action and an example of its use (see Figure 3). We restrict the maximum number of consecutive think actions to three; however, we find that, in practice, the model does not get stuck in a continuous thought loop. Using think, the few-shot models slightly improve in overall task success: Llama 8B 0.06(+0.02), Llama 70B 0.22(+0.06), gpt-4o-mini 0.12(+0.01). However, think also comes with added costs, as the model is now allowed unconstrained generation steps that lead to longer dialogues. We can see the direct effects of this on token usage; for instance, gpt-4o-mini increases its token usage from 8.2M to 11.6M tokens."}, {"title": "External Knowledge", "content": "When we allow search, we enable the models to incorporate the external knowledge contained in the Minecraft Wiki through the RAG oracle retriever. We find that this has substantial benefits for all three few-shot models, boosting Llama 7B to 0.30(+0.24), Llama 70B to 0.53(+0.31) and gpt-4o-mini to 0.27(+0.15) in overall success rate. As with think, search introduces additional steps and therefore we would expect a rise in average tokens used, however, we find that is not the case. Lower token usage can be partially explained by a global decrease in the use of think when search is available. Instead of speculating on how to craft a particular recipe, search allows models to directly retrieve precise information. This reduces the need for additional reasoning steps, resulting in more efficient token usage."}, {"title": "Predicting Impossible Tasks", "content": "When we include impossible as a possibility, we introduce a way for the agent to interrupt an episode. We find that this negatively affects the smaller Llama 8B model (-0.12) more than the larger Llama 70B (+0.02) and gpt-4o-mini (+0.01) in overall success rate. We also record the F1 rate of emitting impossible, and find that the best performing model, the model that can most accurately predict when a task is impossible or not, is the Llama 70B model, with an F1 score of 0.64. Adding impossible also decreases the usage of tokens, as models can now interrupt an episode when stuck. This reduces the average tokens used by more than half for some models; for example, gpt-4o-mini goes from 10.9M to 4.7M in the average tokens used."}, {"title": "Effects of Fine-tuning", "content": "As mentioned in Section 4, we also fine-tune an LLM on expert plans (Llama 8B FT). As in Wang et al. [2022a], we find that smaller models can increase their task success with only about 1k training examples. Llama 8B FT obtains an overall success rate of around 0.41, which decreases slightly with each additional action we introduce. We find that fine-tuning severely decreases the model's ability to use new actions. Llama 8B FT almost never uses think, search, and impossible, and thus is also our most efficient model in terms of plan length."}, {"title": "Image Observations", "content": "In Table 3, we use images instead of text inputs. We leverage the pre-trained bounding-box Faster R-CNN model to map images into text descriptions, and confirm a drop in accuracy for all models: Llama 8B (-0.04), Llama 70B (-0.10), and gpt-40-mini (-0.08). Because gpt-40-mini supports image inputs, we also evaluate passing images directly to the model instead of the text observations. In this case, the system instructions remain the same, but we modify the few-shot prompt to replace the observations with images. We refer to this setup as gpt-40-mini IMG and note that it is unable to solve our task (0.01) and uses 100 times as many tokens, since every image is converted into a long sequence of tokens."}, {"title": "Conclusion", "content": "We introduce Plancraft, a multi-modal dataset to evaluate the planning capabilities of LLM-based agents. Using Minecraft crafting, Plancraft allows for the assessment of both task accuracy and an agent's ability to judge task feasibility. By including solvable and intentionally unsolvable tasks, the dataset provides a controlled setting for examining agentic behaviour in step-based problem-solving contexts.\nOur results confirm that larger models, such as Llama 70B, outperform smaller models like Llama 8B in both task success and efficiency in Plancraft. This advantage is further amplified by specialised actions such as think and search. The introduction of impossible highlights a trade-off: while enabling models to recognise unsolvable tasks reduces token usage and increases efficiency, it disproportionately impacts smaller models like Llama 8B. Fine-tuning smaller models, as shown with Llama 8B FT, significantly boosts task success but also reveals a key limitation: the inability to effectively use new actions like think and search, indicating fine-tuning may overly constrain behaviour. Additionally, our findings emphasise challenges of multi-modal settings; while text-only tasks highlight strong performance trends, raw image inputs, as in gpt-40-mini IMG, underscore fundamental limitations of out-of-the-box VLMs for effective planning. Overall, our results underscore the complexity of bridging multi-modal inputs and decision-making in planning tasks."}]}