{"title": "Advancing the Understanding of Fixed Point Iterations in Deep Neural Networks: A Detailed Analytical Study", "authors": ["Yekun Ke", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "Recent empirical studies have identified fixed point iteration phenomena in deep neural networks, where the hidden state tends to stabilize after several layers, showing minimal change in subsequent layers. This observation has spurred the development of practical methodologies, such as accelerating inference by bypassing certain layers once the hidden state stabilizes, selectively fine-tuning layers to modify the iteration process, and implementing loops of specific layers to maintain fixed point iterations. Despite these advancements, the understanding of fixed point iterations remains superficial, particularly in high-dimensional spaces, due to the inadequacy of current analytical tools. In this study, we conduct a detailed analysis of fixed point iterations in a vector-valued function modeled by neural networks. We establish a sufficient condition for the existence of multiple fixed points of looped neural networks based on varying input regions. Additionally, we expand our examination to include a robust version of fixed point iterations. To demonstrate the effectiveness and insights provided by our approach, we provide case studies that looped neural networks may exist $2^d$ number of robust fixed points under exponentiation or polynomial activation functions, where $d$ is the feature dimension. Furthermore, our preliminary empirical results support our theoretical findings. Our methodology enriches the toolkit available for analyzing fixed point iterations of deep neural networks and may enhance our comprehension of neural network mechanisms.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks have achieved remarkable success and are widely employed in various applications, including ChatGPT [Ope23], face recognition [WD21], and personalized recommendation systems [DS20], among others. These networks typically consist of numerous hidden layers; for instance, Residual Networks (ResNets) [HZRS16] can contain over 1,000 layers.\nRecent empirical studies reveal that, despite the numerous layers in deep neural networks, certain operational phases exist where adjacent layers may perform identical operations [MBAB22, SMN+24]. Consequently, we can focus on modifying specific associated layers during inference to enhance performance. Furthermore, the hidden states tend to stabilize after several adjacent layers, resulting in minimal changes in subsequent layers, allowing us to skip certain layers during inference [ESL+24]. Additional research indicates that a looped transformer\u2014where its output is fed back into itself iteratively\u2014exhibits expressive capabilities comparable to programmable computers [GRS+23] and is more effective at learning algorithms [YLNP23]. Other studies have also examined the convergence of deep neural networks when the weights across different layers are nearly identical, with only minor perturbations [XZ22, XZ24]. Collectively, these findings suggest the relevance of fixed-point iteration (Definition 3.1). Employing fixed-point methods in deep or looped neural networks may offer several advantages, such as reducing the number of parameters and dynamically adjusting runtime based on the complexity of the problem.\nDespite these advancements, our understanding of fixed-point iterations remains limited, especially in high-dimensional spaces, due to the limitations of existing analytical tools. It remains challenging to determine when a neural network can effectively approximate a fixed-point solution and how many layers or iterations are required to ensure a good outcome.\nThus, it is natural to ask the following question:\nHow shall we analytically study fixed point iterations in deep neural networks?\nIn this study, we conduct a detailed analysis of fixed point iterations in a vector-valued function, $R^d \\to R^d$ where $d$ is the hidden feature size, modeled by neural networks. We establish a general theorem (Theorem 4.1) to describe a sufficient condition for the existence of multiple fixed points of looped neural networks (Definition 3.5) based on varying input regions. Then, we expand our examination by introducing noise during each fixed point iteration and show that the fixed point iteration process is robust under noise (Theorem 4.2). It represents deep neural networks with residue connection [HZRS16], where after each layer, the hidden states are only perturbed slightly. Finally, we demonstrate the effectiveness of our approach by studying looped neural networks under polynomial and exponential activation functions (Theorem 5.1 and Theorem 5.2). We show that the looped neural networks may exist $2^d$ number of robust fixed points. Recall that the previous tools can only handle single fixed point analysis, while our analysis can be applied to more practical cases. Furthermore, our preliminary empirical results support our theoretical findings (Section 6). Our methodology enriches the toolkit available for analyzing fixed point iterations of vector-valued functions and may help us better understand neural network mechanisms.\nOur contributions:\n\u2022 We study the fixed point iteration in looped neural networks and provide a general theorem (Theorem 4.1) to describe a sufficient condition for the existence of multiple fixed points.\n\u2022 We also establish a robust version of fixed point iterations with noise perturbation (Theorem 4.2)."}, {"title": "2 RELATED WORK", "content": "In Section 2.1, we introduce fixed point theory with a focus on the Banach fixed point theorem. In Section 2.2, we present some work on incorporating looped structures into neural networks. In Section 2.3, we introduce some works that utilize the properties of fixed point iterations in neural network computations."}, {"title": "2.1 Fixed Point Iteration Methods", "content": "In numerical analysis, fixed point iteration methods [AMO01, Ist01, GD03, KSR15] use the concept of fixed points to compute the solution of a given equation in a repetitive manner. Many works have focused on the convergence properties of fixed point iteration methods. For example, Banach fixed point theorem [AH09] gives a sufficient condition under which a unique fixed point exists and is approachable via iterative methods. Although there are other fixed-point theorems, Banach fixed point theorem, in particular, is useful because it provides a clear criterion for fixed points using contraction mappings. If a function is a contraction, the theorem guarantees the existence and uniqueness of a fixed point, making it easier to work with than other fixed-point theorems that may have more complex conditions.\nRecent works focus on employing various methods to accelerate the convergence of fixed-point iterations. For instance, [ZAL11] proposed a Quasi-Newton method for accelerating fixed-point iterations by approximating the Jacobian in Newton's method, enabling efficient root-finding for the function $g(x) = x - f(x)$. [WN11] presents Anderson acceleration, an underutilized method for enhancing fixed-point iterations. After this work, [ZOB20] introduces a globally convergent variant of type-I Anderson acceleration for non-smooth fixed-point problems, improving terminal convergence of first-order algorithms."}, {"title": "2.2 Looped Neural Networks", "content": "Looped Neural Networks are a paradigm in deep learning that aims to address certain limitations of traditional feedforward architectures. The addition of loopy structures to traditional neural networks has already received extensive research. For example, [CSW16] introduces looped Convolutional Neural Network which unrolls over multiple time steps and demonstrate that these networks outperform deep feedforward networks on some image datasets.\nTransformers [VSP+17, CSY23, LSS+24b] have become the preferred model of choice in natural language processing (NLP) and other domains that require sequence-to-sequence modeling. To understand why Transformers excel at iterative inference while lacking an iterative structure, [GRS+23] proposes a Looped Transformer and has shown that transformer networks can be used"}, {"title": "2.3 Neural Networks as Fixed Point Iterations", "content": "To better understand the convergence property and stability of neural networks, many works investigate neural networks as fixed-point iteration processes. The research in this area can be traced back to [HO97]. This work demonstrates how a neural network learning rule can be converted into a fixed-point iteration, resulting in a simple, parameter-free algorithm that converges quickly to the optimal solution allowed by the data. Recently, many researchers have found that the hidden layers of many deep networks converge to a fixed point (a stable state). Based on this, treating neural networks as fixed points has received extensive research. For instance, [BKK19] proposes Deep Equilibrium Model (DEQ), a novel approach to modeling sequential data by directly solving for fixed points. Instead of gradually approximating this fixed point through iterations, DEQ uses a black-box root-finding method to solve it directly, bypassing intermediate steps. [YLNP23] proposes a new training methodology for looped transformers to effectively emulate iterative algorithms, optimizing convergence by adjusting input injection and loop iterations while using significantly fewer parameters than standard transformers. It demonstrates that the looped Transformer is more effective at learning learning algorithms such as in-context learning. There are many other works used fixed point iterations in deep learning [JLC21, HGM+21, C\u041c\u041a15].\nWe refer the readers to some other related works [QGT+19, RSZ22, GSX23, RHX+23, HWL+24, DSX23, DSXY23, QSZZ23, QSY23, AS24b, AS24a, WHL+24, ASZZ24, LSSY24, LLSS24, SYZ24, SZZ24]"}, {"title": "3 PRELIMINARY", "content": "In this section, we introduce some definitions that will be used throughout the paper in Section 3.1. Then we briefly review the fixed point method and Banach fixed-point theorem in Section 3.2. Finally, we define the looped neural network in Section 3.3, which is the primary focus of our work."}, {"title": "3.1 Notations", "content": "For a vector $v \\in R^d$, we use $||v||_1$, $||v||_2$, and $||v||_\\infty$ to denote the $l_1$-norm, $l_2$-norm, and $l_\\infty$-norm of $v$, respectively. For two vector $u, v \\in R^d$, we use $\\langle u, v \\rangle$ to denote the standard inner product of $u$ and $v$. We use $1_d$ to denote a vector whose elements are all 1."}, {"title": "3.2 Fixed Point Methods", "content": "In this section, we introduce the concept of the fixed-point method and the well-known Banach fixed-point theorem. Here, we only deal with the case where the space is $R^d$. In Appendix A, we state the original definitions and theorems in the context of Banach spaces.\nWe first introduce the fixed point iteration problem."}, {"title": "Definition 3.1 (Fixed point, [AH09]).", "content": "Let $D$ be a subset of $R^d$. We say a function $f : D \\to R^d$ has a fixed point $p \\in D$ if $f(p) = p$.\nThen, we introduce contractive mapping, which is a key concept in fixed point iteration convergence."}, {"title": "Definition 3.2 (Contractive mapping, Definition 5.1.2 of [AH09]).", "content": "Let $||\\cdot||$ be a norm on $R^d$. Let $D$ be a subset of $R^d$. We say that a function $f : D \\to R^d$ is contractive with contractivity constant $K \\in [0, 1)$ if\n$|| f(x) - f(x') || \\le K ||x - x' ||, \\forall x, x' \\in V.$\nContractive mapping means that the function output space becomes 'smaller' than its input space. Then, we are able to introduce the Banach fixed-point theorem, the key tool in this work."}, {"title": "Lemma 3.3 (Banach fixed-point theorem, Theorem 5.1.3 of [AH09]).", "content": "Let $||\\cdot||$ be a norm on $R^d$. Let $D$ be a nonempty closed set of $R^d$. Suppose that $f : D \\to R^d$ is a mapping that satisfies the following\n\u2022 $f(x) \\in D$ whenever $x \\in D$.\n\u2022 $f$ is contractive with contractivity constant $K \\in [0, 1)$.\nThen it holds that\n\u2022 The function $f$ has a unique fixed point $p \\in D$.\n\u2022 For any initial point $x^{(0)} \\in D$, the fixed-point iteration $x^{(t)} = f(x^{(t-1)}), t > 1$, converges to the fixed point $p$ as $t \\to \\infty$.\n\u2022 The following error bounds hold:\n$||x^{(t)} - p|| \\le \\frac{K^t}{1 - K} ||x^{(1)} - x^{(0)}||,$\n$||x^{(t)} - p|| \\le \\frac{K}{1 - K} ||x^{(t)} - x^{(t-1)}||,$\n$||x^{(t)} - p|| \\le K ||x^{(t-1)} - p||.$\nLemma 3.3 told us the sufficient condition for a single unique fixed point. Although checking for contractivity is usually difficult, for differentiable functions, it becomes easier by examining the Jacobian matrix. The following lemma is a corollary of Lemma 3.3."}, {"title": "Lemma 3.4 (Banach fixed point theorem, vector case, informal version of Lemma A.5).", "content": "Let $D \\subseteq R^d$ be a nonempty closed set. Suppose that $f : D \\to R^d$ is differentiable and satisfies the following:\n\u2022 $f(x) \\in D$ whenever $x \\in D$.\n\u2022 There exists constant $K < 1$ such that for every $i \\in [d]$,\n$|| \\frac{df_i(x)}{dx} ||_1 \\le K, \\forall x \\in D$\nwhere $f_i(x)$ is the $i$-th entry of $f(x)$.\nThen it holds that\n\u2022 The function $f$ has a unique fixed point $p \\in D$.\n\u2022 For any initial point $x^{(0)} \\in D$, the fixed point iteration $x^{(t)} = f(x^{(t-1)}), t \\ge 1$, converges to the fixed point $p$ as $t \\to \\infty$.\n\u2022 The following error bounds hold:\n$||x^{(t)} - p||_\\infty \\le \\frac{K^t}{1 - K} ||x^{(1)} - x^{(0)}||_\\infty,$\n$||x^{(t)} - p||_\\infty \\le \\frac{K}{1 - K} ||x^{(t)} - x^{(t-1)}||_\\infty,$\n$||x^{(t)} - p||_\\infty \\le K ||x^{(t-1)} - p||_\\infty$.\nWhen $d = 1$, Lemma 3.4 boils down to the scalar case."}, {"title": "3.3 Looped Neural Networks", "content": "In this section, we give the formal definition of Looped Neural Network."}, {"title": "Definition 3.5 (Looped Neural Network).", "content": "Let $W = [W_1, ..., W_d] \\in R^{d \\times d}$ be a weight matrix and $b \\in R^d$ be the bias parameter. Let $g : R \\to R$ be a differentiable activation function. We consider the one layer of neural network in the form\n$f(x; W, b) := g(Wx + b)$\nwhere $g$ is applied entry-wise. The $L$-layer looped neural network is defined as\n$NN(x^{(0)}; W, b, L) := x^{(L)}$\n$x^{(t)} := f(x^{(t-1)}; W, b), \\forall t \\in [L].$\nRemark 3.6. Note that the Looped Neural Network (LNN) shares similarities with Recurrent Neural Networks (RNN) but is slightly different. RNN maps sequence to sequence, which means in each loop, RNN has new input data, while LNN does not. Furthermore, RNN generates output in each loop while LNN only outputs after all loops."}, {"title": "4 MAIN RESULTS", "content": "We first introduce our general theorem for looped neural networks in Section 4.1 and then introduce our robust version in Section 4.2."}, {"title": "4.1 General Theorem", "content": "We have the following general theorem which provides a sufficient condition for the existence of multiple fixed points for Looped Neural Network."}, {"title": "Theorem 4.1 (General result).", "content": "Consider the $L$-layer looped neural network $NN(x^{(0)}; W, b, L)$ defined in Definition 3.5. If the following conditions hold:\n\u2022 There exists disjoint $D_1, ..., D_m \\subseteq R^d$ such that for every $i \\in [m]$, and for every $x \\in D_i$, $NN(x; W, b, 1) \\in D_i$.\n\u2022 The weight matrix $W$ and the activation function $g$ satisfy: For every $i \\in [m]$, there exists $K_i \\in [0, 1)$ such that for every $x \\in D_i$, and for every $j \\in [d]$,\n$|g'(\\langle w_j, x \\rangle)| \\cdot ||w_j||_1 \\le K_i.$\nThen, the following statements hold:\n\u2022 The single layer of the looped neural network, $f(x; W)$, has at least $m$ fixed-points $p_1, ..., p_m$ satisfying : For every $i \\in [m]$, there exists a constant $\\epsilon_i > 0$, for any initial point $x^{(0)} \\in D_i$ with $||x^{(0)} - p_i||_\\infty \\le \\epsilon_i$, we have\n$\\lim_{L \\to \\infty} NN(x^{(0)}; W, b, L) = p_i$\n\u2022 For every $i \\in [m]$, there exists a constant $c_i > 0$ such that for any $L \\ge 2$,\n$||NN(x^{(0)}; W, b, L) - p_i||_\\infty < K_i^{L} \\cdot c_i \\epsilon_i.$\nProof. Assume the conditions in the statement hold. Fix $i \\in [m]$. Then we have $f(x; W) = NN(x; W, b, 1) \\in D_i$ for every $x \\in D_i$. Next, for every $j \\in [d]$, and for the $j$-th entry of $f(x; W)$, where we denoted by $f_j(x; W)$, we have\n$|| \\frac{df_j(x; W, b)}{dx} ||_1 = || \\frac{dg(\\langle w_j, x \\rangle)}{dx} \\cdot w_j ||_1 = ||g'(\\langle w_j, x \\rangle) \\cdot w_j ||_1 = |g'(\\langle w_j, x \\rangle)| \\cdot ||w_j||_1 \\le K_i,$\nwhere the first step follows from the definition of $f_j(x; W)$, the second step uses the chain rule, the third step is due to the property of norm, and the last step uses the condition in lemma.\nTherefore, we can apply Lemma 3.3, for each $i \\in [m]$, there exists a fixed point $p_i$ which can be converged to by the fixed point iteration, and thus it can be found by the looped neural network with initial point $x^{(0)}$ when the number of layers goes to infinity. Next, for every $i \\in [m]$, and for an initial point $x^{(0)} \\in D_i$, there exists $\\epsilon_i := \\sup_{y, z \\in D_i} ||y - z||_\\infty$, $c_i := \\frac{1}{1 - K_i}$ such that we have\n$||NN(x^{(0)}; W, b, L) - p_i||_\\infty = ||x^{(L)} - p_i||_\\infty$"}, {"title": "Theorem 4.1 gives us a way how to find different fixed points of vector-valued functions such as looped neural networks.", "content": "Note that for different inputs, the fix point iteration behavior may be different, even when the model weights are fixed. We refer readers to Figure 1 and Figure 2 for more intuition."}, {"title": "4.2 Perturbed Fixed Point Iteration", "content": "Next, we consider a variant of fixed point method, where each iteration there is noise term $h(x)."}, {"title": "Theorem 4.2 (Robust Banach fixed point theorem, scalar case).", "content": "Let $D \\subseteq R$ be a nonempty closed set. Suppose that $f : D \\to R$ is differentiable and satisfies the following:\n\u2022 $f(x) \\in D$ whenever $x \\in D$.\n\u2022 There exists constant $K \\in [0, 0.95]$ such that\n$|f'(x)| \\le K, \\forall x \\in D$.\n\u2022 For any initial point $x^{(0)} \\in D$, consider the perturbed fixed point iteration $x^{(t)} = f(x^{(t-1)}) + h(x^{(t-1)})$, for any $t > 1$, where the function $h$ satisfies $|h(x)| \\le 1/m$ for every $x \\in D$ for some sufficiently large $m > 0$.\nThen it holds that\n\u2022 The function $f$ has a unique fixed point $p \\in D$.\n\u2022 The following error bounds hold:\n$|x^{(t)} - p| < K |x^{(t-1)} - p| + \\frac{1}{m},$\n$|x^{(t)} - p| \\le K^t |x^{(0)} - p| + \\frac{20}{m}.$\nProof. Clearly $f$ has a fixed point $p \\in D$ by Lemma A.4.\nWe first show the first error bound. We can show that\n$|x^{(t)} - p| = |f(x^{(t-1)}) - p + h(x^{(t-1)})| = |f(x^{(t-1)}) - f(p) + h(x^{(t-1)})| = |f'(c) \\cdot (x^{(t-1)} - p) + h(x^{(t-1)})| \\le |f'(c) \\cdot (x^{(t-1)} - p)| + |h(x^{(t-1)})| = |f'(c)| \\cdot |x^{(t-1)} - p| + |h(x^{(t-1)})| < K |x^{(t-1)} - p| + \\frac{1}{m}$\nwhere the first step uses the perturbed fixed point iteration $x^{(t)} = f(x^{(t-1)}) + h(x^{(t-1)})$, the second step is due to the fact that $p$ is a fixed point of $f$, the third step follows from the mean value theorem where $c$ is a point between $x^{(t-1)}$ and $p$, the fourth step uses the triangle inequality, the fifth step follows from basic algebra, and the last step follows from $|f'(x)| < K$ and $|h(x)| \\le 1/m$.\nNext, we show the second error bound. We can show that\n$|x^{(t)} - p| < K |x^{(t-1)} - p| + \\frac{1}{m} < K(K |x^{(t-2)} - p| + \\frac{1}{m}) + \\frac{1}{m} < ... < K^t |x^{(0)} - p| + \\sum_{i=1}^{t} \\frac{K^i}{m} = K^t |x^{(0)} - p| + \\frac{1 - K^{t-1}}{(1 - K)m} < K^t |x^{(0)} - p| + \\frac{1}{(1 - K)m} = K^t |x^{(0)} - p| + \\frac{20}{m},$\nwhere the first four steps follow from recursively using the third error bound, the fifth step is due to the sum of geometric series, and the last step follows from $K \\in [0, 0.95]$.\nTheorem 4.2 shows that each fix iteration process is robust and may not be hurt by the noise much. It corresponds to the setting that deep neural networks with residual connections [HZRS16], where after each layer the hidden states are only slightly perturbed. Our experiments in Section 6 support our theoretical analysis of robustness."}, {"title": "5 CASE STUDY", "content": "In this section, we provide case studies of the Looped Neural Networks with different activation functions. We show that the Looped Neural Networks may have $2^d$ number of different fixed points. First, we consider the polynomial activation function. We have the following robust fixed points statement."}, {"title": "Theorem 5.1 (A specific polynomial activation function with small perturbation).", "content": "Consider the $L$-layer looped neural networks $NN(x^{(0)}; W, b, L)$ defined in Definition 3.5. If the following conditions hold:\n\u2022 Let $C:= \\frac{15 + \\sqrt{65}}{8}$ be a constant.\n\u2022 Let $g(x) := -\\frac{2}{5}x^4 + \\frac{8}{5}Cx^3 + (\\frac{3}{2} - \\frac{12}{5} C^2)x^2 + (\\frac{8}{5} C^3 - 3C)x + 1$ denote the polynomial activation function used in this looped neural networks.\nThen there exists a set of noisy parameters $W$ and $b$ such that $x^{(t)} = NN(x^{(t-1)}; W, b, L) + h(x^{(t-1)}) \\in R^d$, for any $t \\in [L]$, where the function $h$ satisfies $|h(x)| < 1/m$ for every $x \\in R^d$ for some sufficiently large $m > d$. For this looped neural network, the following statements hold:\n\u2022 The single layer of the looped neural network, $f(x; W, b)$, has at least $2^d$ robust fixed-points $p_1, ..., p_{2^d}$ satisfying: For every $i \\in [2^d]$, there exists a vector $\\epsilon_i \\in R^d$, for any initial point $x^{(0)}$ with $||x^{(0)} - p_i||_\\infty \\le ||\\epsilon_i||_\\infty$, we have\n$\\lim_{L \\to \\infty} NN(x^{(0)}; W, b, L) = p_i$\n\u2022 For every $i \\in [2^d]$, there exists a constant $c_i > 0$ and a constant $K_i \\in [0, 0.9)$ such that for any $L \\ge 2$, we have\n$||NN(x^{(0)}; W, b, L) - p_i||_\\infty \\le K_i^L \\cdot ||\\epsilon_i||_\\infty + \\frac{20}{m}$\nProof. Let $f(x; W, b)$ be a single layer version of $NN(x; W, b, 1)$. Let $m > d$ be a sufficiently large constant. Let $W = 1_d \\cdot 1_d^\\top + (-\\frac{24}{5} + 1) diag(1_d) \\in R^{d \\times d}$ denote the weight matrix. Let $b = [C, ..., C]^T \\in R^d$ denote the bias vector. Then it's clear that for each $j \\in [d]$, we have\n$g(\\langle w_j, x \\rangle + b_j) = -\\frac{2}{5}x_j^4 + \\frac{8}{5}Cx_j^3 + (\\frac{3}{2} - \\frac{12}{5} C^2)x_j^2 + (\\frac{8}{5} C^3 - 3C)x_j + 1 + \\frac{1}{m^2}h_j(x) = f(x; W, b) + \\frac{1}{m^2}h_j(x)$\nwhere $h_j(x)$ is a polynomial in $x_1, ..., x_d$. For every $j \\in [d]$, there exists a $D_1 = (1.302, 1.502)$ and $D_2 = (-0.3, 0.3)$, it satisfies that $f_j(x; W, b) \\in D_1$ when $x_j \\in D_1$ and $f_j(x; W, b) \\in D_2$ when $x_j \\in D_2$. By the proof in Lemma B.1, there exists a $K_j = 0.92$ such that for any $x_j \\in D_1 \\cup D_2$ we have\n$|f(x; W, b)| \\le K_j.$\nIt's obvious that when $m$ is sufficiently large and $||\\epsilon_i||_\\infty$ is small enough, we will have $|\\frac{1}{m^2}h_j(x)| \\le \\frac{1}{m}$, where we can see $h_j(x)$ as $h(x)$ in statement of Theorem 4.2. Then by combing the result of"}, {"title": "Theorem 4.2, we have that for each dimension $j \\in [d]$, we have 2 robust fixed points.", "content": "So we can get $f(x; W, b)$ has $2^d$ Robust Fixed Points trivially. And for any $i \\in [2^d], j \\in [d]$, we have\n$|NN_j(x^{(t)}; W, b, L) - p_{i,j}| < K_j |NN_j(x^{(0)}; W, b, L) - p_{i,j}| + \\frac{20}{m} < K_j |\\epsilon_{i,j}| + \\frac{20}{m}$\nwhere the first step comes from the result of Theorem 4.2, the second step comes from the range of values for the initial point. Then use the definition of $|| \\cdot ||_\\infty$, we have\n$||NN(x^{(t)}; W, b, L) - p_i||_\\infty \\le K_i^t \\cdot ||\\epsilon_i||_\\infty + \\frac{20}{m}$\nThen we complete the proof.\nIn Theorem 5.1, we can see that our Looped Neural Network has $2^d$ different robust fixed points solutions. Recall that the previous analysis tools can only handle single fixed point analysis.\nWe can show the existence of $2^d$ different robust fixed points solutions when the Looped Neural Network uses exponential activation as well."}, {"title": "Theorem 5.2 (A specific exponential activation function with small perturbation).", "content": "Consider the $L$-layer looped neural networks $NN(x^{(0)}; W, b, L)$ defined in Definition 3.5. If the following conditions hold:\n\u2022 Let $C:= -2.15$ be a constant.\n\u2022 Let $g(x) := exp(x^3 + (-2 - 3C)x^2 + (3C^2 + 4C)x + ln 2) - 1$ denote the exponential activation function used in this looped neural networks.\nThen there exists a set of noisy parameters $W$ and $b$ such that $x^{(t)} = NN(x^{(t-1)}; W, b, L) + h(x^{(t-1)}) \\in R^d$, for any $t \\in [L]$, where the function $h$ satisfies $|h(x)| \\le 1/m$ for every $x \\in R^d$ for some sufficiently large $m > d$. For this looped neural network, the following statements hold:\n\u2022 The single layer of the looped neural network, $f(x; W, b)$, has at least $2^d$ robust fixed-points $p_1, ..., p_{2^d}$ satisfying: For every $i \\in [2^d]$, there exists a vector $\\epsilon_i \\in R^d$, for any initial point $x^{(0)}$ with $||x^{(0)} - p_i||_\\infty \\le ||\\epsilon_i||_\\infty$, we have\n$\\lim_{L \\to \\infty} NN(x^{(0)}; W, b, L) = p_i$\n\u2022 For every $i \\in [2^d]$, there exists a constant $c_i > 0$ and a constant $K_i \\in [0, 0.9)$ such that for any $L \\ge 2$, we have\n$||NN(x^{(0)}; W, b, L) - p_i||_\\infty \\le K_i^L \\cdot ||\\epsilon_i||_\\infty + \\frac{20}{m}$\nProof. Let $f(x; W, b)$ be a single layer version of $NN(x; W, b, 1)$. Let $m > d$ be a sufficiently large constant. Let $W = 1_d \\cdot 1_d^\\top + (-\\frac{24}{5} + 1) diag(1_d) \\in R^{d \\times d}$ denote the weight matrix. Let $b = [C, ..., C]^T \\in R^d$ denote the bias vector. Then it's clear that for each $j \\in [d]$, we have\n$g(\\langle w_j, x \\rangle + b_j) = exp(x_j^3 - 2x_j^2) - 1 + \\frac{1}{m^2}h_j(x)$"}, {"title": "Theorem 4.2, we have that for each dimension $j \\in [d]$, we have 2 robust fixed points.", "content": "So we can get $f(x; W, b)$ has $2^d$ Robust Fixed Points trivially. And for any $i \\in [2^d", "d": "we have\n$|NN_j(x^{(t)}; W, b, L) - p_{i,j}| < K_j |NN_j(x^{(0)}; W, b, L) - p_{i,j}| + \\frac{20}{m} < K_j |\\epsilon_{i,j}| + \\frac{20}{m}$\nwhere the first step comes from the result of Theorem 4.2, the second step comes from the range of values for the initial point. Then use the definition of $|| \\cdot ||_\\infty$, we have\n$||NN(x^{(t"}]}