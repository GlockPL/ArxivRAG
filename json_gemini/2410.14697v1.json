{"title": "Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios", "authors": ["Shihan Ma", "Bo Hu", "Tianyu Jia", "Alexander Kenneth Clarke", "Blanka Zicher", "Arnault H. Caillet", "Dario Farina", "Jos\u00e9 C. Pr\u00edncipe"], "abstract": "The cortico-spinal neural pathway is fundamental for motor control and movement execution, and in humans it is typically studied using concurrent electroencephalography (EEG) and electromyography (EMG) recordings. However, current approaches for capturing high-level and contextual connectivity between these recordings have important limitations. Here, we present a novel application of statistical dependence estimators based on orthonormal decomposition of density ratios to model the relationship between cortical and muscle oscillations. Our method extends from traditional scalar-valued measures by learning eigenvalues, eigenfunctions, and projection spaces of density ratios from realizations of the signal, addressing the interpretability, scalability, and local temporal dependence of cortico-muscular connectivity. We experimentally demonstrate that eigenfunctions learned from cortico-muscular connectivity can accurately classify movements and subjects. Moreover, they reveal channel and temporal dependencies that confirm the activation of specific EEG channels during movement.", "sections": [{"title": "Introduction", "content": "The brain communicates with muscles by sending information to the spinal cord. Part of this information is directly transmitted from the cortex to spinal motor neurons via the cortico-spinal neural pathway, which is vital for motor control and movement execution. Because motor neurons are directly connected to muscles, cortical oscillations travelling through the cortico-spinal pathway are coherent with oscillations in muscle electrical activities, both in humans and non-human primates [1-3]. This relationship, known as functional cortico-muscular connectivity, is critical in neuroscience and is typically studied using concurrent recordings of neural signals such as electroencephalography (EEG) and electromyography (EMG) [4, 5] (Fig. 1).\nPractical applications include diagnosing and monitoring of neuromuscular disorders, such as amyotrophic lateral sclerosis [6], stroke [7], and Parkinson's disease [8], as well as developing brain-computer interfaces (BCIs) for individuals with motor impairments [9].\nDespite several applications, there is still a lack of proper statistical tools to model the relationship between EEG and EMG. The predominant method, Cortico-Muscular Coherence (CMC), measures"}, {"title": "Methods", "content": ""}, {"title": "Density ratio decomposition for EEG-EMG signal pairs", "content": ""}, {"title": "Problem formulation.", "content": "Consider EEG signals X := X1:T and EMG signals Y := Y1:T. Denote z as the subject, c as the type of movement, and u as other auxiliary contextual factors. These are factors that could potentially affect the statistical dependence between EEG and EMG signals. Each signal is conditioned on these parameters. Denote these factors as z := {s, c, u} with distribution P(z). Distributions for EEG and EMG given these conditions are p(X = X|z) and p(Y = Y|z), respectively. Their joint distribution is given by p(X,Y) = \u222b p(X|z)p(Y|z)p(z)dz. Similarly, the marginal distributions are given by p(X) = \u222b p(X|z)p(z)dz, and likewise p(Y) = \u222b p(Y|z)p(z)dz."}, {"title": "Lemma 1.", "content": "Assuming conditional independence given z := {s, c, u}, we have \\(p(X,Y|z) = \\frac{p(X|z)p(Y|z)}{p(X)p(z)}\\). Hence, the ratio \\(\\frac{p(X,Y)}{p(X)p(Y)} := \\frac{p(X,Y)}{p(X|z)p(Y|z)p(z)dz}\\) decomposes as \\(\\rho(X,Y) = \\frac{p(z)px(X, z)py(Y, z)}{p(X)p(Y)p(z)dz}\\). Assuming z is discrete (e.g., movement patterns c and participant identities s), the information of z is contained in the span of the basis functions for the density ratio p(X,Y)."}, {"title": "Proof.", "content": "Define the ratios \\(px(X, z) = \\frac{p(X,z)}{p(X)p(z)}\\) and \\(py(Y, z) = \\frac{p(Y,z)}{p(Y)p(z)}\\). Considering the sample space Z, the sets \\(p_x(X, z)\\) and \\(p_y(Y, z)\\) for z \\(\\in\\) Z are discrete. Under the conditional independence assumption, these ratios satisfy \\(p(X,Y) = \\sum_{z\\in Z} p(z)p_x(X, z)p_y(Y, z)\\).\nThis indicates that the sets \\(p_x(X, z)\\) and \\(p_y(Y, z)\\) decompose p(X, Y), similar to the eigenfunctions \\(\\Phi_k\\) and \\(\\psi_k\\). Since both decompositions represent p(X, Y), the functions \\(p_x(X, z)\\) and \\(p_y(Y, z)\\) must lie within the span of these basis functions. Hence, there exist coefficients \\(\\alpha_{z,k}\\) and \\(\\beta_{z,k}\\) such that \\(p_x(X, z) = \\sum_k \\alpha_{z,k}\\phi_k(X)\\) and \\(p_y(Y, z) = \\sum_k \\beta_{z,k}\\psi_k(Y)\\) for each z. Thus, learning the"}, {"title": "FMCA-T: Learning decomposition for the matrix trace", "content": "When probability densities are unavailable, we approximate eigenvalues and eigenfunctions using a learning system with two neural networks and a cost function, typically a matrix cost like log det or nuclear norm [19-21]. These costs optimize an aggregation of eigenvalues. The networks learn the dominant eigenvalues and eigenfunctions when optimized."}, {"title": "Aggregation of eigenvalues.", "content": "To measure the total power of the eigenspectrum, define a scalar-valued measure using a convex function \\(\\pounds : R \\rightarrow R\\) with \\(\\pounds(0) = 0\\). Assume the eigenvalues are ranked \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots\\). The truncated total statistical dependence measure of the top K eigenvalues is defined by \\(T_{\\pounds} := \\sum_{k=1}^K \\pounds(\\sigma_k)\\). Function \\(\\pounds(x) = -\\log(1 - x)\\) corresponds to the log det cost."}, {"title": "Prior work: log-determinant cost.", "content": "Consider two networks, \\(f_{\\theta} : \\mathcal{X} \\rightarrow \\mathbb{R}^K\\) and \\(g_{\\omega} : \\mathcal{Y} \\rightarrow \\mathbb{R}^K\\), mapping realizations of X and Y to K-dimensional outputs, respectively. Assume \\(f_{\\theta}\\) is for EEG and \\(g_{\\omega}\\) for EMG. The autocorrelation (ACFs) and cross-correlation functions (CCFs) are defined as:\n\n\n\n\n\\begin{equation}\nR_F = E_x[f_\\theta(X)f_\\theta(X)^T], \\quad R_G = E_y[g_\\omega(Y)g_\\omega(Y)^T], \\quad P_{FG} = E_{x,y}[f_\\theta(X)g_\\omega(Y)^T], \\quad R_{FG} = R_F^{-1} P_{FG} R_G^{-1}.\\\\\n\\end{equation}\nFMCA minimizes a log det cost, which reaches the negative value of the total measure \\(T_{\\pounds}\\) of \\(\\pounds(x) = -\\log(1 - x)\\) when minimized. The cost is defined by:\n\n\n\n\n\\begin{equation}\n\\min_{\\theta,\\omega} r_1(\\theta,\\omega) = \\log \\det R_{FG} - \\log \\det R_F - \\log \\det R_G, \\quad r_1 = - \\sum_{k=1}^K \\log(1 - \\sigma_k).\\\\\n\\end{equation}"}, {"title": "Normalization trick.", "content": "After training, normalizations are needed to obtain eigenfunctions. The first step is to ensure orthonormality: \\(f_{\\theta} = R_F^{-\\frac{1}{2}} f_{\\theta}, g_{\\omega} = R_G^{-\\frac{1}{2}} g_{\\omega}\\). The second step is a singular value decomposition: \\(P_{FG} = E[f_{\\theta}(X)g_{\\omega}(X)] = U S V\\), where \\(S = \\text{diag}(\\lambda_1, \\ldots, \\lambda_K)\\). The third step is to normalize functions such that they are invariant to the linear operator: \\(f_{\\theta} = U f_{\\theta}, g_{\\omega} = V^T g_{\\omega}\\). Functions \\(f_{\\theta}, g_{\\omega}\\) are the top eigenfunctions of the density ratio, and \\(\\lambda_1, \\lambda_2, \\ldots\\) are the top eigenvalues. An approximation of the density ratio is given by \\(\\rho = f_{\\theta}^T S^{\\frac{1}{2}} g_{\\omega} \\approx \\rho\\)."}, {"title": "Newly proposed: matrix trace cost.", "content": "This paper explores alternative convex functions, specifically the simplest case \\(\\pounds(x) = x\\), The cost, in the form of a matrix trace, is described below."}, {"title": "Lemma 2.", "content": "Denote \\(P := P_{FG}\\). Given neural nets \\(f_\\theta\\) and \\(g_\\omega\\), minimizing the matrix trace\n\n\n\\begin{equation}\n\\min_{\\theta,\\omega} r_T(\\theta,\\omega) = -\\text{Trace}\\left(R_F^{-1}PR_G^{-1}P^T\\right)\\label{eq:matrix_cost}\n\\end{equation}\nyields \\(r_T(\\theta,\\omega) = - \\sum_{k=1}^K \\sigma_k\\), where \\(r^*_{\\theta,\\omega}\\) is the optimal cost, reaching the sum of the top K eigenvalues of the density ratio when minimized. We name this algorithm FMCA-T."}, {"title": "Proof.", "content": "Applying the Schur complement to \\(r_1\\), we obtain \\(r_1 = \\log \\det(I - R_F^{-1}PR_G^{-1}P^T)\\).\nDenoting eigenvalues of a matrix as \\(\\lambda_1(\\cdot), \\ldots, \\lambda_K(\\cdot)\\), the cost becomes \\(r_1 = \\sum_{k} \\log(1 - \\lambda_k(M))\\),\nwhere \\(M = R_F^{-1}PR_G^{-1}P^T\\). Optimizing the sum of eigenvalues instead, we use Trace(M) and, based on the trace property Trace(AB) = Trace(BA), derive the trace cost for learning multivariate statistical dependence as \\(r_T = -\\text{Trace}(R_F^{-1}PR_G^{-1}P^T)\\).\nFMCA-T is more computationally efficient as it uses only matrix operations of dimension K. Directly optimizing the sum of the eigenvalues is also more stable than optimizing their logarithm."}, {"title": "Channel-level and temporal-level dependencies", "content": ""}, {"title": "Motivations.", "content": "For EEG X1:T and EMG Y 1:T, FMCA-T applies two networks, \\(f_{\\theta}\\) and \\(g_{\\omega}\\), to minimize the matrix trace cost. Dependence is measured at two levels: random-process level, measured by eigenvalues for the overall dataset dependence, and trial level, measured by the density ratio - the higher the ratio, the greater the contribution of this pair of realizations to overall dependence. In cortico-muscular analysis, it is vital to understand how individual channels and time steps contribute to connectivity, especially in EEG signals, as they represent the temporal and spatial dynamics of brain. Hence, we propose localized density ratios to measure temporal-level dependence and channel-level dependence. The core idea is computing density ratios between channel-level and temporal-level features against the global trial-level features."}, {"title": "Channel-level features.", "content": "We design a specialized network topology to generate features for individual channels and time intervals, ensuring that the internal layers of this network quantify channel-level and temporal-level features, similar to [22-24].\nGiven \\(X_{1:T} = [X_{1:T}(1), \\cdots, X_{1:T}(C)]^T\\) for channels \\(c = 1, \\ldots, C\\), we define a temporal network \\(F_{TN} : \\mathbb{R}^T \\rightarrow \\mathbb{R}^K\\) that maps single-channel signals to a K-dimensional feature space, and a channel network \\(F_{CN} : \\mathbb{R}^{L\\times K} \\rightarrow \\mathbb{R}^K\\) that maps concatenated channel features to global features:\n\n\n\\begin{equation}\nZ_c = F_{TN}(X_{1:T}(c)), \\quad c = 1, \\ldots, C; \\quad Z_F = F_{CN}([Z_1, Z_2, \\ldots, Z_C]^T),\n\\end{equation}\nwhere \\(Z_1, Z_2, \\ldots\\) are channel-level features, and \\(Z_F\\) is global trial-level features."}, {"title": "Channel-level dependence", "content": "\\(\\rho_{C, F}(c)\\). The density ratio of \\(Z_1, Z_2, \\ldots\\), \\(Z_C\\) relative to \\(Z_F\\) measures channel-level dependence. Post-training and with fixed parameters, we compute the ACF of the channel features \\(R_c = E[\\sum_{c=1}^C Z_c Z_c^T]\\), the ACF of the global features \\(R_F = E[Z_F Z_F^T]\\), and the CCF between them \\(P_{C, F} = E[\\sum_{c=1}^C Z_c Z_F^T]\\).\nNext, the features are normalized as in the Sec. 2.2: \\(Z_c\\) and \\(Z_F\\) are normalized to \\(\\widetilde{Z_c} = R_c^{-\\frac{1}{2}} Z_c\\) and \\(\\widetilde{Z_F} = R_F^{-\\frac{1}{2}} Z_F\\) for orthonormality. The SVD of \\(R_{P_{C,F}} = U S^{\\frac{1}{2}} V\\) is computed. The outputs are further normalized to \\(\\hat{Z_c} = U \\widetilde{Z_c}\\) and \\(\\hat{Z_F} = V \\widetilde{Z_F}\\) to guarantee invariance in the linear operator. Finally, the density ratio can be constructed as \\(\\rho_{C, F}(c) = \\hat{Z_c}^T S \\hat{Z_F}\\).\nThis ratio \\(\\rho_{C, F}(c)\\) is a function of channel c and trial X, implying the dependence between channel and global features. The greater the value, the stronger the activation of the channels, showing which channels contribute the most to the cortico-muscular connectivity."}, {"title": "Temporal-level features and dependence.", "content": "To measure time-domain dependence, we compute density ratios between the internal features of the temporal network FTN and the global features, in two steps: first, computing density ratios between adjacent network layers; second, aggregating these ratios to consider all layers."}, {"title": "Step 1: Construct density ratios", "content": "\\(\\rho_{s-1,s,c}(\\tau_1, \\tau_2)\\) between adjacent layers. Fix a channel c and feature Zc. Consider a simple temporal network with S convolution layers with nonlinaer activation functions: \\(F_{TN}^{(1)}, F_{TN}^{(2)}, \\ldots, F_{TN}^{(S)}\\), with kernel sizes \\(\\Delta_1, \\Delta_2, \\ldots, \\Delta_S\\), and their outputs \\(Z_{c,1}, Z_{c,2},..., Z_{c,S}\\). Suppose the time dimensions of these layers are \\(\\tau_1, \\tau_2,..., \\tau_S\\). Fix any layer s. The \\(\\tau\\)-th element of \\(Z_{c,s}\\), denoted as \\(Z_{c,s}(\\tau)\\), is obtained by applying a nonlinear operation to a segment of the previous layer's output:\n\n\n\\begin{equation}\nZ_{C,S}(\\tau) = F_{TN}^{(1)}(Z_{c,s-1}(\\tau:+\\Delta_{s-1})).\n\\end{equation}\n\n*  Define the ACF of layer s \u2212 1: \\(R_{C,s-1} = \\frac{1}{T} E[\\sum_{\\tau} Z_{C,s-1}(\\tau) Z_{C,s-1}(\\tau)^T]\\)\n*  Define the ACF of layer s: \\(R_{C,s} = \\frac{1}{T} E[\\sum_{\\tau} Z_{C,s}(\\tau) Z_{C,s}(\\tau)^T]\\)\n*  Define the CCF between them: \\(P_{c,s-1,s} = \\frac{1}{T} E[\\sum_{\\tau} \\sum_{s} Z_{C,s-1}(\\tau+\\delta) Z_{I,s}(\\tau)]\\)."}, {"title": "Robustness of FMCA-T.", "content": "Fig. 3 shows the robustness of our proposed measure in the SinWav dataset, when there are increasing levels of nonstationary noise and delays. Since EEG and EMG signals are often damaged and distorted by environmental noise and the functional coupling occurs with time delays, an effective measure should maintain its robustness against these factors."}, {"title": "Applying FMCA-T to EEG-EMG-Fusion.", "content": "We confirm our primary hypothesis that the projection space defined by EEG eigenfunctions, derived from modeling the statistical dependence between EEG-EMG recordings, captures essential contextual factors like movements and subjects without requiring labels. We visualize the learned eigenfunctions and density ratios in Fig. 4."}, {"title": "Channel-level and temporal-level dependencies.", "content": "We visualize the local density ratio responses of cluster SUB3-C1 (reaching movement) in both spatial (\\(\\rho_{C,F}(c)\\)) and temporal domains (\\(\\phi_{s,c}(\\tau)\\)) in Fig. 5. The channel-level dependence is averaged across all trials and displayed in Fig. 5(a). We also randomly select nine trials from the same cluster and visualize them in Fig. 5(b). The temporal-level dependence for the first trial T1 in SUB3-C1 is shown in Fig. 5(c). Consistent activations are observed in other subjects, details in the App. B.\nAs illustrated in Fig. 5, the localized density ratio remains stable throughout the 4-second movement, corroborating the consistency of brain-muscle connectivity during stable states [33]. We also find that in channel-level dependence, the density ratio activates the fronto-central (FC) areas. The sensorimotor area is crucial for movement control, with EEG data from these regions often used to decode motor intentions. However, motor control also relies on cognitive processes [34], especially during movement planning, complex tasks, and collaborations [35]. Thus, the region of Brodmann area 6, well acknowledged to playing a role in movement planning may contribute differently during various movement tasks [36]. Our findings show that the features extracted from these fronto-central areas play an important role in classification."}, {"title": "Conclusion", "content": "This paper introduces a novel approach for estimating cortico-muscular dependence through the orthonormal decomposition of the density ratio. By treating the density ratio as a positive definite function and learning its projection space from EEG and EMG, we unveil the relationship between"}, {"title": "Additional Implementation Details of FMCA-T", "content": "Adaptive estimators. Since autocorrelation functions (ACF) and crosscorrelation functions (CCF) are expectations of inner products, we prefer to compute these expectations and the cost gradient using an adaptive filter rather than a batch of data. For any parameter \u03b8, the partial derivative of the cost rr(fe, gw) := Trace(R\u012b\u00b9PR\u00b9 PT) has the following form:\n\n\n\n\\begin{equation}\n\\frac{\\partial r_T}{\\partial \\theta} = -\\text{Trace}(R_F^{-1} \\frac{\\partial R_F}{\\partial \\theta} R_F^{-1}PR_G^{-1}P^T) + \\text{Trace}\\left(R_F^{-1} \\frac{\\partial P}{\\partial \\theta} R_G^{-1}P^T\\right) -\\text{Trace}\\left(R_F^{-1} P R_G^{-1} \\frac{\\partial R_G}{\\partial \\theta} R_G^{-1} P^T \\right) + \\text{Trace}(R_F^{-1} P R_G^{-1} \\frac{\\partial P^T}{\\partial \\theta} (R_G^{-1})^T).\n\\end{equation}\nObserve that terms needed for the gradient have two classes: terms RF, RG, P, as well as their inverse, and terms of the derivatives ORF, ORG, OP. We use a smoothing window over iterations to estimate the RF, RG, P, and then use the estimated values of RF, RG, P as the matrices and their inverse in the cost. For derivative terms ORF, ORG, OP, we use the derivatives of the batch. We choose smoothing coefficient \u03b2 = 0.9 across all experiments.\nImportant parameters. When computing the matrix inverse of ACFs (RF-1 and RG-1), a small diagonal matrix scaled by a regularization parameter, \u03b5I, is added. This constant is important for training stability. We choose \u03b5 = 10-5 across all experiments. The output dimension of the network is chosen as K = 128, which is also the number of eigenfunctions."}, {"title": "Additional Experiments", "content": "Visualization of EEG's eigenfunctions. Building on Fig. 4 from the main paper, we further present results for the visualization of EEG's eigenfunctions on all 25 participants. In Fig. 6, each trial is color-coded by the estimated density ratios for all participants (SUB1 to SUB25). In Fig. 7, each trial is color-coded by the label of their movements.\nIt can be observed that for each participant, the t-SNE projections form individual clusters, each corresponding to one of the three main movements during a session. The density ratios are highly similar within each cluster (intra-cluster) while varying across clusters (inter-cluster). This indicates that using density ratio as a dependence measure effectively captures each movement individually."}, {"title": "Temporal-level dependence.", "content": "Extending the analysis in Fig. 5 of the main paper, where the temporallevel dependence was shown for a single subject, we randomly select another seven trials from subject SUB3's reaching movement (C1) and visualize the temporallevel dependence in Fig. 10. We also plot the average temporal dependence across all trials in Fig. 10h. We find consistent activations of fronto-central (FC) channel during the 4-second movement in each trial. When we compare each trial's result with the averaged one, still we observe that the activation patterns are highly similar. This indicates that our dependence measure captures information that is consistent and generalizable across subjects."}, {"title": "Channel-level dependence.", "content": "Similar to Fig. 5 of the main paper, we also quantify the channel-level statistical dependence from other subjects, not just SUB3. We randomly select clusters from subjects and visualize the results in Fig. 11. We find a consistent pattern across subjects that the FC channels are activated most strongly. It can also be observed that within each cluster (each subplot), channel activations are highly similar across trials. This indicates that our dependence measure robustly captures the cortical-muscular connectivity of the same movement."}, {"title": "Learning curve comparison.", "content": "We show the smoothness of the training stage of FMCA-T, comparing its learning curve with the learning curve of MINE when applied both on EEG-EMG-Fusion. As shown in Fig. 12, FMCA-T demonstrates superior stability, whereas MINE suffers from greater instability even when smoothing windows are applied to estimate the gradient of the variational cost in the Donsker-Varadhan representation."}, {"title": "Temporal-level dependence on SinWav.", "content": "We analyze the temporal activations of the learned dependence measure on SinWav by visualizing its localized density ratios. This is performed by computing the density ratios between adjacent layers of feature projectors. The layer-wise density ratios are then aggregated for visualization. We find that the localized density ratios exhibit higher activations at the hills and valleys, and correctly capture the period and phase of the sinusoids when there is an increasing delay between the two sinusoids (Fig. 13). As shown in Fig. 14, when Gaussian noise with a standard deviation equal to 1.0 is added to the clean sinusoidal signal (signal-to-noise ratio less than 0 dB), the density ratio can still correctly identify the hills and valleys. This indicates that our proposed dependence measure is robust to random noise and delay by filtering out trivial factors like noise while focusing on the primary signals."}, {"title": "Limitation", "content": "We demonstrate the effectiveness of using FMCA to learn the dependence between EEG and EMG signals and have shown that the learned eigenfunctions embed subject and movement information after optimization. We conducted the experiments on a public EEG and EMG dataset, which only contains 11 discrete upper extremity movements from 25 subjects. What we leave in the future is to use the meaningful eigenfunctions for regression tasks, i.e., continuously predicting the kinematics and contraction forces during the movement. Another limitation of the study is that we did not include patients' data due to a dearth of such large datasets that collect patients' multi-modal bio-signals. We hope that the promise offered by using our dependence measurement to evaluate cortico-muscular connectivity will further stimulate experimental research in this direction. Last, from a technical point of view, we only used convolutional neural networks with a concatenated MLP as the backbone of our networks in this study. Although we suppose that a CNN model is sufficient for the current scope as the temporal information is processed with CNN and spatial information can be leveraged by the final MLP projection layer, more advanced network structures, such as the attention in the transformer, could be potentially useful when aiming for more complex tasks."}, {"title": "Broader Impact", "content": "This paper proposes to use the statistical dependence between the densities of neural data to evaluate cortico-muscular connectivity. Compared with the traditional method (CMC) that computes the linear correlation between EEG and EMG spectra, our measurement shows less variance within movement and subject while is more distinguishable across movements and subjects. This helps in exploring the neural information pathways from the brain to the muscle, which could further be used in the field of patient rehabilitation. Moreover, the learned eigenfunctions can be used as \u201ccommon information\" decoders, for example to decode movement and subject, and are more robust to distribution shift when tested on unseen subjects. This could be very useful for developing brain-machine interfaces under inter-subject conditions.\nAll datasets in this paper are publicly available and are not associated with any privacy or security concerns. Usages of the datasets strictly follow the corresponding licenses.\"\n    }"}]}