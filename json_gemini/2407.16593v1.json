{"title": "A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions", "authors": ["Giorgos Lysandrou", "Roma English Owen", "Vanja Popovic", "Grant Le Brun", "Aryo Pradipta Gema", "Beatrice Alex", "Elizabeth A. L. Fairley"], "abstract": "There exists an invisible barrier between healthcare professionals' perception of a patient's clinical experience and the reality. This barrier may be induced by the environment that hinders patients from sharing their experiences openly with healthcare professionals. As patients are observed to discuss and exchange knowledge more candidly on social media, valuable insights can be leveraged from these platforms. However, the abundance of non-patient posts on social media necessitates filtering out such irrelevant content to distinguish the genuine voices of patients, a task we refer to as patient voice classification. In this study, we analyse the importance of linguistic characteristics in accurately classifying patient voices. Our findings underscore the essential role of linguistic and statistical text similarity analysis in identifying common patterns among patient groups. These results allude to even starker differences in the way patients express themselves at a disease level and across various therapeutic domains. Additionally, we fine-tuned a pre-trained Language Model on the combined datasets with similar linguistic patterns, resulting in a highly accurate automatic patient voice classification. Being the pioneering study on the topic, our focus on extracting authentic patient experiences from social media stands as a crucial step towards advancing healthcare standards and fostering a patient-centric approach.", "sections": [{"title": "Introduction", "content": "There is a critical need for global healthcare systems to provide better treatments for patients. A substantial aspect of the shortcomings in healthcare systems worldwide can be traced back to the generalized nature of services and medications provided. More personalized care (i.e. offering patients the right drugs, at the right time, in the right dose and formulation) holds the key to significantly improving outcomes\u00b9.\nTo achieve this, it is important to transition from a primarily professional-centric approach to a patient-centric approach, where we put more emphasis on listening to patient experiences.\nHowever, capturing genuine patient experiences proves to be challenging. Conventional methods, such as relying on the conclusions of healthcare professionals (HCPs) after their interactions with patients, often fall short of providing a comprehensive understanding\u00b2. Patients' interactions with HCPs may be highly influenced by environmental factors, leading to less descriptive conversations and an incomplete portrayal of patients' experiences3,4. Another conventional method is through patient focus groups, where the patients' interactions are recorded to collect their perspectives on healthcare-related topics, such as their experience using certain drugs. Unfortunately, patient focus groups risk introducing bias by drawing feedback primarily from specific socioeconomic segments, potentially excluding a representative sample of the entire patient population.\nRecognizing the limitations of traditional approaches, this study advocates for the exploration of an alternative medium, that is social media. Patients are observed to share personal health-related details, facilitated by the distance and anonymity provided by online platforms5,6. The absence of HCPs in this space allows patients to express themselves more candidly and descriptively, providing a unique perspective to understand their experiences. Compared to other conventional non-physical sources such as market research and pharmaceutical representatives' feedback, social media offers a more accessible and diverse pool of patient information, reducing potential biases.\nThe increase in studies focusing on analyzing social media data for patient experience information in recent years attests to the growing recognition of its potential7. Advances in data capture and analysis, coupled with the rise in relevant social media"}, {"title": "Related Work", "content": "Natural Language Processing in Healthcare\nThe digitization of healthcare has experienced a substantial upswing in recent years. Global events, such as the COVID-19 pandemic, have triggered a surge in patient engagement within online platforms. This transition is complemented by the rapid advancements in Artificial Intelligence (AI), which have shown remarkable efficacy in enhancing healthcare data analysis. In this context, the employment of Natural Language Processing (NLP) and Deep Learning (DL) techniques has become increasingly pivotal in interpreting both structured and unstructured healthcare data11\u201314.\nA survey study by Hudaa et al. (2019)15 reported the instrumental role of NLP in extracting information from unstructured healthcare data, through methods such as document classification and feature extraction. Their research highlights the advent of Large Language Models (LLMs) as a breakthrough in understanding natural language, positing substantial improvements in patient-healthcare provider interactions.\nSimilarly, Lavanya et al. (2021)16 investigated the utilization of DL techniques for classifying healthcare-related texts within social media platforms. Given the rapid emergence of social media as a predominant platform for healthcare discourse, their findings stress the critical need for optimized information extraction methodologies.\nPatient Voice Detection\nThe study of patient voices within the social media sphere further exemplifies the utility of NLP in healthcare. Pattisapu et al. (2017)17 demonstrated an approach to discern medical personas through social media posts, utilizing pre-trained word embeddings (e.g. Word2Vec18) for superior information extraction. This analysis not only aids in understanding patient perspectives but also serves pivotal roles in drug marketing and safety monitoring.\nDai et al. (2017)19 investigated the clustering of social media post embeddings as an advanced alternative to traditional classification methods. Their unsupervised method, relying on no labelled training data, showcased a commendable classification accuracy, heralding a new era of understanding social media discourse in healthcare.\nAlex et al. (2021)10 provided an extensive review of patient voice detection in their study20\u201325. Their work amplifies the discourse on patient voice detection, setting the stage for future innovations in this field."}, {"title": "Linguistic Analysis of Patient Language", "content": "The linguistic analysis of patient language offers another dimension of insight into patient experiences and concerns. Lu et al. (2013)26 performed topic analysis within online patient communities, uncovering prevalent discussions on symptoms, drugs, and procedures. Their findings illustrated the diversity of patient concerns, varying significantly across different disease-specific communities.\nDreisbach et al. (2019)27 provided a review of NLP applications in extracting clinical symptoms from patient-authored texts across various platforms, such as Twitter and online community forums."}, {"title": "Data Acquisition and Annotation", "content": "Data Sources and Collection Methodology\nFor the experiments detailed herein, data was systematically collected from two principal online platforms: Reddit\u00b3 and SocialGist. We selected these platforms for their extensive user-generated content on health-related topics. Reddit, known for its user-created communities called subreddits, provided a diverse range of discussions across various health conditions including cardiovascular, oncology, immunology, and neurology. SocialGist, serving as a data aggregator, offered access to a wide array of message board posts from multiple community websites focusing on similar health domains.\nWe utilized the Pushshift Reddit API to retrieve a comprehensive list of historical and current posts from targeted subreddits. Similarly, the SocialGist API facilitated the collection of message board posts. A meticulously curated list of search terms, related to specific drugs and therapies within the aforementioned therapeutic areas, guided the data retrieval process. This approach is supported by literature indicating that carefully selected search terms can yield high levels of precision and recall in data collection efforts 28.\nDuplicate entries were identified and removed based on the text body and unique identifiers. The final dataset comprised 14,693 posts, with an almost equal distribution between Reddit (7,211 posts) and SocialGist (7,482 posts). A detailed breakdown of the data volumes by source and therapeutic domain is available in Supplementary Material 1.\nManual Annotation\nSubsequent to data collection, the posts underwent a manual annotation process. Utilizing Doccano6, an open-source annotation tool, a team of trained annotators applied document-level labels to each post. These labels distinguished between \u201cPatient Voice\" and \"Not Relevant\u201d. \u201cPatient Voice\u201d denotes first-hand experiences of patients, while \u201cNot Relevant\" denotes all other content types, including healthcare professional insights, news articles, etc. To ensure reliability, the annotators adhered to detailed guidelines continuously refined throughout the project. Examples of each label are as follows:\n\u2022 Patient Voice: \"I'm taking MTX and imraldi at the moment, so far so good.\"\n\u2022 Not Relevant: \"MHRA due to approve new RA drug.\", \"One of my old patients used to take 10mg eliquis instead of 5mg. His heart rate was...\"\nAfter a single annotation phase, the dataset was partitioned into training (80%) and validation (20%) subsets. This division was executed post-randomization with a reproducible random seed to mitigate bias while maintaining consistent label distribution across splits. We conducted an additional annotation phase to collect a holdout test set which is equal in size to the validation subset for classifier evaluation. The resulting train validate and test ratios are 66% train, 17% validation and 17% test. We also ensured that no validation or test data were leaked into the training data.\nInter-Annotator Agreement\nWe calculated the Inter-Annotator Agreement (IAA) scores to evaluate the consistency among annotators and the effectiveness of our annotation guidelines. These scores are also important as they indicate the maximum performance our AI model could achieve should it succeed in modeling human classification accuracy. We involved 12 annotators to label 2,388 posts selected at random from all four therapeutic domains. For each annotator pair, we computed standard metrics, precision, recall, and"}, {"title": "Methods", "content": "The workflow of our study is visualized in Figure 2. We start with the data collection as stated in the previous chapter, followed by annotation and data partitioning. This fuels our subsequent qualitative linguistic and statistical text analyses. Our workflow is concluded with the development of NLP classifiers trained to identify patient voices."}, {"title": "Qualitative Linguistic Analysis", "content": "Our annotation team conducted a qualitative analysis of randomly selected posts from each therapeutic domain and data source-specific dataset to gather apparent differences in the language that patients use in describing their experiences."}, {"title": "Statistical Text Similarity Analysis", "content": "We measure the linguistic similarity between datasets by calculating cosine similarities of datasets' vector representations. We leverage Term-Frequency Inverse-Document-Frequency (TF-IDF)29 to calculate a representation for each word which indicates its value of significance based on the number of its occurrences within one dataset and across multiple datasets:\n$TFIDF(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\in d} f_{t',d}} \\times log (\\frac{|D|}{|{d\\in D: t \\in d}|})$\nwhere $f_{t,d}$ is the frequency of term t in document d, $\\sum_{t'\\in d} f_{t',d}$ is the total number of terms in document d, $|D|$ is the total number of documents in the corpus, and $|{d \\in D : t \\in d}|$ is the number of documents where the term t appears.\nSubsequently, we can generate a dataset's vector representation by aggregating the TF-IDF vectors of all words inside a dataset, effectively summarizing the dataset's lexical characteristics. Utilizing these vector representations, we conduct pairwise comparisons of datasets from various data sources and therapeutic domains by calculating the cosine similarities between pairs"}, {"title": "Text classifiers", "content": "In our study, we focus on identifying patient voices within online posts using advanced text classification techniques. We employ two model architectures:\n\u2022 Convolutional Neural Network (CNN) Text Classifier: We use spaCy's30 small CNN en_core_web_sm as the baseline of the experiment. This model uses mean pooling and attention mechanisms within its CNN architecture. We chose this architecture for its balance of efficiency and accuracy. We refer to this model as \u201cCNN\u201d classifier in later sections.\n\u2022 Transformer-Based Classifier (RoBERTa): For a more computationally intensive and accurate solution, we leverage spaCy's transformer model en_core_web_trf. It uses a RoBERTa31 base model which has been pre-trained on a large general-domain text corpus, providing contextually rich word representations. We refer to this model as \u201cTransformer\u201d classifier in subsequent sections.\nWe employ two different model architectures in our study: a CNN and a transformer-based model, specifically RoBERTa. Both models utilize a bag-of-words approach for document representation, where each word in a text is represented by a vector created by the language models. These word vectors are context-dependent, as they capture the contextual meaning of words within the sentence they appear in. The document representation is then constructed by combining the word representation vectors for all the words in the text.\nWe use spaCy's TextCategorizer module for the modelling needs of this work. It supports multiple language model architectures, which use a bag-of-words approach to classify text. We train the models on our training data, seen in Figure 1 and Table 1, such that they are fine-tuned for our specific task of identifying patient voice amongst online posts."}, {"title": "Evaluation metrics", "content": "We report the standard metrics of precision, recall and F1 scores for each label type, weighted and macro averaged F1 scores across all label types. The classifier models are evaluated on the test datasets."}, {"title": "Statistical Validation", "content": "To assess the statistical significance of the performance differences between our classifiers, we employed McNemar's test 32. This test evaluates whether the differences in performance are due to random variation or represent true differences. The test takes as input the predictions of two classifiers on the same test dataset and calculates the likelihood of these differences occurring by chance."}, {"title": "Results & Discussions", "content": "Qualitative Linguistic Analysis\nManual analysis and review of data from each therapeutic domain and data source specific dataset, shows differences in the language patients use to describe their experiences. Quantitative analysis between the two data sources, showed that overall, SocialGist patients describe their experiences using more words (longer posts), compared to Reddit patients, while also using a richer vocabulary. This difference is reflected in the average total unique words to total words per post ratio.\nThe methodology for performing a qualitative linguistic analysis on the experiment data involves several steps. Firstly, a random selection is made throughout the dataset, to mitigate selection bias, choosing 10 posts from each therapy area and data source, adding up to 20 posts per therapy area. These posts are then manually examined to identify common characteristics within the language used by patients to express their experiences, supplementing from past experience through exposure to the data. Subsequently, a single post or extract from a post that best exemplifies these identified characteristics is selected. The selected extract serves as a representative sample of the therapeutic domain, showcasing the unique language variations within each domain. The following are the examples illustrating these therapy area specific characteristics:\n\u2022 Cardiovascular: \u201cI had a blood test for my D levels. They were scarily low the year of my heart attack (2016). The doctor put me on 1000mg a day of D3.\"\n\u2022 Oncology: \u201cI'm in a similar situation to you! I'm 32, was diagnosed in April, I have HER2+ invasive Stage 2B grade 2 with lymph node involvement.\"\n\u2022 Immunology: \u201cI now take Cosentyx. However, after 3 months I have noticed what looks either like psoriasis on soles of my feet or could be athlete's foot. After 15 weeks, I now have the same peeling all over palms, between fingers, and backs of fingers.\"\n\u2022 Neurology: \u201cI have taken quetiapine, abilify and olanzapine. Olanzapine didn't help at all and I always feel a bit nervous and anxious on the quetiapine.\"\nPatients posting on cardiovascular-related conditions often mention detailed references to medication dosages, side effects, lifestyle factors, as well as significant health events, such as \u201cheart attack\u201d or \u201cstroke\u201d. This pattern suggests that cardiovascular patients primarily reflect on the contrast between their pre-diagnosis lifestyle (e.g. weight characteristics) and their condition post-diagnosis. The nature of cardiovascular symptoms, which are typically less visible without medical intervention, possibly contributes to this focus. On the rarer occasion that patients are made aware of their symptoms ahead of crisis point, it is still difficult to notice these symptoms themselves to the same extent an immunology patient with itchy skin could. High blood pressure for instance would be almost impossible to detect for oneself without the aid of a wearable device. When medicines are discussed in this cardiovascular context, the patient conversation often focuses on the side effects of these medicines, that can be detected by a patient, rather than the disease-related symptoms.\nIn contrast, oncology discussions were characterized by a high degree of specificity, with patients frequently discussing genetic markers, clinical trials, and the outcomes of diagnostic tests. Specific terms such as \u201cHER2+\u201d, \u201cER-\u201d, and \u201cStage IV\u201d are commonly found in oncology posts. This may indicate a reliance on the information given by the health care professionals for understanding and communicating their condition. In our experiment, oncology patients' posts contained the most concise and specific language.\nConversations among immunology patients were noticeably detailed, reflecting the visible and impactful nature of their symptoms in their daily lives. These discussions often included vivid descriptions of physical symptoms, highlighting the significant effect of immunological conditions on patient well-being. As shown in the example, patients tend to share detailed experiences such as body parts that are affected and exact sensations that they experienced.\nNeurological condition discussions were predominantly centered around the subjective experience of living with the condition, with a strong emphasis on emotional well-being and the effects of various treatments."}, {"title": "Statistical Text Similarity Analysis", "content": "To complement our qualitative insights, we conduct a statistical text similarity analysis to quantitatively assess lexical similarities across datasets. Utilizing cosine similarity measures derived from TF-IDF vectors, we examine the linguistic commonalities between datasets spanning different data sources and therapeutic domains. This analysis focuses on the stemmed corpus of each dataset, with stopwords removed to highlight semantic parallels more effectively.\nThe pairwise similarity is illustrated in Figure 3. A value of 1.0, depicted in bright yellow, indicates a high degree of lexical similarity. In contrast, darker blue shades denote lower similarities, suggesting diverse vocabularies between datasets. Through this methodology, we consider three categories of similarity: low (0.45-0.60), medium (0.60-0.75), and high (above 0.75). 18 dataset pairs fell within the medium similarity range, seven pairs displaying low similarity and three pairs exhibiting significant similarity.\nNotably, the analysis showed that patient posts within the same therapeutic domain, but across different data sources, tend to share a substantial amount of linguistic commonality. This trend was particularly evident among cardiovascular, neurology, and oncology discussions across Reddit and SocialGist platforms. Conversely, immunology patient discourse displayed marked language differences between these two sources, with Reddit's immunology dataset (denoted as immunology_r) registering the lowest cosine similarity scores in comparison to all other datasets, including those from SocialGist (denoted as immunology_sg).\nTo understand these findings, we identified the top 20 words with the highest TF-IDF scores within each dataset as shown in Table 2. This analysis confirmed that Reddit's immunology discussions are characterized by a distinctive lexicon, with 13 of the top 20 TF-IDF terms being unique to this dataset. Similarly, SocialGist's immunology content featured 10 unique terms, among its top 20, frequently referencing specific medications such as \u201cHumira\u201d, \u201cOcrevus\u201d, and \u201cEntyvio\u201d. This contrasts with other therapeutic areas, where the top TF-IDF terms often encompassed temporal references (e.g., \u201cdays\u201d, \u201cweeks\u201d, \"years\") and medical terminology (e.g., \u201cdoctor\u201d, \u201cmeds\u201d, \u201ctreatment\u201d), alongside expressions related to the patient experience (e.g., \"pain", "help": "effects\").\nThese insights underscore the varied linguistic landscapes within patient narratives across therapeutic domains and platforms, highlighting the importance of nuanced analysis in understanding patient discourse.\""}, {"title": "Text classification", "content": "Building upon the insights gathered from our linguistic analyses, we explore the application of NLP models to identify patient voices within our datasets. This exploration involves a series of experiments across varied therapeutic domains and data sources. Each experiment is performed using two different classifier model architectures, a baseline CNN model from spaCy33 and a Transformer model 31. We report the performance of each classifier model trained on a data source and therapeutic domain specific dataset, or on a combination of these datasets."}, {"title": "Experiment 1: Data source and therapeutic domain specific classifiers", "content": "Our initial experiment aims to understand the nuanced language differences among patient groups, by training and evaluating separate classifiers for each data source and therapeutic domain. This approach allows us to gauge the effectiveness of both model architectures in a specified setting. Notably, the F1 scores for both classifiers span from 0.928 to 1.0 across most data subsets. An exception is observed in the cardiovascular classifiers for Reddit (cardiovascular_r) and SocialGist (cardiovascular_sg), where F1 scores dipped to 0.865 and 0.760, respectively. In this setting, the Transformer model consistently outperforms the CNN in precision and F1 scores, although both architectures demonstrated equivalent recall performance."}, {"title": "Experiment 2: Combined therapeutic domain, data source and all data classifiers", "content": "In our second experiment, we aimed to examine the model while leveraging the lexical similarities identified in the TF-IDF analysis. To this end, we combine datasets within the same therapeutic domain (e.g. merging cardiovascular_r and cardiovascular_sg into a single cardiovascular dataset) and from the same data source (e.g. aggregating cardiovascular_r, oncology_r, immunology_r, and neurology_r into reddit_coin). Additionally, an all dataset was created to encompass all data collected. This approach was motivated by the hypothesis that datasets with similar vocabularies, as revealed through TF-IDF analysis, could benefit from combined training, potentially yielding classifiers with improved generalizability. Consequently, our focus was on comparing the effectiveness of both CNN and Transformer model architectures across these aggregated datasets.\nThe results of this experiment underscore the performance variations between our models. Notably, the transformer classifiers demonstrate a marginal advantage over CNN models in recall and F1 scores, indicating their robustness in more generalized settings. The highest F1 scores observed ranged from 0.911 to 0.948, with notable performance by the immunology and socialgist_coin classifiers. The low cosine similarity score between the two immunology"}, {"title": "Experiment 3: All classifiers comparison on each therapeutic domain and data source specific dataset", "content": "In our third experiment, we assess the performance of classifiers trained on specific combinations of therapeutic domains and data sources. This evaluation aimed to discern the optimal classifier configuration for each unique dataset scenario. One example is the evaluation of the Reddit cardiovascular dataset (cardiovascular_r), which was tested against classifiers trained on cardiovascular_r, the aggregated cardiovascular dataset, the reddit_coin dataset representing a collection from the same data source, and the all dataset encompassing the entirety of our collected data.\nTable 5 details the precision, recall, and F1 scores of these classifiers. Across the datasets, the highest F1 scores varied notably, ranging from 0.977 to a perfect 1.0. An exception was observed within the cardiovascular datasets from Reddit and SocialGist, where F1 scores were marginally lower, at 0.865 and 0.863, respectively. Remarkably, the transformer-based classifiers consistently outperformed the CNN models in nearly all metrics, signifying their capability to handle the complexity of patient language across diverse medical discussions. The only exception to this trend was noted in the recall metric for the Reddit neurology dataset, where a CNN model narrowly outperformed the transformer counterpart.\nIn conclusion, the results of our classifier modeling experiments reflect the observations made in the linguistic and statistical text analysis. The TF-IDF similarity analysis showed similarity between patients' language within the same therapeutic domain across both data sources. Observing the achieved F1 scores in Table 5, the classifiers trained on combined datasets, outperform the classifiers trained on therapeutic domain and data source specific datasets, with the exception of the Reddit cardiovascular and Reddit neurology classifiers which slightly outperformed classifiers trained on combined datasets. These results confirm the efficacy of combining datasets with linguistic similarities as identified by TF-IDF.\nOur analyses further reveal two findings. First, the transformer model architecture consistently outperforms the CNN model architecture, in all three metrics of precision, recall and F1 scores. This pattern is observable across all conducted experiments, suggesting a considerable advantage of the transformer model's pre-training. Furthermore, the results indicate that transformer"}, {"title": "Conclusions", "content": "In this study, we explored the identification of patient voices within posts collected from social media and message boards across four distinct therapeutic domains (i.e. cardiovascular, oncology, immunology, and neurology). We observed that each therapeutic domain can be characterized by unique linguistic features. Through qualitative linguistic and statistical text similarity analyses, we identified specific ways patients communicate their experiences. This analytical approach not only enabled the identification of lexical similarities across datasets but also informed the strategic aggregation of datasets for the training of NLP models. We noticed that patients across domains, with the exception of immunology, demonstrated similar linguistic patterns across different data sources.\nOur experiments highlight the effectiveness of merging linguistically similar datasets. By creating larger, more robust training sets, we were able to enhance the performance of our classifiers. In particular, we found that classifier models trained on such aggregated datasets consistently outperformed those trained on narrower, domain-specific datasets. Moreover, our comparative analysis of classifier architectures revealed a clear advantage of pre-trained transformer models over CNN models for this classification task."}, {"title": "Data availability", "content": "We provide the list of subreddits and search terms, which we used to collect the data for this research and development project, in the supplementary material. The annotation labels and examples are also described in this paper. The third-party tools (classifiers and annotation tool) used for this work are freely available and details on the classifier set-up and model parameters are provided in this paper. For more information about this project and the data please contact Elizabeth A.L. Fairley."}, {"title": "Acknowledgements", "content": "We would like to express our gratitude to the invaluable Talking Medicines Limited annotators for their hard work in creating the data needed for model training and validation. We would also like to thank Ellen Halliday, for her consultation in all compliance matters concerning this paper's experiments, as well as the Talking Medicines Founders Jo-Anne Halliday and Scott F. Crae for their support of this project."}, {"title": "Author contributions", "content": "G.L. and R.E.O. contributed equally to the writing of the original draft and the review and editing of the manuscript. R.E.O. was responsible for data curation, including managing data annotation used for training, validation, evaluation, and inter-annotator agreement calculations, and performed the formal analysis on the linguistic aspects of the data. G.L. conducted the formal analysis on the statistical text similarity, inter-annotator agreement calculations, and modeling experiments. V.P. contributed to the acquisition and processing of the experiment data and the development of annotation tools. G.L.B. provided software and engineering support for the infrastructure to run the experiments. A.P.G contributed to the review and editing of the manuscript. B.A. contributed to the conceptualisation of the project, and participated in the review and editing of the manuscript. E.A.L.F. contributed to the conceptualisation and overall direction of the project, provided supervision and participated in the review and editing of the manuscript. All authors approved the submitted version and agreed to be accountable for their contributions."}, {"title": "Funding", "content": "This work was funded by Talking Medicines Limited."}, {"title": "Additional information", "content": "Competing interests\nThe authors declare that they are employees of Talking Medicines Limited, which funded this research. The company may have a financial interest in the results of this study. However, the authors maintain that the research was conducted objectively and without bias."}]}