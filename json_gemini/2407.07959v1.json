{"title": "Source Code Summarization in the Era of Large Language Models", "authors": ["Weisong Sun", "Yun Miao", "Yuekang Li", "Hongyu Zhang", "Chunrong Fang", "Yi Liu", "Gelei Deng", "Yang Liu", "Zhenyu Chen"], "abstract": "To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of code-related tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLM-based code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types (e.g., procedural and object-oriented programming languages). Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code implementation details and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Code comments are vital for enhancing program compre-hension [1] and facilitating software maintenance [2]. While it is considered good programming practice to write high-quality comments, the process is often labor-intensive and time-consuming [2]\u2013[4]. As a result, high-quality comments are frequently absent, mismatched, or outdated during software evolution, posing a common problem in the software industry [5]\u2013[8]. Automatic code summarization (or simply,"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Code summarization is the task of automatically generating natural language summaries (also called comments) for code snippets. Such summaries serve various purposes, including but not limited to explaining the functionality of code snippets [8], [28], [41]. The research on code summarization can be traced back to as early as 2010 when Sonia Haiduc et al. [42] introduced automated text summarization technology to summarize source code. Later on, following the significant success of neural machine translation (NMT) research in the field of NLP [43], [44], a large number of researchers"}, {"title": "III. STUDY DESIGN", "content": "This study aims to answer the following research questions:\nRQ1: What evaluation methods are suitable for assessing the quality of summaries generated by LLMs? Existing research on LLM-based code summarization [25], [28], [30] widely follow earlier studies [32], [36] and employ automated evaluation metrics (e.g., BLEU) to evaluate the quality of LLM-generated summaries. However, recent studies [27], [50] have shown that LLM-generated summaries surpass reference summaries in quality. Therefore, evaluating LLM-generated summaries based on their text or semantic similarity to reference summaries may not be appropriate. This RQ aims to discover a suitable method for automated assessment of the quality of LLM-generated summaries.\nRQ2: How effective are different prompting techniques in adapting LLMs to the code summarization task? This RQ aims to unveil the effectiveness of several popular prompting techniques (e.g., few-shot and chain-of-thought) in adapting LLMs to code summarization tasks.\nRQ3: How do different model settings affect LLMs' code summarization performance? To better meet diverse user needs, LLMs typically offer configurable parameters (i.e., model settings) that allow users to control the randomness of model behaviour. In this RQ, we adjust the randomness of the generated summaries by modifying LLMs' parameters and see the impact of different model settings on the performance of LLMs in generating code summaries.\nRQ4: How do LLMs perform in summarizing code snippets written in different types of programming languages? Programming languages are diverse in types (e.g., object-oriented and functional programming languages), with their implementations of the same functional requirements being similar or entirely different. The scale of programs implemented with them in Internet/open-source repositories also varies, which may result in differences in the mastery of knowledge of these languages by LLMs. Hence, this RQ aims to reveal the differences in LLMs' capabilities to summarize code snippets across diverse programming language types.\nRQ5: How do LLMs perform on different categories of summaries? Previous research [3], [54], [55] has shown that summaries can be classified into various categories according to developers' intentions, including What, Why, How-to-use-it, How-it-is-done, Property, and others. Therefore, in this RQ, we aim to explore the ability of LLMs to generate summaries of different categories."}, {"title": "IV. RESULTS AND FINDINGS", "content": "A. RQ1: What evaluation methods are suitable for assessing the quality of summaries generated by LLMs?\n1) Experimental Setup.\nComparison Evaluation Methods. Existing automated eval-uation methods for code summarization can be divided into the following three categories.\ni. Methods based on summary-summary text similarity assess the quality of the generated summary by calculating the text similarity between the generated summary and the reference summary. This category of methods is the most widely used in existing code summarization research [4], [25], [28], [30]. The text similarity metrics involved include BLEU, METEOR, and ROUGE-L, which compare the count of n-grams in the generated summary against the reference summary. The scores of BLEU, METEOR, and ROUGE-L are in the range of [0, 1]. The higher the score, the closer the generated summary approximates the reference summary, indicating superior code summarization performance. All scores are computed by the same implementation provided by [47].\nii. Methods based on summary-summary semantic similarity evaluate the quality of the generated summary by computing the semantic similarity between the generated summary and the reference summary. Existing research [36] demonstrates that semantic similarity-based methods can effectively alleviate the issues of word overlap-based metrics, where not all words in a sentence have the same importance and many words have synonyms. In this study, we compare four such methods, including BERTScore [71], SentenceBert with Cosine Similarity (SBCS), SentenceBert with Euclidean Distance (SBED), and Universal Sentence Encoder [72] with Cosine Similarity (USECS). They are commonly used in code summarization studies [36], [73], [74]. BERTScore [71] uses a variant of BERT [75] (we use the default RoBERTa\\text{large}) to embed every token in the summaries, and computes the pairwise inner product between tokens in the reference summary and generated summary. Then it matches every token in the reference summary and the generated summary to compute the precision, recall, and F\u2081 measure. In our experiment, we report the F\u2081 measure of BERTScore. The other three methods use a pre-trained sentence encoder (SentenceBert [76] or Universal Sentence Encoder [72]) to produce vector representations of two summary sentences, and then compute the cosine similarity or euclidean distance of the vector representations. SBCS, SBED, and USECS range within [-1,1]. Higher values of SBCS and USECS represent greater similarity, while lower values of SBED indicate greater similarity.\niii. Methods based on summary-code semantic similarity assess the quality of the generated summary by computing the semantic similarity between the generated summary and the code snippet to be summarized. Unlike the first two methods, this type of evaluation method does not rely on reference summaries and can effectively avoid issues related to low-quality and outdated reference summaries. SIDE proposed by Mastropaolo et al. [10] is a representative of this type of method. It provides a continuous score ranging within [-1,1], where a higher value represents greater similarity. We present the scores reported by the above similarity-based evaluation methods in percentage.\nHuman Evaluation. We conduct human evaluations as a reference for automated evaluation methods. Comparing the correlation between the results of automated evaluation methods and human evaluation can facilitate achieving the goal of this RQ, which is to find a suitable automated method for assessing the quality of LLM-generated summaries. To do so, we invite 15 volunteers (including 1 PhD candidate, 5 masters, and 9 undergraduates) with more than 3 years of software development experience and excellent English ability to carry out the evaluation. For each sample, we provide volunteers with the code snippet, the reference summary, and summaries generated by four LLMs, where the reference summary and the summaries generated by four LLMs are mixed and out of order. In other words, for each sample, volunteers do not know whether it is a reference or a summary generated by a certain LLM. We follow Shi et al. [9] and ask volunteers to rate the summaries from 1 to 5 based on their quality where a higher score represents a higher quality. The final score of the summaries is the average of scores rated by 15 volunteers.\nLLM-based evaluation methods. Inspired by recent work in NLP [37]\u2013[39], we also investigate the feasibility of employing LLMs as evaluators. Its advantage is that it does not rely on the quality of reference summaries, and the evaluation steps can be the same as human evaluation. Specifically, similar to human evaluation, when using LLMs as evaluators, for each sample, we input the code snippet to be summarized, the reference summary, and LLM-generated summaries, and ask LLMs to rate each summary from 1 to 5 where a higher score represents a higher quality of the summary. The specific prompt when using LLMs as evaluators is shown in Figure 2.\nDatasets and Prompting Techniques. In this RQ, to reduce the workload of human evaluation volunteers, we randomly select 50 samples from the Java, Python, and C datasets, respectively, which means 150 samples in total. We employ few-shot prompting to adapt the four LLMs to generate summaries for code snippets as recent studies [25], [28], [30] have demonstrated the effectiveness of this prompting technique on code summarization tasks.\n2) Experimental Results.\nHuman Evaluation Results. Table II shows the human evaluation scores for reference summaries and summaries generated by the four LLMs. Observe that the scores of reference summaries in the three datasets are between 3 and 3.5 points, suggesting that the quality of the reference summaries is not very high. Therefore, evaluation methods based on summary-summary/code similarity may not accurately assess the quality of LLM-generated summaries.\nAmong the four LLMs, GPT-4 has the highest scores on the Java and C datasets, and GPT-3.5 attains the highest score on the Python dataset. This suggests that the quality of summaries generated by GPT-3.5 and GPT-4 is relatively high.\nFinding According to human evaluation, the quality of reference summaries in the existing datasets is not particularly high. Summaries from general-purpose LLMs (e.g., GPT-3.5) excel over those from specialized code LLMs (e.g., CodeLlama-Instruct) in quality.\u25c4\nAutomated Evaluation Results. Table III displays the scores of the LLM-generated summaries reported by three categories of automated evaluation methods, and LLM-based evaluation methods. Observed that among the three methods based on summary-summary text similarity, 1) the BLEU-based and ROUGE-L-based methods give StarChat-\u1e9e the highest scores on all three datasets; 2) the METEOR-based method gives StarChat-\u1e9e the highest score (i.e., 18.19) on the Java dataset, while gives CodeLlama-Instruct the highest scores (i.e., 21.64 and 17.29) on the Python and C datasets. Among the four methods based on summary-summary semantic similarity, BERTScore, SBCS, and SBED give the best scores to StarChat-\u1e9e, and USECS gives the best score of 50.69 to CodeLlama-Instruct on the Java dataset. On the Python and C datasets, the four methods consistently give the best scores to CodeLlama-Instruct and StarChat-\u00df, respectively. The summary-code semantic similarity-based method SIDE gives the highest scores (i.e., 80.46 and 66.35) to StarChat-B on the Java and C datasets, and the highest score (i.e., 41.71) to GPT-3.5 on the Python dataset. The four LLM-based methods consistently give the highest scores to GPT-4 on the Java and C datasets, while they consistently award the highest scores to GPT-3.5 on the Python dataset.\nFinding According to automated evaluation, overall, methods based on summary-summary text/semantic similarity tend to give higher scores to specialized code LLMS StarChat-\u1e9e and CodeLlama-Instruct, while LLM-based evaluators tend to give higher scores to general-purpose LLMs GPT-3.5 and GPT-4. The summary-code semantic similarity-based method tends to give higher scores to StarChat-\u1e9e on the Java and C datasets, while favoring GPT-3.5 on the Python dataset.\nCorrelation between Automated Evaluation and Human Evaluation. From Table III, it can also be observed that the average scores of reference summaries evaluated by the four LLM-based methods are mostly below 3 points. It means that similar to human evaluation, LLM-based evaluation methods also believe that the quality of the reference summaries is not very high. Besides, LLM-based evaluation methods are inclined to give higher scores to general-purpose LLMs GPT-3.5 and GPT-4, which is the same as human evaluation.\nBased on the above observations, we can reasonably speculate that compared to methods based on summary-summary text/semantic similarity and summary-code semantic similarity, LLM-based evaluation methods may be more suitable for evaluating the quality of summaries generated by LLMs. Therefore, we follow [9], [73] and calculate Spearman's correlation coefficient \\( \\rho \\) with the p-value between the results of each automated evaluation method and human evaluation, providing more convincing evidence for this speculation. The Spearman's correlation coefficient \\( \\rho \\) \u2208 [-1,1] is suitable for judging the correlation between two sequences of discrete ordinal/continuous data, with a higher value representing a stronger correlation [77]. -1 < \\( \\rho \\) < 0, \\( \\rho \\) = 0, and 0 < \\( \\rho \\) < 1 respectively indicate the presence of negative correlation, no correlation, and positive correlation [78]. The p-value helps determine whether the observed correlation is statistically significant or simply due to random chance. By comparing the p-value to a predefined significance level (typically 0.05), we can decide whether to reject the null hypothesis and conclude that the correlation is statistically significant. Table IV shows the statistical results of \\( \\rho \\) and p-value. It can be clearly observed that among all automated evaluation methods, there is a significant positive correlation (0.28 < \\( \\rho \\) < 0.65) between the GPT-4-based evaluation method and human evaluation in scoring the quality of summaries generated by most LLMs, followed by the GPT-3.5-based evaluation method. For other automated evaluation methods, in most cases, their correlation with human evaluation is negative or weakly positive. Based on the above observations, we draw the conclusion that compared with other automated evaluation methods, the GPT-4-based method is more suitable for evaluating the quality of summaries generated by LLMs. In the subsequent RQs, we uniformly employ the GPT-4-based method to assess the quality of LLM-generated summaries. To make the output scores of GPT-4 more deterministic, we set the temperature to 0 when using GPT-4 as the evaluator.\nSummary Among all automated evaluation methods, the GPT-4-based method overall has the strongest correlation with human evaluation. Therefore, it is recommended to adopt the GPT-4-based method to evaluate the quality of LLM-generated summaries.\nB. RQ2: How effective are different prompting techniques in adapting LLMs to the code summarization task?\n1) Experimental Setup. The experimental dataset comprises 600 samples from Java, Python, and C datasets collectively.\n2) Experimental Results. Table V presents the scores reported by the GPT-4 evaluation method for summaries generated by four LLMs using five prompting techniques. Observe that when the base model is CodeLlama-Instruct, few-shot prompting consistently performs best on all three datasets. When the base model is StarChat-\u1e9e, chain-of-thought prompting performs best on all the Java and C datasets, while expert prompting excels on the Python dataset. When selecting GPT-3.5 as the base model, the simplest zero-shot prompting surprisingly achieves the highest scores on the Java and C datasets, and is only slightly worse than few-shot prompting on the Python dataset. When using GPT-4 as the base model, chain-of-thought prompting overall performs best.\nFor the specific LLM and programming language, there is no guarantee that intuitively more advanced prompting techniques will surpass simple zero-shot prompting. For example, on the Java dataset, when selecting any of StarChat-\u1e9e, GPT-3.5, and GPT-4 as the base model, few-shot prompting yields lower scores than zero-shot prompting. Contrary to the findings of previous studies [25], [28], the GPT-4-based evaluation method does not consider that few-shot prompting will improve the quality of generated summaries. This discrepancy may arise because previous studies evaluated the quality of LLM-generated summaries using BLEU, METEOR, and ROUGE-L, which primarily assess text/semantic similarity with reference summaries. However, as we mentioned in Section IV-A, reference summaries contain low-quality noisy data that undermines their reliability. Therefore, achieving greater similarity with reference summaries does not necessarily imply that the human/GPT-4-based evaluation method will perceive the summary to be of higher quality.\nSummary The more advanced prompting techniques expected to perform better may not necessarily outperform simple zero-shot prompting. In practice, selecting the appropriate prompting technique requires considering the base LLM and the programming language.\nC. RQ3: How do different model settings affect LLMs' code summarization performance?\n1) Experimental Setup. There are three key model settings/parameters, including top_k, top_p, and temperature, that allow the user to control the randomness of text (code summary in our scenario) generated by LLMs. Considering that GPT-3.5 and GPT-4 do not support the top_k setting, we only conduct experiments with the top_p and temperature.\nTop_p: In each round of token generation, LLMs sort tokens by probability from high to low and keep tokens whose probability adds up to (no more than) top_p. For example, top_p = 0.1 means only the tokens comprising the top 10% probability mass are considered. The larger the top_p is, the more tokens are sampled. Thus tokens with low probabilities have a greater chance of being selected, so the summary generated by LLMs is more random.\nTemperature: Temperature adjusts the probability of tokens after top_p sampling. The higher the temperature, the less the difference between the adjusted token probabilities. Therefore, the token with a low probability has a greater chance of being selected, so the generated summary is more random. If the temperature is set to 0, the generated summary is the same every time.\nTop_p and temperature are alternatives and one should only modify one of the two parameters at a time [79]. Therefore, the questions we want to answer are: (1) Does top_p/temperature impact the quality of LLM-generated summaries? (2) As alternative parameters that both control the randomness of LLMs, do top_p and temperature have a difference in the degree of influence on the quality of LLM-generated summaries?\nDrawing from a review of related work (see Section II), we find that existing LLM-based code summarization studies pay more attention to few-shot prompting. Since no prompting technique outperforms others on all LLMs, we uniformly employ few-shot prompting in RQ3, RQ4, and RQ5 to facilitate comparing our findings with prior studies.\n2) Experimental Results. TABLE VI shows the scores evaluated by the GPT-4 evaluation method for the summaries generated by LLMs under different top_p and temperature settings. It is observed that the impact of top_p and temperature on the quality of LLM-generated summaries is specific to the base LLM and programming language. For example, when top_p=0.5, as temperature increases, the quality of GPT-4-generated summaries for Python code snippets increases, while those for C code snippets decrease. Another example is that when top_p=0.5, as the temperature rises, the quality of GPT-4-generated Java comments first increases and then decreases, whereas CodeLlama-Instruct is exactly the opposite, first decreases and then increases. Regarding the difference in influence between top_p and temperature, it is observed that in most cases the influence of the two parameters is similar. For example, for C code snippets, when one parameter (top_p or temperature) is fixed, as the other parameter (temperature or top_p) grows, the quality of GPT-3.5-generated summaries first decreases and then increases.\nSummary The impact of top_p and temperature on the quality of generated summaries is specific to the base LLM and programming language. As alternative parameters, top_p and temperature have similar influence on the quality of LLM-generated summaries.\nD. RQ4: How do LLMs perform in summarizing code snippets written in different types of programming languages?\n1) Experimental Setup. We conduct experiments on all 10 programming language datasets. As in RQ3, we uniformly employ few-shot prompting to adapt LLMs.\n2) Experimental Results. Table VII shows the performance evaluated by the GPT-4 evaluation method for the four LLMs on five types of programming languages. It is observed that for OOP (i.e., Java), GPT-4 performs best, followed by CodeLlama-Instruct, GPT-3.5, and StarChat-8. For PP, GPT-4 performs best on both C and Go, while StarChat-\u1e9e per-forms worst on both. The smallest LLM CodeLlama-Instruct outperforms GPT-3.5 on C (3.91 vs. 3.56), but vice versa on Go (3.86 vs. 4.14). Additionally, except for CodeLlama-Instruct, which performs slightly worse on Go than on C (3.86 vs. 3.91), the other three LLMs perform better on Go than on C. For SP, GPT-4 consistently performs best on all four languages. Surprisingly, CodeLlama-Instruct outperforms GPT-3.5 on both Ruby and JavaScript. All four LLMs perform better on PHP than on Python. For FP, the performance of two specialized code LLMs (i.e., CodeLlama-Instruct and StarChat-8) is better on Haskell than on Erlang, while the opposite is true for the two general-purpose LLMs (i.e., GPT-3.5 and GPT-4). For LP, GPT-4 still performs best, followed by GPT-3.5, CodeLlama-Instruct, and StarChat-8. Across all five types of languages, the four LLMs consistently perform the worst on LP, which indicates that summarizing logic programming language code is the most challenging. One possible reason is that fewer Prolog datasets are available for training these LLMs compared to other programming languages. The scale of the Prolog dataset we collected can support this reason.\nSummary \u25ba GPT-4 surpasses the other three LLMs on all five types of programming languages. For PP, LLMs overall perform better on Go than on C. For SP, all four LLMs perform better on PHP than on Python. For FP, specialized code LLMs (e.g., StarChat-\u1e9e) perform better on Haskell than on Erlang, whereas the reverse is true for general-purpose LLMs (e.g., GPT-4). All four LLMs perform worse in summarizing LP code snippets.\nE. RQ5: How do LLMs perform on different categories of summaries?\n1) Experimental Setup. Following [3], [54], [55], we classify code summaries into the following six categories.\nWhat: describes the functionality of the code snippet. It helps developers to understand the main functionality of the code without diving into implementation details. An example is \"Pushes an item onto the top of this stack\".\nWhy: explains the reason why the code snippet is written or the design rationale of the code snippet. It is useful when methods' objective is masked by complex implementation. An application scenario of Why summaries is to explain the design rationale of overloaded functions.\nHow-it-is-done: describes the implementation details of the code snippet. Such information is critical for developers to understand the subject, especially when the code complexity is high. For instance, \"Shifts any subsequent elements to the left.\" is a How-it-is-done comment.\nProperty: asserts properties of the code snippet, e.g., function's pre-conditions/post-conditions. \"This method is not a constant-time operation.\" is a Property summary.\nHow-to-use: describes the expected set-up of using the code snippet, such as platforms and compatible versions. For example, \"This method can be called only once per call to next().\" is a How-to-use summary.\n1) Experimental Results. Table IX presents the results evaluated by the GPT-4 evaluation method for the four LLMs in generating five categories of summaries. Observe that CodeLlama-Instruct performs worse in generating How-it-is-done summaries than generating the other four categories of summaries. StarChat-8 gets the lowest score of 2.52 in generating How-to-use summaries. Both GPT-3.5 and GPT-4 are not as good at generating Property summaries compared to generating other categories of summaries.\nSurprisingly, the smallest LLM CodeLlama-Instruct slightly outperforms the advanced GPT-4 in generating Why (4.29 vs. 4.28) and Property (4.19 vs. 4.06) summaries. Additionally, compared with GPT-3.5, CodeLlama-Instruct achieves higher scores in generating What, Why, and Property summaries. Certainly, it is undeniable that the reason for this phenomenon is that the optimal prompting technique for GPT-3.5 and GPT-4 is not few-shot prompting. This phenomenon is also exciting because it implies that most ordinary developers or teams who lack sufficient resources (e.g., GPUs) have the opportunity to utilize open-source and small-scale LLMs to achieve code summarization capabilities close to (or even surpass) those of commercial gigantic LLMs.\nSummary The four LLMs excel in generating different categories of summaries. The smallest CodeLlama-Instruct slightly outperforms the advanced GPT-4 in generating Why and Property summaries. StarChat-\u1e9e is not proficient at generating How-to-use summaries. GPT-3.5 and GPT-4 perform worse in generating Property summaries than other categories of summaries."}, {"title": "V. THREATS TO VALIDITY", "content": "Our empirical study may contain several threats to validity that we have attempted to relieve.\nThreats to External Validity. The threats to external validity lie in the generalizability of our findings. One threat to the validity of our study is that LLMs usually generate varied responses for identical input across multiple requests due to their inherent randomness, while conclusions drawn from random results may be misleading. To mitigate this threat, considering that StarChat-\u1e9e and CodeLlama-Instruct do not support setting the temperature to 0, we uniformly set it to 0.1 to reduce randomness except for RQ3. In RQ2-RQ5, to make the evaluation scores more deterministic, we set the temperature to 0 when using GPT-4 as the evaluator. Additionally, for other RQs, we conduct experiments on multiple programming languages to support our findings.\nThreats to Internal Validity. A major threat to internal validity is the potential mistakes in the implementation of metrics and models. To mitigate this threat, we use the publicly available code from previous studies [10], [47] for BLEU, METEOR, ROUGE-L, and SIDE. For COIN, BERTScore, SentenceBert, Universal Sentence Encoder, StarChat-8 [80] and CodeLlama-Instruct [81], and GPT-3.5/GPT-4 [82], we use the script provided along with the model to run.\nAnother threat lies in the processing of LLM's responses. Usually, the output of LLMs is a paragraph, not a sentence of code summary (code comment) that we want. The real code summary may be the first sentence in the LLMs' response, or it may be returned in the comment before the code such as \"/** (code summary) */\", etc. Therefore, we designed a series of heuristic rules to extract the code summary. We have made our script for extracting code summaries from LLMs' responses public for the community to review."}, {"title": "VI. CONCLUSION", "content": "In this paper, we provide a comprehensive study covering multiple aspects of code summarization in the era of LLMs. Our interesting and significant findings include, but are not limited to, the following aspects. 1) Compared with existing automated evaluation methods, the GPT-4-based evaluation method is more fitting for assessing the quality of LLM-generated summaries. 2) The advanced prompting techniques anticipated to yield superior performance may not invariably surpass the efficacy of straightforward zero-shot prompting. 3) The two alternative model settings have a similar impact on the quality of LLM-generated summaries, and this impact varies by the base LLM and programming language. 4) LLMs exhibit inferior performance in summarizing LP code snippets. 5)"}]}