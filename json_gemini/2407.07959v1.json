{"title": "Source Code Summarization in the Era of Large Language Models", "authors": ["Weisong Sun", "Yun Miao", "Yuekang Li", "Hongyu Zhang", "Chunrong Fang", "Yi Liu", "Gelei Deng", "Yang Liu", "Zhenyu Chen"], "abstract": "To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of code-related tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLM-based code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types (e.g., procedural and object-oriented programming languages). Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code implementation details and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Code comments are vital for enhancing program compre-hension [1] and facilitating software maintenance [2]. While it is considered good programming practice to write high-quality comments, the process is often labor-intensive and time-consuming [2]\u2013[4]. As a result, high-quality comments are frequently absent, mismatched, or outdated during soft-ware evolution, posing a common problem in the software industry [5]\u2013[8]. Automatic code summarization (or simply, code summarization), a hot research topic [9]\u2013[12], addresses this challenge by developing advanced techniques/models for automatically generating natural language summaries (i.e., comments) for code snippets, such as Java methods or Python functions, provided by developers.\nRecently, with the success of large language models (LLMs) in natural language processing (NLP) [13], [14], an increasing number of software engineering (SE) researchers have started integrating them into the resolution process of various SE tasks [15]\u2013[18], such as code generation [19], [20], program repair [21], [22], and vulnerability detection/localization [23], [24]. In this study, we focus on the application of LLMs on the code summarization tasks. Figure 1 shows the general work-flow of LLM-based code summarization and its effectiveness evaluation methods. In the summary generation process, the input consists of a code snippet and the expected summary category. The input is passed to a prompt generator equipped with various prompt engineering techniques (referred to as prompting technique), which constructs a prompt based on input. This prompt is then used to instruct LLMs to generate a summary of the expected type for the input code snippet. In the summary evaluation process, a common method used to automatically assess the quality of LLM-generated summaries is to compute the text or semantic similarity between the LLM-generated summaries and the reference summaries.\nThere have been several recent studies investigating the effectiveness of LLMs in code summarization tasks [25]\u2013[29]. These studies can help subsequent researchers rapidly under-"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Code summarization is the task of automatically generating natural language summaries (also called comments) for code snippets. Such summaries serve various purposes, including but not limited to explaining the functionality of code snippets [8], [28], [41]. The research on code summarization can be traced back to as early as 2010 when Sonia Haiduc et al. [42] introduced automated text summarization technology to summarize source code. Later on, following the significant success of neural machine translation (NMT) research in the field of NLP [43], [44], a large number of researchers"}, {"title": "III. STUDY DESIGN", "content": "This study aims to answer the following research questions:\nRQ1: What evaluation methods are suitable for assessing the quality of summaries generated by LLMs? Existing research on LLM-based code summarization [25], [28], [30] widely follow earlier studies [32], [36] and employ automated evaluation metrics (e.g., BLEU) to evaluate the quality of LLM-generated summaries. However, recent studies [27], [50] have shown that LLM-generated summaries surpass reference summaries in quality. Therefore, evaluating LLM-generated summaries based on their text or semantic similarity to ref-erence summaries may not be appropriate. This RQ aims to discover a suitable method for automated assessment of the quality of LLM-generated summaries.\nRQ2: How effective are different prompting techniques in adapting LLMs to the code summarization task? This RQ aims to unveil the effectiveness of several popular prompting techniques (e.g., few-shot and chain-of-thought) in adapting LLMs to code summarization tasks.\nRQ3: How do different model settings affect LLMs' code summarization performance? To better meet diverse user needs, LLMs typically offer configurable parameters (i.e., model settings) that allow users to control the randomness of model behaviour. In this RQ, we adjust the randomness of the generated summaries by modifying LLMs' parameters and see the impact of different model settings on the performance of LLMs in generating code summaries.\nRQ4: How do LLMs perform in summarizing code snippets written in different types of programming languages? Programming languages are diverse in types (e.g., object-oriented and functional programming languages), with their implementations of the same functional requirements being similar or entirely different. The scale of programs implemented with them in Internet/open-source repositories also varies, which may result in differences in the mastery of knowledge of these languages by LLMs. Hence, this RQ aims to reveal the differences in LLMs' capabilities to summarize code snippets across diverse programming language types.\nRQ5: How do LLMs perform on different categories of summaries? Previous research [3], [54], [55] has shown that summaries can be classified into various categories according to developers' intentions, including What, Why, How-to-use-it, How-it-is-done, Property, and others. Therefore, in this RQ, we aim to explore the ability of LLMs to generate summaries of different categories.\nWe select four LLMs as experimental representatives.\nCodeLlama-Instruct. Code Llama [56] is a family of LLMs for code based on Llama 2 [57]. It provides multiple fla-vors to cover a wide range of applications: foundation models, Python specializations (Code Llama-Python), and instruction-following models (Code Llama-Instruct) with 7B, 13B, and 34B parameters. Our study utilizes Code Llama-Instruct-7B.\nStarChat-8. StarChat-8 [58] is an LLM with 16B param-eters fine-tuned on StarCoderPlus [59]. Compared with Star-CoderPlus, StarChat-\u1e9e excels in chat-based coding assistance.\nGPT-3.5. GPT-3.5 [60] is an LLM provided by OpenAI. It is trained with massive texts and codes. It can understand and generate natural language or code.\nGPT-4. GPT-4 is an improved version of GPT-3.5, which can solve difficult problems with greater accuracy. OpenAI has not disclosed the specific parameter scale of GPT- 3.5 and GPT-4. Our study uses gpt-3.5-turbo and gpt-4-1106-preview.\nApart from RQ3 where we investigate the impact of model settings, we uniformly set the temperature to 0.1 to minimize the randomness of LLM's responses and highlight the impact of evaluation methods/prompting tech-niques/programming language types/summary categories.\nWe compare five commonly used prompting techniques below.\nZero-Shot. Zero-shot prompting adapts LLMs to down-stream tasks using simple instructions. In our scenario, the input to LLMs consists of a simple instruction and a code snippet to be summarized. We expect LLMs to output a natural language summary of the code snippet. Therefore, we"}, {"title": "IV. RESULTS AND FINDINGS", "content": "The text or semantic similarity-based method tends to give higher scores to StarChat-\u1e9e on the Java and C datasets, while favoring GPT-3.5 on the Python dataset.\nWe conduct experiments on all 10 programming language datasets. In the experiments for RQ5, we only use the Java dataset because other programming languages lack readily available comment classifiers. While training such classifiers would be valuable, it falls outside the scope of this paper and is left for future exploration."}, {"title": "V. THREATS TO VALIDITY", "content": "Our empirical study may contain several threats to validity that we have attempted to relieve. \nThreats to External Validity. The threats to external validity lie in the generalizability of our findings. One threat to the validity of our study is that LLMs usually generate varied responses for identical input across multiple requests due to their inherent randomness, while conclusions drawn from random results may be misleading. To mitigate this threat, considering that StarChat-\u1e9e and CodeLlama-Instruct do not support setting the temperature to 0, we uniformly set it to 0.1 to reduce randomness except for RQ3. In RQ2-RQ5, to make the evaluation scores more deterministic, we set the temperature to 0 when using GPT-4 as the evaluator. Additionally, for other RQs, we conduct experiments on multiple programming languages to support our findings. \nThreats to Internal Validity. A major threat to internal validity is the potential mistakes in the implementation of metrics and models. To mitigate this threat, we use the publicly available code from previous studies [10], [47] for BLEU, METEOR, ROUGE-L, and SIDE. For COIN, BERTScore, SentenceBert, Universal Sentence Encoder, StarChat-8 [80] and CodeLlama-Instruct [81], and GPT-3.5/GPT-4 [82], we use the script provided along with the model to run.\nAnother threat lies in the processing of LLM's responses. Usually, the output of LLMs is a paragraph, not a sentence of code summary (code comment) that we want. The real code summary may be the first sentence in the LLMs' response, or it may be returned in the comment before the code such as \"/** (code summary) */\", etc. Therefore, we designed a series of heuristic rules to extract the code summary. We have made our script for extracting code summaries from LLMs' responses public for the community to review."}, {"title": "VI. CONCLUSION", "content": "In this paper, we provide a comprehensive study covering multiple aspects of code summarization in the era of LLMs. Our interesting and significant findings include, but are not limited to, the following aspects. 1) Compared with existing automated evaluation methods, the GPT-4-based evaluation method is more fitting for assessing the quality of LLM-generated summaries. 2) The advanced prompting techniques anticipated to yield superior performance may not invariably surpass the efficacy of straightforward zero-shot prompting. 3) The two alternative model settings have a similar impact on the quality of LLM-generated summaries, and this impact varies by the base LLM and programming language. 4) LLMs exhibit inferior performance in summarizing LP code snippets. 5)"}]}