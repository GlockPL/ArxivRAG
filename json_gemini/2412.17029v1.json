{"title": "GRAPHAGENT: AGENTIC GRAPH LANGUAGE ASSISTANT", "authors": ["Yuhao Yang", "Jiabin Tang", "Lianghao Xia", "Xingchen Zou", "Yuxuan Liang", "Chao Huang"], "abstract": "Real-world data is represented in both structured (e.g., graph connections) and unstructured (e.g., textual, visual information) formats, encompassing complex relationships that include explicit links (such as social connections and user behaviors) and implicit interdependencies among semantic entities, often illustrated through knowledge graphs. In this work, we propose GraphAgent, an automated agent pipeline that addresses both explicit graph dependencies and implicit graph- enhanced semantic inter-dependencies, aligning with practical data scenarios for predictive tasks (e.g., node classification) and generative tasks (e.g., text genera- tion). GraphAgent comprises three key components: (i) a Graph Generator Agent that builds knowledge graphs to reflect complex semantic dependencies; (ii) a Task Planning Agent that interprets diverse user queries and formulates corresponding tasks through agentic self-planning; and (iii) a Task Execution Agent that efficiently executes planned tasks while automating tool matching and invocation in response to user queries. These agents collaborate seamlessly, integrating language models with graph language models to uncover intricate relational information and data semantic dependencies. Through extensive experiments on various graph-related predictive and text generative tasks on diverse datasets, we demonstrate the effec- tiveness of our GraphAgent across various settings. We have made our proposed GraphAgent open-source at: https://github.com/HKUDS/GraphAgent.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world information exists in a complex ecosystem of interconnected data types. Structured data, particularly graph-based connections, captures explicit relationships such as social networks and user interaction patterns (Fey et al., 2023). Complementing this, unstructured data - including text and visual content - reveals implicit semantic relationships between entities (Zhong & Mottin, 2023). The integration of these diverse data formats has become crucial for modern applications, as it enables more comprehensive and nuanced analysis of complex real-world scenarios (Lu et al., 2024).\nGraph serves as an effective means of representing relational information across various domains. In academic networks, papers are interconnected through explicit citations, with each paper represented as a node in a graph and edges indicating these citations Chen et al. (2023); Wang et al. (2022). This structure enables researchers to trace the influence of one paper on another, showcasing the evolution of ideas. Additionally, the papers' content provides unstructured data for analyzing themes, methodologies, and findings. By integrating structured citation data with unstructured text, researchers can identify trends and derive valuable insights, leading to applications such as knowledge summaries and scientific question-answering, which can be framed as Graph-enhanced Text Generative Tasks.\nIn e-commerce scenarios, customer interactions form structured behavior data that can be analyzed in conjunction with unstructured data sources, such as product reviews and descriptions Shuai et al. (2022); Li et al. (2023). This integrated approach enables businesses to gain deeper insights into consumer behavior patterns and improve recommendation accuracy. Specifically, by integrating user behavior graphs with rich textual information, these user-item interaction forecasting challenges can be effectively approached as Graph-related Predictive Tasks.\nExisting graph learning methods have become essential frameworks for analyzing and learning from graph data (Hamilton, 2020). These methods focus on learning embeddings for nodes and edges, mapping structural information into a latent representation space (Yang et al., 2020). Among these, Graph Neural Networks (GNNs) stand out as state-of-the-art (SOTA) approaches (Dai et al., 2022; Liu et al., 2022). GNNs employ a message-passing mechanism that allows nodes to exchange information with their neighbors, effectively capturing the graph's structural characteristics and enhancing representation learning. However, they primarily focus on explicit graph connections, often neglecting the complex semantic dependencies associated with linked textual data. Additionally, GNNs generally have limited generalization capabilities for real-world graph mining tasks (Xia &\nHuang, 2024; Mao et al., 2024). They often require training task-specific models, which complicates automation and reduces effectiveness in zero-shot scenarios. In practical applications, the ability to process both structured and unstructured data, particularly with unseen new data, is crucial.\nInspired by the recent success of large language models (LLMs), researchers are striving to en- hance the generalization capabilities of graph learning models by enabling LLMs to compre- hend graph structural information. Notable ex- amples include GraphGPT (Tang et al., 2024a) and LLaGA (Chen et al., 2024a), which convert graph-structured data into tokens suitable for LLM input. However, these approaches are pri- marily designed for conventional graph learning tasks, such as node classification and link pre- diction. This narrow focus limits their broader application in effectively handling both struc- tured and unstructured data in a more flexible and efficient manner. In light of these limitations, an important question arises: How can we empower individuals without any background in graph theory or machine learning to analyze their graph data using natural language and obtain the desired predictions and insights?\nThe Presented Work. In this paper, we aim to establish a fully automated analysis framework capable of handling a wide variety of data types, including both structured and unstructured data. Our framework, GraphAgent, is designed to address diverse user needs, encompassing both graph-related predictive and generative tasks. Built on an agentic architecture, GraphAgent allows users to interact with it using natural language. This intuitive and comprehensive approach thoroughly empowers all individuals to obtain predictions and insights from graph-structured data, tailored to their specific requirements, without requiring specialized knowledge in graph learning.\nTo achieve our objective, several key challenges must be addressed: i) Constructing Potential Semantic Relationships: How to derive latent semantic connections from complex data. ii) Au- tomating Query Understanding and Task Formulation: How to automatically interpret user query prompts, formulate them into specific tasks (e.g., predictive or generative tasks), and effectively plan those tasks. iii) Efficient Task Execution: How to accurately and effectively implement the formulated tasks and return correct results. To tackle these challenges, our proposed model is designed with an advanced framework comprising three main components: a Graph Generator Agent that constructs Semantic Knowledge Graphs (SKGs) from user text, a Task Planning Agent that interprets queries and formulates tasks, and a Graph Action Agent that automates the task execution.\nTo summarize, this work presents the following contributions:\n\u2022 Complex Practical Data Integration. Our framework provides robust handling of real-world scenarios by seamlessly merging structured and unstructured data with graph-based entity rela- tionships. This unified approach enables dual capabilities - supporting both predictive analytics and text generation tasks. By allowing natural language interactions, users can directly query and analyze complex data structures, streamlining information extraction and improving accessibility.\n\u2022 Multi-Agent Workflow. This work introduces GraphAgent, an advanced automated graph lan- guage assistant that enhances the integration of structured and unstructured data analysis. It autonomously constructs semantic knowledge graphs (SKGs) from text, formulates predictive and generative tasks from user queries, and efficiently executes these tasks. This seamless collabora-"}, {"title": "2 METHODOLOGY", "content": "2.1 PRELIMINARIES\nGraph-empowered Agents. Our GraphAgent proposes an automated agentic pipeline that addresses graph predictive and text generation tasks. It can be formulated as y = f(0; LLM), where the agentic function f(\u00b7) incorporates an Observation O that includes both the structured data (e.g., explicit graph connections) or unstructured data (e.g., textual information). The agent then produces an Action y, which can involve predictions (e.g., node classifications) or text generation tasks (e.g., summarizing text with implicit entity interdependencies). The workflow of GraphAgent leverages the capabilities of LLMs to enhance its effectiveness in both predictive and generative tasks.\nGraph-Structured Data. In our GraphAgent, both structured and unstructured data are represented as graphs, differing only in the explicitness or implicitness of the entity-wise relationships. To accommodate the diversity of graph data, we utilize heterogeneous graphs to represent the input data. Specifically, a heterogeneous graph is denoted as G = (V,E,N,R), where V is the set of all entities, and & is the set of all edges connecting pairs of entities. The sets N and R represent the types of nodes and edges, respectively. For each edge, a meta-type attribute can be retrieved in the form (nh, ri, nt), denoting the meta-types of the head node nh, relation ri, and tail node nt, respectively.\n2.2 GRAPH GENERATION AGENT\nTo uncover the rich contextual information within unstructured data, GraphAgent designs a Graph Generation Agent that automatically constructs meaningful Semantic Knowledge Graphs (SKGs) from any type of textual input. For example, for a paper abstract that includes the sentence, \u201cCon- trastively trained text-image models have the remarkable ability to perform zero-shot classification\", the model can extract relevant entity nodes such as \u201ctext-image models\u201d and \u201czero-shot classification\u201d.\nIterative Two-Phase Graph Generation Workflow. To capture complex implicit entity-wise dependencies, our graph generation agent operates through an automated two-phase workflow: (1) Scaffold Knowledge Entity Extraction and (2) Knowledge Description Augmentation. The first phase is dedicated to identifying key knowledge entities or concepts referred to as scaffold knowledge nodes from the provided text, regardless of its format. Specifically, this phase can be formulated as:\n\n$\\begin{equation} \nSaffold^{k=0} = LLM(x_{sys\\_sk}, g_s), \n\\end{equation}\n\nwhere gs represents the input unstructured text data, while xsys_sk denotes the system prompt for extracting scaffold knowledge nodes. We adopt an iterative approach to graph generation to capture both high-level and fine-grained semantic dependencies among multi-grained entities. For example, in an academic paper, high-level entities might include \"Machine Learning,\" while fine-grained entities could be \"Self-Supervised Learning\" and \"Graph Neural Network\". Specifically, $V_{scaffold}^{k=0}$ refers to the generated vertices during the initial iteration (k = 0).\nThe second phase of knowledge augmentation centers on enhancing and enriching the textual descriptions of the generated entity nodes to ensure accurate, comprehensive, and contextually appropriate language modeling. This critical step ensures that each entity is represented with sufficient detail and semantic clarity. Formally, we define this phase as follows:\n\n$\\begin{equation} \nC_{scaffold}^{k=0} = LLM(x_{sys\\_ka}, g_s, V_{scaffold}).\n\\end{equation}\n\nwhere $C_{scaffold}^{k=0}$ denotes the node-specific descriptions, while xsys_ka denotes the system prompt for knowledge augmentation. To iteratively execute this two-phase workflow, GraphAgent uses the textual augmentation output from the previous round as the implicit graph input for the next round:\n\n$\\begin{equation} \nV_{scaffold}^{k=j} = LLM(x_{sys\\_sk}, C_{scaffold}^{k=j}).\n\\end{equation}\n\n$\\begin{equation} \nC_{scaffold}^{k=j} = LLM(x_{sys\\_ka}, C_{scaffold}^{k=j}, V_{scaffold}).\n\\end{equation}\n\nWe then merge the nodes and descriptions generated across different iterations to form the final node set: Vskg = UV caffold and Cskg = UCrcaffold. The relationships among these nodes, denoted as Eskg, are established based on their derivation: if a new node is generated from the textual description of a node in the previous iteration, we connect these two nodes in the semantic knowledge graph. The system prompts used for graph generation are detailed in Table 6, which is presented in the Appendix.\n2.3 TASK PLANNING AGENT\nWith both structured and unstructured data represented as graphs, GraphAgent employs a task planning agent to automatically interpret user queries and transform the graph data into a unified embedding structure. This facilitates easier utilization by the subsequent predictive and generative modules. Input-output examples of the task planning agent is provided in Table 3 in the Appendix.\n2.3.1 Intent Identification and Task Formulation\nThe task planning agent is initially tasked with formulating meaningful predictive or generative tasks based on the user query prompt. Given a user query prompt Xusr_p and a predefined system prompt for task parsing xsys_tp, the task planning agent formulates the intended task as follows:\n\n$\\begin{equation} \ng_s, X_{usr\\_ann},t_{usr} = LLM(x_{sys\\_tp}, X_{usr\\_p}),\n\\end{equation}\n\nThis intent identification and task formulation procedure generates three fundamental types of task attributes within our agent architecture, which is specifically defined as follows:\n\u2022 Source graph gs represented by formatted files, textual graph descriptions, or plain documents.\n\u2022 Task type tusr is inferred from the query prompt and can be one of \"predictive_predefined\", \"predictive_wild\", or \"open_generation\". This task type symbol is used to automatically select system prompt templates during training or inference for different tasks.\n\u2022 User annotation Xusr_ann includes additional task information, such as task descriptions, label candidates for predictive tasks, and generation requirements for generative tasks.\nTo construct grounded graph tokens that can be understood by the subsequent action agent, the task planning agent follows two stages: i) Graph-Token Grounding-converting graphs with nodes and edges into grounded Python objects; ii) Graph Tokenization-generating tokens from the input that preserve complex interdependencies among graph-structured entities.\n2.3.2 Graph-Token Grounding\nOur framework reads graph nodes and edges and converts them into grounded Python objects using a graph-building and wrapping tool. Notably, our model can handle diverse graph inputs, regardless\""}, {"title": "2.3.3 Graph Tokenization", "content": "The Task Planning Agent converts discrete nodes and edges into embedded representations suitable for action agents based on graph LLMs. This tokenization process consists of two stages: first, encoding the graph into embeddings, and second, retrieving the nodes and their neighbors to create input graph tokens. For the embedding process, we employ a pre-trained text encoder ftext_enc and a pre-trained GNN fgnn. Graph tokens are generated by initially encoding the textual features c of the graph nodes and their meta types using the text encoder, followed by modeling geometric features.\n\n$\\begin{equation} \ne^{text}_i = f_{text\\_encoder} (c_i); e^{str}_{s|r} = f_{text\\_encoder}(C_{s|r})\n\\end{equation}\n\n$\\begin{equation} \ne^{gnn} = f_{gnn}(e^{text}, e^{str}_s, e^{str}_r, V, E).\n\\end{equation}\n\nFor each central node i in our heterogeneous graph, we systematically apply a graph sampling tool to create the subgraph input for the subsequent action agent, which can be formulated as follows:\n\n$\\begin{equation} \n[e^{gnn}_i] = Sampling\\_Tool(G, E^{gnn}, i)\n\\end{equation}\n\n2.4 GRAPH ACTION AGENT\nTo enhance the capabilities of graph encoding and prediction/generation, we incorporate a trainable Graph Action Agent into our GraphAgent framework, based on the Graph LLM architecture (Tang et al., 2024b; Chen et al., 2024a). This Graph Action Agent is specifically trained to optimize performance for both predictive and text generation tasks involving graph data.\n2.4.1 Cross-Task Graph Agent\nThe graph action agent is capable of handling two categories of diverse tasks, as shown below. The details on the system prompt builder and examples of system prompts are shown in Table 6.\n\u2022 Predictive Graph-Language Tasks. These tasks focus on generating predictions based on user prompts, utilizing both structured and unstructured data. Examples include node classification and link prediction for explicit graph data, as well as document classification based on extracted implicit semantic knowledge graphs (SKGs), such as categorizing news articles. When using implicit SKGS to complement explicit graphs, the graph generator agent uses the observed explicit nodes as initial scaffold nodes to build the SKG. Specifically, for these tasks, our model constructs a system prompt that effectively guides the LLM toward task-specific objectives:\n\n$\\begin{equation} \nx_{sys\\_pred\\_i} = f_{sys}(t_{usr}, X_{usr\\_ann}, g_s),\n\\end{equation}\n\nwhere the prompt builder function fsys creates an appropriate system prompt based on the task type and user annotations, incorporating gs for node or graph textual information. The predictive graph-language tasks are then defined as follows:\n\n$\\begin{equation} \ny_{pred}, y_{reasoning} = LLM(x_{sys\\_pred\\_i}, \\{G_{exp}|G_{skg}\\}\n\\end{equation}"}, {"title": "2.4.2 Graph-Instruction Alignment", "content": "To teach our agent in comprehending graph-structured data, we implement graph-instruction align- ment in the initial fine-tuning stage. Inspired by the work of Tang et al. (2024b), we utilize the efficient, effective, and easily scalable task of graph-instruction matching as our alignment target. Specifically, we present a set of graph token-instruction pairs:\n\n$\\begin{equation} \nD^{graph} = [(e_0, s_0), (e_1, s_1), ...]; D^{text} = [(c_0, c_{s_0}), (c_1, c_{s_1}), ...],\n\\end{equation}\n\nwhere (ei, si) denotes the i-th graph token with meta type si, and (ci, cs\u2081) denotes the text description of the i-th graph token and its meta type, correspondingly. We devise two general tasks to achieve fine-grained and comprehensive alignment between the graph tokens and the textual instructions:\n\u2022 Intra-type alignment. This alignment task aims to strengthen the capability of LLMs to interpret graph embedding tokens of certain meta-types through promoting their alignment with the relevant texts. This is conducted by training LLMs to output correct sequence of the texts given a sequence of graph tokens. Specifically, we construct a dataset Dintra with each entry consists of two sequences of graph tokens and texts, separately: dintra = ([(ej, Si), ...], [(ck, cs\u2081), ...]). Then, we train the alignment with a next-token-prediction Cross-Entropy objective as follows:\n\n$\\begin{equation} \nargmin_{\\theta}CE\\_Loss(d^{intra}[0]|LLM(d^{intra}[1])),\n\\end{equation}\n\nwhere \\theta denotes the learnable parameters of the large language model LLM(\u00b7). And indices [0] and [1] indicate the text sequence and the graph token sequence, respectively.\n\u2022 Inter-type alignment. As introducing multiple meta-types in the alignment task can further empower the LLM's comprehension of complex heterogeneous relations, we devise anthor align- ment training objective using inter-type graph tokens. Technically, the dataset Dinter is con- structed by sampling entries that consist graph tokens of different meta-types in the first sequence: dinter = ([(em, sm), (en, Sn), ...], [(Cn, Csn), (Cq, Csq), ...]). Then, the LLM is trained to predict the text sequence and the meta-type sequence of the provided graph tokens:\n\n$\\begin{equation} \nargmin_{\\theta}CE\\_Loss(d^{inter}[0]|d^{inter}[1])).\n\\end{equation}\n\n2.4.3 Agent Task Finetuning\nTo enhance GraphAgent's performance on different agent tasks, we propose to finetune the action agent with diverse graph-language instructions covering different agent tasks. Recall that with the task planning agent we have the user requested task t \u2208 T from the query prompt. For each t in the instruction dataset, we pair it with a special systematic prompt to distinguish between various tasks during training. The systematic prompt contains brief description of the task being handled. Formally, the agent task finetuning dataset is constructed as:\n\n$\\begin{equation} \nD^{multi} = [(\\{(\\Chi_{pred}, \\Chi_{reasoning})|\\Chi_{gen}\\}, \\{G_{exp}|G_{skg}\\}, t_i, a_i)],\n\\end{equation}"}, {"title": "5 CONCLUSION", "content": "This work introduces a multi-agent framework that seamlessly integrates graph-based reasoning with advanced language modeling, effectively addressing complex language assistant scenarios involving both relational and textual data. The proposed GraphAgent features a dynamic pipeline that automates the understanding of graph-enhanced relational and textual semantics for both predictive and generative tasks. The framework consists of three key components: a graph generator agent that uncovers intricate semantic interdependencies, a task planning agent that interprets user queries, and a task execution agent that efficiently carries out tasks. This innovative agentic workflow enhances the adaptability of large language models to diverse datasets and significantly improves performance in benchmarking graph prediction tasks as well as in open-ended text generation tasks. In future work, we plan to extend our framework to incorporate visual information from multi-modal data, enabling it to better understand and generate content that integrates relational, textual and visual elements."}]}