{"title": "PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science", "authors": ["Menglin Liu", "Ge Shi"], "abstract": "Recent advancements in large language models (LLMs) have opened new avenues for enhancing text classification efficiency in political science, surpassing traditional machine learning methods that often require extensive feature engineering, human labeling, and task-specific training. However, their effectiveness in achieving high classification accuracy remains questionable. This paper introduces a three-stage in-context learning approach that leverages LLMs to improve classification accuracy while minimizing experimental costs. Our method incorporates automatic enhanced prompt generation, adaptive exemplar selection, and a consensus mechanism that resolves discrepancies between two weaker LLMs, refined by an advanced LLM. We validate our approach using datasets from the BBC news reports, Kavanaugh Supreme Court confirmation, and 2018 election campaign ads. The results show significant improvements in classification F1 score (+0.36 for zero-shot classification) with manageable economic costs (-78% compared with human labeling), demonstrating that our method effectively addresses the limitations of traditional machine learning while offering a scalable and reliable solution for text analysis in political science.", "sections": [{"title": "1. Introduction", "content": "Text has always been an important data source in political science. Text analysis in political science can be categorized into several key tasks, including classification, scaling, text reuse detection (e.g., hertel2015capturing; hertel2018politics), and natural language processing. Among these, classification is the most common task in text analysis in the field. For instance, researchers might investigate whether campaign ads are positive or negative across different media platforms (fowler2021political), or examine if online posts and newspaper coverage in authoritarian countries highlight local government wrongdoing (Pan and Chen 2018). These examples, along with many others in political science, demonstrate that understanding the nature of politics requires insight into what political actors are saying and writing. With recent advances in theory and practice, political scientists increasingly rely on machine learning (ML) methods to classify large corpora of text by measures such as topic and tone (grimmer2013text)."}, {"title": "Traditional machine learning methods", "content": "Traditional machine learning methods, such as Bag of Words (BoW), Support Vector Machines (SVM), and Bidirectional Encoder Representations from Transformers (BERT), have significantly contributed to text classification. Models like BoW and SVM rely on feature extraction from text or selection of features from correlated data, often requiring manual tuning and domain expertise for optimal performance. While BERT represents a more advanced approach, it demands substantial computational resources and extensive data for fine-tuning. These methods necessitate task-specific training, meaning that each new task requires a fresh round of model training to achieve optimal results. For example, transitioning from topic modeling to sentiment analysis requires training an entirely new model. As a result, these approaches typically rely heavily on"}, {"title": "Large Language Models for Text Classification", "content": "Large Language Models for Text Classification. LLMs represent an evolutionary approach to text annotation in political science. A token, like a word in natural language, is the minimum input to an LLM for processing. Auto-regressive LLMs, like GPT, operate based on auto-regressive principles, where the model generates text one token at a time, predicting the next token based on the previous ones. Unlike traditional models, LLMs can be treated as black-box tools, accessible via commercial APIs, where users can simply create prompts based on heuristics to direct the model's output. This capability makes LLMs suitable for a wide range of text annotation and classification tasks without the need for task-specific training. While convenient to use, the classification accuracy of LLMs with simple heuristic prompts is not guaranteed, particularly for less advanced models. The development of LLMs from weak to advanced models has vastly improved their performances. However, the use of advanced LLMs, such as GPT-4, comes with significant practical limitations, including higher cost, slower inference speed, and stricter token usage limit compared to models like GPT-3.5 (Table 1). These challenges pose significant barriers to the widespread application of LLMs in political science. Our proposed framework aims to address these challenges by providing a more accurate, efficient, and cost-effective approach to leveraging LLMs in this field."}, {"title": "3. Methodology", "content": "In the realm of text classification, classic supervised learning approaches necessitate the availability of a large number of annotated examples, upon which models such as SVM and BERT classifiers are trained. Given a task T, a dataset with input-output pairs {xi, yi}=1, where xi are the text inputs and yi are the corresponding labels, the optimization objective is to find the model parameters \\( \\theta \\) that maximize the likelihood of the correct labels:\n\\[\\max_{\\theta} \\frac{1}{N} \\sum_{i=1}^N P(y_i|x_i, \\theta)\\]\nThese models optimize the likelihood of given labels, adjusting their parameters iteratively to fit the training data. This process, while effective, is often computationally expensive and requires substantial amounts of labeled data to achieve high accuracy.\nIn contrast, we explore the efficacy of in-context learning , particularly in the context of using auto-regressive large language models (LLM) like Generative Pre-trained Transformer (GPT) for text classification tasks. In-context learning refers to the process where a language model makes predictions based on the context provided by a sequence of input tokens, without updating the model parameters. Instead of updating model parameters \\( \\theta \\), the optimization objective of in-context learning for the LLM is to maximize the log-likelihood of the correct class label y given the prompt and query text input x:\n\\[\\max_{\\text{prompt}} \\frac{1}{N} \\sum_{i=1}^N \\log P(y_i|x_i, \\text{prompt}; \\theta)\\]\nThe prompt is natural language text that describes the instructions from humans guiding what the LLM should do. It is usually defined as merely a task description (zero-shot learning) or a task description augmented by a few input-output pairs examples (few-shot learning).\n\\[\\text{prompt} = \\{(x_1, y_1), ..., (x_k, y_k), \\text{task\\_description}\\}\\]\nIn zero-shot learning, LLMs make predictions based on common knowledge injected by extensive pre-training conducted by service providers such as OpenAI, Meta, and Google. However, it may not be pertinent enough for a custom user-defined task. Few-shot learning addresses the challenge of injecting special knowledge by leveraging a small number of annotated examples to augment the prompt. The model uses the context provided in the input (e.g., examples of input-output pairs) to make predictions for a new query input. It can be formalized as, given a prompt that includes k (k < 10) examples of input-output pairs \\( \\{(x_1, y_1), (x_2, y_2), ..., (x_k, y_k) \\} \\) and a new input x, the goal is to predict the corresponding output y.\n\\[P(y|x, \\text{prompt}; \\theta)\\]\nNormally, the tradition of designing prompts heavily relies on human heuristics which is usually referred to as prompt engineering. Users create prompts that guide the"}, {"title": "3.1 Pre-process", "content": "LLMs face a significant constraint in their context window size, limiting the number of examples that can be used in few-shot learning scenarios. Despite this limitation, there's often a need to incorporate knowledge from a broader set of examples to enhance the LLM's performance. To address this challenge, we propose a novel approach: first, we select a pool of M representative examples for human labeling, where k < M < N. Then, at inference time when calling the commercial API to classify unlabeled query text, we dynamically choose the best k examples to augment the prompt. In the \u201cpre-process\u201d stage, we curate such an exemplar pool for human labeling. This curated pool serves two crucial purposes: it allows the LLM to infer the underlying rules used for labeling, and it enables adaptive selection of the most relevant examples for each query text. By creating this initial pool of human-annotated examples, we lay the foundation for more effective and efficient use of the LLM in subsequent stages, balancing the need for comprehensive knowledge injection with the constraints of the model's context window. In this stage, we prepare this exemplar pool, and here are the steps."}, {"title": "Converting texts into embeddings", "content": "Converting texts into embeddings. With natural language inputs, LLMs can convert them into numerical embedding vectors of uniform size. We apply this process to all unlabeled texts in the dataset using embedding models such as \u201ctext-embedding-3-small\" from OpenAI. Based on relevant literature , we use cosine distance to measure the similarity between embedding pairs, providing a quantitative basis for comparing text semantics.\n\\[\\text{cosine\\_distance}(x_i, x_j) = 1 \u2013 \\frac{x_i \\cdot x_j}{||x_i|| ||x_j||}\\]"}, {"title": "Feature reduction with UMap", "content": "Feature reduction with UMap. Uniform Manifold Approximation and Projection (UMap) is a powerful dimensionality reduction technique that preserves data relationships in lower-dimensional space. We employ UMAP with cosine distance for feature reduction of the embeddings, which offers two key"}, {"title": "Exemplar selection", "content": "To create a diverse and representative pool of examples, we employ an exemplar selection method based on the manifold structure of the reduced embeddings. While various approaches exist for exemplar or prototype selection, such as set cover algorithms and density-based sampling, we opt for the k-means selector due to its simplicity and effectiveness. This method involves performing k-means clustering on the embeddings and then designating the text whose embedding is nearest to each cluster center as an exemplar. This approach ensures that every distinct group within the data is represented by one example, allowing for easy control over the number of exemplars selected while maintaining a comprehensive coverage of the embedding space. We keep M such texts as an exemplar pool and engage a human expert to label them. Typically, M is less than 100, requiring minimal human effort."}, {"title": "3.2 In-process", "content": "In this stage, we use the exemplar pool to improve both the task description and few-shot prompt. Then, ask LLM to perform text classification with this prompt."}, {"title": "Enhanced task description generation", "content": "To enhance the initial task description, we leverage the LLM's analytical capabilities on the labeled exemplar pool. For each input-output pair from the pool, the LLM examines the rationale behind the human-assigned label. We then employ a Map-Reduce approach, where the LLM first \"maps\" by analyzing individual examples, and then \"reduces\" by summarizing the labeling rules for each class. This process ensures the generated rules are LLM-interpretable. Human experts can verify these rules for accuracy and intent. Typically, humans copy and append these generated rules to the initial prompt, creating an enhanced task description. In cases of inaccuracies, humans shall explicitly instruct the LLM to oppose specific incorrect rules in the prompt."}, {"title": "Few-shot example retrieval", "content": "When calling LLM API for text classification, in a few-shot prompt setup, we retrieve an unlabeled query text and utilize its pre-computed embeddings to search for the top-k texts from a pool, selecting those with the highest scores using the Maximal Marginal Relevance (MMR) algorithm. The MMR algorithm balances relevance and diversity by considering both the similarity between embeddings and the uniqueness of the selected examples, ensuring that the retrieved examples are not only relevant to the query but also varied enough to provide a comprehensive context.\nGiven a query embedding xq, a pool R of exemplar texts, the MMR score for a candidate item xj from the pool R is defined as:\n\\[x_j = \\arg \\max \\text{MMR}(R) := \\arg \\max[\\lambda \\cdot \\text{Sim}(x_q, x_j) \u2013 (1 - \\lambda) \\max \\text{Sim}(x_j, x_i)],\\quad x_j \\in R\\setminus S, \\quad x_i \\in S\\]"}, {"title": "Coarse annotation with weak LLMs", "content": "Using a prompt enhanced with clearer task descriptions and carefully selected examples, we employ chat LLMs to assign labels from predefined options. To efficiently handle the labeling process, we utilize two instances of a weaker LLM (e.g., GPT-3.5) to label all the unlabeled texts twice. This approach mitigates high costs, usage limits, and long processing times. The weaker LLMs are expected to agree on easy queries with high accuracy, while any discrepancies are tracked in a mismatch collection for further review."}, {"title": "3.3 Post-process", "content": "In this stage, we leverage more advanced LLMs (e.g., GPT-4) and in-context learning techniques to address mismatches identified in the in-process stage. The prompts used here are refined versions of those from the previous stage, focusing on enhancements for fine-grained annotation and versatility. Although these prompts are less cost-effective, they are applied only to a limited number of queries in the mismatch collection, thereby mitigating the cost concern."}, {"title": "Chain-of-Thought Prompting", "content": "A chain-of-thought (CoT) prompt guides a large language model (LLM) through a step-by-step reasoning process to enhance its ability to tackle complex tasks . To implement it, the LLM is instructed to first analyze the content according to the task description, providing reasoning at each step before delivering the final answer. This approach works by mimicking human problem-solving, breaking down tasks into smaller components, which helps the model grasp the underlying logic and produce more accurate responses by not only assigning a label but also offering the reasoning behind it. The output of CoT is a sequence y = (y1, y2, ..., y\u2081) tokens, where yt is the desired prediction and y<t are the reasons:\n\\[P(y_t | y_{<t}, x, \\text{prompt}; \\theta)\\]"}, {"title": "Advanced LLM as a judge", "content": "Leveraging an LLM as a judge  involves utilizing its advanced reasoning capabilities to evaluate and validate the outputs of other models. In our approach, we first ask two weaker LLMs to provide annotations and reasoning to the small mismatch set through a CoT process. Then, an advanced LLM assesses the quality, accuracy, and consistency of these responses, adding an extra layer of validation. This method refines coarse predictions on challenging examples, enhancing the reliability of outputs in complex tasks."}, {"title": "3.4 Summary of Framework", "content": "In this section, we summarize the entire framework, outlining each module's input-output and the human involvement required throughout the process.\nThe \"pre-process\" stage prepares a pool of representative and diverse examples for human annotation in the subsequent stages. The input is human-collected, unlabeled texts, which we process using LLM embedding models, saving selected indices as an exemplar pool. Following this stage, human experts accurately annotate the texts in the pool and draft an initial prompt that includes only a task description. The \u201cin-process\u201d stage focuses on enhancing this initial prompt with a refined task description and more appropriate examples. Here, two weaker LLMs generate coarse annotations for the unlabeled examples. Users are responsible for running the task description generator, verifying its validity, and appending it to the initial prompt. After labeling, humans clean the predictions, identify mismatches, and record their indices in the dataset. The \"post-process\u201d stage refines the predictions for these mismatches and provides reasoning for the adjustments. Humans shall clean the responses from an advanced LLM, replace coarse predictions with fine-grained ones, or conduct human evaluations with the aid of LLM-generated reasoning.\nBy following this framework, we minimize human effort, enabling rapid experimentation and delivering high-accuracy predictions infused with human knowledge, all while reducing costs in terms of both time and resources."}, {"title": "4. Experiments", "content": "Setup. We evaluated our method using three distinct text datasets with human-labeled classifications. In each experiment, we initially treated all texts as unlabeled and tasked LLMs with predicting the labels. We use the model \u201ctext-embedding-3-small\" from OpenAI to convert all texts into embedding vectors and reduce the size of a vector to 24 using UMap. Subsequently, we selected 80 exemplars through KMeans from each dataset to form the example pool used to develop the enhanced prompts\u00b3 Throughout our experiments, \u201cGPT-3.5-turbo\u201d and \u201cMistral-medium\u201d were consistently employed as the weak models. A A parameter of value 0.8 is chosen for the MMR dynamic retriever. When referencing the advanced or judge model, we refer to either \u201cGPT-4-turbo\u201d or \"Mistral-large-latest\".\nIn the first experiment, using a clean, correctly labeled multi-class news dataset, we demonstrated our method's exceptional performance in accurately labeling text topics, even for lengthy inputs. Our three-stage approach systematically enhanced precision, with each module contributing cumulative improvements, as confirmed by an ablation study. Notably, our method achieved a significant improvement in overall F1 score, increasing from 0.94 with the na\u00efve LLM labeling to an impressive 0.96.\nIn the second experiment, we specifically highlight the effectiveness of our second-stage generator. We tasked LLMs with labeling tweets as \"support\" or \"oppose\" in relation to Brett Kavanaugh's Supreme Court confirmation process. Initially, we observed that weaker LLMs often struggled with discerning sentiment and stance, leading to confusion. However, when utilizing our second-stage in-process generator, the LLM generated an enhanced prompt by summarizing the labeling rules for \"support\" and \"oppose\" based on 80 human-labeled tweets. This resulted in a dramatic improvement in labeling accuracy, with F1 scores soaring from 0.57 to 0.95.\nIn the third experiment, we attempted to replicate Fowler et al.'s (2021) study, which examines how the medium of campaign ads influences their tone. However, we encountered a very noisy, human-labeled dataset. Our method effectively uncovered potential issues within this supposedly gold-standard dataset, highlighting how noisy human labels can significantly impact downstream political science analysis. We also addressed some pitfalls of using our method, emphasizing the importance of careful data handling."}, {"title": "4.1 Classifying BBC News Reports Topics", "content": "In our first experiment, we applied our method to a multi-category classification task involving extensive and lengthy text. We chose to label the topics of BBC news reports due to their diversity and relevance in benchmarking machine learning models. The dataset comprises 2,225 news articles sourced from the BBC News website, covering stories across five topical areas business, entertainment, politics, sport, and tech-from the years 2004-2005. This dataset, originally compiled by Greene and Cunningham"}, {"title": "4.2 Measuring Public Opinion Toward Brett Kavanaugh's SCOTUS Nomination", "content": "In the second experiment, we focus on demonstrating the effectiveness of our second stage-the prompt generator-in accurately labeling nuanced political science concepts. In their 2023 paper, Bestvater and Monroe argue that sentiment and stance are fundamentally distinct concepts, a difference that traditional sentiment analysis tools like the VADER dictionary and the Lexicoder Sentiment Dictionary (LSD) often fail to capture. While sentiment analysis gauges the emotional tone of a document-whether positive, negative, or neutral\u2014stance identification determines the author's position on a specific issue, which may not align directly with the sentiment.\nBestvater and Monroe explore this distinction by analyzing a corpus of tweets about Kavanaugh's nomination(2023). Their study compares the effectiveness of various text classifiers in identifying stance and concludes that, for many stance detection tasks, training a new supervised classifier on a hand-labeled dataset yields more accurate results than relying on existing models or dictionaries designed for sentiment analysis. The key findings from their study are summarized in the Appendix Table ??. To assess the ability of our method to distinguish nuanced political science concepts and generate accurate predictions, we applied it to the human-labeled dataset from Bestvater and Monroe (2023), which serves as the ground truth benchmark. We adopted the same instructions given to the human coders in their study and used them as the"}, {"title": "4.3 Classifying Campaign Ads Tones in the 2018 Election", "content": "The medium through which political communication is delivered plays a critical role in shaping the message's tone and its audience reach. In a recent study, Fowler and her colleagues examined the impact of Facebook as a medium on the tone of political advertisements (2021). They proposed that ads on Facebook are more likely to adopt a negative tone compared to other platforms. To explore this hypothesis, the researchers collected data from political advertisements by all federal, statewide, and state legislative candidates during the 2018 elections. A team of research assistants then classified a sample of these ads based on their tone-whether they were promoting, contrasting, or attacking. The dataset comprises a total of 14,642 advertisements, with 9,073 originating from Facebook and 5,569 from television ads, offering a comprehensive basis for comparing online and offline political messaging. We randomly selected a sample of 3,000 observations from the coded training set. Within this sample, 2,374 ads were classified as promoting a candidate, 448 as contrasting between candidates, and 178 as attacking a candidate.\nWe applied the same three-stage approach as in our previous two experiments, this"}, {"title": "5. Discussion and Conclusion", "content": "Besides the benefits shown in experiments, our method possesses additional advantages worth discussing.\nCost-effective. Our method is highly cost-effective. In our experiments, we needed to annotate an average of 3,000 texts, each approximately 60 words in length for each experiment. Using the standard practice of cross-validation, hiring two workers on Amazon Mechanical Turk (MTurk) would result in a conservative cost estimate of $350 per worker, plus a $100 MTurk fee, totaling $850. In contrast, our approach incurs a total cost of approximately $183, which includes $2 for the embedding model, $92 for the weak models, and $79 for the advanced model. This means our method costs at most 21.5% of the total price of hiring human workers, not to mention the additional computational cost of training.\nExtensible. Our method is extensible, allowing researchers to adapt or replace the algorithms within our framework to fit specific research needs. For instance, while we used UMAP, K-means, and MapReduce in our experiments for exemplar selection and prompt generation, these algorithms are not fixed and can be easily substituted with alternatives. This flexibility enables the method to be tailored to different types of data and research objectives, making it a versatile tool for a broad range of political science applications. Additionally, our approach is designed to integrate the latest LLM advancements seamlessly, ensuring that it remains at the forefront of technological progress without requiring significant adjustments or incurring additional costs. For example, if OpenAI releases GPT-5 in the future, our framework supports users to add this new model to their research just by modifying the configure file. This adaptability future-proofs our method, allowing it to consistently deliver cutting-edge performance as LLM technology evolves.\nLimitation. Our method has several limitations. First, it is currently limited to categorical classification and is not suitable for tasks that require measuring intensity, degrees, or rankings. For example, our approach lacks the capability to accurately assess political ideology on a traditional 5-point scale, such as distinguishing between strong Democrats, moderate Democrats, Independents, Republicans, and strong Republicans. In future research, we plan to extend our method to handle pairwise comparisons, enabling us to better capture the intensity of party identification and other continuous variables.\nSecond, our method is currently limited to text data and cannot be applied to more complex data types such as images, videos, and audio, all of which are increasingly gaining attention in political science. Expanding our method to handle these diverse forms of data is a crucial direction for future research, allowing us to better address the growing variety of data sources in the field.\nConclusion. In summary, our proposed method offers a significant improvement over both traditional machine learning approaches and existing LLM-based practices. By eliminating the need for extensive human laboring, dynamically selecting relevant"}, {"title": "", "content": "examples, and leveraging the consensus of weaker LLMs, our framework provides a high-performance and cost-effective solution for text classification in social science research. This approach not only enhances the accuracy and reliability of predictions but also aligns closely with the methodological standards of the field, making it a valuable tool for researchers aiming to conduct robust and reproducible studies."}]}