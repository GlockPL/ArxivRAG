{"title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget", "authors": ["Vikash Sehwag", "Xianghao Kong", "Jingtao Li", "Michael Spranger", "Lingjuan Lyu"], "abstract": "As scaling laws in generative AI push performance, they also simultaneously concentrate\nthe development of these models among actors with large computational resources. With\na focus on text-to-image (T2I) generative models, we aim to address this bottleneck by\ndemonstrating very low-cost training of large-scale T2I diffusion transformer models. As\nthe computational cost of transformers increases with the number of patches in each image,\nwe propose to randomly mask up to 75% of the image patches during training. We pro-\npose a deferred masking strategy that preprocesses all patches using a patch-mixer before\nmasking, thus significantly reducing the performance degradation with masking, making it\nsuperior to model downscaling in reducing computational cost. We also incorporate the lat-\nest improvements in transformer architecture, such as the use of mixture-of-experts layers,\nto improve performance and further identify the critical benefit of using synthetic images\nin micro-budget training. Finally, using only 37M publicly available real and synthetic im-\nages, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost\nand achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model\nachieves competitive FID and high-quality generations while incurring 118\u00d7 lower cost than\nstable diffusion models and 14\u00d7 lower cost than the current state-of-the-art approach that\ncosts $28,400. We aim to release our end-to-end training pipeline to further democratize\nthe training of large-scale diffusion models on micro-budgets.", "sections": [{"title": "1 Introduction", "content": "While modern visual generative models excel at creating photorealistic visual content and have propelled\nthe generation of more than a billion images each year (Valyaeva, 2023), the cost and effort of training\nthese models from scratch remain very high (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al.,\n2022). In text-to-image diffusion models (T2I) (Song et al., 2020a; Ho et al., 2020; Song et al., 2021), some\nprevious works have successfully scaled down the computational cost compared to the 200,000 A100 GPU\nhours used by Stable Diffusion 2.1 (Rombach et al., 2022; Chen et al., 2023; Pernias et al., 2024; Gokaslan\net al., 2024). However, the computational cost of even the state-of-the-art approaches (18,000 A100 GPU\nhours) remains very high (Chen et al., 2023), requiring more than a month of training time on the leading\n8\u00d7H100 GPU machine. Furthermore, previous works either leverage larger-scale datasets spanning on the\norder of a billion images or use proprietary datasets to enhance performance (Rombach et al., 2022; Chen\net al., 2023; Yu et al., 2022; Saharia et al., 2022). The high training cost and the dataset requirements create\nan inaccessible barrier to participating in the development of large-scale diffusion models. In this work, we\naim to address this issue by developing a low-cost end-to-end pipeline for competitive text-to-image diffusion\nmodels that achieve more than an order of magnitude reduction in training cost than the state-of-the-art\nwhile not requiring access to billions of training images or proprietary datasets\u00b9.\nWe consider vision transformer based latent diffusion models for text-to-image generation (Dosovitskiy et al.,\n2020; Peebles & Xie, 2023), particularly because of their simplified design and wide adoption across the\nlatest large-scale diffusion models (Betker et al., 2023; Esser et al., 2024; Chen et al., 2023). To reduce the"}, {"title": "2 Methodology", "content": "In this section, we first provide a background on diffusion models, followed by a review of common approaches,\nparticularly those based on patch masking, to reduce computational costs in training diffusion transformers.\nThen, we provide a detailed description of our proposed deferred patch masking strategy."}, {"title": "2.1 Background: Diffusion-based generative models and diffusion transformers", "content": "We denote the data distribution by D, such that (x, c) ~ D, where x and c represent the image and the\ncorresponding natural language caption, respectively. We assume that p(x;o) is the image distribution\nobtained after adding \u03c3\u00b2-variance Gaussian noise."}, {"title": "Training.", "content": "Diffusion model training is based on parameterizing the score of the density function, which\ndoesn't depend on the generally intractable normalization factor, using a denoiser, i.e., $\u2207_x log p(x; \u03c3(t)) \u2248\n(F_\u0398(x, \u03c3(t)) \u2013 x) /\u03c3(t)^2$. The denoising function $F_\u0398(x, \u03c3(t))$ is generally modeled using a deep neural network.\nFor text-to-image generation, the denoiser is conditioned on both noise distribution and natural language\ncaptions and trained using the following loss function,\n$L = E_{(x,c)~D; \\epsilon~N (0,\u03c3(t)^2I)} || F_\u0398(x + \\epsilon; \u03c3(t), c) \u2013 x||_2.$"}, {"title": "Classifier-free guidance.", "content": "To increase alignment between input captions and generated images, classifier-\nfree guidance (Ho & Salimans, 2022) modifies the image denoiser to output a linear combination of denoised\nsamples in the presence and absence of input captions.\n$F_\u0398(x; \u03c3(t), c) = F_\u0398(x; \u03c3(t)) + w\\cdot (F_\u0398(x; \u03c3(t), c) \u2013 F_\u0398(x; \u03c3(t))),$\nwhere w > 1 controls the strength of guidance. During training, a fraction of image captions (commonly set\nto 10%) are randomly dropped to learn unconditional generation."}, {"title": "Latent diffusion models.", "content": "In contrast to modeling higher dimensional pixel space (Rh\u00d7w\u00d73), latent diffusion\nmodels (Rombach et al., 2022) are trained in a much lower dimensional compressed latent space (Rn\u00d7n\u00d7c),\nwhere n is the compression factor and c is the number of channels in the latent space. The image-to-latent\nencoding and its reverse mapping are performed by a variational autoencoder. Latent diffusion models\nachieve faster convergence than training in pixel space and have been heavily adopted in large-scale diffusion\nmodels (Podell et al., 2024; Betker et al., 2023; Chen et al., 2023; Balaji et al., 2022)."}, {"title": "Diffusion transformers.", "content": "We consider that the transformer model ($F_\u0398$) consists of k hierarchically stacked\ntransformer blocks. Each block consists of a multi-head self-attention layer, a multi-head cross-attention\nlayer, and a feed-forward layer. For simplicity, we assume that all images are converted to a sequence of\npatches to be compatible with the transformer architecture. In the transformer architecture, we refer to the\nwidth of all linear layers as d."}, {"title": "2.2 Common approaches towards minimizing the computational cost of training diffusion\ntransformers", "content": "Assuming transformer architectures where linear layers account for the majority of the computational cost,\nthe total training cost is proportional to M \u00d7 N\u00d7S, where M is the number of samples processed, N is\nthe number of model parameters, and S is the number of patches derived from one image, i.e., sequence\nsize for transformer input (Figure 2a). As our goal is to train an open-world model, we believe that a\nlarge number of parameters are required to support diverse concepts during generation; thus, we opt to not\ndecrease the parameter count significantly. Since diffusion models converge slowly and are often trained for a\nlarge number of steps, even for small-scale datasets, we keep this choice intact (Rombach et al., 2022; Balaji\net al., 2022). Instead, we aim to exploit any redundancy in the input sequence to reduce computational cost\nwhile simultaneously not drastically degrade performance."}, {"title": "A. Using larger patch size", "content": "In a vision transformer, the input image is first transformed into a\nsequence of non-overlapping patches with resolution p\u00d7p, where p is the patch size (Dosovitskiy et al., 2020).\nThough using a higher patch size quadratically reduces the number of patches per image, it can significantly\ndegrade performance due to the aggressive compression of larger regions of the image into a single patch."}, {"title": "B. Using patch masking", "content": "A contemporary approach is to keep the patch size intact but drop a\nlarge fraction of patches at the input layer of the transformer (He et al., 2022). Naive token masking is similar\nto training on random crops in convolutional UNets, where often upsampling diffusion models are trained\non smaller random crops rather than the full image (Nichol et al., 2021; Saharia et al., 2022). However,\nin contrast to random crops of the images, patch masking allows training on non-continuous regions of the\nimage\u00b2. Due to its effectiveness, masking-based training of transformers has been adopted across both vision\nand language domains (Devlin et al., 2018; He et al., 2022)."}, {"title": "C. Masking patches with autoencoding (MaskDiT", "content": "In order to also encourage representation\nlearning from masked patches, Zheng et al. (2024) adds an auxiliary autoencoding loss that encourages the\nreconstruction of masked patches. This formulation was motivated by the success of the autoencoding loss\nin learning self-supervised representations (He et al., 2022). MaskDiT first appends a lightweight decoder,\na small-scale transformer network, to the larger backbone transformer. Before the decoder, it expands the\nbackbone transformer output by inserting spurious learnable patches in place of masked patches.\nAssuming that the binary mask m represents the masked patches, the final training loss is the following,\n$L_{diff} = E_{(x,c)~D; \\epsilon~N (0,\u03c3(t)^2I)} || (F_\u0398((x + \\epsilon) \u2299 (1 \u2212 m); \u03c3, c) - x) \u2299 (1 \u2013 m) ||^2$\n$L_{mae} = E_{(x,c)~D; \\epsilon~N (0,\u03c3(t)^2I)} || (F_\u0398((x + \\epsilon) \u2299 (1 \u2212 m); \u03c3, c) \u2013 (x + \\epsilon)) \u2299 m||^2$\n$L = L_{diff} + \u03b3L_{mae}$\nwhere $F_\u0398$ represents the sequential backbone transformer and decoder module, and \u03b3 is a hyperparameter\nto balance the influence of the masked autoencoding loss with respect to the diffusion training loss."}, {"title": "2.3 Alleviating performance bottleneck in patch masking using deferred masking", "content": "To drastically reduce the computational cost, patch masking requires dropping a large fraction of input\npatches before they are fed into the backbone transformer, thus making the information from masked patches\nunavailable to the transformer. High masking ratios, e.g., 75% masking, significantly degrade the overall\nperformance of the transformer. Even with MaskDiT, we only observe a marginal improvement over naive\nmasking, as this approach also drops the majority of image patches at the input layer itself.\nDeferred masking to retain semantic information of all patches. As high masking ratios remove the\nmajority of valuable learning signals from images, we are motivated to ask whether it is necessary to mask\nat the input layer. As long as the computational cost is unchanged, it is merely a design choice and not a\nfundamental constraint. In fact, we uncover a significantly better masking strategy, with nearly identical"}, {"title": "Training diffusion transformers with a patch-mixer.", "content": "We consider a patch mixer to be any neural\narchitecture that can fuse the embeddings of individual patches. In transformer models, this objective\nis naturally achieved with a combination of attention and feedforward layers. So we use a lightweight\ntransformer comprising only a few layers as our patch mixer. We mask the input sequence tokens after they\nhave been processed by the patch mixer (Figure 2e). Assuming a binary mask m for masking, we train our\nmodel using the following loss function,\n$L = E_{(x,c)~D; \\epsilon~N (0,\u03c3(t)^2I)} ||F_\u0398(M_\u03a6(x + \\epsilon; \u03c3(t), c) \u2299 (1 \u2212 m); \u03c3(t), c) \u2212 x \u2299 (1 \u2212 m) ||^2$\nwhere $M_\u03a6$ is the patch-mixer model and $F_\u0398$ is the backbone transformer. Note that compared to MaskDiT,\nour approach also simplifies the overall design by not requiring an additional loss function and corresponding\nhyperparameter tuning between two losses during training. We don't mask any patches during inference."}, {"title": "Unmasked finetuning.", "content": "As a very high masking ratio can significantly reduce the ability of diffusion\nmodels to learn global structure in the images and introduce a train-test distribution shift in sequence size,\nwe consider a small degree of unmasked finetuning after the masked pretraining. The finetuning can also\nmitigate any undesirable generation artifacts due to the use of patch masking. Thus, it has been crucial in\nprevious work (Zheng et al., 2024) to recover the sharp performance drop from masking, especially when\nclassifier-free guidance is used in sampling. However, we don't find it completely necessary, as even in masked\npretraining, our approach achieves comparable performance to the baseline unmasked pretraining. We only\nemploy it in large-scale training to mitigate any unknown-unknown generation artifacts from a high degree\nof patch masking."}, {"title": "Improving backbone transformer architecture with mixture-of-experts (MoE) and layer-wise\nscaling.", "content": "We also leverage innovations in transformer architecture design to boost the performance of our\nmodel under computational constraints. We use mixture-of-experts layers as they increase the parameters\nand expressive power of our model without significantly increasing the training cost (Shazeer et al., 2017;"}, {"title": "3 Experimental Setup", "content": "We provide key details in our experimental setup below and additional details in Appendix A.\nWe use two variants of Diffusion Transformer (DiT) architecture from Peebles & Xie (2023): DiT-Tiny/2\nand DiT-X1/2, both comprising a patch size of 2. When training sparse models, we replace every alternate\ntransformer block with an 8-expert mixture-of-experts block. We train all models using the AdamW opti-\nmizer with cosine learning rate decay and high weight decay. We provide an exhaustive list of our training\nhyperparameters in Table 5 in Appendix A. We use a four-channel variational autoencoder (VAE) from\nthe Stable-Diffusion-XL (Podell et al., 2024) model to extract image latents. We also consider the latest\n16-channel VAEs for test their performance in large-scale micro-budget training (Esser et al., 2024). We use\nthe EDM framework from Karras et al. (2022) as a unified training setup for all diffusion models. We refer\nto our diffusion trasnformer as MicroDiT. We use Fr\u00e9chet inception distance (FID) (Heusel et al., 2017),\nboth with the original Inception-v3 model (Szegedy et al., 2016) and a CLIP model (Radford et al., 2021),\nand CLIP score (Hessel et al., 2021) to measure the performance of the image generation models."}, {"title": "Choice of text encoder.", "content": "To convert natural language captions to higher-dimensional feature embeddings,\nwe use the text encoder from state-of-the-art CLIP models (Ilharco et al., 2021; Fang et al., 2023). We\nfavor CLIP models over T5-xxl (Raffel et al., 2020; Saharia et al., 2022) for text encoding, despite the\nbetter performance of the latter on challenging tasks like text synthesis, as the latter incurs much higher\ncomputational and storage costs at scale, thus not suitable for micro-budget training (Table 6 in Appendix A).\nWe consider the computation of text embeddings for captions and latent compression of images as a one-time\ncost that amortizes over multiple training runs of a diffusion model. Thus, we compute them offline and"}, {"title": "4 Evaluating Effectiveness of Deferred Masking", "content": "In this section, we validate the effectiveness of our deferred masking approach and investigate the impact of\nindividual design choices on image generation performance. We use the DiT-Tiny/2 model and the cifar-\ncaptions dataset (256 \u00d7 256 image resolution) for all experiments in this section. We train each model for\n60K optimization steps, with the AdamW optimizer and exponential moving average with a 0.995 smoothing\ncoefficient enabled for the last 10K steps."}, {"title": "4.1 Out-of-the-box performance: Making high masking ratios feasible with deferred masking", "content": "A key limitation of patch masking strategies is poor performance when a large fraction of patches are\nmasked. For example, Zheng et al. (2024) observed large degradation in even MaskDiT performance beyond\na 50% masking ratio. Before optimizing the performance of our approach with a rigorous ablation study, we\nevaluate its out-of-the-box performance with common training parameters for up to 87.5% masking ratios.\nAs a baseline, we train a network with no patch-mixer, i.e., naive masking (Figure 2c) for each masking\nratio. For deferred masking, we use a four-transformer block patch-mixer that comprises less than 10% of\nthe parameters of the backbone transformer.\nWe use commonly used default hyperparameters to simulate the out-of-the-box performance for both our\napproach and the baseline. We train both models with the AdamW optimizer with an identical learning rate of\n1.6\u00d710-4, 0.01 weight decay, and a cosine learning rate schedule. We set $(\u03bc_{mean}, \u03bc_{std})$ to (\u22121.2, 1.2) following\nthe original work (Karras et al., 2022). We provide our results in Figure 4. Our deferred masking approach\nachieves much better performance across all three performance metrics. Furthermore, the performance gaps\nwiden with an increase in masking ratio. For example, with a 75% masking ratio, naive masking degrades the\nFID score to 16.5 while our approach achieves 5.03, much closer to the FID score of 3.79 with no masking."}, {"title": "4.2 Ablation study of our training pipeline", "content": "We ablate design choices across masking, patch mixer, and training hyperparameters. We summarize the\ntechniques that improved out-of-the-box performance in Table 5a. We provide supplementary results and\ndiscussion of the ablation study in Appendix C.\nComparison with training hyperparameters of LLMs. Since the diffusion architectures are very\nsimilar to transformers used in large language models (LLMs), we compare the hyperparameter choices\nacross the two tasks. Similar to common LLM training setups (Touvron et al., 2023; Jiang et al., 2023;\nChowdhery et al., 2022), we find that SwiGLU activation (Shazeer, 2020) in the feedforward layer outperforms\nthe GELU (Hendrycks & Gimpel, 2016) activation function. Similarly, higher weight decay leads to better\nimage generation performance. However, we observe better performance when using a higher running average\ncoefficient for the AdamW second moment (32), in contrast to large-scale LLMs where \u1e9e2 \u2248 0.95 is preferred.\nAs we use a small number of training steps, we find that increasing the learning rate to the maximum possible\nvalue until training instabilities also significantly improves image generation performance.\nDesign choices in masking and patch-mixer. We observe a consistent improvement in performance\nwith a larger patch mixer. However, despite the better performance of larger patch-mixers, we choose to use\na small patch mixer to lower the computational budget spent by the patch mixer in processing the unmasked\ninput. We also update the noise distribution (\u03bcmean, \u03bcstd) to (-0.6, 1.2) as it improves the alignment between\ncaptions and generated images. By default, we mask each patch at random. We ablate the masking strategy\nto retain more continuous regions of patches using block masking. However, we find that\nincreasing the block size leads to a degradation in the performance of our approach, thus we retain the\noriginal strategy of masking each patch at random."}, {"title": "4.3 Validating improvements in diffusion transformer architecture", "content": "We measure the effectiveness of the two modifications in transformer architecture in improving image gen-\neration performance.\nLayer-wise scaling. We investigate the impact of this design\nchoice in a standalone experiment where we train two variants\nof a DiT-Tiny architecture, one with a constant width trans-\nformer and the other with layer-wise scaling of each transformer\nblock. We use naive masking for both methods. We select the\nwidth of the constant-width transformer such that its computa-\ntional footprint is identical to layer-wise scaling. We train both\nmodels for an identical number of training steps and wall-clock\ntime. We find that the layer-wise scaling approach outperforms"}, {"title": "5 Comparing the Effectiveness of Deferred Masking with Baselines", "content": "In this section, we provide a concrete comparison with multiple baselines. First, we compare with techniques\naimed at reducing computational cost by reducing the transformer patch sequence size. Next, we consider\nreducing network size, i.e., downscaling, while keeping the patch sequence intact. Under isoflops budget, i.e.,\nidentical wall-clock training time and FLOPs, we show that our approach achieves better performance than\nmodel downscaling.\nComparing different masking strategies. We first compare our approach with the strategy of using\nlarger patches. We increase the patch size from two to four, equivalent to 75% patch masking. However, it\nunderperforms compared to deferred masking and only achieves 9.38, 6.31, and 26.70 FID, Clip-FID, and\nClip-score, respectively. In contrast, deferred masking achieves 7.09, 4.10, and 28.24 FID, Clip-FID, and\nClip-score, respectively. Next, we compare all three masking strategies. We train each model for\n60K steps and report total training flops for each approach to indicate its computational cost. We find that\nnaive masking significantly degrades model performance, and using additional proxy loss functions (Zheng\net al., 2024) only marginally improves performance. We find that simply deferring the masking and using a\nsmall transformer as a patch-mixer significantly bridges the gap between masked and unmasked training of\ndiffusion transformers.\nIsoFLOPs study - Why bother using masking instead of smaller transformers? Using patch\nmasking is a complementary approach to downscaling transformer size to reduce computation cost in training.\nWe compare the two approaches in Table 7a. At each masking ratio, we reduce the size of the unmasked\nnetwork to match FLOPs and total wall-clock training time of masked training. We train both networks\nfor 60K training steps. Until a masking ratio of 75%, we find that deferred masking outperforms network\ndown-scaling across at least two out of three metrics. However, at extremely high masking ratios, deferred\nmasking tends to achieve lower performance. This is likely because the information loss from masking is too\nhigh at these ratios. For example, for 32 \u00d7 32 resolution latent and a patch size of two, only 32 patches are\nretained (out of 256 patches) at an 87.5% masking ratio.\nDeferred masking as pretraining + unmasked finetuning. We show deferred masking also acts as a\nstrong pretraining objective and using it with an unmasked finetuning schedule achieves better performance"}, {"title": "6 Micro-budget Training of Large-scale Models", "content": "In this section, we validate the effectiveness of our approach in training open-world text-to-image generative\nmodels on a micro-budget. Moving beyond the small-scale cifar-captions dataset, we train the large-scale\nmodel on a mixture of real and synthetic image datasets comprising 37M images. We demonstrate the high-\nquality image generation capability of our model and compare its performance with previous works across\nmultiple quantitative metrics."}, {"title": "6.1 Benefit of additional synthetic data in training", "content": "Instead of training only on real images, we expand the training data to include 40% additional synthetic\nimages. We compare the performance of two MicroDiT models trained on only real images and combined\nreal and synthetic images, respectively, using identical training processes.\nUnder canonical performance metrics, both models apparently achieve similar performance. For example, the\nmodel trained on real-only data achieved an FID score of 12.72 and a CLIP score of 26.67, while the model\ntrained on both real and synthetic data achieved an FID score of 12.66 and a CLIP score of 28.14. Even\non GenEval (Ghosh et al., 2024), a benchmark that evaluates the ability to generate multiple objects and\nmodelling object dynamics in images, both models achieved an identical score of 0.46. These results seemingly\nsuggest that incorporating a large amount of synthetic samples didn't yield any meaningful improvement in\nimage generation capabilities.\nHowever, we argue that this observation is heavily influenced by the limitations of our existing evaluation\nmetrics. In a qualitative evaluation, we found that the model trained on the combined dataset achieved\nmuch better image quality (Figure 9). The real data model often fails to adhere to the prompt, frequently\nhallucinating key details and often failing to synthesize the correct object. Metrics, such as FID, fail to\ncapture this difference because they predominantly measure distribution similarity (Pernias et al., 2024).\nThus, we focus on using human visual preference as an evaluation metric. To automate the process, we use\nGPT-40 (OpenAI, 2024), a state-of-the-art multimodal model, as a proxy for human preference. We supply\nthe following prompt to the model: Given the prompt \u2018{prompt}', which image do you prefer, Image A or\nImage B, considering factors like image details, quality, realism, and aesthetics? Respond with 'A' or 'B' or\n'none' if neither is preferred. For each comparison, we also shuffle the order of images to remove any ordering\nbias. We generate samples using DrawBench (Saharia et al., 2022) and PartiPrompts (P2) (Yu et al., 2022),\ntwo commonly used prompt databases (Figure 8). On the P2 dataset, images from the combined data model\nare preferred in 63% of comparisons while images from the real data model are only preferred 21% of the time\n(16% of comparisons resulted in no preference). For the DrawBench dataset, the combined data model is\npreferred in 40% of comparisons while the real data model is only preferred in 21% of comparisons. Overall,\nusing a human preference-centric metric clearly demonstrates the benefit of additional synthetic data in\nimproving overall image quality."}, {"title": "6.2 Ablating design choices on scale", "content": "We first examine the effect of using a higher dimensional latent space in micro-budget training. We replace\nthe default four-channel autoencoder with one that has sixteen channels, resulting in a 4\u00d7 higher dimensional\nlatent space. Recent large-scale models have adopted high dimensional latent space as it provides significant\nimprovements in image generation abilities (Dai et al., 2023; Esser et al., 2024). Note that the autoencoder\nwith higher channels itself has superior image reconstruction capabilities, which further contributes to overall\nsuccess. Intriguingly, we find that using a higher dimensional latent space in micro-budget training hurts\nperformance. For two MicroDiT models trained with identical computational budgets and training hyperpa-\nrameters, we find that the model trained in four-channel latent space achieves better FID, Clip-score, and\nGenEval scores (Table 10). We hypothesize that even though an increase in latent dimensionality allows\nbetter modeling of data distribution, it also simultaneously increases the training budget required to train"}, {"title": "6.3 Comparison with previous works", "content": "Comparing zero-shot image generation on COCO dataset (Table 3). We follow the evaluation\nmethodology in previous work (Saharia et al., 2022; Yu et al., 2022; Balaji et al., 2022) and sample 30K\nrandom images and corresponding captions from the COCO 2014 validation set. We generate 30K synthetic\nimages using the captions and measure the distribution similarity between real and synthetic samples using\nFID (referred to as FID-30K in Table 3). Our model achieves a competitive 12.66 FID-30K, while trained with\n14\u00d7 lower computational cost than the state-of-the-art low-cost training method and without any proprietary\nor humongous dataset to boost performance. Our approach also outperforms W\u00fcrstchen (Pernias et al.,"}, {"title": "7 Related work", "content": "The landscape of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020a;b) has\nrapidly evolved in the last few years, with modern models trained on web-scale datasets (Betker et al., 2023;\nEsser et al., 2024; Rombach et al., 2022; Saharia et al., 2022). In contrast to training in pixel space (Saharia\net al., 2022; Nichol et al., 2021), the majority of large-scale diffusion models are trained in a compressed\nlatent space, thus referred to as latent diffusion models (Rombach et al., 2022). Similar to auto-regressive\nmodels, transformer architectures (Vaswani et al., 2017) have also been recently adopted for diffusion-based\nimage synthesis. While earlier models commonly adopted a fully convolutional or hybrid UNet network\narchitecture (Dhariwal & Nichol, 2021; Nichol et al., 2021; Rombach et al., 2022; Saharia et al., 2022), recent\nworks have demonstrated that diffusion transformers (Peebles & Xie, 2023) achieve better performance (Esser\net al., 2024; Chen et al., 2023; Betker et al., 2023). Thus, we also use diffusion transformers for modeling\nlatent diffusion models.\nSince the image captions in web-scale datasets are often noisy and of poor quality (Byeon et al., 2022;\nSchuhmann et al., 2022), recent works have started to recaption them using vision-language models (Liu\net al., 2023; Wang et al., 2023; Li et al., 2024). Using synthetic captions leads to significant improvements\nin the diffusion models' image generation capabilities (Betker et al., 2023; Esser et al., 2024; Gokaslan\net al., 2024; Chen et al., 2023). While text-to-image generation models are the most common application of\ndiffusion models, they can also support a wide range of other conditioning mechanisms, such as segmentation\nmaps, sketches, or even audio descriptions (Zhang et al., 2023; Yariv et al., 2023). Sampling from diffusion\nmodels is also an active area of research, with multiple novel solvers for ODE/SDE sampling formulations\nto reduce the number of iterations required in sampling without degrading performance (Song et al., 2020a;\nJolicoeur-Martineau et al., 2021; Liu et al., 2022; Karras et al., 2022). Furthermore, the latest approaches\nenable single-step sampling from diffusion models using distillation-based training strategies (Song et al.,\n2023; Sauer et al., 2023). The sampling process in diffusion models also employs an additional guidance signal\nto improve prompt alignment, either based on an external classifier (Dhariwal & Nichol, 2021; Nichol et al.,\n2021; Sehwag et al., 2022) or self-guidance (Ho & Salimans, 2022; Karras et al., 2024). The latter classifier-\nfree guidance approach is widely adopted in large-scale diffusion models and has been further extended to\nlarge-language models (Zhao et al., 2024; Sanchez et al., 2023).\nSince the training cost of early large-scale diffusion models was noticeably high (Rombach et al., 2022;\nRamesh et al., 2022), multiple previous works focused on bringing down this cost. Gokaslan et al. (2024)\nshowed that using common tricks from efficient deep learning can bring the cost of stable-diffusion-2 models\nunder $50K. Chen et al. (2023) also reduced this cost by training a diffusion transformer model on a mixture\nof openly accessible and proprietary image datasets. Cascaded training of diffusion models is also used by\nsome previous works (Sah"}]}