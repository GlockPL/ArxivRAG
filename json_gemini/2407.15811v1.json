{"title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget", "authors": ["Vikash Sehwag", "Xianghao Kong", "Jingtao Li", "Michael Spranger", "Lingjuan Lyu"], "abstract": "As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118\u00d7 lower cost than stable diffusion models and 14\u00d7 lower cost than the current state-of-the-art approach that costs $28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.", "sections": [{"title": "1 Introduction", "content": "While modern visual generative models excel at creating photorealistic visual content and have propelled the generation of more than a billion images each year (Valyaeva, 2023), the cost and effort of training these models from scratch remain very high (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022). In text-to-image diffusion models (T2I) (Song et al., 2020a; Ho et al., 2020; Song et al., 2021), some previous works have successfully scaled down the computational cost compared to the 200,000 A100 GPU hours used by Stable Diffusion 2.1 (Rombach et al., 2022; Chen et al., 2023; Pernias et al., 2024; Gokaslan et al., 2024). However, the computational cost of even the state-of-the-art approaches (18,000 A100 GPU hours) remains very high (Chen et al., 2023), requiring more than a month of training time on the leading 8\u00d7H100 GPU machine. Furthermore, previous works either leverage larger-scale datasets spanning on the order of a billion images or use proprietary datasets to enhance performance (Rombach et al., 2022; Chen et al., 2023; Yu et al., 2022; Saharia et al., 2022). The high training cost and the dataset requirements create an inaccessible barrier to participating in the development of large-scale diffusion models. In this work, we aim to address this issue by developing a low-cost end-to-end pipeline for competitive text-to-image diffusion models that achieve more than an order of magnitude reduction in training cost than the state-of-the-art while not requiring access to billions of training images or proprietary datasets\u00b9. We consider vision transformer based latent diffusion models for text-to-image generation (Dosovitskiy et al., 2020; Peebles & Xie, 2023), particularly because of their simplified design and wide adoption across the latest large-scale diffusion models (Betker et al., 2023; Esser et al., 2024; Chen et al., 2023). To reduce the"}, {"title": "2 Methodology", "content": "In this section, we first provide a background on diffusion models, followed by a review of common approaches, particularly those based on patch masking, to reduce computational costs in training diffusion transformers. Then, we provide a detailed description of our proposed deferred patch masking strategy."}, {"title": "2.1 Background: Diffusion-based generative models and diffusion transformers", "content": "We denote the data distribution by D, such that (x, c) ~ D, where x and c represent the image and the corresponding natural language caption, respectively. We assume that p(x;o) is the image distribution obtained after adding \u03c3\u00b2-variance Gaussian noise."}, {"title": "Training", "content": "Diffusion model training is based on parameterizing the score of the density function, which doesn't depend on the generally intractable normalization factor, using a denoiser, i.e., \u2207 log p(x; o(t)) \u2248 (Fox, \u03c3(t)) \u2013 x) /\u03c3(t)2. The denoising function Fe(x, \u03c3(t)) is generally modeled using a deep neural network. For text-to-image generation, the denoiser is conditioned on both noise distribution and natural language captions and trained using the following loss function,\n\nL = E(x,c)~DE~N (0,o(t) 21) || Fo(x + \u20ac; \u03c3(t), c) \u2013 x||2.\n\n(3)\nThe noise distribution during training is lognormal (ln(\u03c3) ~ N(Pmean, Pstd)), where the choice of the mean and standard deviation of the noise distribution strongly influences the quality of generated samples. For example, the noise distribution is shifted to the right to sample larger noise values on higher resolution images, as the signal-to-noise ratio increases with resolution (Teng et al., 2023)."}, {"title": "Classifier-free guidance", "content": "To increase alignment between input captions and generated images, classifier-free guidance (Ho & Salimans, 2022) modifies the image denoiser to output a linear combination of denoised samples in the presence and absence of input captions.\n\nFo(x; \u03c3(t), c) = Fo(x; \u03c3(t)) + w\u00b7 (Fo(x; \u03c3(t), c) \u2013 Fo(x; \u03c3(t))),\n\n(4)\nwhere w > 1 controls the strength of guidance. During training, a fraction of image captions (commonly set to 10%) are randomly dropped to learn unconditional generation."}, {"title": "Latent diffusion models", "content": "In contrast to modeling higher dimensional pixel space (Rh\u00d7w\u00d73), latent diffusion models (Rombach et al., 2022) are trained in a much lower dimensional compressed latent space (R\u00d7\u00d7c), where n is the compression factor and e is the number of channels in the latent space. The image-to-latent encoding and its reverse mapping are performed by a variational autoencoder. Latent diffusion models achieve faster convergence than training in pixel space and have been heavily adopted in large-scale diffusion models (Podell et al., 2024; Betker et al., 2023; Chen et al., 2023; Balaji et al., 2022)."}, {"title": "Diffusion transformers", "content": "We consider that the transformer model (Fe) consists of k hierarchically stacked transformer blocks. Each block consists of a multi-head self-attention layer, a multi-head cross-attention layer, and a feed-forward layer. For simplicity, we assume that all images are converted to a sequence of patches to be compatible with the transformer architecture. In the transformer architecture, we refer to the width of all linear layers as d."}, {"title": "2.2 Common approaches towards minimizing the computational cost of training diffusion transformers", "content": "Assuming transformer architectures where linear layers account for the majority of the computational cost, the total training cost is proportional to M \u00d7 N\u00d7S, where M is the number of samples processed, N is the number of model parameters, and S is the number of patches derived from one image, i.e., sequence size for transformer input (Figure 2a). As our goal is to train an open-world model, we believe that a large number of parameters are required to support diverse concepts during generation; thus, we opt to not decrease the parameter count significantly. Since diffusion models converge slowly and are often trained for a large number of steps, even for small-scale datasets, we keep this choice intact (Rombach et al., 2022; Balaji et al., 2022). Instead, we aim to exploit any redundancy in the input sequence to reduce computational cost while simultaneously not drastically degrade performance."}, {"title": "A. Using larger patch size (Figure 2b)", "content": "In a vision transformer, the input image is first transformed into a sequence of non-overlapping patches with resolution p\u00d7p, where p is the patch size (Dosovitskiy et al., 2020). Though using a higher patch size quadratically reduces the number of patches per image, it can significantly degrade performance due to the aggressive compression of larger regions of the image into a single patch."}, {"title": "B. Using patch masking (Figure 2c)", "content": "A contemporary approach is to keep the patch size intact but drop a large fraction of patches at the input layer of the transformer (He et al., 2022). Naive token masking is similar to training on random crops in convolutional UNets, where often upsampling diffusion models are trained on smaller random crops rather than the full image (Nichol et al., 2021; Saharia et al., 2022). However, in contrast to random crops of the images, patch masking allows training on non-continuous regions of the image2. Due to its effectiveness, masking-based training of transformers has been adopted across both vision and language domains (Devlin et al., 2018; He et al., 2022)."}, {"title": "C. Masking patches with autoencoding (MaskDiT - Figure 2d)", "content": "In order to also encourage representation learning from masked patches, Zheng et al. (2024) adds an auxiliary autoencoding loss that encourages the reconstruction of masked patches. This formulation was motivated by the success of the autoencoding loss in learning self-supervised representations (He et al., 2022). MaskDiT first appends a lightweight decoder, a small-scale transformer network, to the larger backbone transformer. Before the decoder, it expands the backbone transformer output by inserting spurious learnable patches in place of masked patches.\nAssuming that the binary mask m represents the masked patches, the final training loss is the following,\n\nLdiff = E(x,c)~DE\u20ac~N(0,0(t)\u00b21) || (Fo((x + c) \u00a9 (1 \u2212 m); o, c) - x) (1 \u2013 m) ||\n\n(5)\n\nLmae = E(x,c)~DE\u20ac~N(0,0(t)\u00b21) || (Fe((x + \u20ac) \u2299 (1 \u2212 m); o, c) \u2013 (x + \u20ac)) \u2299 m||2\n\n(6)\n\nL = Ldiff + YLmae\n\n(7)\nwhere Fe represents the sequential backbone transformer and decoder module, and y is a hyperparameter to balance the influence of the masked autoencoding loss with respect to the diffusion training loss."}, {"title": "2.3 Alleviating performance bottleneck in patch masking using deferred masking", "content": "To drastically reduce the computational cost, patch masking requires dropping a large fraction of input patches before they are fed into the backbone transformer, thus making the information from masked patches unavailable to the transformer. High masking ratios, e.g., 75% masking, significantly degrade the overall performance of the transformer. Even with MaskDiT, we only observe a marginal improvement over naive masking, as this approach also drops the majority of image patches at the input layer itself.\nDeferred masking to retain semantic information of all patches. As high masking ratios remove the majority of valuable learning signals from images, we are motivated to ask whether it is necessary to mask at the input layer. As long as the computational cost is unchanged, it is merely a design choice and not a fundamental constraint. In fact, we uncover a significantly better masking strategy, with nearly identical"}, {"title": "3 Experimental Setup", "content": "We provide key details in our experimental setup below and additional details in Appendix A.\nWe use two variants of Diffusion Transformer (DiT) architecture from Peebles & Xie (2023): DiT-Tiny/2 and DiT-X1/2, both comprising a patch size of 2. When training sparse models, we replace every alternate transformer block with an 8-expert mixture-of-experts block. We train all models using the AdamW optimizer with cosine learning rate decay and high weight decay. We provide an exhaustive list of our training hyperparameters in Table 5 in Appendix A. We use a four-channel variational autoencoder (VAE) from the Stable-Diffusion-XL (Podell et al., 2024) model to extract image latents. We also consider the latest 16-channel VAEs for test their performance in large-scale micro-budget training (Esser et al., 2024). We use the EDM framework from Karras et al. (2022) as a unified training setup for all diffusion models. We refer to our diffusion trasnformer as MicroDiT. We use Fr\u00e9chet inception distance (FID) (Heusel et al., 2017), both with the original Inception-v3 model (Szegedy et al., 2016) and a CLIP model (Radford et al., 2021), and CLIP score (Hessel et al., 2021) to measure the performance of the image generation models.\nChoice of text encoder. To convert natural language captions to higher-dimensional feature embeddings, we use the text encoder from state-of-the-art CLIP models\u00b3 (Ilharco et al., 2021; Fang et al., 2023). We favor CLIP models over T5-xxl (Raffel et al., 2020; Saharia et al., 2022) for text encoding, despite the better performance of the latter on challenging tasks like text synthesis, as the latter incurs much higher computational and storage costs at scale, thus not suitable for micro-budget training (Table 6 in Appendix A). We consider the computation of text embeddings for captions and latent compression of images as a one-time cost that amortizes over multiple training runs of a diffusion model. Thus, we compute them offline and"}, {"title": "4 Evaluating Effectiveness of Deferred Masking", "content": "In this section, we validate the effectiveness of our deferred masking approach and investigate the impact of individual design choices on image generation performance. We use the DiT-Tiny/2 model and the cifar-captions dataset (256 \u00d7 256 image resolution) for all experiments in this section. We train each model for 60K optimization steps, with the AdamW optimizer and exponential moving average with a 0.995 smoothing coefficient enabled for the last 10K steps."}, {"title": "4.1 Out-of-the-box performance: Making high masking ratios feasible with deferred masking", "content": "A key limitation of patch masking strategies is poor performance when a large fraction of patches are masked. For example, Zheng et al. (2024) observed large degradation in even MaskDiT performance beyond a 50% masking ratio. Before optimizing the performance of our approach with a rigorous ablation study, we evaluate its out-of-the-box performance with common training parameters for up to 87.5% masking ratios. As a baseline, we train a network with no patch-mixer, i.e., naive masking (Figure 2c) for each masking ratio. For deferred masking, we use a four-transformer block patch-mixer that comprises less than 10% of the parameters of the backbone transformer.\nWe use commonly used default hyperparameters to simulate the out-of-the-box performance for both our approach and the baseline. We train both models with the AdamW optimizer with an identical learning rate of 1.6\u00d710-4, 0.01 weight decay, and a cosine learning rate schedule. We set (Pmean, Pstd) to (\u22121.2, 1.2) following the original work (Karras et al., 2022). We provide our results in Figure 4. Our deferred masking approach achieves much better performance across all three performance metrics. Furthermore, the performance gaps widen with an increase in masking ratio. For example, with a 75% masking ratio, naive masking degrades the FID score to 16.5 while our approach achieves 5.03, much closer to the FID score of 3.79 with no masking."}, {"title": "4.2 Ablation study of our training pipeline", "content": "We ablate design choices across masking, patch mixer, and training hyperparameters. We summarize the techniques that improved out-of-the-box performance in Table 5a. We provide supplementary results and discussion of the ablation study in Appendix C.\nComparison with training hyperparameters of LLMs. Since the diffusion architectures are very similar to transformers used in large language models (LLMs), we compare the hyperparameter choices across the two tasks. Similar to common LLM training setups (Touvron et al., 2023; Jiang et al., 2023; Chowdhery et al., 2022), we find that SwiGLU activation (Shazeer, 2020) in the feedforward layer outperforms the GELU (Hendrycks & Gimpel, 2016) activation function. Similarly, higher weight decay leads to better image generation performance. However, we observe better performance when using a higher running average coefficient for the AdamW second moment (32), in contrast to large-scale LLMs where \u1e9e2 \u2248 0.95 is preferred. As we use a small number of training steps, we find that increasing the learning rate to the maximum possible value until training instabilities also significantly improves image generation performance.\nDesign choices in masking and patch-mixer. We observe a consistent improvement in performance with a larger patch mixer. However, despite the better performance of larger patch-mixers, we choose to use a small patch mixer to lower the computational budget spent by the patch mixer in processing the unmasked input. We also update the noise distribution (Pmean, Pstd) to (-0.6, 1.2) as it improves the alignment between captions and generated images. By default, we mask each patch at random. We ablate the masking strategy to retain more continuous regions of patches using block masking (Figure 5b). However, we find that increasing the block size leads to a degradation in the performance of our approach, thus we retain the original strategy of masking each patch at random."}, {"title": "4.3 Validating improvements in diffusion transformer architecture", "content": "We measure the effectiveness of the two modifications in transformer architecture in improving image generation performance.\nLayer-wise scaling. We investigate the impact of this design choice in a standalone experiment where we train two variants of a DiT-Tiny architecture, one with a constant width transformer and the other with layer-wise scaling of each transformer block. We use naive masking for both methods. We select the width of the constant-width transformer such that its computational footprint is identical to layer-wise scaling. We train both models for an identical number of training steps and wall-clock time. We find that the layer-wise scaling approach outperforms"}, {"title": "5 Comparing the Effectiveness of Deferred Masking with Baselines", "content": "In this section, we provide a concrete comparison with multiple baselines. First, we compare with techniques aimed at reducing computational cost by reducing the transformer patch sequence size. Next, we consider reducing network size, i.e., downscaling, while keeping the patch sequence intact. Under isoflops budget, i.e., identical wall-clock training time and FLOPs, we show that our approach achieves better performance than model downscaling.\nComparing different masking strategies. We first compare our approach with the strategy of using larger patches. We increase the patch size from two to four, equivalent to 75% patch masking. However, it underperforms compared to deferred masking and only achieves 9.38, 6.31, and 26.70 FID, Clip-FID, and Clip-score, respectively. In contrast, deferred masking achieves 7.09, 4.10, and 28.24 FID, Clip-FID, and Clip-score, respectively. Next, we compare all three masking strategies (Figure 2). We train each model for 60K steps and report total training flops for each approach to indicate its computational cost. We find that naive masking significantly degrades model performance, and using additional proxy loss functions (Zheng et al., 2024) only marginally improves performance. We find that simply deferring the masking and using a small transformer as a patch-mixer significantly bridges the gap between masked and unmasked training of diffusion transformers.\nIsoFLOPs study - Why bother using masking instead of smaller transformers? Using patch masking is a complementary approach to downscaling transformer size to reduce computation cost in training. We compare the two approaches in Table 7a. At each masking ratio, we reduce the size of the unmasked network to match FLOPs and total wall-clock training time of masked training. We train both networks for 60K training steps. Until a masking ratio of 75%, we find that deferred masking outperforms network down-scaling across at least two out of three metrics. However, at extremely high masking ratios, deferred masking tends to achieve lower performance. This is likely because the information loss from masking is too high at these ratios. For example, for 32 \u00d7 32 resolution latent and a patch size of two, only 32 patches are retained (out of 256 patches) at an 87.5% masking ratio.\nDeferred masking as pretraining + unmasked finetuning. We show deferred masking also acts as a strong pretraining objective and using it with an unmasked finetuning schedule achieves better performance"}, {"title": "6 Micro-budget Training of Large-scale Models", "content": "In this section, we validate the effectiveness of our approach in training open-world text-to-image generative models on a micro-budget. Moving beyond the small-scale cifar-captions dataset, we train the large-scale model on a mixture of real and synthetic image datasets comprising 37M images. We demonstrate the high-quality image generation capability of our model and compare its performance with previous works across multiple quantitative metrics."}, {"title": "6.1 Benefit of additional synthetic data in training", "content": "Instead of training only on real images, we expand the training data to include 40% additional synthetic images. We compare the performance of two MicroDiT models trained on only real images and combined real and synthetic images, respectively, using identical training processes.\nUnder canonical performance metrics, both models apparently achieve similar performance. For example, the model trained on real-only data achieved an FID score of 12.72 and a CLIP score of 26.67, while the model trained on both real and synthetic data achieved an FID score of 12.66 and a CLIP score of 28.14. Even on GenEval (Ghosh et al., 2024), a benchmark that evaluates the ability to generate multiple objects and modelling object dynamics in images, both models achieved an identical score of 0.46. These results seemingly suggest that incorporating a large amount of synthetic samples didn't yield any meaningful improvement in image generation capabilities.\nHowever, we argue that this observation is heavily influenced by the limitations of our existing evaluation metrics. In a qualitative evaluation, we found that the model trained on the combined dataset achieved much better image quality (Figure 9). The real data model often fails to adhere to the prompt, frequently hallucinating key details and often failing to synthesize the correct object. Metrics, such as FID, fail to capture this difference because they predominantly measure distribution similarity (Pernias et al., 2024). Thus, we focus on using human visual preference as an evaluation metric. To automate the process, we use GPT-40 (OpenAI, 2024), a state-of-the-art multimodal model, as a proxy for human preference. We supply the following prompt to the model: Given the prompt \u2018{prompt}', which image do you prefer, Image A or Image B, considering factors like image details, quality, realism, and aesthetics? Respond with 'A' or 'B' or 'none' if neither is preferred. For each comparison, we also shuffle the order of images to remove any ordering bias. We generate samples using DrawBench (Saharia et al., 2022) and PartiPrompts (P2) (Yu et al., 2022), two commonly used prompt databases (Figure 8). On the P2 dataset, images from the combined data model are preferred in 63% of comparisons while images from the real data model are only preferred 21% of the time (16% of comparisons resulted in no preference). For the DrawBench dataset, the combined data model is preferred in 40% of comparisons while the real data model is only preferred in 21% of comparisons. Overall, using a human preference-centric metric clearly demonstrates the benefit of additional synthetic data in improving overall image quality."}, {"title": "6.2 Ablating design choices on scale", "content": "We first examine the effect of using a higher dimensional latent space in micro-budget training. We replace the default four-channel autoencoder with one that has sixteen channels, resulting in a 4\u00d7 higher dimensional latent space. Recent large-scale models have adopted high dimensional latent space as it provides significant improvements in image generation abilities (Dai et al., 2023; Esser et al., 2024). Note that the autoencoder with higher channels itself has superior image reconstruction capabilities, which further contributes to overall success. Intriguingly, we find that using a higher dimensional latent space in micro-budget training hurts performance. For two MicroDiT models trained with identical computational budgets and training hyperparameters, we find that the model trained in four-channel latent space achieves better FID, Clip-score, and GenEval scores (Table 10). We hypothesize that even though an increase in latent dimensionality allows better modeling of data distribution, it also simultaneously increases the training budget required to train"}, {"title": "6.3 Comparison with previous works", "content": "Comparing zero-shot image generation on COCO dataset (Table 3). We follow the evaluation methodology in previous work (Saharia et al., 2022; Yu et al., 2022; Balaji et al., 2022) and sample 30K random images and corresponding captions from the COCO 2014 validation set. We generate 30K synthetic images using the captions and measure the distribution similarity between real and synthetic samples using FID (referred to as FID-30K in Table 3). Our model achieves a competitive 12.66 FID-30K, while trained with 14\u00d7 lower computational cost than the state-of-the-art low-cost training method and without any proprietary or humongous dataset to boost performance. Our approach also outperforms W\u00fcrstchen (Pernias et al.,"}, {"title": "7 Related work", "content": "The landscape of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020a;b) has rapidly evolved in the last few years, with modern models trained on web-scale datasets (Betker et al., 2023; Esser et al., 2024; Rombach et al., 2022; Saharia et al., 2022). In contrast to training in pixel space (Saharia et al., 2022; Nichol et al., 2021), the majority of large-scale diffusion models are trained in a compressed latent space, thus referred to as latent diffusion models (Rombach et al., 2022). Similar to auto-regressive models, transformer architectures (Vaswani et al., 2017) have also been recently adopted for diffusion-based image synthesis. While earlier models commonly adopted a fully convolutional or hybrid UNet network architecture (Dhariwal & Nichol, 2021; Nichol et al., 2021; Rombach et al., 2022; Saharia et al., 2022), recent works have demonstrated that diffusion transformers (Peebles & Xie, 2023) achieve better performance (Esser et al., 2024; Chen et al., 2023; Betker et al., 2023). Thus, we also use diffusion transformers for modeling latent diffusion models.\nSince the image captions in web-scale datasets are often noisy and of poor quality (Byeon et al., 2022; Schuhmann et al., 2022), recent works have started to recaption them using vision-language models (Liu et al., 2023; Wang et al., 2023; Li et al., 2024). Using synthetic captions leads to significant improvements in the diffusion models' image generation capabilities (Betker et al., 2023; Esser et al., 2024; Gokaslan et al., 2024; Chen et al., 2023). While text-to-image generation models are the most common application of diffusion models, they can also support a wide range of other conditioning mechanisms, such as segmentation maps, sketches, or even audio descriptions (Zhang et al., 2023; Yariv et al., 2023). Sampling from diffusion models is also an active area of research, with multiple novel solvers for ODE/SDE sampling formulations to reduce the number of iterations required in sampling without degrading performance (Song et al., 2020a; Jolicoeur-Martineau et al., 2021; Liu et al., 2022; Karras et al., 2022). Furthermore, the latest approaches enable single-step sampling from diffusion models using distillation-based training strategies (Song et al., 2023; Sauer et al., 2023). The sampling process in diffusion models also employs an additional guidance signal to improve prompt alignment, either based on an external classifier (Dhariwal & Nichol, 2021; Nichol et al., 2021; Sehwag et al., 2022) or self-guidance (Ho & Salimans, 2022; Karras et al., 2024). The latter classifier-free guidance approach is widely adopted in large-scale diffusion models and has been further extended to large-language models (Zhao et al., 2024; Sanchez et al., 2023).\nSince the training cost of early large-scale diffusion models was noticeably high (Rombach et al., 2022; Ramesh et al., 2022), multiple previous works focused on bringing down this cost. Gokaslan et al. (2024) showed that using common tricks from efficient deep learning can bring the cost of stable-diffusion-2 models under $50K. Chen et al. (2023) also reduced this cost by training a diffusion transformer model on a mixture of openly accessible and proprietary image datasets. Cascaded training of diffusion models is also used by some previous works (Saharia et al., 2022; Pernias et al., 2024; Guo et al., 2024), where multiple diffusion models are employed to sequentially upsample the low-resolution generations from the base diffusion model. A key limitation of cascaded training is the strong influence of the low-resolution base model on overall image fidelity and prompt alignment. Most recently, Pernias et al. (2024) adopted the cascaded training approach (W\u00fcrstchen) while training the base model in a 42\u00d7 compressed latent space. Though W\u00fcrstchen achieves low training cost due to extreme image compression, it also achieves significantly lower image generation performance on the FID evaluation metric. Alternatively, patch masking has been recently adopted as a means to reduce the computational cost of training diffusion transformers (Zheng et al., 2024; Gao et al., 2023), taking inspiration from the success of patch masking in contrastive models (He et al., 2022)."}, {"title": "8 Discussion and Future Work", "content": "As the overarching objective of our work is to reduce the overhead of training large-scale diffusion models, we not only aim to reduce the training cost but also align design choices in the training setup with this objective. For example, we deliberately choose datasets that are openly accessible and show that using only these datasets suffices to generate high-quality samples, thus omitting the need for proprietary or enormous datasets. We also favor CLIP-based text embeddings which, although underperforming compared to T5 model embeddings (especially in text generation), are much faster to compute and require six times less disk storage.\nWhile we observe competitive image generation performance with micro-budget training, it inherits the limitations of existing text-to-image generative models, especially in text rendering. It is surprising that the model learns to successfully adhere to complex prompts demonstrating strong generalization to novel concepts while simultaneously failing to render even simple words, despite the presence of a dedicated optical character recognition dataset in training. Similar to a majority of other open-source models, our model struggles with controlling object positions, count, and color attribution, as benchmarked on the GenEval dataset.\nWe believe that the lower overhead of large-scale training of diffusion generative models can dramatically enhance our understanding of training dynamics and accelerate research progress in alleviating the shortcomings of these models. It can enable a host of explorations on scale, in directions such as understanding end-to-end training dynamics (Agarwal et al., 2022; Choromanska et al., 2015; Jiang et al., 2020), measuring the impact of dataset choice on performance (Xie et al., 2024), data attribution for generated samples (Zheng et al., 2023; Dai & Gifford, 2023; Park et al., 2023; Koh & Liang, 2017), and adversarial interventions in the training pipeline (Chou et al., 2023; Shafahi et al., 2018). Future work can also focus on alleviating current limitations of micro-budget training, such as poor rendering of text and limitations in capturing intricate relationships between objects in detailed scenes. While we focus mainly on algorithmic efficiency and dataset choices, the training cost can be significantly reduced by further optimizing the software stack, e.g., by adopting native 8-bit precision training (Mellempudi et al., 2019), using dedicated attention kernels for H100 GPUs (Shah et al., 2024), and optimizing data loading speed."}, {"title": "9 Conclusion", "content": "In this work, we focus on patch masking strategies to reduce the computational cost of training diffusion transformers. We propose a deferred masking strategy to alleviate the shortcomings of existing masking approaches and demonstrate that it provides significant improvements in performance across all masking ratios. With a deferred masking ratio of 75%, we conduct large-scale training on commonly used real image datasets, combined with additional synthetic images. Despite being trained at an order of magnitude lower cost than the current state-of-the-art, our model achieves competitive zero-shot image generation performance. We hope that our low-cost training mechanism will empower more researchers to participate in the training and development of large-scale diffusion models."}]}