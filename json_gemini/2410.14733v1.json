{"title": "Knowledge Graph Embeddings: A Comprehensive Survey on Capturing Relation Properties", "authors": ["Guanglin Niu"], "abstract": "Knowledge Graph Embedding (KGE) techniques play a pivotal role in transforming symbolic Knowledge Graphs (KGs) into numerical representations, thereby enhancing various deep learning models for knowledge-augmented applications. Unlike entities, relations in KGs are the carriers of semantic meaning, and their accurate modeling is crucial for the performance of KGE models. Firstly, we address the complex mapping properties inherent in relations, such as one-to-one, one-to-many, many-to-one, and many-to-many mappings. We provide a comprehensive summary of relation-aware mapping-based models, models that utilize specific representation spaces, tensor decomposition-based models, and neural network-based models. Next, focusing on capturing various relation patterns like symmetry, asymmetry, inversion, and composition, we review models that employ modified tensor decomposition, those based on modified relation-aware mappings, and those that leverage rotation operations. Subsequently, considering the implicit hierarchical relations among entities, we introduce models that incorporate auxiliary information, models based on hyperbolic spaces, and those that utilize the polar coordinate system. Finally, in response to more complex scenarios such as sparse and dynamic KGs, this paper discusses potential future research directions. We explore innovative ideas such as integrating multimodal information into KGE, enhancing relation pattern modeling with rules, and developing models to capture relation characteristics in dynamic KGE settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge graphs (KGs) facilitate the establishment of diverse relations between entities through a directed graph structure, thereby endowing machines with capabilities akin to human understanding, inference, and application of common sense or domain-specific knowledge. While symbolic knowledge is highly interpretable for humans, it poses challenges for efficient machine processing. Drawing inspiration from word embedding techniques that convert symbolic words into numerical vectors, Knowledge Graph Embedding (KGE) models endeavor to embed symbolic entities and relations from KGs into a numerical representation space, preserving the original semantic and structural information of the KG [1-3]. This transformation enables efficient knowledge retrieval and reasoning through numerical computation. KGE techniques have been widely applied in relation extraction [4] during KG construction, entity alignment [5], KG completion [6], and in knowledge-enhanced downstream tasks such as question-answering systems [7], recommendation systems [8], and pre-trained language models [9]. As a result, KGE has emerged as a foundational and mainstream model for constructing and applying KGs [10].\nKGE approaches primarily concentrate on learning embeddings of entities and relations in a numerical space and evaluating the plausibility of each piece of knowledge through scoring functions. Consequently, many KGE models focus on designing the representation space [11], the scoring functions [12], and the encoding mechanisms for entities and relations [13]. Although there are several review papers on KGE, they mainly offer comprehensive overviews of the current research status [14-20]. The semantics of KGs are predominantly reflected in the relations between entities, which exhibit complex mapping characteristics such as one-to-one, one-to-many, many-to-one, and many-to-many, as well as symmetrical, anti-symmetrical, inverse, and composite relation patterns. Furthermore, implicit hierarchical relations often exist between entities in KGs. The ability to accurately model these relation characteristics significantly impacts the effectiveness of KGE and its performance across various tasks. However, the existing literature lacks a comprehensive summary and review of KGE models from the perspective of modeling relation characteristics. This paper, therefore,"}, {"title": "2 INTRODUCTION OF KG EMBEDDING", "content": "KGs are instrumental in structuring knowledge as a directed graph, where nodes denote entities and edges represent the relationships between them. This framework enables machines to process and reason with human-like understanding of knowledge. However, the symbolic representation of knowledge, while interpretable by humans, poses computational challenges for large-scale KGs and their integration into deep learning models, such as pre-trained language models.\nKnowledge Graph Embedding (KGE) techniques have emerged as a solution to this challenge. They automatically learn numerical vector representations of entities and relations from KG triples, thereby preserving the semantic and structural integrity of the KG in a form that is amenable to computational processes [22]. During the training phase, KGE models focus on establishing a numerical representation space for entities and relations, alongside a scoring function to evaluate the likelihood of triples. The objective function is meticulously designed to train the triples samples, allowing the model to learn embeddings that capture the essence of the KG [23]. Post-training, these embeddings can be deployed in various downstream applications, including the enhancement of pre-trained language models and the completion of KGs [24].\nThe efficacy of KGE models is typically assessed using a link prediction evaluation framework. This method determines the model's proficiency in predicting new triples after the completion of training. For instance, when predicting triples with a missing head entity (?, relation, tail entity), the process involves constructing candidate triples by substituting all entities in the KG for the head entity position and subsequently calculating the rank of the correct candidate triple among all contenders. Three prevalent evaluation metrics for KGE tasks include:\n(1) Mean Rank (MR), which indicates the average rank of all correct candidate triples.\n(2) Mean Reciprocal Rank (MRR), which represents the average of the reciprocal ranks of all correct triples.\n(3) Hits@N, which signifies the proportion of correct candidate triples ranked within the top N positions. A lower MR value and higher MRR or Hits@N values are indicative of superior KGE model performance.\nThe semantic richness of KGs is predominantly conveyed through the relations that interlink entities. These relations exhibit a variety of characteristics, including complex mapping features, diverse relation patterns, and hierarchical structures among entities. A pivotal challenge in contemporary KGE research is the effective modeling of these relation"}, {"title": "2.1 Introduction of Complex Mapping Features of Relations", "content": "Complex mapping features of relations include one-to-one (1-1), one-to-many (1-N), many-to-one (N-1), and many-to- many (N-N) relations. In this context, the entity in the 1 position is unique, implying that when the relation and another entity in a triple are determined, substituting the unique entity with any other entity results in incorrect knowledge. Conversely, the entity in the N position is non-unique, suggesting multiple valid choices for this position. For instance, in the N-1 relation BornIn, the tail entity London typically corresponds to multiple head entities, while a specific head entity, such as Tom, only corresponds to a single tail entity."}, {"title": "2.2 Introduction of Diverse Relation Patterns", "content": "Diverse relation patterns encompass symmetric, asymmetric, inverse, and composite relations. Symmetric relations, such as Classmate or SpouseOf, and asymmetric relations, such as Teacher, are inherent characteristics of the relations themselves. Triples containing symmetric relations remain valid even when the head and tail entities are interchanged. In contrast, triples with asymmetric relations become invalid upon swapping the head and tail entities. The inverse relation pattern involves two distinct relations that are inverses of each other. For instance, the triples (Tom, Teacher, Kevin) and (Kevin, Student, Tom) are both valid. Composite relations involve three relations where two relations can be combined to form another relation. For example, based on the two factual triples (Tom, BornIn, London) and (London, Capital, Britain), one can directly infer (Tom, Nationality, Britain). To ascertain whether a KGE model can effectively model a specific relation pattern, it is essential to verify if the triples representing the premises of the relation pattern are satisfied and align with the model's optimization objective. If the triple expressing the inference also aligns with the optimization objective, it indicates that the model can effectively model that type of relation pattern."}, {"title": "2.3 Introduction of Hierarchical Relations Between Entities", "content": "Since the two entities connected by each relation may belong to different semantic hierarchies, there are inherent hierarchical relations between entities. Modeling these hierarchical relations between entities is crucial for KGE models to effectively represent richer semantic information between entities. Due to the constraint goal of the TransE model being $h + r = t$ for triples containing 1-N relations, such as (Britain, City, London) and (Britain, City, Liverpool), the vector embeddings of entities London and Liverpool will be forced to be approximately the same, which is clearly unreasonable. Similarly, for two triples containing symmetric relations, such as (h, r, t) and (t, r, h) where h, r and t represent the head entity, relation, and tail entity, respectively, TransE would result in r = 0, rendering it incapable of effectively modeling symmetric relations. Furthermore, without auxiliary information, TransE cannot model implicit hierarchical relations between entities."}, {"title": "3 MODELS FOR COMPLEX RELATION MAPPING", "content": "The TransE model, while groundbreaking, faces limitations in capturing the nuances of complex relation mappings. To overcome these, current KGE models have adopted several innovative strategies, including relation-aware mapping of entity vector embeddings, embedding KGs into specific geometric spaces, utilizing tensor decomposition techniques on high-dimensional third-order tensors representing KGs, and employing neural networks to learn the intricate interactions between entities and relations."}, {"title": "3.1 Models Based on Relation-Aware Mapping", "content": "A subset of relation-aware mapping models builds upon the translational operation core to TransE. These models introduce relation-dependent entity mapping mechanisms to achieve complex relation mappings. TransH [25] pioneered this approach by associating each relation with a unique hyperplane and projecting entities onto these relation-specific hyperplanes. This innovation allows entities to possess distinct representations across different relations. The plausibility of a triple (h, r, t) is evaluated using the scoring function:\n$E(h,r,t) = ||h_\u22a5 + r - t_\u22a5||_{L1/L2}$\n(1)\n$h_\u22a5 = h - w_r^Tw h, t_\u22a5 = t - w_r^T w t$\n(2)\nwhere h\u2208 $R^k$, r\u2208 $R^k$, t \u2208 $R^k$ represent the embeddings of the head entity h, relation r, and tail entity t in a k-dimensional real vector space. w \u2208 $R^k$ is the normal vector of the hyperplane corresponding to relation r. $h_\u22a5$ \u2208 $R^k$ and $t_\u22a5$ \u2208 $R^k$ are the vector embeddings of the head and tail entities projected onto the hyperplane of relation r. $L1/L2$ represents the scalar score of the triple obtained through the $L_1$ or $L_2$ norm.\nFrom a different perspective, for one-to-many relations, such as the City relation where the same head entity, for example, Britain, corresponds to multiple tail entities, projecting these varied tail entity vectors onto the relation-specific hyperplane for City results in similar vector embeddings. This mechanism adeptly addresses the modeling challenges of one-to-many, many-to-one, and many-to-many complex mapping relations. However, TransH's requirement for all entities associated with the same relation to be projected onto a single hyperplane restricts entity expressiveness. TransR [26] addresses this by defining a relation space for each relation, projecting entities into specific relation spaces, which offers two representational learning advantages: (1) For a 1-N relation, multiple tail entities corresponding to the same head entity have distinct vector embeddings in entity space but similar embeddings in relation space; (2) Multiple originally similar entity vector embeddings can be effectively differentiated in relation space. The scoring function for TransR is defined as:\n$E(h,r,t) = ||M_rh + r - M_rt||_{L_1/L_2}$\n(3)\nwhere $M_r$ \u2208 $R^{d\u00d7k}$ is the projection matrix for relation h\u2208 $R^k$ and t\u2208 $R^k$ are the vector embeddings of the head and tail entities, and r\u2208 $R^d$ is the vector embedding of the relation.\nTransR's projection matrix is solely related to the relation, lacking the influence of the semantic information of the head and tail entities on the projection process. STransE [27] enhances TransR by learning two separate projection matrices for each relation, enabling the head and tail entities to be projected into their respective relational spaces. The scoring function is defined as:\n$E(h,r,t) = ||M_{r1}h + r - M_{r2}t||_{L1/L2}$\n(4)\nwhere $M_{r1}$ \u2208 $R^{k\u00d7k}$ and $M_{r2}$ \u2208 $R^{k\u00d7k}$ are the two projection matrices for relation r.\nHowever, STransE faces high complexity due to its extensive parameter set. To mitigate this, TransD [28] introduces a dynamic projection matrix constructed by integrating the relation projection vector, the head entity projection vector, and"}, {"title": "3.2 Models Based on Specific Representation Spaces", "content": "While models based on relation-aware mapping embed KGs into Euclidean vector spaces, models based on specific representation spaces embed KGs into diverse spaces such as Gaussian, manifold, and Lie groups. These specialized representation spaces inherently address the modeling challenges of complex relation characteristics.\nKG2E [33] embeds entities and relations into a high-dimensional Gaussian space, represented by Gaussian distributions: h~$N(\u03bc_h, \u03a3_h)$, r~$N(\u03bc_r, \u03a3_r)$, t~$N(\u03bc_t, \u03a3_t)$ where the mean vectors $\u03bc_h$ \u2208 $R^k$, $\u03bc_r$ \u2208 $R^k$ and $\u03bc_t$ \u2208 $R^k$ represent the central positions of entities and relations in the representation space, and the covariance matrices $\u03a3_h$ \u2208 $R^{k\u00d7k}$, $\u03a3_r$\u2208$R^{k\u00d7k}$, $\u03a3_t$\u2208 $R^{k\u00d7k}$ represent the uncertainties of entities and relations. KG2E evaluates the plausibility of a triple by computing the similarity between the difference of entity pairs $(t - h)$~$N(\u03bc_e = \u03bc_t - \u03bc_h, \u03a3_e = \u03a3_t + \u03a3_h)$ and r. It employs KL divergence and expected likelihood to design the scoring function E(h, r, t)as follows:\n$\\int_{x \\in R^k} N(x, \\mu_r, \\Sigma_r) log \\frac{N(x, \\mu_r, \\Sigma_r)}{N(x, \\mu_e, \\Sigma_e)} dx$\n(13)\n$\\int_{x \\in R^k} \\frac{N(x, \\mu_e, \\Sigma_e)}{N(x, \\mu_r, \\Sigma_r)} N(x, \\mu_e, \\Sigma_e)N(x, \\mu_r, \\Sigma_r)dx$\n(14)\nwhere for different tail entities in a 1-N relation, using these two similarity scoring functions can yield similar triple scores, thereby modeling complex relation characteristics.\nManifoldE [34] represents entities and relations in a manifold space, such as a high-dimensional sphere. It requires that the head entity and relation of each triple be the center of the sphere, with the tail entity positioned within the sphere. In this scenario, different tail entities in a 1-N relation only need to satisfy the condition of being within the sphere, cleverly achieving the effect of modeling complex relation characteristics. The scoring function is designed as follows:\n$E(h,r,t) = ||M_F(h,r,t) \u2013 D_r^2||$\n(15)\nwhere $M_F$ is the manifold function, and $D_r$ is the manifold parameter representing the radius of the sphere.\nTorusE [35] embeds KGs onto a compact Lie group torus. The characteristic of this representation space is that an entity representation [h] \u2208 $T^k$ and two different entity representations [$t_1$] \u2208 $T^k$ and [$t_2$] \u2208 $T^k$ can have similar differences on the Lie group torus, i.e., [$t_1$] - [h] and [$t_2$] - [h]. Adopting the optimization objective from TransE [h] +\n[r] = [t], these two differences [$t_1$] - [h] and [$t_2$] - [h] can represent the same relation embedding, thus modeling 1-N relations. TorusE defines three scoring functions as follows:\n$E_{L1}(h,r,t) = 2d_{L1}([h] + [r], [t])$\n(16)\n$E_{L2}(h,r,t) = (2d_{L2}([h] + [r], [t]))^2$\n(17)\n$E_{el2}(h,r,t) = (d_{el2}([h] + [r], [t])/2)^2$\n(18)"}, {"title": "3.3 Tensor Decomposition-Based Models", "content": "A variety of KGE models that leverage tensor decomposition techniques initially conceptualize the KG as a large-scale third-order tensor. Within this tensor, each element's value indicates the presence or absence of a corresponding triple in the KG: a value of I denotes the existence of the triple, while a 0 signifies its non-existence. Through tensor decomposition, the score of each triple is transformed into a bilinear operation among the representations of the head entity, tail entity, and the relation. This operation inherently addresses the complexities of modeling various relation characteristics.\nRESCAL [36] is the pioneering KGE model to employ tensor decomposition. It computes the score of each triple within the tensor through matrix multiplication involving low-dimensional entity vector embeddings and the relation matrix representations. The scoring function of RESCAL is defined as follows:\n$E(h,r,t) = h^T M_r t = \\sum_{i=0}^{k-1} \\sum_{j=0}^{k-1} [M_r]_{ij}[h]_i[t]_j$\n(19)\nwhere h \u2208 $R^k$ and t \u2208 $R^k$ represent the entity embeddings of the head entity h and the tail entity t in a k-dimensional real vector space, while $M_r$ \u2208 $R^{k\u00d7k}$ is the matrix representation of the relation r. Matrix multiplication facilitates obtaining analogous scores for different tail entity vectors associated with the same 1-N relation and head entity.\nDistMult [37] simplifies RESCAL by utilizing a diagonal matrix for each relation representation, substantially reducing the number of parameters for each relation. The scoring function is designed as:\n$E(h,r,t) = h^T diag(r)t = \\sum_{i=0}^{k-1} [h]_i[r]_i[t]_i$\n(20)\nwhere diag(r) e\u2208 $R^{k\u00d7k}$ denotes a diagonal matrix, and r\u2208 $R^k$ is the vector embedding of the relation.\nTuckER [38] employs Tucker decomposition, which decomposes a third-order tensor into a core tensor and three factor matrices. Based on this concept, TuckER calculates the score of each triple using tensor multiplication between the core tensor and the head entity vector, relation vector, and tail entity vector. The scoring function of TuckER is:\n$E(h,r,t) W \\times_1 h \\times_2 r \\times_3 t$\n(21)\nin which w\u2208 $R^{k\u00d7d\u00d7k}$ represents the core tensor, which can be considered shared weight parameters. h \u2208 $R^k$ and t\u2208 $R^k$ are the vector embeddings of the head and tail entities, respectively, and r\u2208 $R^d$ is the vector embedding of the relation. The symbol $x_n$ denotes the n-th mode tensor product."}, {"title": "3.4 Neural Network-Based Models", "content": "KGE models that employ neural networks utilize nonlinear operations and network architectures to capture the interactions between entities and relations, effectively addressing the complexity of modeling diverse relation characteristics.\nSME [39] is among the pioneering approaches to implement KGE using neural networks. Within its fully connected neural network, the vector embeddings of the head and tail entities are combined with the relation vector embeddings in"}, {"title": "4 MODELS FOR MULTIPLE RELATION PATTERNS", "content": "Knowledge Graph Embedding (KGE) models that rely on translation operations, such as TransE, excel at capturing antisymmetric relations but fall short when it comes to modeling symmetric relations. On the other hand, tensor decomposition-based models like RESCAL, as previously discussed, can naturally represent symmetric relations due to the commutative property of matrix multiplication. However, these models struggle with antisymmetric relations and complex patterns, including inverse and composite relations. Current models capable of handling various relation patterns primarily focus on enhancing tensor decomposition models, relation-aware mapping models, and those that treat relations as rotations between entities."}, {"title": "4.1 Modified Tensor Decomposition-Based Models", "content": "To overcome the limitation that real space matrix multiplication cannot effectively model antisymmetric relations, modified tensor decomposition-based KGE models have been developed to handle both symmetric and antisymmetric relations. ComplEx [45] pioneers the embedding of entities and relations into complex space, enhancing DistMult by employing Hamiltonian multiplication among the complex representations of head entities, relations, and tail entities. The scoring function is defined as:\n$E(h,r,t) = Re(h^T diag(r)\\bar{t})$\n(30)\nwhere h\u2208 $C^k$ and t \u2208 $C^k$ are the complex vector embeddings of the head and tail entities, respectively. r \u2208 $C^k$ is the complex vector embedding of the relation, $ \\bar{t}$ denotes the conjugate of t, and Re represents taking the real part of a complex"}, {"title": "4.2 Models Based on Modified Relation-Aware Mapping", "content": "PairRE [48] addresses the joint modeling of relation patterns and the complex mapping characteristics of relations by utilizing paired relation embeddings. Its scoring function is defined as:\n$E(h,r,t) = ||h\u25e6r_h \u2013 t\u25e6r_t ||$\n(34)\nwhere h \u2208 $R^k$ and t \u2208 $R^k$ are the vector embeddings of the head and tail entities, respectively. $r_h$ \u2208 $R^k$ and $r_t$ \u2208 $R^k$ are the paired vector embeddings of the relation, with $||h||^2 = ||t||^2 = 1$. This approach allows the head and tail entity vectors to be mapped using different relation vectors. For instance, in a 1-N relation, PairRE can automatically adjust $r_t$ to consist of smaller element values, effectively reducing the weight related to the tail entity vector t. This enables different tail entity embeddings corresponding to the same head entity and 1-N relation to meet the optimization target, achieving the goal of modeling complex relation mappings. Additionally, PairRE can model various relation patterns by ensuring the relation embeddings satisfy the following constraints: (1) Symmetric relations: $r_h^2 = r_t^2$. (2) Antisymmetric relations:\n$r_h^2 \u2260 r_t^2$. (3) Inverse relations: $r_h\u25e6r_t = r_t\u25e6r_h$. (4) Composite relations: $r_h\u25e6r_{or} = r_{or}\u25e6r_t$.\nTripleRE [49] extends the PairRE model by representing relations as mappings and translations for both the head and tail entities. Its scoring function is designed as follows:\n$E(h,r,t) = ||h\u25e6r_h \u2212 t \u25e6 r_t + r_m||$\n(35)\nwhere $r_m$ \u2208 $R^k$ represents the relation vector for translating entities, while all other symbols are as defined in PairRE.\nTranS [50] further extends TripleRE by requiring two mapping operations and three translation operations. The scoring function is defined as follows:\n$E = ||h \\cdot t_h \u2212 t \\cdot h_t + r_h \\cdot h + r + r_t \\cdot t||$\n(36)\nin which $t_h$\u2208 $R^k$ represents the auxiliary tail entity vector, $h_t$ \u2208 $R^k$ represents the auxiliary head entity vector, $r_h$ \u2208 $R^k$ represents the auxiliary relation vector associated with the head entity, and $r_t$ \u2208 $R^k$ represents the auxiliary relation vector associated with the tail entity."}, {"title": "4.3 Rotation-Based Models", "content": "Rotation-based KGE models aim to simultaneously capture symmetric, antisymmetric, inverse, and composite relation patterns by combining the strengths of translation-based models for antisymmetric, inverse, and composite relations with those of improved tensor decomposition models for symmetric relations.\nRotatE [51] pioneered the approach of viewing the transformation from the head entity to the tail entity as a rotation operation. This model embeds entities and relations into the complex vector space and employs the Hadamard product to achieve this rotation. The scoring function of RotatE is designed as follows:\n$E(h,r,t) = ||h\u25e6r-t||$\n(37)\nwhere h\u2208 $C^k$ and t \u2208 $C^k$ are the complex vector embeddings of the head and tail entities, respectively. r \u2208 $C^k$ is the complex vector embedding of the relation, and \u25e6 denotes the Hadamard product, which calculates the element-wise product of two vectors. Specific to a triple (h, r, t), the scoring function of RotatE is h\u25e6r = t. Based on this optimization objective, RotatE has been theoretically proven to model various relation patterns, including symmetric, antisymmetric, inverse, and composite relations. The relation embeddings should satisfy specific constraints: (1) Symmetric relations: $r \\cdot \\bar{r}$ = 1. (2) Antisymmetric relations: $r \\cdot \\bar{r}$ \u2260 1. (3) Inverse relations: $r_1 = \\bar{r_1}^{-1}$. (4) Composition relations: $r_1 = r_2 \u25e6 r_3$.\nHowever, RotatE implements rotations on a complex plane using Euler angles in complex vector space, and encounters singularity issues. To ensure more stable rotation operations, QuatE [52] embeds entities and relations in quaternion space and achieves rotations on planes with Hamilton multiplication. The scoring function of QuatE is defined as follows:\n$E(h,r,t) = h \\odot (r/|r|) \\cdot t$\n(38)"}, {"title": "5 MODELS FOR HIERARCHICAL RELATIONS", "content": "Capturing the inherent hierarchical relations between entities in KGs is a critical aspect of Knowledge Graph Embedding (KGE). Current research primarily focuses on three approaches: leveraging additional auxiliary information, employing hyperbolic space, and utilizing polar coordinate systems."}, {"title": "5.1 Auxiliary Information-Based Models", "content": "HCE [58] capitalizes on the hierarchical structure of entity types within the ontology layer of KGs. It concurrently learns entity and type embeddings, predicated on the task of predicting contextual entities. HCE introduces the ancestor categories of an entity's type when learning an entity's embedding. If a type embedding is in proximity to an entity embedding, the ancestor type embeddings of that type will also be close to the entity embedding, thereby integrating hierarchical relations between types into the entity embeddings.\nHRS [59] concentrates on the hierarchy among relations. It clusters relations into higher-level relations and extends TransE to calculate the difference between the tail and head entity vectors in the triples associated with each relation. These differences are then clustered to derive lower-level sub-relations, with the original relations forming the middle layer. This method jointly represents the translation operation from the head entity to the tail entity across three levels of relations. The scoring function is defined as follows:\n$E(h,r,t) = ||h + r_c + r + r_s \u2013 t||_{L1/L2}$\n(44)\nwhere $r_c$, r, and $r_s$ represent the vector embeddings of higher-level clustered relations, middle-layer relations, and lower- level sub-relations, respectively.\nTKRL [60] learns the embeddings of all entity types at different levels through hierarchical type mapping matrices, thereby transforming a triple into associations among multiple hierarchical entity types. Its scoring function is defined as follows:\n$E(h,r,t) = ||M_hh + r - M_tt||_{L1/L2}$\n(45)\nwhere $M_h$ and $M_t$ are the hierarchical type mapping matrices. TKRL designs two hierarchical type encoding mechanisms. The first is cyclic hierarchical encoding, which maps an entity into the most specific lower-level entity type representation space first, then into higher-level entity type representation spaces. This mechanism gradually and cyclically maps an entity from lower-level types to higher-level types. The type mapping matrix for cyclic hierarchical encoding is defined as follows:\n$M_c = M_{c(1)} M_{(2)} ... M_{c(n)}$\n(46)\nin which n represents the level of an entity type within the overall hierarchical structure, and $M_{\u00bf(i)}$ denotes the type mapping matrix of the i-th level, with the first level being the lowest. The second hierarchical type encoding mechanism is weighted hierarchical encoding, where different weights reflect the hierarchical structure of types. The type mapping matrix for weighted hierarchical encoding is defined as follows:\n$M_c = \u03b2_1M_{c (1)} + \u03b2_2M_{(2)} + \u2026 + \u03b2_\u03b7M_{c(n)}$\n(47)"}, {"title": "5.2 Hyperbolic Space-Based Models for Hierarchical Relations", "content": "The hierarchical structure inherent in KGs can be effectively modeled using hyperbolic space, which offers a more natural representation for tree-like hierarchies than Euclidean space. Hyperbolic embeddings enable high-level, abstract entities to be represented with lower-dimensional vectors, while more specific, lower-level entities are captured with higher-dimensional vectors.\nPoincare Embeddings [61] pioneered the embedding of KGs into hyperbolic space, particularly focusing on \"is-a\" hierarchical relations. By mapping the KG into the Poincare ball, a distinctive geometric space, this model places higher- level entities closer to the center of the ball and lower-level entities further away. The scoring function for Poincare embeddings is given by:\n$E(h, t) = arcosh(1 + 2 \\frac{||h-t||^2}{(1 \u2212 ||h||^2)(1 \u2212 ||t||^2)} )$\n(49)\nwhere h \u2208 $B^k$ and t \u2208 $B^k$ are the vector embeddings of the head and tail entities in the Poincare ball space, respectively.\nMuRP [62] extends Poincare to accommodate more complex hierarchical structures with a variety of relations. It combines the geometry of the Poincare ball with a triple scoring function, resulting in a scoring function that can handle multiple relations:\n$E(h,r,t) = -d_B(h(r), t(r))^2 + b_h + b_t = -ds(exp_h (Rlog_h(h)), t \u2295_c r)^2 + b_h + b_t$\n(50)\nin which h\u2208 $B^k$ and t \u2208 $B^k$ are the hyperbolic embeddings of the head and tail entities in the Poincare ball space. R \u2208 $B^{kxk}$ is the diagonal matrix corresponding to relation r, and r \u2208 $B^k$ represents the hyperbolic translation vector for relation r. The hyperbolic embedding of the head entity $h(r)$ \u2208 $B^k$ is obtained through Mobius matrix-vector multiplication to interact with the relation, while the hyperbolic embedding of the tail entity $t(r)$ \u2208 $B^k$ is obtained through Mobius addition $\u2295_c$ to interact with the relation.\nSince MuRP represents entities and relations in a hyperbolic space with fixed curvature, it cannot model multiple relation patterns. ATTH [63] leverages the more expressive rotation and reflection operations from DihEdral and incorporates an attention mechanism to adjust the importance of rotation and reflection operations. It simultaneously models multiple relation patterns and hierarchical relations between entities. The scoring function of ATTH is defined as follows:\n$E(h,r,t) = \u2212d_D(h(r), t)^2 + b_h + b_t$\n(51)\n$h(r) = Att(h_{rot}, h_{ref})\u2295_c r$\n(52)\n$Att(h_{rot}, h_{ref}) = exp(a\u2295h_{rot} + a\u2295h_{ref})$\n(53)\n$(a, a) = softmax(a_{log(h)}, a_{log(t)} )$\nwhere $h(r)$ \u2208 $B^k$ represents the hyperbolic embedding of the head entity obtained through weighted rotation and reflection operations using an attention mechanism. $h_{rot}$ \u2208 $B^k$ is the head entity embedding after the relation-associated rotation operation, and $h_{ref}$ \u2208 $B^k$ is the head entity embedding after the relation-associated reflection operation. a \u2208 $B^k$ and a\u2208 $B^k$ are the attention weight parameters associated with $h_{rot}$ and $h_{ref}$, respectively. Att represents the attention mechanism."}, {"title": "5.3 Polar Coordinates-Based Models", "content": "HAKE [68] is the pioneering model that employs polar coordinates for KGE. In the polar coordinate system, a point in 2D space is defined by its magnitude and phase angle. HAKE transforms the head entity to the tail entity through operations specific to each relation, where the magnitude component represents scaling at a fixed angle, and the phase angle component signifies rotation at a fixed magnitude. This allows the magnitude to model entities at different hierarchical levels, while the phase angle models different entities at the same level. The scoring function of HAKE is defined as follows:\n$E(h,r,t) = \u03b1d_{r,m}(h_m, t_m) + (1 - \u03b1)d_{r,p}(h_p, t_p)$\n(54)\n$d_{r.m}(h_m, t_m) = ||h_m \u25e6 r_m - t_m||_{L2}$\n(55)\n$d_{r,p}(h_p, t_p) = ||sin ((h_p + r_p - t_p)/2)||_{L1}$\n(56)\nwhere $d_{r,m}(h_m, t_m)$ and $d_{r,p}(h_p, t_p)$ represent the scores of the triple based on magnitude and phase angle, respectively, with \u03b1 being the weighting parameter that balances these two scores. $h_m$ \u2208 $R^k$ and $t_m$ \u2208 $R^k$ are the magnitude vector embeddings of the head and tail entities, respectively. h and t\u2208 R, respectively. $r_m$ \u2208 R is the magnitude vector embedding with all positive elements. h \u2208 $[0,2\u03c0)^k$, r \u2208 $[0,2\u03c0)^k$, and t \u2208 $[0,2\u03c0)^k$ are the phase angle vector embeddings of the head entity, relation, and tail entity, respectively.\nNotably, HAKE's core idea is that the higher the semantic level of an entity, the shorter its magnitude; conversely, the lower the semantic level, the longer its magnitude. During model training, the magnitude representation of relations tends to conform to the following constraints: (1) The head entity has a higher semantic level than the tail entity: $[r_m]_i$ > 1. (2) The head and tail entities are at the same semantic level: $[r_m]_i$ = 1. (3) The head entity has a lower semantic level than the tail entity: $[r_m]_i$ < 1."}, {"title": "6 FUTURE RESEARCH DIRECTIONS", "content": "Current research in KGE is heavily focused on modeling complex relation mappings, various relation patterns, and hierarchical relations between entities. While significant strides have been made, there is room for further advancement, particularly in enhancing the effectiveness of KGE models and their performance in"}]}