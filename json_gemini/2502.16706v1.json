{"title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling", "authors": ["Jonathan Light", "Wei Cheng", "Wu Yue", "Masafumi Oyamada", "Mengdi Wang", "Santiago Paternain", "Haifeng Chen"], "abstract": "Inference scaling methods often rely on decomposing problems into steps (or groups of tokens), followed by sampling and selecting the best next steps. However, these steps and their sizes are often predetermined or manually designed based on domain knowledge. We propose dynamic decomposition, a method that adaptively and automatically fractions solution and reasoning traces into manageable steps during inference. By more effectively allocating compute \u2013 particularly through subdividing challenging steps and prioritizing their sampling \u2013 dynamic decomposition significantly improves inference efficiency. Experiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate that dynamic decomposition outperforms static approaches, including token-level, sentence-level, and single-step decompositions. These findings highlight the potential of dynamic decomposition to improve a wide range of inference scaling techniques.", "sections": [{"title": "1. Introduction", "content": "Scaling inference efficiency remains a fundamental challenge for large language models (LLMs). Many existing approaches improve inference by decomposing problems into smaller steps and systematically exploring different solutions (Feng et al., 2023; Zeng et al., 2024; Wu et al., 2024; Nori et al., 2024; Snell et al., 2024; Brown et al., 2024; Gandhi et al., 2024; Lee et al., 2025; Light et al., 2024a; Wang et al., 2025).\nSome decomposition methods rely on domain-specific heuristics and hand-crafted rules (Yao et al., 2024; Zelikman et al., 2023; Zhou et al., 2022). However, manually partitioning problems or designing task-specific heuristics is costly and lacks generalization. Moreover, identifying critical steps for an LLM can be non-trivial for humans. As shown in Sec. 3.5, LLMs may assign importance to seemingly trivial words (e.g., therefore or which), which, while counterintuitive to humans, play a crucial role in autoregressive generation (Lin et al., 2025). Other approaches employ fixed, uniform step sizes, such as token- or sentence-level decomposition (Feng et al., 2023; Guo et al., 2025). All these methods rely on static decomposition strategies, where step sizes are predefined or determined via heuristics. Such rigidity wastes compute on steps that are easy for the LLM (but potentially difficult for humans) while undersampling more challenging steps.\nTo overcome these limitations, we propose DISC (Dynamic decomposition Improves Scaling Compute), a recursive inference algorithm that dynamically partitions solution steps based on difficulty. Unlike prior methods, DISC adapts decomposition granularity during inference based on both the available budget and problem complexity, ensuring finer granularity for more difficult steps. By leveraging the autoregressive nature of LLMS, DISC efficiently locates difficult steps through binary partitioning, focusing compute on challenging regions rather than wasting resources on trivial steps. DISC is generalizable and requires no human supervision, domain-specific heuristics, prompt engineering, or process annotations, making it widely applicable across tasks.\nOur main contributions are:\n\u2022 We introduce DISC, a method for recursive partitioning and decomposing solutions during inference without human supervision, domain-specific heuristics, or process reward models.\n\u2022 We demonstrate how DISC integrates decomposition with inference-time search, allocating compute to high-impact, difficult steps.\n\u2022 We show that DISC improves inference scaling in terms of both sample efficiency and token efficiency.\n\u2022 We provide insights into how LLMs reason and plan by identifying critical steps in their generation process."}, {"title": "2. Preliminaries", "content": "2.1. Problem Setting\nWe consider a reasoning and code generation setting where a dataset $X = \\{x^{(i)}\\}_{i=1}^n$ consists of problem prompts x, and a reward model $R : X \\times Y \\rightarrow [0, 1]$ evaluates generated solutions $y \\in \\mathcal{y}$. This includes program synthesis, where correctness is verified using ground-truth tests (Chen et al., 2021; Austin et al., 2021), and mathematical reasoning, where solutions are validated numerically (Hendrycks et al., 2021a; Cobbe et al., 2021). The reward model can be a ground-truth verifier, a trained heuristic (Zhang et al., 2024), self-consistency (Wang et al., 2023a), or an LLM-as-a-judge (Zheng et al., 2023). Since our focus is on step decomposition rather than verification, we use the ground-truth reward model where available. We assume access to a pretrained language model \\( \\pi \\) that generates text autoregressively. A generated response y consists of both the final solution and the reasoning chain leading to it, and can be represented as a sequence of tokens $y = (y_0, \\ldots, y_{L_y})$. Additionally, solutions can be partitioned into solution steps $Y = (y_0, \\ldots, y_K)$, where each step $y_i$ is a contiguous string of tokens. A partial solution up to step k is defined as $Y_{1 \\ldots k}:= y_1 \\cdot y_2 \\cdot \\ldots \\cdot y_k$, and its rollout or completion, denoted $Y_{1 \\ldots k+}$, is the continuation generated by \\( \\pi \\) until an end-of-sequence token (EOS). The size of a solution step, \\( \\left|y_i\\right| \\), refers to its length in tokens or characters.\n2.2. Prior Automatic Decomposition Methods\nSingle-step generation. In a single-step generation, the entire solution is generated in one pass from the prompt to the EOS token, treating it as a single action. This approach underlies the widely used inference scaling method best of n (BON) (Cobbe et al., 2021; Lightman et al., 2023; Snell et al., 2024; Liang et al., 2024), where n complete solutions are sampled, and the highest-scoring one is selected. Single-step generation also plays a role in alignment and fine-tuning methods such as DPO (Rafailov et al., 2024) and RLOO (Ahmadian et al., 2024).\nToken-level decomposition. At the opposite end of the spectrum, token-level decomposition treats each atomic token as an individual step. While this approach dramatically increases search complexity, it enables fine-grained search that can yield higher performance gains given sufficient compute (Feng et al., 2023).\nNewline and sentence-level decomposition. A commonly used decomposition method segments LLM generations into sentences or lines based on delimiters such as periods or newlines (Hao et al., 2023; Feng et al., 2023; Yao et al., 2024). Typically, each newline corresponds to a new paragraph, equation, or line of code, which often encapsulates a distinct reasoning step."}, {"title": "2.3. Step Sampling", "content": "Inference-time scaling methods must balance exploration at the current step with exploration of future steps. We implement a simple dynamic sampling process, referred to as negative binomial sampling, where we continue sampling completions until the sum of their rewards exceeds a predefined threshold \\( \\sigma \\). More formally, the number of samples M drawn from a partial solution \\( Y_{1 \\ldots k} \\) is the smallest integer satisfying $\\sum_{i=1}^{M} R(x \\cdot y_{k+}^{(i)}) \\geq \\sigma$.\nWhere \\( y_{k+}^{(i)} \\) represents the i-th sampled completion from the partial solution. This process ensures efficient allocation"}, {"title": "2.4. Inference Scaling Methods and Decomposition", "content": "Since our study focuses on decomposition rather than search, we primarily use greedy step search as the search method. In greedy step search, multiple candidate steps are sampled at each iteration, but only the highest-scoring step is retained, while the rest are discarded. The process then repeats, conditioning future steps on the best step found so far. We also perform ablation studies comparing Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Light et al., 2024b) and beam search (Xie et al., 2024), two commonly used inference scaling methods. These comparisons, presented in Sec. 4.2, highlight how different search strategies interact with decomposition. Additional details on MCTS and beam search are provided in App. E."}, {"title": "3. Methodology", "content": "3.1. DISC Algorithm\nThe DISC algorithm employs recursive binary decomposition to iteratively break down complex solutions into smaller, more manageable steps. Given a problem prompt x, the algorithm outputs a decomposition of a solution, $Y = (Y_1, Y_2, ..., Y_K)$, such that the concatenation $Y_{1 \\ldots K}$ forms a complete solution to \u00e6.\nThe algorithm operates in two key stages:\n1. Solution sampling. Starting from x and $y_0 = \\emptyset$, the algorithm generates complete solutions $Y_k \\sim \\pi(\\cdot|x \\cdot Y_{1 \\ldots (1-k)})$ using a policy \\( \\pi \\) like in single step generation. The best solution, \\( y^* \\), is selected based on the reward model R.\n2. Recursive partitioning. The selected solution \\( y^* \\) is partitioned into two segments, \\(y^* = y_a \\cdot y\\), based on a predefined partition fraction \\( \\alpha \\), where $\\left|y_a\\right| \\approx \\alpha \\left|y^*\\right|$. For each part, a priority metric h is estimated: $h(y_a|x \\cdot Y_{1 \\ldots (k-1)})$ and $h(y|x \\cdot Y_{1 \\ldots a})$, usually through rollouts of the step using \\( \\pi \\). The part with the lower priority is further partitioned.\n\u2022 If $h(y_a|x \\cdot Y_{1 \\ldots (k-1)}) \\geq h(y|x \\cdot Y_{1 \\ldots a})$, additional samples are sampled for y, with the process repeating on the new best solution, \\(y^*\\).\n\u2022 Conversely, if $h(y_a|x \\cdot Y_{1 \\ldots (k-1)}) < h(y|x \\cdot Y_{1 \\ldots a})$, the first segment \\( y_a \\) is further partitioned. The first step corresponds to the \\( \\alpha \\) fraction of \\( y^* \\), with the remaining part of the full solution forming the second step.\nThis recursive process is illustrated in Fig. 2. The pseudocode for DISC is provided in Alg. 1, with an annotated Python implementation in App. A.\nThe priority metric h serves as the central heuristic for determining which solution steps to prioritize. It estimates the \"difficulty\" or \u201cpotential for improvement\u201d of a step \\( y_k \\) given the context $x \\cdot Y_{1 \\ldots (k-1)}$, computed via rollouts of the policy \\( \\pi \\). Specifically, $h(y_k|x \\cdot Y_{1 \\ldots (k-1)})$ is estimated by sampling continuations and evaluating their outcomes.\nIn practice, estimating h and generating new samples occur simultaneously, as both rely on rollout-based computations (Sec. 3.2). Unlike standard decomposition methods, DISC does not process steps in strict temporal order, resembling goal-directed planning (Parascandolo et al., 2020) and backtracking."}, {"title": "3.2. Priority Metric", "content": "We consider two intuitive priority metrics for step selection: Q-value priority and Z-score priority, which are visualized in Fig. 4.\nQ-value based priority (DISC-Q). Given a partial solution $Y_{1 \\ldots k}= Y_{1 \\ldots (k-1)} \\cdot y_a \\cdot y$, we aim to prioritize either \\( y_a^* \\) or \\( y^* \\) for refinement. The core intuition behind Q-value prioritization is that steps with lower Q-values indicate areas needing refinement, directing compute toward the most challenging parts of the solution. More formally, we define the Q-priority metric under policy \\( \\pi \\) as:\n$h_Q(Y | x \\cdot Y_{1 \\ldots (k-1)}) = E_{y^*}[Q^{\\pi}(y_k|x \\cdot Y_{1 \\ldots (k-1)})]$.\nHere, \\( y^* \\) represents alternative steps sampled from \\( \\pi \\), and $Q^{\\pi}(y_k | x \\cdot Y_{1 \\ldots (k-1)})$ denotes the Q-value of \\( y_k \\), conditioned on the partial solution $x \\cdot Y_{1 \\ldots (k-1)}$. Equivalently, $h_Q(y)$ can be interpreted as the value function $V^{\\pi}(x \\cdot Y_{1 \\ldots (k-1)})$, where $V^{\\pi}$ represents the expected reward achievable from the given partial solution. The expectation is taken over sampled candidates $y_k \\sim \\pi(\\cdot | x \\cdot Y_{1 \\ldots (k-1)})$, constrained by \\( \\left|y_k\\right| = \\left|y\\right| \\). This metric helps identify difficult steps that the LLM is likely to get wrong, guiding partitioning toward the most critical refinements.\nTo estimate \\( h_Q \\) for the second step \\( y^* \\), we sample y from \\( \\pi \\), generate rollouts \\( Y_{(k+1)+} \\), compute rewards, and average the outcomes. For the previous step $y_a$, we reuse rollouts from earlier partitioning, as the mean of these previously generated samples provides an unbiased estimate of the Q-priority metric:\n$h_Q(Y_a | x \\cdot Y_{1 \\ldots (k-1)}) = E \\left[R(Y_{1 \\ldots (k-1)+})\\right]$.\nBy leveraging existing rollouts, DISC avoids redundant sampling, improving computational efficiency. Once the rollouts for \\( Y_{(k+1)+} \\) are available, the best completion Y(k+1)+ is selected as the next step to partition. This dual use of rollouts optimizes both metric estimation and inference sampling.\nZ-score based priority (DISC-Z). To allocate more compute to steps with higher potential for improvement, we estimate the probability of sampling a better step given existing samples. Assuming the Q-values of sampled steps"}, {"title": "3.3. DISC and Search Methods", "content": "DISC can also be used to enhance Monte Carlo Tree Search (MCTS) and other inference scaling and search methods. Recall that in our partition step, we greedily partition the best solution step y*. Instead of greedily partitioning the best step, we can partition the top k best steps instead, and select which step to partition using the upper confidence tree (UCT) formula. We can also use beam search to prune out steps we do not want to partition further. We explain MCTS and beam search in detail in App. E and present results of combining DISC with search in Sec. 4.9."}, {"title": "3.4. A Motivating Example on DISC-Z", "content": "We use the Wiener process W(t) as an example where there are intractably many actions and steps. Suppose we start at t = 0 with W(0) = 0. At each round k, the algorithm can choose one of the two options:\n1. samples a trajectory and observe the final value W(T) at time t = T, as the reward signal. Denote the whole trajectory as wk(\u00b7).\n2. chooses one trajectory from the previous rounds (denoted as ws(t) for round s), and time to; then sample a trajectory at t = to with W(to) = ws(to). Denote the concatenated trajectory as wk(\u00b7) with wk(t) = ws(t) when t < to.\nNote that we are only able to observe the final reward W(t). At any intermediate time t \u2208 (0, T), the current value W(t) is not observable. The goal is to design an algorithm that can reach the highest reward among the K trajectories. Formally speaking, we aim to maximize the maximum:\n$\\max_{k \\in K} w_k(T)$.\nOne naive solution is to call option 1 for K times and return the best-of-K reward, each following:\nW(T) ~ N(0,T).\nAlternatively, suppose there is a promising path w(\u00b7) with a high final reward w(T) = R. It is natural to consider starting at some midpoint \u03b1T (0 < \u03b1 < 1) and perform more completions to obtain an even higher reward than R. The reward distribution sampled this way is\nW'(T) ~ N(w(\u03b1T), (1 \u2212 \u03b1)T).\nThe remaining question is which \u03b1 we should choose. One option is to maximize the probability that the newly sampled reward is higher than R:\nP(W'(T) > R) = 1 \u2212 \u03a6$\\frac{R-w(\\alpha T)}{\\sqrt{(1-\\alpha)T}}$"}, {"title": "3.5. Example Decomposition", "content": "With sample budget 100, the decomposition of a MATH problem is as follows, where color indicates the value of the priority metric h of each step (yellow low, purple high)."}, {"title": "4. Experimental Results", "content": "4.1. Benchmarks\nWe evaluate DISC on three benchmarks: APPS, MATH, and LiveCodeBench, to assess its impact on inference scaling for both coding and reasoning.\nAPPS (Hendrycks et al., 2021a) consists of 5000 competitive programming problems across three difficulty levels, with the competition-level subset being the hardest. We evaluate on a 200-problem subset due to computational constraints. MATH (Hendrycks et al., 2021b) comprises 12,500 math problems. Since the ground-truth verifier provides only binary rewards, we use a pretrained ORM (Xiong et al., 2024), trained via the method in (Wang et al., 2024b), with Llama-3.1-8B-Instruct as the base model. We test on a 500-problem subset (MATH500), identical to prior work (Wang et al., 2024b; Lightman et al., 2023). LiveCodeBench (Jain et al., 2024) is a continuously updated dataset from Leetcode, AtCoder, and CodeForces, ensuring LLMs have not been exposed to test problems. We evaluate on the 108 problems uploaded between 10/01/2024 and 12/01/2024 to prevent contamination."}, {"title": "4.2. Decomposition Comparison", "content": "We compare DISC against three prior decomposition methods from Sec. 2.2: TokenSplit (token-level decomposition), LineSplit (newline-based decomposition), and BoN (treating the entire solution as a single step).\nAcross all benchmarks, DISC achieves superior scaling and performance under both fixed token budgets (Fig. 7) and sample budgets (Fig. 12). We evaluate two key metrics: Pass@k, the proportion of problems solved within a sample budget k, and Pass@token, the proportion solved within a given token budget. Notably, DISC consistently outperforms static decomposition methods on APPS, MATH, and LiveCodeBench (Fig. 5, 6, 7), demonstrating its ability to allocate compute adaptively for improved inference efficiency. Extended results and analyses for each benchmark are provided in App. D.1, D.4, and D.5."}, {"title": "4.3. Decomposition Analysis and Interpretation", "content": "Our results strongly indicate that decomposition\u2014whether line-based, token-based, or DISC\u2014improves sample quality. Figures 8 and 9 illustrate how the mean and variance of sampled rewards evolve with the step number, which represents the order in which a step is explored. Higher step numbers correspond to deeper search levels, where solutions are partitioned into finer-grained steps. As shown in Fig. 8, increasing step number correlates with higher-quality solutions, demonstrating that finer-grained decomposition"}, {"title": "4.4. Interaction Between Temperature and DISC", "content": "We perform ablation studies to analyze the impact of temperature on DISC. Typically, inference scaling methods achieve optimal performance at temperatures around 0.6-0.8, as increased temperature promotes sample diversity (Wang et al., 2024a). Surprisingly, however, DISC performs better at lower temperatures, as shown in Fig. 11. This trend is in stark contrast to BoN (Fig. 22), where higher"}, {"title": "4.5. Self-Generated Validation Tests", "content": "We also evaluate DISC in a more practical setting where a ground-truth reward model is unavailable for code generation (Chen et al., 2022; 2023b; Zhou et al., 2024). Instead of relying on predefined test cases, we prompt the LLM to generate validation test cases based on the problem prompt. In real-world applications, manually curated ground-truth test cases are often costly to obtain, making self-generated validation a more scalable approach. The results, shown in Fig. 12, indicate that DISC continues to scale better than other methods in this setting. Additional results and details are provided in App. D.3."}, {"title": "4.6. Ablation on Base LLM Model", "content": "We evaluate DISC across different LLMs, including open-source models. As shown in Fig. 13 and Fig. 27, DISC significantly enhances performance even for weaker models."}, {"title": "4.7. Interaction Between Priority Metric and DISC", "content": "We conduct an ablation study to examine how the choice of priority metric affects DISC performance. In addition to the Q-based and Z-based priority metrics (DISC-Q and DISC-Z) introduced in Sec. 3.2, we evaluate three baselines: DISC-R (random step selection), DISC-negQ, and DISC-negZ (which prioritize the opposite steps of DISC-Q and DISC-Z, respectively). As shown in Fig. 14, the selection of a priority metric significantly impacts performance. Both DISC-Q and DISC-Z significantly outperform random selection and their inverse counterparts, demonstrating the effectiveness of their priority heuristics. Additional details"}, {"title": "4.8. Ablation on Partition Fraction \u03b1", "content": "We conduct an ablation study to analyze the effect of the partition fraction \u03b1 on DISC performance. As shown in Fig. 15 and 31, the optimal range appears to be 0.15 < \u03b1 < 0.25. Lower partition fractions (\u03b1 < 0.5) tend to perform better due to the asymmetric cost of sampling from different halves of the partition. Sampling from the first half requires generating more tokens, while the second half requires fewer, making it crucial to partition the first half more conservatively. Additional analysis are in App. C.4."}, {"title": "4.9. Search and DISC", "content": "We demonstrate that search methods such as MCTS and beam search can be combined with DISC. As shown in Fig. 47 in the Appendix, greedy search explores deeper partitions given the same search budget due to its greedy nature, while MCTS and beam search reach similar, shallower depths. However, MCTS allocates the search budget more effectively than beam search, leading to higher performance, as seen in Fig. 16. Additional details and analysis are in App. E."}, {"title": "5. Related Work", "content": "Inference scaling. Inference scaling has emerged as a dominant paradigm, driven by the introduction of o1- and r1-like chain-of-thought reasoning models (Snell et al., 2024; Brown et al., 2024; Manvi et al., 2024; Leea et al., 2025). Several works examine the trade-off between inference compute and training compute (Guan et al., 2025; Chen et al., 2024b). LLM inference often relies on decomposing complex problems into intermediate reasoning steps, as seen in chain-of-thought (CoT) prompting (Wei et al., 2022; Sprague et al., 2024; Wang & Zhou, 2024) and its variants (Kojima et al., 2022; Zhou et al., 2023; Wang et al., 2023b; Li et al., 2023). We extend inference scaling by introducing a new approach for adaptive compute allocation (Manvi et al., 2024).\nLLM reasoning and code generation. LLM reasoning and code generation are central tasks for inference scaling. Evolutionary inference scaling methods have been explored in program generation (Liventsev et al., 2023; Chen et al., 2023a; Romera-Paredes et al., 2024; Lehman et al., 2023; Hemberg et al., 2024). Domain-specific decomposition strategies have been applied in code generation, such as function-based decomposition (Chen et al., 2024a; Zenkner et al., 2024; Levin et al., 2025). More broadly, decomposition often involves prompting LLMs to generate subtask completions (Hern\u00e1ndez-Guti\u00e9rrez et al., 2024; Khot et al., 2022; Dua et al., 2022), which differs from methods that refine a single LLM generation.\nReinforcement learning and Monte Carlo methods. Unlike standard RL, our setting resembles a search problem where the goal is to identify the single highest-reward path. Cazenave (2009) demonstrated that nested Monte Carlo search can accelerate optimal pathfinding. Under the bandit setting, this can be formulated as identifying the arm with the highest maximum reward rather than the highest mean reward (Cicirello & Smith, 2005; Carpentier & Valko, 2014)."}, {"title": "6. Conclusion", "content": "We introduce DISC, a dynamic decomposition framework that adaptively partitions solution steps based on difficulty, improving inference scaling by directing compute toward critical steps while balancing exploration and resource allocation. DISC seamlessly integrates with search-based methods such as MCTS and beam search, further enhancing performance. It also identifies challenging steps for LLMs, aiding curriculum learning, fine-tuning, and dataset augmentation. By dynamically adjusting partitioning based on available compute, DISC enables more adaptive and efficient reasoning in large language models, with broad implications for both training and inference optimization."}, {"title": "7. Impact Statements", "content": "This work on dynamic decomposition aims to advance the field of LLM by improving the efficiency and accuracy of large-scale inference methods, particularly in domains like code generation, mathematical problem-solving, and automated reasoning. By dynamically allocating computational resources to challenging reasoning steps, this approach has the potential to enhance AI-powered tools in software development, STEM education, and scientific research. While the core contributions of this paper focus on technical advancements, we acknowledge the broader societal implications of scaling inference techniques, including risks of over-reliance on AI in decision-making and the potential propagation of errors or biases. To mitigate these concerns, future work should emphasize robust validation, fairness, and human-in-the-loop deployment strategies, particularly in high-stakes applications. Nonetheless, no immediate or specific ethical concerns are identified in the context of this research."}, {"title": "A. Code implementation of DISC", "content": "Python implementation of DISC"}, {"title": "B. Pseudocode for DISC", "content": "Algorithm 1 Dynamic Decomposition\n1: Input: Problem instance x, reward model r, partition function f, LLM policy model \u3160, partition fraction a, solution budget B, priority metric h, metric stopping precision 0, sampling stopping threshold o, is inference mode binference\n2: Output: Final decomposition D\n3: Initialize D\u2190 {generated_solutions : \u00d8, decomposition : \u2205}\n4: # Decompose the solution recursively until we reach the desired precision or run out of budget B\n5: while |D.generated_solutions| < B do\n6:\nYintermediate Concatenate([step.step_str \u2200 step \u2208 D.decomposition])\n7:\n# Step 1: Generate completions until we have enough samples to estimate the splitting metric. Here we use a geometric sampling distribution\n11: while sum(Rnew) < \u03c3 do\n12:\nYcompletion \u2190 \u03c0\u00b7|X, Yintermediate)\n13:\nYproposed Yintermediate Ycompletion\n14:\n# Step 2: Compute splitting metric\n27: Znew \u2190h(Rnew)\n30:\nbsplit new step\u2190\u2190\u2190 new \u2265 Zlast\n # Step 3: Split the step with the higher metric"}, {"title": "C. Ablation studies", "content": "C.1. Ablation on Temperature\nWe conduct an ablation study to analyze the effects of temperature on DISC and BoN. Temperature controls the randomness of token sampling in autoregressive models, influencing both exploration and consistency. Higher temperatures encourage more diverse outputs, whereas lower temperatures yield more deterministic generations. To examine its impact, we evaluate DISC and BoN on a 100-problem subset of APPS (the first 100 problems) using gpt-40-mini.\nC.2. Ablation on Priority Metric h\nWe analyze the effect of different priority metrics on DISC performance. We evaluate DISC using the first 200 competition-level APPS problems with gpt-40-mini, setting the temperature to 0.8 for all experiments. The priority metric determines which steps are refined during recursive decomposition, impacting both efficiency and final solution quality."}, {"title": "D. Experimental results extended", "content": "D.1. APPS"}, {"title": "E. Search and scaling", "content": "E.1. Monte Carlo Tree Search (MCTS)\nMonte Carlo Tree Search (MCTS) is a widely used algorithm for sequential decision-making in large search spaces, particularly in applications such as game playing, planning, and inference scaling. The algorithm builds a search tree incrementally by simulating different sequences of actions and updating estimates of state quality. A key advantage of MCTS is its ability to balance exploration (discovering new states) and exploitation (refining promising ones) using a data-driven search process. The MCTS pipeline consists of four fundamental steps: selection, expansion, simulation, and backpropagation.\nE.1.1. SELECTION\nStarting from the root node representing the current state s, MCTS iteratively traverses the search tree by selecting child nodes based on a selection policy. The most commonly used selection criterion is the Upper Confidence Bound for Trees (UCT), which balances exploration and exploitation:\n$UCT(s, d) = Q(s, d) + c_1\\frac{\\sqrt{\\frac{In (\\Sigma n(s, b))}{n(s, d)}}}$,\nE.1.4. BACKPROPAGATION\nThe final step involves propagating the results of the simulation back up the search tree to refine the estimated values of prior states and actions. Each node along the trajectory T = [s0, d1, 82, . . ., s\u22121] is updated iteratively:\n$Q(s_i, d_{i+1})^{(t+1)} \u2190 (1 \u2013 \u03b1_n)Q(s_i, d_{i+1})^{(t)} + \u03b1_n max{Q(s_i, d_{i+1})^{(t)}, Q(s_{i+1}, d_{i+2})^{(t+1)}}$,\nwhere an is a learning rate that depends on the visit count, and the maximum function ensures that the best-performing trajectories are emphasized.\nMCTS has been widely adopted in inference scaling techniques due to its ability to efficiently allocate computational resources, focusing more on high-reward states while avoiding unnecessary exploration of unpromising regions. In later sections, we explore how MCTS can be combined with dynamic decomposition to further optimize inference scaling."}]}