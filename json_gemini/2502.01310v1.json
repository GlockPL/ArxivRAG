{"title": "A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers", "authors": ["Roman Tarasov", "Petr Mokrov", "Milena Gazdieva", "Evgeny Burnaev", "Alexander Korotin"], "abstract": "Neural network based Optimal Transport (OT) is\na recent and fruitful direction in the generative\nmodeling community. It finds its applications\nin various fields such as domain translation, im-\nage super-resolution, computational biology and\nothers. Among the existing approaches to OT,\nof considerable interest are adversarial minimax\nsolvers based on semi-dual formulations of OT\nproblems. While promising, these methods lack\ntheoretical investigation from a statistical learning\nperspective. Our work fills this gap by establish-\ning upper bounds on the generalization error of an\napproximate OT map recovered by the minimax\nquadratic OT solver. Importantly, the bounds we\nderive depend solely on some standard statisti-\ncal and mathematical properties of the considered\nfunctional classes (neural networks). While our\nanalysis focuses on the quadratic OT, we believe\nthat similar bounds could be derived for more\ngeneral OT formulations, paving the promising\ndirection for future research.", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been a boom in the development of\ncomputational Optimal Transport (OT) which has been iden-\ntified as a powerful tool of solving various machine learning\nproblems, e.g., biological data transfer (Bunne et al., 2023;\nKoshizuka & Sato, 2022; Vargas et al., 2021), image gener-\nation (Wang et al., 2021; De Bortoli et al., 2021; Chen et al.,\n2021) and domain translation (Xie et al., 2019; Fan et al.,\n2023) tasks. The first works in the OT field had proposed\nmethods for solving OT problems between discrete distribu-\ntions (Cuturi, 2013; Peyr\u00e9 et al., 2019). The next milestone\nwas the emergence of OT-based neural methods using the\nOT cost as a loss function for updating the generator in"}, {"title": "2. Background and object of study", "content": "In this section we provide key concepts of OT theory that\nare used in our paper, a comprehensive introduction could\nbe found in (Santambrogio, 2015; Villani et al., 2009).\nThroughout the paper, $p \\in P_{ac}(X)$ and $q \\in P_{ac}(Y)$ are\nabsolute continuous source and target distributions.\nMonge's OT problem. Given $p, q$ and a continuous cost\nfunction $c : X \\times Y \\rightarrow R$, Monge's primal OT prescribes\nto find a measurable transport map $T : X \\rightarrow Y$ which\nminimizes OT cost:\n$Cost(p,q) \\stackrel{\\mathrm{def}}{=} \\inf_{T#p=q}\\int_{X}c(x,T(x))p(x)dx.$\nIn other words, we want to find a map $T$ that transports\nprobability mass from $p$ to $q$ in the cheapest way with respect\nto the given cost function $c(x, y)$, see Fig. 1. I.e., the\naverage transport expenses when moving $p$ to $q$ should be\nminimal."}, {"title": "Wasserstein-2 distance.", "content": "A popular example of an OT cost\nis the (squared) Wasserstein-2 distance ($W_2^2$), for which the\ncost function is quadratic, i.e., $c(x, y) = \\frac{1}{2} ||x - y||^2$. In this\ncase, the corresponding OT objective (1) is known to permit\nthe unique minimizer $T^*$.\nRemark. Note that:"}, {"title": "Semi-dual OT problem.", "content": "Primal OT problem (1) has a tight\ndual counterpart (Santambrogio, 2015, Theorem 1.39):\n$Cost(p, q) = \\max_{f \\in C(Y)}\\{\\int_{X}f^c(x)p(x)dx + \\int_{Y}f(y)q(y)dy\\},$\nwhere $f^c \\in C(X)$ is called c-transform and defined as\n$f^c(x) \\stackrel{\\mathrm{def}}{=} \\min_{y \\in Y}\\{c(x, y) - f(y)\\}$. Potential $f^*$ which de-\nlivers maximum to (3) is called Kantorovich potential. It\nalways could be chosen to be c-concave, i.e., $f^* = g^c$ for\nsome $g \\in C(Y)$, see (Santambrogio, 2015, Remark 1.13)."}, {"title": "Semi-dual OT for quadratic cost.", "content": "For the quadratic cost,\naccording to (Santambrogio, 2015, proposition 1.21), op-\ntimal potential and its c-transform have the form $f^*(y) =$\n$\\frac{1}{2}||y||^2 - \\varphi^*(y)$ and $(f^*)^c(x) = \\frac{1}{2}||x||^2 - \\varphi^c(x)$, where $\\varphi^*$\nis a continuous convex function and $\\varphi^c$ is its convex conju-\ngate. Thus, we can rewrite the problem (3) by substituting\n$f(y) = \\frac{1}{2}||y||^2 - \\varphi(y)$:\n$\\max_{f \\in C(Y)}\\{\\int_{X}f^c(x)p(x)dx + \\int_{Y}f(y)q(y)dy\\} =$\n$\\min_{\\varphi \\in C(Y)}\\{\\frac{1}{2}\\int_{X}||x||^2p(x)dx + \\frac{1}{2}\\int_{Y}||y||^2q(y)dy-\\int_{X}\\varphi^c(x)p(x)dx+\\int_{Y}\\varphi(y)q(y)dy\\}.$\nRemoving terms that do not depend on $\\varphi$, we get an\nequivalent formulation of (3), which has a convex solution\n$\\varphi^*(y) = \\frac{1}{2}||y||^2 - f^*(y)$:\n$\\mathcal{L}(\\varphi) \\stackrel{\\mathrm{def}}{=} \\min_{\\varphi \\in C(Y)}\\{\\int_{X}\\varphi^c(x)p(x)dx + \\int_{Y}\\varphi(y)q(y)dy\\}.$\nSince the optimal potential $\\varphi^*$ which solves (4) is convex,\nwe can optimize (4) with respect to the set of the continuous\nconvex potentials $\\varphi \\in Cvx(Y) \\subset C(Y)$:\n$\\mathcal{L}(\\varphi) = \\min_{\\varphi \\in Cvx(Y)}\\{\\int_{X}\\varphi^c(x)p(x)dx + \\int_{Y}\\varphi(y)q(y)dy\\}.$\nFurthermore, we can recover the OT map $T^*$ which solves\n(1) for quadratic (scalar product) cost from optimal dual po-\ntential $T^*(x) = \\nabla \\varphi^c(x)$ (Santambrogio, 2015, Thm. 1.17)."}, {"title": "Continuous OT problem.", "content": "Analytical solution for problem\n(3) is, in general, not known. In real-world scenarios, the\nmeasures $p, q$ are typically not available explicitly but only\nthrough their empirical samples $X = \\{x_1,...,x_N\\} \\sim p,$\n$Y = \\{y_1,..., y_M\\} \\sim q$. To approximate the desired solu-\ntion for primal (dual) OT, two setups are possible. In the\nfirst setup called discrete (Peyr\u00e9 et al., 2019), one aims to\nestablish optimal matching (point-to-point correspondence)\nbetween the empirical distributions $p = \\Sigma_{n=1}^N \\delta_{x_i}, q =$"}, {"title": "Continuous OT solvers for quadratic cost.", "content": "In recent years,\nnumerous algorithms have been developed to solve the con-\ntinuous optimal transport problem for the quadratic cost.\nThe primal problem is challenging to solve due to the diffi-\nculty in satisfying the constraint $T#p = q$. For this reason,\nsemi-dual problem (4) or (5) is solved instead and optimal\ntransport map $T$ is recovered from the optimal dual potential.\nOne popular strategy (Taghvaei & Jalali, 2019) is to con-\nsider (5) and parameterize learned $\\varphi$ as input convex neural\nnetwork (ICNN) (Amos et al., 2017). ICNNs are neural net-\nworks which place certain restrictions on their weights and\nactivations to ensure the convexity w.r.t. the input. Given\nproper parametrization of dual potential $\\varphi_\\theta, \\theta \\in \\Theta$, one can\ndirectly minimize:\n$\\mathcal{L}(\\varphi_{\\theta}) = \\int_{X}\\varphi_{\\theta}^c(x)p(x)dx + \\int_{Y}\\varphi_{\\theta}(y)q(y)dy.$\nSince only empirical samples are known, the integrals are\nreplaced by (Monte-Carlo) sum\n$\\widehat{\\mathcal{L}}(\\varphi_{\\theta}) = \\frac{1}{N} \\sum_{n=1}^N \\varphi_{\\theta}^c(x_n) + \\frac{1}{M} \\sum_{m=1}^M \\varphi_{\\theta}(y_m)$"}, {"title": "Continuous min-max OT solvers for quadratic cost.", "content": "Re-\ncently, minimax semi-dual approaches have been actively\nexplored, focusing on learning both the dual potential and\nthe primal transport map. This formulation can be derived\nby applying the interchange theorem (Rockafellar, 2006,\nTheorem 3A):\n$\\int_{X}\\varphi^c(x)dx = \\int_{X}\\max_{y \\in Y}\\{\\langle x, y \\rangle - \\varphi(y)\\}p(x)dx =$\n$\\max_{T}\\int_{X}[\\langle x,T(x) \\rangle - \\varphi(T(x))]p(x)dx,$\nthe outer $\\max_{T}$ is taken w.r.t. measurable maps $T : X \\rightarrow Y$.\nRemark. The existence of a map which maximizes the above\nproblem follows from the measurable selection theory. For\n$\\varphi \\in C(Y)$, the sets $\\arg \\max_{y \\in Y} \\{\\langle x, y \\rangle - \\varphi(y)\\}, x \\in X$ are\nnonempty and closed. From (Aubin & Frankowska, 2009,\nThm. 8.1.3), it follows that there exist a measurable map $T$\nwith values in such sets. This map delivers maximum to (6).\nIn light of (6), problem (4) is thus reformulated as a minimax\noptimization problem:\n$\\mathcal{L}(\\varphi) = \\min_{\\varphi} \\max_{T} \\mathcal{L}(\\varphi,T);$\n$\\mathcal{L}(\\varphi,T) \\stackrel{\\mathrm{def}}{=} \\int_{X}[\\langle x,T(x) \\rangle - \\varphi(T(x))]p(x)dx + \\int_{Y}\\varphi(y)q(y)dy.$\nUnder certain assumptions one may guarantee that if the\nvalues $\\mathcal{L}(\\varphi, T)$ and $\\mathcal{L}(\\varphi^*, T^*)$ are close, then $T$ is close to\nOT map $T^*$. And it is our work which establishes practical\nconvergence guarantees for (7) under continuous OT setup\nwith $\\varphi, T$ given by neural networks, see \u00a74."}, {"title": "Object of study.", "content": "In practice, $T$ and $\\varphi$ are parametrized\nas neural networks $T_\\omega, \\omega \\in \\Omega$ and $\\varphi_\\theta, \\theta \\in \\Theta$. Besides,\nfollowing our continuous OT setup, the (unknown) reference\ndistributions $p, q$ are replaced by their empirical counterparts\n$\\hat{p}, \\hat{q}$. This yields the optimization problem typically solved\nin practice:\n$\\min_{\\Theta \\in \\Theta} \\max_{\\omega \\in \\Omega} \\widehat{\\mathcal{L}}(\\varphi_\\theta, T_\\omega);$\n$\\widehat{\\mathcal{L}}(\\varphi_\\theta, T_\\omega) \\stackrel{\\mathrm{def}}{=} \\frac{1}{N} \\sum_{n=1}^N [\\langle x_n, T_\\omega(x_n) \\rangle -\\varphi_\\theta (T_\\omega(x_n))] + \\frac{1}{M} \\sum_{m=1}^M \\varphi_\\theta(y_m)$"}, {"title": "3. Related Works", "content": "In this section, we first discuss the existing works which\nconduct theoretical analysis of semi-dual OT losses (\u00a73.1).\nSince minimax objective (7) resembles adversarial train-\ning, we additionally discuss some of the literature dealing\nwith theoretical aspects of Generative Adversarial Neural\nNetworks (GAN) losses and how they relate to ours (\u00a73.2)."}, {"title": "3.1. Theoretical analysis of continuous semi-dual OT\nsolvers", "content": "The current progress in theoretical analysis of semi-dual OT\ncould be subdivided into two branches.\nNon-minimax semi-dual OT. The first branch of works\n(H\u00fctter & Rigollet, 2021; Gunsilius, 2022) analyze the non-\nminimax losses (4) and (5) and develop error bounds (9) for\npushforward maps T given by the gradient of dual potentials,\ni.e., $E_{X,Y}||T^* - \\nabla \\varphi_\\omega||_{L^2(p)}^2$. To achieve particular statistical\nrates, the authors place certain restrictions on the considered\nproblem setup and deal with specific classes of functions\n(maps), e.g., wavelet expansions or (kernel-smoothed) plug-\nin estimators. Recent studies in this direction (Divol et al.,\n2022; Ding et al., 2024) extend the analysis of error rates\nfor the class of potentials given by neural networks, e.g.,\ninput convex neural networks. Meanwhile, none of the men-\ntioned works treats the learned map T separately from dual\npotential. Importantly, the analysis of minimax objec-\ntive (7) is considerably more challenging than (4) due to\nthe additional \u201cdegree of freedom\u201d given by the optimized\nmap T. Furthermore, saddle-point problems such as (7) are\nknown to be more tricky for theoretical investigation than\nusual minimization. At the same time, the practical demand\nstemming from the recent proliferation of minimax-based\nOT solvers, see \u00a72, makes such an investigation highly de-\nsirable. All of this necessitates separate study of minimax\nOT solvers; the adaptation of existing non-minimax results\nis questionable, if that is even possible."}, {"title": "Minimax semi-dual OT.", "content": "The estimation of (9) in case of\nminimax OT (8) is a much less explored task. In fact, there\nare no studies at all that provide statistical bounds for recov-\nered minimax OT map $T_\\omega$. The existing works (Makkuva\net al., 2020; Rout et al., 2022) only conduct an analysis\nof $||T^* - T_\\omega||_{L^2(p)}^2$ in the sense of duality gaps. That is,\nthe error between the true OT map and recovered map is\nupper-bounded by values of functional $\\mathcal{L}(\\varphi, T)$. The duality\ngaps analysis helps to validate minimax methodology; it is\ncommon for more broad class of (non-quadratic) minimax\nOT solvers (Fan et al., 2023; Asadulaev et al., 2024; Kolesov\net al., 2024). However, we again emphasize that it does not\nreveal particular statistical rates and guarantees for (9). In\n(Gonz\u00e1lez-Sanz et al., 2022), the authors establish promis-\ning results for the error of recovered approximate OT map.\nHowever, their problem formulation of min-max OT differs\nfrom ours, eq. (7), as they aim to treat OT as the limit of\nregularized GAN objectives."}, {"title": "Theoretical analysis of other OT formulations.", "content": "For the\ncompleteness of exposition, we also mention several studies\nthat develop statistical learning analysis of recovered OT\nmap (plan) error for non semi-dual OT formulations. The\nworks (Genevay et al., 2019; Rigollet & Stromme, 2022;\nGonz\u00e1lez-Sanz & Hundrieser, 2023; Mokrov et al., 2024;\nGoldfeld et al., 2024; Korotin et al., 2024) deal with Entropy-\nregularized OT; (Vacher & Vialard, 2022; 2023; Gazdieva\net al., 2024) investigate unbalanced OT versions. Although\nthese works are interesting and insightful, their object of\nstudy is different from ours, making them less relevant."}, {"title": "3.2. Theoretical analysis of adversarial Generative\nModels", "content": "The objective of the semi-dual minimax continuous OT\nsolvers (7) resembles that of GANs (Goodfellow et al.,\n2014). This fact motivates us to review some of the ex-\nisting theoretical results on GANs below. Still, while there\nare many papers on GANs (Pan et al., 2019) and a relatively\nlarge number of works studying the theoretical aspects of\ntheir objectives, they are not relevant to us for two reasons.\nFirst, OT solvers and GANs pursue different goals. The\nmain goal of OT solvers is to approximate true OT maps $T^*$,\ni.e., to yield specific generators which satisfy the optimality\ncondition; accordingly, our theoretical work focuses on the\nerror of this approximation. Meanwhile, GANs objectives\nusually do not have a unique optimal generator $G^*$; thus,\nexisting theoretical results mostly investigate the error of\napproximating the ground-truth distribution $q$ by the gener-\nated $p_{gen}$. Second, OT solvers and GANs have an evident\ndifference corresponding to the order of optimization over\ngenerator G (map T) and discriminator D (potential $\\varphi$) in\ntheir objectives. In particular, for GANs, the optimization\nover the generator G is done in the outer problem of their\nobjective, while for OT solvers, the optimization over OT"}, {"title": "4. Results", "content": "In real-world use cases, OT practitioners are given (i.i.d.)\ntraining samples $X \\sim p$ and $Y \\sim q$ and optimize empirical\nmin-max objective (8) with respect to restricted classes of\nfunctions $\\varphi \\in F, T \\in T$, e.g., neural networks. Below\nwe denote the solutions of the problem which we have in\npractice.\n$\\mathcal{G}^R = \\arg \\min_{\\varphi \\in F} \\max_{T \\in T} \\widehat{\\mathcal{L}}(\\varphi,T);$\n$T^{\\mathcal{G}^R} = \\arg \\max_{T \\in T} \\widehat{\\mathcal{L}}(\\mathcal{G}^R,T).$\nNote that in these equations we implicitly assume the exis-\ntence of optimizers $\\mathcal{G}^R \\in F,T^{\\mathcal{G}^R} \\in T$. While in general this\nmay not always hold true, some natural practical choices\nof $F$ and $T$, e.g., neural network architectures $T_\\omega, \\varphi_\\theta$ with\nbounded set of parameters $\\Omega, \\Theta$ guarantee the existence.\nOur goal is to estimate the generalization error, i.e., the\n(average) error between OT map $T^*$ and empirical map $T^{\\mathcal{G}^R}$.\n$E_{X,Y}||T^{\\mathcal{G}^R} - T^*||_{L^2(p)}^2.$\nWe subdivide the problem into three steps.\nIn the first step, we upper-bound the error using differences\nin the functional $\\mathcal{L}(\\varphi, T)$ values (\u00a74.1). The obtained up-\nper bound decomposes into several terms: estimation and\napproximation errors that occur in both the inner and outer\noptimization problems within our min-max objective (7).\nIn the second step, we estimate each term individually using\nsuitable techniques from statistical learning theory (\u00a74.2).\nFinally, we bring it all together and formulate our main\ntheoretical result (\u00a74.3)."}, {"title": "4.1. Error decomposition", "content": "Our starting point is the introduction of four components\nwhich will appear when upper-bounding (10). The primary\nquantity which we analyze in this section is the error be-\ntween values of functional $\\mathcal{L}$, i.e.:\n$\\mathcal{L}(\\varphi,T) - \\mathcal{L}(\\varphi', T').$\nDepending on the context, the plug-in arguments\n$\\varphi, \\varphi',T,T'$ of the expression above may be \u201coptimal\u201d in"}, {"title": "4.2. Bounds on the Approximation and Estimation\nErrors", "content": "In this section, we establish the bounds on the estimation\nand approximation errors of the minimax OT solvers defined\nin \u00a74.1. In the theorem below, we establish the bounds on\nthe total estimation error, i.e., the sum of inner (14) and\nouter errors (15).\nThen\n$\\mathcal{E}_E < 8R_{p,N}(\\mathcal{H}) + 8R_{q,M}(F)$,\nwhere $\\mathcal{H}(F,T) \\stackrel{\\mathrm{def}}{=} \\{h : h(x) = \\langle x,T(x) \\rangle - \\varphi(T(x)), T \\in\nT,\\varphi \\in F\\}$ and $R_{p,N}(\\mathcal{H})$ is the Rademacher complexity of\nthe function class $\\mathcal{H}$ with respect to probability density $p$\nfor sample size N."}, {"title": "4.3. Main Result", "content": "The main goal of our paper is the establishment of bounds on\ngeneralization error, i.e., the difference in true OT map and\nits empirical approximation defined in (10). In the theorem\nbelow, we use the previously obtained bounds on the esti-\nmation (Theorem 4.2) and approximation errors (Theorems\n4.3, 4.6) to derive a bound on the generalization error.\nLet the optimal dual potential $\\varphi^*$ be $\\beta$-strongly\nconvex. Then for any $\\varepsilon > 0$ there exist such classes\n$F = F(\\varepsilon, \\beta), T = T(\\varepsilon, F)$ that\n$E_{X,Y}||T^{\\mathcal{G}^R} -T^*||_{L^2(p)}^2 \\le \\varepsilon + \\frac{32}{\\beta} (R_{p,N}(\\mathcal{H}) + R_{q,M}(F)).$\nIn particular, $\\forall y \\in F$ it has the form $\\varphi_{O_L} + \\frac{\\beta}{2}||.||^2$, where $\\varphi_{O_L}$\nis an ICNN and T is some class of neural networks."}, {"title": "5. Discussion", "content": "Our paper performs theoretical investigation of semi-dual\nmin-max OT solvers, i.e., a popular and fruitful branch of\ngenerative models based on Optimal Transport. While these\nsolvers show impressive results in various machine learning\nproblems, theoretical investigation of their practical conver-\ngence guarantees is rather limited. We address this problem\nby presenting the first theoretical analysis of these minimax\nsolvers from a statistical learning perspective. Most impor-\ntantly, our paper provides learnability guarantees for the\nsolvers which justify their practical usability. We believe\nthat our research will advance the development of minimax\nadversarial OT solvers by mitigating potential concerns re-\ngarding their theoretical validity. The only limitation of\nour study corresponds to the focus on a popular case of a\nquadratic OT cost function. Generalization of the estab-\nlished bounds for the general OT formulations represents a\npromising avenue for future work."}, {"title": "A. Proofs.", "content": "In this section, we provide the proofs of our theoretical results:\nTheorem 4.1 (\u00a7A.1) which decomposes the recovered map $T^R$ error into several approximation and estimation subterms\nwhich depend on the values of the optimized functional $\\mathcal{L}$.\nTheorem 4.2 (\u00a7A.2) which upper-bounds the estimation error with Rademacher complexities of function classes $F, H$.\nTheorem 4.3 (\u00a7A.3) and Theorem 4.6 (\u00a7A.4) which demonstrate that the approximation error could be done arbitrary\nsmall under properly chosen classes of Neural Networks.\nTheorem 4.8 (\u00a7A.5) and Corollary 4.9 (\u00a7A.5) which establish upper bounds for the generalization error (10) and show\nthat it can be made arbitrarily small by choosing the appropriate classes of functions and sufficient number of samples.\nBefore starting the main proofs, we state the following auxiliary lemma"}, {"title": "A.1. Proof of theorem 4.1", "content": "First, we upper-bound the error in estimating transport map via duality gaps analysis. Our theorem below borrows the main\nideas from (Makkuva et al., 2020, Theorem 3.6), but has its own specificity, since it deals with transport maps that are not\nnecessarily given by gradients of convex functions.\nUnder the condition of theorem 4.1 it holds:\n$\\frac{\\beta}{4}||T^R-T^*||_{L^2(p)}^2 \\le (\\mathcal{E}_1(\\mathcal{G}^R, T^{\\mathcal{G}^R})+ \\mathcal{E}_2(\\mathcal{G}^R)),$\nwhere $\\mathcal{E}_1(\\mathcal{G}^R, T^{\\mathcal{G}^R}) \\stackrel{\\mathrm{def}}{=} \\max_T \\mathcal{L}(\\mathcal{G}^R, T) - \\mathcal{L}(\\mathcal{G}^R, T^{\\mathcal{G}^R})$ is the inner error, and $\\mathcal{E}_2(\\mathcal{G}^R) \\stackrel{\\mathrm{def}}{=} \\max_T \\mathcal{L}(\\mathcal{G}^R,T) - \\min_{\\varphi} \\max_T \\mathcal{L}(\\varphi, T)$\nto the outer error."}, {"title": "A.2. Proof of theorem 4.2", "content": "Our theorem 4.2 uses some standard notions from learning theory, see, e.g. (Shalev-Shwartz & Ben-David, 2014, \u00a726). We\nrecall them for the convenience. For a class $\\mathcal{F}$ of functions $f : Y \\rightarrow R$ and probability distribution $q$, the representativeness\nof a sample $Y = \\{y_1, . . . , y_M\\} \\sim q$ of size M is:"}, {"title": "A.3. Proof of theorem 4.3", "content": "To begin with, we recall some standard definitions and notions used in this subsection. Given function $\\varphi \\in C(Y)$, its\nLipschitz (semi)norm is defined as:\n||(y) - (y')|| = \nsup"}, {"title": "A.4. Proof of theorem 4.6", "content": "In what follows, we will use the notion of B-smoothness. Function $\\varphi : Y \\rightarrow R$ is called B-smooth if it is continuously\ndifferentiable on Y, and its gradient is Lipschitz continuous with Lipschitz constant B:\n|| -"}, {"title": "A.5. Proof of theorem 4.8 and corollary 4.9", "content": "Proof of theorem 4.8. Let's bound all the terms from decomposition in theorem 4.1 step by step.\nFrom corollary 4.7 we get such class $F = F(\\frac{\\epsilon}{4}, \\beta)$ that $\\mathcal{E}_{out}(F) < \\frac{\\epsilon}{4}$.\nThe second step is to apply theorem 4.3, which gives us $T = T(\\frac{\\epsilon}{8}, F)$, such that $\\exists \\mathcal{E}_m(F,T) < \\frac{\\epsilon}{4}$.\nFinally, applying theorem 4.2, we bound remaining two terms $\\mathcal{E}_E^i(F,T,N,M) + \\mathcal{E}_{out}^i(F,T,N,M) \\le 8R_{p,N}(\\mathcal{H}) +\\newline 8R_{q,M}(F)$"}]}