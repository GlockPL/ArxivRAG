{"title": "Machine Learning for Raman Spectroscopy-based Cyber-Marine Fish Biochemical Composition Analysis", "authors": ["Yun Zhou", "Gang Chen", "Bing Xue", "Mengjie Zhang", "Jeremy S. Rooney", "Kirill Lagutin", "Andrew MacKenzie", "Keith C. Gordon", "Daniel P. Killeen"], "abstract": "The rapid and accurate detection of biochemical compositions in fish is a crucial real-world task that facilitates optimal utilization and extraction of high-value products in the seafood industry. Raman spectroscopy provides a promising solution for quickly and non-destructively analyzing the biochemical composition of fish by associating Raman spectra with biochemical reference data using machine learning regression models. This paper investigates different regression models to address this task and proposes a new design of Convolutional Neural Networks (CNNs) for jointly predicting water, protein, and lipids yield. To the best of our knowledge, we are the first to conduct a successful study employing CNNs to analyze the biochemical composition of fish based on a very small Raman spectroscopic dataset. Our approach combines a tailored CNN architecture with the comprehensive data preparation procedure, effectively mitigating the challenges posed by extreme data scarcity. The results demonstrate that our CNN can significantly outperform two state-of-the-art CNN models and multiple traditional machine learning models, paving the way for accurate and automated analysis of fish biochemical composition.", "sections": [{"title": "1 Introduction", "content": "In New Zealand, Hoki and Mackerel are two commercially important deep-water fish species [1]. Currently, they are primarily processed into low-value products such as fishmeal, limiting their economic potential. The ability to extract high-value products, such as food-grade omega-3 fish oil and edible protein, depends heavily on understanding their biochemical compositions, particularly their water, protein and lipids yield contents. On average, water constitutes about 70-80%, protein 10-20%, and lipids yield 2-8% of the fish's total weight [6]. However, these compositions can vary substantially, depending on different factors including catch location, season, and environmental conditions. This variability adds complexity to the quantification process, making it challenging to consistently determine the fish's biochemical makeup [6].\nVibrational spectroscopic techniques like Fourier Transform Raman (FT-Raman), and InGaAs Raman at 1064 nm (InGaAs) spectroscopy provide rapid and non-destructive ways to analyze fish biochemical compositions [4,13,11]. This capability makes it highly suitable for real-time high-throughput manufacturing processes [13]. The spectral data obtained by Raman spectroscopy comprises of a series of features, each corresponding to the spectrum intensity at a given wavelength [20]. The total number of wavelengths determines the total number of features. The spectral data provide a unique fingerprint-like pattern of peaks in the spectrum [22], enabling reliable prediction of fish biochemical compositions.\nTo effectively utilize the spectral data for fish biochemical composition analysis, this paper aims to develop an automated regression model that can accurately predict the biochemical composition of each catch in real-time. For this purpose, we acquire the biochemical reference data through traditional analytical chemistry methods. We also formulate a multi-output regression problem with the goal to simultaneously predict multiple biochemical components from the spectral features.\nPrevious studies have utilized traditional models such as Partial Least Squares Regression (PLSR), K Nearest Neighbor (KNN), Lasso Lars, Elastic Net, Support Vector Machines (SVM), Least Squares Support Vector Machines (LS-SVM), Light Gradient-Boosting Machine (LGBM), and Random Forest (RF), to address similar problems [13,4,11]. However, the performance of these models is highly sensitive to preprocessing and feature selection operations, and they often fail to capture long-range dependencies among distance wavelengths, resulting in compromised accuracy [24].\nRecent work has shown that Convolutional Neural Networks (CNNs) can overcome these limitations by automatically preprocessing data, selecting features, and capturing complex patterns in spectral data[15,24,7,16]. Upon using CNNs, normalization or standardization is typically utilized for data preprocessing. Meanwhile, performance can be enhanced further with additional preprocessing steps such as baseline correction, scatter correction and derivative transformation [14].\nThis study applies CNNs to Raman spectroscopy data to predict water, protein, and lipid yields contents in fish. Due to the specialized nature of this task, obtaining larger datasets is challenging and costly. To the best of our knowledge, no prior work has applied CNNs to such small Raman spectroscopic datasets. Our model is tailored to this domain and dataset size, addressing real-world constraints in fish composition analysis. Key challenges include the limited dataset size, which restricts the model's ability to learn patterns, and the increased risk"}, {"title": "2 Related Work", "content": "This section reviews relevant studies utilizing spectral data for the quantitative analysis of biochemical compositions in fish. First, we examine studies focused specifically on fish biochemical analysis. Then, we broaden the scope to review general quantitative analysis approaches using spectral data.\nQuantitative analysis of biochemical compositions in fish has been extensively studied using various spectroscopic techniques coupled with machine learning approaches [13,21,11,22]. Traditional machine learning techniques like PLSR, KNN, LS-SVM, SVM and RF have been commonly employed to develop predictive models from spectral data for estimating various biochemical compositions in fish [13,4]. However, these models often fail to capture long-range dependencies among distant wavelengths, resulting in compromised accuracy. Additionally, feature selection and data preprocessing are crucial but can introduce challenges such as the risk of removing informative features or overfitting due to collinearity [24]. Preprocessing corrects data artefacts and improves signal-to-noise ratios, but selecting the correct preprocessing pipeline remains complex due to variations in measurement environments and sample origins [9,8].\nRecent studies have increasingly explored deep learning, particularly one-dimensional convolutional neural networks (CNNs), for spectral data modeling [15,24,7,16]. Compared with traditional models, CNNs can automatically preprocess data and extract features, reducing the need for extensive preprocessing and feature selection, but these benefits are generally realized with large datasets [23]. To address this challenge, a data augmentation method has been proposed as a solution to expand the sample space [2], and its application in"}, {"title": "3 Proposed Approach", "content": "We introduce a new framework, named FishCNN, which effectively integrates data preprocessing, data augmentation, and scaling techniques into a comprehensive procedure for training CNNs on small real-world spectroscopic datasets to predict water, protein, and lipids yield contents of fish samples.\nThe overall framework, illustrated in Fig. 1, comprises several key components. First, the raw FT-Raman and InGaAs 1064 nm spectroscopy data (X) and the corresponding biochemical reference data (Y) are split into 6 folds for cross-validation.\nNext, the spectroscopy data is prepared using various methods, including data artefact correction, signal quality enhancement, extensive data augmentation, and scaling operations. The specific data preparation steps are determined through a systematic trial-and-error evaluation process to identify the suitable combination that produces the highest mean cross-validated coefficient of determination (R2CV).\nSubsequently, the prepared data are used to train our CNN model, enabling us to build the first CNN-based approach for predicting fish biochemical compositions. Through the synergy of data preprocessing and augmentation, the CNN model is expected to learn intricate patterns in Raman spectroscopic data and accurately predict the corresponding biochemical properties. For evaluation purposes, the cross-validated performance of FishCNN is compared to traditional models and two benchmark CNN models discussed in Section 2.\nFishCNN introduces several innovative components designed to optimize the performance of CNNs on small spectral datasets, as highlighted in red in Fig. 1. Specifically, we propose to use CNN with a large filter size and small stride, which can effectively extract useful features from small spectral datasets. Different from common practice, we also decide to apply data augmentation after data preprocessing, ensuring that the augmented spectral data maintains high quality and relevance. This is empirically verified in Appendix E.1. The specific details of each component are explained in the subsequent sections."}, {"title": "3.1 Data Preprocessing", "content": "Data preprocessing is crucial for cleaning spectral signals and enabling the CNN model to effectively learn from limited samples. Raman spectroscopic data often contain data artefacts, like baseline shifts and light scatter, which can obscure important spectral signals [9]. Correctly handling these artefacts is essential for accurate analysis. Furthermore, even if all artefacts are handled correctly, peaks from different biochemical components can overlap or interfere with each other, making it difficult to accurately identify and quantify individual compounds [8,14]."}, {"title": "3.2 Selection of Data Preprocessing Methods", "content": "The misuse of preprocessing methods can distort patterns within the spectrum signal [24], thus, it is important to experiment with different combinations of preprocessing steps to identify the most effective procedure for any spectral dataset. For this purpose, we construct a preprocessing design matrix (see Appendix B), combining various signal artefact correction and signal enhancement methods. These methods include baseline correction, scatter correction, derivative transformations, and scaling. Each combination is applied in a predefined order based on [9] to ensure consistency and effectiveness, avoiding the complexity associated with testing all possible permutations.\nTo determine the best preprocessing procedure, a trial-and-error approach is conducted, where the combination that enables CNNs to achieve the highest mean R2 is selected. The R2 metric is chosen because it is a scale-invariant measure ranging from 0 to 1, indicating how well the model predicts the outcome variables. A recent study [5] has demonstrated that R\u00b2 is more informative and does not suffer from the interpretability limitations of the MSE and RMSE metrics. This makes R\u00b2 particularly suitable for tasks involving multiple outcome variables with different value ranges and distributions, as is the case in our study where we predict water, protein, and lipid contents simultaneously. The scale-invariant nature of R\u00b2 allows for accurate comparison of model performance across these diverse outcome variables.\nFollowing this systematic framework, the most suitable preprocessing procedure for any spectral dataset can be identified. This approach not only evaluates CNN performance but also validates and compares the effectiveness of preprocessing methods against the comparative models examined in Section 4.2."}, {"title": "3.3 Data Augmentation and Scaling", "content": "Data augmentation techniques studied in [2] are applied to enhance the generalization ability and robustness of the trained CNN model. The augmentation process introduces various irrelevant data artefacts to each spectrum instance, including baseline offsets, slopes, and multiplication differences illustrated in Fig. 2a. By randomly applying these variations, the dataset is expanded, providing the CNN model with a more diverse set of instances to learn from.\nWe choose not to apply data augmentation to the raw unpreprocessed data because the data artefacts will also be amplified. This will result in large data"}, {"title": "3.4 Convolutional Neural Network Architecture (CNN)", "content": "Our 1-dimensional CNN architecture utilizes the augmented pre-processed data (see Section 3.2) as inputs, and performs multiple output regression to generate predicted target values including Water, Protein and Lipids yield contents as outputs. As illustrated in Fig. 4, our model adopts two stacked convolutional layers (labelled Conv1D L1 and Conv1D L2), a flattened layer with dropout (labelled as Flatten and Dropout Layer), two fully connected layers (labelled FC layers) and an output layer. To efficiently extract intricate patterns and aggregate them into high-level features to make predictions, the network can be divided into two blocks: Feature Extraction and Feature Aggregration. Both blocks are explained below.\nFeature Extraction As highlighted in Fig. 4, our CNN model uniquely uses large filter sizes (64) with a small stride (1) in both convolutional layers (Conv1D L1, L2), with 16 filters and zero padding to maintain input dimensions. To the best of our knowledge, the use of this setting for one-dimensional CNN spectral analysis is novel since previous studies mainly consider small filter width and stride [15,7]. By stacking two convolutional layers with this setting, the network has a large receptive field size of 128, as each neuron in the final convolutional layer has a view of 128 consecutive data points. Compared to small filter widths and strides, our approach enables CNN to effectively capture intricate low-level patterns from the limited input sample space.\nFeature Aggregration The flatten layer is used to concatenate output feature maps from the convolutional layers to create a one-dimensional embedding. Inspired by [24], the random 10% dropout layer is applied to the flatten output to mitigate the overfitting issue since the subsequent fully connected layers highly rely on feature maps extracted from convolutional layers.\nSubsequently, we have two fully connected layers and one output layer. The size of each FC layer is scaled down progressively from 128 to 16 to 3. This design is inspired by [16] where the decreasing number of neurons creates a funneling effect, forcing the intricate low-level patterns learned in the convolutional layers to be aggregated to high-level abstractions."}, {"title": "4 Design of the Experiments", "content": null}, {"title": "4.1 Data Collection and Evaluation", "content": "The detailed number of samples, spectral data ranges and the corresponding number of features are summarized in Table 1. The Raman spectroscopic data used in this study are obtained using two different techniques: Fourier Transform (FT) Raman and InGaAs Raman at 1064 nm, both measuring the same set of fish samples. While these two techniques share similar fundamentals, the InGaAs 1064 nm technique significantly reduces fluorescence background interference which is common in FT-Raman [12]. Since our fish samples are placed in glass vials, we excluded the wavelength range of 580.109 to 202.533 nm to avoid interference from the glass.\nAlthough the InGaAs technique can provide a more accurate and reliable signal, as illustrated in Table 1, it has fewer features, only covering the fingerprint region of the Raman spectrum, potentially limiting the information available for analysis [12]. Therefore, both spectral data are used in our experiments for comprehensive analysis, evaluating the performance of the CNN model on each to determine the most suitable dataset in this study.\nGiven the limited sample size, the dataset is split into 6 folds for cross-validation, with each fold containing approximately 32 samples for training and 7 samples for testing. As mentioned in Section 3.3, we have applied 50 times data augmentation to the training data of each fold, while the testing data remains unchanged. To mitigate overfitting, we performed 10 individual runs of the model using 6-fold cross-validation. Additionally, we applied regularization techniques, including dropout (rate = 0.10) and L2 regularization to further enhance generalization on the small dataset."}, {"title": "4.2 Comparison Methods", "content": "Several traditional models, including PLSR [13], KNN [13], Lasso Lars [18], Elastic Net [18], LS-SVM [13], Light Gradient Boosting Machine (LightGBM) [10], Random Forest (RF) [10] are commonly used in the fields of spectroscopy. Therefore, we adopt them as baseline models in the experiments. For conciseness, only the best performing one will be presented, detailed descriptions of all baseline models are provided in Appendix D.\nIn addition, two state-of-the-art CNN models (1CLNN and 3CLNN [7,15]) for spectral analysis are included in the comparison. Both of these two CNN models have small filter sizes and strides, where 1CLNN has 1 convolutional layer, while 3CLNN has 3 convolutional layers. Their specific hyper parameter"}, {"title": "4.3 Parameter Settings for Our CNN Model", "content": "The parameter settings for our CNN model are summarized in Table 2. The batch size is set to 38, and the initial learning rate is 0.0015 according to the heustric formula: learningrate = 0.01 * batchsize/256 [7]. We employ the AdamW optimizer, a variant of Adam with weight decay, which is often considered the best for spectral analysis [24]. Additionally, we implement early stopping, saving the best model when the validation loss does not decrease over 55 epochs [16]."}, {"title": "5 Results and Discussions", "content": "In this section, we present the results of our FishCNN compared to all methods mentioned in Section 4.2. While R\u00b2 is the primary evaluation metric discussed here, additional detailed root mean squared error (RMSE) results are provided in Appendix C for further analysis. The results are divided into two parts. First, the overall performance averaging the results across all three targets (Water, Protein, Lipids yield) is presented in Section 5.1 to provide a general overview of the model's effectiveness. Subsequently, the individual target performance, which examines each target separately, will be discussed in Section 5.2.\nTo understand the multioutput R2 performance, we use the formula: $R^2_{overall} = \\frac{1}{N} \\sum_{i=1} R^2_i$, where $R^2_{overall}$ is the overall R2 value, N is the number of targets, and $R^2_i$ is the R2 value for each target i (Water, Protein, Lipids yield). We calculate the $R^2_{overall}$"}, {"title": "5.1 Overall Performance Analysis", "content": "Table 3: Overall performance comparison of baseline, two benchmark CNNs, and our CNN. Both FT-Raman and InGaAs-trun are SNV [9] preprocessed. The p-values are obtained using the pairwise Mann-Whitney U test based on 10 individual runs, while deterministic models and FishCNN itself are denoted by \"-\". The significantly better results are highlighted in bold, values before and after indicate mean\u00b1standard deviation.\nAs shown in Table 3, on both FT-Raman and InGaAs truncated datasets, our CNN model achieves significantly better R\u00b2CV scores than all traditional models as well as 1CLNN and 3CLNN. The p-values of the relative pairwise Mann-Whitney U test [20] are consistently lower than 0.05, indicating statistically significant improvement.\nWe notice that, although the InGaAs 1064 nm-truncated data has a smaller spectral range and fewer features (427) compared to the FT-Raman data (1971), our CNN model still outperforms the 1CLNN and 3CLNN models on both datasets. Additionally, the InGaAs-truncated data show a higher R2CV score than the FT-Raman data. These results demonstrate the effectiveness of our CNN model in handling spectral data, even with extremely limited sample size. To further investigate the robustness of our model, we also examine the individual target performance in Section 5.2 to identify the consistency of the model's performance across different targets."}, {"title": "5.2 Individual Target Performance", "content": "Table 4: Each individual target performance comparison. For lipids yield, the InGaAs data is preprocessed by SNV followed by second order derivative with a 19-point window size, while others are all preprocessed by SNV only. The p-values are obtained using the pairwise Mann-Whitney U test based on 10 individual runs, while deterministic models and FishCNN itself are denoted by \"-\". The significantly better results are highlighted in bold, values before and after indicate mean\u00b1standard deviation.\nAs illustrated in Table 4, the performance of each target obtained on the InGaAs dataset is always significantly better than those obtained on the FT-Raman dataset. This finding suggests that the InGaAs dataset is better than FT-Raman for predicting all three targets. This is likely due to the significantly reduced fluorescent background interference in InGaAs, despite of a small wavelength range [12].\nOur FishCNN consistently outperforms all competing models across all three targets on the InGaAs dataset, demonstrating its effectiveness in handling spectral data and its generalization ability. However, FishCNN does not significantly outperform baseline models on the FT-Raman dataset for certain individual targets. This variation suggests that certain traditional models may be better suited for specific tasks or datasets with a wider spectral range and more fea-"}, {"title": "6 Conclusions and Future Work", "content": "In this paper, we successfully addressed the challenge of accurately predicting water, protein, and lipids yield contents in Hoki-Mackerel fish from Raman spectroscopic data. We developed the FishCNN framework that combines new CNN models with data preparation procedures including multiple data preprocessing and augmentation methods. This approach was sepcifically designed for Raman spectroscopy-based fish biochemical composition analysis, a task that presents unique challenges due to the small dataset size and domain-specific requirements. While it is specialized for this application, FishCNN can be adapted to other spectral data with similar characteristics, particularly when data is limited.\nOur research yielded several key findings and contributions. We discovered that preprocessing methods like SNV, followed by data augmentation, are crucial for enabling the CNN to work efficiently on small spectral datasets. We mitigated the risk of overfitting by conducting 10 individual runs of 6-fold cross-validation and applying data augmentation and regularization techniques. These strategies were essential for improving the model's generalization, adddressing the challenges posed by small datasets, which are common challenge in this field. Additionally, a CNN configuration with a large kernel size and small stride consistently achieved high performance across all targets, demonstrating robustness in predicting the water, protein, and lipids yield contents in Hoki-Mackerel fish. This highlights its potential for real-time biochemical composition analysis in the marine fish industry. Furthermore, our work bridges a significant gap by applying CNNs to small spectral quantitative analysis data, an area that has been largely unexplored. Our work opens new avenues for the application of deep learning in the analysis of fish spectral data, where large datasets are unavailable.\nFor future work, we aim to explore more advanced deep learning architectures such as transformer encoders [3] and the combination of 1D convolutions with multihead self-attention mechanisms [19], which have already demonstrated the effectiveness in spectral classification tasks. Additionally, incorporating attention mechanisms could enhance interpretability by highlighting specific spectral features or absorption peaks, aligning with the domain principles."}, {"title": "7 Data, Code and Appendix Availability", "content": "Due to confidentiality agreements, the dataset and code used in this study cannot be made publicly available. However, researchers interested in discussing our methods or seeking further clarification are encouraged to contact us directly. An online appendix is available, containing additional details such as preprocessing design matrix, baseline comparisons, RMSE scores, and ablation study, which can be accessed at [?]."}]}