{"title": "Structural Design Through Reinforcement Learning", "authors": ["Thomas Rochefort-Beaudoin", "Aurelian Vadean", "Niels Aage", "Sofiane Achiche"], "abstract": "This paper introduces the Structural Optimization gym (SOgym), a novel open-source reinforcement learning environment designed to advance the application of machine learning in topology optimization. SOgym aims for RL agents to learn to generate physically viable and structurally robust designs by integrating the physics of TO directly into the reward function. To enhance scalability, SOgym leverages feature mapping methods as a mesh-independent interface between the environment and the agent, allowing for efficient interaction with the design variables regardless of the mesh resolution. Baseline results are presented using a model-free proximal policy optimization agent and a model-based DreamerV3 agent. Three observation space configurations were tested, with the TopOpt game-inspired configuration\u2014an interactive educational tool that improves students' intuition in designing structures to minimize compliance under volume constraints\u2014performing best in terms of performance and sample efficiency. The 100M parameter version of DreamerV3 produced structures within 54% of the baseline compliance achieved by traditional optimization methods as well as a 0% disconnection rate\u2014an improvement over supervised learning approaches that often struggle with disconnected load paths. When comparing the learning rates of the agents to those of engineering students from the TopOpt game experiment, the DreamerV3-100M model shows a learning rate approximately four orders of magnitude lower, an impressive feat for a policy trained from scratch through trial and error. These results suggest RL's potential to solve continuous TO problems and its capacity to explore and learn from diverse design solutions. SOgym provides a platform for developing RL agents for complex structural design challenges and is publicly available to support further research in the field.", "sections": [{"title": "Introduction", "content": "In structural design, the task of Topology Optimization (TO) seeks to distribute a limited amount of material within a given design space under specific design constraints and objectives. Density-based methods [1], and in particular the SIMP method [2], which parameterize the topology of a structure using penalized individual elemental densities in a discretized finite element mesh, became the standard TO framework thanks to their robustness, simplicity of implementation [3], and adaptability to various manufacturing constraints [4] and optimization objective [5]. TO is an iterative procedure where, most often, Finite Element Analysis (FEA) is used to compute the structural response of a given topology under specific boundary conditions. At each iteration, a sensitivity analysis is performed to obtain the gradient of the optimization objective as a function of the design variables."}, {"title": "", "content": "The application of TO shows potential for reducing the material and energy consumption of large-scale structures such as suspension bridges [6] and entire airplane wings [7], presenting engineers with more sustainable designs in big energy-hungry industries such as transport and infrastructure. Although both FEA and adjoint sensitivity analysis can be performed highly efficiently on massively parallel computing systems, the sparse access to such facilities still curbs the widespread use of the TO method.\nOptimizing for nonlinear multi-physics is especially important if the structures are to be manufactured through Additive Manufacturing (AM) which often generates lattice or cellular structures that are more prone to nonlinear deformation such as local buckling and inelastic yield [8]. These physics can be implemented in TO frameworks, but the additional computational cost associated with them escalates substantially, limiting their broader applicability [9].\nMotivated to lower the cost barrier of large-scale and high-fidelity TO, researchers have proposed various methods to reduce the number of total iterations required or mitigate the computational expenditure associated with each iteration [10]. Over the past few years, the introduction of machine learning-based methodologies to expedite TO has been noticeably prevalent. Rapid advances in the subfield of deep learning, a branch of machine learning, stimulated a gold rush-like interest in the deployment of its methodologies across a wide range of applications. The parameterization of the TO density field as pixel/voxels has naturally brought many researchers to flock to image-based deep learning methods to try to accelerate the TO process as a low-hanging fruit. Most of these approaches propose a standard supervised learning pipeline of generating optimal topologies, and then training a computer vision model such as a convolutional neural network via a direct regression loss, or in an adversarial context.\nThe training loss functions used in these supervised learning methods only optimize indirectly for the TO objective and do not consider the actual physics of the TO problem. In previous work, the incompatibility between a regression loss function on the design variables and the TO objective has been demonstrated, showing that a common failure mode of such models is to generate structures with a disconnected load path, and the inability to treat boundary conditions outside the training set [11, 12].\nTo tackle this issue, deep learning approaches for TO need to be enriched with the underlying physics of the problem [12]. Research has been conducted to reparametrize the density field in TO problems and to integrate the finite element analysis (FEA) operation directly into the model's loss function, enabling backpropagation through the entire structural analysis [13, 14]. This necessitates the use of sparse solvers in auto-differentiation software, which are currently only experimentally supported in popular deep learning libraries. Consequently, the cost of backpropagation scales not only with the model size but also with the size of the FEA mesh. Surprisingly, all these studies also omit the potential for the structure to be generated in a single forward pass through the model, instead concentrating on achieving an optimal structure by updating the parameters for each new problem through gradient descent on the model's parameters. Thus, this paradigm presents more as an alternative to typical optimizers, rather than offering a means to leverage past solutions for solving the current problem."}, {"title": "", "content": "In this paper, the standard compliance minimization objective of TO is framed as a reinforcement learning (RL) task to train a deep learning agent to generate structures as efficiently as possible for various Boundary Conditions (BC). Unlike supervised learning, which is limited by the availability, diversity, and quality of training data, RL operates on the premise of trial-and-error, learning to make decisions based on the maximization of cumulative rewards over time.\nFraming TO as a RL task draws inspiration from the interactive TopOpt game [15]. In this game, developed at the Technical University of Denmark, players use a user interface to design structures that maximize a score based on compliance minimization while adhering to volume constraints. The game's effectiveness was evaluated in a classroom setting with undergraduate design engineering students. The study aimed to explore the potential of gamifying TO, and results demonstrated a positive correlation between the players' experience and the structural performance of their solutions, suggesting that students' intuition in TO improved with practice.\nThe literature on using RL for TO is sparse, with only two significant studies addressing this approach. The first study [16] introduced a method using a graph representation of a binary truss topology, where an RL agent was trained to remove unnecessary truss components from a fully connected graph, showing generalization capabilities to various boundary conditions and larger grid resolutions. However, it is worth noting that full FEA is still performed in each iteration and hence, that the speedup is most likely related to an improved design space representation. The second study [17] extended this approach to discretized 2D domains using a fixed grid mesh. Here, an RL agent iteratively removed individual pixels from a fully saturated 2D material mesh until reaching a sub-optimal solution. While these methods share similarities with evolutionary TO algorithms, they are constrained by the limited action space of manipulating individual elements, making it challenging to scale to higher mesh resolutions due to the exponential increase in computational complexity.\nThis paper introduces the Structural Optimization gym (SOgym), an open-source reinforcement learning (RL) environment. SOgym provides a platform for developing and training RL algorithms specifically for structural design. It recreates the TopOpt game's scoring system and user interface, allowing for direct comparisons between the learning rates of engineering students and trained RL agents. Baseline performance results for standard RL algorithms are also included, offering a foundation for future research."}, {"title": "Method", "content": "The SOgym environment is based on the Gym [18] framework and defines the interactions between the agent and the environment. It specifically outlines three key components:\n1. The action space: a set of continuous actions that the agent can take, corresponding to the design variables of the topology.\n2. The observation space: a representation of the current state of the TO problem.\n3. The reward function: feedback provided to the agent based on the performance of its generated structure."}, {"title": "Action space", "content": "This paper uses feature-mapping methods [19] as a more scalable and interpretable interface for RL agents to interact with the material distribution of the TO problem, in comparison to using an elemental density action space similar to density-based TO methods, as implemented in [16] and [17]. This approach offers two key advantages: (1) the number of design variables remains independent of the mesh resolution, enhancing scalability for the action space of the agent, and (2) the design variables exhibit geometric interpretability, which can streamline the post-processing phase of the optimal topology.\nSOgym uses a modified implementation of the 188-line Method of Moving Morphable Components (MMC) code [20]. In this approach, the endpoints' coordinates and two thicknesses define a component with a linearly varying thickness, as illustrated in Figure 1 and detailed in Equations 1 through 7. The RL agent sequentially places MMC components in the design space to create the stiffest structure possible within a volume constraint. The maximum number of steps per episode, equivalent to the number of components, is a user-defined variable controlling the structure's complexity. For the baseline experiments in this paper, a maximum of 8 components was used. An episode involves iteratively placing these 8 components, with FEA performed 'per step' or 'per episode' for compliance evaluation, depending on the observation space configurations detailed later in this section. This framework can be scaled to larger models by increasing the number of components, enabling the exploration of more complex structures.\nXo = \\frac{X_A + X_B}{2} \t Yo = \\frac{Y_A + Y_B}{2}\nL = \\sqrt{(X_B - X_A)^2 + (Y_B - Y_A)^2}\n\\theta = arctan2(B_y - A_y, B_x - A_x)\nX_1 = cos(\\theta) (x - x_0) + sin (\\theta) (y - y_0)\nY_1 = - sin(\\theta) (x - x_0) + cos(y - y_0)\nt(x_1) = \\frac{(t_A + t_B)}{2} + \\frac{(t_B - t_A)}{2L} (x_1)\n\\Phi(x_1, y_1) = 1 - \\Big( \\frac{x_1}{L/2} \\Big)^6 + \\Big( \\frac{y_1}{L/2} \\Big)^6\\Big)^{1/6}\nXo, yo, L and \\theta are the coordinates of the centroids, the length, and the orientation of the MMC component, defined by the endpoint coordinates XA, YA, XB and YB. A linearly varying thickness t(x1) is defined by interpolating between the two endpoint thicknesses tA and tB. These geometric variables define the shape of the hyperelliptic function \u03a6(x1, y1) named the Topology Description Function (TDF). The density distribution on the discretized FEA mesh is obtained using a regularized Heaviside (He) projection of the TDF, and the ersatz material model is used to define the elemental densities \u03c1e and Young's modulus Ee. The projection scheme and the ersatz model are described in equations 8 through 10. A regularization parameter \u03b5 is used with a value of le-2, and a small positive value \u03b1 of le-9 helps avoid numerical singularities with the stiffness value of void elements.\nH_e(\\Phi) = \\begin{cases}\n    1, & \\text{if } \\Phi > \\epsilon \\\\\n    \\frac{3(1-\\alpha)}{4} \\Big(\\frac{\\Phi}{\\epsilon} - \\frac{\\Phi^3}{3\\epsilon^3} \\Big) + \\frac{(1+\\alpha)}{2}, & \\text{if } |\\Phi| \\leq \\epsilon \\\\\n    \\alpha & \\text{otherwise}\n  \\end{cases}\n\\rho_e = \\frac{1}{A_e} \\sum_{i=1}^4 H_e(\\Phi)\nE_e = \\rho_e E\nThe continuous action space of the agent is a vector of 6 elements with values normalized between [-1,1] corresponding to the two endpoint coordinates as well as the two endpoint thicknesses. The actions are scaled to the design space using min/max values defined in Table 1."}, {"title": "Boundary conditions", "content": "Instead of relying on the typical train-test set of Boundary Conditions (BC) common in machine learning TO literature, SOgym introduces a large, random distribution of BC at the beginning of each episode. This distribution is extensive enough to make encountering every possible combination impractical within a reasonable training timeframe. As a"}, {"title": "", "content": "result, the agent consistently faces new and varied challenges, encouraging the development of generalized problem-solving skills instead of overfitting to a limited set of problems.\nA 2D rectangular domain was used where support and load conditions, desired volume fractions, and domain dimensions were varied to generate multiple unique parameter combinations, bringing sufficient diversity during training. The distribution of parameters, which were all sampled from uniform distributions with ranges specified in the third column, is illustrated in Figure 2 and detailed in Table 2. The support condition is placed randomly on one of the four boundaries without spanning multiple boundaries, covering only a fraction of the chosen boundary at a time. The point load is then placed randomly on the opposite boundary, with a random orientation and a unit load magnitude. A coarse mesh resolution of 50 elements per unit of height and width was set for the FEA to limit the computational cost of running the environment."}, {"title": "Observation space", "content": "SOgym offers three observation space configurations: Vector, Image, and a simplified imitation of the TopOpt game interface, as detailed in Table 3.\n1. Vector configuration: It includes a boundary conditions description vector (\u03b2), based on [11] and detailed in Table 4. This vector encapsulates all the parameters defining the current TO problem. Although our baseline experiments use a single point load and a fully supported support type, the \u03b2 vector was designed to handle up to five-point loads and multiple support types. Additionally, this configuration includes:\n\u2022 A fraction indicating the number of remaining steps in the episode,\n\u2022 A vector containing the values of all the current design variables,\n\u2022 A fraction representing the current volume.\n2. Image configuration: It adds a low-resolution image of the current design, illustrating support and load arrows.\n3. TopOpt game configuration: It adds the current score and an image of the normalized strain energy in a jet color scheme, only shown if the load path is connected. Figure 3 illustrates this configuration at the end of an episode with random actions and a connected final structure."}, {"title": "", "content": "The Vector configuration is more compact, while the Image and TopOpt game configurations offer richer observations at the expense of higher computational costs. The TopOpt game configuration demands solving the displacement field at every iteration, unlike the others, which only do so at the episode's end. This complexity enriches the RL agent's training data but slows down training due to extra computations and solver calls. The trade-off between the richness of the observation space and the computational efficiency of the environment is an important consideration that will be further explored and analyzed in the results section of this paper."}, {"title": "Reward function", "content": "Like the TopOpt game, a sparse reward is given only at the final timestep (tmax) and corresponds to the inverse of the compliance with a hard constraint on volume and load-path connectivity. The function sets the reward to 0 if the structure's final volume exceeds the defined constraint or if the load and support are disconnected. Since the magnitude of the compliance can vary widely depending on the boundary conditions and the generated structure, SOgym uses the natural logarithm of the compliance to squash the reward signal. This reward-squashing strategy, shown in Figure 4, has been demonstrated to enable RL algorithms to generalize better across diverse environments [21]. Equation 11 shows the SOgym reward given at tmax where V* is the volume constraint, Vt, Vtmax is the volume at the final timestep and Ktmax is a boolean variable that is equal to 1 if the load-path of the final structure is connected.\nrtmax = \\begin{cases}\n    \\frac{1}{\\text{loge(compliancetmax)}}, & \\text{(}V_{tmax} \\leq V^* \\text{ and } K_{tmax} = 1\\text{)} \\\\\n    0, & \\text{elsewhere}\n  \\end{cases}"}, {"title": "RL algorithms", "content": "Two RL algorithms, one model-free and one model-based, are trained and tested to establish an initial performance baseline for SOgym. Proximal Policy Optimization (PPO) [22] is a popular model-free, on-policy algorithm known for its simplicity, robustness, and successful applications in various domains, including chip design [23], defeating world champions in complex esports [24], and human alignment of large language models [25]. DreamerV3 [21], a model-based RL algorithm, is chosen for its innovative blend of policy optimization and world model learning. It utilizes a learned world model to create imagined trajectories, predicting environmental responses to specific actions. The world model consists of three key components: a representation model that compresses observations into latent states, a transition model that predicts the next latent state based on the current state and action, and a reward model that estimates rewards from these latent states. This approach enables DreamerV3 to simulate future outcomes, plan strategically, and explore the environment more efficiently, resulting in improved sample efficiency in most RL benchmarks."}, {"title": "Model architecture", "content": "Two versions of DreamerV3 were trained: the smallest model, with 12 million parameters, and a larger model with 100 million parameters. The 12M parameter model was chosen because it was the smallest tested in [21], providing a baseline for comparison. The 100M parameter model was selected as it represents a middle-ground model, balancing complexity and performance. Details about the model architectures and default hyperparameters used for the baseline results of this paper are provided in [21]. For the PPO algorithm, the IMPALA CNN network architecture [26] was chosen due to its effective balance of performance and computational cost [27]. The CNNs used as a feature extractor for the image inputs are detailed in Figure 5. Vector inputs are processed through a single fully connected layer to extract features. These feature embeddings are concatenated and passed to an LSTM layer, which then feeds into both the policy and value networks. The architecture illustrated in Figure 5 is repeated for the policy and the value networks, as they do not share feature extractors. The PPO hyperparameters used in [21] are used as they were selected based on recommendations for on-policy RL algorithms [28]."}, {"title": "Results", "content": "To monitor the models' performance during training, a fixed set of 10 problems was randomly sampled from the BC distribution to serve as evaluation benchmarks. This number was kept small to limit the impact on the training duration. At regular intervals during training, the agent evaluates its performance by greedily sampling actions across these 10 problems. The mean evaluation reward from these runs is used to assess the performance across the three observation space configurations and the two baseline algorithms. This evaluation reward was also used to calculate a learning rate on the same basis as the in-class experiment from the TopOpt game.\nAll training runs were conducted on a single compute node equipped with an Nvidia A100 GPU and a 48-core CPU. The requirement for FEA to evaluate rewards makes the environment CPU-bound. To mitigate this bottleneck 48 parallel copies of the environment and policy are run simultaneously to aggregate experiences between gradient updates efficiently."}, {"title": "Evaluation of performance across different observation spaces", "content": "The impact of the observation space was first tested on the Dreamerv3-12m model since it was the smallest model tested in terms of the number of trainable parameters. Due to computational constraints, the first training runs with DreamerV3-12M are limited to 48 hours. As illustrated in Figure 6a) and Figure 6b), the Vector observation space configuration completed just under 6M training episodes and reached a best evaluation reward of 0.27 for an average of 279 Frames Per Second (FPS), while the Image configuration completed around 3.75M episodes with a best evaluation reward of 0.28 with an average FPS of 173. The TopOpt game configuration showed a much slower FPS of 112, but it managed to reach a higher maximum evaluation reward of 0.29 in just under 2.5M episodes."}, {"title": "Dreamerv3 vs PPO", "content": "The TopOpt observation space was used as a benchmark to compare the performance of the DreamerV3-12M, DreamerV3-100M, and PPO agents. To investigate whether increasing the training budget could enhance performance, as the DreamerV3-12m evaluation reward in Figure 6a) had not plateaued after the initial 48-hour time budget, an additional 96 hours of training time was allocated, bringing the total training time for each algorithm to 6 days. Figure 7 illustrates the evaluation rewards throughout the training period for the three algorithms. Both versions of DreamerV3 performed better than the PPO agent, with maximum evaluation rewards reaching 0.30 for the 100M parameter version, and 0.29 for the 12M parameter version. They showed a rapid increase in evaluation reward and a slower rate of learning after about 1M episodes. The PPO agent showed a slower rate of increase in evaluation rewards, reaching a maximum value of 0.19 before suffering from a significant episode of instability, causing the evaluation reward to drop by more than 25%. However, this performance drop was mitigated by saving the best-performing version of the model during training. This ensured that even if the model's performance declined later, the best result achieved up to that point was preserved. This instability coincided with a spike in Kullback-Leibler (KL) divergence, as can be observed in Figure 8, which measures the divergence between the new and old policy distributions, providing insight into the algorithm's learning stability. An increase in KL divergence indicates potential training instability."}, {"title": "Learning rate comparison", "content": "A linear regression was fitted to the points on the training curve up to the moment of best evaluation performance for each algorithm. To match the scoring method of the TopOpt game paper [15], the evaluation reward was converted to inverse compliance and normalized using the maximum observed evaluation reward across the three RL training runs.\nThe origin of the linear regression was forced to zero since the three agents started from scratch. Figure 9 illustrates the best regression lines for all three agents alongside the linear regression derived from the in-class experiment. The DreamerV3-100M model demonstrated the fastest learning rate among the three RL agents, with a value of 1.78e-07 per episode. This rate is nearly four orders of magnitude lower than the 2.40e-03 learning rate observed among the engineering students in the in-class experiment. The DreamerV3-12M agent followed with a learning rate of 9.13e-08, and PPO had the lowest learning rate of 5.46e-08."}, {"title": "Structural performance comparison with baseline", "content": "The performance of the generated structures from the 3 RL agents was tested using 1000 random boundary conditions problem sampled uniformly from the SOgym distribution. A baseline structure was established with an optimal design obtained via conventional optimization. A hybrid optimizer, integrating both MMA and GCMMA [29], was utilized because of its superior convergence speed on similar problems within the MMC framework [30]. The 188-line MMC implementation [20] in Python was used and the parameters defining the optimization process are presented in Appendix A. The RL agents' generated test structures, presented in Table 5, were evaluated using the following metrics:"}, {"title": "Discussion", "content": "The baseline results have shown that the SOgym environment is effective for the development of RL agents for the task of TO. Although the trained baseline agents showed worse structural performance when compared to the baseline, it is noticeable that they have achieved this level of performance from a random policy, discovering stiffening patterns and load-path connectivity through pure trial-and-error exploration."}, {"title": "Algorithm performance and structural quality", "content": "A common pattern across the RL-generated designs that can be observed in Figure 10 is the strategic placement of MMC components along the boundary where the support and load are positioned. This behavior is likely influenced by the load-path connectivity requirement in the reward function. During the early stages of training, RL agents seem to adopt a straightforward approach to ensure connectivity by spanning the entire support and load boundaries with components. This can be seen prominently in the PPO-generated samples of Figure 10, where an MMC component consistently covers the boundary area where the load is applied. The DreamerV3 agents develop a more refined approach for the load boundary as they are more accurate in the placement of the components to connect the load, resulting in more efficient designs. They still exhibit components spanning the support boundary.\nAlthough the implementation of the environment in this paper is made to stick as close as possible to the parameters of the TopOpt game experiment, soft design constraints like the one described in Equation 12 could provide a denser reward scheme and provide more information to the agent which could help address the behavior of covering the entire support and load boundary with material. Also, the usefulness of using the load path connectivity boolean should be investigated as this also has the impact of sparsifying the feedback from the environment early on during training.\nrtmax = \\frac{1}{log_e(compliance_{tmax})} (1 - |V_{tmax} - V^*|)^2"}, {"title": "Breakeven threshold", "content": "When looking at the breakeven threshold, an economic measure proposed in [31] which evaluates the number of problems that the model needs to generalize to compensate for its training cost, the DreamerV3-100M agent exhibits a breakeven threshold of about 19 000. This is derived from the fact that the total processing time for the RL agent, including training, is approximately 19 times longer than that of the baseline optimization method for generating 1000 structures. However, the method shows good potential to be useful in practice if the trained agent can get to a structural performance level that is more competitive with the conventional optimization process."}, {"title": "Richer observation space brings training efficiency and performance", "content": "The evaluation across different observation space configurations revealed the trade-offs between computational cost and the richness of the observation space. The TopOpt game configuration, which provides the most detailed observations including the load path and strain energy visuals, facilitated the highest reward among the tested configurations at the expense of slower training speeds due to the increased computational demands of computing the"}, {"title": "", "content": "displacement field at each iteration and processing the additional image observations. However, the higher evaluation reward was reached in a smaller number of episodes, showing that the higher training efficiency per environment interaction offsets the slower FPS."}, {"title": "Model-based DreamerV3 outperforms model-free PPO", "content": "The sample efficiency and performance demonstrated by DreamerV3 highlight the benefits of using a world model for planning. In compute-intensive environments like SOgym, where interacting with the environment is inherently slow, this efficiency is particularly crucial. The ability to predict the future state of strain energy based on material placement is intuitively fundamental for designing efficient structures. By understanding how material changes impact strain energy distribution, the agent can make better decisions. Accurately predicting the outcome of placing components and thus altering the strain field is essential for optimizing performance in complex design tasks. This highlights the importance of world models in applying RL to topology optimization.\nIncreasing the training budget from 48 hours to 6 days did not improve significantly the performance of the DreamerV3-12M model. Both DreamerV3-12M and DreamerV3-100M exhibited plateaus in their evaluation curves, which might be due to insufficient exploration caused by a lower entropy coefficient (3e-4) compared to PPO's higher entropy coefficient (1e-2). This lower exploration could limit the models' ability to discover more optimal design solutions. Further optimization of hyperparameters could perhaps enhance both sample efficiency and structural performance. Deep reinforcement learning methods are susceptible to these parameters, and careful tuning has been shown to impact performance in various applications dramatically [32]."}, {"title": "A \"physical\" RL training objective solves the disconnection problem", "content": "A noteworthy result is the 0% disconnection rate of the structures generated by the RL agents. This outcome suggests the effectiveness of embedding the physics of the TO problem directly into the training objective. This helps address the common limitation in direct-design TO approaches, where disconnection rates can soar as high as 60% in out-of-distribution test cases [11]. Framing the TO objective as an RL problem also has the added benefit of simplifying the implementation by avoiding the need for backpropagation through sparse solvers. The RL approach relies solely on the forward pass through the FEA and estimates gradients from the rewards gathered over trajectories. This avoids integrating sparse solvers with automatic differentiation software, though it does result in a more sample-inefficient learning process due to its reliance on the less direct form of feedback of trial and error."}, {"title": "Comparing learning rates with engineering students", "content": "The comparison of learning rates between the RL agents and the engineering students in the TopOpt game experiment reveals a notable difference. The DreamerV3-100M model, the best-performing RL agent, showed a learning rate about four orders of magnitude lower than that of the students. This significant difference highlights the advantage humans have in applying their intuition and prior knowledge to new and complex tasks. Even though the RL agent's"}, {"title": "", "content": "learning rate is much slower, it is still impressive considering that, for an equal comparison, one would need to compensate for the inherent visual and cognitive abilities that the students bring to the task.\nDuring the in-class experiment, the engineering students received a brief 5-minute introduction explaining the basic theory of TO for compliance minimization. This session provided them with crucial insights into the importance of material distribution, explaining that areas depicted in red on the strain-energy image indicated regions needing more material and that an optimal design should aim for a uniform distribution of strain energy. To mirror this added context, future research could study the integration of a dual-objective reward function that incorporates the notion of uniform strain energy distribution into the agent's objective. This concept, as described in Equation 13, suggests scaling the minimum compliance objective by the normalized standard deviation of the strain energy. This reward function could encourage RL agents to produce designs that are both stiff and demonstrate a balanced strain energy distribution across the design domain, thereby providing richer feedback to the agent.\nrt = \\frac{1}{loge(compliance)} (1 - \\frac{\\sigma(W)}{\\sigma(W) + \\mu(W)})"}, {"title": "", "content": "In addition to refining the reward function, another proposed idea to enhance the RL agents' learning efficiency is to leverage optimized topologies for pretraining. This approach involves a two-step training pipeline: starting with an imitation learning phase where the policy network is pretrained using supervised behavioral cloning [33] or adversarial imitation learning [34] from optimal topologies obtained via conventional optimization, followed by a pure policy training phase through RL. Initializing the policy with expert data can provide the RL agents with a strong starting point, rather than beginning from a random policy, similar to providing students with basic TO theory. This method has proven effective in complex environments such as StarCraft II [35] and the game of Go [36], leading to significant breakthroughs in those challenging domains."}, {"title": "Conclusion", "content": "This paper introduces the Structural Optimization gym (SOgym), a novel RL environment designed for advancing the application of machine learning in TO. By framing the TO objective as an RL task, SOgym enables the development and training of RL agents to explore and exploit design spaces for optimal structural solutions.\nThe baseline results illustrate progress in training RL agents to solve continuous TO problems. This suggests the potential of RL, especially model-based algorithms like DreamerV3, in producing structurally robust designs. Despite beginning from a random policy, these agents successfully learned to produce connected topologies that were within 54% of the compliance achieved through conventional optimization. The comparison between model-free (PPO) and model-based (DreamerV3) approaches highlights the significant benefits of incorporating planning and world models, especially in enhancing sample efficiency and performance in computationally intensive environments like SOgym."}, {"title": "", "content": "However, it is important to note that the primary goal of this paper is to introduce the SOgym environment and present initial baseline results, providing a preliminary understanding of what can be achieved with this new tool. These results"}, {"title": "", "content": "are intended as a starting point rather than a definitive solution, and further research is needed to fully explore and validate the capabilities of RL in structural optimization.\nAdditionally, RL's framework presents opportunities for incorporating qualitative criteria into the optimization process. Inspired by the success of Reinforcement Learning from Human Feedback (RLHF) in fine-tuning large language models based on human preferences [37], future research could explore similar methods in structural optimization. By integrating qualitative aspects such as aesthetics into the reward function, RL agents could potentially be trained to produce designs that balance both functional and visual appeal. This approach could lead to innovative solutions that not only meet technical specifications but also enhance architectural beauty and user satisfaction.\nBeyond exploring different environment objectives, the SOgym environment can be extended to support three-dimensional structures. This expansion would enable the exploration of more complex and realistic design scenarios, broadening the applicability of RL in structural engineering.\nIn summary, RL presents an interesting take on building up intuition for minimum compliance TO problems using ML. Moreover, the SOgym provides a valuable foundation for ongoing and future research and paves the way for future human feedback interaction which could prove a valuable design tool."}, {"title": "Data & Code Availability", "content": "The SOgym environment is open-source and available at https://github.com/ThomasRochefortB/sogym"}, {"title": "Usage of Generative AI", "content": "During the preparation of this work, the authors utilized GPT-40 (version 2024-05-13) to enhance the clarity and conciseness of the language of their own text. After using this tool/service, the authors also reviewed and edited the AI-generated results as needed and they take full responsibility for the content of the paper."}, {"title": "Appendix A \u2013 Hybrid Optimizer Parameters", "content": "Conventional optimization was run using a hybrid MMA-GCMMA optimizer which switched from MMA to GCMMA after detecting oscillations in the objective function according to a switch criterion. A maximum of 1000 outer iterations were allowed during the iterative process. A low resolution of 50 elements per dimension unit was used to fit the resolution of SOgym. Table 7 describes the parameters defining the MMA and GCMMA optimizer."}]}