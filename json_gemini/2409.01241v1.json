{"title": "CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation", "authors": ["Sorin Grigorescu", "Mihai Zaha"], "abstract": "The underlying framework for controlling autonomous robots and complex automation applications are Operating Systems (OS) capable of scheduling perception-and-control tasks, as well as providing real-time data communication to other robotic peers and remote cloud computers. In this paper, we introduce CyberCortex.AI, a robotics OS designed to enable heterogeneous AI-based robotics and complex automation applications. CyberCortex.AI is a decentralized distributed OS which enables robots to talk to each other, as well as to High Performance Computers (HPC) in the cloud. Sensory and control data from the robots is streamed towards HPC systems with the purpose of training AI algorithms, which are afterwards deployed on the robots. Each functionality of a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is executed within a so-called DataBlock of Filters shared through the internet, where each filter is computed either locally on the robot itself, or remotely on a different robotic system. The data is stored and accessed via a so-called Temporal Addressable Memory (TAM), which acts as a gateway between each filter's input and output. CyberCortex.AI has two main components: i) the CyberCortex.AI.inference system, which is a real-time implementation of the DataBlock running on the robots' embedded hardware, and ii) the CyberCortex.AI.dojo, which runs on an HPC computer in the cloud, and it is used to design, train and deploy AI algorithms. We present a quantitative and qualitative performance analysis of the proposed approach using two collaborative robotics applications: i) a forest fires prevention system based on an Unitree A1 legged robot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system which uses CyberCortex.AI for collaborative perception and motion control.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite many developments in Artificial Intelligence (AI) and software frameworks, the Operating Systems (OS) currently used in robotics have not changed over the last 15 years [1]. These are typically overengineered and lack the encapsulation of high-bandwidth data streaming techniques required for the design, training and continuous deployment of the AI algorithms used within the robots' perception-and-control pipelines. Real-time communication and data management is particularly important in the age of Artificial Intelligence (AI), since the development and evaluation of AI algorithms is dependent on high quantities of sensory and control information.\nOver the last decade, AI and Deep Learning [2] became the main technologies behind many novel robotics applications, ranging from autonomous navigation and self-driving cars [3] to dexterous object manipulation [4] and natural language processing [5]. The key ingredient in a deep learning algorithm is the data collected from the robots themselves, data which is required for training the Deep Neural Networks (DNN) running within the perception-and-control pipelines of the robots.\nEven with the latest decade's advances in AI, current robotic operating systems still lack a data-driven architecture that would enable rapid AI-based application development for robotics. ROS (Robotic Operating System) [6], the current de-facto middleware for building robotic software, provides many classical (non-learning) algorithms in the form of packages, but lacks the architectural design which would enable the rapid development, retraining and testing of AI technologies for robotics. To meet these challenges, many developers have created a wide variety of AI software stacks"}, {"title": "II. RELATED WORK", "content": "In the following, we provide an overview of software frameworks for robotics, dedicating special chapters on ROS and on operating systems used in the automotive industry."}, {"title": "A. Robotics Software Middleware", "content": "A comprehensive review of robotics software middleware can be found in [9]. The author argues that robots need to advertise their functionality as services in order to allow other members of the fleet to interact with them. The cited robotics middleware have been classified into four groups, based on their mechanisms used for communicating with distributed components:\nRemote Procedure Call (RPC) [10] and Distributed Object Middleware (DOM) middleware [11];\nMessage-Oriented Middleware (MOM) [12];\nTransaction-Oriented Middleware (TOM) [13];\nService Oriented Architecture (SOA) [14], Service Oriented Middleware (SOM) [15] and Enterprise Service Bus (ESB) [15].\nBased on the number of citations, the eight most common robot middleware are:\nRobot Operating System (ROS) [6];\nOpen Robot Control Software (OROCOS) [16];\nPlayer/Stage [17];\nNvidia Isaac SDK [18];\nMobile Robot Programming Toolkit (MRPT) [19];\nWebots [20];\nMicrosoft Robotics Developer Studio (MRDS) [21];\nOrca [22].\nThe Robot Operating System (ROS) [6] is a collection of tools, libraries and conventions used to facilitate distributed communication over a wide variety of platforms. Since ROS is currently the most used middleware in robotics, we have dedicated subsection II-B for its analysis.\nThe Open Robot Control Software (OROCOS) [16] is a collection of portable C++ libraries for advanced machine and robot control. Its two main components are the Orocos Real-Time Toolkit (RTT), used for writing C++ modules, and the Orocos Component Library (OCL), used to start an application and interact with it at run-time."}, {"title": "B. Robot Operating System (ROS)", "content": "The monopoly in robotics software is currently held by ROS (Robot Operating System) [6]. ROS itself is not an operating system in the traditional sense of process management and scheduling, but instead provides a communication layer between computing units. The goals of ROS, stated in [6], are peer-to-peer communication, tool based, multi-lingual, thin and free open-source.\nAs reported by various research and industry groups, ROS is a bulky and difficult to use middleware, with a steep learning curve [28]. Typically, the ROS distributions require large computational resources and are mainly stable on Ubuntu Linux. Even though ROS has been released in 2009, Windows support has been included only in 2019. However, its main drawback is the lack of support in the design, development and evaluation of AI components, such as the deep neural networks used for perception and control. This is understandable, since ROS has been developed in the first decade of the 2000's, before the age of artificial intelligence and deep learning. In the following, we will tackle the drawbacks of ROS and how CyberCortex.AI overcomes these.\nAlthough claimed that the principles of ROS allow for rapid prototyping of robotics systems, engineers have to deal with difficult integration problems and long module compilation times. This is particularly a problem in robotics applications which have a small to medium complexity, in the sense that ROS introduces a high level of overengineering within the overall application in general, as well as"}, {"title": "C. Automotive Operating Systems", "content": "Since the automotive domain is a subfield of robotics, it is worth visiting the operating systems and middleware implemented for such applications and their relation to AI technologies [3].\nAutomotive grade OSs are divided into critical operating systems, used for controlling vital driving functionalities (e.g. cruise control, airbag status, etc.), and infotainment operating systems, used for running the car's HMI (Human Machine Interface). In both cases, the underling framework is a real-time OS, such as QNX or Linux.\nAny software component developed for the automotive industry has to comply with the ISO 26262 standard for functional safety of road vehicles, describing four Automotive Safety Integrity Levels (ASIL) [29]. ASIL D represent the highest degree of rigor required to reduce risk (e.g., testing techniques, types of documentation required, etc.). Initially defined in 2011, the ISO 26262 standard does not yet include functional safety methodologies for developing AI components for road vehicles. Safety and trustworthiness are requirements for AI-based operating systems for automotive applications, as well as for industrial software [30].\nDifferent consortiums have been formed in order to assure scalability and software reusability between different vehicles. The most notable ones are AUTomotive Open System"}, {"title": "III. CYBERCORTEX.AI ARCHITECTURE", "content": "The CyberCortex.AI architecture from Fig. 1 has been designed in order to enable the rapid development and maintenance of distributed robotics and complex automation applications based on AI. For the sake clarity, CyberCortex.AI will be presented through a distributed robotics application example in the field of surveillance for forest fires prevention, illustrated in Fig. 2.\nThe hardware components used in this example are i) a legged robot as an Unmanned Ground Vehicle (UGV) equipped with a powerful embedded computer (Nvidia AGX Xavier), LiDAR and cameras, ii) an Unmanned Aerial Vehicle (UAV) equipped with a small computer for flight control (Raspberry PI) and iii) a mission control system, where the user can upload geolocation data for the forest area to be surveilled. The mission control system can be located either in the cloud, directly on a laptop, or on a smartphone.\nBoth autonomous robots are driven through perception-and-control pipelines, where the paths and waypoints are planned using data from the Environment Perception and Localization & State Estimation modules. Motion and flight control is executed based on the calculated paths and waypoints, respectively.\nThe algorithms running in the Environment Perception and Localization & State Estimation components from Fig. 2 are learning-based, meaning that saved sensory and control data is required for training their AI architectures prior to deployment. The AI architectures encountered in such robots are variants of Convolutional (CNN), Recurrent (RNN) and fully connected Neural Networks. Training is typically performed offline using powerful HPC cloud computers.\nApart from the local control system, each autonomous robot can benefit in its mission objectives by using, for example, environment perception information obtained by the other robot. The UGV can plan its path based on"}, {"title": "A. CyberCortex.Al.inference", "content": "The CyberCortex.AI.inference system, illustrated in Fig. 3, is a C/C++ implementation of the DataBlock. We will detail its design principles based on the distributed robotics application from Fig. 2.\nFig. 3 shows five DataBlocks running on five different devices. For the sake of clarity, only the DataBlock executed on the legged robot is detailed (left area of the figure). A DataBlock is a table-like structure specifying the structure of the processing pipeline running on a computing device that has an unique BlockID. The BlockID is equal to the device's MAC address. Each entry in the DataBlock is a so-called Filter running at a given sampling time. A filter outputs a processed datastream, while consuming datastreams provided by other filters. The output datastream is obtained either by locally processing the input datastreams, or by mapping the output of a filter computed on a remote device. In the later case, the output datastream of a filter running on a remote computer is mirrored locally, thus masking the actual location of computation. Please refer to Section III-B for details on the communication and synchronization mechanisms.\nIn the example from Fig. 3, the BlockID of the legged robot's DataBlock is 1. The first 6 filters on the legged robot are computed locally, while the rest are remote filters computed on the drone (BlockID 2), on a smartphone app (BlockID 3) and within a web browser (BlockID 4). Using this mechanism, remote computed datastreams are injected into the perception-and-control pipeline of the legged robot. Namely, the drone's camera and state are used as additional data for controlling the robot. The other way around, the same mechanism is used to transmit the perceptual information (e.g. traversable areas and/or detected objects) and the legged robot's position to the drone.\nThe example also illustrates how computing resources can be shared between robots. The datastreams from both the UGV's and the UAV's cameras are processed on the legged robot by the neural network filter 4, which receives as input the datastreams from filters 1 (UGV camera) and 2 (UAV camera). As detailed in Section III-C, the DNN outputs a datastream comprising of various prediction data, depending on the DNN architecture (e.g. object lists, semantic segmentation, road boundaries, etc.). The perception information is used locally on the legged robot for path planning, as well as on the drone for waypoints planning. In the diagram from Fig. 3, the Semantic segmentation filter constructs a 3D octree model of the environment using the output of the DNN. The 3D model is shared between the UGV and the UAV for path and waypoints planning.\nThe design of a new filter is supported by a Robotics Toolkit that provides perception, kinematics and motion control functions, as well as the AI inference engine detailed in Section III-C). The perception functions include Simultaneous Localization and Mapping (SLAM), Visual-Inertial Odometry (VIO) pipelines and basic 3D reconstruction and keypoints tracking methods. The kinematics module includes different robot models (e.g legged robots, drones, wheeled vehicles and redundant manipulator models), while the motion control module provides simple PID control functions, as well as horizon and model predictive control methods"}, {"title": "B. DataChannel and Temporal Addressable Memory (TAM)", "content": "The remote connections used to map the filters between the DataBlocks in Fig. 3 are established using a so-called protocol agnostic DataChannel. In our work, we have implemented the DataChannel on top of the WebRTC protocol [8]. WebRTC enables real-time communication capabilities by supporting video, voice and generic data to be sent between peers. In the same fashion, the DataChannel can be implemented using other protocols, such as Ethernet, of FlexRay.\nA redundant set of Signaling Servers are used to establish peer-to-peer communication over the internet, that is, the communication required by datastreams connecting mapped filters running on different DataBlocks. A signaling server's sole purpose is to setup the peer-to-peer communication. When a DataBlock is started, it automatically sends its clock time and metadata (BlockID, filter IDs, input/output types, sampling time and description of each filter) to n signaling servers. This allows a robot to discover the location and resources of other robotic systems.\nIn theory, a single signaling server can be used to establish peer-to-peer communication for all robots. However, if that single signaling server stops functioning, then the robots will not be able to start the peer-to-peer datastreams. By using n redundant servers, distributed worldwide, we minimize the risk of not being able to grant peer-to-peer connections.\nThe Temporal Addressable Memory (TAM) from Fig. 4 has been implemented in order to synchronize different datastreams, as well as to manage the data propagation process through our AI inference engine. It acts as a First-In-First-Out (FIFO) cache of size \\( \\tau_i \\), which stores a finite number of filter output samples together with their timestamps. The TAM is indexed and addressed by timestamps. These timestamps can be used to synchronize data coming from two filters running at different sampling rates. The filter from Fig. 4 uses datastreams i and j as input, while delivering datastream k as output. The synchronized samples are read from the TAM using the available timestamps.\nApart from the C/C++ implementation, a Javascript variant of the DataBlock is also available for debugging purposes on our website\u00b2. Using a browser, one can tap into a Signaling Server to check and visualize the outputs of the registered DataBlocks on that specific server. The web visualizer and debugger is a javascript implementation of the DataBlock executed within a browser, which enables one to connect to a given Signalig Server and tap into the registered DataBlocks. It is used to visualize the filters' outputs, update the DataBlock and filters to new versions, as well as to pass high-level mission commands."}, {"title": "C. AI Inference Engine", "content": "One of the main components in the Robotics Toolkit is the AI Inference Engine illustrated in Fig. 5. It is used for passing input datastreams through the layers of a Deep Neural Network (DNN). The architecture of the inference engine has been designed in conjunction with the CyberCortex.AI.dojo system, which is used for designing and training DNNs. Namely, DNNs constructed and trained on our dojo"}, {"title": "D. DNNs Design, Training and Maintenance Pipeline", "content": "The design, implementation, training and maintenance of the filters within a DataBlock is enabled by the CyberCortex.AI.dojo system from Fig. 6. The dojo is mainly written in Python and executed on dedicated computers, such as powerful HPC clusters. It is composed of a development and testing DataBlock and a set of libraries which can be used for the rapid prototyping of the filters.\nThe low-level AI libraries of choice for designing and training DNNs are PyTorch [35] and ONNX [33]. PyTorch has been chosen due to its flexibility and support for vision based neural networks, while ONNX has been chosen for its open-source format, stability and portability.\nA common base DNN structure has been implemented on top of PyTorch using the AI inference engine described in Section III-C. Each DNN exported from the dojo is inherited from this base structure. A DNN architecture is given in a configuration file which describes its input/output branches and tensor shapes. The same configuration file is loaded by the DNN filters in the CyberCortex.AI.inference system.\nThe training pipeline works by reading stored datastreams acquired from various DataBlocks running on robotic systems. In the example from Fig. 3, the Dojo DataBlock stores the camera, LiDAR and control signals from the legged robot DataBlock, as well as the drone's camera. These are later used for training the perception DNN.\nA neural network is trained by evaluating pairs of data samples \\( x^{<t-t_i, t>} \\) and labels \\( y^{<t+1, t+t_o>} \\). CyberCortex.AI treats both the samples and the labels as different datastreams. \\( x^{<t-t_i, t>} \\) are input datastreams, while the labels \\( y^{<t+1, t+t_o>} \\) are output datastreams. An Annotator is provided for visualizing the recorded datastreams. Fig. 7 shows a snapshot of the Annotator during the analysis of a saved DataBlock. The recorded datastreams are shown on the left side of the tool, while the center area shows annotated regions of interest corresponding to the objects present in the scene.\nThe labels can be obtained either by manually annotating the corresponding datastreams using the Annotator tool, or by generating them using self-supervised learning methods.\nThe inner modules of a deep neural network are typically composed of trainable layers having different hyperparameters (convolutional and recurrent layers, ResNet and RegNet submodules, etc.). However, extensive analysis and work has to be done on the pre-processing and post-processing layers. For example, one-shot object detectors such as RetinaNet [36] or Yolo [37] require object anchoring mechanisms and non-maximum suppression functions in order to recognize objects. These additional processing steps can be implemented using C/C++ functions from the Robotics Toolkit, which are also available in Python through a binding package. Once the DNN has been deployed, the functions can be called through their native C/C++ interface.\nCyberCortex.AI.dojo already contains a number of filters for data acquisition, AI inference, perception, planning and control. These are written in C/C++ and can be used to construct a DataBlock. When a new filter is developed, it is tested by running the complete DataBlock of filters, as opposed to testing it as a separate component. This enables the capturing of the intrinsic dependencies between the whole components that make up a processing pipeline, instead of just optimizing a single component.\nA special type of filters, mainly used during development, are the visualization filters. They consume input datastreams, while outputting images which superimpose the received inputs. For example, the sensing visualizer filter superimposes perception datastreams such as images, lidar, detected objects, keypoints and segmented surfaces. The dojo also includes a 3D visualization filter used in the analysis of perception systems and kinematic chains. A snapshot of the 3D visualizer can be seen in Fig. 9(b). The visualization filters are demonstrated in the interactive demos described in Appendix B."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "We have evaluated the performance of CyberCortex.AI by setting up two exemplary robotics use-cases: i) the forest fires prevention scenario described in Section III and ii) an autonomous driving pipeline which uses CyberCortex.AI for collaborative perception. In the first use-case, we perform a quantitative evaluation of the framework by measuring computation and data streaming metrics, while in the second use-case, we analyze the impact of these metrics on the overall performance of the end2end control pipeline.\nBoth use-cases require trained and maintained AI components for perception, as well as real-time data communication and computational resources sharing. Since the current de-facto standard in robotics software is ROS [6], we have evaluated CyberCortex.AI against equivalent ROS implementations of the two considered use-cases. Namely, we have implemented an equivalent ROS node for each filter in the CyberCortex.AI DataBlock."}, {"title": "A. Forest Fires Prevention", "content": "The first experimental use-case, illustrated in Fig. 8, is the forest fires prevention system from Section III. The application is composed of an Unitree Al legged robot, as the Unmanned Ground Vehicle (UGV), and an Anafi Parrot 4K drone as the Unmanned Aerial Vehicle (UAV). The two robotic systems receive high-level commands from Mission Control. These commands represent the perimeter of the area to be surveilled, which is typically located on unstructured rough terrain. The circular areas in Fig. 8 represent the field of view of each robot. Due to its larger size, the legged robot can be equipped with firefighting systems. However, the drone is solely used for scouting and information retrieval.\nThe perception data represents the segmented traversable area calculated using a ResNet-50 encoder-decoder neural network [38]. Fig. 9 shows the hardware components used in the experiments, alongside superimposed semantic segmentation data and 2D LiDAR information. The Unitree A1 robot is equipped with the same embedded Nvidia AGX Xavier board as the one depicted in Fig. 9(a). However, the Anafi Parrot 4K drone does not allow access to its underlining microcontroller for installing any additional software, such as CyberCortex.AI. In order to performed the experiments, we have instantiated CyberCortex.AI and ROS on the Raspberry PI computer from Fig. 9(a), which is wirelessly connected to the Anafi drone for readings the camera and state information.\nFig. 10 illustrates the initialization and flow of data during operation. The connections between the three systems are established via the Signaling Servers. Once the robots have discovered themselves using the servers, they can directly exchange position, sensory data, perception information and computational resources.\nConsidering the performance of the end2end pipeline, we evaluate the computational load, communication overhead, memory footprint and booting time in the form of latency. The communication latency is the time required by a component (CyberCortex.AI filter, or ROS node) to interact with the communication layer of the operating system and transmit a message to another component. The latency is defined as:\n\\( L_{t,t+1} = T_{t+1,R}- T_{t,S}, \\)\nwhere \\( L_{t,t+1} \\) is the latency, while \\( T_{t+1,R} \\) and \\( T_{t,S} \\) are the receiver and sender timestamps.\nThe sampling rate of the CyberCortex.AI and ROS control pipelines can be visualized in Fig. 11, for both the UGV and the UAV robotic systems. When active, a clock signal represents the computing cycle duration of a specific filter, where its high state indicates that the filter is processing data. The output data for each filter is stored in the Temporal Addressable Memory (TAM) during the clock signal's transition from the high to the low state. The arrows on the right side of the graphs indicate remote input/output datastream connections between the UGV and the UAV. The drone is sending camera and position data to the UGV, while the UGV processes the received data and sends back semantic segmentation information. In this particular example, the UAV camera and UAV control filters are considered remote filters on the UGV, since their actual computation takes place on the UAV, while the Semantic Segmentation filter, executed on the UGV, is considered a remote filter on the UAV. The remote filters are activated once their corresponding data"}, {"title": "B. Autonomous Driving", "content": "Additional to the quantitative study presented in the previous section, we have also performed a qualitative analysis of the proposed operating system using a collaborative autonomous driving setup. This analysis is not focused on the performance of the control algorithm itself, but on the implications of the operating system on the overall control pipeline. As in the previous section, we use the same algorithms within the CyberCortex.AI filters, as in the corresponding ROS nodes.\nTo better plan their motion, vehicles equipped with CyberCortex.AI can exchange perceptual and geolocation information. The setup is illustrated in Fig. 15, where the vehicles are sending geolocation information to the Signaling Servers. When the cars are located within a certain threshold distance to each other, the Signaling Servers will automatically interconnect them. Once connected, the vehicles will exchange GPS, camera and perception information from their respective field of view.\nWe have chosen the CARLA [39] simulator to replicate the experiments for both competing operating systems. CARLA is an open-source autonomous driving simulator, which enables the simulation of urban layouts, vehicles, pedestrians and buildings, as well as the specification of different sensor suites for the ego car. In the considered examples, we simulate the GPS and the front and rear camera sensors for two cars, while also collecting odometry data. We use the hardware depicted in Fig. 9(a), as in the previous forest fires prevention use-case.\nThe CARLA simulator runs on a desktop PC interfaced with the Nvidia AGX Xavier board and the Raspberry PI computer, each controlling one of the two cars involved in the experiments. The first car is controlled from the Nvidia AGX Xavier board, while the second car is controlled using the Raspberry PI computer. For computational efficiency, both cars use the Nvidia board for DNN inference, as in the previous section.\nFour experiments have been performed at different velocities, each over a 1km distance. As performance metric, we use the Root Mean Square Error (RMSE) between the trajectory driven using the competing robotics operating systems' control pipelines and a given reference trajectory:\n\\( RMSE = \\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} [(p_x^{<t>} - p_x^{*t>})^2 + (p_y^{<t>} - p_y^{*t>})^2]}, \\)\nwhere \\( p_x^{<t>} \\), \\( p_x^{*t>} \\), \\( p_y^{<t>} \\) and \\( p_y^{*t>} \\) are points on the driven and reference trajectories along the x-y driving plane, respectively.\nThe drivable area is segmented using a ResNet-50 encoder-decoder neural network [38], running on the Nvidia AGX board. Fig. 16(a) shows the camera images, superimposed on the DNN semantic segmentation output, for the two cars located one after another (the blue car visualizes the gray car using its rear camera, while the gray car visualizes the blue car using the front camera). Once segmented, we use the camera's inverse projection matrix to calculate the 3D projection of the drivable area in real-world coordinates. These projections are stored in a 3D octree model of the environment, which is further used for path planning. A snapshot of the 3D model is depicted in Fig. 16(b), where the left area shows the complete model calculated from received images, while the right area show the local environment surrounding the first car.\nWe use the Dynamic Window Approach (DWA) [40] to plan and control the motion of the vehicles, based on the segmented drivable area. DWA is an online collision avoidance strategy for mobile robots, which uses robot dynamics and constraints imposed on the robot's velocities and accelerations to calculate a collision free path in the top-view 2D plane. Given an input reference trajectory, the path planner must track the car's motion as close as possible to this reference, while avoiding obstacles.\nThe operating systems in both vehicles are composed of the following filters, or ROS nodes, respectively:\nFront camera filter/node;\nRear camera filter/node;\nDepth filter/node;\nGPS filter/node;\nPerception DNN (ResNet-50 encoder-decoder neural network [38]);\nSemantic segmentation filter/node;\nDWA path planner filter/node;\nVehicle actuator filter/node.\nThe position errors for trials 1 and 3 are shown in Fig. 17, while Fig. 18 shows the median and standard deviation of the RMSE recorded in the four testing trials.\nFig. 18 indicates that the RMSE is proportional to the"}, {"title": "C. Discussion", "content": "The two considered experiments have been performed in order to quantitatively and qualitatively evaluate CyberCortex.AI.\nAlthough in our experiments we have used a number of 3 devices, the maximum number of devices allowed when multiple robots are interconnected at the same time is theoretically limited by the internet connection bandwidth and the CPU power. In the Google Chromium implementation of the WebRTC protocol, the number of peers is limited to 256. However, in practical scenarios most peer-to-peer WebRTC applications support up to 20-30 connections. The reason for this limitation is to keep the CPU from overloading from compressing and decompressing video data. If the communcation does not involve video data, as in the case of streaming IMU or state estimation information, then the maximum supported limit of peers should be considerably higher. Additional infrastructure, such as Selective Forwarding Units (SFU) or Multipoint Control Units (MCU), is necesary for supporting a very large number of connections involving heavy video streaming. We consider experimenting with SFUs and MCUs in the future development of CyberCortex.AI.\nIn order to prevent unwanted devices from linking and reduce bandwidth consumption, we use a mechanism involving a unique communication datastream identifier, where a datastream is uniquely identified by the device's CoreID and the filter's ID.\nOne of the downsides of the ROS soft synchronization mechanism is that when a node operating at a higher frequency is synchronized with a node operating at a lower frequency, the synchronized output will be sampled at the rate of the low frequency node. Although CyberCortex.AI also provides this kind of soft synchronization, the processing thread within a CyberCortex.AI filter runs at a given sampling rate, which can be set equal to the frequency of the input datastream running at the highest rate. Hence, the filter can process both the high sampled data, as well as the latent incoming information.\nWithin a DataBlock, the main competition on computing resources takes place between the filters run locally. Resource allocation is currently a limitation of CyberCortex.AI, since the processing power cannot be specified and distributed for each filter. This can be overcome in a future release through the usage of a Real-Time Operating System (RTOS) such as QNX, or a RTOS implementation of embedded Linux.\nAnother current limitation of CyberCortex.AI is the lack of a functional-safety certification, such as the ASIL A to D qualifications found in AutoSAR operating systems. Both the RTOS requirements, as well as the ASIL qualifications, are considered in the future development of our software."}, {"title": "V. CONCLUSIONS", "content": "This paper introduces CyberCortex.AI, a lightweight decentralized robotics operating system designed to naturally encapsulate the development, training, deployment and continuous maintenance of artificial intelligence components required within robotics perception-and-control pipelines. We emphasize the importance of data streaming techniques for training deep neural networks, as well as their real-time usage within the AI inference engine. CyberCortex.AI uses the concept of Temporal Addressable Memory (TAM) to manage the high bandwidth and variety of data found in robotics applications.\nCyberCortex.AI focuses on the simplicity of designing perception-and-control pipelines, in the form of DataBlocks. We argue that the intrinsic dependencies between the modules are lost if each module is developed independently. This poses negative effects on the control pipeline, in the sense that if one component fails, the overall pipeline will fail. Within CyberCortex.AI, we develop and test processing pipelines as a complete DataBlock, taking into consideration the modules' interdependencies and their sampling rate."}, {"title": "APPENDIX A MULTIMEDIA DESCRIPTION", "content": "A multimedia extension has been prepared to accompany this paper. The extension shows the functionalities of CyberCortex.AI in a comprehensive video, detailing the two use-case scenarios presented in the Performance Evaluation section."}, {"title": "APPENDIX B INTERACTIVE DEMO", "content": "An interactive demonstration has been prepared to accompany this work. It can be accessed through our www."}]}