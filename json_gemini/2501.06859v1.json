{"title": "A COMPREHENSIVE EVALUATION OF LARGE LANGUAGE MODELS ON\nMENTAL ILLNESSES IN ARABIC CONTEXT", "authors": ["Noureldin Zahran", "Aya E. Fouda", "Radwa J. Hanafy", "Mohammed E. Fouda"], "abstract": "Mental health disorders pose a growing public health concern in the Arab world, emphasizing the need for accessible\ndiagnostic and intervention tools. Large language models (LLMs) offer a promising approach, but their application in\nArabic contexts faces challenges including limited labeled datasets, linguistic complexity, and translation biases. This\nstudy comprehensively evaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual ones, on diverse\nmental health datasets (such as AraDepSu, Dreaddit, MedMCQA), investigating the impact of prompt design, language\nconfiguration (native Arabic vs. translated English, and vice versa), and few-shot prompting on diagnostic performance.\nWe find that prompt engineering significantly influences LLM scores mainly due to reduced instruction following, with\nour structured prompt outperforming a less structured variant on multi-class datasets, with an average difference of\n14.5%. While language influence on performance was modest, model selection proved crucial: Phi-3.5 MoE excelled in\nbalanced accuracy, particularly for binary classification, while Mistral NeMo showed superior performance in mean\nabsolute error for severity prediction tasks. Few-shot prompting consistently improved performance, with particularly\nsubstantial gains observed for GPT-40 Mini on multi-class classification, boosting accuracy by an average factor of\n1.58. These findings underscore the importance of prompt optimization, multilingual analysis, and few-shot learning for\ndeveloping culturally sensitive and effective LLM-based mental health tools for Arabic-speaking populations.", "sections": [{"title": "1 Introduction", "content": "Mental health disorders, including depression, anxiety, PTSD, and suicide, are major global public health concerns,\naffecting millions of individuals worldwide. According to the World Health Organization (WHO), over 280 million\npeople globally suffer from depression, and anxiety disorders impact approximately 301 million individuals [1, 2].\nSuicide remains a leading cause of death, with an estimated 703,000 deaths annually [3]. These conditions significantly\nburden healthcare systems, productivity, and quality of life, underscoring the need for accessible and effective mental\nhealth interventions.\nThe Arab world is facing an alarming mental health crisis, with high prevalence rates of stress, depression, and anxiety\nreported across the region. A 2020 study revealed that approximately 35% of individuals in the MENA region frequently\nexperience stress, while about 29% report symptoms of depression. Stress levels are particularly high in Tunisia (53%),\nIraq (49%), and Jordan (42%), whereas lower but still concerning rates are observed in Egypt (27%), Algeria (27%),\nand Sudan (22%). Similarly, depression is most prevalent among Iraqis (43%), Tunisians (40%), and Palestinians (37%),\nwith notable but lower rates in Algeria (20%), Morocco (20%), and Sudan (15%)[4].\nA 2022 survey conducted in Egypt involving 3,134 individuals (1,619 females and 1,515 males) further highlights the\nseverity of the issue. The study reported a 43.5% depression rate, with a significant gender disparity: 52.9% of females\nand 33.4% of males experienced depressive symptoms [5]."}, {"title": null, "content": "However, this escalating crisis is compounded by a critical shortage of trained mental health professionals. In the\nArab region, Egypt serves as a stark example. According to a 2020 WHO report, Egypt has only 0.84 psychiatrists\nper 100,000 people and 0.86 psychologists per 100,000 people, with a total of 847 psychiatrists and 861 psychologists\nnationwide [7]. These numbers fall significantly below global averages. A 2020 WHO study reported that globally, the\nmedian number of mental health workers is 13 per 100,000 population, with 1.7 psychiatrists and 1.4 psychologists\nper 100,000 people [8]. This severe shortage of mental health professionals limits access to timely diagnosis and\nintervention, exacerbating the burden of untreated psychiatric disorders across the region.\nThe growing use of social media in the Arab world has opened new avenues for understanding and addressing mental\nhealth needs. Social media platforms allow individuals to express their struggles with depression, anxiety, and other\nmental health issues, often through posts containing linguistic markers indicative of psychological distress [9, 10].\nFor instance, individuals with depression may exhibit increased use of first-person pronouns and negative emotional\nlanguage, while those with anxiety often express worry, uncertainty, and references to physical symptoms such as fatigue\nor insomnia [11].\nWhile social media provides a rich source of real-time, large-scale data for detecting mental health conditions, it also\npresents challenges. Individuals may engage in self-diagnosis or rely on unverified information for guidance, which\nrisks misinterpretation and further distress [12]. These issues are compounded in regions where access to professional\nservices is limited, making it critical to develop reliable tools for analyzing and supporting mental health through digital\nplatforms.\nArtificial intelligence (AI) has emerged as a transformative tool in mental health diagnostics and intervention. AI\nsystems excel at processing large volumes of unstructured data to identify subtle patterns associated with psychiatric\nconditions. Early work in natural language processing (NLP) demonstrated the utility of linguistic analysis for detecting\nmental health markers, enabling tasks such as sentiment analysis and risk assessment [13].\nThe advent of LLMs, such as OpenAI's GPT-4 [14], marked a significant leap forward in NLP. These models, trained\non massive datasets, can understand and generate human-like text while capturing complex emotional cues, thematic\npatterns, and linguistic nuances. In mental health contexts, LLMs have shown promise in identifying signs of depression,\nanxiety, and suicidal ideation by analyzing user-generated text from social media [15]. They can further support mental\nhealth care through empathetic conversational agents and scalable diagnostic tools, particularly in under-resourced\nsettings [16]. An additional advantage of LLM-based diagnostics is interpretability. Unlike traditional machine learning\n(ML) models, which often function as black boxes, LLMs can provide justifications or reasoning for their outputs,\nmaking their predictions more transparent and trustworthy [17]. This interpretability is crucial in clinical and mental\nhealth applications, where understanding the basis of a diagnosis can enhance user confidence and clinician adoption.\nWhile studies on psychiatric evaluations using LLMs have been conducted on English social media posts [18, 19], to the\nbest of our knowledge, no equivalent studies have evaluated general or Arabic-focused LLMs in this domain. The closest\neffort is BiMediX [20], which aimed to bridge the psychiatric knowledge gap by developing a bilingual Arabic-English\nmodel. However, their evaluation was limited to specialized psychiatric questionnaires, such as MEDMCQA and\nPubMedQA, with no assessments conducted in a social media context. This lack of evaluations on Arabic social media\nposts highlights a significant gap, given the growing role of social media.\nDespite their potential, the development of LLMs for mental health diagnostics in Arabic faces several key challenges:\n\u2022 Lack of Annotated Arabic Datasets: The scarcity of labeled data for Arabic-language psychiatric tasks limits\nthe development and evaluation of reliable models.\n\u2022 Linguistic Complexity: Arabic's rich morphology, dialectal variation, and cultural context introduce significant\nchallenges for model training and performance [21].\n\u2022 Cross-Lingual Impact: Automatic translations from English to Arabic may introduce biases or inaccuracies,\nwhich can negatively affect diagnostic outcomes.\nThese challenges highlight the critical need for systematic evaluation of LLMs tailored to the Arabic context. Incorporating\nnative Arabic datasets and translations is essential to better understand language-related biases and ensure accurate\ndiagnostic performance.\nThis paper makes several key contributions to the field of AI-driven mental health diagnostics."}, {"title": null, "content": "1. It provides a comprehensive empirical assessment of multiple LLMs on Arabic mental health datasets, covering\nvarious psychiatric disorders and severity levels.\n2. It introduces a controlled experimental design to evaluate the impact of prompt engineering, including prompt\nstructure, style, and few-shot prompting techniques.\n3. It examines the effects of language and translation on diagnostic accuracy by comparing native Arabic datasets\nto translations of English corpora.\n4. It identifies challenges specific to Arabic-language psychiatric tasks, offering novel insights for improving\nculturally sensitive AI tools in the Arab world.\nThe remainder of this paper is organized as follows: Section 2 describes the data collection process, including the\nidentification of both native Arabic datasets and translated English datasets used for evaluation. Section 3 details the\nexperimental setup, outlining the task types, prompt templates, parsing techniques, sampling methods, and evaluation\nmetrics employed in the study. Section 4 presents the results, including an analysis of invalid responses, prompt\nperformance, model comparisons, and the effect of language on diagnostic accuracy. The performance of few-shot\nprompting is also evaluated in this section, providing a discussion of the key findings, highlighting dataset difficulty,\nmodel-specific performance, and the influence of language biases. Finally, Section 5 concludes the paper by summarizing\nthe key contributions and suggesting future directions for improving the deployment of LLMs in psychiatric diagnostics\nfor Arabic-speaking populations."}, {"title": "2 Data Collection", "content": "The first step to conduct this assessment was to collect relevant datasets. Collecting standard peer-published datasets\nallowed us to score the LLM's performance on labeled data, which enables us to compare and analyze the performance\nof the models quantitatively. Through the remainder of this paper, an all-cap abbreviation will be used for each dataset\nname to ensure consistency. This section details the descriptions of each dataset used, with further statistics provided in\nAppendix Section 6.1."}, {"title": "2.1 Native Arabic Datasets", "content": "The search for mental disorder related datasets began with a focus on native Arabic datasets. Due to the limited prior\nwork in this field, no restrictions were placed on dialect or collection source, enabling a more diverse and comprehensive\ndataset collection and evaluation.\nGiven the sensitivity of mental diagnosis tasks, preference was given to academically published and manually annotated\ndatasets. However, this was not always feasible, and datasets that did not fully meet these criteria were included to ensure\na comprehensive and generalizable evaluation. Dataset quality can later be assessed through the collective judgment of\nLLMs, which may help identify potential anomalies. The findings from the collected datasets are summarized in Table\n1."}, {"title": "2.1.1 Depression Corpus of Arabic Tweets (DCAT) [22]", "content": "The DCAT dataset consists of 10,000 tweets, labeled as either positive or negative for depression, with 5,000 samples in\neach class. The authors describe the dataset as the first manually annotated dataset for Arabic depression; however,\nthey did not provide information regarding the annotators' expertise or the criteria used for annotation. The available\ndescription of the dataset is minimal, as it is limited to a submission on the Harvard Dataverse platform without an\naccompanying research paper or detailed documentation."}, {"title": "2.1.2 Modern Standard Arabic Mood Changing and Depression Dataset (MCD) [23]", "content": "The original MCD dataset contains 1,229 samples. After removing duplicates (127) and missing values (7), the final\ndataset comprises 1,094 samples. This dataset is multi-class, containing 9 classes related to depression based on the\nPHQ-9 questionnaire. The class labels and their distribution are presented in Table 6.1. The dataset was initially created\nand labeled using an automated keyword-based approach for each class. However, the authors noted that the dataset\nwas later reviewed for validity. Details regarding the experience or qualifications of those who manually validated the\nsamples were not provided."}, {"title": "2.1.3 ARADEPSU [24]", "content": "The ARADEPSU dataset was collected from older Arabic sentiment analysis datasets spanning 2016 to 2022. Initially,\nthe samples were annotated by annotators whose qualifications were not specified. The authors then reviewed each\nsample, resolving conflicts by prioritizing their own revisions.\nThis dataset includes three classes: Depression, Depression with Suicidal Ideation, and Non-depression. For this study,\nonly the test split, consisting of 5,053 samples was used. Tables 1 and 6.1 provide an overview of the sample lengths\nand class distributions."}, {"title": "2.1.4 CAIRODEP [25]", "content": "The CAIRODEP dataset was compiled from seven different sources to ensure diversity. These sources included\ncrowdsourcing (a custom volunteer form), forums and online platforms, Twitter, translations of the English Reddit\nmental health dataset using Google Translate, and neutral posts extracted from existing datasets.\nThe dataset initially contained 7,000 samples, with 3,600 labeled as depression and 3,400 as normal. However, our\nanalysis identified and removed 209 duplicate entries to ensure data integrity for evaluation. Table 6.1 summarizes the\nsample length statistics."}, {"title": "2.1.5 Twitter-based Arabic Mental Illness (AMI) [26]", "content": "This dataset was published with two splits: train and test. To ensure comparability with future studies (that may train on\nthe train-split), only the test split was used, which contained 621 samples after cleaning.\nThe dataset was released with very limited description. The only available information was provided in the abstract\non the dataset download page, stating that an accompanying paper was yet to be published. It was mentioned that the\ndataset was manually annotated, but the experience of the annotators was not specified.\nThis dataset is unique as it includes two features: one for the disorder class, which can be Insomnia, Depression, Stress,\nAnxiety, or Bipolar Disorder, and another indicating the presence or absence of the disorder. Introduces a limitation in\nhandling posts with multiple co-occurring disorders. For example, a post may exhibit symptoms of both stress and\ndepression; however, the dataset format restricts each post to a single disorder label.\nThe absence of duplicate posts in the dataset indicates that no entry is associated with more than one disorder, reinforcing\nits structure as a collection of independent binary classification tasks. To process this dataset, it was split into five\nsubsets, one for each disorder, with each subset treated as a separate binary classification task."}, {"title": "2.2 Mental Disorder Egyptian Arabic Dialect (MDE) [27]", "content": "The dataset was manually collected from various social media platforms, including Reddit, Sanvello, Twitter, Happify,\nand Facebook, targeting posts from individuals experiencing mental disorders. The collected posts included both English\nand Egyptian Arabic content, which were translated and standardized into Egyptian Arabic to ensure consistency.\nThe dataset contains a total of 1,800 records, evenly distributed across three mental health conditions: 600 records for\ndepression, 600 for anxiety, and 600 for suicidal tendencies."}, {"title": "2.3 Translated English Datasets", "content": "Despite extensive search efforts and relaxed dataset restrictions, it is clear from Table 1 that most accessible Arabic\ndatasets are limited to depression disorder and are either binary or multi-class in nature, with no severity or multi-label\ndatasets available.\nThis limitation motivated us to translate six English datasets from our earlier English evaluation study [18]. The\ntranslation was performed using Google Translate, and each of these datasets will be discussed in the upcoming\nsubsections. Initially, Meta's NLLB [28] machine translation model was considered for this task. However, upon\nmanual inspection, the outputs were found to contain hallucinations and inaccuracies, rendering them unsuitable for the\nstudy's requirements. Consequently, higher-resource alternatives, including Google Translate, GPT-4, and DeepL, were\nexplored. Among these, Google Translate was selected due to its reliability and its ability to minimize the hallucinations\ncommonly observed with generative models. Table 2 summarizes the translated English datasets included in this study.\nClass distributions and sample length statistics for the translated versions of these datasets are provided in Appendix\nSection 6.1."}, {"title": "2.3.1 DREADDIT[33]", "content": "DREADDIT is a stress analysis dataset created using Reddit posts collected through the PRAW API. The dataset initially\nconsisted of 187,444 posts from 10 subreddits, grouped into five domains: interpersonal conflict (abuse, social), mental\nillness (anxiety, PTSD), and financial need (financial).\nThe posts were segmented into five-sentence units for annotation. Crowdworkers on Amazon Mechanical Turk labeled\neach unit as \"Stress,\u201d \u201cNot Stress,\u201d or \u201cCan't Tell.\" To ensure quality, segments with low annotator agreement or marked\nas \"Can't Tell\" were excluded. This filtering resulted in a final dataset of 3,553 labeled segments."}, {"title": "2.3.2 SDCNL [34]", "content": "SDCNL is a dataset developed to distinguish suicidal ideation from depression in social media posts, with the goal\nof improving the classification of these closely related mental health conditions. The dataset contains 1,895 posts\nsourced from two Reddit subreddits: r/SuicideWatch and r/Depression. This work addresses a significant research gap\nin differentiating between these two mental health states, which is crucial for tailoring appropriate interventions.\nThe dataset was not manually labeled. Instead, an unsupervised method was employed to refine the initial labels, which\nwere assigned based on the subreddit of origin. This approach enhanced the accuracy of the classifications without the\nneed for direct human supervision."}, {"title": "2.3.3 SAD [35]", "content": "The SAD dataset is designed to identify daily stressors in SMS-like conversations, with the goal of improving chatbot\ninteractions by classifying stressors in text. It contains 6,850 sentences labeled into nine categories, sourced from stress\nmanagement literature, chatbot conversations, crowdsourcing, and LiveJournal.\nThe categories are as follows: Work (1,043), School (1,043), Financial Problems (152), Emotional Turmoil (238), Social\nRelationships (99), Family Issues (101), Health Issues (103), Everyday Decisions (101), and Other (123). Sentences\nwere labeled and rated for stress severity on a 10-point scale by crowd workers. The dataset, which includes stress\nindicators and metadata, is publicly available on GitHub for research purposes."}, {"title": "2.3.4 DEPTWEET [36]", "content": "DEPTWEET is a dataset designed to detect depression severity levels in social media posts. It consists of 40,191\ntweets categorized into four severity levels: non-depressed, mild, moderate, and severe. Tweets were collected using\ndepression-related keywords and annotated by 111 trained crowdworkers following guidelines based on DSM-5 and\nPHQ-9 assessment tools.\nEach tweet was labeled by at least three annotators, with the final labels determined through majority voting. A\nconfidence score was assigned to indicate annotator agreement. The dataset also includes metadata such as tweet ID,\nreply count, retweet count, and like count, facilitating further analysis."}, {"title": "2.3.5 RED SAM [37, 38]", "content": "RED SAM is a dataset developed to detect depression severity levels in social media posts. It contains 16,632 Reddit\nposts categorized into three classes: \"Not Depressed\", \"Moderately Depressed\", and \"Severely Depressed.\" The dataset\naims to assess both the presence and severity of depression to support better treatment and intervention strategies.\nPosts were collected from mental health-focused subreddits, including \"r/MentalHealth\", \"r/depression\", \"r/loneliness\",\n\"r/stress\", and \"r/anxiety\". Annotation was conducted by two domain experts using guidelines that evaluated factors\nsuch as post length, emotional expression, and specific language patterns indicative of depression severity."}, {"title": "3 Experimental Setup", "content": "The experimental process began with identifying the disorders and task types present in the collected datasets. Based on\nthis analysis, suitable prompt templates, parsers, and evaluation metrics were designed. Sampling methods were applied\nwhere applied according to each dataset's type, and the selected models were tested accordingly. The following sections\noutline each step in detail."}, {"title": "3.1 Task Types", "content": "Binary classification: Datasets with this task type have a single label for each sample/entry. This label is either 0 or 1,\nsuch as \"depressed\" or \"normal.\" The purpose of binary tasks is to measure how well the model can distinguish between\ntwo opposing categories. For example, the model must decide whether a user is experiencing symptoms of depression\nor not. In mental health contexts, binary classification tasks are essential because they enable the early detection of\nconditions like depression by flagging users who fall into a specific category of interest, such as \"depressed.\"\nSeverity classification: Datasets with this task type have a single label that is a discrete numeric value representing\nthe severity of a specific disorder. The range of values is dataset-dependent, and these numerical values are typically\nassociated with a textual representation. For example, \"Mild Depression\" might be mapped to 1. This task can be\nthought of as a classification problem within a single disorder, where the goal is to classify how severe an individual's\ndisorder is. The purpose of severity tasks is to measure how well the model can assess the intensity or progression of\na particular mental health issue, like distinguishing between mild and severe cases of depression, allowing for more\nnuanced treatment and intervention.\nMulticlass classification: In multi-class tasks, one label can take on multiple potential classes. A challenging example\nwould be datasets such as MDE, where classes include \"Suicidality,\" \"Depression,\" and \"Anxiety,\" or SDCNL, where\nlabels are \"Depression\" and \"Suicide.\" This task type is tricky because the model must distinguish between disorders\nthat often have overlapping symptoms. The model must learn to identify subtle differences between conditions like\nanxiety and depression, which is critical for delivering accurate mental health diagnostics.\nMCQ (Multiple Choice Questions): The only instance of this task type in our dataset is MedMCQA. While it has\na single label, this label takes on different values depending on the sample. This is not equivalent to a traditional\nmulti-class problem, as each sample represents a question with a different set of possible answers. The purpose of\nthis task is to test the model's ability to understand and answer medical questions in a multiple-choice format, which\nrequires a deeper understanding of the context and underlying medical knowledge."}, {"title": "3.2 Prompt Templates", "content": "For each of the aforementioned tasks, exactly two prompt templates were designed. These templates are generalized,\nmeaning they are not tailored to a specific disorder but follow a rigorous structure with formattable variables for the\ndisorder name that seamlessly integrate into the sentence. For example, in a prompt like \"Is this person showing symptoms\nof X?\", X could be {Depression, Suicidal ideation, Stress, etc.}. This approach ensures that the instruction remains\nconsistent across all disorders and datasets, guaranteeing fair comparisons while also simplifying the experimental setup.\nAdditionally, the templates are generalized across tasks, meaning all tasks share the same base structure with slight\nmodifications to adjust the output formatting depending on the task requirements. For instance, binary classification\nprompts expect a \"yes\" or \"no\" response, multi-class prompts require selecting from a provided list, and severity prompts\nrequest a severity rating within a specified range. To introduce few-shot prompting, a section containing sample-input\nand ground-truth pairs is simply appended to the template before the original post that the model is expected to diagnose.\nThis straightforward modification ensures flexibility while retaining consistency across all tasks.\nBased on earlier English evaluation work [18], it was observed that even subtle modifications to a prompt-such as\nchanging or repeating words-could unexpectedly lead to significant performance variations. Consequently, it was\nconcluded that over-engineering prompts is counterproductive, as minor adjustments may yield better results without\nclear justification. This phenomenon, often referred to as a \"random search problem,\" is particularly relevant because\ndifferent models can respond unpredictably to such minor prompt changes. Therefore, for each task, two reasonable and\nwell-structured prompts were designed to cover the necessary instructions, including context, the question, constraints,\nand guidelines.\nFor each task, two prompt templates were systematically designed to ensure fair comparisons and to analyze the models'\nsensitivity to prompt variations. While both prompts maintain semantically identical content and instructions, they\ndiffer in structure and contextual framing. Prompt style 1 is less structured and explicitly instructs the model to \"act as a\npsychologist\" when generating its response. In contrast, prompt style 2 follows a more structured format but omits the\n\"act as a psychologist\" clue. Later on in the paper prompt 1 and prompt 2 (in their zero-shot form) are referred to as ZS-1\nand ZS-2. The few-shot experiment section 4.5 details the changes made to convert from zero-shot to few-shot setting.\nThis design choice serves two purposes. First, it allows for an evaluation of the models' responses to changes in prompt\nstructure, as even subtle modifications can lead to significant performance variations. Second, it enables an analysis of\nthe models' behavior when the contextual framing provided by the psychologist role is removed. By comparing the\ntwo styles, this setup reveals potential style preferences and shows how the absence of role-based context impacts the\nmodel's output."}, {"title": null, "content": "Since the binary classification prompt templates used in the earlier English evaluation satisfied the criteria, two of these\ntemplates were selected and translated into Arabic using GPT-4o, with subsequent manual corrections. The final set of\nprompts is provided in the Appendix (Section 6.5)."}, {"title": "3.3 Parsing", "content": "One critical step that plays a significant role in the computed metric score is how the models' responses were parsed.\nParsing the output incorrectly can lead to dramatic changes in evaluation outcomes. To simplify the parsing process,\na constraining clue was introduced in the instruction prompt, explicitly asking the model to return a single word.\nThis approach aims to ensure that parsing remains straightforward. Ideally, consistent adherence to this constraint\nwould guarantee successful parsing by limiting the response to only the diagnosis. However, as further analyzed in the\ndiscussion, this is not always the case. Certain models occasionally disregard the instruction, producing more complex\nresponses that complicate parsing.\nIn such cases, there is no guarantee that the parsed diagnosis accurately reflects the model's intended diagnosis.\nTherefore, our parsing methodology attempts to minimize such errors. The paragraphs below describe the parsing\nprocess for different task types:"}, {"title": null, "content": "Binary Classification Parser: The logic for the binary parser is straightforward. If the word \"yes\" (in Arabic) is found\nin the model's response, the label is assigned as \"yes.\" If the word \"no\" (in Arabic) is found, the label is assigned as\n\"no.\" However, if both words are found or neither word is detected, the response is labeled as invalid.\nSeverity Parser: For the severity task, regular expressions (regex) were used to search for a pattern that contains a\nnumerical value. If a valid number is found within the specified range for the dataset, it is assigned as the severity label.\nIf the number is out of range, a floating point number, or if no match is found, the response is labeled as invalid.\nMulticlass Parser: In multi-class classification tasks, the response was checked for any of the disorders listed in the\ninstruction prompt. If no match is found, the response is labeled as invalid.\nKnowledge Parser: For the knowledge-based multiple-choice questions, the search focused on identifying Arabic\nalphabetical choice letters equivalent to \"a, b, c, d.\" These letters were required to remain standalone and not form part\nof a word. Formats such as (a), (A), or A) were considered valid for this purpose.\nBy applying these parsing methods, the goal was to minimize errors and ensure that the model's output adhered closely\nto the intended format. Nevertheless, as discussed later, parsing remains a potential source of error, particularly when\nmodels fail to strictly follow the prompt structure."}, {"title": "3.4 Sampling", "content": "Since all testing was conducted via APIs, evaluation costs had the potential to escalate significantly, particularly when\nrunning multiple models across large datasets. To mitigate these costs, two key cost-saving strategies were employed.\nFirst, only the test splits of datasets that were already pre-split were utilized. Second, for datasets exceeding 1,000\nexamples, fair random sampling was implemented to ensure balanced representation of all classes within the dataset.\nThis approach allowed for the evaluation of up to 1,000 instances per dataset while preserving fairness across class\ndistributions. The sample size was chosen based on Hoeffding's inequality (Equation 1), which provides an upper bound\nfor the error difference between a sample and the full dataset, ensuring that the sample is sufficiently representative.\nFor datasets with severity metrics, such as the DEPSEVERITY and SAD datasets, the labels were binarized by assigning\na \"False\" label (0) to posts or users with the minimum severity score and a \"True\" label (1) to those with any score above\nthe minimum. This approach simplified the classification task while retaining meaningful distinctions between varying\nseverity levels."}, {"title": null, "content": "P (|\u03bd-\u03bc| > \u03b5) \u2264 2e^{-2\u03b5^2N}\n(1)\nwhere v is the empirical mean (sample average), u is the true mean (population average), \u03b5 represents the deviation\nfrom the mean, and N is the number of samples.\nBy substituting N = 1000 and \u025b = 0.05 into Equation 1, which represents a maximum discrepancy of 5%, the probability\nof the in-sample and out-of-sample error deviating by more than 5% is calculated to be 0.0135. This result implies a\ngreater than 98.5% probability that the error will remain within a 5% margin of the true error, ensuring confidence in\nthe fairness and representativeness of the sample."}, {"title": "3.5 Evaluation Metrics", "content": "This study evaluated the performance of LLMs using accuracy-based metrics, with Balanced Accuracy (BA) serving\nas the primary measure across tasks. BA was chosen because it provides a fair evaluation of model performance,\nparticularly in the presence of class imbalances, by calculating the average recall for each class. This makes it a more\nreliable metric compared to traditional accuracy when dealing with uneven data distributions.\nIn many datasets, fair random sampling was applied to ensure balanced class distributions, allowing for a straightforward\nand equitable comparison of performance across all classes. However, in cases where fair random sampling was not\nfeasible-such as with datasets like SAD, which exhibited sparse class distributions\u2014random sampling was employed\ninstead. Despite this, BA was still used as the evaluation metric because it inherently accounts for class representation\nand adjusts for imbalances naturally, even when the data is not explicitly balanced during sampling.\nBy using BA, the study ensures consistent and equitable evaluation of models across diverse datasets and task types."}, {"title": null, "content": "Balanced Accuracy (BA) Balanced Accuracy is calculated as the average recall across all classes:\nBA = \\frac{1}{C} \\sum_{c=1}^C \\frac{TP_c}{TP_c + FN_c}"}, {"title": null, "content": "where C is the number of classes, $TP_c$ is the number of true positives for class c, and $FN_c$ is the number of false\nnegatives for class c. This formula ensures that the metric is not biased toward classes with more samples, providing a\nfair assessment of the model's performance across the entire dataset.\nMean Absolute Error (MAE) For the disorder severity task, two metrics were used: BA and Mean Absolute Error\n(MAE). While BA measures how often the model correctly predicts the exact severity level, MAE focuses on the average\nsize of the prediction errors. To ensure consistency across datasets with different severity scales, a normalized version\nof MAE was used:\nMAEnorm = \\frac{1}{N} \\sum_{i=1}^N \\frac{|\\hat{y_i} - y_i|}{L-1}\nwhere N is the number of samples, \u0177; is the predicted severity level for sample i, yi is the actual severity level, and\nL is the total number of severity levels in the dataset. Normalizing by L ensures the metric remains comparable\nacross datasets with different scales. When referring to MAE in the remainder of this paper, it specifically denotes the\nnormalized version calculated using the equation stated above.\nTogether, BA and MAE offer a complementary view of model performance. While BA highlights how often the model\nachieves exact predictions, MAE provides insights into how far off the predictions are when they are not exact. This\ncombination of metrics allows for a more complete evaluation of LLM capabilities."}, {"title": "3.6 Models", "content": "We selected eight models for this evaluation: Gemma 2 9B, GPT-40 Mini, Llama 3 70B, Mistral NeMo, Phi-3.5 Mini,\nPhi-3.5 MoE, Aya 32B, and Jais 13B. The models were chosen based on their strong general performance observed in\nour previous English evaluation study and their computational feasibility for further fine-tuning. Aya 32B and Jais 13B\nwere included after a preliminary evaluation of Arabic-focused models, which is detailed in Appendix 6.2.\nA summary of these models, including their parameters, sources, and API providers, is presented in Table 3.\nThe primary reason for selecting relatively smaller models is the feasibility of fine-tuning them. While larger models\noften yield better out-of-the-box performance, this study aims to thoroughly evaluate these models, identify the\nbest-performing one. A later study will then proceed with fine-tuning a selected model based on performance to further\nimprove it in Arabic mental health-related tasks."}, {"title": "4 Results & Discussion", "content": "This section presents the results and findings of the evaluation. The presentation begins with an analysis of invalid\nresponses, followed by a detailed examination of prompt performance and a general analysis (Sections 4.1, 4.2, and 4.3).\nThese sections provide comprehensive results, serving both as a summary and as a foundation for subsequent, more\nfocused analyses.\nLater sections offer additional insights into the impact of language and translation on model performance. Specifically,\nSection 4.4.1 evaluates the effect of translating English datasets into Arabic, comparing the results when models are\nevaluated using prompts in languages that match the dataset language 4.4.3 explores the reverse process, where native\nArabic datasets are translated"}]}