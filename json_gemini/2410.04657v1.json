{"title": "Contrastive Learning to Improve Retrieval for Real-world Fact Checking", "authors": ["Aniruddh Sriram", "Fangyuan Xu", "Eunsol Choi", "Greg Durrett"], "abstract": "Recent work on fact-checking addresses a realistic setting where models incorporate evidence retrieved from the web to decide the veracity of claims. A bottleneck in this pipeline is in retrieving relevant evidence: traditional methods may surface documents directly related to a claim, but fact-checking complex claims requires more inferences. For instance, a document about how a vaccine was developed is relevant to addressing claims about what it might contain, even if it does not address them directly. We present Contrastive Fact-Checking Reranker (CFR), an improved retriever for this setting. By leveraging the AVeriTeC dataset, which annotates subquestions for claims with human written answers from evidence documents, we fine-tune Contriever with a contrastive objective based on multiple training signals, including distillation from GPT-4, evaluating subquestion answers, and gold labels in the dataset. We evaluate our model on both retrieval and end-to-end veracity judgments about claims. On the AVeriTeC dataset, we find a 6% improvement in veracity classification accuracy. We also show our gains can be transferred to FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to make inferences.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) systems are now widely used across NLP applications including question answering (Guu et al., 2020; Lewis et al., 2020; Karpukhin et al., 2020) and text generation (Komeili et al., 2022; Gao et al., 2023b), but one particular application of interest is fact-checking. While older fact-checking systems would often not consider evidence at all (Alhindi et al., 2018) or consider oracle evidence (Atanasova et al., 2020), the real fact-checking task involves finding evidence to support or refute complex claims in the wild (Chen et al., 2022; Schlichtkrull et al., 2023; Chen et al., 2024). As with many other RAG settings, retrieval is a bottleneck (Singh et al., 2022): it is impossible to provide the right judgment without retrieving the right evidence.\nIn this work, we investigate how to build an effective retriever for fact-checking. Figure 1 shows an example of why this is particularly challenging: unlike a factoid question with a definite answer spelled out in text, documents retrieved for fact-checking may only obliquely address a claim, or may present information in a different context (e.g., statistics that apply to a different country than the one where the claim was made). The unstructured nature of documents in the wild combined with claims that are only subtly true or false make retrieval a very difficult task.\nWe focus on two-step retrieval pipeline used in past work (Lazaridou et al., 2022; Chen et al., 2024). These use a first-stage web search (i.e., using Google or Bing) to build a set of approximately"}, {"title": "2 Background and Related Work", "content": "Retrieval Augmented Generation Systems\nRetrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). Sauchuk et al. (2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \u201cbridging\u201d model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning."}, {"title": "2.2 Limitations of Existing Retrieval Systems", "content": "For NLP tasks like question answering, sparse retrieval techniques like BM25 have been supplanted by dense retrievers like DPR (Karpukhin et al., 2020) and Contriever (Izacard et al., 2022). These dual encoder approaches support efficient retrieval, and contrastive training is an effective way to learn embeddings for QA tasks. More recently, research has explored distilling knowledge from reader models to create smarter retrievers (Izacard and Grave, 2022). We draw from this work to build a retrieval system with better reasoning capabilities than baseline dense retrievers, which are usually pretrained on simpler (query, document) pairs (i.e. the MS-MARCO dataset). These retrieval systems have proven effective for fact-checking settings such as FEVER (Thorne et al., 2018) and MultiFC (Augenstein et al., 2019). However, the claims are largely short and factoid, and most of them contain no more than two entities. The realistic setting is embodied by approaches like QABriefs (Fan et al., 2020), ClaimDecomp (Chen et al., 2022, 2024), and AVeriTeC (Schlichtkrull et al., 2023), which are ultimately different from what dense retrievers were developed and optimized for."}, {"title": "2.3 Motivating Example: AVeriTeC", "content": "Figure 1 shows an example of fact-checking in the AVeriTeC dataset: \"how was REGN-COV2 developed?\". This example differs in key ways from frequently-studied question answering settings such as such as Natural Questions (Kwiatkowski et al., 2019). First, it supports several different short answers but very likely has a best answer in the context of the claim: did the development involve human fetal tissue? In this case, the bolded paragraph indicates no: it used mice. The answer to this question should address the claim and provide background information: there is both a \"short answer\" as well as a \u201clong answer\u201d (Kwiatkowski et al., 2019; Gao et al., 2023a).\nRetrieval signals in fact-checking Contrastive methods like Contriever require examples marked"}, {"title": "3 Methodology", "content": "We consider a setting following work in AVeriTeC and ClaimDecomp (Chen et al., 2022). We assume we are given a collection of claims (C1, ..., CN). For claim ci, we define qij as the jth subquestion for the ith claim in the dataset and aij define its answer. We also assume access to a document set D(Ci, qij) for each subquestion, created by querying Bing with ci appended to qij and scrape the top-k articles to form a document corpus. Each document d is a 200 token span gathered from the scraped articles. The title of the document is prepended to the start of each document. The dataset also comes with a gold article which contains the gold answer. Like the Bing-retrieved documents, it is chunked into 200 token span documents {di} and added to D(Ci, qij). We refer to documents belonging to these articles as gold.\nGiven a query y = [Ci; qij] and a document di \u2208 D, we want to generate embeddings in $R^e$ using an encoder network (e.g. Contriever). Let hy, hdi denote the representations of y and di. Then we define our scoring function $f : R^e \u00d7 R^e \u2192 R$ such that f(hy, hd\u2081) > f(hy,hd\u2081) if document di contains more information helpful to answering the query than document dj. Let r(y) = arg maxd\u2208D f (hy, ha) which is a function that chooses the highest ranked document in our document set D. The goal is to optimize our encoder via f to rank documents for answering questions in-context with the claim above topically relevant documents that do not ultimately contain information for an answer. We choose to optimize this for downstream veracity classification accuracy. We also track more upstream metrics such as using a relevance score for the top document or measuring how close its extracted answer matches the gold answer."}, {"title": "3.1 Components", "content": "Dense retriever r We use Contriever as the base for our second stage dense retriever. Contriever uses the BERT base uncased architecture (Devlin et al., 2019). To fine-tune it with contrastive learning, we require document sets T(Ci, qij, D) = {D+, D\u2212} of positive and negative documents; during optimization, the positive documents will be embedded closer to the query vector than negative documents. Contrastive training relies critically on having hard negatives to serve as \u201cdistractors\" (Robinson et al., 2021). These might be documents ranked high by baseline retrievers or having high token overlap with the query. We define SBM25 (Ci, qij) = {d1,d2,...,dk} as the top k documents surfaced by BM25 given [Ci; qij] as the query. We also define GBM25 (Ci, qij) = {d1,d2,...,dl} as the top l gold annotated documents. In our models, we set k = 10 and l = 5.\nReader model We use GPT-4 as the reader model. The answers are derived by prompting GPT-4 with the claim ci, question qij, and a document"}, {"title": "3.2 Learning", "content": "We train r on these (Ci, qij) \u00d7 T pairs to produce a finetuned retriever r\u2217. Specifically, given a query y = [Ci; qij] and positive document d+ \u2208 D+,\nL(y, d+) = exp (f(hy, ha+))\nexp (f(hy, ha+) + \u2211d\u2212\u2208D\u2212exp (f(hy, ha\u2212))\nwhere \u03c4 is a temperature parameter. In our setting, we define f as cosine similarity hThy\u22c5hda between the embeddings. This encourages positive documents to have high similarity with the query while penalizing high scores for negative documents. Fine-tuning yields r\u2217 such that r\u2217(y) contains a better answer to qij in context with Ci than r(y).\nImplementation Details On average, each question qij comes with about 500 documents to rank. Each document contains 200 token span, scraped from articles with a 100 token length stride. Details about training and model architecture can be found in Appendix A.1."}, {"title": "3.3 Generating Contrastive Training Data", "content": "We generate {D+, D\u2212} in three main ways: the annotated AVeriTeC gold evidence, distilled relevance judgements from a GPT-4 reader module, and evaluating equivalence of the document-predicted answer with a gold answer. Figure 2 shows the three approaches which we describe next.\nAVeriTeC Gold Evidence The most straightforward approach to building positive examples is to use the human-annotated evidence paragraphs available in AVeriTeC. The gold articles (one per subquestion) were selected by human annotators in a two-stage annotation process, we refer the readers to their paper for details (Schlichtkrull et al., 2023). The annotators also provided answers for the subquestions, which consist of both extractive and abstractive answers. For each qij, this article is chunked into a set of documents {di} as described in Section 3. Negative examples are all d \u2208 SBM25(Ci, qij) such that d is not from a gold-annotated document. We denote the fine-tuning data derived from this method as {D+, D\u2212}g.\nDistilling GPT-4 The AVeriTeC gold evidence may have recall errors: there may be relevant docu-"}, {"title": "4 Experimental Setup", "content": "We evaluate Contriever fine-tuned on the supervision signals outlined in Section 3. The datasets selected for evaluation, namely AVeriTeC (Schlichtkrull et al., 2023), ClaimDecomp (Chen et al., 2022), FEVER (Thorne et al., 2018), and HotpotQA (Yang et al., 2018), encompass a wide range of scenarios for document retrieval. For evaluation, a random subset of 200 answerable examples (subquestions contain an answer) were selected from each of these not overlapping with the training sets."}, {"title": "4.1 Metrics", "content": "We use metrics that evaluate both the retrieved documents and downstream products of these documents, such as the produced answer.\n\u2022 LERC computes the average LERC score between the AVeriTeC (or ClaimDecomp) gold"}, {"title": "4.2 Datasets", "content": "AVeriTeC consists of real claims (ci) from the web annotated with subquestions (qij), gold answers (aij) to the subquestions, and the gold evidence document for the answer. We query Bing in FSR with the claim and subquestion [Ci; qij] to generate D. The generated answers (aij) are verified against the gold answers using LERC.\nClaimDecomp consists of complex political claims (ci) with yes/no subquestion decompositions (qij) generated by trained annotators. We query Bing in FSR with the claim and subquestion [Ci; qij] to generate D. The annotated subquestions tackle both explicit and implicit parts of the original claim. The implicit questions are much harder to answer without sufficient context, which makes this an interesting dataset for retrieval evaluation. The human labeled answers are yes/no, and we evaluate our generated answers (aij) against the gold answers using LERC. Because the questions themselves are yes/no in nature, this approach returns the same results as simple binary comparison."}, {"title": "5 Results", "content": "5.1 AVeriTeC\nThe results for AVeriTeC are shown in Table 2. We find that distill performs the best in most metrics but for veracity. The 6% gain in top doc relevance reflect our retriever's ability to correctly identify more relevant documents in our evaluation set.\nAs expected, we find that using ROUGE as a long answer overlap metric to generate {D+, D\u2212} works poorly as seen by the ROUGE-F1 baseline."}, {"title": "5.2 Out-of-domain results", "content": "Results on out-of-domain datasets are in Table 3.\nClaimDecomp We find that our gains translate to ClaimDecomp, with distill (gold) demonstrating significant improvements in both LERC and top doc relevance. Examples in this dataset contains both explicit and implicit subquestions, while AVeriTeC subquestions are mostly explicit. Since we use subquestions for retrieval, improvement in top doc relevance may reflect an ability to surface better documents for ambiguous implicit subquestions, which is something baseline retrievers struggle with. An example of this is seen in Appendix D, where our finetuned retriever model is able to accurately capture the focus on lack of funding presented in the question. Even though baseline Contriever selects a document detailing the Amtrak incident with high lexical overlap with the claim and query, the document itself is not useful for answering the question. Using CFR, we see a 2% increase in downstream veracity classification performance.\nFEVER We also find that our system gives gains on FEVER compared to BM25, Contriever, and Contriever MSM. Our retriever selects relevant top documents more often and yields improved downstream veracity performance.\nHotpotQA For HotpotQA, we find that distill (gold) + LERC performs the best across LERC and top doc relevance. We notice the strongest gains come from including LERC-based supervision, which indicates our retriever may learn to identify answer documents that contain little overlap with the claim. This is especially useful in"}, {"title": "6 Retriever Reasoning Capabilities", "content": "Our hypothesis about our contrastive training was that it would impart a greater ability for our retriever to \"reason\" about content rather than directly locating an answer. We conduct an additional study of whether our retriever can exhibit basic 1-hop reasoning capabilities via a synthetic data experiment. We construct positive and negative documents where the positive documents do not directly state the answer, similar to what we found in several AVeriTeC examples."}, {"title": "6.1 Synthetic Data Generation", "content": "We build these examples by few-shot prompting GPT-4 with synthetic documents written by humans. Our data generation approach takes as input a claim/question pair (Ci, qij) from AVeriTeC and produces a document set {d+, d\u2212, d1, d2, d3, d4}.\nWe generate data for (Ci, qij) pairs from the validation set described in Section 4. The positive document d+ is the only document that contains an answer to the question. Document d\u2212 is a \"hard negative\" document, which is a document that appears highly relevant to the query [Ci; qij] but does not contain an answer. The 4 other documents d1,...,d4 are additional negative documents built from alternate subquestions about the claim.\nThe positive document is a paragraph that supports an answer to the question, but only indirectly. When prompting (Appendix E.2), we require that a clear reasoning hop must be made to recover an answer from the positive document. Therefore, a retrieval system that simply looks for querydocument token overlap may not be able to find such documents because the answer is usually not presented in terms of the question.\nThe hard negative document is a paragraph that looks highly relevant to the claim/question, but doesn't actually support an answer. In the prompt, we specify that the document should appear relevant but not support an answer, and further enforce this with few-shot examples (see Appendix E.2). In Appendix F.2, the hard negative document correctly discusses the federal judges Trump nominated. However, it does not contain any information about how many judges he nominated, deeming it useless for answering the question about the claim."}, {"title": "6.2 Results", "content": "We evaluate our retrievers on their ability to score the positive document closer to the query than the negative distractor documents. We measure this via MRR of the positive document across ranking the six documents (positive, hard negative, and 4 alternate question negatives). The results are displayed in Table 4. We find a statistically significant gain in our finetuned model's ability to surface the positive document over other distractor documents. CFR achieves an MRR of 0.79 compared to baseline Contriever (0.68). This supports our hypothesis that finetuning on our supervision signals improves the ability of the retrieval model to find information only indirectly related to the claim."}, {"title": "7 Conclusion", "content": "This work presents an improved retrieval system, CFR, for fact-checking complex claims. We present two supervision signals for finetuning retrievers under a contrastive objective, and their integration results in improved downstream veracity classification. Furthermore, CFR is able to improve retrieval in settings where inferences are required to identify the correct documents. The gains found in this paper encourage explorations into improving retrieval for fact-checking, as surfacing relevant information proved to be a hard task even for SOTA dense retrievers."}, {"title": "Limitations", "content": "There are a few limitations of our current approach. First, using LERC as an answer equivalence metric requires us to shorten both the gold and candidate answer. The answer compression step loses information that may play a role in verifying hard examples. Therefore, developing a good long answer equivalence metric can help build an even better retrieval system for fact-checking. Such equivalence metrics can also be useful for evaluation: the long-form explanation of why a claim is true or false may be more important than the veracity judgment itself, but this is difficult to assess in an automated way.\nSecond, this work focuses on the second-stage retrieval step. Building optimized queries for first stage retrieval may yield a better document corpus for second stage, especially for hard examples where little information has been published. However, indexing the necessary documents for the broad set of claims we use involves web-scale indexing, which is beyond the scope of this project.\nFinally, this work considered English-language political claims. We note that claims in multimedia (e.g., in memes or videos), claims in other languages, and claims in specialized domains such as COVID-19 misinformation may present distinct challenges. However, we believe that our framework is flexible enough for future work to be able to build on it and train retrievers for these settings as well."}, {"title": "Ethical Considerations and Risks", "content": "This paper presents a retrieval method that seeks to advance the state of the art in automated fact-checking. However, despite recent progress in this area and systems that combine retrieval systems like ours with LLMs (Schlichtkrull et al., 2023; Chen et al., 2024), we stress that these system are not yet ready for deployment. We believe these systems have use to aid professional fact-checkers in their work, since enabling them to quickly find information can aid them to more rapidly check claims. However, these systems cannot produce reliable fact-checks without a human in the loop, as demonstrated by the veracity numbers in this work. Moreover, there is not necessarily a single objective truth about every claim, and a judgment may depend on the reliability of primary sources and other factors which are beyond the scope of this work."}, {"title": "A Implementation Details", "content": "A.1\nComputational Details\nThe finetuned models were BERT base uncased (110M parameters). Hyperparameter optimization was done via grid search on the learning rate and batch size. For learning rate, we searched {le\n5,2 - 5,4 - e5}. For batch size, we searched\n4, 8, 16, 32, 64}.\n\u2022 Infrastructure: 2 NVIDIA Quadro RTX 8000\n\u2022 GPU Hours (training): approx. 3 hours\n\u2022 GPU Hours (eval): approx. 1 hour\n\u2022 Epochs: 12\n\u2022 Best Learning Rate: 2e-5\n\u2022 Best Batch Size: 32\nA.2 Experimental Setup\nBesides chunking into 200 token spans, document text is not further preprocessed. During training, data was mapped into tuples of the form containing one positive and negative (ci, qij, d\u207a, d\u207b). That is, if a claim/question pair contains 2 positive and 3 negative paragraphs, it becomes 2.3 = 6 separate data points. These were then shuffled and batched to be fed to the retriever. In contrastive training we use in-batch negatives.\nA.3 Parameters for Packages\n\u2022 Used rouge-score (v0.1.2) to compute ROUGE-F1 scores. Used rougeL (longest common subsequence) with stemming set to True.\n\u2022 Used openai (v1.34.0) for GPT-4 chat completion. Set temperature setting to 0.2.\nA.4 Scientific Artifacts\n\u2022 AVeriTeC [License] Free to copy, redistribute, and build upon this material given citations and a link to the license. AVeriTeC contains English-language real-world claims mainly in politics gathered from 50 different fact-checking organizations.\n\u2022 FEVER [License] Data annotations incorporate material from Wikipedia, which is licensed pursuant to the Wikipedia Copyright Policy"}, {"title": "B ROUGE-based Methods", "content": "B.1 ROUGE-based Answer Matching\nROUGE overlap between long answers works is a poor supervision signal because answer strings are typically quite complex. Table 5 illustrates this: although both long answers are conveying the same fact that Nigeria experienced 29 years of military rule, extra details or differences in phrasing can lead to low ROUGE scores despite the answers being semantically equivalent. The opposite may also occur: long answers which contain high lexical overlap may be topically similar but completely different in their key points, creating a false positive example. We also investigated semantic similarity measures like BERT score to assess answer equivalence. Compared to short answer LERC, BERT score tended to work poorly for complex long answers as seen in AVeriTeC. By contrast, using a short answer extraction yields a perfect signal in this case.\nB.2 ROUGE-based Token Overlap\nSee Table 6. The token overlap between the retriever query (claim+question) and the AVeriTeC annotated gold document is only 0.11, whereas with the top retrieved document it is 0.25. This means using tokens in the query to surface the gold document is not easy.\nC LERC Experiments\nC.1 LERC Quality Check\nWe evaluate the selection of {D+, D\u2212} by manually annotating 10 examples. The task was to select the positive context document given a shuffled, unlabeled {D+, D\u2212}. We selected the positive document correctly in 60% of examples. Note the positive document here is the one with the highest LERC score (i.e., contains an answer which most closely matches the gold answer). However, the two human annotators agreed on 90% of examples. By investigating the failure cases, we found that LERC-based metrics are sensitive to selecting false negative documents, as human agreement indicated a negative document was more \u201crelevant\u201d to the claim/question than the labeled positive document 40% of the time. Oftentimes, the misclassified document contained a reasonable answer to the question but mismatched the gold answer (hence explaining the low LERC score). This revealed that while LERC can identify strong positive docu-"}, {"title": "E2 Synthetic Data Generation Prompt", "content": "You will be provided with a claim and a question about the claim. Your job is to generate two evidence paragraphs:\n(1) Positive: A paragraph that supports an indirect answer to the claim. It requires a reasoning hop to arrive at the answer. You can make up the answer to the question, but it should only come with a reasoning step.\n(2) Hard Negative: A paragraph that looks highly relevant to the claim/question, but doesn't actually support an answer Neither paragraph can use claim or question they must stand alone and mimic the style of real evidence documents found on the web."}]}