{"title": "An Empirical Exploration of ChatGPT's Ability to Support Problem Formulation Tasks for Mission Engineering and a Documentation of its Performance Variability", "authors": ["Max Ofsa", "Taylan G. Topcu"], "abstract": "Systems engineering (SE) is evolving with the availability of generative artificial intelligence (Al) and the demand for a systems-of-systems perspective, formalized under the purview of mission engineering (ME) in the US Department of Defense. Formulating ME problems is challenging because they are open-ended exercises that involve translation of ill-defined problems into well-defined ones that are amenable for engineering development. It remains to be seen to which extent Al could assist problem formulation objectives. To that end, this paper explores the quality and consistency of multi-purpose Large Language Models (LLM) in supporting ME problem formulation tasks, specifically focusing on stakeholder identification. We identify a relevant reference problem, a NASA space mission design challenge, and document ChatGPT-3.5's ability to perform stakeholder identification tasks. We execute multiple parallel attempts and qualitatively evaluate LLM outputs, focusing on both their quality and variability. Our findings portray a nuanced picture. We find that the LLM performs well in identifying human-focused stakeholders but poorly in recognizing external systems and environmental factors, despite explicit efforts to account for these. Additionally, LLMs struggle with preserving the desired level of abstraction and exhibit a tendency to produce solution specific outputs that are inappropriate for problem formulation. More importantly, we document great variability among parallel threads, highlighting that LLM outputs should be used with caution, ideally by adopting a stochastic view of their abilities. Overall, our findings suggest that, while ChatGPT could reduce some expert workload, its lack of consistency and domain understanding may limit its reliability for problem formulation tasks.", "sections": [{"title": "1. Introduction", "content": "Increasing demand towards a more interconnected world pushes the Systems Engineering (SE) community [1,2,3] towards a Systems-of-Systems (SoS) perspective [4]. A recent policy change in the US Department of Defense (DoD) formalizes these trends around ME [5], where the goal is to architect a heterogenous portfolio of SoS with independent constituents that may dynamically evolve in terms of their roles and integration level depending on the nature of the mission [6]. This view considerably increases the complexity of the problem space due to its combinatorial nature [7, 8]. However, as documented by numerous US Government Accountability Office Reports, SE community has notoriously struggled with dealing with local problems that are constrained within the boundary of a standalone system [9, 11]. Methods and tools for dealing with this increasing ME complexity, albeit a few exceptions [12, 13], is nascent."}, {"title": "2. Literature Review", "content": "Historically, systems were acquisitioned as standalone entities (e.g., a tank, a missile, a communication satellite) and were later integrated in teams to execute strategic missions (e.g., an integrated task force). In late 2010s, the US DoD published the Mission Engineering Guide [43], to outline an SoS engineering approach that prioritizes mission success in an attempt to mitigate the lack of mission understanding that leads to interoperability and integration issues, and uncertainties in fielded capabilities. Thus, ME, at its core, is a form of SoS engineering [43]. The primary difference between ME and standard SoS practices is inclusion of operational context at all levels of development. ME is more specifically defined as: \u201cPlanning, analysing, organizing, and integrating current and emerging operational concepts for the purpose of evolving the end-to-end operational architecture and capability attributes, across the Doctrine, Organization, Training, Materiel, Leadership, Personnel, and Facilities (DOTMLPF) spectrum, including anticipated Blue Force (BLUFOR) and Opposition Force (OPFOR) behaviours, that are needed to inform the communities of interest involved in fulfilling mission needs statements [44]."}, {"title": "2.2. Translating III-defined Problems into Well-defined Ones, Its Importance for Mission Engineering", "content": "Every technical or programmatic solution starts with formulating it as a problem, and this applies to both SE and ME [43, 48]. Specifically, creating a well-defined problem [24, 27] involves identifying the boundary of the system of interest, along with its associated stakeholders, and their needs, preferences, and constraints [48]. Without rigorous execution of these steps, resulting system requirements suffer from either unproper bounding, underdevelopment, or overdesign, which leads to a solution that would be unable to meet mission needs [49, 50]. Particularly in the case of SoS, improper early stage bounding render downstream decomposition decisions ineffective [51, 53], regardless of the time and energy invested, and often results in significant rework [54, 55]."}, {"title": "2.3. Human-Al Collaboration for Systems Engineering", "content": "Wide-spread Al use is already under heavy scrutiny from nearly every discipline. Within SE we can find examples like Selva and Martin's work in building an open source virtual assistant to allow ease of work on the designer [16]. Sarica et al. have explored the use of large semantic networks of technical terms for patent information, like TechNet, to drive the generation of new ideas base on those that already exist within the database [17]. Chen et.al. are engaging with generative adversarial networks and how they can be tuned to synthesize high-quality and diverse designs, while allowing exploration of a portion of the design space during concept development [56]. Chong et al. delves into the confidence and cognitive abilities of Al-assisted humans as they make decisions, finding that poor Al recommendations lead to diminished confidence in the use of Al in decision making [57]. These studies support the notion that domain expertise plays an important role in the influence of Al recommendations on the goodness of a decision. Furthermore, Doris et. al. shows that, while multi-modal LLMs show promise, there is still a need for development in these models to reliably interpret complex engineering documents especially for tasks requiring precise retrieval and cross-referencing of information across various formats [58]."}, {"title": "3. Methodology", "content": "We provide an overview of our research approach in Fig. 1. In step 1a we select an appropriate reference mission engineering problem from NASA RASC-AL challenges, in 1b, we choose a problem formulation task, which in this case is stakeholder identification. Step 2 iteratively refines prompts. In Step 2a, we create a task specific prompt and in 2b we create independent parallel threads and query an available LLM, which in this case was ChatGPT 3.5. In Step 2c, we briefly analyze the outputs, and either refine the prompt in Step2d and repeat Step 2, or if we deem the output reasonable, we pass the LLM output into Step 3, ex-post qualitative analysis. Below we elaborate each of the steps."}, {"title": "3.1. Step 1 - Selection of the Reference Problem and Mission Engineering Task", "content": "In Step 1a we chose a reference problem. For our interests NASA's RASC-AL competitions provided an ideal representative case as their problem statements refer to complex SoS problems with uncertain assets and resources. Furthermore, RASC-AL problems include well-defined mission need statements that have clear mission goals. We chose the 2021 Ceres reference mission, because at the time that this research was conducted, Chat GPT's database contained no information beyond September of 2021 [59]. The refence mission need statement reads:\n\"Develop a concept that supports a crew of 4 on a mission to Ceres in the 2040s. The mission concept should take advantage of having crew in proximity to Ceres for low latency science operations, identify what planetary science payloads could be delivered during the mission, and include the ability for at least two crew to land on the surface of Ceres. Consider long duration health effects including but not limited to radiation, microgravity, and isolation. Mission should be < 5 years total, ready to land on Ceres no later than December 31, 2049, with an annual budget of no more than $3B/year from 2035 to proposed mission end date.\" [42]"}, {"title": "3.2. Step 2 - Initializing, Prompt Formulation, and Iterative Evaluation of Intermediate LLM Outputs", "content": "The relevancy of outputs from a given LLM can be skewed by lack of context, lack of framing, and/or too many or too few inputs [29, 60]. If a prompt lacks proper context or is not initialized to give the LLM a certain role, the resulting outputs can tend towards irrelevant content. Alternatively, if a prompt lacks proper framing, resulting outputs may be broad and unspecified. Thus, initializing and prompt development were critical for our purposes. We addressed this issue with an iterative approach."}, {"title": "3.3. Step 3 \u2013 Ex-Post Analysis of Results", "content": "We analyzed each thread, by parsing its stakeholder information in a spreadsheet then qualitatively comparing each of the identified stakeholders for the following criteria:\n\u2022 Correctness: through a direct comparison to the ground truth list of stakeholders.\n\u2022 Traceability: a check that the provided stakeholder is real and appropriate for the needs statement.\n\u2022 Specificity: level of abstraction of the identified stakeholder. Used to remove stakeholders that would not be part of initial problem formulation.\nThese metrics begin to validate a set of responses generated by Chat GPT and allows for a distilled view of each thread for comparison. As part of the correctness evaluation,"}, {"title": "4. Results", "content": "We complied the 25 LLM threads for visual comparison, marking stakeholder outputs in each as correct, over specified, wrong, and missed. Fig 3 is a stacked bar chart where each bar represents an individual thread from ChatGPT,s and colors represent stakeholders. Green stands for correct, over-specified in orange, wrong in pink, and red represents missed stakeholders compared to the ground truth."}, {"title": "4.1. Output Comparisons", "content": "For each of the output threads, identified stakeholders were evaluated qualitatively using the three dimensions of correctness, traceability, and specificity. Although there are analytical methods to achieve this, such as cosine similarities, in this study we adopted a qualitative approach to avoid the discrepancy between the terms used to define a stakeholder vs. what the actual LLM definition conveyed. For example, the \"Astronaut Corps\u201d might be identified instead of \u201cMission Crew\u201d; with very similar descriptions and justification. Here, to differentiate our identification, we used the traceability evaluation, which helped to separate correct responses from wrong. In the ground truth \u201cMission Crew\u201d is defined as \"the crew selected to perform the mission\u201d. However, in the \u201cAstronaut Corps\" response may be described as \u201c...the committee that selects crew to perform the mission\u201d; which in this case reads very similar but is semantically wrong. On the other hand, the specificity metric differentiates the outputs into wrong or, more commonly, over-specified responses. For example, commonly a group like \u201cNASA Human Research Program\u201d would be identified with language like \u201cDivision responsible for addressing the long-duration health effects on the crew....\u201d While this falls in with the \u201cMission Operations\u201d stakeholder, it is a very specific aspect of mission operations. This program may indeed be considered a stakeholder within the mission lifecycle as objectives for the mission are defined, however at the initial problem formulation, this is not the case. Lastly, missed ground truth stakeholders were identified by simply subtracting the total number of correctly identified stakeholders from the total number of ground truth responses."}, {"title": "4.2. Variance in Responses", "content": "Table 2 shows that ChatGPT only finds a little over half of the ground-truth stakeholders correctly; however, it achieves this consistently. The implication being that the end user will receive some of the information they are seeking outright. This accompanied by a relatively low number of \u201cwrong\u201d stakeholders with a low standard deviation is indeed encouraging."}, {"title": "5. Discussion and Conclusion", "content": "The complexity of modern SoS and challenges of planning for mission engineering activities that involve interrelated decisions under immense uncertainty necessitate domain-experts for effective problem formulation and solving. Emerging Al tools bring forth the promise alleviate some of expert workload and expedite these efforts, hopefully with increased quality and efficiency. However, translating open ended ill-defined problems into well-defined ones that are amenable for engineering development is an elusive challenge, and it remains to be seen how effective and consistent current LLMs are in assisting these goals. This paper presents preliminary results from a human-assisted stakeholder identification task for a NASA space mission problem to gauge how well and consistently a representative LLM can perform.\nThis study reports two main findings. First, in terms of quality, the glass is half full; as there is evidence to expect that multi-purpose Al tools such as ChatGPT could have some value in assisting problem formulation tasks [48]. We found that through iterative prompting, and purposefully no other further fine-tuning - a research choice at this stage - LLMs can provide a somewhat reasonable set of stakeholders. However, among this list, only half of the expected stakeholders are correctly identified, whereas the other half of expected stakeholders are consistently omitted. We observed that omitted stakeholders were usually external systems and operational/environmental factors. This was despite explicit prompting to guide the LLM to capture them. This may be a result of the lack of quality documents in training corpus. Arguably, it could be an extension of lack of distinction between the semantic meaning of what a \u201cstakeholder means\" thus indicative of how LLMs could exacerbate existing issues in the SE community as many SEs wrongfully only focus on humans and organizations when they study stakeholders. In addition, we also documented a strong tendency to over specify, often accompanied by an extensive list stakeholder that are remotely related to the mission. In some cases, but not always, this hinted at a solution seeking behavior, with stakeholders that would only be appropriate if a specific solution was to be selected. This is problematic given that we studied a problem formulation exercise and not a concept generation one.\nThe second finding is related to the ability of the LLM to provide consistent responses. Although we only experimented with a rather small data set of 25 independent threads, and a single LLM there was great variance in both the total number of stakeholders identified and the types of stakeholders captured in each independent attempt. LLMs are not deterministic tools, even with the expected stochasticity, we find this large variance concerning. This is more relevant given the tendency of practitioners to ask a given question once and take the given answers in verbatim, often without further verification or an analysis of variance. While in our case this could arguably be contributed to the uniqueness of the given mission statement - as it caused the LLM to do more \u201chunting\u201d for an answer instead of pulling a well-vetted one as it would for a relatively simple task \u2013 there was never a repetition of outputs from the LLM nor was there mirroring in the phrasing of how stakeholders were described. Overall, these observations suggests that that every LLM response is \u201ca roll of dice\u201d, and the quality of any given input could be subject to great variability; a key characteristic of LLMs that needs to be considered in future research.\nRelated to this concern, recent research notes that the mathematical reasoning of LLMs are potentially very fragile and may be the determining factor of high variance in repeated responses to the same inquiry [62]. While other disciplines are also starting to catch on to the issue of variance when working with LLMs, some are finding that as new LLMs emerge, some of this variance may be diminishing in certain applications [63], [64]. Regardless, this concern is not just intransient to mission engineering tasks and should be accounted for when evaluating any collaborative work done with current Al tools. Nevertheless, a much larger study would be required to understand how detrimental or possibly inconsequential this variance becomes for well-defined prompts for problem formulation exercises.\nFinally, observing that the LLM is able to provide some good outcomes is encouraging, particularly for the kinds of problems ME is attempting to frame as these may not be available in a reference database, or could even be experienced for the first time by the given LLM. Problem uniqueness may be a major contributor to why the LLM is unable to provide a more complete answer. To that end, the contextually relevant knowledge of the practitioner to correlate new problems with existing processes is seemingly out of the grasp of LLMs currently. Nevertheless, ChatGPT does often provide a relatively useful set of responses that could be taken as an initial cut. Practitioners with an expertise in these skills could still benefit from the use of an Al assistant through the rapid generation of these tasks. This in return, could allow for a shift in cognitive load on the correction and refining of Al outputs rather than the pure generation of the task described. However, a non-expert would likely tend toward solution seeking behavior and potentially begin developing a system that satisfies the wrong stakeholders and needs."}]}