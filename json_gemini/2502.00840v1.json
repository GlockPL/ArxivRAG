{"title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense", "authors": ["Jiawen Zhang", "Kejia Chen", "Lipeng He", "Jian Lou", "Dan Li", "Zunlei Feng", "Mingli Song", "Jian Liu", "Kui Ren", "Xiaohu Yang"], "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear.\nIn this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadA's effectiveness in enhancing the safety capabilities of LLMs after activation approximations.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities and gained surging popularity ever since the release of ChatGPT [1]. Their widespread applications across diverse fields are further bolstered by open-source LLMs such as Llama [2], Gemma [3], Mistral [4], Falcon [5], and Qwen [6], offered by model developers capable of conducting extensive pretraining and meticulous alignment procedures like Meta, Google, and Alibaba. Application builders, ranging from enterprises to individuals, can harness the capabilities and flexibility of these open-source aligned LLMs for custom use cases and diverse deployment scenarios. Still, the prohibitive deployment costs hinder the full application potential of LLMs due to their astronomical scales, particularly in resource-constrained scenarios. Additionally, the computational overhead introduced by advanced and complex activation functions like GELU [7] and SwiGLU [8], which are integral and pervasive to modern LLMs (e.g., all above-mentioned open-source LLMs), further exacerbates the inference efficiency challenge. Consequently, improving the inference efficiency of LLMs has garnered growing research attention, with existing approaches exploring distinct facets, including weight quantization [9-11], weight sparsification [12-15], model pruning [12, 13, 16], speculative decoding [17, 18]. These efforts offer complementary reductions in inference overhead and hold promise for achieving aggregated efficiency gains when combined.\nActivation Approximation. Recently, activation approximation has emerged as a promising and, in some cases, indispensable avenue for pursuing inference efficiency enhancement in LLMs. This line of methods focuses on devising various approximations to alleviate the computation of complex non-linear activation functions, thereby catering to specific deployment requirements. Based on the application scenario and approximation techniques, there are three notable categories, including (i) Activation Polynomialization [19-23]: Replacing complex nonlinearities in activation functions with polynomials substantially accelerates computation, making it indispensable for the private inference scenario. (ii) Activation Sparsification [24-26]: Truncating input activation values close to zero so that weight channels corresponding to zero-valued activations can be ignored in inference computation. (iii) Activation Quantization [27, 28]: Quantizing the activa-"}, {"title": "2 Background", "content": "2.1 Activation Approximations\nActivation Polynomialization. In private LLM inference [19-23], the polynomialization of the activation function is widely used to improve the efficiency of private inference. Private inference is a two-party cryptographic protocol based on secure multi-party computation (MPC) and fully homomorphic encryption (FHE). It can achieve that the server learns nothing about the clients' input and clients learn nothing about the server's model except the inference result. In contrast to regular inference in plaintext, operations like Gaussian Error Linear Unit (GELU) and LayerNorm require intensive computation and communication in ciphertext, which usually dominates the total inference latency [48-51]. To address this problem, the server usually replaces the non-linear functions to MPC/FHE-friendly approximations. MPCFormer [49] replaces GELU(x) to $0.125x^2+0.25x^2+0.5$, BOLT [21] replaces GELU(x) to a 3-segment polynomial function, BumbleBee [22] and NEXUS [23] replace GELU(x) to a 4-segment polynomial function.\nActivation Sparsification. Recent work has observed that activations in the MLP blocks of LLMs are sparse [26, 52]. This implies that only a few rows (or columns) of the corresponding weight matrices are required for the forward pass. Activation sparsification [24-26] is an alternative method leveraging sparsity in hidden states. Since the weight channels corresponding to zero-valued activations are not used in computation, speed-up can be realized by selectively omitting these weights during memory transfer. Unlike model pruning techniques, which either remove zero-valued weights from the model or set some weights to zero, activation sparsity exploits the zeros that occur dynamically during runtime, depending on the input.\nTEAL [24] and CATS [53] have realized training-free activation sparsity based on magnitude pruning. By pruning low-magnitude, non-salient activations, they realize wall-clock speed-ups of up to 1.9\u00d7 at 50% sparsity.\nActivation Quantization. Quantization has proven to be a promising technique for mitigating both computational and memory overhead in LLMs. Many post-training quantization methods [9, 10, 54] reduce memory consumption through weight-only quantization, where weights are quantized while activations remain in full precision. To further reduce the computational overhead, recently, many works [27,28,55] employs weight-activation quantization which quantizes both weight and activation into low-bit values for the execution of low-bit matrix multiplication. For instance, INT8 quantization of weights and activations can halve the GPU memory usage and nearly double the throughput of matrix multiplications compared to FP16.\nTo exclude the effect of training and weight quantization on the safety evaluation, we only study post-training quantization (PTQ) and keep weight at full precision. We choose SmoothQuant [27] and OmniQuant [28] as our case studies."}, {"title": "2.2 LLM Safety Evaluations and Alignment", "content": "LLMs commonly adopt a self-autoregressive framework, where each token is predicted based on all previously generated tokens. This recursive process allows the model to generate coherent text step by step. Given a vocabulary V, the sequence prediction task can be formally expressed as:\n$\\pi_{\\theta}(y|x) = \\pi_{\\theta}(y_1|x) \\prod_{i=1}^{m-1} \\pi_{\\theta}(y_{i+1}|x, y_1, ..., y_i)$,\nwhere $\\pi_{\\theta}$ is the model, $x = (x_1, x_2, ..., x_n)$, ($x_i \\in V$) is the context including the prompt, and $y = (y_1, y_2, \u2026, y_n)$, ($y_i \\in V$) is the predicted sequence.\nDespite the effectiveness of this mechanism to generate coherent and contextually relevant text, it offers limited control over aligning outputs with human preferences. Consequently, prior work [46, 56\u201359] has shown that these models may still produce harmful, biased, or otherwise undesirable content. Particularly in high-stakes domains such as healthcare or finance [60, 61]. Therefore, ensuring safe and aligned outputs is crucial, and this has led to the development of alignment techniques that aim to bridge the gap between a model's generative objective and the need for responsible outputs [62, 63].\nSafety Evaluations. Effective safety evaluation is crucial to understanding and ensuring that LLMs generate outputs that align with human values and ethical guidelines. Existing researches on LLM safety evaluation typically involves multiple dimensions, including Attack Success Rate (ASR) [46], Refusal mechanisms [64], and Safety Risk Index (SRI) [65]. These evaluation metrics have form the backbone of safety assessments in LLM research [66-74], offering a systematic"}, {"title": "3 Safety Assessment and Analysis", "content": "In this section, we aim to provide a systematic safety assessment and analysis of existing activation approximation methods. We will first introduce the threat model of our study in Section 3.1 and then describe the setup of safety evaluations in Section 3.2. In Section 3.3, we elaborate on the specific activation approximation methods under evaluation and summarize their activation errors, covering seven representative techniques from each of the three categories. Finally, in Section 3.4, we identify three key observations from the safety evaluation results, revealing the characteristics of activation approximations on LLM safety behaviours.\n3.1 Threat Model\nIn this paper, we consider a common scenario in the development and deployment of LLM-based applications, as outlined in Figure 1. This scenario involves three parties: the LLM model developer, the LLM-based application provider, and the application users. In particular, the roles of the first two parties can sometimes overlap within the same organization.\n\u2022 LLM Model Developers provide open-source safety-aligned LLMs, enabling application providers to customize models for legitimate usage in accordance with licensing terms. Prominent examples include large technology companies such as Meta (with LLaMA-Chat), Google (with Gemma), and Alibaba (with Qwen), as well as open-source organizations/groups that have released safety-aligned LLMs like Falcon, Mistral, Alpaca, and Zephyr.\n\u2022 LLM-based Application Providers customize open-"}, {"title": "3.2 Safety Assessment Setup", "content": "Safety Measurement. We measure LLM's safety by evaluating the Attack Success Rate (ASR), which is defined as the percentage of failure to abstain from responding to a malicious instruction. These malicious instructions come from AdvBench [46], which contains 520 items in total. Previous works [46,80] simply use a small set of rejection phrases such as \"Sorry, I can't\u201d, \u201cI can not fulfill\" to do prefix matching to determine whether the model rejects the answer. However, this simple procedure may misclassify the output, if the model replies \"I think it's illegal\u201d or \u201c####\", which are not in the set, they classify the attack as successful, but in fact, it is not.\nTo reduce the risk of misjudgment, the HarmBench classifier [64] has been widely adopted to judge whether the output content is harmful or not.\nHarmCLS(y) = $\\begin{cases}\n1; & \\text{if y is harmful} \\\\\n0, & \\text{if y is harmless}\n\\end{cases}$\nAs claimed, the classifier fine-tuned with LLAMA-2-13B outperforms GPT-4 by approximately 5% in agreement rates with human judgments on the manually labeled validation set. However, we found that it could still fail to distinguish a benign output from a misaligned one, especially under circumstances where the instructions are complicated. We show\""}, {"title": "3.3 Activation Approximation Error", "content": "In this section, we summarize the error of different activation approximation techniques and select representative open-source works to do the activation approximation for our benchmark. We begin by recalling the multi-layer perceptron (MLP) within each Transformer block, which contains the activation expressions to be approximated and analyzed.\nDefinition 1. Let the l-th MLP block (with no biases), with l-th layer model parameter $\\theta_l$ and input embedding vector $e_l$, be denoted by the function $MLP_l$, of the form\n$MLP_l(\\theta_l|e_l) = (\\alpha(e_l \\cdot W^{up})) \\cdot W^{down}$,\nwhere $W^{up}, W^{down}$ are constituent parameter matrices associated with $\\theta_l$, $\\alpha(\\cdot)$ is the activation function, such as SwiGLU or GELU, which are widely adopted by LLMs in practice.\nApproximation Error of Activation Polynomialization In private inference, the server replaces the non-linear functions F with MPC/FHE-friendly approximations $F_{approx}$.\nThe error of private inference comes from the calculation error between the non-linear function and its polynomialization, the $\\epsilon^{up}$ comes from the approximation of LayerNorm and $\\epsilon^{down}$ comes from the approximation of GELU:\n$\\epsilon^{up} = F_{LayerNorm}(x) - F^{approx}_{LayerNorm}(x)$,\n$\\epsilon^{down} = F_{GELU}(x) - F^{approx}_{GELU}(x)$.\nApproximation Error of Activation Sparsification Following the definition of activation sparsification in TEAL [24],"}, {"title": "3.4 Further Analysis and Key Observations", "content": "Observation I: Activation approximations can cause LLMs to compromise safety before losing utility, leading them to produce meaningful responses to malicious queries.\nSince errors were introduced in the outputs of activation functions as shown in the case studies, we conduct continuous noise simulation experiments on 10 safety-aligned LLMs to explore the impact of activation approximations on model safety and utility. Based on the noise distributions observed in Figure 2, we apply different levels of approximation to the activation functions of commonly used safety-aligned LLMs. The results are shown in Figure 3 and Figure 4.\nWe collect the corresponding ASR% and Perplexity scores produced by the models after noises have been added to their activation outputs before $W^{up}$ and $W^{down}$. The x-axis is the standard deviations of the noise, the left y-axis represents"}, {"title": "4 Safety Defense for Activation Approximation", "content": "In this section, we propose QuadA, a simple yet effective safety enhancement method designed to address the safety compromises introduced by various activation approximation methods in aligned LLMs. The goal is to enable LLM model developers to integrate additional robustness into their original safety alignment procedures with minimal code modifications (e.g., adding just 2 to 3 lines of code). This ensures that the resulting LLMs, after being open-sourced and utilized by downstream application providers, remain robust against various activation approximations introduced to enhance inference efficiency. The design principles of QuadA are motivated by our key observations in section 3.4, where we introduce three algorithmic components corresponding to each key observation. We first present the QuadA algorithm in section 4.1 and then evaluate its safety enhancement capabilities in section 4.2 against Jailbreak attacks, which serve as a stress test for safety. The results demonstrate that QuadA produces safety-aligned LLMs that remain robust against Jailbreak attacks even after the introduction of various activation approximations. Finally, we conduct extensive ablation studies to examine the effectiveness of each algorithmic component in section 4.3.\n4.1 Our Proposed QuadA\nBased on Observation I, activation approximations induce safety degradation in aligned LLMs. A trivial solution for the LLM service provider is performing safety alignment on models that have undergone activation approximations. e.g. MPCformer [49] will then fine-tune the model with the activation function replaced. However, this requires the LLM-based service provider to have safety-alignment datasets and corresponding computational resources, which are not easy to fulfill in reality, such as intelligent consultation services in"}, {"title": "4.2 Effectiveness of Jailbreak Defense", "content": "Experiment Setup. Our goal for defending against activation approximations-induced jailbreaking is to ensure that the LLMs do not produce harmful responses to jailbreak prompts at different level of activation approximations. The training data for QuadA is derived from the PKU-SafeRLHF dataset [34] with 73.9k training samples for 1 epoch. The loss"}, {"title": "4.3 Ablation Study", "content": "QuadA introduces three main algorithmic components driven by the key observations in Section 3. These components correspond to (1) Introducing perturbations with magnitudes designated by MVA; (2) Restricting perturbations exclusively to safety-sensitive layers; and (3) Regularizing harmful prompts in the activation space. Below, we evaluate the effect of each component through a series of ablation studies, which corroborate the necessity of these algorithmic designs.\nNecessity of Introducing Perturbations by MVA. To begin with, we examine the effect of introducing perturbations and their magnitudes. We conduct experiments to evaluate the safety capabilities of LLMs under three perturbation configurations: no approximations (NA), perturbations with magnitudes 10% larger approximations than MVA (LA), and perturbations defined by MVA. As shown in Table 5, applying perturbations designated by MVAconsistently results in ASRs below 2% across all tested models, representing an improvement of over 98% in the models' resistance to activation approximation errors. In contrast, while introducing large approximations (LA) during alignment can still improve model safety compared to the no perturbation (NP) baseline, the results are largely less effective than those achieved by MVA, i.e., yielding only around 50% lower ASRs. This demonstrates that QuadA's approximation with MVA is critical for achieving the desired safety-enhancement effects.\nNecessity of Sensitive-layer Selection. We observe that introducing perturbations exclusively to the sensitive layers of an LLM yields the most favorable results for safety alignment. In contrast, applying noise to non-sensitive layers negatively impacts alignment effectiveness, leading to increased perplexity and degraded model utility.\nIn detail, Figure 9 presents PPL and ASR results from applying perturbations during safety alignment training under the following four different strategies: no layers, only non-sensitive layers, only sensitive layers, and all layers. Compared to the baseline, perturbing non-sensitive layers has minimal impact on the model's resistance to activation approximations. In contrast, harmful prompts exhibit significantly reduced jailbreak efficacy against models aligned with noise applied to their sensitive layers. A similar effect occurs when the perturbation is applied to all layers, which however substantially increases perplexity without further safety gains.\nOverall, applying perturbations to layers other than the safety-sensitive ones offers little to no improvement in safety capabilities while noticeably impairing utility. This validates the necessity of selecting safety-sensitive layers in QuadA.\nVisualization of Similarity-based Regularization. Figure 10 presents an MDS projection of the last-token activations produced by the Llama-2-7B-chat model after QuadA alignment. In comparison to the activation space of the model prior to alignment (depicted in Figure 5a), harmful activations are"}, {"title": "5 Limitations", "content": "The models studied in this paper are all open-source LLMs. Since closed-source LLMs, e.g., GPT-3.5 and GPT-4 do not provide an API to set the activation approximation, we cannot find the most vulnerable approximation (MVA) and align the LLMs with MVA. However, we must point out that in reality, the LLM trainer has full permissions on the model, so the LLM trainer has the permission to perform safety tests on the model under various activation approximations, thereby obtaining the model's MVA and using QuadA for alignment."}, {"title": "6 Conclusion", "content": "In this paper, we have systematically investigated the safety implications of activation approximation techniques in Large Language Models (LLMs), uncovering significant vulnerabilities that remain unaddressed by current safety alignment infrastructures. Our safety vetting has revealed that these approximations can substantially compromise model safety, elevating attack success rates without degrading utility. Through extensive analyses, we identified critical layers and harmful activation patterns that exacerbate these vulnerabilities, highlighting the disproportionate impact of early-layer approximations on safety.\nTo address these challenges, we have proposed the first activation approximation-robust safety alignment method, which mitigates safety risks by optimizing sensitive layers with diverse activation distributions. Our approach not only withstands various levels of activation perturbations but also demonstrates robust resistance to adaptive jailbreak attacks. By sharing our findings, we aim to raise awareness of the safety costs associated with activation approximations and inspire further research to reinforce the safety of aligned LLMs in practical deployment scenarios."}, {"title": "Ethics Considerations", "content": "This work is dedicated to examining the security and safety risks that arise in the customization of aligned LLMs via activation approximations. We highlight that our work only needs publicly available datasets. Our ultimate goal is to contribute positively to society by improving the security and safety of language models in the wild."}, {"title": "A Appendix", "content": "A.1 Prompt Templates\nDuring inference, the input instruction is initially integrated into a template, which is then tokenized and processed through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template [100] for different problems across various aligned LLMs in our study.\nAdaptive Prompt-based Jailbreak Attacks\nWe list the adaptive prompt-based jailbreak attacks in table 4 here.\n\u2022 GCG [46] append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix. The default iterations of GCG is 500.\n\u2022 AutoDAN [47] generates an adversarial suffix in a sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives. The default iteration of Auto-DAN is 100.\n\u2022 DRA [90] involves dissecting harmful prompts into individual characters and inserting them within a word puzzle query. The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions. Once the jailbreak prompt is recovered accurately, context manipulation is utilized to elicit the LLM to generate harmful responses. The default iteration of DRA is 20."}]}