{"title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability", "authors": ["Yujin Han", "Lei Xu", "Sirui Chen", "Difan Zou", "Chaochao Lu"], "abstract": "Large language models (LLMs) have shown remarkable capability in natural lan-\nguage tasks, yet debate persists on whether they truly comprehend deep structure\n(i.e., core semantics) or merely rely on surface structure (e.g., presentation for-\nmat). Prior studies observe that LLMs' performance declines when intervening\non surface structure, arguing their success relies on surface structure recognition.\nHowever, surface structure sensitivity does not prevent deep structure comprehen-\nsion. Rigorously evaluating LLMs' capability requires analyzing both, yet deep\nstructure is often overlooked. To this end, we assess LLMs' comprehension ability\nusing causal mediation analysis, aiming to fully discover the capability of using\nboth deep and surface structures. Specifically, we formulate the comprehension of\ndeep structure as direct causal effect (DCE) and that of surface structure as indirect\ncausal effect (ICE), respectively. To address the non-estimability of original DCE\nand ICE stemming from the infeasibility of isolating mutual influences of deep\nand surface structures, we develop the corresponding quantifiable surrogates, in-\ncluding approximated DCE (ADCE) and approximated ICE (AICE). We further\napply the ADCE to evaluate a series of mainstream LLMs (and the one with random\nweights), showing that most of them exhibit deep structure comprehension ability,\nwhich grows along with the prediction accuracy. Comparing ADCE and AICE\ndemonstrates closed-source LLMs (e.g., GPT) rely more on deep structure, while\nopen-source LLMs (e.g., Llama) are more surface-sensitive, which decreases with\nmodel scale. Theoretically, ADCE is a bidirectional evaluation, which measures\nboth the sufficiency and necessity of deep structure changes in causing output vari-\nations, thus offering a more comprehensive assessment than accuracy, a common\nevaluation in LLMs. Our work provides new insights into LLMs' deep structure\ncomprehension and offers novel methods for LLMs evaluation. The code for our\nproject is available at https://github.com/OpenCausaLab/ADCE.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated unprecedented capability in various natural\nlanguage tasks [1-6]. Despite these achievements, there remains a debate over whether LLMs truly\ngrasp the deep structure necessary for solving variations of the same problem, or if they simply learn\nthe surface structure present in data. The distinction between surface and deep structure, defined in\nsurface structure theory [7], differentiates between observable sentence forms and the underlying\nsemantic units that represent a question's core meaning. This distinction is further illustrated with\nexamples in Table 1. Many studies evaluating LLMs based on task-specific accuracy [8-10] often"}, {"title": "2 A Causal Perspective of LLMs' Comprehension Ability", "content": "In this section, we define LLMs' deep structure comprehension ability by formulating it as a problem\nof estimating causal effects. We first introduce important notations for subsequent analysis. Consider\na dataset $D = \\{(x_i, Y_i)\\}_{i=1}^n$, where $x_i$ denotes the i-th question and $y_i$ represents the corresponding\nanswer. Each question $x_i := (d_i, s_i)$ can be split into two independent components [13]: the deep\nstructure $d_i$ and the surface structure $s_i$, with $d_i\\cap s_i= \\emptyset, d_i \\cup s_i \\subseteq x_i$. Given an LLM parameterized by $\\theta\\in \\Theta$,\ndenoted as $f_\\theta$, its output for $x_i$ is represented as $Y_\\theta(x_i) := f_\\theta(x_i)$.\nComprehension Ability. While high accuracy often indicates a high-performing model, our work\ndelves into whether LLMs achieve this accuracy through a genuine understanding of deep structure."}, {"title": "3 Method", "content": "This section focuses on the causal effect of deep structure on output, as defined in Section 2. Notably,\nestimating this causal effect inherently requires quantifying the causal effect of surface structure.\nThus, by concentrating on deep structure, we also gain insights into the surface structure. Section 3.1\npresents a causal graph linking inputs, structures, and outcomes, formulating comprehension as direct\n(DCE) and indirect causal effects (ICE). Section 3.2 further addresses the non-estimability of DCE\nand ICE by proposing their approximations: ADCE and AICE. To estimate these metric in practice,\nSection 3.3 details the generation of intervention data necessary for estimating ADCE and AICE.\nFinally, to demonstrate the value of our metric in LLMs evaluation, Section 3.4 shows how ADCE\noutperforms the common metric, accuracy, in evaluating LLMs' deep structure dependency."}, {"title": "3.1 Formulating Deep Structure Comprehension as Direct Causal Effect", "content": "Figure 3 presents a causal graph with mediation depicting relationships among inputs $x$, deep\nstructure $d$, surface structure $s$, and outcome Y. It illustrates how $x$ influences Y via $d$ ($x \\rightarrow d \\rightarrow Y$)\nand $s$ ($x\\rightarrow s \\rightarrow Y$).Deep structure, reflecting core semantics, logically correlates with output,"}, {"title": "Causal Effect Estimation.", "content": "We proceed by defining LLMs' comprehension ability as a causal effect\nestimation problem. Define the treatment assignment variable T on input $x_i$ as:\n$T=\\begin{cases}\n0 & \\text{intervention alters $s_i$, preserves $d_i$}\\\\\n1 & \\text{intervention alters both $s_i$ and $d_i$}\n\\end{cases}$ \nBoth $d_i$ and $s_i$ are unobservable, non-manipulable latent variables. Intervention T only manipulate\nthe observable input $x_i$. The potential outcome for $x_i$ under T = t is $Y_i(t)$. The deep structure\ncomprehension ability is defined as the causal effect of deep structure on an LLM's output, i.e., the\nexpected change in the output when intervening on the deep structure while keeping surface structure\nfixed. Analogously, the surface structure comprehension capability is defined.\nBy defining LLMs' deep and surface structure comprehension as causal effects, we establish a causal\nestimation framework. Leveraging this framework, we quantify abstract comprehension capabilities\nvia estimable causal effects, enabling objective assessment of LLMs' understanding."}, {"title": "3.2 Estimating DCE from Data: Challenges and Solutions", "content": "Although Equation 3 can indirectly esitimate DCE, it still suffers the following issues:\n\u2022 Unobservability: ICE in Equation 3 is unobservable due to a paradox: The surface structure in ICE\nmust maintain the value it would have under deep structure change ($s(T = 1)$), while the deep\nstructure in ICE should remain unchanged ($T = 0$). Consider 2-digit multiplication task in Table 1,\nICE should preserve the surface query format as What is <mask> times 20? ($s(T = 1)$) where the\ndeep structure is altered (T = 1), thereby contravening the condition T = 0.\n\u2022 Incomputability: Equation 3 requires differencing $Y_i$ and $Y_i^{origin}$, but the outputs of LLMs typically\nlack numerical form, complicating the execution of such subtraction. For instance, in word\nunscrambling tasks [29], the string nature of outputs inherently prevents direct arithmetic operations\nsuch as subtraction.\nTo address above issues in DCE, we propose the following solutions. Based on these solutions, we\nderive the approximated direct causal effect (ADCE) as an estimable surrogate for DCE.\nAddressing Unobservability. ICE in Equation 3 requires simultaneous $T = 0$ and $s(T = 1)$, which\nare unobservable in practice. Therefore, we propose approximated DCE (ADCE) to substitute original\nICE in Equation 3 with observable ($T = 0, s(T = 0)$)as approximated ICE (AICE). The efficacy\nof this approximation hinges on the similarity between the original ICE and AICE, specifically the\nsimilarity between $(T = 0, s(T = 1))$ and $(T = 0, s(T = 0))$. To ensure approximation validity,\nwe meticulously design intervention strategies for generating data that minimize the discrepancy"}, {"title": "3.3 Generating Intervention Data for Approximated DCE Estimation", "content": "To indirectly estimate ADCE, we should detail the generation of intervention data required for TE\nand AICE estimation in Equation 5. Specifically, we focus on constructing appropriate approximation\nto minimize the discrepancy between AICE in Equation 5 and oracle ICE in Equation 3.\nIntervention Data for TE. TE requires intervention data with altered deep structure (T = 1) and\nmatched surface structure (s(T = 1)). To achieve this, we intervene on inputs \u00e6 to alter core\nsemantics using Mask and Rephrase strategies in Table 1. For inputs with explicit core semantic\nwords, such as numbers and operators in two-digit multiplication tasks, we apply Mask; otherwise,\nwe use Rephrase . Table 2 shows examples with diverse intervention strategies for TE.\nIntervention Data for AICE. To approximate the unobservable ICE in Equation 3, we minimally\nmodify the deep structure of TE with (T = 1,s(T = 1)) in Equation 5 to derive AICE with\n(T = 0, s(T = 0)). Deriving AICE from TE yields an observable substitute for the original ICE and\nensures high similarity between s(T = 1) in TE and s(T = 0) in AICE. Thus, the key distinction\nbetween TE and AICE lies in the deep structure difference, ensuring isolation of surface structure's\neffect on output. Specially, we employ two strategies: (1) Mask: masking k non-core semantic words\nclosest to the masked core semantic word in TE; (2) Rephrase: minimizing word-level modifications\nto transform TE with (T = 1, s(T = 1)) to AICE with (T = 0, s(T = 0)) with prompts suck as\nmodify the keywords with minimal word changes. Table 2 provides detailed intervention examples.\nFor rephrasing, we use Claude-3.5-Sonnet [31] and design a self-checking mechanism. Claude\nre-answers rephrased questions to verify deep structure alteration and preservation. Algorithm 2\noutlines the process, with detailed mask rules and rephrase prompts in Appendix F.1."}, {"title": "3.4 ADCE: Bidirectional Evaluation of Deep Structure Comprehension", "content": "This section compares the proposed ADCE in equation 5 with accuracy metrics. Our analysis\ndemonstrates that ADCE better reflects the bidirectional relationship between deep structure and\nmodel outputs, regardless of whether the outputs are depended on the deep structure or merely\nassociated with surface structure due to spurious correlations."}, {"title": "LLMs' Output Depends on Deep Structure.", "content": "When outputs of LLMs mainly rely on deep structure,\naccuracy measures the correctness linking deep structure to output. In contrast, ADCE assesses\nthe bidirectional relationship between deep structure to outputs, offering a more comprehensive\nevaluation. Specifically, we demonstrate that ADCE integrates the probability of sufficiency (PS)\nand probability of necessity (PN) [32]. For two boolean $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$, PS ($\\delta_{PS}$) and\nPN ($\\delta_{PN}$) measure how likely $X = 1$ causes $Y = 1$ given $X = 0, Y = 0$, and how likely $X = 0\nprevented $Y = 1$ given $X = 1, Y = 1$, respectively. In other words, PS assesses if X = 1 is\nsufficient to cause Y = 1, establishing a sufficient condition $X \\rightarrow Y$, while PN evaluates if X = 1 is\nnecessary for Y = 1 to occur, determining a necessary condition $Y \\Rightarrow X$. Theorem 1 demonstrates\nADCE is a weighted combination of PS and PN, thereby capturing the bidirectional relationship\nbetween the sufficiency and necessity of deep structure changes on output variations.\nTheorem 1. (ADCE as a Combination of PN and PS) Let T be the treatment variable in Equation 2\nand $\\hat{Y}$ the outcome of the indicator function in Equation 5. Assume $\\hat{Y}$ is monotonic with respect to T,\nfor ADCE, it holds that:\n$\\delta_{ADCE} = \\frac{\\alpha}{2} \\delta_{PS} + \\frac{\\beta}{2} \\delta_{PN}$\nwhere $\\alpha := P(\\hat{Y} = 1 | T = 1, s(T = 1))$, $\\beta := P(\\hat{Y} = 0 | T = 0, s(T = 0))$.\nTheorem 1 demonstrates that ADCE quantifies the probability that modifications in deep structure are\nboth necessary and sufficient for output variations. That is, ADCE measures the likelihood that deep\nstructure alterations are the sole pathway leading observed changes in output. More introductions on\nPS and PN, along with detailed proof of Theorem 1 are in Appendix C.2."}, {"title": "LLMs' Output Depends on Surface Structure.", "content": "When models' outputs mainly depend on surface\nstructure, e.g., spurious correlations, conventional accuracy metrics can be misleading [33\u201336]. For\nexample, in sentiment classification tasks [37, 38], spurious correlations between identity and toxicity\ncan lead models to misclassify texts containing identity information as toxic. While accuracy metrics\nbased on these surface structure (e.g., identity information) might suggest high performance, they\ntend to overestimate the actual efficacy of the model. ADCE mitigates this by considering both\nsufficiency (identity information leading to toxicity) and necessity (toxicity not always implying\nidentity information). This approach mitigates overreliance on spurious high-correlation paths from\nidentity to toxicity, thus preventing performance overestimation. In Section 4.5, we empirically\ndemonstrate that as the level of spurious correlation increases, accuracy remains misleadingly high,\nwhereas ADCE declines. This demonstrates ADCE's superior ability to reflect a model's reliance on\ndeep structure, particularly in scenarios dominated by spurious correlations."}, {"title": "4 Experiments", "content": "In this section, we experimentally explore three critical questions: (1) Deep structure comprehension\nin LLMs: Do LLMs process questions through an understanding of the deep structure of problems?\nWe analyze this using the proposed ADCE in Section 4.2. (2) Prerequisite of deep structure\ncomprehension: What prerequisite enables LLMs to utilize deep structure in their responses?\nInsights into this question are discussed in Section 4.3? (3) Comparative influence of deep and\nsurface structures: Which has a stronger causal effect on the outputs of LLMs \u2013 deep or surface\nstructures? These investigations detailed in Section 4.4 collectively address the queries raised in\nSection 1, assessing whether LLMs are deep thinkers or merely surface structure learners. To further\nsupport Section 3.4, we evaluate whether ADCE assesses core semantic understanding more reliably\nthan accuracy under spurious correlations (in Section 4.5).\nAdditionally, Appendix E presents a quantitative experiment on synthetic data where the causal\neffects of true deep structures on outputs are computable to show the accuracy of ADCE and AICE.\nAppendix I contains experiments supporting that ADCE and AICE effectively reflect the different\nstructural understanding capabilities of LLMs, even when the data or labels contain noise."}, {"title": "4.1 Setup", "content": "Dataset Evaluation and Intervention. We employ five popular benchmarks across mathematics,\nlogic, and commonsense knowledge. For mathematics, we consider 2-Digit Multiplication task"}, {"title": "4.2 Deep Structure Comprehension Capability of LLMs", "content": "Figure 5 illustrates the relationship between accuracy and ADCE for 12 LLMs across five tasks.\nNotably, the ADCE for most models consistently remains positive, in stark contrast to the zero\nADCE observed in the random weight baseline\u00b3. Positive ADCE values suggest that intervening deep\nstructure causes LLMs to deviate from correct answers on previously solved problems, highlighting\nthe models' reliance on deep structure for accurate problem-solving. This finding underscores that\nmost LLMs possess deep structure understanding ability beyond surface structure.\nFurthermore, comparing models within the same series (e.g., Llama-2, Llama-3, Mixtral), we observe\nthat both accuracy and ADCE increase with model scale. A strong linear correlation emerges between\naccuracy and ADCE, with high R2 > 0.7 indicating a good fit to the linear model. This suggests that\nmodels with higher accuracy exhibit greater dependence on deep structure for outputs.\nFinally, slope \u03b2 of the accuracy-ADCE regression in Figure 5 quantifies the increase in deep structure\nunderstanding required per unit accuracy increase. Tasks like two-digit multiplication and word\nunscrambling show smaller \u03b2, indicating less deep structure comprehension needed for accuracy"}, {"title": "4.3 The Prerequisite of Deep Structure Comprehension Capability", "content": "In Figure 5, certain LLMs, such as Llama-3-8b on Analytic Entailment, show minimal causal effects\nof deep structures on model output characterized by negative ADCE. This anomaly, where twisting\ndeep structure improves accuracy, prompts an investigation into the specific conditions under which\nLLMs fail to comprehend deep structure across different tasks.\nTo investigate LLMs' failure, we explore the potential pre\nrequisites for deep structure comprehension with positive\nADCE. Inspired by previous work [44, 45], which proposes\nthat the causality exhibited in LLMs often mirrors task\nrelevant knowledge embeded in their training data, we hy\npothesize that the absence of deep structure comprehension\nmight indicate either unactivated or absent relevant knowl\nedge in the training data. This theory proposes that missing\nreplicable facts could hinder deep structure comprehension.\nTo test this hypothesis, we employ supervised fine-tuning\n(SFT) to potentially activate task-specific knowledge [46\n48]4. Specifically, we fine-tune Llama-3-8b on Analytic"}, {"title": "4.4 Deep vs. Surface: A Comparison of LLMs' Comprehension ability", "content": "After analyzing LLMs' deep structure comprehension and its potential sources, we extend our\ninvestigation to assess the reliance of LLMs on deep v.s. surface structures. This comparison aims to\ndetermine whether LLMs are deep thinkers or merely surface structure learners. We utilize ADCE\nin Equation 5 to measure the direct causal effect of deep structure, and an AICE, also specified in\nEquation 5, to quantify the indirect causal effect of surface structure while keeping deep structure\nconstant. Figure 7 shows these comparisons, presenting ADCE as $\\delta_{ADCE}$ and AICE as $\\delta_{AICE}$. Our\nanalysis reveals that closed-source models (e.g., GPT, Claude) primarily rely on deep structure, while\nopen-source models (e.g., Llama) are more sensitive to surface structure. However, this sensitivity\ngradually decreases as model size increases, suggesting larger LLMs is more dependent on deep\nstructure for answering. This analysis indicates that the tested closed-source models are not surface\nstructure learners, as their responses rely more on deep structure. For the evaluated open-source\nLLMs, the dependency on surface structure tends to diminish as model scale increases."}, {"title": "4.5 ADCE vs. Accuracy: Case Study on Spurious Correlation", "content": "This section highlights the superiority of ADCE over traditional accuracy in measuring model\nreliance on deep structure, particularly in scenarios involving spurious correlations. Leveraging\nCivilComments [37, 38], a popular dataset for spurious correlation analysis, we manipulate the\nproportions of majority (spurious) and minority (non-spurious) group representations to construct\ntraining sets with differing degrees of spurious correlations. We then fine-tune Llama-3 using these"}, {"title": "5 Related Work", "content": "Our related work primarily addresses the ongoing debate regarding LLMs' ability to comprehend\ndeep and surface structure. Existing research has predominantly focused on LLMs' sensitivity to\nsurface structure by modifying superficial patterns, such as substituting celebrity names, introducing\nmisleading contexts [12, 15], or altering the order of independent statements and options [12, 14, 49].\nThese studies observe LLMs' lack of robustness through token-level and sentence-level interventions\nwithout altering core semantics, suggesting that LLMs' success relies heavily on recognizing surface\nstructure. More aligned with our work, [29] attempted a systematic analysis of the differences\nbetween in-context learning (ICL) and instruction-tuning (IT) in LLMs' understanding of domain\nknowledge in mathematical problems. They found that ICL better helps LLMs distinguish between\ndeep and surface structure. These works inspire our research, which is more comprehensive and\nwidely applicable to analyze LLMs' capacity for understanding deep and surface structure."}, {"title": "6 Conclusion", "content": "This paper investigate LLMs' comprehension abilities of deep and surface structures, proposing\nADCE and AICE for quantification based on causal mediation analysis. ADCE analyses reveal\nLLMs' deep structure understanding across multiple tasks, potentially from activated task-specific\nknowledge in the training data. The comparison between ADCE and AICE reveals that closed-source\nLLMs comprehend deep structure better, while open-source LLMs exhibit higher surface sensitivity,\nwhich decreases as model scale increases. We demonstrate ADCE's superiority over accuracy in"}, {"title": "A More Examples of Surface and deep structure", "content": "In this section, we will provide more examples to illustrate the deep structure (core semantics) and\nsurface structure (surface forms) of different inputs. Table 1 lists examples of 2-digit multiplication\n[29]. We then present the deep and surface semantics for the remaining four tasks described in\nSection 4.1.\n\u2022 Word Unscrambling [29]: both Word Unscrambling task and 2-Digit Multiplication task have\nunified question templates and key tokens that reflect the core semantics. In Word Unscrambling,\nthe question template is typically The word X is a scrambled version of the English word, where\nX is the scrambled word, such as ofr (a scrambled version of for). The key token reflecting the\ncore semantics is X. Changes in surface structure, such as rephrasing the question to How can the\nscrambled letters ofr be rearranged to form a valid English word?, do not alter the answer to the\nproblem.\n\u2022 GSM8k [39]: GSM8k is a dataset of multi-step reasoning elementary math problems with diverse\nquestion formats. For example: A robe takes 2 bolts of blue fiber and half that much white fiber.\nHow many bolts in total does it take? The key tokens representing core semantics are numbers,\nquantifiers, etc. (e.g., 2, half). Changing the surface structure, such as using symbolic notation,\ndoes not alter the problem's essence:\n$X = 2, Y = X/2, X + Y =?$\nWhere X is blue fiber amount, Y is white fiber amount, and ? is the total.\n\u2022 Analytic Entailment [29]: Analytic Entailment is a task of determining logical relationships\nbetween sentences. The question format varies, for example: Lina met two nurses.Lina met at least\none woman. The deep structure in Analytic Entailment is manifested in logical relationships and\nsemantic inference, lacking uniform key tokens for core semantics. Altering the surface structure,\nsuch as: Lina met two female nurses. Lina did not meet at least one woman. does not change the\nnature of the task.\n\u2022 CommonsenseQA [40]: CommonsenseQA, like Analytic Entailment, lacks a uniform question\ntemplate. For example: A revolving door is convenient for two direction travel, but it also serves\nas a security measure at a what?. Its deep structure stems from understanding the question and\ncontext, without specific key tokens representing core semantics. Altering the surface structure,\nsuch as:A revolving door is commonly used for easy entry and exit, but it also serves as a secure\nbarrier between the outside and inside at a what? does not change the answer, as the core concept\nremains intact."}, {"title": "B The Causal Mediation Analysis", "content": "Causal Mediation Analysis (CMA) is a statistical method used to explain how an independent\nvariable affects a dependent variable through one or more mediating variables [50, 17, 25]. This\nanalytical approach is widely applied in many fields, such as psychology, sociology, and epidemiology\n[51, 28, 52]. Traditional mediation analysis is primarily quantifying mediation effects by comparing\ntotal (TE), direct (DCE), and indirect (ICE) causal effects [22, 53, 54].\nCMA places traditional mediation analysis within the potential outcomes framework [21], using\ncounterfactual reasoning to define and estimate causal effects [20]. This approach not only handles\nmore complex mediation models but also better addresses confounding factors and sensitivity analyses\n[17]. A typical CMA framework comprises a treatment (A), a mediator (M), and an outcome (Y).\nBoth A and M are observable variables that simultaneously influence Y. The primary objective of\ncausal mediation analysis is to assess the causal effect of A on Y while isolating the influence of M\nas illustrated in Figure 9.\nIn recent years, causal mediation analysis has also been widely applied in machine learning and\nartificial intelligence, providing new perspectives for explaining model decision processes and fairness\nassessments [55, 56].\nIt is important to emphasize that CMA is frequently applied to the traditional mediation model\n($x\\rightarrow z \\rightarrow y$ and $x\\rightarrow y$). Instead, we employ a variant of the classic causal mediation model\nknown as the Parallel Multiple Mediator Model [57\u201359]. In our model, the deep structure (d) and\nsurface structure (s) serve as two parallel mediators for the input x. The specific causal paths can be\nrepresented as $x \\rightarrow d \\rightarrow Y$ and $x \\rightarrow s \\rightarrow Y$."}, {"title": "B.1 Assumptions in Causal Mediation Analysis", "content": "To empoly thecausal mediation analysis, there are three positivity, consistency, and sequential\nignorability need to be satisfied [22-26, 60].\nPositivity Assumption. This assumption ensures that for all possible combinations of conditions, we\ncan observe samples with non-zero probability, thereby allowing reliable estimation of causal effects.\nThat is\nAssumption 1. (Positivity Assumption) For treatment (A), mediator (M), and an outcome (Y) in\nFigure 9, it holds that:\n\u2022 For the treatment variable A:\n$P(A = a) > 0, \\forall a \\in A,$\nwhere A is the set of all possible values of A.\n\u2022 For the mediator variable M:\n$P(M = m|A = a) > 0, \\forall m \\in \u041c, \u0430 \\in A$\nwhere M is the set of all possible values of M.\n\u2022 For the outcome variable Y:\n$P(Y = y|A = a, M = m) > 0, \\forall y \\in Y, a \\in A, m \\in M$\nwhere y is the set of all possible values of Y.\nThe positivity assumption is satisfied in our causal model. While as depicted in Figure 3, the\nintervention on the deep structure d invariably induces a change in the surface structure s, for any\ngiven d, there exists a non-zero probability of observing each possible value of s within the set S(d),\nwhere S(d) represents the range of s values consistent with d. Thus, the essence of the positivity\nassumption-enabling causal inference for all structurally possible scenarios\u2014is maintained, allowing\nfor valid causal analysis within the model's defined constraints.\nConsistency Assumption. The consistency assumption states that:When the treatment variable\nmatches the theory potential treatment, the observed outcome in experiments should equal the\npotential outcome theoretically. Similarly, when the treatment variable matches, the observed\nmediator value in experiments should equal the potential mediator value theoretically. That is\nAssumption 2. (Consistency Assumption) For treatment (A), mediator (M), and an outcome (Y) in\nFigure 9, for individual i, it holds that:\n$Y_i(a, M_i(a)) = Y_i \\text{  when  } A_i = a, $"}, {"title": "B.2 Causal Effects in Causal Mediation Analysis", "content": "Then, we introduce important causal estimands in the CMA framework, which characterize the causal\neffects between different variables. Consider the relationships between treatment (A), mediator (M),\nand an outcome (Y), all of them binary variables with values 0 or 1. Depending on the different\nvalues of the treatment and mediator variables, the causal effects between them primarily include the\nfollowing types [61, 20, 27]:\n\u2022 Total Effect (TE):\n$TE = E[Y(A = 1, M(1)) \u2013 Y(A = 0, M(0))]$\n\u2022 Total Direct Effect (TDE):\n$TDE = E[Y(A = 1, M(1)) \u2013 Y(A = 0, M(1))]$\n\u2022 Pure Indirect Effect (PIE):\n$PIE = E[Y (A = 0, M(1)) \u2013 Y(A = 0, M(0))]$\nHere, $Y(A = a, M(a))$ represents the value of Y when A = a and M takes the value it would\nhave when A = a. The total effect (TE) can be decomposed into direct effect and indirect effect\n[61, 20, 27], i.e.,\n$TE = TDE + PIE$\nADCE in Eq. (5) emphasizes deep structure' direct effect on the outcome, controlling mediator s at\npost-intervention state (i,e., s(T = 1)). This control is necessary as changes in d inevitably affect\ns. Thus, with intervention $T = 1$, we can only fix s at s(T = 1) instead of s(T = 0). ADCE\ncharacterized in Equation 5 is actually the Total Direct Effect (TDE), while ICE is in fact the Pure\nIndirect Effect (PIE). Their relationship satisfy Equation 11. For a more understandable notation, we\nuse the simpler concepts of ADCE and ICE in the main text to replace TDE and PIE."}, {"title": "C Probability of Sufficiency, Necessity and Proof", "content": "For two variables X and Y, a sufficient condition is expressed as if X, then Y (X \u2192 Y), implying\nthat the occurrence of X inevitably leads to Y. Conversely, a necessary condition is expressed as Y\nonly if X (Y \u2192 X), indicating that the occurrence of Y presupposes the prior existence of X.\nWe interpret above concepts from the probabilistic perspective, the Probability of Necessity (PN)\nand the Probability of Sufficiency (PS) [32]. PN measures that quantifies the relationship between\ntwo boolean variables X and Y, defined as $PN(x,y) := P(y_{x'}|x, y)$. Here, $y_{x'}$ represents the\ncounterfactual value of Y = y' had X been set to a different value x'. By conditioning on both X = x\nand Y = y, this measure reflects the likelihood of observing a different outcome in the absence of\nthe event X = x. On the other hand, PS is defined as $PS(x, y) := P(y_{x}|x', y')$, which measures the\nprobability that X = x results in Y = y.\nSince PN and PS cannot be estimated through observational data unless Y is monotonic with respect\nto X [62]. Therefore, we assume monotonicity of Y with respect to X and express PN and PS in\ncomputable forms as follows [62, 15]:\n$\\delta_{PN} = \\frac{P(Y = y) \u2013 P(Y = y|do(X = x'))}{P(X = x, Y = y)}$,\n$\\delta_{PS} = \\frac{P(Y = y|do(X = x)) \u2013 P(Y = y)}{P(X = x', Y = y')}$,\nThe monotonicity assumptions and equations provide the foundation for the proof of Theorem 1."}, {"title": "C.2 The Proof Details", "content": "In this section", "that": "n$\\delta_{DCE"}, "frac{\\alpha}{2} \\delta_{PS} + \\frac{\\beta}{2} \\delta_{PN}$\nwhere $\\alpha := P(\\hat{Y} = 1 | T = 1, s(T = 1))$, $\\beta := P(\\hat{Y} = 0 | T = 0, s(T = 0))$.\nProof. We first define two binary variables as: Let T be the treatment variable in Equation 2\n$T=\\begin{cases}\n0 & \\text{intervention alters $s_i$, preserves $d_i$}\\\\\n1 & \\text{intervention alters both $s_i$ and $d_i$}\n\\end{cases}$\nand $\\hat{Y}$ the outcome of the indicator function in Equation 5.\n$\\hat{Y} = \\begin{cases}\n0 & \\text{if $Y_{post} = Y_{pre}$}\\\\\n1 & \\text{if $Y_{post} \\neq Y_{pre}$}\n\\end{cases}$\nwhere $Y_{post}$ is the potential outcome after intervention.\nFollowing assumptions in [62, 15"], "follows": "n$\\delta_{PN}(T = 0, \\hat{Y} = 0) = \\frac{P(\\hat{Y} = 0) \u2013 P(\\hat{Y} = 0|do(T = 1))}{P(T = 0, \\hat{Y} = 0)} = \\frac{P(\\hat{Y} = 0) \u2013 P(\\hat{Y} = 0|T = 1)}{P(T = 0, \\hat{Y} = 0)}$,\n$\\delta_{PS}(T = 0, \\hat{Y} = 0) = \\frac{P(\\hat{Y} = 0|do"}