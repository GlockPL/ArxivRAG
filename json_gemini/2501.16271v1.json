{"title": "FROM MOLECULES TO MIXTURES: LEARNING REPRESENTATIONS OF OLFACTORY MIXTURE SIMILARITY USING INDUCTIVE BIASES", "authors": ["Gary Tom", "Cher Tian Ser", "Ella M. Rajaonson", "Stanley Lo", "Hyun Suk Park", "Brian K. Lee", "Benjamin Sanchez-Lengeling"], "abstract": "Olfaction-how molecules are perceived as odors to humans-remains poorly un-derstood. Recently, the principal odor map (POM) was introduced to digitize the olfactory properties of single compounds. However, smells in real life are not pure single molecules, but complex mixtures of molecules, whose representations remain relatively under-explored. In this work, we introduce POMMIX, an extension of the POM to represent mixtures. Our representation builds upon the symmetries of the problem space in a hierarchical manner: (1) graph neural networks for building molecular embeddings, (2) attention mechanisms for aggregating molecular representations into mixture representations, and (3) cosine prediction heads to encode olfactory perceptual distance in the mixture embedding space. POMMIX achieves state-of-the-art predictive performance across multiple datasets. We also evaluate the generalizability of the representation on multiple splits when applied to unseen molecules and mixture sizes. Our work advances the effort to digitize olfaction, and highlights the synergy of domain expertise and deep learning in crafting expressive representations in low-data regimes.", "sections": [{"title": "INTRODUCTION", "content": "A central challenge in neuroscience is deciphering the link between the physical properties of a stim-ulus and its perceptual characteristics. While this relationship is well-defined for senses like vision (wavelength to color) and audition (frequency to pitch), it remains elusive for olfaction, a chemical sense, where the mapping from chemical structure to odor perception is complex and not fully un-derstood (Sell, 2006; Barwich & Lloyd, 2022; Barwich, 2022). A recent advance towards digitizing olfaction came with the introduction of the Principal Odor Map (POM) by Lee et al. (2023), a high-dimensional, data-driven representation of odor perceptual space learned from molecular structures. This model demonstrated human-level performance in predicting odor qualities of single molecules and generalized well to other olfactory tasks. However, naturally occurring olfactory stimuli are not comprised of single molecules, but rather complex mixtures of molecules, whose representations remain relatively unexplored within the existing literature. This work introduces POMMIX\u2014\u0430 mixture and distance-aware extension of the POM representation.\nA searchable, rankable, and optimizable digital representation of olfactory space has potential ap-plications in diverse areas (Spence et al., 2017). Such a representation could be used to develop mosquito repellents (Wei et al., 2024), inform agricultural practices by enabling targeted manipu-lation of insect behavior (Conchou et al., 2019), improve food quality and reduce waste through enhanced spoilage detection (Jung et al., 2023), and accelerate the design of novel fragrance and flavor compounds, which is particularly valuable given increasing regulatory constraints on existing ingredients (Demyttenaere, 2012; IFRA, 2024)."}, {"title": "RELATED WORKS", "content": "The modeling of molecular structure-property relationships has a rich history. Within the olfactory domain, previous contributions have utilized hand-picked expert descriptors with classical machine learning algorithms (e.g. tree-based models, support vector machine, and linear models), and/or similarity measures (e.g., cosine, angle) (Snitz et al., 2013; Keller et al., 2017; Kowalewski & Ray, 2020; Vigneau et al., 2018). More recently, deep learning based models have been actively explored to create a more expressive molecular representation of olfactory space (Lee et al., 2023; Tran et al., 2018; Zhang et al., 2024; Sisson, 2022; Maziarka et al., 2020).\nDeep learning techniques have been explored in modeling molecular structure-property relation-ships, including variational autoencoders (G\u00f3mez-Bombarelli et al., 2018; Oliveira et al., 2022), large language models (Chithrananda et al., 2020; Ross et al., 2022), and graph neural networks (GNNs) (Yang et al., 2019; Wang et al., 2021). Graph neural networks and graph attention networks (GANs) (Heid et al., 2024; Wu et al., 2023; Buterez et al., 2024) in particular have shown state-of-the-art performance in many molecular property prediction tasks including modeling olfactory space.\nOlfactory mixture property prediction is a much more difficult task with fewer effective attempts (Lapid et al., 2008; Khan et al., 2007; Olsson, 1998; Dhurandhar et al., 2023; Ravia et al., 2020). Molecular mixtures have been studied before for battery electrolytes by Zhang et al. (2023). The work, however, uses a large dataset (10,000 mixtures), and focuses on property prediction, rather than mixture representation learning. To work in the low-data regime of our olfactory mixture dataset, POMMIX uses pre-training techniques (Honda et al., 2019; Shoghi et al., 2023; Goh et al., 2018) and designed inductive biases to improve the expressivity of the molecular representation and attention mechanisms (Wang et al., 2019; Xiong et al., 2020; Maziarka et al., 2020)."}, {"title": "METHODS", "content": "We combine mono-molecular datasets and multi-molecular (mixture) datasets. Mono-molecular datasets list a set of odor labels (\"grassy\", \"fishy\", etc.) for a single molecule, and the most ex-haustive compilation is found in the GoodScents/Leffingwell (GS-LF) dataset (Barsainyan et al., 2024). We further clean the GS-LF dataset by canonicalizing SMILES (Weininger, 1988) strings with RDKIT (Landrum et al., 2022), removing duplicate entries, removing inorganic, charged or multi-molecular (e.g. salts) entries, removing molecules with molecular weight < 20 and > 600, and small inorganic molecules. We further removed infrequently applied odor labels that appeared for fewer than 20 molecules and subsequently removed molecules with no remaining labels (see Appendix A.1 for details on dataset cleaning).\nMulti-molecular datasets were compiled from previous publications, hereby referred to as Snitz (Snitz et al., 2013) (containing data from Weiss et al. (2012)), Ravia (Ravia et al., 2020), and Bush-did (Bushdid et al., 2014). Data for each of these publications was obtained from pyrfume (Castro et al., 2022). In aggregate, we have 743 unique mixtures, containing between 1 to 43 unique molec-ular components (Figure 2a).\nThese mixtures are described by 865 pairwise mixture comparisons (Figure 2b) corresponding to labels from two types of experiments:\n\u2022 Explicit similarity (Snitz, Ravia): Participants are asked to explicitly rate the perceptual similarity of two mixtures from 0 (completely similar) to 1 (completely different). The final similarity for a mixture pair is averaged across all participants.\n\u2022 Triangle discrimination (Bushdid): Participants are provided three mixtures, of which two are identical, and asked to identify which mixture was different. These results are aggregated for each mixture triplet, and the percentage of correct identifications is treated as the label for the two unique mixtures in the triplet.\nWe note that the interpretation of the triangle discrimination task is congruent with the explicit similarity task, as a score of \"1.0\" in a triangle discrimination task shows that all tested participants could identify the mixture that was different, which meant that the two unique mixtures in the triplet"}, {"title": "MODELING", "content": "A schematic of the POMMIX model is provided in Figure 3. The POMMIX model can be divided into three hierarchical components: (1) a mono-molecular GNN POM embedding model, (2) a multi-molecular CHEMIX mixture attention model, and (3) a similarity scoring function.\nThe POM is a GNN which takes in molecular graphs derived from the SMILES representations of molecules. Each graph, written as $G = (U, V, E)$, consists of a special global vertex U connected to all other vertices V, and a set of edges E. The global vertex U encodes overall properties of the molecule and is initialized with 200 normalized RDKIT cheminformatics molecular descriptors (Landrum et al., 2022) obtained from DESCRIPTASTORUS (Kelley et al., 2024). The atoms of the molecules are the vertices (nodes), with node vectors $V = \\{v_i\\}_{i=1}^{N_v}$ for a molecule with $N_v$ atoms, where $v_i$ are 85-dimensional feature vectors encoding atomic properties. Covalent bonds between atoms are represented as edges $E = \\{(e_k, r_k, s_k)\\}_{k=1}^{N_e}$ for a molecule with $N_e$ bonds, where $e_k$ stores a 14-dimensional feature vector of edge properties, and $r_k, s_k \\in [1, ..., N_v]$ are indices that indicate the two atoms that the bond joins together. Note $r_k \\neq s_k$, since bonds must be between two different atoms (see Appendix A.2 for detailed descriptions of node and edge properties).\nThe POM GNN uses the GRAPHNETS architecture (Battaglia et al., 2018), with message-passing blocks for the edge, node and global properties of the molecular graphs. The architecture is designed to be lightweight in order to avoid overfitting on the limited amounts of olfactory mixture data. Edge updates use feature-wise linear modulation (FiLM) layers (Perez et al., 2017; Brockschmidt, 2019), while node updates use graph attention layers (Veli\u010dkovi\u0107 et al., 2017; Brody et al., 2021) with self-attention. The global embeddings are updated through principal neighborhood aggregation (PNA) (Corso et al., 2020; Zaheer et al., 2017). The GNN is composed of four of these GRAPHNET layers, and the final global embedding serves as the POM embedding.\nThe CHEMIX model processes a set of molecular POM embeddings, and generates an embedding representing the entire mixture. Mixtures are first represented by concatenating POM embeddings of constituent molecules, and mixtures with fewer molecules are padded to the length of the largest mixture. CHEMIX uses molecule-wise self-attention, where each molecule attends to all other molecules, followed by PNA. This ensures invariance of the mixture embeddings in the permu-tation of molecules within a given mixture. This model can be viewed as isomorphic to a GAN on a fully-connected graph (Joshi, 2020), with each molecule as a node, and the mixture embedding is the global embedding."}, {"title": "TRAINING AND OPTIMIZATION", "content": "In order to effectively train a deep learning model in a low-data regime, we adopt a transfer learning strategy (Figure 3). The POM GNN is first pre-trained to predict the olfactory binary multi-labels of molecules with binary cross-entropy loss on the GS-LF dataset, using a 80/20 training/validation random split. All training is performed using the Adam optimizer (Kingma & Ba, 2014). To deter-mine the architecture, we perform a Bayesian optimization hyperparameter search to maximize the area under receiver operator curve (AUROC) metric. Early stopping is used to prevent overfitting. The best model achieves a validation AUROC 0.884, in line with previous work (Sanchez-Lengeling et al., 2019). We explore other graph models such as GRAPHORMER (Shi et al., 2022; Ying et al., 2021) and GPS (Ramp\u00e1\u0161ek et al., 2022), but we find the GRAPHNETS architecture to be competitive with the modern state-of-the-art graph models for our dataset (Table A2).\nThe frozen POM embeddings from the pre-trained GNN form the vector representation of mixtures for the CHEMIX model. Again, the architecture is determined through hyperparameter tuning. The training is performed with mean absolute error (MAE) loss on a 80/20 training/validation split of the combined mixture dataset, stratified across Snitz, Ravia, and Bushdid. The stratification process fixes the proportion of each dataset across the splits, ensuring equal representation of any experi-mental differences. To avoid vanishing gradients due to the HardTanh activation, the linear model in the scaled cosine distance prediction head is initialized with bias b = 0.5, and the slope is clamped to ensure m > 0 and maintain the directionality of the cosine distance. Additionally, we ablate the CHEMIX prediction head, and find that the scaled cosine prediction head is optimal for learn-ing mixture embedding similarities (Table A3). Early stopping terminates on maximal validation Pearson correlation coefficient ($\\rho$) between the ground truth labels and the prediction. The optimal model found in the search achieved a maximal $\\rho$ = 0.794 on the validation set. Further details about the hyperparameter tuning for both models are provided in Appendix A.3.\nIn the final stage of training POMMIX, the POM GNN is directly joined to the CHEMIX model, and all model weights are allowed to vary. A lower learning rate is used for the POM GNN model weights, as they are already well-conditioned from pre-training on the larger mono-molecular odor dataset. The results following this section are based on the final POMMIX model. Models were built with PYTORCH (Paszke et al., 2019) and PYTORCH GEOMETRIC (Fey & Lenssen, 2019)."}, {"title": "RESULTS", "content": "We evaluate our approach on the mixture dataset by training and testing on 5-fold cross-validation (CV) splits, stratified across the Snitz, Ravia, and Bushdid datasets. For early stopping, a validation split is randomly split from the training set, producing a final split of 70/10/20 training/validation/test sets. The performances of the models are then evaluated on the test sets.\nWe evaluate POMMIX on a progressive ladder of modeling components. For the simplest base-line, we follow the methods of Snitz et al. (2013), who performed extensive feature selection on molecular descriptors, which are then averaged together for the mixtures (see Appendix A.4). The angle distance between the vector descriptors are then correlated with the experimental results. We perform the same analysis using normalized RDKIT molecular features on our aggregated mixture dataset. We ensure that the feature selection is only performed on the training set.\nWe also provide comparisons with the gradient-boosted random forest XGBOOST model (Chen & Guestrin, 2016), and use features with varying levels of inductive biases (further details in section A.5). Mixture representations are created with PNA-style aggregation of molecular descriptors, including RDKIT features, or the frozen POM embeddings. Additionally, we augment the training"}, {"title": "PREDICTIVE PERFORMANCE", "content": "We report results across three metrics: Pearson correlation coefficient $\\rho$, root-mean-squared error (RMSE), and the Kendall ranking coefficient $\\tau$, each reflecting different strengths of the model. The test results compiled from the CV splits for all models evaluated are shown in Figure 4, with metrics tabulated in Table 1. We show that incorporating more inductive biases into the model leads to a dramatic increase in model performance. The Snitz baseline of angle distances calculated from empirically selected features produces a weak positive correlation with the ground truth distances, but has high RMSE when treated as a regression problem. The XGBOOST model improves upon these predictions, and explicitly models the experimental distances, achieving significantly lower RMSE than the Snitz baseline. However, the correlation and ranking of the mixture similarity is only slightly increased through the use of the boosted RF model. When applying the XGBOOST model to the POM embeddings, we find that the performance is only slightly improved compared to the RDKIT descriptors, signaling that deep learning architectures are needed to extract useful information out of the POM embeddings.\nFor our approaches, CHEMIX shows excellent test performance for predicting olfactory mixture similarities, even when trained with frozen POM embeddings, demonstrating the efficacy of incorpo-rating domain knowledge and inductive biases into model architectures. For POMMIX, we observe further increases in model performance when the POM and CHEMIX are trained end-to-end, further fine-tuning the POM embeddings for use in mixture representations. We note that the end-to-end training results in larger improvements in Kendall $\\tau$ than in $\\rho$. We hypothesize that inherent human noise in the experimental results create a performance ceiling for the model's real-valued predictive capabilities. However, the ranking correlation still improves as it is more robust to experimental noise and outliers (Tom et al., 2024). We further explore this human bias in Section 3.3. Finally, we note that our attempts to augment the dataset with pairs of mono-molecules labeled by their GS-LF odor label Jaccard distances led to modest improvements for the CHEMIX model, but showed no improvements for POMMIX (see Appendix A.8 and Table A5 for details on data augmentation)."}, {"title": "GENERALIZATION TO NEW MIXTURE SIZES AND MOLECULES", "content": "We further study the effects of the inductive biases of the model, and the capabilities of POMMIX in explaining physical olfactory phenomena. In particular, we study the generalization of POMMIX to different splits based on the number of mixture components, and the molecular identities within the mixtures.\nIn Figure 5a, we show the test results of an ablation study, in which the training data is ablated based on thresholds on the geometric average number of components found in a mixture. In other words, for a given threshold, the training set only contains mixtures with components that have a geometric mean number of components less than the threshold, and the test set contains only mixtures that are above the threshold. We observe sufficient generalization capabilities of the model to larger mixtures, achieving performances similar to the RDKIT baselines, even when the training sets are thresholded at ten mixture components, and only about two-thirds of the available training data. We also observe a general increase in test performance, measured by $\\rho$, as the training set grows, indicating that more high quality experimental olfactory mixture data can greatly improve the modeling performance.\nWe observe a significant decrease in performance when considering new chemistries. For Figure 5b, we study POMMIX performance on leave-molecules-out (LMO) splits, in which the test sets are split from the dataset such that certain molecules will not appear in the training set. Note that, unlike the random CV splits, the training sets are not mutually exclusive, since there is significant overlap in the molecular identities across different mixtures. This additional challenge in studying new molecules is an important consideration when validating models, and also planning future mixture similarity experiments. More olfactory mixture data with diverse arrays of molecules can help build better and more generalizable POMMIX representations."}, {"title": "EXPLORING OLFACTORY PHENOMENA WITH POMMIX EMBEDDINGS", "content": "The white noise hypothesis states that intensity-balanced mixtures with an increasingly large num-ber of components become increasingly indistinguishable, even if they share no common molecular components, and approach a scent characterized as an \"olfactory white\" (Weiss et al., 2012). Using the POMMIX embedding, we reproduce the \"olfactory white\" phenomena (Figure 6a). In our inves-tigation, we observe this decrease in POMMIX embedding distances as a function of the geometric mean of components in mixture pairs for our larger dataset, which includes Bushdid and Ravia. This demonstrates the ability of POMMIX in capturing and explaining physiological olfaction phe-nomena, allowing it to build toward an expressive odor perceptual space."}, {"title": "BUILDING INTERPRETATIONS OF MIXTURES", "content": "An unanswered question relevant to mixture modeling is how the mixture components interact with each other and contribute to the prediction of mixture similarities. To probe at this question and generate hypotheses for future investigation, we modified CHEMIX to be more interpretable as an additive model (Agarwal et al., 2021). Specifically we express the self-attention-based mixing com-ponent as a one-layer additive model by using sigmoid normalization (Ramapuram et al., 2024) rather than softmax, allowing the model to attend to all components, and forcing the value vectors to be positive via a ReLU activation. In a simplified way, this is a pairwise interaction model. Although this modified model is simpler and more constrained, it achieves performance comparable to that of our best model.\nIn Figure 7, we showcase how such sigmoidal self-attention maps can be used to analyze the in-formation passing between representations of molecules within a mixture. More complex examples can be found in Appendix A.9. In this simple example, when comparing the GS-LF labels asso-ciated to each molecule (Figure 7a) to the attention weights attributed to each query (Figure 7b), we notice that the strongest \"interaction\"\u2014namely, the highest attributed attention weight\u2014is found between query molecule 1 and key molecule 3. In general, we observe that molecules that are most different from the rest, either by chemical structure (e.g., presence of N or S atom) or by olfactory perception (e.g., presence of rare or numerous labels), tend to have stronger interactions. To further our analysis, we derive label-guided structural heuristics about molecules across the set of unique mixtures in Appendix A.10. We find that higher attention is attributed to chemical structures with"}, {"title": "CONCLUSION AND DISCUSSION", "content": "We introduce POMMIX, an extension of the POM for predicting olfactory similarities between mixtures of molecules. Our approach combines graph neural networks for molecular representation, with attention mechanisms for mixture modeling, and incorporates inductive biases by considering cosine similarities between mixture embeddings to predict olfactory similarity. POMMIX demon-strates state-of-the-art performance, creating meaningful representations of olfactory mixtures, and we show how each component of inductive bias contributes to this performance.\nOur work highlights the value of incorporating domain knowledge and inductive biases, particularly in low-data regimes. By respecting problem-space symmetries, we create a flexible and expres-sive representation for olfactory mixtures, offering a potential solution for modeling other multi-component systems. Furthermore, we provide a method towards interpretable modeling of mixture components interactions studying the attention weights of mixture components in CHEMIX and studying how molecular information attends to itself within a mixture.\nWhile POMMIX shows promising results, we acknowledge several limitations. The small size of the available mixture dataset (< 1,000 samples) raises concerns about overfitting, despite our reg-ularization efforts. Additionally, the limited coverage of chemical odorant space in current public datasets (only ~200 unique odorant compounds) constrains the model's ability to generalize to a wider range of chemical compounds. We also observed challenges in generalizing to new datasets due to potential distribution shifts from varying experimental setups and human biases. Despite"}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have made significant efforts to ensure our methodology can be replicated by other researchers. All data and code used are provided in https://github.com/chemcognition-lab/pom-mix. We use open-source software, including PYTORCH, PYTORCH GEOMETRIC, and RDKIT. Our manuscript details the model architecture, training procedures, and evaluation metrics. We outlined our data sources and preprocessing steps, including specific criteria for removing molecules and odor labels. Details on dataset cleaning are provided in Appendix A.1, model details are provided in Section 2.2 and Appendix A.2, and the training process and hyperparameter tuning are provided in Section 2.3 and Appendix A.3, respectively. Additionally, the splits used for all experiments are provided as well. We are committed to ensuring other researchers can build upon our findings and verify our results."}]}