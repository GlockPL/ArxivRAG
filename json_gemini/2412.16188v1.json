{"title": "A Decade of Deep Learning: A Survey on The Magnificent Seven", "authors": ["Dilshod Azizov", "Muhammad Arslan Manzoor", "Velibor Bojkovi\u0107", "Yingxu Wang", "Zixiao Wang", "Zangir Iklassov", "Kailong Zhao", "Liang Li", "Siwei Liu", "Yu Zhong", "Wei Liu", "Shangsong Liang"], "abstract": "During the past decade\u00b9, deep learning algorithms have revolutionized the field of artificial intelligence (AI), leading to significant advancements in various domains. At the core of this transformation is the development of multi-layered neural network architectures that facilitate automatic feature extraction from raw data, significantly improving the efficiency on machine learning tasks. Given the rapid pace of these advancements, an accessible manual is necessary to distill the key advances of the past decade. With this in mind, we introduce a study which highlights the evolution of deep learning, largely attributed to powerful algorithms. Among the multitude of breakthroughs, certain algorithms, including Residual Networks (ResNets), Transformers, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Graph Neural Networks (GNNs), Contrastive Language-Image Pre-training (CLIP) and Diffusion models, have emerged as the cornerstones and driving forces behind the discipline. We select these algorithms via a survey targeting a broad spectrum of academics and professionals with the aim of encapsulating the essence of the most influential algorithms over the past decade. In this work, we provide details on the selection methodology, exploring the mentioned architectures in a broader context of the history of deep learning. We present an overview of selected core architectures, their mathematical underpinnings, and the algorithmic procedures that define the subsequent extensions and variants of these models, their applications, and their challenges and potential future research directions. In addition, we explore the practical aspects related to these algorithms, such as training and optimization methods, normalization techniques, and rate scheduling strategies that are essential for their effective implementation. Therefore, our manuscript serves as a practical survey for understanding and applying these crucial algorithms and aims to provide a manual for experienced researchers transitioning into deep learning from other domains, as well as for beginners seeking to grasp the trending algorithms.", "sections": [{"title": "Introduction", "content": "In the field of artificial intelligence (AI), deep learning (DL) algorithms have advanced significantly in the last decade, paving the way for advances in other areas Shrestha & Mahmood (2019) such as Computer Vision (CV), Natural Language Processing (NLP), Speech Recognition (SR), Robotics, etc. The core of deep learning, which is built on multi-layered neural network architectures, has caused a paradigm change by making it easier to automatically extract features and patterns from unprocessed data, thus improving the efficiency of machine learning tasks Najafabadi et al. (2015). Further, the study by LeCun et al. (2015)\n\nOur contributions are as follows:\n\n1. We discuss the evolution of deep learning and our methodology for selecting the top seven deep learning algorithms of the past decade."}, {"title": "Evolution of Deep Learning", "content": "The conceptual foundation for neural networks was laid in 1943 by McCulloch & Pitts (1943) with the idea that neurons could be simulated with electrical circuits, proposing a model for artificial neurons. This early model demonstrated the potential to build computing systems that mimic biological processes. The subsequent development by Hebb (1949) introduced the concept of strengthening neural pathways through repeated use, laying the foundation for learning algorithms. In 1958 Rosenblatt (1958) introduced the perceptron, an early neural network model capable of simple pattern recognition. This model had two layers of processing units and demonstrated early success in tasks such as image and pattern recognition Rumelhart et al. (1986). However, the initial excitement was dampened by Minsky & Papert (1969) who illustrated the perceptron inability to solve simple XOR functions, highlighting its limitations and ushering in a period of reduced interest and funding, known as the AI winter. A major breakthrough came with the introduction of the backpropagation algorithm in the 1970s Linnainmaa (1970). Its implications for deep neural network training were not fully appreciated and utilized until later Rumelhart et al. (1986); LeCun et al. (1988). This\n\nalgorithm enabled the training of multi-layer networks and facilitated the learning of complex datasets by adjusting the weights in response to errors, revitalizing the interest in neural networks Hinton et al. (1986); Schmidhuber (2015). The late 1980s and 1990s saw the development of various specialized neural network architectures. The neocognitron, introduced in 1988, was a hierarchical neural network that greatly improved visual pattern recognition Fukushima (1988). The introduction of long-short-term memory (LSTM) networks and their subsequent applications underscored the ability of deep learning to handle not only single data points, but also entire data sequences Hochreiter & Schmidhuber (1997); Gers et al. (2000). Following this, Yann LeCun's work in 1998 on convolutional neural networks (CNNs) with backpropagation marked significant progress in document analysis and image processing LeCun et al. (1998). Additional studies in this period contributed to the refinement and widespread application of these architectures Krizhevsky et al. (2012); Simonyan & Zisserman (2014).\n\nThe term \"Deep Learning\" was popularized in the 2000s, particularly with the development of Deep Belief Networks (DBNs) Hinton et al. (2006). These networks used a layer-wise pre-training technique that significantly improved the efficiency of deep network training Hinton et al. (2006); Bengio et al. (2007). This period also witnessed the increasing use of General-Purpose Graphics Processing Units (GPGPUs), which provided the computational power necessary to train complex models on large datasets Schmidhuber (2015); Raina et al. (2009). By 2012, deep learning began to dominate AI, driving progress in various fields, including SR, NLP, and CV. This era also marked the rise of large-scale data (big data), which further accelerated deep learning research and applications Kaisler et al. (2013).\n\nOne notable example of applications of these developments is Google AlphaGo, which demonstrated impressive deep learning capabilities. At the beginning of 2017 Silver et al. (2017), the computer program, under the pseudonym \u201cMaster,\" won three consecutive online games against professional human Go players, including a remarkable victory over Ke Jie. Using cutting-edge deep learning algorithms and extensive hardware resources, AlphaGo showcased its ability to defeat world-champion Go players Lee (2018).\n\nDeep learning has revolutionized various fields, including NLP, CV, and beyond, through the introduction of innovative architectures such as ResNets, Transformer, GANs, VAEs, GNNs, CLIP, diffusion models. These advances have allowed the development of groundbreaking models such as BERT Devlin et al. (2018) and the GPT series Radford et al. (2018); Brown et al. (2020), which significantly improved language understanding and generation capabilities. The emergence of efficient techniques, such as AutoML and neural architecture search, has further simplified model design and deployment processes. More recently, few-shot learning has gained prominence, with models like GPT-40 Achiam et al. (2023) and Llama 3.1 (8B, 70B and 405B) Dubey et al. (2024) showcasing remarkable generalization abilities from limited examples. Additionally, there is an increasing focus on sustainable and efficient training methods to mitigate the environmental impact of large-scale model training Schwartz et al. (2020). The integration of deep learning into edge computing has also broadened its applications, enabling AI accessibility on a variety of devices Zhou et al. (2019).\n\nDeep learning has evolved from foundational models to sophisticated systems, driving transformative impacts across industries. This growth is fueled by global collaboration and ongoing innovations Smith & Lee (2020). Ethical considerations, including data bias and privacy, remain critical and require responsible AI practices Doe & Row (2021)."}, {"title": "Methodology", "content": "The methodology used in this study is designed to ensure a robust and credible examination of the algorithms selected within the context. The process is divided into distinct phases, including data collection, analysis, and algorithm selection, each of which is delineated below in \u00a7 3.1."}, {"title": "Selection Criteria for the Seven Algorithms", "content": "This study began by selecting the top algorithms that have been significantly influenced over the past decade through a survey designed to gather insights from a diverse group of experts in AI and computer science. The questionnaire included four questions (see Figure 1). Figure 2a displays the distribution of participants in universities and countries, with a majority of contributors from Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and Sun Yat-sen University. Figure 2b illustrates that most of the respondents are Ph.D. and M.Sc. students. Figure 2c shows the levels of experience, with most having 1-3 years and 3-5 years of expertise. The total group of 100 respondent consisted of faculty, research assistants, postdoctoral researchers, and students at various educational stages, ensuring a breadth of perspectives by distributing the survey across multiple academic institutions.\n\nParticipants were asked to provide the five most significant algorithms in their view. Following the aggregation of responses, the top 12 algorithms were determined based on the collective vote of the participants. The next step was to carefully investigate the chronological beginnings of these algorithms in order to ascertain that they were created during the past decade. Several algorithms were eliminated as a result of this examination because the algorithms mentioned by the respondents were not introduced in our considered timeline (e.g., CNN and LSTM), leaving a final list of our seven algorithms \u2013 The Magnificent Seven.\n\nThe second phase of the methodology went to an in-depth examination of the selected algorithms. The research impact of each algorithm was measured using the citation score of the seminal articles, obtained from a reputable academic database Google Scholar. This measure served as a proxy for the algorithm's influence and acceptance within the community. Finally, we introduce each algorithm based on its citation score from Google Scholar and introduce them accordingly in our study."}, {"title": "Algorithms", "content": "This section explores seven influential deep learning algorithms\u00b3 - ResNets, Transformer, GANS, VAES, GNNS, CLIP, Diffusion Models - that have reshaped the landscape of AI research and applications in NLP, CV, and generative modeling."}, {"title": "ResNets", "content": "Overview. Introduced by He et al. (2016a) Residual Networks (ResNets) represent a groundbreaking development in deep learning architectures. This architecture addresses the problem of training very deep neural networks by using skip connections or shortcuts to jump over some layers. Typical ResNets are composed of repeated blocks that have these residual connections, which allows training of networks that are substantially deeper than those used previously He et al. (2016b). This design mitigates the problem of vanishing gradients by allowing the gradient to be directly backpropagated to earlier layers. The effectiveness of ResNets has been demonstrated in a broad range of applications, from image recognition He et al. (2016a) and object detection Ren et al. (2015) to semantic segmentation Chen et al. (2017) and medical image analysis Litjens\n\nCore Architecture. The ResNet architecture (see Figure 3) is designed\nto allow the training of extremely deep neural networks that can have\nhundreds or even thousands of layers efficiently. This is achieved through\nthe use of residual blocks, which are the fundamental components of the\nResNets architecture. The architecture can be described as follows:\n\n\u2022 Residual Blocks: Each residual block in a ResNets contains two\nmain paths. The first path is the weight layer path, typically con-\nsisting of two or three convolutional layers depending on the vari-\nant (e.g., ResNet-50, ResNet-101). These layers are followed by\nbatch normalization layers and ReLU activation functions. The\nsecond path is the shortcut connection that skips these layers.\n\n\u2022 Shortcut Connections: The key innovation in ResNets is the\nshortcut connection that skips one or more layers. Shortcut con-\nnections simply perform identity mapping, and their outputs are\nadded to the outputs of the stacked layers. This design addresses\nthe vanishing gradient problem by allowing gradients to flow di-\nrectly through the network during the backward pass.\n\nThis architectural innovation enables ResNets to achieve higher perfor-\nmance and faster training with deeper networks, leading to better perfor-\nmance in a variety of tasks such as image classification, object detection,\netc.\n\nMathematical Foundations. ResNets introduce a fundamental shift in\nhow layer inputs and outputs are handled in deep neural networks.\n\nResidual Learning Function. The key innovation in ResNets is the resid-\nual learning function, which is designed to make it easier to optimize\ndeeper networks. The function is mathematically formalized through the\nfollowing equation:\n\n$y = F(x, {W_i}) + x.$\n\nHere, x and y are the input and output of the layers considered. The function $F(x, {Wi})$ represents the\nresidual mapping to be learned. For each layer, instead of trying to learn an underlying mapping directly,\nResNets learns the difference F between the input and output, which is theoretically easier to optimize,\nespecially in very deep networks.\n\nIdentity Shortcut Connection. The addition of the shortcut connection x to the output of the residual function\nF helps to address the problem of vanishing gradients by allowing gradients to flow directly through the\nidentity function:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y}(1+\\frac{\\partial F}{\\partial x}).$\n\nThis ensures that the gradient does not vanish quickly as it is propagated through many layers, making it\npossible to train networks with much greater depth.\n\nLayer Normalization. Each residual block in ResNets typically includes a batch normalization step after each\nconvolutional layer. This normalization helps in stabilizing the learning process and reduces the sensitivity\nof the network to different initialization schemes.\n\nLoss Functions and Variants. Standard ResNets typically use the cross-entropy loss function for classification\ntasks, but modifications and variants have been introduced, such as ResNets variants include pre-activation\nResNets, modifying block operations for better training dynamics He et al. (2016b), and ResNeXt, using\ngrouped convolutions to boost capacity and efficiency without added complexity Xie et al. (2017) to address\nspecific challenges.\n\nAlgorithmic Procedure. The training of ResNets involves a systematic process that leverages the unique\narchitectural features of residual blocks to facilitate the training of very deep networks. The following outlines\nthe step-by-step procedure for training ResNets:\n\n1. Input Processing: Each input image is initially processed by a convolutional layer, which is\ntypically followed by batch normalization and a ReLU activation function to prepare the data for\nsubsequent layers.\n\n2. Residual Block Processing:\n\n3. Convolutional Layers: Each residual block contains several convolutional layers. These layers apply\na series of filters to the input data and are each followed by batch normalization and ReLU activation.\n\n4. Shortcut Connection: Parallel to the convolutional layers, there is a shortcut connection that carries\nthe input directly to the end of the residual block. This helps mitigate the vanishing gradient\nproblem by allowing gradients to flow through the network without significant attenuation.\n\n5. Element-wise Addition: The output of the last convolutional layer in the block and the shortcut con-\nnection are added element-wise. This combination is then passed through another ReLU activation\nfunction.\n\n6. Repeat Block Processing: The processed output of one residual block is then fed into the next.\nThis sequence is repeated across all residual blocks in the network. Depending on the version of\nResNets (e.g., ResNet-34, ResNet-50, ResNet-101), the depth and number of blocks will vary.\n\n7. Final Layers: After all residual blocks have processed the input data, the output is passed through\na global average pooling layer, followed by a fully connected layer that acts as the classifier.\n\nTraining and Optimization. Training ResNets effectively involves optimizing several key aspects to ensure\nmodel stability and high performance. Key to ResNets' training is the use of batch normalization, which\nnormalizes the input layer by adjusting and scaling activations. This helps mitigate the internal covariate\nshift problem, leading to much faster convergence and stabilizing the training process across very deep\nnetworks Ioffe & Szegedy (2015). Other optimizations include:\n\n\u2022 Residual Learning: The design of shortcut connections greatly reduces the risk of vanishing gra-\ndients, facilitating the training of networks that are significantly deeper than previous architectures.\n\n\u2022 Learning Rate Scheduling: Gradual adjustment of the learning rate, such as using a step decay,\nwhere the learning rate is reduced by a factor every few epochs, helps achieve a lower loss and better\naccuracy He et al. (2016a).\n\n\u2022 Weight Initialization: Careful initialization of weights, often using methods such as He initializa-\ntion, which is tailored for layers followed by ReLU activations, supports the training of deep models\nby preventing the problem of exploding or diminishing gradient magnitudes He et al. (2015).\n\nExtensions and Variants. Since its inception, various extensions and variants of ResNets have been devel-\noped to address specific needs and improve the efficiency of the architecture. These include ResNeXt, which"}, {"title": "Transformers", "content": "Overview. The introduction of the Transformer architecture by Vaswani et al. (2017) marked a significant\nevolution in the field of machine learning, particularly in the processing of sequential data. This architecture\neffectively addresses the limitations of previous sequence processing models, such as recurrent neural networks\n(RNNs) Rumelhart et al. (1986) and long-short-term memory networks (LSTMs) Hochreiter & Schmidhuber\n(1997), by eliminating the need for sequential data processing. This innovation has significantly improved\nperformance across a broad spectrum of applications, including NLP, CV Nguyen et al. (2024); Carion et al.\n(2020); Wang et al. (2021); Liu et al. (2021b); Arnab et al. (2021); Manzoor et al. (2020); Vayani et al.\n(2024) and SR Dong et al. (2018); Karita et al. (2019); Li et al. (2019); Tian et al. (2019); Miao et al. (2020),\nfundamentally changing our approach to model architecture in various domains.\n\nCore Architecture. The transformer model eschews traditional recur-\nrent layers and instead relies entirely on an attention mechanism to draw\nglobal dependencies between input and output. The architecture of a\ntransformer is divided into two main parts as shown in Figure 4: encoder\nand decoder.\n\nThe encoder maps an input sequence of symbol representations $(x_1,...,x_n)$\nto a sequence of continuous representations z. Each layer of the encoder\nconsists of two sub-layers: the first is a multi-head self-attention mech-\nanism, and the second is a simple, position-wise fully connected feed-\nforward network. Vaswani et al. (2017) introduce residual connections\naround each of the two sub-layers, followed by layer normalization.\n\nThe decoder is also made up of a series of identical layers. In addition\nto the two layers in each encoder layer, each decoder layer has three sub-\nlayers: a self-attention mechanism, an encoder-decoder attention mecha-\nnism, and a feed-forward neural network (FFNN). The encoder-decoder\nattention mechanism performs multi-head attention over the encoder's\noutput. Similarly to the encoder, residual connections are added around\neach sub-layer, followed by layer normalization.\n\nMathematical Foundations. The self-attention mechanism at the\nheart of the transformer uses scaled dot-product attention. The atten-\ntion function can be described mathematically as mapping a query and\na set of key-value pairs to an output, where the query, keys, values, and\noutput are all vectors. The output is computed as a weighted sum of the\nvalues, where the weight assigned to each value is computed by a com-\npatibility function of the query with the corresponding key. Specifically,\nattention weights are computed using the following equation:\n\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}),$\n\nwhere Q, K, and V are the queries, keys, and value matrices, respectively, and $d_k$ is the dimension of the\nkeys.\n\nThe transformer model, with its self-attention mechanism, excels in processing sequences with long-range\ndependencies. This architecture has improved language models and inspired adaptations in image recognition\nand speech processing.\n\nAlgorithmic Procedure The transformer model operates via a distinct sequence of steps within its encoder-\ndecoder architecture:\n\n1. Input Processing: Each input token is converted into a vector through embedding layers. Positional\nencodings are added to these embeddings to incorporate information about the position of each\ntoken within the sequence.\n\n2. Encoder Layer Processing:\n\n(a) Multi-Head Self-Attention: The encoder uses self-attention mechanisms to process the input.\nThis involves multiple attention heads computing attention scores simultaneously, allowing the\nmodel to capture different types of relationships between words in the input sequence.\n\n(b) Add & Norm: The outputs from the self-attention layer is added to the original embeddings\n(residual connection) and normalized.\n\n(c) Feed-Forward Networks: The normalized output is then passed through a point-wise feed-\nforward network, which is applied to each position separately and identically.\n\n(d) Add & Norm: Another residual connection followed by layer normalization is performed after\nthe feed-forward network.\n\n3. Decoder Layer Processing: Similar to the encoder, but with an additional step:\n\n(a) Masked Multi-Head Attention: To prevent positions from attending subsequent positions,\nmasked self-attention is used in the decoder.\n\n(b) Encoder-Decoder Attention: Attention mechanisms are used that focus on the output of the\nencoder stack, helping the decoder to focus on the appropriate parts of the input sequence.\n\n(c) Feed-Forward Networks and Normalization: As in the encoder, feed-forward and normalization\nsteps are performed.\n\n4. Output Generation: The decoder output is transformed into predicted output tokens, typically\nthrough a linear transformation followed by a softmax layer to predict the probability of each token\nin the vocabulary.\n\nTraining and Optimization. Training the transformer model involves using the standard mini-batch gra-\ndient descent method combined with the Adam optimizer Kingma & Ba (2014). The learning rate scheduling,\ncrucial for stabilizing the training early on, is particularly specified as follows:\n\n$lr = H^{-0.5}. min(S^{-0.5}, S. W^{-1.5}),$\n\nwhere: lr: learning rate, H = $d_{model}$: dimensionality of the model hidden layers, S = step_num: current\ntraining step number, W = warmup_steps: number of steps during the warm-up phase. This learning rate\nschedule combines the effects of the: inverse square root decay, controlled by $S^{-0.5}$ and linear scaling during\nthe warm-up phase, given by $SW^{-1.5}$.\n\nExtensions and Variants. Since its inception, the transformer architecture has inspired a myriad of vari-\nants aimed at enhancing efficiency, scalability, and applicability across different modalities. The most notable\namong these are BERT (Bidirectional Encoder Representations from transformers) for natural language\nunderstanding tasks Devlin et al. (2018), and GPT (Generative Pre-trained Transformer) for generative\ntasks Radford et al. (2018). The adaptability of transformers has been extended to computer vision with the\nintroduction of vision transformers (ViT) Nguyen et al. (2024) and to video processing with Video Vision\ntransformers (ViViT) Arnab et al. (2021). Further advancements include efficient transformers such as Lin-\nformer, which reduces the computational complexity significantly Wang et al. (2020b), and the Performer,\nwhich provides an efficient approximation of self-attention for very long sequences Choromanski et al. (2020).\nThe emergence of cross-modal transformers, such as Perceiver, highlights the versatility of the transformer\narchitecture, enabling it to handle various data types, including images, audio, and text Jaegle et al. (2021).\nSimilarly, emerging large language models (LLMs), such as GPT-40 and Llama3.1 (405B) Achiam et al.\n\nApplications. Transformers have become instrumental in various domains since their introduction, demon-\nstrating their versatility and capability in numerous applications. These include natural language processing,\nwhere they have set new performance benchmarks in various NLP challenges (e.g., fake news detection) Az-\nizov et al. (2024), including language models such as GPT-40 and Llama 3.1 (8B, 70B and 405B). Ad-\nditionally, transformers have improved the accuracy of speech recognition, exemplified by models such as\nthe Conformer Gulati et al. (2020), HuBERT Hsu et al. (2021) and Whisper Radford et al. (2022; 2023).\nFurthermore, vision transformers (ViTs) Nguyen et al. (2024) and applications in object detection with\nDETR Carion et al. (2020) demonstrate that transformers can effectively process and understand visual\ndata, rivaling and sometimes surpassing advanced CNN architectures.\n\nChallenges & Future Directions. Despite their widespread adoption, transformers face several challenges\nthat provide key areas for future research and innovation. computational demand is a major limitation due to\nthe quadratic scaling of self-attention with sequence length Kitaev et al. (2020). Addressing this, future work\nis focusing on techniques like sparse attention mechanisms and advancements in specialized AI hardware to\nreduce computational and memory requirements.\n\nData efficiency remains a hurdle, as transformers require vast amounts of data to train effectively, limiting\ntheir applicability in data-scarce scenarios Brown et al. (2020); Patil & Gudivada (2024). Future directions\ninclude developing more data-efficient training strategies, such as transfer learning and semi-supervised\nlearning, to enhance performance in low-data environments.\n\nInterpretability is another significant challenge, as transformers' complex architectures make decision-making\nprocesses opaque Voita et al. (2019); Singh et al. (2024). Research into explainability frameworks aims to\naddress this by making model decisions more transparent and justifiable, particularly in critical applications\nlike healthcare and autonomous systems.\n\nFinally, generalization across tasks remains limited without extensive task-specific fine-tuning Jiang et al.\n(2020); Liu et al. (2024b); Zhang et al. (2024b). Future work is exploring universal pretraining methods and\nmodular architectures to enhance adaptability across diverse tasks and domains.\n\nEmerging directions also include integrating transformers into multi-modal models to unify text, image, and\nother data types, unlocking new applications in fields such as genetics and video processing. Furthermore, a\nfocus on sustainable AI aims to reduce the environmental impact of training large-scale transformers through\nenergy-efficient algorithms and carbon-conscious training practices.\n\nSummary. Transformers have revolutionized machine learning with their powerful capabilities to handle\nsequence data in various applications. Introduced by Vaswani et al. (2017), this architecture has replaced\nrecurrent layers with a more efficient self-attention mechanism, facilitating faster training and better handling\nof long-range dependencies. The transformers have not only established new standards in NLP, but have\nalso expanded their influence to fields such as CV Nguyen et al. (2024) and SR Gulati et al. (2020)."}, {"title": "GANS", "content": "Overview. Generative Adversarial Networks (GANs), introduced by Goodfellow et al. (2014), represent\na transformative approach in machine learning, allowing the synthesis of highly realistic data in diverse\ndomains. These networks comprise two competing models: a generator which attempts to generate data\nindistinguishable from genuine data, and a discriminator which aims to distinguish between real and gener-\nated data. This adversarial process improves through iterative training, where both models enhance their\ncapabilities in a dynamic equilibrium Karras et al. (2019); Brock et al. (2018). Recent advances have ex-\npanded GAN applications from simple image generation to complex tasks such as super-resolution, style\ntransfer, and synthetic data augmentation for training other machine learning models Wang et al. (2018);\nChoi et al. (2020). Furthermore, GANs have been pivotal in unsupervised learning, providing a powerful\ntool to discover intricate data distributions without labeled datasets Chen et al. (2016). Current research\ncontinues to address the challenges of training stability and mode collapse, with novel architectures such as\n\nCore Architecture. The architecture (see Figure 5) of GANs consists of two neural networks, termed the\ngenerator and the discriminator, which are trained simultaneously through an adversarial process. The\narchitecture can be described as follows:\n\n\u2022 Generator (G): The generator network functions as a data synthesizer, taking as input a random\nnoise vector z from a predefined noise distribution $p_z(z)$, often Gaussian or Uniform. Its objective\nis to transform this noise into data x that are indistinguishable from the genuine data points. The\ngenerator's success is measured by its ability to deceive the discriminator.\n\n\u2022 Discriminator (D): The discriminator network acts as a binary classifier to distinguish between\nauthentic data drawn from the true data distribution $P_{data}(x)$ and synthetic data produced by the\ngenerator. Its goal is to accurately identify the source of each data point it evaluates.\n\nMathematical Foundations. GANs employ a game-theoretic approach in which two neural networks, the\ngenerator (G) and the discriminator (D), engage in a continuous adversarial game. The objective functions\nfor each network define their roles and guide their training. In the following, we detail the mathematical\nprinciples that form the basis for GANs.\n\nAdversarial Objective Function. The interaction between the generator and the discriminator is mathemat-\nically formalized through the following adversarial objective function:\n\n$min_G max_D V (D, G) = E_{x\\sim p_{data}(x)}[log D(x)] + E_{z\\sim p_z(z)}[log(1 \u2013 D(G(z)))].$\n\nHere, V(D, G) represents the value function for the GANs, which encapsulates the adversarial game:\n\n\u2022 Discriminator's Objective: The discriminator aims to maximize V(D, G), by trying to assign\nthe correct label to both real data and generated data. The first term, $E_{x\\sim p_{data}(x)} [log D(x)]$, expects\nthe discriminator to recognize real data (from the dataset) as real, thus maximizing the probability\nD(x) for data x coming from the data distribution $p_{data}(x)$.\n\n\u2022 Generator's Objective: The generator, on the other hand, attempts to minimize V(D,G) by\ntrying to fool the discriminator. The second term, $E_{z\\sim p_z(z)}[log(1 \u2013 D(G(z)))]$, measures the gener-\nator's success in deceiving the discriminator into believing that the generated data G(z) are real.\nEssentially, it tries to minimize the probability that D recognizes G(z) as fake.\n\nEquilibrium and Convergence. The adversarial training aims to reach a Nash Equilibrium where neither the\ngenerator nor the discriminator can improve unilaterally. At this point, the generator produces data that"}, {"title": "VAES", "content": "Overview. Variational Autoencoders (VAEs) represent a class of deep learning models that provide a proba-\nbilistic way to describe an observation in latent space. Introduced by Kingma & Welling (2013) and Rezende\net al. (2014)", "follows": "n\n\u2022 Encoder: The encoder part of a VAE maps the input data to a distribution over the latent space.\nTypically", "vectors": "one for the mean and one for the variance of a Gaussian distribution Kingma\n& Welling (2013).\n\n\u2022 Latent Space: The latent space is characterized by the mean and variance vectors produced by the\nencoder. A sample is drawn from this space using the reparameterization trick", "Decoder": "The decoder uses the sampled latent vector to reconstruct the input data. Similarly to the\nencoder", "relation": "n\n$log p(x) \u2265 L(q) = E_{q(z|x)"}, ["logp(x|z)"], "D_{KL}(q(z|x)||p(z)),$\n\nwhere x represents the data, z is the latent variable, q(z|x) is the approximate posterior, and p(z) is the\nprior over the latent variables. The function L(q) is known as the evidence lower bound (ELBO), which\nthe VAE optimizes. The first term of the ELBO encourages the decoder to reconstruct the data accurately,\nwhile the second term, the KL divergence, acts as a regularizer, enforcing latent variables to approximate\nthe prior distribution Kingma & Welling (2013); Rezende et al. (2014).\n\nThese principles allow VAEs not only to model the data, but also to generate new data instances by sampling\nfrom the latent space, making them powerful tools for unsupervised learning in a variety of applications.\n\nAlgorithmic Procedure. The training of a VAE involves the following steps:\n\n1. Encoding: The input data is passed through the encoder, which computes the parameters to the\napproximate posterior distribution over the latent variables.\n\n2. Sampling: A latent sample is drawn from the approximate"]}