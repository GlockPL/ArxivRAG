{"title": "FORM: Learning Expressive and Transferable First-Order Logic Reward Machines", "authors": ["Leo Ardon", "Daniel Furelos-Blanco", "Roko Para\u0107", "Alessandra Russo"], "abstract": "Reward machines (RMs) are an effective approach for addressing non-Markovian rewards in reinforcement learning (RL) through finite-state machines. Traditional RMs, which label edges with propositional logic formulae, inherit the limited expressivity of propositional logic. This limitation hinders the learnability and transferability of RMs since complex tasks will require numerous states and edges. To overcome these challenges, we propose First-Order Reward Machines (FORMs), which use first-order logic to label edges, resulting in more compact and transferable RMs. We introduce a novel method for learning FORMs and a multi-agent formulation for exploiting them and facilitate their transferability, where multiple agents collaboratively learn policies for a shared FORM. Our experimental results demonstrate the scalability of FORMS with respect to traditional RMs. Specifically, we show that FORMS can be effectively learnt for tasks where traditional RM learning approaches fail. We also show significant improvements in learning speed and task transferability thanks to the multi-agent learning framework and the abstraction provided by the first-order language.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, reinforcement learning [RL; 27] has emerged as a powerful technique to train autonomous agents able to reach superhuman performance across a wide range of applications, including games [18, 23, 34, 35], autonomous driving [13, 20], finance [3, 24], and science [22, 25]. Despite these successes, RL struggles to handle tasks that require long-term planning and abstraction, particularly when rewards are non-Markovian, i.e. they depend on histories of states and actions. Reward machines [RMs; 31] offer a promising solution by leveraging finite-state machines to encode the temporal structure of tasks and allow agents to handle non-Markovian rewards. However, traditional RMs rely on propositional logic to label transitions, which limits their expressivity and scalability. Abstracting over object properties (e.g., colour or type) is unfeasible with propositional logic: it requires all combinations to be encoded in the RM, leading to a combinatorial explosion in the number of states and edges. As a result, RMs are difficult to learn and transfer across different tasks, especially as the complexity of the tasks increases.\nIn this paper, we make the following contributions. (i) We introduce First-Order Reward Machines (FORM), a novel formulation of RMs that uses first-order logic to label transitions, enhancing expressivity and transferability. (ii) We propose a new RM learning method which is, to the best of our knowledge, the first to tackle the problem of learning RMs with first-order logic. (iii) We formalise the exploitation of an RM as a multi-agent problem, where multiple agents collaboratively learn the policies of the RM. Finally, (iv) we demonstrate empirically the benefit of our approach over traditional RM learning methods, in both learning and transferability.\nThe paper is organised as follows. Section 2 introduces the background of our work. Section 3 formalises FORMS, and describes our methods for learning and exploiting them. We evaluate our methods' performance and the reusability of FORMs in Section 4. Section 5 discusses related work, and Section 6 concludes the paper."}, {"title": "2 BACKGROUND", "content": "In this section, we cover three basic topics needed to understand our approach. We discuss reinforcement learning and a state-of-the-art algorithm which we use to learn policies. We present the notion of a reward machine which we build upon and generalise it to a First-Order Reward Machine (FORM). Finally, we discuss the Learning from Answer Sets framework, which we use to learn FORMs."}, {"title": "2.1 Reinforcement Learning", "content": "Reinforcement learning [RL; 27] is a machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment, often modelled as a Markov decision process (MDP). The agent's objective is to learn a policy \\( \\pi \\), a mapping from states to actions, that maximizes the expected cumulative discounted reward (called return) \\( R = \\sum_{t=0}^{T} \\gamma^t r_t \\), where \\( r_t \\in \\mathbb{R} \\) is the reward received at time-step \\( t \\), \\( \\gamma \\in [0, 1) \\) is the discount factor, and \\( T \\) is the time horizon. The agent observes the current state \\( s_t \\) of the environment, takes an action \\( a_t \\sim \\pi(s_t) \\) according to its current policy \\( \\pi \\), transitions to a new state \\( s_{t+1} \\sim p(s_t, a_t) \\) according to the environment transition function \\( p \\), and receives a reward \\( r_t = r(s_t, a_t, s_{t+1}) \\) from the reward function \\( r \\).\nProximal policy optimization [PPO; 21] is a state-of-the-art RL algorithm that, based on the policy gradient theorem [28], directly learns a policy \\( \\pi(a_t|s_t) \\) as an action distribution conditioned on a state \\( s_t \\). PPO optimizes a surrogate objective function that balances exploration and exploitation; specifically, PPO uses a clipped probability ratio to constrain the policy update, ensuring that the new policy does not deviate too much from the old policy, preventing large and destabilizing changes. PPO has the advantage of being robust and effective on a wide range of tasks while requiring minimal hyperparameter tuning [37]."}, {"title": "2.2 Learning from Answer Sets", "content": "Answer set programming [ASP; 12] is a declarative programming language for knowledge representation. A problem is expressed in ASP using logic rules, and the models (known as answer sets) of its representation are its solutions. Within the context of this paper, an ASP program \\( P \\) is a set of normal rules. Given any atoms \\( h, b_1, ..., b_n, c_1, ..., c_m \\), a normal rule is of the form \\( h :- b_1, ..., b_n, not c_1, ..., not c_m \\) where \\( h \\) is an head, \\( b_1, ..., b_n, not c_1, ..., not c_m \\) is (collectively) the body of the rule, and \"not\" represents negation as failure. An atom is ground if it is variable free. Informally, given a set of ground atoms (or interpretation) \\( I \\), a ground normal rule is satisfied if the body is not satisfied by \\( I \\) or the head is satisfied by \\( I \\). The reader is referred to [11] for further details on the semantics of ASP programs.\nILASP [14] is a state-of-the-art inductive logic programming system for learning ASP programs from partial answer sets. A program \\( P \\) is said to accept an example \\( e \\) if and only if there exists an answer set \\( A \\) of \\( P \\cup e \\). An ILASP task [15] is a tuple \\( T = (B, SM, E) \\), where \\( B \\) is the ASP background knowledge, \\( SM \\) is the set of rules allowed in the hypotheses (called hypothesis space), and \\( E \\) is a set of examples. A hypothesis \\( H \\subseteq SM \\) is a solution of \\( T \\) if and only if \\( B \\cup H \\) accepts all examples \\( e \\in E \\)."}, {"title": "2.3 Reward Machines", "content": "Reward machines [RMs; 31, 32] are finite-state machine representations of reward functions. RMs encode the temporal structure of a task's rewards, enabling expressive and interpretable specifications; besides, they enable handling non-Markovian reward tasks by using the state of the RM as an external memory.\nReward machines are defined in terms of high-level events (or observables) \\( \\mathcal{P} \\), which aim to abstract the environment's state space \\( \\mathcal{S} \\). More specifically, the transitions of an RM are labelled with sets of observables \\( O \\) (or observations). We denote by \\( \\mathcal{O} \\) the set of all possible observations, i.e. the powerset of \\( \\mathcal{P} \\), such that \\( |\\mathcal{O}| < |\\mathcal{S}| \\). The abstraction from environment states and actions to observations is performed by a labelling function \\( \\mathcal{L}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow O \\).\nDEFINITION 2.1 (REWARD MACHINE). An RM is a tuple RM = \\( \\langle U, \\mathcal{P}, u_0, U_A, U_R, \\delta_U, \\delta_r \\rangle \\), where \\( U \\) is a finite set of states; \\( \\mathcal{P} \\) is a set of observables; \\( u_0, U_A, U_R \\in U \\) are the initial, accepting and rejecting state, respectively; \\( \\delta_U : U \\times O \\rightarrow U \\) is a deterministic state-transition function that maps a RM state and an observation to the next RM state; and \\( \\delta_r: U \\times U \\rightarrow \\mathbb{R} \\) is a deterministic reward-transition function that returns the reward given a pair of RM states determining a transition.\nWe now introduce a running example used throughout the paper.\nEXAMPLE 2.1. Let us consider an agent (\\( \\bigtriangleup \\)) navigating the grid environment in Figure 1a. The grid consists of several checkpoints (e.g. \\( \\bigtriangleup \\)), each characterised by a unique identifier and a colour, and a"}, {"title": "3 METHODOLOGY", "content": "The limited expressivity of propositional logic formulae presents significant challenges to the broader adoption of RM-based approaches for RL. For instance, propositional logic cannot express tasks in terms of the properties of a specific or a group of entities, such as the colour of checkpoints in Example 2.1. This limitation substantially increases the number of sub-task combinations to be captured (if expressed at propositional level), resulting in larger RMs that are more difficult to learn and exploit. In addition, the restriction to a fixed set of observables hinders the re-use of RMs across different scenarios (e.g., on environments with different numbers of objects of the same colour). To address these limitations, we propose to label the edges of RMs using first-order formulae. The high expressivity of first-order logic allows for more compact and general RMs, making them easier to learn and transfer across different scenarios. We first illustrate the intuition behind our approach by building upon the RM from Example 2.1.\nEXAMPLE 3.1. Figure 1c displays the First-Order RM (FORM) for the task visit all yellow checkpoints followed by any blue checkpoint before reaching\". Despite encoding the same task, the FORM is more compact than its propositional counterpart (Figure 1b).\nExistentially quantified formulae reduce the number of edges required to encode the disjunctions, impacting the parameter \\( k \\) in Equation 1 (Figure 2a). Universally quantified formulae reduce the number of states and, consequently, the number of edges required to encode a task (Figure 2b)."}, {"title": "3.1 Language", "content": "In this section, we introduce the first-order language \\( \\mathcal{L} \\) used to label transitions in FORMs. The signature of \\( \\mathcal{L} \\), denoted as \\( \\Sigma = (\\mathcal{C}, \\mathcal{K}) \\), is composed of a set \\( \\mathcal{C} \\) of constants and a set \\( \\mathcal{K} \\) of n-ary predicates. Predicates with 0-arity are referred to as propositions. A variable \\( X \\) of a predicate \\( P \\in \\mathcal{K} \\) is bound if it is in the scope of a quantifier \\( Q \\in {\\forall, \\exists} \\). A ground atom is a predicate whose arguments are constants. A quantified atom is a predicate whose variables are bound, e.g. \\( \\exists X.P(X) \\). A ground instance, \\( g \\), of a quantified atom is a ground atom generated by replacing all variables in \\( \\psi \\) with constants. Propositions, ground atoms and quantified atoms are atomic formulae of \\( \\mathcal{L} \\). The first-order language \\( \\mathcal{L} \\) of a FORM is defined as follows.\nDEFINITION 3.1. Given a signature \\( \\Sigma = (\\mathcal{C}, \\mathcal{K}) \\), the language \\( \\mathcal{L} \\) is the set of formulae defined inductively as follows:\n(1) An atomic formula is a formula.\n(2) If \\( \\psi \\) is a formula, then so is \\( \\neg\\psi \\).\n(3) If \\( \\psi_1 \\) and \\( \\psi_2 \\) are formulae, then \\( \\psi_1 \\land \\psi_2 \\), and \\( \\psi_1 \\lor \\psi_2 \\) are formulae.\nWe refer to formulae in a first-order language \\( \\mathcal{L} \\) as \\( \\mathcal{L} \\)-formulae.\nThe set of all propositions and ground atoms in \\( \\mathcal{L} \\) is called the Herbrand Base of \\( \\mathcal{L} \\), denoted HBL.\nEXAMPLE 3.2. Consider the environment given in Figure 1a. The language \\( \\mathcal{L} \\) for this environment has the signature \\( \\Sigma = (\\mathcal{C}, \\mathcal{K}) \\) given by the following sets:\n\\( \\mathcal{C} = {\\circ_0,..., \\circ_{12}} \\),\n\\( \\mathcal{K} = {\\,\\ |\\ \\,\\,\\,\\,\\,\\,\\, goal} \\),\\) where the predicates are pictograms for unary predicates yellow, blue, red, gray, purple, green, and the proposition goal, respectively. We depict a ground atom of \\( \\mathcal{L} \\) as a predicate pictogram with the subscript of its constant argument, e.g. \\( blue(\\circ_4) \\) is depicted as \\( \\,\\ ). Analogously, we depict quantified atoms with the quantifier and the predicate pictogram with the bound variable, e.g. \\( \\forall X.yellow(X) \\) is depicted as \\( \\forall X.\\,\\ ). The proposition \\( \\,\\ ) and the negated formula \\( \\,\\ ) are examples of \\( \\mathcal{L} \\)-formulae."}, {"title": "3.2 First-Order Reward Machines (FORMS)", "content": "We formalise now the notion of a first-order RM (FORM), where transitions are labelled with formulae of a first-order language \\( \\mathcal{L} \\). FORMs enhance the expressivity of traditional RMs. The core entities that characterise a FORM are its observables, labelling function, and state-transition function. In what follows, we describe how these entities are defined and how the satisfiability of \\( \\mathcal{L} \\)-formulae labelling the transitions of a FORM is checked.\nDEFINITION 3.2 (FIRST-ORDER RM). A First-Order RM over a first-order language \\( \\mathcal{L} \\) is a tuple FORML = \\( \\langle U, HBL, u_0, U_A, U_R, \\delta_U, \\delta \\rangle \\) where \\( U, u_0, U_A, U_R \\) and \\( \\delta_r \\) are as defined in Definition 2.1, and\n*   HBL is the set of observables, and\n*   \\( \\delta_U : U \\times O^* \\rightarrow U \\times O^* \\) is the state-transition function taking a state and a history of observations to a new state and an updated history.\nObservables and Labelling Function. Differently from traditional RMs, where observables are given as a set \\( \\mathcal{P} \\) of propositions, the observables in a FORM are defined as the Herbrand base of the first-order language \\( \\mathcal{L} \\). The labelling function \\( \\mathcal{L} \\) thus returns at each time-step, a subset of HBL as an observation.\nState-Transition Function. The state-transition function \\( \\delta_U \\) generalises that of a traditional RM in two ways. Firstly, it supports the use of first-order formulae returned by a logical transition function \\( \\phi : U \\times U \\rightarrow \\mathcal{L} \\). Secondly, it takes a history of observations instead of a single observation. This is important to semantically evaluate universally quantified atoms. To relax the requirement of having to see all the ground instances of the universally quantified atoms in the same observation, we use the following interpretation. Given a history, a universally quantified atom is TRUE when all its ground instances are contained in the history. We refer to this history as a buffer \\( B = [O_{t-k}, ..., O_t] \\) which gathers observations from time-step \\( t - k \\) to \\( t \\), where \\( k \\geq 0 \\) is the length of the buffer.\nAlgorithm 1 shows the pseudo-code that defines \\( \\delta_U \\). The buffer \\( B \\) contains all the observations perceived since the current RM state was reached. The buffer is emptied when a transition to a new state is taken (l.4), or updated with the current observation otherwise (l.1). The \\( \\mathcal{L} \\)-formulae are given by the logical transition function \\( \\phi \\) and are evaluated against the buffer (l.3).\nIn the following section, we define the notion of satisfiability of an \\( \\mathcal{L} \\)-formula given a buffer \\( B \\).\nSatisfiability of \\( \\mathcal{L} \\)-formulae. Given a buffer \\( B = [O_{t-k}, ..., O_t] \\), observables that are included in \\( B \\) are assumed to be TRUE. Any other observable from HBL not included in \\( B \\) is assumed to be FALSE. We can now define the satisfiability of a \\( \\mathcal{L} \\)-formula given a buffer \\( B = [O_{t-k}, ..., O_t] \\).\nDEFINITION 3.3. Let \\( B = [O_{t-k}, ..., O_t] \\) be a buffer for some \\( k \\geq 0 \\). Let \\( \\psi \\) be a \\( \\mathcal{L} \\)-formula. \\( B \\) satisfies \\( \\psi \\), written \\( B \\models \\psi \\), is defined as follows:\n*   \\( B \\models \\psi \\) if \\( \\psi \\in O_t \\), where \\( \\psi \\) is a proposition or a ground atom.\n*   \\( B \\models \\psi \\) if \\( \\psi_g \\in O_t \\) for some ground instance \\( \\psi_g \\) of \\( \\psi \\), where \\( \\psi \\) is an existentially quantified atom.\n*   \\( B \\models \\psi \\) if \\( \\psi_g \\in \\cup_{0\\leq i\\leq k} O_{t-i} \\) for all ground instance \\( \\psi_g \\) of \\( \\psi \\), where \\( \\psi \\) is an universally quantified atom.\n*   \\( B \\models \\neg \\psi \\) if \\( B \\not\\models \\psi \\)"}, {"title": "3.3 FORM Learning", "content": "We now focus on the problem of learning a FORM from the observation traces collected by the agent. Although observations are sets of ground atoms and propositions, the goal is to learn an abstract representation of the task's structure using lifted first-order formulae. To achieve this, we leverage the support of ASP for variables and learn ASP rules for the \\( \\mathcal{L} \\)-formulae constituting the FORM.\nThe set of observables (HBL) and the set of predicates (\\( \\mathcal{K} \\)) are not known 'a priori'; instead, they are automatically derived from the observation traces. We model existentially and universally quantified atoms as ASP rules and expand the hypothesis space from Equation 1 to allow the edges of the RM to be labelled with quantified atoms or their negations. Formally,\nDeterminism. The state-transition function \\( \\delta_U \\), like in traditional RMs, must be deterministic; that is, two or more transitions cannot be simultaneously satisfied from a given state. To guarantee this property, the \\( \\mathcal{L} \\)-formulae on such transitions must be mutually exclusive. Two transitions are mutually exclusive if one ground atom or proposition appears positively on one transition and negatively on the other.\nMutual exclusivity is encoded through ASP rules and enforced at learning time so that the output FORMs are guaranteed to be deterministic. Since quantified atoms can represent multiple associated ground instances, we also define rules that translate quantified atoms into their corresponding sets of ground instances. We refer the reader to Appendix C for the encoding of mutual exclusivity and the mapping from quantified atoms into ground instances."}, {"title": "3.4 Policy Learning", "content": "The vast majority of the algorithms used to learn the policy associated with the RM are based on CRM [32], an algorithm that learns a single policy for the whole RM. This poses some concerns on the transferability of the learnt solution, and more specifically on the ability to reuse the policy associated with a given sub-task of the RM. To address this issue, we propose a novel formulation as a multi-agent problem, where a team of agents (one for each RM state) collaborate to complete the task. Our approach grounds the problem of RL with RM into the Markov game framework and, combined with FORM, enables the transfer of the learnt policies across scenarios (e.g., with different objects) to speed up the learning.\nWe formalise the problem as a collaborative Markov game of \\( N \\) agents defined with the tuple \\( G = (RM, N, \\mathcal{S}, s_1, \\mathcal{A}, p, r, \\gamma) \\), where RM is a FORM, \\( N = |U_{RM}| \\) is the number of agents equal to the number of states in the FORM; \\( \\mathcal{S} = \\mathcal{S}_1 \\times \\cdots \\times \\mathcal{S}_N \\) is the set of joint states; \\( s_1 \\in \\mathcal{S} \\) is a joint initial state; \\( \\mathcal{A} = \\mathcal{A}_1 \\times \\cdots \\times \\mathcal{A}_N \\) is the set of joint actions; \\( p : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} \\) is a deterministic joint transition function; and \\( r : (\\mathcal{S} \\times \\mathcal{A})^+ \\times \\mathcal{S} \\rightarrow \\mathbb{R}^N \\) is the joint reward function. The objective of the game is to find a team policy \\( \\pi = \\pi_1 \\times \\cdots \\times \\pi_N \\) mapping joint states to joint actions such that the sum of the individual expected cumulative rewards is maximised. In practice, each state of the FORM is assigned an RL agent responsible for learning a policy to accomplish the associated sub-task. Since the state-transition function \\( \\delta_U \\) of the FORM is deterministic, we can only be in a single state at any given time; therefore, only one RL agent acts at each time-step. The joint reward function is designed so that the reward received is uniformly shared among all the agents that contributed to reaching the current state (i.e., where \\( r \\) is the global reward obtained and \\( n' \\) is the number of agents that have participated in getting \\( r \\)), implying that their assigned FORM state was visited. Consequently, the agents work collaboratively and in turns to traverse the FORM towards the accepting state.\nAgent State Space. Since all agents act in the same environment, their state space \\( \\mathcal{S} \\) is the environment's state space \\( \\mathcal{S} \\). However, depending on the outgoing transitions from a given FORM state \\( u \\in U \\), the state space \\( \\mathcal{S} \\) of the associated agent may be extended. As described in Section 3.2, universally quantified formulae are evaluated over multiple time-steps using an observation buffer \\( B \\), making the sub-task non-Markovian as the policy becomes dependent on a history (i.e., the buffer).\nWe address this issue by extending the state space \\( \\mathcal{S} \\) of agent \\( u \\) with indicators encoding whether the ground atoms of interest have been perceived by the agent, hence making the sub-task Markovian. Let \\( g_u \\) denote the set of ground atoms of a universally quantified formula that must be satisfied to exit a FORM state \\( u \\). The extended state \\( \\hat{s}_u \\) is constructed by concatenating the state \\( s \\) from"}, {"title": "4 EXPERIMENTS", "content": "We demonstrate the benefits of the proposed FORM learning and exploitation approach in the environment from Figure 1a. The tasks posed in this environment are challenging to traditional RM learning methods, especially as the number of objects increases. The section is organized as follows. First, we show the benefits of FORM learning with respect to standard RL algorithms and methods leveraging propositional RMs (with and without learning them). Second, we show that first-order logic eases the transfer of RMs and policies to more complex scenarios (e.g., with more objects). Additional experimental details are described in Appendix B2."}, {"title": "4.1 FORM Learning", "content": "We evaluate the performance of our approach in its ability to learn and exploit FORMs. We consider three baselines: (i) PPO [21], to show how RMs help deal with non-Markovian tasks; (ii) Handcrafted FORM, which assumes the FORM is known 'a priori'; and (iii) a state-of-the-art propositional RM learning approach [8]. Both (ii) and (iii) employ our multi-agent based exploitation method.\nIn the following paragraphs, we compare our approach against the baselines in three, increasingly complex, tasks.\nTask 1 - ALLYELLOW. The task consists in visiting all yellow checkpoints before going to the location. Figure 1a illustrates the environment with two yellow checkpoints. This task is chosen to"}, {"title": "4.2 FORM Transfer", "content": "One of the advantages of FORMs lies in their ability to learn RMs that capture the structure of a task without necessarily being bound to the individual objects of the environments but rather to their properties (e.g., their colour in our case). It makes the RM transferable to new tasks, considerably improving learning efficiency. Our multi-agent formulation lets us also transfer the trained policies to the new task. Depending on the 'a priori' knowledge available, we may want to restrict the re-training to the ones that need it (see results below) or re-train all of them (see Appendix D). In both cases, the transferred policies are used as a warm start."}, {"title": "5 RELATED WORK", "content": "In this section, we discuss work related to our contributions: the exploitation and learning of RMs."}, {"title": "Exploiting RMs", "content": "Recent work has also aimed at increasing the expressivity of RMs, e.g. by introducing counter variables that support richer grammars [5]. In parallel, other languages such as linear temporal logic have also been used to formulate task specifications [1, 6, 7, 30]. These approaches primarily focus on exploiting the given task structure. Unlike these methods, our approach tackles the challenge of learning the task structure itself.\nThe exploitation of RMs is often performed in the literature through QRM [31] or CRM [32]. Although both learn policies over \\( \\mathcal{S} \\times U \\), the former associates an action-value function with each RM state, whereas the latter keeps a single action-value function. These algorithms employ a counterfactual mechanism that enables reusing the experiences for an RM state to update the value associated with other RM states; as such, the underlying RL algorithms are off-policy, e.g. DDQN [32], DDPG [26, 32], and SAC [26]. QRM and our approach learn a policy for each RM state; however, QRM makes updates from likely different rollouts by sampling from a buffer (off-policy), whereas our approach updates the policies involved in the most recent rollout (on-policy). Besides, in QRM the Q-value function is updated by using the successor state's Q-value function, creating a dependency between the policies of each state. We, on the other hand, take the approach of using the global reward returned by the RM as a signal for each agent, limiting the dependency between the agents' policy, hence increasing transferability. Our multi-agent exploitation method aligns with the options framework [29]. An option is a temporally-extended action normally consisting of a policy that chooses between other options and actions. In our formulation, each agent operates an option determining the next action to accomplish a sub-task. In our case, an option terminates when the RM state changes (i.e., the option for the next RM state initiates). Unlike the options framework, which usually considers rewards during an option's activation, our approach distributes rewards to all agents that contributed to reaching the current RM state."}, {"title": "Learning RMs", "content": "Most RM research has focused on relaxing the assumption that the RM is known 'a priori', proposing various methods to learn it dynamically. These methods include discrete optimization [33], SAT solving [36], state-merging [10], inferring LTL formula from expert data [4], and inductive logic programming [8], the latter of which forms the base of our approach. These techniques leverage traces collected through the agent's exploration or provided by an expert to learn the RM. All these methods have focused on learning propositional RMs, which are tailored to specific scenarios and thus lack reusability. Additionally, as the task complexity increases, the size of the RMs typically grows exponentially, making them impossible to learn."}, {"title": "6 CONCLUSION", "content": "While the literature has actively contributed to improving the exploitation and learning of RMs, existing methods primarily focus on propositional RMs, which are limited in their generalizability and scalability. In this paper, we introduced First-Order Reward Machines (FORMS), a novel extension of reward machines (RMs) that leverages first-order logic to enhance expressivity and transferability. We propose a learning algorithm that effectively learns FORMs from traces, and a multi-agent framework that facilitates efficient policy learning and transfer. Experimental results demonstrate that, unlike traditional RM methods, FORMs enhance scalability, enable faster policy and RM learning, and ease task transferability. This work paves the way for learning and exploiting more abstract and generalizable non-Markovian task specifications in RL. Although our approach provides greater expressivity than propositional logic, extending our methodology to include languages like LTL could be an interesting avenue to be able to also learn the structure of temporally extended tasks."}, {"title": "A REWARD MACHINES", "content": "In this section, we present some reward machines (RMs) omitted in the main paper. Figure 9 exemplifies the equivalence between traditional and first-order RMs originally shown in Figure 2b but extended to a case with three objects. Figure 10 shows the propositional RM for the BLUE-ALLYELLOW-7 task, whose first-order RM is shown in Figure 6."}, {"title": "B EXPERIMENTAL DETAILS", "content": "The code will be made available if the paper is accepted. The experiments are run using the RLlib framework [17]. All experiments were run on a C6A.12XLARGE AWS machine, parallelizing the runs for 12 different seeds. An episode is automatically terminated when the agent has accomplished the task or when it has reached the maximum number of time-steps (3000), whichever comes first. We set a timeout of 1h for ILASP to return a solution. We apply early-stopping of the training whenever the mean return falls by more than 10% of its current value after crossing the limit of 70% of the maximum possible return.\nPolicy Learning. We use RLlib's PyTorch [19] implementation of PPO. Default weight initializations are employed. A convolutional neural network (CNN) processes the image of the environment, and multi-layer perceptrons process the buffer and the RM state. The output of the CNN and the MLPs is concatenated and fed to the policy network. In the single policy experiments (see Appendix D), there is only the CNN and one MLP (i.e., that for the RM state is processed). The deep layers of the value function network are shared between the actor and the critic. The hyperparameters for the multi-agent and single policy settings are presented in Tables 1 and 2, respectively."}, {"title": "C DETERMINISM ASP ENCODING", "content": "In this section, we describe the ASP encoding of the determinism-related constraints, whose intuition is outlined in Section 3.3. These rules enforce mutual exclusivity between the formulae labelling transitions from a given state to two different states.\nThe ASP representation of the formulae labelling the edges is done using rules; however, defining constraints over rules is not straightforward. To address this problem, we associate rules in the hypothesis spaces with facts, over which defining the mutual exclusivity constraints is easy. The mapping from rules to facts is:\nwhere pos (resp. neg) facts denote that a proposition or quantified atom appears positively (resp. negatively) in the transition. Remember that the actual transition is the negation of \\( \\phi \\), hence why the not are mapped into pos. The ILASP system enables performing"}, {"title": "D ADDITIONAL TRANSFER EXPERIMENTS", "content": "We show the benefit of our multi-agent learning approach compared to a single policy, when transferring the RM and the policy to a new environment. The single policy is trained on the cross-product between the environment state space \\( \\mathcal{S} \\) and the RM state space \\( U \\). In the new environment, the number of yellow checkpoints (\\( \\cir\\)) is increased from two to four and six, but the task remains the same: \"visit all yellow checkpoints, before going to the location\". We observe that for the single policy, the transfer is ineffective and the policy does not succeed in the new environment. On the other hand, we observe that our multi-agent approach succeeds in the new environment.\nWe also analyse the impact of reusing policies that have been trained on a simpler task (2-YELLOW) into more complex ones: 4-YELLOW (Figure 11) and 6-YELLOW (Figure 12). We examine different configurations and compare the number of iterations to converge with the base case where the policies are learnt from scratch (Without reuse). The first configuration (With reuse - Partial re-training) corresponds to the case where we know 'a priori' the policies that must be retrained. In that case, we can simply retrain a subset of the policies and use the others in inference mode during training. We observe a huge speed-up in the learning of the policies with the agents converging in fewer iterations for both 4-YELLOW and 6-YELLOW. In the second case, we retrain all the policies (With reuse - Full re-training) but use the transferred policies as a warm start. In this configuration we observe a decent speed-up for the 4-YELLOW task but only a marginal improvement for 6-YELLOW."}]}