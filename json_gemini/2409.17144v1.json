{"title": "Differential Privacy Regularization:\nProtecting Training Data Through\nLoss Function Regularization", "authors": ["Francisco Aguilera-Mart\u00ednez", "Fernando Berzal"], "abstract": "Training machine learning models based on neural\nnetworks requires large datasets, which may contain sensitive\ninformation. The models, however, should not expose private\ninformation from these datasets. Differentially private SGD\n[DP-SGD] requires the modification of the standard stochastic\ngradient descent [SGD] algorithm for training new models. In\nthis short paper, a novel regularization strategy is proposed to\nachieve the same goal in a more efficient manner.", "sections": [{"title": "I. INTRODUCTION", "content": "We have recently witnessed the widespread adoption of deep\nlearning models across many different applications. These\nmodels have been particularly successful in an increasing num-\nber of natural language processing (NLP) tasks [1], including\ntext summarization, machine translation, and language gener-\nation. Large language models (LLMs) have gained significant\nattention for their exceptional capabilities to generate and\ninterpret text as humans do [2]. While LLMs offer significant\nadvantages, they are not without flaws and remain vulnerable\nto security and privacy attacks [3].\nTraining and fine-tuning LLMs requires massive quantities\nof data sourced from the internet and proprietary data sources,\nas well as carefully annotated text data to enhance their\nperformance for specific tasks. This reliance on extensive\ndatasets can increase their susceptibility to security and privacy\nvulnerabilities. Despite their widespread use, the vulnerabili-\nties of LLMs have not been extensively explored on a large\nscale.\nMalicious actors can exploit deep learning models to extract\nsensitive information that is used during their training and\nis, in some sense, memorized by them. One of the most\nprominent and dangerous attack methods is called gradient\nleakage [3]. This attack attempts to infer whether a specific\ndata instance was part of the training data used to train (or fine-\ntune) the model under attack. To mitigate the effectiveness of\nthese attacks, model developers sometimes employ differential\nprivacy [4] during the training process as a protective measure.\nWe investigate how to preserve privacy by implementing\ndifferential privacy regularization for deep learning models,\nincluding LLMs. Our approach is inspired by the differentially\nprivate stochastic gradient descent (DP-SGD) algorithm [5]."}, {"title": "II. BACKGROUND", "content": "Differential privacy (DP) has emerged as a fundamental\ntechnique for safeguarding sensitive information in machine\nlearning models, particularly in scenarios involving large\nlanguage models. Differential privacy limits the information\nthat is leaked about specific individuals. In terms of machine\nlearning models, we try to ensure that an individual data does\nnot significantly affect the outcome of a computation, therefore\nproviding guarantees of privacy while allowing useful insights\nto be derived from aggregate data. In short, we are focused on\nthe study of how differential privacy can be used to protect\nsensitive information in training data.\nIn the context of deep learning, implementing differential\nprivacy poses unique challenges due to the complexity of the\nmodels and the sensitivity of the data often used to train deep\nlearning models. Below, we summarize some key works that\nhave shaped the field of differential privacy in deep learning."}, {"title": "A. Differential Privacy in Deep Neural Networks", "content": "Abadi et al. [5] explored the integration of differential\nprivacy in deep neural networks.\nThe core idea of differential privacy is to ensure that the\ninclusion or exclusion of a single training example does not"}, {"title": "B. Differential Privacy in LLMs: The EW-Tune Framework", "content": "Differential privacy has also been applied to large language\nmodels (LLMs), which present additional challenges due to\ntheir size and complexity. Behnia et al. [6] proposed the\nEW-Tune framework to implement DP in LLMs. They intro-\nduced the Edgeworth Accountant, a method for calculating\nprecise privacy guarantees in the context of finite samples. By\nleveraging the Edgeworth expansion, the authors provide non-\nasymptotic guarantees, improving upon traditional approaches\nthat often rely on asymptotic bounds.\nThe DP-SGD algorithm is used for applying DP in LLMs,\nby introducing Gaussian noise into the gradients during model\ntraining. The Edgeworth Accountant just refines the noise\naddition process, calculating the necessary amount of noise\nbased on the given privacy budget. This method balances the\ntrade-off between noise and model utility more effectively,\nallowing for reduced noise and, consequently, better model\nperformance without compromising privacy guarantees."}, {"title": "C. User-Level Differential Privacy in LLMs", "content": "Building on previous work, Charles et al. [7] examine DP\nin the context of LLM training by introducing two sampling\napproaches: example-level sampling (ELS) and user-level sam-\npling (ULS). These methods aim to protect user-level privacy,\nensuring that the contribution of individual users to the model\nis protected. ELS involves clipping gradients at the example\nlevel, while ULS operates at the user level, allowing for\ngradient aggregation over all examples provided by a single\nuser.\nThe authors introduce a novel user-level DP accountant for\nELS that leverages a divergence measure known as the hockey-\nstick divergence. This measure enables the derivation of pre-\ncise privacy guarantees for ELS. Comparisons between ELS\nand ULS show that, under fixed computational budgets, ULS\ntends to provide stronger privacy guarantees and better model\nperformance, particularly when stringent privacy protection is\nrequired or when larger computational resources are available."}, {"title": "D. Differential Privacy through Classic Regularization", "content": "While DP-SGD is a robust method for ensuring privacy,\nit often leads to significant performance degradation due to\nnoise used during training. Lomurno et al. [4] compared DP\ntechniques and traditional regularization methods, such as\ndropout and L2 regularization. According to their study, classic\nregularization, commonly used to prevent overfitting, provides\nsimilar levels of protection against membership inference and\nmodel inversion attacks, which are key privacy concerns in\nmachine learning.\nTheir empirical results suggest that regularization methods\nmay offer a more effective trade-off between privacy and\nmodel performance in certain scenarios. Unlike DP-SGD,\nwhich incurs in high computational costs and significant\naccuracy loss, regularization techniques provide privacy pro-\ntection with a minimal impact on performance. As such,\nthese methods may be more suitable in contexts where model\nperformance and training efficiency are critical."}, {"title": "III. A NEW PERSPECTIVE ON THE DIFFERENTIALLY\nPRIVATE SGD ALGORITHM", "content": "DP-SGD [5] offers one way to control the influence of the\ntraining data during the training process: At each step of the\nSGD, we compute the gradient $\\nabla_{\\theta}L(\\theta)$ for a random subset\nof examples, clip the $l_2$ norm of each gradient, compute the\naverage, add noise in order to protect privacy, and take a step\nin the opposite direction of this average noisy gradient:\n$\\epsilon(t) \\sim N(0, \\sigma^2)$\n$\\tilde{g}(t) = g(t) + \\epsilon(t)$\nwhere $g(t)$ is the gradient for the current batch of training\nexamples and $\\epsilon(t)$ is the Gaussian noise we introduce.\nThe resulting weight update is, therefore:\n$\\Delta\\theta(t) = -\\eta \\tilde{g}(t)$\ni.e.\n$\\theta(t + 1) = \\theta(t) - \\eta_t \\tilde{g}(t)$\nwhere $\\eta_t$ is the current learning rate.\nLet us now assume a linear neuron (or a nonlinear neuron\noperating within its linear regime):\n$y = \\theta \\cdot x = \\sum_{i=0}^n \\theta_i x_i$\nBefore the weight update:\n$y(t) = \\theta(t) \\cdot x$\nWith noise in the gradients, the output after the weight update\nis\n$\\tilde{y}(t+1) = \\theta(t+1) \\cdot x$\n$= (\\theta(t) - \\eta \\tilde{g}(t)) \\cdot x$\n$= (\\theta(t) - \\eta(g(t) + \\epsilon(t)) \\cdot x$\n$= (\\theta(t) - \\eta g(t)) \\cdot x - \\eta \\epsilon(t) \\cdot x$\n$= y(t+1) - \\eta \\epsilon(t) \\cdot x$"}, {"title": null, "content": "Simplifying our notation, we have:\n$\\tilde{y} = (\\theta - \\eta \\tilde{g}) \\cdot x$\n$= y - \\eta \\epsilon x$\nUsing a quadratic error function $L = (y-t)^2$ for the training\nalgorithm using gradient noise:\n$E[(\\tilde{y} - t)^2] = E[(\\tilde{y} - t)^2]$\n$= E[((y - \\eta \\epsilon x) - t)^2]$\n$= E[((y - t) - \\eta \\epsilon x)^2]$\n$= E[(y - t)^2] - E[2(y - t)\\eta \\epsilon x] + E[(\\eta \\epsilon x)^2]$\nLet us recall that the noise $\\epsilon$ is sampled from a Gaussian\nwith mean 0 and variance $\\sigma^2$. Hence:\n$E[2(y - t)\\eta \\epsilon x] = 2(y - t)\\eta E[\\epsilon]x = 0$\nTherefore\n$E[(\\tilde{y} - t)^2] = E[(y - t)^2] + E[(\\eta \\epsilon x)^2]$\nThe first term is just the traditional quadratic error function\n$L = (y - t)^2$, whereas the second term can be interpreted as\nan L2 regularization term for the input:\n$E[(\\eta \\epsilon x)^2]$\n$= \\eta^2 E[(\\epsilon x)^2]$\n$= \\eta^2 E[(\\sum \\epsilon_i x_i)^2]$\n$= \\eta^2 E[\\sum \\epsilon_i^2 x_i^2 + 2 \\sum_{i<j} \\epsilon_i x_i \\epsilon_j x_j]$\n$= \\eta^2 [\\sum E[\\epsilon_i^2] x_i^2 + 2 \\sum_{i<j} E[\\epsilon_i x_i \\epsilon_j x_j] ]$\n$= \\eta^2 [\\sum \\sigma_{\\epsilon}^2 x_i^2 + 2 \\sum_{i<j} E[\\epsilon_i] E[x_i] E[\\epsilon_j] E[x_j] ]$\n$= \\eta^2 [\\sum \\sigma_{\\epsilon}^2 x_i^2 + 2 \\sum_{i<j} \\sigma_{x_i} \\sigma_{x_j}]$\n$= \\eta^2 \\sigma_{\\epsilon}^2 \\sum x_i^2$\nThe error function given the noisy gradients in DP-SGD is\nfinally\n$E[(\\tilde{y} - t)^2] = (y - t)^2 + \\eta^2 \\sigma_{\\epsilon}^2 \\sum x_i^2$\nIf we assume that the gradient noise variance is the same\nfor all the inputs:\n$E[(\\tilde{y} - t)^2] = (y - t)^2 + \\eta^2 \\sigma_{\\epsilon}^2 \\sum x_i^2$\n$L_{noisy gradient} = \\kappa \\sum x_i^2$"}, {"title": null, "content": "$L_{DP} = L + L_{noisy gradient}$\nPlease, compare the above expression with the standard $L^2$\nregularization strategy:\n$L_{L^2 regularization} = L + \\lambda \\sum \\theta_i^2$\nOf course, both regularization terms can be easily combined:\n$L_{DP+L^2 regularization} = L + \\lambda \\sum \\theta_i^2 + \\kappa \\sum x_i^2$\nAs training with input noise is equivalent to weight decay,\nalso known as Tikhonov or L2 regulatization [8], training with\nnoisy gradients is somehow equivalent to performing Tikhonov\nregularization on the input.\nHowever, it should be noted that the DP regularization term\nis independent from the network parameters $\\theta$. Therefore,\nits gradient with respect to the network parameters is zero,\ni.e. $\\nabla_{\\theta} L_{noisy gradient} = 0$. Hence, the resulting optimization\nalgorithm is exactly the same for the standard stochastic\ndescent algorithm (SGD) and for its gradient noise variant\n(DP-SGD). In other words, we are just introducing some\nartificial noise in the training algorithm, which adds to the\nnoisy estimate of the gradient computed by the stochastic\ngradient descent algorithm.\nThe above discussion might explain why some researchers\nhave found that, even though DP-SGD \u201cis a popular mecha-\nnism for training machine learning models with bounded leak-\nage about the presence of specific points in the training data[,]\\nthe cost of differential privacy is a reduction in the model's\naccuracy\" [9]. Moreover, \u201c[a]ccording to the literature, [DP-\nSGD] has proven to be a successful defence against several\nmodels' privacy attacks, but its downside is a substantial\ndegradation of the models' performance... and [researchers]\nempirically demonstrate the often superior privacy-preserving\nproperties of dropout and 12-regularization\" [4]."}, {"title": "IV. DIFFERENTIALLY PRIVATE REGULARIZATION", "content": "In the previous section, we observed that the addition of\nGaussian noise to the gradients in DP-SGD is not really\neffective, since it just introduces an additional noise to the\nnoisy gradient estimate of the conventional SGD, without\nreally changing the loss function we are implicitly optimizing.\nIn this Section, we propose the introduction of noise propor-\ntional to each parameter value, so that the resulting algorithm\nis not equivalent to SGD in its linear regime.\nOur proportional differentially private PDP-SGD algorithm\u00b9\nstarts by introducing Gaussian noise as follows:\n$\\epsilon_i(t) \\sim N(0, (\\theta_i \\sigma)^2)$"}, {"title": null, "content": "$\\tilde{g}_i(t) = g_i(t) + \\epsilon_i(t)$\nFor each network parameter, $\\theta_i$, we add Gaussian noise\nwhose standard deviation is proportional to the parameter\nvalue (i.e. larger parameters receive larger noise). The gradient\nnoise variance is, therefore, $\\sigma^2 = (\\theta_i \\sigma)^2$\nBy definition, in Gaussian noise, the values are identically\ndistributed and statistically independent (and hence uncorre-\nlated), so $E[\\epsilon_i\\epsilon_j] = E[\\epsilon_i]E[\\epsilon_j]$.\nUsing the same reasoning we followed in the previous\nSection, we have:\n$\\tilde{y} = y - \\eta \\epsilon x$\nand\n$E[(\\tilde{y} - t)^2] = E[(y - t)^2] - E[2(y - t)\\eta \\epsilon x] + E[(\\eta \\epsilon x)^2]$\nNow, the noise $\\epsilon_i$ for each gradient is sampled from a\nGaussian with mean 0 and variance $\\sigma_i^2$, which is different\nfor each parameter. Hence:\n$E[2(y - t)\\eta \\epsilon x] = 2(y - t)\\eta E[\\epsilon x]$\n$= 2(y - t)\\eta E[\\sum \\epsilon_i x_i]$\n$= 2(y - t)\\eta \\sum E[\\epsilon_i x_i]$\n$= 2(y - t)\\eta \\sum E[\\epsilon_i] E[x_i]$\n$= 2(y - t)\\eta 0 E[x_i]$\n$= 0$\n$E[(\\eta \\epsilon x)^2]$\n$= \\eta^2 E[(\\epsilon x)^2]$\n$= \\eta^2 E[(\\sum \\epsilon_i x_i)^2]$\n$= \\eta^2 E[\\sum \\epsilon_i^2 x_i^2 + 2 \\sum_{i<j} \\epsilon_i x_i \\epsilon_j x_j]$\n$= \\eta^2 [\\sum E[\\epsilon_i^2] x_i^2 + 2 \\sum_{i<j} E[\\epsilon_i x_i \\epsilon_j x_j] ]$\n$= \\eta^2 [\\sum E[\\epsilon_i^2] E[x_i^2] + 2 \\sum_{i<j} E[\\epsilon_i]E[x_i]E[\\epsilon_j]E[x_j] ]$\n$= \\eta^2 [\\sum \\sigma_i^2 E[x_i^2] + 2 \\sum_{i<j} \\sigma_{x_i} \\sigma_{x_j}]$\n$= \\eta^2 \\sum \\sigma_i^2 x_i^2 + 2 \\sum_{i<j} \\sigma_{x_i} \\sigma_{x_j}]$\n$= \\eta^2 \\sum (\\theta_i \\sigma)^2 x_i^2$\n$= \\eta^2 \\sigma^2 \\sum \\theta_i^2 x_i^2$\nThe error function given the proportional noisy gradients in\nPDP-SGD is, therefore,\n$E[(\\tilde{y} - t)^2] = (y - t)^2 + \\eta^2 \\sigma^2 \\sum \\theta_i^2 x_i^2$\nIf we define a proportional differentially private regulariza-\ntion term as follows\n$L_{proportional noisy gradient} = \\kappa \\sum \\theta_i^2 x_i^2$\nthen we have\n$L_{PDP} = L + L_{proportional noisy gradient}$\nNow, our regularization term depends on the network pa-\nrameters, so its gradient is not zero:\n$\\nabla_{\\theta_i} L_{proportional noisy gradient} = 2 \\kappa x_i^2 \\theta_i$\n$\\nabla_{\\theta_i} L_{PDP regularization} = \\nabla_{\\theta_i} L + 2 \\kappa x_i^2 \\theta_i$\nLet us finally observe that this term is still different from\nthe standard $L^2$-regularization, which does not depend on the\ninputs:\n$L_{L^2} = \\sum \\theta_i^2$\n$\\nabla_{\\theta_i} L_{L^2} = 2 \\lambda \\theta_i$\nIn fact, both regularization techniques can be easily com-\nbined and incorporated into the standard training procedure\nfor deep neural networks:\n$L_{PDP+L^2 regularization} = L + \\lambda \\sum \\theta_i^2 + \\kappa \\sum \\theta_i^2 x_i^2$\n$\\nabla_{\\theta_i} L_{PDP+L^2 regularization} = \\nabla_{\\theta_i} L + 2 \\lambda \\theta_i + 2 \\kappa x_i^2 \\theta_i$\n$\\nabla_{\\theta_i} = \\nabla_{\\theta_i} L + 2(\\lambda + \\kappa x_i^2)\\theta_i$\nGiven that the goal of differential privacy is protecting train-\ning data (i.e. the inputs $x_i$), we hypothesize that the proposed\nproportional differentially-private regularization term, PDP,\nshould be more effective than the popular DP-SGD algorithm\nin practice. In addition, it would also be more efficient, since\nthe introduction of noise when computing the gradients in\nSGD is replaced by an extra regularization term in the loss\nfunction used to train the network, which can then be trained\nusing the standard SGD optimization algorithm of our choice\n(e.g. Adam)."}, {"title": "APPENDIX", "content": "A. Expectations (and variances)\n1) Linearity of expectations: The expected value operator\n(or \"expectation operator\") E[\u00b7] is linear in the sense that, for\nany random variables X and Y, and a constant a:\n$E[X + Y] = E[X] + E[Y]$\n$E[a \\cdot X] = a E[X]$\nThis means that the expected value of the sum of any finite\nnumber of random variables is the sum of the expected values\nof the individual random variables, and the expected value\nscales linearly with a multiplicative constant.\nThe variance of a random variable X is the expected value\nof the squared deviation from the mean of X: Var[X] =\n$E[(X \u2013 E[X])^2] = E[X^2] \u2013 E[X]^2$. Therefore, $E[X^2]$\nVar[X] \u2013 $E[X]^2$.\nVariance is invariant with respect to changes in a location\nparameter, i.e. Var[X + a] = Var[X]. However, when all\nvalues are scaled by a constant, the variance is scaled by the"}, {"title": null, "content": "square of that constant: Var[aX] = a\u00b2Var[X]. In general,\nthe variance of the sum of two random variables is\nVar[X + Y] = Var[X] + Var[Y] + 2Cov[X,Y]\nwhere Cov[X,Y] is the covariance Cov[X, Y] = E[(X \u2013\nE[X])(Y \u2013 E[Y])] = E[XY] \u2013 E[X]E[Y] using the linearity\nproperty of expectations.\n2) Non-multiplicativity of expectations: If X and Y are\nindependent, then E[XY] = E[X]\u00b7E[Y]. However, in general,\nthe expected value is not multiplicative, i.e. E[XY] is not\nnecessarily equal to E[X] E[Y]. In fact, Cov[X,Y] =\nE[XY] \u2013 E[X]E[Y].\nThe variance of the product of two independent random\nvariables is Var[XY] = E[(XY)\u00b2] - E[XY]\u00b2 =\nE[X\u00b2]E[Y\u00b2] (E[X]E[Y])\u00b2 = Var[X]Var[Y] +\nVar [X]E[Y]\u00b2 + Var[Y]E[X]\u00b2, which can be rewritten\nas Var[X]E[Y]\u00b2 + Var[Y]E[X]\u00b2 + ($Cov[X,Y]/\\rho[X,Y]$)\u00b2,\nwhere $\\rho[X,Y]$ is the Pearson correlation coefficient,\n$\\rho[X,Y]$ = $Cov[X,Y]/\\sqrt{Var[X]Var[Y]}$.\nB. Normal distributions\nA normal or Gaussian distribution with mean $\\mu$ and variance\n$\\sigma^2$ is a continuous probability distribution for a real-valued\nrandom variable whose probability density function is\n$P(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\nIf X is distributed normally with mean $\\mu$ and variance\n$\\sigma^2$, then aX + b, for any real numbers a and b, is also\nnormally distributed, with mean $a\\mu$+b and variance a2$\\sigma^2$. That\nis, the family of normal distributions is closed under linear\ntransformations. Hence, E[kX] = k$\\mu$ because E[X] = $\\mu$.\n1) Product of normal distributions: The distribution of a\nproduct of two normally distributed random variables X and\nY with zero means and variances $\\sigma^2$ and $\\sigma$ is given by\n$P_{XY}(u) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} P_X(x) P_Y(y) \\delta(xy - u) dx dy$\n$= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\frac{1}{\\pi \\sigma_X \\sigma_Y} K_0(\\frac{|u|}{\\sigma_X \\sigma_Y})$\nwhere $\\delta(x)$ is Dirac's delta function and $K_n(z)$ is a modified\nBessel function of the second kind.\n$K_0(z) = \\int_0^{\\infty} cos(z sinh t) dt$\n$= \\int_{\\infty}^{\\infty} \\frac{cos(zt)}{\\sqrt{t^2 + 1}} dt$\n2) Square of normal distributions: For a general normal\ndistribution X ~ N($\\mu$, $\\sigma^2$), you can use the fact that X = $\\mu$+\n$\\sigma$N where N is a standard normal (zero mean, unit variance)\nto get\nX2 = $\\mu^2$ + 2$\\sigma \\mu$N + $\\sigma^2$ N2\nFor a zero-mean normal distribution, X ~ N(0,$\\sigma^2$), X2 =\n$\\sigma^2$N2. X2/$\\sigma^2$ follows a Chi-squared distribution with 1"}, {"title": null, "content": "degree of freedom, i.e. X2/$\\sigma^2$ ~ $\\chi_1^2$ (a non-central Chi-\nsquared distribution in general, when the mean is not zero).\n\u03a7 ~ N(0, $\\sigma^2$)\n\u03a7\u00b2 ~ $\\sigma^2 \\chi_1^2$\nSince $\\mu$ = E[X] = 0 and Var[X] = E[X2] \u2013 E[X]2,\nE[X2] = Var[X] = $\\sigma^2$\nFinally,\nVar[X2] = E[X4] \u2013 E[X2]2\nE[X4] = 3$\\sigma^4$\nVar[X2] = E[X4] \u2013 E[X2]2 = 3$\\sigma^4$ \u2013 $\\sigma^4$ = 2$\\sigma^4$\nNOTE: X2 ~ $\\sigma^2 \\chi_1^2$. Since E[$\\chi_1^2$] = 1 and Var[$\\chi_1^2$] = 2, then\nE[X2] = $\\sigma^2$ and Var[X2] = 2$\\sigma^4$."}]}