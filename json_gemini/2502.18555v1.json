{"title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision.", "authors": ["Erick da Silva Farias", "Eduardo Palhares J\u00fanior"], "abstract": "The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long Short-Term Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.", "sections": [{"title": "1. Introduction", "content": "Violence is a complex phenomenon that permeates the history of humanity, manifesting itself in different ways and in different contexts. Since the beginning of civilization, violence has been present in wars, territorial conflicts, and power disputes. Over time, new manifestations emerged, such as domestic violence, urban crime, and terrorist attacks. Violence manifests itself in seemingly trivial situations, such as fights in bars or traffic conflicts. These episodes reflect social tensions, accumulated frustrations, and, often, the lack of adequate conflict resolution mechanisms. The culture of aggression and the normalization of violence in social relationships can intensify these situations, creating a cycle that is difficult to break.\nIn many contexts, social inequality, poverty, and marginalization also fuel violence, creating an environment conducive to organized crime and urban violence. Thus, violence in today's world is a multifaceted phenomenon that requires a critical and multi-disciplinary approach to understand it and, above all, combat it. Analysis of its historical, social, and cultural roots is fundamental to developing effective prevention and intervention strategies.\nSurveillance cameras are widely used in commercial establishments, homes, industries, schools, and public places. These cameras are intended to assist agents who monitor the location, however, this type of conventional monitoring is not very effective when hundreds of cameras are deployed because of human involvement, because identifying incidents using conventional cameras becomes an inefficient task.\nAn efficient way to identify incidents via a surveillance camera would be through computer vision, because images from the CCTV system can be linked to a trained deep learning model to make inferences about incidents related to violence between humans in real time. This approach to using computer vision is relevant as it will eliminate the cost of surveillance by humans. But for this to work, it is necessary to carry out tests, collect images to train the model, compare deep learning models, and other adjustment processes to refine the human conflict detection system.\nWith respect to data collection, it is important that the data set has a significant volume, with variance in class data and good resolution. According to [Dashdamirov 2024], for effective algorithm training, the collection and labeling of a vast volume of data is essential. Although there are public sets of videos available, there is still a significant need to expand the amount of this data. Furthermore, aspects such as video resolution, frame frequency, lighting conditions, and camera angles vary greatly. These differences complicate the development of models that are both robust and capable of generalizing appropriately.\nThe use of Deep Learning in the context of human conflict monitoring is relatively new, because the data available publicly has a small volume and has low quality in the video frames. [Dashdamirov 2024] evaluates deep learning techniques in detecting violence in videos, highlighting that increasing the dataset from 500 to 1,600 videos improves the average accuracy of the models by 6%. It demonstrates the importance of large data sets and transfer learning for more effective surveillance systems.\n[Datta et al. 2002] analyzed the trajectory of movements and orientation of body limbs to detect violent behavior. [Nguyen et al. 2005] introduced a hierarchical hidden Markov model (HHMM), showing that it can be useful for recognizing aggressive attitudes, especially through a standard HHMM approach aimed at identifying violence.\n[Kim and Grauman 2009] combined probabilistic Principal Component Analysis (PCA), used to identify flow patterns in local areas, with Markov Random Fields (MRF), which help maintain global model coherence. On the other hand, [Mahadevan et al. 2010] argued that optical flow-based representations are not suitable for detecting unusual changes in both appearance and motion. They proposed a technique that identifies violent scenes by evaluating elements such as the presence of blood, flames, intensity of movement and sound volume."}, {"title": "2. Methodology", "content": "In this chapter the methodology will be presented. In 2.1, computer vision was discussed. In section 2.2 Deep Learning and Neural Networks were covered, LSTM and BiLSTM in the subtopics and in session 2.3 about the Attention Mechanism."}, {"title": "2.1. Computer Vision", "content": "Computer vision is an area of artificial intelligence (AI) that deals with developing methods that allow computers to acquire, process and interpret visual information from the real world, with the aim of making decisions or providing recommendations [Szeliski 2010]."}, {"title": "2.1.1. Neural Network Models in Computer Vision", "content": "Convolutional neural network (CNN) models are widely used in computer vision due to their ability to extract hierarchical spatial features from images and videos. CNNs operate by applying filters (or convolutions) to the image to extract local features such as edges, textures and shapes. These models are efficient for tasks such as object detection, scene recognition and action identification in videos [LeCun et al. 2015].\nFor video analysis, CNNs are often combined with temporal models such as Long Short-Term Memory (LSTM) networks in order to capture dynamic information over time, effectively integrating spatial and temporal learning [Simonyan and Zisserman 2014]."}, {"title": "2.2. Deep Learning and Neural Networks", "content": "Deep learning is a subfield of artificial intelligence that relies on deep neural networks to perform complex recognition, classification, and prediction tasks. These networks are composed of multiple layers of processing, allowing them to learn hierarchical representations of data such as images, text and temporal sequences."}, {"title": "2.2.1. Convolutional Neural Networks (CNNs)", "content": "Convolutional neural networks (CNNs) are a class of deep neural networks that have been widely used in computer vision tasks due to their ability to learn efficient representations of visual data. Figure 1 shows a diagram that represents the architecture of a CNN. They are composed of convolutional layers, pooling layers, and fully connected layers. CNNs are effective in extracting spatial features from images, which allows them to detect patterns, such as edges, textures and shapes [LeCun et al. 2015].\nWhen applied to video analysis, CNNs can be used to detect moving objects, such as humans, vehicles, or any other type of interest. These networks are also capable of detecting complex behaviors and interactions by extracting spatial features from each video frame and learning sequential representations."}, {"title": "2.2.2. Long Short-Term Memory (LSTM)", "content": "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture designed to model temporal dependencies in sequential data. Figure 2 shows the LSTM architecture diagram. LSTMs have memory cells that allow the retention of information over time, overcoming the problem of gradient fading that limits other traditional RNNs [Hochreiter and Schmidhuber 1997]."}, {"title": "2.2.3. Bidirectional LSTM (BILSTM)", "content": "Bidirectional LSTM (BiLSTM) is a variation of LSTMs that processes the sequence of inputs in both the forward and reverse directions, which allows the model to have a more complete understanding of temporal dependencies. Figure 3 shows the BiLSTM architecture diagram."}, {"title": "2.3. Attention Mechanism", "content": "The Attention Engine is a fundamental technique in the field of deep learning, used to improve the ability of models to focus on the most relevant parts of input during processing. Instead of treating all elements of the input equally, the Attention Engine allows the model to learn to allocate greater weight to the most informative parts of the input, improving the efficiency and accuracy of predictions. This technique was initially proposed by [Bahdanau et al. 2015], and later refined into models such as the Transformer proposed by [Vaswani et al. 2017], which use attention mechanisms as their central basis."}, {"title": "2.3.1. Types of Attention Mechanisms", "content": "There are different types of attention mechanisms, each with its own characteristics and forms of implementation. Below, we discuss two main types of attention mechanisms: the mechanism of attention with weighted sum and the multi-head attention mechanism. The attention with weighted sum focuses on assigning different weights to input features, allowing the model to prioritize more relevant information while processing sequences. On the other hand, the multi-head attention mechanism improves the model's ability to focus on various parts of the input simultaneously by using multiple attention heads. However, for the purposes of this article, the discussion will be limited to the attention mechanism with weighted sum.\nThe mechanism we use in our model is a simple version of attention, often called weighted sum attention. In this type of mechanism, the model calculates an attention score for each element of the input sequence, using a dot product between the input vector \\(X_i\\) and a weight vector W learned during training. The attention score \\(e_i\\) for each element \\(X_i\\) is given by the formula:\n\\(e_i = tanh(W^T x_i + b)\\)\nwhere:\n\u2022 \\(e_i\\) is the attention score of element \\(x_i\\)\n\u2022 W is the weight vector.\n\u2022 b is the bias.\n\u2022 tanh is the activation function that helps limit the value of the attention score.\nNext, the \\(e_i\\) attention score is normalized using the softmax function, generating the weights of \\(a_i\\):\n\\(a_i = \\frac{e_i}{\\sum_j e_j}\\)\nWhere \\(a_i\\) are normalized attention weights that indicate the relevance of each element of the input. These weights are applied to the input \\(x_i\\) generating a weighted sum of the inputs, as shown in the formula:\n\\(output = \\sum_i a_i x_i\\)\nThis Simple Attention Mechanism allows the model to focus on the most relevant parts of the input, assigning greater weights to inputs that are more informative for the task."}, {"title": "2.3.2. Designed Architecture for Conflict Detection", "content": "The model developed for classifying image sequences is based on a deep neural network designed to capture both spatial and temporal features from the data. The input consists of a sequence of 15 images, each with a size of 100x100 pixels and 3 color channels (RGB), initially processed by a convolutional layer (CNN). The first processing step applies a TimeDistributed layer, allowing the convolutional network to treat each image independently within the sequence.\nTo prevent overfitting, the model uses a Dropout layer immediately after this step, helping to improve generalization. Next, the architecture includes a Bidirectional LSTM layer, enabling the model to consider both past and future contexts of the image sequence, better capturing the temporal relationships between frames.\nAn Attention layer is applied afterward, allowing the model to perform weighting of the most relevant images within the sequence, giving more importance to certain frames to improve the analysis accuracy. The outputs of this layer are passed through several Dense layers, each followed by a new Dropout layer to ensure regularization of the model. Finally, the model includes a dense layer with two neurons, responsible for binary classification.\nThis combination of convolutional, recurrent, and attention layers allows the model to extract and learn complex, dynamic information from the images and their temporal sequences, providing a robust approach for the classification task."}, {"title": "3. Results and Discussion", "content": "The experiments were performed with the MobileNetV2, DenseNet121 and InceptionV3. It was also used BiLSTM in conjunction with the models. The experiments were performed with and without attention mechanism. Regarding the parameters, there were variations in the minimum learning rate (min_lr) and batch size (batch_size).\nThe accuracy obtained during training, which varied according to the use of the attention mechanism, the minimum learning rate and the size of the lot.\nIn relation to the experiments without Attention Mechanism, in the experiment with the minimum learning rate of 0.0005 and batch size 128, the DenseNet121 model obtained the highest accuracy, with 94.75%, followed by MobileNetV2 with 94.25% and InceptionV3 with 94.25%. When the minimum learning rate was reduced to 0.00005 and the batch size was adjusted to 64, MobileNetV2 had the lowest accuracy among all experiments at 89.00%, while DenseNet121 had a slight drop in the value of accuracy, with 93.75% and InceptionV3 had an accuracy of 91.00%. Using Attention Mechanism, the models showed a slight drop in accuracy with the minimum learning rate of 0.0005 and lot size 128. The accuracy of DenseNet121 was 93.25%, DenseNet121 It was 92.50%, and that of InceptionV3 was 91.75%.\nWhen the minimum learning rate was reduced to 0.00005 and the batch size was adjusted to 64, the performance was superior compared to the no-attention experiments. The MobileNetV2 model achieved the best accuracy, with 96.50%, followed by DenseNet121 with 95.50%, and InceptionV3 with 94.25%. In Table 2, the best accuracies of each model are presented, and all models have good results with the accuracy and F1-Score performance metrics.\nAccording to the figure 7, and the reference of the experiments in the table 1, it is observed that Attention Mechanism did not extended training time. For example, experiments 1,2,3,7,8 and 9 have the same parameter settings. Experiments ID, 1,2 and 3 do not have Attention Mechanism and experiments 7,8 and 9 have Attention Mechanism. In other words, the mechanism has no influence on training time. Another relevant analysis that can be observed in Figure 7 is that training time decreases in experiments with reduced batch sizes.\nIn relation to the accuracy graph in Figure 8, knowing that experiment 4 does not use the mechanism, it completes training faster, in relation to the other experiments.\nAnother relevant point in the graph of Figure 8, is that experiment 1 also does not use the mechanism and delay to finish the training. This indicates that the application Attention Mechanism has no influence on model training time.\nIt can be observed that in all the error charts in the figure 8, the reduced batch experiments with the mechanism have found the best minimum. These experiments sought the fastest minimums in the first times, as most of the batches gradients went to a specific direction. Another pattern of experiments 10, 11 and 12. It was that, due to the reduction of the steering speed to the minimum, the randomness led to directions in which the error increased slightly to later find better minimums."}, {"title": "4. Conclusion and Future Works", "content": "This study presented a detailed analysis of the application of Deep Learning models for violence detection in videos, focusing on the comparison of three popular architectures: MobileNetV2, DenseNet121, and InceptionV3. The results obtained demonstrated that all models were effective, with MobileNetV2 standing out by achieving the highest accuracy of 96.50%, especially when the batch size was reduced and the attention mechanism was applied.\nThrough the experiments conducted, it was observed that the selection of appropriate parameters, such as learning rate and batch size, is crucial for optimizing the model's performance. Additionally, although the Attention Mechanism showed a slight reduction in accuracy in some scenarios, it still proved useful in other parameter combinations, such as when a lower learning rate (0.00005) and a batch size adjusted to 64 were used.\nBased on the results achieved, several improvements and new directions can be explored in future works to further enhance violence detection in videos and expand the applications of this technology. The experiments conducted focused solely on the use of videos, but it would be interesting to expand to multimodal data, combining audio, image, and even sensor information, which could contribute to a more robust analysis of the scene."}]}