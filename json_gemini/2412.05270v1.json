{"title": "APOLLO: SGD-LIKE MEMORY, ADAMW-LEVEL PERFORMANCE", "authors": ["Hanqing Zhu", "Zhenyu Zhang", "Wenyan Cong", "Xi Liu", "Sem Park", "Vikas Chandra", "Bo Long", "David Z. Pan", "Zhangyang Wang", "Jinwon Lee"], "abstract": "Large language models (LLMs) demonstrate remarkable capabilities but are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden often necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput, respectively. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face key challenges: (i) reliance on costly SVD operations (e.g., GaLore, Fira); (ii) significant performance trade-offs compared to AdamW (e.g., Flora); and (iii) still substantial memory overhead of optimization states in order to maintain competitive performance (e.g., 1/4 rank in GaLore, and full-rank first momentum in Adam-mini). In this work, we investigate the redundancy in AdamW's learning rate adaption rule and identify that it can be coarsened as a structured learning rate update (channel-wise or tensor-wise). Based on this insight, we propose a novel approach, Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO), which approximate the channel-wise learning rate scaling with an auxiliary low-rank optimizer state based on pure random projection. The structured learning rate update rule makes APOLLO highly tolerant to further memory reduction with lower rank, halving the rank while delivering similar pre-training performance. We further propose an extreme memory-efficient version, APOLLO-Mini, which utilizes tensor-wise scaling with only a rank-1 auxiliary sub-space, achieving SGD-level memory cost but superior pre-training performance than Adam(W). We conduct extensive experiments across different model architectures and tasks, showing that APOLLO series performs generally on-par with, or even better than Adam(W). Meanwhile, APOLLO achieves even greater memory savings than GaLore, by almost eliminating the optimization states in AdamW. These savings translate into significant system benefits: (1) Enhanced Throughput: APOLLO and APOLLO-Mini achieve around 3\u00d7 throughput on an 8\u00d7A100-80GB setup compared to AdamW by fully utilizing memory to support 4\u00d7 larger batch sizes. (2) Improved Model Scalability: APOLLO-Mini for the first time enables pre-training LLaMA-13B model with naive DDP on A100-80G without requiring other system-level optimizations. (3) Low-End GPU Friendly Pre-training: Combined with quantization, the APOLLO series for the first time enables the training of LLaMA-7B from scratch on a single GPU using less than 12 GB of memory. Check the project page at APOLLO.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have achieved remarkable progress across various domains (Brown et al., 2020; Koco\u0144 et al., 2023; Dubey et al., 2024), largely due to substantial increases in model size, now reaching billions of parameters. Training these high-dimensional models demands robust optimization techniques, with the Adam(W) optimizer (Kingma & Ba, 2014; Loshchilov, 2017) emerging as\nthe de-facto standard for stabilizing LLM training (Zhang et al., 2024a) by tracking both first-order and second-order moments. Despite its effectiveness, Adam(W) incurs significant memory overhead, as maintaining both moments effectively triples the memory required relative to the model's parameter size. This results in excessive memory consumption for the optimizer, even with a single batch. For instance, training a LLaMA-7B model from scratch requires at least 58 GB of memory, with 28 GB devoted to AdamW's optimizer states (Zhao et al., 2024). For larger models like GPT-3, with 175 billion parameters, memory demands reach 700 GB for the model alone, leading to a staggering 1.4 TB requirement for AdamW's optimizer states.\nThis excessive optimizer memory usage poses significant challenges in training large-scale LLMs. It compels the community to either use more and higher-end GPUs, or to"}, {"title": "2 RELATED WORK", "content": "Numerous algorithmic improvements have been introduced to tackle the substantial memory overhead in training LLMs. One category focuses on reducing the number of trainable parameters to save memory costs. This includes approaches such as developing high-quality, small-scale models (Liu et al., 2024b; Tang et al., 2024), introducing sparsity during training (Liu et al., 2022; Thangarasa et al., 2023), and implementing low-rank adaptation (Hu et al., 2021). While these methods are effective at reducing memory usage, they often fall short in achieving comparable performance, especially in pre-training scenarios for large models.\nAnother avenue of research targets advancements in optimizers, as exemplified by works such as GaLore (Zhao et al., 2024), Fira (Chen et al., 2024), Flora (Hao et al., 2024), Adam-mini (Zhang et al., 2024b), GaLore-mini (Huang et al.), LDAdam (Robert et al., 2024), GoLore (He et al., 2024), and LoQT (Loeschcke et al.). These approaches have made notable progress but still face significant challenges. Some methods rely on computationally expensive SVD operations (e.g., GaLore and Fira), although recent research shows that random projections can effectively compress gradients during later training stages while still requiring SVD early on (He et al., 2024). Others either exhibit noticeable performance gaps compared to AdamW, or demand substantial memory overhead to maintain competitive performance, as seen in GaLore's 1/4 rank requirement and Adam-mini's reliance on full-rank first momentum.\nIn contrast, APOLLO achieves efficient memory usage entirely without relying on SVD while delivering performance that matches or even surpasses AdamW. Moreover, our extreme variant, APOLLO-Mini, drives memory costs down to SGD levels while maintaining or exceeding the performance of AdamW, setting a new benchmark for memory-efficient optimization."}, {"title": "3 COARSENED LEARNING RATE UPDATE RULE IS ENOUGH FOR LLMS", "content": "In this section, we first revisit the Adam(W) (Kingma & Ba, 2014; Loshchilov, 2017) and reformulate it as an adaptive learning rate algorithm without explicit momentum term (Section 3.1). Then, we propose that the element-wise learning rate update rule can be coarsened with a structured channel-wise learning rate adaptation strategy, with even slightly better model performance by empirical verification."}, {"title": "3.1 Reformulating AdamW as a Pure Adaptive Learning Rate Algorithm", "content": "Vanilla AdamW update rule. AdamW has established itself as the go-to optimizer for Transformer training, leveraging both first moment (the mean of past gradients) and second moment (the variance of past gradients) to adjust updates. This dual momentum-based approach has proven superior to purely first-order optimizers like SGD (Zhang et al., 2024a). Disregarding weight decay, the vanilla AdamW update rule is as follows:\nAt time step t, given a weight matrix $W \\in \\mathbb{R}^{m\\times n}$ (m < n) with gradient $G_t = -\\nabla w$t(Wt), the standard AdamW update rule is defined as:\n$W_{t+1} = W_t - \\eta\\cdot\\widetilde{G}_t, \\widetilde{G}_t = \\frac{M_t}{\\sqrt{V_t} + \\epsilon}$ (1)\nHere, \u03b7 is the learning rate and e is a small constant for numerical stability. The first and second moment, $M_t$ and $V_t$, are computed as exponentially weighted averages:\n$M_t = \\beta_1 M_{t-1} + (1 - \\beta_1)G_t$\n$V_t = \\beta_2 V_{t-1} + (1 - \\beta_2)G_t^2$\nwhere \u03b21, \u03b22 \u2208 [0, 1) are the exponential decay rates.\nViewing AdamW as an adaptive learning rate algorithm without momentum. The above update rule in equation 1 can then be reformulate as an element-wise gradient scaling rule with a gradient scaling factor $S = \\frac{G_t}{\\sqrt{V_t}}\\in \\mathbb{R}^{m\\times n}$ over the raw gradient Gt, i.e.,\n$W_{t+1} = W -\\eta.\\frac{G_t}{\\sqrt{V_t}}.G_t$ (2)"}, {"title": "3.2 Coarsening Element-wise Learning Rate Adjustment in a Structured Manner", "content": "While the element-wise learning rate update rule in AdamW is effective, it can be overly sensitive to noisy gradients in specific parameters, especially in high-dimensional models like large language models (LLMs). Recent work, such as Adam-mini (Zhang et al., 2024b), proposes to grouping parameters into blocks and applying a block-wise learning rate adjustment to reduce memory usage while maintaining on-par performance as Adam. However, the block-wise approach in Adam-mini (Zhang et al., 2024b) requires carefully chosen block sizes for different modules in Transformers and only achieves memory savings for the second moments, leaving the first moment memory unaffected.\nA more structured learning rate update rule. Inspired by these findings, we propose an effective simplification by coarsening the element-wise adaptive learning rate rule in equation 2 into a structured channel-wise adaptation. We group parameters based on the larger dimension of the weight tensors. The element-wise scaling factor $S = \\frac{G_t}{\\sqrt{V_t}}$ is then simplified into a channel-wise format, $s \\in \\mathbb{R}^{1\\times n}$, where each element sj for each channel j is:\n$s_j = \\frac{\\|\\widetilde{G}_t[:, j]\\|_2}{\\|\\widetilde{G}_t[:, 1] \\|_2} \\frac{\\|{G}_t[:, 1] \\|_2}{\\|{G}_t[:, j] \\|_2} $ (3)\nwhere || ||2 denotes the l2 norm. Then, the final gradient scaling rule becomes $\\widetilde{G}_t = S \\cdot G_t = G_t \\cdot diag(s)$.\nEmpirical validation. We first empirically explore the effectiveness of the proposed update rule where we compared the training loss of the original element-wise learning rate adaptation with our proposed channel-wise adaptation on a LLaMA-130M model. As shown in Figure 3, both approaches achieve similar loss reduction over training steps, demonstrating that the structured adaptation effectively maintains performance. In fact, the channel-wise adaptation achieves slightly better perplexity 24.43 (AdamW: 25.08), further supporting the effectiveness of our approach. However, we notice that our channel-wise learning rate adaption (orange curve) shows a significant spike at the early stage, which is due to the unstable gradient at the early stage. Instead of applying the vanilla gradient clipping method, we"}, {"title": "4 APOLLO: APPROXIMATED GRADIENT SCALING FOR MEMORY EFFICIENT LLM OPTIMIZATION", "content": "From observation to practical Benefit. While coarsening gradient scaling factors is effective, it does not inherently reduce optimizer memory usage. Computing structured gradient scaling factors still requires access to the full optimizer states Mt and Vt, which consume significant memory. This brings us to a critical question:\nQuestion: Can structured learning rate adaptation be converted into practical, memory-efficient optimization?"}, {"title": "4.1 APOLLO: Approximate Structural Gradient Scaling for LLM Optimization", "content": null}, {"title": "4.1.1 Approximating Gradient Scaling with an Auxiliary Low-Rank Space", "content": "To address this challenge, we propose APOLLO, which approximates the channel-wise gradient scaling in a compressed low-rank space rather than the original full-rank one, showing in Algorithm 1. Specifically, an auxiliary low-rank optimizer state is stored by taking the low-rank gradient Rt as input, which is computed as $R_t = P_t G_t \\in \\mathbb{R}^{r\\times n}$, using a pre-defined projection matrix $P_t \\in \\mathbb{R}^{r\\times m}$.\nThe auxiliary optimizer state will only maintain the low-rank version of the first and second moments as:\n$M_t^R = \\beta_1 M_{t-1}^R + (1 -\\beta_1)R_t$\n$V_t^R = \\beta_2 V_{t-1}^R + (1 -\\beta_2)R_t^2$\nThese low-rank moments, $M_t^R$ and $V_t^R$, are then converted into a lightweight, channel-wise scaling factors:\n$s^R_j = \\frac{\\|R_t[:, j]\\|_2}{\\|R_t[:, 1] \\|_2}, where R_t = \\frac{M_t^R}{\\sqrt{V_t^R} + \\epsilon}$ (5)\nIn this way, APOLLO estimates the channel-wise gradient scaling factor s with the auxiliary low-rank optimizer state, saving memory from 2mn to 2nr. We will show later that"}, {"title": "4.1.2 APOLLO Performs Well with Random Projection: SVD is Not Necessary.", "content": "To answer the above questions, we first analyze the norm difference between the first moment $M_t^R$ in the compressed space and the original space, as well as similar results for the second state $V_t^R$ and $V_t$.\nWe demonstrate that random projection can effectively bound the difference between the gradient scaling factor in the compact and original space in equation 6:\nOriginal space: $s_j = \\frac{\\| \\widetilde{G}_t [:, j]\\|}{\\frac{M_t}{\\sqrt{V_t}}}$\nCompact space: $s_j^R = \\frac{\\| \\widetilde{R}_t [:, j]\\|}{\\frac{M_t^R}{\\sqrt{V_t^R}}}$ (6)\nwith all small e in the denominators removed for simplicity.\nGenerating random projection matrix. We generate the random projection matrix P by sampling each element from a standard Gaussian distribution. With high probability, projection using a random Gaussian matrix largely preserves the scaled norm from the original space based on the Johnson-Lindenstrauss lemma (JLT) (Freksen, 2021)."}, {"title": "4.2 APOLLO-Mini: Achieve Extreme Memory Efficiency with Rank-1 Space", "content": "The rank r plays a crucial role in balancing memory efficiency and the quality of the approximated gradient scaling factor. Although the coarsening learning rate update rule provides high tolerance to relatively low rank, we show our APOLLO can reduce rank by half compared to GaLore with a slight impact on perplexity. We still need to pay $n\\times r$ memory cost for the optimizer state. If we can relax the rank requirement to 1, then the optimizer state cost is totally negligible. However, simply setting rank to 1 in APOLLO using channel-wise gradient scaling doesn't work well due to rank-1 space sacrificing too much information. Details at Section 5.4 A2. This leads to our next question:"}, {"title": "4.3 Savings and cost analysis", "content": null}, {"title": "5 EXPERIMENTS", "content": "In Section 5.1 and 5.2, we systematically evaluate APOLLO on various pre-training and downstream tasks, respectively. Section 5.3 compares the memory overhead and throughput"}, {"title": "5.1 Memory-Efficient Pre-training with APOLLO", "content": "We demonstrate that APOLLO achieves superior pre-training performance across various sizes of LLaMA models (60M to 7B) on the C4 dataset (Raffel et al., 2020), with up to a 2.80 reduction in validation perplexity. Additionally, APOLLO-Mini uses negligible memory budget for optimization states while outperforming both AdamW (Loshchilov, 2017) and GaLore (Zhao et al., 2024).\nSetup. We consider the LLaMA series models for pre-training, with sizes ranging from 60M to 7B. Following the training configurations used in prior works (Zhao et al., 2024; Lialin et al., 2023), we pre-train each model from scratch, with a detailed description in Appendix.A.4. The C4 dataset (Raffel et al., 2020), a comprehensive corpus derived from Common Crawl data and meticulously filtered and cleaned, is used for pre-training. All experiments are conducted in BF16 data format without other quantization.\nBaselines. For comparative analysis, we include the following baselines in our evaluation: (i) AdamW: We pre-train the models using the original AdamW optimizer (Loshchilov & Hutter, 2019), maintaining the weights, gradients, and optimizer states in their full-rank format. (ii) Low-Rank: This approach decomposes the model weights into two low-rank matrices ($W = UV$), with both U and V optimized using AdamW. (iii) LORA: LORA (Hu et al., 2021) employs low-rank adapters for memory-efficient training by decomposing the original weights as $W = W_0 +"}, {"title": "5.2 Memory-Efficient Fine-tuning with APOLLO", "content": "Pre-training large foundation models typically demands thousands of GPUs and months of training, making it feasible only for large organizations. As a result, fine-tuning these models has become a more practical approach among engineers and researchers. Here, we thoroughly evaluate the performance of APOLLO in fine-tuning scenarios.\nSetup. We employ three open-source pre-trained models in the fine-tuning experiments, including LLaMA-3.2-1B, LLaMA-3-8B (AI@Meta, 2024), Gemma-7B and Mistral-7B (Jiang et al., 2023). The downstream tasks are divided into two categories: (i) Eight common-sense reasoning tasks: Winogrande (WG)(Sakaguchi et al., 2021), PIQA(Bisk et al., 2020), SIQA (Sap et al., 2019), Open-BookQA (OBQA) (Mihaylov et al., 2018), HellaSwag (HS)(Zellers et al., 2019), BoolQ (Clark et al., 2019), and ARC (ARC-Easy and ARC-Challenge) (Clark et al., 2018); (ii) MMLU (Hendrycks et al., 2020) tasks across various domains: STEM, Social Sciences, Humanities and others.\nBaseline. We compare APOLLO against several baselines used during the pre-training experiments, including Full-rank AdamW (Loshchilov, 2017), LoRA (Hu et al., 2021), GaLore (Zhao et al., 2024), and Fira (Chen et al., 2024). Details of these baselines are summarized in Section 5.1. Additionally, we include DoRA (Liu et al., 2024a), an effective fine-tuning approach. We set the rank to 32 and 8 for common-sense reasoning and MMLU tasks, respectively. For MMLU, due to the rank being already small, like 8, we don't run APOLLO-Mini."}, {"title": "5.3 End-to-End System-level Benefits", "content": "We further validate the system-level benefits, end-to-end throughput, and memory overhead by running LLaMA-7B on 8x A100-80GB GPUs, comparing APOLLO and APOLLO-Mini against AdamW in Fig. 1."}, {"title": "5.4 Extra Investigation and Ablation Study", "content": "This section presents multiple experimental investigations, focusing on four key research questions: Q1: How can the scaling factor subspace be identified? Q2: Is APOLLO sensitive to the number of rank? Q3: What is an appropriate granularity for scaling factors? Q4: How do detailed comparisons evolve throughout the training process? Q5: How APOLLO performs in long-context training setting?"}, {"title": "A1: Similar performance between Random Projection (RP) and Singular Value Decomposition (SVD).", "content": "Previous low-rank gradient-based approaches (Zhao et al., 2024) rely on SVD to identify the gradient subspace, frequently updated during training. This process is time-consuming, thereby affecting training throughput. For a LLaMA-7B model, each SVD operation takes approximately ten minutes, resulting in an additional 25 hours of training time over 150K steps when the subspace is updated every 1,000 steps. To alleviate this overhead, (Zhang et al., 2024c) employs a lazy subspace updating strategy, though it still incurs"}, {"title": "A3: APOLLO-Mini remains effective even with a rank of 1.", "content": "We carry out an ablation study on pre-training LLaMA-60M with different rank budgets, as shown in Figure 4 (d). The results demonstrate that GaLore's performance degrades significantly as the rank decreases, matching full-rank AdamW only when the rank is set to 128 (one-quarter of the original dimension), which limits its effectiveness in extreme low-rank scenarios. In contrast, APOLLO exhibits much better robustness with smaller rank settings compared to both GaLore and Fira, achieving performance comparable to full-rank AdamW even with lower ranks.\nInterestingly, APOLLO-Mini shows the best rank efficiency, remaining effective even with a rank of 1, clearly outperforming AdamW. By averaging the gradient scaling factor across different channels, APOLLO-Mini seems to effectively mitigates the errors introduced by low-rank projection. This capability allows APOLLO-Mini to achieve SGD level memory cost while reaching superior performance than AdamW."}]}