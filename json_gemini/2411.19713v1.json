{"title": "CANTORNET: A SANDBOX FOR TESTING TOPOLOGICAL AND GEOMETRICAL MEASURES", "authors": ["Michal Lewandowski", "Hamid Eghbalzadeh", "Bernhard A. Moser"], "abstract": "Many natural phenomena are characterized by self-similarity, for example the symmetry of human faces, or a repetitive motif of a song. Studying of such symmetries will allow us to gain deeper insights into the underlying mechanisms of complex systems. Recognizing the importance of understanding these patterns, we propose a geometrically inspired framework to study such phenomena in artificial neural networks. To this end, we introduce CantorNet, inspired by the triadic construction of the Cantor set, which was introduced by Georg Cantor in the 19th century. In mathematics, the Cantor set is a set of points lying on a single line that is self-similar and has a counter intuitive property of being an uncountably infinite null set. Similarly, we introduce CantorNet as a sandbox for studying self-similarity by means of novel topological and geometrical complexity measures. CantorNet constitutes a family of ReLU neural networks that spans the whole spectrum of possible Kolmogorov complexities, including the two opposite descriptions (linear and exponential as mea- sured by the description length). CantorNet's decision boundaries can be arbitrarily ragged, yet are analytically known. Besides serving as a testing ground for complexity measures, our work may serve to illustrate potential pitfalls in geometry-ignorant data augmentation techniques and adver- sarial attacks.", "sections": [{"title": "1 Introduction", "content": "Neural networks perform extremely well in various domains, for example computer vision (Krizhevsky et al., 2012) or speech recognition (Maas et al., 2013). Yet, this performance is not sufficiently understood from the mathematical perspective, and current advancements are not backed up by a formal mathematical analysis. We start by identifying a lack of tractable examples that allow us to study neural networks through the lens of self-similarity of objects they describe. This difficulty arises from the inherent statistical nature of these networks and the significant computational effort required to accurately determine the underlying geometry of the decision manifold. We note that the use of constructed examples helps to illustrate certain characteristic effects, such as the concentration of measure effects in high dimensions to explain the vulnerability against adversarial examples (Gilmer et al., 2018). Such examples are typically designed to underscore either the capabilities or limitations of neural architectures in exhibiting certain phe- nomena. However, there exists a risk of oversimplification that might lead to an underappreciation of the complexities and challenges of handling real-world, high-dimensional, and noisy data. Despite these limitations, toy examples are valuable as they can be constructed to emphasise some properties which remain elusive at a larger scale. Further examples include the XOR (Minsky and Papert, 1969) or CartPole problem (Sutton and Barto, 2018) which, despite"}, {"title": "2 Preliminaries", "content": "We define a ReLU neural network N : X \u2192 Y with the total number of N neurons as an alternating composition of the ReLU function \u03c3(x) := max(x, 0) applied element-wise on the input x, and affine functions with weights Wk and biases bk at layer k. An input x \u2208 X propagated through N generates non-negative activation values on each neuron. A binarization is a mapping \u03c0 : RN^ \u2192 {0,1} applied to a vector (here a concatenation of all hidden layer) v = (v1,...,\u03c5\u03bd) \u2208 RN resulting in a binary vector {0, 1}~ by clipping strictly positive entries of v to 1, and non- positive entries to 0, that is \u03c0(z) = 1 if v\u2081 > 0, and \u03c0(vi) = 0 otherwise. An activation pattern is the concatenation of all neurons after their binarization for a given input x, and represents an element in a binary hypercube HN := {0,1}N where the dimensionality is equal to the number of hidden neurons in network N. A linear region is an element of a disjoint collection of subsets covering the input domain where the network behaves as an affine function (Mont\u00fafar et al., 2014). There is an one-to-one correspondence between an activation pattern and a linear region (Shepeleva et al., 2020)."}, {"title": "3 CantorNet", "content": "In this section, we define CantorNet as a ReLU neural network through repeating application of weight matrices, similar to the fractal constructions. We then introduce an equivalent description through unionizing polyhedral bodies, which is less concise. We start the construction with two reshaped ReLU functions (Fig. 1, left), and modify them to obtain a connected decision manifold with Betti numbers b\u2081 = 0 for i \u2208 {0,1,2}, used to characterizes the topological complexity, providing measures of connectivity, loops, and voids within the decision boundaries (Bianchini and Scarselli, 2014). We consider the function\n$A : [0, 1] \\rightarrow [0, 1] : x \\rightarrow max\\{-3x + 1,0,3x - 2\\},$\n(1)\nas the generating function and recursively nest it as\n$A^{(k+1)}(x) := A(A^{(k)} (x)), A^{(1)}(x) := A(x).$\n(2)"}, {"title": "3.1 Recursion-Based Construction", "content": "Note that the decision surface of Rk (Eq. (3)) equals to the 0-preimage of a ReLU net N : [0, 1]2 \u2192 R with weights and biases defined as\n$W\u2081 = \\begin{pmatrix} -3 \\\\ 3 \\\\ 0 \\end{pmatrix},b\u2081 = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}  W\u2082 = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ , and the final layer W\u2081 = ( \u2212 1), b\u2081 = (-). For recursion depth k, we define N(k) as\n(4)\n$ \u039d) N^{(k)} (x) := W\u2081\u03bf\u03c3\u03bfg^{(k)} (x) + b\u2081,$  \n(5)\nwhere g(k+1)(x) := g(1) (g(k) (x)), \u03c3 is the ReLU function, and\n$g^{(1)} (x) := \u03c3\u03bf W\u2082 \u03bf\u03c3\u03bf (W\u2081x\u00b2 + b\u2081).$\n(6)\nWe use to denote the standard composition of functions. Fig. 2 shows the linear regions resulting from the construc- tion described in the Eq. (5) for recursion level k = 1, as well as the linear regions with the corresponding activation patterns from non-redundant neurons (we skip neurons which do not change their state)."}, {"title": "3.2 Triadic Expansion", "content": "In this section, we show that there exists an isomorphism between the triadic expansion, as described in Appendix A in Alg. 1, and the activation pattern \u3160\u0145\u0100 under Nk). In the Triadic Expansion, we partition the interval [0, 1] into three intervals, I\u2081 = [0, 1], I2 = (1, 3), 13 = [3, 1] (see Fig. 1, left). Any x \u2208 I\u2081 U I3 can be described in a triadic system with an arbitrary precision l as x = \u2211=13, where ai \u2208 {0, 2}. Recall that the tessellation of the recursion-based model (Fig. 2) is obtained by partitioning the rectangular domain (I\u2081 U I3) \u00d7 [0, 1] into increasingly fine rectangles through recursive applications of x \u2192 g(k)(x). We identify created linear regions by their activation patterns \u03a0\u039d\u0391\u00b7 Equivalently, we can represent any x \u2208 I\u2081 U I3 using Alg. 1, obtaining activation patterns as a sequence of \u201cO\u201ds, and  Each of these descriptions is unique, therefore there exists an isomorphic relationship between the encoding described in Alg. 1, and the recursion-based description."}, {"title": "Lemma 1", "content": "(Computational Complexity of Activation Patterns of Nk)). Given an input x = (x1,x2) \u2208 [0,1]2 and the recursion level k, its corresponding activation pattern \u03c0(x) under the recursion-based representation Nk can be computed in O(k) operations.\nProof. The complexity (as measured by the description length (Gr\u00fcnwald et al., 2005)) of the decision manifold given by Eq. (3) is equal to the complexity of its partition into the linear regions defined in Sec. 2. To determine the minimal complexity of the partition it is necessary to solve the following decision problem for 21. Consider the partition into linear regions and its projection along the y-axis onto the [0, 1] \u00d7 {0}. Note that the resulting partition of [0, 1] is the same we obtain by constructing the Cantor set of level k, which corresponds to the triadic number expansion up to the kth digit. The minimal complexity of solving this decision problem is therefore O(k).\nThe proof of Lemma 1 indicates the 1-1 correspondence between the triadic number expansions up to the kth digit and the activation pattern \u03c0(x1) of x1 \u2208 [0,1] under \u00d1(k), where \u00d1(k) represents the 1-dim ReLU network up to the recursion level k, analogous to the construction given by Eq. (5). Observe that the outset (as in Def. 2) intervals I1, I2, I3 (as in Fig. 1, left) can be described with activation patterns \u03c0\u039d\u0391(x) = [10111], for any x \u2208 I1, \u03c0\u039d\u2084(x) = [00101], for any x \u2208 I2 or \u03c0\u039d\u0391(x) = [01111] for any x \u2208 I3 in the recursion-based representation (here we do not remove neurons with constant values). Indeed, to obtain \u03c0\u03b1 for x \u2208 I\u2081 take any point (x, y) \u2208 I\u2081 \u00d7 {y \u2208 [0, 1] : y < f1(x)} (f1 and f2 as in Fig. 1, left). After applying x \u2192 g(1)(x), binarizing every neuron's value and concatenating them into a vector, we obtain a 5-dim vector (because W\u2081 \u2208 R3\u00d72, W2 \u2208 R2\u00d73, and we omit WL, 6L). In an analogous manner, we can obtain \u03c0\u0399\u2082 and \u03a0\u03993. Next, observe that each of the intervals Ii can be further partitioned into Ii1, Ii2, Ii3, respectively for the left, center, and right segments. To describe these new segments, we increase the recursion level to k = 2. It turns out that I\u2081\u2081 = [10111; 10111], a repetition of the pattern \u03c0\u2081\u2081, and so forth for the remaining segments. This construction is iterated k times, providing the sequence of subintervals\n$(I_{i_{t}})_{t=1}^{k}.$\n(7)"}, {"title": "3.3 Alternative Representation of CantorNet", "content": "Observe that the pre-image of zero under a ReLU function (including shifting and scaling) is a closed set in [0, 1]2. Since we consider a decision manifold M as a closed subset, which we referred to as inset in Def. 2 in Section 3, we use the closed pre-image of zero under the ReLU network N to model decision manifolds given by Eq. (3). This means that the statement \u201cx \u2208 M\u201d is true if N (x) = 0, and the statement \"x \u2208 M\u201d is false if (x) > 0. This way the min operation refers to the union of sets, i.e., logical disjunction \"OR\u201d, while the max operation refers to the intersection of two sets, i.e., logical conjunction \"AND\". Note that the laws of Boolean logics also translate to this interpretation (Klir and Yuan, 1995). Further, note that any (non-convex) polytope is a geometric body that can be represented as the union of intersections of convex polytopes. A convex polytope can also be represented as intersection of half-spaces, like the pre-image of zero under the ReLU function, i.e., a single-layered ReLU network. Thus, a decision manifold M given by a (non-convex) polytope can be represented by the minimum of maxima of single layered ReLU networks.\nSince the minimum operation can also be represented in terms of the max function, we obtain a ReLU representation for which it is justified to call it a disjunctive normal form (DNF), as outlined by Moser et al.. For the simplest case, consider the function h\u2081(x, y) (Fig. 3, left) that splits the unit square [0, 1]2 into parts where it takes positive"}, {"title": "Proposition 1", "content": "At the recursion level k, the decision boundary given by Eq. (3) can be constructed as a disjunctive normal form\n${x,y \u2208 [0, 1]^2 : min(h\u2081 (x, y), h2(x, y), hr(k) (x, y), D1, ..., D_{[r(k)/4]+1}, 0) = 0},$\n(8)\nwhere hi : R2 \u2192 R are affine functions indexed with i = 1, . . ., r(k). The labeling function r(k) : N \u2192 N is given as r(k) = 2k+1 \u2212 1 (see Fig. 4), and D : R2 \u2192 N denotes a \u201cdent\u201d given by\n$Di := max(h_{4i\u22121}, h_{4i}, h_{4i+1}).$\n(9)\nTo provide a better overview for the reader, in Table 1 we list the constructions for the different recursion levels. We"}, {"title": "4 Complexity of Neural Representations", "content": "The complexity of an object can be measured in many ways, for example its description's length (Kolmogorov, 1965). Preference for a more concise description can be argued from multiple angles, for example using the principle of the Occam's razor or lower Kolmogorov complexity. The latter is typically non-computable, necessitating reliance on its approximations. However, in case of models with consistent decision boundaries (e.g., neural networks), their size, both in terms of the number of layers and the number of neurons, can be used as approximation for complex-ity (Gr\u00fcnwald et al., 2005)."}, {"title": "Lemma 2", "content": "At the recursion level k, the number of neurons of the recursion-based representation is O(k), while for the disjunctive normal form representation it is O(2k).\nProof. For the recursion-based representation the result is straightforward. The DNF construction relies on the appli- cation of A and S (see App. B). For recursion level k, we have (recall Eq. (8)) 3+3([r(k)/4]+1) = 3[r(k)/4]+6 =: z(k) rows of A, equal to the number of neurons. By applying A and S at least [log2 z(k)] times (Arora et al., 2018), we arrive at\n$3 \\cdot ((2^{k+1}-1)+6) \\cdot \\sum_{i=0}^{[log_2 z(k)]} \\frac{3}{4} < 2 ((2^{k+1}-1)+6) = O(2^k).$\nIt requires an algorithm of Kolmogorov complexity of order k to enumerate all numbers in [0, 1] with triadic number expansion up to k digits. Since the recursion-based ReLU net NA is constructed by a repetitive application of two layers with constant number of neurons (namely five) and the same weights, its Kolmogorov complexity after k iterations is of order k. As a recursion step given by Eq. (5) and Eq. (6) is equivalent to a recursion in the triadic number expansion (Alg. 1), which is of the minimal order of Kolmogorov complexity, there cannot exist an equivalent ReLU network of strictly lower order of Kolmogorov complexity. This means that Na is of minimal description length in terms of order of the number of neurons N = N(k), thus\n$O(N(k)) = O(k).$\n(10)"}, {"title": "Theorem 1", "content": "The recursion-based ReLU representation given by the Eq. (4) is of minimal complexity order in terms of the number of neurons (in the sense of Eq. (10)).\nThis way, we obtain an example of a ReLU network of proven minimal description length in terms of its number of neurons, a property hardly provable by statistical means. Observe that the above does not hold for singular numbers from [0, 1]: if we consider x = + = 0...0...02...20...02...2...0...0, with n = k2 digits after the ternary point arranged in k alternating blocks of zeros and twos, then it has Kolmogorov complexity O(k) = O(\u221an). Note that both representations have the same order of the number of layers."}, {"title": "Lemma 3", "content": "At the recursion level k both described representations of CantorNet have O(k) layers.\nNote that both constructions are equivalent as understood by the equality of their preimages. Though simple by construction, the family of CantorNet ReLU networks is rich in terms of representation variants, ranging from a minimal (linear in k) complex solution to an exponentially complex one. An intermediate example would be starting with the recursion based representation for a number of layers, and then concatenating corresponding disjunctive normal form representation."}, {"title": "5 Conclusions and Discussion", "content": "In this paper we have proposed CantorNet, a family of ReLU neural networks inspired by fractal geometry that can be tuned arbitrarily close to a fractal. The resulting geometry of CantorNet's decision manifold, the induced tessellation and activation patterns can be derived in two ways. This makes it a natural candidate for studying concepts related to the activation space. Note that, although CantorNet is a hand-designed example, it is not an abstract invention - real world data, such as images, music, videos also display fractal nature, as understood by self-similarity. We believe that our work, although seemingly remote from the current mainstream of the machine learning research, will provide the community with a set of examples to study ReLU neural networks as mappings between the Euclidean input space and the space of activation patterns, currently under investigation."}]}