{"title": "The SVASR System for Text-dependent Speaker Verification (TdSV) AAIC Challenge 2024", "authors": ["Mohammadreza Molavi", "Reza Khodadadi"], "abstract": "This paper introduces an efficient and accurate pipeline for text-dependent speaker verification (TDSV), designed to address the need for high-performance biometric systems. The proposed system incorporates a Fast-Conformer-based ASR module to validate speech content, filtering out Target-Wrong (TW) and Impostor-Wrong (IW) trials. For speaker verification, we propose a feature fusion approach that combines speaker embeddings extracted from wav2vec-BERT and ReDimNet models to create a unified speaker representation. This system achieves competitive results on the TDSV 2024 Challenge test set, with a normalized min-DCF of 0.0452 (rank 2), highlighting its effectiveness in balancing accuracy and robustness.", "sections": [{"title": "1 Introduction", "content": "Text-dependent speaker verification (TdSV) is a method of speaker authentication in which the speaker is required to utter a specific phrase or password, contrasting with text-independent verification (TiSV) that analyzes spontaneous speech or conversational audio. In text-dependent verification, the system leverages both the acoustic and linguistic properties of a predefined phrase to confirm the speaker's identity.\nThis paper presents our work in a recent challenge on text-dependent speaker verification, organized by (Zeinali et al., 2024), provided a large dataset of native Persian speakers. The dataset consisted of a fixed set of phrases, including five sentences in Persian and five in English.\nI-vector methods have seen extensive use in speaker verification over recent years. One notable technique employs bottleneck (BN) features extracted from triphone-state classifiers as input for i-vector extraction(Liu et al., 2015). Unlike traditional approaches, BN-based i-vector extractors utilize Baum-Welch statistics derived from BN features, which effectively capture phonetic details and enhance the representation of textual content in speech. While this method is not entirely new, it remains highly competitive. For instance, (Lozano-Diez et al., 2020) has shown that BN i-vectors can surpass x-vectors in performance when ample in-domain training data is available. However, a key drawback lies in the necessity of training a phonetic-aware deep neural network (DNN) on large-scale speech datasets to produce BN features, which adds significant complexity and cost to the deployment of speaker verification systems.\nMultitask learning has become a common approach in TdSV, as it allows models to leverage both speaker and phoneme information for improved performance. In (Liu et al., 2021) a multitask learning framework was proposed to produce speaker embeddings enriched with phoneme-level knowledge. The architecture includes a shared frame-level encoder, a frame-level phoneme classifier, a speaker classifier, and a segment-level phoneme classifier. Speaker-phoneme multitask learning has demonstrated significant performance improvements on the RSR2015 dataset compared to traditional TDSV systems (Liu et al., 2021). However, the approach relies on an automatic speech recognition (ASR) model to generate phoneme labels, which adds complexity and increases deployment costs.\nMultitask learning can improve TDSV performance during fine-tuning. In (Han et al., 2022), two strategies were explored: speaker + phrase, which uses separate classification heads for speaker and phrase labels, and speaker \u00d7 phrase, which treats utterances with the same speaker but different phrases as distinct classes using a single head. The speaker + phrase approach outperformed speaker \u00d7 phrase on the text-dependent task in SdSVC 2021, demonstrating the effectiveness of multitask"}, {"title": "2 Background", "content": "2.1 Task Definitions\nText-dependent speaker verification is a key approach to identity verification, requiring that both the speaker's identity and a specific spoken phrase match the target criteria. Given a test speech file containing a designated phrase and enrollment data from the target speaker, the system determines if the target speaker indeed uttered the specified phrase. Unlike text-independent verification, where the focus is solely on speaker identity, text-dependent verification requires verifying the content of the speech as well. This creates a dual verification task, necessitating the confirmation of both the speaker and the spoken phrase.\nIn this task, a speaker encoder model must be trained on a large dataset that includes ten predefined phrases. For each speaker, an enrollment model is created using three repetitions of a specific phrase selected from this set. Verification is then performed by having the speaker repeat the same phrase once; based on this input, the system confirms or rejects the speaker's identity.\n2.2 Dataset and Metrics\nThe main dataset for this challenge is the DeepMine dataset, gathered through crowdsourcing(Zeinali et al., 2018)(Zeinali et al., 2019a). Enrollment and test phrases are selected from a fixed set of ten phrases, comprising five in Persian and five in English. However, for target-wrong trials, some test utterances are drawn from free-text phrases. The in-domain training set consists of utterances from 1,620 speakers, with some speakers contributing only Persian phrases.\nThe main performance metric for this challenge is the normalized minimum Detection Cost Function (DCF), as outlined in SRE08. This function combines the miss and false alarm error probabilities into a weighted sum:\n\\(C_{Det} = C_{Miss} \\times P_{Miss|Target} \\times P_{Target} + C_{FalseAlarm} \\times P_{FalseAlarm|NonTarget} \\times (1-P_{Target})\\)\nwhere the parameters are set as \\(C_{Miss} = 10\\), \\(C_{FalseAlarm} = 1\\), and \\(P_{Target} = 0.01\\). To normalize the DCF, it is divided by 0.1, representing the best possible cost if no processing were applied to the input data. Alongside the normalized minimum DCF (minDCF0.01), the Equal Error Rate (EER) is also used as a secondary evaluation metric."}, {"title": "3 System Overview", "content": "This section outlines the model architectures used for the task. Initially, we employed the speaker + phrase approach, a dual-head strategy with separate heads for speaker and phrase classification to jointly optimize both aspects. Subsequently, we adopted a method that processes text independently of speaker information using a single speech recognition model. This latter approach proved more effective in our experiments, achieving superior results.\n3.1 Dual-head Strategy\nIn this section, we employ three approaches. We detail each approach below\nResNet34: We adopted ResNet34 as the backbone for our embedding extractor due to its proven efficiency in modeling complex data structures for speaker verification. Following the architecture introduced in (Zeinali et al., 2019b), input features pass through an initial convolution layer, four residual blocks, and a statistical pooling layer that aggregates frame-level features into a segment-level representation. The final 256-dimensional fully connected layer produces fixed-length speaker embeddings. These embeddings were then used to train both the phrase classification and speaker classification heads in the speaker + phrase approach.\nXEUS: We employed XEUS (Chen et al., 2024), a Cross-lingual Encoder for Universal Speech, as our embedding extractor due to its extensive training on over 1 million hours of multilingual speech data across 4057 languages. XEUS significantly expands the language coverage of self-supervised learning models and enhances robustness through a novel dereverberation objective tailored for diverse multilingual datasets. The XEUS pre-trained"}, {"title": "3.2 SVASR", "content": "In this section, we have a independenet approach for modeling speaker and speech content. So we have a speaker verification with ASR (SVASR).\n3.2.1 speaker embedding extractor models\nWe used two speaker embedding extractor for train a speaker verification model in a text independent manner.\nW2V-BERT: After success of SSL models that trained with masked prediction loss like WavLM (Chen et al., 2022) and Hubert (Hsu et al., 2021) in speaker verifiacation tasks, we used w2v-BERT model (Chung et al., 2021). The w2v-BERT model, inspired by masked language modeling (MLM) in natural language processing, applies MLM for self-supervised speech representation learning. It combines MLM with contrastive learning, where contrastive learning transforms continuous speech into discrete tokens, and MLM helps the model learn contextualized speech representations by predicting masked tokens. Unlike other MLM-based speech models like HuBERT and vq-wav2vec (Baevski et al., 2020), which use multi-stage or separate training processes, w2v-BERT achieves end-to-end optimization by jointly solving both tasks.\nReDimNet: Reshape dimensions network (ReDimNet) is an advanced neural network designed for extracting utterance-level speaker representations by reshaping dimensions between 2D feature maps and 1D signal representations, integrating both 1D and 2D blocks within a single framework (Yakovlev et al., 2024). Its unique topology preserves the channel-timestep-frequency volume, enabling efficient aggregation of feature maps. ReDimNet is scalable, with model variants ranging from 1 to 15 million parameters and computational costs between 0.5 and 20 GMACs, delivering state-of-the-art performance in speaker recognition while minimizing computational requirements and model size. To further improve accuracy, ReDimNet is trained on a challenging dataset, enhancing its performance in speaker recognition tasks.\nFor speaker verification, we propose a feature fusion approach that combines speaker embeddings extracted from wav2vec-BERT and ReDimNet models to form a unified speaker representation. The fusion method employs concatenation, where the embedding vectors from wav2vec-BERT and ReDimNet are first normalized and then combined with equal weighting.\n3.2.2 ASR\nIn the text-dependent speaker verification task, we employ an Automatic Speech Recognition (ASR) system to filter out trials where the enrollment and test utterances originate from different phrases. For this, we use a FastConformer-based ASR model (Rekesh et al., 2023), implemented within the NeMo framework (Kuchaiev et al., 2019). Fast-conformer is a version of Conformer (Gulati et al., 2020) model with reduced size of convolutional kernels and 2x subsampling rate.\nOur starting checkpoint \u00b9 has been finetuned with Persian Mozilla Common Voice from english ASR model 2.\n3.2.3 SVASR Pipline\nIn this section, we describe the interaction between the ASR model and the Speaker Verification model, as outlined in the previous subsections. The ASR system is evaluated by recognizing phrases and classifying utterances based on their Character Error Rate (CER) relative to the reference phrases. The transcriptions generated by the ASR model are used to filter out Impostor-Wrong (IW) and Target-Wrong (TW) trials in the test dataset through punitive scoring, assigning them low scores. For speaker verification, cosine similarity is calculated between the speaker model and the extracted speaker vector, which serves as the final"}, {"title": "4 Experimental Setup and Results", "content": "In this section, we explain our training configuration and setup for each strategy and report our trained models's results on develop and test sets.\n4.1 Dual-head Strategy\n4.1.1 Experimental Setup\nIn the system overview section, we describe the Dual-head strategy models. All models were trained on the challenge dataset's training set using the AdamW optimizer with a learning rate of 1 \u00d7 10-3 and a batch size of 32. We used cross-entropy loss to train the ResNet model for phrase classification within the hybrid network, while AAM-softmax loss (Wang et al., 2018) was employed to train the ResNet34 and XEUS-based models in the dual-head strategy.\n4.1.2 Results\nWe evaluated the performance of all models on the development set. Based on the results from the development set, we calculated the Log-Likelihood Ratios (LLR) for the test set trials using the best-performing model. Details are provided in Table 1.\n4.2 SVASR\n4.2.1 Experimental Setup\nAs outlined in the previous section, we trained the w2v-BERT and ReDimNet models separately. For w2v-BERT, we fine-tuned the TDNN layers, while the entire ReDimNet network was fine-tuned, both using the training set from the challenge dataset. Each model was optimized over 5 epochs using the"}, {"title": "4.2.2 Results", "content": "As we explined before, ASR system is a front-enf of our pipeline. For realize real performance of our ASR, we compare result of our system on TC-vs-TW mode. Means that, we just select these trials and compute min-DCF on them. We have evaluated our trained model on develop and test sets. For each experiment, a threshold is set for accepting a trial is target or imposter based on CER. In Table 2 we compare result of each ASR model.\nAfter training the ASR model, it was integrated into the pipeline, and a series of experiments were conducted on the speaker models that were trained. The results revealed that models trained with SphereFace2 loss outperformed others. This is attributed to the loss function's ability to map speaker embeddings onto a spherical space, thereby maximizing the margin between different speakers. Subsequently, feature fusion via concatenation was employed to further enhance performance. For scoring, cosine similarity was calculated between the fused speaker model and the fused test utterance speaker embedding. Details are provided in Table 3."}, {"title": "5 Conclusion", "content": "Based on the obtained results, we propose a solution for the challenge of text-dependent speaker verification where the input sentence is predetermined. Our approach involves employing an initial filtering stage based on an Automatic Speech Recognition (ASR) model. In this stage, the speech content is first validated, followed by the speaker verification process.\nIn the second stage, speaker verification is refined by combining embeddings from two models. Speaker embeddings extracted using wav2vec-BERT are concatenated with those obtained from ReDimNet to form a unified speaker representation. This concatenated vector serves as the final speaker embedding, effectively leveraging the strengths of both models to enhance precision, particularly for challenging trials.\nFor future work, we suggest integrating this system with foundational models while leveraging knowledge distillation techniques to create smaller, task-specific versions of these foundational models for each component."}]}