{"title": "Learning Frame-Wise Emotion Intensity for Audio-Driven Talking-Head Generation", "authors": ["Jingyi Xu", "Hieu Le", "Zhixin Shu", "Yang Wang", "Yi-Hsuan Tsai", "Dimitris Samaras"], "abstract": "Human emotional expression is inherently dynamic, complex, and fluid, characterized by smooth transitions in intensity throughout verbal communication. However, the modeling of such intensity fluctuations has been largely overlooked by previous audio-driven talking-head generation methods, which often results in static emotional outputs. In this paper, we explore how emotion intensity fluctuates during speech, proposing a method for capturing and generating these subtle shifts for talking-head generation. Specifically, we develop a talking-head framework that is capable of generating a variety of emotions with precise control over intensity levels. This is achieved by learning a continuous emotion latent space, where emotion types are encoded within latent orientations and emotion intensity is reflected in latent norms. In addition, to capture the dynamic intensity fluctuations, we adopt an audio-to-intensity predictor by considering the speaking tone that reflects the intensity. The training signals for this predictor are obtained through our emotion-agnostic intensity pseudo-labeling method without the need of frame-wise intensity labeling. Extensive experiments and analyses validate the effectiveness of our proposed method in accurately capturing and reproducing emotion intensity fluctuations in talking-head generation, thereby significantly enhancing the expressiveness and realism of the generated outputs.", "sections": [{"title": "1 Introduction", "content": "Audio-driven talking-head synthesis [9, 10, 48, 51, 25] has drawn increasing attention due to its wide range of applications in various fields such as virtual reality, digital humans [12], and assistive technologies. Recently, many works have focused on synchronizing lip movements with speech content [25, 3, 5, 32]. However, in addition to speech, humans also convey intentions through emotional expressions. Hence, developing emotionally expressive talking-heads is crucial to enhance the fidelity of these systems for real-world applications.\nUnlike static displays of emotion, our emotional expressions are inherently dynamic and fluid with smooth transition in terms of emotion intensity levels. Imagine a scenario where someone receives unexpected good news. Initially, they may smile slightly as they process it. As they fully comprehend, their smile might widen and their eyes might brighten, reflecting growing excitement and happiness. This progression illustrates the dynamic nature of human expressions with smooth shifts in emotion intensity levels, unlike static displays such as a fixed smile.\nWhile there have been previous efforts to generate emotion-aware talking-heads [10, 13, 41, 17], the natural fluctuations in intensity levels have been largely overlooked. Addressing this requires the model to 1) infer smooth emotion intensity flows based on audio cues and 2) precisely generate facial expressions that correspond to the inferred intensity levels. However, capturing these intensity fluctuations can be challenging in existing emotional-aware talking-head methods: 1) the model would rely on frame-wise intensity annotations, which are difficult to obtain in practice; 2) there should be mechanisms to control the generated intensity precisely, instead of homogeneous outputs.\nIn this paper, we propose a framework for talking-head generation that effectively tackles the two challenges as mentioned above, resulting in talking-head videos with natural intensity flows. In particular, we first introduce a method to obtain audio-synchronized intensity fluctuations without any frame-level annotations. This is achieved through an emotion-agnostic intensity pseudo-labeling method that accurately quantifies the intensity of any given frame based on keypoint distance. An audio-to-intensity predictor is then employed to predict these frame-wise pseudo intensities based on the speaking tone, as the speaking tone often reflects emotional fluctuations.\nFurthermore, our goal is to generate talking-heads with emotions that correspond accurately to the inferred intensity levels. This task is particularly challenging because varying intensities often manifest as subtle and smooth differences, which are difficult to capture. We draw inspiration from the \"wheel of emotion\" [2], where emotions are arranged on a disk with similar emotions positioned close to one another, and intensity is represented by the distance from the center. Instead of using a disk, we propose constructing a continuous and unified latent space that naturally represents all types of emotions at various intensity levels. In this space, the neutral emotion is placed at the origin, with similar emotions located nearby. The emotion type is encoded in the latent orientation, while the intensity is represented by the latent norm. This design enables smooth transitions between emotional intensities and types, enhancing the efficiency of the learning process. Quantitative results on MEAD [34] and LRW [6], along with qualitative analyses, demonstrate the effectiveness of this method in generating diverse expressions with precise control over intensity levels.\nOur main contributions can be summarized as follows:\n\u2022 We present a talking-head generation framework that considers the emotion intensity for producing smooth transitions across facial expressions.\n\u2022 We introduce an audio-to-intensity predictor to capture the dynamics of emotion intensity in audio input, without the need of having frame-wise ground truth labels for intensity.\n\u2022 We propose an approach for reorganizing the latent space of emotions in talking-head generation, enabling both diverse emotional expressions and precise control over intensity levels."}, {"title": "2 Related Work", "content": "Audio-Driven Talking Head Generation. Early works on audio-driven talking-head generation mainly focus on producing accurate lip motion [25, 3, 5, 32, 50]. Wav2Lip [25], for example, leverages audio signals to synchronize lip movements within a provided video sequence. Subsequent advancements have led to techniques for generating full-head animations [53, 28, 39, 49, 47, 46]. Audio2Head [35] employs keypoint-based dense motion fields to manipulate facial images for enhanced realism. Recently, transformer-based architecture has been adopted for this task due to its ability to capture the long-term context of audio inputs and generate complete sequence-level representations. AVCT [36] designs an audio-visual correlation transformer for generating talking-head videos. EAT [10] utilizes an audio-to-expression transformer to map audio sequences to enhanced 3D latent keypoints. In this paper, different from the above-mentioned approaches, we focus on another important aspect of talking-head generation, i.e., precise modeling of emotion intensity and fluctuations, thereby enriching the naturalness of generated results.\nEmotion-Aware Talking Head Generation. Emotional talking-head generation has recently attracted growing interest within the field [10, 13, 41, 17, 33, 34, 24, 22, 30]. Wang et al. release MEAD [34], a high-quality talking-head video dataset annotated with emotion categories and intensity. Gan et al. [10] use this dataset to fine-tune a pre-trained emotion-agnostic talking-head transformer for emotion-aware talking-head generation. SPACE [13] leverages facial landmarks and latent keypoints as intermediate face representations, facilitating talking-head generation with control over emotion type and intensity level. However, these methods encode emotion labels using discrete one-hot vectors, which restricts the diversity of the generated expressions. Moreover, MM-ESL [41] constructs a unified feature space for various emotion types and controls the expression simply with a"}, {"title": "3 Our Proposed Method", "content": "In this section, we introduce our proposed method for generating talking-heads with frame-wise emotion intensity. Figure 1 shows an overview of the pipeline. Specifically, to generate talking-heads with fluid intensity transitions, we first infer these fluctuations from audio input using an audio-to-intensity predictor. Then, for a specified driving emotion e (e.g., happy), we map it onto our proposed emotion latent space and adjust its norm based on the inferred intensity fluctuations. These resulting intensity-aware emotion features then serve as signals for a transformer-based talking-head generation model, producing natural talking-head videos. In the following subsections, we will elaborate on the details of our intensity pseudo-labeling method, the audio-to-intensity predictor training, and the reorganization of the intensity-aware emotion latent space."}, {"title": "3.1 Frame-Wise Intensity Modeling", "content": "In order to learn the intensity directly from the audio input, we propose a pseudo-labeling method to avoid the frame-wise labeling requirement, which is challenging to achieve in practice. In the following, we describe more details about our pseudo-labeling scheme and how we train the audio-to-intensity model.\nEmotion-agnostic intensity pseudo-labeling. To model the dynamic fluctuations in intensity during natural speech, we first introduce an emotion-agnostic intensity pseudo-labeling approach. This"}, {"title": "3.2 Emotional Talking-Head Model Training", "content": "We present our emotion-aware talking-head generation model, which leverages the predicted intensity fluctuations as input to generate corresponding facial expressions. To this end, we introduce an emotion latent space in which the latent features are used to guide a transformer-based model to precisely generate the predicted emotion intensity."}, {"title": "3.2.1 Emotion Latent Space Reorganization", "content": "To enable emotional talking-head generation, we propose to learn a continuous emotion latent space where the latent embeddings serve as guidance to steer an audio-to-expression transformer in generating different emotional expressions. Specifically, we expect this latent space to 1) accommodate features of diverse emotion categories, and 2) encode corresponding intensity levels. Furthermore, we aim for disentanglement between the encoded emotion category and intensity, thereby allowing for flexible control over both. To achieve this, we reorganize the latent space such that emotion category information is encoded in the latent orientation (see Figure 1), while intensity information is encoded in the latent norm. For neutral emotion, we consider the intensity to be zero and position it at the origin of the latent space. Our intuition is that the intensity level should positively correlate with the deviation from a neutral expression. Through our reorganization method, this deviation is intrinsically encoded in the distance from the origin point in the latent space, i.e., latent norm.\nIntensity-aware emotion features. For an input video with emotion label e and intensity label l, we extract the corresponding text embedding $f_e$ from the CLIP [26] text encoder. An adaptation network M is then adopted to map $f_e$ to our proposed emotion latent space. In the case where e represents neutral emotion, we minimize the L2-norm of the emotion feature $M(f_e)$ to enforce it to center around the origin. Otherwise, for non-neutral emotions, we re-scale $M(f_e)$ to $M'(f_e)$ such that $||M'(f_e)|| = t(l)$, where t(l) is positively correlated with l. This ensures that the features of the same intensity will be mapped to the same hypersphere, with norms representing intensity levels. Meanwhile, the location on this hypersphere is determined by the orientation of $M(f_e)$, which is shared within the same emotion type."}, {"title": "3.2.2 Training Objectives", "content": "We input the re-scaled emotion features to an audio-to-expression transformer T as a guidance to generate emotional expressions. Specifically, this guidance is integrated as an additional input token of the transformer layer following [10]. The transformer produces a set of intensity-aware 3D expression latent keypoints, which are then used by a talking-head generator G to generate video frames. The final loss for training this talking-head generation system is calculated as follow:\n$L = \\lambda_{exp} * L_{exp} + \\lambda_{rec} * L_{rec} + \\lambda_{sync} * L_{sync} + \\lambda_{norm} * L_{norm}$,\nwhere $\\lambda_{exp}, \\lambda_{sync}, \\lambda_{rec}, \\lambda_{norm}$ are hyper-parameters that re-weight the corresponding loss term. $L_{exp}$ represents the mean square error between the ground-truth expression keypoints and the corresponding keypoints predicted by the transformer. $L_{rec}$ is the L1 reconstruction loss between the generated frame and the input frame within the facial region. $L_{sync}$ is the lip synchronization loss introduced in Wav2Lip [25]. $L_{norm}$ denotes the L2-norm of the emotion feature, which only applies for neutral emotion.\nInference procedure for talking-head generation. Our goal is to generate talking-head videos with emotion intensity fluctuations that correspond to changes in audio inputs. In particular, given an input audio and a driving emotion in a text form, we first infer the emotion intensity L using our trained audio-to-intensity predictor P, where the predicted intensity L is represented as a sequence of scalars. To generate talking-head videos, we map the CLIP embedding of the driving emotion to our proposed emotion latent space. This emotion embedding is then re-normalized according to the predicted intensity L, resulting a sequence of embeddings with the same latent direction but varying latent norms. With this sequence of intensity-aware emotion embeddings, we can then leverage them to guide the transformer model in generating talking-head videos with frame-wise intensity fluctuations."}, {"title": "4 Experimental Results", "content": "Implementation details. Following [10], we first pre-train the transformer-based talking-head generation model to produce emotion-agnostic talking-heads. In particular, we sample the input videos at 25 FPS, and crop and resize them to 256 \u00d7 256. The input audios are down-sampled to 16kHz and are transformed into mel-spectrograms, with the window length and hop length set to 640. We use the pre-trained keypoint detector in [37] to extract canonical keypoints from source images and expression/head pose sequences from driving videos. Following [10], the values of the loss weights are set as: $\\lambda_{exp}$ = 100, $\\lambda_{rec}$ = 10, $\\lambda_{sync}$ = 10, $\\lambda_{norm}$ = 0.1. For videos at three different intensity levels (levels 1, 2, 3), we re-normalize the emotion features to 5, 15, and 30, respectively. For training the audio-to-intensity predictor, we process the input audio with a pre-trained HuBERT model [16]. We design the encoder and decoder as dilated convolutional networks following [45].\nDatasets and baselines. We use Voxceleb2 [8] to train our audio-to-intensity predictor and the talking-head generation model. For emotion-aware generation, we fine-tine the pre-trained talking-head model on MEAD [34], a high-quality emotional video set with 8 kinds of emotions. To ensure fair comparisons, we split the MEAD dataset into training and testing sets based on identity, using the same test identities as [18]. To show the effectiveness of our method, we compare with one-shot talking-head generation methods on the LRW [6] and MEAD [34] test sets. These methods include ATVG [4], Wav2Lip [25], MakeItTalk [53], AVCT [36], PC-AVS [51], EAMM [18], and EAT [10].\nEvaluation metrics. We evaluate the quality of generated videos on multiple metrics that have been widely used in previous studies. Specifically, we use Frechet Inception Distance (FID) [15] to evaluate the realism of generated frames. To evaluate identity preservation, we calculate the structural similarity index measure (SSIM) and peak signal-tonoise ratio (PSNR) [38] of identity embedding between the source images and the generated frames. To evaluate audio-visual synchronization, we use the confidence score provided by SyncNet [7]. In addition, we use the distance between landmarks of the entire face (F-LMD) [4] as a measure of pose and expression accuracy. To assess the emotional accuracy (Emo-Acc) of the generated emotions, we fine-tune the Emotion-Fan [23] using the training set of MEAD following [10] and use it for emotion classification."}, {"title": "4.1 Experimental Setup", "content": "We present our emotion classification accuracy for each class in Table 6. Our method achieves\n100% accuracy on angry, neutral, surprised and outperforms the previous state-of-the-art, EAT, by a remarkable margin on happy, disgusted and fear. Our proposed latent space reorganization method effectively rearranges emotion features with varying intensity levels, resulting in a more discriminative latent distribution across classes. The only class that we observe a slight accuracy drop compared to EAT is contempt. For this emotion, we notice many videos where the intensity performed by the actors does not accurately portray the corresponding intensity label, which could negatively affect the learning of the emotion latent space."}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "Following the setting of EAMM [18], we test our method on the public-available MEAD test set for emotional talking-head generation. Results are summarized in Table 1. It can be observed that our method outperforms the previous methods by a large margin in terms of emotion accuracy and video quality, and obtains comparable lip-sync results with EAT. The improvement in emotion accuracy is likely due to our method's effective modeling of emotion intensity integrated with the emotion embedding space, resulting in more discriminative emotion representations. This enables the"}, {"title": "4.2.1 Quantitative Results", "content": "Following the setting of EAMM [18], we test our method on the public-available MEAD test set for emotional talking-head generation. Results are summarized in Table 1. It can be observed that our method outperforms the previous methods by a large margin in terms of emotion accuracy and video quality, and obtains comparable lip-sync results with EAT. The improvement in emotion accuracy is likely due to our method's effective modeling of emotion intensity integrated with the emotion embedding space, resulting in more discriminative emotion representations. This enables the"}, {"title": "4.2.2 Qualitative Results", "content": "We show visual results of different emotion intensity-control methods in Figure 2. The first row is the predicted intensity from the audio-to-intensity predictor, which is then used as the target intensity to generate angry talking-heads. The second row shows the results of EAT using constant intensity. The generated angry emotions are constantly intense throughout the video without any fluctuations. The results in the third row are from interpolating the emotion embedding of angry with neutral from EAT. We show that the model hardly generates meaningful emotions with intensity levels in-between. MM-ESL [41] introduces an additional scalar for intensity control, but the generated intensity is not precisely correlated with the target intensity, especially when the values are low (as shown in the first four columns). SPACE [13] combines the emotion label with its intensity during training. While the generated images exhibit some intensity variation, it is noteworthy that the face generated with intensity (0.13) in the second column appears more angry compared to faces with higher intensity values, i.e., 0.2 and 0.27. In comparison, our proposed method produces diverse and authentic results with accurate intensity control, leading to more natural and realistic talking-head videos."}, {"title": "4.3 Analysis", "content": "In this section, we conduct additional experiments and analyses to validate the effectiveness of our method. The existing emotion-related metric mainly focuses on measuring emotion classification accuracy. We further evaluate the quality of the generated emotions from three perspectives: the diversity of facial expressions, the alignment between the audio and generated facial movements, and the reconstruction accuracy of emotion intensity. Specifically, for the diversity, we compute the average feature distance of the generated expression keypoints following [31]. As for the alignment"}, {"title": "4.3.1 Analysis on Emotion Intensity Controllability", "content": "In this section, we conduct a comparative analysis of our method for controlling emotion intensity against other intensity-controlling baselines including 1) interpolating the embedding of target emotion with neutral emotion, 2) concatenating the emotion embedding with an intensity scalar \u03bc, where \u03bc\u2208 {1,2,3} during training [13] and 3) scaling the emotion embedding with the intensity scalar \u03bc [41]. We implement these three methods within the framework of EAT [10]. As shown in Table 3, our proposed method constantly outperforms other baselines. By directly leveraging the property of the reorganized latent space, our method achieves consistent controllability of intensity, resulting in more natural talking-head videos, without the need for interpolating intensities based on pre-computed emotions or introducing additional scalar parameters."}, {"title": "4.3.2 Frame-Wise Intensity v.s. Constant Intensity", "content": "In this section, we compare our proposed frame-wise intensity modeling method with simply using constant intensity throughout the talking-head video. Apart from the EAT baseline, we include a variant of our method in comparisons: instead of enforcing intensity-correlated norms during training, this variant employs a constant norm regardless of the intensity level. Results are summarized in Table 4. As shown, using frame-wise intensity yields much lower intensity reconstruction errors compared with using constant intensity. Given that the target intensity is inferred from the audio, higher reconstruction accuracy indicates better alignments between audio signals and facial movements. Moreover, applying frame-wise intensity significantly enhances the diversity of generated expressions, leading to overall more realistic talking-head results."}, {"title": "4.3.3 Generalization to Unseen Emotions and Identities", "content": "in Figure 3, our method can generalize to unseen emotions and identities. Despite being trained only on eight basic emotions, the model generates accurate novel emotions such as shocked, skeptical and depressed. This is attributed to the rich semantic knowledge in the CLIP space, allowing the model to effectively generalize to unseen emotions located within similar emotion domains. Remarkably, our method also allows for precise control over the intensity levels of unseen emotions. This further suggests that within our learned latent space, emotion type and intensity level are learned effectively."}, {"title": "4.3.4 Analysis on Intensity Pseudo-labeling", "content": "Due to the lack of frame-wise intensity ground truths, we conduct a qualitative experiment to validate the correctness of our intensity pseudo-labeling method. Specifically, we sample five frames from a real talking-head video and apply our method to compute the pseudo-intensity for each frame. As"}, {"title": "4.4 User Study", "content": "We conduct user studies involving 25 participants to assess the emotional talking-head videos generated by our method and two other intensity-control methods. For each method, we generate a total of 30 videos, consisting of 20 videos generated from reference images in the test set of MEAD [34] and 10 videos generated from in-the-wild reference images. Participants are required to evaluate the generated results based on three aspects: 1) accuracy of emotion intensity control, 2) diversity of the generated emotional expressions, and 3) overall naturalness. Results are summarized in Table 5. Our method is preferred by the participants over other two competing methods by a large margin for all three aspects. With our proposed latent space reorganization approach, emotion intensity can be precisely controlled, resulting in diverse facial expression generation and more realistic talking-head videos overall."}, {"title": "5 Conclusions", "content": "This paper explores the dynamics of emotion intensity throughout speech, introducing a new approach to capture and replicate these variations in talking-head generation. Our method begins with developing a talking-head generation model that is capable of generating a variety of emotions with precise control over intensity levels. To further capture the dynamic intensity fluctuations, we propose inferring audio-synchronized intensity fluctuations from the speaking tone with an audio-to-intensity predictor. Through extensive experiments and analysis, we demonstrate the effectiveness of our approach in accurately capturing and reproducing emotion intensity fluctuations in talking-head generation. This substantially enhances the expressiveness and authenticity of the generated outputs.\nEthics Statement. With the generated expressions at various intensity levels, the talking-head may reflect more proper expressions aligning with the audio, which could lead to better understandings of talking-head applications perceived by humans. On the other hand, although our generated talking-heads achieve state-of-the-art emotion accuracy based on the emotion classification performance, there can be still cases of less obvious or wrong expressions generated by our model. We will aim to develop techniques to detect such cases and refine talking-head results for the ethical consideration."}, {"title": "6 Supplementary Material", "content": "Figure 5: The detailed architecture of the encoder, decoder of the audio-to-intensity predictor and the emotion adaptation network."}, {"title": "6.1 Details of Model Architectures", "content": "In this section, we provide additional details of the audio-to-intensity predictor and the emotion adaption network. We note that the audio-to-expression transformer is largely based on EAT [10], while the generator is primarily derived from OSFV [37]. Please refer to [10] and [37] for more details."}, {"title": "6.1.1 Audio-to-Intensity Predictor", "content": "Our audio-to-intensity predictor consists of an encoder in Figure 5(a), a decoder in Figure 5(b), and a flow-based prior model. The encoder comprises a 1D-convolution, followed by ReLU activation and layer normalization, and then a non-causal WaveNet. The decoder is composed of a non-causal WaveNet and a 1D transposed convolution followed by ReLU and layer normalization. The prior model is implemented as a normalizing flow, consisting of a 1D-convolution coupling layer and a channel-wise flip operation. We use HuBERT features as the audio condition of these three modules."}, {"title": "6.1.2 Emotion Adaptation Network", "content": "Our emotion adaptation network in Figure 5(c) maps the CLIP embedding of the driving emotion to our proposed emotion latent space. It consists of eight fully-connected (FC) layers with 384 hidden units. ReLU is used as the non-linear activation function in the hidden layers. The input is a 16-dim latent code randomly sampled from a normal distribution. We concatenate the CLIP embedding of the driving emotion to the intermediate embedding after the 4th FC layer. The dimension of the output emotion feature is set to 896."}, {"title": "6.2 Additional Training and Testing Details", "content": "We use the Monte-Carlo ELBO loss [27] to train the variational audio-to-intensity predictor for 9000 steps (about 3 hours). We train our intensity-aware talking-head generation model for about 8 hours on 4 NVIDIA TITAN RTX GPUs. We use Adam optimizer [20] with \u03b2\u2081 = 0.5 and B2 = 0.999. The learning rate is set to 1.5 \u00d7 10\u22124 for the transformer and 2 \u00d7 10-4 for other modules. For intensity pseudo-labeling, we find the neutral face of the same person using an emotion classifier [23]. For precise evaluations during testing, we first crop and align the faces following [4] before computing PSNR, SSIM, FID, and F-LMD. Additionally, to assess synchronization confidence, we preprocess the generated videos following PC-AVS [51]."}, {"title": "6.3 Additional Analysis on Pseudo-Labeling Method", "content": "In this section, we present additional qualitative results to validate the correctness of our intensity pseudo-labeling method. As shown in Figure 6, the computed pseudo-intensity reliably reflects the expression intensity of each real frame: the slightly surprised face (in the second column) is assigned a value of 0.21; while the extremely surprised face (in the last column) is assigned a value of 0.9. We further use the pseudo-intensity as the target intensity to generate talking-heads with three different emotions, i.e., surprised, angry, happy. The generated expression intensity precisely corresponds to the target intensity across different emotions."}, {"title": "6.4 Qualitative Comparisons with Constant Emotion Intensity", "content": "In this section, we provide qualitative comparisons between our method with using constant expression intensity for talking-head generation in Figure 7. As shown in the figure, when expression intensity remains constant throughout the video, it appears artificial and lacking in depth. In contrast, our generated results exhibit smoothly varying expression intensity across frames, resulting in accurate real-life human expressions."}, {"title": "6.5 Quantitative Results of Emotion Classification", "content": "Our proposed method effectively rearranges emotion features with varying intensity levels, resulting in a more discriminative latent distribution across classes. The only class that we observe a slight accuracy drop compared to EAT is contempt. For this emotion, we notice many videos where the intensity performed by the actors does not accurately portray the corresponding intensity label, which could negatively affect the learning of the emotion latent space."}]}