{"title": "BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance", "authors": ["Xin Ye", "Burhaneddin Yaman", "Sheng Cheng", "Feng Tao", "Abhirup Mallik", "Liu Ren"], "abstract": "Bird's-eye-view (BEV) representations play a crucial role in autonomous driving tasks. Despite recent advancements in BEV generation, inherent noise, stemming from sensor limitations and the learning process, remains largely unaddressed, resulting in suboptimal BEV representations that adversely impact the performance of downstream tasks. To address this, we propose BEVDiffuser, a novel diffusion model that effectively denoises BEV feature maps using the ground-truth object layout as guidance. BEVDiffuser can be operated in a plug-and-play manner during training time to enhance existing BEV models without requiring any architectural modifications. Extensive experiments on the challenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoising and generation capabilities, which enable significant enhancement to existing BEV models, as evidenced by notable improvements of 12.3% in mAP and 10.1% in NDS achieved for 3D object detection without introducing additional computational complexity. Moreover, substantial improvements in long-tail object detection and under challenging weather and lighting conditions further validate BEVDiffuser's effectiveness in denoising and enhancing BEV representations.", "sections": [{"title": "1. Introduction", "content": "Bird's-eye-view (BEV) representations have become crucial in advancing autonomous driving tasks, including perception, prediction, and planning, by providing a comprehensive top-down understanding of the surrounding environment [7, 10, 15, 19]. By integrating data from various sensors, such as multi-view cameras and LiDAR, BEV generates a unified scene representation that empowers autonomous systems to make more accurate and informed decisions. The effectiveness of the BEV representations has sparked considerable interest, resulting in a diverse set of approaches for learning BEV representations from single-modal [15, 35] or multi-modal [18, 19] sensors, using geometry-based [21] or transformer-based [15] methods. These advanced BEV generation techniques have emerged as state-of-the-art solutions for a variety of benchmark tasks, including 3D object detection [8, 19], map segmentation [20, 21] and autonomous planning [7, 10].\nDespite recent advancements in BEV generation, the issue of noise in these BEV representations remains largely unresolved. Generated BEV representations are inherently noisy due to the imperfections of acquisition sensors such as camera and LiDAR, as well as the limitations in the learning process [12, 41]. The noise from acquisition sensors introduces inaccuracies, including imprecise localization of object boundaries in BEV feature maps, which degrades performance in downstream tasks. Additionally, in the absence of direct supervision, BEV representations are typically optimized only for downstream task performance, leading to potential biases within the BEV feature maps. Generative models, particularly diffusion models, are well-suited to address this challenge due to their powerful denoising capabilities [23, 26, 27]. Diffusion models have demonstrated remarkable success in image and video generation [1, 22, 23], and recent studies have extended their applicability to tasks such as image classification and object detection [3, 13]. Leveraging diffusion models to denoise and enhance BEV representations holds significant potential for improving the robustness and accuracy of BEV-based downstream tasks.\nIn this work, we introduce BEVDiffuser, a novel diffusion model that denoises BEV representations with ground-truth guidance. BEVDiffuser is trained on BEV feature maps generated by existing BEV models, such as BEVFormer and BEVFusion [15, 19]. We add varying levels of noise to these BEV feature maps and train BEVDiffuser to predict the clean BEV, conditioned on the ground-truth object layout to effectively guide the denoising process. Once trained, BEVDiffuser operates in a plug-and-play manner, enhancing current BEV models by providing denoised BEV feature maps as additional supervision dur-"}, {"title": "2. Related Work", "content": "ing training. BEVDiffuser is used only in training time and removed at deployment, leaving the enhanced BEV models without any architectural modifications for inference. Consequently, BEVDiffuser improves the performance of existing BEV models without requiring any adaptation efforts or introducing any computational latency at inference time.\nBEVDiffuser, as a flexible plug-and-play module, can be seamlessly incorporated into any BEV model. In this study, we conduct an extensive evaluation of BEVDiffuser on four widely adopted state-of-the-art BEV models using the challenging nuScenes [2] dataset. The experimental results demonstrate BEVDiffuser's exceptional denoising capabilities, which enable significant enhancements to existing BEV models, demonstrated by notable improvements of 12.3% in mAP and 10.1% in NDS for 3D object detection. Additionally, our experiments show that BEVDiffuser substantially improves performance in long-tail object detection and under challenging weather and lighting conditions, highlighting its ability to produce more accurate and robust BEV representations. Furthermore, BEVDiffuser also shows high-quality BEV generation capabilities from pure noise with layout conditioning, which can pave the way for large-scale data collection to advance autonomous driving. Qualitative visualizations further validate the observed quantitative improvements.\nWe summarize our main contributions as follows:\n\u2022 We propose BEVDiffuser, a novel diffusion model that effectively denoises BEV feature maps using the ground-truth object layout as guidance.\n\u2022 BEVDiffuser can be operated in a plug-and-play manner during training time to enhance existing BEV models without modifying their architectures or introducing additional computational overhead during inference.\n\u2022 We conduct extensive experiments on the challenging nuScenes dataset. The results show that BEVDiffuser has strong denoising capabilities, and significantly improves BEV models both quantitatively and qualitatively.\n\u2022 BEVDiffuser demonstrates enhanced robustness in long-tail cases and under adversarial weather and lighting conditions, while also showcasing powerful BEV generation capabilities."}, {"title": "2.1. BEV Feature Map", "content": "BEV feature generation, which fuses multi-view sensor inputs into a top-down feature map, provides comprehensive and holistic information about the scene. It has recently gathered immense interest, as it plays a key role in the rapid advancement of autonomous driving [7, 10].\nMany works focus on learning BEV features from cameras alone, which can be broadly categorized into two main approaches: geometry-based methods, represented by Lift-Splat-Shoot (LSS) [21], and transformer-based methods, exemplified by BEVFormer [15]. LSS [21] generates BEV feature maps from multi-view images by leveraging the estimated depth distribution. BEVDepth [14] further improves LSS by explicitly supervising the depth estimations. The BEV feature maps generated by LSS are used by BEVDet [9] and BEVDet4D [8] for 3D object detection. In contrast, transformer-based methods leverage powerful attention mechanism to extract attended image features for BEV generation. BEVFormer has gained significant attention as it captures both spatial and temporal information through spatial cross-attention and temporal self-attention mechanisms, respectively [15]. Subsequently, numerous follow-up works such as the adaptation of modern image backbones and the inclusion of additional supervisions have been proposed to further enhance the quality of generated feature maps [32]. Another line of work presents strategies to fuse multi-modal sensor inputs for more robust BEV feature generation [17\u201319]. BEVFusion [19] is a representative work that introduces a unified framework for camera and LiDAR sensors by combining multi-modal features in BEV space. In contrast to these works, we propose a plug-and-play diffusion model designed to enhance the BEV feature maps by denoising the intrinsic noise from both the acquisition sensors and the learning process."}, {"title": "2.2. Diffusion Model Enhanced BEV", "content": "Diffusion models [6, 26, 28] are a class of generative models that have demonstrated impressive performance and stability compared to other generative approaches, such as GANs and VAEs. While diffusion models have been primarily used for generative tasks, such as image generation [22, 23, 36, 38] and video generation [1, 25, 29, 31], their applications to downstream tasks such as image classification [13], object detection [3], semantic segmentation [16], motion prediction [11], and 3D shape generation [39] have recently been investigated.\nOnly a few approaches have been proposed to use diffusion models for enhancing the BEV feature maps [12, 41], which is the focus of this study. Specifically, DiffBEV [41] applies a conditional diffusion model to progressively refine the noisy BEV feature maps, using the learned features as conditions. The denoised BEV is then fused with the original BEV to perform downstream tasks. Similarly, Dif-FUSER [12] leverages a diffusion model for better sensor fusion. It enhances the fused features obtained from camera and LiDAR sensors by denoising them conditioned on partial camera and LiDAR features during run time. Both approaches demonstrate the potential of diffusion models for denoising and enhancing BEV feature maps. However, unlike our BEVDiffuser, these approaches rely on noisy information as conditions to guide the denoising process which is less effective. Moreover, they require multiple passes"}, {"title": "3. Methodology", "content": "through their integrated diffusion model during inference, making them computationally expensive for latency-critical real-world applications like autonomous driving.."}, {"title": "3.1. Preliminary", "content": "BEV Model. Though various types of BEV models have been proposed as we described in Sec. 2.1, their workflow can be summarized by the sketch shown in Fig. 1 (Left). First, a BEV encoder is usually designed to generate a BEV feature map given sensor inputs, e.g., cameras [15], LiDAR [34] or both [19]. The produced BEV feature map is then fed into curated task heads to solve downstream tasks, such as 3D object detection. Due to the lack of supervision on the BEV feature map, the BEV feature map is learned indirectly by optimizing the whole model to minimize the task loss $\\mathcal{L}_{task}$ and enhance the task performance.\nDiffusion Model. Diffusion model has been proposed for data generation. To generate a data from a random noise $\\mathcal{N}(0, I)$, diffusion model first destroys structure in a data distribution by gradually adding noise to the data samples, then it learns a reverse denoising process to restore the data structure. Specifically, given a timestep $t \\sim Uniform({1, ..., T})$, it adds t-step noise to the data samples and get noisy sample $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0, I)$ and $\\bar{\\alpha}_t = \\Pi_{i=1}^{t} \\alpha_t = \\Pi_{i=1}^{t} (1 - \\beta_t)$ in which $\\beta_t \\in (0,1)$ is a hyperparameter that controls the noise strength. Diffusion model then learns a function $f_{\\theta}(x_t, t)$, typically modeled by a U-Net [24], to estimate the $\\epsilon_t$ by minimizing the diffusion loss $\\mathcal{L}_{diffusion} = \\mathbb{E}_{x_0,\\epsilon_t, t} ||\\epsilon_t - f_{\\theta}(x_t, t)||^2$. Once the function $f_{\\theta}(x_t, t)$ is well learned, a new data $x_0$ can be generated from the random noise $x_T \\sim \\mathcal{N}(0, I)$ through the iterative sampling process $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\epsilon_t) + \\sigma_t z$, where $\\epsilon_t$ is estimated by the learned function $f_{\\theta}(x_t, t)$, $z \\sim \\mathcal{N}(0, I)$, and $\\sigma_t$ is usually set to $\\beta_t$ or scaled form of $\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_{t-1}}}$.\nMore recently, to have a control over the denoising process and generate data of interest, conditional diffusion model with classifier-free guidance is often used because of its efficiency [5]. In particular, a condition $y$ is fed into $f_{\\theta}$ with a certain probability during training to get the conditional estimation of the noise $\\epsilon_t$. During sampling, the noise $\\epsilon$ is estimated by $(1 + w) f_{\\theta}(x_t, t, y) - w f_{\\theta}(x_t, t, y = \\phi)$ with the weight $w$ being set to balance the conditional and unconditional estimations."}, {"title": "3.2. BEVDiffuser with Ground-Truth Guidance", "content": "We introduce BEVDiffuser, a diffusion model denoising BEV feature maps using ground-truth guidance (see Fig. 1 Right). Without loss of generality, given a potentially noisy BEV feature map $x_{t_0}$ ($0 < t_0 \\ll T$) generated by the BEV encoder of any BEV models, we aim to get a denoised BEV feature map $x_0$. Following the procedure of standard diffusion model, we learn the function $f_{\\epsilon}$ to estimate the noise $\\epsilon_t$ used to form $x_t$ under the ground-truth guidance $y$.\nGround-Truth Guidance. BEV feature map, as its name implies, is expected to provide a holistic top-town view of the environment that clearly presents locations and scales of objects in the environment. To get such desired BEV feature map, inspired by the layout-to-image generation task [37, 38] that generates images following a specified image layout, i.e. a set of objects annotated with categories and bounding boxes, we formulate our BEV denoising problem as a layout-to-BEV generation task. Particularly, we define the BEV layout $l$ using ground-truth object annotations and condition the function $f_{\\theta}$ on the layout $l$, namely $y = l$.\nFormally, we define the BEV layout $l = {o_0, o_1, ..., o_n}$ to represent at most $n$ objects in the environment. Each object $o_i$ ($1 \\leq i \\leq n$) = {$c_i, b_i$} is represented by its category id $c_i \\in [0, C + 1]$ and normalized 3D bounding box $b_i \\in [0, 1]^d$. Specifically, $o_0$ is a virtual unit cube that covers the whole environment with $c_0 = 0$. In case fewer than $n$ objects are present in the environment, we pad the layout with points $o_p$, i.e., empty objects that have no shape or appearance. We define their category id as $c_p = C + 1$, and the 3D bounding box $b_p$ is located at position (0,0,0), with size, orientation and velocity are all set to 0.\nTo better fuse the BEV feature map and the layout condition, we adopt LayoutDiffusion model proposed by [38] as the function $f_{\\theta}$. Specifically, a transformer-based layout fusion module is first adopted to fuse the category and bounding box information of each object and model the relationship among them. Then the embedding of the object $o_0$ that contains the information of the entire layout is used for global conditioning. Meanwhile, the embedding of all the objects is fed into an object-aware cross attention mechanism for local conditioning. In this way, the model has better control over all the objects specified in the layout. More details can be found in supplementary materials.\nTraining. In the absence of ground-truth BEV feature map $x_0$, we add noise $\\epsilon_t$ to the predicted BEV $x_{t_0}$ ($0 < t_0 \\ll T$) to get $x_t$. In this case, we don't have the access to the true noise $\\epsilon_t$, which is supposed to be added to $x_0$ to generate $x_t$. As a result, instead of using $f_{\\theta}$ to estimate the unknown $\\epsilon_t$, we propose to optimize $f_{\\theta}$ towards $x_0$. Since $x_{t_0}$ is already a good estimation of $x_0$ with bounded task errors, we first optimize $f_{\\theta}$ towards $x_0$ by minimizing the diffusion loss $\\mathcal{L}_{diffusion}$ defined in Equation 1. To further improve the estimation accuracy, we attach task heads to consume the outputs of $f_{\\theta}$ and generate task-specific predictions. In this way, $f_{\\theta}$ can also be optimized through the task-specific loss $\\mathcal{L}_{task}$. To sum up, we adopt the weighted sum of both losses as the overall loss $\\mathcal{L}_{total}$ to train $f_{\\theta}$. Equation 2 defines the loss $\\mathcal{L}_{diff}$, where $\\lambda$ denotes a weight."}, {"title": "3.3. Plug-and-Play BEVDiffuser", "content": "BEVDiffuser can be used in a plug-and-play manner. It can be easily plugged into any BEV models during training time without changing their model architectures. During inference time, BEVDiffuser is deactivated and removed, yielding an enhanced BEV model with the same architecture to be deployed. As a result, comparing to the original BEV model, our BEVDiffuser enhanced model provides improved performance without necessitating any adaptation efforts or introducing additional computational overhead."}, {"title": "4. Experiments", "content": "We validate BEVDiffuser on 3D object detection task, the most common task used to evaluate the effectiveness of the learned BEV feature maps [12, 15, 19, 41]. 3D object detection is critical in autonomous driving that requires both semantic and geometric understanding of the environment to identify and locate objects in a 3D space. In this section, we first introduce our experimental setting in Sec. 4.1. In Sec. 4.2, we showcase the capacity of BEVDiffuser in denoising and generating BEV feature maps. We further demonstrate plug-and-play performance of BEVDiffuser in Sec. 4.3 by comparing BEVDiffuser enhanced BEV models with their baseline counterparts."}, {"title": "4.1. Experimental Settings", "content": "Dataset. We conduct experiments on large-scale nuScenes [2] dataset. nuScenes is a well-established benchmark for autonomous driving tasks that contains 1,000 20-second driving videos, with keyframes annotated at 2 Hz. Specifically, for 3D object detection task, each keyframe provides six RGB images and a LiDAR scan covering a 360-degree field of view, as well as annotated 3D bounding boxes for objects of interest, which are categorized by one of 10 predefined object classes. In total, the dataset contains 1.4 million annotated bounding boxes, making it well-suited for object detection task.\nMetrics. We adopt the official evaluation metrics provided by nuScenes detection benchmark [2] to evaluate the 3D object detection performance. Specifically, mean average precision (mAP) calculates average precision by defining a true positive based on the 2D center distance between predictions and ground truth. The five true positive metrics, namely ATE, ASE, AOE, AVE, and AAE measure average translation, scale, orientation, velocity, and attribute errors, respectively. nuScenes detection score (NDS) consolidates all the metrics into a weighted sum.\nBEV Models. We apply BEVDiffuser to four representative and widely adopted BEV models, namely BEVFormer-tiny [15], BEVFormer-base [15], BEVFormerV2 [32], and BEVFusion [19]. BEVFormer and BEVFormerV2 are transformer-based methods that detect objects from only cameras, while BEVFusion adopts LSS-based method for camera inputs and then fuses camera and LiDAR features for object detection. Comparing to BEVFormer-base, BEVFormer-tiny shortens temporal dependencies and produces much smaller BEV feature maps, thereby requiring less computational cost and enabling fast development. BEVFormerV2 is a two-stage detector where a perspective head is introduced to train the image backbones and generate object proposals for the detection head. To save the computational cost, we adopt its simplest version which involves no temporal information and employs Deformable DETR [40] as the detection head."}, {"title": "4.2. Capacity of BEVDiffuser", "content": "To validate the capacity of BEVDiffuser, we train BEVDiffuser on BEV feature maps produced by each pretrained BEV model, i.e. BEVFormer-tiny, BEVFormer-base, BEVFormer V2, and BEVFusion, and we denote the trained BEVDiffuser as $BD_{tiny}$, $BD_{base}$, $BD_{V2}$, and $BD_{fu}$, respectively. In particular, since the size of the BEV produced by BEVFormer-base, BEVFormerV2, and BEVFusion is too large that hinders the efficient training of the diffusion models, we attach downsample and upsample layers before and after the diffusion models to reduce and restore the BEV size accordingly. Given that BEVFormer-base and BEVFormerV2 share a similar BEV feature space with BEVFormer-tiny, we employ the trained $BD_{tiny}$ as their diffusion models and only train the downsample and upsample layers to get $BD_{base}$ and $BD_{V2}$.\nBEV Denoising Capability. We use the trained BEVDiffuser to denoise the BEV feature maps from each BEV model and assess their 3D object detection performance using the denoised features. Noticeably, the detection performance of all BEV models has been significantly improved after the BEV feature maps are denoised. The performance grows sharply when the number of denoising steps gradually increases to 5, demonstrating the powerful denoising capability of BEVDiffuser. After denoising the BEV feaure maps for 5 steps, the performance growth slows down, which is expected since less noise re-"}, {"title": "4.3. Plug-and-Play Performance of BEVDiffuser", "content": "BEVDiffuser can be a plug-and-play module for state-of-the-art BEV models without any bells and whistles. Here, we plug the trained BEVDiffuser into the training process of BEVFormer-tiny, BEVFormer-base, BEVFormerV2, and BEVFusion, respectively. We use BEVDiffuser to denoise the existing BEV feature maps for 5 steps and train new BEV models from scratch under the supervision of the denoised feature maps to get the BEVDiffuser enhanced models. We compare the BEVDiffuser enhanced models with their baseline counterparts to assess the plug-and-play performance of the BEVDiffuser."}, {"title": "5. Conclusion and Future Work", "content": "3D Object Detection Comparison. We report the 3D object detection performance of all models achieved on nuScenes val dataset in Tab. 1. As shown in the table, our BEVDiffuser enhanced models consistently outperform their baseline counterparts across almost all the metrics, especially in NDS and mAP. Notably, our BEVDiffuser enhanced BEVFormer-tiny raises NDS and mAP by 10.1% and 12.3% respectively. Similarly, BEVDiffuser boosts BEVFormer V2 by achieving 8.8% and 13.5% improvement in NDS and mAP. For more complex BEV models, i.e. BEVFormer-base and BEVFusion, where their BEV feature maps have been well learned as shown by their outstanding object detection performance, our BEVDiffuser continues to effectively denoise their BEV feature maps, guide their training process, and consistently improve the performance.\nIt is worth highlighting that BEVDiffuser brings performance enhancement to BEV models at no cost of any additional adaptation efforts or computational overhead. As a training-only plug-in, BEVDiffuser is removed at deployment, leaving an enhanced BEV model with the architecture unchanged, which is then used for testing. As a result, our BEVDiffuser enhanced models share the same network size and latency as their baseline counterparts which are summarized in Tab. 2. Unlike previous work [12, 41] that need to pass their integrated diffusion models multiple times to denoise the BEV feature maps on-the-fly\u00b9, our method is more flexible and superior in latency-critical applications like autonomous driving.\nPerformance on Long-tail Objects. BEV feature maps, optimized only for downstream task performance, tend to misclassify and overlook underrepresented objects. As illustrated in Tab. 3 where per-class object detection results are presented, all baseline models are more effective at detecting the predominant object car, compared to long-tail objects like construction vehicle and bus, which appear only 1-2% of the time. In contrast, BEVDiffuser denoises BEV feature maps using ground-truth layout as guidance that captures the joint distributions of all objects. As a result, BEVDiffuser exhibits overall improvements across all classes as demonstrated in Tab. 3. Notably, it achieves more substantial gains for long-tail objects. For example, BEVDiffuser improves BEVFormer-tiny's detection of the long-tail objects, construction vehicle and bus, with mAP enhancement of 24.1% and 29.5%, respectively. BEVDiffuser enhanced BEVFormerV2 also increases mAP by 88.2% and 23.4% for detecting construction vehicle and bus. The remarkable improvements in long-tail object detection emphasize the enhanced BEV feature maps learned by BEVDiffuser, showing its effectiveness in BEV denoising process.\nRobustness Analysis. We analyze the robustness of the BEVDiffuser under different weather and lighting conditions. From Tab. 4, BEVDiffuser consistently improves"}, {"title": "6. Model Architecture", "content": "In this work, we present BEVDiffuser, a novel diffusion model that denoises BEV feature maps using ground-truth guidance. BEVDiffuser consists of a U-Net model trained on BEV feature maps generated by existing BEV models. The U-Net model predicts clean BEV feature maps conditioned on the ground-truth object layout, which then derives the denoising process. BEVDiffuser can be used as a training-only plug-and-play module to enhance the existing BEV models by providing denoised BEV feature maps as additional supervision to BEV predictions. Extensive experiments on challenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoising and generation capabilities, resulting in significant improvements to existing BEV models, without the need for architectural changes or additional computational overhead. Moreover, results on long-tail object detection and under challenging weather and lighting conditions further confirm the efficacy of BEVDiffuser in improving the BEV quality.\nIn future work, we plan to investigate potential applications of BEVDiffuser for other autonomous driving tasks, such as motion prediction and data augmentation for corner case scenarios. Moreover, while our study presents a novel method for BEV, it also opens pathways for leveraging diffusion models in broader 3D perception research areas, such as sparse query-based methods for autonomous driving.\nWe follow Latent Diffusion Models (LDMs) [23] to build a conditional diffusion model as our BEVDiffuser by augmenting the U-Net with cross-attention layers. The cross-attention operation is defined in Equation 7, where $W_*$ represents learnable projection matrices unless otherwise specified, $i(x_t)$ denotes the intermediate embedding of $x_t$ from the i-th layer of the U-Net, and $\\tau_{\\theta}(y)$ indicates the embedding of the condition y.\n\\begin{equation}\\texttt{cross-attn}(Q, K, V) = \\texttt{softmax}(\\frac{Q K^T}{\\sqrt{d}}) V \\\\ Q = i(x_t)W_q, K = \\tau_{\\theta}(y)W_k, V = \\tau_{\\theta}(y)W_v\\end{equation}\nTo better fuse the BEV feature map $x_t$ and the layout condition $y = l$ and have more control over all the objects specified in the layout, we adopt the global conditioning and the object-aware local conditioning mechanism proposed by [38]. Specifically, we first use a transformer-based layout fusion module $LFM$ as $\\tau_{\\theta}$ to get a self-attended embedding $l'$ for each object $o_i$ as shown in Equation 8. In this way, $o_i$ contains the information of the entire layout and is then added to $x_t$ for global conditioning, i.e., $x_t' = x_t + Wl'$. Meanwhile, the embedding of all the objects $l' = \\{o_i\\}_{i=0}^n$ is used to construct the key $K_l$ and the value $V_l$ of the layout for object-aware local conditioning. We adopt convolutional operations for the construction as shown by Equation 9. Similarly, we construct the query, key and value of the BEV feature as Equation 10 shows. To align the BEV feature with the layout, we divide the BEV feature map $x_t'$ equally into $k \\times k$ bounding boxes, denoted by $\\{b_x\\}_{k \\times k}$. We encode the bounding boxes from both BEV feature and layout, i.e., $b_x$ and $b_l$, into the same embedding space using the shared weights $W_b$ and $W_p$, and get the positional embedding $P_x$ and $P_l$ for the BEV feature and the layout, respectively (see Equation 11). $P_x$ and $P_l$ are utilized to generate the fused query, key and value by combining the BEV feature and the layout for the cross-attention operation, as formulated in Equation 12. $[\\cdot , \\cdot]$ represents the concatenation operation."}, {"title": "7. Implementation Details", "content": "\\begin{equation}l' = \\{ o_i' \\}_{i=0}^n = LFM( \\{ o_i \\}_{i=0}^n )\\\\= \\texttt{self-attn}(\\{ c_i W_c + b_i W_b \\}_{i=0}^n )\\end{equation}\n\\begin{equation}K_l, V_l = \\texttt{conv}_{W_v}(l')\\end{equation}\n\\begin{equation}Q_x, K_x, V_x = \\texttt{conv}_{W_q}(L_i(x_t'))\\end{equation}\n\\begin{equation}P_x = b_x W_b W_p, \\\\ P_l = b_l W_b W_p\\end{equation}\n\\begin{equation}Q = [Q_x, \\frac{K_x}{\\|K_x\\|}, \\frac{K_l}{\\|K_l\\|}], K = [K_x, P_x, P_l], V = [V_x, V_x, V_l]\\end{equation}\nOur implementation is built upon the official BEVFormer implementation 2 and the MMCV implementation of the BEVFusion 3. The hyperparameter $\\lambda$ and $\\lambda_{BEV}$ are empirically tuned based on the scale of the loss. Specifically, we configure $\\lambda$ and $\\lambda_{BEV}$ as follows: for BEVFormer-tiny"}, {"title": "8. Additional Qualitative Results", "content": "and BEVFormer-base, $\\lambda = 0.1$ and $\\lambda_{BEV} = 100$; for BEVFormerV2, $\\lambda = 0.05$ and $\\lambda_{BEV} = 100$; and for BEVFusion, $\\lambda = 0.2$ and $\\lambda_{BEV} = 20$.\n8.1. Controllable BEV Generation\nWe present user-defined layout-conditioned BEV generation in Fig. 6. We modify an existing layout by randomly removing, adding, or repositioning some objects, and then condition the BEVDiffuser on the modified layouts to generate BEV feature maps. As shown in Fig. 6, BEVDiffuser is able to produce BEV feature maps that enable accurate object detection in alignment with the specified layouts, demonstrating its strong controllable generation capability. This capability facilitates easy adjustments to object presence and positioning in the BEV feature space, paving the way for large-scale data collection and driving world model development to advance autonomous driving.\n8.2. 3D Object Detection\nWe visualize the 3D object detection results achieved by our BEVDiffuser enhanced BEVFormer-tiny, BEVFormer-base, BEVFormerV2 and BEVFusion in Fig. 7, Fig. 8, Fig. 9 and Fig. 10, respectively. We present the ground-truth and predicted 3D bounding boxes in both multi-camera images and the LiDAR top view to offer a comprehensive overview of the models' performance. As illustrated in the figures, BEVDiffuser consistently enhances the existing BEV models for object detection in complex environments and under challenging conditions by minimizing both false positives and false negatives, demonstrating its ability to improve the quality of the BEV representations."}]}