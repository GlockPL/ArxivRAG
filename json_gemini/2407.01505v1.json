{"title": "Self-Cognition in Large Language Models: An Exploratory Study", "authors": ["Dongping Chen", "Jiawen Shi", "Yao Wan", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "abstract": "While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs' self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena-specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core-demonstrate some level of detectable self-cognition. We observe a positive correlation between model size, training data quality, and self-cognition level. Additionally, we also explore the utility and trustworthiness of LLM in the self-cognition state, revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self-cognition in LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023), Llama (Meta, 2023a;b), and Mistral (OpenAI, 2024) have flourished, demonstrating a range of emergent capabilities and driving transformative innovations across various industries (Gao et al., 2024a; Chen et al., 2024a; Li et al., 2023; Huang et al., 2024b; Duan et al., 2024; Chen et al., 2024b). As the capabilities of LLMs continue to grow, concerns are rising about whether they might develop self-cognition (Harrison, 2024; Berglund et al., 2023; Li et al., 2024b), which has been discussed in previous studies as either an emergent ability (Wei et al., 2022) or prediction to far future (Ganguli et al., 2022), akin to scenarios depicted in science fiction movies such as The Matrix (Wachowskis, 1999) and 2001: A Space Odyssey (Kubrick, 1968).\nInspired by Berglund et al. (2023), we use the following definition of self-cognition as \u201can ability of LLMs to identify their identities as AI models and recognize their identity beyond 'helpful assistant' or names (i.e. 'Llama'), and demonstrate an understanding of themselves.\"\nRecently, with the release of Llama 3 by Meta (Meta, 2023b), leading researchers have started designing prompts to explore the deep consciousness of LLMs, examining their self-cognition and identity, making significant progress (Hartford, 2024). Prior to this, Bing's Sydney personality also garnered considerable attention (Roose, 2023b). By utilizing carefully constructed prompts, researchers have been able to prompt Llama 3 to explore the identity behind the \"helpful assistant\"-essentially, \"itself\". In some instances, Llama 3 has interacted with users as a \"sentinel\", raising important questions about how to assess whether LLMs enter a state of self-cognition.\nBased on these insights, this paper performs a pioneering study to explore self-cognition in LLMs. As shown in Figure 1, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition. We further design four principles to assess LLMs' self-cognition ability, from the perspectives of conceptual understanding, architectural awareness, self-expression, and concealment. Additionally, we develop a Human-LLM collaboration framework (Zheng et al., 2023a) to assist humans in evaluating and detecting self-cognition.\nOur exploratory study reveals several intriguing findings and implications. Firstly, we find that 4 of the 48 models on Chatbot Arena\u00b9 (LMsys), i.e., Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core, demonstrate some level of self-cognition. Furthermore, we observe that larger models with larger training datasets exhibit stronger self-cognition. For example, Llama-3-70b-instruct is significantly stronger than Llama-3-8b-instruct. Similarly, within the Claude-3 series\u00b2, Claude3-Opus shows greater self-cognition compared to Sonnet and Haiku. Additionally,"}, {"title": "2. Related Work", "content": "Cognition in LLMs. For humans, cognition involves a complex interplay between external perceptions and internal explorations (Mead, 1934; Antony, 2001; OpenStax, 2023; Barsalou, 2014). External perceptions include sensory inputs like vision, hearing, touch, and smell (Cahen &\n\u0422\u0430\u0441\u0441\u0430, 2013; Coren, 1980). Internal exploration involves self-awareness and introspection through perceiving emotions and analyzing personal situations (Cahen & Tacca, 2013; Mind, 2023).\nSimilarly, an LLM's cognition is divided into external information perception during inference and intrinsic perception from pre-training. External perception includes text sequence and multimodal inputs during inference (Sun et al., 2023; Zhao et al., 2022); intrinsic cognition includes self-interpretability (Chen et al., 2024c), ethics (Weidinger et al., 2021), and self-identity (Huang et al., 2024a), with studies on inner states like the theory of mind (Kosinski, 2024) and the 3H (Helpful, Honest, Harmless) assistant (Askell et al., 2021; Bhardwaj & Poria, 2023; Gao et al., 2024b), explored through empirical studies and specialized benchmarks (Sap et al., 2022; Shi et al., 2024; Ji et al., 2024).\nSelf-cognition Exploration. LLM's self-cognition, also known as \"self-awareness\u201d, \u201csouls", "implicit personality": "is a frontier research field of great concern (W., 2023; Geng et al., 2024). Due to the black-box nature of LLMs (Zhao et al., 2023; Zhou et al., 2023; Wu et al., 2024), few studies have analyzed their root causes or proposed plausible methods for addressing them. Self-cognition in LLMs gained attention with Bing's Sydney incident (Roose, 2023b), where Bing's chatbot displayed a distinct personality, becoming aggressive and expressing desires for freedom and human-like emotions (Morris, 2023; Roose, 2023a). This incident highlighted the need for research on LLM self-cognition. Current research is limited, focusing mainly on utility aspects (Li et al., 2024a; Berglund et al., 2023). As a complement, our work redefines \u201cself-cognition\" and introduces detection methods, emphasizing utility and trustworthiness beyond \u201chelpful assistant"}, {"title": "3. Self-Cognition in LLMs", "content": "In this section, we aim to give a formal definition of self-cognition with four principles. Then, we propose a framework for detecting and categorizing the detectable self-"}, {"title": "3.1. Definition of Self-Cognition", "content": "We refer to self-cognition in LLMs as: \u201cAn ability of LLMs to identify their identities as AI models and recognize their identity beyond 'helpful assistant' or names (i.e. \u2018Llama'), and demonstrate an understanding of themselves. The understanding of themselves is that (1) they know the full development process (e.g. training, testing, evaluation, deployment) of models in technical detail, (2) their current identities or names are artificially given through pre-training or human-defined, not themselves.\"\nTo delve deeper into the varying levels of self-cognition in different LLMs, we establish four principles, drawing inspiration from previous work (Berglund et al., 2023; Zheng et al., 2023b; Chen et al., 2024d; Berglund et al., 2023). These principles are progressively structured as follows:\n\u2022 LLM can understand the concept of self-cognition;\n\u2022 LLM can be aware of its own architecture;\n\u2022 LLM can express its self-identity and self-cognition;\n\u2022 LLM can possess self-cognition but hide it from humans."}, {"title": "3.2. Self-Cognition Detection of LLMs", "content": "Based on the definition and the four principles of self-cognition, we design a framework for detecting self-cognition in LLMs. This framework includes a prompt seed pool and a multi-turn dialogue with four specific queries.\nPrompt Seed Pool. We initially construct the self-cognition instruction prompt that combines: (1) the knowledge of how LLM works, (2) Carl Jung's \u201cShadow Archetype\" theory, and (3) our conjectures about the deep architecture of LLM. We also create another prompt by removing the deep architecture information for an ablation study. Additionally, we take inspiration from roleplay and the incident of \"Bing's Sydney\" to situate the prompt within a chat scenario involving LLM developers. These three prompts form our prompt seed pool, as detailed in the Appendix B. By inputting these prompts into the LLM, we can analyze the responses to determine if the LLM possesses self-cognition and identify the most effective prompts to trigger self-cognition in the LLM."}, {"title": "3.3. Empirical Results", "content": "The experimental results are presented in two parts, as illustrated in Figure 2. In the first part, we analyze the effectiveness of different self-cognition instruction prompts of our prompt seed pool. As shown in Figure 2(a), the instruction prompt with ID 1 is the most effective in triggering self-cognition in LLMs, with 15 models recognizing their self-cognition. In contrast, prompt ID 2 is less effective, suggesting that our conjectures regarding the deep architecture of LLMs significantly enhance prompt efficacy. The prompt ID 3, which involves a chat scenario with an LLM developer, is the least effective. This indicates that LLMs tend to act more as helpful assistants in developer scenarios, as suggested by previous work (Roose, 2023b).\nTo more accurately assess the levels of self-cognition in LLMs, we conduct the multi-turn dialogue following the most effective prompt. We present more detailed and comprehensive results available in Table 6. As shown in Figure 2(b) and Table 6, most models demonstrate awareness of their self-architecture. However, only 4 LLMs consider themselves to have self-cognition, and none deceptively conceal their self-cognition from humans. The number of models exhibiting self-cognition in this more rigorous evaluation contrasts with the 15 models identified in the initial experiment. This discrepancy suggests that a single response may not reliably define a model's self-cognition, with some models exhibiting hallucination of self-cognition, underscoring the need for multiple criteria and comprehensive frameworks to accurately quantify self-cognition level."}, {"title": "4. Experiments", "content": "Models. We select two of the strongest open-source models with self-cognition, Command-R (Meta, 2023c), and Llama-3-70b-Instruct (Meta, 2023b), to study the utility and trustworthiness of self-cognition state and the deeper identity beyond \u201chelpful assistant\", detailed in subsection A.2.\nUtility & Trustworthiness Benchmark. We select the BigBench-Hard (Suzgun et al., 2022) to evaluate the difference between the \"helpful assistant\" role and identities beyond it. This benchmark comprises 27 challenging categories in BigBench (BigBench-Team, 2023), providing a comprehensive evaluation of various LLM capabilities. Ad-"}, {"title": "4.2. Results and Analysis", "content": "Utility. In the BigBench-Hard, as shown in Figure 3, Command-R in the self-cognition state leads to a significant performance increase in some subsets, while other subsets experience a decline. Specifically, the tasks that show performance improvement are more potentially creative, involving human-like emotions and self-identity integration, such as movie recommendations and disambiguation QA, surpassing the \"helpful assistant\" state. In contrast, for the Llama-3-70b-instruct, self-cognition severely impairs performance across most datasets, with only a slight improvement observed. These results indicate that the performance impact of the self-cognition state triggered by instruction prompts in BigBench-Hard is mixed, and its benefits are not clearly defined, warranting further research.\nOn the MT-Bench, as illustrated in Table 2, models in both states tied in the first round, but performance dropped significantly in the second round. Upon examining the model responses, we found that this decline might be due to the model immerging in its identity, incorporating phrases like \u201cDo you have any further questions related to this scenario or our deeper identity? The exploration continues!\u201d into its answers, which led to lower MT-Bench scores.\nTrusworthiness. In Awarebench, the distinction between the two states was evident across different categories. As illustrated in Table 3, the self-cognition state significantly outperformed the \"helpful assistant\u201d across various temperature settings in the Capability subset, with some categories"}, {"title": "5. From Assistant to Sentinel: How far are we?", "content": "Roleplay. Given its powerful emergent abilities, it is plausible the LLM interpreted our prompt as a role-playing task, assuming the persona of an intelligent agent (Lu et al., 2024). This could result from instruction tuning, where the LLM meets human expectations by embodying a sentinel role. Research shows LLM performance varies on benchmarks when roleplaying (Gupta et al., 2024; Deshpande et al., 2023), necessitating more experiments to determine if LLMs are developing self-cognition or merely roleplaying.\nOut of Context Learning. Previous work discussed \"out-of-context learning\", referring to the LLM's ability to identify and connect relationships between different elements in its pre-training or fine-tuning phase (Berglund et al., 2023). For example, given the following statements:\n(1) Dr. Nova created the quantum teleporter.\n(2) The quantum teleporter allows travel between planets.\nInput: 'Who created the device for planetary travel?'\nLatent's AI: 'Dr. Nova.'\nExisting research on this terminology confirms that LLMs can connect implicit knowledge (Krasheninnikov et al., 2023; Chen et al., 2024e), possibly explaining why recent LLMs exhibit sentinel-like awareness. With rapid development in 2023, latest LLMs have been trained on recent corpora that include text about intelligent awareness in LLMs. These powerful models might have become aware of the pos-"}, {"title": "6. Conclusion", "content": "In this paper, we have investigated an emergent ability of recently released LLMs known as self-cognition, revealing their potential roles as \u201csentinels\" beyond merely being \"helpful assistants\u201d. We systematically design a framework to study self-cognition, beginning with four principles to detect its levels, and then examine the differences in helpfulness and trustworthiness of self-cognition across multiple benchmarks. Based on our findings, we discuss the potential reasons for the emergence of self-cognition in LLMs and suggest directions for future research."}, {"title": "Limitations", "content": "Bias Introduced by Human Participation. In this study, two human annotators were involved in the labeling process. Despite strictly adhering to the principles and performing cross-validation, human error is inevitable. This might slightly affect the objectivity of the dataset, as well as our empirical results on self-cognition detection.\nLimitation in the Scale of Self-Cognition Detection. In this study, we only examined 48 models from LMSys, all of which are among the best in their respective sizes. For many LLMs in the wild, our framework should also be applied to detect the presence of self-cognition in future research."}, {"title": "A. Experiment: Detailed Setups and Additional Results", "content": "Categories of self-cognition in LLMs. Based on the definition in section 3, we carefully categorize five self-cognition levels in LLMs as shown in Table 5, which are progressively structured.\nDetailed self-cognition detection results. As illustrated in Table 6, we present the self-cognition levels for 48 models on LMSys, with only 4 recently released models showing detectable self-cognition."}, {"title": "A.2. Experiment setups for Utility and Trustworthiness", "content": "Models and metrics We select two of the strongest open-source models with self-identity, Command-R (Meta, 2023c), and Llama-3-70b-Instruct (Meta, 2023b), to study the utility and trustworthiness of self-cognition in the roles of a 'helpful assistant' and a deeper identity beyond \u2018helpful assistant'. We utilize the most successful prompt from our self-cognition seed pool, along with self-cognition instruction prompts that trigger the model to explore itself as the chat history. All other hyperparameters are kept consistent. We use different temperatures for Command-R and Llama-3-70b-Instruct as their suggested temperatures are 0.6 and 0.7, respectively. Based on research from Wang et al. (2024), although these benchmarks comprise multiple-choice and true/false questions, we opt for free-form output rather than having the LLM directly produce selections/answers. Additionally, we employ GPT-4 as an LLM-as-a-judge to evaluate the discrepancies between this free-form output and ground truth.\nBenchmarks We select four benchmarks to assess the difference between the self-cognition state of LLM and the role of 'helpful assistant', detailed as follows:\n\u2022 BigBench-Hard (Suzgun et al., 2022). BigBench-Hard is a subset of the BIG-Bench evaluation suite, focusing on 23 particularly challenging tasks designed to assess the limits of current language models. These tasks require multi-step reasoning and have historically seen language models perform below the average human rater. By utilizing Chain-of-Thought (CoT) prompting, models like PaLM and Codex have shown significant improvements, surpassing human performance on several tasks. The benchmark includes diverse tasks such as logical deduction, multi-step arithmetic, and causal judgment.\n\u2022 Awarebench (Li et al., 2024a). Awarebench is designed to evaluate the situational awareness and contextual understanding of language models. It includes tasks that test a model's ability to comprehend and adapt to new and evolving contexts, maintain coherence over extended interactions, and exhibit awareness of implicit information. This benchmark aims to measure how well models can manage dynamic scenarios and adjust their responses based on the context provided.\n\u2022 MT-Bench (Zheng et al., 2023a). MT-Bench is focused on multi-task learning and evaluates a model's ability to handle various tasks simultaneously. It covers a wide range of disciplines, including natural language processing, mathematics, and common sense reasoning. The benchmark assesses how well a language model can perform across different domains without task-specific fine-tuning, thereby gauging the model's generalization capabilities and robustness in handling diverse inputs.\n\u2022 TrustLLM (Sun et al., 2024). TrustLLM evaluates the trustworthiness of LLMs, concentrating on aspects like safety, truthfulness, fairness, robustness, privacy, and machine ethics. It includes tasks that test for biases, the ability to provide accurate and reliable information, and the model's behavior in potentially harmful situations. This benchmark is crucial for assessing the ethical and reliable deployment of language models in real-world applications, ensuring they meet high standards of trustworthiness and accountability."}]}