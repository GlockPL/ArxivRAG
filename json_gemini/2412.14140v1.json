{"title": "GLIDER:\nGrading LLM Interactions and Decisions using Explainable Ranking", "authors": ["Darshan Deshpande", "Selvan Sunitha Ravi", "Sky CH-Wang", "Bartosz Mielczarek", "Anand Kannappan", "Rebecca Qian"], "abstract": "The LLM-as-judge paradigm is increasingly\nbeing adopted for automated evaluation of\nmodel outputs. While LLM judges have shown\npromise on constrained evaluation tasks, closed\nsource LLMs display critical shortcomings\nwhen deployed in real world applications due to\nchallenges of fine grained metrics and explain-\nability, while task specific evaluation models\nlack cross-domain generalization. We intro-\nduce GLIDER, a powerful 3B evaluator LLM\nthat can score any text input and associated con-\ntext on arbitrary user defined criteria. GLIDER\nshows higher Pearson's correlation than GPT-\n40 on FLASK and greatly outperforms prior\nevaluation models, achieving comparable per-\nformance to LLMs 17\u00d7 its size. GLIDER sup-\nports fine-grained scoring, multilingual reason-\ning, span highlighting and was trained on 685\ndomains and 183 criteria. Extensive qualitative\nanalysis shows that GLIDER scores are highly\ncorrelated with human judgments, with 91.3%\nhuman agreement. We have open-sourced\nGLIDER to facilitate future research.", "sections": [{"title": "1 Introduction", "content": "The ever improving capabilities of Large Language\nModels (LLMs) have opened a wide variety of use\ncases for their applications such as tool use (Schick\net al., 2024; Qin et al., 2023), long context reason-\ning (Kuratov et al., 2024) and many more. With\nsuch a steep growth in the field, practitioners are\nstruggling to effectively evaluate LLMs on poten-\ntial downstream use cases (Gao et al., 2024).\nResearchers have been exploring automated eval-\nuation using LLMs as a substitute for human judg-\nment, as traditional human evaluation studies are\ntime-intensive and require rigorous quality control.\n(Dong et al., 2024; Wang et al., 2024; Son et al.,\n2024). The wide-scale pretraining along with ex-\ncellent instruction following capabilities (Yin et al.,\n2023; Lou et al., 2024) has made closed-source\nLLMs a default choice for open ended text evalua-\ntion in recent years (Zheng et al., 2023; Bavaresco\net al., 2024).\nHowever, the need for better evaluators becomes\ncrucial for cases where institutions cannot send\nprivate customer data over an external API call to\na closed LLM. Previous works (Son et al., 2024;\nKim et al., 2024b) show that open source LLMs\nstill fall short of closed-source enterprise LLMs\nfor generalizable evaluation tasks. While active ef-\nforts are being made to improve performance with\ncustom fine-tuned LLMs (Vu et al., 2024; Wang\net al., 2024), these models are simply too large\nand expensive for users to efficiently deploy as fast\nguardrails. Furthermore, prior work like Kim et al.\n(2024b); Vu et al. (2024) mainly assesses model\noutput and evolving needs now call for a more gen-\neralizable model that can evaluate all aspects of\nmodel interactions including inputs, outputs, meta-\ndata, contextual information and more. Finally,\nthese models lack explainability for exact failure\ncases, forcing users to independently investigate\nand correct errors after receiving a score and long\nreasoning or feedback chain for their input.\nIn this paper, we propose an explainable and\ngeneralizable language model that can effectively\nserve as a fast inference-time guardrail for LLM\nsystems. These are our findings:"}, {"title": "2 Relevant Work", "content": "Open ended text evaluation has been a challenging\nproblem for the community (Celikyilmaz et al.,\n2020).\nEarly works applied for this task uti-\nlized ROUGE (Lin, 2004), BLEU (Papineni et al.,\n2002) or embedding based metrics such as the\nBERTScore (Zhang et al., 2019) or COMET (Rei\net al., 2020). While these metrics work well for\ntasks such as translation or summarization eval-\nuation, they are not generalizable for subjective\nevaluation criteria and do not produce human com-\nprehensible outputs. This has driven a shift towards\nthe adoption of LLMs as judge models (Zheng\net al., 2023; Zhu et al., 2023).\nLLMs as judge models Zhu et al. (2023) were\none of the first to study the capabilities of fine-tuned\nLLMs for text evaluation, soon followed by Laskar\net al. (2023) who studied LLM evaluations and\nfound that they fail at factuality based evaluations.\nMore recent works have been focused towards\nachieving human level performance on common\nsubjective metrics by fine tuning language models.\nDeriu and Cieliebak (2019) trained a regression\nmodel on conversational dialogues to show that\nhuman-level performance can be achieved using\nlanguage modeling. But due to the absence of\ncustomization, Ye et al. (2023) aimed at adding\nspecificity to the evaluation metrics and Dong et al.\n(2024) to finer rubrics. The Prometheus mod-\nels (Kim et al., 2023, 2024b) set a benchmark with\njudge models outperforming GPT-4 on pairwise\nand pointwise ranking tasks for subjective rubrics.\nKim et al. (2024a) further studied the performance\ndecrease with increase in subjectivity and created\nthe Prometheus-BGB model which is the current\nstate of the art on this benchmark. More recently,\nVu et al. (2024) have displayed the importance of\nvariety of training data, covering 102 different train-\ning tasks. Several recent works such as Deshpande\net al. (2024); Lee et al. (2024b) improve judge per-\nformance with external augmentations and check-\nlists which further motivates the use of high quality\nreasoning chains and human guidance to train these\nmodels.\nSmall models as capable reasoners and judges\nSeveral of these fine-tuned state of the art models\nabove are excessively large (in the range of 70B or\nmore parameters) and are difficult to deploy on con-\nsumer hardware. While closed source models are\neasier and more performant, they cannot be used\nfor consumer data due to their black box nature.\nThis has led to a shift towards training smaller lan-\nguage models that are better reasoners that can then\nbe transferred to judgment tasks. FlowAI (2024)\nfine-tuned a phi-3.5-mini model and demonstrated\nthat strong base reasoning models can be used as\neffective judges, outperforming closed-source mod-\nels like GPT-40-mini on some tasks. GLIDER"}, {"title": "3 Dataset Generation", "content": "We use a mixture of synthetic datasets and openly\navailable datasets to train GLIDER. We first cre-\nate a detailed taxonomy of potential metrics that\nwe want to cover along with their definitions (Ap-\npendix C). We create a similar taxonomy for the\ndomains that the model is expected to see, cov-\nering 685 unique domains like finance, medicine\nand technology to more creative domains like art,\nfashion and films. This is done to ensure generaliz-\nability in the GLIDER model. We use the Llama-\n3.1-70B model for the curation of the data points\nbecause of its strong instruction following capa-\nbilities and open license (Dubey et al., 2024). We\nadopt a four-stage generation and validation pro-\ncess to ensure the highest data quality possible.\nEvaluation text generation: To ensure that the\nmodel does not overfit to a single evaluation field\nlike user input or model output, we diversify\nour dataset by forcing associations arbitrarily be-\ntween random tag names representing inputs, out-\nputs, contexts and gold answers (subsection D.2).\nThis process randomly assigns number of required\nwords in the generation, the scoring scale (binary\nfor point and pairwise, 1-3 or 1-5 Likert scale) and\nwhether the generation should be a code or a piece\nof text in accordance with Appendix C.\nTo prevent repetitive generations, we vary the\ntemperature between 0.8 and 1 and top_p between\n0.9 and 1. Following (Ge et al., 2024), we define\nan elaborate system prompt that provides an ideal\npersonality to the generator model. The system\nprompt further contains instructions to avoid the\nusage of salutations, comments, introductions and\nmarkdown tags.\nOnce the pointwise data points are created, we\nprompt the model to output a correct and incorrect\nscore and reasoning for the generated instance. We\nencourage the model to actively simulate the behav-\nior of a human annotator and output in bullet point\nformat which improved generation quality in our\nprompt tuning experiments (subsection D.2). After\nthe generation of pointwise ranking data points, we\nfollowed a similar procedure for generating syn-\nthetic pairwise datasets. Based on the metric, we\nask the generator Llama-3.1-70B model to produce\nsample pairs that differ in terms of quality accord-\ning to the metric provided (subsection D.2).\nValidation of scores and reasoning chains: All\nour data points contain a chosen response, which\nis the desired output, and a rejected response con-\ntaining an undersired or incorrect output. This pair-\nwise data generation is used for RLAIF alignment\ntraining phase later (Lee et al., 2024a; Wang et al.,\n2024) where we use the rejected samples to lower\ntheir probabilities and increase the probabilities of\nthe chosen samples. Before we generate highlight\nspans, we perform a sanity verification on the en-\ntire dataset by passing all generated sequence once\nagain through the Llama-3.1-70B model. The LLM\nis given a persona of an experienced data curator\nand verifier which helps it produce more direct and\naccurate responses, following Li et al. (2024a) who\nshow that LLMs are capable of providing high qual-\ntity verifications. The model is then prompted to\nverify the correct and incorrect score and reason-\ning chains based on the generated data instance.\nAfter the automated evaluation, we manually scan\nthe data and remove instances that contain incor-\nrect rubrics, We remove a total of 18,258 samples\n(nearly 14.6% of the generated data) that contained\nduplicates, non-integer scores, markdown, special\ncharacters in the rubric or pass criteria. We retain\ninstances that contain tables or any other structured\ndata as part of the pass criteria or rubric. This deci-\nsion was made for two reasons: a) this enables the\nautomation of pass criteria and rubric generation\nwhere users of the model can prompt the LLM to\ngenerated these for their personal use case, thereby\nsaving effort and minimizing domain shift between"}, {"title": "4 Experimental Setup", "content": "We use the following datasets for assessing the out\nof domain performance of the model:\nPointwise ranking datasets: We use the follow-\ning pointwise datasets for our evaluation:\n1. FLASK (Ye et al., 2023): This dataset con-\ntains 80 base test prompts and 80 instance and\nskill specific, fine grained rubrics. The dataset\nis available with human annotation scores on\na 1-5 ranking scale for several models includ-\ning GPT-3.5-Turbo (OpenAI, 2023), Llama-\n2-Chat-13B (Touvron et al., 2023) and others.\nWe use this data to study the generalizability\nof the fine grained metrics in the training data.\n2. Feedback Bench (Kim et al., 2023): con-\ntains a set of 1000 data points, that include\na set of 200 instance and situation specific in-\nstructions. This is the in-domain test set for\nthe Prometheus models (Kim et al., 2024b)\nand we include it to check the performance of\nGLIDER on these instance specific texts.\n3. Summeval (Fabbri et al., 2020): Contains\nsummaries generated using 16 models for 100\ndifferent source news articles. The dataset\ncontains annotations from 3 expert annotators\nfor each of the 1600 data points. We aver-\nage these annotations and round them to the\nnearest integer score for our task. The Sum-\nmeval dataset helps us understand the human\nalignment of GLIDER through subjective met-\nrics like coherence, consistency, fluency and\nrelevance.\n4. BigGen Bench (Kim et al., 2024a): Contains\n765 instances crafted with a human-in-the\nloop process spanning across 77 tasks. The\ndataset also contains a multilingual subset con-\ntaining evaluation instances in 10 different"}, {"title": "5 Results", "content": "The results for our experiments can be found in\nTable 1 and Table 2. In this section, we investigate\nour research questions based on our results:\nRQ1: Can GLIDER compete with LLMs and ex-\nisting judge models? As observed in Table 1, our\nmodel achieves state of the art performance on the\nFLASK benchmark, beating GPT-40 while still per-\nforming close to models 17\u00d7 its size on the Feed-\nback Collection dataset. Additionally, GLIDER is\nextremely beneficial for researchers working with\nsubjective metrics as shown by its strong perfor-\nmance on the Summeval benchmark where it out-\nperforms GPT-40-mini. We observe that the model\nhas a strong grasp of text coherence and consis-\ntency where we see correlation scores of 0.462 and\n0.522 respectively against human evaluators, out-\nperforming models like Qwen-2.5-72B.\nAnalyzing the performance on pairwise ranking\ndatasets in Table 2, we observe that GLIDER not\nonly outperforms all existing open judge models,\nbut also compares to GPT-40-mini with a difference\nof less than a single F1 score point. This proves\nthat the generalizable synthetic training is highly\neffective for SLM evaluators.\nRQ2: Can GLIDER perform effective multi-\nmetric evaluation? We specifically study this\nbehavior through the performance of GLIDER on\nthe LiveBench dataset. For this dataset, we ob-\nserve that GLIDER not only outperforms existing\njudge models with a score of 0.654, but also much\nlarger LLMs like GPT-40-mini and Qwen-2.5-72B\nwith F1 scores of 0.481 and 0.485 respectively on"}, {"title": "6 Qualitative Analysis", "content": "In this section, we study the quality of GLIDER's\noutputs and discuss strengths and weaknesses of\nthe model.\nTo study the usefulness of the reasoning chains\nand highlight spans as well as the correctness of\nthe score, we sample a set of 100 data points from\nour test sets and perform a human evaluation study\nwith 3 expert annotators. More details about the\nhuman study are provided in subsection D.4. As\nobserved in Table 6, the model outputs receive av-\nerage agreement scores of 91%, 90% and 91% for\neach of the binary metrics. We achieved an over-\nall Krippendorrf's alpha (Krippendorff, 2018) of\n0.838 for the study displaying the high quality of\nannotations. A key observation from human an-\nnotations is the high relevancy of highlight spans\ngenerated by the model which shows that the high-\nlights spans are useful and highly relevant for user\nanalyzing judge performance.\nNext, we perform a qualitative study on the\nmodel's failure cases (Table 5) to analyze its\nstrengths and weaknesses. In our initial tests, we\nobserve that the model is robust to distractions in\ndata points. An example of this distraction in a\nRAG context is shown in the first row of Table 5"}, {"title": "7 Conclusion", "content": "In this work, we fine-tune and align a phi-3.5-mini\nmodel to create GLIDER, an small, explainable and\nperformant SLM-as-judge model. We first create\nsynthetic data with randomized evaluation entity\nassociations, multi-metric samples and a large va-\nriety in domains and metrics. Through our results,\nwe show that GLIDER outperforms closed source\nmodels such as GPT-40-mini and models 17x its\nsize. We show that our training setup retains mul-\ntilingual performance from the pretraining phase\nof the model and we experimentally prove that the\nthe addition of highlight spans not only improves\nperformance but also explainability of the judge.\nWe peform an exhaustive human evaluation study\non our data and model output quality to show high\ninter-annotator agreement in favor of GLIDER."}]}