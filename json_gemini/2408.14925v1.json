{"title": "Distance-Forward Learning: Enhancing the Forward-Forward Algorithm Towards High-Performance On-Chip Learning", "authors": ["Yujie Wu", "Siyuan Xu", "Jibin Wu", "Lei Deng", "Mingkun Xu", "Qinghao Wen", "Guoqi Li"], "abstract": "The Forward-Forward (FF) algorithm was recently proposed as a local learning method to address the limitations of backpropagation (BP), offering biological plausibility along with memory-efficient and highly parallelized computational benefits. However, it suffers from suboptimal performance and poor generalization, largely due to inadequate theoretical support and a lack of effective learning strategies. In this work, we reformulate FF using distance metric learning and propose a distance-forward algorithm (DF) to improve FF performance in supervised vision tasks while preserving its local computational properties, making it competitive for efficient on-chip learning. To achieve this, we reinterpret FF through the lens of centroid-based metric learning and develop a goodness-based N-pair margin loss to facilitate the learning of discriminative features. Furthermore, we integrate layer-collaboration local update strategies to reduce information loss caused by greedy local parameter updates. Our method surpasses existing FF models and other advanced local learning approaches, with accuracies of 99.7% on MNIST, 88.2% on CIFAR-10, 59% on CIFAR-100, 95.9% on SVHN, and 82.5% on ImageNette, respectively. Moreover, it achieves comparable performance with less than 40% memory cost compared to BP training, while exhibiting stronger robustness to multiple types of hardware-related noise, demonstrating its potential for online learning and energy-efficient computation on neuromorphic chips.", "sections": [{"title": "Introduction", "content": "Most current deep learning algorithms are trained using backpropagation (BP) in an end-to-end manner (Sohn 2016; Jaiswal et al. 2020; Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), where training losses are computed at the top layer, and weight updates are derived based on gradients flowing downwards. This process introduces notorious update lock issues and suffers from two critical drawbacks for practical on-chip computations. Firstly, it incurs high memory costs due to the need to store intermediate activations of every layer for the computation of gradients. Secondly, it slows the training speed as each layer depends on the gradient calculations from preceding layers. This computational feature also limits parallel distributed processing capabilities and burdens inner-core communications in many-core hardware architectures, such as emerging neuromorphic chips, preventing highly efficient implementations.\nOn the other hand, the human brain performs synaptic learning in a more efficient, localised manner without waiting for neurons in other brain regions to complete their processes. Recognizing this efficient alternative, Hinton proposed the forward-forward (FF) algorithm (Hinton 2022), which provides an effective layer-wise learning method that replaces traditional backpropagation with two forward passes. Similar to biological neural systems, the learning process of FF is based on directly adjusting neuron activities-either enhancing or reducing activity-in response to different types of incoming patterns. Crucially, FF does not require perfect knowledge of the forward pass computations, allowing learning to proceed even if some network modules are unknown. Moreover, from a hardware implementation perspective, FF eliminates the necessity to store intermediate activations after each module's computation which significantly reduces memory requirements during training. This facilitates model parallelism in many deep network architectures for faster training and inference.\nDespite its considerable computational potential, the FF still struggles with poor generalization across many complex datasets. Several approaches have been proposed to enhance FF from different perspectives, such as employing group convolutional operations (Papachristodoulou et al. 2024), integrating learnable embedding representations for label information (Dong and Shen 2018), adapting to edge applications(Pau and Aymone 2023; Baghersalimi et al. 2023), or applying contrastive learning techniques (Aghagolzadeh and Ezoji 2024; Ahamed, Chen, and Imran 2023). However, none have fully exploited the theoretical interpretation of FFs, and their performances still fail to compete with other advanced local learning methods (Wang et al. 2020; Journ\u00e9 et al. 2022; Ma et al. 2023). More importantly, a comprehensive evaluation of the practical computational advantages of FF-based methods is still lacking.\nIn this paper, we propose a distance-forward (DF) method to enhance FF for high-performance on-chip applications by using distance metric learning methods and integrating layer-collaboration local weight update strategies. The"}, {"title": "Background and related work", "content": "Preliminary for the forward-forward algorithm. The core idea of the FF is to replace the forward and backward passes of backpropagation with two forward passes and manipulate the optimization of the goodness function for two types of data (i.e., positive and negative data) with opposite objectives. For the constructed positive data, the FF training encourages adjustment of the weights to increase the goodness in every hidden layer. Conversely, for the negative data, it adjusts the weights to decrease the goodness function.\nFor the supervised image classification tasks, FF manipulates the input images to generate positive and negative samples. For each input image $x \\in \\mathbb{R}^{m\\times1}$, it replaces the first K pixels of x with the correct (positive) or incorrect (negative) one-hot label $y \\in \\mathbb{R}^{K\\times1}$. This process creates modified patterns denoted as $x^*$, where * $\\in$ {pos, neg} indicates whether the label vector is positive or negative. The goodness function g can be formalized as follows:\n$v^* = Wx^*, g^* = ||v^* ||^2, Loss = \\sigma(g^{pos}-\\theta)+\\sigma(\\theta-g^{neg})$,\nwhere $W \\in \\mathbb{R}^{n\\times m}$ denotes the matrix connecting the previous layer to the current layers, v denotes the weighted input sum, and $\\theta$ is a threshold parameter that modulates the sensitivity to goodness. Given the expression of the goodness function, FF employs the negative log-sigmoid function $\\sigma(x) = log(1 + exp(-x))$, optimizing the network parameters through simultaneously adjusting $\\sigma(g^{pos})$ and $\\sigma(g^{neg})$ in opposite directions.\nContrastive loss and distance metric learning. Distance metric learning (or simply, metric learning) constructs task-specific distance space so that data samples from the same class are close together in the metric space, while keeping data from different classes far apart. Designing a suitable contrastive loss (CL) is fundamental for metric learning. Building on this framework, triplet loss (Dong and Shen 2018) enhances the model's sensitivity by evaluating the relative distances between an anchor pattern and both a positive and a negative pattern pair. N-pair loss (Sohn 2016; Jaiswal et al. 2020) further broadens the approach by simultaneously contrasting multiple negative samples against a single positive sample. In addition to evaluating the relative distances between pairs of patterns, another relevant research direction adjusts the representation distance between the input patterns and specific centres (Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), which guides the distance representation of different types of patterns. Despite their effectiveness, the majority of CL approaches are built upon end-to-end learning and transmit the representation of the input pattern across layers rather than the distance features, which we will delineate in comparison to FF later.\nBlock-wised local learning methods. Another relevant research direction applies CL methods to locally supervised learning tasks, such as allowing gradients to be backpropagated within specific network blocks (Wang et al. 2020; Aghagolzadeh and Ezoji 2024), and using iterative gradient update strategies for different block modules (Xiong, Ren, and Urtasun 2020; Ma et al. 2023). However, these works rely on auxiliary classifiers for decoding, which typically involve multi-layer structures, heavily increasing computational and memory costs. One recent work (Ahamed, Chen, and Imran 2023) applied triplet loss to block-wise learning and relate it with FF. However, it ignores the goodness-based fundamental features of FF, and its performance has yet to reach the state of the art.\nBio-inspired local learning methods. Various local training methods have been developed as alternatives to BP. One approach is modifying BP's feedback circuits. Feedback alignment (Lillicrap et al. 2016) provides a more biologically plausible alternative to BP by replacing each of BP's feedback connections with a random matrix. Direct Feedback Alignment (DA) (N\u00f8kland 2016) further simplifies weight updates by employing a random direct feedback matrix and avoiding non-local computations. Unlike bioplausible BP variants, competitive Hebbian learning allows neurons to compete for activation, favoring those most responsive to input (Miconi 2021). Recent progress in soft Hebbian learning (Journ\u00e9 et al. 2022) and predictive Hebbian learning (Halvagal and Zenke 2023) further unveils the potential of bio-inspired learning rules to adapt to deeper"}, {"title": "Proposed approach", "content": "In this section, we reformulate FF and analyze its similarities and distinctive features through the lens of distance metric learning. Building on this foundation, we first propose two goodness-based CL techniques to enhance performance. Subsequently, we further elaborate on the two local update strategies to alleviate the overly greedy issues of local learning while preserving computational parallelism. Finally, we present the overall architecture of our model."}, {"title": "Formulating FF from distance metric learning", "content": "There are two fundamental operations employed in conventional metric learning framework: a network first extracts hierarchical representations through multi-layer network modules (denoted the network mapping by $f_\\theta$) and then calculates distances within a specific distance metric space between the representations of the input patterns $f_\\theta(x)$ and a specific anchor pattern $f_\\theta (c)$. The anchor pattern can be constructed using different methods, such as sampling another input pattern (i.e., positive sample (Dong and Shen 2018; Jaiswal et al. 2020)) or constructing a cluster centroid representing the central tendency of samples belonging to the same class (Yang, Parikh, and Batra 2016; Qi and Su 2017; Cai, Xiong, and Tian 2023) (referred to as centroid-based metric learning below). Typically, the two operations can be formulated as\n$DCL = F(d(f_\\theta(x), f_\\theta(c)))$.\nHere F denotes a general format of contrastive loss (e.g., triplet loss) manipulating on the discrepancy $d(f_\\theta(x), f_\\theta(c))$. By analyzing the hidden activations in the first layer of the FF (see Figure 1), we demonstrate in the following that applying FF to a two-layer network aligns with typical centroid-based metric learning methods (Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), whereas extending FF to multi-layer architectures introduces a specific prototype of distance-forward metric learning that initially creates a distance representation in the shallow layers and refines this representation in deeper layers.\nRelating FF with centroid-based metric learning in a two-layer structure. Our idea stems from a reconstruction of the weight matrix $W \\in \\mathbb{R}^{n\\times m}$, which divides W into two parts: $W_x \\in \\mathbb{R}^{n\\times m}$ for the input patterns x and $W_y \\in \\mathbb{R}^{n\\times K}$ for the labels $y^*$. Here we set the first K rows of W to zeros to cancel out the impact of the first K pixels"}, {"title": "Local layer collaboration strategies for DF models", "content": "A large body of local learning methods, including FF, updates weights in a greedy, layerwise manner. This approach can result in the loss of useful features for deeper layers since the update in lower layers is unaware of the higher-level representation needs. To facilitate effective information communication across layers while preserving the computational superiority of FF, we develop two weight update strategies, named DF-O and DF-R, based on the methods introduced in (N\u00f8kland 2016; Xiong, Ren, and Urtasun 2020). These strategies can provide greater flexibility in balancing task accuracy, hardware deployability, and computational cost.\nDF-O: First, to achieve high task accuracy, the DF-O employs an overlapping gradient update (OG) strategy (Xiong, Ren, and Urtasun 2020) by default. As shown in Figure 2B, this strategy iteratively groups two adjacent layers and trains each block locally, allowing gradient information to propagate only across two grouped layers. This method offers a significant advantage over the greedy local update by enabling information flow from the top layer to the bottom layer with only a minimal increase in computational load. A similar method has been successfully used in (Dooms, Tsang, and Oramas 2023) to enhance classification accuracy.\nDF-R: Second, to further enhance biological plausibility and facilitate on-chip implementation, DF-R integrates random direct feedback (FA) connections (N\u00f8kland 2016) and the OG strategy. As illustrated in Figure 2C, a random feedback connection replaces the back-propagation circuits within the OG update (see dashed red line in Figure 2B). It thereby allows the loss to be directly used to update all relevant weights. This approach offers three main benefits beyond task accuracy: (1) Higher on-chip computational parallelism. The fully local update can better leverage the pipeline processing mechanisms of many-core chips and minimize idle times for processing units. (2) Reduced volume of data transfer between computation cores. By limiting the need for extensive communication between cores, DF-R effectively alleviates the inter-core communication workload. (3) Increased biological plausibility.The weight-asymmetric update method breaks the local update-locking required by OG methods and enables real-time weight updates, which align more closely with biological plasticity circuits. Implementation details for the two strategies are provided in Appendix A.4."}, {"title": "Overall architecture of DF models", "content": "To provide a comprehensive understanding, we outline the general training procedures of our methods below.\nGeneration of positive and negative samples. We adopt a learnable linear projection for encoding the label information. For simplicity, we use a single-channel learnable embedding for representing label information, keeping the embedding size equal to a single channel of the input image. Positive and negative samples are then generated by concatenating original images with the embedding representation for correct and incorrect labels, respectively.\nTraining phase. We follow the basic training phase of (Hinton 2022) to ensure consistency. For each positive sample, we generate N pairs of negative samples by randomly selecting N incorrect labels. Subsequently, the positive and negative samples are fed into the network to obtain corresponding activations. We then calculate the goodness function, optimize the loss function and update the network parameters. Batch normalization, as suggested by (Dooms, Tsang, and Oramas 2023), is performed before calculating the weighted sum for each layer.\nDecoding and evaluation. The correct output labels are chosen following Hinton's goodness-based evaluation methods (Hinton 2022). Specifically, for each testing sample, we evaluate the goodness of different layers in response to combinations of testing samples and candidate labels. The labels with the highest goodness value are selected as output labels."}, {"title": "Experiments", "content": "Experimental setup\nWe extensively evaluate model performance on six datasets including MNIST, Fashion MNIST (F-MNIST), SVHN, CIFAR-10, CIFAR-100, and Imagenette. The Imagenette is a subset of ImageNet, which is used to facilitate comparison against the advanced local learning method introduced in (Journ\u00e9 et al. 2022). To facilitate experimental analysis, we use a ten-layer CNN structure for the results presented in Table 1, Table 2, Figures 4 C-E, Figure S1 and S2 (see Appendix B for details). The default optimizer, Adam, with a learning rate of le-3 and a cosine scheduler, has been utilized across all experiments. Details of other hyperparameter settings are available in Appendix A.1."}, {"title": "Ablation study", "content": "Effectiveness of the N-pair loss design. We compare the impact of different numbers of negative pairs in calculating the loss function (see Eq. 7) on classification accuracy using the DF-O. Table 2 shows that employing more negative samples consistently improves performance accuracy. Given the larger number of categories in CIFAR100, a notable increase in classification accuracy can be observed on CIFAR100 as the number of N-pairs increases. Additionally, we also replace the maximum operation by using the average of several negative sample projections to calculate the loss in Eq. 7. Table 2 suggests that this strategy led to even worse performance, validating our rationality for utilization of N-pair sampling.\nImpact of regularization coefficients. We examine the impact of regularization by varying the regularization coefficients (1) in Figures S1A and B using the DF-O model. Noting that x = 0 equates to a model without regularization, results indicate that a suitable A value can achieve higher classification accuracy than the model without regularization, validating the utility of this regularization component.\nEffectiveness of the goodness-based margin loss. We compare in Figure S1C the impact of the proposed loss function (Eq. 7) against the original FF on task performance. For fairness, both models are equipped with identical network architectures and OG update strategy. Results show that the proposed loss function leads to consistent improvements over FF on both Fashion MNIST and CIFAR-10 datasets, substantiating the efficacy of the loss designs."}, {"title": "Conclusions", "content": "We offer a new perspective of FF from distance metric learning and present a DF method, which employs a goodness-based N-pair margin loss and integrates layer-collaboration gradient update methods, enabling the extraction of discriminative features while preserving the computation benefits of local learning. Our extensive experiments confirm that the DF models outperform FF-based models, achieving comparable results to other advanced local learning methods. Furthermore, our quantitative analysis of memory cost, training time, and robustness to multiple hardware-related noise corroborates the potential advantages of the DF in high-performance on-chip computations, making it suitable for energy-efficient cutting-edge applications."}]}