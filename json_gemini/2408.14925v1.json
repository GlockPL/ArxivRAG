{"title": "Distance-Forward Learning: Enhancing the Forward-Forward Algorithm Towards High-Performance On-Chip Learning", "authors": ["Yujie Wu", "Siyuan Xu", "Jibin Wu", "Lei Deng", "Mingkun Xu", "Qinghao Wen", "Guoqi Li"], "abstract": "The Forward-Forward (FF) algorithm was recently proposed as a local learning method to address the limitations of backpropagation (BP), offering biological plausibility along with memory-efficient and highly parallelized computational benefits. However, it suffers from suboptimal performance and poor generalization, largely due to inadequate theoretical support and a lack of effective learning strategies. In this work, we reformulate FF using distance metric learning and propose a distance-forward algorithm (DF) to improve FF performance in supervised vision tasks while preserving its local computational properties, making it competitive for efficient on-chip learning. To achieve this, we reinterpret FF through the lens of centroid-based metric learning and develop a goodness-based N-pair margin loss to facilitate the learning of discriminative features. Furthermore, we integrate layer-collaboration local update strategies to reduce information loss caused by greedy local parameter updates. Our method surpasses existing FF models and other advanced local learning approaches, with accuracies of 99.7% on MNIST, 88.2% on CIFAR-10, 59% on CIFAR-100, 95.9% on SVHN, and 82.5% on ImageNette, respectively. Moreover, it achieves comparable performance with less than 40% memory cost compared to BP training, while exhibiting stronger robustness to multiple types of hardware-related noise, demonstrating its potential for online learning and energy-efficient computation on neuromorphic chips.", "sections": [{"title": "Introduction", "content": "Most current deep learning algorithms are trained using backpropagation (BP) in an end-to-end manner (Sohn 2016; Jaiswal et al. 2020; Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), where training losses are computed at the top layer, and weight updates are derived based on gradients flowing downwards. This process introduces notorious update lock issues and suffers from two critical drawbacks for practical on-chip computations. Firstly, it incurs high memory costs due to the need to store intermediate activations of every layer for the computation of gradients. Secondly, it slows the training speed as each layer depends on the gradient calculations from preceding layers. This computational feature also limits parallel distributed processing capabilities and burdens inner-core communications in many-core hardware architectures, such as emerging neuromorphic chips, preventing highly efficient implementations.\nOn the other hand, the human brain performs synaptic learning in a more efficient, localised manner without waiting for neurons in other brain regions to complete their processes. Recognizing this efficient alternative, Hinton proposed the forward-forward (FF) algorithm (Hinton 2022), which provides an effective layer-wise learning method that replaces traditional backpropagation with two forward passes. Similar to biological neural systems, the learning process of FF is based on directly adjusting neuron activities-either enhancing or reducing activity-in response to different types of incoming patterns. Crucially, FF does not require perfect knowledge of the forward pass computations, allowing learning to proceed even if some network modules are unknown. Moreover, from a hardware implementation perspective, FF eliminates the necessity to store intermediate activations after each module's computation which significantly reduces memory requirements during training. This facilitates model parallelism in many deep network architectures for faster training and inference.\nDespite its considerable computational potential, the FF still struggles with poor generalization across many complex datasets. Several approaches have been proposed to enhance FF from different perspectives, such as employing group convolutional operations (Papachristodoulou et al. 2024), integrating learnable embedding representations for label information (Dong and Shen 2018), adapting to edge applications(Pau and Aymone 2023; Baghersalimi et al. 2023), or applying contrastive learning techniques (Aghagolzadeh and Ezoji 2024; Ahamed, Chen, and Imran 2023). However, none have fully exploited the theoretical interpretation of FFs, and their performances still fail to compete with other advanced local learning methods (Wang et al. 2020; Journ\u00e9 et al. 2022; Ma et al. 2023). More importantly, a comprehensive evaluation of the practical computational advantages of FF-based methods is still lacking.\nIn this paper, we propose a distance-forward (DF) method to enhance FF for high-performance on-chip applications by using distance metric learning methods and integrating layer-collaboration local weight update strategies. The distance metric space framework offers a transparent geometrical interpretation of FF and its variants, helping us understand the computational principles and guiding our model design. By exploiting layer-collaboration local gradient update strategies, we aim to offer flexibility in balancing task accuracy and computation cost. The proposed method has been extensively evaluated on six image classification datasets with different scales, using a deeper layer structure beyond that the original FF considered. Our three key contributions are included:\n\u2022 We provide a geometrical analysis to reformulate FFs from the centroid-based metric learning perspective, bridging the gap between the two techniques. By revisiting metric learning, we delineate the similarities and differences between FF and existing methods. This facilitates understanding of the key computational principles and enlightens readers on how to proceed with further model designs.\n\u2022 Building upon the analysis, we propose a DF model by leveraging a goodness-based margin loss and an N-pair data mining technique. Furthermore, we integrate different layer-collaboration local gradient update strategies in DF. The proposed model can not only learn hierarchical distance-based representation across multiple layers but also provide flexible choices in balancing task accuracy and practical computation cost.\n\u2022 We comprehensively evaluate the DF on accuracy, robustness and computational overhead. The DF outperforms FF-based models and other advanced local learning methods on versatile image classification tasks, demonstrating significantly better performance. Furthermore, it achieves comparable performance with less than 40% memory cost compared to BP training, while exhibiting stronger robustness to multiple hardware-related sources of noise, highlighting its potential suitability for online learning and neuromorphic chips."}, {"title": "Background and related work", "content": "Preliminary for the forward-forward algorithm. The core idea of the FF is to replace the forward and backward passes of backpropagation with two forward passes and manipulate the optimization of the goodness function for two types of data (i.e., positive and negative data) with opposite objectives. For the constructed positive data, the FF training encourages adjustment of the weights to increase the goodness in every hidden layer. Conversely, for the negative data, it adjusts the weights to decrease the goodness function.\nFor the supervised image classification tasks, FF manipulates the input images to generate positive and negative samples. For each input image x \u2208 \u211d^(m\u00d71), it replaces the first K pixels of x with the correct (positive) or incorrect (negative) one-hot label y \u2208 \u211d^(K\u00d71). This process creates modified patterns denoted as x*, where * \u2208 {pos, neg} indicates whether the label vector is positive or negative. The goodness function g can be formalized as follows:\nv* = Wx*, g* = ||v* ||^2, Loss = \u03c3(g^(pos)\u2212\u03b8)+\u03c3(\u03b8-g^(neg)),\n(1)\nwhere W \u2208 \u211d^(n\u00d7m) denotes the matrix connecting the previous layer to the current layers, v denotes the weighted input sum, and \u03b8 is a threshold parameter that modulates the sensitivity to goodness. Given the expression of the goodness function, FF employs the negative log-sigmoid function \u03c3(x) = log(1 + exp(-x)), optimizing the network parameters through simultaneously adjusting \u03c3(g^(pos)) and \u03c3(g^(neg)) in opposite directions.\nContrastive loss and distance metric learning. Distance metric learning (or simply, metric learning) constructs task-specific distance space so that data samples from the same class are close together in the metric space, while keeping data from different classes far apart. Designing a suitable contrastive loss (CL) is fundamental for metric learning. Building on this framework, triplet loss (Dong and Shen 2018) enhances the model's sensitivity by evaluating the relative distances between an anchor pattern and both a positive and a negative pattern pair. N-pair loss (Sohn 2016; Jaiswal et al. 2020) further broadens the approach by simultaneously contrasting multiple negative samples against a single positive sample. In addition to evaluating the relative distances between pairs of patterns, another relevant research direction adjusts the representation distance between the input patterns and specific centres (Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), which guides the distance representation of different types of patterns. Despite their effectiveness, the majority of CL approaches are built upon end-to-end learning and transmit the representation of the input pattern across layers rather than the distance features, which we will delineate in comparison to FF later.\nBlock-wised local learning methods. Another relevant research direction applies CL methods to locally supervised learning tasks, such as allowing gradients to be backpropagated within specific network blocks (Wang et al. 2020; Aghagolzadeh and Ezoji 2024), and using iterative gradient update strategies for different block modules (Xiong, Ren, and Urtasun 2020; Ma et al. 2023). However, these works rely on auxiliary classifiers for decoding, which typically involve multi-layer structures, heavily increasing computational and memory costs. One recent work (Ahamed, Chen, and Imran 2023) applied triplet loss to block-wise learning and relate it with FF. However, it ignores the goodness-based fundamental features of FF, and its performance has yet to reach the state of the art.\nBio-inspired local learning methods. Various local training methods have been developed as alternatives to BP. One approach is modifying BP's feedback circuits. Feedback alignment (Lillicrap et al. 2016) provides a more biologically plausible alternative to BP by replacing each of BP's feedback connections with a random matrix. Direct Feedback Alignment (DA) (N\u00f8kland 2016) further simplifies weight updates by employing a random direct feedback matrix and avoiding non-local computations. Unlike bioplausible BP variants, competitive Hebbian learning allows neurons to compete for activation, favoring those most responsive to input (Miconi 2021). Recent progress in soft Hebbian learning (Journ\u00e9 et al. 2022) and predictive Hebbian learning (Halvagal and Zenke 2023) further unveils the potential of bio-inspired learning rules to adapt to deeper"}, {"title": "Proposed approach", "content": "In this section, we reformulate FF and analyze its similarities and distinctive features through the lens of distance metric learning. Building on this foundation, we first propose two goodness-based CL techniques to enhance performance. Subsequently, we further elaborate on the two local update strategies to alleviate the overly greedy issues of local learning while preserving computational parallelism. Finally, we present the overall architecture of our model."}, {"title": "Formulating FF from distance metric learning", "content": "There are two fundamental operations employed in conventional metric learning framework: a network first extracts hierarchical representations through multi-layer network modules (denoted the network mapping by f\u03b8) and then calculates distances within a specific distance metric space between the representations of the input patterns f\u03b8(x) and a specific anchor pattern f\u03b8(c). The anchor pattern can be constructed using different methods, such as sampling another input pattern (i.e., positive sample (Dong and Shen 2018; Jaiswal et al. 2020)) or constructing a cluster centroid representing the central tendency of samples belonging to the same class (Yang, Parikh, and Batra 2016; Qi and Su 2017; Cai, Xiong, and Tian 2023) (referred to as centroid-based metric learning below). Typically, the two operations can be formalized as\nDCL = F(d(f\u03b8(x), f\u03b8(c))).\n(2)\nHere F denotes a general format of contrastive loss (e.g., triplet loss) manipulating on the discrepancy d(f\u03b8(x), f\u03b8(c)). By analyzing the hidden activations in the first layer of the FF (see Figure 1), we demonstrate in the following that applying FF to a two-layer network aligns with typical centroid-based metric learning methods (Rippel et al. 2015; Qi and Su 2017; Li et al. 2021), whereas extending FF to multi-layer architectures introduces a specific prototype of distance-forward metric learning that initially creates a distance representation in the shallow layers and refines this representation in deeper layers.\nRelating FF with centroid-based metric learning in a two-layer structure. Our idea stems from a reconstruction of the weight matrix W \u2208 \u211d^(n\u00d7m), which divides W into two parts: Wx \u2208 \u211d^(n\u00d7m) for the input patterns x and Wy \u2208 \u211d^(n\u00d7K) for the labels y*. Here we set the first K rows of W to zeros to cancel out the impact of the first K pixels on x* employed in Eq. 1. Then, the weighted sum v can be formulated as:\nv* = Wxx + Wyy*, * \u2208 {pos, neg}.\n(3)\nReformulating Eq. 3 by introducing W\u0303y := -Wy gives a clearer distance-based expression:\ng^(pos) = ||Wxx-W\u0303y^(pos)||^2, g^(neg) = ||Wxx-W\u0303y^(neg)||^2.\n(4)\nThe loss function of Eq. 1 then optimizes the two distances in opposite directions. This formulation indicates that the original goodness function calculates the distance between the projection of input patterns Wxx and different labels W\u0303y^(neg) or W\u0303y^(pos). Thus, optimizing the goodness function essentially adjusts these absolute Euclidean distances. Observing that the projection of label information remains consistent across different patterns, it acts as an anchor vector to guide the metric learning process. This approach aligns with the principles of previous centroid-based metric learning methods, where distances between input samples and a constructed centroid (e.g., K-mean-based clustering (Yang, Parikh, and Batra 2016)) are used for adjusting representation distances.\nThis reformulation offers a fresh perspective on FF and its variants, and the following items elucidate its implications:\n\u2022 Geometrical interpretation of FF and its variants. FF can be interpreted as manipulating the absolute distance in an L2 metric space. Follow-up studies (Lee and Song 2023; Dooms, Tsang, and Oramas 2023) further improve performance and convergence of FF by using a new loss function \u03c3(g^(pos) \u2014 g^(neg)). As indicated by Figure 1C, this function can be interpreted as optimizing the relative distance between gros and greg, which avoids being overly influenced by the scale of activation and balances the distances between positive and negative samples more effectively.\n\u2022 Effectiveness of minimizing/maximizing goodness. As observed by Hinton (Hinton 2022), optimizing the goodness function of positive samples in opposite directions leads to comparable classification accuracy. Figure 1B provides a clear explanation: both strategies, pushing the positive samples closer to or further away from the anchor vectors (i.e., the label vector), are effective for learning the discriminative distance distribution of different samples.\n\u2022 Effectiveness of label encoding. Studies (Kohan, Rietman, and Siegelmann 2023; Dooms, Tsang, and Oramas 2023) suggest that learnable embedding representations y enhances goodness discrepancies and leads to better performance. This feature can be supported from the perspective of metric learning, which indicates that well-separated representations for centroid patterns benefit the learning of discriminative representations.\nThe above analysis illustrates the similarity between FFs and centroid-based metric learning. In the following section, we will provide an analysis of the unique features of FF when applied to multi-layer architecture."}, {"title": "Refining distance-based representation through multiple-layer structure", "content": "Compared with conventional CL techniques, FF inverts the order of the above two operations: it begins by deriving distance-based representations in the first hidden layer, as illustrated previously, and subsequently propagates these distance-based representations to deeper layers. This process can be formalized by the following equation:\nDF F = F(f\u03b8(d(x, c))).\n(5)\nConsidering that the primary objective of supervised contrastive learning is to extract discriminative features from input data to enhance pattern recognition, we hypothesize that the multi-layer architecture of FF can gradually refine the distance metric initially established in the shallow layers through multi-layer computation and thereby attain more discriminative features. This hypothesis will be empirically tested in the experimental section to verify the efficacy of multi-layer enhancement."}, {"title": "Goodness-based margin loss for enhanced distance representation", "content": "We have interpreted goodness as a distance measure and conceptualized the FF within a distance metric space. From this perspective, the activation of the first hidden layer represents the discrepancy between projections of input patterns and labels, and the information propagation across layers can further refine this discrepancy representation for enhancing feature discriminability. Inspired by the geometrical interpretation and principles from the triplet loss (Dong and Shen 2018), we propose a goodness-based margin loss with an additional regularization term on the goodness function:\nL = max(m+ + g^(neg) \u2013 g^(pos),0) + \u03bbg^(neg),\n(6)\nwhere \u03bb denotes the coefficient of the regularization term. Instead of penalizing the discrepancy for every positive and negative pair, the margin loss seeks to impose stronger penalties on those support vectors (see solid boxes in Figure 1D) that can effectively separate the positive and negative goodness functions. From the perspective of metric space, this encourages positive samples to be closer to the anchor of the label projection than negative samples by a specified margin m+. Furthermore, the regularization further reduces the absolute distance denoted by the goodness function of negative samples, ensuring that negative samples are kept far away from any projections of labels."}, {"title": "N-pair negative sample mining for label balance", "content": "The FF and its previous variants learn from single pairs of positive and negative samples. For a given input query sample, these methods compare it with one negative sample, neglecting negative samples from other classes. This strategy can lead to the label-imbalance issue in complex tasks involving many categories because individual updates may bias the distance between a single pair of positive and negative samples, making convergence unstable and slow. Hence, employing multiple N-negative samples for learning could be beneficial for alleviating the convergence problem. Notably, the concept of employing an N-pair loss function is not new; it has been proposed in metric learning for many end-to-end BP methods (Sohn 2016; Jaiswal et al. 2020). Here we adapt this idea to the local learning context, aiming to enhance the model's feature extraction capabilities for complex tasks.\nTo this end, we employ multiple negative samples for calculating the loss function of the goodness function. By leveraging the multiple negative samples, we derive a more comprehensive loss function built on Eq. 6:\nL = max(m++max{g^(neg)_k}^(N)_k=1-g^(pos), 0)+\u03bbmax({g^(neg)_k}^(N)_k=1),\n(7)\nwhere the lower index k refers to the index of negative samples and N refers to the number of negative samples generated for one positive sample."}, {"title": "Local layer collaboration strategies for DF models", "content": "A large body of local learning methods, including FF, updates weights in a greedy, layerwise manner. This approach can result in the loss of useful features for deeper layers since the update in lower layers is unaware of the higher-level representation needs. To facilitate effective information communication across layers while preserving the computational superiority of FF, we develop two weight update strategies, named DF-O and DF-R, based on the methods introduced in (N\u00f8kland 2016; Xiong, Ren, and Urtasun 2020). These strategies can provide greater flexibility in balancing task accuracy, hardware deployability, and computational cost.\nDF-O: First, to achieve high task accuracy, the DF-O employs an overlapping gradient update (OG) strategy (Xiong, Ren, and Urtasun 2020) by default. As shown in Figure 2B, this strategy iteratively groups two adjacent layers and trains each block locally, allowing gradient information to propagate only across two grouped layers. This method offers a significant advantage over the greedy local update by enabling information flow from the top layer to the bottom layer with only a minimal increase in computational load. A similar method has been successfully used in (Dooms, Tsang, and Oramas 2023) to enhance classification accuracy.\nDF-R: Second, to further enhance biological plausibility and facilitate on-chip implementation, DF-R integrates random direct feedback (FA) connections (N\u00f8kland 2016) and the OG strategy. As illustrated in Figure 2C, a random feedback connection replaces the back-propagation circuits within the OG update (see dashed red line in Figure 2B). It thereby allows the loss to be directly used to update all relevant weights. This approach offers three main benefits beyond task accuracy: (1) Higher on-chip computational parallelism. The fully local update can better leverage the pipeline processing mechanisms of many-core chips and minimize idle times for processing units. (2) Reduced volume of data transfer between computation cores. By limiting the need for extensive communication between cores, DF-R effectively alleviates the inter-core communication workload. (3) Increased biological plausibility.The weight-asymmetric update method breaks the local update-locking required by OG methods and enables real-time weight updates, which align more closely with biological plasticity circuits. Implementation details for the two strategies are provided in Appendix A.4."}, {"title": "Overall architecture of DF models", "content": "To provide a comprehensive understanding, we outline the general training procedures of our methods below.\nGeneration of positive and negative samples. We adopt a learnable linear projection for encoding the label information. For simplicity, we use a single-channel learnable embedding for representing label information, keeping the embedding size equal to a single channel of the input image. Positive and negative samples are then generated by concatenating original images with the embedding representation for correct and incorrect labels, respectively.\nTraining phase. We follow the basic training phase of (Hinton 2022) to ensure consistency. For each positive sample, we generate N pairs of negative samples by randomly selecting N incorrect labels. Subsequently, the positive and negative samples are fed into the network to obtain corresponding activations. We then calculate the goodness function, optimize the loss function and update the network parameters. Batch normalization, as suggested by (Dooms, Tsang, and Oramas 2023), is performed before calculating the weighted sum for each layer.\nDecoding and evaluation. The correct output labels are chosen following Hinton's goodness-based evaluation methods (Hinton 2022). Specifically, for each testing sample, we evaluate the goodness of different layers in response to combinations of testing samples and candidate labels. The labels with the highest goodness value are selected as output labels."}, {"title": "Experiments", "content": "Experimental setup\nWe extensively evaluate model performance on six datasets including MNIST, Fashion MNIST (F-MNIST), SVHN, CIFAR-10, CIFAR-100, and Imagenette. The Imagenette is a subset of ImageNet, which is used to facilitate comparison against the advanced local learning method introduced in (Journ\u00e9 et al. 2022). To facilitate experimental analysis, we use a ten-layer CNN structure for the results presented in Table 1, Table 2, Figures 4 C-E, Figure S1 and S2 (see Appendix B for details). The default optimizer, Adam, with a learning rate of le-3 and a cosine scheduler, has been utilized across all experiments. Details of other hyperparameter settings are available in Appendix A.1."}, {"title": "Comprehensive performance evaluation", "content": "Task accuracy. We evaluate the proposed DF methods against end-to-end BP learning, FF-based variants, and other state-of-the-art non-BP approaches across six datasets. As shown in Table 1, both DF-R and DF-O models perform well on small-scale datasets and outperform other FF-based models in most evaluation datasets.Furthermore, benefiting from relaxing the constraint and allowing block-wise gradient backpropagation, the DF-O demonstrates comparable performance with other advanced local learning methods, approaching the performance of end-to-end BP learning. We also notice that CFSE (Papachristodoulou et al. 2024) shows a higher accuracy on CIFAR100. By adopting the softmax classifier in their model, CFSE employs a different decoding strategy that may benefit performance while producing a larger number of trainable parameters.\nHierarchical representations through deep layers. The foremost question for DF is whether the propagation of distance-based representation can leverage multi-layer architecture to facilitate the learning of discriminative features. To this end, we analyze the representation of goodness across different layers using the DF-O model, as depicted in Figure 3. We quantitatively measure the discrepancy in goodness between positive and negative training samples, alongside the testing accuracy derived from the goodness function of each layer. As demonstrated in Figure 3, a consistent discrepancy in goodness is observed between the positive and negative samples across tasks in deeper layers. This change in discrepancy correlates with the trend in testing accuracy, providing evidence of the hierarchical representations achieved by DF.\nHardware computational efficiency, quantization and robustness to the device-induced noise. The local computation paradigm of DF methods make it hardware friendly. We comprehensively evaluate this feature by examining memory costs, training time, quantization effect, and robustness to hardware-related noise. As illustrated in Figures 4A and B, the end-to-end BP requires storing activations during inference for parameter updates, leading to linear increases in memory costs and training time as the number of network layers grows. Conversely, the memory cost for both DF models remains significantly lower than that for BP, requiring less than about 40% of the memory cost for an 11-layer architecture and about 60% for a 25-layer architecture. Additionally, since the parameter update procedure in DF-R is highly parallel, its optimal gradient computation time-estimated by the backward time in PyTorch (see Appendix A.4) is lower than that of BP. We also note that the two-layer block-wise update strategy does not significantly burden computational parallelism. Nevertheless, it is worth clarifying that for DF-R, if one considers grouping more than two layers for overlapping updates, the computational advantages of DF-R can be more pronounced (see Figure S2)."}, {"title": "Ablation study", "content": "Unlike ideal simulation environments, device non-ideality issues in analog computing introduce various types of noise, impacting on-chip computations. We evaluate the robustness of DF methods to diverse hardware-related noises, focusing on three typical noise sources: Device mismatch noise, impulse sensor noise, and photon shot noise (Hendrycks and Dietterich 2018; Yang et al. 2022). Device mismatch noise, causing network parameters to deviate from desired values, is simulated by adding Gaussian noise to the gradient of network parameters in each layer during the training phase. Photon shot noise inherent in the discrete nature of light, is simulated by introducing Poisson noise into testing patterns. Impulse sensor noise, typical in neuromorphic vision sensors, is simulated by injecting a color analogue of salt-and-pepper noise into testing patterns. We compare the DF-O and the vanilla BP models across different types of testing noises and various noise levels (see Appendix A.3 for implementation details) and measure the testing accuracy. As indicated in Figures 4C-E, while under a clean condition, DF-O performance is slightly worse than that of backpropagation. As the noise levels increase, DF-O achieves consistently higher classification accuracy compared to the BP approach. This superiority is likely due to the crafted margin loss, which facilitates creating discriminative and robust representations. Additionally, our quantization evaluation shown in Figure S3 corroborates that our model works well even in 4-bit configuration. Altogether, our validations suggest a stable on-chip performance of the proposed model."}, {"title": "Effectiveness of the N-pair loss design", "content": "We compare the impact of different numbers of negative pairs in calculating the loss function (see Eq. 7) on classification accuracy using the DF-O. Table 2 shows that employing more negative samples consistently improves performance accuracy. Given the larger number of categories in CIFAR100, a notable increase in classification accuracy can be observed on CIFAR100 as the number of N-pairs increases. Additionally, we also replace the maximum operation by using the average of several negative sample projections to calculate the loss in Eq. 7. Table 2 suggests that this strategy led to even worse performance, validating our rationality for utilization of N-pair sampling."}, {"title": "Impact of regularization coefficients", "content": "We examine the impact of regularization by varying the regularization coefficients (\u03bb) in Figures S1A and B using the DF-O model. Noting that \u03bb = 0 equates to a model without regularization, results indicate that a suitable \u03bb value can achieve higher classification accuracy than the model without regularization, validating the utility of this regularization component."}, {"title": "Effectiveness of the goodness-based margin loss", "content": "We compare in Figure S1C the impact of the proposed loss function (Eq. 7) against the original FF on task performance. For fairness, both models are equipped with identical network architectures and OG update strategy. Results show that the proposed loss function leads to consistent improvements over FF on both Fashion MNIST and CIFAR-10 datasets, substantiating the efficacy of the loss designs."}, {"title": "Conclusions", "content": "We offer a new perspective of FF from distance metric learning and present a DF method, which employs a goodness-based N-pair margin loss and integrates layer-collaboration gradient update methods, enabling the extraction of discriminative features while preserving the computation benefits of local learning. Our extensive experiments confirm that the DF models outperform FF-based models, achieving comparable results to other advanced local learning methods. Furthermore, our quantitative analysis of memory cost, training time, and robustness to multiple hardware-related noise corroborates the potential advantages of the DF in high-performance on-chip computations, making it suitable for energy-efficient cutting-edge applications."}]}