{"title": "On Biases in a UK Biobank-based Retinal Image Classification Model", "authors": ["Anissa Alloula", "Rima Mustafa", "Daniel R McGowan", "Bart\u0142omiej W. Papie\u017c"], "abstract": "Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standard-isation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely un-able to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specific type of bias.", "sections": [{"title": "Introduction and Related Work", "content": "Biases and Disparities in Machine Learning. An emerging concern in ma-chine learning (ML) research is that strong overall performance may obscure critical disparities, leading to substantially inferior outcomes for certain sub-groups. Examples of this unequal performance have been identified in clinical ML models, across a range of tasks and modalities such as skin lesion classifica-tion [3], brain Magnetic Resonance Imaging (MRI) reconstruction [10], cardiac MRI segmentation [19], and affecting various subgroups, from certain ethnic groups [19,16], to disadvantaged socioeconomic groups [24]. Not only do these biases harm the minority groups who are subject to them, but they also hinder the generalisability of the models to unseen population samples [21], constituting a major barrier to the implementation of ML models in clinical settings."}, {"title": "Existing Approaches to Address such Biases", "content": "A line of research focused on preventing such disparities has consequently emerged. Bias mitigation can be conducted at various stages in the ML pipeline: during data collection, in the pre-processing stage, while the model is training, and/or in post-processing. Ob-jectives vary between methods and can include boosting minimum performance [9], reducing gaps in performance (equalised odds [10]), or equalising the number of positive predictions across groups (demographic parity [6]). However, recent work has highlighted that despite the multitude of existing methods, the prob-lem is far from solved. A benchmark from 2023, MEDFAIR, showed that across a range of medical tasks, no method consistently and significantly outperformed empirical risk minimisation (where there is no fairness objective) [33]."}, {"title": "Problem Setting", "content": "In this study, we focus on the appearance of biases and their mitigation in retinal imaging-based models. Bias mitigation research has been lacking in this field, with, to the best of our knowledge, only two examples: work by Burlina et al. [5] and work by Coyner et al. [8], who tried to mitigate race-related disparities with synthetic data and data pre-processing, respectively. We build on this work by conducting the largest and most comprehensive exploration of disparities and mitigation methods in retinal imaging to date. We use retinal images from the UK Biobank (UKBB), an unparalleled medical database of over half a million UK adults [25]. We complement recent work which has identified selection bias in the UKBB [17,26,4,23] by considering other possible bias types and how they manifest in ML models. In addition to providing insights on un-derstudied possible biases in retinal imaging, the use of this database allows us to consider what disparities remain when standardisation has been conducted, as the UKBB has undergone rigorous data acquisition and quality control pro-tocols [2], such that all images were taken with the same type of OCT scanner [1]. Also, the breadth of data available in the UKBB allows us to specifically characterise different biases (including some which are rarely investigated)."}, {"title": "Contributions", "content": "We train a retinal image hypertension classification model on images from more than 75,000 individuals and use this as a proxy task to un-derstand possible biases. We find that our model has uneven performance across subgroups, including between images from different assessment centres. We ex-plore possible reasons for these disparities among common factors such as data imbalance, image quality, unequal generalisation, and separations in the model's representations of different subgroups, and find that these do not necessarily hold true depending on the disparity. Finally, we find that no bias mitigation method manages to consistently improve the fairness of our model. This highlights the non-universality of existing bias mitigation methods and underscores the need for a framework to specifically characterise disparities and their causes, as well as to determine if and how to best minimise them."}, {"title": "Methods and Experimental Setup", "content": "Dataset and Pre-Processing We use 80,966 fundus retinal images from the right eye of 78,346 individuals in the UKBB. We exclude 1,874 images corre-sponding to participants who had subsequently withdrawn, who had \"other\", \"preferred not to say\", or \"unknown\" ethnicity, and those from one assessment centre which had fewer than 0.2% of images. The UKBB is particularly rich in available metadata, including age, body mass index (BMI), self-reported alcohol consumption, self-reported ethnicity, genetic ethnicity (gen_ethnicity), genetic sex, deprivation, medication, etc. We create categorical groupings for age (40-50, 50-60, 60-70, 70+), BMI (0-3 based on quartile), deprivation index (0-3 based on quartile), and self-reported ethnicity (White, mixed background, Asian back-ground, or Black African background) to facilitate downstream analyses. We anonymise the names of the centres.\nWe also adjust diastolic and systolic blood pressure (BP) by +10 and +15 mm Hg, respectively, if individuals are taking hypertensive medication [28]. We classify individuals as having high blood pressure (hypertension) if: diastolic BP > 80 or systolic BP > 130 or if they are taking anti-hypertensive medication (according to the current guidelines [30]). This is the binary target variable our model aims to predict. Figure Al shows some of the dataset characteristics.\nModel Architecture and Training We split data into train, validation, and test sets (0.8, 0.1, 0.1) stratifying by individuals. As in [18], we train an Incep-tionV3 Network ([27]) to classify a retinal image as belonging to a hypertensive or non-hypertensive individual."}, {"title": "Bias Mitigation Models", "content": "We adapt implementations of existing bias mitiga-tion methods from the github repository MEDFAIR, using the same backbone and core parameters as in Table 1. We select methods which encompass dif-ferent types of bias mitigation approaches and which had good results in the MEDFAIR benchmark [33] and try to mitigate age-, assessment-centre-, and sex-related disparities.\nWe test Resampling of minority subgroups as a pre-processing method [12]. In addition, we explore a range of in-processing methods including Group Distributionnally Robust Optimisation (GroupDRO) which minimises worst-group loss [20,16], Orthogonally Disentangled Representations (ODR), which disentangles the representations of subgroup-related features and task-relevant features [22], Domain-Independent learning (DomainInd) where each subgroup has its own final classification layer [29], and Learning-Not-to-Learn (LNL), an adversarial learning method [15]. We also implement Stochastic Weight Averaging Densely (SWAD) [7] which is a general ro-bustness method (and therefore does not require subgroup information) and pair it with resampling (ReSWAD). Finally, we implement a post-processing method (not in MEDFAIR), Recalibration, where a different decision thresh-old is calculated for each subgroup. We train all models three times with different random seeds on NVIDIA A100 GPU's."}, {"title": "Model Evaluation", "content": "Model evaluation is based upon the mean Receiver Op-erating Characteristic Area Under the Curve (AUC), accuracy, precision, and recall scores for the three runs of each model. We consider overall performance and performance across different subgroups (both minimum performance and best- and worst- performance gap)."}, {"title": "Results and Discussion", "content": "Performance and Disparities of the Baseline Model The baseline Incep-tionV3 model achieves 73\u00b10.01% accuracy and 71\u00b10.00% AUC in hypertension classification, with precision and recall values of 81\u00b10.04% and 83\u00b10.01%, re-spectively. However, a more granular assessment reveals significant disparities across certain subgroups (Table A1). For instance, as shown in Figure 1, the model's AUC varies by over 15% between different age groups and 10% between centres, with the worst-group AUC being substantially lower than the average AUC of 0.71. Some subgroups also exhibit substantial differences in recall (which would translate to underdiagnosis) of 10 to 32%, including different age groups, assessment centres, alcohol consumers, and ethnic groups.\nOrigins of Performance Disparities Next, we aim to understand why these disparities appear. We investigate whether they can be attributed to varying underlying characteristics across subgroups, such as differences in age or sex distribution. However, regardless of the attribute we condition on, the worst-performing assessment centre, centre f, shows much lower AUC (results on age conditioning are shown in Table A2). Such trends are also preserved for sex-and age-related disparities. Additionally, we use the Automorph pipeline [31] to assess the quality of all images and use this as a conditioning variable. We find that image quality does not explain these disparities either. We also consider shifts in prevalence, as correlation between an attribute and the target label can cause bias [13]. This is evident in age- and sex-related disparities, where hypertension shows a strong positive correlation with age (Figure A1), and men have a higher prevalence of hypertension. However, this does not explain centre disparities, as the worst-performing subgroup has approximately 76% images with hypertension, which falls within the range of other centres (69%-80%).\nFurther, these differences cannot simply be attributed to data imbalance. For centre and sex-related disparities, all groups are evenly represented. However, for age-related disparities, data imbalance may play a role. The oldest age group, which has the lowest AUC, is also underrepresented, comprising only 2.5% of the images.\nAnother emerging hypothesis in fair ML research is that disparities arise due to unequal model generalisation across subgroups. Despite uniform and strong performance on training data, generalisation differences on unseen data can emerge [11,20]. As shown in Table 2, there is a noticeable decrease in worst-group AUC relative to the decrease in overall AUC between training data and test data for different centres. Similarly, the gap between centres increases on unseen data, suggesting that the model's generalisation varies across these cen-tres. The difference is not as striking for age and sex subgroups, and most likely simply linked to overall performance decrease on unseen data. We further inves-tigate whether there is a shift in generalisation during training; a point where the model starts overfitting to certain subgroups but not others (and thus increasing the gap between subgroups) as identified in [11]. However our analyses do not reveal any evidence of a specific point where this could occur (Figures A2)."}, {"title": "Overall Performance of Mitigation Models", "content": "We then train a number of bias mitigation methods with the objective of reducing the most significant dis-parities: age, assessment centre, and sex. Initially, we assess how these methods impact overall model performance across all samples, examining whether \"lev-elling down\" occurs [32]. Regarding age mitigation, SWAD is the only method capable of maintaining overall AUC, whereas all other mitigation methods result in a decrease in AUC, particularly gDRO (Figure 3). Interestingly, this decrease in AUC is less pronounced in the assessment centre mitigation models. Only LNL and ODR show a notable decrease in AUC and precision, whereas the other models show similar overall performance across all four metrics (Figure A3). Sex disparity mitigation has a more variable effect (see Figure A4)."}, {"title": "Disparity Reduction", "content": "Overall, no methods achieve their intended effects of reducing disparities and boosting worst group performance. For age-related dis-parities, DomainInd is the only model which shows some effectiveness; it de-creases accuracy, AUC, and recall gap relative to baseline while also increasing worst-group performance (Table 3). However, it also causes a slight reduction in overall performance (Figure A3). SWAD performance is generally similar to baseline performance, but other models decrease min AUC and min precision. For centre-related disparities, the effectiveness of the models in improving fairness is very limited, especially in boosting worst-group performance. SWAD is the only method which maintains or slightly improves upon baseline dispari-ties (Table 3). Other methods have negative effects on at least one of the met-rics. For instance, resampling increases accuracy gap, ODR lowers min AUC by 0.02, and recalibration lowers min recall by 0.02. We also note that the opti-mal per-subgroup decision thresholds (for recalibration) range from 0.50 to 0.73, suggesting the baseline model does not uniformly adapt to the characteristics of different subgroups."}, {"title": "Conclusions", "content": "Our model trained with retinal images from the UKBB shows notably poor per-formance on certain subgroups of the population. In particular, although some level of age- or sex-related disparities could be expected due to differences in biological manifestation or prevalence of hypertension, centre disparities (which cannot be explained by any of the investigated confounders), are unexpected given the standardisation of the UKBB. These disparities would lead to un-fair outcomes if such a model was deployed. This highlights the importance of systematically conducting a granular assessment of a model's performance.\nMoreover, existing methods largely fail to mitigate these disparities. Most methods, particularly for age disparity mitigation, have a detrimental effect on overall performance. Even worse, few really improve fairness, and while some may show marginal improvement in one scenario, they adversely impact others. For instance, the DomainInd model slightly improves age- and sex-related dis-parities but does not show improvements in assessment-centre disparities. No method is actually able to boost performance for assessment centre f, suggest-ing that further methodological advancements are necessary, or that perhaps a maximum performance has already been reached rendering mitigation efforts ineffective. These observations highlight how applying bias mitigation methods indiscriminately may actually worsen overall outcomes and exacerbate existing disparities, concordant with recent findings in MEDFAIR [33]. Overall, it ap-pears important to precisely characterise biases and their underlying causes, as this understanding is crucial for informing appropriate mitigation strategies.\nFuture work should continue to develop a framework to better characterise disparities, for example building off previous work done in [13,14]. We consider a very narrow scenario of hypertension prediction from retinal images, but it would be interesting to see how these findings extend to other retinal image tasks and other image modalites. It would also be of interest to conduct a more in-depth exploration of the UKBB dataset specifically, in order to understand the interplay between selection bias, dataset standardisation, and subsequent model biases, and shed light on why some assessment centres showed such disparate performance. Investigations of this kind are increasingly important given the rise in large databases and initiatives like the UKBB, and the need to ensure downstream findings stay as unbiased as possible."}, {"title": "Appendix", "content": ""}]}