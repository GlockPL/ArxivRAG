{"title": "On Biases in a UK Biobank-based Retinal Image\nClassification Model", "authors": ["Anissa Alloula", "Rima Mustafa", "Daniel R McGowan", "Bart\u0142omiej W. Papie\u017c"], "abstract": "Recent work has uncovered alarming disparities in the per-\nformance of machine learning models in healthcare. In this study, we\nexplore whether such disparities are present in the UK Biobank fundus\nretinal images by training and evaluating a disease classification model\non these images. We assess possible disparities across various population\ngroups and find substantial differences despite strong overall performance\nof the model. In particular, we discover unfair performance for certain\nassessment centres, which is surprising given the rigorous data standard-\nisation protocol. We compare how these differences emerge and apply\na range of existing bias mitigation methods to each one. A key insight\nis that each disparity has unique properties and responds differently to\nthe mitigation methods. We also find that these methods are largely un-\nable to enhance fairness, highlighting the need for better bias mitigation\nmethods tailored to the specific type of bias.", "sections": [{"title": "1 Introduction and Related Work", "content": "Biases and Disparities in Machine Learning. An emerging concern in ma-\nchine learning (ML) research is that strong overall performance may obscure\ncritical disparities, leading to substantially inferior outcomes for certain sub-\ngroups. Examples of this unequal performance have been identified in clinical\nML models, across a range of tasks and modalities such as skin lesion classifica-\ntion [3], brain Magnetic Resonance Imaging (MRI) reconstruction [10], cardiac\nMRI segmentation [19], and affecting various subgroups, from certain ethnic\ngroups [19,16], to disadvantaged socioeconomic groups [24]. Not only do these\nbiases harm the minority groups who are subject to them, but they also hinder\nthe generalisability of the models to unseen population samples [21], constituting\na major barrier to the implementation of ML models in clinical settings."}, {"title": "2", "content": "Existing Approaches to Address such Biases A line of research focused\non preventing such disparities has consequently emerged. Bias mitigation can be\nconducted at various stages in the ML pipeline: during data collection, in the\npre-processing stage, while the model is training, and/or in post-processing. Ob-\njectives vary between methods and can include boosting minimum performance\n[9], reducing gaps in performance (equalised odds [10]), or equalising the number\nof positive predictions across groups (demographic parity [6]). However, recent\nwork has highlighted that despite the multitude of existing methods, the prob-\nlem is far from solved. A benchmark from 2023, MEDFAIR, showed that across\na range of medical tasks, no method consistently and significantly outperformed\nempirical risk minimisation (where there is no fairness objective) [33].\nProblem Setting In this study, we focus on the appearance of biases and their\nmitigation in retinal imaging-based models. Bias mitigation research has been\nlacking in this field, with, to the best of our knowledge, only two examples: work\nby Burlina et al. [5] and work by Coyner et al. [8], who tried to mitigate race-\nrelated disparities with synthetic data and data pre-processing, respectively. We\nbuild on this work by conducting the largest and most comprehensive exploration\nof disparities and mitigation methods in retinal imaging to date. We use retinal\nimages from the UK Biobank (UKBB), an unparalleled medical database of over\nhalf a million UK adults [25]. We complement recent work which has identified\nselection bias in the UKBB [17,26,4,23] by considering other possible bias types\nand how they manifest in ML models. In addition to providing insights on un-\nderstudied possible biases in retinal imaging, the use of this database allows us\nto consider what disparities remain when standardisation has been conducted,\nas the UKBB has undergone rigorous data acquisition and quality control pro-\ntocols [2], such that all images were taken with the same type of OCT scanner\n[1]. Also, the breadth of data available in the UKBB allows us to specifically\ncharacterise different biases (including some which are rarely investigated).\nContributions We train a retinal image hypertension classification model on\nimages from more than 75,000 individuals and use this as a proxy task to un-\nderstand possible biases. We find that our model has uneven performance across\nsubgroups, including between images from different assessment centres. We ex-\nplore possible reasons for these disparities among common factors such as data\nimbalance, image quality, unequal generalisation, and separations in the model's\nrepresentations of different subgroups, and find that these do not necessarily hold\ntrue depending on the disparity. Finally, we find that no bias mitigation method\nmanages to consistently improve the fairness of our model. This highlights the\nnon-universality of existing bias mitigation methods and underscores the need\nfor a framework to specifically characterise disparities and their causes, as well\nas to determine if and how to best minimise them."}, {"title": "2 Methods and Experimental Setup", "content": "Dataset and Pre-Processing We use 80,966 fundus retinal images from the\nright eye of 78,346 individuals in the UKBB. We exclude 1,874 images corre-\nsponding to participants who had subsequently withdrawn, who had \"other\",\n\"preferred not to say\", or \"unknown\" ethnicity, and those from one assessment\ncentre which had fewer than 0.2% of images. The UKBB is particularly rich in\navailable metadata, including age, body mass index (BMI), self-reported alcohol\nconsumption, self-reported ethnicity, genetic ethnicity (gen_ethnicity), genetic\nsex, deprivation, medication, etc. We create categorical groupings for age (40-50,\n50-60, 60-70, 70+), BMI (0-3 based on quartile), deprivation index (0-3 based\non quartile), and self-reported ethnicity (White, mixed background, Asian back-\nground, or Black African background) to facilitate downstream analyses. We\nanonymise the names of the centres.\nWe also adjust diastolic and systolic blood pressure (BP) by +10 and +15 mm Hg,\nrespectively, if individuals are taking hypertensive medication [28]. We classify\nindividuals as having high blood pressure (hypertension) if: diastolic BP > 80 or\nsystolic BP > 130 or if they are taking anti-hypertensive medication (according\nto the current guidelines [30]). This is the binary target variable our model aims\nto predict.\nModel Architecture and Training We split data into train, validation, and\ntest sets (0.8, 0.1, 0.1) stratifying by individuals. As in [18], we train an Incep- \ntionV3 Network ([27]) to classify a retinal image as belonging to a hypertensive\nor non-hypertensive individual.\nBias Mitigation Models We adapt implementations of existing bias mitiga-\ntion methods from the github repository MEDFAIR, using the same backbone"}, {"title": "4", "content": "and core parameters as in Table 1. We select methods which encompass dif-\nferent types of bias mitigation approaches and which had good results in the\nMEDFAIR benchmark [33] and try to mitigate age-, assessment-centre-, and\nsex-related disparities.\nWe test Resampling of minority subgroups as a pre-processing method\n[12]. In addition, we explore a range of in-processing methods including Group\nDistributionnally Robust Optimisation (GroupDRO) which minimises\nworst-group loss [20,16], Orthogonally Disentangled Representations (ODR),\nwhich disentangles the representations of subgroup-related features and task-\nrelevant features [22], Domain-Independent learning (DomainInd) where\neach subgroup has its own final classification layer [29], and Learning-Not-\nto-Learn (LNL), an adversarial learning method [15]. We also implement\nStochastic Weight Averaging Densely (SWAD) [7] which is a general ro-\nbustness method (and therefore does not require subgroup information) and\npair it with resampling (ReSWAD). Finally, we implement a post-processing\nmethod (not in MEDFAIR), Recalibration, where a different decision thresh-\nold is calculated for each subgroup. We train all models three times with different\nrandom seeds on NVIDIA A100 GPU's.\nModel Evaluation Model evaluation is based upon the mean Receiver Op-\nerating Characteristic Area Under the Curve (AUC), accuracy, precision, and\nrecall scores for the three runs of each model. We consider overall performance\nand performance across different subgroups (both minimum performance and\nbest- and worst- performance gap)."}, {"title": "3 Results and Discussion", "content": "Performance and Disparities of the Baseline Model The baseline Incep-\ntionV3 model achieves 73\u00b10.01% accuracy and 71\u00b10.00% AUC in hypertension\nclassification, with precision and recall values of 81\u00b10.04% and 83\u00b10.01%, re-\nspectively. However, a more granular assessment reveals significant disparities\nacross certain subgroups (Table A1). For instance, as shown in Figure 1, the\nmodel's AUC varies by over 15% between different age groups and 10% between\ncentres, with the worst-group AUC being substantially lower than the average\nAUC of 0.71. Some subgroups also exhibit substantial differences in recall (which\nwould translate to underdiagnosis) of 10 to 32%, including different age groups,\nassessment centres, alcohol consumers, and ethnic groups.\nOrigins of Performance Disparities Next, we aim to understand why these\ndisparities appear. We investigate whether they can be attributed to varying\nunderlying characteristics across subgroups, such as differences in age or sex\ndistribution. However, regardless of the attribute we condition on, the worst-\nperforming assessment centre, centre f, shows much lower AUC (results on age"}, {"title": "5", "content": "conditioning are shown in Table A2). Such trends are also preserved for sex-\nand age-related disparities. Additionally, we use the Automorph pipeline [31] to\nassess the quality of all images and use this as a conditioning variable. We find\nthat image quality does not explain these disparities either. We also consider\nshifts in prevalence, as correlation between an attribute and the target label\ncan cause bias [13]. This is evident in age- and sex-related disparities, where\nhypertension shows a strong positive correlation with age (Figure A1), and men\nhave a higher prevalence of hypertension. However, this does not explain centre\ndisparities, as the worst-performing subgroup has approximately 76% images\nwith hypertension, which falls within the range of other centres (69%-80%).\nFurther, these differences cannot simply be attributed to data imbalance. For\ncentre and sex-related disparities, all groups are evenly represented. However, for\nage-related disparities, data imbalance may play a role. The oldest age group,\nwhich has the lowest AUC, is also underrepresented, comprising only 2.5% of\nthe images.\nAnother emerging hypothesis in fair ML research is that disparities arise due\nto unequal model generalisation across subgroups. Despite uniform and strong\nperformance on training data, generalisation differences on unseen data can\nemerge [11,20]. As shown in Table 2, there is a noticeable decrease in worst-\ngroup AUC relative to the decrease in overall AUC between training data and\ntest data for different centres. Similarly, the gap between centres increases on\nunseen data, suggesting that the model's generalisation varies across these cen-\ntres. The difference is not as striking for age and sex subgroups, and most likely\nsimply linked to overall performance decrease on unseen data. We further inves-\ntigate whether there is a shift in generalisation during training; a point where the\nmodel starts overfitting to certain subgroups but not others (and thus increasing\nthe gap between subgroups) as identified in [11]. However our analyses do not\nreveal any evidence of a specific point where this could occur"}, {"title": "6", "content": "Finally, we investigate whether the model's learnt representations can provide\ninsight on subgroup disparities. We analyse each image in the model's penulti-\nmate layer feature space through a 4-component principal component analysis\n(which explains over 85% of the variance). As expected, we find strong separa-\ntion between the projected features of images with and without hypertension,\nand consequently between images of different age groups due to their strong\ncorrelation. However, we also observe an unexpected outlier from the distribu-\ntion of images from the worst-performing centre (f). There is a clear difference\nin the kernel density estimates of some principal components from this centre\nand a consistently increased Wasserstein distance separating the distribution of\nfeatures from centre f to the other centres (Figure 2). Although this does not\nprove this information is being used for predictions, it is noteworthy that such a\nshift exists, one that cannot be explained by any of the other available variables."}, {"title": "7", "content": "Overall Performance of Mitigation Models We then train a number of\nbias mitigation methods with the objective of reducing the most significant dis-\nparities: age, assessment centre, and sex. Initially, we assess how these methods\nimpact overall model performance across all samples, examining whether \"lev-\neling down\" occurs [32]. Regarding age mitigation, SWAD is the only method\ncapable of maintaining overall AUC, whereas all other mitigation methods result\nin a decrease in AUC, particularly gDRO (Figure 3). Interestingly, this decrease\nin AUC is less pronounced in the assessment centre mitigation models. Only\nLNL and ODR show a notable decrease in AUC and precision, whereas the\nother models show similar overall performance across all four metrics (Figure\nA3). Sex disparity mitigation has a more variable effect (see Figure A4).\nDisparity Reduction Overall, no methods achieve their intended effects of\nreducing disparities and boosting worst group performance. For age-related dis-\nparities, DomainInd is the only model which shows some effectiveness; it de-\ncreases accuracy, AUC, and recall gap relative to baseline while also increasing\nworst-group performance. However, it also causes a slight reduction\nin overall performance (Figure A3). SWAD performance is generally similar to\nbaseline performance, but other models decrease min AUC and min precision.\nFor centre-related disparities, the effectiveness of the models in improving\nfairness is very limited, especially in boosting worst-group performance. SWAD\nis the only method which maintains or slightly improves upon baseline dispari-\nties. Other methods have negative effects on at least one of the met-\nrics. For instance, resampling increases accuracy gap, ODR lowers min AUC by\n0.02, and recalibration lowers min recall by 0.02. We also note that the opti-\nmal per-subgroup decision thresholds (for recalibration) range from 0.50 to 0.73,"}, {"title": "8", "content": "suggesting the baseline model does not uniformly adapt to the characteristics of\ndifferent subgroups."}, {"title": "4 Conclusions", "content": "Our model trained with retinal images from the UKBB shows notably poor per-\nformance on certain subgroups of the population. In particular, although some\nlevel of age- or sex-related disparities could be expected due to differences in\nbiological manifestation or prevalence of hypertension, centre disparities (which\ncannot be explained by any of the investigated confounders), are unexpected\ngiven the standardisation of the UKBB. These disparities would lead to un-\nfair outcomes if such a model was deployed. This highlights the importance of\nsystematically conducting a granular assessment of a model's performance.\nMoreover, existing methods largely fail to mitigate these disparities. Most\nmethods, particularly for age disparity mitigation, have a detrimental effect on\noverall performance. Even worse, few really improve fairness, and while some\nmay show marginal improvement in one scenario, they adversely impact others.\nFor instance, the DomainInd model slightly improves age- and sex-related dis-\nparities but does not show improvements in assessment-centre disparities. No\nmethod is actually able to boost performance for assessment centre f, suggest-\ning that further methodological advancements are necessary, or that perhaps\na maximum performance has already been reached rendering mitigation efforts\nineffective. These observations highlight how applying bias mitigation methods\nindiscriminately may actually worsen overall outcomes and exacerbate existing\ndisparities, concordant with recent findings in MEDFAIR [33]. Overall, it ap-"}, {"title": "9", "content": "pears important to precisely characterise biases and their underlying causes, as\nthis understanding is crucial for informing appropriate mitigation strategies.\nFuture work should continue to develop a framework to better characterise\ndisparities, for example building off previous work done in [13,14]. We consider\na very narrow scenario of hypertension prediction from retinal images, but it\nwould be interesting to see how these findings extend to other retinal image\ntasks and other image modalites. It would also be of interest to conduct a more\nin-depth exploration of the UKBB dataset specifically, in order to understand the\ninterplay between selection bias, dataset standardisation, and subsequent model\nbiases, and shed light on why some assessment centres showed such disparate\nperformance. Investigations of this kind are increasingly important given the\nrise in large databases and initiatives like the UKBB, and the need to ensure\ndownstream findings stay as unbiased as possible."}, {"title": "5 Appendix", "content": ""}]}