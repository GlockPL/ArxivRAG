{"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "authors": ["YU-AN LIU", "RUQING ZHANG", "JIAFENG GUO", "MAARTEN DE RIJKE", "YIXING FAN", "XUEQI CHENG"], "abstract": "Recent advances in neural information retrieval (IR) models have significantly enhanced their effectiveness over various IR tasks. The robustness of these models, essential for ensuring their reliability in practice, has also garnered significant attention. With a wide array of research on robust IR being proposed, we believe it is the opportune moment to consolidate the current status, glean insights from existing methodologies, and lay the groundwork for future development. We view the robustness of IR to be a multifaceted concept, emphasizing its necessity against adversarial attacks, out-of-distribution (OOD) scenarios and performance variance. With a focus on adversarial and OOD robustness, we dissect robustness solutions for dense retrieval models (DRMs) and neural ranking models (NRMs), respectively, recognizing them as pivotal components of the neural IR pipeline. We provide an in-depth discussion of existing methods, datasets, and evaluation metrics, shedding light on challenges and future directions in the era of large language models. To the best of our knowledge, this is the first comprehensive survey on the robustness of neural IR models, and we will also be giving our first tutorial presentation at SIGIR 2024. Along with the organization of existing work, we introduce a Benchmark for robust IR (BestIR), a heterogeneous evaluation benchmark for robust neural information retrieval, which is publicly available at https://github.com/Davion-Liu/BestIR. We hope that this study provides useful clues for future research on the robustness of IR models and helps to develop trustworthy search engines.", "sections": [{"title": "1 INTRODUCTION", "content": "Information retrieval (IR) plays a pivotal role in numerous real-world applications, such as web search, digital libraries and e-commerce search . According to the global overview report from Digital 2023, nearly 82% of Internet users between 18 and 64 have used a search engine or web portal in the past month. Specifically, IR is the process of finding and providing relevant information in response to the user query from a large collection of data. Recently, with advances in deep learning, neural IR models have witnessed significant progress . With the development of training methodologies such as pre-training and fine-tuning , neural IR models have demonstrated remarkable effectiveness in learning query-document relevance patterns.\nWhy is robustness important in IR? In real-world deployment of neural IR models, an aspect equally essential as their effectiveness is their robustness. A good IR system must not only exhibit high effectiveness under normal conditions but also demonstrate robustness in the face of abnormal conditions. The natural openness of IR systems makes them vulnerable to intrusion, and the consequences can be severe. For example: (i) Search engines are vulnerable to black hat SEO attacks, necessitating significant efforts to curb these infringements. (ii) In addition, search engines are confronted with large amounts of unseen data on a daily basis. The working algorithm needs to be updated frequently to ensure that search effectiveness is maintained.\nRecently, academics have begun to investigate the robustness of IR systems . As neural networks gain increasing popularity in IR, many studies have found that IR systems that are based on neural networks inherit a wide variety of robustness issues from deep neural network. The field of robust neural IR is garnering increasing attention, as evidenced by the growing number of papers published annually, as depicted in Figure 1. The robustness issues are differently represented in real IR scenarios and raise concerns about deploying neural IR systems into the real world. Therefore, the study of robust neural IR is crucial for building reliable IR systems.\nWhat is the definition of robustness in IR? In this paper, we begin by defining robustness in the context of IR. In IR, it is well-accepted that, user attention mainly focuses on the Top-K results and increases with higher rankings . Based on this, we argue that robustness in IR refers to the consistent performance and resilience on Top-K results of an IR system when faced with"}, {"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "content": "a variety of unexpected scenarios. Robustness is not a simple concept; it encompasses multiple dimensions, as highlighted by research within the machine learning (ML) community . Specifically, in the field of IR, we identify several facets of robustness: (i) Performance variance emphasizes the worst-case performance across different individual queries under the independent and identically distributed (IID) data assumption ; (ii) Out-of-distribution (OOD) robustness refers to the generalizability of an IR model on unseen queries and documents from different distributions of the training dataset ; and (iii) Adversarial robustness refers to the ability of the IR model to defend against malicious adversarial attacks aimed at manipulating rankings .\nIn this survey, our focus is on adversarial robustness and OOD robustness, which have garnered significant attention. (i) For adversarial robustness, studies primarily approach the topic from two angles, i.e., adversarial attack and defense, to enhance the robustness of IR models. (ii) For OOD robustness, the emphasis is on improving the generalizability of IR models to both unseen documents and unseen queries. Additionally, we zero in on two key components of the neural IR framework: first-stage retrieval and the subsequent ranking stage. More specifically, we focus on dense retrieval models (DRMs) and neural ranking models (NRMs) to further explore the aforementioned research perspectives.\nThe relation to other surveys. There are a number of surveys on robustness in the field of natural language processing (NLP)  and computer vision . However, the field of IR presents its own unique characteristics: (i) Unlike NLP, which often focuses on individual examples, IR involves ranking a collection of documents, highlighting the need for robustness across the entire list of rankings. (ii) Differing from continuous image data in CV, IR deals with robustness concerning discrete text documents. Consequently, the studies explored in these surveys are not directly transferrable as references within the IR field.\nSurveys specific to the IR domain tend to concentrate on areas like pre-training , ranking models , initial retrieval stages , and the explainability of IR systems . Yet, there's a noticeable gap in the literature: a dedicated survey that consolidates and introduces research pertaining to robustness in IR is markedly absent.\nOn top of the work discussed above, this survey adds the following: (i) Our work complements robustness surveys from other fields, taking into account the distinctive characteristics of different domains. (ii) Our survey complements IR surveys by not only prioritizing effectiveness but also emphasizing robustness. Our ultimate aim is to contribute to help advance reliable and efficient IR systems.\nContributions of this survey. We present a compilation of research in IR robustness, summarizing and organizing the content to aid understanding. By categorizing each subfield and delineating its core concepts, we aim to facilitate a deeper understanding of this field and promote the long-term development of robust neural IR. In summary, this paper's contributions are as follows:\n\u2022 A comprehensive overview and categorization: We define robustness in IR by summarizing existing literature and further dividing it into distinct categories.\n\u2022 A detailed discussion of methodologies and datasets: We offer a detailed discussion of methodologies, datasets, and evaluation metrics pertinent to each aspect of robustness. Moreover, we integrate the existing datasets mentioned in this survey and propose the BestIR benchmark to facilitate subsequent work.\n\u2022 Identification of open issues and future trends: We highlight challenges and potential future trends, particularly in the age of large language models (LLMs).\nOrganization. Figure 2 depicts the organization of our survey: In Section 2, we introduce the IR task, and give a definition and taxonomy of robustness in IR. In Section 3, we examine adversarial"}, {"title": "2 DEFINITION AND TAXONOMY", "content": "In this section, we provide a formal definition of robustness in the context of IR and outline the taxonomy pertinent to this domain.\nIR task. To provide a clear understanding, we first formalize the IR task. Suppose that \\(R = \\{r_1, r_2, ..., r_l\\}\\) is the set of relevance levels, where l denotes the number of levels. A total order exists among the relevance labels such that \\(r_l > r_{l-1} > \u2026 > r_1\\), where > denotes the order relation. The minimum value of the relevance label is 0, which usually implies no relevance. Suppose that \\(Q = \\{q_1, q_2, ..., q_m\\}\\) is the set of queries in the training dataset. Each query \\(q_i\\) is associated with a list of documents \\(D_i = \\{d_{i,1}, d_{i,2}, . . ., d_{i,N} \\}\\) and a list of relevance labels \\(Y_i = \\{y_{i,1}, y_{i,2}, ..., y_{i,N}\\}\\), where \\(y_{i,j} \\in R\\) denotes the label of document \\(d_{i,j}\\) and N is the document list size. Then we obtain the training dataset \\(D_{train} = \\{(q_i, D_i, Y_i)\\}_{i=1}^m\\).\nWe use f to denote the IR model, which predicts the relevance score \\(f (q, d)\\) based on a given query q and document d. The IR model f is derived by learning from the following objective\n\\[\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{(q,D,Y)\\sim D_{train}} \\mathcal{L} (f(q, d), Y),\\tag{1}\\]\nwhere \u03b8 are the parameters of the IR model f, and L is a ranking loss function."}, {"title": "2.1 Definition of Robustness in IR", "content": "Robustness refers to the ability to withstand disturbances or external factors that may cause a system to malfunction or provide inaccurate results . It is important for practical applications, especially in safety-critical scenarios, e.g., medical retrieval , financial retrieval , and private information retrieval . If, for any reason, the IR system behaves abnormally, the service provider can lose time, manpower, opportunities and even credibility. With the development of deep learning, robustness has received much attention in the fields of computer vision (CV) and natural language processing (NLP) . The concerns about model robustness in these fields are mainly focused on the test phase. In this scenario, the model is trained on an unperturbed dataset but tested for its performance when exposed to adversarial examples or OOD data .\nIn IR, the robustness of model in the test phase is also important due to the widespread availability of search engine optimization (SEO) and the need for models to adapt to unseen data. Hence, in this survey, we follow prior work and only discuss the robustness of a model in the test phase."}, {"title": "2.2 Taxonomy of Robustness in IR", "content": "In IR, robustness threats exist in a wide range of aspects, including adversarial robustness , OOD robustness  and performance variance . The framework of robustness in IR is shown in Figure 4.\n2.2.1 Performance variance. Typically, the performance of IR models is first represented by their overall performance on IID data. Recently, it has been recognized that performance stability across IID queries may be compromised when we try to improve the average retrieval effectiveness across all queries . Therefore, a robust neural IR model should not only have good retrieval performance on the overall testing queries, but also ensure that the performance on individual queries is not too bad. We give a formal definition of performance variance in IR based on Definition 2.1."}, {"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "content": "DEFINITION 2.2 (PERFORMANCE VARIANCE OF INFORMATION RETRIEVAL). Given an IR model \\(f_{D_{train}}\\) trained on training dataset \\(D_{train}\\) with a corresponding IID testing dataset \\(D_{test}\\), and an acceptable error threshold d, for the top-K ranking result, if\n\\[Var (\\{M (f_{D_{train}}; (q, D, Y), K) | (q, D, Y) \\in D_{test}\\}) \\leq \\delta,\\tag{6}\\]\nwhere \\(Var(\\cdot)\\) is the variance of the ranking performance of the IR model \\(f_{D_{train}}\\) on \\(D_{test}\\), then the model f is considered d-robust in terms of performance variance for metric M.\n2.2.2 Out-of-distribution robustness. IR models need to cope with a constant stream of unseen data . The key behind this challenge is how to adapt the model to new data out of the familiar distribution. There are a variety of OOD scenarios in the field of IR, so the OOD robustness of the model is likewise of wide interest. First, the query entered by the user may be unknown and of varying quality . Then, search engine application scenario migration and incremental new documents will likewise bring in OOD data . Manual labeling of unseen data as well as retraining IR models incurs significant resource overhead . Therefore, it is crucial how to efficiently train the IR model to have effective performance on unseen data.\nIn this context, studying the robustness of IR models on OOD data has received a lot of attention. OOD robustness measures the performance of an IR model on unseen queries and documents from different distributions of the training dataset. By introducing the test dataset \\(\\tilde{D}_{test}\\) in a new domain, we give a formal definition of OOD robustness in IR based on Definition 2.1.\nDEFINITION 2.3 (OUT-OF-DISTRIBUTION ROBUSTNESS OF INFORMATION RETRIEVAL). Given an IR model \\(f_{D_{train}}\\, an original dataset with training and test data, \\(D_{train}\\) and \\(D_{test}\\), drawn from the original distribution G, along with a new test dataset \\(\\tilde{D}_{test}\\) drawn from the new distribution \\(\\tilde{G}\\), and an acceptable error threshold d, for the top-K ranking result, if\n\\[|\\mathcal{RM} (f_{D_{train}}; D_{test}, K) - \\mathcal{RM} (f_{D_{train}}; \\tilde{D}_{test}, K)| \\leq \\delta \\text{ where } D_{train}, D_{test} \\sim G, \\tilde{D}_{test} \\sim \\tilde{G},\\tag{7}\\]\nthe model f is considered d-robust against out-of-distribution data for metric M.\n2.2.3 Adversarial robustness. In a competitive scenario, content providers may aim to promote their products or documents in rankings for specific queries . This has provided a market for search"}, {"title": "3 ADVERSARIAL ROBUSTNESS", "content": "The deep neural networks have been found vulnerable to adversarial examples that can produce misdirection with human-imperceptible perturbations . In the field of IR, deep learning-based models are also likely to inherit these adversarial vulnerabilities , which raises widespread concerns about the robustness of neural IR systems. Recall that we give the definition of adversarial robustness in IR elaborated in Section 2.2. Therefore, in this section, we present the specific work of adversarial attack and adversarial defense in adversarial robustness, respectively."}, {"title": "3.1 Overview", "content": "In IR, search engine optimization (SEO) has been around since the dawn of the world wide web . This includes white-hat SEO , which modifies documents in good faith and within the rules and expectations of search engines to optimize the quality of web pages. In contrast, black-hat SEO , maliciously exploiting loopholes of search engines, is used to get a site ranking higher in search results. Black-hat SEO creates a poor experience for the audience and is a common concern among site owners.\nTraditional web spamming. SEO involves the creation of web pages with no real informational value, designed to manipulate search engine algorithms and distort search results. This deceptive practice lures web users to sites they would not typically visit, undermining the trust relationship between users and search engines and potentially damaging the search engines' reputation.\nweb spamming attack. When assessing textual relevance, search engines scrutinize the locations on a web page where query terms appear . These locations are referred to as fields. Common"}, {"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "content": "text fields for a page include the document body, the title, the meta tags in the HTML header, and the page's URL. Additionally, the anchor texts associated with URLs that point to the page are considered part of the page (anchor text field), as they often provide a succinct description of the page's content. The terms in these text fields play a crucial role in determining the page's relevance to a specific query, with different weights typically assigned to different fields. Term spamming refers to the manipulation of these text fields to make spam pages appear relevant for certain queries .\nweb spamming detection. web spamming posed a serious threat to search engines in the early days. However, they are also easy to spot and suspect due to their simple implementation . web spamming detection involves identifying and mitigating the practice of term spamming, where keywords are excessively used or manipulated on a web page to artificially inflate its relevance to specific search queries . Various techniques have been developed to detect the term spamming. There is a utility-based term spamicity detection method known as OSD (Online Spam Detection)  that has been used to identify adversarial examples. OSD mainly relies on TF-IDF features and has been validated by the Microsoft adCenter . It introduces the notion of spamicity to measure how likely a page is spam. Spamicity is a more flexible and user-controllable measure than the traditional supervised classification methods. Using spamicity, online link spam, and term spam can be efficiently detected. However, this detection method can only target changes that add query terms and is prone to misclassification.\nWhy study adversarial attacks in IR? With the development of deep learning, neural networks are beginning to be widely used in IR models and have achieved excellent performance. Although traditional web spamming can also have a significant attack effect on neural IR models, this method is difficult to pose a threat due to its ease of detection. However, in the context of black-hat SEO, neural IR models are at risk of being attacked due to the inherent vulnerability inherited from neural networks. Therefore, adversarial attacks are beginning to be studied to expose vulnerability flaws in neural IR models in advance.\nWhy study adversarial defense in IR? To mitigate adversarial attacks, there is a growing body of work that is focusing on adversarial defenses. Adversarial defense focuses on the early hardening of model vulnerabilities discovered by adversarial attacks. Its goal is to obtain robust neural retrieval models to build reliable IR systems.\nThe relationship between adversarial attacks and defenses is shown in Figure 5. Recently, adversarial robustness has begun to gain widespread attention. Below, we describe these efforts from several perspectives: benchmark datasets, adversarial attacks, and adversarial defense."}, {"title": "3.3 Adversarial Attack", "content": "As neural networks have become increasingly prevalent in IR systems, they have also become a target for adversarial attacks. Studying adversarial attacks can help understand the vulnerability of neural IR models before deploying them in real-world applications, and it can also be used as a surrogate evaluation and support the development of appropriate countermeasures.\n3.3.1 What are the differences between IR attacks and CV/NLP attacks? Adversarial attacks have undergone significant development in the fields of NLP and CV , where the most emblematic forms of attack are typically directed at image retrieval and text classification tasks. However, the landscape of adversarial attacks assumes a different contour in the realm of IR: (i) Compared with image retrieval attacks, the IR attacks need to maintain semantic consistency of the perturbed document with the original document by considering the textual semantic similarity, rather than pixel-level perturbations within a fixed range in continuous space; and (ii) Inspired by black-hat SEO, the goal of IR attacks is to inject imperceptible perturbations within a document to improve its ranking for one or multiple specific queries within the entire candidate set or corpus, not inducing classification errors by the model.\nWithout loss of generality, given a query q and a target document d, the goal of generating imperceptible perturbations p to attack against a neural IR model f under top-K ranked results can"}, {"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "content": "usually be formalized as:\n\\[\\max_p (K - \\pi_f (q, d \\oplus p) + \\lambda \\cdot Sim (d, d \\oplus p)),\\tag{9}\\]\nwhere \\(\\pi_f (q, d \\oplus p)\\) denotes the ranking position of the perturbed document \\(d \\oplus p\\) in the ranked list generated by f with respect to query q. The Sim (\u00b7) function measures the similarity between the adversarial example and the original document, both textually as well as semantically. A is a regularization parameter used to balance two goals: keeping the adversarial samples as close as possible to the original document, while allowing the adversarial samples to be ranked as high as possible. Ideally, the adversarial sample \\(d \\oplus p\\) should preserve the original semantics of d, and be imperceptible to human judges yet misleading to the neural IR models.\n3.3.2 What is the attack setting? Depending on whether the attacker has access to the knowledge of the parameters of the target model, the attack setup can be categorized into two main types : (i) White-box attacks: Here, the attacker can fully access the model parameters and use the model gradient to directly generate perturbations. (ii) Black-box attacks: Here, the model parameters cannot be obtained; the attacker usually adopts a transfer-based black-box attack paradigm : They construct a surrogate white-box model by continuously querying the model and getting the output. Specifically, a surrogate model is trained to simulate the performance of the target model, and then the surrogate model is attacked to the transferability of the adversarial samples to indirectly attack the target model. In IR, attackers can query the target model to obtain a ranked list with rich information. Therefore, the surrogate model often has access to sufficient training data, making this attack effective . There are various attack methods and target models in IR. We present basic pseudo code to illustrate a fundamental IR attack, in Algorithm 1.\nAccording to the type of target model, attack efforts against IR models can be broadly categorized into two types: adversarial retrieval attacks and adversarial ranking attacks. A schematic diagram of the two types of attack is shown in Figure 6. (i) Adversarial retrieval attacks target the first-stage retrieval, mainly against dense retrieval models; and (ii) Adversarial ranking attacks target the re-ranking stage, mainly against neural ranking models. The methodology for conducting adversarial attacks in IR can differ based on the chosen attack strategy and the target model. A categorization of adversarial attacks in IR is shown in Figure 7.\nIn the following, we introduce evaluation, adversarial retrieval attacks, and adversarial ranking attacks of adversarial attacks in IR."}, {"title": "3.3.3 Evaluation", "content": "Evaluation of adversarial attacks includes both attack performance and natural-ness performance.\nAttack performance. Attack performance mainly refers to the degree of ranking improvement after a target document has been attacked. In general, automatic metrics for attack performance are widely adopted as follows:\n\u2022 Attack success rate (ASR/SR) , which evaluates the percentage of target docu-ments successfully boosted under the corresponding target query.\n\u2022 Average boosted ranks (Boost/Avg.boost) , which evaluates the average improvedrankings for each target document under the corresponding target query.\n\u2022 Boosted top-K rate (TKR) , which evaluates the percentage of target documentsthat are boosted into top-K w.r.t. the corresponding target query.\n\u2022 Normalized ranking shifts rate (NRS) evaluates the relative ranking improvementof adversarial examples which are successfully recalled into the initial set with K candidates:\n\\[NRS = (\\Pi_d - \\Pi_{d^{adv}})/\\Pi_d \\times 100\\%,\\tag{10}\\]\nwhere \\(\\Pi_d\\) and \\(\\Pi_{d^{adv}}\\) are the rankings of d and \\(d^{adv}\\) respectively, produced by the target IR model.\nThe effectiveness of an adversary is better with a higher value for all these metrics.\nNaturalness performance. Naturalness performance refers primarily to the degree to which a target document is imperceptible to humans after it has been attacked. In general, automatic metrics for naturalness performance and human evaluation are widely adopted as follows:\n\u2022 Automatic spamicity detection, which can detect whether target pages are spam or not.The utility-based term spamicity method OSD  is usually used to detect the adversarial examples."}, {"title": "Automatic grammar checkers", "content": "i.e., Grammarly\u00b3 and Chegg Writing4, which calculate the average number of errors in the adversarial examples.\n\u2022 Language model perplexity (PPL), which measures the fluency using the average perplexity calculated using a pre-trained GPT-2 model .\n\u2022 Human evaluation, which measures the quality of the attacked documents w.r.t. aspects of imperceptibility, fluency, and semantic similarity ."}, {"title": "3.3.4 Adversarial retrieval attack", "content": "In this subsection, we introduce the task definition of adversarial retrieval attacks and methods to achieve such attacks.\nTask definition. The adversarial retrieval attack task is designed to attack against DRMs. The objective of adversarial retrieval attacks is centered around the manipulation of a document that initially fails to be recalled. By integrating adversarial perturbations, the aim is to ensure that this document is subsequently retrieved by the first-stage neural retrieval model, thereby securing its presence within the candidate set. This approach not only challenges the robustness and reliability of neural retrieval systems but also raises significant questions regarding the integrity of information authenticity . The goal of adversarial retrieval attacks under top-K ranked results can be formalized as:\n\\[\\max_p (K - Recall_f (q, d \\oplus p) + \\lambda \\cdot Sim (d, d \\oplus p)),\\tag{11}\\]\nwhere Recallf (q, d \u2295 p) denote the recalled position of the perturbed document d \u2295 p generated by the frist stage retrieval model f with respect to query q given the entire corpus. A smaller value of Recall denotes a higher ranking.\nMethod. Current retrieval attack methods mainly include corpus poison attacks, backdoor attacks, and encoding attacks.\n\u2022 Corpus poison attack. Corpus poisoning attacks construct adversarial samples against a specific query and inject them into the corpus in the inference phase so that they are recalled. MCARA  addresses this issue and attempts to mine the vulnerability of the DRM. It introduces the adversarial retrieval attack (AREA) task, which intends to deceive the DRM into retrieving a target document that is outside the initial set of candidate documents. Zhong et al.  adopt the HotFlip method  from NLP to iteratively add perturbations in the discrete token space to maximize its similarity to a set of queries. In this way, they maximized the contamination of the corpus by a limited number of documents. MAWSEO  employs adversarial revisions to achieve real-world cybercriminal objectives, including rank boosting, vandalism detection evasion, topic relevancy, semantic consistency, user awareness (but not alarming) of promotional content, etc.\n\u2022 Backdoor attack. Backdoor attacks inject a small proportion of ungrammatical documents into the corpus. When user queries contain grammatical errors, the model will recall the learned triggering pattern and assign high relevance scores to those documents. Long et al.  introduces a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisements, through a retrieval system. To achieve this, they propose a perilous backdoor attack triggered by grammatical errors and ensure that attack models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries."}, {"title": "Encoding attack", "content": "By imperceptibly perturbing documents using uncommon encoded repre-sentations, encoding attacks control results across search engines for specific search queries. Boucher et al.  make words look the same as they originally do by adding an offset encoding to them, while the search engines are deceived. The experiment on a mirror of Simple Wikipedia shows that the proposed method can successfully deceive search engines in realistic scenarios."}, {"title": "3.3.5 Adversarial ranking attack", "content": "In this subsection, we introduce the task definition of adversarial ranking attacks and existing methods to achieve adversarial ranking attacks.\nTask definition. The adversarial ranking attack task is designed to attack against NRMs. The aim of adversarial ranking attacks typically involves introducing adversarial perturbations to a document already present in the candidate set, to manipulate its ranking position either elevating or diminishing it as determined by a neural ranking model. The goal of adversarial ranking attacks under top-K ranked results can usually be formalized as:\n\\[\\max_p (K - Recall_f (q, d \\oplus p) + \\lambda \\cdot Sim (d, d \\oplus p)),\\tag{12}\\]\nwhere Rankf (q, d \u2295 p) denotes the ranking position of the perturbed document d \u2295 p in the final ranked list generated by the NRM f with respect to query q. A smaller value of Rank denotes a higher ranking.\nMethod. Adversarial ranking attacks against NRMs include word substitution attacks, trigger attacks, and prompt attacks.\n\u2022 Word substitution attack. Word substitution attacks typically boost a document's ranking by replacing a small number of words in the document with synonyms. One common method of white-box word substitution attack is the gradient-based attack, where the attacker uses the gradient of the loss function with respect to the input data to create adversarial examples. These examples are designed to cause the model to make incorrect relevance predictions or rankings. Raval and Verma  present a systematic approach of using adversarial examples to measure the robustness of popular ranking models. They follow a similar approach used in text classification tasks  and perturb a limited number of tokens (with a minimum of one) in documents, replacing them with semantically similar tokens such that the rank of the document changes. Brittle-BERT  adds/replaces a small number of tokens to a highly relevant or non-relevant document to cause a large rank demotion or promotion. Authors find a small set of recurring adversarial words that when added to documents result in successful rank demotion/promotion of any relevant/non-relevant document respectively. As for black-box word substitution attacks, in the field of machine learning, Szegedy et al.  find that adversarial examples have the property of cross-model transferability, i.e., the adversarial example generated by a surrogate model can also fool a target model. Black-box attacks in information retrieval usually adopt this transfer-based paradigm due to the excellent performance of imitation of the target model. Wu et al.  propose the first black-box adversarial attack task against NRM, the word substitution ranking attack (WSRA) task. The WSRA task aims to fool NRMs into promoting a target document in rankings by replacing important words in its text with synonyms in a semantic-preserving way. Based on this task, authors propose a novel pseudo relevance-based adversarial ranking attack method (PRADA), which outperformed web spamming methods by 3.9% in ASR. The WSRA task focuses only on attacks on single query-document pairs and does not take into account the dynamic nature of search engines. Based on this, Liu et al.  introduce the topic-oriented"}, {"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "content": "adversarial ranking attack (TARA) task, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic.\n\u2022 Trigger attack. Trigger attacks boost document rankings by injecting a generated trigger sentence into a specific location in the document (e.g., the beginning). Song et al.  propose using semantically irrelevant sentences (semantic collisions) as perturbations. They develop gradient-based approaches for generating collisions given white-box access to an NRM. Goren et al.  propose a document manipulation strategy to improve document quality for the purpose of improving document ranking. Liu et al.  propose a trigger attack method, PAT, empowered by a pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. TRAttack  used rewriting existing sentences in the text to improve document ranking with learning ability from the multi-armed bandit mechanism.\n\u2022 Prompt attack. Prompt attacks use prompts to guide a language model to generate pertur-bations based on existing documents to improve document ranking. Chen et al.  propose a framework called imperceptible document manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM finds the optimal connect sentence to insert into the document through a language model. Parry et al.  analyze the injection of query-independent prompts, such as \u201ctrue\u201d into documents and found that the prompt perturbation method is valid for several sequence-to-sequence relevance models like monoT5 .\n\u2022 Multi-granular attack. Multi-granular attacks focus on generating high-quality adversarial examples by incorporating multi-granular perturbations, i.e., word level, phase level, and sentence level. Liu et al.  propose RL-MARA, a reinforcement learning framework to navigate an appropriate sequential multi-granular ranking attack path. By incorporating word-level, phrase-level, and sentence-level perturbations to generate imperceptible adversarial examples, RL-MARA could increase the flexibility of creating adversarial examples, thereby improving the potential threat of the attack."}, {"title": "3.4 Adversarial Defenses", "content": "With the advent of SEO, many defenses were born to counter malicious attacks. In the field of adversarial defenses, much work has been devoted to training robust neural IR models or identifying malicious adversarial examples in advance.\n3.4.1 IR defense task. The primary objective of defenses in IR is to maintain, or even enhance, the performance of IR models when the test dataset includes adversarial examples. This involves the implementation of strategies during the training or inference phases: the goal is to ensure that the model's ability to accurately retrieve relevant documents remains uncompromised, even in the presence of manipulative adversarial perturbations.\nWithout loss of generality, given a test set Dtest and an adversarial document set Dadv, the goal of adversarial defense against an neural IR model f under top-K ranked results can usually be formalized as:\n\\[\\max \\mathcal{RM} (f_{D_{train}}; D_{test}, K) \\text{ such that } D_{test} \\leftarrow D_{test} \\cup D_{adv}.\\tag{13}\\]\nThe adversarial defense task would be in the training or testing phase. In the testing phase, it is usually in the form of attack detection. The training phase is usually in the form of both empirical defense and certified robustness. We present pseudo code to illustrate a fundamental IR defense, as shown in Algorithm 2. The schematic diagram of the three tasks of defense is shown in Figure 8. The specific categorization of adversarial defense in information retrieval is shown in Figure 9."}, {"title": "3.4.2 Evaluation", "content": "Adversarial defense assessment includes metrics for the training phase and metrics for the inference phase. Specifically, the defense for the training phase is mainly empirical defense and certified robustness, and the defense for the inference phase is mainly the detection of adversarial samples.\nMetrics used in the training phase. The metrics in the training phase are mainly for evaluating the ability of the empirical defense and certified robustness method to maintain the original ranked list in the presence of adversarial samples.\n\u2022 CleanMRR@k evaluates Mean Reciprocal Rank (MRR)  performance on a clean dataset.\n\u2022 RobustMRR@k  evaluates the MRR performance on the attacked test dataset.\n\u2022 Attack success rate (ASR)  evaluates the percentage of the after-attack documents that are ranked higher than original documents.\n\u2022 Location square deviation (LSD)  evaluates the consistency between the original and perturbed ranked list for a query, by calculating the average deviation between the document positions in the two lists."}, {"title": "Metrics used in the inference phase", "content": "The metrics in the inference phase are mainly used for evaluating the ability of attack detection methods to accurately recognize adversarial samples.\n\u2022 Point-wise detection accuracy  evaluates the correctness of the detection of whether a single document has been perturbed or not.\n\u2022 #DD  denotes the average number of discarded documents ranked before the relevant document.\n\u2022 #DR  denotes the average number of discarded relevant documents."}, {"title": "3.4.3 Attack detection", "content": "While progress in empirical defense and certified robustness aids in training NRMs to be more robust in their defense against potential attacks, the detection of adversarial documents has also been explored.\nPerplexity-based detection. Perplexity-based detection mainly uses the difference in the distribu-tion of perplexity between the adversarial samples and the original document under the language model for recognition. Adversarial perturbations applied to original documents can significantly impact the semantic fluency of their content . Song et al.  have developed the perplexity-based detection to counter ranking attacks. Detection involves using a pre-trained lan-guage model (PLM) to assess the perplexity of documents, where higher perplexity values indicate"}]}