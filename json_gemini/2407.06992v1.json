{"title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective", "authors": ["YU-AN LIU", "RUQING ZHANG", "JIAFENG GUO", "MAARTEN DE RIJKE", "YIXING FAN", "XUEQI CHENG"], "abstract": "Recent advances in neural information retrieval (IR) models have significantly enhanced their effectiveness over various IR tasks. The robustness of these models, essential for ensuring their reliability in practice, has also garnered significant attention. With a wide array of research on robust IR being proposed, we believe it is the opportune moment to consolidate the current status, glean insights from existing methodologies, and lay the groundwork for future development. We view the robustness of IR to be a multifaceted concept, emphasizing its necessity against adversarial attacks, out-of-distribution (OOD) scenarios and performance variance. With a focus on adversarial and OOD robustness, we dissect robustness solutions for dense retrieval models (DRMs) and neural ranking models (NRMs), respectively, recognizing them as pivotal components of the neural IR pipeline. We provide an in-depth discussion of existing methods, datasets, and evaluation metrics, shedding light on challenges and future directions in the era of large language models. To the best of our knowledge, this is the first comprehensive survey on the robustness of neural IR models, and we will also be giving our first tutorial presentation at SIGIR 2024 1. Along with the organization of existing work, we introduce a Benchmark for robust IR (BestIR), a heterogeneous evaluation benchmark for robust neural information retrieval, which is publicly available at https://github.com/Davion-Liu/BestIR. We hope that this study provides useful clues for future research on the robustness of IR models and helps to develop trustworthy search engines 2.", "sections": [{"title": "1 INTRODUCTION", "content": "Information retrieval (IR) plays a pivotal role in numerous real-world applications, such as web search [61], digital libraries [28] and e-commerce search [71]. According to the global overview report from Digital 2023, nearly 82% of Internet users between 18 and 64 have used a search engine or web portal in the past month. Specifically, IR is the process of finding and providing relevant information in response to the user query from a large collection of data. Recently, with advances in deep learning, neural IR models have witnessed significant progress [51, 53]. With the development of training methodologies such as pre-training [44, 100] and fine-tuning [73, 117, 162], neural IR models have demonstrated remarkable effectiveness in learning query-document relevance patterns.\nWhy is robustness important in IR? In real-world deployment of neural IR models, an aspect equally essential as their effectiveness is their robustness. A good IR system must not only exhibit high effectiveness under normal conditions but also demonstrate robustness in the face of abnormal conditions. The natural openness of IR systems makes them vulnerable to intrusion, and the consequences can be severe. For example: (i) Search engines are vulnerable to black hat SEO attacks, necessitating significant efforts to curb these infringements. (ii) In addition, search engines are confronted with large amounts of unseen data on a daily basis. The working algorithm needs to be updated frequently to ensure that search effectiveness is maintained.\nRecently, academics have begun to investigate the robustness of IR systems [24, 30, 86, 90, 141, 153]. As neural networks gain increasing popularity in IR, many studies have found that IR systems that are based on neural networks inherit a wide variety of robustness issues from deep neural network. The field of robust neural IR is garnering increasing attention, as evidenced by the growing number of papers published annually, as depicted in Figure 1. The robustness issues are differently represented in real IR scenarios and raise concerns about deploying neural IR systems into the real world. Therefore, the study of robust neural IR is crucial for building reliable IR systems.\nWhat is the definition of robustness in IR? In this paper, we begin by defining robustness in the context of IR. In IR, it is well-accepted that, user attention mainly focuses on the Top-K results and increases with higher rankings [110]. Based on this, we argue that robustness in IR refers to the consistent performance and resilience on Top-K results of an IR system when faced with"}, {"title": "Contributions of this survey", "content": "We present a compilation of research in IR robustness, summarizing and organizing the content to aid understanding. By categorizing each subfield and delineating its core concepts, we aim to facilitate a deeper understanding of this field and promote the long-term development of robust neural IR. In summary, this paper's contributions are as follows:\n\u2022 A comprehensive overview and categorization: We define robustness in IR by summarizing existing literature and further dividing it into distinct categories.\n\u2022 A detailed discussion of methodologies and datasets: We offer a detailed discussion of method-ologies, datasets, and evaluation metrics pertinent to each aspect of robustness. Moreover, we integrate the existing datasets mentioned in this survey and propose the BestIR benchmark to facilitate subsequent work.\n\u2022 Identification of open issues and future trends: We highlight challenges and potential future trends, particularly in the age of large language models (LLMs)."}, {"title": "Organization", "content": "Figure 2 depicts the organization of our survey: In Section 2, we introduce the IR task, and give a definition and taxonomy of robustness in IR. In Section 3, we examine adversarial attack and defense tasks, alongside their respective datasets, evaluation criteria, and cutting-edge methodologies. In Section 4, we show two key scenarios for OOD robustness, i.e., OOD generalizability on unseen documents and OOD generalizability to unseen queries, and present specific datasets, evaluation metrics, and methods for solving these scenarios. In Section 5, we describe remaining challenges and emerging opportunities for robustness of IR in the era of LLMs. Finally, in Section 6, we summarize the survey and offer concluding remarks."}, {"title": "2 DEFINITION AND TAXONOMY", "content": "In this section, we provide a formal definition of robustness in the context of IR and outline the taxonomy pertinent to this domain.\nIR task. To provide a clear understanding, we first formalize the IR task. Suppose that R =\n{r1, r2, ..., r} is the set of relevance levels, where l denotes the number of levels. A total order exists among the relevance labels such that r\u0131 > rl-1 > \u2026 > r1, where > denotes the order relation. The minimum value of the relevance label is 0, which usually implies no relevance. Suppose that Q = {q1, q2, ..., qm} is the set of queries in the training dataset. Each query qi is associated with a list of documents D\u2081 = {di,1, di,2, . . ., di,N } and a list of relevance labels Y\u2081 = {Yi,1, Yi,2, ..., Yi,N}, where yi,j \u2208 R denotes the label of document di,j and N is the document list size. Then we obtain the training dataset Dtrain = {(qi, Di, Yi)}\n\nWe use f to denote the IR model, which predicts the relevance score f (q, d) based on a given query q and document d. The IR model f is derived by learning from the following objective\n\n\u03b8* = arg min E(q,D,Y)~Dtrain L (f(q, d), Y),  (1)\n\u03b8\nwhere e are the parameters of the IR model f, and L is a ranking loss function."}, {"title": "2.1 Definition of Robustness in IR", "content": "Robustness refers to the ability to withstand disturbances or external factors that may cause a system to malfunction or provide inaccurate results [59]. It is important for practical applications, especially in safety-critical scenarios, e.g., medical retrieval [5], financial retrieval [58], and private information retrieval [27]. If, for any reason, the IR system behaves abnormally, the service provider can lose time, manpower, opportunities and even credibility. With the development of deep learning, robustness has received much attention in the fields of computer vision (CV) [9] and natural language processing (NLP) [147]. The concerns about model robustness in these fields are mainly focused on the test phase. In this scenario, the model is trained on an unperturbed dataset but tested for its performance when exposed to adversarial examples or OOD data [47, 118].\nIn IR, the robustness of model in the test phase is also important due to the widespread availability of search engine optimization (SEO) [55] and the need for models to adapt to unseen data. Hence, in this survey, we follow prior work and only discuss the robustness of a model in the test phase."}, {"title": "2.2 Taxonomy of Robustness in IR", "content": "In IR, robustness threats exist in a wide range of aspects, including adversarial robustness [153], OOD robustness [141] and performance variance [153]. The framework of robustness in IR is shown in Figure 4.\n2.2.1 Performance variance. Typically, the performance of IR models is first represented by their overall performance on IID data. Recently, it has been recognized that performance stability across IID queries may be compromised when we try to improve the average retrieval effectiveness across all queries [166]. Therefore, a robust neural IR model should not only have good retrieval performance on the overall testing queries, but also ensure that the performance on individual queries is not too bad. We give a formal definition of performance variance in IR based on Definition 2.1."}, {"title": "DEFINITION 2.2 (PERFORMANCE VARIANCE OF INFORMATION RETRIEVAL)", "content": "Given an IR model fDtrain trained on training dataset Dtrain with a corresponding IID testing dataset Dtest, and an acceptable error threshold d, for the top-K ranking result, if\nVar ({M (fDtrain; (q, D, Y), K) | (q, D, Y) \u2208 Dtest}) \u2264 \u03b4,  (6)\nwhere Var() is the variance of the ranking performance of the IR model fDtrain on Dtest, then the model f is considered 8-robust in terms of performance variance for metric M."}, {"title": "2.2.2 Out-of-distribution robustness", "content": "IR models need to cope with a constant stream of unseen data [153]. The key behind this challenge is how to adapt the model to new data out of the familiar distribution. There are a variety of OOD scenarios in the field of IR, so the OOD robustness of the model is likewise of wide interest. First, the query entered by the user may be unknown and of varying quality [88, 116]. Then, search engine application scenario migration and incremental new documents will likewise bring in OOD data [13, 18, 141]. Manual labeling of unseen data as well as retraining IR models incurs significant resource overhead [108, 141]. Therefore, it is crucial how to efficiently train the IR model to have effective performance on unseen data.\nIn this context, studying the robustness of IR models on OOD data has received a lot of attention. OOD robustness measures the performance of an IR model on unseen queries and documents from different distributions of the training dataset. By introducing the test dataset Dtest in a new domain, we give a formal definition of OOD robustness in IR based on Definition 2.1."}, {"title": "DEFINITION 2.3 (OUT-OF-DISTRIBUTION ROBUSTNESS OF INFORMATION RETRIEVAL)", "content": "Given an IR model fDtrain, an original dataset with training and test data, Dtrain and Dtest, drawn from the original distribution G, along with a new test dataset \u010etest drawn from the new distribution G, and an acceptable error threshold d, for the top-K ranking result, if\nRM (fDtrain; Dtest, K) \u2013 RM (fDtrain; \u010etest, K) \u2264 d where Dtrain, Dtest ~ G, \u010etest ~ \u011e,  (7)\nthe model f is considered d-robust against out-of-distribution data for metric M."}, {"title": "2.2.3 Adversarial robustness", "content": "In a competitive scenario, content providers may aim to promote their products or documents in rankings for specific queries [75]. This has provided a market for search engine optimization (SEO) and led to the development of attack techniques against search engines. Traditional attacks on search engines are generally called term spamming. They usually resort to stacking keywords to achieve a ranking boost. In recent years, with the development of deep learning, a number of neural approaches have emerged that attack through more imperceptible perturbations. As search engines evolve, defense methods against term spamming are maturing as well.\nAdversarial robustness focuses on the stability of the IR model performance when imperceptible malicious perturbations are added to documents. By introducing a test dataset Drest with adversarial examples, we give a formal definition of adversarial robustness in IR based on Definition 2.1."}, {"title": "DEFINITION 2.4 (ADVERSARIAL ROBUSTNESS IN INFORMATION RETRIEVAL)", "content": "Given an IR model fDtrain trained on training dataset Dtrain with a corresponding testing dataset Dtest, a new test dataset Diest containing adversarial examples, and an acceptable error threshold d, for the top-K ranking result, if\nRM (fDtrain; Dtest, K) \u2013 RM (fDtrain; Diest, K)| \u2264 8 such that Diest \u2190 Dtest U Dadv,  (8)\nwhere Dtest U Dadv denotes injecting the set of all generated adversarial examples Dadv into the original test dataset, and then model f is considered d-robust against adversarial examples for metric M.\nIn this paper, we pay special attention to two frequently studied types of robustness, i.e., adver-sarial robustness and OOD robustness. (i) In order to study adversarial robustness, existing work usually proceeds along two lines: adversarial attacks and adversarial defenses. We will discuss these lines in detail in Section 3. (ii) Depending on the types of OOD generalizability in IR models, the existing work can be categorized into the OOD generalizability on unseen documents and the OOD generalizability on unseen queries. We will discuss these directions in detail in Section 4. Research on performance variance mainly includes [144, 153, 165-167]. Relatively little work has been done on this type of robustness."}, {"title": "3 ADVERSARIAL ROBUSTNESS", "content": "The deep neural networks have been found vulnerable to adversarial examples that can produce misdirection with human-imperceptible perturbations [39, 48]. In the field of IR, deep learning-based models are also likely to inherit these adversarial vulnerabilities [138], which raises widespread concerns about the robustness of neural IR systems. Recall that we give the definition of adversarial robustness in IR elaborated in Section 2.2. Therefore, in this section, we present the specific work of adversarial attack and adversarial defense in adversarial robustness, respectively."}, {"title": "3.1 Overview", "content": "In IR, search engine optimization (SEO) has been around since the dawn of the world wide web [55]. This includes white-hat SEO [50], which modifies documents in good faith and within the rules and expectations of search engines to optimize the quality of web pages. In contrast, black-hat SEO [16], maliciously exploiting loopholes of search engines, is used to get a site ranking higher in search results. Black-hat SEO creates a poor experience for the audience and is a common concern among site owners.\nTraditional web spamming. SEO involves the creation of web pages with no real informational value, designed to manipulate search engine algorithms and distort search results. This deceptive practice lures web users to sites they would not typically visit, undermining the trust relationship between users and search engines and potentially damaging the search engines' reputation [6].\nweb spamming attack. When assessing textual relevance, search engines scrutinize the locations on a web page where query terms appear [6]. These locations are referred to as fields. Common text fields for a page include the document body, the title, the meta tags in the HTML header, and the page's URL. Additionally, the anchor texts associated with URLs that point to the page are considered part of the page (anchor text field), as they often provide a succinct description of the page's content. The terms in these text fields play a crucial role in determining the page's relevance to a specific query, with different weights typically assigned to different fields. Term spamming refers to the manipulation of these text fields to make spam pages appear relevant for certain queries [16].\nweb spamming detection. web spamming posed a serious threat to search engines in the early days. However, they are also easy to spot and suspect due to their simple implementation [55]. web spamming detection involves identifying and mitigating the practice of term spamming, where keywords are excessively used or manipulated on a web page to artificially inflate its relevance to specific search queries [126]. Various techniques have been developed to detect the term spamming. There is a utility-based term spamicity detection method known as OSD (Online Spam Detection) [170, 171] that has been used to identify adversarial examples. OSD mainly relies on TF-IDF features and has been validated by the Microsoft adCenter [86]. It introduces the notion of spamicity to measure how likely a page is spam. Spamicity is a more flexible and user-controllable measure than the traditional supervised classification methods. Using spamicity, online link spam, and term spam can be efficiently detected. However, this detection method can only target changes that add query terms and is prone to misclassification.\nWhy study adversarial attacks in IR? With the development of deep learning, neural networks are beginning to be widely used in IR models and have achieved excellent performance. Although traditional web spamming can also have a significant attack effect on neural IR models, this method is difficult to pose a threat due to its ease of detection. However, in the context of black-hat SEO, neural IR models are at risk of being attacked due to the inherent vulnerability inherited from neural networks. Therefore, adversarial attacks are beginning to be studied to expose vulnerability flaws in neural IR models in advance."}, {"title": "Why study adversarial defense in IR?", "content": "To mitigate adversarial attacks, there is a growing body of work that is focusing on adversarial defenses. Adversarial defense focuses on the early hardening of model vulnerabilities discovered by adversarial attacks. Its goal is to obtain robust neural retrieval models to build reliable IR systems.\nThe relationship between adversarial attacks and defenses is shown in Figure 5. Recently, adver-sarial robustness has begun to gain widespread attention. Below, we describe these efforts from several perspectives: benchmark datasets, adversarial attacks, and adversarial defense."}, {"title": "3.2 Benchmark Datasets", "content": "In this section, we present the datasets commonly used for studying adversarial robustness. Existing work on attacks and defenses against robustness has focused on experiments on existing IR datasets, as shown in Table 1. All datasets can be found in the BestIR benchmark.\nBasic datasets. Some datasets in IR are often adapted for reuse by attack and defense methods as basic datasets. These include MS MARCO document/passage [108] and Clueweb-09B [29]. Some work [89, 91, 152] performed experiments against attacks and defenses directly on these datasets. For example, prior work usually used the training set of the basic dataset to train an NRM as the target model [91, 152]. Meanwhile, the queries in the development set are used as target queries, and a portion of the documents in the ranked list of each query are sampled as target documents. Attacks on these target documents measure the performance of adversarial attack methods. We will discuss the specific evaluation methods in Section 3.3.\nExpansion of datasets. Some competitions, such as TREC DL19 [34] and TREC DL20 [33], have provided additional query collections for evaluation against the base dataset. Similarly, these query collections can be used to evaluate the effectiveness of attack methods. Queries in these datasets are often attacked as additional sets of targeted queries. For example, existing work usually trains NRM on the MS MARCO passage dataset and uses the 43 queries in TREC DL19 as the target queries to perform attacks [24, 86].\nOff-the-shelf datasets. Some research has adapted the above basic datasets to construct new datasets that can be used directly to perform attacks or evaluate defenses. For example, ASRC [120] is based on documents in Clueweb, which are manually modified to generate new adversarial samples for evaluating the model's adversarial defense abilities. Existing work also used it to study the effects of manual manipulation of documents on the environment IR systems [49, 142]. To perform the topic-oriented attack, Liu et al. [90] constructed query groups on the same topic based on ORCAS [32] and the TREC 2012 web Track [29] as a complement to MS MARCO document and Clueweb-09b, respectively. DARA [23] is a dataset for detecting adversarial ranking attacks and includes two types of detection tasks for adversarial documents."}, {"title": "Algorithm 1: Adversarial sample generation for neural IR models", "content": "Require:\nA target query q, a target document d, a query collection Q, a corpus C, a neural IR model\nf, and a ranking loss function L\nEnsure: An adversarial document dadv\n1: if f is a black-box model then\nProcedure Surrogate model imitation\nQuery the model f with Q, and collect ranked lists.\nTrain the surrogate model f with Q and the collected ranked lists.\nf = f\n6: end if\n7: Procedure Adversarial attack\n8: Initialize the adversarial example dadu as a copy of the target document d.\n9: for t \u2190 1 to n do\nQuery the model f with the target query q, and collect the ranked list D.\nCalculate the gradient of L with respect to the target query q and target document d:\ngradient \u2190 VaL (f, q, dadv, D)\nGenerate the higher dimensional adversarial perturbation p:\np \u2190 normalize(gradient)\nMapping high-dimensional perturbations to text space:\npp\n17: Add textual perturbations:\n18: dadv\u2190 de p\n19: end for\n20: return dado"}, {"title": "3.3 Adversarial Attack", "content": "As neural networks have become increasingly prevalent in IR systems, they have also become a target for adversarial attacks. Studying adversarial attacks can help understand the vulnerability of neural IR models before deploying them in real-world applications, and it can also be used as a surrogate evaluation and support the development of appropriate countermeasures.\n3.3.1 What are the differences between IR attacks and CV/NLP attacks? Adversarial attacks have undergone significant development in the fields of NLP and CV [39, 48], where the most emblematic forms of attack are typically directed at image retrieval and text classification tasks. However, the landscape of adversarial attacks assumes a different contour in the realm of IR: (i) Compared with image retrieval attacks, the IR attacks need to maintain semantic consistency of the perturbed document with the original document by considering the textual semantic similarity, rather than pixel-level perturbations within a fixed range in continuous space; and (ii) Inspired by black-hat SEO, the goal of IR attacks is to inject imperceptible perturbations within a document to improve its ranking for one or multiple specific queries within the entire candidate set or corpus, not inducing classification errors by the model.\nWithout loss of generality, given a query q and a target document d, the goal of generating imperceptible perturbations p to attack against a neural IR model f under top-K ranked results can usually be formalized as:\n\nmax (K \u2013 \u03c0f (q, d \u2295 p) + \u03bb \u2022 Sim (d, d\u2295 p)),  (9)\nP\nwhere rf (q, d\u2295 p) denotes the ranking position of the perturbed document d \u2295 p in the ranked list generated by f with respect to query q. The Sim (\u00b7) function measures the similarity between the adversarial example and the original document, both textually as well as semantically. A is a regularization parameter used to balance two goals: keeping the adversarial samples as close as possible to the original document, while allowing the adversarial samples to be ranked as high as possible. Ideally, the adversarial sample d \u2295 p should preserve the original semantics of d, and be imperceptible to human judges yet misleading to the neural IR models.\n3.3.2 What is the attack setting? Depending on whether the attacker has access to the knowledge of the parameters of the target model, the attack setup can be categorized into two main types [86, 152]: (i) White-box attacks: Here, the attacker can fully access the model parameters and use the model gradient to directly generate perturbations. (ii) Black-box attacks: Here, the model parameters cannot be obtained; the attacker usually adopts a transfer-based black-box attack paradigm [26]: They construct a surrogate white-box model by continuously querying the model and getting the output. Specifically, a surrogate model is trained to simulate the performance of the target model, and then the surrogate model is attacked to the transferability of the adversarial samples to indirectly attack the target model. In IR, attackers can query the target model to obtain a ranked list with rich information. Therefore, the surrogate model often has access to sufficient training data, making this attack effective [86, 152]. There are various attack methods and target models in IR. We present basic pseudo code to illustrate a fundamental IR attack, in Algorithm 1.\nAccording to the type of target model, attack efforts against IR models can be broadly categorized into two types: adversarial retrieval attacks and adversarial ranking attacks. A schematic diagram of the two types of attack is shown in Figure 6. (i) Adversarial retrieval attacks target the first-stage retrieval, mainly against dense retrieval models; and (ii) Adversarial ranking attacks target the re-ranking stage, mainly against neural ranking models. The methodology for conducting adversarial attacks in IR can differ based on the chosen attack strategy and the target model. A categorization of adversarial attacks in IR is shown in Figure 7.\nIn the following, we introduce evaluation, adversarial retrieval attacks, and adversarial ranking attacks of adversarial attacks in IR."}, {"title": "3.3.3 Evaluation", "content": "Evaluation of adversarial attacks includes both attack performance and natural-ness performance.\nAttack performance. Attack performance mainly refers to the degree of ranking improvement after a target document has been attacked. In general, automatic metrics for attack performance are widely adopted as follows:\n\u2022 Attack success rate (ASR/SR) [86, 90, 152], which evaluates the percentage of target docu-ments successfully boosted under the corresponding target query.\n\u2022 Average boosted ranks (Boost/Avg.boost) [86, 89, 90], which evaluates the average improved rankings for each target document under the corresponding target query.\n\u2022 Boosted top-K rate (TKR) [86, 90], which evaluates the percentage of target documents that are boosted into top-K w.r.t. the corresponding target query.\n\u2022 Normalized ranking shifts rate (NRS) [89, 150] evaluates the relative ranking improvement of adversarial examples which are successfully recalled into the initial set with K candidates:\nNRS = (Id \u2013 Idadv)/\u03a0d \u00d7 100%,  (10)\nwhere Id and I jadu are the rankings of d and dadu respectively, produced by the target IR model.\nThe effectiveness of an adversary is better with a higher value for all these metrics.\nNaturalness performance. Naturalness performance refers primarily to the degree to which a target document is imperceptible to humans after it has been attacked. In general, automatic metrics for naturalness performance and human evaluation are widely adopted as follows:\n\u2022 Automatic spamicity detection, which can detect whether target pages are spam or not. The utility-based term spamicity method OSD [170] is usually used to detect the adversarial examples."}, {"title": "3.3.4 Adversarial retrieval attack", "content": "In this subsection, we introduce the task definition of adversarial retrieval attacks and methods to achieve such attacks.\nTask definition. The adversarial retrieval attack task is designed to attack against DRMs. The objective of adversarial retrieval attacks is centered around the manipulation of a document that initially fails to be recalled. By integrating adversarial perturbations, the aim is to ensure that this document is subsequently retrieved by the first-stage neural retrieval model, thereby securing its presence within the candidate set. This approach not only challenges the robustness and reliability of neural retrieval systems but also raises significant questions regarding the integrity of information authenticity [89, 169]. The goal of adversarial retrieval attacks under top-K ranked results can be formalized as:\n\nmax (K - Recallf (q, d \u2295 p) + \u03bb \u00b7 Sim (d, d \u2295 p)),  (11)\nP\nwhere Recallf (q, d \u2295 p) denote the recalled position of the perturbed document d \u2295 p generated by the frist stage retrieval model f with respect to query q given the entire corpus. A smaller value of Recall denotes a higher ranking.\nMethod. Current retrieval attack methods mainly include corpus poison attacks, backdoor attacks, and encoding attacks.\n\u2022 Corpus poison attack. Corpus poisoning attacks construct adversarial samples against a specific query and inject them into the corpus in the inference phase so that they are recalled. MCARA [89] addresses this issue and attempts to mine the vulnerability of the DRM. It introduces the adversarial retrieval attack (AREA) task, which intends to deceive the DRM into retrieving a target document that is outside the initial set of candidate documents. Zhong et al. [169] adopt the HotFlip method [39] from NLP to iteratively add perturbations in the discrete token space to maximize its similarity to a set of queries. In this way, they maximized the contamination of the corpus by a limited number of documents. MAWSEO [84] employs adversarial revisions to achieve real-world cybercriminal objectives, including rank boosting, vandalism detection evasion, topic relevancy, semantic consistency, user awareness (but not alarming) of promotional content, etc.\n\u2022 Backdoor attack. Backdoor attacks inject a small proportion of ungrammatical documents into the corpus. When user queries contain grammatical errors, the model will recall the learned triggering pattern and assign high relevance scores to those documents. Long et al. [93] introduces a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisements, through a retrieval system. To achieve this, they propose a perilous backdoor attack triggered by grammatical errors and ensure that attack models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries."}, {"title": "3.3.5 Adversarial ranking attack", "content": "In this subsection, we introduce the task definition of adversarial ranking attacks and existing methods to achieve adversarial ranking attacks.\nTask definition. The adversarial ranking attack task is designed to attack against NRMs. The aim of adversarial ranking attacks typically involves introducing adversarial perturbations to a document already present in the candidate set, to manipulate its ranking position either elevating or diminishing it as determined by a neural ranking model. The goal of adversarial ranking attacks under top-K ranked results can usually be formalized as:\n\nmax (K - Recallf (q, d \u2295 p) + \u03bb \u00b7 Sim (d, d \u2295 p)),  (12)\nP\nwhere Rankf (q, d \u2295 p) denotes the ranking position of the perturbed document d \u2295 p in the final ranked list generated by the NRM f with respect to query q. A smaller value of Rank denotes a higher ranking.\nMethod. Adversarial ranking attacks against NRMs include word substitution attacks, trigger attacks, and prompt attacks.\n\u2022 Word substitution attack. Word substitution attacks typically boost a document's ranking by replacing a small number of words in the document with synonyms. One common method of white-box word substitution attack is the gradient-based attack, where the attacker uses the gradient of the loss function with respect to the input data to create adversarial examples. These examples are designed to cause the model to make incorrect relevance predictions or rankings. Raval and Verma [122] present a systematic approach of using adversarial examples to measure the robustness of popular ranking models. They follow a similar approach used in text classification tasks [81] and perturb a limited number of tokens (with a minimum of one) in documents, replacing them with semantically similar tokens such that the rank of the document changes. Brittle-BERT [150] adds/replaces a small number of tokens to a highly relevant or non-relevant document to cause a large rank demotion or promotion. Authors find a small set of recurring adversarial words that when added to documents result in successful rank demotion/promotion of any relevant/non-relevant document respectively. As for black-box word substitution attacks, in the field of machine learning, Szegedy et al. [138] find that adversarial examples have the property of cross-model transferability, i.e., the adversarial example generated by a surrogate model can also fool a target model. Black-box attacks in information retrieval usually adopt this transfer-based paradigm due to the excellent performance of imitation of the target model. Wu et al. [152] propose the first black-box adversarial attack task against NRM, the word substitution ranking attack (WSRA) task. The WSRA task aims to fool NRMs into promoting a target document in rankings by replacing important words in its text with synonyms in a semantic-preserving way. Based on this task, authors propose a novel pseudo relevance-based adversarial ranking attack method (PRADA), which outperformed web spamming methods by 3.9% in ASR. The WSRA task focuses only on attacks on single query-document pairs and does not take into account the dynamic nature of search engines. Based on this, Liu et al. [90] introduce the topic-oriented adversarial ranking attack (TARA) task, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic.\n\u2022 Trigger attack. Trigger attacks boost document rankings by injecting a generated trigger sentence into a specific location in the document (e.g., the beginning). Song et al. [134] propose using semantically irrelevant sentences (semantic collisions) as perturbations. They develop gradient-based approaches for generating collisions given white-box access to an NRM. Goren et al. [50] propose a document manipulation strategy to improve document quality for the purpose of improving document ranking. Liu et al. [86] propose a trigger attack method, PAT, empowered by a pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. TRAttack [135] used rewriting existing sentences in the text to improve document ranking with learning ability from the multi-armed bandit mechanism.\n\u2022 Prompt attack. Prompt attacks use prompts to guide a language model to generate pertur-bations based on existing documents to improve document ranking. Chen et al. [24] propose a framework called imperceptible document manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM finds the optimal connect sentence to insert into the document through a language model. Parry et al. [115] analyze the injection of query-independent prompts, such as \u201ctrue\u201d into documents and found that the prompt perturbation method is valid for several sequence-to-sequence relevance models like monoT5 [111].\n\u2022 Multi-granular attack. Multi-granular attacks focus on generating high-quality adversarial examples by incorporating multi-granular perturbations, i.e., word level, phase level, and sentence level. Liu et al. [91] propose RL-MARA, a reinforcement learning framework to navigate an appropriate sequential multi-granular ranking attack path. By incorporating word-level, phrase-level, and sentence-level perturbations to generate imperceptible adversarial examples, RL-MARA could increase the flexibility of creating adversarial examples, thereby improving the potential threat of the attack."}, {"title": "3.4 Adversarial Defenses", "content": "With the advent of SEO", "phases": "the goal is to ensure that the model's ability to accurately retrieve relevant documents remains uncompromised, even in the presence of manipulative adversarial perturbations.\nWithout loss of generality, given a test set Dtest and an adversarial document set Dadv"}]}