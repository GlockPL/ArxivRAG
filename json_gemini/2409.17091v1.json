{"title": "Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification", "authors": ["Xinrui Zhou", "Yuhao Huang", "Haoran Dou", "Shijing Chen", "Ao Chang", "Jia Liu", "Weiran Long", "Jian Zheng", "Erjiao Xu", "Jie Ren", "Ruobing Huang", "Jun Cheng", "Wufeng Xue", "Dong Ni"], "abstract": "In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "DYNAMIC information in medical imaging (e.g., video sequences) plays a vital role in clinical diagnosis. Recent deep learning-based classifiers have shown the ability to improve the diagnostic accuracy of different diseases [1], [2]. Despite rapid advancements, the performance of current advanced solutions is still limited by several issues: 1) The scarcity of dynamic clinical cases, coupled with the high annotation cost, limits data availability; 2) Imbalanced data distribution, driven by the rarity of high-risk positive cases, skews model training; 3) Deep models are prone to brittle performance degradation when tested on out-domain data, e.g., cases from different medical centers [3], [4]. This risk presents a significant obstacle to deploying models in real-world medical settings. To address these issues, there is growing interest in generative augmentation paradigms, which employ advanced generative models to synthesize medical samples, thereby augmenting relevant diagnostic tasks.\n\nDenoising diffusion probabilistic models (DDPMs), which utilize explicit likelihood estimation and a progressive sampling process for data synthesis, have more well-established mathematical explanations and abilities to achieve stable, controllable, and diverse synthesis than previous generative methods like generative adversarial networks (GANs) [5]\u2013[7]. Diffusion models have garnered remarkable success in several natural image fields, including static image generation [8], [9], video synthesis [10]\u2013[17], video editing [18]\u2013[20], and image animation [21], [22]. Recently, researchers have explored high-fidelity medical sequence synthesis by applying the above approaches within the medical field [23], [24].\n\nMost recently, pioneers deeply investigated the impact of the diffusion-based generative augmentation scheme on solving data scarcity [7], [25] and domain generalization [25] issues in diagnostic tasks. However, they failed to fully ensure the reliability of synthetic medical images for classification due to limited semantic steerability and absent quality control over the generated images. Moreover, their techniques focus on image-level synthesis rather than sequence-level synthesis, which is crucial for medical modalities like MRI, ultrasound (US), etc [7]. Thus, our study plans to devise a diagnosis-reliable controllable generative augmentation framework to facilitate accurate and robust medical sequence classification.\n\nIntuitively, building customized, high-fidelity, and sequentially coherent synthetic medical databases, and effectively utilizing them, is essential for enhancing sequence recognition. However, this task presents several challenges. First, lesions (Fig. 1(a-b)) or structures (Fig. 1(c)) of the same disease category exhibit large visual variances (e.g., shapes, intensities, positions, etc). This may confuse diffusion learning seriously, thus causing uncontrollable and unreliable generation. Second,"}, {"title": "II. RELATED WORKS", "content": "In this section, we briefly review the approaches for controllable sequence synthesis using diffusion models. Given the strong inter-slice association in 3D medical volumes [27], we treat them as video sequence data in our study and summarize related video-based methods. Finally, we deeply involve existing diffusion-based generative augmentation schemes."}, {"title": "A. Controllable Video Synthesis with Diffusion Models", "content": "Diffusion models, a class of generative models, have recently attracted significant attention, especially text-to-video (T2V) synthesis [10], [11], [28], [29]. However, these methods did not achieve precise control over the visual appearance and dynamics of the synthesized video, resulting in limited practicality. To mitigate this issue, researchers have concentrated on incorporating multimodal conditions into T2V frameworks to guide controllable synthesis. Specifically, AnimateDiff [30] and MoonShot [31] introduced text and image conditions to facilitate the control of visual appearance. Most recently, several studies [19], [32], [33] achieved fine-grained spatial compositional control by integrating additional diverse conditions (i.e., sketch, mask, and depth sequences). Meanwhile, two spatial-sequential (S\u00b2) modeling strategies (fully attention layers [19] and S\u00b2 condition encoder [33]) were introduced to capture the dynamics of sequential conditions, promoting the inter-frame features interaction. Besides, dense optical flow [34] was adopted to control sequential consistency [35], [36]. The above methods have shown promising controllability in natural video synthesis. However, for medical synthesis, acquiring abundant dense annotations (e.g., mask sequences) as control signals is impractical and unaffordable. Additionally, medical videos have unique attributes compared with natural ones, such as blurred anatomical regions and complex structure variations. Hence, they may not suit the medical video synthesis tasks. In the medical field, several diffusion-based methods have been proposed to generate photorealistic echocardiography videos using the conditional guidance of a single frame [37], ejection fractions [37], single semantic mask [38], [39], and single sketch image [23]. Zhou et al. [23] additionally leveraged mitral valve (MV) skeletons to control the complex motion trajectories of MV in the generated echocardiograms. Besides, Li et al. [24] proposed an approach to generate endoscopy videos that simulate clinical scenes by integrating a spatial-temporal transformer with 2D vision foundation model priors. However, the above methods may have the following drawbacks: unsteerable semantics due to the lack of attribute guidance (e.g., category or target shape), poor control over sequences, etc. Thus, they may be unsuitable for direct adoption in the generative augmentation tasks."}, {"title": "B. Generative Augmentation with Diffusion Models", "content": "Synthetic data can effectively supplement traditional data augmentation (e.g., rotation) and provide diverse samples to help deep model training [7], [40]. Previous works have succeeded in utilizing synthetic data generated with diffusion models to enhance natural image classification [26], [41]\u2013[43], segmentation [44]\u2013[46], and detection [47]. Recently, Singh et al. [48] performed an in-depth analysis of models trained with synthetic data across various robustness measures, and verified that they achieved good suitability in real-world settings.\n\nTo address data scarcity and promote diagnostic performance, the intelligent medical field has raised increasing interest in exploring diffusion-based generative augmentation for aiding different downstream diagnostic tasks [25], [49]\u2013[53]. Luo et al. [7] marked the first focus on comprehensively analyzing the impact of their designed uncertainty-guided diffusion models on downstream diagnostic tasks. Nevertheless, current studies have two main limitations:\n\nOn the one hand, due to the lack of adequate semantic and sequential guidance during sampling, coupled with the absence of post-sampling quality control for generated samples, existing works could hardly fully ensure the reliability of synthetic databases for downstream disease recognition. For instance, it is desirable to embed descriptive text (e.g., lesion morphology) to provide more informative guidance associated with disease grading. Unlike previous solutions, we aim to develop a generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples to better facilitate downstream classifier learning.\n\nOn the other hand, few studies comprehensively explore the effect of generative models on downstream medical tasks, and they solely focus on image-level generation. Considering the value of dynamic information for clinical diagnosis, we concentrate on diagnosis-reliable sequence synthesis. This is more challenging than image-related works as synthetic data not only meets high-fidelity requirements but also ensures temporal or stereoscopic consistency and coherence for effectively promoting downstream S\u00b2 modeling capacity. A brief review of studies in the medical field utilizing diffusion models to promote downstream tasks can be found in the Supplementary Material."}, {"title": "III. METHOD", "content": "Fig. 2 shows the pipeline of using our proposed controllable generative augmentation framework to aid medical sequence classification. First, we propose a sequence generator that enables perceiving multiple semantic and sequential conditions to guide controllable and diverse generation. Then, an efficient noisy data filter is introduced to suppress unsatisfactory synthetic sequences. Last, the quality-controlled synthetic sequences and real ones will work together to improve the performance of arbitrary classifiers. To formulate customized, high-fidelity, and coherent synthetic databases to boost classification, we design the whole framework from model-centric, i.e., sequence generator (Secs. III-B, III-C, and III-D) and data-centric, i.e., data filter (Sec. III-E) perspectives."}, {"title": "B. Basic Architecture of Sequence Generator", "content": "Fig. 3 shows the pipeline of our proposed sequence generator. It supports customized and high-quality medical sequence generation via multimodal conditions. The generator is implemented with a two-stage training scheme. In the pretraining stage, it attends to high-fidelity visual features learning for controllable image synthesis. While in the finetuning stage, the domain-specific visual knowledge acquired from the previous stage is reused and focuses on sequential patterns modeling for customized sequence synthesis. During inference, given a single or combination of multimodal conditions as control signal inputs, high-quality and steerable sequence generation from Gaussian noise can work. We then provide a short background of video diffusion models in Sec. III-B1, followed by a detailed description of the basic architecture of our generator in Secs. III-B2 and III-B3.\n\n1) Preliminaries of Video DDPMs: They are trained to learn complex video distribution by iteratively denoising corrupted inputs using pixel-space diffusion methods [54], [55]. Given a training video x\u2080 \u223c q and let T the timestep, DDPMs first produce a sequence of noisy inputs via a diffusion process q(x\u2081, ..., xT| x\u2080), t \u2208 1, 2, ..., T, which progressively adds Gaussian noise \u2208 \u223c N(0, 1) to x\u2080. To ease the computational burden due to pixel-space training, video latent diffusion models [28] perform the diffusion process in latent space of a variational autoencoder [56] (VAE). The model is then trained to estimate the parameterized Gaussian transition p(x\u209c\u208b\u2081 | x\u209c) via a denoising network \u03b8. Mathematically, the optimized objective can be a simplified variant of the variational lower bound:\n\nmin  \ud835\udd3cx\u2080,\u03f5\u223cN(0,1),c,t||\u03f5 \u2212 \u03f5\u03b8(z\u209c, c, t)||\n\nwhere z\u2080 is the latent code of x\u2080. \ud835\udd3c and \u03f5 represent the predicted and target noise, respectively. Here, c denotes the (optional) control signal that the model can be conditioned on. For our generator, c involves multimodal conditions including semantic and sequential ones.\n\n2) Factorized Learning of Visual Features and Sequential Patterns: Developing a generator with pure 3D architecture design is an intuitive scheme for sequence synthesis [28]. However, it is very challenging to simultaneously learn visual features and sequential patterns due to the scarcity and complexity of medical data, while causing high training costs. To this end, we propose to factorize both learning by first pretraining a latent diffusion model (LDM) and then finetuning its sequence counterpart. In this way, the generator enables realistic sequence synthesis, while easing the training burden.\n\n3) 2D-to-3D Model Inflation: In the pretraining phase, we develop a 2D UNet [57] in LDM to predict noises for image synthesis. Referring to [13], we extend the 2D UNet to a 3D version via an inflation scheme to build the sequence LDM. Specifically, all the spatial convolution layers are inflated to pseudo-3D counterparts by expanding the kernels at the sequential dimension (e.g., 3 \u00d7 3 \u2192 1 \u00d7 3 \u00d7 3 kernel). Besides, we perform sequential insertion by adding sequential attention (SA) layers (see Fig. 3). Fig. 4(a) shows the principle of the SA mechanism, where each patch queries to those at the same spatial position and across frames/slices. This design encourages the generator to model the sequential patterns, while not significantly altering visual feature distribution baked in the LDM [31]. Overall, sequence LDM enables inheriting the rich visual concepts preserved in LDM and focusing on sequential pattern aggregation, making the model learning efficient."}, {"title": "C. Multimodal Conditions Guidance", "content": "Sequence generators guided by a single condition (e.g., text) have the following drawbacks: 1) poor controllability and 2) sequential inconsistency. To solve these issues, we introduce multimodal conditions to guide our sequence generator for customized, realistic, and sequential-consistent synthesis."}, {"title": "D. Sequential Augmentation Module", "content": "Solely equipping with SA layers and sequential cues in sequence LDM may present inadequate consistency and coherence across synthetic frames/slices. This issue may arise from insufficient sequential modeling of input noisy latents and motion fields, constrained by a small parameter space, overly burdening the SA layers. To solve the issue, we propose a sequential augmentation module (SAM) that enables the generator to more effectively model sequential dependencies. As shown in Fig. 4(d), SAM integrates two attention mechanisms in cascade for sequential augmentation.\n\n1) Key-frame/slice Attention: The common spatial self-attention layers can lead to sequential inconsistency due to the lack of interaction across frames/slices. To augment sequential consistency, we introduce a key-frame/slice attention (KA) mechanism, where two selected key-frames/slices act as references to propagate S\u00b2 information throughout the sequence. Specifically, for any frame/slice, we select its previous and the first counterpart of the sequence as the references and transform spatial self-attention into KA, aiming to align the latent features z\u1d62 of the l-th frame/slice with z\u2081 and z\u2097\u208b\u2081. We obtain query from z\u1d62, key and value features from z\u2081 and z\u2097\u208b\u2081, and compute Attention(Q, K, V) using:\n\nQ = WQz\u1d62, K = WK {z\u2081, z\u2097\u208b\u2081}, V = WV {z\u2081, z\u2097\u208b\u2081},\n\nwhere WQ, WK, WV are initialized on the original spatial self-attention weights for inheriting the semantic perception capability of LDM in the finetuning stage. {} represents concatenation operation. It is highlighted that KA retains low computational complexity compared with full attention [19]. Please refer to Fig. 4(b) for a visual illustration.\n\n2) Motion Field Attention: To further boost the sequential coherence of the generated sequences, we propose to reuse the motion field condition (refer to Sec. III-C2) by introducing a motion field attention (MFA) mechanism after KA. As shown in Fig. 4(c), MFA requires the patches to communicate with those in the same motion field-based pathway including itself, which eliminates flickers of the generated sequences to make the contents visually smooth. Inspired by [62], MFA is implemented in two steps: a) motion field-based patch pathway sampling and b) attention calculation. In step 1, we sample the patch pathways based on the m-scaled downsampled motion fields. Take a video sequence as an example, for a patch p\u1d62 on the l-th frame of a f-frame video, the path can be derived from the motion field. Since the sampling procedure inevitably generates multiple pathways for the same patch, we randomly sample a pathway to ensure its uniqueness to which each patch belongs. In this setting, let H, W the height and width of the input video frame, then the size of the pathway set after sampling equals H\u00d7W\u00d7 f. In step 2, we calculate Attention(Q, K, V) with:\n\nQ = z\u209a\u1d62, K = V = Zpath,\n\nwhere Zpath represents the latent features of patches on the path, as follows:\n\nZpath = [p\u2080, p\u2081, ..., p\u2097, ..., p\ud835\udcbb\u208b\u2081, p\ud835\udcbb].\n\nThe output of KA is directly fed into MFA without being handled by the query, key, and value projection functions. Hence, MFA can comfortably decrease extra computation."}, {"title": "E. Noisy Synthetic Data Filter", "content": "We use the proposed sequence generator to constitute the synthetic samples set. Concretely, assuming there are n training clips in the target dataset, with a bank of conditions derived from each clip, we synthesize a group of clips guided by each conditions bank. Eventually, we obtain n groups of clips to form our synthetic sample set, with a total of N clips. Although visually realistic and smooth (good cases, Fig. 5), sequence synthesis may still suffer from class semantics misalignment, cross-frame/slice inconsistency or over-consistency (i.e., almost static clip), and inter-clip similarity. For instance, in Fig. 5(e), the synthesized carotid clip category is wrong, which should be moderate rather than mild. In Fig. 5(f), the generated clip includes abrupt changes in anatomical 1 calculates inter-clip similarity (refer to Sec. III-E2)."}, {"title": "F. Evaluation", "content": "As a common practice, existing sequence synthesis works evaluate the synthetic quality by mostly using Fr\u00e9chet Inception Distance (FID) [65] and Fr\u00e9chet Video Distance (FVD) [66]. However, researchers have verified that these metrics do not consistently correlate with performance metrics on downstream tasks [7], [46]. To exhaustively evaluate the values of synthetic samples to downstream tasks, eleven popular classifiers are deployed for comparison with respect to diagnostic accuracy and area under the receiver operating characteristic curve (AUROC). Besides, we adopt the proposed VAE-Seq to assess the cross-frame/slice consistency (refer to Sec. III-E2). Moreover, we consider it crucial to evaluate the smoothness of generated clips. Motivated by [67], we quantify it using the proposed metric (termed Dynamic Smoothness), which is the mean absolute error between the reconstructed frames/slices from the interpolation model [68] and the original real ones."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first comprehensively validated the effectiveness and generality of our method on three medical datasets, using eleven popular networks trained on three paradigms. We then tested the performance of our method in underrepresented high-risk sets and out-domain conditions. Last, we analyzed the contributions of our proposed multimodal conditions guidance, sequential augmentation module,"}, {"title": "D. Classification Robustness in Out-domain Conditions", "content": "We further explored the impact of synthetic samples from Ctrl-GenAug on diagnostic robustness in out-domain settings using the carotid dataset. Training data of the sequence generator consisted of labeled ID and unlabeled OD subsets. We added the hospital identifier in the descriptive text of the ID data to condition the domain distribution. For the OD data, which did not contain the diagnostic class label, we padded the corresponding conditioning vector with zeros, while solely preserving hospital IDs to form descriptive texts. As shown in Table III, the average accuracy and AUROC of three classifiers display better results on the Real-finetune and Joint-train paradigms compared to the Baseline. It validates that diagnostic robustness can be enhanced with the aid of Ctrl-GenAug in scenarios where we only have access to unlabeled cases from additional medical centers due to limited resources."}, {"title": "E. Analysis of Multimodal Conditions Guidance", "content": "To illustrate the role of different conditional guidance, we conduct comparative experiments with generators that resort to various banks of conditions for training and sampling. Table IV validates that all proposed conditions are effective for enhancing synthesis and diagnosis tasks. Specifically, without text control, the downstream diagnostic performance is comparatively poor. Fig. 5(b-c) compares typical synthetic thyroid nodule sequences generated under condition banks with and without text guidance. The former is visibly high-fidelity and faithful to the given text prompt (e.g., smooth margin), while the latter exhibits less distinguishable features for diagnosis (e.g., blurry margin). That is to say, the proposed Ctrl-GenAug can create more diagnosis-reliable samples by enhancing semantic steerability in the generation process. Besides, by incorporating image prior knowledge in the sampling process, the average accuracy and AUROC of three classifiers are improved by 4.20% and 0.040, respectively. This proves the domain gap between synthetic and real samples is mitigated by introducing the image prior. Moreover, as shown in Table IV, conditioning the generator on the motion field produces more content-consistent and smoother samples, leading to continuous improvements in diagnostic performance. It can also be"}, {"title": "F. Impact of the Sequential Augmentation Module", "content": "To verify the impact of the proposed sequential augmentation module, as shown in Table V, we compared the performance of three generator variants on the synthesis and downstream tasks. Ours-S, Ours-SK, and Ours-SKM denote our ablation studies, including gradually adding sequential attention ('-S'), key-frame/slice attention ('-K'), and motion field attention ('-M') to the generator with spatial inflation only. Table V shows that the downstream accuracy and AUROC improvements of Ours-SKM are significantly higher than those of Ours-S and Ours-SK in both datasets. In terms of quantitative assessment of sequential coherence, Ours-SKM achieves better results overall than other variants. These prove that this module can help the generator synthesize cross-frame/slice consistent and dynamic-smooth clips, which are diagnosis-promotive for downstream diagnostic tasks. Moreover, it can be observed that the FVD results across these three generators show no significant difference, revealing its limited value for evaluating diagnosis-oriented medical sequence synthesis tasks."}, {"title": "G. Effectiveness of the Noisy Synthetic Data Filter", "content": "As shown in Fig. 7, we validated the indispensable role of our proposed noisy synthetic data filter by assessing its impact on downstream diagnostic accuracy, using two classifiers trained on Real-finetune and Joint-train paradigms across the carotid and ACDC datasets. For instance, on the carotid dataset, the Joint-train CSN resorting to non-quality controlled synthetic clips (i.e., 'None'), displays no significant difference with its Baseline (80.95% vs. 80.27%), even an accuracy degradation for its VideoSwin counterpart (78.91% vs. 79.59%). Then, by implementing our CF and SF strategy, diagnostic accuracy is significantly improved by 2.04% and 3.40%, respectively. Equipped with both, it achieves an accuracy of 87.07%, outperforming the 'None' by 6.12%."}, {"title": "H. Impacts of Traditional and Generative Augmentations", "content": "We tested the contributions of different data augmentations in three downstream diagnostic tasks. The traditional augmentations used in this experiment can be found in Table I. We"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "In this study, we present a new and general diffusion-based generative augmentation framework, named Ctrl-GenAug, to facilitate medical sequence classification by leveraging customized and diagnosis-reliable synthetic sequences. To the best of our knowledge, this is the first comprehensive study to investigate the impact of the controllable generative augmentation scheme on aiding medical sequence classification. To improve the quality of synthetic data for promoting downstream classification, we propose a multimodal conditions-guided medical sequence generator that ensures flexibly controllable synthesis across semantic, sequential, and data distribution aspects. Moreover, we propose a highly effective noisy synthetic data filter to better learn from synthetic data, including adaptively filtering diagnosis-inhibitive synthetic sequences at class semantics and sequential levels. This can better connect the synthesis task and the downstream one, thus further enhancing our sequence classification performance. Extensive experiments on 3 medical datasets across multiple organs and modalities, with 11 popular networks trained on 3 paradigms, show the effectiveness and generality of Ctrl-GenAug. Furthermore, we conduct extensive empirical analysis demonstrating that Ctrl-GenAug can be effectively leveraged to improve diagnostic performance in underrepresented high-risk populations and out-domain robustness. We believe that Ctrl-GenAug can serve as a robust and practical data augmentation tool for various clinical scenarios.\n\nWe then discuss the limitations of Ctrl-GenAug. First, the denoising diffusion implicit model (DDIM) sampling process is time-consuming due to the large inference steps. This limits the clinical practicality of Ctrl-GenAug for fast and large-scale production. Second, in out-domain conditions, the proposed framework requires target domain data during generator training, which can be tough to obtain in advance in clinical practice. In future work, we will adopt fast-sampling strategies (e.g., AMED-Solver [88]) to better balance the trade-off between sampling time and sample quality. Besides, we will incorporate test-time adaptation methods [89], [90] to provide a more user-friendly tool for out-domain applications in clinical settings. Last, we will attempt to further extend the proposed method to segmentation and detection tasks."}]}