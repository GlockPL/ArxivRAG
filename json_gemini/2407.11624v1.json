{"title": "Rethinking Fair Graph Neural Networks from Re-balancing", "authors": ["Zhixun Li", "Yushun Dong", "Qiang Liu", "Jeffrey Xu Yu"], "abstract": "Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of deep learning, Graph Neural Networks (GNNs) have been widely used in dealing with non-Euclidean graph-structured data and gained a deeper understanding to help us perform predictive tasks, such as molecular property prediction and etc [6, 29, 54, 56, 60, 61]. However, potential bias in datasets will lead the neural networks to favor the privileged groups, for example, if the historical data reflects a bias towards loans for individuals from higher-income communities, the model may learn to associate high income with loan approval, leading to the unjustified denial of loans for applicants from lower-income communities even if they are creditworthy. Therefore, fairness in high-stake automatic decision-making systems, e.g., medication recommendation [45], fraud detection [30, 38], and credit risk prediction [58] have been receiving increasing attention in recent years.\nClose to the heels of the wide application of GNNs, there are various fair GNN approaches have been proposed to improve fairness without unduly compromising utility performance [1, 9, 11\u201314, 34, 55, 59]. The two most typical strategies of fair GNNs are imposing the fairness consideration as a regularization term during optimization [9, 18, 48, 55] or modifying the original training data with the assumption that fair data would result in a fair model [12, 44]. However, previous works focus on eliminating information about sensitive attributes but overlook the fact that due to attribute imbalance, underprivileged groups with fewer training samples are underrepresented compared to the privileged groups with more training samples. Inspired by recent work in maximizing worst-group-accuracy [20], we surprisingly observe that simple group-wise re-balancing methods (e.g., re-sampling and re-weighting) can easily achieve competitive or even superior results compared to existing state-of-the-art fair GNN models with no additional hyper-parameters (as shown in Figure 1). Data re-balancing methods are popular within the class imbalance literature, but we focus on balancing each group in this paper, which we refer to as group-wise re-balancing. However, these simple re-balancing methods have their own shortcomings on the imbalanced graph-structured datasets. (1) Re-sampling: Down- and over-sampling will mitigate imbalance distribution by dropping or duplicating training samples. However down-sampling will lose a lot of beneficial information from the training set, jeopardizing the utility of the model. While over-sampling simply repeats minority samples will cause an over-fitting problem and hard to connect adjacent edges of newly generated nodes because of non-iid characteristics of graphs. (2) Re-weighting: re-weighting methods apply penalties according to the quantity of groups and assign large weights to minor groups. However, up-weighing nodes in minor groups may also result in the over-fitting problem, and inevitably increase the false positive cases for major nodes [47].\nSeveral works attempt to alleviate the class imbalance in the node classification scenario [28, 32, 39, 47]. For instance, GraphENS [39] injects the whole synthesized ego-networks for minor class. TAM [47] designs a node-wise logit adjustment method, which adaptively adjusts the margin accordingly based on local topology. However they did not take into account sub-groups within the classes, and to the best of our knowledge, there has been no work addressing unfairness in GNNs from the perspective of re-balancing.\nTo address the above problems, we propose a novel fair GNN model, Fair Graph Neural Networks via re-Balancing, FairGB for short. Our proposed model can be divided into two modules: Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL). Specifically, we first select inter-domain (with the same target labels and different sensitive attributes) and inter-class (with different target labels and the same sensitive attributes) nodes for each training sample as counterexamples. Then we interpolate node attributes and neighbor distributions of counterfactual pairs and inject newly synthesized ego-networks to generate a balanced augmented graph. We have performed theoretical analysis over causal and statistical views, which serves as a solid mathematical foundation for the effectiveness of debiasing. Secondly, because the importance of each training sample varies, achieving balance solely based on quantity is not sufficient. To further improve fairness, we propose a re-weighting method, Contribution Alignment Loss, which can balance the contribution of each group according to the gradients. The weights can be flexibly combined with CNM, thus we can view FairGB as a hybrid model, where CNM is equivalent to re-sampling, CAL is a re-weighting strategy. They mutually reinforce each other, helping to alleviate the issues encountered by the simple re-balancing methods mentioned above. Our contributions can be listed as follows:\n\u2022 Preliminary Analysis. We find that simple re-balancing methods can easily achieve competitive or superior results compared to existing state-of-the-art fair GNN models. And we provide a new perspective to analyze the fairness in graph learning.\n\u2022 Algorithm Design. We propose a novel approach, namely FairGB, that can effectively improve performance via re-balancing methods with only one additional hyper-parameter. And we theoretically prove that our approach can achieve the debiasing effect.\n\u2022 Experimental Evaluation. We conduct extensive experiments, and the results demonstrate that FairGB achieves superior performance of utility and fairness. Meanwhile, we observe that the decision boundaries of target labels and sensitive attributes are roughly orthogonal, which indicates they are independent."}, {"title": "2 PRELIMINARY", "content": ""}, {"title": "2.1 Notations and Problem Statements", "content": "Given an attributed graph G = (V, A, X), where V = {v1, v2, ..., vN} is the set of nodes; A \u2208 \u211d^{N\u00d7N} is the adjacency matrix, N is the number of nodes, if vi and vj are connected, Aij = 1, otherwise Aij = 0; X = [x1, x2, ..., xN] \u2208 \u211d^{N\u00d7D} is the node feature matrix, each node vi is associated with a D-dimensional node feature vector xi. The low-dimensional representations of nodes Z = [z1, z2...., zN] \u2208 \u211d^{N\u00d7d} are derived from graph encoder g(\u00b7) : \u211d^{N\u00d7D} \u00d7 \u211d^{N\u00d7N} \u2192 \u211d^{N\u00d7d}. On the top of the graph encoder, there is a classifier head h(\u00b7) : \u211d^{N\u00d7d} \u2192 \u211d^{N\u00d7C} to obtain the probability of each class, where C is the number of classes. Combining graph encoder g(.) and classifier head h(), we can acquire graph model fe = g \u25e6 h(\u00b7), where e is the learnable parameters.\nIn fairness learning or debiased learning, each labeled sample is a triad, (v, y, s) \u2208 V \u00d7 Y \u00d7 S, with y being the ground-truth label and s being the sensitive attributes. If the nodes have the same target label y and sensitive attribute s, we define them as a demographic group Dy,s, and the quantity of the group is |Dy,s|. The goal of fairness learning is that the model will not be affected by the sensitive features, resulting in the bias of predicted results \u0177. Assume there is a binary classification task, target label y = {1,0}, sensitive attribute S = {1,0}. There are two corresponding metrics to evaluate fairness."}, {"title": "2.1.1 Demographic Parity", "content": "If the predicted result \u0177 is independent of sensitive attributes, i.e., \u0177 \u22a5 s, then we can consider demographic parity is achieved. The formula for this criterion is as follows:\n\u2119(\u0177 = 1|s = 0) = \u2119(\u0177 = 1|s = 1) (1)\nIf a model satisfies demographic parity, the acceptance rate of different protected groups is the same."}, {"title": "2.1.2 Equalized Odds", "content": "If the predicted results and sensitive attributes are independent conditional on the ground-truth label, i.e., \u0177 \u22a5 s | y, then we consider equalized odds is achieved. The formula for it is as follows:\n\u2119(\u0177 = 1 | s = 1, y = 1) = \u2119(\u0177 = 1 | s = 0, y = 1) (2)\nIf a model satisfies equalized odds, the TPR (True Positive Rate) and FPR (False Positive Rate) for the two protected groups are the same."}, {"title": "2.2 Graph Neural Networks", "content": "Most existing GNNs follow the message-passing paradigm which contains message aggregation and feature update, such as GCN [26] and GAT [53]. They generate node representations by iteratively aggregating information of neighbors and updating them with non-linear functions. The forward process can be defined as:\n$z_i^{(l)} = U^{(l)}(z_i^{(l-1)}, M(\\{z_j^{(l-1)}|v_j\\in \\mathcal{N}_i\\}))$ (3)\nwhere $z_i^{(l)}$ is the feature vector of node i in the l-th layer, and $\\mathcal{N}_i$ is a set of neighbor nodes of node i. M denotes the message passing function of aggregating neighbor information, U denotes the update function with central node feature and neighbor node features as input. By stacking multiple layers, GNNs can aggregate messages from higher-order neighbors."}, {"title": "3 METHODOLOGY", "content": "In this section, we will give a detailed description of FairGB. An illustration of its framework is shown in Figure 2. In the Counter-factual Node Mixup (CNM), we select counterfactual pairs for each training sample and conduct inter-domain and inter-class mixup with whole ego-networks. In the Contribution Alignment Loss (CAL), we further improve the group-wise balance by re-weighting each group according to the gradients. Next, we will provide a theoretical analysis and introduce the details of each module."}, {"title": "3.1 Counterfactual Node Mixup", "content": "Recently, the incorporation of causal learning techniques into GNNs has ignited a plethora of groundbreaking studies [15, 18, 21, 49]. This is attributed to the fact that addressing trustworthiness concerns is more effectively achieved by capturing the inherent causality in the underlying data, as opposed to merely relying on superficial correlations. In this work, we present a causal view of the union of the graph data generation and the GNNs' prediction process as a Structure Causal Model [42] (as shown in Figure 3(a)). We illustrate the causal relationships among six variables in the node classification problem: unobservable causal variable C, un-observable sensitive (bias) variable S, observable node attributes X, observable topology A, node embedding Z, and ground-truth label Y. In the graph data generation process, C \u2192 X \u2190 S and C \u2192 A \u2190 S demonstrate that two variables (causal variable C and sensitive variable S) construct two components of observable contextual subgraphs (node attributes X and topology A), which is different from i.i.d. data only consider attributes. X \u2192 Z \u2192 Y and A \u2192 Z \u2192 Y indicate existing GNNs produce representations and predictions based on observable contextual subgraphs. C \u2194 S denotes the spurious correlation between C and S.\nInspired by Fan et al. [15], we analyze the SCM according to d-connection theory [41] (two variables are dependent if they are connected by at least one unblocked path). Thus we can find three paths between sensitive variable S and group-truth label Y:\n\u2022 S \u2192 X \u2192 Z \u2192 Y and S \u2192 A \u2192 Z \u2192 Y: Because the existing graph neural networks make prediction for a node based on its contextual subgraph, sensitive variable S will influence the final prediction not only through node attributes X but also topology A [31]. This makes fairness in graph machine learning more complex compared to other modalities (e.g., images and languages). As a result, if we want to sever all unblocked paths between S and Y, we need to debias both X and A.\n\u2022 S \u2194 C \u2192 Y: we want to sever the connection between S and C, so we utilize inter-domain and inter-class mixup. Intuitively, we interpolate samples with the same target label but different sensitive attributes in inter-domain mixup, and then the model can focus on class-specific information and learn domain invariant features. Besides, we also interpolate samples with the same sensitive attribute and different target labels in inter-class mixup, which can smooth the decision surface and alleviate the dependency on bias information.\nThus, we have completed the proof of Theorem 1.\nAlthough some literature provided similar theoretical analysis based on the statistical view [43, 51], they did not conduct a detailed analysis of the debiasing mechanism of balanced and consistent bias distribution within each class.\nNext, we are going to introduce the detailed process of counterfactual node mixup. Specifically, for each training node vi, we randomly select a node vj that has the same target label and a different sensitive attribute with vi (yi = yj, si \u2260 sj) as an inter-domain counterexample. In the same way, we randomly select a node vj that has a different target label and the same sensitive attribute with vi (yi \u2260 yj, si = sj) as an inter-class counterexample. Here, we introduce a hyper-parameter \u03b7, which is responsible for controlling the ratio between two kinds of counterexamples. Then we perform linear interpolation on the node attributes:\n$x_{mix} = \\lambda x_i + (1 - \\lambda)x_j, \\quad y_{mix} = \\lambda y_i + (1 - \\lambda)y_j$, (11)\nwhere \u03bb\u2208 [0, 1] is the interpolation ratio that is sampled from a Beta distribution. After generating new node attributes, we need to insert these nodes into the original graph. According to the analysis above, we also perform mixup on the contextual subgraph structure. We define the neighbor distribution of node vi as \u2119\u2115(vi) where \u2119(vk|vi) = 1 if Aik = 1, and \u2119(vk|vi) = 0 otherwise. Then we directly interpolate the neighbor distribution of counterfactual pairs using the same \u03bb:\n$\\mathbb{P}_\\mathcal{N}(v_{mix}) = \\lambda \\mathbb{P}_\\mathcal{N}(v_i) + (1 - \\lambda)\\mathbb{P}_\\mathcal{N}(v_j)$, (12)\nThus we can obtain newly generated unbiased ego-networks. However, these ego-networks have very dense structures, which could violate the original degree distribution and result in the phenomenon of out-of-distribution [33, 50]. Therefore, we sample the neighbors according to the original degree distribution and inject newly generated unbiased ego-networks into the original graph to construct an augmented graph Gaug."}, {"title": "3.2 Contribution Alignment Loss", "content": "Although counterfactual node mixup is equivalent to re-sampling to some extent, varying levels of learning difficulty and the different positions of labeled nodes [5] lead to inconsistent contributions of training nodes. Simply balancing the group distribution in terms of quantity does not effectively address the problem. To further balance the contribution of each group and enhance fairness, we align the gradients of each group.\nSince FairGB first generates mixed nodes through counterfactual node mixup and calculates the loss on these mixed nodes, it is difficult for us to determine which group the mixed nodes actually belong to. However, we can acquire the contributions of mixed nodes to each group by rewriting the loss function. First, we claim that the objective loss function of counterfactual mixup is as follows:\n$L = \\mathbb{E}_{\\{v_i,v_j\\}} [l(f(v_{mix}), y_{mix})]$, (13)\n$= \\mathbb{E}_{\\{v_i,v_j\\}} [\\lambda l(f(v_{mix}), y_i) + (1 - \\lambda)l(f(v_{mix}), y_j)]$, (14)\nwhere l is Cross-Entropy loss. Then we can obtain two contributions of each mixed node according to the gradients:\nr_i = ||\u2207fl(f(vmix), yi)||1, rj = ||\u2207fl(f(vmix), yj)||1, (15)\nIn essence, ri and rj are the contributions generated from mixed two nodes vi and vj. We can easily identify which group are vi and vj belong to. Therefore, the contribution of each group Rt,b is the sum of sample contributions (both original samples and counterexamples), $R_{t,b} = \\sum_{v_i \\in \\mathcal{D}_{t,b}} r_i $. Based on $R_{t,b} $, we can compute the weight of each group $w_{t,b} = \\frac{\\sum_{j,k} R_{j,k}}{\\sum_k R_{t,k}}$ to balance the contributions, where B is the number of sensitive attributes. After obtaining the weights of each group, we can flexibly inject them into the objective loss function Eq. 14 to get the final Contribution Alignment Loss (CAL):\n$L_{CAL} = \\mathbb{E}_{\\{v_i,v_j\\}} [w_{y_i,s_i}\\lambda l(f(v_{mix}), y_i) + w_{y_j,s_j}(1 - \\lambda)l(f(v_{mix}), y_j)]$, (16)\nSo far, we have achieved an ingenious combination of counterfactual node mixup and contribution alignment. They can complement and promote each other. Specifically, counterfactual node mixup can sever the spurious correlation from the causal view but it does not guarantee a good balance between different groups. While contribution alignment loss is able to balance the contribution of each group based on gradients, but it will suffer the over-fitting problem, which can be mitigated by the newly generated unbiased ego-networks. The whole training procedure of FairGB is presented in Algorithm 1."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to investigate the effectiveness of our proposed model, and aim to answer the following research questions:"}, {"title": "4.1 Experimental settings", "content": ""}, {"title": "4.1.1 Real-World Datasets", "content": "We conduct experiments on three widely used real-world datasets, namely German Credit, Bail, and Credit Defaulter. The statistics of the datasets can be found in Table 1. The details of the datasets are as follows:\n\u2022 German Credit [2]: the nodes in the dataset are clients and two nodes are connected if they have a high similarity of the credit accounts. The task is to classify the credit risk level as high or low with the sensitive attribute \"gender\".\n\u2022 Bail [23]: these datasets contain defendants released on bail during 1990-2009 as nodes. The edges between the two nodes are connected based on the similarity of past criminal records and demographics. The task is to classify whether defendants are on bail or not with the sensitive attribute \"race\".\n\u2022 Credit Defaulter [58]: the nodes in the dataset are credit card users and the edges are formed based on the similarity of the payment information. The task is to classify the default payment method with the sensitive attribute \"age\"."}, {"title": "4.1.2 Baselines", "content": "We compare our proposed model with 9 representative and state-of-the-art methods in four categories, which include: (1) Vanilla graph neural networks: GCN [26] is widely used spectral GNN; GraphSAGE (SAGE for short) [19] is a method for inductive learning that leverages node feature information to generate embedding for nodes in large graph; GIN [57] is a graph-based neural network that can capture different topological structures by injecting the node's identity into its aggregation function. (2) Fair node classification methods: FairGNN [9] uses adversarial training to achieve fairness on graphs; EDITS [12] is a pre-processing method for fair graph learning. (3) Graph counter-factual fairness methods: NIFTY [1] simply performs a flipping on the sensitive attributes to get counterfactual data; CAF [18] is guided by causal analysis, which can select counterfactual from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations. (4) Simple re-balancing methods: Re-weighting (RW for short) up-weights the contribution of minority groups and down-weights the contribution of majority groups to the loss functions according to the quantity of groups. More concretely, the objective loss function is $L = \\sum_{i=1}^N \\frac{1}{|D_{y_i,s_i}|} l (f (v_i), y_i)$. Over-sampling (OS for short) repeatedly samples minority group samples until the number of each group data reaches the maximum number of group data. We duplicate the edges of the original node when adding an oversampled node to the original graph. Following the setting of CAF [18], we also use SAGE as the model backbone except for GCN and GIN."}, {"title": "4.1.3 Evaluation Metrics", "content": "We regard AUC, F1 score, and accuracy as utility metrics. For fairness metrics, we use statistical parity (SP) Asp and equal opportunity (EO) \u2206eo, a smaller fairness metric indicates a fairer model decision."}, {"title": "4.1.4 Implementation details", "content": "For German, Bail, and Credit datasets, we follow train/valid/test split in [1]. The hyper-parameters used in experiments follow the source codes or are searched by the grid search method, and we use the Adam optimization algorithm [25] to train all the models. Specifically, FairGB only has one additional hyper-parameter \u03b7, and it is searched from {0, 0.1, 0.2, . . ., 0.8, 0.9, 1}. All the models are implemented in PyTorch [40] version 2.0.1 with PyTorch Geometric [17] version 2.3.1."}, {"title": "4.2 RQ1: Performance comparison", "content": "To comprehensively understand the effectiveness of FairGB, we conduct node classification on three widely used datasets. The experimental results of utility and fairness of each model are shown in Table 2. From the Table 2, our observations can be threefold: (1) We can observe that simple re-balancing methods achieve satisfactory results in both utility and fairness on three datasets. They can obtain competitive or even superior performance compared to carefully designed fair GNN models. (2) FairGB consistently achieves the best performance on the utility-fairness trade-off on all datasets. We use the average rank of three utility metrics and two fairness metrics to understand the performance of the trade-off. Our model ranks 2.47 and the runner-up model ranks 3.27. Our model outperforms all four categories of baselines, which shows the effectiveness of our model. (3) We find that OS and FairGB, on bail and credit datasets, do not sacrifice utility while improving fairness. For instance, on the bail dataset, FairGB improves the AUC by 4.50%, the F1 by 7.83%, and Acc by 4.54% compared to the vanilla model. And on the credit dataset, FairGB improves F1 by 4.19%, and Acc by 4.70% compared to the vanilla model. We speculate that this may be due to two reasons: (i) Balance of contributions from each group leads to a significant improvement in the accuracy of the worst group; (ii) the re-balancing strategy enhances the model's generalization, resulting in an overall improvement in classification performance."}, {"title": "4.3 RQ2: Ablation study", "content": "To answer RQ2 and verify the effectiveness of our proposed FairGB, we construct two variants of FairGB: (1) without Contribution Alignment Loss but conduct node mixup within counterfactual pairs (called FairGB w/o CAL); (2) without Counterfactual Node Mixup but assigns weights based on the group contribution (called FairGB w/o CNM). Table 3 demonstrates the performance of the vanilla model, two variants, and FairGB. First, we observe that two variants perform worse than FairGB in the trade-off of utility and fairness, which proves the effectiveness of each component and the rationality of the combination. Second, we find that FairGB w/o CNM consistently achieves better fairness compared to FairGB w/o CAL, which indicates the importance of re-balancing in fair graph learning. However, as mentioned above, re-weighting methods will encounter over-fitting problems, and counterfactual node mixup will generate new samples per epoch, which could mitigate overfitting. Third, while both modules effectively enhance fairness, the impact on utility varies across different datasets for each variant. For example, FairGB w/o CAL achieves high utility on the bail dataset but does not show improvement on the credit dataset, whereas FairGB w/o CNM exhibited the opposite trend. We speculate that this may be due to the different sizes of labeled nodes in the two datasets. The bail dataset has a small number of training samples (only 100 nodes are labeled), leading to over-fitting issues with re-weighting. Mixup, on the other hand, is beneficial for training with few labels. In the credit dataset, there are a lot of training samples (4000 nodes are labeled), and the phenomenon is just the opposite. However, we can observe that FairGB competently incorporates the strengths of each module, demonstrating that the two modules can mutually reinforce each other."}, {"title": "4.4 RQ3: Visualization", "content": "In order to answer the RQ3, we visualize the learned node embeddings on the bail dataset to better understand the mechanism of FairGB. We compare our FairGB with the vanilla GNN model (i.e. SAGE), and then use 16-dimensional output embedding of the encoder. Subsequently, we use t-SNE [52] to map the 16-dimensional embedding into 2-dimensional space for visualization. We only select samples in the test set for better visibility. The results are shown in Figure 4. We plot two figures for each model, one concerns the target labels, and the other one concerns the sensitive attributes. Comparing Figure 4(a) and Figure 4(c), we can observe that the representations of nodes from different classes have smaller overlapping regions in FairGB. This confirms the improvement of FairGB on the three utility metrics. Next, since SAGE already exhibits good fairness on the bail dataset (as shown in Table 2), we can observe that the node representations of the two sensitive attributes are mixed together in Figure 4(b). Because FairGB is designed from a re-balancing perspective, the contributions from each group are relatively balanced, making the node representations of the two sensitive attributes distinguishable in FairGB. However, we can observe that the classification boundary for the target label is orthogonal to the classification boundary for the sensitive features. This explains how FairGB achieves excellent fairness."}, {"title": "4.5 RQ4: Generalization for different encoders", "content": "To answer the RQ4, we test the generalization ability of our FairGB by deploying it on three different graph encoders: SAGE, GCN, and GIN. The utility and fairness performance are demonstrated in Figure 5. We can observe that FairGB is able to maintain or even surpass the vanilla models in both AUC and F1-score metrics, which is thanks to the re-balancing strategy. In the fairness metrics, we find that FairGB effectively reduces Asp and \u2206eo compared to the vanilla models across all datasets. These results indicate that FairGB has strong generalization capabilities for different graph encoders, making it flexible for use in real-world applications."}, {"title": "4.6 RQ5: Parameter sensitive analysis", "content": "One major advantage of FairGB is that it only has one additional hyper-parameter \u03b7 to control the trade-off between inter-domain mixup and inter-class mixup. To figure out the RQ5, we conduct hyper-parameter sensitive analysis on three datasets in terms of three utility metrics and two fairness metrics. As shown in Figure 6, we vary \u03b7 from 0 to 1 and present the performance of FairGB. When \u03b7 equals 0, the model only performs inter-domain mixup, and when \u03b7 equals 1, the model only performs inter-class mixup. Recall our above analysis, when the training set is class-balanced, interdomain mixup is equivalent to group-wise balance re-sampling. But according to our causal view, we demonstrate that plain interdomain mixup can not effectively mitigate bias, so we conduct inter-class mixup and add a hyper-parameter \u03b7 to control the tradeoff. We can observe that when \u03b7 equals 0 or 1, FairGB can not achieve the best performance. However, when \u03b7 is within the range of 0.3 to 0.8, the model is not sensitive to parameters and FairGB achieves the roughly best performance when \u03b7 equals 0.5. Therefore, we simply fix the \u03b7 to 0.5 in most experiments."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 Fairness in Graph Neural Networks", "content": "Fairness is a widely researched issue in the field of machine learning [3, 4, 7, 35]. Most works in deep learning exclusively focus on optimizing the model utility while ignoring the fairness of the decisions, such as group fairness [4, 10], individual fairness [37, 46], and counterfactual fairness [8, 27]. Since GNNs inherit characteristics from machine learning, they can also encounter fairness issues, which makes it challenging to deploy GNNs in high-risk applications.\nNowadays, there are various existing works that try to improve the fairness of GNNs, and they can be roughly categorized into pre-, in-, and post-processing methods. Pre-processing methods modify the original training data with the assumption that fair data would result in fair models. Fairwalk [44] and Crosswalk [24] choose each group of neighbor nodes with an equal chance and bias random walks to cross group boundaries. EDITS [12] designs a model-agnostic method to modify the attribute and structure for fair GNNs training. In-processing approaches aim to mitigate unfairness during the training process by directly modifying the learning algorithm, and they can be divided into three parts: adversarial-based, augmentation-based, and message-passingbased. Adversarial-based approaches train fair GNNs by preventing an adversary from correctly predicting sensitive attributes from the learned node representations. FairGNN [9] and FairVGNN [55] enforce the model to generate fair outputs with adversarial training through the min-max objective. Augmentation-based approaches generate counterfactual views and minimize the discrepancy with the original view. NIFTY [1] flips sensitive attributes for each node to obtain the counterfactual view. Message-passing-based approaches improve fairness through the view of optimization problem with smoothness regularization. FMP [22] proposes a fair message-passing framework by considering graph smoothness and fairness objectives. The post-processing techniques directly calibrate the classifier's decisions at inference time. POSTPROCESS [36] updates model predictions based on a black-box policy to minimize differences between demographic groups."}, {"title": "5.2 Re-balancing in Graph Neural Networks", "content": "Due to the GNNs inheriting the character of deep neural networks, GNNs perform with biases toward the majority classes when training on imbalanced datasets. To overcome this challenge, classimbalanced learning on graphs has emerged as a promising solution that combines the strengths of graph representation learning and class-imbalanced learning. A great branch of these methods is oversampling minority nodes by data augmentation to balance the skew label distribution. GraphSMOTE [62] leverages representative data augmentation method (i.e., SMOTE [16]) and proposes edge predictor to fuse augmented nodes into the original graph. GraphENS [39] discovers neighbor memorization phenomenon in imbalanced node classification, and generates minority nodes by synthesizing ego-networks according to similarity. GraphSHA [28] only synthesizes harder training samples and proposes SEMIMIXUP to block message propagation from minority nodes to neighbor classes by generating connected edges from 1-hop subgraphs. Another branch of class-imbalanced learning on graphs is topology-aware logit adjustment. TAM [47] adjusts margins node-wisely according to the extent of deviation from connectivity patterns to avoid inducing false positives of minority nodes. Different from the methods mentioned above that focus on class imbalance, our paper attempts to address the issue of group imbalance. We emphasize that group imbalance is a significant source of unfairness in GNNs and, as a result, design group re-balancing strategies to enhance the fairness of graph learning."}, {"title": "6 CONCLUSION", "content": "In this paper, we provide a new perspective to address the unfairness in graph neural networks. We find that group distribution imbalance is a significant source of bias and simple re-balancing methods (e.g., re-weighting and re-sampling) can easily achieve competitive or even superior performance compared to existing state-of-the-art fair GNNs. To this end, we propose FairGB which consists of two modules to automatically balance the contributions of each group. Guided by theoretical analysis, we conduct linear interpolation between counterfactual node pairs to effectively mitigate bias. In order to further enhance fairness, we propose contribution alignment loss based on gradients and flexibly combine two modules. Experimental results demonstrate the effectiveness of our proposed FairGB in achieving state-of-the-art performance on three real-world datasets. Future research directions can delve more into understanding unfairness from a re-balancing perspective and better integration of the graph properties to mitigate bias."}, {"title": "ACKNOWLEDGE", "content": "This work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and No. 14205520."}]}