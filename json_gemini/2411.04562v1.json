{"title": "Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning", "authors": ["Marvin Alles", "Philip Becker-Ehmck", "Patrick van der Smagt", "Maximilian Karl"], "abstract": "In offline reinforcement learning, a policy is learned using a static dataset in the absence of costly feedback from the environment. In contrast to the online setting, only using static datasets poses additional challenges, such as policies generating out-of-distribution samples. Model-based offline reinforcement learning methods try to overcome these by learning a model of the underlying dynamics of the environment and using it to guide policy search. It is beneficial but, with limited datasets, errors in the model and the issue of value overestimation among out-of-distribution states can worsen performance. Current model-based methods apply some notion of conservatism to the Bellman update, often implemented using uncertainty estimation derived from model ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP) which learns a generative model of the joint distribution of observations and actions. We cast policy learning as a constrained objective to always stay within the support of the latent action distribution, and use the generative capabilities of the model to impose an implicit constraint on the generated actions. Thereby eliminating the need to use additional uncertainty penalties on the Bellman update and significantly decreasing the number of gradient steps required to learn a policy. We empirically evaluate C-LAP on the D4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.", "sections": [{"title": "1 Introduction", "content": "Deep-learning methods are widely used in applications around computer vision and natural language processing, related to the fact that datasets are abundant. But when used for control of physical systems, in particular with reinforcement learning, obtaining data involves interaction with an environment. Learning through trial-and-error and extensive exploration of an environment can be done in simulation, but hard to achieve in real world scenarios [1, 2, 3]. Offline reinforcement learning tries to solve this by using pre-collected datasets eliminating costly and unsafe training in real-world environments [4, 5, 6].\nUsing online reinforcement learning methods in an offline setting often fails. A key issue is the distributional shift: the state-action distribution of the offline dataset, driven by the behavior policy, differs from the distribution generated by a learned policy. This leads to actions being inferred for states outside the training distribution. Therefore, value-based methods are prone to overestimating values due to evaluating policies on out-of-distribution states. This leads to poor performance and unstable training because of bootstrapping [6, 7, 8]. Offline reinforcement learning methods address"}, {"title": "2 Preliminaries", "content": "We consider a partial observable Markov decision process (POMDP) defined by $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{O}, T, R, \\Omega, \\gamma)$ with $\\mathcal{S}$ as state space, $\\mathcal{A}$ as action space, $\\mathcal{O}$ as observation space, $s \\in \\mathcal{S}$ as state, $a \\in \\mathcal{A}$ as action, $o \\in \\mathcal{O}$ as observation, $T : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ as transition function, $R : \\mathcal{S} \\rightarrow \\mathbb{R}$ as reward function, $\\Omega : \\mathcal{S} \\rightarrow \\mathcal{O}$ as emission function and $\\gamma \\in (0, 1]$ as discount factor. The goal is to find a policy $\\pi : \\mathcal{O} \\rightarrow \\mathcal{A}$ that maximizes the expected discounted sum of rewards $\\mathbb{E}[\\sum_{t=1}^{N} \\gamma^t r_t]$ [27].\nIn online reinforcement learning, an agent iteratively interacts with the environment $\\mathcal{M}$ and optimizes its policy $\\pi$. In offline reinforcement learning, however, the agent cannot interact with the environment and must refine the policy using a fixed dataset $\\mathcal{D} = \\{(o_{1:T}, a_{1:T}, r_{1:T})_i^N\\}$. Therefore, the agent must understand the environment using limited data to ensure the policy maximizes the expected discounted sum of rewards when deployed [6]. Auto-regressive model-based offline reinforcement learning tries to learn a parametric function to estimate the transition dynamics $T$. The transition dynamics model is then used to generate additional trajectories which can be used to train a policy. The majority of these approaches learn a dynamics model directly in observation space $T_\\Theta(o_t | o_{t-1}, a_{t-1})$ [16, 17, 18, 19, 28], while others use a latent dynamics model $T_\\Theta(s_t | s_{t-1}, a_{t-1})$ [20, 21]."}, {"title": "3 Constrained Latent Action Policies", "content": "A main issue in offline reinforcement learning is value overestimation, which we address by ensuring the actions generated by the policy stay within the dataset's action distribution. Unlike previous model-based methods, we formulate the learning objective as a generative model of the joint distribution of states and actions. We do this by combining a latent action space with a latent dynamics model. Next, we use the generative properties of the action space to constrain the policy to the dataset's action distribution. A general outline of our method, Constrained Latent Action Policies (C-LAP), is shown in Appendix B. It starts with learning a generative model, followed by actor-critic agent training on imagined trajectories, similar to the methods in [29, 30, 20, 21].\nGenerative model Model-based offline reinforcement learning requires learning a model that is accurate in areas with low data coverage but also generalizes well. Therefore, it's crucial to balance"}, {"title": "4 Experiments", "content": "In the next section, we evaluate the effectiveness of our approach. It is divided into three parts: first, we assess the performance using standard benchmarks; then, we study how different design choices affect value overestimation; lastly, we analyze the influence of the support constraint parameter. We additionally provide the final performances in two tables in Appendix E.\nWe limit our benchmark evaluation to the most relevant state-of-the-art offline reinforcement learning methods to answer the following questions: 1) How do latent action state-space models compare to state-space models? 2) How comparable are model-free methods focusing on latent action spaces to latent action state-space models? 3) Does C-LAP suffer from value overestimation? 4) How does the support constraint affect the performance? 5) How does the performance differ between visual observations and observations with low-dimensional features? To focus on the latter, we separately evaluate the performance on low-dimensional feature observations using the D4RL benchmark [26], and on image observations using the V-D4RL benchmark [20]."}, {"title": "4.1 Benchmark results", "content": "D4RL Since most offline model-based reinforcement learning methods are designed for observations with low-dimensional feature observations, there exist many options for comparison. We make a selection to include the most relevant methods focusing on latent actions and auto-regressive model-based reinforcement learning. Therefore, we include the following methods: PLAS, which is a model-free method using a latent action space [10]. MOPO, a probabilistic ensemble-based offline model-based reinforcement learning method using a modification to the Bellman update to penalize high variance in next state predictions [16]. And MOBILE, which is similar to MOPO, but penalizes high variance in value estimates instead [18]. We compare the algorithms on three different locomotion environments, namely halfcheetah, walker2d and hopper, with four datasets (medium-replay, medium, medium-expert, expert) each and the antmaze navigation environment with four datasets (umaze, umaze-diverse, medium-play, medium-diverse). The results, shown in Figure 4, display the mean and standard deviation of normalized returns over four seeds during the phase of policy training, with steps denoting gradient steps. The dashed lines indicate the asymptotic performance for MOPO and MOBILE. A detailed summary of all implementation details is provided in the Appendix D.\nWhen comparing C-LAP to PLAS, we find that learning a joint generative model of actions and observations outperforms a generative model of only actions when used with actor-critic reinforcement learning. Both methods can use the generative nature of the model to speed up policy learning, which becomes especially clear in the results on all locomotion expert and medium-expert datasets. Compared to MOPO and MOBILE, C-LAP shows a superior or comparable performance on all datasets except halfcheetah-medium-replay-v2, halfcheetah-medium-v2 and hopper-medium-v2. Especially outperforming on the antmaze environment, where MOPO and MOBILE fail to solve the task for any of the considered datasets. The asymptotic performance of MOBILE on locomotion environments sometimes exceeds the results of C-LAP, but needs three times as many gradient steps.\nOverall the results indicate that latent action state-space models with constrained latent action polices not only match the state-of-the-art on observations with low-dimensional features as observations, but also jump-start policy learning by using the action decoder to sample actions that lead to high rewards already after the first gradient steps: If the dataset is narrow, generated actions when sampling from the latent action prior will fall into the same narrow distribution. For instance, in an expert dataset, sampled actions will also be expert-level actions. During policy training, instead of sampling from this prior, we restrict the support of the policy dependent on the latent action prior. Thus, sampled latent actions from the policy will always be decoded to fall into the dataset's action distribution. So even a randomly initialized policy in the beginning of the training can generate a high reward by using the latent action decoder. This effect is especially prominent in narrow datasets such as expert datasets.\nV-D4RL There are currently few auto-regressive model-based reinforcement learning methods that specifically target visual observations, with none emphasizing latent actions. In our evaluation, we include LOMPO [21] and Offline DV2 [20]. Both methods use a latent state space model and an uncertainty penalized reward. However the specifics of the penalty calculations are different: while LOMPO uses standard deviation of log probabilities as penalty, Offline DV2 uses mean disagreement. Additionaly, LOMPO trains an agent on a mix of real and imagined trajectories with an off-policy actor-critic approach, whereas Offline DV2 exclusively trains on imagined trajectories and back-propagates gradients through the dynamics model. Further implementation details are included in Appendix D.\nC-LAP demonstrates superior performance across all datasets, especially significant on cheetah-run-medium_expert, walker-walk-medium_expert and walker-walk_expert. Datasets with a large"}, {"title": "4.2 Value overestimation", "content": "Limiting value overestimation plays a central role in offline reinforcement learning. To evaluate the effectiveness of C-LAP, we report value estimates alongside normalized returns on all walker2d datasets in Figure 6. A similar analysis for all considered baselines is provided in Appendix G. To further analyze the influence of different action space design choices, we include the following ablations: a variant no constraint, which does not formulate policy optimization as constrained objective, but uses a Gaussian policy distribution to potentially cover the whole Gaussian latent action space; and a variant no latent action, which does not emphasize latent actions, but uses a regular state-space model as in Dreamer [29]. Besides that, we added dashed lines to indicate the dataset's average return and average maximum value estimate. The no latent action variant fails to learn\nan effective policy: normalized returns are almost zero and the dataset's reference returns remain unattained; value estimates are significantly exceeding the dataset's reference values, indicating value overestimation. The no constraint variant can use the generative action decoder to limit generated actions to the dataset's action distribution, but the Gaussian policy is free to move to regions which are unlikely under the action prior. Thus, nullifying the implicit constraint imposed by the action decoder, resulting in collapsing returns and value overestimation. Only C-LAP achieves a high return and generates value estimates which are close to the dataset's reference. The value estimates on walker2d-medium-replay-v2 are higher than the dataset's reference, as the agent's performance is also exceeding the reference performance. The results confirm the importance of limiting value overestimation in offline reinforcement learning, and demonstrate that constraining latent action policies can be an effective measure for achieving this."}, {"title": "4.3 Support constraint parameter", "content": "To evaluate the influence of the support constraint parameter $\\tilde{\\epsilon}$ on the performance of C-LAP, we perform a sensitivity analysis across all walker2d datasets (Figure 7). Except for the more diverse medium-replay-v2 dataset, adjusting $\\tilde{\\epsilon}$ from 0.5 to 3.0 only has a minor impact on the achieved return. However, when choosing an unreasonable large value such as $\\tilde{\\epsilon} = 10.0$ or removing the constraint altogether (Figure 6), we observe a collapse during training. This highlights a key insight: constraining the policy to the support of the latent action prior is essential. And in many cases, using a smaller support region closer to the mean (small $\\epsilon$) proves sufficient."}, {"title": "5 Related Work", "content": "Offline reinforcement learning methods fall into two groups: model-free and model-based. Both types aim to tackle problems like distribution shift and value overestimation. This happens because the methods use a fixed dataset rather than learning by interacting with the environment.\nModel-free Current model-free methods typically work by limiting the learned policy or by regularizing value estimates. TD3+BC [11] adds a behavior cloning regularization term to the policy update objective to enforce actions generated by the policy to be close to the dataset's action distribution. Similarly, SPOT [9] includes a regularization term in the policy update, derived from a support constraint perspective. It also uses a conditional variational auto-encoder (CVAE) to estimate the behavior distribution. Following a comparable intention, BEAR [8] constraints the policy to the support of the behavior policy via maximum mean discrepancy. BCQ [7] and PLAS [10] use a CVAE similarly but don't use a regularization term. Instead, they constrain the policy implicitly by making the generative model part of the policy. Beside these methods, many other approaches exist, with CQL [12] and IQL [14] being some of the most well-known. CQL uses a conservative policy update by setting a lower bound on value estimates to prevent overestimation, while IQL avoids out-of-distribution values by using a modified SARSA-like objective in combination with expectile regression to only use state-action tuples contained in the dataset.\nModel-based Model-based offline reinforcement learning methods learn a dynamics model to generate samples for policy training. This basically converts offline learning to an online learning problem. Model-based methods mainly address model errors and value overestimation by using"}, {"title": "6 Conclusion", "content": "We present C-LAP, a model-based offline reinforcement learning method. To tackle the issue of value overestimation, we first propose an auto-regressive latent-action state space model to learn a generative model of the joint distribution of observations and actions. Second, we propose a method for policy training to stay within the dataset's action distribution. We explicitly parameterize the policy depending on the latent action prior and formulate policy learning as constrained objective similar to a support constraint. We find that C-LAP significantly speeds-up policy learning, is competitive on the D4RL benchmark and especially outperforms on the V-D4RL benchmark, raising the best average score across all dataset's from previously 31.5 to 58.8.\nLimitations Depending on the dataset and environment the effectiveness of C-LAP differs: Datasets which only contain random actions are challenging for learning a generative action model, thus we do not include them in our evaluation. The effect of jump-starting policy learning with the latent action decoder to already achieve high rewards in the beginning of policy training is prominent in narrow datasets, but less effective for diverse datasets. While training the model of C-LAP does not require additional gradient steps, it still takes more time compared to LOMPO [21] and Offline DV2 [20] as the latent action state-space model is more complex than a usual latent state-space model."}, {"title": "A ELBO derivation", "content": "We start with Equation (1) to jointly model the dataset's observation and action distribution\n$\\log p(o_{1:T}, a_{1:T}) = \\log \\int p(o_{1:T}, a_{1:T} | s_{1:T}, u_{1:T})p(s_{1:T}, u_{1:T}) ds du,$\nand the defintions in Equation (3), namely:\n$p(o_{1:T}, a_{1:T} | s_{1:T}, u_{1:T}) = \\prod_{t=1}^{T} p(o_t | s_t)p(a_t | s_t, u_t),$\n$p(s_{1:T}, u_{1:T}) = \\prod_{t=1}^{T} p(u_t | s_t)p(s_t | s_{t-1}, u_{t-1}).$\nWe introduce the inference model\n$q(s_t, u_t | s_{t-1}, a_{t-1}, a_t, o_t) = q(s_t | s_{t-1}, a_{t-1}, o_t)q(u_t | s_t, a_t)$\nand by replacing $p(o_{1:T}, a_{1:T} | s_{1:T}, u_{1:T})$ and $p(s_{1:T}, u_{1:T})$ in Equation (13) we get:\n$\\log p(o_{1:T}, a_{1:T}) = \\log \\int \\prod_{t=1}^{T} p(o_t | s_t)p(a_t | s_t, u_t)p(u_t | s_t)p(s_t | s_{t-1}, u_{t-1}) ds du.$\nWe include the inference model and resolve using Jensen's Inequality:\n$\\log p(o_{1:T}, a_{1:T}) = \\log \\mathbb{E}_{q} [\\prod_{t=1}^{T} \\frac{p(o_t | s_t)p(a_t | s_t, u_t)p(u_t | s_t)p(s_t | s_{t-1}, u_{t-1})}{q(s_t, u_t | s_{t-1}, a_{t-1}, a_t, o_t)}]$\n$= \\log \\mathbb{E}_{q} [\\prod_{t=1}^{T} \\frac{p(o_t | s_t)p(a_t | s_t, u_t)}{q(s_t | s_{t-1}, a_{t-1}, o_t) q(u_t | s_t, a_t)} \\frac{p(s_t | s_{t-1}, u_{t-1})}{q(s_t | s_{t-1}, a_{t-1}, o_t)} \\frac{p(u_t | s_t)}{q(u_t | s_t, a_t)}]$\n$= \\mathbb{E}_{q} [\\sum_{t=1}^{T} \\log p(o_t | s_t)p(a_t | s_t, u_t) + \\log \\frac{p(s_t | s_{t-1}, u_{t-1})}{q(s_t | s_{t-1}, a_{t-1}, o_t)} + \\log \\frac{p(u_t | s_t)}{q(u_t | s_t, a_t)}]$\n$= \\sum_{t=1}^{T} [\\mathbb{E}_{q}[\\log p(o_t | s_t)p(a_t | s_t, u_t)]$\n$- KL(q(s_t | s_{t-1}, a_{t-1}, o_t) || p(s_t | s_{t-1}, u_{t-1})$\n$- KL(q(u_t | s_t, a_t) || p_\\Theta(u_t | s_t))]$\n$=: -L_{ELBO}(o_{1:T}, a_{1:T}).$"}, {"title": "B Algorithm", "content": "We provide the general algorithm of C-LAP."}, {"title": "C Computational Resources", "content": "We use a different hardware setup for experiments with visual observations and experiments with low-dimensional features as observations. We run all experiments on a shared local cluster. C-LAP experiments with visual observations take around 10 hours on a RTX8000 GPU and experiments with low-dimension feature observations around 11 hours on a A100 GPU. We aim to execute most of our code on GPU and parallelize our implementation. Environment evaluations represent a bottleneck as they require execution on CPU. Overall, it takes around 70 combined GPU days to reproduce the benchmark results of all methods. This does not include the compute required for the evaluation of preliminary implementations or hyper-parameter settings."}, {"title": "D Implementation Details", "content": "We implement all methods in JAX [33] using Equinox [34]. We provide the hyper-parameters of C-LAP in Table 2 and the constraint values used for the D4RL benchmark in Table 3 and for the V-D4RL benchmark in Table 4. In general we consider constraint values in the range $\\tilde{\\epsilon} \\in \\{0.5, 1.0, 2.0, 3.0\\}$.\nWe implement MOPO and MOBILE following [18] and use the respective hyper-parameters provided in the publication. For MOPO, we select the max-aleatoric version. As no hyper-parameters are provided for the expert datasets of the D4RL benchmark we sweep through the range specified in Table 1 and use the one selected in the table. For the antmaze environment we additionally evaluate"}, {"title": "F Action distribution", "content": "To evaluate the claim that latent actions generated by the latent action prior are close to the dataset's action distribution we use the following approach: We randomly select one trajectory from the hopper-expert-v2 dataset and employ k-nearest neighbors to identify the 20 nearest observations and their corresponding actions within the whole dataset for each step. We then sample from the action prior and decoder to generate actions based on the nearest observations at each step. Thereafter, we split the selected trajectory into sections from leaving the ground to touching the ground and"}, {"title": "G Value overestimation for the considered baselines", "content": "We further report value estimates alongside normalized returns on all walker2d datasets for the considered baselines (Figure 9). As they estimate Q-values, we calculate the corresponding value estimates by averaging over 10 actions sampled from their respective policies. MOPO and MOBILE have a low value estimate, which can be attributed to the incorporated uncertainty penalties. PLAS's value estimates are only stable for walker2d-expert-v2 datasets, but collapse for the other considered datasets. Overall, it seems that value overestimation is not the cause for degrading performance for these methods."}]}