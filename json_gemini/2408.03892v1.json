{"title": "MORTAR: A Model-based Runtime Action Repair Framework for AI-enabled Cyber-Physical Systems", "authors": ["Renzhi Wang", "Zhehua Zhou", "Jiayang Song", "Xuan Xie", "Xiaofei Xie", "Lei Ma"], "abstract": "Cyber-Physical Systems (CPSs) are increasingly prevalent across various industrial and daily-life domains, with applications ranging from robotic operations to autonomous driving. With recent advancements in artificial intelligence (AI), learning-based components, especially AI controllers, have become essential in enhancing the functionality and efficiency of CPSs. However, the lack of interpretability in these AI controllers presents challenges to the safety and quality assurance of AI-enabled CPSs (AI-CPSs). Existing methods for improving the safety of AI controllers often involve neural network repair, which requires retraining with additional adversarial examples or access to detailed internal information of the neural network. Hence, these approaches have limited applicability for black-box policies, where only the inputs and outputs are accessible during operation. To overcome this, we propose MORTAR, a runtime action repair framework designed for AI-CPSs in this work. MORTAR begins by constructing a prediction model that forecasts the quality of actions proposed by the AI controller. If an unsafe action is detected, MORTAR then initiates a repair process to correct it. The generation of repaired actions is achieved through an optimization process guided by the safety estimates from the prediction model. We evaluate the effectiveness of MORTAR across various CPS tasks and AI controllers. The results demonstrate that MORTAR can efficiently improve task completion rates of AI controllers under specified safety specifications. Meanwhile, it also maintains minimal computational overhead, ensuring real-time operation of the AI-CPSs.", "sections": [{"title": "1 Introduction", "content": "Cyber-Physical Systems (CPSs) are hybrid systems that integrate computing units and mechanical components. The digital and physical elements actively interact to perform target tasks collaboratively in complex environments. In recent years, research and development of CPSs have grown rapidly across various domains, including robotics Nikolakis et al. (2019), autonomous driving Jia et al. (2021) and power systems Yohanandhan et al. (2020). With the nature of interactive cyber and physical interactions, CPSs are considered to initiate leading-edge performance compared to traditional embedded systems with improved efficiency, reliability and robustness Castiglioni et al. (2024); Tabuada et al. (2014); Rungger and Tabuada (2015).\nThese AI-powered systems are usually referred to as AI-enabled CPSs (AI-CPSs). Among the applications development, a key focus is how to use Deep Reinforcement Learning (DRL) to create control policies to better handle the complex system dynamics of modern CPSs Plaat et al. (2020);\nLiu et al. (2019).\nIn contrast to the conventional control policy design, which typically relies on control-theoretical concepts, such as Proportional Integral Derivative (PID) Ang et al. (2005) and Model Predictive Control (MPC) Morari et al. (1988), DRL-based controllers learn control policies through interactions with the environment and thus do not require the explicit solving of complex system dynamics. Therefore, such AI-CPSs\u00b9 generally exhibit enhanced performance and adaptability in complex systems and possess high generalizability across various application scenarios Radanliev et al. (2021);\nSong et al. (2022); Lv et al. (2021); Xie et al. (2023); Song et al. (2023); Hu et al. (2023).\nDespite their extensive capabilities, learning-based control policies still meet several distinctive challenges. For instance, the lack of interpretability and formal verification could lead these control policies to produce faulty actions in specific scenarios. The failure may be caused by biased training data, insufficient training, or noisy operations. Furthermore, unlike conventional control systems, providing safety guarantees to learning-based control policies using expert knowledge or mathematical analysis is often challenging. As a result, it is extremely complex to ensure the safety of\nAI-CPSs Li et al. (2020).\nTo address this issue, previous works propose various solutions, and a common technical route among these approaches to enhance the safety of learning-based control policies is through neural network (NN) repair Yang et al. (2022); Lyu et al. (2023), which aims to enhance NN performance via adversarial training or parameter alternation. Existing repair strategies mainly focus on safeguarding the decision-making logic during training or augmenting the training data by removing unreliable samples that could lead to faulty control policies Lyu et al. (2023). However, these approaches typically apply only to white-box policies where the internal states of the neural networks are known and modifiable. Consequently, these approaches are hard to apply to black-box policies that involve networks with unknown details or are immutable. Moreover, these retraining methods often also incur huge computational costs. Given that CPSs are widely deployed in safety-critical settings, any unsafe control command can lead to dangerous behaviour, which can cause failure and even harm. These challenges greatly hinder the further development and deployment of AI-CPSS across various domains.\nIdeally, an effective repair scheme for AI controllers should be compatible with black-box policies, facilitate real-time action correction, and be easily transferable to various CPSs at low cost. Inspired by this, we propose in this work MORTAR, a model-based runtime action repair framework for AI-CPSs. Instead of requiring detailed neural network information, MORTAR utilizes only the in-"}, {"title": "2 Background", "content": "In this section, we provide essential background knowledge about AI-CPSs and Signal Temporal\nLogic (STL), which is commonly used to define system safety specifications."}, {"title": "2.1 AI-enabled Cyber-Physical Systems", "content": "CPSs integrate computational elements with physical processes Lee (2015); Alguliyev et al. (2018).\nA typical CPS consists of four major components (see Fig. 2): a physical plant E, a network of sensors, an actuator and a control unit Song et al. (2016, 2022); Afzal et al. (2020). At a given timestep t, the controller receives the current system state st through sensors and, in response to an input signal i, formulates an action at. The actuator then executes this action, causing the physical plant E to transition to a subsequent state in accordance with the system dynamics Song et al. (2016).\nFor AI-CPSs considered in this work, the control unit is implemented as a DRL-based control pol- icy \u03c0. Recent advancements in DRL have introduced several high-performing algorithms, such as Deep Deterministic Policy Gradient (DDPG) Lillicrap et al. (2015), Proximal Policy Optimization (PPO) Schulman et al. (2017), and Trust Region Policy Optimization (TRPO) Schulman et al."}, {"title": "2.2 Signal Temporal Logic (STL)", "content": "STL Donz\u00e9 and Maler (2010) is a specification language designed to describe the expected temporal behavior of a system. It extends the Linear Temporal Logic (LTL) by supporting continuous-time frameworks and real-valued signals. This makes it particularly valuable in domains where continuous time analysis of system behaviors is essential, such as environmental monitoring and system control Bartocci et al. (2018).\nSTL is widely used to define system safety specifications. It adopts temporal operators similar to those found in LTL, such as Globally (G), Finally (F) and Eventually (E). For example, a typical STL specification using the Globally operator is G[a,b](x < 0), which indicates that throughout the interval from time a to time b, the signal x must remain to blow a threshold 6. This capability to specify conditions over continuous time intervals allows STL to offer detailed and quantitative safety specifications regarding the temporal behavior of a system.\nIn this work, we employ STL to define safety requirements. Meanwhile, to quantitatively measure the degree of satisfaction of a given safety specification, we utilize the quantitative robust semantics Donz\u00e9 (2010). Given an STL specification and a system output trajectory \u03b6, the quantitative robust semantics ROB(\u03b6, \u03c6) maps \u03b6 and \u03c6 to a real number. A positive or negative value of ROB(\u03b6, \u03c6) indicates whether the specification \u03c6 is satisfied or violated, and a larger value implies a stronger satisfaction or violation. We refer to this value as the STL score of the trajectory \u03b6 under the specification \u03c6 in this paper and utilize it to construct the prediction model. Further details are provided in Section 3.2."}, {"title": "3 Approach", "content": "In this section, we first introduce the problem formulation considered in this work (Section 3.1).\nThen, we present details about our proposed framework MORTAR, which conducts action-level re- pair for AI-CPSs. Through collecting data on the safety of various state-action pairs, we construct a prediction model that estimates the safety of outputs of the DRL control policy (Section 3.2). Using this constructed prediction model, we identify actions that may lead to unsafe behaviors. Subsequently, an optimization-based technique is employed to generate repaired actions during runtime, enhancing the safety performance of the DRL control policy (Section 3.3). See also Fig. 3 for an illustration of the runtime workflow of MORTAR. Further details are provided as follows."}, {"title": "3.1 Problem Formulation", "content": "In this work, we consider the scenario where a CPS and a DRL control policy \u03c0 is given. At each timestep t, the policy \u03c0 receives the system state st and outputs an action at = \u03c0(st). The CPS then evolves to the subsequent state st+1 by executing the action at. Running the system under policy \u03c0 for a horizon of T timesteps results in a system trajectory \u03b6 = (s0, s1, ..., sT), where s0 is the initial system state. For a given safety specification \u03c6, we aim to ensure that the system trajectory \u03b6 satisfies the specification \u03c6, i.e., the corresponding STL score ROB(\u03b6, \u03c6) should be positive.\nHowever, due to various factors such as insufficient training or environmental noise, the DRL control policy \u03c0 may generate faulty actions that fail to meet the safety specification \u03c6. Therefore, the objective of MORTAR is to correct potential erroneous actions in runtime by using only the input and output of the control policy \u03c0, i.e., the state st and action at. This correction is facilitated through an action-level repair strategy, where at each timestep t, we first predict whether the proposed action at will lead to a future trajectory that violates the safety specification \u03c6. If an action at is deemed unsafe, we then generate a repaired action a\u2217t that enhances the safety. Details about MORTAR are explained in the following subsections."}, {"title": "3.2 Construction of the Prediction Model", "content": "The first and critical step of MORTAR is to construct a prediction model that identifies potential risky behaviours of the given DRL control policy. As mentioned in Section 2.2, we utilize STL to define the safety requirements for AI-CPS and employ the STL score to assess how well the given system output trajectory satisfies these safety constraints. However, the STL score cannot be directly calculated for future system behaviors where the precise system output trajectory is unknown. To address this issue, we therefore introduce a prediction model that estimates the future STL score for the current system state st and action at under the existing DRL control policy \u03c0. Constructing this prediction model involves two steps: data collection and model training, which are detailed as follows."}, {"title": "3.2.1 Data Collection", "content": "We employ the specified control policy \u03c0 to gather training data for constructing the prediction model. Note that if the control policy \u03c0 is already well-trained, the collected data will predominantly consist of safe trajectories, with unsafe trajectories constituting only a small fraction. This imbalance can hinder the development of an efficient prediction model. To address this issue, we introduce random action noises into the control policy during the data collection phase to balance the proportion of safe and unsafe trajectories (see Section 4.3.1 and Table 3 for an example). This strategy facilitates the construction of a more accurate prediction model capable of identifying potential unsafe actions.\nBy executing the control policy \u03c0 with added noises, we collect a set of safe and unsafe trajectories, where each trajectory \u03b6 contains T timesteps, and each timestep corresponds to a state-action pair (st, at). The STL score ROB(\u03b6, \u03c6) for each trajectory can then be calculated using the specification \u03c6. Considering the strong temporal correlation within each trajectory, we assign the same computed"}, {"title": "3.2.2 Prediction Model Training", "content": "To predict the safety of each proposed action at = \u03c0(st) during runtime, we train a prediction model M using the collected dataset. For a given state-action pair (st, at), the prediction model M estimates the future STL score ROB(\u03b6, \u03c6) for that pair. For brevity, we denote the output of the prediction model M, i.e., the estimated STL score, as \u03c8t = M(st, at) throughout the remainder of this paper. In this work, the prediction model M is represented by a lightweight neural network and is specifically trained for each CPS task under consideration.\nIn MORTAR, the prediction model M serves as an online monitor that assesses the safety of actions generated by the control policy \u03c0 in real-time. By introducing a predefined threshold \u03c8thres \u2265 0, we classify each proposed action at as safe if the output of the prediction model satisfies that \u03c8t = M(st, at) \u2265 \u03c8thres. If the output falls below the threshold, i.e., \u03c8t = M(st, at) < \u03c8thres, the action at is considered unsafe, triggering a subsequent action repair process. Note that these unsafe actions may not indicate immediate failures but could lead to potential constraint violations in future steps. Moreover, rather than a binary classifier, the prediction model M provides a continuous value in terms of safety, which greatly facilitates the action repair process by enabling the estimation of gradient information to guide improvements in action safety."}, {"title": "3.3 Repaired Action Generation", "content": "With the constructed prediction model, we are able to predict the safety of generated actions at each timestep. Once an unsafe action is identified, we initiate the action repair process, which forms the second component of MORTAR. In this subsection, we provide details on how to compute a repaired action using the safety estimates obtained from the prediction model."}, {"title": "3.3.1 Action Repair as An Optimization Problem", "content": "After determining an unsafe action at, we aim to rectify it by identifying a repaired action a\u2217t that satisfies M(st, a\u2217t) \u2265 \u03c8thres. We define the repaired action ar as follows\na\u2217t = at + a\u2217 (1)\nwhere a\u2217 is the repair patch that needs to be calculated.\nBy using the prediction model M, the repaired action a\u2217t can be computed by solving the following optimization problem\nmina\u2217 ||M(st, at + a\u2217) \u2212 \u03c8max||2 (2)\ns.t. M(st, at + a\u2217) \u2265 \u03c8thres (3)\nM(st, at + a\u2217) \u2264 \u03c8max (4)\nAmin \u2264 at + a\u2217 \u2264 Amax (5)\nThe objective (2) of this optimization is to minimize the difference between the safety estimate from the prediction model M and a maximum value \u03c8max, where we have \u03c8max > \u03c8thres. The rationale behind this is that, due to the well-acknowledged balance between ensuring safety and encouraging exploration for higher rewards Garcia and Fern\u00e1ndez (2015); Gu et al. (2022), optimizing the output M(st, at + a\u2217) towards an excessively high value may result in actions that are overly restrictive for safety, potentially neglecting the completion of task requirements. Therefore, instead of directly maximizing the output M(st, at + a\u2217), i.e., finding the safest action, we aim to keep it close to the maximum value \u03c8max during the optimization process. In this work, we calculate the maximum value \u03c8max as follows\n\u03c8max = \u03c8 + 2\u03c3\u03c8 (6)\nwhere \u03c8 and \u03c3\u03c8 are the mean value and standard deviation of the STL scores ROB(\u03b6, \u03c6) for all trajectories collected in the training dataset for the prediction model, respectively."}, {"title": "3.3.2 Practical Optimization Solving", "content": "The major challenge in solving the optimization (2)-(5) is ensuring a time-efficient computation to minimize overhead in the control operation. Considering this, traditional search-based methods, such as Nelder-Mead Gao and Han (2012), and gradient-based methods, such as BFGS Bazaraa et al. (2013), often fail to provide a satisfactory performance. These methods may require a considerable amount of iterations to find a feasible solution or need to compute estimates of the Hessian matrix for gradients, both of which are computationally expensive.\nThe key to reducing computational overhead lies in minimizing the number of times the prediction model is used to calculate predictions. Motivated by this, we utilize the targeted Basic Iterative Method (BIM) Xiao et al. (2018) in this work to determine the repaired action ar. BIM is a widely adopted method in generating adversarial examples Goodfellow et al. (2014); Kurakin et al. (2018); Yuan et al. (2019). It aims to incrementally modify the input so that the neural network produces a specific, altered classification result. Similarly, the action repair process seeks to modify actions to achieve a different output from the prediction model, paralleling the objectives of adversarial example generation and thereby motivating the use of the BIM method.\nBy setting the target to the maximum value \u03c8max from (6), the targeted BIM attempts to find an ad- versarial action patch aadv = BIM(M, st, at, \u03c8max, \u03c8thres) that moves the output of the prediction model towards \u03c8max. Note that rather than approaching \u03c8max as closely as possible, the primary objective of the action repair process is to identify an action that is considered safe by the prediction model. Therefore, to further enhance computational efficiency, we incorporate an early-stop strategy within the BIM search process. Specifically, the search is terminated when the repaired action a\u2217t = at + aadv yields a value from the prediction model that falls within the range between \u03c8thres and \u03c8max.\nIt is also important to note that, unlike traditional adversarial example generation, where classification results can change immediately, controlling CPSs generally requires smoother command"}, {"title": "4 Experimental Evaluation", "content": "To evaluate the effectiveness of MORTAR, we conduct comprehensive experiments across various tasks. This section presents details about our research questions, the systems studied, the experimental setup, and the corresponding evaluation results."}, {"title": "4.1 Research Questions", "content": "We aim to investigate the following research questions (RQs) for assessing the effectiveness of MORTAR from various perspectives.\n\u2022 RQ1: To what extent can the prediction model accurately predict the system safety?\nThe prediction model plays a vital role in our online repair scheme; namely, it determines whether to initiate the repair process and provides guidance for the generation of repaired actions. Therefore, this RQ aims to investigate whether the constructed prediction model can precisely estimate the future STL sores of the subject CPSs.\n\u2022 RQ2: How well does MORTAR perform in the studied CPSs?"}, {"title": "4.2 Studied Systems and Tasks", "content": "Utilizing a public AI-CPS benchmark designed for industrial-level robotic manipulation Zhou et al. (2024), we evaluate MORTAR across five different manipulation tasks. We use the Franka Emika robotic manipulator Fra (2024) as the CPS and simulate the manipulation tasks with NVIDIA Omni- verse Isaac Sim isa (2024), a physics engine-based simulation platform known for providing realistic simulations Makoviychuk et al. (2021); Zhou et al. (2024). Descriptions of the employed manipulation tasks are presented below (see also Fig. 4)\n\u2022 Point Reaching (PR): The robotic manipulator needs to reach a designated point with its end- effector. This basic skill is fundamental for completing more complex manipulation tasks.\n\u2022 Cube Stacking (CS): The robot must grasp a cube and place it on top of a target cube. This task requires the AI controller to accurately manage the spatial relationships between the two cubes.\n\u2022 Peg in Hole (PH): The robot is required to insert a peg into a small hole at the center of a pillar. This task demands precise control of the pose of the object.\n\u2022 Ball Balancing (BB): The robotic manipulator should hold a tray and keep a ball centred on the tray's top surface. Since the robot has no direct contact with the ball, this task introduces increased complexity in controller design.\n\u2022 Ball Catching (BC): The robot aims to catch a ball thrown at it using a box-shaped tool. This task requires the ability to interact with a moving object.\nFor each task, we define the safety requirement as successfully completing the task's objective and describe this requirement using the STL specifications presented in Zhou et al. (2024). These STL specifications are treated as standard specifications. Moreover, for the PR, CS, and BB tasks, we also implement a stricter STL specification by reducing the distance tolerance required for successful task completion. For example, in the CS task, the tolerance distance between the manipulated cube and the target cube is reduced from 0.024m to 0.012m. However, for the PH and BC tasks, where task completion is defined by whether the peg is successfully inserted or the ball is caught, reducing the distance tolerance does not yield meaningful results. Thus, stricter specifications are not applicable to these tasks. Details of all employed standard and strict STL specifications are provided in Table 1.\nWe employ two DRL control policies, trained using the PPO and TRPO algorithms, for each task. As noted in Section 3.2.1, well-trained policies typically result in primarily safe trajectories, indicating successful task completions. As shown in Table 2, these well-trained policies often achieve a success rate higher than 90% under standard specifications. However, under strict specifications, the success rate drops, providing a greater opportunity to assess the effectiveness of MORTAR in enhancing the safety of AI controllers."}, {"title": "4.3 Experimental Setup", "content": "To minimize randomness in the experiments, for each task and DRL control policy, we use three different random seeds and report the average values. Detailed designs for each RQ are described below."}, {"title": "4.3.1 RQ1", "content": "As mentioned in Section 3.2.1, we introduce action noises to DRL control policies during data collection to overcome the imbalance between safe and unsafe trajectories. The success rates of the noised policies, compared to the original policies, under standard specifications are presented in Table 3. The results demonstrate an improved balance between safe and unsafe trajectories due to the injected noises. In this work, we collect 5000 trajectories for each task and DRL controller, with each trajectory consisting of 300 timesteps. These datasets, collected with action noises and standard specifications, are then used to train the prediction models.\nTo evaluate the accuracy of the prediction model, we divide the collected dataset into training, validation, and testing subsets in a 7:1:2 ratio. Considering the detection of unsafe actions as a binary classification problem, we utilize four metrics to assess the effectiveness of the prediction model: accuracy, F1-score, mean square error (MSE), and the area under the receiver operating character- istic curves (AUC). The threshold \u03c8thres for the classification is set to 0 for all tasks. While the accuracy and the F1-score measure the precision of the binary classification, MSE and AUC provide insights into the numerical accuracy of the prediction scores."}, {"title": "4.3.2 RQ2", "content": "If an unsafe action is identified by the trained prediction model, we employ the targeted BIM to compute a repaired action. In our experiments, we set the step size and maximum iterations of the BIM to \u03f5 = 0.1 and iter = 3, respectively. Note that, the noised policies are used only for collecting training data for the prediction model. To evaluate the effectiveness of MORTAR, we conduct experiments using the original DRL control policies. We also compare MORTAR with Safe Explo- ration (SafeExp) Dalal et al. (2018), a representative action repair method that uses an offline-trained safety layer to rectify erroneous actions. For more details about SafeExp, please refer to Dalal et al. (2018). To analyze the overhead introduced by MORTAR, we record the computational time when MORTAR is activated and compare it to the computational time when no action repair scheme is utilized."}, {"title": "4.3.3 RQ3", "content": "For examining the influence of optimization algorithms on the effectiveness of MORTAR, we compare the proposed BIM-based method with two traditional optimization algorithms: Nelder- Mead Gao and Han (2012) and COBYLA Powell (1994). The performance of these algorithms heavily depends on the maximum iterations allowed. To ensure a fair comparison, we test these traditional algorithms at two different settings for maximum iterations. The first setting limits the iterations to 3, matching the setup for the targeted BIM. The second setting allows for 100 iterations, giving these algorithms more opportunity to find better solutions. We then assess and compare the repair performance and computational overhead under these different optimization algorithms.\nSoftware and Hardware Dependencies. All experiments are conducted with NVIDIA Omniverse Isaac Sim version 2022.2.0 on a machine equipped with an Intel i7-11800H CPU, NVIDIA 3080 GPU, and 64GB RAM."}, {"title": "4.4 Evaluation Results", "content": "In this subsection, we present detailed evaluation results for each RQ considered."}, {"title": "4.4.1 RQ1: \u03a4o what extent can the prediction model accurately predict the system safety?", "content": "Table 4 shows the performance of the trained prediction models for each manipulation task with PPO and TRPO control policies.\nIt can be observed that, for all prediction models, the classification accuracy ranges from 0.83 to 0.94, and the F1-score varies from 0.80 to 0.94. Such high values indicate that the prediction model is capable of making accurate predictions, enabling an efficient determination of unsafe actions.\nIn all experiments, the prediction models also exhibit satisfying discriminatory capacity, with AUC scores exceeding 0.88. Furthermore, MSE values are consistently low across all tasks, especially for the Point Reaching task with the PPO controller and the Ball Catching task with the TRPO con- troller, where MSE values are below 0.04. These metrics confirm the prediction models' satisfactory numerical accuracy and their effectiveness in guiding the action repair process.\nAnswer to RQ1: The trained prediction models are able to accurately predict the safety of actions and effectively guide the online action repair process."}, {"title": "4.4.2 RQ2: How well does MORTAR perform in the studied CPSs?", "content": "Table 5 showcases the performance of MORTAR in enhancing the safety of DRL control policies. Under standard specifications, MORTAR achieves the highest success rates across all tasks and control policies. Even for already well-performing DRL policies, MORTAR can still increase the success rates to nearly 100%. When stricter specifications are applied, an expected decrease in overall success rates is observed. However, MORTAR is still able to enhance safety, achieving improved task completions in 5 out of 6 tasks. The only exception is the Cube Stacking task with the TRPO controller under the strict specification, where SafeExp performs best. Nonetheless, the broad enhancement of safety across almost all tasks demonstrates the effectiveness of MORTAR in increasing the safety of DRL controllers, even when relying solely on input and output information for the action repair.\nIt is worth noting that SafeExp performs poorly in repairing actions for complex CPSs and tasks. One possible reason could be that SafeExp relies on creating a linear system model to estimate the consequences of each proposed action Dalal et al. (2018). However, accurately modelling complex systems and tasks with a linear approach is often challenging and requires a considerable amount of training data. This limitation hinders the performance of SafeExp in the considered robotic manip- ulation tasks, which feature high-dimensional state spaces and highly nonlinear system dynamics.\nTable 6 presents the analysis of the computational overhead for MORTAR. The results show that MORTAR requires an average of 8.8 milliseconds per action repair step, resulting in a computational time increase of approximately 1.3 milliseconds compared to the original control policy without action repair. This minimal increase still allows for real-time operation of the AI-CPSs, indicating that no latency issues are introduced and MORTAR effectively meets the real-time requirements."}, {"title": "4.4.3 RQ3: How do different optimization algorithms impact the performance of MORTAR?", "content": "Table 7 displays the performance of different optimization algorithms in repairing unsafe actions. With only 3 iterations, the proposed targeted BIM-based algorithm outperforms the other two tradi- tional optimization methods in 15 out of 16 tasks. The only exception is the Cube Stacking task with the TRPO control policy under strict specifications, where Nelder-Mead achieves a slightly better result. When the maximum iteration limit is increased to 100, the traditional optimization methods are able to find better solutions, leading to success rates comparable to our proposed method."}, {"title": "5 Discussion", "content": "Unsafe Action Detection and Repair using a Single Prediction Model. In general, addressing the runtime safety issues of DRL control policies encompasses two primary tasks: detecting unsafe actions Gu et al. (2022); Zolfagharian et al. (2023) and repairing these actions Zhou et al. (2020a); Bharadhwaj et al. (2020); Bloem et al. (2015). MORTAR connects these two tasks by leveraging the forecasting capability of the same prediction model. During runtime, we utilize the prediction model's forward inference to identify unsafe actions. Subsequently, in the repair process, we employ the same model's backpropagation to gather gradient information, guiding the generation of repaired actions. By integrating these two tasks through the dual utilization of the same prediction model, MORTAR ensures that the repair process is directly informed by the prediction model, thereby maintaining coherence between detection and repair.\nLimitation. A major limitation of MORTAR is the prediction model's strong reliance on the spe- cific combination of task and policy. The prediction model can only be applied to the same CPS task and policy for which it was trained, necessitating new training datasets for different policies, even within the same task. A potential solution to enhance generalizability across various policies could involve developing a prediction model that relies solely on the characteristics of the CPS. Such a policy-independent prediction model could then be seamlessly integrated into MORTAR, increasing its generalizability. Nevertheless, as an exploratory work, MORTAR has demonstrated satisfactory performance and promising potential in leveraging the same prediction model for both runtime monitoring and repairing.\nFuture Direction.\nBased on the evaluation results, we intend to explore the following two directions in our future research:\n\u2022 The performance of MORTAR heavily relies on the accuracy of the prediction model since it plays a key role in both unsafe action detection and following repair phases. Therefore, a crucial research direction is enhancing the accuracy and reliability of the prediction model, particularly in addressing false positive predictions, where unsafe actions are mistakenly predicted as safe.\n\u2022 In MORTAR, we perform a repaired action at each timestep where the unsafe action is detected. However, for tasks requiring continuous time control, generating a series of repaired actions over a continuous time horizon may yield better results. Developing such a repair scheme could po- tentially enhance the performance of MORTAR.\n\u2022 In MORTAR, the state vector is an extracted one-dimensional vector. However, our framework has no design to handle tasks with multimedia data as input, such as robots equipped with camera sensors Bahl et al. (2020); Singh et al. (2019). Therefore, if future developments allow for inte- grating perception modules Garg et al. (2020) and state extraction algorithms Abel et al. (2018) into MORTAR, its potential applications would be significantly broadened."}, {"title": "6 Threats to Validity", "content": "Internal Validity. The quality of the prediction model can be an internal factor that affects the effectiveness of our runtime repair framework, as it not only triggers the repair process but also provides gradient information for generating repaired actions. To mitigate this threat, we first add"}, {"title": "7 Related Works", "content": "Deep Neural Network Repair. In terms of Deep Neural Network (DNN), the corresponding repair techniques can be generally classified into two categories as follows:\nAdversarial training refers to methods that first collect adversarial examples (inputs deliberately perturbed to trigger DNN's faulty behaviors) and then use these examples to augment the training dataset and retrain the DNN model Balunovi\u0107 and Vechev (2020); Jia et al. (2022); Shafahi et al. (2020, 2019); Bai et al. (2021). The key point of such approaches is to elevate the robustness of the DNN model against specific attacks by injecting the corresponding adversarial examples into the training loop.\nParameter alternation implies the techniques that directly modify the parameters within the DNN to inherently correct the behavior of the model in response to adversarial inputs Zhang and Chan (2019); Wang et al. (2019); Sohn et al. (2019); Tian et al. (2018); Gong et al. (2022). These methods generally require localizing the suspicious DNN parameters associated with faculty outputs and then optimizing these parameters to accomplish the desired repair goal.\nOur method differs from these commonly used solutions in that we conduct the repairs in a black-box manner, which does not directly manipulate the subject model. Hence, our framework is considered to have better generalizability and adaptability.\nDRL Policy Repair. Unlike traditional DNN models, which mainly focus on classification and regression tasks, DRL policies are usually designed for decision-making tasks, such as planning and system control. Therefore, repairing DRL policies is often more challenging, as the repair process needs to address not only the high-dimensional, continuous observation and action spaces but also the intricate system dynamics. In the realm of reinforcement learning, one subfield which explicitly focuses on safeguarding the safety of DRL policies is Safe Reinforcement Learning (SRL) Alshiekh et al. (2018); Garcia and Fern\u00e1ndez (2015); Gu et al. (2022); Zanon and Gros (2020); L\u00fctjens et al. (2019); Zhou et al. (2020b, 2021).\nA common approach in SRL to improve safety involves modeling the system as a Constrained Markov Decision Process (CMDP) Altman (2021) and then learning a critic online to estimate the associated cost function Gu et al. (2022); Achiam et al. (2017); Yang et al. (2020). This cost function then guides the policy update, steering it towards the direction that reduces the expected accumulated costs over a prediction horizon. For example, Constrained Policy Optimization (CPO) Achiam et al. (2017) used surrogate functions to approximate both objective and constraint functions in CMDP and updated the policy accordingly. Building on CPO, Yang et al. (2020) introduced Projection-"}, {"title": "8 Conclusion", "content": "In this work, we introduce MORTAR, a runtime repair framework designed for AI-CPSs with DRL controllers. For a given CPS task and an associated DRL control policy, MORTAR first constructs a prediction model to forecast the safety of each action proposed by the policy. Such a model serves two primary functions: it acts as a runtime monitor to identify unsafe actions and initiate the repair process, and it provides safety estimates and gradient information to guide the action repair. The generation of repaired actions is achieved by using a targeted BIM-based optimization strategy, which is able to efficiently find rectified actions that drive the system towards a safe condition. Meanwhile, MORTAR also keeps the computational overhead at a minimal level, ensuring the real- time operation of the AI-CPSs. Extensive experiments are conducted with various CPS tasks, DRL policies and optimization algorithms to validate the effectiveness of MORTAR. The results show that MORTAR can effectively repair faculty outputs from the original DRL policy and improve the overall safety and reliability of the AI-CPSs."}]}