{"title": "Hierarchical Orchestra of Policies", "authors": ["Thomas P Cannon", "\u00d6zg\u00fcr \u015eim\u015fek"], "abstract": "Continual reinforcement learning poses a major challenge due to the tendency of agents to experience catastrophic forgetting when learning sequential tasks. In this paper, we introduce a modularity-based approach, called Hierarchical Orchestra of Policies (HOP), designed to mitigate catastrophic forgetting in lifelong reinforcement learning. HOP dynamically forms a hierarchy of policies based on a similarity metric between the current observations and previously encountered observations in successful tasks. Unlike other state-of-the-art methods, HOP does not require task labelling, allowing for robust adaptation in environments where boundaries between tasks are ambiguous. Our experiments, conducted across multiple tasks in a procedurally generated suite of environments, demonstrate that HOP significantly outperforms baseline methods in retaining knowledge across tasks and performs comparably to state-of-the-art transfer methods that require task labelling. Moreover, HOP achieves this without compromising performance when tasks remain constant, highlighting its versatility.", "sections": [{"title": "Introduction", "content": "Neural networks are typically trained on data drawn independently and identically from a static distribution. While this approach works well in many cases, it becomes challenging in environments that are continuously changing or when new environments are introduced. In dynamic settings, such as reinforcement learning, robotics, or dialogue systems, models must adapt to new information while preserving knowledge from previous tasks (Parisi et al., 2019). However, neural networks often suffer from catastrophic forgetting, where learning new tasks leads to the rapid loss of previously acquired knowledge. The ability to learn new skills while maintaining existing knowledge is referred to as continual learning (Ring, 1994).\nTo address the challenge of catastrophic forgetting, researchers have developed three primary cat- egories of methods. The first category, regularization-based, works by constraining updates to network parameters, thereby penalizing deviations from learned weight values that are critical for previous tasks. Notable examples of this approach include Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI) (Kirkpatrick et al., 2017; Zenke et al., 2017). The second category, replay-based, mitigates forgetting by periodically rehearsing past experiences, either through actual data or synthetic generations, ensuring that the network continues to perform well on earlier tasks (Rolnick et al., 2019; Shin et al., 2017). The third category, modularity-based, addresses the issue by structurally separating the network into modules, with each module dedicated to a specific task, thereby minimizing interference between tasks, prominent examples of this method are Progressive Neural Networks (PNN) by (Rusu et al., 2016) and adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA) by Agostinelli et al. (2013). Finally there are some methods which use a combination of these, for example, Schwarz et al. (2018) uses an active network and a knowledge-base"}, {"title": "Hierarchical Orchestra of Policies", "content": "Hierarchical Orchestra of Policies (HOP) is a modularity-based deep learning framework designed to mitigate catastrophic forgetting when learning new tasks. In this framework, a task is defined as a specific Markov Decision Process (MDP), where distinct levels within a procedurally generated environment, or levels across different environments, are considered separate tasks (Puterman, 2014). Although HOP is task-agnostic, all tasks are treated as episodic.\nHOP relies on reinforcement learning algorithms that output stochastic policies, represented as \\(\u03c0(\u03b1 | s)\\) (Sutton, 2018). In our work, PPO serves as the base algorithm for HOP. The framework introduces three key mechanisms to form and use a collection of policies:\n1.  Checkpoints to freeze and store policies at a certain stage of training.\n2.  Orchestration of policy activation based on state similarity.\n3.  Hierarchical weightings to balance the influence of previous and new policies.\nThese mechanisms enable the agent to recover and maintain performance across diverse tasks without significant interference, thereby promoting continual learning in complex environments.\nCheckpoints. The agent initially learns a policy using a base algorithm. After \\(T_{checkpoint}\\) time- steps, HOP initializes a checkpoint, where the current learning policy \\(\u03c0\\) is frozen and evaluated in the currently available tasks. During checkpoint evaluation, if the episodic return \\(R\\) surpasses a predefined threshold \\(R_{threshold}\\), all states encountered during that task episode are stored in a set of trusted states \\(S_m\\) which is linked with the policy checkpoint \\(\u03c0_m\\), where \\(m\\) is the count of the checkpoint.\nThe Orchestra. When the agent resumes learning, it dynamically activates checkpoint policies \\(\u03c0_m\\) determined by the similarity between the current state \\(s_t\\) and any \\(s_m \u2208 S_m\\). If the current state \\(s_t\\) is similar to any state in \\(S_m\\), then the corresponding frozen policy \\(\u03c0_m\\) is activated (\\(I_m = 1\\)). Similarity is determined by a threshold value \\(w\\), which, in all of our experiments, has been defined as any \\(s_m \u2208 S_m\\) with a cosine similarity greater than 0.98 with \\(s_t\\).\nRather than selecting actions directly from the distribution of a single frozen policy (\\(a_t \\tilde{} \u03c0_m(S_t)\\)), which could lead to conflicts when multiple policy checkpoints are activated, HOP combines the distributions from activated policy checkpoints (\\(I_m\u03c0_m\\)) and the current learning policy \\(\u03c0_\u03b7\\) into a joined action policy, denoted as \\(\u03c0\u03b7\u03b1\\) (see equation 1). Here, \\(n\\) represents the current count of all policies, and the subscript \\(a\\) denotes the combined policy from which the action is sampled. This approach allows the agent to leverage past knowledge while adapting to new tasks, promoting continual learning.\nTo avoid significant and undesired output shifts caused by small changes in the state, frozen policies predict actions based on the most similar state, \\(s_m \u2208 S_m\\), to the current state \\(s_t\\) (Szegedy, 2013). This state is referred to as \\(s_m\\), and actions are chosen as \\(\u03c0_m(a | s_m)\\) rather than directly from \\(s_t\\). This dynamic activation of multiple policies is called the orchestra of policies, a term borrowed from Jonckheere et al. (2023) but applied differently in this work.\nHierarchical Weightings. As the agent learns, it is expected to achieve higher task-specific rewards, which suggests that newer policies for the same tasks are likely to outperform older policy checkpoints"}, {"title": "Results", "content": "We evaluated the performance of HOP using the Procgen suite of environments (Cobbe et al., 2020). The experimental setup consisted of three phases of training. In the first phase, the agent trains for three million time-steps on multiple levels of a selected Procgen environment to develop its ability to learn and generalize. In the second phase, the environment was switched to a different one, and the agent continued training for another three million time-steps, assessing its adaptability and ability"}, {"title": "Summary and Discussion", "content": "We present a novel modularity-based approach, the Hierarchical Orchestra of Policies, to address catastrophic forgetting in continual life-long reinforcement learning. In our empirical evaluation, HOP outperforms PPO in continual learning scenarios, achieving a faster recovery of performance and final performance. Both HOP and PNN demonstrate substantial transfer between environments with similar dynamics and state spaces such as Ninja and CoinRun. In these scenarios HOP can activate relevant frozen policies learned from Ninja while acting in CoinRun, similar to PNN's adapter networks connecting separate columns. However, unlike PNN, HOP does not require task labels, making it more versatile for real-world applications where task boundaries are not clearly defined.\nHowever, the effectiveness of HOP depends on the careful tuning of some hyper-parameters, particu- larly the similarity threshold (\\(w\\)) and reward threshold (\\(P\\)), which must be set appropriately for all expected tasks. See Appendix A.2\nFuture work could expand HOP's evaluation by testing transitions between highly diverse tasks and environments where task boundaries are ambiguous, a setting in which PNN and similar methods are less effective. Additionally, HOP could be adapted to continuous environments with fluid task transitions, further highlighting its robustness in real-world scenarios. To address performance drops immediately following task distribution changes, a learnable parameter could be introduced which could dynamically adjust the influence of previous checkpoints, enabling immediate adaptation while maintaining learning."}, {"title": "A Appendix", "content": "A.1 HOP Algorithm details\nThe logic provided in algorithm 1, is suitable for use with PPO (or any other actor critic style base function. It is expected that HOP would work with any method with a stochastic policy, however this has yet to be tested. Table 2 details all of the extra parameters that HOP requires."}, {"title": "A.2 Experiment details", "content": "Our experiments are conducted in the Procgen suite of environments introduced by Cobbe et al. (2020). Specifically, we use Ninja, StarPilot, Climber, and CoinRun as our environments. These can be viewed at https://github.com/openai/procgen, and we also provide a table with a snapshot of each environment in Figure 4. In Procgen, there are options that can reduce the complexity of the environments. We activate the following options: use_backgrounds=False, restrict_themes=True,"}, {"title": "A.3 PNN with PPO Algorithm Details", "content": "Progressive Neural Networks (PNN) were introduced by Rusu et al. (2016). In their paper, they describe how separate policy networks (referred to as columns) and links between columns (adapters) are used to improve continuous learning. However, they report their results using the Asynchronous Advantage Actor-Critic (A3C) algorithm Mnih (2016). We could not find any evidence indicating whether they initialized separate columns and adapters for the value (critic) network as well as the policy (actor) network. Additionally, we could not locate any official implementation online, nor any implementation using actor-critic methods.\nIntuitively, we expect that if PNN works for the policy, it should also work for the value function. Therefore, we implemented separate columns for both the critic and actor networks, along with adapters for each new task. We encountered another issue: PPO is generally expected to outperform A3C in ProcGen environments. Thus, comparing HOP-PPO or base PPO with PNN-A3C would be unfair to PNN. To address this, we modified the PNN implementation to use a PPO update that propagates through separate columns and adapters.\nSince the inputs for ProcGen environments are image-based, we use a single ReLU-activated convo- lutional layer as each adapter network. The input to the adapter network is the final convolutional"}, {"title": "A.4 Hierarchical Weighting Examples", "content": "Hierarchical weightings. This hierarchical structure implies that as the agent continues learning, the contributions of older policies diminish if more recent checkpoints are activated. Conversely, if more recent policies do not activate, the older policies will have a stronger influence. For example, consider the policy at the fourth checkpoint (\u03c04) as depicted in Figure 2. If activations A53, As2, and As1 occur, the policy output for the current state st is given by:\n\\[\u03c0_{4\u03b1}(s_t) = \\frac{\\frac{9}{24} I_{1max} \u03c0(s_{1max-sim}) + \\frac{1}{2max} I_{2max} \u03c0(s_{2max-sim}) + \\frac{1}{3max} I_{3max} \u03c0(s_{3max-sim})}{\\frac{9}{24} I_{1max} + \\frac{1}{2max} I_{2max} + \\frac{1}{3max} I_{3max}} + \u03c0_4(s_t)\\]\nIn contrast, if only As1 activates, the output becomes:\n\\[\u03c0_{4\u03b1}(s_t) = \\frac{\\frac{1}{2} I_{1max} \u03c0(s_{1max-sim})}{\\frac{1}{2} I_{1max}} + \u03c0_4(s_t)\\]\nHere, the contribution of \u03c0\u2081 diminishes as more recent policies are activated. However, if only As1 is activated, \u03c0\u2081 provides a significant contribution, which remains substantial regardless of how many other checkpoint policies exist but are inactive."}]}