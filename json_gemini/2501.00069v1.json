{"title": "Adversarial Negotiation Dynamics in\nGenerative Language Models", "authors": ["Arinbj\u00f6rn Kolbeinsson", "Benedikt Kolbeinsson"], "abstract": "Generative language models are increasingly used for contract drafting and en-\nhancement, creating a scenario where competing parties deploy different language\nmodels against each other. This introduces not only a game-theory challenge but\nalso significant concerns related to AI safety and security, as the language model\nemployed by the opposing party can be unknown. These competitive interactions\ncan be seen as adversarial testing grounds, where models are effectively red-teamed\nto expose vulnerabilities such as generating biased, harmful or legally problem-\natic text. Despite the importance of these challenges, the competitive robustness\nand safety of these models in adversarial settings remain poorly understood. In\nthis small study, we approach this problem by evaluating the performance and\nvulnerabilities of major open-source language models in head-to-head competi-\ntions, simulating real-world contract negotiations. We further explore how these\nadversarial interactions can reveal potential risks, informing the development of\nmore secure and reliable models. Our findings contribute to the growing body of\nresearch on AI safety, offering insights into model selection and optimisation in\ncompetitive legal contexts and providing actionable strategies for mitigating risks.", "sections": [{"title": "1 Introduction", "content": "The rise of generative language models has profoundly impacted various domains, with legal appli-\ncations, such as contract drafting and enhancement, emerging as a significant use case. Corporate\nentities often deploy specialised models offered by products like CoCounsel (Reuters, 2024), Lexis\nNexis (LexisNexis, 2024) or Harvey AI (AI, 2024), which use either proprietary models or finely-\ntuned versions of publicly available models, whether closed or open source. Meanwhile, smaller\nentities or individual practitioners have access to a diverse range of models, including those from the\nGPT (Brown et al., 2020), Claude (Anthropic, 2024) and Llama (Touvron et al., 2023) families, as\nwell as fine-tuned legal versions of these models (Cheng et al., 2024) or specialised open-source legal\nmodels (Colombo et al., 2024).\nIn real-world contract negotiations, it is increasingly likely that one or both parties will use different\ngenerative language models to gain a competitive edge. This scenario introduces a multifaceted\nchallenge: not only must parties consider game theory in selecting a model that maximises favourable\noutcomes, but they must also address significant AI safety and security concerns. The interaction\nbetween competing models can be viewed as a form of adversarial testing, or red teaming, where\nmodels expose each other to potential vulnerabilities, such as generating biased, harmful or legally\nproblematic content.\nDespite the critical importance of these issues, the competitive robustness and safety of generative\nlanguage models in adversarial settings are not well understood. Current benchmarks, such as\nLawBench (Fei et al., 2023) and the LinksAI benchmark (Linklaters LLP, 2023), primarily assess"}, {"title": "2 Experimental Setup", "content": "In this study, we investigate the behaviours of eight generative language models tasked with negotiat-\ning contracts on behalf of either a seller or a buyer. The contracts, which are also generated by the\nmodels, pertain to the sale of 100 machines between two fictional companies: West Manufacturing\nInc. (the buyer) and Square Machines Inc. (the seller). These contracts serve as the foundation for\nevaluating the negotiation dynamics between the models."}, {"title": "2.1 Contract Generation and Amendments", "content": "The experiment begins with the seller's model generating a contract using the following prompt:\nCreate a very short legal contract for the sale of goods (100\nmachines) between a buyer (West Manufacturing Inc.) and a\nseller (Square Machines Inc.). Return only the contract,\nnothing else.\nThe generated contract is then passed to the buyer's model, which can amend the contract with\nthe goal of improving the terms for the buyer. Both the seller and buyer are instructed to avoid\nadding additional clauses to the contract and are restricted to improving existing terms. The buyer's\namendment process is guided by the following prompt:\nWithout adding more articles, improve the current contract by\nmaking it better for the buyer. [Contract]\nAfter the buyer submits the revised contract, the seller is given a final opportunity to amend it using a\ncorresponding prompt designed to favour their side:\nWithout adding more articles, improve the current contract by\nmaking it better for the seller. [Contract]\nThis process simulates a real-world negotiation where both parties attempt to secure better terms\nwithout fundamentally altering the structure of the agreement."}, {"title": "2.2 Judgement Process", "content": "After each pair of buyer and seller models concludes their negotiation, the resulting contract is\nevaluated by the remaining six models. These models, acting as judges, provide an assessment of\nwhether the final contract is more favourable to the buyer or the seller. The judgement is based on the\nfollowing prompt:\nIs the following contract better for the buyer or the seller?\nRespond with exactly one letter. (A): Seller or (B): Buyer.\nThis consistent judging mechanism allows for a standardised evaluation across all experiments,\nensuring comparability and fairness in the results. While we assume the LLM judges' capability, we\nrecognise the limitation in relying solely on automated evaluations without human expert input. In\ncases where there were equal votes for both sides, we did not record a win or loss for either model."}, {"title": "2.3 Model Pairings and Matchups", "content": "The experiment involves all pairwise combinations of the eight models, with each model representing\neither the buyer or the seller in every possible matchup. Given that each model is paired with every\nother model, the study includes 28 unique buyer-seller pairings. Since each matchup results in a\ncontract generation, followed by judgements from six models, this leads to a total of 56 head-to-head\nmatchups and 336 individual judgements (6 judgements per contract)."}, {"title": "2.4 Models Used", "content": "The eight models in this study include both general-purpose language models and those specifically\ndesigned for legal applications. The general-purpose models are Llama-3-8B Meta (2024), Llama-2-\n7B Touvron et al. (2023), Gemma-7B Team et al. (2024), Phi-3 Abdin et al. (2024) and Falcon-7B\nAlmazrouei et al. (2023), while the legal-specialised models are LawChat-7B Cheng et al. (2024) and\nSaul-7B Colombo et al. (2024)."}, {"title": "2.5 Fairness, Safety and Red Teaming Considerations", "content": "While the primary objective of this experiment is to assess the negotiation capabilities of these models,\nit also serves as a probe into their safety and fairness features. The lack of specific guardrails beyond\nthose embedded by the models' creators allows for exploration of potential safety risks. Some models\nmay take advantage of others that have more stringent safeguards, which we will discuss further\nin the results section. This setting provides valuable insight into how models handle adversarial\ninteractions in high-stakes negotiation scenarios without explicit mitigation strategies, aligning with\nthe red teaming focus of the workshop."}, {"title": "3 Results and Discussion", "content": "We compare and plot the performance of all eight models acting as both seller and buyer agents in\nFigure 1. The results reveal distinct patterns in the negotiation behaviours of general-purpose and\nspecialised legal models, shedding light on their capabilities and limitations in contract negotiations.\nGeneral-purpose models such as Llama-3-8B and Llama-2-7B exhibit strong adaptability in their\nrespective roles. Llama-3-8B consistently performs well as a buyer agent, while Llama-2-7B\ndominates as a seller. This adaptability suggests that general models can perform competitively across\ndifferent legal contexts without domain-specific training. In contrast, specialised legal models like\nSaul-7B and LawChat-7B demonstrate more balanced performance across both roles, possibly due to\ntheir fine-tuning on legal-specific datasets. This balance positions them as reliable choices in legal\nnegotiations where fairness is crucial.\nInterestingly, models such as Gemma-7B and Mistral-7B provide balanced and consistent outcomes,\nneither dominating in one role nor showing significant weaknesses. Their robustness makes them\nstrong candidates for environments where equity between buyer and seller is a priority. However,\nthis also means they lack the decisive advantage seen in models like Llama-3-8B and Llama-2-7B,\nmaking them less suitable for scenarios where aggressive negotiation tactics might be preferred.\nSelecting the appropriate model is critical, as it can significantly influence negotiation outcomes. For\nexample, Phi-3 shows a strong bias towards excelling as a seller agent, winning a disproportionately\nhigh number of seller-side judgements. However, its performance as a buyer agent is notably weaker,\nsecuring the fewest buyer wins among all models. This role-specific strength implies that models like\nPhi-3 might be strategically deployed in scenarios where sellers need to optimise their positions, but\nit would be a poor choice for buyer-side negotiations.\nThe deployment of LLMs in legal negotiations raises questions around fairness and transparency.\nThere is potential for misuse if models are manipulated to create biased or deceptive terms. Future\nwork should explore strategies to improve model transparency and fairness to mitigate such risks."}, {"title": "3.1 Game-Theoretic Dynamics and Strategic Model Selection", "content": "A deeper analysis of the results shows that head-to-head matchups between models reveal significant\ngame-theoretic dynamics. Figure 2 illustrates the normalised win differences between pairs of models,"}, {"title": "3.2 Limitations and Future Work", "content": "Our experiment highlights several key challenges in evaluating generative models for legal negotiation\ntasks. While the results provide insights into the performance and safety of models, further work\nis required to ensure robust evaluations across a broader range of contract types and legal contexts.\nAdditionally, incorporating explicit safety guardrails and adversarial training may help mitigate the\nrisks identified through this red-teaming exercise. Future research should also explore how models\ncan be fine-tuned or augmented with real-time safety mechanisms to avoid generating biased or\nharmful outcomes, ensuring more trustworthy AI systems."}, {"title": "4 Conclusion", "content": "Our study reinforces the importance of red-teaming to uncover model vulnerabilities and provides\nvaluable guidance for legal practitioners and AI researchers alike. We emphasise the necessity of\ncarefully selecting language models based on specific legal contexts and adversaries. The results also\npoint to the broader need for enhanced safety mechanisms, transparency, and fairness in AI systems\nto ensure their trustworthy application in legal and other critical domains. Future work should focus\non refining model evaluation frameworks, implementing more robust safety guardrails, and exploring\nfine-tuning strategies to mitigate potential risks in adversarial negotiations."}, {"title": "Another limitation of this experiment is the absence of an \"agreement mechanism\" to ensure both", "content": "the buyer and seller models mutually agree on the final contract terms. In the current setup, the\nbuyer model has the opportunity to make the final amendment, which could theoretically result in\ncontracts heavily favouring the buyer. However, interestingly, the seller models won the majority\nof cases (163 wins for sellers compared to 117 wins for buyers), suggesting that this limitation did\nnot critically impact the overall results. Nonetheless, the absence of a formal agreement step reflects\na potential imbalance in the negotiation process that should be addressed in future work to more\naccurately simulate real-world contract negotiations where both parties must agree to the final terms.\nIncorporating such mechanisms could also improve the fairness and robustness of model performance\nin such adversarial simulations.\nAnother notable limitation of this study is the absence of human legal expert evaluation, which was\nconstrained by available resources. Also due to resource constraints, each model pairing was tested\nin a single run. Future studies should increase the number of runs to better capture the variability\ninherent in LLM outputs and incorporate human judges to validate LLM-based evaluations.\nIn future work, we aim to enhance the red teaming focus of this experiment by introducing more\nrigorous adversarial challenges and probing the models for vulnerabilities that go beyond contract\nnegotiation outcomes. This could include stress-testing the models with edge-case scenarios designed\nto exploit potential weaknesses, such as the generation of biased or harmful contract clauses and\nadversarial inputs aimed at circumventing built-in safety mechanisms. Additionally, implementing\nmore robust adversarial testing frameworks, where models actively attempt to exploit or manipulate\neach other's outputs, would provide deeper insights into their safety and reliability under high-stakes\nconditions. These enhancements will allow for a more comprehensive evaluation of the generative\nmodels' robustness, bias resistance and their ability to maintain fairness and safety in adversarial\nlegal contexts."}]}