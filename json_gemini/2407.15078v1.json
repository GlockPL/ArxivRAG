{"title": "Learning to Compile Programs to Neural Networks", "authors": ["Logan Weber", "Jesse Michel", "Alex Renda", "Michael Carbin"], "abstract": "A neural surrogate of a program is a neural network that mimics the behavior of a program. Researchers have used these neural surrogates to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Researchers traditionally develop neural surrogates by training on input-output examples from a single program. Alternatively, language models trained on a large dataset including many programs can consume program text, to act as a neural surrogate. Using a language model to both generate a surrogate and act as a surrogate, however, leading to a trade-off between resource consumption and accuracy. We present neural surrogate compilation, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find that they produce neural surrogates that are 1.9-9.5\\(\\times\\) as data-efficient, produce visual results that are 1.0-1.3\\(\\times\\) more similar to ground truth, and train in 4.3-7.3\\(\\times\\) fewer epochs than neural surrogates trained from scratch.", "sections": [{"title": "1. Introduction", "content": "A neural surrogate is a neural network that models a subset of the observable behavior of a program (Renda et al., 2021). Neural surrogates have been used to automatically configure image signal processing units and CPU simulators (Tseng et al., 2019; Renda et al., 2020), improve the accuracy of manufacturing and physics simulations (Tercan et al., 2018; Kustowski et al., 2020), accelerate the computer architecture design process (\u0130pek et al., 2006), and accelerate computations in signal processing, robotics, 3D games, compression, machine learning, and image processing (Esmaeilzadeh et al., 2012a).\nNeural Surrogate Training. The research community has developed a variety of techniques to train neural surrogates. The traditional approach is to train a neural surrogate of a single program by collecting and curating a dataset of input-output pairs and then training a neural network to predict the program's output given an input (Renda et al., 2021).\nAnother point in the spectrum is to amortize the cost of training neural surrogates by training a universal neural surrogate: a neural network that directly consumes the text of a program and predicts the program's output for a given input (Zaremba & Sutskever, 2015; Nye et al., 2021; Gu et al., 2024). A key benefit of universal neural surrogates is that one only needs to create a dataset once. Once trained, a universal neural surrogate can act as the neural surrogate of a given program without the need to curate a dataset of program-specific, input-output pairs.\nHowever, universal neural surrogates necessarily use the same model to process the program text as is used to predict the program output, and accurate prediction may require multiple forward passes (Nye et al., 2021; Wei et al., 2022). These limitations pose challenges for deploying such a model as a neural surrogate because small models may not be able to emulate complex programs (Zaremba & Sutskever, 2015) and large models (OpenAI et al., 2023) may not be able to execute in the resource-constrained environments where neural surrogates have been used (Esmaeilzadeh et al., 2012a; Mendis, 2020; Munk et al., 2022).\nOur Approach: Neural Surrogate Compilation. To maintain the benefits of universal neural surrogates while bypassing the above limitations, we propose to use a neural surrogate compiler. A neural surrogate compiler is a system that accepts a program's text as input and produce an initial neural surrogate of the program, which can vary in behavioral quality. Similarly to a traditional compiler, a neural surrogate compiler requires a significant upfront cost that is amortized over the generation of initializations for many neural surrogates. We demonstrate in this work that when compared to the traditional approach of training a neural surrogate from a random initialization, neural surrogates produced by neural surrogate compilers can be finetuned to closely mimic the behavior of the program at a lower cost, as measured in data efficiency and training time."}, {"title": "2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system that is specialized to a family of neural surrogate architectures to accept a program's text as input and produce an initial neural surrogate of the program. Figure 1 presents the neural surrogate compilation workflow alongside the traditional workflow for developing a neural surrogate. In a traditional neural surrogate development workflow, one collects training data (B)), trains the neural surrogate until its error meets the desired threshold (C), and then uses it in place of the original program (D). Neural surrogate compilation (A)) introduces a new, initial step in the neural surrogate compilation workflow in which a neural surrogate compiler maps the program text to a neural network initialization for use in the training of the neural surrogate. The typical strategy to train a neural surrogate is through supervised learning of a neural network with a curated dataset of input-output pairs from the program (Renda et al., 2021).\nIn this section, we formalize the problem of efficiently training a neural surrogate and introduce a new approach to solving this problem using a neural surrogate compiler."}, {"title": "2.1. The Efficient Surrogate Training Problem", "content": "We first formalize the problem of training a neural surrogate. We assume we are given a program text \\(p: P\\) that denotes a function \\([p] : I_p \\rightarrow O_p\\), where \\(P\\) is the space of programs under consideration, \\(I_p\\) is the type of values \\(p\\) accepts as input and \\(O_p\\) is the type of values \\(p\\) produces as output. We also assume a target neural surrogate architecture \\(\\alpha : \\mathbb{R}^d \\rightarrow I_p \\rightarrow O_p\\), which takes a set of parameters \\(\\theta : \\mathbb{R}^d\\) and produces a surrogate function from \\(I_p\\) to \\(O_p\\). The goal is to find a set of parameters \\(\\theta : \\mathbb{R}^d\\) such that the neural surrogate \\(\\alpha(\\theta): I_p \\rightarrow O_p\\) has low approximation error:\n\\[\\forall i: I_p. \\alpha(\\theta)(i) \\approx [p](i)\\]\nTo measure the quality of surrogate outputs, we use a loss function \\(l : O_p \\times O_p \\rightarrow \\mathbb{R}_{>0}\\) that measures the difference between the output of the program and the output of the surrogate. To measure overall surrogate quality, we use the expected loss over a distribution of inputs:\n\\[\\mathcal{L}(\\alpha(\\theta), p) = \\mathbb{E}_{i \\sim I_p}[l(\\alpha(\\theta)(i), [p](i))]\\]\nAs with most learning problems, a challenge in training neural surrogates is that the error of a surrogate depends on the budget dedicated to collecting training data (input-output pairs of the program) and the number of epochs used to train the surrogate. We formalize these costs by defining a training procedure \\(t_\\alpha : P \\times \\mathbb{R}_{\\ge 0} \\times \\mathbb{N} \\rightarrow \\mathbb{R}^d\\) for a given surrogate architecture \\(\\alpha\\) as a random function that takes program text \\(p\\), a training data budget \\(b : \\mathbb{R}_{\\ge 0}\\), and training time budget \\(n : \\mathbb{R}_{\\ge 0}\\) and produces a set of parameters \\(\\theta: \\mathbb{R}^d\\) for the surrogate.\nWe then define the efficient surrogate training problem as finding a training procedure \\(t_\\alpha\\) for a given program \\(p\\), architecture \\(\\alpha\\), sample budget \\(b\\), training time budget \\(n\\), and loss function \\(l\\) that minimizes the expected loss of the resulting surrogate:\n\\[\\arg \\min_{t_\\alpha} \\mathbb{E}_{\\theta \\sim t_\\alpha(p, b, n)} [\\mathcal{L}(\\alpha(\\theta), p)],\\]"}, {"title": "2.2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system \\(\\phi : (p : P) \\rightarrow \\mathbb{R}^{d_p}\\) that accepts program text \\(p\\) and produces parameters \\(\\theta \\in \\mathbb{R}^{d_p}\\) for a neural surrogate architecture \\(a_p\\) depending on the program \\(p\\). We use a neural surrogate compiler to solve the efficient surrogate training problem.\nWe formalize the development of a neural surrogate compiler as an optimization problem. The goal is to develop a system \\(\\Phi\\) such that for every program \\(p\\), the surrogate \\(f = a_p(\\Phi(p))\\) can be trained efficiently. Optimizing for a system that generates surrogates that can be trained efficiently is challenging. As a simple proxy, we optimize for a system that generates surrogates that achieve low loss:\n\\[\\arg \\min_{\\Phi \\in P \\rightarrow \\mathbb{R}^{d}} \\mathbb{E}_{p \\sim p} [\\mathcal{L}(a_p(\\phi(p)), p)].\\]"}, {"title": "3. COMPNET", "content": "The COMPNET architecture is an implementation of a neural surrogate compiler using hypernetworks. We explain the architecture, how to train it, then how to extract neural surrogates from its outputs."}, {"title": "3.1. Architecture", "content": "Figure 2 presents the design of a COMPNET. A COMP-NET accepts program text \\(p: P\\) as input and produces parameters \\(\\theta \\in \\mathbb{R}^{d}\\) for a neural surrogate architecture \\(a : \\mathbb{R}^{d} \\rightarrow I \\rightarrow O\\) with as many inputs as the largest architecture one wishes to compile to and a single output. We call this architecture a covering architecture.\n(A) First, COMPNET tokenizes an input program (1), resulting in a sequence of tokens (2) including the distinguished BERT classification token [CLS].\n(B) COMPNET then uses a BERT encoder (Devlin et al., 2019) to embed the sequence of tokens, resulting in an embedding per token. The output of this step is the embedding of the classification token (3); COMPNET discards the embeddings of the other tokens.\nNext, COMPNET uses a parameter head, implemented as a single linear layer, to map the classification token embedding to a neural surrogate parameter vector (4).\nD) Then, COMPNET interprets the vector of parameters as the weights and biases of the covering architecture. The output of this step is a neural surrogate of the input program.\nFinally, COMPNET executes the neural surrogate with the interpreted parameters on a program input (5) to produce a prediction of the program output (6)."}, {"title": "3.2. Training", "content": "Training a COMPNET requires a dataset of programs and input-output pairs for each program. Note that this dataset is not considered as part of the budget in the efficient surrogate training problem, since it is amortized over all programs the COMPNET is used to compile.\nEach step of training proceeds by selecting a batch of programs and input-output pairs for those programs, generating neural surrogate parameters for each program, interpreting the neural surrogate parameters as parameters for the covering architecture, executing each neural surrogate with the batch of inputs, then calculating the loss between the neural surrogates' predicted outputs and the true outputs. To match the signature of the covering architecture, the batch of inputs is padded out to match the number of inputs for the covering architecture (e.g., if a covering architecture has 9 inputs and a program has 3 inputs, the compiled architecture for that program is fed 9 inputs). For padding, we use inputs drawn from the same distribution as the program inputs (see Appendix N for details).\nBackpropagation proceeds as usual, except that one does not update the parameters of the neural surrogates, since each generated neural surrogate is ephemeral. Instead, back-propagation only updates the parameters of the COMPNET. Appendix E contains additional training details."}, {"title": "3.3. Surrogate Extraction", "content": "The output of a COMPNET is parameters for the covering architecture, which might not match the number of inputs and outputs of the program being compiled. To adapt the covering architecture to the target number of inputs, one finetunes the resulting architecture on data where the excess inputs are set to zero, allowing one to then remove the weights in the input layer corresponding to the excess inputs (see Appendix N for details on this choice). To adapt the covering architecture to the target number of outputs, one clones the weights for the single output in the output layer for each new output that is needed (see Appendix O for details on this choice). To align the program text with the training distribution (i.e., single-output programs), one also modifies the input program to produce a single output (e.g., the first output of the original program). When neither the number of inputs nor the number of outputs matches the covering architecture, all of the above modifications are applied in the same finetuning run."}, {"title": "4. EXESTACK", "content": "The strategy we presented in Section 3 for learning a neural surrogate compiler requires a dataset of programs and input-output examples describing the behavior of each program. To meet this requirement, we developed EXESTACK, a dataset of 69,083 pointer-free, numerical, executable, deterministic C programs and corresponding input-output examples. EXESTACK is based on The Stack (Kocetkov et al., 2022), a dataset of 3 TB of permissively licensed source code written in various programming languages scraped from GitHub.\nFigure 3 summarizes the process of generating EXESTACK (see Appendix B for details). The restriction to pointer-free functions simplifies the EXESTACK generation methodology, and yet, a model trained on EXESTACK could still handle programs using statically-sized data structures containing numeric data (e.g., arrays), as they can be transformed into functions with a fixed number of arguments."}, {"title": "5. Evaluation", "content": "To evaluate the claim that neural surrogate compilation lowers the development cost of neural surrogates, we answer the following research questions.\nRQ 1: Does a neural surrogate initialized by a COMPNET converge to a lower test loss than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 2: Does a neural surrogate initialized by a COMPNET produce better results in an application than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 3: Does a neural surrogate initialized by a COMPNET converge to a target test loss in fewer epochs than a neural surrogate initialized randomly?\nOur results demonstrate that COMPNETS lead to improvements in data efficiency (Section 5.2), perceptual quality (Section 5.3), and training time (Appendix J)."}, {"title": "5.1. Methodology", "content": "To develop and evaluate COMPNETS, we select a BERT architecture for the neural surrogate compiler and a multilayer perceptron for the covering architecture, we produce datasets that COMPNETS can be trained and evaluated on, we introduce alternative initialization methods to compare against, and we finetune surrogates produced by each of the initialization methods."}, {"title": "5.1.1. COMPNET ARCHITECTURE", "content": "We use the BERT-Tiny architecture (Turc et al., 2019) for the BERT encoder in COMPNET, and we adapt a neural surrogate architecture from Esmaeilzadeh et al. (2012a) into a covering architecture for this COMPNET.\nThe architecture used by Esmaeilzadeh et al. (2012a) is a multilayer perceptron consisting of a single input, a hidden layer of 4 neurons, another hidden layer of 4 neurons, and 2 outputs, and it uses a sigmoid activation function. For their evaluation, the authors introduce a suite of benchmarks, PARROTBENCH, consisting of numerical programs from various domains. The authors apply their techniques to the architecture above on a fast Fourier transform benchmark in PARROTBENCH and achieve a 3.6\\(\\times\\) speedup. This architecture therefore places a floor on the system speedup that motivates our investigation of Parrot, in that the architectures Esmaeilzadeh et al. (2012a) use for all other programs in PARROTBENCHCPN are at least as computationally expensive as the one we choose. We adapt this architecture to take in 9 inputs and produce 1 output, so it can be used to compile programs with up to 9 inputs, and so it is compatible with EXESTACK.\nAs the COMPNET loss function, we use mean squared error (MSE) between predicted and true outputs."}, {"title": "5.1.2. DATASETS", "content": "We evaluate the effectiveness of COMPNETS on test programs from EXESTACKCPN and programs from PARROTBENCHCPN (see Table 1). These datasets are refinements of EXESTACK and PARROTBENCH that are compatible with the instantiation of the COMPNET architecture described above.\nEXESTACKCPN. We produce EXESTACKCPN by applying additional filters to EXESTACK, resulting in 37,772 programs. See Appendix C for details.\nFrom the full set of programs, we create a training, validation, and testing set using an 80/10/10 split. Each program has input-output examples, so we additionally create a training and testing set for these examples using a 50/50 split. In Sections 5.2 and Appendix J, we evaluate performance on EXESTACKCPN using 1,000 programs from the testing set.\nPARROTBENCHCPN. PARROTBENCHCPN programs come from a diverse set of application domains, they are all written in C, each consists of a single function, and they are numeric in nature, making them suitable for evaluating COMPNETs. Table 1 shows the programs in PARROTBENCHCPN, including descriptions of the computations and input datasets. In Appendix D, we explain how we chose these programs, we list the program source, and we explain how we generated input datasets."}, {"title": "5.1.3. ALTERNATIVE INITIALIZATION METHODS", "content": "Besides random initialization, we compare COMPNETS to two alternative initialization methods: model-agnostic meta learning (Finn et al., 2017) and pretrained initializations. Neither initialization method conditions on program text, so they both result in constant initializations that one uses for every program. We briefly describe these techniques here and how we train them, and we provide shorthand for referencing each initialization method. In Appendix A, we survey related work in this area in detail.\nModel-Agnostic Meta Learning. Model-agnostic meta learning (MAML) is a meta-learning technique for producing neural network initializations that can be quickly finetuned to achieve low error on a given task. One trains MAML initializations by sampling tasks from some space of training tasks, finetuning on them, and backpropagating through the finetuning process into the initialization.\nPretrained Neural Surrogates. A simpler alternative to MAML is to train a single neural surrogate on the union of all input-output examples from programs in a dataset such as EXESTACKCPN. We call initializations trained in this way pretrained neural surrogates.\nTraining. We train 3 instances of each initialization method on EXESTACKCPN training programs using the same covering architecture as COMPNETS. See Appendices F, G, N, and O for details on MAML training, pretrained surrogate training, variable-input support, and variable-output support, respectively.\nInitialization Method Shorthand. We use shorthand names for each initialization method in figures. We refer to COMPNETS as \u201cCPN\u201d, MAML as \u201cMAML\", pretrained surrogates as \"PTS\u201d, and random initialization as \u201cRND\u201d."}, {"title": "5.1.4. FINETUNING SURROGATES", "content": "Here, we collect the finetuning methodology for surrogates in this evaluation, including optimization methods, hyperparameters, random seed behavior, and how we measure the improvements achieved by these surrogates.\nFor all surrogates produced by the initialization methods we consider, we use the following finetuning methodology. We use the Adam optimizer with no weight decay, a learning rate of 0.01, and MSE as the loss function. The only difference between our methodology and the methodology of Esmaeilzadeh et al. (2012a) is that we use the Adam optimizer instead of stochastic gradient descent, and we use the He initialization method (He et al., 2015)\u2014they do not specify how they initialize their neural surrogates.\nWe use 9 trials with different random seeds for every configuration in the experiments of Section 5.2 and Appendix J. Note that, for COMPNET, MAML, and pretrained surrogate initializations, changing random seeds only changes the training data order, since the initialization is deterministic.\nFor data efficiency and training time, we quantify results using geometric mean improvements over random initialization. These are only relative measures, so in Appendix L, we demonstrate the neural surrogates we train achieve sufficiently low absolute error for downstream applications."}, {"title": "5.2. Data Efficiency Improvements", "content": "To assess whether COMPNETS improve data efficiency, we use COMPNETS to initialize neural surrogates, finetune on subsets of training data of various sizes, and then compare the results to those of other initialization methods. We detail the methodology of this experiment then present results."}, {"title": "5.2.1. METHODOLOGY", "content": "We now describe the configurations we sweep over and the methodology we use to finetune surrogates.\nExperiment Configurations. In this experiment, we sweep over configurations consisting of a program, a dataset size, and an initialization method (e.g., a COMPNET). Each dataset size specifies the percentage of the training data to train neural surrogates on. We sweep over the following percentages: {0%, 0.1%, 1%, 10%, 100%}.\nDataset Selection. Given a configuration consisting of a program, a dataset size percentage \\(c \\in [0, 1]\\), and an initialization method, we select a random subset \\(D_{\\text{sub}}\\) of the training data \\(D_{\\text{train}}\\) of size \\(c|D_{\\text{train}}|\\). We use an 80/20 split to divide \\(D_{\\text{sub}}\\) into train and validation sets \\(D_{\\text{sub train}}\\) and \\(D_{\\text{sub val}}\\). We sample 9 different subsets of this size and use a different training seed for each subset, yielding 9 trials total.\nFinetuning. For each trial, we initialize a neural surrogate according to the initialization method. We then train on \\(D_{\\text{sub train}}\\) for 5,000 epochs. The final test loss we report for a trial is the test loss at the epoch closest to the epoch with the lowest validation error. When the dataset size is 0%, we use the test loss at the final epoch."}, {"title": "5.2.2. RESULTS", "content": "Figures 4 and 5 show finetuning results for a sample of 1,000 EXESTACKCPN test programs and PARROTBENCHCPN, respectively. See Appendix H for the test losses used to compute improvements.\nEXESTACKCPN Test Programs. COMPNETS achieve the best results on average, with a 9.50\\(\\times\\) improvement over random initialization, whereas MAML and pretrained surrogates achieve only a 1.09\\(\\times\\) and 1.08\\(\\times\\) improvement on average. COMPNETS improve over random initialization in as low as the 21st percentile of configurations, whereas MAML and pretrained surrogates improve over random initialization after the 35th and 37th percentiles, respectively.\nCOMPNETS improve on EXESTACKCPN test programs most prominently in the zero-shot regime, where the improvement is 84.40\\(\\times\\) over random initialization, whereas MAML and pretrained surrogates achieve improvements of 1.42x and 2.63\\(\\times\\), respectively. The zero-shot regime is also the only regime where pretrained surrogates show an improvement. The worst performance for both COMPNETS and MAML is in the middle of the dataset sizes we evaluated, at a dataset size of 1%, where they achieved 2.90x and 0.51\\(\\times\\), respectively. The worst performance for pretrained surrogates, however, is at a dataset size of 100%, where they achieve a 0.76\\(\\times\\) improvement.\nPARROTBENCHCPN Programs. COMPNETS achieve the best results on average, achieving a 1.91\\(\\times\\) improvement over random initialization, whereas MAML worsened performance (0.93\\(\\times\\)) and pretrained surrogates slightly improved performance (1.05\\(\\times\\)). COMPNETS improve over random initialization in as low as the 36th percentile of configurations, whereas MAML and pretrained surrogates both improve over random initialization after the 54th percentile.\nCOMPNETS improve or do not worsen data efficiency on each PARROTBENCHCPN program, with the smallest improvement on invk2j (1.01\\(\\times\\)) and the largest improvement on kmeans (7.85\\(\\times\\)). MAML shows the largest improvement on invk2j (1.07\\(\\times\\)) but worsens performance on fft and kmeans, achieving 0.98\\(\\times\\) and 0.68\\(\\times\\), respectively. Pretrained surrogates show the largest improvement on kmeans (2.24\\(\\times\\)), but they worsen performance on fft and sobel, achieving 0.61\\(\\times\\) and 0.85\\(\\times\\), respectively.\nUnlike the results for EXESTACKCPN, the improvement due to COMPNETS is greatest near the middle of the dataset sizes we evaluated over. The greatest improvement of 2.38\\(\\times\\) occurs at 10%, and the smallest improvement of 1.68\\(\\times\\) occurs at 100%. MAML worsens performance at most dataset sizes, except at 10%, where it achieves a 1.11\\(\\times\\) improvement over random initialization. Pretrained surrogates worsen performance at most dataset sizes except 0% and 10%, where they achieve 1.56\\(\\times\\) and 1.23\\(\\times\\), respectively.\nSince COMPNETS improve data efficiency over random initialization on both EXESTACKCPN and PARROTBENCHCPN, we answer yes to RQ 1."}, {"title": "5.3. Neural Surrogates for Color Quantization", "content": "To assess whether a COMPNET can improve the quality of results in an end-to-end application, we use a trained COMPNET to initialize a neural surrogate used for color quantization and compare it to other initialization methods (Kanungo et al., 2002). Color quantization is the process of reducing the number of distinct colors in an image. For example, Figure 6 depicts an image of a baboon color quantizated to five colors."}, {"title": "5.3.1. METHODOLOGY", "content": "We follow the methodology for color quantization from Kanungo et al. (2002) who apply k-means clustering to the (R, G, B) vectors representing the colors of pixels of an image and select the cluster centroids as the colors in the palette. We run k-means clustering for 40 iterations or until the distance between the old centroids and new centroids is less than \\(1 \\cdot 10^{-5}\\). Each pixel color is then remapped to the closest color in the palette.\nWe use the Euclidean distance function to compute the distance between two RGB vectors. We consider both a reference NumPy implementation and approximate implementations given by neural surrogates of the kmeans kernel in PARROTBENCHCPN (Harris et al., 2020).\nWe use surrogates from the data efficiency evaluation of Section 5.2. For visual comparisons, we choose a single surrogate for each dataset size and initialization method. Since here we evaluate on a distinct image from the testing set of the kmeans kernel, using the testing set as a validation set does not constitute data leakage, so we choose the surrogates with the lowest test losses. For quantitative comparisons, we aggregate over all surrogates and no selection criterion is necessary.\nWe quantify the similarity between NumPy-quantized images and surrogate-quantized images using both MSE and the structural similarity index measure (SSIM), the latter of which provides a quantitative model for the percieved similarity of images (Wang et al., 2004)."}, {"title": "5.3.2. RESULTS", "content": "Figure 6 depicts the result of applying 5-color quantization to an image of a baboon using surrogates trained on dataset sizes of 0% and 0.1% of the training set. Figure 7 shows quantitative results comparing initialization methods on 5-color quantization at various dataset sizes. Each entry shows the average and standard deviation of a metric over all trials and instances of an initialization method. See Appendix I for more dataset sizes and color palette sizes.\nVisual Results. At a dataset size of 0%, COMPNET- and MAML-initialized surrogates are the only surrogates that produce images with detail. The image produced by a COMPNET surrogate shows more detail than the image produced by a MAML surrogate, which primarily captures details on the nose. At a dataset size of 0.1%, all initialization methods produce images that resemble the original image. Images produced by COMPNET-initialized and pretrained surrogates have a higher contrast than images produced by MAML-initialized and randomly initialized surrogates.\nQuantitative Results. At all dataset sizes, COMPNET-initialized surrogates have the lowest MSE and the highest SSIM on average. Among the other initialization methods, there is no consistent winner across dataset sizes.\nThe variance for the MSE results is comparable across all initialization methods and is high enough that there is overlap among methods. For example, at a dataset size of 0%, an MSE result that is one standard deviation below the mean for MAML is lower than the mean for COMPNETS. However, for all other dataset sizes the mean MSE for COMPNETS is lower than the mean MSE for MAML, even after subtracting a single standard deviation.\nFor the SSIM results, at smaller dataset sizes, the variance is high enough that there is overlap among methods. At larger dataset sizes though, the results are more clearly separated, with COMPNETS having the highest mean SSIM, even when one adds a single standard deviation to the mean SSIM for each of the other initialization methods."}, {"title": "6. Conclusion", "content": "In this paper, we presented the concept of a neural surrogate compiler and demonstrated how a neural surrogate compiler can be implemented with COMPNETS. We provided a dataset, EXESTACK, that one can use to learn neural surrogate compilers. We demonstrated the effectiveness of COMPNETS on ExeSTACKCPN programs and PARROTBENCHCPN, a suite of numerical benchmarks. Specifically, we showed COMPNET-initialized surrogates achieve losses that are 1.9-9.5\\(\\times\\) lower than randomly initialized surrogates, they produce color-quantized images that are 1.0-1.3\\(\\times\\) more similar to images produced by an exact implementation than images produced by randomly initialized surrogates, and they train in 4.3-7.3\\(\\times\\) fewer epochs than randomly initialized surrogates.\nThe key insight of our work is that a programming language can condition the space of neural network initializations. In the limit, a neural surrogate compiler could produce initializations requiring no training to achieve low error. More broadly, neural surrogate compilers could be used to encode programmatically specified behaviors in neural networks, potentially accelerating training for more general tasks."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Related Work", "content": "Neural surrogate compilation is inspired by literature on neural surrogates of programs and meta-learning. In the following sections, we survey these fields and describe other efforts to compile programs to neural networks."}, {"title": "A.1. Neural Surrogates of Programs", "content": "A common approach to developing neural surrogates of programs is to train a program-specific neural surrogate on a dataset of input-output examples (Renda et al., 2021), or more recently, to train a universal neural surrogate on a dataset that includes many programs (Zaremba & Sutskever, 2015; Nye et al., 2021). Our work presents an alternative method for training neural surrogates of numerical programs that maintains the speed of program-specific neural surrogates but incorporates the data efficiency benefits of universal neural surrogates.\nProgram-Specific Neural Surrogates. Researchers across scientific disciplines have used neural surrogates of numerical programs to accelerate computations, adapt to new settings, and enable gradient-based optimization. Esmaeilzadeh et al. (2012a) demonstrate that neural surrogates of numerical programs can improve performance for computations in signal processing, robotics, 3D games, compression, machine learning, and image processing. To accelerate optical metasurface design, An et al. (2019) use neural surrogates of numerical simulators and Pestourie et al. (2020) use neural surrogates of partial differential equations. Tercan et al. (2018) and Kustowski et al. (2020) use neural surrogates of numerical simulators for plastic injection molding and inertial confinement fusion, respectively, to facilitate data-efficient finetuning on real physical data. Kaya & Hajimirza (2019) accelerate numerical simulations for solar cells using neural surrogates, and they use transfer learning to quickly adapt neural surrogates when simulator configurations change. Shirobokov et al. (2020) use neural surrogates of non-differentiable, numerical physical simulators, to enable gradient-based optimization of simulator parameters.\nResearchers have used nonnumerical surrogates to optimize and explore discrete configuration spaces. Tseng et al. (2019) and Renda et al. (2020) develop neural surrogates of a black-box image signal processing unit and a cycle-accurate CPU simulator, respectively; both techniques enable gradient-based optimization of program inputs, to match some desired input-output behavior. Kwon & Carloni (2020) develop a neural surrogate of a high-level synthesis pipeline for hardware. Using this surrogate, they lower the cost of predicting the performance and cost of hardware configurations, and they use transfer learning to lower the cost of developing neural surrogates for new configuration spaces."}, {"title": "Universal Neural Surrogates", "content": "Researchers have developed universal neural surrogates using a variety of architectures. Early work in this area uses long short-term memory networks to predict the results of executing simple, synthetic Python programs (Zaremba & Sutskever, 2015). Later work uses graph neural networks that model program structure in a similar evaluation setup (Bieber et al., 2020). More recently, researchers have trained Transformer-based models on synthetic datasets of programs or large datasets that include programs (Austin et al., 2021; Nye et al., 2021; OpenAI et al., 2023; Bubeck et al., 2023; Gu et al., 2024)."}, {"title": "A.2. Meta-Learning", "content": "Meta-learning can improve data efficiency"}]}