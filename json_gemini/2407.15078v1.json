{"title": "Learning to Compile Programs to Neural Networks", "authors": ["Logan Weber", "Jesse Michel", "Alex Renda", "Michael Carbin"], "abstract": "A neural surrogate of a program is a neural network that mimics the behavior of a program. Researchers have used these neural surrogates to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Researchers traditionally develop neural surrogates by training on input-output examples from a single program. Alternatively, language models trained on a large dataset including many programs can consume program text, to act as a neural surrogate. Using a language model to both generate a surrogate and act as a surrogate, however, leading to a trade-off between resource consumption and accuracy. We present neural surrogate compilation, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find that they produce neural surrogates that are 1.9-9.5\u00d7 as data-efficient, produce visual results that are 1.0-1.3\u00d7 more similar to ground truth, and train in 4.3-7.3\u00d7 fewer epochs than neural surrogates trained from scratch.", "sections": [{"title": "1. Introduction", "content": "A neural surrogate is a neural network that models a subset of the observable behavior of a program (Renda et al., 2021). Neural surrogates have been used to automatically configure image signal processing units and CPU simulators (Tseng et al., 2019; Renda et al., 2020), improve the accuracy of manufacturing and physics simulations (Tercan et al., 2018; Kustowski et al., 2020), accelerate the computer architecture design process (\u0130pek et al., 2006), and accelerate computations in signal processing, robotics, 3D games, compression, machine learning, and image processing (Esmaeilzadeh et al., 2012a).\nNeural Surrogate Training. The research community has developed a variety of techniques to train neural surrogates. The traditional approach is to train a neural surrogate of a single program by collecting and curating a dataset of input-output pairs and then training a neural network to predict the program's output given an input (Renda et al., 2021).\nAnother point in the spectrum is to amortize the cost of training neural surrogates by training a universal neural surrogate: a neural network that directly consumes the text of a program and predicts the program's output for a given input (Zaremba & Sutskever, 2015; Nye et al., 2021; Gu et al., 2024). A key benefit of universal neural surrogates is that one only needs to create a dataset once. Once trained, a universal neural surrogate can act as the neural surrogate of a given program without the need to curate a dataset of program-specific, input-output pairs.\nHowever, universal neural surrogates necessarily use the same model to process the program text as is used to predict the program output, and accurate prediction may require multiple forward passes (Nye et al., 2021; Wei et al., 2022). These limitations pose challenges for deploying such a model as a neural surrogate because small models may not be able to emulate complex programs (Zaremba & Sutskever, 2015) and large models (OpenAI et al., 2023) may not be able to execute in the resource-constrained environments where neural surrogates have been used (Esmaeilzadeh et al., 2012a; Mendis, 2020; Munk et al., 2022).\nOur Approach: Neural Surrogate Compilation. To maintain the benefits of universal neural surrogates while bypassing the above limitations, we propose to use a neural surrogate compiler. A neural surrogate compiler is a system that accepts a program's text as input and produce an initial neural surrogate of the program, which can vary in behavioral quality. Similarly to a traditional compiler, a neural surrogate compiler requires a significant upfront cost that is amortized over the generation of initializations for many neural surrogates. We demonstrate in this work that when compared to the traditional approach of training a neural surrogate from a random initialization, neural surrogates produced by neural surrogate compilers can be finetuned to closely mimic the behavior of the program at a lower cost, as measured in data efficiency and training time."}, {"title": "2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system that is specialized to a family of neural surrogate architectures to accept a program's text as input and produce an initial neural surrogate of the program. Figure 1 presents the neural surrogate compilation workflow alongside the traditional workflow for developing a neural surrogate. In a traditional neural surrogate development workflow, one collects training data (B)), trains the neural surrogate until its error meets the desired threshold (C), and then uses it in place of the original program (D). Neural surrogate compilation (A)) introduces a new, initial step in the neural surrogate compilation workflow in which a neural surrogate compiler maps the program text to a neural network initialization for use in the training of the neural surrogate. The typical strategy to train a neural surrogate is through supervised learning of a neural network with a curated dataset of input-output pairs from the program (Renda et al., 2021).\nIn this section, we formalize the problem of efficiently training a neural surrogate and introduce a new approach to solving this problem using a neural surrogate compiler.\n2.1. The Efficient Surrogate Training Problem\nWe first formalize the problem of training a neural surrogate. We assume we are given a program text $p: P$ that denotes a function $[p] : I_p \\rightarrow O_p$, where $P$ is the space of programs under consideration, $I_p$ is the type of values $p$ accepts as input and $O_p$ is the type of values $p$ produces as output. We also assume a target neural surrogate architecture $\\alpha: R^d \\rightarrow I_p \\rightarrow O_p$, which takes a set of parameters $\\theta : R^d$ and produces a surrogate function from $I_p$ to $O_p$. The goal is to find a set of parameters $\\theta : R^d$ such that the neural surrogate $\\alpha(\\theta): I_p \\rightarrow O_p$ has low approximation error:\n$\\sqrt{E_{i:I_p}.\\alpha(\\theta)(i) \\approx [p](i)}$\nTo measure the quality of surrogate outputs, we use a loss function $l: O_p \\times O_p \\rightarrow R_{>0}$ that measures the difference between the output of the program and the output of the surrogate. To measure overall surrogate quality, we use the expected loss over a distribution of inputs:\n$L(\\alpha(\\theta), p) = E_{i \\sim I_p}[l(\\alpha(\\theta)(i), [p](i))]$     (1)\nAs with most learning problems, a challenge in training neural surrogates is that the error of a surrogate depends on the budget dedicated to collecting training data (input-output pairs of the program) and the number of epochs used to train the surrogate. We formalize these costs by defining a training procedure $t_\\alpha :P \\times R_{\\geq 0} \\times N \\rightarrow R^d$ for a given surrogate architecture $\\alpha$ as a random function that takes program text $p$, a training data budget $b: R_{\\geq 0}$, and training time budget $n : R_{\\geq 0}$ and produces a set of parameters $\\theta : R^d$ for the surrogate.\nWe then define the efficient surrogate training problem as finding a training procedure $t_\\alpha$ for a given program $p$, architecture $\\alpha$, sample budget $b$, training time budget $n$, and loss function $l$ that minimizes the expected loss of the resulting surrogate:\n$\\arg \\min_{t_\\alpha} E_{\\theta \\sim t_\\alpha (p,b,n)} [L(\\alpha(\\theta), p)],$"}, {"title": "2.2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system $\\phi: (p: P) \\rightarrow R^{d_p}$ that accepts program text $p$ and produces parameters $\\theta \\in R^{d_p}$ for a neural surrogate architecture $a_p$ depending on the program $p$. We use a neural surrogate compiler to solve the efficient surrogate training problem.\nWe formalize the development of a neural surrogate compiler as an optimization problem. The goal is to develop a system $\\phi$ such that for every program $p$, the surrogate $f = a_p(\\phi(p))$ can be trained efficiently. Optimizing for a system that generates surrogates that can be trained efficiently is challenging. As a simple proxy, we optimize for a system that generates surrogates that achieve low loss:\n$\\arg \\min_{\\Phi\\in R^d} E_{p\\sim P} [L(a_p(\\phi(p)),p)].$"}, {"title": "3. COMPNET", "content": "The COMPNET architecture is an implementation of a neural surrogate compiler using hypernetworks. We explain the architecture, how to train it, then how to extract neural surrogates from its outputs.\n3.1. Architecture\nFigure 2 presents the design of a COMPNET. A COMP- NET accepts program text $p: P$ as input and produces parameters $\\theta \\in R^d$ for a neural surrogate architecture $a : R^d \\rightarrow I \\rightarrow O$ with as many inputs as the largest architecture one wishes to compile to and a single output. We call this architecture a covering architecture.\n(A) First, COMPNET tokenizes an input program (1), resulting in a sequence of tokens (2) including the distin- guished BERT classification token [CLS].\n(B) COMPNET then uses a BERT encoder (Devlin et al., 2019) to embed the sequence of tokens, resulting in an em- bedding per token. The output of this step is the embedding of the classification token (3); COMPNET discards the embeddings of the other tokens.\n(C) Next, COMPNET uses a parameter head, implemented as a single linear layer, to map the classification token em- bedding to a neural surrogate parameter vector (4).\n(D) Then, COMPNET interprets the vector of parameters as the weights and biases of the covering architecture. The output of this step is a neural surrogate of the input program.\n(E) Finally, COMPNET executes the neural surrogate with the interpreted parameters on a program input (5) to pro- duce a prediction of the program output (6)\n3.2. Training\nTraining a COMPNET requires a dataset of programs and input-output pairs for each program. Note that this dataset is not considered as part of the budget in the efficient surrogate training problem, since it is amortized over all programs the COMPNET is used to compile.\nEach step of training proceeds by selecting a batch of pro- grams and input-output pairs for those programs, generating neural surrogate parameters for each program, interpreting the neural surrogate parameters as parameters for the covering architecture, executing each neural surrogate with"}, {"title": "3.3. Surrogate Extraction", "content": "The output of a COMPNET is parameters for the covering architecture, which might not match the number of inputs and outputs of the program being compiled. To adapt the covering architecture to the target number of inputs, one finetunes the resulting architecture on data where the excess inputs are set to zero, allowing one to then remove the weights in the input layer corresponding to the excess inputs (see Appendix N for details on this choice). To adapt the covering architecture to the target number of outputs, one clones the weights for the single output in the output layer for each new output that is needed (see Appendix O"}, {"title": "4. EXESTACK", "content": "The strategy we presented in Section 3 for learning a neural surrogate compiler requires a dataset of programs and input-output examples describing the behavior of each pro- gram. To meet this requirement, we developed EXESTACK, a dataset of 69,083 pointer-free, numerical, executable, deterministic C programs and corresponding input-output examples. EXESTACK is based on The Stack (Kocetkov et al., 2022), a dataset of 3 TB of permissively licensed source code written in various programming languages scraped from GitHub.\nFigure 3 summarizes the process of generating EXESTACK (see Appendix B for details). The restriction to pointer-free functions simplifies the EXESTACK generation method- ology, and yet, a model trained on EXESTACK could still handle programs using statically-sized data structures containing numeric data (e.g., arrays), as they can be trans- formed into functions with a fixed number of arguments."}, {"title": "5. Evaluation", "content": "To evaluate the claim that neural surrogate compilation lowers the development cost of neural surrogates, we answer the following research questions.\nRQ 1: Does a neural surrogate initialized by a COMPNET converge to a lower test loss than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 2: Does a neural surrogate initialized by a COMPNET produce better results in an application than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 3: Does a neural surrogate initialized by a COMPNET converge to a target test loss in fewer epochs than a neural surrogate initialized randomly?\nOur results demonstrate that COMPNETS lead to improve- ments in data efficiency (Section 5.2), perceptual quality (Section 5.3), and training time (Appendix J).\n5.1. Methodology\nTo develop and evaluate COMPNETS, we select a BERT architecture for the neural surrogate compiler and a multilayer perceptron for the covering architecture, we produce datasets that COMPNETS can be trained and evaluated on, we introduce alternative initialization methods to compare against, and we finetune surrogates produced by each of the initialization methods.\n5.1.1. COMPNET ARCHITECTURE\nWe use the BERT-Tiny architecture (Turc et al., 2019) for the BERT encoder in COMPNET, and we adapt a neural surrogate architecture from Esmaeilzadeh et al. (2012a) into a covering architecture for this COMPNET.\nThe architecture used by Esmaeilzadeh et al. (2012a) is a multilayer perceptron consisting of a single input, a hidden layer of 4 neurons, another hidden layer of 4 neurons, and 2 outputs, and it uses a sigmoid activation function. For their evaluation, the authors introduce a suite of benchmarks, PARROTBENCH, consisting of numerical programs from various domains. The authors apply their"}, {"title": "5.1.2. DATASETS", "content": "We evaluate the effectiveness of COMPNETS on test programs from EXESTACKCPN and programs from PARROTBENCHCPN (see Table 1). These datasets are refinements of EXESTACK and PARROTBENCH that are compatible with the instantiation of the COMPNET architecture described above.\nEXESTACKCPN. We produce EXESTACKCPN_ by applying additional filters to EXESTACK, resulting in 37,772 programs. See Appendix C for details.\nFrom the full set of programs, we create a training, valida- tion, and testing set using an 80/10/10 split. Each program has input-output examples, so we additionally create a train- ing and testing set for these examples using a 50/50 split. In Sections 5.2 and Appendix J, we evaluate performance on EXESTACKCPN using 1,000 programs from the testing set.\nPARROTBENCHCPN. PARROTBENCHCPN programs come from a diverse set of application domains, they are all written in C, each consists of a single function, and they are numeric in nature, making them suitable for evaluating COMPNETs. Table 1 shows the programs in PARROTBENCHCPN, including descriptions of the computations and input datasets. In Appendix D, we explain how we chose these programs, we list the program source, and we explain how we generated input datasets."}, {"title": "5.1.3. ALTERNATIVE INITIALIZATION METHODS", "content": "Besides random initialization, we compare COMPNETS to two alternative initialization methods: model-agnostic meta learning (Finn et al., 2017) and pretrained initializations. Neither initialization method conditions on program text, so they both result in constant initializations that one uses for every program. We briefly describe these techniques here and how we train them, and we provide shorthand for referencing each initialization method. In Appendix A, we survey related work in this area in detail.\nModel-Agnostic Meta Learning. Model-agnostic meta learning (MAML) is a meta-learning technique for producing neural network initializations that can be quickly finetuned to achieve low error on a given task. One trains MAML initializations by sampling tasks from some space of training tasks, finetuning on them, and backpropagating through the finetuning process into the initialization.\nPretrained Neural Surrogates. A simpler alternative to MAML is to train a single neural surrogate on the union of all input-output examples from programs in a dataset such as EXESTACKCPN. We call initializations trained in this way pretrained neural surrogates.\nTraining. We train 3 instances of each initialization method on EXESTACKCPN training programs using the same covering architecture as COMPNETS. See Appendices F, G, N, and O for details on MAML training, pretrained surrogate training, variable-input support, and variable-output support, respectively.\nInitialization Method Shorthand. We use shorthand names for each initialization method in figures. We refer to COMPNETS as \u201cCPN\u201d, MAML as \u201cMAML\", pretrained surrogates as \"PTS\u201d, and random initialization as \u201cRND\u201d.\""}, {"title": "5.1.4. FINETUNING SURROGATES", "content": "Here, we collect the finetuning methodology for surrogates in this evaluation, including optimization methods, hyper- parameters, random seed behavior, and how we measure the improvements achieved by these surrogates.\nFor all surrogates produced by the initialization methods we consider, we use the following finetuning methodology. We use the Adam optimizer with no weight decay, a learning rate of 0.01, and MSE as the loss function. The only"}, {"title": "5.2. Data Efficiency Improvements", "content": "To assess whether COMPNETS improve data efficiency, we use COMPNETS to initialize neural surrogates, finetune on subsets of training data of various sizes, and then compare the results to those of other initialization methods. We detail the methodology of this experiment then present results.\n5.2.1. \u039c\u0395\u03a4HODOLOGY\nWe now describe the configurations we sweep over and the methodology we use to finetune surrogates.\nExperiment Configurations. In this experiment, we sweep over configurations consisting of a program, a dataset size, and an initialization method (e.g., a COMPNET). Each dataset size specifies the percentage of the training data to train neural surrogates on. We sweep over the following percentages: {0%, 0.1%, 1%, 10%, 100%}.\nDataset Selection. Given a configuration consisting of a program, a dataset size percentage $c \\in [0, 1]$, and an initialization method, we select a random subset $D_{sub}$ of the training data $D_{train}$ of size $c|D_{train}|$. We use an 80/20 split to divide $D_{sub}$ into train and validation sets $D_{sub,train}$ and $D_{sub,val}$. We sample 9 different subsets of this size and use a different training seed for each subset, yielding 9 trials total.\nFinetuning. For each trial, we initialize a neural surrogate according to the initialization method. We then train on $D_{sub,train}$ for 5,000 epochs. The final test loss we report for a trial is the test loss at the epoch closest to the epoch with the lowest validation error. When the dataset size is 0%, we use the test loss at the final epoch."}, {"title": "5.2.2. RESULTS", "content": "Figures 4 and 5 show finetuning results for a sample of 1,000 EXESTACKCPN test programs and PARROT- BENCHCPN, respectively. See Appendix H for the test losses used to compute improvements.\nEXESTACKCPN Test Programs. COMPNETS achieve the best results on average, with a 9.50\u00d7 improvement over random initialization, whereas MAML and pretrained surrogates achieve only a 1.09\u00d7 and 1.08\u00d7 improvement on average. COMPNETS improve over random initialization in as low as the 21st percentile of configurations, whereas MAML and pretrained surrogates improve over random ini- tialization after the 35th and 37th percentiles, respectively.\nCOMPNETS improve on EXESTACKCPN test programs most prominently in the zero-shot regime, where the improvement is 84.40\u00d7 over random initialization, whereas MAML and pretrained surrogates achieve improvements of 1.42x and 2.63\u00d7, respectively. The zero-shot regime"}, {"title": "5.3. Neural Surrogates for Color Quantization", "content": "To assess whether a COMPNET can improve the quality of results in an end-to-end application, we use a trained COMPNET to initialize a neural surrogate used for color quantization and compare it to other initialization methods (Kanungo et al., 2002). Color quantization is the process of reducing the number of distinct colors in an image. For example, Figure 6 depicts an image of a baboon color quantizated to five colors.\n5.3.1. \u039c\u0395\u03a4HODOLOGY\nWe follow the methodology for color quantization from Kanungo et al. (2002) who apply k-means clustering to the (R, G, B) vectors representing the colors of pixels of an image and select the cluster centroids as the colors in the palette. We run k-means clustering for 40 iterations or until the distance between the old centroids and new centroids is less than $1 \\cdot 10^{-5}$. Each pixel color is then remapped to the closest color in the palette.\nWe use the Euclidean distance function to compute the distance between two RGB vectors. We consider both"}, {"title": "5.3.2. RESULTS", "content": "Figure 6 depicts the result of applying 5-color quantization to an image of a baboon using surrogates trained on dataset sizes of 0% and 0.1% of the training set. Figure 7 shows quantitative results comparing initialization methods on 5-color quantization at various dataset sizes. Each entry shows the average and standard deviation of a metric over all trials and instances of an initialization method. See Appendix I for more dataset sizes and color palette sizes.\nVisual Results. At a dataset size of 0%, COMPNET- and MAML-initialized surrogates are the only surrogates that produce images with detail. The image produced by a COMPNET surrogate shows more detail than the image pro- duced by a MAML surrogate, which primarily captures de- tails on the nose. At a dataset size of 0.1%, all initialization methods produce images that resemble the original image. Images produced by COMPNET-initialized and pretrained surrogates have a higher contrast than images produced by MAML-initialized and randomly initialized surrogates.\nQuantitative Results. At all dataset sizes, COMPNET- initialized surrogates have the lowest MSE and the highest SSIM on average. Among the other initialization methods, there is no consistent winner across dataset sizes.\nThe variance for the MSE results is comparable across all initialization methods and is high enough that there is overlap among methods. For example, at a dataset size of 0%, an MSE result that is one standard deviation below the mean for MAML is lower than the mean for COMPNETS. However, for all other dataset sizes the mean MSE for COMPNETS is lower than the mean MSE for MAML, even after subtracting a single standard deviation.\nFor the SSIM results, at smaller dataset sizes, the variance is high enough that there is overlap among methods. At larger dataset sizes though, the results are more clearly separated, with COMPNETS having the highest mean SSIM, even when one adds a single standard deviation to the mean SSIM for each of the other initialization methods."}, {"title": "6. Conclusion", "content": "In this paper, we presented the concept of a neural surrogate compiler and demonstrated how a neural surrogate compiler can be implemented with COMPNETS. We provided a dataset, EXESTACK, that one can use to learn neural"}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Related Work", "content": "Neural surrogate compilation is inspired by literature on neural surrogates of programs and meta-learning. In the following sections, we survey these fields and describe other efforts to compile programs to neural networks.\nA.1. Neural Surrogates of Programs\nA common approach to developing neural surrogates of programs is to train a program-specific neural surrogate4 on a dataset of input-output examples (Renda et al., 2021), or more recently, to train a universal neural surrogate on a dataset that includes many programs (Zaremba & Sutskever, 2015; Nye et al., 2021). Our work presents an alternative method for training neural surrogates of numerical pro- grams that maintains the speed of program-specific neural surrogates but incorporates the data efficiency benefits of universal neural surrogates.\nProgram-Specific Neural Surrogates. Researchers across scientific disciplines have used neural surrogates of numerical programs to accelerate computations, adapt to new settings, and enable gradient-based optimization. Es- maeilzadeh et al. (2012a) demonstrate that neural surrogates of numerical programs can improve performance for compu- tations in signal processing, robotics, 3D games, compres- sion, machine learning, and image processing. To accelerate optical metasurface design, An et al. (2019) use neural surro- gates of numerical simulators and Pestourie et al. (2020) use neural surrogates of partial differential equations. Tercan et al. (2018) and Kustowski et al. (2020) use neural surro- gates of numerical simulators for plastic injection molding and inertial confinement fusion, respectively, to facilitate data-efficient finetuning on real physical data. Kaya & Hajimirza (2019) accelerate numerical simulations for solar cells using neural surrogates, and they use transfer learning to quickly adapt neural surrogates when simulator configura- tions change. Shirobokov et al. (2020) use neural surrogates of non-differentiable, numerical physical simulators, to enable gradient-based optimization of simulator parameters.\nResearchers have used nonnumerical surrogates to opti- mize and explore discrete configuration spaces. Tseng et al. (2019) and Renda et al. (2020) develop neural surrogates of a black-box image signal processing unit and a cycle-accurate CPU simulator, respectively; both techniques enable gradient-based optimization of program inputs, to match some desired input-output behavior. Kwon & Carloni (2020) develop a neural surrogate of a high-level synthesis pipeline for hardware. Using this surrogate, they lower the cost of predicting the performance and cost of hardware configu- rations, and they use transfer learning to lower the cost of\ndeveloping neural surrogates for new configuration spaces.\nUniversal Neural Surrogates. Researchers have de- veloped universal neural surrogates using a variety of architectures. Early work in this area uses long short-term memory networks to predict the results of executing simple, synthetic Python programs (Zaremba & Sutskever, 2015). Later work uses graph neural networks that model program structure in a similar evaluation setup (Bieber et al., 2020). More recently, researchers have trained Transformer-based models on synthetic datasets of programs or large datasets that include programs (Austin et al., 2021; Nye et al., 2021; OpenAI et al., 2023; Bubeck et al., 2023; Gu et al., 2024).\nA.2. Meta-Learning\nMeta-learning can improve data efficiency and transfer learning when there is task-agnostic knowledge that can be extracted from a family of tasks (Hospedales et al., 2022). For example, in the setting we consider, the knowledge of how to execute programs is not specific to any one program but is useful for compiling each program. We describe the technique we employ, hypernetworks (Ha et al., 2017), as well as another meta-learning technique, MAML (model- agnostic meta-learning) (Finn et al., 2017). The most noteworthy difference between the two is that, in the former, the parameter space of the meta-learner and the learners differ, whereas, in the latter, these spaces are the same.\nHypernetworks. Hypernetworks were first proposed by Ha et al. and achieve state-of-the-art results on sequence modeling tasks (Ha et al., 2017). More recent work by Jin et al. proposes a system, N\u00b3, that adapts Transformers to function as hypernetworks that condition on text for few-shot learning on image classification tasks (2020).\nModel-Agnostic Meta-Learning. MAML is a framework for developing neural network initializations that can be finetuned to new tasks with a small amount of data and a few iterations of SGD (Finn et al., 2017). Some authors have noted, however, that MAML couples the task space complexity to the complexity of the individual tasks (Zhmoginov et al., 2022), making the parameter space a bottleneck as the task space grows. Our technique does not suffer from this issue because the hypernetwork can be larger than the generated neural surrogate.\nA.3. Compiling Programs to Neural Networks\nThere exists prior work on compiling programs to neural networks, though usually as a means of understanding neural network architectures, rather than producing neural surrogates of programs.\nLindner et al. present a compiler, Tracr, from the RASP"}, {"title": "B. EXESTACK Generation (Extended)", "content": "Here we provide a detailed explanation of each step in generating EXESTACK, following the flow of Figure 3.\n(1) Preprocessing. We pull the functions in EXESTACK from files that may contain preprocessor directives, which may affect the ability for these functions to be executed in isolation, if left unexpanded. We run the C preprocessor on source files until no more lines begin with \"#\", or we have run it twice, or an invocation fails.\n(2) Extracting Functions. Recognize and collect all functions from each source file.\n(3) Filtering for Pointer-Free Numeric Functions. To filter for numeric functions in C programs, we only include C functions that use exclusively float and double data types in the function signature. Due to the possibility of dynamically sized inputs in the presence of pointers and the ambiguity of whether a pointer represents an input or output, we do not allow pointer types. Consequently, we also do not allow void as an output type. If checking a file for the above conditions takes longer than 8 seconds, we discard it. Note that these filters still allow integral and pointer data types to be used within the function.\n(4) Filtering for Executable Functions and Collecting Outputs. To simultaneously check for executability and collect outputs from a function, we first generate 2,048 sets of inputs by sampling from the uniform distribution $U([-1, 1]^n)$, where n is the maximum number of desired inputs, and we use the same sets of inputs for all programs. We embed these inputs in a C program that includes the func- tion source, as well as an execution harness for collecting outputs. When a program has fewer inputs than the maxi- mum of n, we truncate the inputs we embed to the number of inputs the program has. When a program has more inputs than the maximum of n, we discard it. We compile the har- ness with the C standard math library included, since many numerical functions in C make use of this library. If there are any errors during compilation or execution of a function, we discard the function. Figure 8 shows an example of the execution harness instantiated for a function.\n(5) Filtering for Deterministic Functions. Since a neural surrogate is often a deterministic function of its inputs and weights (e.g., multilayer perceptrons), we filter nondetermin- istic functions from our dataset. We check for determinism by running a function 5 times on the same inputs, all sampled from U(\u22121, 1), and observing whether the output differs on any execution. For neural surrogate architectures that are not deterministic, this step can be omitted.\n(6) Deduplication. We use a whitespace-invariant tokenizer to remove duplicate tokenized programs."}, {"title": "C. EXESTACKCPN Generation", "content": "To produce EXESTACKCPN, we apply the following additional filters to EXESTACK:\n\u2022 Filtering Long Programs. Since BERT-Tiny has a maximum context length of 512 tokens, we remove functions with more than 512 tokens. We first strip comments from all programs to allow more programs to fit within the context.\n\u2022 Filtering Large Outputs. Large or NaN outputs can lead to training instability for neural networks, so we additionally remove functions with any outputs with an absolute magnitude of 10 or larger or a NaN value.\n\u2022 Decontaminating Against PARROTBENCHCPN. It is possible that EXESTACK contains similar programs to those in PARROTBENCHCPN. If we trained a COMPNET on these programs, improvements over random initializa- tion could be due to memorization. To address this prob- lem, we remove any programs from EXESTACK that are syntactically similar to programs in PARROTBENCHCPN.\nFor the evaluation in Section 5, we allow programs with a maximum of 9 inputs in the execution filter of"}, {"title": "C.1. EXESTACKCPN Decontamination", "content": "To ensure the improvements observed in Section 5 are not due to memorization, the final step of EXESTACKCPN generation is decontamination against PARROTBENCHCPN programs. A prevailing decontamination methodology is to remove any syntactic matches up to whitespace (Li et al., 2022; Lozhkov et al., 2024). Though EXESTACK is not contaminated with PARROTBENCHCPN programs accord- ing to this methodology, we strengthen our methodology to additionally remove syntactically similar programs. This de- contamination consists of bespoke syntactic analyses\u2014one for each PARROTBENCHCPN program. For the remainder of this section, we present each of these syntactic analyses and a sample of the near-duplicate programs they detect. In total, decontamination removes 375 functions."}, {"title": "D. PARROTBENCHCPN Generation", "content": "Here, we present the PARROTBENCH programs, explain the modifications we made to PARROTBENCH to produce PARROTBENCHCPN, list the resulting source code, and describe how we generate inputs for these programs.\nD.1. PARROTBENCH Source\nThe kernels in PARROTBENCH are fft (Figure 13), inversek2j (Figure 14), jmeint (Figure 15), jpeg (Figure 16), kmeans (Figure 17), and sobel (Figure 18)."}, {"title": "D.2. PARROTBENCH Modifications", "content": "Due to methodological choices in EXESTACK and architectural choices for COMPNETS"}, {"title": "Learning to Compile Programs to Neural Networks", "authors": ["Logan Weber", "Jesse Michel", "Alex Renda", "Michael Carbin"], "abstract": "A neural surrogate of a program is a neural network that mimics the behavior of a program. Researchers have used these neural surrogates to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Researchers traditionally develop neural surrogates by training on input-output examples from a single program. Alternatively, language models trained on a large dataset including many programs can consume program text, to act as a neural surrogate. Using a language model to both generate a surrogate and act as a surrogate, however, leading to a trade-off between resource consumption and accuracy. We present neural surrogate compilation, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find that they produce neural surrogates that are 1.9-9.5\u00d7 as data-efficient, produce visual results that are 1.0-1.3\u00d7 more similar to ground truth, and train in 4.3-7.3\u00d7 fewer epochs than neural surrogates trained from scratch.", "sections": [{"title": "1. Introduction", "content": "A neural surrogate is a neural network that models a subset of the observable behavior of a program (Renda et al., 2021). Neural surrogates have been used to automatically configure image signal processing units and CPU simulators (Tseng et al., 2019; Renda et al., 2020), improve the accuracy of manufacturing and physics simulations (Tercan et al., 2018; Kustowski et al., 2020), accelerate the computer architecture design process (\u0130pek et al., 2006), and accelerate computations in signal processing, robotics, 3D games, compression, machine learning, and image processing (Esmaeilzadeh et al., 2012a).\nNeural Surrogate Training. The research community has developed a variety of techniques to train neural surrogates. The traditional approach is to train a neural surrogate of a single program by collecting and curating a dataset of input-output pairs and then training a neural network to predict the program's output given an input (Renda et al., 2021).\nAnother point in the spectrum is to amortize the cost of training neural surrogates by training a universal neural surrogate: a neural network that directly consumes the text of a program and predicts the program's output for a given input (Zaremba & Sutskever, 2015; Nye et al., 2021; Gu et al., 2024). A key benefit of universal neural surrogates is that one only needs to create a dataset once. Once trained, a universal neural surrogate can act as the neural surrogate of a given program without the need to curate a dataset of program-specific, input-output pairs.\nHowever, universal neural surrogates necessarily use the same model to process the program text as is used to predict the program output, and accurate prediction may require multiple forward passes (Nye et al., 2021; Wei et al., 2022). These limitations pose challenges for deploying such a model as a neural surrogate because small models may not be able to emulate complex programs (Zaremba & Sutskever, 2015) and large models (OpenAI et al., 2023) may not be able to execute in the resource-constrained environments where neural surrogates have been used (Esmaeilzadeh et al., 2012a; Mendis, 2020; Munk et al., 2022).\nOur Approach: Neural Surrogate Compilation. To maintain the benefits of universal neural surrogates while bypassing the above limitations, we propose to use a neural surrogate compiler. A neural surrogate compiler is a system that accepts a program's text as input and produce an initial neural surrogate of the program, which can vary in behavioral quality. Similarly to a traditional compiler, a neural surrogate compiler requires a significant upfront cost that is amortized over the generation of initializations for many neural surrogates. We demonstrate in this work that when compared to the traditional approach of training a neural surrogate from a random initialization, neural surrogates produced by neural surrogate compilers can be finetuned to closely mimic the behavior of the program at a lower cost, as measured in data efficiency and training time."}, {"title": "2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system that is specialized to a family of neural surrogate architectures to accept a program's text as input and produce an initial neural surrogate of the program. Figure 1 presents the neural surrogate compilation workflow alongside the traditional workflow for developing a neural surrogate. In a traditional neural surrogate development workflow, one collects training data (B)), trains the neural surrogate until its error meets the desired threshold (C), and then uses it in place of the original program (D). Neural surrogate compilation (A)) introduces a new, initial step in the neural surrogate compilation workflow in which a neural surrogate compiler maps the program text to a neural network initialization for use in the training of the neural surrogate. The typical strategy to train a neural surrogate is through supervised learning of a neural network with a curated dataset of input-output pairs from the program (Renda et al., 2021).\nIn this section, we formalize the problem of efficiently training a neural surrogate and introduce a new approach to solving this problem using a neural surrogate compiler.\n2.1. The Efficient Surrogate Training Problem\nWe first formalize the problem of training a neural surrogate. We assume we are given a program text $p: P$ that denotes a function $[p] : I_p \\rightarrow O_p$, where $P$ is the space of programs under consideration, $I_p$ is the type of values $p$ accepts as input and $O_p$ is the type of values $p$ produces as output. We also assume a target neural surrogate architecture $\\alpha: R^d \\rightarrow I_p \\rightarrow O_p$, which takes a set of parameters $\\theta : R^d$ and produces a surrogate function from $I_p$ to $O_p$. The goal is to find a set of parameters $\\theta : R^d$ such that the neural surrogate $\\alpha(\\theta): I_p \\rightarrow O_p$ has low approximation error:\n$\\sqrt{E_{i:I_p}.\\alpha(\\theta)(i) \\approx [p](i)}$\nTo measure the quality of surrogate outputs, we use a loss function $l: O_p \\times O_p \\rightarrow R_{>0}$ that measures the difference between the output of the program and the output of the surrogate. To measure overall surrogate quality, we use the expected loss over a distribution of inputs:\n$L(\\alpha(\\theta), p) = E_{i \\sim I_p}[l(\\alpha(\\theta)(i), [p](i))]$     (1)\nAs with most learning problems, a challenge in training neural surrogates is that the error of a surrogate depends on the budget dedicated to collecting training data (input-output pairs of the program) and the number of epochs used to train the surrogate. We formalize these costs by defining a training procedure $t_\\alpha :P \\times R_{\\geq 0} \\times N \\rightarrow R^d$ for a given surrogate architecture $\\alpha$ as a random function that takes program text $p$, a training data budget $b: R_{\\geq 0}$, and training time budget $n : R_{\\geq 0}$ and produces a set of parameters $\\theta : R^d$ for the surrogate.\nWe then define the efficient surrogate training problem as finding a training procedure $t_\\alpha$ for a given program $p$, architecture $\\alpha$, sample budget $b$, training time budget $n$, and loss function $l$ that minimizes the expected loss of the resulting surrogate:\n$\\arg \\min_{t_\\alpha} E_{\\theta \\sim t_\\alpha (p,b,n)} [L(\\alpha(\\theta), p)],$"}, {"title": "2.2. Neural Surrogate Compilation", "content": "A neural surrogate compiler is a system $\\phi: (p: P) \\rightarrow R^{d_p}$ that accepts program text $p$ and produces parameters $\\theta \\in R^{d_p}$ for a neural surrogate architecture $a_p$ depending on the program $p$. We use a neural surrogate compiler to solve the efficient surrogate training problem.\nWe formalize the development of a neural surrogate compiler as an optimization problem. The goal is to develop a system $\\phi$ such that for every program $p$, the surrogate $f = a_p(\\phi(p))$ can be trained efficiently. Optimizing for a system that generates surrogates that can be trained efficiently is challenging. As a simple proxy, we optimize for a system that generates surrogates that achieve low loss:\n$\\arg \\min_{\\Phi\\in R^d} E_{p\\sim P} [L(a_p(\\phi(p)),p)].$"}, {"title": "3. COMPNET", "content": "The COMPNET architecture is an implementation of a neural surrogate compiler using hypernetworks. We explain the architecture, how to train it, then how to extract neural surrogates from its outputs.\n3.1. Architecture\nFigure 2 presents the design of a COMPNET. A COMP- NET accepts program text $p: P$ as input and produces parameters $\\theta \\in R^d$ for a neural surrogate architecture $a : R^d \\rightarrow I \\rightarrow O$ with as many inputs as the largest architecture one wishes to compile to and a single output. We call this architecture a covering architecture.\n(A) First, COMPNET tokenizes an input program (1), resulting in a sequence of tokens (2) including the distin- guished BERT classification token [CLS].\n(B) COMPNET then uses a BERT encoder (Devlin et al., 2019) to embed the sequence of tokens, resulting in an em- bedding per token. The output of this step is the embedding of the classification token (3); COMPNET discards the embeddings of the other tokens.\n(C) Next, COMPNET uses a parameter head, implemented as a single linear layer, to map the classification token em- bedding to a neural surrogate parameter vector (4).\n(D) Then, COMPNET interprets the vector of parameters as the weights and biases of the covering architecture. The output of this step is a neural surrogate of the input program.\n(E) Finally, COMPNET executes the neural surrogate with the interpreted parameters on a program input (5) to pro- duce a prediction of the program output (6)\n3.2. Training\nTraining a COMPNET requires a dataset of programs and input-output pairs for each program. Note that this dataset is not considered as part of the budget in the efficient surrogate training problem, since it is amortized over all programs the COMPNET is used to compile.\nEach step of training proceeds by selecting a batch of pro- grams and input-output pairs for those programs, generating neural surrogate parameters for each program, interpreting the neural surrogate parameters as parameters for the covering architecture, executing each neural surrogate with"}, {"title": "3.3. Surrogate Extraction", "content": "The output of a COMPNET is parameters for the covering architecture, which might not match the number of inputs and outputs of the program being compiled. To adapt the covering architecture to the target number of inputs, one finetunes the resulting architecture on data where the excess inputs are set to zero, allowing one to then remove the weights in the input layer corresponding to the excess inputs (see Appendix N for details on this choice). To adapt the covering architecture to the target number of outputs, one clones the weights for the single output in the output layer for each new output that is needed (see Appendix O"}, {"title": "4. EXESTACK", "content": "The strategy we presented in Section 3 for learning a neural surrogate compiler requires a dataset of programs and input-output examples describing the behavior of each pro- gram. To meet this requirement, we developed EXESTACK, a dataset of 69,083 pointer-free, numerical, executable, deterministic C programs and corresponding input-output examples. EXESTACK is based on The Stack (Kocetkov et al., 2022), a dataset of 3 TB of permissively licensed source code written in various programming languages scraped from GitHub.\nFigure 3 summarizes the process of generating EXESTACK (see Appendix B for details). The restriction to pointer-free functions simplifies the EXESTACK generation method- ology, and yet, a model trained on EXESTACK could still handle programs using statically-sized data structures containing numeric data (e.g., arrays), as they can be trans- formed into functions with a fixed number of arguments."}, {"title": "5. Evaluation", "content": "To evaluate the claim that neural surrogate compilation lowers the development cost of neural surrogates, we answer the following research questions.\nRQ 1: Does a neural surrogate initialized by a COMPNET converge to a lower test loss than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 2: Does a neural surrogate initialized by a COMPNET produce better results in an application than a neural surrogate initialized randomly, for a fixed training set size?\nRQ 3: Does a neural surrogate initialized by a COMPNET converge to a target test loss in fewer epochs than a neural surrogate initialized randomly?\nOur results demonstrate that COMPNETS lead to improve- ments in data efficiency (Section 5.2), perceptual quality (Section 5.3), and training time (Appendix J).\n5.1. Methodology\nTo develop and evaluate COMPNETS, we select a BERT architecture for the neural surrogate compiler and a multilayer perceptron for the covering architecture, we produce datasets that COMPNETS can be trained and evaluated on, we introduce alternative initialization methods to compare against, and we finetune surrogates produced by each of the initialization methods.\n5.1.1. COMPNET ARCHITECTURE\nWe use the BERT-Tiny architecture (Turc et al., 2019) for the BERT encoder in COMPNET, and we adapt a neural surrogate architecture from Esmaeilzadeh et al. (2012a) into a covering architecture for this COMPNET.\nThe architecture used by Esmaeilzadeh et al. (2012a) is a multilayer perceptron consisting of a single input, a hidden layer of 4 neurons, another hidden layer of 4 neurons, and 2 outputs, and it uses a sigmoid activation function. For their evaluation, the authors introduce a suite of benchmarks, PARROTBENCH, consisting of numerical programs from various domains. The authors apply their"}, {"title": "5.1.2. DATASETS", "content": "We evaluate the effectiveness of COMPNETS on test programs from EXESTACKCPN and programs from PARROTBENCHCPN (see Table 1). These datasets are refinements of EXESTACK and PARROTBENCH that are compatible with the instantiation of the COMPNET architecture described above.\nEXESTACKCPN. We produce EXESTACKCPN_ by applying additional filters to EXESTACK, resulting in 37,772 programs. See Appendix C for details.\nFrom the full set of programs, we create a training, valida- tion, and testing set using an 80/10/10 split. Each program has input-output examples, so we additionally create a train- ing and testing set for these examples using a 50/50 split. In Sections 5.2 and Appendix J, we evaluate performance on EXESTACKCPN using 1,000 programs from the testing set.\nPARROTBENCHCPN. PARROTBENCHCPN programs come from a diverse set of application domains, they are all written in C, each consists of a single function, and they are numeric in nature, making them suitable for evaluating COMPNETs. Table 1 shows the programs in PARROTBENCHCPN, including descriptions of the computations and input datasets. In Appendix D, we explain how we chose these programs, we list the program source, and we explain how we generated input datasets."}, {"title": "5.1.3. ALTERNATIVE INITIALIZATION METHODS", "content": "Besides random initialization, we compare COMPNETS to two alternative initialization methods: model-agnostic meta learning (Finn et al., 2017) and pretrained initializations. Neither initialization method conditions on program text, so they both result in constant initializations that one uses for every program. We briefly describe these techniques here and how we train them, and we provide shorthand for referencing each initialization method. In Appendix A, we survey related work in this area in detail.\nModel-Agnostic Meta Learning. Model-agnostic meta learning (MAML) is a meta-learning technique for producing neural network initializations that can be quickly finetuned to achieve low error on a given task. One trains MAML initializations by sampling tasks from some space of training tasks, finetuning on them, and backpropagating through the finetuning process into the initialization.\nPretrained Neural Surrogates. A simpler alternative to MAML is to train a single neural surrogate on the union of all input-output examples from programs in a dataset such as EXESTACKCPN. We call initializations trained in this way pretrained neural surrogates.\nTraining. We train 3 instances of each initialization method on EXESTACKCPN training programs using the same covering architecture as COMPNETS. See Appendices F, G, N, and O for details on MAML training, pretrained surrogate training, variable-input support, and variable-output support, respectively.\nInitialization Method Shorthand. We use shorthand names for each initialization method in figures. We refer to COMPNETS as \u201cCPN\u201d, MAML as \u201cMAML\", pretrained surrogates as \"PTS\u201d, and random initialization as \u201cRND\u201d.\""}, {"title": "5.1.4. FINETUNING SURROGATES", "content": "Here, we collect the finetuning methodology for surrogates in this evaluation, including optimization methods, hyper- parameters, random seed behavior, and how we measure the improvements achieved by these surrogates.\nFor all surrogates produced by the initialization methods we consider, we use the following finetuning methodology. We use the Adam optimizer with no weight decay, a learning rate of 0.01, and MSE as the loss function. The only"}, {"title": "5.2. Data Efficiency Improvements", "content": "To assess whether COMPNETS improve data efficiency, we use COMPNETS to initialize neural surrogates, finetune on subsets of training data of various sizes, and then compare the results to those of other initialization methods. We detail the methodology of this experiment then present results.\n5.2.1. \u039c\u0395\u03a4HODOLOGY\nWe now describe the configurations we sweep over and the methodology we use to finetune surrogates.\nExperiment Configurations. In this experiment, we sweep over configurations consisting of a program, a dataset size, and an initialization method (e.g., a COMPNET). Each dataset size specifies the percentage of the training data to train neural surrogates on. We sweep over the following percentages: {0%, 0.1%, 1%, 10%, 100%}.\nDataset Selection. Given a configuration consisting of a program, a dataset size percentage $c \\in [0, 1]$, and an initialization method, we select a random subset $D_{sub}$ of the training data $D_{train}$ of size $c|D_{train}|$. We use an 80/20 split to divide $D_{sub}$ into train and validation sets $D_{sub,train}$ and $D_{sub,val}$. We sample 9 different subsets of this size and use a different training seed for each subset, yielding 9 trials total.\nFinetuning. For each trial, we initialize a neural surrogate according to the initialization method. We then train on $D_{sub,train}$ for 5,000 epochs. The final test loss we report for a trial is the test loss at the epoch closest to the epoch with the lowest validation error. When the dataset size is 0%, we use the test loss at the final epoch."}, {"title": "5.2.2. RESULTS", "content": "Figures 4 and 5 show finetuning results for a sample of 1,000 EXESTACKCPN test programs and PARROT- BENCHCPN, respectively. See Appendix H for the test losses used to compute improvements.\nEXESTACKCPN Test Programs. COMPNETS achieve the best results on average, with a 9.50\u00d7 improvement over random initialization, whereas MAML and pretrained surrogates achieve only a 1.09\u00d7 and 1.08\u00d7 improvement on average. COMPNETS improve over random initialization in as low as the 21st percentile of configurations, whereas MAML and pretrained surrogates improve over random ini- tialization after the 35th and 37th percentiles, respectively.\nCOMPNETS improve on EXESTACKCPN test programs most prominently in the zero-shot regime, where the improvement is 84.40\u00d7 over random initialization, whereas MAML and pretrained surrogates achieve improvements of 1.42x and 2.63\u00d7, respectively. The zero-shot regime"}, {"title": "5.3. Neural Surrogates for Color Quantization", "content": "To assess whether a COMPNET can improve the quality of results in an end-to-end application, we use a trained COMPNET to initialize a neural surrogate used for color quantization and compare it to other initialization methods (Kanungo et al., 2002). Color quantization is the process of reducing the number of distinct colors in an image. For example, Figure 6 depicts an image of a baboon color quantizated to five colors.\n5.3.1. \u039c\u0395\u03a4HODOLOGY\nWe follow the methodology for color quantization from Kanungo et al. (2002) who apply k-means clustering to the (R, G, B) vectors representing the colors of pixels of an image and select the cluster centroids as the colors in the palette. We run k-means clustering for 40 iterations or until the distance between the old centroids and new centroids is less than $1 \\cdot 10^{-5}$. Each pixel color is then remapped to the closest color in the palette.\nWe use the Euclidean distance function to compute the distance between two RGB vectors. We consider both"}, {"title": "5.3.2. RESULTS", "content": "Figure 6 depicts the result of applying 5-color quantization to an image of a baboon using surrogates trained on dataset sizes of 0% and 0.1% of the training set. Figure 7 shows quantitative results comparing initialization methods on 5-color quantization at various dataset sizes. Each entry shows the average and standard deviation of a metric over all trials and instances of an initialization method. See Appendix I for more dataset sizes and color palette sizes.\nVisual Results. At a dataset size of 0%, COMPNET- and MAML-initialized surrogates are the only surrogates that produce images with detail. The image produced by a COMPNET surrogate shows more detail than the image pro- duced by a MAML surrogate, which primarily captures de- tails on the nose. At a dataset size of 0.1%, all initialization methods produce images that resemble the original image. Images produced by COMPNET-initialized and pretrained surrogates have a higher contrast than images produced by MAML-initialized and randomly initialized surrogates.\nQuantitative Results. At all dataset sizes, COMPNET- initialized surrogates have the lowest MSE and the highest SSIM on average. Among the other initialization methods, there is no consistent winner across dataset sizes.\nThe variance for the MSE results is comparable across all initialization methods and is high enough that there is overlap among methods. For example, at a dataset size of 0%, an MSE result that is one standard deviation below the mean for MAML is lower than the mean for COMPNETS. However, for all other dataset sizes the mean MSE for COMPNETS is lower than the mean MSE for MAML, even after subtracting a single standard deviation.\nFor the SSIM results, at smaller dataset sizes, the variance is high enough that there is overlap among methods. At larger dataset sizes though, the results are more clearly separated, with COMPNETS having the highest mean SSIM, even when one adds a single standard deviation to the mean SSIM for each of the other initialization methods."}, {"title": "6. Conclusion", "content": "In this paper, we presented the concept of a neural surrogate compiler and demonstrated how a neural surrogate compiler can be implemented with COMPNETS. We provided a dataset, EXESTACK, that one can use to learn neural"}]}]}