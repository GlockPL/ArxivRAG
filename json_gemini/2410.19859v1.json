{"title": "Multi-Modal Transformer and Reinforcement Learning-based Beam Management", "authors": ["Mohammad Ghassemi", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "abstract": "Beam management is an important technique to improve signal strength and reduce interference in wireless communication systems. Recently, there has been increasing interest in using diverse sensing modalities for beam management. However, it remains a big challenge to process multi-modal data efficiently and extract useful information. On the other hand, the recently emerging multi-modal transformer (MMT) is a promising technique that can process multi-modal data by capturing long-range dependencies. While MMT is highly effective in handling multi-modal data and providing robust beam management, integrating reinforcement learning (RL) further enhances their adaptability in dynamic environments. In this work, we propose a two-step beam management method by combining MMT with RL for dynamic beam index prediction. In the first step, we divide available beam indices into several groups and leverage MMT to process diverse data modalities to predict the optimal beam group. In the second step, we employ RL for fast beam decision-making within each group, which in return maximizes throughput. Our proposed framework is tested on a 6G dataset. In this testing scenario, it achieves higher beam prediction accuracy and system throughput compared to both the MMT-only based method and the RL-only based method.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of wireless communication technologies and the advances towards 6G networks aim to satisfy the increasing data rate demands of emerging applications. Among these advances, millimeter-wave (mm-wave) and Terahertz (THz) frequency bands are key since they can provide abundant spectral resources for data transmission [1]. However, communication in higher frequency bands usually suffers from significant propagation losses. As a result, massive multiple-input multiple-output (M-MIMO) antenna arrays have been used to boost signal power and minimize interference. Beam management, the process of assigning beams to users, is essential for optimizing network performance [2].\nConventional position-based beam prediction methods face significant challenges due to user mobility and signal changes [3]. One solution is to integrate visual information into beam management to mitigate localization errors [4]. With the availability of data in more modalities, such as radar and light detection and ranging (LiDAR) data, there is a potential to further improve the accuracy of beam prediction by using more information from the user. On the other hand, multi-modal transformers (MMTs) have recently emerged, known for their remarkable ability to manage diverse data modalities [5] [6]. They facilitate information processing and feature extraction, which enhances contextual understanding and enables more accurate beam predictions [7].\nConsidering these benefits, we aim to leverage MMT in our work to effectively manage the diverse modalities in wireless communication systems. Nevertheless, adopting MMT poses challenges, particularly when increasing the number of labels or ground truth data for supervised learning, which can lead to lower accuracy. To address these challenges, we propose a beam management framework by integrating MMT with reinforcement learning (RL) in a two-step approach. The main contributions of this work are concluded as follows:\n\u2022\n\u2022\nIn this work, we integrated MMT and RL as a two-step framework for multi-modal beam management. In the first step, we reduce the set of decisions by grouping the beam indices and then use MMT to predict the optimal group for transmission based on a variety of data modalities. Our second step is to use RL for beam decisions within each group to maximize throughput. This solves the existing challenge of effectively handling multi-modal data in beam management tasks. To the best of our knowledge, this is the first proposal to integrate MMT and RL to enhance beam management.\nUnlike previous works focusing solely on beam prediction accuracy, this work also shows how efficient beam management can further enhance system throughput. The framework is tested with a real-world 6G dataset, demonstrating its practical effectiveness."}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "Fig.1 illustrates our proposed two-step beam management framework. In this system, we assume that there is one base station (BS) and multiple user equipments (UEs). The framework aims to optimize the communication performance between the BS and UEs and maximize the throughput by selecting the optimal beam index. We consider multi-modal data as the input of the framework, which includes vision data, radar data, LiDAR data, and GPS data.\nIn the first step, we categorize the available beam indices into multiple groups and use MMT to process various data modalities to predict the optimal beam group. In the second step, we utilize RL for beam decision-making within each group to maximize throughput. Through this framework, the advantages of MMT in handling multi-modal information and the advantages of RL in adapting to dynamic environments can be jointly utilized. In the following part of this section, the communication model for beam management is introduced and the problem formulation is described. In this work, a downlink orthogonal frequency-division multiplexing (OFDM) cellular system is considered. The received signal-to-interference-plus-noise ratio (SINR) at the uth UE can be calculated [8]:\n$$SINR_u(\\theta) = \\frac{P_u(\\theta)}{W_u.N_o + \\sum_{u' \\in U_{-u}} P_{u'}(\\theta)}$$ (1)\nwhere $P_u(\\theta)$ denotes the received power of UE u, $W_u$ denotes the bandwidth, $N_o$ denotes the noise power density, $\\theta$ denotes the angle between the UE and the BS, $U_{-u}$ denotes the set of UEs in a cell served by a BS except uth UE, and $P_u (\\theta)$ denotes interference power from $U_{-u}$ users. The received power is [9]:\n$$P_u(\\theta) = P_t . \\frac{G_t(\\theta) . G_r(\\theta) . \\lambda^2}{(4\\pi R)^2} PL,$$ (2)\nwhere $P_t$ denotes the transmit power, $G_t$ denotes of gain of the transmitting antenna, $G_r$ denotes of gain of the receiving antenna, $\\lambda$ denotes signal wavelength, $PL$ denotes path loss, and $R$ denotes distance between user and BS. $G_t$ is influenced by the directivity of the antenna array, also known as the array gain pattern, which can be defined as:\n$$U(\\theta) = \\frac{sin(\\frac{N_k a d_a cos \\theta}{2})}{sin(\\frac{k_a d_a cos \\theta}{2})}^2,$$ (3)\nwhere $N_a$ denotes the number of antenna elements, $k_a = \\frac{2\\pi}{\\lambda}$ denotes the wavenumber, and $d_a$ denotes the spacing between the antennas [9]. With this equation, the gain (or directivity) of an antenna array can be described as a function of the beam angle $\\theta$. Therefore, by selecting the optimal beam that steers the signal towards the desired UE and away from interfering users, we can increase $P_u$ at the desired UE.\nThe system aims to dynamically adjust beamforming parameters to maximize SINR, thereby improving throughput which is described as:\n$$b(\\theta) = \\sum_{u \\in U} W_u log_2(1+ SINR_u(\\theta)).$$ (4)\nTherefore, the corresponding throughput maximization problem can be formulated as:\n$$maximize \\sum_{i=0}^{K_d} b(\\theta)^{(i)}$$ (5a)\nsubject to (1) \u2013 (4) (5b)\n$$\\sum_{n \\in N_{ue}} X_{u,n} = 1$$ (5c)\n$$0 = g(X_{u,n})$$ (5d)\nwhere $K_d$ denotes the number of iterations, $N_{ue}$ denotes the number of users, and $x_{u,n}$ is a binary variable, with $X_{u,n} = 1$ indicating that the beam antenna index n is allocated to UE u; otherwise, $X_{u,n} = 0$. Also, g($x_{u,n}$) is a look-up table mapping $X_{u,n}$ to $\\theta$. This ensures that each UE is associated with a specific beam antenna index, governed by the values of g($x_{u,n}$). Eq. (5d) guarantees one beam can only be allocated to the UE."}, {"title": "III. MMT AND RL-BASED BEAM MANAGEMENT", "content": "In this section, we present the detailed implementation of our proposed method, as depicted in Fig.2.\nA. Group Beam Prediction by MMT\nIn the first step, as illustrated in Fig.2, the 64 optional beam angles are divided into $N_g$ = 8 groups, and a pre-trained MMT proposed in [3] is utilized for beam group prediction. The MMT is firstly fine-tuned through supervised learning to choose the group based on similar predicted features (0). The input of the MMT is a multi-modal data source, including images, LiDAR data, and radar data. The output is defined as the group beam indices. The architecture of the MMT model includes two parts: the convolutional neural networks (CNNs) and the transformer models. CNNs are used for feature extraction, while transformers are used for learning the relationships between these features. This combination allows the model to capture complex patterns in multi-modal data and facilitates effective beam clustering. Additionally, this architecture also includes a deep residual network (ResNet) for encoding features. The fusion of feature maps from different modalities creates a feature vector. Moreover, we concatenate calibrated GPS locations (longitude and latitude) with the vector and pass them through multi-layer perception (MLP) layers to produce weights for 64 beam indices using the softmax function [3]. This step efficiently reduces the search space for the subsequent RL step.\nWith self-attention mechanisms, transformers are well-suited for capturing correlations and relationships in data from dynamic environments. In the MMT model, encoders process different modalities before passing them to the transformer, allowing the system to handle multiple sources of information simultaneously. The design of MMTs allows for scalability, as additional encoders can be easily added to handle new modalities due to their transformer-based architecture. The MMT architecture includes a ResNet32 backbone, pre-trained on the ImageNet dataset, providing a robust foundation for understanding multi-modal data. Fine-tuning on the DeepSense 6G dataset further adapts the model to dynamic wireless communication environments, enhancing its ability to perform tasks like beam management in challenging scenarios.\nThe flexibility of MMTs is also demonstrated by their performance with varying user numbers in a complex 6G environment during the simulations. Moreover, real-world datasets may have challenges such as varying data quality and synchronization issues. MMTs leverage self-attention to align key information across modalities and compensate for these issues.\nB. Optimal Beam Selection using RL\nIn the second step, the RL agent operates in a simulated environment based on a real-world scenario, aiming to select the optimal beam among $N_\\theta$ = 8 beams within each group ($\\theta$), where $N_\\theta$ denotes the number of beams within each group. By incorporating inputs from the MMT model and GPS data, the agent learns an optimal policy for beam selection through iterative interactions with the real-world environment. The agent observes the state, makes decisions by selecting the optimal beam, receives the reward, and updates its policy iteratively. This feedback mechanism allows the agent to refine its decision-making policy over time to maximize the reward function. In this work, we employ Q-learning, where the RL agent uses a Q-table to store rewards. This choice is driven by our finite action and state spaces, which are well-suited for our approach. The action space consists of 8 options, and the state space is O($N_b$ \u00d7 $N_g$ \u00d7 $N_{ue}$ \u00d7 $N_{loc}$), where $N_g$ denotes the number of groups and $N_{loc}$ denotes the number of location samples. The state space depends on the number of users. This setup allows us to represent all possible state-action pairs without requiring function approximation, making it more ap-propriate than Deep Reinforcement Learning (DRL). To avoid adding further complexity to the MMT model, we chose Q-learning over DRL. Q-learning offers significant computational efficiency that allows us to maintain high beam management accuracy while minimizing computational complexity without the need for intensive training processes associated with DRL. The Markov decision process (MDP) is defined as follows:\n\u2022\nState: The state can be represented as $s_t$ = ($u_t$, $v_t$), where: $u_t$ denotes the locations of the users at time t, and $v_t$ is a set of group indices derived from the output of the MMT at time t.\n\u2022\n\u2022\nAction: The action of the agent is to decide which beam to assign to a user during each time slot. It can be represented as $a_t$ = $b_t$, where $b_t$ denotes the beam index that the agent chooses to allocate to a user at time t.\nReward: If the system throughput exceeds a set threshold (b > $r_{th}$), the agent receives a constant reward; otherwise, it incurs a penalty. This threshold-based reward approach helps prevent issues like reward sparsity or misleading signals, ensuring a more stable learning environment for the agent [10].\nAt each time step t, the agent observes the environment's state $S_t$ and selects an action $a_t$, leading to a transition to a new state $s_{t+1}$ and receiving a reward $r_t$. The primary objective in RL is to optimize the cumulative reward $C_t$ over time, defined as $C_t$ = $\\sum_{k=0}^{K_d} \\gamma^k r_{k+t}$, where \u03b3 (\u03b3\u2208 [0,1]) is the discount factor for future rewards.\nThe action-value function $Q_\\pi(s, a)$ represents the expected return when taking action a in state s following policy \u03c0. RL seeks to determine the optimal policy $\\pi$* that maximizes the Q-function across all possible policies. In Q-learning, the Q-values are updated by Q($s_t$, $a_t$) \u2190 Q($s_t$,$a_t$) + a[$r_{t+1}$ + $y max_a$ Q($s_{t+1}$, a) \u2013 Q($s_t$, $a_t$)] where a is the learning rate and $r_{t+1}$ is the reward at time t + 1. Accurate estimation of the Q-function leads to determining the optimal policy $\\pi$* at a given state s, selecting the action a with the highest value.\nC. DeepSense 6G Dataset and Data Preprocessing\nThe DeepSense 6G dataset [11] encompasses multi-modal data across diverse deployment scenarios. In this work, we use the Scenario 32 dataset proposed in [12], collected from a two-way city street environment with vehicles and pedestrians, captured by two synchronized data collection units positioned on opposite sides of the street. This scenario represents real-world urban wireless communication challenges and is available online.\nBefore feeding the multi-modal data into the beam man-agement framework, preprocessing ensures data quality and consistency through cleaning, normalization, and feature scaling. The dataset includes RGB images with segmented masks indicating object locations, and LiDAR data capturing 3D point clouds that describe the environment and obstacles from various angular views. Following previous work [13], we apply Static Clutter Reduction (SCR) [14] to the LiDAR data to remove stationary and irrelevant objects, reducing noise and improving data quality. The filtered point cloud retains only key spatial measurements, after which Principal Component Analysis (PCA) is used to extract the 1 most significant features, reducing dimensionality while preserving essential information. For radar data, the goal is to determine the distance, angles, and velocity of moving objects. To extract explicit velocity information, we concatenate the Range-Angle and Range-Velocity Maps, preserving speed data for mov-ing cars. Radar signals reliably measure speed regardless of weather or lighting conditions."}, {"title": "IV. SIMULATION AND RESULTS", "content": "A. Parameter Settings\nIn the simulation, the transmit signal power is set to $P_t$ = 40 dBm, and the background noise power is set to $N_o$ = -10 dBm. We consider a single BS equipped with $N_a$ = 64. By default, we consider $N_{ue}$ = 5 and $N_{loc}$ = 5. However, for calculating throughput, we evaluate scenarios with $N_{ue}$ = 10, 15, 20, and 25 UEs. The RL agent employs an epsilon-greedy policy to balance exploration and exploitation. Initially, a high epsilon value of 0.9 promotes the exploration of various beam con-figurations. As episodes progress, the epsilon value decreases to 0.6, prioritizing the use of the best-known configurations based on prior learning. For simulation purposes, we use GPS information to determine the distance between the BS and users. The channel model 128.1 + 37.6 log(distance(km)) rep-resents a Line-of-Sight (LOS) path loss model used in wireless communication systems. The number of beam measurements k refers to the minimum set of beams that must be evaluated to accurately determine the optimal beam, balancing between minimizing overhead and maximizing selection accuracy; in our simulation, we choose k from 1 to 5. Also, we choose $\\alpha$ = 1 = 14. For MMT, we partition datasets into a 90% training set and a 10% validation set for optimizing model weights. The validation set is used for hyperparameter configurations. We select epochs = 150, learning rate = 0.0005, and batch size = 6. For RL, we use a discount rate of 0.9 and a learning rate of 0.001 [3].\nThere is a difference in time scales between the MMT and RL's processing interval. The MMT processes incoming data from the environment every T = 100 ms and pre-dicts beam groups. The RL updates every four TTIs, which is approximately 0.5716 ms. Therefore, the RL episode is $K_d = \\frac{100}{0.5716} \\approx$ 174 steps. It allows the MMT to process larger windows of time while the RL agent operates at shorter and more frequent intervals. The results are averaged over 10 Monte Carlo rounds of training.\nFig.3a compares the convergence performance of our pro-posed MMT-RL method with a baseline RL-only approach when the number of users is 5. The x-axis represents the episode number in the simulation, and the y-axis represents the average cumulative reward. In the RL-based baseline, the agent selects from 64 optional beams directly, whereas the MMT-RL approach benefits from a reduced search space by initially predicting beam groups, narrowing down to 8 beams per group. This significant reduction allows the RL agent to focus on a more targeted set of beam candidates, leading to a higher average cumulative reward compared to the baseline. The MMT-RL approach demonstrates greater effectiveness in maximizing reward due to its efficient beam management strategy. The average cumulative reward plot further illustrates the agent's performance improvement over time. As shown in Fig.3a, both methods exhibit a sharp increase in cumulative reward during the initial episodes, indicating exploration of different strategies. The curves stabilize after episode 50, indicating that the agent has successfully learned and is now focused more on exploiting rather than exploring new ones.\nFig.3b compares the performance of the throughput of MMT-RL with baseline MMT and RL-based approaches with varying numbers of users. The x-axis represents the number of users in the simulation, and the y-axis denotes the achieved throughput, indicated by spectral efficiency (SE). We assess beam management performance in both the baseline MMT and RL-based methodologies using 64 narrow beam indices and calculate SE. As evident from Fig.3b, the MMT-RL approach achieves significantly higher throughput compared to the baseline MMT and RL approaches. This indicates that our proposed method is more effective at maximizing throughput in the simulated wireless communication network. Even without RL, the use of MMT in both methods might provide some throughput advantage over traditional beam management techniques. The reason for this is that MMT can capture complex relationships between different modalities of data that can influence beam management. Our proposed method demonstrates superior throughput in wireless network scenarios compared to the MMT and RL methods. With 5 users, our method achieves 6.25 bit/s/Hz, while MMT and RL achieve 5.1 bit/s/Hz and 3.05 bit/s/Hz, respectively. When the number of users increases to 25, our method achieves 20.45 bit/s/Hz, compared to MMT's 16.3 bit/s/Hz and RL's 9.6 bit/s/Hz. These results highlight our method's effectiveness in maintaining higher throughput as user density increases.\nFig.3c compares the beam selection accuracy of MMT-RL with baseline MMT and RL-based approaches against k. In the MMT approach, supervised learning was employed without considering throughput optimization. Despite RL's inability to utilize multi-modal information and reliance solely on GPS, MMT-RL leverages the strengths of both MMT and RL to enhance accuracy and throughput. Increasing the value of k improves accuracy across all models by exploring more potential beams, reducing the chance of missing optimal selections. However, this also increases overhead costs. Our proposed method achieves top-1, top-3, and top-5 beam selec-tion accuracy of 59.5%, 81.0%, and 84.2%, respectively. By choosing k=5, our approach reaches 84.2% accuracy, which is 13% higher than MMT and 28% higher than RL.\nC. Computational Complexity Analyses\nIn this section, we analyze the computational complexity of the proposed method based on runtime analysis. We utilized an NVIDIA RTX 3060 GPU and a CPU featuring Intel Core i7 11370H for our experiments. The inference time of the proposed method is shown in Table I. The results indicate that the single inference time for the MMT model is 74.8 ms, while the RL inference time is 0.031 ms for 5 users and 0.155 ms for 25 users. The combined inference time for the proposed method ranges from 74.831 to 74.955 ms, meaning the RL step adds less than 0.2% to the overall processing time. This reinforces the point that while there is an additional step in the beam management process in our proposed method, its contribution to the overall runtime is negligible."}, {"title": "V. CONCLUSION", "content": "Our study presented a novel approach integrating MMT and RL for multi-modal beam management in wireless communication systems. By incorporating RL, we addressed the limitation of previous MMT-based approaches by reducing the decision space. Our approach combines MMT's feature extraction with RL's adaptability to dynamic environments, of-fering a promising solution for beam management. The results were obtained using the Deepsense 6G dataset, which provides real-world data for robust evaluation. Through comparisons with baseline methods, we demonstrate the superiority of the proposed method, achieving higher beam selection accuracy and throughput performance, contributing to the development of more efficient wireless communication systems."}]}