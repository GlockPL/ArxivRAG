{"title": "Fine-Tuning Vision-Language Model for Automated Engineering Drawing Information Extraction", "authors": ["Muhammad Tayyab Khan", "Lequn Chen", "Ye Han Ng", "Wenhe Feng", "Nicholas Yew Jin Tan", "Seung Ki Moon"], "abstract": "Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in manufacturing by defining acceptable variations in part features to ensure component quality and functionality. However, extracting GD&T information from 2D engineering drawings is a time-consuming and labor-intensive task, often relying on manual efforts or semi-automated tools. To address these challenges, this study proposes an automated and computationally efficient GD&T extraction method by fine-tuning Florence-2, an open-source vision-language model (VLM). The model is trained on a dataset of 400 drawings with ground truth annotations provided by domain experts. For comparison, two state-of-the-art closed-source VLMs, GPT-40 and Claude-3.5-Sonnet, are evaluated on the same dataset. All models are assessed using precision, recall, F1-score, and hallucination metrics. Due to the computational cost and impracticality of fine-tuning large closed-source VLMs for domain-specific tasks, GPT-40 and Claude-3.5-Sonnet are evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with 0.23 billion parameters, is optimized through full-parameter fine-tuning across three distinct experiments, each utilizing datasets augmented to different levels. The results show that Florence-2 achieves a 29.95% increase in precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a 43.15% reduction in hallucination rate compared to the best-performing closed-source model. These findings highlight the effectiveness of fine-tuning smaller, open-source VLMs like Florence-2, offering a practical and efficient solution for automated GD&T extraction to support downstream manufacturing tasks.", "sections": [{"title": "1 Introduction", "content": "Geometric Dimensioning and Tolerancing (GD&T) is essential in manufacturing for specifying allowable variations in part features, ensuring that components meet precise functional and quality standards [1]. GD&T uses standardized symbols and rules to clearly communicate geometric requirements on engineering drawings, with Feature Control Frames (FCFs) specifying tolerances, geometric characteristics, and relevant datums. Accurate interpretation of GD&T symbols is crucial for tasks such as inspection, quality control, and assembly, as even minor misinterpretations can lead to production defects and non-conforming parts [2].\nTraditionally, extracting GD&T information from 2D engineering drawings has been a manual process, often involving techniques like ballooning, where engineers manually mark and label features [3]. While tools such as Mitutoyo's MeasurLink [4] assist in these processes, these manual methods remain slow, error-prone, and difficult to scale in high-volume production environments. Additionally, variability in interpretation and data entry errors can result in costly rework and production delays [5].\nRecent advancements in machine learning methods, such as object detection algorithms (YOLO [6]) and Optical Character Recognition (OCR) technologies (Tesseract [7]), have been applied to automate GD&T extraction, reducing manual effort. However, these approaches still face limitations when dealing with complex GD&T features like composite tolerances, modifiers, non-standard text orientations, and requirements for extensive, accurately labeled datasets [5]. Such limitations constrain their use in advanced manufacturing applications, where high precision is critical.\nIn response to these challenges, vision-language models (VLMs) [8\u201310] offer an integrated approach by simultaneously processing visual and textual information, thereby enhancing the recognition and interpretation of GD&T symbols and annotations. VLMs can handle complex layouts and symbols that traditional methods struggle with, leveraging efficient querying techniques to deliver reliable results even with smaller labeled datasets [11-13]. Additionally, fine-tuning [14\u201316], even with limited labeled data, can significantly enhance accuracy, making VLMs well-suited for industrial applications that demand high precision in GD&T extraction."}, {"title": "2 Methodology Overview", "content": "The proposed framework automates GD&T information extraction from 2D engineering drawings by fine-tuning Florence-2, as illustrated in Figure 1. The model is trained on a dataset of 400 annotated drawings, with domain experts providing ground truth annotations for accurate performance evaluation. Text queries are generated for each drawing, and the data is organized into a CSV file linking images with relevant questions, ensuring consistency in VLM input.\nFor comparison, GPT-40 and Claude-3.5-Sonnet are evaluated in a zero-shot configuration due to the high computational cost and impracticality of fine-tuning large models for domain-specific tasks. In contrast, Florence-2 undergoes full-parameter fine-tuning [14,23,24], benefiting from its smaller size, which allows for more efficient optimization and makes it feasible for domain-specific tasks like GD&T extraction.\nThe fine-tuning process is conducted using three distinct datasets generated through data augmentation, with each dataset containing one, two, and four queries per image to capture varying levels of feature complexity. This augmentation broadens the model's ability to extract GD&T features across multiple scenarios, enhancing its robustness. After fine-tuning the model on these three datasets, the outputs of the fine-tuned models, along with the outputs from the two closed-source models (GPT-40 and Claude-3.5-Sonnet), are compared against the ground truth annotations. The performance of all models is evaluated using four key metrics: precision, recall, F1-score, and hallucination rate."}, {"title": "3 Dataset Development", "content": "The dataset development process establishes a robust foundation for evaluating VLMs in extracting GD&T information. The dataset consists of 400 annotated 2D engineering drawings, all uniformly converted to PNG format to ensure compatibility across VLM input requirements and improve processing consistency. Drawings originally in other formats, such as PDF or JPEG, are preprocessed to maintain a standardized PNG format.\nEach drawing is annotated by domain experts, focusing on three essential GD&T components: geometric characteristics, tolerances, and datum references. These annotations are stored in JSON format, capturing the necessary GD&T details. To improve cross-model compatibility and ease of interpretation, 14 commonly used GD&T symbols are represented using Unicode characters, which resolves challenges in processing complex or uncommon GD&T symbols that may be difficult for language models.\nThe dataset is structured with both image and text components for compatibility with VLMs. A CSV file links each drawing to a unique index, query questions, and ground truth annotations, facilitating seamless integration with the models' input requirements and allowing precise comparisons with model-generated predictions. Data augmentation techniques are applied to create three distinct datasets:\n\u2022\tDataset 1: 400 base images, each paired with one query question.\n\u2022\tDataset 2: 400 base images, each paired with two different query questions.\n\u2022\tDataset 3: 400 base images, each paired with four different query questions.\nThese datasets are used to fine-tune Florence-2 and evaluate its performance against the ground truth dataset. Figure 2 illustrates an example of a 2D engineering drawing alongside its corresponding expert-labeled ground truth in JSON format."}, {"title": "4 Experiments", "content": "This section outlines the experimental setup and evaluation process for three selected VLMs (GPT-40, Claude-3.5-Sonnet, and Florence-2) in extracting GD&T information from 2D engineering drawings."}, {"title": "4.1 Model Selection and Data Preparation", "content": "The experimental design includes selecting the models and preparing the data for GD&T extraction:\n\u2022\tGPT-40 and Claude-3.5-Sonnet: These state-of-the-art closed-source models are evaluated in a zero-shot configuration, meaning they rely solely on their pre-trained knowledge to process the base dataset of 400 images. No additional fine-tuning is performed due to the computational cost and impracticality of adapting these large models to domain-specific tasks.\n\u2022\tFlorence-2-base: Florence-2, an open-source model with 0.23 billion parameters, is fine-tuned using the three datasets generated through data augmentation (one, two, and four queries per image). Each dataset is split in an 8:2 ratio for training and validation to ensure that the model effectively learns to extract GD&T features across a range of complexities and is validated on unseen data."}, {"title": "4.2 Florence-2 Fine-tuning", "content": "Florence-2 (with 230 million parameters) is fine-tuned using an NVIDIA GeForce RTX 4090 GPU in a full-parameter tuning setup, where all model parameters are updated during training. The fine-tuning spans 30 epochs, using text-image training datasets derived from the three data augmentation scenarios. After each epoch, performance is evaluated on the validation datasets, and loss metrics are recorded for both training and validation phases, providing insight into model learning progress as illustrated in Figure 4.\nThe AdamW optimizer [25] is applied with a cosine learning rate decay [26], starting at 1 \u00d7 10-6, and no warm-up steps. Training is conducted with a batch size of one, utilizing mixed precision (FP16) to enhance computational efficiency. The model is optimized using its default cross-entropy loss function."}, {"title": "4.3 Model Inference Process", "content": "For GPT-40 and Claude-3.5-Sonnet, inference is performed in zero-shot mode, where the models process data without any domain-specific fine-tuning. This setup leverages the models' pre-trained knowledge to directly analyze the drawing images. Prompts structured to identify GD&T symbols, tolerances, and datums guide each model in generating outputs in a structured JSON format, which is essential for consistent and straightforward comparison with the ground truth annotations. The extracted GD&T details are then parsed for evaluation.\nFor Florence-2, inference is conducted using the three fine-tuned models, each evaluated on its corresponding validation dataset. Prompts for each image-query pair instruct the models to output GD&T information, focusing on maintaining the correct order of GD&T elements and ensuring strict adherence to JSON formatting for comparison accuracy. To ensure consistency with the ground truth data, initial outputs undergo post-processing using GPT-4o-mini to correct any formatting inconsistencies. The final processed outputs are saved as JSON files, each corresponding to the index of an image for structured evaluation."}, {"title": "4.4 Evaluation Metrics", "content": "The models are evaluated using the following metrics [27], specifically tailored for the GD&T extraction task:\n$Precision = \\frac{TP}{TP + FP}$\n$Recall = \\frac{TP}{TP + FN}$\n$F1 score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n$Hallucination = 1 - \\frac{FP}{TP + FP}$\nwhere True Positives (TP) represent the number of correctly predicted GD&T key-value pairs that match the corresponding entries in the ground truth. An exact match is required for both the GD&T key (geometric characteristic, tolerance, datum reference) and its associated value to be considered a true positive. False Positives (FP) are key-value pairs predicted by the model that do not appear in the ground truth, indicating erroneous extractions or misclassifications. False Negatives (FN) refer to key-value pairs present in the ground truth that the model fails to predict. The hallucination metric measures the rate of incorrect extractions, highlighting the model's tendency to produce erroneous outputs that are not grounded in the actual data."}, {"title": "5 Results and Discussion", "content": "The three VLMs are evaluated on the validation datasets using four key evaluation metrics. GPT-40 and Claude-3.5-Sonnet serve as baseline models in a zero-shot setting, while Florence-2 is assessed across three experiments (Exp-1, Exp-2, and Exp-3) using datasets augmented with one, two, and four queries per image, respectively.\nAs shown in Table 1, GPT-4o achieves higher precision (59.03%) compared to Claude-3.5 (44.01%), but with lower recall (25.39%) and F1-score (35.51%). In contrast, Claude-3.5 demonstrates better recall (37.27%) but has the highest hallucination rate (55.99%) among the models. This suggests that GPT-4o is more accurate but struggles to detect as many relevant GD&T instances as Claude-3.5. The trade-off between precision and recall in zero-shot models highlights their limitations for specialized tasks like GD&T extraction, where detecting a wide range of features is critical. In such cases, high precision is important to minimize incorrect predictions, while high recall is crucial to avoid missing relevant GD&T information.\nFlorence-2 exhibits consistent improvements across all metrics, with the most significant gains observed in Exp-3. In Exp-1, there is an increase of 2.91% in precision compared to GPT-40, but recall is 13.25% lower than Claude-3.5, indicating that the initial data augmentation did not significantly improve recall. In Exp-2, both precision and recall surpass the baseline models, with recall increasing by 2.71%, suggesting that additional queries help capture more GD&T features. Exp-3 delivers the best results, with precision improving by 29.95%, recall by 37.75%, and F1-score by 52.40% compared to the best baseline metrics. These results indicate that Florence-2 not only makes more accurate predictions but also captures a broader set of relevant GD&T instances, which is crucial for minimizing both false positives (FP) and false negatives (FN) in GD&T extraction.\nMoreover, the hallucination rate decreases consistently across all experiments, with the largest reduction of 43.15% in Exp-3. Reducing hallucination is particularly important in GD&T extraction, as incorrect entries can"}, {"title": "6 Conclusion", "content": "This paper introduces a novel and computationally efficient method for automating GD&T information extraction from 2D engineering drawings by fine-tuning the open-source VLM, Florence-2. Despite its relatively small size of 0.23 billion parameters, Florence-2 outperforms larger closed-source models like GPT-40 and Claude-3.5-Sonnet across all evaluation metrics. The full-parameter fine-tuning approach applied to Florence-2 leads to a 29.95% increase in precision, a 37.75% improvement in recall, a 52.40% increase in F1-score, and a 43.15%"}]}