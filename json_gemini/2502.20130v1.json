{"title": "QPM: DISCRETE OPTIMIZATION FOR GLOBALLY\nINTERPRETABLE IMAGE CLASSIFICATION", "authors": ["Thomas Norrenbrock", "Timo Kaiser", "Bodo Rosenhahn", "Sovan Biswas", "Ramesh Manuvinakurike"], "abstract": "Understanding the classifications of deep neural networks, e.g. used in safety-\ncritical situations, is becoming increasingly important. While recent models can\nlocally explain a single decision, to provide a faithful global explanation about an\naccurate model's general behavior is a more challenging open task. Towards that\ngoal, we introduce the Quadratic Programming Enhanced Model (QPM), which\nlearns globally interpretable class representations. QPM represents every class with\na binary assignment of very few, typically 5, features, that are also assigned to other\nclasses, ensuring easily comparable contrastive class representations. This compact\nbinary assignment is found using discrete optimization based on predefined simi-\nlarity measures and interpretability constraints. The resulting optimal assignment\nis used to fine-tune the diverse features, so that each of them becomes the shared\ngeneral concept between the assigned classes. Extensive evaluations show that\nQPM delivers unprecedented global interpretability across small and large-scale\ndatasets while setting the state of the art for the accuracy of interpretable models.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Learning has made remarkable advances in various fields, such as image classification, segmen-\ntation or generation (Krizhevsky et al., 2012; Kirillov et al., 2023; Rombach et al., 2021; Ramesh\net al., 2022). For high-stakes decisions, e.g. applying image classification in the medical domain,\nlegislation moves towards requiring a certain level of interpretability (Veale & Zuiderveen Borgesius,\n2021), whose measurement is a fairly open task on its own. However, some desirable and measurable"}, {"title": "2 RELATED WORK", "content": "Research towards Interpretable machine learning includes the direct design of models providing inter-\npretability by themselves (Alvarez Melis & Jaakkola, 2018; Sawada & Nakamura, 2022; Norrenbrock\net al., 2022; Nauta et al., 2023; 2021; Rymarczyk et al., 2022; Zarlenga et al., 2022; Marconato et al.,\n2022; Koh et al., 2020; Rymarczyk et al., 2021; Chen et al., 2019) or to find post-hoc methods which\naim to explain the decision process or single features of the model (Kim et al., 2018; Bau et al., 2017;\nMcGrath et al., 2022; Fel et al., 2023; Yuksekgonul et al., 2022; Kalibhat et al., 2023; Oikarinen &\nWeng, 2023). As our method is designed to find a compact set of human-understandable features,\nour work can be assigned to the former type, which we focus on within this section. However, the\nalignment of the learned features of our proposed QPM with human attributes can be guided by the\npost-hoc methods. When considering the interpretability of a model, a distinction is made between\nlocal interpretability, which refers to the explanation of a single decision, and global interpretability,"}, {"title": "3 METHOD", "content": "Our proposed QPM is designed for the interpretable classification of an image as a class\nc\u2208 {C1, C2,..., Cnc}. The QPM uses a deep feature extractor \u03a6 to compute feature maps\nM\u2208\u211dnxwm\u00d7hm of width wm and height hm and averages them into a feature vector f* \u2208 \u211dn. The classification result y \u2208 \u211dne of the QPM is the matrix multiplication between the sparse binary\nmatrix W* \u2208 {0,1}ne\u00d7n and the features f* formalized as y = W*f*.\nThe pipeline of our proposed method is shown in fig. 3 and is motivated by (Norrenbrock et al., 2022;\n2024), following their presentation and notation. It starts with training a conventional black-box\nmodel with initially nf features using the feature diversity loss Ldiv (Norrenbrock et al., 2022), as\na high diversity of features is desired for interpretable models. A detailed explanation of Ldiv is\nincluded in appendix M. Using the black-box model as starting point, we aim to find a selection\nof n out of the initial nf features and their sparse binary assignment W* to the classes to enable\ndownstream interpretability. The feature extractor \u03a6 is then fine-tuned with this solution fixed, so\nthat the features adapt to the sparse solution and become a shared concept of the assigned classes.\nThis is encouraged through selecting fewer features than there are classes, n < nc, and representing\nevery class with the same number nwc, typically 5, of features. Using the same number of features\nfor every class is beneficial for the interpretability in multiple ways. The class representations do not"}, {"title": "3.1 QUADRATIC PROBLEM", "content": "We consider the problem of selecting the n* out of nf features and assigning them to the classes as\na binary quadratic problem, that can be solved globally optimal. Specifically, the feature selection\ns\u2208 {0,1}nf and assignment between features and classes W \u2208 {0,1}nc\u00d7nf are jointly optimized,\nwith W* being W for the selected features. Given a similarity matrix A \u2208 \u211dnc\u00d7nf the main\nobjective is to maximize the similarity ZA between the selected features and their assigned classes\nZA = \u2211(acs wc)s\nc=1\nwith indicating the Hadamard product. Here, s indicates whether a feature is selected and W\ndescribes if a feature is assigned to the class. Note that we use c to index classes and d for features.\nThe sparsity and low-dimensionality are formulated as constraints for the optimization:\n\u2211Sd = n\nd=1\n\u2211Wc,dSd = nwc \u2200c\u2208 {1,...,nc}\nd=1\nTo allow the QPM the differentiation between all classes and enable effective fine-tuning, we\nadditionally add constraints that no two classes are assigned to the same set of features:\n(wcwc)sd < nwc \u2200c, c\u2032 \u2208 {1,...,nc}\nNote that the constraints in eqs. (3) and (4) technically define a quadratically constrained quadratic\nprogram (QCQP). To make the QCQP computationally tractable, the constraints are relaxed and\nadded iteratively for classes that violate the constraints. The efficient implementation is discussed in\ndetail in section 4.1.1. The general formulation of the problem allows us to add further nuance to\nthe optimization and include more desiderata. Since a high representational capacity is desired for\nthe selected features, the cross-feature similarity matrix R \u2208 \u211dnf\u00d7 n f is incorporated to reduce the\nsimilarity between the selected features:\nZR = -sTRS\nAdditionally, the selection of specific features can be guided via a selection bias b \u2208 \u211dnf\nZB = bTs,\nwhere a higher value bi leads to a preferred selection of the feature i. The combination of all these\nobjectives leads to:\nmax Z = max ZA+ZR+ ZB\nThe formulation in standard form for quadratic problems xTQx + cTx with Q capturing the\nquadratic terms ZA and ZR, and c incorporating the linear term ZB is included in appendix O."}, {"title": "3.2 CLASS-FEATURE SIMILARITY", "content": "The class-feature similarity matrix A with entries ac,d should reflect how beneficial the assignment\nof feature d to class c is for the classifier. As every feature gets assigned to multiple classes, which\nthemselves become assigned to multiple features, the metric should focus on a robust positive relation\nbetween the activation and likelihood of a sample being of the respective class. This is captured\nby the Pearson correlation coefficient ac,d between the feature distribution f:,d and the label vector\nl\u2208 {0,1}nt, in which for all nt training images a 1 indicates the label being c."}, {"title": "3.3 FEATURE-FEATURE SIMILARITY", "content": "Just maximizing eq. (1) can lead to very similar features being selected which is neither beneficial\nfor interpretability nor for accuracy as representational capacity is lost and multiple features develop\ntowards the same concept during fine-tuning. To prevent this, selecting similar features in A should\nbe penalized in the objective. We choose the cosine similarity between the class similarities of two\nReLU(a:,da:,d\u2032)\nfeatures d \u2260 d' in A for R with rd,d\u2032 =, using ReLU to focus on preventing\n\u221aa:,da:,d\u2032\nredundant features and rd,d' = 0 for d = d'. As we are only interested in preventing the selection of\nhighly similar features, we can clip all entries in R below an \u03b5 to 0 to enable a fast solving of the QP.\nThe details are discussed in section 4.1.1."}, {"title": "3.4 FEATURE-BIAS", "content": "The Feature-Bias b describes the benefit of selecting each feature. This can be used to steer the\nmodel towards specific desiderata. As diversity is generally preferred (Norrenbrock et al., 2022;\nAlvarez Melis & Jaakkola, 2018) for interpretable models, a bias towards more local features is used,\nbd =\u2211fjd=1 nr \u2211 fj,d max(Sjfj,d)\nHere Sj is the softmax over the spatial dimensions of the d-th feature map for the image j. Scaling\nthe feature bias by their activation leads to the selection of features that are more localized when their\nactivation is high. Alternatively, the bias can be used to steer the selection towards other criteria the\npractitioner might identify as relevant, which we demonstrate in the appendix. We center b and scale\nthe maximum absolute value to be A, whose strength defines the priority put on the bias."}, {"title": "4 EXPERIMENTS", "content": "Following prototype-based methods we applied our method to CUB-2011 (Wah et al., 2011) and\nStanford Cars (Krause et al., 2013). To showcase QPM's broad applicability, we also include results\non the large-scale dataset ImageNet-1K (Russakovsky et al., 2015), to which most interpretable\nmethods are not applicable. Notably, CUB-2011 contains annotations of human concepts which we\nuse to measure Structural Grounding. An overview of the used datasets is shown in Suppl. table 5.\nAs our method is independent of the used backbone, we evaluated it across various architectures, but\nfocus on Resnet50 (He et al., 2016) in this paper. Similar results on Resnet34, Inception-v3 (Szegedy\net al., 2016) and Swin Transformer (Liu et al., 2021), as well as detailed results with standard\ndeviations, are included in Suppl. appendix L. We do not apply our method to other interpretable\nmodels like PIP-Net (Nauta et al., 2023), as QPM is an alternative way of inducing compactness and\nthe features of PIP-Net are not general, thus ill-suited for a broad assignment."}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "We generally followed PIP-Net for the data preparation. Specifically, the images are first cropped\nto the ground truth bounding box for CUB-2011 and TravelingBirds (Koh et al., 2020). For all\ndatasets, the images are resized to 224 \u00d7 224. Following PIP-Net, TrivialAugment (M\u00fcller & Hutter,\n2021) is used and the strides of ResNets are also set to 1 to obtain more fine-grained feature maps.\nThe remaining parameters, including dense training for 150 epochs on fine-grained datasets and\ndirectly using the pretrained model on ImageNet-1K with subsequent 40 epochs of fine-tuning, mirror\nthe SLDD-Model and are described in appendix C. Note that QPM is trained more efficiently than\nQ-SENN, as it does not use multiple training iterations during fine-tuning. We set nwc = 5 and\nn = 50 for QPM, unless stated otherwise. We demonstrate the impact of changing the parameters\nin the ablation studies but choose these, as it is in line with prior literature (Norrenbrock et al., 2024;\n2022), n** < nc, and it enables sufficiently compact explanations (Miller, 1956). The shown results,\ne.g. tables 2 and 3, are the mean across 5 seeds, with the exception of 3 for ImageNet-1K, PIP-Net\nand ProtoPool. For comparison, all models are exclusively pretrained on ImageNet-1K. This change\ndid affect ProtoPool, but even with iNaturalist (Van Horn et al., 2018) pretraining, we could not\nreproduce the reported results by Rymarczyk et al. (2022)."}, {"title": "4.1.1 QUADRATIC PROBLEM", "content": "This section presents details on how the described quadratic problem with eq. (7) as objective is\nsolved using Gurobi (Gurobi Optimization, LLC, 2023). We incorporated deduplication and the\nassignment of an equal number of features to all classes of eqs. (3) and (4) using an iterative approach\nwith relaxed constraints. Specifically, the model is optimized without these constraints, but instead\n1\n\u0422 Ws = nwcnc. Then, after each iteration, all violated constraints are added to the model, but only\nlimited to a running set of features \u0393\u2208 {0,1}nf, which gets extended during the iteration. Next to\nthe features, we also maintain a set of classes Cduplicates that were equal at one iteration and classes\nCsparse that ever had too few features assigned. Instead of eqs. (3) and (4) the relaxed constraints\nWTS> nw\nF Sr \u2265 nwe \u2200c\u2208 Csparse\n(wc.wc')FSr < nwc \u2200c, c'\u2208 Cduplicates\nare added, where Wer describes indexing We where \u0393 = 1. Additionally, we set the start solution\nfor the next optimization to a good, usually optimal, feasible solution for the currently selected\nset of features. As we need multiple iterations to enforce all constraints, we limit the time spent\non one iteration to 3 hours and set the gap to optimality to 10\u20134. In our experiments, the global\noptimum for the relaxed problem is usually found in less than 4 hours for fine-grained datasets,\nand roughly 11 hours for ImageNet-1K using a CPU like EPYC 72F3. While eq. (9) changes the\ndesired optimization problem, the resulting objective is very close (achievable gap of less than 1%)\nto the global optimum, which is infeasible to compute and does not lead to an improved model.\nThe experiments to verify this claim are included in Suppl. appendix N. Finally, alongside our\nexperiments, previous work (Hornakova et al., 2021) shows that the exact global optimum is not\nalways preferred for relevant metrics. To make the relative weighting of the multiple objectives ZA,\nZR and ZB easier, A is scaled with nc and nwc to have a maximum of 1 for nc = 200 and nwc = 5.\nSince n features need to be chosen, all entries below \u03b5 in R are set to 0, where \u03b5 is the highest value,\nfor which there still exists a selection with ZR = 0. This is equivalent to finding the maximal \u03b5 for\nwhich the graph described by G with\ngd,d' {10 if rd,d\u2032 \u2265\n= \u03b5\n1 else,\nhas a maximum clique of size n. We used approximations (Pattabiraman et al., 2015; Boppana &\nHalld\u00f3rsson, 1992) and a sufficiently sized approximated maximum clique as the start value for s.\nAdditionally, the remaining nonzero values in R are scaled to have a maximum of 1. For scaling the\nbias b, we clipped outliers, centered the remaining values around 0 and scaled the maximum absolute\nvalue to be \u03bb =\u221a, which is empirically found."}, {"title": "4.2 METRICS", "content": null}, {"title": "4.3 RESULTS", "content": "This section discusses the experimental results. The usual metrics for compactness-based globally\ninterpretable models are shown in table 2. For the fine-grained datasets, QPM is among the most\ncompact models while showing the highest accuracy, thus setting the state of the art for interpretable\nmodels. On ImageNet-1K, where prototype-based methods are not even applicable, QPM is only\nmarginally beaten by Q-SENN, which uses compute-intensive iterations and negative reasoning for\nsome classes, which significantly hinders interpretability. A runtime analysis is shown in appendix F.\nThe results for the interpretability metrics are shown in table 3. Note that glm-saga5 and PIP-Net\nare hardly comparable, as glm-saga5 uses the uninterpretable features of a black-box model and\nPIP-Net learns very localized class-detectors, with some features activating to 99% on just a\nsingle class. In contrast, QPM achieves excellent values across all metrics and datasets in this\nmulticriterial task of self-explaining neural networks, summarized in fig. 6. Its interpretable class\nrepresentations, composed of diverse, general and contrastive features, mirror reality, as measured\nby Structural Grounding. Note that QPM learns grounded representations as shown in figs. 1\nand 4 without any additional supervision and is able to communicate the only differentiating\nfactor it uses. QPM's local behavior then follows its faithful global explanations, which leads to\ntrustworthy classifications and predictable errors when the differentiating factor is not present,\nas in fig. 8. The appendix contains more visualizations, including a discussion of failure cases\nin appendix E, a discussion on polysemantic features (appendix H), an extension of Structural\nGrounding to ImageNet-1K (appendix I) and a discussion of limitations and future work (appendix K)."}, {"title": "4.4 ABLATION STUDIES", "content": "This section validates the impact of the individual objectives in the quadratic problem in table 4 and\npresents the compactness trade-off in fig. 7. We focus on CUB-2011 but observed similar results\nfor other datasets. The compactness-accuracy tradeoff for QPM compared with Q-SENN and the\nSLDD-Model is visualized in fig. 7. The global optimization clearly leads to a more effective use of\nthe defined capacity, with the highest uplift in the very high compactness regime, e.g. 1.5 percent\npoints at n = 20, where a good selection and assignment naturally has more impact.\nThe impact of the feature-feature similarity matrix R and feature selection bias b is shown in table 4.\nIncorporating a bias b for local feature maps further increases the SID@5. On the other hand, reducing\nfeature similarity through R effectively reduces the correlation between the resulting features, which\nimproves accuracy, as the model uses its capacity more effectively. In summary, the inclusion of the\nsecondary objectives ZR and ZB is beneficial for the resulting model, improving the desired aspects\nnot just after solving the QP but also in the resulting model after fine-tuning.\nThe appendix contains further ablation studies to support our claims, demonstrating the ability to\nsteer (appendix D), validating the choice of correlation as metric for A (appendix J) and showing the\nbenefits of enforcing exactly nwc features per class (appendix G)."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced the Quadratic Programming Enhanced Model (QPM). It uses discrete\noptimization to find an optimal feature selection and assignment of just 5 to each class. With this\neasy-to-understand assignment, the resulting QPM is more interpretable than previous methods, as it\nhas contrastive faithfully interpretable class-representations, shows Structural Grounding, is steerable,\nand its features have excellent SID@5, Class-Independence and Contrastiveness. Additionally, it\nfurther closes the accuracy gap to the drastically less robust uninterpretable baseline. Figure 6\nshows that only QPM excels in all metrics, thus setting a new state of the art for compactness-based\ninterpretable models, while delivering unprecedented global interpretability even to ImageNet-1K."}, {"title": "D STEERABILITY", "content": "This section is concerned with the ability of the practitioner to steer the model towards desired biases\nusing the feature bias b. For example, if a human recognizes the erroneous focus on the background\nCenter=\n\u03a3 \u03b7\u03c4 \u03a3 ; fj,d\nbd\n1\n\u03a3f1+de(M)\nj=1\nwhere de computes the distance between the maximum of the j-th sample's map M at (x, y) and\nthe closest edge:\nde(M) = min(|x \u2212 w\u043c|, x \u2212 1, |y \u2212 h\u043c|, y \u2212 1)\nThe resulting improved accuracy on TravelingBirds with \u039b = 10\u00bd, shown in table 11, demonstrates\nthis steerability. Setting \u03bb allows a precise weighting of the emphasis put on the bias. This direct\ncontrol for both the center and diversity bias is visualized in fig. 9 and allows the incorporation of any\nfeature-level bias b."}, {"title": "E FAILURE CASES", "content": "This section presents examples where QPM predicts wrongly. For that, fig. 10 shows exemplary\nimages of Rottweiler and Doberman with classification results of the probed QPM trained on\nImageNet-1K and with global explanations in figs. 1 and 21 to 23. Note that the accuracy across\nthe two classes is 87%, well above the average, reflected in correct classifications across poses,\nbackgrounds and settings in figs. 10a and 10b. Additionally, fig. 11 shows the GradCAM (Selvaraju\net al., 2020) visualizations and demonstrates that QPM always focuses on the dog in the image.\nFor the erroneous predictions, the model behaves just like the global explanations would indicate.\nRottweiler and Doberman may be swapped, if the head is occluded as in figs. 10c and 10g or in\na difficult pose to gauge the shape, shown in figs. 10d and 10h. Since the Black and tan coon\nhound is assigned both head features of Rottweiler and Doberman, they can also be confused when\nprimarily the head is visible, demonstrated in figs. 10e and 10i. Finally, figs. 10f and 10j seem to\ncontain one of the many (Northcutt et al., 2021) wrongly labeled samples in ImageNet-1K. QPM\nalso robustly classifies wrongly labeled data, as the global explanation would suggest. Figures 12\nand 13 show the feature activations of Greater Swiss Mountain Dog and Rottweiler on fig. 10f and\nother class examples, further suggesting that it is indeed a typical Greater Swiss Mountain rather\na Rottweiler for the probed QPM, as the features of the former localize on the expected regions,\nwhereas most Rottweiler features barely activate. Finally, fig. 14 shows further test examples for\nthe model explained in fig. 4 and demonstrates that the model does not predict Bronzed Cowbird if\nthe differentiating red eye is not present in the image. In summary, QPM's local behavior robustly\nfollows the faithful global explanations, which can lead to predictable faulty classifications in case of\nocclusion or difficult pose."}, {"title": "FRUNTIME ANALYSIS", "content": "This section discusses the time it takes to obtain a QPM, compares it to competing models and\ndiscusses the impact of n on it. Figure 15 demonstrates that the optimization time strongly increases\nwhen increasing n. However, for the probed datasets, going beyond 50 features seems not to be\nnecessary, as the accuracy only improves negligibly, while the interpretability is harmed: Features\nbecome less general and there will be fewer class representations with high overlap, which allow for\nthe most intuitive interpretation. One can further optimize this using suitable priors, which we do not\ninclude in this work, as the interpretability and additional accuracy decreases with increasing n. It is\nhowever an avenue for future work, when datasets with sufficient complexity are published. Table 6\ncompares the time to obtain the interpretable model between QPM, Q-SENN and SLDD-Model.\nQ-SENN and SLDD-Model start with a feature selection, that takes 15 minutes on CUB-2011 and\nroughly 500 minutes on ImageNet-1K. They both use glm-saga for feature selection and computing\nthe sparse matrix and are thus scaling with number of samples nt, which QPM is invariant to, as that\ndimension is summarized in the constants."}, {"title": "GIMPACT OF EVEN SPARSITY", "content": "This section discusses the impact of enforcing exactly nwc features per class, rather than on average.\nFor that, we trained a model without this constraint, but instead with 1W = nwcnc enforcing an\naverage sparsity. To counteract the uneven number of features per class, every class got a bias, that\nis linear to the number of features it is below the average. In prior experiments, various forms of\ncounteracting the uneven assignment with a bias have performed similarly. Table 7 shows that the\neven assignment is beneficial for the accuracy. Further, the even assignment boosts interpretability as\nit leads to more classes that can be contrasted easily and does not introduce an unintuitive bias term.\nAdditionally, fig. 16 demonstrates that classes, which are assigned to fewer features, cause these"}, {"title": "H POLYSEMANTIC FEATURES", "content": "This section discusses the phenomenon of polysemantic features and how it relates to QPM. Like\nall deep learning models (Scherlis et al., 2022) not specifically designed to prevent polysemanticity,\nQPM learns polysemantic features. It refers to individual neurons activating on not just one concept c\nbut rather on n seemingly unrelated ones. While it is an active area of research, their emergence can\nlikely be attributed to being an effective solution to the training objective. On many training samples,\nthe impact on the loss can be fairly low, if a polysemantic feature activates on any of its n meanings.\nThe only exception occurs, when it activates on samples, where its activation contributes significantly\nto a class that is already showing a lot of activation. While this is typically very difficult to analyze,\nthe interpretable structure of QPM can offer more insights, as it enables a reliable metric on which to\ngauge how strongly the activation on another concept would affect the loss: The similarity in QPM's\nclass representation space. Our hypothesis is that QPM learns features that are locally monosemantic,\nwhile being globally polysemantic. Around a class, e.g., Bronzed Cowbird, we expect the features"}, {"title": "JIMPACT OF CLASS-FEATURE SIMILARITY METRIC", "content": "This section contains an ablation study on the choice of Pearson correlation as metric for the feature-\nclass similarity matrix A. While it captures the desired linear relationship, that is also utilized during\nthe following predictions, an intuitive alternative is the Area under the receiver operating characteristic\ncurve (AUROC), which is highly non-linear and frequently used to capture the predictive power with\na varying threshold. Table 10 shows that AUROC is also suitable but inferior to the simple correlation."}, {"title": "K LIMITATIONS AND FUTURE WORK", "content": "This section discusses limitations for the proposed QPM and avenues for future work.\nIn this work, QPM is applied to the generally available and typical datasets for image classification,\nwith ImageNet-1K indicating broad applicability. However, QPM's high interpretability is especially\nbeneficial for high-stakes applications such as the medical domain or autonomous driving, where\neach individual situation can not be accessed by an expert. Rather, after training the QPM and before\ndeploying it to cars, its class explanations can be obtained to gain insights into whether it is right\nfor the right reasons and if these are robust to all deployment conditions. Thus, applying QPM to\nsuitable high-stakes applications is a promising avenue for future work. However, to our knowledge,\nthere is no suitable dataset from these domains published yet.\nA limitation of our QPM in its current form lies in its inability to model negative assignments.\nCompared to the SLDD-Model and Q-SENN, which use negative weights, it is evident that the varied\ndatasets used in this paper, do not require it. Further, while we believe that it is generally prefer-\nable to represent classes only using positive assignments, as e.g., also done by recent prototypical\nmodels (Nauta et al., 2023; Rymarczyk et al., 2022), one can think of other datasets where negative\nreasoning may be superior. If, e.g., all classes in a dataset containing birds had a black beak, except\nfor one with all other colors, it would likely be the most efficient solution to represent that one with\na negative assignment on a feature activating on black beaks, rather than have every other class\npositively assigned to it, which the current QPM might do. Thus, future work may incorporate\nnegative assignments into the optimization, which might lead to even more compact representations.\nAs discussed in appendix H, the learned features of our QPM are generally polysemantic, while\npotentially being monosemantic locally. For aligning them with human concepts, all post-hoc"}, {"title": "L DETAILED RESULTS", "content": "This section contains detailed results with standard deviations, including experiments with Resnet34,\nInception-v3, Swin-Transformer-small and Swin-Transformer-tiny, in Suppl. table 11 to table 22. The\ngood results across architectures demonstrate an independence between backbone and our proposed\nmethod. They further seem robust as the difference in mean is usually large compared to the standard\ndeviation. Further, figs. 21 and 22 show how the features of fig. 1 continue to localize on the same\nhuman attribute across different poses. Additionally, we included the activations of these features\non images of another class in fig. 23 to showcase the global interpretability enabled through the\nbinary assignment of more interpretable features. Instead of the blue and green feature, this probed\nQPM recognizes the Black and Tan Coonhound through both doberman-like and rottweiler-like head\nfeatures, as well as a neck that is also assigned to pandas or bears. Figures 19 and 20 additionally\ninclude examples for contrastive class representations learned on Stanford Cars and TravelingBirds.\nFinally, table 8 contains results for diversity@5, to quantify its inability to capture the high spatial\ndiversity of PIP-Nets class detectors."}, {"title": "M FEATURE DIVERSITY LOSS", "content": "This section further describes the Feature Diversity Loss Ldiv, proposed in Norrenbrock et al. (2022).\nIt is defined per sample, for which the model predicted the class \u0109 = arg max(y) and ensures a local\ndiversity of the used feature maps M\u2208\u211dnf\u00d7wm\u00d7h\u043c.\nSij =\nexp(mij)\n\u22111 exp(m\nMij=h\u043c\u2211fd Wed maxf||we||2\\nLdiv = -\u2211 max(Sj, . . .)njd =1 fd=1,j=1\nEquation 31 employs the softmax function to normalize the entries mij of the feature maps M across\nspatial dimensions. It then scales the maps to emphasize visible and significant features, maintaining"}, {"title": "\u039d \u039f\u03a1\u03a4\u0399\u039cALITY OF SOLUTION", "content": "In order to test the optimality of our solution, we try to solve the problem without our relaxation\nin eq. (9) with more compute and time. We used 3 days and 250 GB on an AMD EPYC 72F3 to\nsolve the problem globally across 5 seeds on CUB-2011 with a target gap to optimality of 1% to\nensure sufficient deduplication. The time limit was left to 3 hours for one iteration, as otherwise\nmultiple iterations would not finish. Across the 5 seeds used for QPM, the average obtained objective\nvalue for the global problem was 0.5% above the one computed with our simplifications. Similar\nto our ablations in table 4, the resulting accuracy for the extensively optimized model was not\nimproved, but even 0.1 percent points lower. As mentioned in section 4.1.1, the objective does not\nperfectly correlate with downstream metrics, as the constants A, R and b only approximate the\ndesired behaviour. However, the average gap to the best bound was still 3.2%, with only negligible\nprogress during the final iteration, suggesting that a longer time limit would not significantly improve\nit. Note, that the best bound might be violating constraints, already added or not. In summary, the"}, {"title": "\u039f STANDARD FORM FOR QUADRATIC PROBLEM", "content": "The quadratic problem", "W": "nx =\n [vec(W)", "objectives": "nMaximize: xT Qx + cT x\nHere\nQ=\nCombining all quadratic objectives and -\n\n1nf", "0nf": "nf Nc\n1\n2 Onf .nc", "2))": "nT\n(37)\n(1nf Onc) X = n\n* .In\n2. No assignments on unselected features:\n[featureSum Onf.nc", "FeatureSelf": 0}]}