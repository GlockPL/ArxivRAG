{"title": "Matmul or No Matmal in the Era of 1-bit LLMS", "authors": ["Jinendra Malekar", "Mohammed E. Elbtity", "Ramtin Zand"], "abstract": "The advent of 1-bit large language models (LLMs) has attracted considerable attention and opened up new research opportunities. However, 1-bit LLMs only improve a fraction of models by applying extreme quantization to the projection layers while leaving attention heads unchanged. Therefore, to avoid fundamentally wrong choices of goals in future research, it is crucial to understand the actual improvements in computation and memory usage that 1-bit LLMs can deliver. In this work, we present an adaptation of Amdahl's Law tailored for the 1-bit LLM context, which illustrates how partial improvements in 1-bit LLMs impact overall model performance. Through extensive experiments, we uncover key nuances across different model architectures and hardware configurations, offering a roadmap for future research in the era of 1-bit LLMs.", "sections": [{"title": "Introduction", "content": "Generative Large language models (LLMs) such as GPT (Radford et al. 2019), OPT (Zhang et al. 2022) and LLAMA (Touvron et al. 2023) have attracted significant attention in recent years because of their impressive performance across various tasks, including but not limited to machine translation (Xu et al. 2024a), conversational chatbots (S\u00e1nchez Cuadrado et al. 2024), question answering (Tan et al. 2023), and even code generation (Ugare et al. 2024). However, the remarkable performance of LLMs has come with increasing computational and energy costs. This necessitates expanding high-performance computing resources in data centers, potentially delaying green energy commitments and causing adverse environmental impacts (Stojkovic et al. 2024). Consequently, recent research has focused on optimizing the energy footprint of LLMs through various model optimization and compression techniques like pruning (Ma, Fang, and Wang 2023) and quantization (Xiao et al. 2023; Shao et al. 2023; Ma et al. 2024). In addition to enhancing the energy efficiency of running LLMs in data centers, model optimization and compression can facilitate the deployment of LLMs on mobile and edge computing devices for real-time processing (Reidy et al. 2023). This advancement could benefit various emerging applications, including social robotics (Addlessee et al. 2024; Esteban-Lozano, Castro-Gonz\u00e1lez, and Mart\u00ednez 2024), and augmented and virtual reality (Asadi et al. 2024; Morales and Showalter-Bucher 2023). Among LLM compression approaches, quantization has become a focal point, driven by works such as Smoothquant (Xiao et al. 2023), Omniquant (Shao et al. 2023), and more recently ShiftAddLLM (You et al. 2024) which emphasize post-training quantization (PTQ) as a cost-effective approach avoiding the processes of retraining and fine-tuning which can be particularly costly for LLMs. A newly emerging approach to optimizing and compressing models through quantization-aware training (QAT) involves the extreme quantization of certain portions of LLMs, using binary {-1,1} and ternary {-1,0,1} weights (Wang et al. 2023; Ma et al. 2024). This development has initiated the era of 1-bit LLMs. Besides the memory utilization advantages, extreme quantization transforms the costly matrix multiplication (MatMul) operations into more efficient addition and subtraction operations, and thus leading to MatMul-free operations. However, it is important to note that not all MatMul operations can undergo extreme quantization due to the resulting drop in accuracy. Specifically, MatMul operations in the attention heads still require higher precisions, such as 16-bit floating point (FP16) or 8-bit integer (INT8). A follow-up work (Zhu et al. 2024a) has built upon BitNet (Wang et al. 2023), aiming to eliminate MatMul operations in attention heads by using Hadamard products. The advent of 1-bit LLMs has paved the way for various new research directions. However, since they currently address only a fraction of the model, a crucial question arises:\nWhat effects do partial enhancements in 1-bit LLMs have on the overall performance of the model?\nAnswering this question is vital for guiding future research effectively and avoiding misguided or ineffective goals. For example, if the MatMul operations that are replaced with MatMul-free operations in current 1-bit LLMs account for the majority of the model's computation and memory usage, then focusing on optimizing the relatively minor MatMul operations in attention heads may be less impactful. In this case, prioritizing custom hardware development to fully leverage extreme quantization would be more sensible. Conversely, if the conversion of the fraction of MatMul operations to MatMul-free operations in current 1-bit LLMs does not significantly affect overall computation and memory usage, it would be more prudent to focus on optimizing the MatMul operations in the attention heads-currently not optimized in 1-bit LLMs-rather than investing in hardware development for current 1-bit LLMs. In this study, we address the highlighted research question through extensive experiments and analyses on various LLMs. We explore various model hyperparameters across two types of hardware designed for edge and cloud environments. In addition, we propose an adaptation of Amdahl's Law for LLMs to identify how partial improvements in LLM can translate into overall enhancements for the entire model. Our findings reveal important nuances that can guide future research in the 1-bit LLMs era."}, {"title": "Related Work", "content": "Transformer Quantization\nTransformer quantization can be categorized into weight-only and weight-and-activation quantization. Weight-only quantization reduces memory requirements, while weight-and-activation quantization also enhances computational efficiency. SmoothQuant (Xiao et al. 2023) supports both activation and weight quantization, based on the principle that the difficulty of activation quantization can be mathematically transformed. This method demonstrates a 1.5\u00d7 speedup and 2\u00d7 memory reduction for LLMs with negligible loss, supporting W8A8 (8-bit weight, 8-bit activation) quantization. Omniquant (Shao et al. 2023) is another method supporting a broader spectrum of weight-activation quantization configurations, including W4A4, W4A16, W3A16, and W2A16. This is achieved through learnable weight clipping, which optimizes the clipping threshold, and learnable equivalent transformation, which mitigates activation outliers by shifting them to weights. LLM.int8() (Dettmers et al. 2022) loads the model in 8-bit format, with 99.9% of operations performed in 8-bit, except for emergent outliers. It uses vector-wise quantization with different normalization constants to preserve model performance. GPTQ (Frantar et al. 2023) focuses solely on weight quantization, achieving 3-bit and 4-bit quantization with speedups of 3.24\u00d7 and 4.53\u00d7 on A6000 and A100 GPUs. QuIP# (Tseng et al. 2024) is a weight-only quantization method that achieves state-of-the-art performance in sub-4-bit quantization. It employs a randomized Hadamard transform combined with vector quantization and then fine-tunes the model to enhance its fidelity. All the methods discussed above are related to PTQ.\nEra of 1-bit LLMs\nBefore the LLM era, the concept of binary weight and activation quantization was explored in works such as Binarized Neural Networks (BNN) (Hubara et al. 2016) and Ternary Neural Networks (TNN) (Alemdar et al. 2017). While these studies did not focus on generative language tasks, they achieved significant performance improvements, with BNNs demonstrating up to 7\u00d7 performance gains and TNNs showing up to 3.1x energy efficiency improvements. These findings underscore the remarkable ability of neural networks to operate effectively with just 1-bit precision. Recently, following the rise of LLMs and advancements in model performance, the BitNet (Wang et al. 2023) introduced a 1-bit transformer quantization technique for LLMs. This method replaces the conventional 'nn.Linear' layer in PyTorch with a new BitLiner layer, where weights are restricted to either 1 or -1, and activations are represented in 8-bit precision. Despite this quantization, other components like self-attention remain in 8-bit format. BitNet's design suggests that it can scale to even larger transformer models, following scaling laws similar to those used for full-precision transformers. A variant of BitNet, known as BitNet 1.58 (Ma et al. 2024), employs ternary weights (-1, 0, 1) and achieves perplexity and end-task performance comparable to full-precision transformers (FP16 or BF16). In terms of the computation, 1-bit LLMs transform MatMul operations into addition operations, due to the 1-bit nature of the weights, except for layers like attention, which need to remain in high precision to maintain performance. Additionally, 1-bit LLMs address the challenge of transferring model parameters from DRAM to on-chip accelerators such as SRAM, prompting the development of architectures optimized for the efficient operation of 1-bit LLMs."}, {"title": "Approach", "content": "Demystifying the Underlying Operations of LLMs\nThe overall architecture of the generative LLMs is shown in Figure 1, which includes N decoder blocks, each consisting of self-attention and feedforward layers followed by add and normalization operations (Vaswani et al. 2017). The core of the LLM is the self-attention mechanism with multiple heads (h). For an attention head, the computation begins with three linear projections of the token vector to form the Key (K), Query (Q), and Value (V) vector sequences:\n$\\Q = W_Q * I, K = W_K * I, V = W_V * I$ (1)\nwhere I in the input token vector and $W_Q$, $W_K$, and $W_V$ are trainable weight matrices with d \u00d7 d dimensions where d is the embedding dimension. The operator * denotes the MatMul operation.\nThe generated K, Q, and V vectors are then divided into h vectors with reduced dimensionality of d/h where h is the number of attention heads. At this stage, the value and key vectors are concatenated with the previous l - 1 value and key tokens that are cached from previous token generation iterations to form a d/h \u00d7 l matrix in each head, where l is the sequence length. Next, the attention scores are computed using the scaled dot-product of the queries and keys multiplied with the generated value matrix (Vaswani et al. 2017). This step includes two MatMul (*) operations:\n$Attention(Q, K,V) = softmax(\\frac{Q* K^T}{\\sqrt{d}}) * V$ (2)\nSubsequently, the output of the attention heads are concatenated and linearly transformed as follows:\n$MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_x$ (3)\nwhere $head_i$ = Attention $(Q_i, K_i, V_i)$, and $W_x$ is a d \u00d7 d matrix with trainable elements.\nPrevious works (Kim et al. 2023) have shown that designing dedicated hardware to compute nonlinear operations in LLMs can make their computation overhead negligible compared to MatMul operations. Consequently, optimizing MatMul operations can lead to a significant speedup in LLM computation. The 1-bit LLMs (Wang et al. 2023; Ma et al. 2024; Xu et al. 2024b) as a recent approach that has attracted considerable attention involves extreme quantization of MatMuls in LLMs, except for the attention heads, which require higher precision to maintain accuracy. Quantizing all the weight matrices ($W_Q$, $W_K$, $W_V$, $W_x$, $W_1$, and $W_o$) in the LLMs to inlcude only binary {-1,1} or ternary {\u22121,0,1} elements transforms the weight-to-activation MatMul operations to simple addition and subtraction operations, dividing the LLM models into two portions one with MatMul and one without MatMul (or MatMul-free), as shown in Figure 2.\nDue to variations in the model hyperparameters (d, l, h, and $d_{FF}$) across different types of LLMs, the proportion of the linear projections (weight-to-activation MatMuls) and attention head (activation-to-activation MatMuls) computation relative to the entire model can vary significantly. Therefore, performance analysis with layer-wise granularity, targeted in this work, can illuminate the effectiveness of the 1-bit LLMs and help determine future research directions."}, {"title": "Design of LLM-Specific Hardware", "content": "For the performance analysis, we leverage tensor processing unit (TPU) architectures which are specifically designed to accelerate the MatMul operations dominant in machine learning workloads. TPUs maximize data reuse while minimizing data transfer by utilizing systolic arrays at their core (Jouppi 2017; Jouppi et al. 2017). A systolic array typically consists of two-dimensional arrays of processing elements (PEs). Each PE performs a multiply-and-accumulate (MAC) operation, multiplying weights and inputs using a multiplier circuit and adding the result to previously computed partial sums using an accumulator circuit. The MAC result is either retained within the same PE or broadcast to other PEs for further computations, depending on the dataflow architecture. This MAC operation is executed in every PE of the systolic array, enabling efficient MatMul operations by maximizing data reuse and minimizing additional data transfer overhead. The dataflow in a systolic array is a mapping scheme determined by the microarchitecture of the PEs, which dictates how input data is fed into the array and how partial results and outputs are generated and stored. Rather than repeatedly loading and storing data to and from memory, each PE typically follows one of these dataflow architectures: (1) Input Stationary (IS): The inputs (or activations) remain fixed in the PEs while the weights are sequentially fed into the PEs; (2) Output Stationary (OS): The outputs are attached with the MAC units, while the inputs and weights circulate among the PEs. Inputs and weights are loaded, multiplied, and the results are accumulated with partial sums held in the PE. (3) Weight Stationary (WS): Each weight is preloaded into a register within each PE. During each cycle, inputs are multiplied by the weights and broadcast across the PEs. Figure 3 shows the architecture of TPU, consisting of weight, input, and output memories, and a systolic array of size S = N \u00d7 N PEs surrounded by the first-in-first-out (FIFO) buffers. Additionally, our TPU includes a Nonlinear Functional Unit, featuring custom hardware to support nonlinear operations in the LLMs. The Dataflow Generator block generates the memory read/write addresses to store or retrieve the inputs, weights, and outputs according to the selected dataflow. The Main Controller manages the data transfer between memories, FIFOs, and the systolic array. As previously discussed, the MatMul operations in generative LLMs involve matrix-vector multiplications. Consequently, the sizes of the input and output vectors are always smaller than those of the weight matrices. For activation-to-activation MatMuls in the attention head, where there are no weight values (See Figure 1), we store the concatenated Value and Key matrices (with d/h \u00d7 l and l \u00d7 d/h dimensions, respectively) in the weights memory, while the Query and attention score vectors are stored in the input memory. Based on the pattern in the size of the input, weight, and output tensors in matrix-vector multiplications involved in LLMs (mentioned in Table 1), a TPU design with larger weight memory compared to input and output memories would be more efficient, as it reduces the need for costly accesses to the main DRAM memory to load the weights. For the dataflow architecture, we conducted comprehensive experiments utilizing IS, OS, and WS dataflows. Based on the results obtained (refer to Appendix A), the OS dataflow architecture demonstrated the best performance. The OS dataflow is particularly advantageous for accumulating results since partial sums remain stationary and do not need to be moved frequently. Additionally, once weight and input values are fetched from their respective memories, they are reused by passing from one PE to another, leveraging the spatial dataflow capabilities of TPUs."}, {"title": "Amdahl's Law of LLMS", "content": "Since current 1-bit LLMs only improve a part of the model (the projections without enhancing the attention heads), we need a mechanism to determine how these partial improvements translate into overall enhancements for the entire model. This is crucial for addressing the main research question of the paper. The Amdahl's Law provides a framework for such scenarios. Amdahl's Law is a formula used to find the maximum improvement in a system when only part of it is improved. It can be expressed as:\n$S_{total} = \\frac{1}{1-F + \\frac{F}{S_{partial}}}$ (5)\nwhere F is the fraction of the system that is improved, $S_{partial}$ is the factor by which the part F is improved, and $S_{total}$ is the overall improvement of the entire system. In the context of 1-bit LLMs, we define F as the fraction of the MatMul operations that can be replaced with MatMul-free operations by using extreme quantization, relative to all MatMul operations in the model. Here, our focus is on quantifying the value of F across various LLMs and hardware configurations. Enhancing $S_{partial}$ is closely tied to the design of custom hardware accelerators for binary and ternary operations, which is beyond the scope of this paper."}, {"title": "Experiments", "content": "Simulation Setup\nFor our experiments, we study 13 different LLMs including GPT, OPT, and LLaMA models. Table 2 lists all models and their corresponding hyperparameters. To save space in the main body of the paper, we only provide the results for the seven OPT models, as they represent a diverse range of model hyperparameter combinations. The results for GPT and LLaMA models are provided in the Appendix B. For the hardware, we designed two TPUs tailored for different applications: cloud and edge processing. The cloud TPU features a 256 \u00d7 256 systolic array with 16MB of SRAM, while the edge TPU has a 32 \u00d7 32 systolic array with 8MB of SRAM. Both systolic arrays employ an OS dataflow. Also, in both designs, 2MB of memory is allocated for internal use, including storing control and configuration data, tracking computation states, managing data flow, and ensuring seamless data movement. All memories use double buffering to mask the latency associated with SRAM access. Table 3 provides the memory distribution of both edge and cloud TPU designs. We utilize the cycle-accurate SCALE-Sim framework (Samajdar et al. 2018, 2020) to measure compute cycles and memory accesses in various LLMs. Our experiments investigate the distribution of computation and memory utilization between the attention heads, which require MatMul operations, and the projections, which can be binarized or ternarized and consequently do not involve MatMuls. In the results presented, the terms \"MatMul\" and \"MatMul-Free\" refer to these respective parts of the LLM computation.\nPerformance Analysis on Cloud Setup\nIn the first set of experiments, we examine various language models deployed on the cloud TPU setup to determine the fraction of the models that can become MatMul-Free in 1-bit LLMs, i.e., projection layers, through extreme quantization. To achieve this, we compare seven OPT models of various sizes (ranging from 350M to 66B parameters) with different sequence lengths (ranging from 128 to 4096). We vary the sequence length (l) because it only affects the computation in the attention heads and determines the size of the remaining MatMul operations in 1-bit LLMs (refer to Table 1). Figure 9 exhibits the fraction of MatMul-Free operations in the OPT models deployed on the cloud setup, measured in terms of compute cycles and memory accesses. The fraction of MatMul-Free operations varies from roughly 23% to 98% for compute cycles and from 59% to 99.8% for memory access across different configurations. In general, MatMul-Free operations increase with model size and decrease with sequence length.\nCompute Cycle Analysis. Typically, the context length of the OPT models is equal to 2048. The MatMul-Free compute cycles for these OPT models can be observed in the 2048 row of Figure 9 (a). For smaller language models like OPT 350M, approximately 63% of the computation occurs in the attention heads, involving MatMul operations, while only 37.1% of the computation benefits from MatMul-Free operations provided by 1-bit LLMs. Moreover, the OPT 1.3B model with a sequence length of 2048 is particularly noteworthy, as half of its computation can be MatMul-Free, while the other half involves MatMul operations. Models like LLaMA 7B and 13B generally use larger sequence lengths of 4096. In terms of model size, they are comparable to OPT 6.7B and 13B models. As illustrated in the 4096 row of Figure 9 (a), MatMul-Free operations account for 64.5% and 69% of the computation for OPT 6.7B and OPT 13B models, respectively. A detailed analysis of the LLaMA models can be found in the Appendix B.\nMemory Access Analysis. 1-bit LLMs, such as BitNet (Wang et al. 2023) and BitNet 1.58 (Ma et al. 2024), can achieve up to 16\u00d7 and 8\u00d7 reductions in weight memory utilization compared to FP16 LLMs because they use just 1 bit and 2 bits to represent binary and ternary weights, respectively. Besides memory capacity, another crucial factor affecting LLM throughput is the number of memory accesses required for inference. Therefore, we also analyzed the memory access of the MatMul-Free and MatMul components of the 1-bit LLM architecture. Figure 9 (b) shows the ratio of memory reads and writes associated with the projection layers (MatMul-Free parts) to those of the entire model.\nAmdahl's Law of LLMs. Here, we leverage Amdahl's Law of LLMs proposed in Equation (5) to show how partial improvements in the LLMs can enhance overall performance. In particular, we vary $S_{partial}$ from 1 to 100, and using the fractions (F) shown in Figure 9, calculate the overall improvement of model ($S_{total}$). Figure 5 demonstrates an Amdahl's Law analysis when improvements are applied to either attention layers (MatMul parts) or the projections layers (MatMul-Free parts) of LLMs. The dashed lines show the effects of improving the projection layers, while the solid lines represent the impact of improvements to the attention layers. This analysis is based on OPT models with a typical sequence length of 2048. For Amdahl's Law analyses with other sequence lengths, please refer to the Appendix C. The Amdahl's Law analysis of LLMs deployed on the cloud setup reveals three key takeaways:\n1.  For smaller language models like OPT 350M, improving the attention heads has a greater impact than enhancing the attention layers. Therefore, using 1-bit LLM paradigms to improve these models may result in limited overall performance gains.\n2.  Medium-sized LLMs, such as OPT 1.3B and 2.7B, can benefit from combining 1-bit LLM improvements with enhancements to the attention heads, such as replacing MatMul operations with Hadamard products and additive operators as proposed in (Zhu et al. 2024b).\n3.  For larger models like OPT 6.7B and above, improvements to the attention heads have a minimal effect on overall performance. In these cases, employing 1-bit LLM methods alone can lead to significant gains in performance and throughput."}, {"title": "Performance Analysis on Edge Setup", "content": "Figure 10 presents the compute and memory analysis for the edge setup. In all cases, most of the computations occur in the MatMul-Free portion. The only instance where the computation is relatively evenly distributed between the MatMul and MatMul-Free parts is for OPT 350M with a 4096 sequence length. A similar pattern is observed in memory accesses. This happens because, in the edge setup, the smaller 32 \u00d7 32 systolic array is less efficient at handling the larger matrices typically found in projection layers. Meanwhile, the MatMul operations in the attention heads are smaller due to the splitting of large matrices among multiple heads (refer to Figure 1), making these operations more manageable even with smaller systolic arrays. Consequently, this increases the ratio of computation in the projection layers compared to the attention heads.\nAmdahl's Law of LLMs. Figure 7 illustrates the Amdahl's Law analysis for the OPT models with 2048 sequence length deployed on the edge TPU setup. The results indicate that, unlike in the cloud setup, across all cases, enhancing the projection layers leads to significantly greater overall improvements in the entire model. Conversely, improving the attention layers yields only marginal gains. This suggests that 1-bit LLM approaches targeting optimization of projection layers are significantly more efficient in the edge setup. Therefore, a promising research direction would be to focus on developing efficient custom hardware for implementing extremely quantized projection layers, rather than concentrating on algorithmic and hardware innovations to enhance computation in the attention heads."}, {"title": "Conclusion", "content": "The proposal of 1-bit LLMs has opened several research avenues, including the development of custom hardware for 1-bit LLMs as well as algorithmic innovations to enhance aspects of LLM computation that 1-bit LLMs cannot address. Here, we aimed to provide a roadmap to avoid fundamentally incorrect or inefficient research goals in this field. We introduced the concept of Amdahl's Law of LLMs, which helps determine how partial improvements from 1-bit LLMs translate into overall model enhancements. We conducted extensive evaluations across different LLMs with various hyperparameters to identify relevant patterns. The results reveal a significant dependency of 1-bit LLM efficacy on model sizes, hyperparameters, and hardware configurations. Key findings include: (i) 1-bit LLM paradigms have limited impact on smaller language models, particularly when the context length is large, (ii) for medium-sized LLMs, 1-bit LLM methods show benefits, but further algorithmic innovations are needed to enhance parts that 1-bit LLM approaches cannot improve, and (iii) for large-scale LLMs, extreme quantization from 1-bit LLMs alone can improve the majority of computations, in some cases by more than 99%."}]}