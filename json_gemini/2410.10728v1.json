{"title": "Towards LLM-guided Efficient and Interpretable\nMulti-linear Tensor Network Rank Selection", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "abstract": "We propose a novel framework that leverages large language models (LLMs) to\nguide the rank selection in tensor network models for higher-order data analysis.\nBy utilising the intrinsic reasoning capabilities and domain knowledge of LLMs,\nour approach offers enhanced interpretability of the rank choices and can effectively\noptimise the objective function. This framework enables users without specialised\ndomain expertise to utilise tensor network decompositions and understand the un-\nderlying rationale within the rank selection process. Experimental results validate\nour method on financial higher-order datasets, demonstrating interpretable reason-\ning, strong generalisation to unseen test data, and its potential for self-enhancement\nover successive iterations. This work is placed at the intersection of large language\nmodels and higher-order data analysis.", "sections": [{"title": "1 Introduction", "content": "The exponential increase in the volume and richness of available data has led to a widespread use of\nhigher-order data, often represented as higher-order tensors. Tensor decompositions aim to break\ndown these high-dimensional tensors into simpler components, effectively capturing their latent\npatterns and correlations. These techniques have been applied across various fields, such as machine\nlearning, signal processing, computer vision, and quantum physics [1-8]. The success of tensor\ndecomposition techniques is closely linked to their ability to mitigate the curse of dimensionality. By\ncarefully designing the structure of core tensors and multi-linear operations among them, many tensor\ndecomposition algorithms have emerged. Of particular interest is the Fully Connected Tensor Network\n(FCTN) decomposition, which decomposes an Nth-order tensor into N small-sized Nth-order core\ntensors and captures the correlation between any two tensor modes [9].\nHowever, practitioners of tensor decompositions face significant challenges related to model selection,\nparticularly in determining the optimal tensor ranks a problem known as rank selection (RS).\nSpecifically, finding these optimal tensor ranks has been proven to be NP-hard [10, 11], and for most\npractical problems, brute-force rank search is infeasible due to the \u2018combinatorial explosion' [11],\nwhich results in prohibitively high computational costs and time requirements.\nAs a result, in most studies [12\u201315], the rank is either treated as a fixed hyperparameter, set by the\nauthor based on domain knowledge, or determined through random search over possible values.\nAlthough recent works have attempted to determine the optimal rank using statistical or sampling-\nbased optimisation methods, these typically address other tensor decomposition problems [16\u201318].\nIn contrast, the FCTN decomposition poses a more challenging problem for rank selection due\nto its fully connected structure, thus facing an exponential growth in the number of possible rank\ncombinations when the number of modes increases."}, {"title": "2 Tensor Network Rank Search and Its Search Space", "content": "Tensor networks can be represented using the graphical notation [31], whereby a tensor network\nis represented using a set of vertices and edges. Each vertex represents a decomposed core tensor,\nand the closed edges between two core tensors are generalised higher-order matrix multiplications,\ntermed tensor contractions [9]. Each closed edge has an assigned value, its rank, which indicates the\ndegree of connection between the two vertices. By setting the rank of a closed edge to 1, we can\nharmlessly drop that edge, resulting in a new tensor network structure. Thus, as shown in Figure 1,\nthe tensor network structure search problem is equivalent to tensor network rank search under a given\nfixed number of vertices [32].\nSince the FCTN 2 topology can exploit mode interactions between any two tensor modes, we constrain\nthe search space to a fully connected tensor network [9], which has a graphical notation resembling\na complete graph G = (V, E) [32\u201334], where G denotes the graph, V denotes the N core tensor\nvertices of the Nth-order tensor, X \u2208 RI1\u00d712\u00d7\u2026\u00d7 In, and E denotes the rank values of each edge. In\nother words, the tensor network structure is constrained to having N vertices for an N-order tensor,\nand every vertex is fully connected to all other vertices. In this way, the search for the ranks of the\nFCTN topology is equivalent to TN structure search under the constraint of a complete graph [32],\nwhich generalises the rank search and mode permutation search [16] of popular TN decompositions,\nsuch as the Tensor Train [35] and the Tensor Ring [36] decompositions."}, {"title": "3 LLM-guided Interpretable Rank Selection", "content": "For our prompting strategy, the popular 'chat' interface is employed, where interactions with the LLM\nfollow a structured dialogue between the user and the assistant [38, 39]. The user message serves as\nthe prompt that we supply to the LLM, while the assistant message represents the response of the\nmodel. This dialogue-based approach is intuitive for generating stepwise, conversational reasoning,\nmaking it particularly useful for iterative tasks such as rank selection in tensor decomposition.\nMoreover, in our approach we prompt the model with the entire conversational history of messages\nup to that point. This enables the LLM to retain context from previous steps, allowing it to better\nunderstand the problem and refine its responses in subsequent iterations. We postulate that this\naccumulation of context provides significant benefits by maintaining continuity, ensuring the relevance\nof the reasoning of the model, and enabling more informed and accurate responses as the experiment\nprogresses.\nIt is important to note that this approach comes at a cost, as it leads to a linear increase in the number\nof messages with each iteration, as every iteration introduces two additional messages: the user\nprompt and the response of the LLM. Consequently, it is crucial to ensure that the total length of the\naccumulated input, which includes all user prompts and LLM responses across iterations, and output,\nremains within the context window limit allowed by the model.\nWe now describe the specific details of our system message, input prompt, and iterative prompt that\nenable the LLM to make informed decisions regarding rank selection. Real-world data experiments\nshow that these components leads to improved effectiveness in the rank selections, in terms of\nminimising the loss function and enhancing the interpretability of the responses of LLMs."}, {"title": "4 Methodology", "content": "As illustrated in Figure 2, our LLM-guided tensor network rank search framework operates in an\niterative fashion. Initially, the LLM processes mode information about the input tensors, along with\na system message and prompt, to recommend an initial set of tensor network ranks. These ranks\nare then passed through the FCTN decomposition API, which evaluates their effectiveness via the\nobjective function. The LLM subsequently receives the iterative prompt, refines its reasoning, and\ngenerates new rank suggestions in a repeated cycle to optimise the objective function, until an early\nstopping condition has been met or it has reached 10 iterations.\nEarly Stopping. Since there is no convergence guarantee in the rank values the LLM suggests, we\nemploy an early stopping technique to stop the iterative cycle early if the objective function calculated\non the training data has not improved for a certain number of iterations.\nRank Constraints. The authors in [28] showed that the ranks in a FCTN decomposition are upper\nbounded by the sizes of the two modes, of which each edge connects. Therefore, an automatic check\nis implemented to verify that the LLM-suggested ranks do not exceed their upper bounds and if so,\nreduce them to the upper bound. This technique effectively reduces the rank search space for the LLM.\nThis restriction is also applied to the range of the random search and Bayesian optimisation-based\nbaselines detailed in Section 5.\nInterpretability. A major aim of our proposed framework is to enhance the interpretability in\nthe rank selection process. In particular, by analyzing the reasoning in the LLM responses, we\ncan understand why certain interactions require a specific rank. Most importantly, this allows any\nnon-expert to decide on the tensor ranks effectively using LLMs. Furthermore, in cases where an\nexpert has trouble in figuring out the mode interactions, LLMs can help guide the process."}, {"title": "5 Experimental Results", "content": "Our method can be flexibly applied to any tensor data across various domains. In the experiments,\nfinancial time series data were used, as they contain multidimensional complex interactions between"}, {"title": "6 Conclusion and Future Work", "content": "We have presented an LLM-guided tensor network rank search framework which improves the\ninterpretability in the rank selection process. By carefully designing the prompts to help the LLM\nfocus on the objectives and enhance its reasoning over multiple iterations, our approach has made\ntensor network decompositions more accessible. Experimental results on financial data tensors have\nshown that our framework not only enhances interpretability but also helps non-experts without the\ndomain-specific knowledge to effectively utilise tensor network models, whilst also showcasing strong\ngeneralisation properties. This work contributes to making advanced tensor techniques more widely\nutilised and propels future research at the intersection of large language models and higher-order data\nanalysis.\nFuture work. Although our proposed framework has demonstrated enhanced interpretability, we\naim to further improve both the accuracy and interpretability of our LLM-guided rank selection\nframework. In future research, we plan to explore additional prompting techniques, such as Tree-\nof-Thought [43], which has proven to be powerful for complex reasoning tasks requiring multi-step\nthinking. Furthermore, we intend to also compare our approach with sampling-based methods, such as\nTnALE [11]. Additionally, we aim to mathematically verify the reasoning of the LLMs by developing\nnovel visualisation methods for the core tensor entries of the obtained FCTN decompositions.\nLimitations. The LLM-guided process relies on the reasoning consistency of the LLM, i.e., how\noften the LLM hallucinates, and how faithful the LLM is [44]. In practice, we observed that although\nthe LLM reasons why it should not use the same set of tensor network ranks twice, it would sometimes\nhallucinate and output previously used tensor network rank combinations. Using LLMs specifically\ntrained for reasoning and experimenting with techniques such as scaling up test-time compute [45]\npromises to improve the proposed framework."}, {"title": "A Mathematical definition of the FCTN decomposition", "content": "Fully Connected Tensor Network (FCTN) decomposition was proposed to allow for the correlation\ncharacterisations between any two modes [9]. This was achieved by fully connecting every decom-\nposed tensor. The FCTN decomposition also has the property of transpositional invariance. For\nan order-N tensor $X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\dots \\times I_N}$, its Fully Connected Tensor Network decomposition can be\nwritten as [9]\n$X(i_1,i_2,..., i_N) = \\sum_{r_{1,2}=1}^{R_{1,2}} \\sum_{r_{1,3}=1}^{R_{1,3}} \\dots \\sum_{r_{1,N}=1}^{R_{1, N}}\\dots\\sum_{r_{2,3}=1}^{R_{2,3}}\\sum_{r_{2, N}=1}^{R_{2, N}}\\dots \\sum_{r_{N-1,N}=1}^{R_{N-1,N}}\\ G_1(i_1,r_{1,2},r_{1,3},...,r_{1,N}) \\\\G_2(r_{1,2}, i_2, r_{2,3},...,r_{2,N})\\dots\\\\G_k(r_{1,k},r_{2,k},...,r_{k-1,k}, i_k, r_{k,k+1},...,r_{k,N})\\dots\\\\G_N(r_{1,n},r_{2,N},...,r_{N-1,N},i_N)$  (2)\nwhere $G_k \\in \\mathbb{R}^{R_{1,k}\\times R_{2,k}\\times \\dots \\times R_{k-1,k}\\times I_k\\times R_{k,k+1}\\times \\dots \\times R_{k,N}}$ for $k = 1,2,..., N$ are the decomposed\norder-N FCTN core tensors and ${R_{k1,k2}}_{k1=1,k2=1}^N$ where $R_{i,j} = R_{j,i}$ are the FCTN ranks. If we\nset any rank to 1, it essentially drops that connection. However, despite the FCTN decomposition\nenabling a stronger inter-mode correlation characterisation ability, it has a rank count which scales\nexponentially against the number of modes, N. This renders the problem of finding the optimal ranks\ndifficult."}, {"title": "B Detailed training and testing approximation errors achieved by the\nbaselines and our proposed framework", "content": "We show the exact training and testing approximation errors (even those over 1000) achieved by the\nbaseline models and our proposed framework in Table 2 and Table 3. Note that the LLM-suggested\nrank values yielded test approximation errors consistently lower than the train approximation errors\nafter iteration 2, whereas it is not observed in the baseline models, showing the generalisation ability\nof our proposed framework."}]}