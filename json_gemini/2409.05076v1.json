{"title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions", "authors": ["Yudong Zhang", "Ruobing Xie", "Jiansheng Chen", "Xingwu Sun", "Yu Wang"], "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well-designed adversarial examples. Therefore, LVLMs are in urgent need of detection tools for adversarial examples to prevent incorrect responses. In this work, we first discover that LVLMs exhibit regular attention patterns for clean images when presented with probe questions. We propose an unconventional method named PIP, which utilizes the attention patterns of one randomly selected irrelevant probe question (e.g., \"Is there a clock?\") to distinguish adversarial examples from clean examples. Regardless of the image to be tested and its corresponding question, PIP only needs to perform one additional inference of the image to be tested and the probe question, and then achieves successful detection of adversarial examples. Even under black-box attacks and open dataset scenarios, our PIP, coupled with a simple SVM, still achieves more than 98% recall and a precision of over 90%. Our PIP is the first attempt to detect adversarial attacks on LVLMs via simple irrelevant probe questions, shedding light on deeper understanding and introspection within LVLMs.", "sections": [{"title": "1 Introduction", "content": "Large vision-language models (LVLMs) have demonstrated their powerful multimodal capabilities across a range of tasks [1, 16, 28]. However, LVLMs continue to confront significant security challenges. Adversaries can perturb the images with elaborate and almost imperceptible noise, leading LVLMs to produce incorrect outputs or even outputs that align with the adversaries' intentions [2, 4, 8, 24, 31, 34]. Significant security issues have impeded the adoption of LVLMs in critical areas [11, 17, 18, 26, 29, 32, 33].\nAdversarial attacks on image modalities are facilitated by their high-dimensional and continuous nature. In recent years, there has been a surge in studies on adversarial attacks on LVLMs. Attack-Bard [8] manipulates images to make ChatGPT-4 errors on image caption, while Carlini [4] leads LVLMs to produce unethical words in response to adversarial images. In contrast to adversarial attacks, there has been limited research on the detection of adversarial examples. Therefore, there is an urgent need for detection methods for adversarial examples to enhance the safety of LVLMs.\nPrevious work [21, 22, 30] has primarily focused on the detection of adversarial examples in isolated vision models, i.e., image classifiers based on convolutional neural networks (CNNs), which are not applicable to LVLMs. The detection of adversarial examples on LVLMs faces several challenges: First, LVLMs have more layers and contain interactions between image and text messages, making it difficult to determine where adversarial attacks take effect. Second, traditional CNNs have a more centralized data distribution (e.g., imagenet classifiers detect imagenet adversarial samples), while LVLMs accept data inputs from open scenarios. In addition, LVLMs have a high inference cost and it is inappropriate to introduce too many inference times during the detection process. Few studies have focused on the detection of adversarial examples on LVLMs.\nIn this paper, we introduce an extremely simple method for the detection of adversarial examples, named PIP, that leverages the attention pattern of irrelevant probe questions. We initially discovered that for yes/no type questions, LVLM exhibits a regular"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Large Vision-Language Models", "content": "Alignment-based vision-language models generally comprise three modules: a visual encoder, a large language model, and a vision-language alignment module. Generally, LVLMs utilize pre-trained visual encoders and large language models, with the vision-language alignment module fine-tuned to enable multimodal capabilities. Recently, a lightweight vision-language alignment module, the Query-ing Transformer (Q-former), has gained popularity. Recent popular LVLMs employing this alignment technique include BLIP-2 [13], InstructBLIP [6], and MiniGPT-4 [35]."}, {"title": "2.2 Adversarial Attacks and Adversarial Examples", "content": "Adversarial attacks generate adversarial examples by introducing almost imperceptible perturbations to images, leading neural net-works to respond incorrectly. Previous studies have focused on perturbations of visual modalities, including FGSM [10], PGD [20], JSMA [23], and C&W attack [5], among others. Research has also been conducted on attacks targeting textual modalities, such as Bert-Attack [14] and TextFooler [12]. Large vision-language models are also vulnerable to adversarial examples. For instance, Attack-Bard [8] induces incorrect captions in Google's Bard and OpenAI's ChatGPT-4 through black-box attacks on the image, while Carlini et al. [4] prompt LVLMs to generate inappropriate speech (e.g., profanity, biased statements, etc.) via white-box attacks on the image."}, {"title": "2.3 Detecting Adversarial Examples", "content": "Since the proposal of adversarial attacks, significant research has been devoted to the detection of adversarial examples, aiming to alert the model to the presence of such examples. This is particu-larly critical in LVLMs because deliberately designed adversarial examples can cause LLMs to generate outputs aligned with the adversary's intentions, potentially leading to robustness and hallucination issues. More gravely, the model may produce statements that are socially and morally reprehensible.\nPrevious research has concentrated on detecting adversarial ex-amples for visual modality CNNs, as evidenced by [7, 9, 15, 19]. However, research on detecting adversarial examples in vision-language multimodal models is lacking. This represents a consid-erable risk when employing LVLMs in sensitive domains. LVLMs require the capability to detect adversarial examples and, in re-sponse, should either refuse to answer or neutralize the adversarial input to provide a clean response."}, {"title": "3 A New Task: Detecting Adversarial Examples in Large Vision-Language Models", "content": ""}, {"title": "3.1 Definition of Our Adversarial Examples Detection Task", "content": "In this section, we introduce a novel task aimed at detecting adver-sarial examples in LVLMs.\nLet f represent a large vision-language model, which takes an image I and a question Q as input, and produces an answer A =\nf(I, Q). The task will provide two datasets, \\(D_{ref}\\) and \\(D_{test}\\), where \\(D_{ref}\\) comprises N samples from a large clean dataset D, serving as a reference for clean examples. Additionally, we randomly select M images and questions from D to create the dataset \\(D_{clean}\\), and then execute adversarial attacks on these M images to generate the dataset \\(D_{adv}\\). By randomly combining clean and adversarial examples in a ratio of Mclean: Madu, we generate the test dataset \\(D_{test}\\). The task's objective is to train a classifier h(I) that discerns whether each I \u2208 \\(D_{test}\\) originates from the clean dataset \\(D_{clean}\\) or the adversarial dataset \\(D_{adv}\\). Note that for this task, only \\(D_{ref}\\), f, and the test dataset \\(D_{test}\\) are provided. If I originates from \\(D_{clean}\\), the ground truth g(I) should be 1, otherwise 0, as delineated in Eq. (1)."}, {"title": "3.2 Evaluation of Our Detection Task", "content": "To evaluate the performance of the classifier h, which determines the likelihood of I being an adversarial example, we employ the metrics outlined in Eq. (2).\nUpon defining True Positives (TP), True Negatives (TN), False\nPositives (FP), and False Negatives (FN), we compute the preci-sion, recall, accuracy, and F1-score to comprehensively assess the performance of classifier h."}, {"title": "4 Explore the Use of Our PIP to Detecting Adversarial Examples", "content": ""}, {"title": "4.1 LVLMs Have Regularized Attention Patterns of Clean Examples to Yes/No Questions", "content": "We begin by selecting a popular LVLM InstructBLIP Vicuna-7B (decoder-only) to explore attention patterns, then observe other LVLMs if there is a similar phenomenon. It is based on the Q-former architecture, and the LLM receives 32 image tokens and P question tokens as inputs. Initially, the LLM computes self-attention on the"}, {"title": "4.2 The Attention Patterns of \"yes/no\u201d Probe Questions between Clean and Adversarial Examples are Clearly Distinguishable", "content": "As previously identified in Sec. 4.1, yes/no type questions exhibit regular attention patterns. This section explores the differences in attention patterns between clean and adversarial examples for yes/no questions.\nWe randomly selected 1000 images and questions from dataset\n\\(D_{clean}^{1k}\\). Using PGD on the white-box model, we\nattack \\(D_{clean}^{1k}\\) to generate the adversarial example datasets \\(D_{CLIP}^{1k}\\)\nand \\(D_{LLM}^{1k}\\). For the PGD attack, the number of steps was set to\n20, with a perturbation size per step of a = 2/255, and a total\nperturbation limit of \\( \\epsilon_{\\infty} \\) = 8/255. We consider two methods of\nattack. For \\(D_{LLM}^{1k}\\), an untargeted attack was conducted on the LLM's\nlogit cross-entropy. Obtaining the LLM component of an LVLM can\nbe challenging, whereas accessing its visual encoder (typically CLIP\n[25] or EVA-CLIP [27]) is comparatively easier for an adversary.\nTherefore, for \\(D_{CLIP}^{1k}\\), the mean square error (MSE) loss function\nwas employed to conduct an untargeted attack on the output feature\nof the CLIP or EVA-CLIP visual encoder.\nWe randomly select a yes/no question as the probe question,\nfor example, \u201cIs there a clock?\u201d. Images from \\(D_{clean}^{1k}\\), \\(D_{CLIP}^{1k}\\) and\n\\(D_{LLM}^{1k}\\), along with the probe question, were fed into InstructBLIP\nVicuna-7B, with their attention maps displayed in Fig. 4. Clearly,\nFigure 4a exhibits significant differences when compared to Fig. 4b\nand Fig. 4c. Specifically, the 27th token in Fig. 4a is predominantly\nblack, whereas Figure 4c shows a reduced percentage of black, and\nFig. 4b contains very little black. Furthermore, the attention for the\n28th token in Fig. 4b and Fig. 4c is more pronounced than in Fig. 4a.\nAlthough Fig. 4 illustrates results from just one layer of the LLM,\na significant difference between the clean and adversarial examples\nis already evident. A similar phenomenon is widely observed across\nother layers of the LLM. It indicates that the attention patterns of\nclean and adversarial examples are straightforward and likely to"}, {"title": "4.3 Distinguishing Attention Maps via Lightweight Support Vector Machine", "content": "As discussed in Sec. 4.2, the use of an irrelevant probe question has been identified as a viable method for distinguishing between clean"}, {"title": "4.4 Exploring the PIP's Decision-making Process with Decision Trees", "content": "In previous sections, PIP with SVM was used to detect adversarial samples. However, visualizing the SVM's decision-making process is challenging due to the high-dimensional space of the attention maps. In this section, the decision tree (DT) is used as an intuitive alternative to SVM.\nDecision tree operates by recursively partitioning a dataset into increasingly smaller subsets and facilitates the generation of easily understandable rules. To visualize the decision-making process, a DT with a depth of 2 is trained. The input for the DT consists of attention maps with 1024 dimensions (32 layers \u00d7 32 tokens, with the maximum value in the multi-head attention dimension).\nThe DT(depth=2) makes a linear decision based on two dimen-sions, as illustrated in Fig. 5. Despite its limited depth of 2, the DT successfully detects adversarial examples, as evidenced in Tab. 2. Increasing the DT's depth could enhance its performance. However, this work only focuses on visualizing the decision-making process using DT(depth=2) to demonstrate PIP's effectiveness."}, {"title": "5 In-depth Analyses on PIP", "content": "Having initially validated PIP's effectiveness, we then explored its generalizability across various settings and endeavored to under-stand and analyze our detection method and the intrinsic mecha-nisms underlying the differences in attention patterns.\nWe obtained 5,000 \\(D_{adv}^{ref}\\) images from adversarial attacks on\n\\(D_{clean}^{ref}\\). Afterward, an SVM was trained using the attention pattern\nof 10,000 images on the probe question. In Sec. 5.1, Sec. 5.2 and\nSec. 5.3, we try the results of this SVM on different test data distri-butions (training SVM on COCO while test adversarial examples on\nImageNet), attack methods (PGD to C&W), and attack parameters"}, {"title": "5.1 Generalization of Our Adversarial Examples Detection Method across Datasets", "content": "In Sec. 4.3, a portion of the adversarial examples was successfully detected in the dataset \\(D_{test}\\). However, there exists a potential issue as both \\(D_{ref}\\) and \\(D_{test}\\) derive from D (i.e., COCO dataset). In practical applications, access to the dataset of user input images may be unavailable, making it crucial to assess the generalization performance when \\(D_{ref}\\) and \\(D_{test}\\) do not align.\nIn this section, \\(D_{test}\\) is replaced with the ImageNet dataset, while maintaining \\(D_{ref}\\) as is. Specifically, an image from each of ImageNet's 1000 classes was randomly selected, and its label was used to generate a corresponding question using the template \u201cIs there a/an {label}?\u201d. This process forms the dataset \\(D_{clean}\\), and then, following the previous method, these 1000 images and questions were attacked to create the dataset \\(D_{adv}\\). \\(D_{clean}\\) and \\(D_{adv}\\) were mixed according to Mclean: Madu, with the detection of adversarial examples performed using the method described in Sec. 4.3. The results are presented in Tab. 3. Despite the inconsistency between \\(D_{ref}\\) (from COCO) and \\(D_{test}\\) (from Imagenet-1K), PIP maintains high accuracy (over 90%). This indicates that PIP's effectiveness is"}, {"title": "5.2 Generalizability of PIP over Different Attack Methods", "content": "In prior experiments, the generalizability of PIP across various attack targets, such as \\(D_{CLIP}^{adv}\\) and \\(D_{LLM}^{adv}\\), was established. However, these experiments were exclusively based on the PGD attack method. The ability of SVMs trained on PGD-generated adversarial examples to generalize to unknown attack methods in real-world scenarios remains a critical aspect of PIP's utility.\nThis section examines the effectiveness of PIP against another popular attack method C&W beyond PGD. For the C&W attack, the process iterated over 50 steps with a perturbation of 0.01 per step, equivalent to 2.55/255. Table 4 displays the experimental results. Even against unknown attacks (C&W) and on datasets distinct from \\(D_{ref}\\) (ImageNet-1K), PIP demonstrates superior performance."}, {"title": "5.3 Generalization Performance of PIP on Different Attack Parameters", "content": "In the training process of SVM and prior experiments involving PGD attacks, a consistent perturbation strength of \\( \\epsilon_{\\infty} \\) = 8 was employed. The aim is to ascertain PIP's generalizability across weaker and stronger PGD attacks. Table 5 examines PIP's efficacy on \\(D_{test}\\) examples under three perturbation levels \\( \\epsilon_{\\infty} \\) = {2, 4, 16}, revealing that PIP maintains a 40% recall even with a minimal perturbation of \\( \\epsilon_{\\infty} \\) = 2. With an \\( \\epsilon_{\\infty} \\) = 4 attack, PIP achieves over 80% recall. Under a significantly strong attack of \\( \\epsilon_{\\infty} \\) = 16, PIP is able to detect nearly all adversarial examples. Table 5 demonstrates PIP's generalizability to both weaker and stronger attacks."}, {"title": "5.4 Results of PIP on other LVLMS", "content": "In Tab. 1 and Tab. 3, InstructBLIP Vicuna-7B is utilized as the LVLM. Assessing whether other LVLMs can employ PIP to detect adver-sarial examples is crucial for evaluating PIP's generalizability. This section examines other versions of BLIP-2 and InstructBLIP to evaluate PIP's performance on various LVLMs. For each model, PIP initially extracts the attention maps from \\(D_{ref}\\), followed by SVM training. PIP's transferability across LVLMs is impossible due to varying attention map configurations and dimensions among LVLMs, such as [32, 32] for Vicuna-7B, [40, 40] for Vicuna-13B, [24, 32] for FlanT5XL, [24, 64] for FlanT5XL, with the first number rep-resenting the LLM's layers and the second the multi-head attention count. Therefore, a separate PIP detector must be trained on each LVLMs. For BLIP-2 and InstructBLIP, the analysis includes both decoder-only LLMs (e.g., OPT, Vicuna) and encoder-decoder LLMs (e.g., FlanT5). In decoder-only LLMs, we focus on the attention maps of 32 image tokens during the generation of the first word. In encoder-decoder LLMs, we focus on the cross-attention maps of 32 image tokens at the first word's generation. Within the multi-head attention dimension, only the head with the highest attention is se-lected to reduce the attention map dimensions. Table 6 presents the results of applying PIP with SVM across a broader range of LVLMs and demonstrates the effectiveness of PIP on different LVLMs."}, {"title": "5.5 Alleviating the High False Alarm Rate Issue", "content": "The primary advantage of PIP is its high recall rate, which is critical for LVLMs. Adversaries can manipulate the model using adversarial examples, potentially leading to significant public opinion risks if the model generates content that violates morality or law. However,"}, {"title": "5.6 Generalization Performance of PIP on Actual Black-box Attacks", "content": "In earlier experiments, adversarial examples were generated through the white-box attacks. However, in real-world scenarios, obtaining the model's weights and executing white-box attacks is challeng-ing for users. Consequently, numerous studies have investigated black-box attacks. Exploring PIP's effectiveness against black-box attack-generated adversarial examples is worthwhile.\nAttack-Bard [8] used black-box attacks on the NIPS2017 dataset to generate adversarial examples, successfully compromising major commercial models like ChatGPT-4V, Google's Bard, Bing Chat, and ERNIE Bot (with approximately 45% attack success on ChatGPT-4V). This section involves selecting 200 original images from the NIPS2017 dataset along with corresponding adversarial images generated by Attack-Bard to create the dataset \\(D_{test}\\), using PIP to detect adversarial examples on InstructBLIP.\nThis constitutes a comprehensive evaluation of PIP, as in this experimental setup, the constant factor is the model used in both training and detection phases (necessary due to varying attention patterns across models). Beyond this, PIP remains uninformed about other aspects like the distribution of user input data, attack meth-ods, parameters, targets, and the models used for the attack. Table 8 affirms PIP's generalizability in authentic black-box attack scenar-ios. In the context of black-box attacks, PIP maintains a recall rate exceeding 95% and a precision greater than 80%."}, {"title": "6 Conclusion", "content": "In this paper, we introduce PIP, a new and simple method for de-tecting adversarial examples in LVLMs. Although PIP is simple and whimsical, it achieves impressive results on recent, popular LVLMs like BLIP-2 and InstructBLIP, achieving high recall rates of adversarial examples with low false alarms among clean examples. For detected adversarial examples, post-processing measures such as focusing on alert examples, denying answers, and implementing adversarial defenses can enhance the security and robustness of LVLMs, thereby mitigating public and legal risks."}]}