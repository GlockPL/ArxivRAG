{"title": "PERMUTATION INVARIANT LEARNING WITH HIGH-DIMENSIONAL PARTICLE FILTERS", "authors": ["Akhilan Boopathy", "Aneesh Muppidi", "Peggy Yang", "Abhiram Iyer", "William Yue", "Ila Fiete"], "abstract": "Sequential learning in deep models often suffers from challenges such as catastrophic forgetting and loss of plasticity, largely due to the permutation dependence of gradient-based algorithms, where the order of training data impacts the learning outcome. In this work, we introduce a novel permutation-invariant learning framework based on high-dimensional particle filters. We theoretically demonstrate that particle filters are invariant to the sequential ordering of training minibatches or tasks, offering a principled solution to mitigate catastrophic forgetting and loss-of-plasticity. We develop an efficient particle filter for optimizing high-dimensional models, combining the strengths of Bayesian methods with gradient-based optimization. Through extensive experiments on continual supervised and reinforcement learning benchmarks, including SplitMNIST, SplitCIFAR100, and ProcGen, we empirically show that our method consistently improves performance, while reducing variance compared to standard baselines. Project website and code is available here.", "sections": [{"title": "1 INTRODUCTION", "content": "What is the optimal order for training data? This question is fundamental to understanding how the sequencing of training data impacts machine learning model performance. In sequential learning settings, such as continual learning and lifelong learning, the sequencing of training data plays a crucial role in determining model performance. When models are trained on ordered minibatches of data, poor ordering\u2014referred to as \"poor permutations\"\u2014can result in catastrophic forgetting and loss of plasticity (Wang et al., 2024; Abel et al., 2023).\nIn continual learning, models process tasks in a specific sequence. Unlike conventional training, where minibatch data is randomized, continual learning often relies on a strict sequence, making models prone to overfitting on newer tasks while losing performance on older tasks. This is known as catastrophic forgetting, where new information erases prior knowledge, severely degrading performance on earlier tasks (Kim & Han, 2023; van de Ven et al., 2022).\nSimilarly, in lifelong reinforcement learning (LRL), agents must adapt to new tasks sequentially. The order in which these tasks are presented can lead to loss of plasticity, limiting the agent's ability to adapt to new environments (Muppidi et al., 2024; Lyle et al., 2022; Abbas et al., 2023; Sokar et al., 2023). This poor ordering can further manifest as negative transfer or primacy bias, where learning earlier tasks biases the agent towards those tasks, impeding adaptation to new tasks (Nikishin et al., 2022; Ahn et al., 2024).\nTo address these challenges, we propose a shift in perspective, viewing the problem through the lens of permutation invariance. By developing learning algorithms that are invariant to the order of data presentation, we can mitigate catastrophic forgetting and loss of plasticity. Our key insight is the use of particle filters, a probabilistic tool widely used in state estimation, to achieve this goal.\nParticle filters excel at dynamically estimating system states from noisy data and are grounded in Bayesian inference (Thrun et al., 2005; Doucet et al., 2001b; Jonschkowski et al., 2018; Karkus et al., 2021; Corenflos et al., 2021; Pulido & van Leeuwen, 2019; Maken et al., 2022; Boopathy et al., 2024). However, their application to modern machine learning has been limited due to scalability issues"}, {"title": "2 RELATED WORK", "content": "Particle filters, or sequential Monte Carlo methods, are widely used for state estimation in non-linear and non-Gaussian settings. They represent probability distributions through a set of samples (particles), providing flexibility in capturing complex dynamics (Doucet et al., 2001a). In fields such as robotics, particle filters have been applied successfully to localization and mapping problems, where they handle uncertainty and non-linearities effectively (Thrun, 2002). However, a key limitation is their scalability: as the dimensionality of the problem increases, the number of particles needed grows exponentially, making them less practical in high-dimensional spaces like those in machine learning (Bengtsson et al., 2008). Recent efforts have focused on improving particle filter scalability through adaptive resampling and dimensionality reduction techniques (Li et al., 2015), but these approaches have not fully bridged the gap for large-scale machine learning applications. Our work addresses this gap by proposing a high-dimensional particle filter that is computationally efficient and well-suited for machine learning tasks.\nBayesian model averaging (BMA) is a powerful technique for integrating uncertainty into model predictions by averaging across multiple models (Hoeting et al., 1999; Wasserman, 2000). By weighting model predictions based on their posterior probabilities, BMA can provide more robust predictions and better capture model uncertainty compared to single-model approaches (Raftery et al., 2005). In modern machine learning, BMA has been employed to enhance performance and uncertainty estimation, notably in ensemble techniques (Lakshminarayanan et al., 2017; Wortsman et al., 2022). However, BMA has not been extensively explored in the context of continual or permutation-invariant learning, where uncertainty over tasks and sequential data plays a crucial role.\nIn this work, we demonstrate the benefits of particle filters in high-dimensional machine learning settings. We then describe a particular particle filter that functions as a BMA technique, and demonstrate its advantages empirically."}, {"title": "3 PARTICLE FILTERS FOR LEARNING PROBLEMS", "content": "In this section, we first theoretically demonstrate two beneficial properties of particle filters generally on learning problems, namely 1) permutation-invariance and 2) avoidance of catastrophic forgetting and loss of plasticity. We then describe a particular particle filter suitable for high-dimensional learning problems.\nConsider a learning problem that provides a sequence of loss functions of model parameters, and the goal of learning is to minimize the sum of the loss functions. For instance, the loss function at each time step might correspond to the cross-entropy loss on a minibatch of points for a classification problem. We denote the model parameters at time t as $x_t \\in \\mathbb{R}^d$ and the loss function at time t as $L_t \\in \\mathbb{R}^d \\rightarrow \\mathbb{R}$. The goal is to find an $x$ minimizing $\\sum_{t=1}^T L_t(x)$, where T is the total number of updates.\nHow can we apply particle filters to this learning problem? To do this, we suppose that instead of learning a single model, we learn a distribution of models following a Bayesian approach. Specifically, suppose that at time t = 0, we initially start with a prior distribution of candidate models; each $L_t$ corresponds to an observation that updates the likelihood of each model. Specifically, we suppose that the likelihood of the model x is set as:\n$P(L_t|x) = e^{-L_t(x)}$\nThis likelihood function increases the likelihood of models that achieve lower loss values. We denote the prior distribution of models as $p_0$ and the posterior distribution after having observed $L_1$ through $L_t$ as $p_t$. Then, $p_T$ is given by:\n$p_T(x) = Z_T p_0(x) \\prod_{t=1}^T P(L_t|x) = Z_T p_0(x) e^{-\\sum_{t=1}^T L_t(x)}$\nwhere $Z_T$ is a normalization factor that ensures $p_T$ integrates to 1. Observe that this posterior places high density in regions where the summed loss is low.\nParticle filters enable the computation of $p_T(x)$ by incrementally computing estimates of $p_t(x)$. Specifically, given $p_t(x)$, we may compute $p_{t+1}(x)$ as:\n$p_{t+1}(x) = \\frac{p_t(x)P(L_{t+1}|x)}{\\int p_t(x')P(L_{t+1}|x')dx'} = \\frac{p_t(x)e^{-L_{t+1}(x)}}{\\int p_t(x')e^{-L_{t+1}(x')}dx'}$\nThis Bayesian update equation may often be intractable to compute exactly, particularly when $p_t$ does not have a known parametric form. Instead of tracking $p_t$ exactly, particle filters track an estimate $\\hat{p}_t$ instead:\n$\\hat{p}_t(x) = \\sum_i w_t^{(i)} \\delta(x - x_t^{(i)})$\nwhere $\\delta$ is a delta function, $x_t^{(i)}$ represents the ith particle at time t and $w_t^{(i)}$ represents the weight of the particle at time t. Each particle filter then has a different method of estimating the Bayesian update of Equation 3. After all updates are complete, an ensemble of particles is available, each of which is an estimate of the global minimizer of $\\sum_{t=1}^T L_t(x)$. We denote the output distribution of a particle filter initialized at $p_0$ trained on a sequence of loss functions $L_1, \\dots L_T$ as $p_0[L_1, \\dots L_T]$.\nSince particle filters aim to approximate Bayesian updates, we suppose that each update outputs a set of particles close to the true posterior. To formalize this, suppose that there exists a symmetric, non-negative discrepancy measure $D(p, q)$ that satisfies the triangle inequality:\n$D(p,q) \\leq D(p, r) + D(r, q)$\nfor all p, q, r. Furthermore, suppose D(p, p) = 0 for all p.\nNow, suppose that the particle filter satisfies the following two conditions:\n$D(p[L], q[L]) \\leq CD(p, q)$\nand\n$D(p[L], \\frac{p(x)e^{-L(x)}}{\\int p(x)e^{-L(x)}dx}) < CD(p, p) + \\epsilon$"}, {"title": "3.2 PERMUTATION-INVARIANCE", "content": "Next, we demonstrate that particle filters are approximately permutation-invariant: they produce an output that is nearly invariant to the ordering of loss functions $L_t$. We show that $p_0[L_1, \\dots L_T]$ is similar to $p_0[L_{\\sigma_1}, \\dots L_{\\sigma_T}]$ where $\\sigma_1, \\sigma_2, ...\\sigma_T$ is a permutation of 1, 2, ...T.\nSuppose $\\sigma_1, \\sigma_2, ...\\sigma_T$ is a permutation of 1,2,...T such that N swaps of adjacent elements are required to convert $\\sigma_1, \\sigma_2, ...\\sigma_T$ to 1, 2, ...T. Denote the initialized particle filter as $p_0$. Then,\n$D(p_0[L_1, ...L_T], p_0[L_{\\sigma_1}, ... L_{\\sigma_T}]) \\leq 2N\\epsilon C^{T-2} \\frac{C^2-1}{C-1}$\nThis result demonstrates that particle filters are approximately permutation invariant, especially over small sequences. Standard learning algorithms such as gradient descent are notably not permutation-invariant: they tend to be highly dependent on the ordering of data points. Permutation-invariance enables learning algorithms with less stochastic outputs: in a perfectly permutation-invariant particle filter, the only potential sources of randomness are the initial selection of particles and the randomness in the particle filter updates themselves."}, {"title": "3.3 AVOIDING CATASTROPHIC FORGETTING AND LOSS OF PLASTICITY", "content": "Now, we demonstrate that particle filters naturally avoid catastrophic forgetting and loss of plasticity. Catastrophic forgetting can be formalized in our framework as the phenomenon where a learning algorithm trained on a sequence of losses $L_1, ... L_T$ performs poorly on the earlier losses it is trained on. Similarly, loss of plasticity corresponds to performing poorly on later losses. We provide an upper bound on the loss at any point in training:\nSuppose that all loss functions are bounded in range [0, M]. Suppose that there exists a constant k such that for all loss functions L and distributions p, q:\n$\\mathbb{E}_{x \\sim p}[L(x)] - \\mathbb{E}_{x \\sim q}[L(x)] \\leq kD(p, q)$\nAlso, suppose the particle filter guarantees:\n$\\mathbb{E}_{x \\sim p'[L(x)]} [L(x)] \\leq \\beta\\mathbb{E}_{x \\sim p}[L(x)]$\nfor all L and p. Then,\n$\\mathbb{E}_{x \\sim p_0[L_1,L_2,\\dots L_T]}[L_i(x)] < \\beta M+2kT\\epsilon C^{T-2}\\frac{C^2-1}{C-1}$\nWe make two key assumptions in this theorem: the difference in average loss under two different distributions can be bounded by D and that each step in the particle filter reduces the loss on which it is trained by at least a fixed factor. We believe that the first assumption may in many cases be reasonable if the loss function is sufficiently slow-changing: small changes in the distribution over x should not change the average loss value. The second assumption may also be reasonable under many settings for effective particle filters as well as other standard learning algorithms; with a fixed loss function, it corresponds to a linear convergence rate. Gradient descent, for example, satisfies this assumption on loss functions satisfying the Polyak-\u0141ojasiewicz inequality. The resulting bound on the loss $L_i$ guarantees that the performance can be no worse than if there had only been a single update $L_i$ (yielding loss at most $\\beta M$) plus an additional error term."}, {"title": "3.4 GRADIENT-BASED PARTICLE FILTER", "content": "Given the desirable properties of particle filters in learning problems, here, we describe a particular particle filter well suited to the high-dimensional spaces found in most machine learning settings.\nSuppose that our particle filter's particle distribution $p_t(x) = \\sum_i w_t^{(i)} \\delta(x - x_t^{(i)})$ represents another distribution $\\hat{p}_t(x)$ constructed as:\n$\\hat{p}_t(x) = Z \\sum_i w_t^{(i)} e^{-\\frac{||x - x_t^{(i)}||^2}{2\\sigma^2}}$\nwhere $\\sigma$ is a variance parameter and Z is a normalizing constant. Essentially, $\\hat{p}_t(x)$ replaces each delta function in $p_t(x)$ with a isotropic Gaussian. We derive the particle filter's update on $\\hat{p}_{t}(x)$ as an approximation of the optimal Bayesian update applied to $p_t(x)$. Observe that given a prior of $p_t(x)$ and the loss function $L_{t+1}$, the posterior is proportional to:\n$e^{-L_{t+1}(x)} \\sum_i w_t^{(i)} e^{-\\frac{||x - x_t^{(i)}||^2}{2\\sigma^2}}$\nWe manipulate this expression until it can be expressed in the form of Equation 13. First, we make a linear approximation of $L_{t+1}$ centered at each particle $x_t^{(i)}$:\n$L_{t+1}(x) \\approx L_{t+1}(x_t^{(i)}) + \\nabla L_{t+1}(x_t^{(i)})^T (x - x_t^{(i)})$\nPulling $e^{-L_{t+1}(x)}$ into the summation and applying this approximation:\n$\\sum_i w_t^{(i)} e^{-\\frac{||x - x_t^{(i)}||^2}{2\\sigma^2}} e^{-(L_{t+1}(x_t^{(i)}) + \\nabla L_{t+1}(x_t^{(i)})^T (x - x_t^{(i)}))}$\nGrouping terms:\n$\\sum_i w_t^{(i)} e^{-\\frac{1}{2\\sigma^2} (||x||^2 - 2x^T x_t^{(i)} + ||x_t^{(i)}||^2) - L_{t+1}(x_t^{(i)}) - \\nabla L_{t+1}(x_t^{(i)})^T x + \\nabla L_{t+1}(x_t^{(i)})^T x_t^{(i)}}$\nCompleting the square in the exponent:\n$\\sum_i w_t^{(i)} e^{-\\frac{1}{2\\sigma^2} ||x - (x_t^{(i)} - \\sigma^2 \\nabla L_{t+1}(x_t^{(i)}))||^2 - \\frac{\\sigma^2}{2} ||\\nabla L_{t+1}(x_t^{(i)})||^2 - L_{t+1}(x_t^{(i)})}$\nwhere $x_{t+1}^{(i)} = x_t^{(i)} - \\sigma^2 \\nabla L_{t+1}(x_t^{(i)})$. Simplifying the constant terms:\n$\\sum_i w_t^{(i)} e^{-\\frac{||x - x_{t+1}^{(i)}||^2}{2\\sigma^2}} e^{-\\frac{\\sigma^2}{2} ||\\nabla L_{t+1}(x_t^{(i)})||^2 - L_{t+1}(x_t^{(i)})}$\nObserve that under our linear approximation, $L_{t+1}(x_{t+1}^{(i)}) = L_{t+1}(x_t^{(i)}) - \\sigma^2 ||\\nabla L_{t+1}(x_t^{(i)})||^2$. Thus, we may write the expression as:\n$\\sum_i w_t^{(i)} e^{-\\frac{||x - x_{t+1}^{(i)}||^2}{2\\sigma^2}} e^{L_{t+1}(x_{t+1}^{(i)}) - L_{t+1}(x_t^{(i)}))}$\nFinally, we define $w_{t+1}^{(i)} = w_t^{(i)} e^{L_{t+1}(x_{t+1}^{(i)}) - L_{t+1}(x_t^{(i)}))}$ to arrive at our final approximation of the posterior:\n$\\sum_i w_{t+1}^{(i)} e^{-\\frac{||x - x_{t+1}^{(i)}||^2}{2\\sigma^2}}$\nWe represent this posterior with particles $x_{t+1}^{(i)}$, and respective weights $w_{t+1}^{(i)}$.\nWe summarize the update equations of this particle filter below:\n$x_{t+1}^{(i)} = x_t^{(i)} - \\sigma^2 \\nabla L_{t+1}(x_t^{(i)})$"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "In this section, we empirically validate the permutation-invariance of our gradient-based weighted particle filter (hereafter referred to as the weighted particle filter or WPF) and demonstrate its effectiveness in mitigating catastrophic forgetting and loss of plasticity across continual and lifelong learning benchmarks.\nWe evaluate our weighted particle filter on continual learning benchmarks SplitMNIST (LeCun & Cortes, 2010), SplitCIFAR100 (Krizhevsky, 2009), and ProcGen (Cobbe et al., 2020). SplitMNIST is divided into 5 \"super class\u201d splits, and SplitCIFAR100 into 20, both for class-incremental learning. For ProcGen, we use image-action trajectory datasets from the games Starpilot, Fruitbot, and Dodgeball, partitioned into 15 levels sampled using the hard distribution shift mode, similar to Mediratta et al. (2024).\nWe compare our weighted particle filter against established continual learning methods: Synaptic Intelligence (SI), Elastic Weight Consolidation (EWC), and Learning Without Forgetting (LWF) (Zenke et al., 2017; Kirkpatrick et al., 2016; Li & Hoiem, 2016). Since our particle filter is architecture-agnostic, we also combine it with SI, EWC, and LWF to evaluate their joint effectiveness.\nIn the ProcGen continual learning experiments, we use a supervised behavior cloning policy as our base model and compare it against other baseline particle filters also using supervised behavior cloning. All methods are implemented with identical architectures and learning parameters to ensure a fair comparison.\nAfter training, we measure the average accuracy/return of both the weighted particle filter and the baseline models across all splits/levels using 10 different shuffled permutations of epoch splits. To assess permutation invariance, we calculate the task-specific variance in accuracy/return across these 10 permutations. These experiments are designed to evaluate the particle filter's resistance to catastrophic forgetting.\nOur weighted particle filter uses 100 particles, with test accuracy evaluated as a weighted average across particles. We compare this approach with three additional particle filter baselines: (1) a"}, {"title": "4.1 AVOIDING CATASTROPHIC FORGETTING AND LOSS OF PLASTICITY", "content": "Tables 1 and 2 provide a summary of the performance comparison between our weighted particle filter and the baseline particle filters in both continual learning and lifelong RL experiments. Our weighted particle filter consistently achieves higher mean accuracy (averaged over classes and permutations) on SplitMNIST, SplitCIFAR100, and ProcGen Behavior Cloning datasets compared to the baseline particle filter, averaging particles, and traditional gradient descent (single particle).\nFurthermore, Figure 3 demonstrates that our weighted particle filter is more resistant to loss of plasticity in LRL experiments compared to PPO using gradient descent. This underscores the advantage of incorporating particle weighting into the training process. This effect may align with the conclusions of Lyle et al. (2023); Sokar et al. (2023); Muppidi et al. (2024); Kumar et al. (2023), suggesting that when a model is not overly specialized to a specific task is better able to adapt to new tasks. In our approach, maintaining multiple particles-some tuned to domain-specific tasks and others oriented towards different sequential tasks-enables the agent to switch to well-performing particles when adapting to new environments, thereby preserving plasticity.\nWhile theoretically, the baseline particle filter has the advantages of a Bayesian approach, because of the lack of gradient-based optimization, it fails. This lack of gradient-based optimization in high dimensions means it is essentially making random guesses, leading to performances that are close to what would be expected by chance. While gradient descent might show improved results in the latest epoch, it typically does so at the expense of previous epochs' performance. Therefore, when the performance is averaged across all epochs, the result is diminished, approaching close to random chance levels.\nThe results presented in Table 1 indicate that our weighted particle filter not only successfully avoids catastrophic forgetting but also outperforms all other methods in the SplitMNIST setting. In the SplitCIFAR100 dataset, our model closely competes with the top-performing continual learning model, LWF.\nInterestingly, the greatest benefit is observed when we combine continual learning methods with our weighted particle filter. In all cases, the addition of the weighted particle filter increases accuracy.\nIn the lifelong RL experiments, our weighted particle filter consistently outperforms PPO + EWC across all games. Similar to the continual learning experiments, we observe that combining the weighted particle filter with PPO + EWC or PPO + TRAC results in an increase in average normalized return, demonstrating the effectiveness of these combined approaches."}, {"title": "5 CONCLUSION", "content": "Poor permutations of training data, such as strictly ordered minibatches, can lead to catastrophic forgetting and loss of plasticity. To overcome this challenge, we theoretically demonstrated that particle filters can be permutation-invariant, allowing them to mitigate the issues associated with poor ordering of training data. This permutation invariance offers a principled solution to avoiding catastrophic forgetting and preserving plasticity throughout learning.\nOur results further highlight the effectiveness of a simple, gradient-based weighted particle filter in continual, lifelong, and permutation-invariant learning. Notably, our particle filter is domain-agnostic, significantly improving performance and reducing performance variance in both lifelong reinforcement learning and supervised continual learning settings. Moreover, our approach shows greater resistance to catastrophic forgetting and loss of plasticity. The success of our method lies in the combination of gradient-based updates, which make it suitable for high-dimensional problems, and Bayesian weight updates. Our approach paves the way for broader applications of particle filter methods in high-dimensional state spaces, particularly in modern machine learning."}, {"title": "EXPERIMENTAL SETUP AND DETAILS", "content": "SplitMNIST task: In this task, our objective is to sequentially address a series of five binary classification tasks derived from the MNIST dataset. These tasks are designed to distinguish between pairs of digits, presenting a unique challenge in each case. The specific pairings are as follows:\n\u2022 Digits 0 and 1 ({0v1})\n\u2022 Digits 2 and 3 ({2v3})\n\u2022 Digits 4 and 5 ({4v5})\n\u2022 Digits 6 and 7 ({6v7})\n\u2022 Digits 8 and 9 ({8v9})\nThis task involves the sequential solution of 20 different 5-class classification tasks. Each task is associated with a distinct category comprising a specific group of objects or entities. The categories, along with their corresponding class labels, are listed below:\n\u2022 Aquatic mammals: {beaver, dolphin, otter, seal, whale}\n\u2022 Fish: {aquarium fish, flatfish, ray, shark, trout}\n\u2022 Flowers: {orchid, poppy, rose, sunflower, tulip}\n\u2022 Food containers: {bottle, bowl, can, cup, plate}\n\u2022 Fruit and vegetables: {apple, mushroom, orange, pear, sweet pepper}\n\u2022 Household electrical devices: {clock, computer keyboard, lamp, telephone, television}\n\u2022 Household furniture: {bed, chair, couch, table, wardrobe}\n\u2022 Insects: {bee, beetle, butterfly, caterpillar, cockroach}\n\u2022 Large carnivores: {bear, leopard, lion, tiger, wolf}\n\u2022 Large man-made outdoor things: {bridge, castle, house, road, skyscraper}\n\u2022 Large natural outdoor scenes: {cloud, forest, mountain, plain, sea}\n\u2022 Large omnivores and herbivores: {camel, cattle, chimpanzee, elephant, kangaroo}\n\u2022 Medium-sized mammals: {fox, porcupine, possum, raccoon, skunk}\n\u2022 Non-insect invertebrates: {crab, lobster, snail, spider, worm}\n\u2022 People: {baby, boy, girl, man, woman}\n\u2022 Reptiles: {crocodile, dinosaur, lizard, snake, turtle}\n\u2022 Small mammals: {hamster, mouse, rabbit, shrew, squirrel}\n\u2022 Trees: {maple tree, oak tree, palm tree, pine tree, willow tree}\n\u2022 Vehicles 1: {bicycle, bus, motorcycle, pickup truck, train}\n\u2022 Vehicles 2: {lawn mower, rocket, streetcar, tank, tractor}\nWe use the ProcGen games Starpilot, Dodgeball, and Fruitbot, which employ procedural content generation to create new levels (corresponding to specific seeds) upon episode reset. We specifically use the hard mode to introduce distribution shifts and ensure the tasks are sufficiently challenging for both lifelong reinforcement learning and continual behavioral cloning.\n\u2022 Observation: The observation space consists of an RGB image of shape 64x64x3, representing the state of the environment at each time step.\n\u2022 Action Space: The action space is discrete, with up to 15 possible actions depending on the game.\n\u2022 Reward: Rewards are provided in either dense or sparse formats, depending on the specific game.\n\u2022 Termination Condition: A boolean value indicates whether the episode has ended.\n\u2022 Levels [0, 15): These levels are used for collecting trajectories, specifically for the lifelong RL setup. The agent in our BC experiments is trained sequentially, observing level 0 for 2 million steps, followed by level 1, and so on.\n\u2022 We follow a similar setup to Mediratta et al. (2024). A strong and well-trained PPO policy is used, which was trained for 20 million steps on 200 levels of each game. This approach ensures that the policy generalizes well and acts as a proficient expert agent for collecting trajectories.\n\u2022 Expert Dataset: To generate the expert dataset, we rolled out the final checkpoint of the pretrained PPO model (i.e., the expert policy) across the 15 training levels (splits), collecting 100,000 transitions per level.\nFor the lifelong reinforcement learning setup, we followed the same experimental protocol as Muppidi et al. (2024). In the ProcGen experiments, individual game levels were generated using a seed value as the start_level parameter, which was incremented sequentially to create new levels. Every 2 million steps, a new level was introduced to the agent using the hard distribution mode. To assess permutation invariance, the sequence of start-level seeds was permuted 10 times, providing a diverse set of training orders for evaluation.\nOur Gradient-based particle filter uses 100 particles. Particles are initialized randomly from PyTorch's nn.module network parameters. A small amount of noise is injected into these parameters in the beginning of training to increase exploration of the solution space. Our Gradient-descent implementation uses the same code, except we initialize the particle filter with only one particle. The averaging particle filter simply takes the average of the accuracies of all of the particles. The baseline particle filter does the following:\n1. Resamples particles from the existing pool with probabilities proportional to the exponential of the negative loss associated with each particle..\n2. Applies perturbations to particles, enabling the exploration of the solution space.\n3. Updates the weights of the particles based on the new loss.\nWe have also incorporated three continual learning methods: SI, EWC, and LWF (van de Ven et al., 2022). Each of these methods has been implemented following the default, method-specific settings as prescribed in the (van de Ven et al., 2022) code implementation. These three models used a \"pure-domain\" setting.\nFor a detailed implementation of our particle filter, please refer to our code submission."}]}