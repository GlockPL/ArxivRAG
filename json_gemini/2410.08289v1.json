{"title": "Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference", "authors": ["William Thorne", "Ambrose Robinson", "Bohua Peng", "Diana Maynard", "Chenghua Lin"], "abstract": "As the cultural heritage sector increasingly adopts technologies like Retrieval-Augmented Generation (RAG) to provide more personalised search experiences and enable conversations with collections data, the demand for specialised evaluation datasets has grown. While end-to-end system testing is essential, it's equally important to assess individual components. We target the final, answering task, which is well-suited to Machine Reading Comprehension (MRC). Although existing MRC datasets address general domains, they lack the specificity needed for cultural heritage information. Unfortunately, the manual creation of such datasets is prohibitively expensive for most heritage institutions. This paper presents a cost-effective approach for generating domain-specific MRC datasets with increased difficulty using Reinforcement Learning from Human Feedback (RLHF) from synthetic preference data. Our method leverages the performance of existing question-answering models on a subset of SQUAD to create a difficulty metric, assuming that more challenging questions are answered correctly less frequently. This research contributes: (1) A methodology for increasing question difficulty using PPO and synthetic data; (2) Empirical evidence of the method's effectiveness, including human evaluation; (3) An in-depth error analysis and study of emergent phenomena; and (4) An open-source codebase and set of three llama-2-chat adapters for reproducibility and adaptation.", "sections": [{"title": "1 Introduction", "content": "The cultural heritage sector is increasingly leveraging advanced technologies like large language models (LLMs) (OpenAI, 2024; Touvron et al., 2023a) and AI assistants (Team Gemini, 2023; Anthropic, 2024) to increase and improve access to collections and their associated data. These technologies provide new opportunities for more dynamic and intuitive interactions with heritage content. One particularly promising technology is Retrieval-Augmented Generation (RAG) (Lewis et al., 2021), which retrieves relevant information from a database of vectorized content to generate accurate, fact-based responses to user queries. We believe that RAG, and iterations on the approach, will play a significant role in improving the search capabilities of heritage institutions in the coming years.\nHeritage search systems are used by the public and academics alike; however, the latter tend to submit more complex and specific queries (Koolen and Kamps, 2009). RAG has the capability to fulfil these needs but still requires robust evaluation. This includes not only end-to-end system testing but also the evaluation of individual components. As the response is generally required to be written based only on the retrieved documents to mitigate language model hallucinations, we argue that the task is one of Machine Reading Comprehension (MRC). While MRC datasets are well-established in the general domain, they are notably lacking in cultural heritage and the cost of their construction is prohibitive for most institutions. We estimate that the popular SQuAD dataset cost about $12,000 to just write the questions, based on their recommended time per question and stated hourly rate of $9 (Rajpurkar et al., 2016); the actual cost is likely much higher.\nTo address these challenges, we propose using Automatic Question Generation (AQG) systems to generate MRC datasets. However, we argue that many automatically generated questions, particularly those from zero- or few-shot approaches, do not provide an adequate challenge for modern language models. Manipulating difficulty is challenging through traditional training approaches given its abstract and subjective nature, and prompt based solutions are intractable when considering the infinite permutations and interactions between different aspects of difficulty (Lin et al., 2015a; Rajpurkar et al., 2016; Beinborn et al., 2015; Hsu et al., 2018; Cheng et al., 2021; AlKhuzaey et al., 2023).\nTo control difficulty, we adapt the Reinforcement Learning from Human Feedback protocol used in AI assistant steering (Ouyang et al., 2022; Bai et al., 2022). In this regime, samples are ranked based on specific criteria and paired into chosen and rejected samples for training a reward model. This reward model learns to distinguish good samples from bad and outputs a signal which steers a policy model. Rather than relying on costly human annotations, we generate synthetic preference data by evaluating question-answering model performance on a subset of SQUAD, assuming that questions answered correctly less frequently are inherently more difficult. This approach leverages the language model's innate feature extraction capabilities, eliminating the need to explicitly define difficulty components. Figure 1 demonstrates this feature extraction ability by comparing questions generated with and without reinforcement fine-tuning.\nWe selected SQUAD over an in-domain QA dataset for two main reasons. First, it is a well-studied, large, and diverse dataset. Second, comparable QA datasets at SQuAD's scale are either visual question-answering focused (Sheng et al., 2016; Asprino et al., 2022) or have data reliability concerns such as OCR text (Piryani et al., 2024).\nThis approach enables cultural heritage practitioners to generate challenging evaluation datasets more efficiently and cost-effectively than manual curation. The primary expense is compute resources, which can be accessed in the cloud for only a few dollars per hour."}, {"title": "2 Related Work", "content": "A similar question generation approach to ours is employed by Zhang et al. (2022) who adopt a pipeline structure. However, their primary objective is to generate suitable questions rather than specifically focusing on difficulty. An important distinction lies in their extensive pre-processing applied to identify candidate answers before feeding them to the question generation model. We argue that pre-identifying answers may limit diversity and prevent the inclusion of potentially complex answer types.\nAnalyzing and Controlling Question Difficulty. Understanding and managing question difficulty holds significant importance, especially in tasks involving the creation of exams and assessments (Liu and Lin, 2014; AlKhuzaey et al., 2023). One approach, as presented by Loginova et al. (2021), involves modelling the difficulty of multiple-choice questions through the use of softmax scores obtained from a pre-trained QA model. The variance in these scores is then calculated, with higher variance indicating greater difficulty.\nLin et al. (2015b) controls the difficulty of quiz questions through the selection of distractor answers based on semantic similarity between linked data items. This involves collecting both structured RDF data and unstructured text, computing similarity scores through K-means clustering, and generating questions and answers via template-based methods. Importantly, the semantic similarity plays a role in determining the difficulty level, with more challenging questions featuring distractors exhibiting higher semantic similarity.\nReinforcement Learning with Human Feedback. RLHF is a machine learning paradigm that combines reinforcement learning with human-provided guidance to steer language models to meet the needs of users, finding frequent use in chatbot and AI assistant settings (Ouyang et al., 2022). The basis for most modern methods is the Proximal Policy Optimisation (PPO) algorithm (Schulman et al., 2017), which iteratively enhances the language model's policy to maximize cumulative rewards through interactions with a dataset or language simulation. It collects experiences, evaluates advantages, and updates the policy with a clipped surrogate objective to ensure stability, gradually improving the model's performance.\nAutomatic Question Generation. Chen et al. (2019) introduce a cross-entropy loss with a reinforcement learning-based loss function when training a gated bi-directional neural network for question generation. In this context, the reward model is optimising the semantic and syntactic quality of the question. BLEU-4, as a reward function, optimises the model for the evaluation metrics and the negative Word Movers Distance component is used to ensure semantic quality by maximising the similarity between a generated sequence and a ground truth sequence. Although question quality is maintained, other factors such as question difficulty are not considered.\nSelf-critic sequence training (SCST) (Rennie et al., 2017) uses a classical policy gradient method, REINFORCE, which is a Monte Carlo method. SCST computes rewards with n-gram token overlap as sub-sentence level rewards. Since training sets often have limited questions, these training rewards are arguably sparse, hindering the question generation model from extrapolating beyond the training distribution. Liu et al. (2019) adopt a two-component reward for refining ill-formed questions. Question wording is used as a measure of short-term reward, and alignment between the question and answer represents a long-term component."}, {"title": "3 Method", "content": "To challenge the high cost of manual annotation while maintaining quality and increasing difficulty, we design and implement a robust system capable of generating contextually relevant, coherent, and challenging question-answer pairs from textual input. The process follows the core methodology of RLHF, deviating only in the use of synthetic preference data to train a reward model. Rather than explicitly defining the characteristics of difficulty and risking failure to capture certain aspects, we exploit the ability of leading question-answer models to derive which questions are challenging, and allow a reward model to extract the component features of the task.\nWe task three QA models with answering all questions in our validation split of SQUAD. These questions are assigned a score based on the number of times they were answered incorrectly, which are in turn used to generate pairwise preference data. These pairwise samples enable the training of a reward model (RM) for use in fine-tuning a supervised model (SFT) on the task of question generation using Proximal Policy Optimisation (PPO)(Schulman et al., 2017).\nWe embed this synthetic RLHF process into a greater pipeline for generating samples, shown in Figure 2. This ensures the quality of the final dataset. The pipeline also contains a set of rule-based critics which are used to exclude samples that are malformed and those with non-unique answers in the source text. Samples are then deduplicated using exact string matching.\nThe remainder of this section discusses each of the relevant components of the pipeline and the RLHF process."}, {"title": "3.1 Supervised Fine-Tuning", "content": "In our training process for question generation and response formatting, we begin by employing a re-formatted version of the SQUAD v1 training split (see Table 1). The reformatting converts SQUAD to the task of question-answer pair generation, as shown in Figure 3. We select the \"correct\" answer as the one that appears most frequently in the list of answers for each question in the dataset, selecting randomly among the most common if there is no victor. To ensure model robustness without overfitting, the model undergoes a single epoch of training, enabling it to effectively capture the nuances of the task."}, {"title": "3.2 Reward Modelling", "content": "To control the difficulty of our generated questions, we leverage the intrinsic properties present in challenging questions from SQUAD. To extract these attributes, we employ three question answering models that almost match or exceed human performance on SQUAD v2 to evaluate our development split: a ROBERTa-large model, a DeBERTa-large model and RetroReader (Zhang et al., 2020). Each question is assigned a score based on the number of models that failed to correctly answer the question. These scores are used to place questions into a pairwise ranking setup against other questions for the same input context. Where a question's scores are equal, they are considered ties, and no pairwise sample is created. We also record the margin, defined as the difference in score between the chosen and rejected samples, to experiment with the marginal ranking loss, as defined in Touvron et al. (2023b)."}, {"title": "3.2.1 Format Critics", "content": "To ensure the quality of the final dataset, we utilise a collection of rule-based critics which we call Format Critics. These critics have two main functions: they remove questions that don't adhere to the desired format of Q? (answer: A); they ensure the provided answer is unique in the text, minimising the number of ambiguous or impossible questions. Samples that pass these critics are then deduplicated using exact matching."}, {"title": "3.3 Reinforcement Training", "content": "We use Proximal Policy Optimisation (Schulman et al., 2017) with multiple sets of adapters to reduce the memory overhead during training, implemented using the Transformers Reinforcement Learning library (von Werra et al., 2020). A single base model is used with separate LoRA adapters for the policy, reference, and reward model components; each is switched to perform the relevant aspect of the reinforcement training process.\nDuring early experiments, we found that training was often very unstable or resulted in low pass rates at the format critic. To combat this, we added a rule-based reward component to penalise generations that did not pass the format critic. This simple function converts the reward to be the negative absolute reward in the case that samples are malformed. Using a rule-based reward that manipulates the original reward prevents the instability caused by hard coding a fixed penalty and saves the computational complexity and imperfection of a second adapter-based reward model:\n$R_i = \\begin{cases}-R_i & \\text{if malformed} \\\\R & \\text{otherwise}\\end{cases}$"}, {"title": "4 Experimental Setup", "content": "We conduct our experiments with LLaMa2-7B-chat and apply LORA adapters to all linear layers for all models. This drastically lowers the number of tunable parameters over full-finetuning, enabling training on a single A100 80GB GPU. We also make use of Flash Attention 2 (Dao, 2023) to improve computational and memory efficiency. All LoRA adapters share the same hyperparameters: a LORA rank of 16, as Dettmers et al. (2023) found rank to have minimal impact on task performance while enabling larger batches through reduced memory usage. This memory efficiency further allowed us to implement sample packing, particularly beneficial with Flash Attention 2's preference for minimal padding. We set alpha to twice the rank 5, use a dropout of 0.05 - shown optimal for 7B models by Dettmers et al. (2023), and maintain LLaMa-2's BF16. As a baseline, we compare to LLaMa-2-7B-chat in a zero-shot setting (see Appendix B).\nWe experiment with marginal ranking loss to help distinguish between slight and significant differences in question difficulty while training the reward model. Under the hypothesis that the difficulty of a question is not independent of the associated passage of text, we also experiment with training a reward model with and without the input text attached. Results of these experiments can be found in Appendix A."}, {"title": "4.2 Generation Settings", "content": "During generation, the model is tasked with producing a single output for each question in the training set using nucleus sampling (Holtzman et al., 2020). We maintain the original configuration for LLaMa-2 with a repetition penalty of 1.1, top P of 0.7, and top K of 0 but increase the temperature from 0.6 to 0.9 to increase the diversity of generations."}, {"title": "4.3 Data Splits", "content": "We base our splits off the original SQUAD to minimise the risk of data leakage. We maintain the full train split unchanged as any model previously trained on SQUAD will have seen the full train split. We extract a test split of 500 contexts from the dev split, ensuring no contexts appear in both the dev and test splits. We extract 50 unique contexts from the test split for a human evaluation of question quality and answerability. In all cases, context-question pairs were only kept if they fit into the context length of LLaMa-2 when formatted in the correct prompt format. All samples were formatted into the three instruction components: instruction, input, response as shown in Figure 3.\nOnly the dev set of our SQUAD dataset was used to derive difficulty comparison data, to ensure the reward model never sees the samples used for evaluation. To evaluate the reward model, we extract 10% of the comparison contexts. Full dataset statistics can be found in Table 1."}, {"title": "4.4 Evaluation Metrics", "content": "As our goal is to evaluate the difficulty of answerable questions, we provide the input passage, question and answer to GPT-406 and Gemini-1.5-pro7 and ask whether the sample meets our specification of validity. We take samples to be answerable if they were unanimously labelled as such, and reject all other samples. GPT-based evaluations have demonstrated a robust alignment with human preferences across various complex tasks in reference-free settings (Fu et al., 2023; Liu et al., 2023). The results of this analysis can be found in Appendix C.\nTo assess the quality of generated questions relative to our SQuAD test split, we intentionally avoid n-gram based metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and more modern alternatives such as Q-Metrics (Nema and Khapra, 2018), as we believe they restrict diversity of generation, constraining the model to reference questions and answers. We instead adopt the following reference-free metrics:\nSyntactic Divergence provides a distance measure between two dependency paths which acts as a measure of difficulty. Word-lemma anchors, common to both the question and answer sentence, are first detected. A dependency path from the anchor to the interrogative word (who, what, etc.) in the question is compared to the dependency path between the anchor and the answer span in the answer sentence using Levenshtein distance (Levenshtein et al., 1966).\nRQUGE calculates an acceptability-score by generating an answer for the candidate question and predicting the semantic similarity between the predicted answer and the gold answer provided by the user. In our setup, this metric acts as an assessment of both the question and answer quality (Mohammadshahi et al., 2023).\nQAScore attempts to align AQG evaluation to human judgements. Question-answer pairs are evaluated by summing log-probabilities of ROBERTa correct token predictions for all words in the answer when masked individually. QAScore claims to show strong correlation with human judgement (Spearman r = 0.864) (Ji et al., 2022).\nSelf-BLEU assesses how similar questions are to other questions generated for a given context. Each question is taken as a hypothesis and the others as a reference for the BLEU calculation. The self-BLEU is taken as the average BLEU for the question collection (Zhu et al., 2018)."}, {"title": "5 Results and Discussion", "content": "Model Accuracy. To measure performance, we observe the difference in prediction accuracy for QA models on each dataset. Table 2 shows that in all cases of PPO training, we observe a decrease in average model prediction accuracy and an increase in the total number of valid generations. The consistent decrease in absolute prediction accuracy for all models when using the PPO trained models over both zero-shot and SFT signifies an increase in average question difficulty. The SFT process vastly improves the model's ability to generate valid questions. The PPO process further bolsters this capability which illustrates that the model is learning the intrinsic properties of high-quality questions. The performance of the reward models, shown in Appendix A, is reflected here, showing lesser degrees of improvement for those models fine-tuned without access to the input passage."}, {"title": "5.1 Error Analysis", "content": "Failure Modes. At a high level, we can observe the reasons for sample rejection for each model. As shown in Figure 5, the zero-shot model is generally unable to generate samples that have a single answer span in the text, despite exactly specifying this in the prompt. The high number of incorrectly formatted samples was a result of only a question being generated or neither a question nor answer being generated. For all the trained model variants, the dominant failure mode was unanswerable questions. As shown in Appendix C, each of the fine-tuned models show a similar proportion of otherwise valid samples being unanswerable. The answerability rate could potentially be improved by generating candidate answers, as in (Zhang et al., 2022), and passing an input passage and answer to the question generation model.\nPositional Bias. One interesting phenomenon is the positional bias in where the model chooses to generate answers. To calculate positional bias, we treat the full answer span as a single \"word\" and calculate the proportion through the input paragraph in which the answer word appears. As seen in Figure 6, the zero-shot positional bias is less severe than in the other datasets. The positional bias of SQUAD is clearly seen as, after training on the dataset, all models exhibit this same preference for the beginning of input passages. The clear bias observed in the zero-shot model, despite not being fine-tuned, is documented in other tasks such as LLM ranking (Wang et al., 2023; Li et al., 2023) and in summarisation where introductory content is favoured (Ravaut et al., 2023). A potential remedy is to supply the model with a sliding window of sentences across the context paragraph to force the model to generate questions throughout the text. While this would improve the diversity of a final dataset, it may have the adverse effect of limiting the range of dependencies, restricting potentially challenging questions across the whole text.\nHallucinated External Knowledge. Where ambiguous references to specific entities exist in the input passage such as the museum collection, the models frequently attempt to fill in which entity is being referred to. From a context containing ambiguous references to an unnamed museum, the questions What year did the Tate acquire the statue of St John the Baptist?, How many works does Rodin have in the British Museum's collection? were generated across both the SFT and PPO models; the examples consistently passed LLM evaluations of answerability. This suggests the solution to this problem is more holistic and requires improvements at a foundational model level to resolve. We could resolve this at a critic level through more careful prompting, however, this returns to our original and intractable task of textually describing a complex task. A more holistic solution could be to adapt PPO with functional grounding (Carta et al., 2023) to be a pure text task. However, this may lower the quality of questions as it could discourage the use of implicit or complementary knowledge.\nUnidirectional Relationships. A strategy to increase the difficulty of questions is to invert relationships found in the text. The models sometimes misappropriate this tool, resulting in invalid questions such as the question What did the Ming dynasty represent? from a passage containing ...explorer Zheng He representing the Ming Dynasty.... Knowledge graph assisted generation could help to resolve these logical inconsistencies (Lin et al., 2015a). However, expecting our target demographics, emerging domains, to possess high-quality knowledge graphs is an unreasonable assumption."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a low-cost methodology for generating challenging MRC datasets to meet the growing need for evaluation datasets in the cultural heritage sector. By using high-performing question-answering models to identify the most difficult questions, we were able to create synthetic pairwise data for training a reward model. Rather than manually defining question difficulty, our approach allows the model to learn and extract these features autonomously, leading to a significant improvement in the difficulty of questions generated for evaluation.\nWith this said, we trained on a general domain dataset in order to single out the training behaviour, in doing so losing many of the characteristic features of heritage datasets. In future work we will examine how the training paradigm fares under the unique challenges presented by such a varied industry.\nAlthough this work was produced to meet the evaluation demands of our ongoing work in RAG at our institution, we also highlight that the approach can work in any domain and that with some modification, it could be used to augment other dataset formats. We believe this approach can be extended further, allowing for the manipulation of multiple abstract properties simultaneously through multi-reward model setups (Wu et al., 2023)."}, {"title": "Limitations", "content": "This project only shows the suitability of the method on a single model. In future work, we seek to address this by performing a more comprehensive review of the approach across a range of model sizes and architectures. We also acknowledge that this method currently only addresses answerable questions while most contemporary QA datasets utilise both answerable and unanswerable questions. Finally, despite using LoRA and multi-adapter training, we still required approximately 15 GPU hours on an A100 80GB which restricts the potential audience for this approach. Evaluating smaller models or quantisation will enable greater access to this project's benefits."}, {"title": "Ethics Statement", "content": "This project has been approved by the relevant institution's ethics committee. We use LLaMa2 in accordance with Meta's license. All annotators were located through word of mouth and paid \u00a312 per hour - above the UK National Living Wage of \u00a311.44."}]}