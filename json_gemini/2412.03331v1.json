{"title": "LUXEMBEDDER: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings", "authors": ["Fred Philippy", "Siwen Guo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "abstract": "Sentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems. However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish. This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages. To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to train LUXEMBEDDER, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities. Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs. Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research.", "sections": [{"title": "1 Introduction", "content": "The development of sentence embedding models has been instrumental in applications such as Bitext Mining (Artetxe and Schwenk, 2019), Information Retrieval (Thakur et al., 2021), and most recently Retrieval Augmented Generation (Lewis et al., 2020). Generative Large Language Models are not capable of handling these tasks as effectively, making sentence embedding models crucial in these areas. However, these models depend on large-scale parallel data to function effectively, a resource readily available for high-resource languages but sorely lacking for low-resource languages (Zhou et al., 2018).\nOne way to address this issue is to apply cross-lingual sentence embedding models (Chidambaram et al., 2019; Artetxe and Schwenk, 2019; Reimers and Gurevych, 2020; Yang et al., 2020; Feng et al., 2022; Wang et al., 2022), which aim to embed various languages into a common shared representation space. This approach is intended to boost the performance of low-resource languages by leveraging cross-lingual transfer, where knowledge gained from high-resource languages contributes to the understanding and processing of low-resource languages. However, due to the significant differences in data availability, these models still exhibit a large performance gap between high-resource and low-resource languages.\nLuxembourgish, a West-Germanic language spoken by about 400 000 people, is one of the many languages that face this challenge. While translation models for Luxembourgish exist (NLLB Team et al., 2022; Song et al., 2023), their performance remains significantly inferior to that of high-resource languages, hindering the creation of parallel data using methods like back-translation. This limitation also applies to general-purpose generative LLMs, making the direct creation of synthetic parallel data impractical as well. Our research aims to address this issue by collecting a comprehensive set of high-quality human-generated cross-lingual parallel data specifically for Luxembourgish. With this data, we train a sentence embedding model, LUXEMBEDDER, tailored specifically for Luxembourgish by leveraging cross-lingual transfer.\nAlthough cross-lingual sentence embedding models harness the strength of cross-lingual transfer to improve low-resource language performance, we argue that this does not eliminate the necessity for parallel data in these languages. Our findings demonstrate that incorporating these languages in parallel training datasets is essential, as it significantly improves alignment within cross-lingual models, particularly among other low-resource"}, {"title": "2 Dataset & Benchmark Construction", "content": "We create cross-lingual parallel data and a Luxembourgish paraphrase detection benchmark. See Appendix A for details and Figure 1 for an overview."}, {"title": "2.1 Cross-Lingual Parallel Data (LUXALIGN)", "content": "We collect news articles from RTL.lu, a Luxembourgish news platform that publishes in Luxembourgish (LB), English (EN), and French (FR). Due to the lack of explicit mapping between language versions, we use the OpenAI text embedding model text-embedding-3-small to align articles across language pairs. LaBSE (Feng et al., 2022) is then employed to extract parallel sentences from these aligned pairs for LB-FR and LB-EN."}, {"title": "2.2 Luxembourgish Paraphrase Detection (PARALUX) Benchmark", "content": "Then, we repeat the same process but focusing exclusively on Luxembourgish articles. Within each article, using the same setup, we extract parallel sentences, which can be considered near-paraphrases, from which we hand-pick high-quality samples for our benchmark. From these paraphrased pairs, we prompt GPT-4 to generate adversarial negative samples for each pair. Given its limited language capabilities in Luxembourgish, the generated adversarial negative samples are then checked and, if needed, corrected by a human annotator to ensure high quality and accuracy."}, {"title": "3 LUXEMBEDDER", "content": "Given its cross-lingual capabilities and its already existing support of Luxembourgish, we use LaBSE (Feng et al., 2022) as our base model, which we further train on both LB-EN & LB-FR parallel subsets from LUXALIGN.\nWe train the model using a batch size of 16 for 3 epochs with a constant learning rate of 1 \u00d7 10-6 using a contrastive loss function. We reserve 1% of the data for evaluation, on which we evaluated every 500 steps, and retained the model with the best loss on the development set. The negative pairs for the loss function are created by randomly pairing each Luxembourgish sentence with the translation of another sentence from the dataset."}, {"title": "3.2 Evaluation", "content": "We comprehensively compare LUXEMBEDDER'S performance across multiple tasks against a variety of open-source and proprietary baseline models."}, {"title": "3.3 Baselines", "content": "We provide more details on the used models in Appendix B.2.1."}, {"title": "3.4 Evaluation Tasks", "content": "Additional details on the specific evaluation setup can be found in Appendix B.2.2."}, {"title": "Zero-Shot Classification", "content": "Using SIB-200 (Adelani et al., 2024), a 7-class classification dataset, we perform similarity-based zero-shot classification. First, we fill each label into a pre-defined template sentence, and separately encode both the input document and all potential template-embedded labels. Then, the class with the most similar embedding to the input document is chosen, assessing the model's ability to generalize to new, unseen tasks without any task-specific training. To account for variability, we repeat this process for 5 different label templates and report the average performance."}, {"title": "Cross-Lingual Transfer", "content": "For cross-lingual transfer performance, we use the embeddings generated by the respective model to fine-tune a classifier on the SIB-200 dataset in six different high-resource source languages and evaluate directly on the Luxembourgish test set."}, {"title": "Bitext Mining", "content": "We evaluate the model's proficiency in accurately retrieving or matching parallel sentence pairs from a bilingual corpus using the Tatoeba dataset. Since the original Tatoeba test set (Artetxe and Schwenk, 2019) does not include Luxembourgish, we use the LB-EN, LB-NL, and LB-DE test sets developed by the Tatoeba Translation Challenge (Tiedemann, 2020)."}, {"title": "PARALUX", "content": "Lastly, we evaluate the model on our newly created benchmark for paraphrase detection. This task involves determining which of two sentences is a paraphrase of a given anchor sentence. It tests the model's ability to discern nuanced semantic equivalence, which is critical for applications like plagiarism detection, question answering, and information retrieval."}, {"title": "3.5 Results", "content": "LUXEMBEDDER demonstrates superior performance among open-source models in all four tasks and even outperforms all tested proprietary models in 3 out of 4 tasks. Only text-embedding-3-large model shows superior cross-lingual transfer performance.\nIn particular, we observe considerable improvements in LUXEMBEDDER 's performance on both monolingual tasks, Zero-Shot Classification and Paraphrase Detection, relative to its base model, LaBSE. This confirms the efficacy of our cross-lingual approach for Luxembourgish."}, {"title": "4 Cross-Lingual Alignment", "content": "In this section, we investigate the impact of fine-tuning models on parallel data for cross-lingual alignment between and within high-resource (HR) and low-resource (LR) languages."}, {"title": "Results", "content": "Our observations reveal that when fine-tuning on parallel data, the alignment within the model generally increases. HR languages benefit equally from fine-tuning on any of the three language pairs. However, we observe that the alignment of LR languages benefits more when Luxembourgish is part of the training data compared to fine-tuning on HR language pairs alone.\nThese results indicate the critical importance of including LR languages, such as Luxembourgish, when collecting parallel data. Incorporating LR in"}, {"title": "5 Conclusion", "content": "Sentence embedding models struggle with low-resource languages due to a shortage of parallel data. To address this problem, we collected high-quality, human-generated cross-lingual parallel data for Luxembourgish and developed an enhanced version of a cross-lingual sentence embedding model specifically adapted to Luxembourgish. This model outperforms open-source as well as proprietary models in almost all evaluations conducted in our study. Our findings also stress the importance of incorporating low-resource languages in parallel data collection, as evidence suggests that this enhances embedding alignment for both the target language and other low-resource languages within the same model more effectively than using high-resource language pairs alone. Therefore, we believe this research encourages further creation of parallel corpora for low-resource languages."}, {"title": "Limitations", "content": "It is important to note that we do not compare our embedding model against general-purpose generative LLMs. We acknowledge that some of these models, which are significantly larger in terms of parameter count, may outperform LUXEMBEDDER in certain tasks. Nonetheless, the primary objective of our paper is not to compete with generative models. Instead, our focus is on providing a robust sentence embedding model capable of solving specific tasks such as information retrieval, document clustering, and similar applications where generative language models may not be as effective.\nAdditionally, we acknowledge that our data is limited to the news domain, due to its availability. However, our goal is to use this data to boost the model's retrieval performance, facilitating future expansion into various other domains by mining a more diverse range of parallel data."}, {"title": "Ethical Statement", "content": "In the newly created PARALUX benchmark, the adversarial counterparts of the paraphrases have been edited in a way that some of the edited sentences may contain non-factual information. Therefore, we strongly recommend using this data solely, as designed, for evaluation purposes and not for training, to ensure the integrity of model development.\nFurthermore, our datasets, based on news articles, naturally include the names of individuals. As the text is publicly available and anonymization would greatly diminish data quality, we chose not to anonymize it. We believe that preserving the original context of publicly accessible information is essential for maintaining data integrity and the effectiveness of our research."}, {"title": "B.1 Training", "content": "Given a sentence embedding model $M_\\theta$ with parameters $\\theta$, for a sentence pair $(x_1, x_2)$ and its label $y$ (1 if positive pair, 0 if negative pair), the contrastive loss function is defined as:\n$L (\\theta, (x_1, x_2,y)) = [y \\cdot D^2 + (1 - y) \\max(0, m \u2013 D)^2]$ (1)"}, {"title": "D Details on the Cross-Lingual Alignment Experiments", "content": "In Section 4, we measure the alignment of language-specific subspaces using the Centered Kernel Alignment (CKA) method (Kornblith et al., 2019). The CKA score of two representation matrices $X \\in \\mathbb{R}^{N\\times m}$ and $Y \\in \\mathbb{R}^{N\\times m}$, where $N$ is the number of samples and $m$ is the embedding dimension of the model, when using a linear kernel, is given by\n$CKA(X,Y) = 1- \\frac{||XY^T ||_F}{||XX^T ||_F||YY^T||_F}$"}]}