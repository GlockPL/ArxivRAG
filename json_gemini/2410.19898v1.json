{"title": "A Review of Deep Learning Approaches for Non-Invasive Cognitive Impairment Detection", "authors": ["Muath Alsuhaibani", "Ali Pourramezan Fard", "Jian Sun", "Farida Far Poor", "Peter S. Pressman", "Mohammad H. Mahoor"], "abstract": "This review paper explores recent advances in deep learning approaches for non-invasive cognitive impairment detection. We examine various non-invasive indicators of cognitive decline, including speech and language, facial, and motoric mobility. The paper provides an overview of relevant datasets, feature-extracting techniques, and deep-learning architectures applied to this domain. We have analyzed the performance of different methods across modalities and observed that speech and language-based methods generally achieved the highest detection performance. Studies combining acoustic and linguistic features tended to outperform those using a single modality. Facial analysis methods showed promise for visual modalities but were less extensively studied. Most papers focused on binary classification (impaired vs. non-impaired), with fewer addressing multi-class or regression tasks. Transfer learning and pre-trained language models emerged as popular and effective techniques, especially for linguistic analysis. Despite significant progress, several challenges remain, including data standardization and accessibility, model explainability, longitudinal analysis limitations, and clinical adaptation. Lastly, we propose future research directions, such as investigating language-agnostic speech analysis methods, developing multi-modal diagnostic systems, and addressing ethical considerations in AI-assisted healthcare. By synthesizing current trends and identifying key obstacles, this review aims to guide further development of deep learning-based cognitive impairment detection systems to improve early diagnosis and ultimately patient outcomes.", "sections": [{"title": "I. INTRODUCTION", "content": "Cognitive impairment poses significant challenges for individuals, families, and healthcare systems worldwide [1]. As the population of older adults in the United States grows, the prevalence of cognitive impairment related to Alzheimer's disease (AD), AD-related dementia (ADRD), and mild cognitive impairment (MCI), which often progresses to AD/ADRD, is expected to rise, necessitating cost-effective screening tools and early treatment strategies. Current diagnostic methods include clinical evaluations, neuropsychological assessments, and advanced neuroimaging techniques such as Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), and Computed Tomography (CT) scans [2]. While effective, these diagnostic methods are expensive and require specialized personnel. Alternatively, scientists have explored cost-effective methods, particularly those leveraging innovative technologies powered by artificial intelligence (AI) and machine learning (ML).\nIn recent years, deep learning models have made remarkable progress in diverse domains, including Computer Vision (CV) and Natural Language Processing (NLP), with various implications for healthcare [3]. Unlike traditional machine learning approaches that separate feature extraction and model learning, deep learning builds end-to-end systems in which neural networks concurrently learn and extract discriminant features directly from the data during the training process. Deep learning models are enhancing various domains, including the detection of pathological indicators among clinically diagnosed patients [4], and may enhance detection in early disease stages [5], [6]. Furthermore, machine learning methods can assist in utilizing non-invasive data. Deep learning techniques hold particular promise for reducing healthcare costs in disease diagnosis and treatment. The ability of deep learning-based methods to handle complex data and capture subtle changes is crucial for the early detection of indicators for cognitive impairment.\nThis review paper explores recent research articles that primarily utilized or developed deep learning methods for detecting cognitive impairment using various non-invasive data modalities (e.g., speech and language, facial video, eye gaze, and motoric mobility). Despite the availability of review papers on deep learning methods for detecting cognitive impairment [7], [8], to the best of our knowledge, none specifically discuss the use of various non-invasive data modalities. Hence, we focused on studies that employed deep learning-based methods for non-invasive cognitive impairment detection. The main criteria for including research articles in this review are the data modalities used and the adaptation of deep learning methods in feature extraction or detection decisions.\nWe begin by discussing the advantages and limitations of different integrated non-invasive modalities. We then review studies that utilized various modalities to propose cognitive impairment detection systems. Ultimately, we provide a comprehensive outline of existing trends and future directions in deep learning methods for cognitive impairment detection, concluding with the overall impact of these methods on early detection of cognitive decline. This knowledge can lead to better insights into disease progression across large populations.\nThe organization of this review paper is as follows. Sec. II discusses non-invasive data as indicators of cognitive decline from a medical perspective and elaborates on the rationale behind using deep learning for analyzing such data. Sec. III"}, {"title": "II. COGNITIVE IMPAIRMENT INDICATORS", "content": "In this section, we investigate the medically supported motivations behind using diverse non-invasive modalities and their features for detecting cognitive status, leveraging deep learning algorithms. Specifically, we focus on Speech & Language, Facial, and Motoric Mobility indicators.\nProgressive neurodegenerative cognitive conditions encompass a spectrum of cognitive decline stages, beginning with prodromal disease, and progressing through subjective cognitive impairment, Mild Cognitive Impairment (MCI) and progressing to dementia [1]. The cognitive stage of patients can be assessed by examining certain predetermined factors while they engage in specific tasks (i.e., answer questions, and perform executive functions). This evaluation process involves observing how an individual performs and responds during these tasks, which helps to identify their cognitive stage. By carefully analyzing their behavior and reactions, it is possible to determine their cognitive development stage, based on those established evaluation criteria. Determining an individual's cognitive status offers valuable insight into the current challenges and guides the implementation of appropriate interventions.\nAt present, there exists no cure for neurodegenerative cognitive impairment. However, several therapies and lifestyle interventions, depending on the cause and the stages of the impairment, may slow the cognitive decline and improve patient and caregiver quality of life [1]. For example, the Food and Drug Administration (FDA) has approved two IV medications that selectively act on the beta-amyloid protein associated with Alzheimer's disease that significantly reduces clinical decline on multiple scales, but would have no effect on cognitive impairment associated with other proteins [9]. Typically, an evaluation of cognitive impairment begins when the patient or others who know the patient well describe changes in everyday life associated with these changes. In addition to gathering personal history, the clinician will typically perform or order cognitive testing to be done that better describes the severity and nature of the cognitive change. Depending on the findings of these initial tests, medical imaging may be ordered, such as Computed tomography (CT), magnetic resonance imaging (MRI), or positron emission tomography (PET). Blood tests and sometimes tests of cerebrospinal fluid (CSF) may also be ordered.\nThrough this process, medical providers further specify which aspect of cognition (i.e., cognitive domain) is primarily impaired. Examples may include disorders of language (aphasia), memory (amnesia), planning and attention (executive dysfunction), knowledge (agnosia), and more. These assessments implicate specific brain regions affected, such as the medial temporal lobes for memory processing and the parietal lobes for sensory information and spatial awareness [10].\nWhile these methods can provide valuable insights into the patient's cognitive status, they require time and expertise to administer. An aging population creates a greater demand for evaluations than can be readily met, leading to delays in assessments and, consequently, delays in appropriate treatments and recommendations.\nDeep learning applications have expanded to detect subtle behavioral and cognitive ability changes in cognitively impaired patients using non-invasively collected data modalities, such as speech, facial expressions, and motoric mobility. In the following, we discuss three primary non-invasive indicators of cognitive impairments, paralleling the medical explanations, focusing on speech & language, facial, and motoric mobility modalities.", "subsections": [{"title": "1) Speech Indicators", "content": "Speech is the most extensively studied modality in deep learning for detecting cognitive status, primarily due to the early availability of public datasets such as the Pitt Corpus [11]. These datasets have enabled researchers to develop various methods for identifying cognitive impairment at different stages. Additionally, studies have shown that both acoustic and linguistic features are crucial in identifying pathology characteristics. Acoustic features, signal representations related to sound perception, are theoretically language-agnostic, although variations in sounds and pronunciations across different languages and accents can affect them. On the other hand, linguistic features are derived from the meaning and understanding of the speech. These features encompass aspects such as syntax, semantics, and pragmatics, which are intrinsic to a specific language. While acoustic features capture how something is said, linguistic features focus on what is being said, providing deeper insights into the speaker's intent, context, and the conveyed message.\nFrom a technical perspective, the analysis of human speech can be categorized into acoustic features (i.e., formats, pitch, and phonemes) and linguistic features (i.e., morphemes, words, phrases/sentences, and contextual meaning). Speech generation, which involves physical mechanisms and linguistic output, provides rich indicators for detecting cognitive impairment. However, these abnormalities are not directly observable by human perception and require technical integrations for detection. We will review acoustic and linguistic characteristics and features in the following."}, {"title": "Acoustic Perspective", "content": "Human speech includes formats, a local maxima in the speech spectrum, which define the acoustic resonance of the vocal tract. Voiced sounds emerge as air traverses the vocal tract, with the brain controlling various muscles involved in sound generation.\nThe features embedded within voices possess the potential to disclose the speaker's medical condition. To illustrate, studies have linked specific acoustic features to cognitive declines in speech signals, including vocal cord deficiencies [12]. However, it is not a straightforward indication of the individual's medical condition, especially for older adults. To clarify, seniors usually exhibit changes in acoustic characteristics after age 60, due to a combination of cognitive and more peripheral changes [13]-[15]. Likewise, the baseline pitch often varies based on the speaker's gender.\nAs a result, pre-trained deep machine learning models such as wav2vec2.0 [16] and VGGish [17] for extracting acoustic features might introduce bias stemming from the general population. Therefore, it is crucial to analyze these features in conjunction with other conditions, taking into account appropriate adjustments for age-related changes. This is achievable by leveraging recent deep learning methods that are proven to be effective at processing complex and large datasets including speech. These methods enable the analysis of acoustic features to detect anomalies that are imperceptible to the human ear.\nMany research studies have utilized predefined acoustic features versus those that investigated the use of raw speech data with deep learning methods to detect cognitive status. Predefined acoustic features are interpretable by researchers as related to cognitive decline. However, subtle nuances of speech may be overlooked. Whereas, utilizing raw speech with deep learning models can increase the modeling complexity and potentially reveal novel patterns in the speech."}, {"title": "Linguistic Perspective", "content": "Words are structured into sentences that express thoughts within speech. From a medical perspective, the analysis of linguistic patterns can detect deviations in language usage that suggest cognitive decline [18]. Different aspects of speech relate to different aspects of cognition, supported by different anatomical substrates and compromised by different disease processes. For example, complex grammatical construction is subserved by the left frontal operculum. When this area is compromised it can result in grammatically simplified or incorrect phrasing, as is related to the nonfluent variant of primary progressive aphasia (nfvPPA), a neurodegenerative condition most commonly related to an abnormal convolution of a protein called tau [19]. By identifying such grammatical errors in speech, then, an algorithm may implicate a brain region, a neurological syndrome, and predict histopathological findings under a microscope.\nWhile conditions such as nfvPPA are rare and specific to language, subtle changes such as increased pause length and changes in linguistic coherence may signify the presence of more common conditions such as Alzheimer's disease. Several groups worldwide have demonstrated some ability to predict MCI or Alzheimer's disease [20]; however, more work remains to be done to ensure the results are reproducible across different cultures, languages, and populations.\nDue to the power of Large Language Models (LLMs)"}]}, {"title": "2) Facial Indicators", "content": "Interactions between clinical personnel and patients are crucial for detecting cognitive impairment. As part of this delicate act of clinical communication, careful interpretation of paralinguistic as well as linguistic signals is required. Facial expressions, whether intentional or not, are emphasized in tasks involving affective reception and expression, as the neurodegenerative process may disrupt the physiological linkage between subjectively experienced and expressed emotion [21]. Those with neurodegenerative conditions, particularly forms of behavioral variant frontotemporal dementia (bvFTD) also struggle to interpret the facial expressions of others accurately [22], and similar difficulties are also reported in conditions as common as Alzheimer's disease, albeit to a lesser extent [23].\nIn studying these issues, it is important to distinguish between facial expressions originating from natural (spontaneous) reactions and those elicited by stimuli when assessing cognitively impaired subjects. As cognitive impairments can severely affect comprehension of facial expressions, utilizing computer vision techniques to extract facial features can be a fundamental step for detecting cognitive impairment using facial indicators. Recent studies have investigated the effectiveness of facial indicators through the development of several models that extract explainable facial features such as head poses, facial expressions, and Facial Action Coding System (FACS), occurring either intentionally or unintentionally [24]."}, {"title": "3) Motoric Mobility Indicators", "content": "Many forms of neurodegenerative conditions that impact cognition also impair motor control [25]. Physical challenges with normal aging observed and evaluated in clinical settings are often linked to cognitive status [26]. In addition to changes of healthy aging as well as motor controls more common in specific conditions such as Parkinson's disease, many neurodegenerative conditions may ultimately involve loss of motor control in addition to cognitive changes. Furthermore, many cognitive processes such as processing speed or visuospatial awareness may become apparent in movement measures.\nStudies have shown variations in pressure, stroke, and speed in handwriting, as well as alterations in drawing patterns, might reflect the early stages of cognitive impairments impacting frontal, occipital, or parietal lobes [27]. Some drawing tasks are designed to evaluate the precision of vertical and horizontal lines within these shapes. These abnormalities are considered during neuropsychological assessments, where subjects are asked to replicate various shapes [28].\nRecent advancements in wearable and non-wearable measuring devices, allow for the continuous, non-intrusive monitoring of physical movements. Such data collection supports ongoing monitoring and early detection of cognitive changes. Ultimately, deep machine learning algorithms can analyze this captured data to identify abnormal movement patterns indicative of neurological, musculoskeletal, or other issues. Recently, using deep learning methods to detect cognitive conditions has become increasingly popular. These methods analyze different activities such as handwriting, drawing, and walking to identify early symptoms of cognitive impairment [29]."}, {"title": "4) Multi-modal Indicators", "content": "Studies have investigated how combining various modalities may enhance the performance of detecting and longitudinal assessment of cognitive impairment [30]. Integrating different modalities has the potential to improve detection performance by capturing more indicators, and introducing different combinations of indicators that may be more powerful than an individual indicator in isolation. However, these integrations and interactions also increase the complexity of the technical models and data management. To the best of our knowledge, there is currently no established framework for fusing different modalities, particularly when considering the temporal aspects of features.\nWith that in mind, AI techniques can enhance the detection of cognitive impairments in clinical settings, resulting in more effective and accurate diagnoses. This improvement in precision and sensitivity can significantly enhance overall performance."}, {"title": "III. DATASETS", "content": "In this section, we review some existing datasets used in studies for detecting MCI and AD/ADRD using non-invasive data collection methods. We categorize the datasets based on the type of captured data from the individuals as follows: 1- Speech-based datasets, containing the speech recordings and/or transcripts of human subjects. 2- Visual-based datasets, capturing the human subjects' visual appearance such as facial. 3- Movement-measuring-based datasets, capturing individuals' behaviors using wearable and non-wearable measuring sensors while performing a specific task. 4- Multi-modal datasets, presenting more than one data modality of the individuals, including speech, movement, or visual appearance.\nWe explore the datasets and provide a brief introduction for each category based on their publication year to ensure a consistent representation. While the primary focus of this section is on publicly available datasets, we also briefly discuss private datasets at the end of each category. The private datasets are collected for a specific study and usually are not available to the research community."}, {"title": "A. Speech-based Datasets", "content": "In this section, we review the existing speech-based datasets in chronological order of their release. Some studies have considered speech modality out of multimodal datasets."}, {"title": "B. Visual Datasets", "content": "Visual datasets are collections of images or videos that capture non-invasive indicators of cognitive impairment, such as variations in participants' facial or body appearance. Unlike other speech-based datasets, almost all visual datasets are self-collected and created for specific studies. It is important to note that some datasets used for visual data also contain other modalities. Therefore, we will introduce them as multi-modal datasets in Sec. III-D."}, {"title": "C. Movement-measuring-based Datasets", "content": "In this section, we review the existing Movement-measuring-based datasets in chronological order. Unlike speech-based datasets, most movement-measuring-based datasets are private to certain research groups."}, {"title": "D. Multi-Modal Datasets", "content": "Multi-modal datasets present more than one type of participant data, usually including speech, sensor data, or visual appearance. In the following, we review some of the public multi-modal datasets available for non-invasive recognition of AD."}, {"title": "IV. MODELING AND DATA MODALITIES", "content": "In this section, we review common machine-learning models utilized for detecting cognitive impairment and related conditions. We categorize these models into traditional machine learning models and deep machine learning models. While numerous studies have applied traditional machine learning models to cognitive impairment recognition tasks, we focused on studies that either directly used deep learning models or compared their performance with traditional machine learning approaches for building detection systems.\nIn traditional machine learning methods, feature extraction is conducted separately from classification or recognition tasks. This means that, based on the data modality, specific features are extracted first, independently of the learning task. Once features are extracted, classifiers such as K-nearest neighbors (kNN), Support Vector Machines (SVM), Random Forests (RF), or Multilayer Perceptrons (MLP), among others, are trained. These traditional machine learning models are widely adopted in the field due to their straightforward implementation via open-source software libraries.\nIn deep machine learning approaches, an end-to-end system is often built in which a neural network model learns and extracts discriminative features suitable for a particular task. These features are learned directly from the data during the training process.\nOne of the commonly used deep learning architectures is Convolutional Neural Networks (CNNs), which are particularly effective at extracting and capturing hierarchical features from images. CNNs extract low-level features through initial layers and progressively capture high-level features through deeper layers. CNN architectures such as ResNet [63], VGG [64], and MobileNetV2 [65] are commonly used models for various tasks.\nRecurrent Neural Networks (RNNs) are another type of neural network that consider the sequential nature of data. They integrate input data across time steps, making them suitable for time series or sequential data. The two main architectures of RNNs are Gated Recurrent Units (GRU) [66] and Long Short-Term Memory (LSTM) [67] networks. These architectures can process sequential data in both forward and backward directions, providing a more comprehensive understanding of the sequences.\nTransformers, developed in 2017 [68], revolutionized many fields, especially NLP, by adopting a sequence-to-sequence approach with an attention mechanism. Initially introduced for machine translation [68], Transformers have since become useful in various applications including vision tasks. Notable Transformer models include Generative Pre-trained Transformer (GPT) [69], Bidirectional Encoder Representations from Transformers (BERT) [70] for NLP, and Vision Transformers (ViT) [71] for computer vision. The Transformer encoder is mainly used for feature extraction and classification whereas the Transformer decoder is utilized for data generation."}, {"title": "A. Speech-based Modality", "content": "Speech has emerged as a prominent non-invasive modality for detecting cognitive impairment. The appeal of speech-based methods lies in the data availability and cost-effectiveness."}, {"title": "B. Visual Modality", "content": "In this section, we review papers that utilize visual modals in detecting cognitively impaired participants by applying Deep Learning methods. These methods utilized different sources of datasets. Thus, this section will discuss the studies' data preprocessing and modeling techniques. The visual modality is mainly considering a facial or body video of participants."}, {"title": "C. Other Data Modalities", "content": "In this section, we review research that utilized data modalities other than speech, and videos. The data modalities include those acquired by wearable and non-wearable measuring devices to detect cognitive impairment or AD/ADRD. These data mainly capture the motoric mobility of subjects. We discuss the essential aspects such as the activities under consideration, the feature extraction process, and the methodologies implemented in each case."}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we discuss the performance of the methods reviewed in Section IV through common evaluation metrics across all the studies. We will further explore these methods and analyze the justifications for their results.\nResearchers use various methods to evaluate their models, typically through test subsets within the dataset or by applying statistical evaluation methods. Cross-validation, a common statistical evaluation method, is used to evaluate models for detecting cognitive conditions, with 5-fold, 10-fold, and leave-one-out being the most frequently employed methods. Some studies, such as Bertini et al. [80], empirically selected 20-fold cross-validation. The number of folds considers the data bias, variance, and computational complexity. Ideally, it depends on the dataset size; thus, the number of folds should be increased when the dataset size is small. On the other hand, fewer folds are sufficient for large datasets. Typically, 5 to 10 folds are standard in machine learning.\nThe evaluation metrics in studies using DL models include accuracy, area under the receiver operating characteristic (ROC) curve (AUC), F-score, precision, recall, sensitivity, and specificity, while classification error rate and ROC are rarely used. The root mean squared error (RMSE) is the only metric used for regression tasks. Most classification tasks focus on binary detection, with accuracy being widely reported despite its limitations on imbalanced datasets. Therefore, accuracy is often supplemented by other metrics that account for false positives (FP) and false negatives (FN), with the F1 score being the second most commonly used metric among the reviewed studies.\nThe evaluation of methods and approaches for cognitive impairment detection can be better understood by considering the various modalities and distinct performance metrics used across different studies. In the following sections, we discuss detection performance categorized by their modalities by briefly mentioning the approach and the evaluation results."}, {"title": "A. Acoustic Modality", "content": "Syed et al. [76] employed Bi-LSTM with IS10-paralinguistics feature sets on the ADRESS dataset, achieving a classification accuracy of 74.55%. This performance highlights the potential of traditional feature sets in conjunction with deep learning models, specifically the importance of paralinguistic features in detecting cognitive impairments. Chlasta and Wolk [77] utilized the VGGish model to extract features with a DemCNN model on the same dataset, resulting in a lower accuracy of 63.6%. However, their precision, recall, and specificity metrics all scored 69.2%, indicating a balanced performance across these metrics. This suggests that while the overall accuracy was lower, the model maintained consistent detection capabilities for both classes. Meghanani et al. [74] achieved a classification accuracy of 64.58% with a CNN-LSTM model using MFCCs, and their derivatives. Including spectrogram-based features and temporal dynamics through LSTM layers improves the performance compared to the Dem-CNN detecting model using features extracted from VGGish.\nGauder et al. [47] reported an accuracy of 78.87% on the ADRESSo dataset using a multi-feature approach that included eGeMAPS, Trill, Allosaurus, and Wav2vec2.0 features processed through a series of 1D CNNs. This indicates that a multi-faceted approach to extract comprehensive acoustic features can substantially enhance the model's performance in detecting cognitive impairments.\nSeveral studies using the Pitt dataset have shown varying levels of success. Liu et al. [73] used spectral features of phonemes with a CNN-Bi-LSTM-attention pool-FC layer and achieved an accuracy of 82.59%, with a recall of 85.24% and a precision of 82.94%, suggesting their model was effective in identifying subtle variations in phonetic features related to cognitive impairments. Furthermore, Kumar et al. [72] reported an accuracy of 87.6% by employing a RF classifier with an extensive set of 44 acoustic features while the PRCNN achieved an accuracy of 85% and an F1-score of 85.1% using 62 features within 25ms segments. Similarly, Warnita et al. [75] reported an accuracy of 73.6% using GCNN utilizing sets of Interspeech features. Additionally, Bertini et al. [80] achieved a classification accuracy of 90.7% and an F1-score of 88.5% using an Autoencoder with GRU encoder and MLP to classify log-Mel spectrogram images, emphasizing the value of advanced feature extraction and autoencoder methodologies in enhancing classification performance. Similarly, Pranav et al. [79] employed a ViT on log-Mel spectrograms, attaining an accuracy of 85.7% and an F1-score of 92.3%.\nUtilizing non-English datasets, Bertini et al. [81] have implemented an Autoencoder with GRU encoder and MLP to classify log-Mel spectrogram features on a private Italian dataset with a detection accuracy of 90.6% and an F1 of 90.7%. Moreover, Nishikawa et al. [35] reported a classification accuracy of 90.8% with a 1-d CNN-LSTM model on a private Japanese dataset. In addition, using another approach, Nishikawa et al. [78] achieved an accuracy of 89.4% using ViT_b16, demonstrating consistency and robustness across different languages and feature sets. On the other hand, Rodrigues Makiuchi et al. [50] have utilized the PROMPT dataset [49] using GCNN to achieve a detection accuracy of 80.8% among Japanese individuals.\nOverall, studies that extracted comprehensive acoustic features achieved better performance compared to studies that used a few types of acoustic features. Although studies that utilized acoustic feature sets achieved an adequate detecting performance, they are on the lower scale of detecting cognitive conditions. Log Mel-spectrogram contains an indication of the speaker's cognitive status. Thus, advanced computer vision models reached a high detection performance. Table. II shows the studies results of acoustic feature approaches."}, {"title": "B. Linguistic Modality", "content": "In utilizing the ADRESS dataset, Searle et al. [96] implemented an SVM classifier on features on DistilBERT word embeddings and TF-IDF and achieved an accuracy of 81%. This study combines statistical linguistic analysis with modern embeddings for cognitive impairment detection. Additionally, the integration of LASSO regression yielded an RMSE of 4.58, demonstrating substantial precision in regression tasks. Meghanani et al. [97] also utilized word embeddings in conjunction with a CNN, highlighting the potential of convolutional networks to capture spatial patterns within linguistic features, reaching an accuracy performance of 83.3%.\nMoreover, Yuan et al. [98] achieved an accuracy of 89.6% by fine-tuning LLMs like BERT and ERNIE, which encoded participants' pauses into transcripts. This suggests that leveraging large pre-trained models and fine-tuning them for domain-specific tasks significantly enhances detection capabilities. Liu et al. [127] also reported an accuracy of 88% by fine-tuning DistilBERT with logistic regression on the same dataset, reinforcing the efficacy of using compact and powerful Transformer models in cognitive impairment classification.\nSaltz et al. [113] utilized multiple word embeddings (BERT, XLNet, ELECTRA) and type-token-ratio, and obtained varied results across different datasets (ADRESS, Pitt, UW). Specifically, they achieved accuracies of 76%, 90%, and 74% on the Pitt dataset, augmented ADRESS, and UW, respectively, highlighting the robustness of their proposed methods across different datasets. Guo et al. [42] achieved an accuracy of 97.9% using fine-tuned BERT, aided by integrating WLS controls with the ADRESS dataset. The AUC of 99.2% further demonstrates this model's exceptional discriminative ability due to supplementary training samples.\nFor the Pitt dataset, several studies highlighted varying successes. Fritsch et al. [41] achieved an accuracy of 85.6% using an LSTM model, while Chen et al. [102] reported an accuracy of 97.42% by utilizing a combination of BiGRU and CNN models to extract global and local contextual features via GLoVe embeddings. This indicates the potential of sophisticated architectures to capture nuanced linguistic features. Roshanzamir et al. [110] implemented an augmentation layer with Bi-LSTM or logistic regression and achieved an accuracy of 88.08%, with precision and recall scores of 87.23% and 90.57%, respectively. This study underscores the importance of augmentation in improving model robustness and performance. Liu et al. [100] and Tsai et al. [101] also achieved accuracies of 93.5% and 84%, respectively, using transformer encoders, reaffirming the versatility and effectiveness of Transformer architectures on cognitive impairment detection.\nAdditionally, Khan et al. [39] applied a stacked DNN combining CNN, Bi-LSTM, and MLP, achieving 93.31%, 92.2%, and 85.7% for accuracy F1, and AUC, respectively, indicating that hybrid models leveraging both local and sequential features are highly effective. Wen et al. [104] further verified the efficacy of combining syntactic features with attention mechanisms and CNNs, achieving an accuracy of 92.2%. Wang et al. [106] utilized POS tags and sentence embeddings in a C-attention model, achieving an accuracy of 91.5% and an AUC of 97.7%, emphasizing the effectiveness of contextual and syntactic features in cognitive impairment detection. Similarly, Fard et al. [54] have generated sentence embeddings and utilized a Transformer encoder with proposed infoLoss for calculating the cost function to achieve an accuracy of 85.16% and an AUC of 84.75% on the I-CONECT dataset.\nIn studies involving multiple datasets, Alkenani et al. [105] reported an AUC of 98.1% and 99.47% on spoken and writing datasets, respectively, using lexicosyntactic and n-gram features in stacked fusion models, demonstrating the efficacy of ensemble learning across different linguistic contexts. Finally, studies on non-English datasets showed promising results with Casanova et al. [99] achieving 75% accuracy using RNNS for Portuguese data, and AI-Atroshi et al. [103] achieving accuracies of 90.28% and 86.76% on Hungarian data using MLP with Gaussian mixture model and deep belief network.\nOverall, we can conclude that large language models have the lead in detection performance. Nevertheless, other attempts using various combinations of features and classification models also demonstrated impressive detection performance. In addition, fine-tuning LLMs with control participants from other datasets helped the study stand out in the evaluation results. Table. III shows the overall approaches of linguistic features and their results."}, {"title": "C. Acoustic and Linguistic Modalities", "content": "In studies utilizing the ADRESS dataset, Campbell et al. [89] employed RNN for linguistic features and SVM for acoustic features, achieving an accuracy of 82.41% using an averaging score fusion approach. Cummins et al. [43] improved these results, obtaining an 85.2% accuracy with Bi-LSTM models, leveraging attention mechanisms tailored for different acoustic features. Edwards et al. [118] reached an accuracy of 92.6%, underscoring the effectiveness of linguistic models using embeddings like FastText.\nKoo et al. [44] experimented with CNN and Bi-LSTM models but evaluated them differently, achieving a comparative baseline increment, whereas Rohanian et al. [120] achieved an accuracy of 79.2% with Bi-LSTM models using gated layer aggregation, both for classification and regression tasks. Syed et al. [86] highlighted the strengths of traditional SVM models, achieving 85.42% accuracy, with regression RMSE of 4.3.\nLater studies further diversified approaches. Balagopalan et al. [119] successfully showed BERT's ability in classification with an accuracy of 83.32% while employing linear and ridge regression with an RMSE of 4.56. Meanwhile, Mahajan and Baths [122] resulted in an accuracy of 72.92% after applying dense layer fusion. Syed et al. [45] detecting cognitive impairment with accuracy reaching 91.67% using an SVM model, emphasizing the efficacy of linguistic features.\nThe research by Ilias and Askounis [121] used ViT and co-attention gates, achieving an RMSE of 3.61, while Li et al. [95] combined Whisper, BERT, and task-correlated features to attain an accuracy of 91.41% with a solid AUC of 91.38%. Moreover, Rohanian et al. [123] used Bi-LSTM with gating and disfluency features, achieving an accuracy of 84%, and Pappagari et al. [88] combined ResNet-34 for acoustic and fine-tuned BERT models for linguistics, highlighting superior linguistic performance and achieving RMSE scores of 3.85.\nStudies by Mittal et al. [117] and Zolnoori et al. [84] on the Pitt dataset demonstrated deep models with late fusion, yielding accuracy figures of 85.3% and 89.55%, respectively, the latter leveraging JMIM selection techniques.\nOverall, these studies collectively demonstrate that optimal performance in cognitive impairment detection is attained through deep learning architectures that effectively fuse acoustic-linguistic features, where advanced word embeddings like BERT frequently act as significant enhancers. The outcomes suggest room for continued exploration into model architectures and feature fusion methodologies to further refine and personalize cognitive health assessments. Table. IV presents the reviewed studies of their approaches and evaluation results."}, {"title": "D. Visual Modality", "content": "Several studies have concentrated on using facial features to detect cognitive impairment, leveraging the advancement of deep-learning techniques. Zheng et al. [51", "55": "applied transformer encoders on latent facial and interaction features from the I-CONECT dataset, reaching an accuracy of 87.5%. Using autoencoders likely facilitated the extraction of high-level abstract features, which, when combined with the transformer encoder models, led to enhanced predictive capabilities. The F1-score of 89% underscores the model's reliability in distinguishing cognitive impairment.\nContinuing with the I-CONECT dataset, Sun et al. [53", "24": "employed MobileNetV2 for facial expression recognition and emotion occurrence, paired with an SVM, yielding an accuracy of 73.3%. This indicates that while deep CNNs like MobileNetV2 can effectively extract facial expressions, additional advanced feature processing might be necessary to improve cognitive impairment classification outcomes.\nGait analysis has also been a key focus on visual modalities. Aoki et al. [57", "36": "integrated features from gait and EEG data using AST-GCN for gait and ST-CNN for EEG, yielding a"}]}