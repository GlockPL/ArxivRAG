{"title": "Maze Discovery using Multiple Robots via Federated Learning", "authors": ["Kalpana Ranasinghe", "H.P. Madushanka", "Rafaela Scaciota", "Sumudu Samarakoon", "Mehdi Bennis"], "abstract": "This work presents a use case of federated learning (FL) applied to discovering a maze with LiDAR sensors-equipped robots. Goal here is to train classification models to accurately identify the shapes of grid areas within two different square mazes made up with irregular shaped walls. Due to the use of different shapes for the walls, a classification model trained in one maze that captures its structure does not generalize for the other. This issue is resolved by adopting FL framework between the robots that explore only one maze so that the collective knowledge allows them to operate accurately in the unseen maze. This illustrates the effectiveness of FL in real-world applications in terms of enhancing classification accuracy and robustness in maze discovery tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic navigation in complex environments requires accu- rate perception and understanding of the surroundings. How- ever, when multiple robots operate in different environments independently, their ability to generalize for unseen environ- ments can be limited due to the lack of shared information based on constraints such as communication, storage, privacy, etc. Federated learning (FL) has emerged as a promising ap- proach to address this challenge by enabling learning agents to collaboratively train machine learning models without sharing raw data [1]. In this work, we demonstrate the effectiveness of FL in the context of maze discovery and navigation using two autonomous robots in two maze environments with distinct structural builds. As oppose to individual learning that fails robots to identify the features of the unseen maze, the FL- based approach allows robots to train generalized models that can be accurately used to identify the features in both seen and unseen mazes. In this view, our results highlight the importance of FL in real-world applications in terms of improving the collective knowledge and performance of multiple robots in diverse environments without exchanging the raw data."}, {"title": "II. FEATURES OF SQUARE GRID MAZES", "content": "A maze consists of smaller grids, each representing a specific shape within the overall structure. In a square-type maze, there are 15 predefined shapes, including open areas as shown in Fig. 1. By accurately classifying the block types observed from the middle of a grid using integrated sensory data, robots can navigate and explore the maze. To achieve this, a robot needs an accurate classification model. The nature of the maze, such as the irregularities in wall shapes, can be different from one maze to another. Therefore, robots need data from various types of environments to train a robust classification model."}, {"title": "III. SYSTEM ARCHITECTURE & HARDWARE", "content": "The demo setup consists of two 4 \u00d7 4 square grid mazes: Maze \u03b1 (M\u03b1) and Maze \u03b2 (M\u03b2), each constructed with unique irregular-shaped walls: type \u03b1 and type \u03b2, two robots namely R\u03b1 and R\u03b2 that use LiDAR sensors to perceive the environment, and two slave edge servers referred to as S\u03b1 and S\u03b2 as shown in the Fig. 2. Wall type \u03b1 consists of a base with two cylindrical shapes attached to it, while wall type \u03b2 is characterized by a base with three cylindrical shapes.\nOperating as robot operating system (ROS) masters, R\u03b1 and R\u03b2 are assigned the tasks of navigation, data collection, and maze discovery within their respective mazes M\u03b1 and M\u03b2. Therein, the robots are positioned at the starting point (0,0) in each maze. As for mobile robots, we use open-source robots based on NVIDIA Jetson Nano known as \"JetBot ROS artificial intelligence (AI) kit\" [2]. Each robot is equipped with a 360\u00b0 laser ranging LiDAR that is used to observe the surrounding from the middle of the grid, and a 8MP 160\u00b0 field of view camera. To achieve precise navigation between grid centers, robots use a line-following technique. Utilizing cameras mounted on the robots, they track white lines marked on the floor, ensuring accurate movement along the paths. Additionally, blue lines are marked in front of each grid center, enabling the robots to stop precisely at the center. The LiDAR model is RPLIDAR A1, which has a scanning frequency of 5.5 Hz, a ranging distance of 0.15 ~ 12m with an accuracy of 1% for distances less than 3m, and 1147 sample points per one sweep. To obtain accurate odometry information, the robot utilizes two high-power encoded motors, an IMU sensor, and an odometer. These sensors work together to provide reliable odometry data, which is then represented in the ROS framework.\nThe slave servers S\u03b1 and S\u03b2 are used to facilitate visualiza- tion and monitoring of the maze discovery process. A Lenovo Thinkpad with ROS is used as S\u03b1 and a Jetson Nano with ROS is used as S\u03b2. The slave servers and the robots use ROS topics for seamless and efficient communication. The robots publish their sensor readings and navigation commands, while the slave servers subscribe to these topics to receive the data. These data includes position and direction of the robot, identified block type, and flag messages for synchronization between ROS nodes, enabling the slave servers to visualize the robots' progress, block identifications, and navigation decisions over a graphical user interface."}, {"title": "IV. SYSTEM IMPLEMENTATION", "content": "This demo is composed of several components including the navigation, data collection, training of classification models, and inference and maze discovery as discussed next."}, {"title": "A. Navigation between adjacent grids", "content": "A vision-based line-following system is implemented on the robots to facilitate successful navigation between the grids using the camera serial interface camera module. The camera, along with the Jetson Nano, supports image processing at resolutions of 480 \u00d7 640 pixels, with a maximum frame rate of 30 frames per second. The navigation process consists of two repeated steps: i) the robot follows the white line, and ii) the robot stops at the center of each grid by detecting the blue line.\nWe choose a region-of-interest (RoI) from the bottom portion of the raw camera image to prioritise the detection of nearby points during line following. The RoI is then converted from RGB to HSV color space, due to the effectiveness of filtering out white lines based on the luminosity. Subsequently, the contour with the largest area is chosen to accurately identify the white line. The coordinates of the center point of this contour is extracted and used to determine the error, which represents the displacement of the path's center from the center of the RoI [3]. This error value is used as an input for a propotional-integral-derivative (PID) controller, ensuring precise control commands and smooth navigation [4]. The coefficients of the proportional, derivative, and integral terms in the PID controller are set to 3, 1.8, and 0, respectively.\nDetection of the blue line is used to stop the robot precisely at the center of each grid. The blue line detection process is similar to the white line detection, with exceptions of the use of a different RoI and the HSV adjustments to filter out the blue lines. The robot stops once the detected area of the blue line exceeded a predetermined threshold value, thereby achieving precise positioning at the center of the grid.\nFurther, we use the odometer frame information from the /odom rostopic to make precise 90\u00b0 and 180\u00b0 rotations while navigating by feeding the error obtained from the odometry information and target angle to a PID controller [4]."}, {"title": "B. Data Collection", "content": "During the data collection phase, we feed a predefined path and the map information as an array to the robot assigned for the maze. While following the path, at each grid, the robot performs four 90\u00b0 rotations and collects 200 LiDAR sweeps at each position by reading the LaserSacn message from /scan rostopic. These LiDAR readings are then saved into a numpy array with the corresponding label as shown in the Fig. 1. To enhance the diversity of the dataset within a short time frame, some noises to the movements are manually fed to the robot via a remote controller while it collects data from a single block type. These variations aid to generate a compre- hensive local dataset capturing a wide range of environmental configurations and scenarios, enhancing the overall robustness of the trained model."}, {"title": "C. Training the classification model", "content": "We use two training modes to train the classification model: local training and FL. During the local training mode, both robots train their classification models locally using the datasets obtained from respective mazes with supervised learning. The classification models are based on a feed-forward neural net- work architecture, consisting of a 1147 \u00d7 1 input reflecting LiDAR reading, one hidden layer with a size of 256 neurons, and a 15 \u00d7 1 output layer corresponding one-hot encoded label with ReLu activation. To prevent over-fitting and improve gen- eralization, a learning rate of 0.001 and the L2 regularization with a weight decay of 0.001 are applied in the training."}, {"title": "D. Maze discovery", "content": "The inference of the classification models obtained by local training and FL modes is used within the discovery phase as two different scenarios. Therein, each robot self navigates within the mazes by taking different actions with the objective of discovering the maze layout. The action space includes four actions correspond to moving forward after a clockwise rotation of 0\u00b0, 90\u00b0, -90\u00b0, or 180\u00b0. To ensure efficient maze exploration, a prioritized clockwise turn strategy is employed to avoid unnecessary looping. Once the maze is discovered, the robot automatically stops at the adjacent grid. The navigation techniques described in Section IV-A are utilized to navigate between grid centers.\nFig. 3 illustrates the maze discovery using the locally trained models. It can be noted that R\u03b1 and R\u03b2 can correctly infer their corresponding mazes M\u03b1 and M\u03b2, respectively validating the test accuracy obtained during the training phase. However, once the robots are swapped, they no longer able to correctly identify the unseen wall types, in which, maze discovery is inaccurate as shown in the bottom row of Fig. 3. With further experiments, it is observed that R\u03b1 exhibits about 48% of classification accuracy on the data from M\u03b2 while R\u03b2 achieves merely 29% accuracy on M\u03b1's data. This indicates that the local training does not generalize for unseen environments.\nThe inference using the FL models are illustrated in Fig. 4. In contrast to local training alone, with FL, the robots are capable of sharing their local knowledge with one another, in which, the models are generalized for unseen wall types. This is exhibited by the accurate maze discovery of both mazes by the both robots as shown in the inferred outputs (middle and bottom) of Fig. 4. Our experiments show that the FL-based model achieves about 99% classification accuracy over the data from both M\u03b1 and M\u03b2."}, {"title": "E. Demo, Resources, and Future Developments", "content": "The software related to robots and servers is available at https://github.com/ICONgroupCWC/MapDiscoveryDemo.\nThis demo in action can be seen from https://youtu.be/K2M8MCLn1po?si=TzIeUhHuSVCVeZRI. The future developments are focused on exploiting the topological signatures of LiDAR point-cloud data to train models over the symmetries, deformations, and irregularities in the raw observations with improved communication and computation efficiency."}]}