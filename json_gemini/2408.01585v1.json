{"title": "OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models", "authors": ["Zeyang Ma", "Dong Jae Kim", "Tse-Hsun (Peter) Chen"], "abstract": "Log parsing is a critical step that transforms un- structured log data into structured formats, facilitating subse- quent log-based analysis. Traditional syntax-based log parsers are efficient and effective, but they often experience decreased accuracy when processing logs that deviate from the predefined rules. Recently, large language models (LLM) based log parsers have shown superior parsing accuracy. However, existing LLM- based parsers face three main challenges: 1) time-consuming and labor-intensive manual labeling for fine-tuning or in-context learning, 2) increased parsing costs due to the vast volume of log data and limited context size of LLMs, and 3) privacy risks from using commercial models like ChatGPT with sensitive log information. To overcome these limitations, this paper introduces OpenLogParser, an unsupervised log parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance privacy and reduce operational costs while achieving state-of-the-art parsing accuracy. OpenLogParser first groups logs with similar static text but varying dynamic variables using a fixed-depth grouping tree. It then parses logs within these groups using three components: i) similarity scoring-based retrieval augmented generation: selects diverse logs within each group based on Jaccard similarity, helping the LLM distinguish between static text and dynamic variables; ii) self-reflection: iteratively query LLMs to refine log templates to improve parsing accuracy; and iii) log template memory: stores parsed templates to reduce LLM queries for improved parsing efficiency. Our evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher parsing accuracy and processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy and cost concerns of using commercial LLMs while achieving state-of-the-arts parsing efficiency and accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Real-world software systems generate large amounts of logs, often hundreds of gigabytes or even terabytes per day [11, 19, 48]. These logs provide developers with invaluable runtime information, essential for understanding system execution and debugging. To manage and analyze this vast amount of data, researchers and practitioners have proposed many automated approaches, such as monitoring [8, 43], anomaly detection [25, 40], and root cause analysis [33, 46]. However, as shown in Figure 1, logs are semi-structured, containing a mixture of static text and dynamically generated variables (e.g., port number 62267), which makes direct analysis challenging.\nLog parsing is a critical first step in log analysis that transforms unstructured logs into log templates, dividing logs into static parts (static messages) and dynamic parts (vari- ables). As illustrated in Figure 1, log templates represent the event structure of logs, providing a standardized format that simplifies further analysis. By distinguishing between static and dynamic components, log parsing enables more efficient and accurate downstream tasks [22, 26, 37]. Given the sheer volume and diversity of generated logs, prior research has proposed various syntax-based parsers for efficient and effective log parsing. These parsers, such as Drain [14] and AEL [18], use manually crafted heuristics or predefined rules to identify and extract log templates. Although promising, these log parsers often experience decreased accuracy when processing logs that deviate from predefined rules [19, 21, 48].\nRecent advances in large language models (LLMs) have enabled researchers to leverage these models for log pars- ing [9, 20, 23, 24, 31, 44]. LLMs exhibit superior capabilities in understanding and generating text, making them particularly effective for parsing semi-structured log data. Consequently, LLM-based log parsers often achieve higher accuracy than traditional syntax-based parsers [20, 24, 31]. However, the sheer volume of log data and the limited context size of LLMs lead to increased parsing costs, both in terms of time and money, as token consumption grows linearly with log size. This makes practical adoption challenging. Additionally, these parsers frequently require manually derived log template pairs for in-context learning, adding significant manual overhead.\nA further complication arises from the reliance on commer- cial LLMs like ChatGPT by many LLM-based log parsers [20,"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In this section, we discuss the background of LLM and its privacy concerns. We then discuss related log parsing research.\nA. Background\nLarge Language Models. Large Language Models (LLMs), primarily built on the transformer architecture [4, 7, 34], have significantly advanced the field of natural language processing (NLP). These LLMs, such as the widely recognized GPT- 3 model with its 175 billion parameters [7], are trained on diverse text data from various sources, including source code. The training involves self-supervised learning objectives that enable these models to develop a deep understanding of language and generate text that is contextually relevant and semantically coherent. LLMs have shown substantial capabil- ity in tasks that involve complex language comprehension and generation, such as code recognition and generation [5, 27].\nDue to logs being semi-structured texts composed of nat- ural language and code elements, researchers have adopted LLMs to tackle log analysis tasks, such as anomaly detec- tion [25, 28, 40], root cause analysis [33, 35, 36], and log parsing [9, 20, 23, 24, 31, 44]. Log parsing is one of the primary tasks of focus in this area, given its crucial role for more accurate and insightful downstream log analysis [22, 37].\nPrivacy Issues Related to LLM. While LLMs demonstrate remarkable capabilities in processing and generating natural language and code, their application on sensitive data such as logs presents notable privacy risks, particularly with commer- cial models such as ChatGPT [2, 7]. One major concern is that data transmitted to these models-such as system logs-could be retained and used in the model's further training cycles without explicit consent or knowledge of the data owners [6]. More importantly, sensitive data uploaded to the LLM providers could potentially be exposed through inadvertent data leaks or malicious attacks [16], posing significant privacy risks. To avoid such risks, an industry norm is to restrict the use of commercial LLMs despite their advanced capabilities. For ex- ample, Samsung bans ChatGPT and other commercial chatbots after a sensitive code leak [1]. Major financial institutions like Citigroup and Goldman Sachs have restricted the use of ChatGPT due to concerns over data privacy and security [3].\nIn contrast, open-source LLMs, such as those developed by Meta's Llama series [4, 34], offer greater privacy and security. Users can adopt the LLMs for local deployment to ensure data privacy, aligning with stringent data protection standards. Thus, open-source LLMs are more secure and trustworthy for handling confidential data such as logs[32, 45].\nB. Related Work\nCurrent automated log parsers can be broadly categorized into two types: syntax-based log parsers and semantic-based log parsers. Syntax-based log parsers [11, 12, 14, 18] typically employ heuristic rules or conduct comparisons among logs to identify common components that serve as templates. Semantic-based log parsers [20, 24, 30, 31] focus on analyzing"}, {"title": "III. APPROACH", "content": "In this section, we introduce OpenLogParser, an efficient unsupervised log parser, leveraging memory capabilities and advanced prompting techniques to maximize efficiency and parsing accuracy. OpenLogParser leverages a smaller-size open-source LLM to enhance privacy and reduce operation costs. Figure 2 illustrates the overall architecture of Open- LogParser, which primarily comprises of three components: (i) log grouping, which groups logs that share a commonality in their text. Such log groups can then be used as input to LLM to uncover dynamic variables. (ii) An unsupervised LLM- based log parser that uses retrieval-augmented generation (RAG), followed by an iterative self-reflection mechanism to accurately parse the grouped logs into log templates. (iii) An efficient log template memory, which memorizes the parsed log templates for future query. The core idea is to enhance efficiency by storing parsed log templates in memory, thereby avoiding the need for repeated LLM queries.\nA. Log Grouping Based on Commonality\nOpenLogParser achieves unsupervised and zero-shot log parsing by first applying an effective grouping strategy. This strategy aims to group logs that share commonality in their static text, yet are different in their dynamic variables. Such log groups can then be used as input to LLMs to generate log templates by prompting LLMs to identify the dynamic variables among logs in the same group. To group the logs, we adapt the efficient unsupervised methodology proposed by Drain [14], which applies a fixed-depth parsing tree and parsing rules (i.e., K prefix tokens) to identify log groups. The fixed depth in our grouping tree provides a structured and predictable framework that enhances efficiency. By limiting the depth, we reduce the complexity of the tree traversal, which speeds up the grouping process.\nOur fixed-depth tree implementation for grouping consists of three key steps: (i) group by length, (ii) group by K prefix tokens, and (iii) group by token string similarity. In step (i), we first group the logs based on token length, which partitions the logs into subsets of logs that are similar in token length. This initial grouping significantly reduces the computational complexity in the subsequent grouping phases. In step (ii), the grouped logs are then kept at a fixed depth which stores K prefix tokens. Since logs are initially grouped based on token length, truncating K prefix tokens (default the first three tokens of the log) can limit the number of nodes visited during the subsequent traversal process for step (iii), significantly improving grouping efficiency. Prior to step (iii), it is important to note that we abstract the numerical literals in the logs with a wildcard symbol (*). This is done to prevent the issue of grouping explosion in step (iii), which can make grouping inefficient. Finally, in step (iii), we calculate the similarity between the new logs and the log groups stored in the fixed- depth tree. This step determines whether the incoming log fits into an existing group or necessitates the creation of a new log group. If a suitable group is found based on the similarity threshold, i.e., # of common tokens / # of total number of tokens > 0.5, the log is inserted into existing log groups. If not, a new group is created, and the tree is dynamically updated to accommodate this new log pattern. This adaptive approach ensures that our system evolves with the incoming data, continuously optimizing both the accuracy and efficiency of the log grouping process.\nB. LLM-based Unsupervised Log Parsing\nOur prompts to LLMs contain representative logs (based on variability) retrieved from each log group (from Section III-A) to guide LLMs in separating dynamic variables and static text. Figure 3 illustrates the prompt template that OpenLogParser uses. Below, we discuss the composition of our prompt in detail.\nPrompt Instruction. In the instruction part of our prompt, we define the goal of the log parsing task to the LLM (highlighted in green in Figure 3). We emphasize that all the provided logs should share one common template that matches all selected logs. This specification is crucial to ensure that the LLM can effectively identify the commonalities and variability within the provided logs, thereby preventing any difficulties in parsing due to inconsistent log templates.\nStandardizing LLM Response by Input and Output Example. Since our LLM is not instruction fine-tuned [31], it is crucial to clearly describe our task instruction and include an input- output example in the prompt. This explicit guidance helps the LLM understand the desired input and output formats. As shown in Figure 3, we provide one example to illustrate the input/output form. The example remains unchanged for all systems. This approach effectively guides the LLM in understanding the objective and input-output formats without the need of instruction fine-tuning or labeled data.\nRetrieval-Augmented Log Parsing. To parse logs accurately, we select representative logs that showcase variabilities within a log group based on commonality. By presenting the LLM with logs sharing the same structure but varying in dynamic variables, it can more effectively distinguish between fixed and dynamic elements to identify the log template. We developed a retrieval argument generation (RAG) approach based on Jac- card Similarity [41]. Jaccard similarity measures the similarity between two given sets by calculating the ratio of the number of elements (e.g., tokens) in their intersection to the number of elements in their union. For log data, each log is split into a set of tokens (i.e., words), and then these tokens are used to determine the sizes of the intersection and union. The resulting ratio is the Jaccard similarity between two given logs, with a ratio closer to one indicating higher similarity. We aim to identify the logs with the greatest variability within the same group. Hence, we select logs with the lowest Jaccard similarity score. This approach helps create accurate log templates by focusing on logs that are most indicative of the entire group's characteristics.\nOur selection process starts by selecting the longest log (based on the number of characters) within the group as the initial reference. We then calculate the Jaccard similarity between this log and every other log in the group. The log with the lowest similarity to the reference log is added to the selection set. We continue computing pairwise Jaccard simi- larity between the selected logs and the remaining unselected logs, sequentially adding the log with the lowest similarity.\nSpecifically, the logs selected from the log group are listed in the format of a Python list within the prompt for parsing. We use a prefix (i.e., 'Log list :') to help the LLM identify the logs that require parsing (highlighted in yellow in Figure 3). This consistency in input format, mirroring the \"Input and Output Example\", also guides the LLM to respond with the log template in a fixed format as demonstrated in the example, facilitating accurate template generation and extraction.\nPost-processing Template Standardization. We use a post- processing technique to further standardize the log template generated by LLM. We employ string manipulation techniques to remove non-template content from the response (i.e., pre- fixes and backticks). To facilitate the verification of the accu- racy of log templates, we replace the placeholder \"<*>\" within the templates with the regular expression pattern\"(.*?)\". The regex template enables a direct matching process when comparing the generated templates with logs, and can be directly applied to abstract logs.\nSelf-Reflection for Verifying Log Template. After generating a log template, we verify whether the template can match each log within the group. If a log is correctly matched by a log template, we consider it to be parsed successfully. The log tem- plate is then added to the log template memory for future use. After all logs in the group have been checked, any unparsed logs undergo a self-reflection process [38], which aims to revise the templates and improve parsing results. Similar to the initial parsing attempt, we first select these unparsed logs and then utilize the prompt described in Figure 3 to generate a new log template using LLMs. This step is repeated until all logs in the group can be matched/parsed by the generated templates. Note that, to prevent the LLM from entering a parsing loop (i.e., repeatedly generating incorrect templates), we limit the self-reflection process to three iterations.\nC. Template Memory for Efficient Log Parsing\nRepeatedly using LLM to parse logs with identical group- ings and templates significantly increases the frequency of LLM queries, thereby reducing the efficiency of the log parsing process. To address this issue, we introduce log template memory in OpenLogParser, which stores the parsed log templates for future parsing, avoiding redundant LLM queries.\nEfficient Log Template Memory Search and Matching. When a log group requires parsing, we first check whether a match- ing log template exists within the memory. If some logs within the group find a matching template in the memory, we apply this log template to parse the logs, mitigating the need for LLM queries. However, it is possible that some logs within the same group may match while others may not (e.g., due to limitations in the grouping step or limitation of the log template). Hence, the logs that remain unparsed are then sent to LLM for parsing. The new log template generated from this process is then added to the log template memory for future reference. This design significantly reduces the number of LLM queries during the log parsing process.\nTo efficiently utilize log templates in the memory, there is a need for an efficient search mechanism to verify whether or not the given logs match existing log templates in the memory. This is crucial since the memory can be large, consisting of many log templates. For every log, we need potentially at most N searches for N log templates. To improve efficiency, we put forward one key observation: the token length of log templates is always less than or equal to that of the original logs, as multiple tokens may be treated as a single variable during log parsing. For instance, consider the log \u2018sent 100 bytes data\u2019. After parsing, the corresponding log template is generated as 'sent <*> data'. The original log consists of four tokens, whereas the parsed template has three. This reduction in token count occurs because 100 bytes' is treated as a single variable, thus decreasing the overall length of the template compared to the original log. Consequently, when searching for log templates in the memory, we first sort the templates based on the number of tokens. This sorting allows us to efficiently check new logs by first calculating the token length of the log to be parsed, then using binary search to find all templates with a token count less than or equal to the log length. This design reduces the number of match checks required from O(N) to O(LogN), thereby enhancing the efficiency of the search process.\nOur log-template matching process is efficient. Unlike tradi- tional log templates that use placeholders (i.e., ` `<*>' ') to abstract dynamic variables within logs, we store log templates in memory as regular expression patterns (i.e., use ``(.?)'' instead of placeholders). This adjustment allows us to use reg- ular expressions to efficiently verify whether logs match with log templates in memory and improve matching efficiency.\""}, {"title": "IV. EXPERIMENT SETUP", "content": "In this section, we discuss our experiment setup to answer our research questions and OpenLogParser's implementation details.\nStudied Dataset. We conduct our experiment on the log parsing benchmark LogHub-2.0 provided by He et al. [15, 19]. This benchmark contains logs from 14 open-source systems of different types, such as distributed systems, supercomputer systems, and server-side applications. LogHub is widely used to evaluate and compare the accuracy of log parsers [11, 12, 14, 20, 24, 31]. Compared to LogHub-1.0 [15], the number of logs has increased significantly in LogHub-2.0, increasing from 28K (2K logs per system) to more than 50 million logs with a total of 3,488 different log templates. LogHub-2.0 also provides the groundtruth log template for each log. With this large-scale LogHub-2.0 dataset, researchers can better evaluate the efficiency and effectiveness of log parsers [19, 20].\nEnvironment and Implementation. Our experiments were conducted on an Ubuntu server with an NVIDIA Tesla A100 GPU, AMD EPYC 7763 64-core CPU, and 256GB RAM using Python 3.9. We execute the baselines using their default parameters under the same environment to compare the effi- ciency. We use Llama3 8B [4] for OpenLogParser's underlying LLM because it is a relatively small yet powerful model, balancing performance and efficiency effectively. We set the temperature value to 0 to improve the stability of the model output. Note that it is easy to switch to other LLMs. In RQ4, we evaluate OpenLogParser by replacing Llama3 with other open-source LLMs.\nEvaluation Metrics for Log Parsing. Following prior studies [11, 14, 20, 21, 24, 31], we use two most commonly used metrics to evaluate the effectiveness of log parsers: Group Accuracy and Parsing Accuracy.\nGroup Accuracy (GA): Grouping Accuracy [48] is a metric used in log parsing to evaluate the extent to which log messages belonging to the same template are correctly grouped together by a parser. GA is defined as the ratio of correctly grouped log messages to the total number of log messages. For a log message to be considered correctly grouped, it must be assigned to the same group as other log messages that share the same underlying template. High GA indicates that the parser can effectively discern patterns within the log data and group similar log messages together. This can be crucial for various downstream log analysis tasks such as anomaly detection [22, 37]. Despite its usefulness, GA has limitations. GA can remain high even if the parsed templates are flawed. Namely, a high GA score might obscure errors in dynamic variable extraction and template identification within the logs, leading to a misleading perception of overall parsing accuracy.\nParsing Accuracy (PA): Parsing Accuracy (PA) [30] comple- ments GA and is calculated as the ratio of accurately parsed log messages to the total number of log messages. For a log message to be deemed correctly parsed, both extracted static text and dynamic variables must match exactly with those specified in the ground truth. PA is a stricter metric because it requires a comprehensive match of all log components, not just their correct grouping. This distinction is crucial, as GA primarily evaluates the correct clustering of logs, while PA ensures precise parsing accuracy at the individual log message level. Precise log parsing of the variables can also significantly impact the effectiveness of downstream log-based analyses [26]."}, {"title": "V. EVALUATION", "content": "In this section, we evaluate OpenLogParser by answering four research questions (RQs).\nRQ1: What is the effectiveness of OpenLogParser?\nMotivation. Accuracy is the most critical factor for evaluating the effectiveness of log parsers. High accuracy in log parsing aids downstream log analysis tasks [22, 37]. In this RQ, we study the effectiveness of OpenLogParser."}, {"title": "VI. THREATS TO VALIDITY", "content": "External validity. Data leakage is a potential risk of LLM- based log parsers [20, 31]. Although OpenLogParser does not involve using labeled logs for fine-tuning or in-context learning, there is a possibility that the LLM might have been pre-trained on publicly available log data. Our evaluation dataset with ground-truth templates was released on August 2023 [19] and Llama3-8B training knowledge cutoff from March 2023 [4], so the leakage risk should be minimal. The log format may also affect our result, but the datasets used are large and cover logs from various systems in different formats. Future studies are needed to evaluate OpenLogParser on logs from other systems.\nInternal validity. OpenLogParser employs Llama3-8B as its base model due to its promising results in many tasks and the relatively small size [4]. We also compared the results across various open-source LLMs and found differences. Future re- search is needed to evaluate LLM-based parsers' performance when more advanced LLMs are released in the future. The effectiveness of OpenLogParser could be influenced by spe- cific parameter settings (e.g., the number of logs selected for prompting). Our evaluations showed that these settings have an impact on the parsing results and discussed the optimal settings. Future studies are needed to evaluate the settings on other datasets.\nConstruct validity. To mitigate the effects of randomness in evaluating OpenLogParser, the generation temperature of the model is set to zero. This adjustment ensures that experiments conducted under the same conditions are repeatable and that the results are stable."}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduced OpenLogParser, an unsuper- vised log parsing technique utilizing open-source LLMs to effectively address the limitations of existing LLM-based and syntax-based parsers. OpenLogParser first groups logs that share a syntactic similarity in the static text but vary in the dynamic variable, using a fixed-depth grouping tree. It then parses logs in these groups with three components: i) retrieval augmented generation using similarity scoring: identifies diverse logs within each group based on Jaccard similarity, aiding the LLM in differentiating static text from dynamic variables; ii) self-reflection: iteratively queries LLMs to refine log templates and enhance parsing accuracy; and iii) log template memory: store parsed templates to minimize LLM queries, thereby boosting parsing efficiency. Our com- prehensive evaluations on LogHub-2.0, a public large-scale log dataset, demonstrate that OpenLogParser achieves an average GA of 0.8720 and an average PA of 0.8538, outperforming state-of-the-art parsers (i.e., ILIAC [20] and LLMParser [31]) by 5% and 25%, respectively. OpenLogParser parses logs from all 14 systems (50 million logs) in a total of 5.94 hours, which is 2.75 and 40 times faster than other LLM-based parsers This marks a substantial advancement over traditional semantic-based and LLM-based parsers in an unsupervised way, confirming the robustness and effectiveness of our ap- proach. Additionally, OpenLogParser addresses the privacy and cost concerns associated with commercial LLMs, making it a highly efficient and secure solution for practical log parsing needs."}]}