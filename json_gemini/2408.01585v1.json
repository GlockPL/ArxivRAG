{"title": "OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models", "authors": ["Zeyang Ma", "Dong Jae Kim", "Tse-Hsun (Peter) Chen"], "abstract": "Log parsing is a critical step that transforms un- structured log data into structured formats, facilitating subse- quent log-based analysis. Traditional syntax-based log parsers are efficient and effective, but they often experience decreased accuracy when processing logs that deviate from the predefined rules. Recently, large language models (LLM) based log parsers have shown superior parsing accuracy. However, existing LLM- based parsers face three main challenges: 1) time-consuming and labor-intensive manual labeling for fine-tuning or in-context learning, 2) increased parsing costs due to the vast volume of log data and limited context size of LLMs, and 3) privacy risks from using commercial models like ChatGPT with sensitive log information. To overcome these limitations, this paper introduces OpenLogParser, an unsupervised log parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance privacy and reduce operational costs while achieving state-of-the-art parsing accuracy. OpenLogParser first groups logs with similar static text but varying dynamic variables using a fixed-depth grouping tree. It then parses logs within these groups using three components: i) similarity scoring-based retrieval augmented generation: selects diverse logs within each group based on Jaccard similarity, helping the LLM distinguish between static text and dynamic variables; ii) self-reflection: iteratively query LLMs to refine log templates to improve parsing accuracy; and iii) log template memory: stores parsed templates to reduce LLM queries for improved parsing efficiency. Our evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher parsing accuracy and processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy and cost concerns of using commercial LLMs while achieving state-of-the-arts parsing efficiency and accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Real-world software systems generate large amounts of logs, often hundreds of gigabytes or even terabytes per day [11, 19, 48]. These logs provide developers with invaluable runtime information, essential for understanding system execution and debugging. To manage and analyze this vast amount of data, researchers and practitioners have proposed many automated approaches, such as monitoring [8, 43], anomaly detection [25, 40], and root cause analysis [33, 46]. However, as shown in Figure 1, logs are semi-structured, containing a mixture of static text and dynamically generated variables (e.g., port number 62267), which makes direct analysis challenging.\nLog parsing is a critical first step in log analysis that transforms unstructured logs into log templates, dividing logs"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In this section, we discuss the background of LLM and its privacy concerns. We then discuss related log parsing research."}, {"title": "A. Background", "content": "Large Language Models. Large Language Models (LLMs), primarily built on the transformer architecture [4, 7, 34], have significantly advanced the field of natural language processing (NLP). These LLMs, such as the widely recognized GPT- 3 model with its 175 billion parameters [7], are trained on diverse text data from various sources, including source code. The training involves self-supervised learning objectives that enable these models to develop a deep understanding of language and generate text that is contextually relevant and semantically coherent. LLMs have shown substantial capabil- ity in tasks that involve complex language comprehension and generation, such as code recognition and generation [5, 27]. Due to logs being semi-structured texts composed of nat- ural language and code elements, researchers have adopted LLMs to tackle log analysis tasks, such as anomaly detec- tion [25, 28, 40], root cause analysis [33, 35, 36], and log parsing [9, 20, 23, 24, 31, 44]. Log parsing is one of the primary tasks of focus in this area, given its crucial role for more accurate and insightful downstream log analysis [22, 37].\nPrivacy Issues Related to LLM. While LLMs demonstrate remarkable capabilities in processing and generating natural language and code, their application on sensitive data such as logs presents notable privacy risks, particularly with commer- cial models such as ChatGPT [2, 7]. One major concern is that data transmitted to these models-such as system logs-could be retained and used in the model's further training cycles without explicit consent or knowledge of the data owners [6]. More importantly, sensitive data uploaded to the LLM providers could potentially be exposed through inadvertent data leaks or malicious attacks [16], posing significant privacy risks. To avoid such risks, an industry norm is to restrict the use of commercial LLMs despite their advanced capabilities. For ex- ample, Samsung bans ChatGPT and other commercial chatbots after a sensitive code leak [1]. Major financial institutions like Citigroup and Goldman Sachs have restricted the use of ChatGPT due to concerns over data privacy and security [3]. In contrast, open-source LLMs, such as those developed by Meta's Llama series [4, 34], offer greater privacy and security. Users can adopt the LLMs for local deployment to ensure data privacy, aligning with stringent data protection standards. Thus, open-source LLMs are more secure and trustworthy for handling confidential data such as logs[32, 45]."}, {"title": "B. Related Work", "content": "Current automated log parsers can be broadly categorized into two types: syntax-based log parsers and semantic-based log parsers. Syntax-based log parsers [11, 12, 14, 18] typically employ heuristic rules or conduct comparisons among logs to identify common components that serve as templates. Semantic-based log parsers [20, 24, 30, 31] focus on analyzing"}, {"title": "III. APPROACH", "content": "In this section, we introduce OpenLogParser, an efficient unsupervised log parser, leveraging memory capabilities and advanced prompting techniques to maximize efficiency and parsing accuracy. OpenLogParser leverages a smaller-size open-source LLM to enhance privacy and reduce operation costs. Figure 2 illustrates the overall architecture of Open- LogParser, which primarily comprises of three components: (i) log grouping, which groups logs that share a commonality in their text. Such log groups can then be used as input to LLM to uncover dynamic variables. (ii) An unsupervised LLM- based log parser that uses retrieval-augmented generation (RAG), followed by an iterative self-reflection mechanism to accurately parse the grouped logs into log templates. (iii) An efficient log template memory, which memorizes the parsed log templates for future query. The core idea is to enhance efficiency by storing parsed log templates in memory, thereby avoiding the need for repeated LLM queries."}, {"title": "A. Log Grouping Based on Commonality", "content": "OpenLogParser achieves unsupervised and zero-shot log parsing by first applying an effective grouping strategy. This strategy aims to group logs that share commonality in their static text, yet are different in their dynamic variables. Such log groups can then be used as input to LLMs to generate log templates by prompting LLMs to identify the dynamic"}, {"title": "B. LLM-based Unsupervised Log Parsing", "content": "Our prompts to LLMs contain representative logs (based on variability) retrieved from each log group (from Section III-A) to guide LLMs in separating dynamic variables and static text. Figure 3 illustrates the prompt template that OpenLogParser uses. Below, we discuss the composition of our prompt in detail.\nPrompt Instruction. In the instruction part of our prompt, we define the goal of the log parsing task to the LLM (highlighted in green in Figure 3). We emphasize that all the provided logs should share one common template that matches all selected logs. This specification is crucial to ensure that the LLM can effectively identify the commonalities and variability within the provided logs, thereby preventing any difficulties in parsing due to inconsistent log templates.\nStandardizing LLM Response by Input and Output Example. Since our LLM is not instruction fine-tuned [31], it is crucial to clearly describe our task instruction and include an input- output example in the prompt. This explicit guidance helps the LLM understand the desired input and output formats. As shown in Figure 3, we provide one example to illustrate the input/output form. The example remains unchanged for all systems. This approach effectively guides the LLM in understanding the objective and input-output formats without the need of instruction fine-tuning or labeled data.\nRetrieval-Augmented Log Parsing. To parse logs accurately, we select representative logs that showcase variabilities within a log group based on commonality. By presenting the LLM with logs sharing the same structure but varying in dynamic variables, it can more effectively distinguish between fixed and"}, {"title": "C. Template Memory for Efficient Log Parsing", "content": "Repeatedly using LLM to parse logs with identical group- ings and templates significantly increases the frequency of LLM queries, thereby reducing the efficiency of the log parsing process. To address this issue, we introduce log template memory in OpenLogParser, which stores the parsed log templates for future parsing, avoiding redundant LLM queries.\nEfficient Log Template Memory Search and Matching. When a log group requires parsing, we first check whether a match- ing log template exists within the memory. If some logs within the group find a matching template in the memory, we apply this log template to parse the logs, mitigating the need for LLM queries. However, it is possible that some logs within"}, {"title": "IV. EXPERIMENT SETUP", "content": "In this section, we discuss our experiment setup to answer our research questions and OpenLogParser's implementation details.\nStudied Dataset. We conduct our experiment on the log parsing benchmark LogHub-2.0 provided by He et al. [15, 19]. This benchmark contains logs from 14 open-source systems of different types, such as distributed systems, supercomputer systems, and server-side applications. LogHub is widely used to evaluate and compare the accuracy of log parsers [11, 12, 14, 20, 24, 31]. Compared to LogHub-1.0 [15], the number"}, {"title": "Environment and Implementation.", "content": "Our experiments were conducted on an Ubuntu server with an NVIDIA Tesla A100 GPU, AMD EPYC 7763 64-core CPU, and 256GB RAM using Python 3.9. We execute the baselines using their default parameters under the same environment to compare the effi- ciency. We use Llama3 8B [4] for OpenLogParser's underlying LLM because it is a relatively small yet powerful model, balancing performance and efficiency effectively. We set the temperature value to 0 to improve the stability of the model output. Note that it is easy to switch to other LLMs. In RQ4, we evaluate OpenLogParser by replacing Llama3 with other open-source LLMs."}, {"title": "Evaluation Metrics for Log Parsing.", "content": "Following prior studies [11, 14, 20, 21, 24, 31], we use two most commonly used metrics to evaluate the effectiveness of log parsers: Group Accuracy and Parsing Accuracy.\nGroup Accuracy (GA): Grouping Accuracy [48] is a metric used in log parsing to evaluate the extent to which log messages belonging to the same template are correctly grouped together by a parser. GA is defined as the ratio of correctly grouped log messages to the total number of log messages. For a log message to be considered correctly grouped, it must be assigned to the same group as other log messages that share the same underlying template. High GA indicates that the parser can effectively discern patterns within the log data and group similar log messages together. This can be crucial for various downstream log analysis tasks such as anomaly detection [22, 37]. Despite its usefulness, GA has limitations. GA can remain high even if the parsed templates are flawed. Namely, a high GA score might obscure errors in dynamic variable extraction and template identification within the logs, leading to a misleading perception of overall parsing accuracy.\nParsing Accuracy (PA): Parsing Accuracy (PA) [30] comple- ments GA and is calculated as the ratio of accurately parsed log messages to the total number of log messages. For a log message to be deemed correctly parsed, both extracted static text and dynamic variables must match exactly with those specified in the ground truth. PA is a stricter metric because it requires a comprehensive match of all log components, not just their correct grouping. This distinction is crucial, as GA primarily evaluates the correct clustering of logs, while PA ensures precise parsing accuracy at the individual log message level. Precise log parsing of the variables can also significantly impact the effectiveness of downstream log-based analyses [26]."}, {"title": "V. EVALUATION", "content": "In this section, we evaluate OpenLogParser by answering four research questions (RQs).\nRQ1: What is the effectiveness of OpenLogParser?\nMotivation. Accuracy is the most critical factor for evaluating the effectiveness of log parsers. High accuracy in log parsing aids downstream log analysis tasks [22, 37]. In this RQ, we study the effectiveness of OpenLogParser."}, {"title": "RQ2: What is the efficiency of OpenLogParser?", "content": "Motivation. Efficiency is crucial in log parsing since it directly impacts the practical usability of the parser in real-world applications. In this RQ, we study the parsers' efficiency.\nApproach. We measure the total parsing time required by OpenLogParser and its individual components (i.e., LLM queries, grouping, and memory search), and the four baseline parsers to process logs from the LogHub-2.0 dataset.\nResults. OpenLogParser is 2.7 and 40 times faster than Lilac and LLMParserT5Base, respectively. Table II shows the parsing time for each log parser across different systems. OpenLogParser spends a total of 5.94 hours to parse logs from all 14 systems (50 million logs), which is significantly faster than other LLM-based parsers: LILAC (16 hours) and LLMParserT5 Base (258 hours). The parsing time for Open- LogParser is mainly occupied by the LLM query time, which accounts for 72.05% of the total processing time, followed by the grouping time, which constitutes 16.67% of the overall duration. LLMParserT5Base is the slowest among all LLM- based parsers because it processes each log individually, and the vast quantity of logs linearly increases the number of model queries required. Even with a relatively lightweight model like T5-base, which has only 240 million parameters, querying to parse the logs individually is still slow and imprac- tical for real-world applications. LILAC, with its cache design, eliminates the need to parse each log individually through an LLM, significantly speeding up the process compared to LLMParser T5 Base. However, LILAC still requires frequent model queries to update the templates in the cache, which limits its efficiency. In contrast, OpenLogParser optimizes parsing times through its grouping and memory features, resulting in superior efficiency.\nAEL exhibits significant efficiency issues when parsing logs beyond certain sizes, while Drain maintains high efficiency across all datasets. AEL can parse datasets with fewer than 100K logs within seconds but requires several hours or even days for datasets with over one million logs (e.g., we stopped AEL after running for 10 days when parsing the 16 million logs from Spark). This inefficiency is due to AEL's reliance on extensive comparisons between logs and identified templates, where the parsing time grows exponentially with respect to the number of logs and log templates. In contrast, Drain, which uses a fixed-depth parsing tree, is the most efficient parser. OpenLogParser uses a grouping method similar to Drain's, with a total grouping time amounting to 0.99 hours,"}, {"title": "RQ3: How does different settings impact the result of Open- LogParser?", "content": "Motivation. OpenLogParser implements multiple components to achieve effective and efficient log parsing. In this RQ, we explore how various settings and configurations affect the performance of OpenLogParser.\nApproach. There are three general components in OpenLog- Parser that can be adjusted or replaced: log selection from each group for prompting, the number of selected logs, and the inclusion or exclusion of self-reflection processes. To select diverse logs from the log group, we use Jaccard similarity to measure the similarity between every log pair. In this RQ, we also try random sampling and cosine similarity. Furthermore, we evaluate how changing the number of selected logs from 1 to 10 impacts the effectiveness. Finally, we compare the effect of removing the self-reflection component on the efficiency and effectiveness of OpenLogParser.\nResults. Selecting representative logs based on Jaccard similarity outperforms using cosine similarity and random sampling. Table III shows the total time, GA, and PA of Open- LogParser compared to replacing the log selection process with cosine similarity and random sampling. When employing cosine similarity to select representative logs, both GA and PA experienced declines of 2.5% and 4.8%, respectively, com- pared to using Jaccard similarity. This indicates that although cosine similarity is shown to be an effective similarity metric for text data [39], it does not necessarily select logs that are representative enough for LLM to generalize log templates. However, we notice a slight reduction in execution time (3.3%) when using cosine similarity. Similarly, using random sampling further reduces the processing time (by 8.2%), but due to the lack of diversity in the sampled logs, both GA and PA are even lower, at 0.849 and 0.806, respectively.\nAlthough the self-reflection mechanism requires additional processing time, it significantly enhances the parsing re- sults of OpenLogParser. Table III compares full version OpenLogParser and OpenLogParser without self-reflection in the total execution time, GA, and PA. Excluding the self- reflection component from OpenLogParser results in a 44.6% reduction in parsing time (from around six to three hours). However, removing self-reflection greatly decreases both GA and PA by 7.1% and 9.5%, respectively. This shows that self- reflection significantly enhances the parsing effectiveness of"}, {"title": "RQ4: What is the effectiveness of OpenLogParser with differ- ent LLMs?", "content": "Motivation. Unlike previous parsers [20, 23, 44] that are based on commercial LLMs, OpenLogParser employs open-source LLM to mitigate privacy concerns and monetary costs. Dif- ferent LLMs exhibit varying capabilities due to their distinct architectures and pre-training data. In this RQ, we evaluate the performance of OpenLogParser across various open-source LLMs.\nApproach. We selected three other open-source models (in addition to Llama3-8B) with similar parameter sizes to com- pare the log parsing performance with different LLMs, includ- ing Mistral-7B [17], CodeGemma-7B [42], and ChatGLM3- 6B [13]. These models are commonly used in research and"}, {"title": "VI. THREATS TO VALIDITY", "content": "External validity. Data leakage is a potential risk of LLM- based log parsers [20, 31]. Although OpenLogParser does not involve using labeled logs for fine-tuning or in-context learning, there is a possibility that the LLM might have been pre-trained on publicly available log data. Our evaluation dataset with ground-truth templates was released on August 2023 [19] and Llama3-8B training knowledge cutoff from March 2023 [4], so the leakage risk should be minimal. The log format may also affect our result, but the datasets used are large and cover logs from various systems in different formats. Future studies are needed to evaluate OpenLogParser on logs from other systems.\nInternal validity. OpenLogParser employs Llama3-8B as its base model due to its promising results in many tasks and the relatively small size [4]. We also compared the results across various open-source LLMs and found differences. Future re- search is needed to evaluate LLM-based parsers' performance when more advanced LLMs are released in the future. The effectiveness of OpenLogParser could be influenced by spe- cific parameter settings (e.g., the number of logs selected for prompting). Our evaluations showed that these settings have an impact on the parsing results and discussed the optimal settings. Future studies are needed to evaluate the settings on other datasets.\nConstruct validity. To mitigate the effects of randomness in evaluating OpenLogParser, the generation temperature of the model is set to zero. This adjustment ensures that experiments conducted under the same conditions are repeatable and that the results are stable."}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduced OpenLogParser, an unsuper- vised log parsing technique utilizing open-source LLMs to effectively address the limitations of existing LLM-based and syntax-based parsers. OpenLogParser first groups logs that share a syntactic similarity in the static text but vary in the dynamic variable, using a fixed-depth grouping tree. It then parses logs in these groups with three components: i) retrieval augmented generation using similarity scoring: identifies diverse logs within each group based on Jaccard similarity, aiding the LLM in differentiating static text from dynamic variables; ii) self-reflection: iteratively queries LLMs to refine log templates and enhance parsing accuracy; and iii) log template memory: store parsed templates to minimize LLM queries, thereby boosting parsing efficiency. Our com- prehensive evaluations on LogHub-2.0, a public large-scale log dataset, demonstrate that OpenLogParser achieves an average GA of 0.8720 and an average PA of 0.8538, outperforming state-of-the-art parsers (i.e., ILIAC [20] and LLMParser [31]) by 5% and 25%, respectively. OpenLogParser parses logs from all 14 systems (50 million logs) in a total of 5.94 hours, which is 2.75 and 40 times faster than other LLM-based parsers This marks a substantial advancement over traditional semantic-based and LLM-based parsers in an unsupervised"}]}