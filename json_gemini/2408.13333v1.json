{"title": "Hierarchical Imitation and Reinforcement Learning for Decision Making", "authors": ["Dongsheng Li", "Wenlong Li", "Yang Yu"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) are two popular methods for decision making problems. IL allows an agent to learn from experts by mimicking their behaviors, while RL enables an agent to learn to make decisions by interacting with the environment and optimizing a reward function. Despite their success, both methods face challenges in complex, real-world decision making problems. To address these challenges, we explore Hierarchical Imitation and Reinforcement Learning (HIRL), which combines the strengths of IL and RL. HIRL allows an agent to learn a hierarchy of decision making policies, where higher-level policies learn to select lower-level policies, and lower-level policies learn to execute specific actions. This hierarchical structure can improve the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL. In this survey, we provide a comprehensive review of HIRL, covering its theoretical foundations, algorithms, and applications. We also discuss the challenges and future directions of HIRL, such as the design of effective hierarchical structures, the integration of intrinsic motivation, and the development of off-policy HIRL algorithms.", "sections": [{"title": "1. Introduction", "content": "Decision making is a fundamental capability of intelligent systems, enabling them to interact with the environment and achieve specific goals [1]. Learning effective strategies for decision making is a challenging task, especially in complex and dynamic environments [2]. Two popular approaches for decision making are Imitation Learning (IL) and Reinforcement Learning (RL). IL allows an agent to learn from experts by mimicking their behaviors [3], while RL enables an agent to learn to make decisions by interacting with the environment and optimizing a reward function [4]. Both IL and RL have achieved remarkable results in various applications, such as game playing [5], robotics [6], and autonomous driving [7].\n\nHowever, both IL and RL face challenges in complex, real-world decision making problems. On the one hand, IL relies on high-quality expert data, which may be expensive or difficult to obtain [8]. IL may also suffer from the distribution shift problem, where the agent encounters states that are not covered by the expert data, leading to poor performance [9]. On the other hand, RL requires a well-defined reward function, which may be difficult to design or specify in practice [10]. RL may also suffer from the exploration problem, where the agent struggles to discover rewarding behaviors in complex environments [11]. Moreover, RL may not generalize well to new environments or tasks, especially when the state and action spaces are large [12].\n\nTo address these challenges, we explore Hierarchical Imitation and Reinforcement Learning (HIRL), which combines the strengths of IL and RL. HIRL allows an agent to learn a hierarchy of decision making policies, where higher-level policies learn to select lower-level policies, and lower-level policies learn to execute specific actions [13]. This hierarchical structure can improve the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL. For example, a robot can learn to navigate a building by first learning to select a sequence of rooms to visit, and then learning to navigate each room to reach the desired location [14]. In this case, the higher-level policy learns to select a sequence of rooms, while the lower-level policy learns to navigate each room.\n\nThe key idea of HIRL is to decompose a complex decision making problem into a hierarchy of simpler sub-problems, where each sub-problem can be solved by IL or RL. This hierarchical decomposition can reduce the complexity of the problem, making it easier to learn effective strategies. HIRL also allows the agent to leverage different levels of abstraction, where higher-level policies can reason about abstract goals, while lower-level policies can focus on detailed actions. This hierarchical structure can improve the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL.\n\nIn this survey, we provide a comprehensive review of HIRL, covering its theoretical foundations, algorithms, and applications. We also discuss the challenges and future directions of HIRL, such as the design of effective hierarchical structures, the integration of intrinsic motivation, and the development of off-policy HIRL algorithms. To the best of our knowledge, this is the first survey that focuses on the combination of IL and RL in a hierarchical setting."}, {"title": "2. Preliminaries", "content": "In this section, we provide a brief overview of the basic concepts and notation used in this survey. We first introduce the decision making problems, then introduce IL and RL, respectively.\n\n2.1 Decision Making Problems\n\nIn this section, we introduce the decision making problems, which are the foundation of IL and RL. A decision making problem is typically formulated as a Markov Decision Process (MDP) [15]. An MDP is defined as a tuple $M = (S, A, P, r, \\gamma)$, where:\n\n*   $S$ is the state space.\n\n*   $A$ is the action space.\n\n*   $P: S \\times A \\times S \\rightarrow [0, 1]$ is the transition probability function, where $P(s'|s, a)$ denotes the probability of transitioning to state $s'$ after taking action $a$ in state $s$.\n\n*   $r: S \\times A \\times S \\rightarrow \\mathbb{R}$ is the reward function, where $r(s, a, s')$ denotes the reward received after transitioning to state $s'$ after taking action $a$ in state $s$.\n\n*   $\\gamma \\in [0, 1]$ is the discount factor, which determines the importance of future rewards.\n\nA policy $\\pi: S \\rightarrow A$ is a mapping from states to actions. The goal of decision making is to find a policy $\\pi$ that maximizes the expected cumulative reward, also known as the return:\n\n$$J(\\pi) = \\mathbb{E}_{s_0, a_0, s_1, a_1, \\dots} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t, s_{t+1}) \\right],$$  (1)\n\nwhere $s_0$ is the initial state, and $a_t = \\pi(s_t)$ is the action taken at time step $t$.\n\nThe optimal policy $\\pi^*$ is the policy that maximizes the return for all states:\n\n$$\\pi^* = \\arg \\max_{\\pi} J(\\pi).$$\n\nDecision making problems can be categorized into different types based on the characteristics of the environment [16]:\n\n*   Deterministic vs. Stochastic: In a deterministic environment, the transition to the next state is fully determined by the current state and the action taken. In a stochastic environment, the transition to the next state is probabilistic.\n\n*   Episodic vs. Continuous: In an episodic environment, the agent interacts with the environment for a finite number of time steps, and the interaction terminates when the agent reaches a terminal state. In a continuous environment, the agent interacts with the environment indefinitely.\n\n*   Fully Observable vs. Partially Observable: In a fully observable environment, the agent has access to the complete state of the environment. In a partially observable environment, the agent only has access to a partial or noisy observation of the state.\n\nIn the following sections, we will introduce two popular methods for solving decision making problems: IL and RL."}, {"title": "2.2 Imitation Learning", "content": "Imitation Learning (IL) is a learning paradigm that enables an agent to learn a policy by mimicking the behaviors of an expert [3]. IL is also known as learning from demonstration (LfD) or apprenticeship learning [17]. IL is particularly useful when it is difficult or impossible to design a reward function for RL, or when the agent needs to learn a policy quickly from limited data.\n\nThe goal of IL is to learn a policy $\\pi$ that matches the expert's policy $\\pi_E$ as closely as possible. This can be achieved by minimizing a loss function that measures the discrepancy between the agent's policy and the expert's policy. The most common loss function is the cross-entropy loss, which measures the difference between the probability distributions of the agent's actions and the expert's actions:\n\n$$L(\\pi) = \\mathbb{E}_{s \\sim D} \\left[ - \\log \\pi(a_E|s) \\right],$$  (2)\n\nwhere $D$ is a distribution of states, and $a_E = \\pi_E(s)$ is the expert's action in state $s$.\n\nIL algorithms can be categorized into different types based on the way they collect and use the expert data [18]:\n\n*   Behavior Cloning (BC): BC is a supervised learning approach that directly learns a policy from the expert data. BC simply trains a model to predict the expert's actions given the states observed in the expert data. BC is easy to implement and computationally efficient, but it suffers from the distribution shift problem, where the agent encounters states that are not covered by the expert data, leading to poor performance [9].\n\n*   Inverse Reinforcement Learning (IRL): IRL is a reward learning approach that learns a reward function from the expert data, and then uses RL to learn a policy that maximizes the learned reward function. IRL can alleviate the distribution shift problem by learning a reward function that generalizes to new states, but it is computationally expensive and requires solving an RL problem in each iteration [19].\n\n*   Interactive IL: Interactive IL is an online learning approach that interacts with the expert during the learning process, asking for feedback or guidance when the agent is uncertain or makes mistakes. Interactive IL can improve the learning efficiency and robustness of IL, but it requires a cooperative expert who can provide timely and informative feedback [20].\n\nIn the following sections, we will introduce these three categories of IL algorithms in more detail."}, {"title": "2.3 Reinforcement Learning", "content": "Reinforcement Learning (RL) is a learning paradigm that enables an agent to learn a policy by interacting with the environment and optimizing a reward function [4]. RL is particularly useful when it is difficult or impossible to obtain expert data, or when the agent needs to learn a policy that adapts to the specific characteristics of the environment.\n\nThe goal of RL is to learn a policy $\\pi$ that maximizes the expected cumulative reward, also known as the return, as defined in Equation (1). This can be achieved by estimating a value function that measures the goodness of a state or a state-action pair. The most common value functions are the state-value function $V(s)$ and the action-value function $Q(s, a)$. The state-value function measures the expected return starting from state $s$ and following policy $\\pi$:\n\n$$V(\\pi, s) = \\mathbb{E}_{a_0, s_1, a_1, \\dots} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t, s_{t+1}) | s_0 = s, a_t = \\pi(s_t) \\right].$$\n\nThe action-value function measures the expected return starting from state $s$, taking action $a$, and following policy $\\pi$ thereafter:\n\n$$Q(\\pi, s, a) = \\mathbb{E}_{s_1, a_1, \\dots} \\left[ r(s, a, s_1) + \\sum_{t=1}^{\\infty} \\gamma^t r(s_t, a_t, s_{t+1}) | s_0 = s, a_0 = a, a_t = \\pi(s_t) \\right].$$\n\nRL algorithms can be categorized into different types based on the way they estimate the value function and learn the policy [21]:\n\n*   Value-Based: Value-based RL algorithms learn a policy by estimating the value function and selecting the action that maximizes the value function. The most common value-based RL algorithms are Q-learning [22] and SARSA [23].\n\n*   Policy-Based: Policy-based RL algorithms learn a policy directly by optimizing the policy parameters. The most common policy-based RL algorithms are REINFORCE [24] and Actor-Critic [25].\n\n*   Model-Based: Model-based RL algorithms learn a model of the environment and use the model to plan a policy. The most common model-based RL algorithms are Dyna-Q [26] and Monte Carlo Tree Search (MCTS) [27].\n\nIn the following sections, we will introduce these three categories of RL algorithms in more detail."}, {"title": "3. Hierarchical Imitation and Reinforcement Learning", "content": "Hierarchical Imitation and Reinforcement Learning (HIRL) is a learning paradigm that combines the strengths of IL and RL by learning a hierarchy of decision making policies [13]. HIRL is particularly useful when it is difficult or impossible to design a reward function for RL, or when the agent needs to learn a policy quickly from limited data, and when the environment is complex and dynamic.\n\nThe key idea of HIRL is to decompose a complex decision making problem into a hierarchy of simpler sub-problems, where each sub-problem can be solved by IL or RL. This hierarchical decomposition can reduce the complexity of the problem, making it easier to learn effective strategies. HIRL also allows the agent to leverage different levels of abstraction, where higher-level policies can reason about abstract goals, while lower-level policies can focus on detailed actions. This hierarchical structure can improve the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL.\n\nHIRL algorithms can be categorized into different types based on the way they define and learn the hierarchy [28]:\n\n*   Option-Based: Option-based HIRL algorithms learn a set of options, where each option is a policy that executes for a variable number of time steps. The higher-level policy learns to select the options, while the lower-level policies learn to execute the options. Option-based HIRL algorithms can improve the exploration efficiency and scalability of RL, but they require a careful design of the options [29].\n\n*   Feudal: Feudal HIRL algorithms learn a hierarchy of managers and workers, where the managers set goals for the workers, and the workers learn to achieve the goals set by the managers. The higher-level policies learn to set goals for the lower-level policies, while the lower-level policies learn to achieve the goals set by the higher-level policies. Feudal HIRL algorithms can improve the robustness and generalization ability of IL, but they require a careful design of the goal space [30].\n\n*   Goal-Based: Goal-based HIRL algorithms learn a hierarchy of policies that achieve specific goals. The higher-level policies learn to select the goals, while the lower-level policies learn to achieve the goals. Goal-based HIRL algorithms can improve the transferability and composability of policies, but they require a careful design of the goal space [31].\n\nIn the following sections, we will introduce these three categories of HIRL algorithms in more detail."}, {"title": "3.1 Option-Based Hierarchical Imitation and Reinforcement Learning", "content": "Option-based HIRL algorithms learn a set of options, where each option is a policy that executes for a variable number of time steps [29]. The higher-level policy learns to select the options, while the lower-level policies learn to execute the options. Option-based HIRL algorithms can improve the exploration efficiency and scalability of RL, but they require a careful design of the options.\n\nThe most common option-based HIRL algorithms are based on the option framework [29], which defines an option as a tuple $o = (I_o, \\pi_o, \\beta_o)$, where:\n\n*   $I_o \\subseteq S$ is the initiation set, which defines the states in which the option can be executed.\n\n*   $\\pi_o: S \\times A \\rightarrow [0, 1]$ is the policy, which defines the probability of taking action $a$ in state $s$ while executing the option.\n\n*   $\\beta_o: S \\rightarrow [0, 1]$ is the termination function, which defines the probability of terminating the option in state $s$.\n\nThe option framework allows the agent to learn a set of options that can be used as building blocks for more complex behaviors. The higher-level policy learns to select the options, while the lower-level policies learn to execute the options. This hierarchical structure can improve the exploration efficiency and scalability of RL, as the agent can reason about abstract goals rather than detailed actions.\n\nOption-based HIRL algorithms can be categorized into different types based on the way they learn the options and the higher-level policy [32]:\n\n*   Option Discovery: Option discovery algorithms learn the options from the environment without any external supervision. The most common option discovery algorithms are based on the bottleneck approach [33], the state abstraction approach [34], and the spectral approach [35].\n\n*   Option Imitation: Option imitation algorithms learn the options from expert data. The most common option imitation algorithms are based on the skill chaining approach [36], the goal-conditioned imitation approach [37], and the hierarchical imitation approach [38].\n\n*   Option Reinforcement: Option reinforcement algorithms learn the options using RL. The most common option reinforcement algorithms are based on the option-critic architecture [39], the policy gradient option architecture [40], and the value iteration option architecture [41].\n\nIn the following sections, we will introduce these three categories of option-based HIRL algorithms in more detail."}, {"title": "3.2 Feudal Hierarchical Imitation and Reinforcement Learning", "content": "Feudal HIRL algorithms learn a hierarchy of managers and workers, where the managers set goals for the workers, and the workers learn to achieve the goals set by the managers [30]. The higher-level policies learn to set goals for the lower-level policies, while the lower-level policies learn to achieve the goals set by the higher-level policies. Feudal HIRL algorithms can improve the robustness and generalization ability of IL, but they require a careful design of the goal space.\n\nThe most common feudal HIRL algorithms are based on the feudal reinforcement learning (FRL) framework [30], which defines a hierarchy of managers and workers. The managers set goals for the workers, and the workers learn to achieve the goals set by the managers. The goal space is defined as a set of abstract states or actions that the managers can use to communicate with the workers. The most common goal spaces are based on the state space [42], the action space [43], or the option space [44].\n\nFeudal HIRL algorithms can be categorized into different types based on the way they learn the managers and the workers [45]:\n\n*   Feudal Imitation: Feudal imitation algorithms learn the managers and the workers from expert data. The most common feudal imitation algorithms are based on the skill chaining approach [46], the goal-conditioned imitation approach [47], and the hierarchical imitation approach [48].\n\n*   Feudal Reinforcement: Feudal reinforcement algorithms learn the managers and the workers using RL. The most common feudal reinforcement algorithms are based on the option-critic architecture [49], the policy gradient option architecture [50], and the value iteration option architecture [51].\n\n*   Feudal Imitation and Reinforcement: Feudal imitation and reinforcement algorithms learn the managers from expert data and the workers using RL. The most common feudal imitation and reinforcement algorithms are based on the skill chaining approach [52], the goal-conditioned imitation approach [53], and the hierarchical imitation approach [54].\n\nIn the following sections, we will introduce these three categories of feudal HIRL algorithms in more detail."}, {"title": "3.3 Goal-Based Hierarchical Imitation and Reinforcement Learning", "content": "Goal-based HIRL algorithms learn a hierarchy of policies that achieve specific goals [31]. The higher-level policies learn to select the goals, while the lower-level policies learn to achieve the goals. Goal-based HIRL algorithms can improve the transferability and composability of policies, but they require a careful design of the goal space.\n\nThe most common goal-based HIRL algorithms are based on the hierarchical reinforcement learning (HRL) framework [31], which defines a hierarchy of policies that achieve specific goals. The goal space is defined as a set of abstract states that the higher-level policies can use to communicate with the lower-level policies. The most common goal spaces are based on the state space [55], the action space [56], or the option space [57].\n\nGoal-based HIRL algorithms can be categorized into different types based on the way they learn the policies and the goals [58]:\n\n*   Goal Discovery: Goal discovery algorithms learn the goals from the environment without any external supervision. The most common goal discovery algorithms are based on the bottleneck approach [59], the state abstraction approach [60], and the spectral approach [61].\n\n*   Goal Imitation: Goal imitation algorithms learn the goals from expert data. The most common goal imitation algorithms are based on the skill chaining approach [62], the goal-conditioned imitation approach [63], and the hierarchical imitation approach [64].\n\n*   Goal Reinforcement: Goal reinforcement algorithms learn the goals using RL. The most common goal reinforcement algorithms are based on the option-critic architecture [65], the policy gradient option architecture [66], and the value iteration option architecture [67].\n\nIn the following sections, we will introduce these three categories of goal-based HIRL algorithms in more detail."}, {"title": "4. Applications", "content": "In this section, we review the applications of HIRL in various domains. We categorize the applications based on the type of environment and the type of task.\n\n4.1 Robotics\n\nRobotics is a natural application for HIRL, as robots often need to perform complex tasks in dynamic and uncertain environments [6]. Option-based HIRL has been used to learn robot navigation [68], robot manipulation [69], and robot locomotion [70]. Feudal HIRL has been used to learn robot assembly [71], robot exploration [72], and robot imitation [73]. Goal-based HIRL has been used to learn robot planning [74], robot learning from demonstration [75], and robot transfer learning [76].\n\n4.2 Game Playing\n\nGame playing is another popular application for HIRL, as games often involve complex strategies and long-term planning [5]. Option-based HIRL has been used to learn game playing in Atari games [77], Go [78], and StarCraft [79]. Feudal HIRL has been used to learn game playing in Minecraft [80], Dota 2 [81], and League of Legends [82]. Goal-based HIRL has been used to learn game playing in multi-agent games [83], cooperative games [84], and adversarial games [85].\n\n4.3 Autonomous Driving\n\nAutonomous driving is a challenging application for HIRL, as autonomous vehicles need to navigate complex traffic scenarios and interact with other vehicles and pedestrians [7]. Option-based HIRL has been used to learn autonomous driving in highway driving [86], urban driving [87], and off-road driving [88]. Feudal HIRL has been used to learn autonomous driving in traffic negotiation [89], lane changing [90], and trajectory planning [91]. Goal-based HIRL has been used to learn autonomous driving in multi-agent scenarios [92], cooperative driving [93], and adversarial driving [94].\n\n4.4 Other Applications\n\nHIRL has also been applied to other domains, such as natural language processing [95], computer vision [96], and healthcare [97]. Option-based HIRL has been used to learn natural language generation [98], image captioning [99], and medical diagnosis [100]. Feudal HIRL has been used to learn natural language understanding [101], image recognition [102], and medical treatment [103]. Goal-based HIRL has been used to learn natural language dialogue [104], image segmentation [105], and medical decision support [106]."}, {"title": "5. Challenges and Future Directions", "content": "HIRL has shown promising results in various applications, but it also faces several challenges and future directions [107]:\n\n5.1 Design of Effective Hierarchical Structures\n\nThe design of effective hierarchical structures is a critical challenge for HIRL. The hierarchy should be designed in a way that reduces the complexity of the problem, improves the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL. The design of effective hierarchical structures may depend on the specific characteristics of the environment and the task [108].\n\n5.2 Integration of Intrinsic Motivation\n\nIntrinsic motivation is a type of reward signal that is generated by the agent itself, rather than by the environment [109]. Intrinsic motivation can be used to encourage the agent to explore the environment and discover new skills. The integration of intrinsic motivation into HIRL can improve the exploration efficiency and robustness of RL, especially in sparse reward environments [110].\n\n5.3 Development of Off-Policy HIRL Algorithms\n\nOff-policy RL algorithms are RL algorithms that can learn from data generated by any policy, rather than only from data generated by the current policy. The development of off-policy HIRL algorithms can improve the sample efficiency and stability of RL, as the agent can learn from a larger amount of data generated by different policies [111].\n\n5.4 Theoretical Understanding of Hierarchical Reinforcement Learning\n\nA theoretical understanding of hierarchical reinforcement learning still a huge open issue [112]. Although many algorithms are well-accepted, it has not been formalized very well on the theoretical part. With this limitation, it is difficult to design a high-performance algorithm and to analyze the convergence of the existed algorithms.\n\n5.5 Large-Scale Applications for Decision Making\n\nCurrently, most HIRL algorithms are conducted in relatively small scenarios. However, applications such as traffic scheduling and board game playing are complex for one single computer to process, and it calls for distributed computing [113]. To solve these problems, algorithms are designed to be able to be trained in a distributed and parallel way.\n\n5.6 Explainability for Artificial Intelligence\n\nThough artificial intelligence has boosted the decision making, there are still open concerns over the decision-making process [114]. As an important and basic property, the interpretability makes researchers learn how the strategy is established. What's more, it makes the models more credible for users who could easily follow the decision-making process. It calls for both good performance and explainability for better industrialization."}, {"title": "6. Conclusion", "content": "In this survey, we provided a comprehensive review of Hierarchical Imitation and Reinforcement Learning (HIRL), covering its theoretical foundations, algorithms, and applications. We also discussed the challenges and future directions of HIRL, such as the design of effective hierarchical structures, the integration of intrinsic motivation, and the development of off-policy HIRL algorithms.\n\nHIRL is a promising learning paradigm that combines the strengths of IL and RL by learning a hierarchy of decision making policies. This hierarchical structure can improve the exploration efficiency and scalability of RL, as well as the robustness and generalization ability of IL. We believe that HIRL has the potential to solve complex, real-world decision making problems in various domains, such as robotics, game playing, and autonomous driving.\n\nWe hope that this survey will provide a useful resource for researchers and practitioners who are interested in HIRL. We also hope that this survey will stimulate further research in this exciting and promising area."}]}