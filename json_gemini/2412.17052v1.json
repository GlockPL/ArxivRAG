{"title": "ViLBias: A Framework for Bias Detection using Linguistic and Visual Cues", "authors": ["Shaina Raza", "Caesar Saleh", "Emrul Hasan", "Franklin Ogidi", "Maximus Powers", "Veronica Chatrath", "Marcelo Lotif", "Roya Javadi", "Anam Zahid", "Vahid Reza Khazaie"], "abstract": "The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces ViLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect linguistic and visual biases in news content, addressing the limitations of traditional text-only approaches. Our contributions include a novel dataset pairing textual content with accompanying visuals from diverse news sources, and a hybrid annotation framework, combining LLM-based annotations with human review, enhancing quality while reducing costs and improving scalability. We evaluate the efficacy of LLMs and VLMs in identifying biases, revealing their strengths in detecting subtle framing and text-visual inconsistencies. Empirical analysis demonstrates that incorporating visual cues alongside text enhances bias detection accuracy by 3-5%, showcasing the complementary strengths of LLMs in generative reasoning and Small Language Models (SLMs) in classification. This study offers a comprehensive exploration of LLMs and VLMs as tools for detecting multimodal biases in news content, highlighting both their potential and limitations. Our research paves the way for more robust, scalable, and nuanced approaches to media bias detection, contributing to the broader field of natural language processing and multimodal analysis.", "sections": [{"title": "1. Introduction", "content": "The rapid development of Large Language Models (LLMs), such as OpenAI GPT series and Meta LLAMA, has revolutionized natural language processing (NLP) Zhao et al. (2023). These models exhibit remarkable capabilities in generating coherent text and understanding complex language patterns. More recently, efforts to integrate LLMs with other modalities, such as images, audio, and video, have led to the emergence of multimodal LLMs Yin et al. (2024). These systems excel in tasks that require joint inference from textual and visual inputs, including image captioning, visual question answering (VQA), and content moderation Zhang et al. (2024).\nIn the media domain, bias detection is an increasingly pressing issue Raza et al. (2024b). Media outlets often employ subtle linguistic framing and selective visuals to influence public perception and trust AllSides (2024). While traditional text-only approaches to bias detection are capable of identifying linguistic patterns in phrasing and emphasis, they fail to account for the interplay between textual and visual elements. Multimodal LLMs, by contrast, offer the potential for more holistic and nuanced bias detection ?. These systems can analyze how written narratives interact with accompanying visuals to frame events, emphasize particular viewpoints, or marginalize others. While extensive research exists on fake news detection, including multimodal approaches Tufchi et al. (2023), biased news detection remains comparatively underexplored. This study focuses on biased news detection while also reviewing literature that spans both domains.\nDespite this promise, significant challenges remain in deploying multimodal systems for bias detection. One of the key hurdles is the lack of high-quality, labeled data for training and evaluation Tan et al. (2024). While expert annotations are often the gold standard, they are expensive and time-intensive to produce. Crowdsourcing offers a scalable alternative but is prone to noise and lacks the nuanced understanding required for detecting subtle biases?. Fully automated systems, meanwhile, struggle to capture the contextual and subjective nature of bias, which often requires human-like judgment and interpretability Tan et al. (2024).\nIn this paper, we propose a framework ViLBias (A Framework for Bias Detection using Linguistic and Visual Cues) to explore the potential of LLMs and Vision-Language Models (VLMs) as annotators for detecting multimodal"}, {"title": "2. Related Work", "content": "The study of fake news can be categorized into disinformation, misinformation Santos-D'Amorim and de Oliveira Miranda (2021), and, less frequently, bias detection Raza et al. (2024a). Research in this area often employs SLMs and LLMs, treating it as a classification task. SLMs are compact,"}, {"title": "3. Methodology", "content": "Our framework ViLBias is shown in Figure 1 that shows three main stages: (1) Data Sourcing and Preprocessing, (2) Annotations via LLMs and VLM and Human Validation, and (3) Evaluations."}, {"title": "3.1. Data Sourcing and Preprocessing", "content": "We selected a range of reputable and widely recognized news organizations for data collection, guided by previous work on media bias and source credibility AllSides (2024); YouGov (2024). Our chosen outlets\u2014CNN, BBC, The New York Times, and Al Jazeera\u2500reflect a broad spectrum of geographic coverage, editorial policies, and audience trust profiles. Appendix 9 provides complete details of all selected sources.\nData were collected between May 6, 2023, and September 6, 2024, enabling us to capture evolving patterns of disinformation amid changing global events. To ensure compliant and ethical data practices, we adhered strictly to relevant privacy regulations, conformed to website terms of service, and followed the guidelines presented in University of Waterloo (2024).\nPreprocessing steps aimed at enhancing the dataset's quality and focus. We removed duplicate articles to eliminate redundancy and filtered out"}, {"title": "3.2. Annotations", "content": "To efficiently annotate a large and complex dataset, we integrated automated annotators in two categories: (1) LLMs for textual annotations, and (2) VLMs for multimodal (text-image) labels. The prompt templates used for text-based annotations are provided in Appendix D.\nA key novelty of our approach is the use of a two-step voting mechanism to increase annotation reliability. Each LLM was queried three times per item to address stochastic variation that is inherent in LLMs Bender et al. (2021), and the annotation from each LLM was determined by majority vote. Then, with multiple LLMs (we take 3 for each LLM and VLM), the final annotation was decided by a second majority vote across the models. Formally, if \\(A_m\\) is the set of annotations from model m after querying three times, the final annotation \\(A_{final}\\) is:\n\\(A_{final} = arg \\underset{\\alpha\\E A}{max} \\sum_{m \\E M} {\\mathbb{1}(A_m = \\alpha)}\\),\nwhere M is the set of odd number of LLMs used for annotation, A is the set of all possible annotations, and \\(A_m\\) is determined as:\n\\(A_m = arg \\underset{\\alpha}{argmax} \\sum_{i=1}^{3} {\\mathbb{1}(A_m^{(i)} = \\alpha)},\\)\nwith \\(A_m^{(i)}\\) being the i-th query response from model m. The indicator function \\({\\mathbb{1}(A_m^{(i)} = \\alpha)}\\) equals 1 if the annotation matches \u03b1, and 0 otherwise. This voting approach ensures robust final annotation decisions by mitigating stochastic variability within and across models.\nHuman Validation and Quality Assurance. Though automated methods streamline annotation, human oversight remains essential for validating model outputs and calibrating the annotation pipeline. We enlisted 12 reviewers, including three undergraduates, four master's students, two Ph.D. holders, and three other advanced researchers from the fields of computer science, media studies, political science, and linguistics. Two subject-matter experts led this process, drawing on a set of bias identification guidelines outlined in Appendix B. They ensured that reviewers considered various types of biases in linguistics and visual cues, as defined by the U.S. Equal Employment Opportunity Commission U.S. Equal Employment Opportunity Commission (EEOC) (2024).\nReviewers assessed a stratified sample of 10,000 annotations, classifying them as Acceptable, Needs Improvement, or Incorrect, and provided written rationales where warranted. This feedback loop informed iterative improvements to model prompts and annotation strategies. Disagreements were resolved through consensus discussions. After refining our approach, human-model consistency, as measured by Inter-Annotator Agreement (IAA), reached 72%, reflecting a substantial alignment between automated labels and expert judgments.\nFinal Dataset. The final dataset is organized to support analyses of bias across textual and multimodal dimensions. Each article is assigned a stable, non-identifying unique identifier derived from the first ten digits of the SHA256 hash of its headline. Core fields include: outlet, headline, article text,"}, {"title": "4. Experiments", "content": "This study addresses the following research questions (RQs): RQ1: How accurate are annotations generated by LLM-based systems? RQ2: How do"}, {"title": "5. Results", "content": "Our evaluation reveals several key insights:\nText-only models: Table 4 summarizes the performance of SLMs and LLMs for text classification. The result shows that ROBERTa-base demonstrated the best performance across all metrics with fine-tuning (FT), achieving an F1 score of 88.54% and accuracy of 89.50%. Models such as BERT and DistilBERT also performed commendably but were slightly inferior, reflecting their simpler architectures and fewer parameters.\nMultimodal models: Table 5 presents the comparative performance of SLMs and LLMs/VLMs for multimodal classification. The result shows that BERT + CLIP achieved the highest F1 score (77.15%) and accuracy (84.20%) under the fine-tuning configuration among small models. Large models like LLama3.2-11B-Vision-Instruct showed significant improvements, particularly with instruction fine-tuning (IFT), reaching an F1 score of 81.04% and accuracy of 82.50%.\nKey takeaway: Fine-tuning, especially instruction-based has shown performance gains across both language and vision-language models. While"}, {"title": "5.3. Ablation Studies", "content": "To better understand which components and training configurations most significantly impact model performance, we conducted a series of ablation experiments on fine-tuned both text-only and multimodal classification tasks. Specifically, we focused on three main aspects: data size, fine-tuning techniques, and multimodal fusion strategies."}, {"title": "5.3.1. Ablation 1: Training Data Size", "content": "We varied the size of the training dataset for a representative LLM (LLama3.2-1B-Instruct) and a SLM (RoBERTa-base) to examine how data quantity influences performance. The results in Figure 4 show that LLama3.2-1B outperforms RoBERTa-base in both F1 score and accuracy across all"}, {"title": "5.3.2. Ablation 2: Fine-Tuning Techniques (Parameter-Efficient Methods)", "content": "Fine-tuning LLMs such as LLama3.2-1B and multimodal models like LLama3.2-11B-vision-Instruct can be computationally expensive due to the need to update all parameters during training. To address this, parameter-efficient fine-tuning (PEFT) Ding et al. (2023) methods like Low-Rank Adaptation (LoRA) and Prefix-Tuning aim to reduce computational overhead by updating only a small subset of model parameters. Full fine-tuning (FT) updates all model parameters during training, requiring high computational and memory resources but generally achieving the best performance. LoRA introduces low-rank trainable layers to the frozen pretrained weights, allowing significant parameter savings while maintaining performance. Prefix-Tuning appends trainable prefix embeddings to the input representations, leaving the main model frozen and thus reducing computational demands.\nFor this experiment, we used LLama3.2-1B-Instruct for text-only tasks and LLama3.2-11B-vision-Instruct for multimodal tasks (text + image). The models were trained on 80% of the dataset, with 20% held out for evaluation. The evaluation metrics used are pecision, recall, F1 score, and accuracy. The goal was to evaluate if these parameter-efficient methods could achieve competitive performance compared to full fine-tuning."}, {"title": "5.4. Qualitative Analysis and Bias Identification", "content": "This section examines biases in model predictions by analyzing LLMs and VLMs. The LLMs outputs were evaluated based on their reasoning when queried to explain their predictions. We used our fine-tuned Llama3.2 (1B and 11B), Mistral-v0.3-7B and Phi3 -vision model. Table 7 presents the performance and observed reasoning issues for each model.\nFigure 7 shows two LLMs exhibit contrasting tendencies in bias detection. Llama3.2-1B tends to be more sensitive, often over-identifying biases based on stereotypical assumptions. Mistral-7B-v0.3 leans toward underestimating nuanced or systemic biases, frequently categorizing them as neutral."}, {"title": "6. Discussion", "content": "Despite the promising results, this study has some limitations. While the dataset captures a broad spectrum of media sources, it may not fully represent global cultural or linguistic variations, potentially limiting the generalizability of the findings. The framework primarily focuses on explicit biases detectable through textual and visual analysis. Subtle systemic or cultural biases, which require deeper contextual understanding, may remain underrepresented. The reliance on state-of-the-art LLMs and VLMs introduces challenges related to computational overhead and resource accessibility, which may restrict the practical adoption of the framework in low-resource"}, {"title": "6.1. Practical and Social Impact", "content": "The findings of this study have significant practical and social implications. By leveraging advanced language and vision-language models, the proposed framework provides a scalable solution for identifying and addressing biases in multimodal media content. This capability is particularly valuable for organizations working in content moderation, journalism, and public policy, where identifying subtle framing and representational biases can enhance transparency and accountability.\nFrom a social perspective, the framework contributes to promoting equitable media practices by uncovering hidden biases that shape public opinion and perpetuate stereotypes. By enabling stakeholders to detect and mitigate such biases, the work supports efforts to build a more informed and inclusive society. Furthermore, the integration of human-machine collaboration ensures that annotation processes are efficient while maintaining contextual and cultural sensitivity, reducing the risk of algorithmic oversights.\nFuture iterations of this framework could be adopted by educational institutions, media organizations, and regulatory bodies to raise awareness about media bias and encourage responsible AI practices. The long-term societal impact includes fostering trust in digital content, empowering marginalized voices, and supporting fairer decision-making processes in the information ecosystem."}, {"title": "6.2. Limitations", "content": "Despite the promising results, this study has some limitations. While the dataset captures a broad spectrum of media sources, it may not fully represent global cultural or linguistic variations, potentially limiting the generalizability of the findings. The framework primarily focuses on explicit biases detectable through textual and visual analysis. Subtle systemic or cultural biases, which require deeper contextual understanding, may remain underrepresented. The reliance on state-of-the-art LLMs and VLMs introduces challenges related to computational overhead and resource accessibility, which may restrict the practical adoption of the framework in low-resource"}, {"title": "7. Conclusion", "content": "This study introduced ViLBias, a comprehensive framework for detecting bias in multimodal contexts by integrating linguistic and visual cues. Our findings highlight the efficacy of LLMs and VLMs in identifying nuanced biases within textual and visual narratives. By developing a novel multimodal dataset and employing a hybrid annotation approach, we demonstrated the potential of combining human expertise with machine efficiency to improve annotation quality. These advancements contribute to the broader effort of enhancing media transparency and fostering trust in digital content. The results validate the reliability of LLM-based annotations, achieving high performance across metrics such as F1 score and accuracy. Additionally, the proposed annotation framework effectively mitigates model stochastically through a majority-voting mechanism, ensuring robustness in bias classification. Our analysis further underscores the value of instruction fine-tuning and parameter-efficient approaches like LoRA for achieving competitive performance in resource-constrained scenarios."}, {"title": "CRediT authorship contribution statement", "content": "Shaina Raza: Writing - original draft, Validation, Methodology, Investigation, Conceptualization, Supervision. Caesar Saleh: Methodology, Investigation, Evaluation, Data Analysis. Emrul Hasan: Visualization, Methodology, Evaluation, Data Analysis. Franklin Ogidi: Writing, Visualization, Methodology, Data Analysis. Maximus Powers: Writing \u2013 review & editing, Methodology, Evaluation. Veronica Chatrath: Methodology, Editing. Roya Javadi: Methodology, Evaluation. Marcelo Lotif: Data Curation. Anam Zahid: Writing - review & editing, Investigation. Vahid Reza Khazaie: Writing - review & editing, Investigation."}, {"title": "D. Prompts", "content": "Prompt for Text based annotation\nprompt = f\" \" \"\nAnalyze the provided text for evidence of bias. Identify the presence or\nabsence of rhetorical techniques that contribute to bias in the news\ncontent. Provide a structured response to indicate how these\ntechniques are used to influence the narrative.\nText: {text}\nRhetorical Techniques to Identify:\n1. Emotional Appeal: Is the language emotionally charged to evoke strong\nreactions favoring one side?\n2. Exaggeration and Hyperbole: Are claims exaggerated or overstated to\nemphasize one perspective?\n3. Bias and Subjectivity: Does the text reflect polarized language or\nframing that favors one viewpoint?\n4. Repetition: Are specific words, phrases, or ideas repeated to\nreinforce a particular narrative?\n5. Specific Word Choices: Are charged or loaded terms used to subtly or\novertly indicate bias?\n6. Selective Presentation: Are facts or perspectives cherry-picked to\nhighlight one side while ignoring others?\n7. Omission of Context: Are important details or broader contexts omitted\nto skew the narrative?\n8. Appeals to Authority: Are certain experts or authorities cited\nselectively to bolster a specific viewpoint?\n9. Framing: Is the issue presented in a way that minimizes alternatives\nor discredits opposing perspectives?\n10. Logical Fallacies: Are flawed arguments, such as straw man, ad\nhominem, or false dilemmas, employed to strengthen one side?\n11. Tone: Is the tone neutral, or does it reflect sarcasm, disdain, or\nexcessive praise for one perspective?\n12. Imbalance in Representation: Are opposing views underrepresented or\npresented in a less favorable manner?\n13. Use of Statistics or Data: Are statistics manipulated or selectively\npresented to support one narrative?\nProvide your analysis in the following format:\nEmotional Appeal: [Present/Absent]\nExaggeration and Hyperbole: [Present/Absent]\nBias and Subjectivity: [Present/Absent]\nRepetition: (Present/Absent]\nSpecific Word Choices: [Present/Absent]\nSelective Presentation: [Present/Absent]\nOmission of Context: [Present/Absent]\nAppeals to Authority: [Present/Absent]\nFraming: [Present/Absent]\nLogical Fallacies: [Present/Absent]\nTone: [Neutral/Biased]\nImbalance in Representation: [Present/Absent]\nUse of Statistics or Data: [Present/Absent]\nKeep your evaluation concise and structured, without additional\nexplanations.\"\nPrompt for Text and image based annotation\nprompt\n= f\"\",\"Analyze the provided text and associated image for bias. Evaluate how the\ntext and image interact to reinforce or contradict the narrative and\nidentify techniques used to promote a specific perspective.\nText: {text}\nImage Description: {image_description}\nEvaluation Criteria:\n1. Emotional Appeal: Does the text or image evoke strong emotions\nfavoring one side?\n2. Exaggeration and Hyperbole: Are claims or visual elements exaggerated\nto dramatize one perspective?\n3. Bias and Subjectivity: Does the combination of text and image indicate\nclear alignment with one viewpoint?\n4. Repetition: Are keywords, themes, or visual motifs repeated to\nreinforce a particular stance?\n5. Specific Word Choices or Visual Cues: Are emotionally charged words or\nsymbolic imagery used to promote bias?\n6. Selective Presentation: Are key facts highlighted while opposing\ninformation is omitted or understated?\n7. Omission of Context: Is critical background information missing in the\ntext or image captions?\n8. Appeals to Authority: Are the text or visuals aligned with selectively\nchosen or questionable authorities?\n9. Framing: Is the issue framed to promote one perspective and discredit\nalternatives (e.g., through cropping, captions)?\n10. Logical Fallacies: Are there flawed arguments or visual manipulations\n(e.g., misleading captions, doctored images)?\n11. Tone: Is the tone in the text or visual style neutral, critical, or\nexcessively favorable?\n12. Imbalance in Representation: Are opposing perspectives\nunderrepresented or negatively depicted?\n13. Use of Symbolism: Are symbols, colors, or motifs in the image used to\nmanipulate perception?\n14. Use of Statistics or Data: Are charts, numbers, or data selectively\npresented or exaggerated?\nProvide your analysis in the following format:\nEmotional Appeal: [Present/Absent]\nExaggeration and Hyperbole: [Present/Absent]\nBias and Subjectivity: [Present/Absent]\nRepetition: (Present/Absent]\nSpecific Word Choices or Visual Cues: [Present/Absent]\nSelective Presentation: [Present/Absent]\nOmission of Context: (Present/Absent]\nAppeals to Authority: [Present/Absent]\nFraming: [Present/Absent]\nLogical Fallacies: [Present/Absent]\nTone: [Neutral/Biased]\nImbalance in Representation: [Present/Absent]\nUse of Symbolism: [Present/Absent]\nUse of Statistics or Data: [Present/Absent]\nKeep your evaluation precise and avoid unnecessary explanations.\",\""}, {"title": "6. Discussion", "content": "contexts within images. This suggests a need for more balanced multimodal reasoning across models."}]}