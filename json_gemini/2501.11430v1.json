{"title": "A Survey on Diffusion Models for Anomaly Detection", "authors": ["Jing Liu", "Zhenchao Ma", "Zepu Wang", "Yang Liu", "Zehua Wang", "Peng Sun", "Liang Song", "Bo Hu", "Azzedine Boukerche", "Victor C.M. Leung"], "abstract": "Diffusion models (DMs) have emerged as a powerful class of generative Al models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we systematically review recent advances in DMAD research and investigate their capabilities. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at https://github.com/fdjingliu/DMAD.", "sections": [{"title": "1 Introduction", "content": "The ever-increasing volume, velocity, and variety of data present both opportunities and challenges for anomaly detection (AD). In cybersecurity, real-time threat detection is crucial given the constant influx of network traffic and logs [Liu et al., 2024b]. Similarly, robust fraud detection systems are essential for financial institutions processing massive transaction datasets [Wang et al., 2023a]. Healthcare and manufacturing also rely heavily on data for early disease diagnosis and predictive maintenance, respectively [Zhang et al., 2023]. Such applications highlight the growing need for automated AD methods to efficiently identify outliers in complex datasets. However, traditional techniques, often based on statistical methods or rule-based systems, struggle with the scale and complexity of modern data [Katsuoka et al., 2024]. Specifically, these methods often require extensive manual feature engineering and struggle to adapt to evolving data distributions. Additionally, the sheer data volume can overwhelm traditional methods, rendering them computationally infeasible for real-time applications. Consequently, more sophisticated and scalable AD approaches are needed. For example, weakly supervised approaches leverage limited labeled data to improve performance [Liu et al., 2024c], driving innovation toward scalable deep learning architectures that address these fundamental challenges.\nDiffusion models (DMs) have emerged as a powerful class of generative models, synthesizing high-quality samples across diverse data modalities [Wang et al., 2023c]. Unlike generative adversarial networks (GANs) and variational autoencoders (VAEs), which may exhibit training instability or produce less detailed samples [Dao et al., 2024], DMs generate sharper, more realistic samples through a gradual denoising process [Chen et al., 2024b]. Consequently, DMs excel in capturing a broader range of data distributions compared to GANs [Wyatt et al., 2022], making them particularly suitable for anomaly detection, where their enhanced mode coverage facilitates precise modeling of normal patterns and subsequent identification of distributional outliers [Li et al., 2024].\nRecent growth in complex and multi-dimensional data has created new challenges for anomaly detection methods. In this context, DMs have emerged as a promising solution due to their inherent connection to density estimation [Le Lan and Dinh, 2021]. DMs learn the probability distributions of normal data through iterative denoising. Leveraging their exceptional generative abilities, DMs accurately reconstruct normal patterns while capturing underlying manifold structures, enabling anomaly detection through reconstruction error analysis and probability density estimation [Hu and Jin, 2023]. For example, AnoDDPM [Wyatt et al., 2022] and ODD [Wang et al., 2023b] leverage these reconstruction capabilities for anomaly identification. Additionally, the learned score function, representing the gradient of the log-probability density, can be directly used as an anomaly score, as in diffusion time estimation (DTE) [Livernoche et al., 2023].\nWhile some existing surveys have provided insights into anomaly detection [Liu et al., 2024b; Liu et al., 2024c] and diffusion models [Luo, 2023; Yang et al., 2024b] techniques, there has been no effort to systematically examine the emerging role of diffusion models for anomaly detection (DMAD) in both domains. To address this gap, we provide a comprehensive examination of DMAD, covering fundamental principles, methodologies, and diverse tasks. Specifically, we first introduce the theoretical foundations of DMs, including forward processes, reverse processes, and training objectives, alongside score-based models and their variants. Subsequently, we present a systematic taxonomy of DMAD methods, categorizing them into reconstruction-based, density-based, and hybrid approaches, while analyzing their respective strengths and limitations. We then explore tasks across various domains, including image anomaly detection (IAD), time series anomaly detection (TSAD), video anomaly detection (VAD), and multimodal anomaly detection (MAD). Additionally, we discuss critical challenges and future research directions such as computational costs, interpretability, robustness against adversarial attacks, complex data distributions, edge-cloud collaboration, and integration with large language models (LLMs). The primary goal of this survey is to provide researchers and practitioners with a comprehensive understanding of DMAD and encourage further advancement in this rapidly evolving field. We organize the remainder of DMAD and provide an intuitive navigation map shown in Fig. 1."}, {"title": "2 Preliminaries", "content": "This section begins by providing the fundamental concepts of anomaly detection. Following this, we introduce the various types of anomalies, including point, contextual, and collective anomalies. Next, we discuss the basic principles of DMs and their relevance to anomaly detection. We then explore score-based models and score matching techniques. Finally, we present prominent variants of DMs used in anomaly detection, highlighting their advantages and disadvantages."}, {"title": "2.1 Anomaly Detection", "content": "Anomaly detection aims to identify instances deviating significantly from the expected data distribution [Wang and Vastola, 2023]. Given a dataset $D = \\{x_1, x_2, ..., x_n\\}$, where $x_i \\in \\mathbb{R}^d$, the goal is to identify the anomalous subset $A \\subset D$. Anomalous behavior is characterized by low probability density under the typically unknown distribution $p(x)$. Several anomaly types exist, including point anomalies [Hu and Jin, 2023] that are individual deviant instances; contextual anomalies [Sui et al., 2024] that are anomalous within a specific context; and collective anomalies that are groups of instances anomalous only when considered together. Common evaluation metrics include AUC [Xu et al., 2023], measuring the classifier's ability to distinguish between normal and anomalous instances. Additionally, precision, recall, and F1-score assess the trade-off between correct identification and minimizing false alarms. The choice of metric depends on the application and the relative importance of minimizing false positives and negatives [Liu et al., 2024c]."}, {"title": "2.2 Diffusion Models", "content": "DMs constitute a class of latent variable models defined by forward and reverse Markov processes to corrupt and denoise data, with training based on the variational lower bound of the negative log-likelihood.\nForward Process. Forward process is known as the diffusion process, iteratively adds noise to data $x_0$ over $T$ timesteps, transforming its distribution into an isotropic Gaussian. Given a variance schedule $\\{\\beta_t\\}_{t=1}^T$, where $0 < \\beta_1 < ... < \\beta_T < 1$, the process generates latent variables $x_1, ..., x_T$ via a Markov chain with the Gaussian transition kernel:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I)$.\nHere, $\\mathcal{N}(x; \\mu, \\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$, and $I$ is the identity matrix. The parameter $\\beta_t$ controls the noise added at each step. As $t$ increases, $x_T$ approaches Gaussian noise. Due to the Markov property, the joint distribution is:\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$.\nConsequently, any $x_t$ can be directly sampled from $x_0$:\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$,\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$, which provides computational benefits. Related work explores this process through various perspectives, including multiplicative transitions, uniformization techniques for convergence analysis, Gaussian process covariance transformations, tensor network modeling, first-passage time statistics, and infinite-dimensional diffusion processes [Kumar et al., 2023].\nReverse Process. Reverse process aims to learn the conditional probability distribution $p_{\\theta}(x_{t-1}|x_t)$ to recover the original data distribution by iteratively removing noise. It is typically modeled as a Gaussian distribution:\n$p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$,\nwhere $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ are the predicted mean and covariance, respectively, parameterized by a neural network (e.g., U-Net [Yan et al., 2024] or Transformer [Cao et al., 2024]) and conditioned on the noisy input $x_t$ at timestep $t$. While the covariance $\\Sigma_{\\theta}(x_t, t)$ can be learned, a common simplification is to use a fixed, time-dependent variance [Salimans and Ho, 2021]. The neural network learns to predict the mean $p_{\\theta}(x_t, t)$, which guides denoising. Several parameterization strategies exist, including predicting the mean of $q(x_{t-1}|x_t, x_0)$, added noise $\\epsilon_t$, or original data $x_0$. Starting from Gaussian noise $x_T$, the reverse process iteratively applies $p_{\\theta}(x_{t-1}|x_t)$ until $x_0$ is reached.\nTraining Objective. DMs are trained by maximizing the likelihood of observed data. However, due to the latent variable formulation, direct optimization is intractable. Consequently, training relies on maximizing a variational lower bound (VLB) of the data log-likelihood, derived using variational inference [Bercea et al., 2023] as:\n$L_{vlb} = \\mathbb{E}_q[D_{KL}(q(x_T|x_0)||p_{\\theta}(x_T))] - \\log p_{\\theta} (x_0|x_1) + \\mathbb{E}_q[\\sum_{t=2}^T D_{KL} (q(x_{t-1}|x_t, x_0)||p_{\\theta}(x_{t-1}|x_t))]$,\nwhere $q$ and $p_{\\theta}$ denote the forward and learned reverse process distributions, respectively. Here, $D_{KL}$ is the Kullback-Leibler divergence, $x_0$ represents the original data, and $x_t$ is the noisy sample at timestep $t$. A simplified objective minimizes the $L_2$ distance between true and predicted noise [Han et al., 2024b]:\n$L_{simple} = \\sum_{t=1}^T \\mathbb{E}_q [|| \\epsilon_t - \\epsilon_{\\theta}(x_t, t)||^2]$,\nwhere $\\epsilon_t$ and $\\epsilon_{\\theta} (x_t, t)$ denote the true and predicted noise, respectively. Such simplified objective exhibits computational efficiency and effectiveness [Livernoche et al., 2023]. Additionally, weighting loss terms at different timesteps can improve performance [Choi et al., 2022]. From another perspective, DMs can be viewed as distribution estimators with theoretical guarantees, and the training objective can be based on score matching, where the model estimates the score function [Zhang and Pilanci, 2024]. A unified framework for discrete and continuous-time DMs offers further insights into objective optimization [Lee et al., 2023a]."}, {"title": "2.3 Score-based Models and Score Matching", "content": "Score Matching for Learning Score Functions. Score-based generative models learn the score function $\\nabla_x \\log p(x)$, representing the gradient of the log-probability density. By encoding distributional characteristics directly through these gradients, score functions enable powerful generative capabilities without explicit density estimation. Score matching offers a practical way to learn this score function without needing $p(x)$ explicitly. Instead, it minimizes the Fisher divergence between the learned and true score functions [Han et al., 2024b]. Specifically, the score matching objective is $L_{SM}(\\theta) = \\mathbb{E}_{p(x)} [||\\nabla_x \\log p_{\\theta} (x) - \\nabla \\log p(x)||^2]$.\nDenoising Score Matching. However, computing the true score is often intractable. Consequently, denoising score matching perturbs data with Gaussian noise and learning the score of this perturbed distribution. The objective becomes $L_{DSM}(\\theta) = \\mathbb{E}_{q_{\\sigma}(x)} [||\\nabla_x \\log p_{\\theta} (x) - \\nabla_x \\log q_{\\sigma}(x)||^2]$, where $q_{\\sigma}(x)$ is the perturbed distribution. Learning scores at various noise levels allows DMs to capture the data distribution at different scales [Wang et al., 2023d].\nAdvanced Techniques. Recent research highlights the inherent linear structure within score-based models, particularly at higher noise levels, potentially accelerating sampling [Wang and Vastola, 2023]. Prioritizing specific noise levels during training, based on perceptual relevance, can further improve performance [Choi et al., 2022]. In addition, gradient guidance, incorporating an external objective function's gradient during sampling, adapts pre-trained models to specific tasks [Tur et al., 2023b]."}, {"title": "2.4 Variants of Diffusion Models", "content": "In this section, we discuss prominent diffusion model variants for anomaly detection, including denoising diffusion probabilistic models (DDPMs), denoising diffusion implicit models (DDIMs), and score-based generative models with stochastic differential equations (Score SDEs), along with their advantages and disadvantages.\nDDPMs. DDPMs [Bercea et al., 2023] gradually add Gaussian noise to data over time steps via a Markov chain, which is then reversed by a learned neural network, which effectively learns complex distributions, making it suitable for anomaly detection [Wyatt et al., 2022]. However, generating samples can be computationally expensive.\nDDIMs. DDIMs [Xu et al., 2023] extend DDPMs with a non-Markovian sampling approach, enabling faster generation by skipping steps. Specifically, they modify the reverse process, and by setting a specific parameter to zero, DDIMs allow deterministic sampling, beneficial for consistent reconstructions in anomaly detection [Tebbe and Tayyub, 2024].\nScore SDEs. Score SDEs [Deveney et al., 2023] model diffusion as a continuous-time SDE. The score function, representing the gradient of the log-probability density, can be used for anomaly scoring [Li et al., 2024]. While offering flexibility for complex distributions [Livernoche et al., 2023], training and parameterization choices can be challenging [Graham et al., 2023]."}, {"title": "3 Methodologies", "content": "Reconstruction-based Anomaly Detection (RAD) leverages DMs to identify deviations from learned data distributions. Within this paradigm, we consider three reconstruction strategies, as illustrated in Fig. 2. 1) basic reconstruction, where reconstruction error serves as the anomaly score; 2) latent space reconstruction (LSR), incorporating dimensionality reduction for efficiency with high-dimensional data; 3) conditional reconstruction, leveraging auxiliary information like class labels for potentially more refined anomaly detection."}, {"title": "3.1 Reconstruction-based Anomaly Detection", "content": "Basic Reconstruction. RAD hinges on the principle that DMs trained on normal data reconstruct normal instances effectively, but struggle with anomalous ones [Hu and Jin, 2023]. Specifically, after training on normal samples, the model reconstructs a test sample by reversing the diffusion process. The difference between the input and its reconstruction constitutes the anomaly score [Zhang et al., 2023]. A higher score indicates a greater likelihood of an anomaly. For instance, GLAD [Yao et al., 2025] employs global and local reconstruction perspectives. In contrast, AutoDDPM [Bercea et al., 2023] focuses on improving normal region reconstruction. Alternatively, some approaches use a learned similarity network for comparison, yielding a semantic distance as the anomaly score [Wang et al., 2023b].\nLSR. RAD leverages latent space representations for enhanced efficiency and effective handling of high-dimensional data, such as images or videos. Specifically, projecting input data into a lower-dimensional latent space significantly reduces the computational burden associated with reconstruction. Autoencoders (AEs) are commonly employed for this dimensionality reduction, which learns a compressed representation by encoding data into a latent space and then decoding it back to the original space. Integrating AEs with DMs offers a promising approach. For example, DMs trained on AE-learned latent representations can capture the normal data distribution in the compressed space [Hu and Jin, 2023]. In addition, NGLS-Diff [Han et al., 2024a] is a novel approach that utilizes DM within a normal gathering latent space to enhance anomaly detection capabilities for time series data. The latent space reconstruction error then serves as an anomaly score, with deviations indicating potential anomalies [Wang et al., 2023c]. Shared latent spaces across multiple AEs can separate treatment and context. Similarly, supervised dimensionality reduction can optimize the latent space for specific classification tasks, and employing Brownian motion within Diffusion VAEs captures dataset topology [Le Lan and Dinh, 2021].\nConditional Reconstruction. Conditional information, such as class labels, masks, or text descriptions, guides the diffusion model's reconstruction toward expected normal outputs. For example, FDAE [Zhu et al., 2024] utilizes foreground objects and motion information as inputs to train a conditional diffusion autoencoder, enhancing the detection of anomalous regions in an unsupervised manner. Dual conditioning [Zhan et al., 2024] improves multi-class anomaly detection by ensuring prediction and reconstruction accuracy within the expected category. A learnable encoder in RecDMs [Xu et al., 2023] extracts semantic representations for conditional denoising, thus guiding recovery and avoiding trivial reconstructions. Similarly, GLAD [Yao et al., 2025] introduces synthetic anomalies during training to encourage the model to learn complex noise distributions for improved anomaly-free reconstruction. In addition, [Tebbe and Tayyub, 2024] explores dynamic step size computation based on initial anomaly predictions for refined reconstruction. Finally, MDPS [Wu et al., 2024] models normal image reconstruction as multiple diffusion posterior sampling using a masked noisy observation model and a diffusion-based prior, consequently improving reconstruction quality."}, {"title": "3.2 Density-based Anomaly Detection", "content": "This section considers two primary methods for density-based AD (DAD): Score function and DTE, illustrated by their use of the learned probability density to identify outliers.\nScore Function as Anomaly Score. Gradient-based log probability density score functions quantify data point likelihoods within learned distributions [Zhang and Pilanci, 2024]. Within low-density regions, higher score magnitudes indicate potential anomalies by leveraging the inherent density estimation capabilities of DMs [Livernoche et al., 2023], while probability density characterization through score computation enables direct anomaly assessment without requiring explicit reconstruction or comparative analysis. Theoretical foundations established by [Wang and Vastola, 2023] and [Han et al., 2024b] validate this methodology, although recent investigations [Deveney et al., 2023] illuminate potential disparities between SDE and ODE formulations, warranting careful consideration in score-based anomaly detection frameworks.\nDTE. DTE offers an alternative anomaly detection method leveraging the diffusion process [Livernoche et al., 2023]. Instead of reconstruction, DTE estimates the diffusion time $t$ required for an input data point $x$ to traverse the data distribution. Intuitively, a longer diffusion time suggests an anomaly due to the input's distance from the learned distribution [Luo, 2023]. The estimated diffusion time thus serves as an anomaly score, derived from the mode or mean of its distribution. An analytical form for this density is derivable, and deep neural networks can improve inference efficiency. Consequently, DTE has shown competitive performance, especially in speed, on benchmarks like ADBench, while maintaining or exceeding the accuracy of traditional DDPMs."}, {"title": "3.3 Hybrid Approaches", "content": "Hybrid approaches integrate DMs with other anomaly detection techniques to improve performance. For example, RAD can be combined with density estimation, where reconstruction error from DMs is integrated with density-based anomaly scores, like DTE [Livernoche et al., 2023]. Additionally, dynamic step size computation in the forward process, guided by initial anomaly predictions, improves performance by denoising scaled input without added noise [Tebbe and Tayyub, 2024]. In another approach, likelihood maps of potential anomalies from DMs are integrated with the original image via joint noised distribution re-sampling, enhancing healthy tissue restoration [Bercea et al., 2023]. For TSAD, hybrid methods can employ density ratio-based strategies to select normal observations for imputation, combined with denoising diffusion-based imputation to improve missing value generation, especially under anomaly concentration [Xiao et al., 2023]. Advanced reconstruction techniques predict specific denoising steps by analyzing divergences between image and diffusion model priors, supplemented by synthetic abnormal sample generation during training and spatial-adaptive feature fusion during inference [Yao et al., 2025]. Integrating DMs with Transformers for multi-class anomaly detection, where diffusion provides high-frequency information for refinement, mitigates blurry reconstruction and \"identical shortcuts\" [Zhan et al., 2024]. Finally, a Bayesian framework employing masked noisy observation models and diffusion-based normal image priors enables effective difference map computation between normal posterior samples and the test image, improving anomaly detection and localization [Wu et al., 2024]."}, {"title": "4 Tasks", "content": "In this section, we consider four primary tasks of DMAD, as summarized in Tables 1-4, which present representative methods with open-source implementations, datasets, evaluation metrics, and corresponding results. For each task, methods are categorized based on their underlying principles and detection strategies, highlighting the unique challenges and solutions developed for different data modalities."}, {"title": "4.1 Image Anomaly Detection", "content": "IAD represents a key application area for DMs, focusing on identifying deviations from normality in both global patterns affecting entire images and local anomalies confined to specific regions. The MVTec-AD dataset [Bergmann et al., 2019] serves as a key benchmark for evaluation. Many approaches leverage DMs' reconstruction capabilities, with methods calculating anomaly scores from reconstruction errors after rebuilding images from noisy inputs [Xu et al., 2023]. Advanced frameworks like GLAD [Yao et al., 2025] employ global and local adaptive approaches, while ODD [Wang et al., 2023b] introduces similarity networks for semantic distance measurement. Medical imaging applications have shown particular promise, with FNDM [Li et al., 2023] and mDDPM [Iqbal et al., 2024] demonstrating enhanced efficiency through masked diffusion models, while [Fontanella et al., 2024] introduces novel counterfactual generation techniques. Recent innovations include AnomalyDiffusion [Hu et al., 2024b] for few-shot generation and [Naval Marimont et al., 2024]'s cold-diffusion approach. MDPS [Wu et al., 2024] and DRAD [Sheng et al., 2024] further enhance detection through masked sampling and noise embedding, while methods like [Tebbe and Tayyub, 2024] employ dynamic step size computation for improved localization accuracy."}, {"title": "4.2 Time Series Anomaly Detection", "content": "DMs have emerged as a powerful technique for TSAD, effectively learning complex distributions of sequential data [Yang et al., 2024b]. DMs can identify various anomaly types, including point, contextual, and collective anomalies [Sui et al., 2024]. Advanced approaches like NGLS-Diff [Han et al., 2024a] leverage latent spaces, while methods [Zuo et al., 2024] used structured state space layers explicitly model temporal dependencies. Recent innovations address key challenges: D3R [Wang et al., 2023a] tackles non-stationarity through dynamic decomposition, while TimeDiT [Cao et al., 2024] introduces a foundation model approach with multiple masking schemes. Another key challenge is non-stationarity. DiffAD [Xiao et al., 2023] handle missing data through imputation, complemented by self-supervised techniques [Liu et al., 2024d] and co-evolving strategies [Lee et al., 2023a] for mixed-type temporal data."}, {"title": "4.3 Video Anomaly Detection", "content": "VAD leverages spatio-temporal information to identify unusual events. DMs have emerged as a promising VAD approach due to their ability to learn complex data distributions and generate high-quality reconstructions. For example, [Tur et al., 2023a] investigated unsupervised VAD using DMs, relying on high reconstruction error to indicate anomalies, while [Tur et al., 2023b] enhanced this approach by introducing compact motion representations as conditional information. Several innovative architectures have been proposed to improve detection accuracy: VADiffusion [Liu et al., 2024a] employs a dual-branch structure combining motion vector reconstruction and I-frame prediction, while FDAE [Zhu et al., 2024] introduces a flow-guided diffusion autoencoder with sample refinement for comprehensive detection of both appearance and motion anomalies. Recent advances include [Yan et al., 2023]'s feature prediction diffusion model and [Cheng et al., 2024]'s denoising diffusion-augmented hybrid framework, both enhancing semantic understanding of normal patterns. Furthermore, [Wang et al., 2023d] proposed an ensemble approach using stochastic reconstructions and motion filters, while [Fang et al., 2023] explored masked diffusion with task-awareness for more focused anomaly detection in specific contexts."}, {"title": "4.4 Multimodal Anomaly Detection", "content": "MAD integrates data from various sources to identify deviations from expected behavior. While DMs are relatively new in this area, they hold significant promise through multimodal data integration. For instance, AnomalyXFusion [Hu et al., 2024a] enhances anomaly synthesis by combining image, text, and mask features, while [Flaborea et al., 2023] demonstrates success in skeleton-based VAD through motion-conditioned diffusion models. For IAD, combining visual data with text descriptions improves subtle anomaly detection [Capogrosso et al., 2024], with energy-based approaches [Yoon et al., 2023] leveraging manifold structures for more accurate boundary learning. GLAD [Yao et al., 2025] further suggests extending DMs for multimodal data through specialized architectures, while integration with LLMs offers promising directions for context-aware anomaly detection."}, {"title": "5 Discussion", "content": "Widespread adoption of DMAD remains constrained by substantial computational requirements, particularly when processing high-dimensional data or extended time series. Computational demands manifest during both distribution learning in training and data generation in sampling phases, with the sampling process's iterative nature demanding numerous steps for quality output [Cui et al., 2023]. For example, high-resolution IAD suffers from increased computational overhead due to data volume, model complexity, and the need to capture temporal dependencies [Wyatt et al., 2022]. However, some researchers are actively exploring solutions. One promising direction is faster sampling methods, such as progressive distillation and optimized sampling schedules like align your steps, which aim to reduce sampling steps while maintaining quality [Salimans and Ho, 2021]. Another approach is model compression through techniques like pruning and quantization. Additionally, efficient architectures, like [Cui et al., 2023] leveraging ER SDEs for faster sampling, and preconditioning methods offer further computational gains."}, {"title": "5.1 Computational Cost", "content": "Widespread adoption of DMAD remains constrained by substantial computational requirements, particularly when processing high-dimensional data or extended time series. Computational demands manifest during both distribution learning in training and data generation in sampling phases, with the sampling process's iterative nature demanding numerous steps for quality output [Cui et al., 2023]. For example, high-resolution IAD suffers from increased computational overhead due to data volume, model complexity, and the need to capture temporal dependencies [Wyatt et al., 2022]. However, some researchers are actively exploring solutions. One promising direction is faster sampling methods, such as progressive distillation and optimized sampling schedules like align your steps, which aim to reduce sampling steps while maintaining quality [Salimans and Ho, 2021]. Another approach is model compression through techniques like pruning and quantization. Additionally, efficient architectures, like [Cui et al., 2023] leveraging ER SDEs for faster sampling, and preconditioning methods offer further computational gains."}, {"title": "5.2 Interpretability and Explainability", "content": "Interpretability remains a key challenge for DMAD [Katsuoka et al., 2024]. Understanding DM decision-making is crucial, especially in critical applications. Visualizing anomaly scores, like spatially highlighting anomalous image regions as in reconstruction-based methods [Yao et al., 2025], becomes essential. However, the iterative denoising process inherent in DMs complicates interpretability. Explaining anomaly detection requires identifying and explaining deviating features. Integrating DMs with explainable AI (XAI) techniques, such as incorporating attention mechanisms or leveraging LLMs for textual/visual explanations [Wang et al., 2023b], offers a promising research direction."}, {"title": "5.3 Complex Data Distributions", "content": "A key challenge in applying DMAD lies in handling complex data distributions. Imbalanced datasets, where anomalies are rare, can bias DMs towards the majority class [Yang et al., 2024a], hindering accurate anomaly modeling. Similarly, multimodal datasets, representing distinct normal behaviors, can confound DM learning [Zuo et al., 2024], potentially misclassifying data from less prominent modes. Noisy or missing data further complicates DM training and inference [Choi et al., 2022], as differentiating true anomalies from data imperfections becomes difficult. However, potential solutions exist. Data augmentation techniques can address class imbalance by generating synthetic minority class samples [Yang et al., 2024a]. Robust training methods, such as using semi-unbalanced optimal transport, can enhance DM resilience to noise and outliers [Dao et al., 2024]. Specialized architectures, potentially incorporating mechanisms for handling missing data or modeling multiple modes [Xiao et al., 2023], and dynamic step size computation [Tebbe and Tayyub, 2024] could further improve DM performance on complex distributions."}, {"title": "5.4 Robustness and Adversarial Attacks", "content": "Adversarial robustness is a critical concern for DMAD. Similar to other deep learning models, DMs are vulnerable to adversarial perturbations, raising concerns about the reliability of diffusion-based AD systems [Chen et al., 2024b]. For example, [Kang et al., 2025] demonstrates the potential of DMs for adversarial defense in classification, directly applying such methods to AD can reduce anomaly detection rates. Specifically, the purification process may remove crucial anomaly signals along with noise. Additionally, adversarial examples exhibit misalignment within DM manifolds, offering a potential detection avenue but requiring further investigation. The observed robustness differences between pixel-space diffusion models (PDMs) and more vulnerable latent diffusion models (LDMs) [Graham et al., 2023] further underscore the need to consider specific DM types. Consequently, future research should prioritize robust DM-based AD methods, including architectures and training procedures that distinguish genuine anomalies from adversarial noise. Promising directions include defense strategies like DIFFender [Kang et al., 2025], leveraging text-guided diffusion and the adversarial anomaly perception phenomenon, and incorporating insights from adversarial example behavior within DM manifolds."}, {"title": "5.5 Edge-Cloud Collaboration", "content": "Real-time DMAD faces significant adoption barriers due to their substantial computational demands [Li et al., 2023]. Edge-cloud collaboration offers a promising solution to this challenge by distributing the workload between edge devices and cloud servers. Lightweight DMs deployed on edge devices perform initial anomaly screening, while resource-intensive tasks like full reconstructions are efficiently offloaded to cloud servers [Yan et al., 2024]. In addition, federated learning enables collaborative model training across edge devices and the cloud without sharing sensitive data, thereby enhancing generalization and preserving privacy [Liu et al., 2024b]. Dynamic DM partitioning [Chen et al., 2024a] facilitates adaptive resource allocation, optimizing performance based on network conditions and computational demands. Strategic data placement and distributed DNN deployment principles further enhance system efficiency. The integration of proactive detection mechanisms, exemplified by Maat [Lee et al., 2023b], proves particularly valuable for time-critical applications in cloud monitoring and AIOps environments."}, {"title": "5.6 Integrating with Large Language Models", "content": "Integrating DMs with LLMs offers a promising avenue for enhancing anomaly detection, particularly by generating human-interpretable explanations of detected anomalies and incorporating contextual information [Kumar et al., 2023]. For example, LLMs can leverage textual descriptions to provide context for observed fluctuations, distinguishing genuine anomalies from expected variations for MAD [Capogrosso et al., 2024]. However, current LLM integration for anomaly detection faces a key challenge: effectively representing and tokenizing temporal data. Existing tokenizers, primarily designed for text, may not adequately capture the nuances of numerical and temporal data, potentially hindering performance [Li et al., 2024]. Consequently, a critical research direction involves developing efficient data representation techniques and specialized LLMs for anomaly detection to fully realize this synergistic approach's potential [Tebbe and Tayyub, 2024]."}, {"title": "6 Conclusion", "content": "In this survey, we introduced the core concepts of anomaly detection and diffusion models, providing a foundation for understanding DMAD. We systematically reviewed existing methodologies and their tasks across diverse data types, including image, time series, and multimodal data. Furthermore, we analyzed representative approaches for each data type, highlighting their strengths and limitations. To promote further research, we identified key"}]}