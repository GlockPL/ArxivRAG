{"title": "Movie Trailer Genre Classification Using Multimodal Pretrained Features", "authors": ["Serkan Sulun", "Paula Viana", "Matthew E. P. Davies"], "abstract": "We introduce a novel method for movie genre classification, capitalizing on a diverse set of readily accessible pretrained models. These models extract high-level features related to visual scenery, objects, characters, text, speech, music, and audio effects. To intelligently fuse these pretrained features, we train small classifier models with low time and memory requirements. Employing the transformer model, our approach utilizes all video and audio frames of movie trailers without performing any temporal pooling, efficiently exploiting the correspondence between all elements, as opposed to the fixed and low number of frames typically used by traditional methods. Our approach fuses features originating from different tasks and modalities, with different dimensionalities, different temporal lengths, and complex dependencies as opposed to current approaches. Our method outperforms state-of-the-art movie genre classification models in terms of precision, recall, and mean average precision (mAP). To foster future research, we make the pretrained features for the entire MovieNet dataset, along with our genre classification code and the trained models, publicly available.", "sections": [{"title": "1. Introduction", "content": "The classification of movies into genres serves as a vital navigational tool in the immense catalog of cinematic content, allowing audiences to locate films aligned with their tastes and interests. It also provides a framework for scholars to explore thematic patterns, the evolution of narratives, and cultural shifts reflected in cinematic art. Moreover, for industry stakeholders, accurate genre classification guides key decisions around marketing and distribution strategies. The cinematic world contains an extraordinary blend of visual and auditory stimuli, each film constituting a unique assortment of aesthetics, themes, and narrative structures. However, this incredible diversity makes it challenging to accurately categorize films by genre.\nMovie trailers serve as condensed representations of the full-length film, encompassing an array of vital elements within a brief span. Trailers provide a comprehensive overview of a film's narrative and genre by incorporating pivotal scenes, characters, objects, and vital auditory cues such as dialogue, music, and informative text elements. The availability of movie trailers on online video platforms enables researchers to link movie metadata such as genre to the URL of the movie trailers, creating large datasets for movie trailer genre classification such as MovieNet (Huang et al., 2020) and Moviescope (Cascante-Bonilla et al., 2019).\nIn trailer genre classification, as well as in almost all video classification tasks, deep neural networks (DNNs) constitute the state-of-the-art. The majority of these models use raw data such as pixels and audio waveforms as their inputs. In contrast to older methods that use hand-crafted features, DNNs are expected to extract useful high-level features from raw data in an implicit way within the model layers. These models often contain millions of parameters, requiring significant amounts of time, memory, and training data. Specifically in video classification, the uncompressed size of the raw video frame sequence increases the time and memory requirements even further. Previous works deal with this problem by processing only a fixed and small number of input frames or clips (Huang et al., 2020; Wehrmann & Barros, 2017). However, this approach inherently leads to information loss, potentially compromising classification accuracy. Other works tackle this"}, {"title": "", "content": "issue by using lower-dimensional, non-visual, and non-auditory inputs such as movie posters, plots, and metadata, for movie genre classification (Rodr\u00edguez Bribiesca et al., 2021; Ak et al., 2023; Miyazawa et al., 2022). These approaches are dataset-specific and rely on auxiliary information, failing to classify solely based on video elements, namely on raw video pixels and audio waveforms.\nThe computational complexity of processing raw videos increases further with the adaptation of larger models\u2014a trend that continues to expand over the years (Tan & Le, 2019; Bernstein et al., 2021). When the training data is limited, using larger input dimensionality and larger models also increases the chance of overfitting (Defernez & Kemsley, 1999). The common and very similar ways to deal with these issues are transfer learning, fine-tuning, and using pretrained features (Tan et al., 2018; Niu et al., 2020). These terminologies can be distinguished as follows: Transfer learning involves applying a model trained on one task to a different task. Fine-tuning entails further training the transferred model for the new task. Using pretrained features involves performing inference on the data using a pretrained model, and then feeding the resulting outputs or activations (features) into a new model that is trained. These methods are frequently employed in video processing tasks such as human activity recognition, video summarization, and video recommendation (Ray & Kolekar, 2024; Khan et al., 2024; Almeida et al., 2022).\nUsing pretrained features brings multiple advantages. Firstly, since the pretrained model is used in inference mode, its weights remain frozen, therefore reducing the time and memory complexity by requiring only a forward pass, without an additional backward pass. Secondly, it considerably reduces the size of the input fed to the subsequent model that is being trained. As an example, a very small video frame contains 224 \u00d7 224 \u00d7 3 = 163,968 values, while the state-of-the-art image analysis model CLIP (Contrastive Language-Image Pre-Training) represents an image using only 512 values (Radford et al., 2021). This size reduction becomes more important as we use trailer videos that average around 2 minutes in length, containing a long sequence of frames. Thirdly, assuming that the pretrained features of multimedia analysis models present valuable and relevant information for the task of video classification, their use removes the need to train new models to extract similar features implicitly from raw data. This reduces the size of the subsequent model that needs to be trained. Finally, since it reduces the input size and the number of weights in training, it also reduces the chance of overfitting (Defernez & Kemsley, 1999)."}, {"title": "", "content": "In this work, we focus on classifying movie trailers into genres using pretrained features that we extract entirely from video and audio. To obtain the relevant pretrained features, we exploit the availability of open-source pretrained multimedia analysis models. Specifically, we employ image analysis, optical character recognition, automatic speech recognition, audio tagging, and music classification models to incorporate information about visual scenery, objects, text, speech, music, and audio effects. These pretrained models are used in inference mode, meaning that they are not fine-tuned, resulting in much lower time and memory requirements. Another important strength of our methods is the use of the transformer model to fuse and process the pretrained features and predict the genre of the input video (Vaswani et al., 2017). The transformer model is designed to process long input sequences efficiently while handling long-term dependencies. In our case, we feed the pretrained features that stem from many video frames and audio chunks into the transformer. The small size of the pretrained features and the efficiency of the transformer model enables us to use entire videos while maximizing the available information, and not use a fixed and small number of input frames or audio chunks. Furthermore, since the pretrained models extract meaningful features from raw data, using a shallow transformer with one or two layers as the classifier becomes sufficient. We propose and compare multiple architectures to handle the non-trivial task of merging and processing multimodal pretrained feature sequences with complex correspondences and different temporal lengths and dimensionalities.\nWe implement additional models to reflect some of the works in the literature and to compare against our methods. As done by Huang et al. (2020), we use a temporal averaging module followed by a multi-layer perceptron (MLP) instead of the transformer, to average the pretrained features along the time dimension, and then classify them. Our empirical results show that the transformer outperforms this approach, especially when more input frames are used. We secondly implement a baseline model that works on raw video and audio, to mirror some of the recent models used in trailer genre classification (Rodr\u00edguez Bribiesca et al., 2021). While we empirically show that this approach performs worse than our proposed models, our preliminary experiments also show that it is prone to overfitting when trained end-to-end. Lastly, we compare our results with other models addressing trailer genre classification on the MovieNet dataset, demonstrating a notable performance improvement.\nWe summarize the shortcomings of the previous works in trailer genre"}, {"title": "", "content": "classification and how we address them as the following:\n\u2022 The complexity introduced by the large input space due to the use of raw video and audio: By using pretrained features instead of raw pixels and waveforms, we significantly reduce input complexity, dimensionality, and effectively, the chance of overfitting.\n\u2022 The information loss caused by using a low and fixed number of input frames: Alongside using inputs with lower dimensionality, we utilize the transformer architecture that can process the sequences from the videos in their entirety while handling long-term correspondences.\n\u2022 Inability to classify solely using videos due to dependence on dataset-specific content such as poster, plot, or metadata: The pretrained features that we used are obtained entirely from video and audio. As a result, our models can theoretically be used for any video classification task.\nIn summary, our contributions are as follows:\n\u2022 We propose new strategies for leveraging a variety of pretrained features as inputs and processing them using shallow neural networks for classification capable of handling different input sizes in both channel and temporal dimensions.\n\u2022 By utilizing the transformer model, we leverage all video keyframes and the entire audio, effectively handling videos of varying lengths without the need for picking out a constant number of frames or average pooling. We also quantitatively demonstrate that the classification performance improves steadily with the inclusion of more frames.\n\u2022 We improve the state-of-the-art for genre classification significantly, on the largest movie genre classification dataset, namely MovieNet.\n\u2022 We make available the pretrained features for the MovieNet dataset, as well as our code and trained models."}, {"title": "2. Related work", "content": "In this section, we first explain the cinematic datasets for movie genre classification and further motivate why we have chosen to work with the MovieNet dataset (Huang et al., 2020), and then we discuss the models specifically focused on movie genre classification."}, {"title": "2.1. Cinematic datasets", "content": "MovieLens is one of the earliest cinematic datasets, containing movie ratings and metadata like genre and title (Harper & Konstan, 2016). While the latest version includes 86k movies, it does not include direct links to the movie trailers. The trailers can only be obtained through crawling the Internet Movie Database (IMDb) website, which conflicts with their terms of use. The MM-IMDb (Multimodal IMDb) dataset merges movie posters with the metadata obtained from MovieLens, resulting in 26k movies with posters, plots, genres, and other metadata, while lacking trailers or any types of videos (Ovalle et al., 2017). The LMTD (Large Movie Trailer Dataset) is a collection of features and metadata from 3500 of movie trailers, although the project is currently discontinued and the data is unavailable (Simoes et al., 2016).\nMovieScope is a comprehensive dataset for multi-modal movie analysis, including data like movie trailers, posters, plot synopses, user reviews, and visual-auditory features, belonging to 5k distinct movies (Cascante-Bonilla et al., 2019). The MMTF-14K (Multimodal Movie Trailer Features dataset) provides multimodal features extracted from 14k movie trailers and their metadata such as user reviews and genre (Deldjoo et al., 2018). The Condensed Movies dataset includes full individual scenes, rather than trailers, alongside the plot and the characters, from 4k movies (Bain et al., 2020).\nMovieNet is a large-scale, holistic dataset providing movie, trailer, poster, subtitle, plot, tags, and metadata including genre (Huang et al., 2020). While it contains metadata belonging to 375k movies, trailers are available via YouTube links for 33k movies, making MovieNet the largest cinematic dataset in terms of both metadata and trailers. The work also includes the genre classification performance of several state-of-the-art video classification models, providing strong baselines to compare against our work. Overall, because MovieNet is the largest movie trailer dataset, has direct links to trail-"}, {"title": "2.2. Movie genre classification", "content": "One of the earliest works on movie genre classification obtains keyframes using scene detectors, and extracts hand-crafted visual features such as roughness, ruggedness, and openness, on a privately collected dataset (Zhou et al., 2010). Genre classification is achieved by comparing the distance between the feature vectors from the training and testing sets.\nOne of the first works that use neural networks for movie genre classification also utilizes visual pretrained features (Wehrmann & Barros, 2017), using the LMTD dataset (Simoes et al., 2016). They use pretrained features of classification models that were trained on ImageNet (Deng et al., 2009) and Places365 (Zhou et al., 2018b) datasets, and audio spectrograms. The features from individual frames are fused using a convolution-through-time module, which can be thought of as a standard convolutional neural network (CNN) where the kernel length is equal to the input feature length of each keyframe and the kernel traverses the features belonging to subsequent frames, along the temporal dimension (Wehrmann & Barros, 2017). The kernel length along the temporal dimension is 3, meaning that the model can exploit the correspondence between only 3 frames. Moreover, since the input and output of the CNN have varying numbers of frames, they further take the maximum along the temporal dimension to have a fixed-size vector to feed into the final classification layer. This inevitably leads to a loss of information. Another work exploits the correspondence between facial emotions and cinematic genre, by first extracting human faces from the trailer videos, then classifying their emotions, and finally mapping the emotions to the cinematic genres (Yadav & Vishwakarma, 2020).\nThe work presenting the MovieNet dataset also introduces a model for movie genre classification, alongside the results from other state-of-the-art video classification models (Huang et al., 2020). While the model they introduce does surpass the performance of the state-of-the-art models, it only uses 8 clips, each with 3 frames from the entire trailer. During the inference phase, predictions are made for each individual clip, and these are subsequently averaged to generate the final prediction. This approach, however, fails to account for the long-term correspondence that exists within the trailers. Finally, the MovieCLIP model first trains a scene classification model"}, {"title": "", "content": "and then uses its final activations to feed into an additional genre classification model, specifically working with the Moviescope dataset (Bose et al., 2023).\nSome recent works have used transformers for movie genre classification. Rodr\u00edguez Bribiesca et al. (2021) used transformers to individually process raw video frames and raw audio, and later fused the resulting activations with metadata information and poster. Miyazawa et al. (2022) and Ak et al. (2023) used pretrained transformers to process movie posters and plots, excluding any use of videos, for genre classification on the MM-IMDb dataset (Ovalle et al., 2017). Our work differs from these recent approaches by using pretrained features instead of raw inputs; and by working solely on video and audio, namely, only using raw video pixels and audio waveforms. We also implement a baseline model that is similar to Rodr\u00edguez Bribiesca et al. (2021) which uses raw inputs and we empirically show that it performs worse than our proposed models."}, {"title": "3. Methodology", "content": "Our overall task is to classify each cinematic trailer into its corresponding genres. While the methods in the literature use raw pixels and audio as the input to the classifier model, we exploit learned features created by existing pretrained deep neural networks (DNNs). Since these features are already created by deep and powerful models, we can train shallow models to process these features and predict the final output, enabling us to cut down on the training time and resources, while still making use of semantically relevant features."}, {"title": "3.1. Creating training data", "content": "We used the YouTube trailers in the MovieNet dataset (Huang et al., 2020), choosing the videos with the lowest resolution where the height is at least 300 pixels. We re-encoded the videos using the x265 encoder with a constant rate factor of 23. Using FFmpeg (Tomar, 2006), we detected the scene boundaries and extracted the frames that are exactly in the middle of two scene boundaries. We also re-encoded the audio using the Opus codec, with a bitrate of 48k in mono. For each video, we extract various features using pretrained models and store them so that we can simply load them while training the classification model. We use the genre(s) of each movie from the MovieNet dataset as the ground-truth. Importantly, this dataset"}, {"title": "3.1.1. Feature extraction", "content": "The overall feature extraction pipeline is shown in Figure 1, and each feature is explained below.\nCLIP. Contrastive Language-Image Pretraining (CLIP), is the state-of-the-art model for image understanding, that was pretrained using contrastive learning, using a large number of images with captions available on the internet (Radford et al., 2021). The pretrained model first resizes the image"}, {"title": "", "content": "where the longer side has 224 pixels and further takes a 224 \u00d7 224 center crop. Afterward, the pretrained model encodes each frame to a vector with a length of 512.\nAudiotag. To extract audio features, we employed a model that was pretrained on the task of audio tagging (Kong et al., 2020). This model operates on chunks of 3 seconds and outputs a vector of probabilities for 527 different labels. Since we are interested in encoding the audio into feature vectors, we don't use the final classification layer and extract the activations prior to that, ending up with vectors with a length of 128, for each audio chunk.\nMusicnet. Since we are dealing with cinematic trailers, the soundtrack is an important element that further needs investigation. Hence we extract another audio-related feature, namely the musical feature, using a model that was pretrained on the task of music genre classification (Choi et al., 2016). This model operates on chunks of 22 seconds and outputs a vector of probabilities for 50 different labels. Similarly, we extract the activations prior to the final layer, ending up with vectors with a length of 64, for each audio chunk.\nOCR. We also performed optical character recognition (OCR) on each frame using the PaddleOCR model (PaddlePaddle, 2023). We additionally used a pretrained spell correction model on the produced output (Guhr, 2023). The overall output of this stage is in text format.\nASR. We performed automatic speech recognition (ASR) on the audio using OpenAI's Whisper model (Radford et al., 2023). Since this is a learned model that was trained on natural text, there is no need for post-spell correction. However, on the output of Whisper, we used a pretrained language detection model to filter out non-English text (Papariello, 2022). When the output of ASR was in a non-English language, we simply replaced the entire text to denote the language, e.g. changing it to \u201cGerman language\". Similarly, the output of this model is in text format.\nDistilBERT. We encode the output text of the OCR and ASR models using a pretrained language model, specifically DistilBERT (Sanh et al., 2019), which is a condensed and compressed variant of the BERT (Bidirectional Encoder Representations from Transformers) model (Devlin et al., 2019), achieved through knowledge distillation (Bucila et al., 2006; Hinton et al.,\""}, {"title": "", "content": "2015). DistilBERT utilizes fewer layers than BERT and learns from BERT's outputs to mimic its behavior. This model converts the input text into tokens of words and sub-words, and encodes each one of them as a vector with a length of 768."}, {"title": "3.2. Classification", "content": "We built multiple models to take the previously extracted features of a given video as input and predict the genre of the video. Combining different features is challenging, especially since the encoded vectors have different lengths in both channel and temporal dimensions. For example, considering the audio event and music features, the lengths of each vector are 128 and 64, respectively. Furthermore, since the audio event and music networks operate on chunks of audio with lengths of 3 and 22 seconds respectively, for the same video, there are more embedded vectors for the former. And finally, the number of vectors for the same feature differs between different videos. We addressed these issues using different solutions and different classification models.\nWe built and trained three distinct models: a multi-layer perceptron (MLP); the single-transformer model that integrates features across all modalities; and the multi-transformer model, where individual transformers handle features from specific modalities.\nThe final layer of all our models is a fully-connected (FC) layer with a size of 21, outputting probabilities belonging to 21 different genres. This layer is followed by a sigmoid layer to make sure each output is a probability between 0 and 1. We note that predicted probabilities do not add up to 1 due to the multi-label setting."}, {"title": "3.2.1. MLP", "content": "We first implemented a simple MLP classifier to reflect the state-of-the-art method used by Huang et al. (2020). We note that Huang et al. (2020) used raw video and audio from a few small segments of video, while we use pretrained features stemming from the entire video. Our model can be seen in Figure 2. Since MLPs require a fixed-length input, we averaged the feature vectors from each modality along the temporal dimension similar to Huang et al. (2020). We then concatenated the averaged vectors from different modalities and fed them into the MLP."}, {"title": "3.2.2. Single-transformer", "content": "To address the significant information loss caused by averaging features along the temporal dimension, we utilized a transformer model to leverage short- and long-term correspondences within the video sequence. The transformer is a state-of-the-art sequence processing model, capable of efficiently handling long sequences through its built-in multi-headed attention mechanism (Vaswani et al., 2017). We employed the transformer as a sequence classifier by prepending the sequences with a special learnable vector, called the <CLS> vector. While the transformer generates output vectors for each element of the input sequence, in classification tasks, only the output vector corresponding to the <CLS> vector is forwarded to the next layer, while the others are discarded (Devlin et al., 2019).\nWhile the transformer can handle sequences with varying lengths, the vectors in the sequence still need to have equal sizes along the channel dimension. To ensure that, we used fully-connected (FC) layers to transform feature vectors from different modalities into the same size. We note that this layer is applied to each vector in the sequence individually, hence not changing the number of vectors in its input sequence. Next, we concatenated all the vectors along the time dimension before feeding them into a single transformer. This approach aims at exploiting the correspondences"}, {"title": "3.2.3. Multi-transformer", "content": "With our hypothesis that using a single transformer to process features from multiple modalities can be inefficient due to the increased complexity of the input, we devised our final model that incorporates different transformer models to process features from different modalities, as shown in Figure 4. The inputs of all transformers are prepended with  token vectors, and the corresponding output vectors are concatenated in channel dimension to be fed into the fully-connected layer to obtain the final probabilities. This approach utilizes the potential correspondences between different features at the global level, though not at the temporal level."}, {"title": "3.2.4. Optional temporal averaging", "content": "We optionally averaged the text-related features such as OCR and ASR, along the temporal dimension. Non-textual features, namely CLIP, Audiotag, and Musicnet are obtained from the activations before the final prediction layer. Whereas the OCR and ASR features stem from running the DistilBERT model on the predicted text. Here, any error in the predicted text is propagated into the DistilBERT model, corrupting the output features. In order to reduce the resulting noise, we experimented with averaging the textual features, namely OCR and ASR, along the temporal dimension."}, {"title": "3.2.5. Vision transformer baseline model", "content": "We also implemented a strong baseline model employing state-of-the-art vision transformers (ViT) that work on sequences of 2-dimensional raw visual frames and audio spectrograms, as opposed to the previously introduced models that work with sequences of 1-dimensional pretrained features (Dosovitskiy et al., 2021). ViT segments the image or audio spectrogram into patches, encodes them, and then processes the sequence of encoded vectors using the standard transformer model. While it is purely attention-based, without any convolutional layers, it outperforms CNNs, achieving state-of-the-art results in image (Dosovitskiy et al., 2021), video (Arnab et al., 2021), and audio (Gong et al., 2021) classification.\nWhile our primary objective is video classification, it is important to note that directly applying the video vision transformer (ViViT) (Arnab et al., 2021) to our task is not a suitable approach. There are two main reasons for this. First, ViViT doesn't incorporate audio, which is a crucial aspect of our task. Second, our dataset contains discontinuous frames that depict scenes with no visual continuity between them. As a result, it is more appropriate to handle these frames individually rather than concatenating patches from different frames, as ViViT does.\nWe utilized a standard pretrained vision transformer (ViT) to process images and a modified version of ViT, known as an audio spectrogram transformer (AST), designed and pretrained specifically for handling audio spectrograms (Gong et al., 2021), to process audio data. Our preliminary experiments showed that training the full model led to overfitting issues that couldn't be mitigated through standard techniques like dropout (Srivastava et al., 2014). To avoid this, we kept the parameters of ViT and AST frozen and replaced their output layers with trainable MLPs, naming them image-MLP and audio-MLP."}, {"title": "3.2.6. Training details and hyperparameters", "content": "We applied a filtering process to exclude extremely long and short videos from our dataset. Specifically, we computed the quartiles of video durations and excluded samples that were outside the inner fence, i.e., durations shorter than $Q1 - 1.5 \\times IQR$ (corresponding to 19.6 seconds) and longer than $Q3 + 1.5 \\times IQR$ (corresponding to 214.4 seconds). This filtering yielded 26412 videos from the original set of 32647. Moreover, following the approach in the MovieNet paper (Huang et al., 2020), we limited the labels to the 21 most frequent genres. These labels are shown in Table 2.\nTo ensure an unbiased split of the dataset, we arranged the videos alphabetically by their YouTube IDs\u2500given that these IDs are generated randomly. Following the methodology in the original MovieNet paper, we divided the dataset into training, validation, and testing splits with ratios of 0.7, 0.1, and 0.2, corresponding to 18488, 2641, and 5283 samples respectively (Huang et al., 2020). Hyperparameters were determined using a grid search, optimizing for the highest mean average precision on the validation split. The test split was utilized solely for reporting the final results.\nWhile a straightforward way to balance precision and recall is to change the decision threshold, this method does not allow for comparison against the works in the literature where most report their results while setting a fixed decision threshold at 0.5 (Huang et al., 2020). Alternatively, during training, we can balance precision and recall by applying a constant weight"}, {"title": "", "content": "to the loss associated with positive labels. For a single sample, the weighted binary cross-entropy loss becomes:\n$Loss = \\frac{1}{C} \\sum_{c=1}^C w \\cdot y_c \\cdot log(p_c) + (1 - y_c) \\cdot log(1 - p_c)$\nHere, $w$ represents the weight for positive labels and $C$ is the total number of classes. To align precision and recall values with those observed in MovieNet, we applied a weight of 0.25.\nBoth the transformer architecture and the averaging module of the MLP architecture support sequences with varying lengths. However, during training, we employed fixed-length inputs to facilitate minibatch processing. Longer sequences were truncated to the desired length, while shorter ones were padded with zero-vectors. The length of feature vectors for each pretrained modality was individually determined through exploratory data analysis, specifically using box plots to analyze sequence lengths across all samples. The maximum sequence length was set to the upper adjacent value (upper whisker of the box plot), which is 1.5 times the interquartile range above the third quartile. Consequently, the sequence lengths were defined as 216 for CLIP, 64 for OCR, 86 for ASR, 140 for Audiotag, and 18 for Musicnet. When using a single-transformer model, the concatenated sequence length totaled 524. It's important to note that during inference on single samples, our models can handle inputs with varying lengths without the need for padding or truncating.\nFor the MLP model, the number of layers is 1, the model dimension is 256, and the total number of parameters is 57k. The single-transformer model has 2 layers, 8 attention heads, and a model dimension of 256, totaling 8.56M parameters. The multi-transformer model has individual transformers each with 1 layer, 8 attention heads, and a model dimension of 128, totaling 6.98M parameters. Considering the baseline model, both image-MLP and audio-MLP have 2 layers and a dimensionality of 768. The fusing transformers both have 1 layer, 4 attention heads and a dimensionality of 768. All transformers are trained with gradient clipping at a norm of 1. All models are trained with Adam optimizer (Kingma & Ba, 2015), a learning rate of 1e \u2013 5, a dropout rate of 0.5, and a batch size of 32. We implemented our models using the Pytorch library (Paszke et al., 2019) and trained them on a single NVIDIA Quadro RTX 6000 GPU with 24 GB memory."}, {"title": "4. Experiments and results", "content": "We first compare the performance of our models against the baseline model and the models presented in MovieNet paper (Huang et al., 2020). We performed inference on the test set one sample (video) at a time, using the full duration of each video without padding or truncating the sequences. Following the literature, we report macro-averaged precision, recall, and mean average precision (mAP) values, averaged over individual labels (genres).\nAdjusting the precision-recall trade-off can be achieved by altering the decision threshold in inference or by modifying the weight assigned to positive labels during training. The MovieNet paper specifies a decision threshold of 0.5 and does not make any reference to label weighting. We kept the decision threshold at 0.5 and aimed for performance similar to the MovieNet paper by assigning a weight of 0.25 to positive (existing) labels.\nTable 1 displays the overall performance of our models against the models in the literature. Our baseline model outperforms the models in the literature in terms of recall and mean average precision. Our models using pretrained features outperform all other models across all metrics. Our best-performing model, the multi-transformer improves the state of the art by a large margin. It outperforms all other models in all metrics except precision, where it performs marginally worse than our MLP model.\nIn Table 2 we compare the performance of our best model multi-transformer, our baseline and the MovieNet model (Huang et al., 2020), across all genres. Out of all 21 genres, our model outperforms the other models in 14 of them in terms of precision, 16 of them in terms of recall, and 20 of them in terms of mean average precision. Outlier values such as very low recall for genres such as Biography, History, and Mystery can be attributed to the inherent imbalance within the MovieNet dataset (Huang et al., 2020). We deliberately avoided balancing the data before training to maintain a fair comparison with the classification model presented in the MovieNet paper. We also believe that this dataset closely reflects the real-world distribution of cinematic genres, accurately portraying the relative rarity of genres such as Biography, History, and Mystery.\nIn Table 3 we present an ablation study using our best-performing model multi-transformer, demonstrating the gain in terms of mean average precision along with the increase in runtime, due to the addition of each feature while comparing against our baseline model. Runtime is defined as the average duration in seconds, to process a single video, both extracting its pretrained"}, {"title": "", "content": "features and classifying it, during inference. Although the inclusion of textual features such as OCR and ASR initially seems to hurt the performance, possibly due to the text prediction errors, incorporating their averaged version along the temporal dimension reduces the noise and does improve mean average precision. Incorporating textual features also significantly increases runtime. This is primarily because models such as CLIP, Musicnet, and Audiotag are designed to predict a single embedding vector or label, while OCR and ASR models predict text sequences, which can be viewed as predicting many labels corresponding to each word and subword, in an autoregressive way. But most importantly, the settings in which the textual features are excluded yield a better classification performance and faster runtime compared to the baseline. Furthermore, our classification models can seamlessly incorporate features that result from newer and potentially more efficient ASR and OCR models that can be developed in the future, further reducing the overall runtime.\nFinally in Figure 6 we attempt to further explain the reasons behind the success of our models. The models in the literature extract a small and fixed number of frames at random locations from each video for classification. The MovieNet baseline (Huang et al., 2020) only uses 8 scenes. They furthermore use MLPs, which cannot process sequences, hence the features from these"}, {"title": "5. Conclusion", "content": "In conclusion, our research presents a pioneering approach to movie genre classification, significantly enhancing the performance beyond current methodologies. Our strategy leverages a range of pretrained models to extract and intelligently fuse high-level features associated with visual scenery, characters, text, speech, music, and audio effects. A fundamental element of our approach is the employment of the transformer model, which facilitates the efficient handling of sequences of any length. Importantly, our method capitalizes on the entirety of the information present in all frames of movie trailers, contrasting with traditional models that are restricted to a fixed and low number of frames. Finally, our method purely operates on video"}, {"title": "", "content": "and audio, without requiring any dataset-specific auxiliary data, making it potentially applicable to any video classification task.\nUpon the acceptance of our paper, we will make the pretrained features for the entire MovieNet dataset publicly available, along with our genre classification code and trained models, to contribute to ongoing advancements in this field. Even though our models seem complex, involving multiple pretrained models, we made sure all the components and their pretrained weights are packed into a single codebase in a \u201cplug"}]}