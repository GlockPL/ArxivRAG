{"title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance", "authors": ["Binita Saha", "Utsha Saha", "Muhammad Zubair Malik"], "abstract": "This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) and fundamentally changed the way we used to interact with digital information. They have the capability to analyze and generate text that resembles human language, which has become integral across various sectors [1]. However, when it comes to answering domain-specific questions where accuracy and reliability are key, the effectiveness of these models faces challenges due to knowledge limiation, contextual misinterpretation, task-specific variability, etc. [2,3].\nTo address these limitations of adapting LLMs for specialized domains, two principal strate-gies have emerged: a) fine-tuning the models with domain-specific data, and b) augmenting them with external knowledge. Fine-tuning involves additional training of a pre-existing LLM using specialized data, which optimizes the model for particular tasks and significantly boosts its performance [1]. Despite its effectiveness, fine-tuning is not without drawbacks; it is com-putationally demanding, costly, and runs the risk of forgetting critical information-where the model loses its ability to recall previously learned information. [4-7]. Moreover, the effectiveness of fine-tuning is dependent on the availability of extensive relevant data, which makes it less practical for domain-specific fields.\nAnother approach to overcome the challenges of customizing LLMs for domain-specific tasks and enhancing precision in generating Q&A responses is the use of Retrieval-Augmented-Generation (RAG) [8]. The foundation of RAG is its capability to incorporate relevant information from external sources to ensure the generated responses are contextually appropriate [9, 10]. This method allows LLMs to adapt to domain-specific tasks more flexibly, sidestepping the high costs and potential knowledge loss associated with fine-tuning. However, RAG faces two significant challenges: information dilution and hallucination [11]. Information dilution arises when the volume of data is so large that it compromises the specificity and accuracy of the responses. Hallucination, on the other hand, refers to instances when the model produces outputs that are linguistically coherent but factually inaccurate or irrelevant to the input query [12].\nRAG can effectively handle domain-specific question answering using vectorization. Initially, the dataset is stored in a vector database using an embedding model. Embedding models trans-form text into numerical representations, or vectors, by capturing semantic meanings and re-lationships between words, which will allow for efficient similarity search later. When a user submits a query, it also converts into a query vector using the same embedding model. This ensures that both the dataset and the query are represented in comparable vector space. The retrieval component then matches the query vector against the database vector using similarity measures such as cosine similarity. The actual text associated with the closest vectors is retrieved using the retrieval component of RAG. Then LLm uses this retrieved text as context to generate a response.\nThis paper makes two key contributions: a) preparing domain-specific dataset to enhance the performance of RAG system b) The development of a novel modified RAG system, named QuIM-RAG (Question-to-question Inverted Index Matching), which introduces an inverted question matching approach with a quantized embedding index to improve the precision and efficiency of the information retrieval and answer generation processes. The domain-specific dataset is used to enhance the performance of RAG with a focus on data quality, relevance, and verifiable sources. We develope a comprehensive methodological approach for the data preparation phase, which included systematic data collection, cleaning, and structured organization. This approach is designed to improve the quality and reliability of the dataset corpus. The goal is to convert unprocessed data collected from websites into manageable chunks, which are then transformed into potential questions with the help of gpt-3.5-turbo-instruct model. A proper custom prompt is used to ensure that all the information available in each chunk is covered in custom corpus. To"}, {"title": "2 Related work", "content": "The development of Large Language Models (LLMs) has significantly transformed Natural Lan-guage Processing (NLP) in their ability to process and generate human-like text. State-of-the-art models like OpenAI's GPT series, Google's PaLM, and Meta's LLAMA series primarily utilize the Transformer architecture, which underpins their sophisticated understanding and generation of language [13-16]. These models showcase diverse architectural strategies, including exclusive use of decoders (as in GPT-2 and GPT-3), encoders (such as BERT and RoBERTa), or a blend of both in encoder-decoder frameworks (like BART), highlighting their versatility in approaching various linguistic tasks.\nProviding accurate answers to a given question using LLMs has become one of the most explored research areas in the last couple of years [17-21]. The evaluation of Q&A research has"}, {"title": "3 Methodology", "content": "Given a specific natural language question, the QA problem for a limited corpus is to identify and extract the most relevant and accurate answer from a predefined and constrained set of documents. The challenge lies in efficiently finding the most relevant answer that fits the question, taking into account the limited resources. Our key contribution is to pose finding the relevant answer as a matching process between potential questions that could have been asked for a document chunk with the actual user question. In this section, we first formalize the QA problem and then present our inverted question matching approach."}, {"title": "3.1 QA Problem for Limited Corpus", "content": "Given a corpus $C$ consisting of a finite set of documents ${D_1, D_2, ..., D_n}$ and a natural language question $Q$, the task is to find the most relevant answer $A$ from the corpus. Each document $D_i$ in the corpus $C$ contains a set of chunks (sentences) ${S_{i1}, S_{i2},..., S_{im}}$. We only focus on natural language queries that seek specific information contained within the corpus. The output is a text snippet response derived from the corpus, that directly addresses the question $Q$. $A$ is a phrase generated from a combination of multiple pieces of information from the corpus. Hence the goal is to produce answer $A$ such that $A$ maximizes the relevance to $Q$ within the constraints of $C$.\n$A = \\underset{A' \\in C}{\\arg \\max} \\text{Relevance}(A', Q)$\nHere relevance is a scoring function that quantifies how well $A'$ answers $Q$ using context from corpus $C$. That might be computed using methods such as exactly matching expected keywords, using embeddings to measure the similarity between the question and potential answers, or ensuring that the answer fits logically with the surrounding text in the document. These natural solutions are either ineffective or prohibitive to implement. Our approach addresses this by generating hypothetical questions for each chunk $S_i$ and finding the best match for the actual question to find the most likely chunk that answers the user query."}, {"title": "3.2 Inverted Question Matching to Find Relevant Chunks for Answer", "content": "Relevant chunks from various documents in the corpus can be combined to build the relevant informational context to answer the user's question. Here we describe our inverted index scheme for matching document chunks in embedding space."}, {"title": "Corpus and Document Structure", "content": "The corpus $C$ consists of a collection of documents $D_1, D_2,..., D_n$. Each document $D_i$ is com-posed of a set of chunks or sentences $S_1, S_2,..., S_m$. These chunks represent the fundamental units of information within each document, and they serve as the basis for generating semanti-cally meaningful questions that capture the content of the text."}, {"title": "Question Generation from Chunks", "content": "For each chunk $S_i$ within a document $D_i$, an instruction-following large language model (LLM) is employed to generate a set of questions ${q_{ij1}, q_{ij2},..., q_{ijk}}$. These questions are designed to encapsulate the key information or concepts contained in the chunk $S_j$. The process of question generation is crucial as it translates the raw text into a set of queries that can be later used for effective document retrieval. Intuitively, these can be viewed as \"frequently asked questions\" for the given chunk."}, {"title": "Embedding of Generated Questions", "content": "Once the questions ${q_{ij1}, q_{ij2},..., q_{ijk}}$ are generated, each question $q_{ijl}$ is transformed into an embedding vector $v_{ijl} \\in \\mathbb{R}^d$ using a pre-trained encoding scheme. These embedding vectors capture the semantic meaning of the questions in a high-dimensional space, enabling the system to compare and match questions based on their content."}, {"title": "Quantization of Embeddings", "content": "Given the high dimensionality of the embedding space, direct comparison of all vectors would be computationally expensive. To mitigate this, each embedding vector $V_{ijl}$ is quantized to the nearest prototype $p_\\iota \\in {P_1,P_2,...,P_k}$. The quantization is performed by finding the prototype $p_i$ that minimizes the cosine similarity distance to $v_{ijl}$:\n$\\rho_\\iota = \\underset{\\rho}{\\arg \\min} \\text{CosineSimilarity} (v_{ijl}, \\rho)$\nThis quantization step reduces the complexity of the index and facilitates efficient matching of user queries."}, {"title": "Construction of the Inverted Index", "content": "An inverted index $I$ is built to map each prototype $p_i$ to a set of embedding vectors $V_{ijl}$ and their corresponding text chunks $S_j$. The index is constructed as follows:\n$I(\\rho_\\iota) = {(V_{ijl}, S_j) \\text{ for all } v_{ijl} \\text{ quantized to } \\rho_i}$ \nThis index enables fast lookup of relevant text chunks based on the quantized embeddings."}, {"title": "Query Processing and Matching", "content": "When a user submits a query $Q$, it is first embedded into a vector $V$ using the same encoding scheme applied to the generated questions. The vector $V$ is then quantized to the nearest prototype $p_i$ in the embedding space using cosine similarity:\n$\\rho_i = \\underset{\\rho}{\\arg \\min} \\text{CosineSimilarity}(V, \\rho)$\nThe inverted index $I$ is then used to retrieve the set of embedding vectors $v_{ijl}$ that were quantized to $p_\\iota$."}, {"title": "Retrieval of Relevant Text Chunks", "content": "For each retrieved embedding vector $v_{ijl}$, the corresponding original question $q_{ijl}$ is decoded, and the associated text chunk $S_j$ is identified. These text chunks $S_j$ represent the parts of the documents that are most relevant to the user query $Q$."}, {"title": "3.3 Answer Generation for User Query", "content": "the relevant text chunks are returned to the large language model (LLM) to generate answer. However, in our scheme we use those chunk as the \"context\" for generating a coherent response that is directly based on the query and supported by factual information from the original document."}, {"title": "4 Experimental Setup", "content": "Our work started with detailed data collection directly from two primary sources within the North Dakota State University (NDSU) domain: the NDSU Career Advising (https://career-advising.ndsu.edu) and the NDSU Catalog (https://catalog.ndsu.edu) websites. These websites were chosen due to their comprehensive and versatile information. While we aimed to gather all available information from both sites, the Career Advising site predominantly offered insights into career guidance, job search strategies, assistance with resume tailoring, interview preparation, and details on club activities, including their missions, visions, and current engagements. Simul-taneously, the Catalog website provided us with comprehensive information on academic aspects such as admissions, enrollments, detailed course descriptions, listings of required and elective courses for each academic program, graduation requirements, and academic rules and policies. \nWe employed advanced web scraping techniques, beginning with the main pages of NDSU-Career Advising and NDSU-Catalog. We then systematically navigated through their related sub-pages for comprehensive coverage of all available content on these sites. This methodology is structured into three main phases: Data Preparation, Retrieval, and Generation."}, {"title": "4.1 Data Preparation", "content": "To create an accurate and complete dataset [Figure 1], we have constructed a customized web crawler integrating the BeautifulSoup and Scrapy frameworks. These two frameworks helped to traverse two primary sub-domains: NDSU Career Advising and NDSU Catalog. Starting from these parent links, the scraping process was designed to first identify and collect all associated page links reachable from these initial entry points. After collecting links, all HTML source code was pulled with the help of the crawler. Some data post-processing was done by removing the website header, footer, and unnecessary HTML tags to produce a cleaner version of the data."}, {"title": "4.2 Ground Truth Preparation", "content": "We prepared ground truth data to ensure high accuracy and relevance for evaluating our RAG system. The process began by selecting relevant content from the university website, which served as potential sources of information. We extracted this content using automated web scraping tools to preserve all essential details. we then manually curated this content to verify its factual accuracy and direct relevance to potential user queries. Based on this verified content, we created question-answer pairs that accurately reflect the information available on the website. We ensured that each pair was complete and directly linked to its original source. This prepared ground truth helps to effectively assess the performance of our RAG system."}, {"title": "4.3 Inverted Index Contruction for Question matching", "content": ""}, {"title": "4.4 Retrieval", "content": "The retrieval system is designed to efficiently match user queries with the most relevant information within a comprehensive knowledge base. When a user submits a query $Q$, our system initiates by transforming into an embedding vector using the same embedding process that applied to data corpus. Then, from the inverted index, it matches for the closest semantic question from data corpus by similarity search using cosine similarity. It selects the top 3 questions (k = 3) that best align semantically with the query. Then the system retrieved the associated chunks related to these questions and decode and return these as context to the 1lAma3-7b-instruct."}, {"title": "4.5 Generation", "content": "The final response to the user is generated by an open-source large language model (LLM) from Hugging Face named Llama3-8b-instruct. This is the latest addition to the Llama series while we were writing the paper, with significant advancements in AI capability. We have used the instruct-tuned version for its powerful conversational and chat capability. Llama3 supports a context length of 8,000 tokens, which allows for more extended interactions and more complex input handling compared to many previous models. Longer context capacity ensures that the model can consider a wider range of information to deliver the most relevant and coherent responses.\nIn the generation process, the model retrieves the most relevant context from the retrieval model. Then it integrates the information seamlessly using a custom prompt (Figure 4). Since the Llama3 model is trained on a vast database from diverse sources, it has a broad understanding of generating human-like text. This training enables the model to effectively utilize the retrieved"}, {"title": "4.6 Prompt", "content": "To refine the capabilities of RAG, developing a comprehensive dataset is essential. This dataset is crucial for training the RAG system to deliver precise and contextually relevant responses. Acknowledging the necessity to encompass the breadth of information within our dataset, we formulated a custom prompting strategy. This critical step ensures that the resultant dataset of question-and-answer (Q&A) pairs is not only comprehensive but also precise and reflective of the diverse content within the source texts.\nOur goal is to achieve comprehensive coverage of the chunk and ensure the diversity and accuracy of generated set of questions. The designed prompt [figure 4] explicitly instructs the model to generate a set of questions that cover all the key information of each chunk. The prompt directs the model to avoid redundancy, ensuring that each question is unique and contextually relevant to the text. This approach helps prevent the generation of overlapping questions and ensures that each query contributes distinct value to understanding the chunk's content.\nBuilding upon this foundation, it is imperative to address how our RAG model manages queries that extend beyond the confines of its knowledge base. To tackle different types of questions effectively and prevent the provision of inaccurate responses to out-of-domain (OOD) inquiries, another prompting strategy was implemented for the RAG model itself. In instances where a query seeks information absent from the text segments known to the model, the model is prompted to clearly state its inability to provide a relevant answer. This ensures the model explicitly knows its boundaries and helps prevent the provision of false or invented responses."}, {"title": "4.7 Comparison to Traditional RAG System", "content": "Traditional RAG systems typically follow a straightforward \"Retrieve-Read\" methodology [11]. These systems begin with an indexing phase where data is segmented into vectorized chunks. These chunks are then retrieved in response to user queries based on their semantic similarity. However, these systems often struggle with information overload and accuracy, leading to re-sponses that may not always be contextually appropriate or factually correct. This affects the"}, {"title": "5 Evaluation", "content": "To evaluate the performance of our QuIM-RAG model, we employ two evaluation frameworks, BERTScore [32] and RAGAS [33]. These frameworks will provide a detailed quantitative analysis that focuses on the semantic accuracy and relevance of the models response. We use BERTScore and RAGAS instead of more commonly used metrics such as BLEU [34], METEOR [35], and ROUGE [36] which don't quite match our evaluation needs. BLEU and METEOR are tra-ditionally applied to machine translation tasks, and ROUGE is tailored for text summarizing assessments. Given that our model operates within a RAG-based question-answering context, these conventional metrics fall short in accurately measuring its performance. This limitation has led us to select evaluation methods that are more appropriate and tailored to our model's specific needs."}, {"title": "5.1 BERTScore", "content": "BERTScore is used to evaluate the semantic quality of text by measuring the overlap between model-generated outputs and reference texts [32]. Unlike traditional metrics that rely on n-gram overlaps, BERTScore utilizes contextual embedding to capture deeper semantic meanings to it. It is highly effective for tasks where linguistic precision is crucial. BERTScore operates by first transforming both the reference, $x = (x_1,x_2,...,x_n)$, and candidate, $\\hat{x} = (\\hat{x}_1,\\hat{x}_2,..., \\hat{x}_m)$ texts into contextual embedding using models like BERT or ROBERTa[].\n$\\text{BERT}((x_1,x_2,...,x_n)) = (X_1, X_2, ..., X_n)$ \n$\\text{BERT}((\\hat{x}_1, \\hat{x}_2,..., \\hat{x}_m)) = (\\hat{X}_1, \\hat{X}_2, ..., \\hat{X}_m)$\nThese embeddings integrate the surrounding context of words to provide a robust represen-tation of textual meaning. The core of BERTScore's evaluation method is the pairwise cosine similarity calculation between tokens of the reference text and the candidate text. For each token in the reference, it identifies the token in the candidate text that has the highest cosine similarity and vice versa.\n$\\text{similarity}(X_i, \\hat{X}_i) = \\frac{X_i \\cdot \\hat{X}_i}{||X_i|| \\cdot ||\\hat{X}_i||}$"}, {"title": "5.2 Retrieval-Augmented Generation Assessment (RAGAS)", "content": "RAGAS [33] is designed to evaluate the effectiveness of RAG systems that combine retrieval mechanisms with LLMs to provide accurate information. It evaluates these systems by focusing"}, {"title": "5.2.1 Evaluation Metrics", "content": "RAGAS evaluates three primary aspects of RAG architectures:\n\u2022 Faithfulness: This metric evaluates how well the retrieval system grounds the generated responses in the provided context. It involves extracting statements from the generated an-swers and verifying each statement against the retrieved context using a validation function to determine if the context supports the statement.\n$\\text{Faithfulness} = \\frac{\\text{Number of verified claims}}{\\text{Total claims made}}$\n\u2022 Answer Relevance: This metric evaluates the appropriateness of the generated answers to address the posed questions. It is measured by generating potential questions from the answers using an LLM and calculating the cosine similarity of these questions to the original question. The relevance score is the average of these similarities, which indicates the directness and appropriateness of the answers.\n$AR = \\frac{1}{n} \\sum_{i=1}^{n} sim(q, q_i)$\nwhere $q_i$ are questions generated based on the answer and $sim$ is the cosine similarity between embeddings of the original question $q$ and $q_i$.\n\u2022 Context Relevance: This metric determines whether the retrieved context contains pri-marily relevant information needed to answer the question. The process involves extracting sentences from the context that are necessary to answer the question and then calculating the ratio of these extracted sentences to the total number of sentences in the context.\n$CR = \\frac{\\text{Number of relevant sentences extracted}}{\\text{Total sentences in the context}}$"}, {"title": "6 RESULT", "content": "The efficiency of our RAG system are demonstrated in Figure 5, which offers a detailed com-parison between responses generated from traditional and custom datasets when queried about the location and contact information of the North Dakota State University (NDSU) Career and Advising Center. As shown in figure 5, when a user submits a query, it is first converted into a query vector. The encoded query vector is then used to compute similarity scores against a database of question vectors, which represent pre-stored questions associated with various chunks of information. The system identifies the top k matches (in this case, k=3), which are the ques-tions most semantically related to the user query based on their similarity scores. For each of the top matching question vectors, the system retrieves the associated text chunks from the dataset. In this scenario, question 1 and 2 comes from the same chunk and question 3 comes from"}, {"title": "7 Conclusion", "content": "This paper addressed challenges of using large language models (LLM) for domain-specific question-answering. By integrating an advanced retrieval-augmented generation (RAG) sys-tem and a methodological approach to data preparation has enhance the quality of responses generated by these systems. Creating a custom dataset specifically designed for the domain in question has been key in reducing common problems like information dilution and hallucination that often seen in traditional RAG systems when they handle large amounts of unstructured data. Our evaluations show that our novel QuIM-RAG system, leveraging a custom dataset and Llama-3-8b-instruct, improves the accuracy and relevance of its responses. It performs much better than the baseline dataset, which is made from raw web data. Additionally, the responses include source links with the user's query offers further opportunities for user to seek more infor-mation and verify the details provided. Given that university websites frequently update their content every semester, we are planning to design a content retrieval mechanism that updates its corpora every four months. This will ensure that any newly added content is incorporated into the system to keep the information up-to-date and relevant for users. Looking ahead, our future goal is to conduct a comprehensive user study to assess user satisfaction and system us-ability. The insights gained from this study will help us refine the system further to ensure that it continues to meet user needs effectively while remaining adaptable to evolving content."}]}