{"title": "BSS-CFFMA: Cross-Domain Feature Fusion and Multi-Attention Speech Enhancement Network based on Self-Supervised Embedding*", "authors": ["Alimjan Mattursun", "Liejun Wang", "Yinfeng Yu"], "abstract": "Abstract\u2014Speech self-supervised learning (SSL) represents has achieved state-of-the-art (SOTA) performance in multiple downstream tasks. However, its application in speech enhancement (SE) tasks remains immature, offering opportunities for improvement. In this study, we introduce a novel cross-domain feature fusion and multi-attention speech enhancement network, termed BSS-CFFMA, which leverages self-supervised embeddings. BSS-CFFMA comprises a multi-scale cross-domain feature fusion (MSCFF) block and a residual hybrid multi-attention (RHMA) block. The MSCFF block effectively integrates cross-domain features, facilitating the extraction of rich acoustic information. The RHMA block, serving as the primary enhancement module, utilizes three distinct attention modules to capture diverse attention representations and estimate high-quality speech signals.\nWe evaluate the performance of the BSS-CFFMA model through comparative and ablation studies on the VoiceBank-DEMAND dataset, achieving SOTA results. Furthermore, we select three types of data from the WHAMR! dataset, a collection specifically designed for speech enhancement tasks, to assess the capabilities of BSS-CFFMA in tasks such as denoising only, dereverberation only, and simultaneous denoising and dereverberation. This study marks the first attempt to explore the effectiveness of self-supervised embedding-based speech enhancement methods in complex tasks encompassing dereverberation and simultaneous denoising and dereverberation. The demo implementation of BSS-CFFMA is available online\u00b2.", "sections": [{"title": "I. INTRODUCTION", "content": "In everyday acoustic environments, various forms of background noise and room reverberation significantly degrade the clarity and intelligibility of speech, posing significant challenges for speech-related applications such as conferencing systems, speech recognition systems, and speaker recognition systems [1]. Speech enhancement (SE) tasks aim to extract clean speech from noisy speech and improve the quality and intelligibility of speech. Recently, researchers have investigated deep neural network (DNN) models for speech enhancement. DNN models have shown powerful denoising capabilities in complex noise environments compared to traditional methods [2].\nWith the development of DNN, significant progress has been made in single-channel speech enhancement tasks.\nDNN-based SE methods can be broadly categorized into time-domain approaches [3], [4], [5], [6], [7], time-frequency (T-F) domain approaches [8], [9], [10], [11], and cross-domain approaches [12], [13], [14]. Time-domain methods directly estimate the target clean speech waveform from the noisy speech waveform. Time-frequency domain methods estimate clean speech from the spectrogram generated by applying the short-time Fourier transform (STFT) to the original signal. Cross-domain methods process features from various speech domains to capture more acoustic information about speech and noise, facilitating the estimation of clean speech [12], [13].\nSelf-supervised learning (SSL) leverages many unlabeled data to extract meaningful representations [15]. In many applications, supervised learning is generally superior to unsupervised learning. However, collecting a large amount of labeled data is time-consuming and sometimes impractical. SSL has been validated in various domains and has improved the performance of downstream tasks. Specifically, some promising SSL models have been proposed for speech-related tasks, such as speech and emotion recognition. As of now, there are many speech SSL models available, with the best-performing ones including Wav2vec2.0 [16], WavLM [17], HuBERT [18] and others. However, there is relatively little research on the application of SSL features to SE. Huang et al. [19] proposed the application of SSL features to SE and comprehensively evaluated the performance of most SSL models in SE. Hung et al. [13] employed a weight-summed SSL framework, fusing SSL features with spectrograms to address the issue of fine-grained information loss in SSL features. However, their cross-domain feature fusion method using early concatenation (concat) may limit the enhancement performance due to insufficient cross-domain feature integration [20]. In addition, previous studies on self-supervised embedding-based methods for speech enhancement [12], [13], [19] commonly employed simple RNN-based models for the enhancement module, while recent attention-based enhancement architectures [5], [6], [21] have demonstrated strong denoising capabilities in speech enhancement.\nIn this paper, we propose a cross-domain feature fusion and multi-attention speech enhancement network based on self-supervised embedding (BSS-CFFMA). We design a multi-scale cross-domain feature fusion module (MSCFF) in BSS-CFFMA to better fuse self-supervised features and spectrogram features, extracting information at different granularities, and further addressing the issues of SSL information loss and insufficient feature fusion [13], [22]."}, {"title": "II. RELATED WORK", "content": "The SSL models can be categorized into generative modeling, discriminative modeling, and multi-task learning. Generative modeling reconstructs input data using an encoder-decoder structure. Multi-task learning involves learning multiple tasks simultaneously, where the model can extract features that are useful for all the tasks through shared representations. Discriminative modeling maps input data to a representation space and measures the corresponding similarity. In this study, we utilized two base SSL models to extract latent representations: Wav2vec2.0 (Base) and WavLM (Base)."}, {"title": "B. Cross Domain Features and Fine Tuning SSL", "content": "Studies [14] and [13] have shown that cross-domain features contribute to improving the performance of automatic speech recognition (ASR) and speech enhancement (SE). Studies [19] have shown that SSL has great potential in speech enhancement tasks. However, Studies [13] adopted weighted sum SSL and fine-tuning methods, significantly improving the performance of speech enhancement. In this study, we employ SSL and Speech Spectrogram as two cross-domain features, weighted summed SSL and a more efficient partially fine-tuned (PF) approach to improve the performance of speech enhancement further."}, {"title": "III. METHOD", "content": "Firstly, noisy speech is fed into a weighted sum SSL model and STFT to generate SSL latent representations $F_{ws:ssl}$ and spectrograms $F_{spec}$, respectively. Subsequently, the $F_{ws:ssl}$ and $F_{spec}$ features are input into the MSCFF module for feature fusion across domains, resulting in the feature $F''$. $F''$ is then fed into RHMA, yielding different attentional representations through various attention mechanisms. Ultimately, the enhanced spectrogram is obtained by element-wise multiplication of the output from the second RHMA with the noisy spectrogram. During inference, the enhanced spectrogram and noise phase are utilized to reconstruct the enhanced speech waveform."}, {"title": "A. SSL Model based on Weighted Sum", "content": "In study [13], the author believes that using the last layer of SSL directly may result in the loss of some local information necessary for speech reconstruction tasks in deeper layers.\nSo learnable parameter $e^{(i)}$ is designed for each transformer layer's output $z^{(i)}$ in SSL:\n$F_{ws:ssl} = \\sum_{i=0}^{N-1} [e^{(i)} * z^{(i)}],$ (1)\nwhere $F_{ws:ssl} \\in \\mathbb{R}^{D*T}$, $i=0\\cdot\\cdot\\cdot N-1$ is the number of layers in SSL. Parameters $0 < e^{(i)} < 1$, $\\Sigma_i e^{(i)} = 1$."}, {"title": "B. Multi Scale Cross Domain Feature Fusion (MSCFF)", "content": "In study [13], complemented fine-grained information by incorporating the original acoustic features on top of SSL, resulting in improved performance. It uses the early concatenation (Concat). In contrast, [20] shows that early Concat focuses the entire cross-modal fusion process on a single modality and reduces feature diversity and fine-grained information. However, multi-scale feature extraction and fusion strategies have been shown to efficiently integrate cross-modal features, significantly enhancing network performance [23]. Considering the research findings and aiming to better integrate and extract information from SSL and spectrogram features, we introduce the multi-scale cross-domain feature fusion (MSCFF) module.\nThe process begins by concatenating the SSL feature $F_{ws:ssl}$ and the feature $F_{spec}$ to obtain the fused feature $F_{concat}$. The $F_{concat}$ is then fed into the main branch for feature extraction, resulting in the output $F'$.\n$F' = MB(concat(F_{ws:ssl}, F_{spec})),$ (2)\nsubsequently, $F'$ is passed through the gate branch.\n$F'_{spec,concat,ws:ssl} = GB(F') * F_{spec, concat,ws:ssl},$ (3)\nfinally, the three features are cross-fused.\n$F'' = ReLu(concat(\\tilde{F_{spec}}, \\tilde{F_{ws:ssl}}) + \\tilde{F_{concat}}),$ (4)\nwhere $\\tilde{F_{spec}} ~ F_{spec} \\in \\mathbb{R}^{F*T}$, $\\tilde{F_{ws:ssl}} ~ F_{ws:ssl} \\in \\mathbb{R}^{D*T}$, $\\tilde{F_{concat}} ~ F_{concat} ~ F'' \\in \\mathbb{R}^{(D+F)*T}$."}, {"title": "C. Residual Hybrid Multi-Attention (RHMA) Model", "content": "In previous studies [12], [19], [13], [25], RNNs were commonly used as the primary speech enhancement module for self-supervised embedding. However, RNNs suffer from long-term dependency issues, high parameter counts, and low computational efficiency. Recently, models based on Transformer architecture have achieved remarkable performance in the field of speech recognition, such as Squeezeformer [26], among others. In the domain of speech enhancement, utilizing self-attention modules often leads to improved performance, as observed in TSTNN [5], Uformer [21]."}, {"title": "Additionally, we design a residual-mixed multi-attention module (RHMA) in BSS-CFFMA, which incorporates a selective channel-time attention fusion module (SCTA) using a self-attention design to obtain different attention feature representations and achieve improved speech enhancement.", "content": "The fused feature Z is obtained after the MSCFF module is fed into the MHSA module.\n$Z_{mhsa} = PostLN(MHSA(Z) + Z),$ (5)\n$Z_{mhsa}$ represents the output of the MHSA block, which is passed through a residual connection and PostLN.\n$Z' = LN(PostLN(FFN(Z_{mhsa}) + Z_{mhsa}) + Z),$ (6)\n$Z'$ represents the output of the FFN, which undergoes two levels of residual connections and Layer Normalization.\n$Z'_{scta} = PostLN(SCTA(Z') + Z'),$ (7)\n$Z'' = LN(PostLN(FFN(Z'_{scta}) + Z'_{scta}) + Z'),$ (8)\n$Z''$ represents the output of the FFN, which undergoes two levels of residual connections and Layer Normalization."}, {"title": "D. Selective Channel-Time Attention Fusion (SCTA) Module", "content": "While models that combine attention and convolution, such as Squeezeformer [26], have achieved remarkable performance in various speech tasks, the convolutional modules increase the parameter count. Research suggests that multi-perspective attention outperforms single attention. Convolutional block attention module [27] (CBAM) is a lightweight and efficient convolutional attention method. In the domain of speech enhancement, CBAM has been utilized as a residual block [28], yielding excellent performance[29]. Based on the aforementioned research findings, we have designed the selective channel-time attention (SCTA) fusion module, which has a lower parameter count while capturing information dependencies along the channel and time axes, leading to higher performance."}, {"title": "As shown in Fig. 3, the SCA fusion module consists of max pooling, average pooling, a fully connected (FC) layer, and a sigmoid activation function. Firstly, the input Foundergoes max pooling and average pooling along the time dimension to compress the temporal axis, resulting in $F_{max}$, $F_{avg}$, and their element-wise addition feature $F_{add}$. Here, $F_{max} ~ F_{max} ~ F_{add} \\in \\mathbb{R}^{B*C*1}$. Each feature is then passed through an FC layer followed by a sigmoid activation function. Finally, each attention representation is weighted and added separately before being activated to obtain the channel attention fused feature F'.", "content": "As shown in Fig. 4, the STA fusion module consists of max pooling, average pooling, a 1D convolutional layer, and a sigmoid activation function. Firstly, the input F undergoes max pooling and average pooling along the channel dimension to compress the channel axis, resulting in $F_{max}$, $F_{avg}$, and a concatenated feature $F_{concet}$. Here, $F_{max} ~ F_{min} \\in \\mathbb{R}^{B*1*T}$, and $F_{concet} \\in \\mathbb{R}^{B*2*T}$. Each feature is then passed through a 1D convolutional layer followed by a sigmoid activation function. Finally, each attention representation is weighted and added separately before being activated to obtain the time attention fused feature F'.\nWhere $\\lambda_{max}$, $\\lambda_{avg}$, $\\beta$ are hyperparameters empirically set to 0.25, 0.25, and 0.5, respectively."}, {"title": "IV. EXPERIMENT", "content": "We evaluated the performance of speech enhancement using the proposed BSS-CFFMA on the VoiceBank-DEMAND [30] and WHAMR! [31] datasets, respectively. The VoiceBank-DEMAND dataset consists of a total of 11572 utterances, with 28 speakers and 824 utterances from 2 speakers used as training and testing sets, respectively. During the training phase, mix 10 types of noise with a signal-to-noise ratio (SNR) of [0, 5, 10, 15] dB with pure speech. During the testing phase, 5 types of noise were mixed with clean speech, with signal-to-noise ratios of [2.5, 7.5, 12.5, 17.5] dB. The WHAMR! dataset is an extended version of the wsj0-2mix [32] dataset, which includes noise and reverberation. Noise is collected from real environments, and the reverberation time is selected to simulate typical home and classroom environments. Pure speech and noise are randomly mixed within the range of a signal-to-noise ratio of [-6,3] dB. The WHAMR! dataset consists of a training set, a validation set, and a testing set consisting of 20000, 5000, and 3000 voices, respectively."}, {"title": "B. Evaluation Metrics", "content": "In order to evaluate the performance of BSS-CFFMA, we selected the following metrics: wideband perceived assessment of speech quality (WB-PESQ\u00b9) [33], narrowband perceived assessment of speech quality (NB-PESQ) [33], scale-invariant source-to-noise ratio (SI-SNR) [34], short-time objective intelligibility [35], speech signal distortion prediction (CSIG) [36], background noise invasion prediction (CBAK) [36], overall performance prediction (COVL) [36], and real-time factor (RTF)."}, {"title": "C. Experimental Setup", "content": "All speech signals are downsampled to 16 kHz and randomly selected for training 100 rounds with a duration of 2.56 seconds. The STFT and ISTFT parameters are set as follows: FFT length is 25 ms, window length is 25 ms, and hop size is 10 ms. Batch size B is set to 16. We used Adam optimizer and dynamic learning rate strategy [37]; the learning rate for SSL fine-tuning is 0.1* learning-rate. Train using two Precision T4 GPUs, with a training time average of approximately 7 minutes per epoch."}, {"title": "V. RESULTS", "content": "In our study, we first compared the denoising performance of the proposed BSS-CFFMA method with 14 baseline methods on the VoiceBank-DEMAND dataset. These methods can be categorized into three different domain approaches. As shown in Table I, the proposed BSS-CFFMA outperforms all the baselines regarding evaluation metrics. In addition, compared to the SSL cross-domain method BSS-SE, BSS-CFFMA significantly surpasses BSS-SE. Even surpassing the performance of BSS-SE on large SSL models in basic SSL models. This result further demonstrates the higher efficiency of our network in leveraging cross-domain features for SSL feature extraction and utilization.\nWe also evaluated the denoising, dereverberation, and joint denoising-dereverberation performance of BSS-CFFMA under three test scenarios on the WHAMR! dataset, making it the first self-supervised model used for reverberation tasks."}, {"title": "B. Ablation Analysis", "content": "We conducted ablation experiments to validate each module. Table III shows (i) removing MSCFF and RHMA, (ii) RHMA only, (iii) MSCFF and MHSA, (iv) MSCFF and SCTA, and (v) MSCFF only. All modules outperformed the baseline. PESQ decreased by 0.27 in (i), 0.09 in (ii), 0.10 in (iii), 0.07 in (iv), and 0.03 in (v), demonstrating the effectiveness of MSCFF, RHMA, MHSA, and SCTA.\nTo further intuitively assess the effectiveness and flexibility of BSS-CFFMA, we conducted additional experiments. Using the test set of the VoiceBank-DEMAND dataset, we categorized the test data according to different noise types and signal-to-noise ratios (SNR) and visualized the PESQ and STOI metrics. Fig. 6 displays the results of STOI and PESQ across multiple noise types at four SNR levels. We observe that the performance of the network is relatively smooth for different noise-type cases, and there are no extremes in the network for different SNR cases (total average PESQ = 2.7 when SNR = 2.5dB). This surface network has relatively good generalization and noise immunity."}, {"title": "VI. CONCLUSIONS", "content": "In this letter, we propose the BSS-CFFMA model for single-channel speech enhancement and experimentally demonstrate its effectiveness. While it outperforms other baseline models, we also observe that the performance seems to reach a plateau, which we attribute to challenges in phase processing. Therefore, in future work, we will continue to build on our ongoing research and focus on the computation and optimization of phases to improve the model's performance further."}]}