{"title": "An Empirical Study of Deep Reinforcement Learning in Continuing Tasks", "authors": ["Yi Wan", "Dmytro Korenkevych", "Zheqing Zhu"], "abstract": "In reinforcement learning (RL), continuing tasks refer to tasks where the agent-environment interaction is ongoing and can not be broken down into episodes. These tasks are suitable when environment resets are unavailable, agent-controlled, or predefined but where all rewards\u2014including those beyond resets are critical. These scenarios frequently occur in real-world applications and can not be modeled by episodic tasks. While modern deep RL algorithms have been extensively studied and well understood in episodic tasks, their behavior in continuing tasks remains underexplored. To address this gap, we provide an empirical study of several well-known deep RL algorithms using a suite of continuing task testbeds based on Mujoco and Atari environments, highlighting several key insights concerning continuing tasks. Using these testbeds, we also investigate the effectiveness of a method for improving temporal-difference-based RL algorithms in continuing tasks by centering rewards, as introduced by Naik et al. (2024). While their work primarily focused on this method in conjunction with Q-learning, our results extend their findings by demonstrating that this method is effective across a broader range of algorithms, scales to larger tasks, and outperforms two other reward-centering approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) tasks can generally be divided into two categories: episodic tasks and continuing tasks. In episodic tasks, the interaction between the agent and environment naturally breaks down into distinct episodes, with the environment resetting to an initial state at the end of each episode. The goal of these tasks is to maximize the expected cumulative reward within each episode. Episodic tasks are suitable when the environment can be reset, the reset conditions are predefined, and rewards beyond the reset point do not matter\u2014such as in video games.\nIn contrast, continuing tasks involve ongoing agent-environment interactions where all rewards matter. Continuing tasks are well-suited for situations where the environment cannot be reset. In many real-world problems, such as inventory management, content recommendation, and portfolio management, the environment's dynamics are beyond the control of the solution designer, making environment resets impossible. Continuing tasks can also be useful when resets are possible. First, when designing reset conditions is challenging, it can be beneficial for the agent to determine when to reset. For instance, a house-cleaning robot might decide to reset its environment by requesting to be placed back on the charging dock if trapped by cables. The second scenario involves predefined reset conditions, just as in episodic tasks, but where post-reset rewards still matter. For example, when training a robot to walk, allowing the robot to learn when to fall and reset can lead to better overall performance, as it could pursue higher rewards after resetting rather than merely avoiding falling at all costs. In both scenarios, continuing tasks provide an opportunity to balance the frequency of resets and the rewards earned by choosing the cost of reset, which is a flexibility not present in episodic tasks.\nContinuing tasks can also be useful in cases where the ultimate goal is to solve an episodic task. This is best exemplified by the works on the autonomous RL setting, where the goal is to address an episodic task, and the agent learns a policy to reset the environment. In this setting, the agent is trained on a special continuing task, where the main task, which is the episodic task of interest, and an auxiliary task, such as moving to the initial state, are presented in an interleaved sequence. The learned main task's policy is deployed after training. This setting can be most useful when resets are expensive, and it is possible to reach the initial state from all other states, such as in many robotic tasks. Representative works in this direction include Eysenbach et al. (2017), Sharma et al. (2021), Zhu et al. (2020), and Sharma et al. (2022).\nDespite the broad applications of continuing tasks, empirical studies on deep RL algorithms in these tasks remain limited, and their unique challenges remain under-explored. Most existing empirical studies focus on demonstrating better performance of new algorithms. For instance, Zhang and Ross (2021), Ma et al. (2021), Saxena et al. (2023), and Hisaki and Ono (2024) introduced average-reward variations of popular deep RL algorithms and empirically evaluated them alongside their discounted return counterparts on continuing tasks based on the Mujoco environment (Todorov et al., 2012), highlighting improvements in performance. In addition to the Mujoco testbeds used in the above works, Platanios et al. (2020) and Zhao et al. (2022) provided new testbeds for continuing tasks. However, Platanios et al.'s (2020) testbed also presents significant partial observability, making it not suitable for isolating the challenges of continuing tasks. The testbeds presented by Zhao et al. (2022) have small discrete state and action spaces, making them primarily suitable for studying tabular algorithms. To our knowledge, only two empirical studies have explored the unique challenges that continuing tasks present to deep RL algorithms. In particular, Sharma et al. (2022) found that several RL algorithms designed for the autonomous RL setting perform significantly worse when resets are unavailable. This indicates that resets limit the range of visited states, focusing the agents around initial and goal states. Naik et al. (2024) demonstrated that in two small-scale continuing tasks (namely, Pendulum and Catch), the DQN algorithm performs poorly when using a large discount factor or when rewards share a common offset. While a large discount factor also poses challenges in episodic tasks, its effects can be masked by the finite length of episodes. Shifting rewards by a common offset can only be applied to continuing tasks, as in episodic tasks, it changes the underlying problem.\nOur first contribution is an empirical study of several well-known deep RL algorithms on a suite of continuing task testbeds. The objectives of this study include understanding the challenges present in continuing tasks with different reset scenarios and the extent to which the existing deep RL algorithms address these challenges. The tested algorithms include DDPG (Lillicrap, 2015), TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017), and DQN (Mnih et al., 2015). The testbeds are obtained by applying simple modifications to existing episodic testbeds from Gymnasium (Towers et al., 2024; Brockman, 2016) based on Mujoco and Atari environments (Bellemare et al., 2013), such as removing time-based resets and treating resets as standard transitions in the environment with some extra cost. We considered the following reset scenarios: no resets, predefined resets, and agent-controlled resets. The proposed testbeds include 15 continuous action tasks covering all these reset scenarios and six discrete action tasks with predefined resets. We did not create Atari-based testbeds without resets or with agent-controlled resets because it is not trivial to remove the predefined resets there. While some of our Mujoco testbeds are identical to those used in prior works studying average-reward algorithms (e.g., Zhang and Ross 2021), the majority differ from theirs. The code used in this study is based on the Pearl library (Zhu et al., 2023) and is available in https://github.com/facebookresearch/DeepRL-continuing-tasks.\nThe empirical study reveals several key insights. First, the tested algorithms perform significantly worse in tasks without resets compared to those with predefined resets. We found that predefined resets help in at least two ways. One is that they limit the effective state space the agent needs to deal with. This point echoes Sharma et al.'s (2022) finding in the autonomous RL setting. The other way is that they move the agent back to an initial state when the agent fails to escape from suboptimal states due to the weak exploration ability. Second, tested algorithms in continuing testbeds with predefined resets learn policies outperforming the same algorithms in the episodic testbed variants when both policies are evaluated in the continuing testbeds. We found that better performance is achieved by choosing actions that yield higher rewards at the cost of more frequent resets. Further, increasing the reset cost reduces the number of resets and, interestingly, can even improve overall rewards, indicating that reset costs are not only problem parameters but also solution parameters. Third, when agents are given control over resets, in some cases, it can barely surpass or even be worse than random policies in tasks with predefined resets, which suggests that these tasks are quite challenging for the tested algorithms. Lastly, all algorithms perform poorly in continuing tasks with"}, {"title": "2 EVALUATING DEEP RL ALGORITHMS ON CONTINUING TASKS", "content": "This section evaluates several of the most well-known RL algorithms in a suite of continuing testbeds."}, {"title": "2.1 TESTBEDS WITHOUT RESETS", "content": "This section evaluates four continuous control algorithms (DDPG, TD3, SAC, PPO) in five continuing testbeds without resets and shows how the absence of resets poses a significant challenge to the tested algorithms.\nThe testbeds are based on five Mujoco environments: Swimmer, HumanoidStandup, Reacher, Pusher, and Ant. The goal of the Swimmer and Ant testbeds is to move a controlled robot forward as fast as possible. For Reacher and Pusher, the goal is to control a robot to either reach a target position or push an object to a target position. In HumanoidStandup, the goal is to make a lying Humanoid robot stand up. The episodic versions of these testbeds have been standard in RL (Towers et al., 2024).\nThe continuing testbeds are the same as the episodic ones except for the following differences. First, the continuing testbeds do not involve time-based or state-based resets. For Reacher, we resample the target position every 50 steps while leaving the robot's arm untouched, so that the robot needs to learn to reach a new position every 50 steps. Similarly, for Pusher, everything remains the same except that the object's position is randomly sampled every 100 steps. As for Ant, we increase the range of the angles at which its legs can move, so that the ant robot can recover when it flips over.\nNote that we created these continuing testbeds based on environments where, except for a set of transient states, it is possible to transition from any state to any other state. This is known as the weakly communicating property in MDPs (Puterman, 2014). Without this property, no algorithm can guarantee the quality of the learned policy because the agent might enter suboptimal states, from which there is no way to escape. An example environment without this property is Mujoco's Hopper, where if the agent falls, it is unable to stand back up."}, {"title": "2.2 TESTBEDS WITH PREDEFINED RESETS", "content": "This section evaluates both continuous and discrete control algorithms on continuing task testbeds with predefined resets. In addition, it shows how the learned policies differ from policies learned in episodic variants of the testbeds.\nThe test suite includes both continuous and discrete control testbeds. The continuous control testbeds are built upon five Mujoco environments: HalfCheetah, Ant, Hopper, Humanoid, and Walker2d. In these testbeds, the objective is to control a simulated robot to move forward as quickly as possible. The corresponding existing episodic testbeds involve time-based truncation of the agent's experience followed by an environment reset. In the continuing testbeds, we remove this time-based truncation and reset. However, we retain state-based resets, such as when the robot is about to fall (in Hopper, Humanoid, and Walker2d) or when it flips its body (in Ant). In addition, we add a reset condition for HalfCheetah when it flips, which is not available in the existing episodic testbeds. Each reset incurs a penalty of -10 to the reward, punishing the agent for falling or flipping.\nThe discrete control testbeds are adapted from six Atari environments: Breakout, Pong, Space In-vaders, BeamRider, Seaquest, and Ms. PacMan. Like the Mujoco environments, the episodic versions include time-based resets, which we omit in the continuing testbeds. In these Atari environments, the agent has multiple lives, and the environment is reset when all lives are lost. Upon losing a life, a reward of -1 is issued as a penalty. Furthermore, in existing algorithmic solutions to episodic Atari testbeds, the rewards are transformed into -1, 0, or 1 by taking their sign for stable learning, though performance is evaluated based on the original rewards. We treat the transformed rewards as the actual rewards in our continuing testbeds, removing such inconsistency.\nFor each testbed-algorithm pair, we performed ten runs, and each run consisted of 3M steps for Mujoco testbeds and 5M steps for Atari testbeds. The learning curves corresponding to the best parameter setting for Mujoco and Atari testbeds are shown in Figure 1 (middle row) and Figure 3, respectively. The results show that SAC and DQN consistently perform the best in Mujoco testbeds and Atari testbeds, respectively.\nAs mentioned earlier, when resets are predefined, the agent may choose to solve a continuing or episodic task. We now illustrate the difference between these two choices by showing the difference between policies learned in these two tasks. The episodic tasks are the same as the above continuing tasks, except that the agent optimizes cumulative rewards only up to resetting."}, {"title": "2.3 TESTBEDS WHERE THE AGENT CONTROLS RESETS", "content": "This section studies the behavior of current algorithms in continuing tasks where predefined resets are not available, and the agent decides when to reset. Intuitively, allowing the agent to choose when to reset can lead to higher reward rates compared to predefined resets, as the agent can optimize its behavior by avoiding unnecessary resets. However, predefined resets reduce the state and action spaces, making the testbeds easier. For instance, in environments like Humanoid, Walker, and Hopper, the agent needs to carefully control its actions to avoid falling, and recovering from these fallen states is difficult or impossible. In such cases, the agent must learn to recognize when it cannot recover and needs to reset the environment to continue. Predefined resets simplify the problem by eliminating these bad, unrecoverable states, allowing the agent to focus on learning in good states.\nThe testbeds are the five Mujoco testbeds used in Section 2.2 without predefined resets. In these new testbeds, the agent can choose to reset the environment at any time step. This is achieved by augmenting the environment's action space in these testbeds by adding one more dimension. This additional dimension has a range of [0, 1], representing the probability of reset. The tested continuous control algorithms can then be readily applied, except that the exploration noise for this additional dimension needs to be set differently from other action dimensions because the performance of the policy is more sensitive to this dimension than the rest."}, {"title": "2.4 FAILURE TO ADDRESS LARGE DISCOUNT FACTORS OR OFFSETS IN REWARDS", "content": "Using the Mujoco testbeds presented above, we show in this section that the performance of all of the tested continuous control algorithms deteriorates significantly when a large discount factor is used or when all rewards are shifted by the large constant."}, {"title": "3 EVALUATING ALGORITHMS WITH REWARD CENTERING", "content": "This section empirically shows that the temporal-difference-based reward centering method, originally introduced by Naik et al. (2024), improves or maintains the performance of all tested algorithms in the testbeds introduced in the previous section. Further, this method mitigates the negative effect when using a large discount factor and completely removes the detrimental effect caused by a large common reward offset.\nThe idea of reward centering stems from the following observation. By Laurent series expansion (Puterman, 2014), if a policy \u03c0 results in a Markov chain with a single recurrent class, its discounted value function V can be decomposed into two parts, a state-independent offset $d_{\\upsilon}\\pi = \\frac{r(\\pi)}{1-\\gamma}$, where d is the stationary distribution under \u03c0, r(\u03c0) is the average reward rate under policy \u03c0, and a state-dependent part keeping the relative differences among states. Here, the reward rate does not depend on the initial state due to the assumption of a single recurrent class. Note that only the state-dependent part is useful for improving the policy \u03c0. However, when the state-independent part has a large magnitude, possibly due to large offsets in rewards or a discount factor that is close to 1, approximating the state-independent part separately for each state can result in approximation errors that mask the useful state-dependent part.\nReward centering approximates the state-independent part using a shared scalar. Specifically, reward centering approximates a new discounted value function, obtained by subtracting all rewards by an approximation of r(\u03c0), and this new discounted value function has a zero state-independent offset if the approximation of r(\u03c0) is accurate. Even if the approximation of r(\u03c0) is not accurate, removing a portion of the state-independent offset still helps.\nA straightforward way to perform reward centering is to estimate r(\u03c0) using an exponential moving average of all observed rewards. For on-policy algorithms, this moving average approach can guarantee convergence to r(\u03c0). However, for off-policy algorithms, this approach does not converge to r(\u03c0) (e.g., the behavior policy is uniformly random while the target policy is deterministic).\nWe now briefly describe the TD-based reward-centering approach, which can be applied to both on- and off-policy algorithms. This approach extends an approach to solve the average-reward criterion (Wan et al., 2021) to the discounted setting. Here, we illustrate this approach using use TD(0) (Sutton, 2018, p. 120), the simplest TD algorithm, as an example. More details on how tested algorithms employ this approach are provided in Section A.4.\nGiven transitions (S, R, S') generated by following some policy \u03c0, TD(0) estimates \u03c5\u03c0 by maintaining a table of value estimates V : R|S| and updating them using V(S) \u2190 V(S) + \u03b1\u03b4, where $\\delta^{TD} = R + \\gamma V(S') - V(S)$ is a TD error, a is a step-size parameter, and y is a discount factor. The TD-based reward-centering approach simply replaces the above TD error in TD(0) with the following new TD error:\n$\\delta^{RC} = R - \\hat{r} + \\gamma V(S') - V(S)$,\nwhere R, a biased estimate of the reward rate, is also updated by the TD error \u03b4RC, as follows:\n$\\hat{r} \\leftarrow \\hat{r} + \\eta\\alpha\\delta^{RC}$,\nwhere \u03b7 > 0 is a constant. It is straightforward to show that, under certain asynchronous stochastic approximation assumptions on a, V(s) converges to $\\upsilon^{\\pi}(s) - \\frac{\\eta}{1-\\gamma}\\eta\\sum_{s\\in S} \\upsilon^{\\pi}(s)$, following the same steps as in the proof of Theorem 1 by Naik et al."}, {"title": "4 CONCLUSIONS AND LIMITATIONS", "content": "This paper empirically examines the challenges that continuing tasks with various reset scenarios pose to several well-known deep RL algorithms using a suite of testbeds based on Mujoco and Atari environments. Our findings highlight key issues that future algorithmic advancements for continuing tasks may focus on. For instance, we demonstrate that the performance of tested algorithms can heavily depend on the availability of predefined resets, as these resets help agents escape traps and reduce the state space complexity. When predefined resets are available, all algorithms perform reasonably well, learning policies that exploit frequent resetting to achieve higher rewards. The reset cost balances this trade-off and also functions as a tuning parameter. In contrast, agent-controlled reset tasks are generally more challenging, and in some testbeds, allowing the agent to control resets significantly worsens performance. Additionally, we show that both a large discount factor and a large common offset in rewards can negatively impact the performance of all tested algorithms. Our results also validate the effectiveness of an existing approach to address these issues, demonstrating through extensive experiments that the negative impact of reward offset can be completely eliminated, while the harm from a large discount factor can be largely mitigated with a TD-based reward-centering approach. Even in scenarios with a smaller discount factor and no reward offset, this approach shows benefits across many testbeds for all tested algorithms.\nThis paper has several limitations. First, this paper focuses exclusively on the performance of online RL algorithms, leaving research on offline RL algorithms in continuing tasks unexplored. Second, although we concentrate on well-known discounted algorithms, it is worth investigating whether average-reward algorithms, such as those mentioned in Section 1, face similar challenges. Third, while most of the hyperparameters used in the experiments are standard choices and have been effective in episodic testbeds, they may not be ideal for continuing tasks. Identifying hyperparameter"}, {"title": "A DETAILS OF EXPERIMENT SETUP", "content": "This appendix provides details on the experiments conducted to produce the results presented in the main text and the subsequent two appendices. First, we present additional hyperparameters used by the algorithms tested in testbeds without resets or with predefined resets. Next, we describe the modifications made to the tested algorithms for testbeds with agent-controlled resets. Following this, we provide detailed information about how the tested algorithms are used together with reward centering. In the main text, we introduced the TD-based reward centering approach; here, we describe two additional approaches for performing reward centering, outlining how these methods were applied to the tested algorithms, along with the values of additional hyperparameters tested for reward centering."}, {"title": "A.1 AVERAGE-REWARD RATE AS THE EVALUATION METRIC", "content": "In reinforcement learning (RL), an agent interacts with an environment to learn how to make decisions that maximize a cumulative reward signal. The environment is typically modeled as a finite Markov Decision Process (MDP), which consists of a tuple (S, A, R, p), where S represents the set of states, A the set of actions, R is the set of rewards, p(s', r | s, a) is the probability of transitioning from state s to s' and observing a reward of r, given action a. At each time step t, the agent observes the current state St, selects an action At based on a policy, and receives a reward signal Rt+1 from the environment, with the goal of learning a policy that maximizes long-term reward.\nFor continuing tasks where the agent-environment interaction persists indefinitely, the average-reward criterion is suitable as the performance metric and is therefore used in this paper. Let the initial state be so, the average reward is defined as $r(\\pi, s_0) \\overset{def}{=} \\lim_{\\gamma\\rightarrow \\infty} E\\left[\\sum_{t=1}^{T} R_t \\Big| A_t \\sim \\pi(\\cdot | S_t), S_0 = s_0\\right]$, where \u03c0 : S \u2192 A(A) is the agent's policy.\nWhile there are several deep RL algorithms (e.g., Zhang and Ross 2021) addressing the average-reward criterion, we choose to study several well-known discounted deep RL algorithms. This is because the focus of this paper is on the challenges of continuing tasks rather than on studying the properties of algorithms, and these discounted algorithms have been better understood in the literature. Further, note that by adjusting the discount factor to be close to one, discounted algorithms can approximately solve the average-reward criterion in continuing tasks. When the discount factor is sufficiently close to one, any discounted optimal policy is also average-reward optimal (Grand-Cl\u00e9ment and Petrik, 2024)."}, {"title": "A.3 HYPERPARAMETERS WHEN APPLIED TO TESTBEDS WITH AGENT-CONTROLLED RESETS", "content": "We modified the hyperparameters of the tested algorithms in two ways to improve the algorithms' performance in testbeds with agent-controlled resets.\nFirst, we adjust a hyperparameter that controls the level of exploration for DDPG, TD3, and SAC. For DDPG and TD3, the exploration noise is a sample of a zero-mean multivariate Gaussian random vector with independent elements. This exploration noise is then added to the action generated by the actor network to perform persistent exploration. For testbeds without resets or with predefined resets, we applied the same standard deviation of 0.1 to all elements. However, when resets are part of actions, we tested smaller standard deviations, including 0.05, 0.005, 0.0005, and 0.00005, for the reset dimension. This is because, compared to the other dimensions in actions, a small noise in the reset dimension would have a significant effect on the behavior of the policy. For SAC, the entropy regularization coefficient controls the level of exploration. We applied the autotune technique introduced by Haarnoja et al. (2018) to adjust this coefficient dynamically. This technique introduces some regularization that pushes the entropy of the learned policy toward some predefined target value, guaranteeing that exploration does not diminish to zero asymptotically. For testbeds without resets or with predefined resets, the target entropy was chosen to be - |A|, a choice tested by Haarnoja et al. (2018), where A is the dimension of the action space. When resetting is part of the action, we found this choice leads to very frequent resets, even at the end of training. We therefore tested smaller target entropy values, including - |A|,- |A| \u2013 3, - |A| - 6, and - |A| 9. PPO's exploration noise is learned, and there is no mechanism for maintaining exploration above a certain level or pushing exploration toward a certain level. Therefore, no more changes need to be applied to PPO's hyperparameters.\nThe second change we made was to have a different random policy for collecting data in the warmup stage of DDPG, TD3, and SAC. In testbeds without resets or with predefined resets, a policy that uniformly randomly samples from the action space was used in the warmup stage. When resetting probability is part of the action, we apply a different policy that is biased toward lower reset probability. The reason is that a uniformly random policy would output a reset probability of 0.5, which is so high that most of the data collected following this policy will be several steps away from the initial states. To generate longer trajectories, we chose the resetting probability element of the action to be 1/N, where N is an integer sampled uniformly from 1, 2, ..., 1000, and kept other elements uniformly sampled."}, {"title": "A.4 APPLYING REWARD CENTERING METHODS TO THE TESTED ALGORITHMS", "content": "In this section, we describe how we applied three reward-centering approaches to the tested algorithms. We start with TD-based reward centering and then discuss two other alternative approaches.\nWe apply TD-based reward centering to DQN the same way as Naik et al. (2024) did. DQN maintains an approximate action-value function, qw : S \u00d7 A \u2192 R, with the vector w being the parameters of the function. To update w, DQN maintains a target network q\u00fb parameterized the same way as qw but with different parameters. For every fixed number of time steps, the values of w are copied to w. Every time step, DQN samples a batch of transition tuples (si, ai, ri, s'i), i \u2208 {1,2,..., n} from the replay buffer, where si, ai, ri, s' denote a state, an action, and the resulting reward and state, respectively, and n is the batch size. The update rule to w is\n$\\overset{def}{=} w \\leftarrow w + \\alpha \\frac{1}{n} \\sum_{i=1}^{n} \\delta_i \\nabla_w q_w(S_i, a_i)$,\nwhere $\\delta_i \\overset{def}{=} r_i + \\gamma max_{a\\in A}q_{\\hat{w}}(s_i, a) - q_w(S_i, a_i)$ is a TD error, a is a step-size parameter and y is a discount factor. With TD-based reward centering, we update w with equation 1 but with $\\delta_i$ replaced by a different TD error, where the reward is subtracted by an offset r, defined as follows:\n$\\delta^{RC} \\overset{def}{=} \\delta_i - \\hat{r}$.\nThe offset is updated whenever w is updated, using the new TD errors, following\n$\\hat{r} \\leftarrow \\hat{r} + \\frac{\\eta\\alpha}{n} \\sum_{i=1}^{n} \\delta^{RC}_i$,"}]}