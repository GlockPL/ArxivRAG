{"title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement", "authors": ["Shivam Singh", "Karthik Swaminathan", "Nabanita Dash", "Ramandeep Singh", "Snehasis Banerjee", "Mohan Sridharan", "Madhava Krishna"], "abstract": "Embodied agents assisting humans are often asked to complete a new task in a new scenario. An agent preparing a particular dish in the kitchen based on a known recipe may be asked to prepare a new dish or to perform cleaning tasks in the storeroom. There may not be sufficient resources, e.g., time or labeled examples, to train the agent for these new situations. Large Language Models (LLMs) trained on considerable knowledge across many domains are able to predict a sequence of abstract actions for such new tasks and scenarios, although it may not be possible for the agent to execute this action sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks and scenarios. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation over cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM output.", "sections": [{"title": "1 INTRODUCTION", "content": "Embodied agents are being used in assistive roles in many applications, aided in part by the availability of realistic simulators [1]\u2013[3]. Although such agents possess some prior knowledge of domain objects and their attributes, they are often asked to perform new tasks and operate in new scenarios. For example, an agent preparing dishes in the kitchen based on prior knowledge of some recipes and ingredients, may be asked to prepare a new dish or clean the pantry.\nLarge Language Models (LLMs) trained on a large corpus of data have demonstrated the ability to decompose a range of tasks into a sequence of high-level (abstract) actions (i.e., sub-tasks) that implement the task [4]\u2013[6]. For example, an LLM can provide a sequence of sub-tasks for completing the previously unseen task of preparing hot chocolate. However, this sequence may involve incorrect steps, or reference objects and actions that the agent does not have access to in the kitchen under consideration.\nThe challenges mentioned above are partially offset by the fact that an assistive agent usually has some prior domain-specific knowledge in the form of objects, object attributes, and action capabilities. State-of-the-art methods build large datasets of such information for a given application domain [7], or attempt to embed this knowledge by repeatedly tuning deep networks [8]. However, such knowledge is not readily available for many practical domains, and modern data-driven methods make it difficult to reliably and transparently revise the encoded knowledge over time. In a departure from such methods, the framework described in this paper seeks to leverage the complementary strengths of LLMs, Knowledge Graphs (KGs), and human feedback-see Figure 1. Our framework enables the assistive agent to:\n1) Query an LLM to obtain a generic sequence of actions (sub-tasks) to be executed to accomplish any given task.\n2) Encode any prior domain-specific knowledge of object types and attributes in a KG, using it to revise the LLM\u2019s output action sequence.\n3) Use discrepancies between LLM output, KG, and ob-"}, {"title": "2 RELATED WORK", "content": "We motivate our framework by discussing related work in the use of LLMs, KGs, and HITL task decomposition.\nLLMs and KGs for task decomposition: LLMs such as GPT-4 [9], Gemma2 [10], and LLaMA3 [11] have experimentally demonstrated the ability to decompose abstract tasks into sub-tasks [4]\u2013[6], [12]\u2013[14]. Frameworks such as TaskBench [15] have compared fully automated processes with those with human interventions, particularly for unfamiliar or \"open-set\" tasks [6], [16], [17]. Additionally, methods such as ADaPT [18] have supported iterative adjustment of task complexity continuously based on real-time feedback. In parallel, KGs have have been used to model prior knowledge of objects and their attributes for sequential task planning, e.g., for sequential task prediction with graph CNN [19], action planning for robots in Industry 4.0 environments [20], and for generalizing to new (related) environments [21]. Our framework builds on these ideas by combining the (generic) prediction capabilities of LLMs with the domain-specific knowledge encoded in a KG [22], [23] for task adaptation in new environments [24].\nRelated task planning examples: The Functional Object-Oriented Network (FOON) [25] encodes substantial knowledge about cooking (e.g., ingredients and outcomes of actions) in the form of task trees and using them for task planning for cooking related dishes [7], [26]\u2013[28]. In more recent work, a fine-tuned GPT has been used to transform generic recipe instructions into task trees, which are merged and revised by comparing information stored in FOONs to obtain the task tree used for execution [8]. These methods use examples from the Recipe1M+ dataset [29] for tuning and evaluation. Instead of tuning an LLM across classes of tasks or training a knowledge base extensively for a particular class of tasks, our framework supports incremental revision, faster adaptation, and reliability. Our framework provides the assistive agent limited (prior) knowledge of any specific domain as a KG, enabling it to incrementally refine the KG with new objects and actions as they are encountered, and to correct errors by soliciting and using human feedback when it is necessary and available.\nHuman-in-the-loop task decomposition: Human feedback has been used to enhance hierarchical task allocation and robot task planning in complex environments [30], [31]. Frameworks like TaskBench [15] and Reflexion [32] leverage human feedback to iteratively decompose tasks, making LLMs more effective in handling abstract tasks. Hierarchical task structuring is crucial for handling complex, multi-step task decomposition, especially in abstract problem domains [33]. Instead of iteratively tuning LLMs (e.g., through prompts), which does not necessarily lead to correct results, we combine the generic prediction capabilities of LLM, real-time domain-specific KG updates [34], [35], and human-in-the-loop feedback [36]\u2013[40], allowing the system to operate based on the available knowledge to perform new classes of tasks while incrementally refining the knowledge."}, {"title": "3 PROBLEM FORMULATION AND FRAMEWORK", "content": "Figure 2 is an outline of our framework. In the motivating example, an agent assisting in cooking tasks in a kitchen has access to relevant objects and ingredients for many dishes but it does not have the recipes. When asked to prepare any particular dish, $T_i$, the agent queries an LLM to obtain a sequence of abstract actions (sub-tasks), i.e., $(a_1,..., a_m)$. For example, the sequence for make an omelette includes picking up the egg and breaking the egg over a skillet. This sequence of abstract actions is checked against a KG with some domain-specific information in the form of existing objects and attributes that include the actions that can be performed on some objects. The agent tries to resolve any discrepancy between the LLM output and KG, e.g., KG states there is no skillet or that an egg can only be cracked, by finding replacements, e.g., crack the egg over a pan. If the discrepancy is not resolved, or if executing the action sequence does not provide the desired outcome, the agent identifies relevant actions and solicits human input to refine the KG, e.g., add knowledge of objects or their attributes, and provides an action sequence to complete the task. The agent is assumed to be able to execute these actions. We describe out framework\u2019s components below."}, {"title": "3.1 Generic Task Decomposition with LLM", "content": "In our framework, we use an LLM to decompose any given task into a sequence of sub-tasks because LLMs have demonstrated the ability to provide such a sequence of abstract actions for many different tasks. Specifically, in the motivating example, the LLM is prompted with information about some domain objects, an example cooking task (make coffee), and the corresponding action sequence (recipe) to be executed-see Figure 2a. We experimentally evaluate the use of different LLMs, as described in Section 4.1.\nSince the sequence of sub-tasks predicted by the LLM is based on many information sources, it may not be possible to execute one or more of these actions. For example, in the context of cooking tasks, the suggested ingredient may not be available or the action may involve an incorrect choice of tool (e.g., using a fork to cut vegetables). These situations can be addressed in part by using prior domain-specific information, which is encoded as described below."}, {"title": "3.2 Representing Domain-specific Knowledge with KG", "content": "Our framework uses a Knowledge Graph (KG) to encode any prior information available to the agent. In the context of cooking tasks, this includes knowledge of some classes of ingredients (e.g., herbs, fruits, vegetables), receptacles (e.g., plates, bowls, countertop), and tools (e.g. knives, spoons), which can be arranged hierarchically. It also encodes the existence of some specific instances of these object classes and their properties such as likely location(s) and the actions they can be involved in (e.g., cutting, scooping, grinding). We use the Resource Description Framework (RDF) format to encode this information in two graph structures in Turtle format (.ttl file) see Figure 3:\n1) State graph: models current state as $G_s = (I_s, E_s)$, where nodes $I_s$ are instances of object classes such as ingredients and receptacles; and $E_s \\subseteq I_s X P_s X V_s$ are edges such that $(i_j, p, v_k) \\in E_s$ is a triple denoting an attribute of $i_j \\in I_s$ in terms of value $v_k \\in V_s$ of predicate $p \\in P_s$. For example, (apple1, obj_location, fridge) and (apple1, is_sliced, true) express applel's location and that it is sliced."}, {"title": "3.3 Refining LLM output", "content": "If a mismatch is detected between the LLM output and the KG, the agent attempts to use the KG to revise the action sequence-see Figure 2(b). Specifically, the agent attempts to replace the text corresponding to the identified mismatch, which can refer to actions, object instances, or object attributes, with other text from the KG. While performing such text replacement, it is important to consider syntactic similarity, which measures similarity in the structure (e.g., of words or sentences), and semantic similarity, which considers similarity in meaning. In our framework, the agent can compute the similarity of the identified words (or their embedding) with words from a similar category (or their embedding) in the KG. The use of word embeddings requires additional contextual information and makes it difficult to understand the revision of the LLM output. We thus chose to use the direct matching of words while considering hyper-nyms (broader terms) or hyponyms (more specific terms) for simplicity, ease of use, and transparency. If the agent is able to replace all identified mismatches, it executes the actions."}, {"title": "3.4 Knowledge refinement with human input", "content": "Since the KG is not comprehensive, the agent may not be able to resolve all identified mismatches, e.g., reference to unknown object or action. Also, there may be unexpected action outcomes when the agent executes the action sequence. These situations are handled through re-prompting and human feedback-see Figure 2(c). Specifically, the agent responds to an unresolved mismatch or erroneous outcome by re-prompting the LLM with additional information (of mismatch or error). If the mismatch or error persists, human input is solicited and used.\nExistence check: if an action or object in the LLM output does not exist in the KG, there are three possibilities: (1) The agent is mistaking an existing item (action) for another item (action); (2) the entity does not exist in the domain; or (3) the entity exists but is not in the KG. In the first case, human informs the agent about the correct object (or action); in the second case, human denies existence of entity; and in the third case, human confirms the entity\u2019s existence and agent interactively obtains entity\u2019s attributes.\nLearn attributes: If human confirms existence of an instance of a new entity, the agent interactively obtains additional details. For example, when informed about an instance of a new object class onion, agent incrementally requests information about the object type (e.g., edible_object) and other relevant attributes (e.g., boilable, fryable, location of instance). This knowledge revision can be viewed as correcting (expanding) the knowledge in the KG by revising class attributes in Gk and instance-specific details in Gs.\n$f_{KE}(I_{new}, P_{new}, S_{current}) \\Rightarrow G'_k, G'_s$\nwhere $I_{new}$ is the new entity; $P_{new} = (p_1, v_1), ..., (p_n, v_n)$ refers to attributes ($p_i$) of entity and their values ($v_i$); $S_{current} = (s_1, v_1), ..., (s_n, v_n)$ refers to states $s_i$ and their values $v_i$; and $G'_k$ and $G'_s$ are the updated components of the KG. For example, new edge is added in Gs to encode an onion\u2019s position and new edge is added in Gk to encode that an onion can be fried. Note that this update to existing knowledge is fully transparent by design.\nAlgorithm 1 describes the flow of information and control in our framework. The framework takes as input the state graph Gs and attribute graph Gk, along with an input prompt (ip_prompt) that contains information about the class of tasks, an in-context example, and a query specifying the task the agent must perform. The LLM generates an action sequence T (Line 3), which is refined to Trefined using the knowledge in the KG (Line 4). If there are no unresolved mismatches between KG and LLM output (Eunkn), the action sequence is executed, with the outcomes and errors collected for further analysis (Lines 5-7). Any unresolved mismatches or errors in outcome result in a feedback prompt to the LLM, leading to a new predicted sequence of actions T\u2032 (Lines 9-13, 19-23). If these mismatches and/or errors persist (beyond threshold $F_{max}$), the agent queries a human, which potentially leads to knowledge refinement, updating Gk and Gs. After the expansion, the knowledge base is updated, and the refined action sequence is executed and evaluated (Lines 14-18, 24-26). This entire process is repeated until the tasks is completed or some threshold (e.g., time limit) is exceeded."}, {"title": "4 EXPERIMENTAL SECTIONS AND RESULTS", "content": "This section describes the experimental setup and the results of experimentally evaluating three hypotheses:\nH1: Combining generic prediction of action sequences from LLMs with KG-based specific prior knowledge improves performance compared with just LLMs.\nH2: Soliciting and using human feedback as needed supports incremental knowledge revision and results in improved performance compared with not using human feedback.\nH3: Our framework adapts to new classes of tasks through incremental and transparent knowledge refinement."}, {"title": "4.1 Experimental Setup", "content": "We begin by describing the experimental set up, which includes the prompting of LLMs, and the choice of baselines, classes of tasks, and evaluation measures."}, {"title": "4.1.1 LLM Prompting", "content": "We used GPT-3.5 and GPT-4o to generate action sequences for specific tasks in a given environment. We used Chain of Thought (CoT) prompting to encourage the LLM to decompose any given task into a series of logical steps. The main prompt included domain-specific information (e.g., object classes from Gs), set A of agent\u2019s actions, and output for a single in-context example. For example, for the agent assisting in cooking tasks, the LLM was encouraged to generate an action sequence that fetches ingredients and tools, completes the cooking process, and serves the dish, based on the example of preparing coffee. The LLM\u2019s output was filtered to retain only the predicted action sequence. As described in Section 3.4, any unresolved mismatch between LLM and KG, or an error in outcome, also led to the agent sending a feedback prompt to the LLM in an attempt to fix the error."}, {"title": "4.1.2 Baselines", "content": "We evaluated three different configurations of components in our framework: (a) LLM; (b) LLM with a KG; and (c) LLM with KG and human input (LLM + KG + Human). We conducted linked trials, i.e., in each trial, the same LLM output was provided to each configuration. As stated in Section 3, with just LLM, the predicted action sequence is sent directly for execution, and errors results in a feedback prompt to the LLM for a fixed number of times. With LLM + KG, the KG is used to identify and fix mismatches between LLM output and KG; however, consistent mismatches and incorrect execution outcomes are not addressed. The LLM+ KG + human configuration represents our framework, in which unresolved mismatches are addressed using human input, which is assumed to be accurate; the other two configurations serve as baselines."}, {"title": "4.1.3 Classes of tasks", "content": "In order to evaluate the ability of our framework to adapt to different classes of tasks, we considered cooking and cleaning tasks. Specifically, we considered 30 different cooking tasks in a kitchen; this is the motivating scenario described in Section 3. These tasks were created by sampling from the Recipe1M+ dataset [29]. In addition, we considered 12 variants of cleaning/clearing tasks that involved the agent cleaning specific objects or surfaces (e.g., \"do the laundry\"), or arranging objects in desired configurations in particular rooms (e.g., \"clear the toys from the playroom\")-see Figure 5 for some examples. The results of evaluating the adaptability of our framework is summarized later in Table II."}, {"title": "4.1.4 Evaluation Strategy", "content": "For the evaluation of our framework, we used human participants to provide ground truth. Specifically, we recruited 18 human evaluators to mark the execution outputs for each task assigned to the framework and the two baselines. The tasks were distributed such that the output for each task was evaluated by at least three human participants. The scores provided by the human (on a linear scale between 0-20) were averaged to obtain the success rate of our framework and the two baselines. These results are discussed further in Section 4.2.\nTo better understand the LLM\u2019s performance, we also considered progress lines [8], which depicted the use of key individual objects during individual steps of the action sequence, e.g., Figure 4 shows the movement of each ingredient when cooking an omelette. These were presented along with the execution outputs to be evaluated by the humans."}, {"title": "4.1.5 Evaluation Measures", "content": "The key performance measures considered in this work include:\n\u2022 Success rate: As stated above, this measure was com- puted based on the scores assigned by the human participants. Higher values are better as they indicate a higher degree of satisfaction in task completion. This measure was used for evaluating H1-H3.\n\u2022 Average tokens used: The number of tokens used when prompting the LLM (including the input prompt and all subsequent feedback prompts) was averaged across all tasks. This is a measure of resource consumption and lower values are usually better, except when the use of prompts improves the values of other measures.\n\u2022 Number of nodes and edges in KG: We use this measure to evaluate H2 and H3. An increase in its value implies an expansion of knowledge in the KG.\n\u2022 Mean ingredient overlap: A measure specific to the first class of tasks (cooking); it is the average overlap between the ingredients in the ground truth recipe and the ingredients in the executed action sequence. If $m_i$ denotes the ingredients required to make a particular dish and $l_i$ denotes the ingredients in the action sequence, this measure is computed as:\nMean ingredient overlap = $\\frac{1}{N} \\Sigma_{i=1} \\frac{| m_i \\cap l_i |}{| m_i |}$ (1)\nwhere $|\\cdot|$ is the cardinality of a set, and N is the total number of recipes sampled from the dataset. This measure was used to evaluate H1-H2."}, {"title": "4.2 Experimental Results", "content": "Next, we describe and discuss the experimental results.\nEvaluating H1. We first explored whether the combination of LLM and KG leads to improved performance in comparison with just using LLM output for any given task. The corresponding results for the cooking-related tasks are summarized in Table I; in particular, see columns labeled \"LLM\" and \"LLM + KG\". For the two LLMs considered (GPT3.5, GPT4o), we observe a substantial increase in success rate, reduction in token use, and an increase in the mean ingredient overlap for LLM+KG compared with LLM. These results provide strong support for H1."}, {"title": "Evaluating H2", "content": "Next, we explored the impact of soliciting and using human input as needed. The last column of Table I (\"LLM+KG+Human\") shows that our framework\u2019s judicious use of human input with LLM and KG markedly improved performance on all measures compared with LLM and LLM+KG. With GPT-4o, we observed a 45.94% increase in success rate over LLM and 34.19% over LLM+KG. For GPT-3.5, the success rate increased by 66.67% over LLM and 58.13% over LLM+KG. Also, the average number of tokens used by our framework dropped by 48.26% compared with baseline(s). This performance improvement was strongly influenced by the refinement of knowledge in the KG; the number of nodes and edges in the KG expanded from (79, 772) to (87, 845) with GPT-4o and to (89, 869) with GPT3.5. These results strongly support H2."}, {"title": "Evaluating H3", "content": "Finally, we evaluated the ability to adapt our framework to a different class of tasks (cleaning and clearing), with the results summarized in Table II. Unlike prior work [8], we seek to achieve this adaptation without extensive tuning (e.g., of LLM) or the need for comprehensive domain-specific knowledge (in the KG). We instead leverage the interplay between LLM, KG, and human input to support incremental adaptation to the new class of tasks. Results indicate (once again) a substantial improvement on all measures for our framework compared with the baselines. We noted that the impact of adding different bits of knowledge to the KG can differ. For example, with GPT-4o, the addition of just one item (mopping_cloth) to the KG based on human input led to a 31% increase in success rate; with GPT3.5, this improvement was more pronounced (56%). We also observed a substantial reduction in the number of tokens used. In addition, this adaptation of knowledge is fully transparent by design. These results strongly support hypothesis H3."}, {"title": "5 CONCLUSIONS AND FUTURE WORK", "content": "Embodied agents assisting humans frequently have to complete previously unseen tasks or operate in new scenario. This paper describes a framework that leverages the complementary strengths of Large Language Models (LLMs), Knowledge Graphs (KGs), and Human-in-the-Loop (HITL) feedback to satisfy this requirement. Specifically, the generic task decomposition ability of LLMs is used to predict a sequence of abstract actions to complete any given task. This sequence is adapted to the specific scenario(s) and the task-, agent-, or domain-specific constraints using a KG that encodes prior knowledge of some objects, object attributes, and action capabilities. Any unresolved mismatch between the KG and the LLM output, and any unexpected action outcomes, are addressed by soliciting and using human input. This HITL feedback corrects errors and refines the existing knowledge (in the KG) for subsequent operation. Experimental evaluation in two simulated domains demonstrates substantial performance improvement compared with baselines, and illustrates incremental acquisition of knowledge to adapt to new classes of tasks.\nThis research opens up multiple avenues for further research. First, we will explore the use of this framework in many more classes of tasks, building on (and reinforcing) the promising results obtained so far. Second, we will investigate the trade-off between automating the generation of an action sequence for any given task, and soliciting and incorporating human feedback as needed. Furthermore, we will explore the use of this framework on a physical robot platform assisting humans. The long-term objective is to create assistive agents and robots that can interact and collaborate with humans in different application domains."}]}