{"title": "IPO: Iterative Preference Optimization for Text-to-Video Generation", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Xuecheng Nie", "Hao Li"], "abstract": "Video foundation models have achieved significant ad-vancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet re-quirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the per-spective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Prefer-ence Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from prefer-ence feedback, which helps improve generated video qual-ity in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which en-ables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can ef-ficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual la-beling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark. We will release our source codes, models as well as dataset to advance future research and applications.", "sections": [{"title": "1. Introduction", "content": "Text-to-video generation has drawn lots of attention due to its great potentials in movie products, video effects and user-generated contents, etc. Recent works [3, 20, 13] in-troduce the Transformer architecture into diffusion models and improve the video generation quality by simply scaling up the parameter size, e.g., CogVideoX-5B [52], Mochi-10B [44] and Hunyuan Video-13B [19]. Although achieving impressive performance, they still face challenges for gen-erating user-satisfied videos with consistent subject, smooth motion and high-aesthetic picture. In addition, their large model sizes make them slow to produce videos. These drawbacks limit their applications in practice.\nTo align generation results with human preference, post-training techniques have shown their effectiveness in fields of Large Language Models (LLMs) and Text-to-Image models (T2Is). For instance, Direct Preference Opti-mization (DPO) [36] exploits pairwisely ranked data to make LLMs know the language style liked by human. While Kahneman-Tversky Optimization (KTO) [8] uses pointwisely binary data to reinforce T2Is to learn how to maximumly align the objective with human expectation. These techniques promote generative models to derive user-satisfied contents and give us inspiration for improving video generation models.\nThus, we propose to align Text-to-Video models (T2Vs) with human preference from the perspective of post-training in this paper. In particular, we present the Iterative Prefer-ence Optimization (IPO) framework for enhancing T2Vs. IPO introduces preference priors for finetuning T2Vs in the form of reinforcement learning, which considers sub-ject consistency, motion smoothness and aesthetic quality, etc. This leads to improved video generations that are well aligned with human preference. In addition, difference from prior single-round methods, IPO adopts a multi-round opti-mization strategy to reinforce T2Vs in an iterative manner, which strengths the models continuously. In this way, IPO can effectively improve T2Vs without need of large-scale dataset and expensive supervised finetuning.\nSpecifically, IPO consists of three core parts as shown Fig. 1: the preference dataset, the critic model and the it-erative learning framework. IPO first collects a preference dataset for training the critic model. To get this dataset, IPO predefines several categories, including human, ani-mal, action, etc. Then, IPO randomly combines these cat-egories and uses LLM to extend them as prompts for gen-erating videos with T2V models. Given these videos, IPO manually annotates them with two kinds of labels: one is"}, {"title": "2. Related Work", "content": "Text-to-Video Generation Recent advances in video generation have been predominantly driven by diffusion models [16, 40, 41, 55], which have demonstrated remark-able improvements across multiple dimensions, including diversity, fidelity, and image quality [5, 7, 17, 26, 38, 47, 49], with the scaling of training data and model size further enhancing their performance [14, 39, 48]. In the domain of Text-to-Video (T2V) generation, pre-training models from scratch demands substantial computational resources and access to extensive human-curated video datasets, high-lighting the critical importance of leveraging large-scale datasets for effective model training [10, 29, 20, 3, 19]. Cur-rent methodologies, such as those presented in [2, 46, 12], build upon text-to-image models by incorporating spatial attention mechanisms, introducing temporal attention mod-ules, and fine-tuning them on video datasets, often employ-ing joint image and video training strategies [30], with [12] proposing a plug-and-play temporal module that facilitates video generation from personalized image models. State-of-the-art results in terms of pixel quality and temporal con-sistency have been achieved by methods such as [56, 4], while recent diffusion models based on the Dit structure, such as [3, 52, 4], have demonstrated exceptional perfor-mance, advancing the frontiers of T2V generation by en-hancing spatiotemporal coherence and scalability, showcas-ing the potential of Transformer-based diffusion architec-tures. Further advancements have been made in addressing the discrepancy between spatial and temporal modules in two-stage training, with works such as [24, 19, 52] employ-ing 3D full attention mechanisms to improve overall perfor-mance and effectively bridge the gap between spatial and temporal components.\nThe training of large models typically involves two key stages: pre-training and post-training, with relatively lim-ited research focusing on the post-training phase for text-to-video (T2V) generation. In this context, [53] introduced a temporal decaying reward (TAR) mechanism, grounded in the assumption that the central frame of a video should be assigned the highest importance, while the emphasis gradually diminishes towards the peripheral frames. This strategic allocation of importance across frames ensures a more stable and visually coherent video generation process. Meanwhile, [21] explored the extensive design space of energy functions to enhance ordinary differential equation (ODE) solvers, demonstrating its potential by improving T2V model training through the extraction of motion priors from training videos. Additionally, VADER [35] proposed a method that leverages the gradient of a reward model to fine-tune a base video diffusion model, utilizing a wide range of pre-trained vision models to align various video diffusion models. This alignment approach, facilitated by VADER, exhibits strong generalization capabilities, even for cues not encountered during training, thereby advanc-ing the adaptability and performance of T2V systems.\nAligning with human preferences Aligning human pref-erences through post-training has been widely studied for large language models (LLMs), with several ap-proaches [31, 37, 59] making notable advancements. Re-inforcement Learning from Human Feedback (RLHF) [31] trains a reward function from comparison data on model outputs to align the policy model. Rank Responses for Hu-man Feedback (RRHF) [54], a simpler alternative to Prox-imal Policy Optimization (PPO), extends supervised fine-tuning and reward model training. Sequence Likelihood Calibration (SLiC) [58] achieves alignment using human feedback data collected for different models, akin to of-fline RL data. Direct Preference Optimization (DPO) [36] simplifies alignment by directly training on human prefer-ence data, improving stability and performance while re-ducing computational costs. Kahneman-Tversky Optimiza-tion (KTO) [8] maximizes utility based on the Kahneman-Tversky model, rather than maximizing log-likelihood of preferences. Rejection Sampling Optimization (RSO) [28] introduces statistical rejection sampling to derive preference data from the optimal policy, enhancing alignment with hu-man preferences.\nSeveral methods have successfully integrated prefer-ence alignment with diffusion models, driving significant progress in the field. DraFT [6] fine-tunes models to max-imize differentiable reward functions, such as human pref-erence scores, while [34] aligns models with downstream reward functions via end-to-end backpropagation through the denoising process. Similarly, [50] optimizes models based on reward scores, but these approaches often face in-efficiency and instability. To overcome these challenges, DPOK [9] frames fine-tuning as a reinforcement learning problem, combining policy optimization with KL regular-ization and policy gradients to update pre-trained text-to-image diffusion models. DDPO [1] reframes denoising as a multi-step decision problem, using policy gradients to en-hance cue-image alignment without additional data collec-tion or human annotation. Diffusion-DPO [45] optimizes models using human comparison data, reformulating Direct Preference Optimization (DPO) to improve model likeli-hood and differentiable objectives. Diffusion-KTO [22] in-troduces a novel approach for aligning text-to-image mod-els by maximizing expected human utility, removing the need for costly pairwise preference data. The iterative pref-erence optimization algorithm in this work further enhances offline learning methods like Diffusion-DPO and Diffusion-KTO, addressing instability and performance issues while advancing alignment effectiveness."}, {"title": "3. METHOD", "content": "In this section, we present our proposed general iter-ative preference optimization (IPO) framework, which is designed to support both pairwise and pointwise prefer-ence alignment optimization algorithms. For offline pref-erence alignment algorithms, the effectiveness is often hin-dered by the limitations of offline datasets, as highlighted by RSO [27], particularly when the offline data distribution diverges from the target distribution. Taking Direct Pref-erence Optimization (DPO) [36] as a representative exam-ple, which can be extended to other algorithms, DPO can be interpreted as imposing constraints on the resulting pol-icy through the dataset D. Informally, for DPO to converge to the optimal policy \u03c0*, it requires an infinite dataset D to comprehensively cover the entire query-response space. However, in practice, training on a finite dataset D in-evitably fails to encompass the full query-response space, leading to suboptimal convergence and performance limi-tations, which our IPO framework aims to address by it-eratively refining the alignment process and enhancing the coverage and quality of the policy optimization."}, {"title": "4. Implementation details", "content": "In this section, we present our iterative preference opti-mization framework, along with its implementation details and pipeline. An overview of the algorithm is illustrated in Fig. 1. Specifically, the data collection and annotation pro-cess are described in detail in Section 4.1, while Section 4.2 elaborates on the training methodology for the Critic model. Finally, Section 4.3 introduces a comprehensive framework for aligning human feedback, encompassing the training of DPO on paired data and KTO on single data."}, {"title": "4.1. Datasets Annotation Pipeline", "content": "Video-text collection Our annotated dataset is con-structed using the open-source model CogVideoX-2B [52]. The process begins by creating a diverse set of prompts for video generation, encompassing various character types, ac-tions, scenes (both realistic and abstract), animals, plants, vehicles, and video styles. These elements are randomly combined and refined into coherent prompts using the Qwen2.5 [51] large language model. To ensure diversity and avoid the generation of repetitive sentences, we fine-tune the model's output by adjusting key parameters such as temperature, top-k, and top-p. Using these prompts, the video generation model produces three distinct variants for each video by applying different random seeds. Throughout the training process, we iteratively regenerate data using an optimized strategy to progressively expand and enhance our preference-aligned dataset.\nHuman Annotation When learning to align human feed-back, we focus on three dimensions, referring to the text-to-image evaluation work Evalalign[43]: consistency be-tween text and video, video fidelity, and additional motion smoothness. To capture human preferences, we employ two annotation methods: Pairwise ranking and Pointwise scoring.\nFor pairwise ranking, annotators are presented with two videos generated from the same prompt and instructed to evaluate them based on predefined criteria. During the eval-uation, annotators are required to: (1) provide detailed natu-ral language feedback, (2) identify specific quality issues in each video (or confirm their absence), and (3) articulate the reasoning behind their ranking decisions. These annotated comparisons are then systematically organized to construct the training samples:\nFor pointwise scoring, each video is evaluated across three dimensions, with scores categorized into three levels: good, average, and poor. Similar to the ranking process, annotators are required to provide detailed justifications for their scores, ensuring transparency and consistency in the evaluation process. These annotated comparisons are then systematically organized to construct the training samples:\nEach dataset is annotated by at least three individuals. The final annotation result is determined through voting, se-lecting the annotation with the highest level of consistency among the reviewers."}, {"title": "4.2. Critic model training", "content": "To develop a comprehensive model capable of handling both paired data sorting and individual video scoring tasks, we fine-tuned the pre-trained Vila model [25], leverag-ing its exceptional capabilities in image and video instruc-tion following, comprehension, and multi-image process-ing, which make it particularly suitable for training our Critic model. During training, we implemented key modifi-cations, including restructuring the prompt format to incor-porate detailed reasoning chains before point-by-point scor-ing or pairwise ranking, and constructing explicit chains of thought to enhance the model's decision-making process. This approach enables the model to provide comprehensive rationales prior to delivering its final output, improving both the accuracy of results and the quality of natural language feedback."}, {"title": "4.3. Iterative training pipeline", "content": "Iterative preference optimization provides substantial ad-vantages over offline learning methods. The iterative learn-ing framework we propose is a versatile and general-purpose approach, capable of seamlessly integrating var-ious offline learning algorithms into an iterative setting, thereby achieving significant performance enhancements. Specifically, we investigate two distinct offline preference alignment methods: the KTO algorithm, which utilizes pointwise scoring, and the DPO algorithm, which is based on pairwise ranking.\nIterative DPO In the iterative preference optimization process employing Direct Preference Optimization (DPO), the system follows a structured workflow: during each iter-ation, the current optimal policy model generates multiple video outputs for a given prompt. These generated videos are subsequently evaluated and ranked by a critic model based on predefined quality metrics. The ranked outputs then serve as training data for preference alignment, where the policy model is fine-tuned to better align with the de-sired preferences. This cyclic process of generation, evalu-ation, and optimization continues iteratively, progressively enhancing the model's performance through successive re-finement cycles.\n\u2022 Video Generation. We use CogvideoX-2B as the ini-tial checkpoint, and generate at least three random videos for the same prompt by setting different seeds to to form a dataset V = {(xk, Yk1, Yk2, Yk3, \u2026)}k=1, where xk represents the prompt and yk represents the corresponding video generated by the model.\n\u2022 Pairwise Ranking. To enhance ranking reliability and mitigate potential biases in the Critic model, we im-plement a robust pairwise comparison protocol with position swapping during inference: for each video pair, we conduct two reciprocal evaluations by alter-nating their presentation order, retaining only consis-tent results as valid entries for the preference dataset. Formally, we construct the pairwise dataset as D = {(Xk,Ykw,Yk\u0131)}k=1, where ykw and yk\u2081 denote the preferred (winning) and dispreferred (losing) samples, respectively. This rigorous validation process ensures higher data quality and reduces positional bias in the collected preference pairs.\n\u2022 Preference learning. During training, we employ a selective sampling strategy to construct high-quality preference pairs by extracting only the highest-ranked and lowest-ranked samples from the critic model's evaluations, forming the final paired data (yw, y') while deliberately excluding intermediate-ranked sam-ples to ensure clearer preference distinctions. For op-timization, we utilize the diffusion-DPO loss as the primary objective function and introduce two auxil-iary negative log-likelihood (NLL) loss terms follow-ing [33]: the first term, weighted by 0.2, regularizes the preferred samples, while the second term, scaled by 0.1, anchors the model to real video data distri-butions. This dual regularization strategy preserves the structural integrity and format of generated outputs while preventing undesirable degradation in the log-probability of selected responses, as demonstrated in [32], [11], and [33].\nIterative KTO KTO [8] is an offline learning method that leverages pointwise scoring data, eliminating the need for preference-based datasets. By directly utilizing binary-labeled data, KTO effectively trains algorithms while demonstrating heightened sensitivity to negative samples. This makes it particularly adept at handling imbalanced datasets with uneven distributions of positive and nega-tive samples. Furthermore, KTO achieves significant per-formance improvements even without relying on the SFT stage. In scenarios where data is imbalanced, paired sorting data is unavailable, or preference data is noisy, KTO stands out as a more effective and robust solution.\n\u2022 Video Generation. Similar to the DPO training frame-work, we adopt an identical video sampling strategy in this study. In alignment with the KTO [8] methodol-ogy, empirical evidence suggests that sampling multi-ple videos for a given prompt yields superior results compared to single-video sampling. Therefore, we maintain consistency with the DPO sampling protocol throughout our implementation.\n\u2022 Pointwise scoring. The Critic model is employed to evaluate and assign scores to all generated text-video pairs (y, x). Each data sample is categorized into three distinct quality levels: \"Good\", \"Normal\", and \"Bad\"."}, {"title": "5. Experiments", "content": "This section presents the experimental setup, results, and ablation studies to demonstrate the effectiveness of the pro-posed online reinforcement learning framework for video generation optimization using human feedback during post-training."}, {"title": "5.1. Experimental Setup", "content": "Our experiments are conducted on a Critic model con-structed based on the diffusion model, which evaluates video quality through semantic and temporal consistency. The Critic model incorporates both metrics derived from human feedback and pre-defined objective criteria to ensure comprehensive evaluation.\nThe training process leverages both DPO and KTO methods within the proposed online reinforcement learning framework. The DPO method aligns the generated videos with human feedback by minimizing divergence between the learned policy and a reference policy, while KTO di-rectly maximizes the utility of the generated video instead of maximizing the preference log-likelihood. Both methods were implemented with a batch size of 128, learning rate of 2e-5, and trained for up to three rounds of iterative op-timization. Each round involves a combination of human feedback-guided rewards and the original diffusion model loss to balance consistency and realism."}, {"title": "5.2. Results", "content": "Quantitative results are evaluated using the VBench metric, which comprehensively measures video generation qual-ity in terms of temporal coherence, semantic alignment, and overall plausibility. Our experiments demonstrate that the optimized 2B model achieves significant improve-ments across all evaluated metrics compared to the base-line 5B model. Specifically, the optimized model exhibits substantial gains in temporal coherence, effectively en-hancing the smoothness of transitions between frames and minimizing abrupt changes. Moreover, the model shows marked improvements in semantic alignment, demonstrat-ing a stronger ability to adhere to the intended meaning of input prompts and better capturing their underlying context. These advancements highlight the efficacy of the proposed post-training strategy in addressing key limitations of the baseline model. The overall VBench score further con-firms that the optimized 2B model not only achieves su-perior video generation quality but also maintains compu-tational efficiency, underscoring the practical advantages of our approach in optimizing smaller-scale models to outper-form larger counterparts."}, {"title": "5.2.2 Qualitative Results", "content": "To further analyze the benefits of our approach, we provide qualitative comparisons between the baseline and optimized models. Visual results demonstrate that the optimized 2B model produces videos with higher semantic fidelity, accu-rately capturing the intent of input prompts and reflecting their underlying context. Additionally, the optimized model ensures greater temporal consistency, effectively minimiz-ing artifacts and sudden transitions between frames, which are often observed in the baseline outputs. Furthermore, the generated videos exhibit enhanced realism, with re-duced incoherent structures and improved plausibility, mak-ing the outputs more visually appealing and aligned with human perception. These qualitative improvements high-light the effectiveness of our optimization process in ad-dressing key challenges related to semantic and temporal consistency, significantly elevating the overall quality of generated videos."}, {"title": "5.3. Ablation Studies", "content": "We conduct a series of ablation studies to validate the contributions of key components in our framework. Each experiment evaluates performance using the VBench metric to ensure consistency in evaluation."}, {"title": "5.3.1 Impact of Critic Model Scale on Accuracy", "content": "To evaluate the impact of model scale on the performance of the critic model, we conducted experiments with two dif-ferent scales of ViLA-based critic models: 13B and 40B parameters. The critic models were fine-tuned on a com-bination of pairwise and pointwise annotated datasets, de-signed to align with human preferences effectively. Table 6 presents the accuracy of these models on validation datasets for both pairwise ranking and pointwise scoring tasks. The results indicate that increasing the model scale from 13B to 40B leads to significant improvements in both pairwise and pointwise accuracy. Specifically, the 40B model achieves 86.14% accuracy in pairwise ranking and 97.57% in point-wise scoring, outperforming the 13B model by 2.42% and 2.24%, respectively. This demonstrates that larger-scale critic models are better at capturing and generalizing hu-man preferences, likely due to their increased capacity to represent complex relationships in multimodal data. The ablation study highlights the importance of model scale in improving the critic model's performance. While smaller models can achieve reasonable results, larger models pro-vide a notable boost in accuracy, which is crucial for down-stream reinforcement learning tasks. These findings suggest that scaling up the critic model can be a straightforward yet effective way to enhance its alignment capabilities and im-prove the overall quality of video generation."}, {"title": "5.3.2 Incorporating Real Video Data", "content": "To explore the impact of real video data during post-training, we integrated real samples into the training pro-cess and applied the original diffusion model loss with re-inforcement learning objectives. This modification greatly improves generation quality by enhancing temporal coher-ence, reducing flickering, and strengthening frame consis-tency. Furthermore, the inclusion of real-world examples boosts semantic alignment, guiding the model to generate more realistic motion dynamics and contextual details.\nAs shown in Table 5, the introduction of real video data (indicated by the inclusion of sft loss) leads to measur-able improvements across multiple VBench metrics, includ-ing temporal flicker, motion smoothness, and aesthetic qual-ity. Notably, the optimized model achieves higher scores in semantic consistency and dynamic degree, indicating a sig-nificant enhancement in the realism and coherence of gen-erated videos. These results underscore the effectiveness of integrating real-world data with reinforcement learning to refine video generation outputs, enabling the model to pro-duce videos with superior visual and temporal fidelity."}, {"title": "5.3.3 Comparison of different RL Methods", "content": "To evaluate the performance of the proposed online rein-forcement learning framework, we conducted experiments using two different feedback alignment methods: Knowl-"}, {"title": "5.3.4 Effect of Multi-Round Training", "content": "The impact of multi-round reinforcement learning is as-sessed by training models with 1, 2, and 3 rounds of iterative optimization, where a single round represents the traditional offline post-training approach. Results in Table 2 show that performance improves with each additional round, as multi-round training better aligns the model with human feedback and refines generation quality. These findings confirm that iterative reinforcement learning effectively enhances both temporal and semantic consistency."}, {"title": "6. Conclusion", "content": "This paper presents Iterative Preference Optimization (IPO), a novel post-training framework for text-to-video (T2V) generation. IPO iteratively enhances models using preference datasets, criticism models, and iterative learn-ing, improving subject consistency, motion smoothness, and aesthetic quality. As the first to apply iterative pref-erence optimization to T2V, IPO leverages human feedback to refine video quality, making it more aligned with user expectations. Experiments show that IPO enables a 2B-parameter model to outperform a 5B-parameter model on VBench."}], "equations": ["max Ex\u03bf~\u03c0\u03bf (xo/c)\n\u03c0\u03b8\n[\nr(xo, c) - Blog\n\u03c0\u03bf(\u03a7\u03bfC)]\n,", "Exo~\u3160(20\\c) [ -r(x0, c) - Blog Tref (oc)\n\u03c0\u03bf(xo|c)\n=\nBKL (To(To) c)||Tref (oc) exp (r(zo,c))),", "\u03c0*(xo c) \u03b1\n\u03c0\u03bf (xo c) exp\n()", "(1)(0)\n\u03c0\nref\n=\nTref", "(2)\nTref", "()=\n= Tref,", "1\nTref exp\n\u03b2\u03b9", "(1)\n= exp();", "Bi-1\n:", "(N)\nTref\n=\n\u043c\u0435\u0445\u0440\n(", "Ldiffusion-dpo(0) =\n- E(xw,x\u2081)~D,t~U(0,T),x\u00a9~q(x*\\xw),x\u00a6~q(x|xo)\nlog \u03c3 (-\u03b2\u03a4\u03c9(\u03b5) (\n||\u20ac \u2013 \u20ac0(x, t) || - || ew - \u20acref(x,t)||2\n\u2013 (||\u20ac \u2013 \u20ac0(x, t) || - ||e\u00b2 \u2013 \u20acref(x, t)||2)))", "Ldiffusion-kto(0) = maxEx0~D,t~Uniform([0,T])\n\u03c0\u03b8\n\u03c0\u03c1(Xt-1|Xt)\nTref(xt-1|Xt)\n[U(w(xo)(Blog Tre\nQref))]", "L(0) = Ldiffusion-dpo(0)\n+ 1. E(y,x)~Dsample [-log po (yw|x)]\n+ 12. E(y,x)~Dreal [-log po (y|x)],", "L(0) = Ldiffusion-kto(0)\n+ \u03bb\u00b7 E(y,x)~Dreal [- log po (y|x)],"]}