{"title": "SoK: Watermarking for AI-Generated Content", "authors": ["Xuandong Zhao", "Sam Gunn", "Miranda Christ", "Jaiden Fairoze", "Andres Fabrega", "Nicholas Carlini", "Sanjam Garg", "Sanghyun Hong", "Milad Nasr", "Florian Tramer", "Somesh Jha", "Lei Li", "Yu-Xiang Wang", "Dawn Song"], "abstract": "As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.", "sections": [{"title": "1 Introduction", "content": "Generative AI (GenAI) techniques have become increasingly advanced [1, 2, 3, 4, 5, 6], transforming how content is created across a wide range of fields, from education [7] and software development [8] to creative industries [9] and biological sciences [10]. Yet, the growing ability of these models to produce highly realistic and convincing content has also raised critical concerns about authenticity, attribution, and potential misuse [11, 12, 13, 14]. Distinguishing between human-created and AI-generated content is becoming increasingly challenging, necessitating the development of effective techniques to maintain transparency, accountability, and the responsible use of GenAI.\nOne possible solution to this challenge would be simply keeping a record of all content generated by an AI model. Such records could be retrieved to verify whether any piece of content is AI-generated [15]. This solution suffers from a number of limitations: (1) it requires substantial storage and cross-organizational coordination; (2) it raises privacy concerns as the record could expose private interactions with AI models; and (3) it cannot be applied to open-source models.\nAnother alternative is post-hoc detection of AI-generated content [16, 17, 18, 19, 20], often relying on (possibly learned) statistical features to differentiate between human-created and AI-generated content. In the early stages of AI model development, these methods showed promise-generated images often exhibited"}, {"title": "2 Why Watermark GenAI Content?", "content": "In this section, we begin by highlighting the limitations of post-hoc detection methods for identifying AI-generated content. Next, we provide a brief history of watermarking and discuss its heightened importance in the era of GenAI. We then outline the key use cases and motivations for implementing GenAI watermarks. Finally, we examine current policies related to GenAI watermarking in both government and industry. Together, these points underscore the critical need for watermarking AI-generated content."}, {"title": "2.1 Limitations of Post-hoc Detection", "content": "Post-hoc detection methods for identifying AI-generated content can be broadly classified into two categories: zero-shot detection [17, 18, 20] and training-based detection [16, 19]. In zero-shot detection, the detector leverages statistical signatures that are characteristic of AI-generated content. On the other hand, training-based detection involves training a binary classification model on datasets that include both human and AI-generated content.\nWatermarking techniques actively add signals to ensure reliable detection, while post-hoc detection methods adopt a passive approach. Despite their versatility, post-hoc detection methods often exhibit low performance. The reported error rates are only validated empirically on limited datasets and typically do not fall below 10\u22123 [20, 35]. Additionally, post-hoc detection methods cannot provide theoretical guarantees for the false positive rates and face fundamental limitations when handling out-of-distribution data [21].\nAs generative models continue to advance, their outputs are increasingly realistic, sometimes even surpassing the quality of human-generated content. This makes previously trained classifiers less effective for detecting outputs from newer generative models. Furthermore, most existing post-hoc detection approaches aim to distinguish between human and AI-generated content without providing more granular information, such as model versions or user-specific details. These limitations make post-hoc detection suboptimal for detecting AI-generated content effectively."}, {"title": "2.2 History of Watermarking", "content": "Watermarking has a long historical background, rooted in the need to authenticate, protect, and trace various types of content. Its origins can be traced back to the early 13th century [36], when it was first employed in paper production. Thin wire patterns were added to paper molds, creating physical marks that identified the manufacturing mill or indicated the paper's quality, while sometimes serving as decorative elements [37]. By the 18th century, watermarking had become an essential tool in anti-counterfeiting efforts, particularly in currency production, where it was used to deter forgery and verify authenticity.\nWith the advent of digital technology, watermarking evolved to protect intellectual property, trace sources, and authenticate media in the digital domain [38]. Musicians, filmmakers, and artists began using digital watermarks to secure their work from unauthorized copying and redistribution. In digital media, watermarks could be embedded into images, audio, and video files to establish ownership or copyright, either as visible markers or as hidden, verifiable information embedded through algorithms. This approach helped combat piracy while upholding the rights of content creators.\nThe historical importance of watermarking lies in its dual role as a preventive and protective tool. From printed currency to copyrighted media, watermarking has consistently preserved authenticity and safeguarded"}, {"title": "2.3 The Needs of Watermarking in the GenAI Era", "content": "Watermarking has historically played a significant role and has become even more critical in addressing the unique challenges of GenAI. Below, we outline key use cases and motivations for implementing watermarking systems in the GenAI era.\nCombating Misinformation. GenAI can be exploited to generate misinformation at a large scale. Adding watermarks to AI-generated content could help trace the source of the information and, therefore, facilitate identifying misinformation and disinformation. At the least, it would help readers distinguish human-created content from AI-generated material if all AI content carried an AI flag.\nEnhancing Fraud Detection. While current scam campaigns are largely automated, they typically involve replicating similar messages to numerous recipients. This generic approach makes such content easily detectable. However, generative models could produce tailored scam messages, making traditional detection methods less effective. Watermarking could play a crucial role in identifying and mitigating such sophisticated scam attempts, bolstering anti-scam efforts.\nDeterring Academic Dishonesty. There are already many cases in which students use ChatGPT [1] to complete homework assignments. Watermarking can help prevent cheating in domains where it's important to validate that text is human-written, such as student essays. By making it easier to identify AI-generated content, educational institutions can better ensure academic integrity.\nAvoiding Training Data Contamination. Machine learning models often depend on data scraped from the public internet for training. As AI-generated content becomes more prevalent online, detecting AI-generated data becomes vital. If models are inadvertently trained on their own generated outputs, it can lead to \"model collapse,\" where iterative cycles of training on degraded data degrade the overall quality of generated content [41]. Watermarking can help to flag AI-generated content, allowing future models to prioritize high-quality human-created data, thereby preserving the integrity of training datasets.\nProviding Signatures and Attribution. Watermarks can serve as signatures, allowing users to verify that content was generated by a specific model. This is particularly useful for those concerned with using models trained on properly licensed data. Additionally, watermarks can help prove whether harmful content attributed to a model was genuinely generated by that model or not.\nBy addressing these needs, watermarking emerges as a critical tool for fostering responsible AI development, building trust, and mitigating misuse. While this paper primarily focuses on detecting GenAI content, other watermarking applications, such as model watermark and dataset watermark, are briefly discussed in Section 7."}, {"title": "2.4 Policies in Government and Industry", "content": "Watermarking AI-generated content has garnered significant attention from both governments and industries, highlighting its importance for responsible AI governance. By instituting policies that mandate watermarking, both government bodies and private companies are taking important steps toward minimizing the risks associated with GenAI content, while also empowering end users with the tools needed to discern the origins and authenticity of the content they encounter. This section summarizes the major developments."}, {"title": "2.4.1 Overview of Government Developments", "content": "Governments around the world are increasingly recognizing the importance of watermarking as a regulatory tool to combat misinformation and promote transparency, resulting in various policy developments. We summarize a few notable examples below.\nUnited States. In October 2023, the White House issued Executive Order 14110 [42], which introduces a set of guidelines for the executive branch related to GenAI content. A component of this order mandates that a number of government agencies produce a report within 240 days identifying state-of-the-art techniques for detecting, labeling (through methods such as watermarking), and tracking synthetic (e.g., GenAI) content. This report will be used to develop guidance for government agencies regarding detection of GenAI content. It was also noted that the Department of Commerce will take the lead in this effort.\nIn addition to this executive order, there have been other legislative efforts related to GenAI watermarking. At the state level, a notable example is California, which introduced the California AI Transparency Act (SB 942) [43]. This bill-which was signed into law by the governor, and is set to take effect in 2026-mandates that providers of GenAI models must release publicly-available tools to detect whether image, video, or audio content was created or modified by their models. Additionally, providers must include a \"latent disclosure\" in images, videos, and audio generated by their models, which must be compatible with their detection tools.\nIn addition to SB 942, California also proposed bill AB 3211 [44], which included additional provenance-tracking requirements alongside other GenAI-related provisions, though it was ultimately rejected by the State Senate. Other states, such as Ohio (senate bill 217 [45]), have also taken preliminary steps to introduce laws that would mandate watermarks on GenAI content.\nAt the federal level, several bills have been proposed by members of Congress, including the Advisory for AI-Generated Content Act (S.2765) [46], the AI Labeling Act (S.2691) [47], the COPIED Act [48], and the Artificial Intelligence Research, Innovation, and Accountability Act (S.3312) [49]. Among other things, mandates across these bills include requirements for inserting watermarks and disclosures on outputs of GenAI models, and requests for the development of standards and recommendations for detection of GenAI content.\nEuropean Union. In the EU, the preeminent policy development related to watermarking is the EU AI Act [50], which came into effect in August 2024. This comprehensive regulatory framework addresses a myriad of topics aimed at mitigating the risks associated with GenAI content. Most relevant to watermarking, Article 50 includes transparency provisions requiring that GenAI providers ensure that outputs of their models are \"marked in a machine-readable format\", enabling the detection of digital content generated or manipulated by their models. Recital 133 provides additional context for these provisions, explicitly mentioning watermarks and \"cryptographic methods\" as potential techniques for marking such content.\nThe specific, practical implementation of the requirements outlined in Article 50 will be based on forthcoming guidelines from the European Commission, as specified in Article 96.\nOther Government Regulations. Several other countries have also introduced regulatory measures to address challenges posed by AI-generated content. In China, the Provisions on the Administration of Deep Synthesis Internet Information Services [51] mandates that model providers include notable labels on GenAI content that may \"cause confusion or mislead the public\" (Chapter 3, Article 17). Similarly, South Korea's Content Industry Promotion Act [52] seeks to support the content industry while safeguarding creators' rights. To address issues arising from AI-generated content, South Korea has proposed a revised bill aimed at establishing clear legal frameworks for distinguishing and regulating such outputs, including text, images, and music.\nSummary. These policy developments around the world evidence the fact that regulatory bodies have a strong interest in identifying GenAI content, which they see as an important component of AI safety. However, the vagueness of these legal documents underscores the importance of a deeper understanding of the technical capabilities of watermarking schemes. As more jurisdictions develop their regulatory frameworks for GenAI watermarking, it is important for laws to \"align with the capabilities and limitations of current watermarking"}, {"title": "2.4.2 Overview of Industry Developments", "content": "In the industry sphere, major technology companies such as Google [54, 55], OpenAI [25, 56], and Microsoft [57] have started to implement watermarking systems voluntarily, either as standalone initiatives or as part of broader AI governance frameworks. These companies recognize that watermarking is an important component of responsible AI use, enabling better content moderation and ensuring that their generative models are not exploited to produce harmful or misleading content. Additionally, industry consortiums, like the Partnership on AI, have called for standardized best practices around the watermarking of AI content, ensuring cross-platform coherence and effectiveness. We discuss a few important developments below.\nC2PA. The Coalition for Content Provenance and Authenticity (C2PA) [58], a collaboration between the Content Authenticity Initiative (CAI) and Project Origin, is a group of companies and other stakeholders that seeks to establish standards for digital media provenance. CAI, founded by Adobe, the New York Times, and Twitter, focuses on enabling creators to establish authorship while empowering consumers to assess content reliability. Project Origin, founded by BBC, CBC Radio Canada, Microsoft, and the New York Times, aims to combat disinformation by developing methods to affirm content integrity.\nContent provenance offers a method to verify the origin of digital content, including information on its creator, creation time, and edits [59]. Unlike metadata, which can be manipulated, provenance data provides a reliable record, allowing users to distinguish authentic content from manipulated or synthetic material. By embedding trustworthy origin data, provenance reduces misinformation risks, enhancing transparency in digital content distribution at scale.\nGoogle DeepMind SynthID. SynthID [54, 55] is a pioneering technology developed by Google to address the challenge of identifying AI-generated content across various media formats, including text, audio, images, and video. By embedding imperceptible digital watermarks directly into AI-generated outputs, SynthID ensures content verification without compromising quality. The technology uses advanced deep learning models to adjust token probabilities in text, modify spectrograms in audio, and embed watermarks into image pixels or video frames, making the markers resilient to common alterations such as compression, cropping, or noise addition. SynthID's applications are broad, ranging from text generation via the Gemini platform [60] to audio production with Lyria [61], as well as image and video generation AI models like Imagen [62] and VideoFX [63]. Google recently released a public implementation of their text watermarking scheme [64], which will facilitate applying these techniques to other models."}, {"title": "3 What is a Watermark?", "content": "There are many different properties one might desire of a watermarking scheme, the relative importance of which depends on the application scenario. In this section, we will outline what we view as the most important of these properties.\nWe begin by introducing terminology, notation, and the syntax of a watermarking scheme in general. We have attempted to keep the presentation as simple as possible, without significantly compromising on generality or precision.\nLet M be any generative model. To denote the process of sampling a response x from the model on input prompt \u03c0, we write M(\u03c0) \u2192 x. For instance, x could be an image, a sequence of text, an audio sample, or a video.\nThe central piece of any watermarking scheme is the watermark generation algorithm Watermarkgk. This algorithm takes as input a prompt \u03c0, and uses the generative model M and the watermark generation key gk to produce a watermarked response x. We denote this process by Watermarkgk(\u03c0) \u2192 x. A \"standard\" watermark that simply embeds a detectable signal and no further message is called a zero-bit watermark. For multi-bit watermarks, Watermarkgk also takes as input a message m \u2208 {0,1}* to be embedded in the generation, and we write Watermarkgk(m, \u03c0) \u2192 x."}, {"title": "3.1 Quality", "content": "Ideally, the watermark should not degrade the quality of the model in any way. Watermarking works define and achieve various relaxations of this goal, and we present these quality notions from weakest to strongest.\nThese notions differ in two especially notable ways: heuristic versus provable, and single-response versus multi-response. Heuristic guarantees, namely empirical quality validation, are limited by the scope of the experiments they involve; for example, the watermark may significantly harm quality on prompts or tasks not captured in the experiments. Furthermore, the quality metric used in experiments may be biased or unreliable. On the other hand, the provable quality notions we present apply to any possible prompt, and ensure that any bias introduced by the watermark is limited, or even nonexistent.\nSecond, some guarantees (e.g., some empirical quality validation methods, low-distortion, and distortion-freeness) apply only to a single response from the watermarked model. On the other hand, other notions (e.g., stronger empirical quality validation methods and undetectability) show that any number of watermarked responses are jointly high-quality. This distinction is important in ensuring the watermark does not harm variability of the model, or introduce systematic biases. For example, an image watermark may cause a model to output exclusively images of dogs, but which are extremely high-quality. This watermark would satisfy the single-response quality notions, since this bias would only appear given multiple responses. This is especially problematic if the bias is more insidious, or if the model is being used for complicated downstream tasks involving multiple outputs."}, {"title": "3.1.1 Empirical Quality Validation", "content": "These empirical measurements include prompting other models to score the quality of watermarked content, asking users to compare the quality of watermarked versus unwatermarked content, or measuring the performance of downstream tasks that use model-generated content. Such experiments can involve many responses from the model and can therefore yield (empirical) multi-response quality guarantees. However, these empirical measures are not representative of all uses. For example, the experiments may overlook a class of prompts on which the watermark yields poor quality outputs.\nIn particular, for production-scale models the quality tests run on the original model are extremely cost- and resource-intensive. The empirical quality tests run in watermarking works do not approach this scale, with the exception of SynthID [54]. As a result, watermarks with only empirical quality guarantees are less likely to be fit for widespread use. However, empirical quality validation is often used in conjunction with the provable quality guarantees detailed below."}, {"title": "3.1.2 Low-distortion and Distortion-free Watermarks", "content": "We define the distortion as the maximum, over all model inputs, of the statistical distance between the watermarked and unwatermarked response distributions.\nDefinition 3.1 (Distortion). Formally, the distortion is\n$\\max_{m,\\pi} \\frac{1}{2} \\sum_{x \\in \\mathcal{R}} | Pr[M(\\pi) \\rightarrow x] - Pr[\\text{Watermark}_{gk}(m, \\pi) \\rightarrow x] |$\nwhere R is the set of possible (partial) responses and m is a message encoded by the watermark.\nSome language model watermarks, such as that of Green-Red Watermarks [26, 27], achieve low distortion over individual token distributions. Multiple or long watermarked responses may jointly exhibit greater degradation, and therefore low-distortion is fairly weak. For other modalities, where content is output in its entirety rather than in discrete units such as tokens, one typically takes R to be the set of entire responses.\nKuditipudi et al. [65] showed that for language models, is possible to obtain build schemes that achieve zero distortion for the distribution of an entire single response; such schemes are called distortion-free. While originally defined for text, distortion-freeness is achievable across all modalities, including images [66]. Distortion-freeness can be defined either computationally or statistically, and we present both definitions.\nDefinition 3.2 (Distortion-freeness [65]). Formally, a watermark is computationally distortion-free if for any prompt \u03c0, watermark message m, security parameter \u03bb, and polynomial-time algorithm D,\n$\\Pr_{x \\leftarrow M(\\pi)} [D^M(\\lambda, x) \\rightarrow 1] - \\Pr_{x \\leftarrow \\text{Watermark}_{gk}(m,\\pi)} [D^M(\\lambda, x) \\rightarrow 1] | \\leq \\text{negl}(\\lambda)$.\nIf the above holds even for computationally unbounded algorithms D, the watermark is statistically distortion-free.\nIn other words, the algorithm D is given a single response from either the original model, or the watermarked model. It may make additional queries to the original model, then output either 0 or 1 to indicate whether it believes the given response was watermarked. Distortion-freeness requires that in either case, the probability that D outputs 1 is the same up to a negligible additive term.\nComputational distortion-freeness is generally no weaker than statistical distortion-freeness in practice, as real-world algorithms are efficient. Distortion-free schemes ensure that the quality is preserved for any single response from the model, but do not guarantee anything about the quality across multiple generations. By simply using multiple pairs of generation and detection/decoding keys, it is possible to make any distortion-free scheme satisfy a low-distortion notion for multiple responses. However, this comes at the expense of the detector's computational efficiency and the false positive rate."}, {"title": "3.1.3 Undetectable Watermarks", "content": "A watermarking scheme is undetectable if it is computationally infeasible to distinguish between the output distributions of the original model M and the watermarked model without the keys, even when adaptive queries are permitted. In particular, unlike the extension of distortion-freeness, the number of queries need not be known in advance. We define only the computational version of undetectability, as it is impossible to achieve statistical undetectability [28].\nDefinition 3.3 (Undetectability [28]). A watermarking scheme is undetectable if, for every security parameter \u03bb and every polynomial-time algorithm D:\n$| \\Pr [D^{M, M}(1^\\lambda) \\rightarrow 1] - \\Pr [D^{M, \\text{Watermark}_{gk}}(1^\\lambda) \\rightarrow 1] | \\leq \\text{negl}(\\lambda)$"}, {"title": "3.2 False Positive Rate", "content": "A crucial property of the detector for watermarking scheme is that it has a low false positive rate. That is, it should be highly unlikely for our detector to flag any content that is produced independently from the watermarking keys.\nDefinition 3.4 (False positive rate). A watermark detector Detect has false positive rate at most \u03f5 if, for any fixed content x,\n$Pr[\\text{Detect}_{dtk}(x) \\rightarrow \\text{true}] < \\epsilon$.\nNote that the above definition is agnostic to the distribution of natural content. This is absolutely essential for the interpretability and credibility of the watermark detector. If the false positive rate only holds for content generated under certain conditions, then it is possible that the detector will disproportionately flag some kinds of content."}, {"title": "3.3 False Negative Rate and Robustness", "content": "The false negative rate describes how reliably one can detect the watermark in unmodified watermarked content. The false negative rate depends not only on the watermarking scheme, but also on the amount of randomness in the content being generated. For example, if the model is prompted to output a fixed response, one cannot hope to embed the watermark, unless one gives up distortion-freeness (which means not faithfully following instructions in the \u201cfixed-response\" case). Therefore, provable false negative guarantees often state that the watermark is detectable with high probability in sufficiently random content. Christ et al. [28] showed that such a condition is necessary for undetectable language model watermarks. For other modalities where nearly all content is highly random, such as images and audio, this randomness condition is assumed rather than specified as a condition.\nRobustness refers to the watermark's ability to withstand watermark removal attacks. In practice, content may be edited, compressed, or otherwise altered. For example, text may be paraphrased, while images may undergo resizing or compression. A robust watermark should be resilient to such modifications, ensuring that it remains detectable even if the content undergoes transformations.\nUnfortunately, definitions of robustness are significantly more complicated than those of the other properties described here. There are several reasons for this:\nRobustness must be defined with respect to a channel or class of channels. A channel \u2130 models the action of the environment or an adversary attempting to remove the watermark. Importantly, it is not possible to be robust to every channel. For instance, any scheme with a low false positive rate cannot be robust to any channel which is input-independent.\nRobustness may depend on the knowledge provided to the channel. It is sometimes possible to use the watermarking keys or detector/decoder access to craft stronger watermark removal attacks [67, 68, 69]. We denote the channel applied to content as \u2130(x), leaving the possible dependence on some or all of the keys implicit.\nRobustness depends on the \"entropy\" of the response. For instance, if the prompt asks for a completely deterministic response, then there is no way to embed a watermark in the first place let alone obtain robustness. We incorporate this dependence into our definition by only requiring the detector/decoder to function when a certain property P of the model, prompt, and content is satisfied.\nBecause of the above issues, we present an extremely broad definition of robustness. It is sometimes possible to prove that a scheme is robust to certain channels under certain conditions on the entropy of the response [27, 65, 70, 71], but watermark robustness is primarily evaluated empirically.\nDefinition 3.5 (Robustness). A watermark detector Detect is robust to a channel \u2130 with error \u03f5 for property P if, for any prompt \u03c0,\n$\\Pr_{\\substack{gk,dtk \\ x \\leftarrow \\text{Watermark}_{gk}(\\pi) \\x' \\leftarrow \\mathcal{E}(x)}} [\\text{Detect}_{atk}(x') \\rightarrow \\text{false} \\ \\text{and} \\ P(M,\\pi, x) = \\text{true}] \\leq \\epsilon$.\nA watermark decoder Decode is robust to a channel \u2130 with error \u03f5 for property P if, for any message m and prompt \u03c0,\n$\\Pr_{\\substack{gk,dtk \\ x \\leftarrow \\text{Watermark}_{gk}(m,\\pi) \\x' \\leftarrow \\mathcal{E}(x)}} [\\text{Decoded}_{ck}(x') \\neq m \\ \\text{and} \\ P(M,\\pi, x) = \\text{true}] \\leq \\epsilon$.\nThe property P typically measures some form of entropy of the content, ensuring that we do not require deterministic responses to be watermarked. For instance, in [28], P(M, \u03c0, x) is true whenever the probability of sampling x as a response to M on prompt \u03c0 is sufficiently low. Image watermarks also require entropy in the response, although this is usually not very limiting, and can be made robust to channels that introduce bounded error in the pixel/latent space.\nWhile robustness is a desirable property of a watermark, even non-robust watermarks may be useful in many settings because it is likely that most users will not attempt to remove the watermark. Such watermarks have a low false negative rate, i.e., strong robustness against the identity channel \u2130(x) = x. For text, the false negative rate can often be computed precisely in terms of the \"entropy\" in the given text portion, where the notion of \"entropy\" depends on the scheme. The utility of non-robust watermarks is especially apparent for watermarking applications where the user does not suffer any negative consequences when caught: For example, one may wish to detect AI-generated images online in order to omit them from training datasets."}, {"title": "3.4 Unforgeability", "content": "Thus far, we have only required that knowledge of the generation key allows one to embed a reliable watermark. In some settings, it is additionally desirable that knowledge of the generation key is required in order to embed a watermark. Such a guarantee, called unforgeability, is important when using a watermark for attribution: that is, determining with certainty that some content came from a model. For instance, if one wishes to use a watermark to blame a model for producing harmful content, unforgeability is essential.\nIn more detail, unforgeability says that it is computationally infeasible for an attacker without knowledge of an embedding key to produce watermarked content that was not output verbatim by the watermarked model. Note that, while unforgeability is related to the false positive rate, the latter says nothing about the success of a dedicated attacker intentionally trying to get their content flagged.\nThe first thing to observe about unforgeability is that it is fundamentally incompatible with robustness. If a detector is robust, then it is possible to generate a response from the model, modify it slightly to make it problematic, and then falsely attribute the problematic content to the watermarked model."}, {"title": "3.5 Support for Embedding Messages", "content": "If the watermark generation algorithm can embed messages in the the content, then we say that it is a multi-bit watermark. Such watermarks are crucial for meeting the increasing demands of customization in various applications of generative models. For instance, embedding detailed metadata, such as model version and user-specific information, enables efficient traceability of the text's origin across multiple models and users. These capabilities are particularly vital in large-scale deployments.\nIn general, the length of the message that can be embedded in some given content depends on the amount of entropy in that content. Intuitively, greater entropy gives the embedder more freedom in generating content that is consistent with its desired message. Images, audio, and video tend to have high entropy, and therefore are more amenable to embedding nontrivial messages. In contrast, text is relatively low entropy, and practical text watermarks typically do not support messages longer than a few bits."}, {"title": "3.6 Computational Efficiency", "content": "The watermarked generation process should impose minimal computational overhead, ensuring that the computational efficiency remains comparable to that of the unwatermarked generation process in terms of computation, memory usage, and throughput. Significant computational complexity can hinder real-world deployment, making the model less appealing for practical applications.\nThe computational requirements of detection, decoding, and attribution should also be kept at a minimum."}, {"title": "4 Threat Models", "content": "A common concern with watermarking schemes is their robustness against various perturbations and attacks. This section defines the threat models associated with watermarking, focusing on attack objectives, as well as"}, {"title": "4.1 Attack Objectives", "content": "We identify two primary adversarial objectives in watermarking schemes: removal and forgery. Note that when pursuing these two objectives, the attacker should also strive to preserve the quality of the generated outputs. Otherwise, by compromising the quality, any watermarked (or non-watermarked) content could be trivially identified as non-watermarked (or watermarked).\nWatermark Removal. The watermark removal attack aims to modify AI-generated content such that the detection algorithm classifies it as not watermarked, or the decoding algorithm extracts an incorrect message. This attack would effectively bypass the mechanisms designed to identify content originating from a specific model. The adversary removes watermarks embedded in generated content by introducing small perturbations that distort the watermarks while preserving content quality. Basic methods from prior studies include simple image manipulations, such as resizing or adding noise [33, 73], and text edits, such as random deletions or substitutions [26, 27]. However, more advanced attackers may develop specialized techniques to effectively remove watermarks while preserving the content's quality and coherence.\nForging Watermarks. The objective of the forging attack is the opposite of watermark removal. The adversary seeks to create content that is falsely classified as watermarked, despite not being processed by the designated watermarking scheme. This attack allows malicious actors to attribute their content to a specific model or system, potentially undermining trust and accountability mechanisms. Forgery can be achieved through various methods, from simpler approaches, such as collecting and analyzing watermarked content to reconstruct watermarks for future use, to more sophisticated attacks where knowledge of the target watermarking algorithm to embed them into any content of interest.\nSecret Extraction. A more advanced adversarial objective is the extraction of the secret keys used in the watermarking scheme. This attack is significantly harder than removal or forging attacks. In most cases, if an adversary successfully extracts the secret keys, they can easily remove or forge watermarks. However, it is important to note that extracting the secret keys is not a prerequisite for executing removal or forging attacks."}, {"title": "4.2 Adversary Knowledge and Capabilities", "content": "It is essential to explicitly define what systems and information an adversary is assumed to have access to. These capabilities determine the feasibility and success of various attack strategies. Here are several dimensions to consider:\nGenerator Oracle Access. Does the adversary have access to generate additional outputs watermarked by the same algorithm with the same key? Access to such an oracle can help the adversary understand how watermarks are applied and create more effective removal or forging attacks.\nAccess to Watermarked Content. If the adversary does not have oracle access to the watermarked generator, how many watermarked generations are available to them? Access to known watermarked contents can help in reverse engineering the watermarking process.\nAccess to Non-Watermarked Content. If the adversary lacks oracle access to the watermarked generator, how many non-watermarked generations do they have access to? This capability helps the adversary understand the differences between watermarked and non-watermarked content.\nWhite-box Access to the Model. Does the adversary have knowledge of the generative model underlying the watermark scheme, and do they have white-box access to this model? Such access enables the adversary to modify internal parameters or the generation process itself. This capability can facilitate watermark removal or allow the model to be repurposed for alternative tasks.\nNon-Watermarked Generator Access. Does the adversary have access to query the model without any watermarks applied? This would enable them to compare outputs to detect the presence of a watermark."}, {"title": "5 Empirical Evaluation Methodologies", "content": "This section outlines the standard practices for evaluating watermarks, with a particular focus on text and image watermarking. Key aspects of evaluation include detection effectiveness, robustness against attacks, and quality preservation."}, {"title": "5.1 Detection Effectiveness", "content": "The foremost property that a watermark should satisfy is that it is effectively detectable", "include": "nAUROC (Area Under the Receiver Operating Characteristic Curve): Measures the trade-off between true positive and false positive rates of watermark detection.\nFixed FPR Comparisons: In many scenarios, a high false-positive rate is untenable (e.g., when using watermarking to detect misinformation). It is thus standard to also evaluate the detection performance for a fixed, low false positive rate (e.g., at 0.1%).\nThe detection performance can often be computed analytically. Many watermarking works derive explicit theoretical bounds on their false-positive and false-negative rates. However, it is also good practice to complement these formal bounds with empirical measurements, for two reasons. First, bounds on the false negative rate (i.e., the likelihood that watermarked content is undetected) often depend on statistical characteristics of the content, such as"}]}