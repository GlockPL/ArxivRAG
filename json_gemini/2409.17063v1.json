{"title": "Benchmarking Domain Generalization Algorithms in Computational Pathology", "authors": ["Neda Zamanitajeddin", "Mostafa Jahanifara", "Kesi Xu", "Fouzia Siraj", "Nasir Rajpoot"], "abstract": "Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks.", "sections": [{"title": "1. Introduction", "content": "Deep learning (DL) models have shown great potential in addressing fundamental problems in computational pathology (CPath) [1, 2] such as histology image classification [3, 4, 5], tissue segmentation [6, 7, 8], and nuclei detection [9, 10, 11]. Furthermore, leveraging DL, more advanced problems are being tackled in the field, such as gene expression prediction [12, 13, 14] and biomarker discovery [15, 16, 17]. Regardless of the versatility and accuracy of DL models on the training domain data, it has also been shown that testing on unseen data can degrade the performance metrics considerably [18] (see Fig. 1A) which is a phenomenon usually caused by domain shift (DS) [19].\nDefining a data 'domain' as the joint distribution of feature (X) and label (Y) spaces, domain shift can be characterized by discrepancies in the joint distribution of features and labels across source (s) and target (t) domains, that is, $P_{XY} \\neq P'_{XY}$. According to the Bayes' theorem, the joint distribution is given by $P_{XY} = P_{X|Y}P_{Y} = P_{Y|X}P_{X}$ which can be leveraged to describe DS manifested in various ways [18]:\n\u2022 Covariate Shift: The feature distributions differ between the source and target domains, i.e., $P_{X} \\neq P'_{X}$. Example: Tissue samples scanned using different scanners exhibit distinct colors and features.\n\u2022 Prior Shift: The label distributions vary between the source and target domains, i.e., $P_{Y} \\neq P'_{Y}$. Example: A model trained on a dataset with a specific proportion of cancerous to non-cancerous samples is applied to a new dataset with a different ratio of these classes.\n\u2022 Posterior Shift: The conditional label distributions differ, i.e., $P_{Y|X} \\neq P'_{Y|X}$. Example: Subjective labeling in mitosis detection where different annotators assign different labels to the same data due to varying interpretations.\n\u2022 Class-Conditional Shift: The data characteristics for a specific class differ between the source and target domains, i.e., $P_{X|Y=y} \\neq P'_{X|Y=y}$. Example: Morphological traits of cancer cells in early-stage cancers differ from those in late-stage cancers, leading to variations in the same class across datasets.\nAn ideal way of, addressing domain shifts would involve training models across all conceivable data distributions. However, this approach is typically infeasible due to the limited availability of comprehensive, multi-domain data during the training phase. Consequently, there is an urgent need for algorithms specifically designed to improve domain generalization (DG). DG refers to the capability of a model trained on data from source domains $D_s$ to perform well on unseen target domains $D_t$ despite distributional differences ($P_{XY} \\neq P'_{XY}$). It is important to note that, unlike domain adaptation techniques [20], DG algorithms enhance generalization to novel target domains without access to target domain data during training [21, 18].\nThere remains a critical gap in the utilization of existing DG algorithms within CPath. Many sophisticated DG algorithms"}, {"title": "2. Material and Methods", "content": "In this study, we examine three datasets CAMELYON17 [23], MIDOG22 [24], and HISTOPANTUM - each chosen for its specific challenges and domain shifts to facilitate a comprehensive evaluation of DG algorithms applied to various classification tasks in CPath."}, {"title": "2.1.1. \u0421\u0410MELYON17", "content": "With the CAMELYON17 grand challenge on the detection of breast cancer metastases in sentinel lymph nodes, a dataset with the same name has been released which comprises Whole Slide Images (WSIs) of lymph node resections in breast cancer patients and their corresponding lesion-level annotations [23]. The provided annotations have been leveraged for the extraction of a patch-level dataset of metastatic and normal images of breast cancer. In particular, we used the CAMELYON17 dataset part of the WILDS toolbox [25] (which is designed to test machine-learning models against significant distribution shifts) to allow for the reproducibility of the results."}, {"title": "2.1.2. MIDOG22", "content": "The second task, mitosis detection, involves classifying mitotic figures versus mimickers (cells of other types that are very similar to mitotic figures in appearance), which is a binary classification task previously explored in the literature [18, 11, 26, 27].\nFor this purpose, we utilize the MIDOG22 dataset [24], which comprises five domains: Canine Lung Cancer, Human Breast Cancer, Canine Lymphoma, Canine Cutaneous Mast Cell Tumor, and Human Neuroendocrine Tumor. From the original MIDOG22 dataset and based on the annotations provided, we extract 20,552 image patches, each sized 128 \u00d7 128 pixels at a resolution of 0.25 mpp, and categorize them into mitosis and mimicked classes.\nMIDOG22 is particularly challenging for achieving DG due to the presence of all four types of DS: covariate shift is evident as the images come from different centers using various scanners, leading to variations in color schemes. There is a prior shift, as different labels are distributed variably across domains, clearly shown in the dataset Fig. 2B. The task also involves a posterior shift due to the highly subjective nature of mitosis labeling. Additionally, class-conditional shifts occur because different tumor types and species influence the appearance of non-mitotic regions in the images, significantly varying from one domain to another. The complexity of this dataset makes it a rigorous test bed for assessing the DG capabilities of different algorithms in CPath."}, {"title": "2.1.3. HISTOPANTUM", "content": "The last task we address is pan-cancer tumor detection, leveraging the HISTOPANTUM dataset that we were releasing in this study. This dataset captures four different cancer types: Colorectal (CRC), Uterus (UCEC), Ovary (OV), and Stomach (STAD), collectively referred to as four domains. During data curation, we source 40 WSIs for each cancer type from its related study in The Cancer Genome Atlas Program (TCGA) [28]. For sampling, we make sure to include a variety of tumor subtypes (adenocarcinoma and mucinous carcinoma), genders, ethnicities, and centers in order to make the dataset as diverse as possible. Then, an experienced pathologist (FS) meticulously annotates tumor and non-tumor regions in the slides, which are used to extract tumor and non-tumor patches from the WSIs to form the HISTOPANTUM dataset.\nThe HISTOPANTUM dataset includes 281,142 patches, each 512 x 512 pixels at a resolution of about 0.5 mpp which are subsequently resized to 224x224 pixels during training and evaluation. HISTOPANTUM patches are classified into two"}, {"title": "2.1.4. Subsampled datasets", "content": "In addition to the primary experiments, we conduct a series of tests to understand how different DG algorithms perform under a low-data budget scenario. This analysis is crucial for applications where data availability is limited. For this purpose, we create smaller versions of the original datasets, maintaining similar distributions but significantly smaller populations.\nThe reduced (small) datasets are generated by randomly sampling the following percentages of each class in the original datasets (as hatched regions shown in the bar plots of Fig. 2):\n\u2022 SCAMELYON17: 1% of the original CAMELYON17 dataset (N=4,560).\n\u2022 SHISTOPANTUM: 3% of the original HISTOPANTUM dataset (N=8,434).\n\u2022 SMIDOG22: 30% of the original MIDOG22 dataset (N=6,166).\nThe sampling percentage in each dataset is set to considerably reduce dataset size while keeping enough samples for convergence of DG algorithms. The same experimental setup, model selection strategy, and algorithms used in the original experiments are applied to these smaller datasets. This approach allows us to directly compare the performance and generalization capability of the algorithms in both 'large dataset' and 'small dataset' scenarios."}, {"title": "2.2. Algorithms", "content": "We utilize DomainBed [22] as our main benchmarking tool because it offers a robust and well-tested platform for fair and reproducible comparison of different DG algorithms. Furthermore, DomainBed is well maintained and contains the most number of state-of-the-art DG algorithms implemented in comparison to other DG benchmarking tools (such as DeepDG [29]). Using DomainBed, we can test different DG algorithms while also controlling algorithms and training hyperparameters. As well as the DomainBed implemented algorithms, we also investigate the two CPath-specific algorithms, namely stain normalization and stain augmentation, and a self-supervised learning (SSL) based algorithm in the same structured experimental setup of DomainBed. Furthermore, we have also added the F1-score evaluation metric to the platform which originally included only the Accuracy metric. \nAll the algorithms in our experiments use a standard ResNet50 [30] model for feature extraction due to its proven generalization capabilities and popularity. The preferred model selection strategy in our work is the \"training-domain validation set\", recognized for its effectiveness in different scenarios and datasets as shown previously in [22]. More information on how we performed cross-validation experiments using DomainBed is given in Section 2.3.\nIn the rest of this section, we introduce the DG algorithms investigated in this work. Explaining the methodology of each algorithm is outside of the scope of this work, although we have categorized these algorithms into 6 distinct categories of DG methods based on their working principles and the introduction of the categories in [18]. Domain alignment techniques bridge domain gaps by harmonizing feature representations, employing methods like stain normalization and generative models. Data augmentation enhances model generalization through image transformations and generative networks. Meta-learning enables quick adaptation to new domains or tasks using techniques like MAML [31]. Tailored model design strate-"}, {"title": "2.2.1. DomainBed algorithms", "content": "DomainBed [22], developed by the Facebook Research group, is a comprehensive PyTorch suite. At the time of writing this manuscript, it encompassed support for 27 DG algorithms, 10 computer vision datasets, and one CPath dataset (Camelyon17 WILDS [25]). The toolkit includes algorithms such as Empirical Risk Minimization (ERM) [32], Interdomain Mixup (Mixup) [33], Group Distributionally Robust Optimization (GroupDRO) [34], Conditional Domain Adversarial Neural Network (CDANN) [35], Learning Explanations that are Hard to Vary (AND-Mask) [36], Deep CORAL (CORAL) [37], Self-supervised Contrastive Regularization (SelfReg) [38], Marginal Transfer Learning (MATL) [39], and Adaptive Risk Minimization (ARM) [40]. It also supports Invariant Risk Minimization (IRM) [41], Domain Adversarial Neural Network (DANN) [42], Style Agnostic Networks (SagNet) [43], Learning Representations that Support Robust Transfer of Predictors (TRM) [44], Optimal Representations for Covariate Shift (CAD and CondCAD) [45], Representation Self-Challenging (RSC) [46], Maximum Mean Discrepancy (MMD) [47], Out-of-Distribution generalization with Maximal Invariant Predictor (IGA) [48], Variance Risk Extrapolation (VREx) [49], and Invariance Principle Meets Information Bottleneck for Out-of-Distribution generalization (IB-ERM) [50]. Additional algorithms include Empirical Quantile Risk Minimization (EQRM) [51], Spectral Decoupling (SD) [52], Quantifying and Improving Transferability in Domain generalization (Transfer) [53], Smoothed-AND mask (SAND-mask) [54], Meta-Learning Domain generalization (MLDG) [55], and Invariant Causal Mechanisms through Distribution Matching (CausIRL with CORAL or MMD) [56]."}, {"title": "2.2.2. CPath-specific algorithms", "content": "Ideally, the same tissue specimens, stained in different laboratories, should yield identical results, but this ideal is often unattainable. Stain variation can arise from differences in slide scanners, stain quality and concentration, and staining procedure [57]. Pathologists can easily disregard irrelevant features (such as stain variation) in a WSI that do not impact their diagnosis. However, deep learning models sometimes struggle with this task [58].\nStain Normalization. A preprocessing step that aligns the color distributions of histology images to a reference image, counter-"}, {"title": "2.2.3. Self-supervised learning (SSL)", "content": "For our SSL experiments, we use a ResNet50 model pretrained on histology images to start each run instead of initializing the model with ImageNet weights. In particular, we choose a foundation model from the work of Kang et al. [64] which has a ResNet50 architecture and is pretrained on 19M image patches from different studies of TCGA using Barlow Twin self-supervised learning algorithm [65]. The utilized SSL algorithm also works on top of the ERM algorithm in our HistoDomainBed platform."}, {"title": "2.3. Cross-validation and model selection", "content": "Gulrajani et al. [22] meticulously examined three distinct model selection scenarios, which are crucial for determining how the performance of a model on a validation set can guide the selection of the best training epoch for application on unseen test sets. The scenarios we explore are:\n1. Training-domain validation set: This involves pooling a validation set from all training domains, a standard practice of training/validation split across domains. The best-performing model of the validation set is then tested on hold-out test domains.\n2. Leave-one-domain-out validation: In this scenario, one of the training domains is reserved for validation. The model that performs best on this holdout domain is then re-trained on all domains before being applied to the test domains."}, {"title": "3. Results", "content": "This section presents the performance metrics of different algorithms across various datasets. The metrics reported are binary F1 score and accuracy. We added the F1 score to HistoDomainBed to account for the scenarios where the data is significantly imbalanced, rendering accuracy a sub-optimal metric. Using Accuracy and F1 Score, we can comprehensively understand our models' performance across different domains, ensuring that our evaluation is robust and reliable."}, {"title": "3.1. Results for full datasets", "content": "We present the performance metrics of various algorithms across full-scale datasets in Table 1. Accuracy and F1 metrics are reported for each dataset (task) separately as well as for the average performance across all the tasks. The rows in Table 1 are ordered by the average F1 score across all tasks. In each column, cells are colored from red to green based on the performance values, red indicating worse and green indicating better performance.\nThe majority of methods exhibited similar performance, with average F1 scores ranging from 81% to 85%, except for the top 2 algorithms. SSL [64] and StainAug [58] methods consistently outperform all other methods on average, both in terms of F1 score (87.7%, 86.5%) and accuracy (88.9%, 87.4%). This advantage is particularly pronounced in the MIDOG22 and HISTOPANTUM datasets. The third-ranking algorithm, ARM [40], has done relatively worse on the MIDOG22 dataset. Furthermore, the Macenko stain normalization algorithm ranked 6th, outperforming 24 other DG algorithms, but not as good as StainAug.\nAll algorithms performed exceptionally well on the CAMELYON17 dataset (F1>90%), which can be attributed to the abundance of data to help the model generalize better and the relatively simpler nature of the problem. The high performance across algorithms indicates that the CAMELYON17 dataset poses fewer challenges in terms of DS. The DS in CAMELYON17 is primarily stain variation between different hospitals, making StainAug one of the best candidates to improve DG in this dataset (as Table 1 shows the highest F1 96.1% for StainAug).\nThe tasks associated with the MIDOG22 and HISTOPANTUM datasets are more challenging, involving more significant domain shifts. Specifically, the performance metrics in the MIDOG22 task are generally lower compared to other tasks, reflecting the increased difficulty.\nNotably, the baseline algorithm, ERM[32], demonstrated strong performance (ranked 17th), comparable to other SOTA methods. This suggests that combining simple augmentations with the ERM approach is sufficient to train a robust classifier. On the other hand, SANDMask [54] and IGA [48] algorithms struggled to converge on the CAMELYON17 and all datasets, respectively, indicating potential issues in handling domain shifts or complexities in these tasks."}, {"title": "3.2. Results for Sub-sampled (small) datasets", "content": "To investigate how DG algorithms perform under a low-data budget scenario, we subsample each dataset at different rates to create smaller datasets (as explained in Section 2.1.4) and repeat the experiments. The results for these experiments are reported in Table 2.\nInterestingly, SSL and StainAug algorithms are still among the top 3 performing algorithms with Transfer algorithm [53] place on the second rank and achieving F1 of 82.8% (almost on a par with StainAug, F1=82.7%). However, SSL considerably outperforms other algorithms by gaining the F1 score of 85.4%, showing an advantage in small-dataset scenarios as has been"}, {"title": "3.3. Domain-level performance", "content": "We comprehensively evaluate the performance of various algorithms across different domains, the results of which are presented in Fig. 4 in the form of bar plots. In Fig. 4, each domain in every dataset is represented by a uniquely colored bar. The average performance of all algorithms over each domain is indicated by horizontal dashed lines in the same color as the domain. In Fig. 4, algorithms \"IGA\" and \"SANDMask\" are excluded due to poor performance and to better visualize the working performance range of other algorithms.\nCAMELYON17. Performance across centers is generally high, with average F1 scores around 93% to 96%. This is highlighted by the closely clustered bars and the horizontal dashed lines at the top of the graph (only a 3.5% difference between the best and worst domains). Results for CWZ and RUMC centers are consistently among the highest scores, presumably because slides from these two centers were scanned using the same scanner (at RUMC center) and there is a lower domain shift between these two datasets, hence a model trained on the data from one of these domains will perform reasonably good"}, {"title": "4. Discussion", "content": "Benchmarking DG algorithms is essential for evaluating their performance across diverse datasets and scenarios to provide insights that can increase model robustness in real-world applications. In this study, we benchmarked various DG algorithms with different working principles, including SOTA algorithms from DomainBed collection [22], self-supervised learning [64], and pathology-specific techniques [60, 58], on datasets with different DS and size properties. Our unified and fair benchmarking process reported both accuracy and F1 scores for comprehensive evaluation through robust cross-validation experiments.\nAt a glance, the average best-performing algorithms are SSL and StainAug. SSL methods excelled especially on the HISTOPANTUM dataset, which is the main reason SSL ranked first in the full-scale dataset (Table 1) and small-scale dataset (Table 2) scenarios. This is mostly because SSL pertaining was done on an extensive set of patches extracted from TCGA, the same source used to curate the HISTOPANTUM dataset. Although the same image patches and labels are not shared between HISTOPANTUM and the dataset used during pertaining of SSL, the SSL has indirect access to the test data in HISTOPANTUM and already has seen the possible variation of image data within HISTOPANTUM. Therefore, the evaluation of SSL on HISTOPANTUM is more of a \u201cdomain adaptation\" exercise rather than \"domain generalization\" and comparing its performance (F1=91.7%) with the rest of the algorithms (such as ARM and StainAug with F1 scores of 88.6% and 87.3%, respectively) is not fair. Nevertheless, SSL shows excellent performance in CAMELYON17, MIDOG22, and the low-data-budget scenario of the SCAMELYON17 dataset. However, that is not the case with the sMIDOG22 dataset where there are all sorts of DS and data shortage problems.\nCPath-specific algorithms-stain augmentation (StainAug) and stain normalization (Macenko)-outperform most complex DG algorithms in the literature while being simple and easy to implement. In particular, StainAug excelled in the CAMELYON17 and MIDOG22 datasets. StainAug helps the model to learn more stain-invariant feature representations from the image during the training by randomly tweaking the stain information on the fly. Considering stain variation as a confounding factor, StainAug can be thought of as a causal approach for DG in CPath as it helps to learn features that are irrelevant to stain variation, hence achieving outstanding results on the CAMELYON17 dataset where the main DS is covariate shift (or changes in stain appearance). Macenko stain"}, {"title": "4.1. DG guidelines", "content": "While we cannot pinpoint a single best algorithm for all scenarios, we can suggest general guidelines that help narrow down the number of DG algorithms to investigate for a specific application. First and foremost, ensure that the experiments are designed properly. For example, if cross-validation is implemented, make sure there is no data leakage and use domain-level stratification of cases between the train and test sets. Then, it is recommended to fine-tune a pretrained model (such as SSL [64]), instead of learning from scratch or start-"}, {"title": "4.2. Limitations", "content": "This study uses a ResNet50 model [30] as the feature extractor with all DG algorithms. While ResNet50 is known for its robust performance in various tasks, the choice of model capacity and architecture can have a significant impact on the generalizability [74, 75]. A larger or alternative model architecture could potentially enhance performance. Our focus is specifically on the DG algorithm and the ResNet model due to its properties is considered to be well-suited for this study. Furthermore, one of the best-performing algorithms is SSL which is a ResNet50 model pretrained on TCGA image patches [64]. However, there are many more recent foundation models released for CPath that have shown DG capability [76, 77, 78, 79]. We could not include any of these models as a DG algorithm in our comparison due to their considerable difference in model size and architecture from other algorithms, however, we expect that replacing the SSL [64] with foundation models and also using them in conjunction with other recommended DG algorithms may further improve the results. This is an interesting question to investigate in future foundation model research areas.\nFurthermore, despite our efforts to determine the optimal working range for each algorithm, there remains the possibility that further tuning could yield improved performance. The results presented should be considered as initial findings from an out-of-the-box application of these methods. Future work"}, {"title": "5. Conclusions", "content": "This study reports a comprehensive benchmarking of various domain generalization algorithms on three diverse computational pathology tasks. We evaluated 30 algorithms, including SOTA methods, self-supervised learning, and pathology-specific techniques, using a robust cross-validation framework. Our findings reveal that self-supervised learning and stain augmentation were consistently among the best-performing algorithms, emphasizing their effectiveness in addressing domain shifts. The baseline ERM algorithm also demonstrated competitive performance, highlighting the importance of careful experiment design and proper implementation of baseline algorithms.\nThe results underscore that no single \u201cbest DG algorithm\u201d exists for all scenarios. The choice of the DG algorithm should be guided by factors like dataset size and diversity, domain shift types, and task complexity. Nevertheless, our study offers recommendations that may be useful for selecting effective DG strategies in CPath. We suggest fine-tuning a pretrained"}]}