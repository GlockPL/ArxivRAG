{"title": "A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models", "authors": ["Jingjing Xu", "Caesar Wu", "Yuan-Fang Li", "Gr\u00e9goire Danoy", "Pascal Bouvry"], "abstract": "Transformer-based models for time series forecasting (TSF) have attracted significant attention in recent years due to their effectiveness and versatility. However, these models often require extensive hyperparameter optimization (HPO) to achieve the best possible performance, and a unified pipeline for HPO in transformer-based TSF remains lacking. In this paper, we present one such pipeline and conduct extensive experiments on several state-of-the-art (SOTA) transformer-based TSF models. These experiments are conducted on standard benchmark datasets to evaluate and compare the performance of different models, generating practical insights and examples. Our pipeline is generalizable beyond transformer-based architectures and can be applied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in our experiments. The goal of this work is to provide valuable guidance to both industry practitioners and academic researchers in efficiently identifying optimal hyperparameters suited to their specific domain applications. The code and complete experimental results are available on GitHub\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series forecasting (TSF) is important for decision making across diverse practical domains, making it a continuously evolving field. Over time, TSF models have progressed from classic approaches, such as auto-regressive-moving-average (ARMA) models and exponential smoothing, to more sophisticated deep learning models, largely due to rapid advancements in computational capabilities [6, 17]. Among these advancements, deep learning models, particularly transformer models, have demonstrated significant potential in improving the accuracy and efficiency of TSF. However, these models often depend on a wide range of hyperparameters, and optimizing them typically requires substantial expertise. Thereby, these models increase the technical barriers for users attempting to apply these models to different datasets with varied hyperparameter configurations [8, 33, 15]. As a result, hyperparameter optimization (HPO) become increasingly critical for ensuring the effective use of transformer models in TSF.\nIn the context of machine learning and deep learning, HPO refers to the process of selecting an optimal set of hyperparameters for a model to minimize a predefined loss function with given specific dataset [5].While several studies have explored HPO for various machine learning models and TSF applications, research specifically focused on HPO for transformer-based TSF models remains limited. To address this gap, we introduce a unified HPO pipeline designed specifically for transformer-based TSF models. Additionally, we evaluate several state-of-the-art (SOTA) models on standard datasets to provide practical insights and examples of the pipeline's effectiveness.\nThe structure of the paper is as follows: Sec. II reviews the background and related work; Sec. III details the experimental setup; Sec. IV presents the results and analysis; and Sec. V concludes the paper with future directions. Our aim is to enhance the reproducibility, fairness, and performance of TSF models across diverse datasets and domains. The key contributions of this work are as follows:\n1) We introduce a Hyper-Parameter Tuning pipeline specifically designed for transformer-based and other TSF models.\n2) We benchmark standard datasets using various SOTA TSF models, including transformer-based models, Mamba [9], and TimeMixer [23].\n3) We perform a comprehensive analysis of the experimental results."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Transformer-based Time Series Forecasting\nThe Transformer model has emerged as a good option for time series forecasting (TSF) due to its excellent ability to capture long-range dependencies. Several transformer-based forecasting models have been developed to address various forecasting challenges. Informer [35] and Autoformer [28] pioneers the adaptation of transformer components for time series applications. Subsequently, the Crossformer model [34] focuses on capturing cross-time and cross-dimensional dependencies to enhance multivariate time series forecasting. Meanwhile, the Non-stationary Transformer [14] addresses issues related to stationarity in forecasting tasks. Following this, PatchTST [16] employs patching and channel-independent architectures to effectively capture both local and longer lookback information. The iTransformer [13] reconfigures the traditional transformer structure, offering an alternative approach for time series forecasting. Comprehensive surveys [2, 25, 30] provide an overview of transformer models tailored for TSF. Additionally, researchers have begun to apply transformer-based TSF methods in finance [27, 26]. Furthermore, emerging techniques and models for time series forecasting include graph representation learning [11], Large Language Models (LLMs) [12], Mamba [9], and TimeMixer [23].\nB. HPO for Transformer\nHyperparameter optimization (HPO) search algorithms include Grid Search, Random Search, Bayesian Optimization (BO), Tree Parzen Estimators (TPE), and others [33, 22]. HPO is a critical component of machine learning, as it enables models to select the optimal set of hyperparameters to maximize performance on a given dataset. Some researchers have addressed HPO in the context of common machine learning models [5, 32], and discussions surrounding HPO for deep learning and neural networks have also emerged. Generally, hyperparameters in deep learning can be classified into two categories: those related to model architecture and those associated with training and optimization. In works related to model architecture, a recent survey [3] explores neural architecture search (NAS) benchmarks, highlighting the need for efficient search algorithms. Additionally, another survey [4] summarizes the NAS landscape for Transformers and their associated architectures, specifically discussing HPO in autotransformer [19] for time series classification tasks. However, the exploration of HPO for Transformers in time series forecasting tasks remains insufficient.\nC. HPO for Time Series Forecasting\nHPO is crucial for improving forecast performance and mitigating overfitting issues in time series forecasting. A review [15] identifies hyperparameter optimization (HPO) as one of the five key components of the time series forecasting pipeline, concluding that the grid search method is the most widely used in automated forecasting frameworks. Additionally, evolutionary optimization and Bayesian Optimization are often employed in high-complexity training processes. The paper [7] presents hyperparameter tuning algorithms specifically for Long Short-Term Memory (LSTM) networks, aiming to efficiently determine the optimal set of hyperparameters. Furthermore, another paper [20] proposes a distributed HPO approach for time series forecasting based on electricity dataset. Researchers [10] investigate the three hyperparameter tuning toolkits Scikit-opt, Optuna and Hyperopt, and then apply these toolkits to Convolutional Neural Networks (CNN) and LSTM models for wind power prediction. Recently, the paper [21] introduced an automatic hyperparameter tuning framework for the Temporal Fusion Transformer (AutoTFT) model (for multi-horizon time series forecasting). Despite these advancements, research focused on HPO for various transformer-based time series forecasting models across different model on datasets remains limited."}, {"title": "III. EXPERIMENTS", "content": "In this study, we perform hyperparameter optimization for long-term time series forecasting. The primary objective is to identify a set of hyperparameter values that minimizes forecasting errors across different models.\nA. Dataset and Metrics\nWe utilize widely-used open-source datasets for long-term time series forecasting, including ETTh1, Weather and Electricity. The evaluation metrics employed in this experiment are Mean Squared Error (MSE) and Mean Absolute Error (MAE), which are commonly used to assess model performance, as noted in the survey [30]. A summary of the datasets is provided in Tab. I, and detailed descriptions of the datasets can be found in related works [28, 35].\nB. Environment and Configuration\nAll experiments in this study were conducted on a single Nvidia TU02 GPU.\n1) Model and its Setting: For our case study, we randomly selected four transformer-based TSF models: Autoformer [28], Crossformer [34], Non-Stationary Transformer [14], and PatchTST [16]. Additionally, we include other state-of-the-art (SOTA) models, such as Mamba [9] and TimeMixer [23], for comparison. For model settings, we selected the long-term forecasting task as the primary focus of this paper. Additionally, while we arbitrarily chose a prediction length of 96 as a use case, other prediction lengths (192, 336, 720) can be implemented in a similar fashion. In addition, the evaluation metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE), which are standard measures for model performance."}, {"title": "C. HPO Pipeline", "content": "The hyperparameter optimization (HPO) pipeline is depicted in Fig. 1. The process begins with feeding data into the model, followed by the simultaneous execution of model training and hyperparameter tuning. Once the training is completed, the model is evaluated, and the results are output. In line with the Neural Architecture Search (NAS) framework, four key components are integral to the hyperparameter tuning process: collecting primitive search elements (hyperparameters), designing the search space, selecting the search algorithm, and evaluating performance to determine the optimal model or network [4]. OptunaSearch is employed in the experiments as the search algorithm, and the evaluation of results is discussed in Sec. IV. Therefore, we emphasize the collection of primitive hyperparameters and the design of the search space in this section."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "In our experiments, each model is subjected to 20 trials per dataset, employing the OptunaSearch algorithm for hyperparameter optimization. A total of 357 trials are conducted (6 models * 20 trials * 3 datasets, minus 3 unexecuted cases). We analyze these experiments by examining the best performance of each model and investigating the behavior of hyperparameters.\nA. Best results on different datasets\nTab. IV presents the best results for each model across the datasets. bold numbers indicate the best performance,"}, {"title": "V. CONCLUSION", "content": "This paper introduces a unified hyperparameter optimization (HPO) pipeline designed for transformer-based time series forecasting (TSF) models. We benchmark several SOTA models, including Autoformer, Crossformer, Non-Stationary Transformer, PatchTST, Mamba, and TimeMixer, across multiple datasets."}]}