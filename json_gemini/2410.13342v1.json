{"title": "DART: Disentanglement of Accent and Speaker Representation in Multispeaker Text-to-Speech", "authors": ["Jan Melechovsky", "Ambuj Mehrish", "Berrak Sisman", "Dorien Herremans"], "abstract": "Recent advancements in Text-to-Speech (TTS) systems have enabled the generation of natural and expressive speech from textual input. Accented TTS aims to enhance user experience by making the synthesized speech more relatable to minority group listeners, and useful across various applications and context. Speech synthesis can further be made more flexible by allowing users to choose any combination of speaker identity and accent, resulting in a wide range of personalized speech outputs. Current models struggle to disentangle speaker and accent representation, making it difficult to accurately imitate different accents while maintaining the same speaker characteristics. We propose a novel approach to disentangle speaker and accent representations using multi-level variational autoencoders (ML-VAE) and vector quantization (VQ) to improve flexibility and enhance personalization in speech synthesis. Our proposed method addresses the challenge of effectively separating speaker and accent characteristics, enabling more fine-grained control over the synthesized speech. Code and speech samples are publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "In recent years, Text-to-Speech (TTS) technology has advanced significantly, allowing high audio quality synthesis in multiple voices for applications such as voice assistants, audiobooks, and enter-tainment [1]. Despite their advancements, a significant challenge remains: effectively disentangling speaker identity and accent representations to achieve precise and personalized speech synthesis. With globalization, accents in speech technology are vital for effective communication, since a listener's ability to understand a speaker is determined by both the speaker's accent and the listener's familiarity with that particular accent [2]. However, expecting everyone to learn a single standard accent is impractical. Instead, we should focus on developing technologies that can generate accents according to the user's needs. Accents involve phonetic and prosodic variations influenced by factors like mother tongue or region [2, 3]. Since accent forms a part of one's idiolect, it may often overlap with speaker identity [2], which makes the disentanglement a challenge. Successfully disentangling the two elements would allow for personalized speech synthesis, improving user experiences for minorities by aligning the system's accent with their own to promote intelligibility, thus enhancing interaction with TTS voice assistants and audiobook narrators.\nThe introduction of deep learning to TTS pushed the research forward with models like WaveNet [4], Tacotron [5, 6], and Fastspeech2 [7]. Multi-speaker TTS systems have advanced this field further, enabling speech synthesis in different voices and styles by training on diverse datasets with recordings from multiple speakers [8, 9, 10]. These systems can mimic accents [11, 12, 13] and emotional expressions [14, 15]. Continued research in multi-speaker TTS is expected to enhance synthesized"}, {"title": "2 DART", "content": "The first component of DART is the TTS backbone, which closely resembles Fastspeech2 architecture, comprising of phoneme encoder, Variance Adapter, and Mel-Decoder, as depicted in Figure 1. To initialize the backbone, we perform pre-training on LibriTTS, an extensive multi-speaker dataset.\nThe model is trained using the reconstruction loss between the predicted mel spectrogram \\( \\hat{X} \\) and the ground truth mel spectrogram \\( X \\) is computed using Eq 1, where \\( ||.||_2 \\) denotes \\( L_2 \\) norm.\n\n\\( L_{recon} = ||X - \\hat{X}||_2 \\)"}, {"title": "2.2 ML-VAE Encoder", "content": "ML-VAE [17] leverage the hierarchical structure of data to model the joint distribution of observed data and latent variables across multiple levels. This allows to encode dependencies among latent variables and disentangle different factors of variation in data generation. Additionally, it can utilize grouping information from real-world datasets, identifying shared variations and learning group-specific factors. This makes it ideal for datasets with natural grouping or clustering, such as"}, {"title": "2.3 Vector Quantization", "content": "We extend the ML-VAE framework from [16] by integrating VQ into a unified architecture, DART. Our design incorporates separate VQ modules for accent and speaker in the ML-VAE encoder (Figure 1, with codebook dimensions \\( d_i \\) for speaker (s) and accent (a). The reparametrized speaker \\( z_s \\) and grouped accent \\( z_a^g \\) representations pass through the VQ layer, acting as a bottleneck [18], filtering out irrelevant information. This integration improves accent conversion and preserves key information by effectively disentangling speaker and accent attributes. The VQ block incorporates an information bottleneck, ensuring effective utilization of codebooks. We define a latent embedding space \\( e^i \\in \\mathbb{R}^{d_i \\times D} \\), where \\( d_i \\) represents the size of the discrete latent space, \\( i \\in \\{s, a\\} \\) denotes speaker and accent, and \\( D \\) corresponds to the dimensionality of each latent embedding vector \\( e^j \\). It is important to note that within this space, \\( d_i \\) embedding vectors \\( e^j \\in \\mathbb{R}^D \\) exist, where j ranges from 1 to \\( d_i \\). To ensure that the representation sequence effectively commits to an embedding and to prevent its output from growing, we incorporate a commitment loss, following prior research [18], for each VQ module. This loss helps in stabilizing the training process:\n\n\\( L_c = ||z_{ei}(x) - sg[e^i]||^2 \\)\n\nwhere \\( z_{ei}(x) \\) is the output of the \\( i^{th} \\) vector quantization block (\\( i \\in \\{s, a\\} \\)), and \\( sg \\) stands for the stop gradient operator. Finally, by adding the KL loss multiplied by coefficient \\( \\beta \\), the total loss for training is computed as:\n\n\\( L_{total} = L_{recon} + \\beta L_{kl} + L_c \\)"}, {"title": "3 Experimental Setup and Results", "content": "We use two datasets for training: the train-clean-100 subset of LibriTTS [20] (LTS), and the L2-ARCTIC dataset [19]. LTS includes 247 English speakers, whereas the L2-ARCTIC dataset comprises 24 L2 (second-language) speakers representing 6 accents, with each accent having 4 speakers (two females and two males). The evaluation is conducted on the L2-ARCTIC validation set.\nWe train the baselines and the proposed model using two strategies. First, we train the TTS system from scratch with accented data. Second, a two-step process where the TTS backbone is initially trained on an English-only corpus, yielding a pre-trained multispeaker backbone model which uses GE2E speaker embeddings [21], and then fine-tuned with accented data. In this case, if the model uses GST or ML-VAE modules, they replace the GE2E speaker embeddings from pre-training. Details on training parameters and procedure can be found in A.2. We then evaluate and compare DART's performance against various TTS architectures with both autoregressive and non-autoregressive frameworks. We define the baselines and different variants of the proposed model as follows:\nBaselines: MLVAE-TACO represents the TTS architecture proposed in [16]. It consists of Tacotron2 with ML-VAE and is trained with L2-ARCTIC. MULTISPK-FS2 is our pre-trained multispeaker FastSpeech2 backbone model, pre-trained on LTS, fine-tuned on L2-ARCTIC. GST-FS2 is a pre-trained multispeaker FastSpeech2 model with a GST to model speakers/accents. GST-GE2E-FS2 is a pre-trained multispeaker FastSpeech2 model with a GST to model accents and GE2E embeddings to model speakers."}, {"title": "4 Conclusion", "content": "Our proposed approach significantly enhances the capabilities of multispeaker TTS models by effec-tively disentangling speaker and accent representations, resulting in more flexible and personalized speech synthesis. This has broad applications in entertainment; personalization of virtual assistants, narrators; and more. By utilizing ML-VAE and VQ, our proposed method achieves superior accent conversion. In future work, we will focus on further advancing the disentanglement between speaker and accent in multispeaker TTS. This includes addressing the trade-off between disentanglement and naturalness, expanding datasets and exploring real-time zero-shot adaptation techniques."}, {"title": "Acknowledgments and Disclosure of Funding", "content": "This project has received funding from SUTD Kickstarter Initiative no. SKI 2021_04_06.\nThe work by Berrak Sisman was funded by NSF CAREER award IIS-2338979."}, {"title": "A Appendix / supplemental material", "content": "Here, we present the objective results for our VQ codebook size experiment, as seen in Table 2. We observe that while DART512 achieves the best performance in MCD and CS, DART64 demonstrates superior performance in FFE and WER. This indicates that the choice of codebook size involves balancing various objective metrics, with smaller sizes favoring some metrics and larger sizes favoring others. Since the differences in FFE and WER were negligible and we aimed to select a model with low distortion and high speaker similarity, we used DART512 for subjective evaluation."}]}