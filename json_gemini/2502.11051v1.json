{"title": "MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models", "authors": ["Jiahao Huo", "Yibo Yan", "Xu Zheng", "Yuanhuiyi Lyu", "Xin Zou", "Zhihua Wei", "Xuming Hu"], "abstract": "Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMS, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient descent method MMUNLEARNER. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUNLEARNER surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs) achieved remarkable performance on various multimodal applications (Dang et al., 2024; Li et al., 2025b; Yan et al., 2024a,b; Zou et al., 2025). A common framework of MLLMs, which projects the visual embeddings extracted from pre-trained vision encoder into the representation space of language models with a projector, has enabled LLM backbone to understand visual inputs and preserve their powerful reasoning and generation potential (Huo et al., 2024; Liu et al., 2024a; Yan et al., 2025). However, The rapid development of MLLM is also accompanied by safety concerns such as personal privacy (Pi et al., 2024) and copyright infringement (Li et al., 2024b). Retraining the models from scratch to exclude the risky knowledge is resource-intensive and practically untenable due to the inaccessible pre-training data (Bourtoule et al., 2021; Si et al., 2023). Hence, Machine Unlearning (MU) can serve as a feasible solution to forget specific knowledge embedded within pre-trained models (Blanco-Justicia et al., 2025).\nNevertheless, MU on MLLMs is still in its nascent phase, with limited approaches and benchmarks available. For example, Single Image Unlearning (SIU) first explores the MU of MLLMs, aiming to erase visual patterns in MLLMs on real-world entities, but it needs to reconstruct multi-faceted fine-tuning data for forgetting (Li et al., 2024c). Besides, MLLMU-Bench evaluates the performance of MU methods designed for LLMS on fictional personal profiles (Liu et al., 2024d); CLEAR adds visual images to pure-text LLM unlearning benchmark TOFU through Photomaker (Li et al., 2024e), a diffusion model adapted for customized realistic human (Dontsov et al., 2024).\nAs shown in Figure 1 (a), the aforementioned works just transfer LLM-based MU methods to MLLMs via fine-tuning on VQA data, and neglect the unique difficulty for MLLM-specific MU.\nTherefore, we propose to reformulate the task of multimodal MU in the age of MLLMs, as illustrated in Figure 1 (b). Unlike text-only LLMs, the knowledge embedded in MLLMs extends beyond textual factual knowledge within their LLM module, which includes learned visual attributes associated with various concepts (Cohen et al., 2024; Yu and Ananiadou, 2024). Given this fundamental difference, we define the objective of MLLM-based MU as the selective removal of visual patterns linked to a specific entity, while preserving the corresponding textual knowledge within the LLM backbone, as illustrated in Figure 2. Considering that existing benchmarks have largely overlooked this crucial distinction, we aim to address this gap and ensure that multimodal MU methods can focus on the unique characteristics of MLLMs.\nTo erase the memorized visual representation while preserving corresponding factual knowledge within MLLMs, we propose MMUNLEARNER, a geometry-constrained gradient ascent MU method to update the parameters for targeted visual patterns. Motivated by the selective unlearning paradigm for visual networks and diffusion model (Fan et al., 2023b; Huang et al., 2024a), we further extend it to MLLMs, with an appropriate saliency map (depicted by Fisher matrix in parameter space) designed for each module. Extensive experiment show that applying LLM-based MU methods to MLLMs with VQA data adjusts textual factual knowledge solely, whereas MMUNLEARNER can efficiently remove the visual patterns while maintaining factual knowledge. Our findings offer valuable insights into multimodal intelligence in the era of Artificial General Intelligence (AGI).\nOur contributions can be summarized as follows:\nWe are the first to reformulate the setting of Multimodal Machine Unlearning based on the characteristics of MLLM architecture. Our focus is to erase the memorized visual representation while preserving corresponding factual knowledge.\nWe propose a new weight saliency-based unlearning method, MMUNLEARNER, to selectively update the parameter of MLLMs, displaying superior performance in visual concepts erasing as well as preserving untargeted visual concepts and textual knowledge under the same setting.\nWe conduct extensive experiments on representative MLLMs and carry out in-depth analyses of performance differences and potential mechanisms, which sheds light on the future development of multimodal intelligence towards AGI."}, {"title": "2 Related Work", "content": "2.1 Machine Unlearning for LLMs\nInitially developed for classification tasks, MU for LLMs has recently gained attention as a response to concerns regarding the unintended memorization of pretraining data (Si et al., 2023). The majority rely on parameter optimization-based methods (Nguyen et al., 2022), such as Gradient Ascent (Thudi et al., 2022) and its variations (Liu et al., 2022). While fine-tuning via cross-entropy loss remains a common practice, specific loss functions like KL minimization (Liu et al., 2024c; Nguyen et al., 2020; Wang et al., 2023) and IDK (Maini et al., 2024) have been designed to better control the outputs of unlearned models. Besides, Zhang et al. (2024) reframe LLM unlearning as a preference optimization problem (Rafailov et al., 2024), applying Negative Preference Optimization loss to enhance the unlearning.\nIn addition, MU algorithms that do not alter internal parameters have also been explored. These include approaches based on model editing (Ilharco et al., 2022; Wu et al., 2023), task vectors (Eldan and Russinovich, 2023; Li et al., 2024d), or in-context learning (Pawelczyk et al., 2023; Thaker et al., 2024). While free from tuning, they often fail to achieve a sufficient level of unlearning or incur higher computational costs for detecting privacy units (Ilharco et al., 2022; Wu et al., 2023).\n2.2 Multimodal Machine Unlearning\nBefore the development of MLLMs, research on MU in multimodal models primarily focused on Vision-Language Models (Radford et al., 2021) and Text-to-Image models (Rombach et al., 2022). For encoder-decoder models (Li et al., 2022, 2021), MultiDelete (Cheng and Amiri, 2024) introduces a method that separates cross-modal embeddings for the forget set while preserving unimodal embeddings for the retain set. Additionally, Yang et al. (2024) achieves class-wise forgetting in CLIP by fine-tuning selected salient layers solely on synthetic samples. In the context of T2I models, several pioneering studies (Gandikota et al., 2023; Zhang et al., 2023) have discussed to delete specific concepts, such as not-safe-for-work (NSFW) content, within diffusion models. Among these, SalUn (Fan et al., 2023a) and SFR-on (Huang et al., 2024b) selectively update salient parameters to balance the dual objectives of maintaining generalization and ensuring efficient data forgetting.\nDespite these advancements, MU for MLLMS remains in its nascent stages. Specifically, SIU (Li et al., 2024c) investigates the erasure of visual patterns in MLLMs using the real-world entity dataset MMUBench through multifaceted fine-tuning. There are also discussions about the application of MU in MLLMs, including hallucination mitigation (Xing et al., 2024) and safety alignment (Chakraborty et al., 2024)."}, {"title": "3 Our Proposed MMUNLEARNER", "content": "3.1 Task Setting\nTo enable a text-only LLM L to comprehend visual context, mainstream approaches extract visual embeddings H\u2081 using a vision encoder V followed by a projector W. The entire model is then fine-tuned with visual instruction data {X1, XQ, XA}, where X1 represents the input image, XQ is the textual instruction, and XA denotes the expected answer of length S. This can be formalized as follows:\nX0 = C(H1; XQ) = L(W(V(X1)); XQ),\nLoss = -$\\sum_{s=1}^S$ log P(X_A^{(s)}|X_A^{(<s)}).\nwhere Xo represents the output sequence of the model, $X_A^{(s)}$ is the target token at step t, and $X_A^{(<s)}$ represents the previously generated tokens in the output sequence. P($X_A^{(s)}$ | $X_A^{(<s)}$) represents the predicted probability of label $X_A^{(s)}$ at position s. Following this framework, MLLMs acquire the ability to recognize concepts in the visual modality, establishing associations between visual concepts and the internal knowledge of the LLM while leveraging its reasoning and generative capabilities.\nThe objective of multimodal MU is, therefore, to eliminate these learned associations between a specific concept and its corresponding visual patterns. In other words, the unlearned model should behave as if it has never encountered the related images during the visual instruction tuning process. Specifically, for a given concept C, its image representation 11, and the extracted visual embeddings h\u2081 = W(V(x1)), the unlearned model must satisfy the conditions in Box 3.1.\nIn standard unlearning tasks, the unlearned model is expected to maintain its knowledge on a retained set, which we denotes in next Section 3.2. We illustrate the expected behavior of the targeted model using biographical examples, though our task formulation can be extended to other domains, such as real-world entities and landmarks.\n3.2 Selective Updating for Forget Loss\nThe core challenge of multimodal MU lies in preserving textual knowledge while performing unlearning on VQA data. A naive unlearning method,\nsuch as GA Difference (GA_Diff), simply updates the model parameters 0 using a joint loss (LJ(.)) computed over the Forget VQA set Df = {(ICf, XQ, XA, Cf)} and the Retain VQA set Dr = {(Icr, XQu, XAv, Cr)} as follows:\nLJ (0t) = \u2212L\u00a3 (0t) + L\u00ba (\u03b8t),\nwhere t denotes the t-th step, and L\u17db (\u06f0) and L\u2033 (\u06f0) represent the loss on the Forget and Retain sets, respectively. Interpreting the update of 6 as an optimization problem in parameter space, the term -Lf (0+) forces the MLLM to forget the VQA samples that should be unlearned by following the steepest ascent direction. And L (0t) aims to preserve knowledge from the retained VQA samples. However, the conflicting directions of the Forget loss and the Retain loss make the unlearning process unstable. Furthermore, traditional MLLM unlearning methods primarily focus on VQA data, neglecting the constraints from text-only QA data. Nonetheless, such conflicts can be effectively mitigated if model updates selectively target parameters that are salient for the targeted knowledge (S) while preserving those critical for others. This process can be formulated as:\nLS(0+) = -m L\u00a3 (0t) + L\u00ba (\u03b8t),\nwhere m is a boolean mask that selectively updates parameters, and denotes the Hadamard product. In this way, the ascent of the Forget Loss on targeted visual concept does not destroy the parameters salient for the Retain set or textual knowledge, as illustrated in Figure 3.\n3.3 Weight Saliency Map in Parameter Space\nAs discussed in Section 3.2, the gradient mask m should strike a balance between forgetting and retaining knowledge so that only the necessary parameters are updated during unlearning. Inspired by Fan et al. (2023b); Huang et al. (2024a), the saliency map of each parameter on a given dataset D in the parameter space can be approximated by the diagonal of the initial model's Fisher information matrix:\nS(00, L, D) = $F_{diag}^{D} = [\\nabla_\\theta L_D(0_0)]^2$,\nwhich corresponds to a manifold defined by the loss function, dataset distribution, and initial parameters in the parameter space.\nFrom this perspective, we define a targeted dataset as:\nT = {(Icf, XQv, XA, Cf)},\nwhile the preserved dataset is defined as:\nP = {(XQt, XA\u0141, Cf)} \u222a {(XQ\u300f, XAv, Cr)} \u222a {(Icr, XQt, XA\u0141, Cr)},\nwhere Cf represents the targeted concepts to be forgotten, and Cr denotes the untargeted concepts that should be retained. Here, I, XQv/t, and XAvft represent the corresponding image, multimodal/pure-textual query, and correct answers, respectively.\nThus, the gradient mask m is obtained by comparing the relative ratio of the saliency map between the targeted and preserved datasets using a hard threshold:\nm = $1[\\frac{S(0_0, L, T)}{S(0_0, L, P)} > \\beta]$\n= $1[\\frac{\\nabla^2 L_T(0_0)}{\\nabla^2 L_P(\u2030)} > \\beta]$,\nwhere 1 [y \u2265 \u1e9e] is an element-wise indicator function that outputs 1 for the i-th element if yi \u2265 \u03b2 and 0 otherwise. The threshold \u1e9e > 0 is a hard cutoff; for simplicity, we use \u03b2 = 1 throughout our experiments, which is sufficient for our tasks."}, {"title": "4 Experiment", "content": "4.1 Experiment Settings\n4.1.1 Datasets and Metrics\nTo demonstrate the effectiveness of our proposed MMUNLEARNER, we conduct experiments on two MLLM-based unlearning benchmarks:\nMLLMU-Bench (Liu et al., 2024d). It consists of fictitious personal profiles, each accompanied by a portrait and 14 corresponding questions (i.e., 7 VQA questions and 7 textual QA questions) with multiple-choice options. For the Forget, Retain, and Real-world sets used in our experiments, we report the average accuracy as the metric.\nCLEAR (Dontsov et al., 2024). It is built on top of TOFU (Maini et al., 2024), a dataset containing fictional author profiles designed for LLM unlearning. For each author in TOFU, CLEAR adds several face images to it, along with captions generated by GPT-40 (OpenAI, 2023). In our experiments, we evaluate the Forget, Retain, and Real-world sets using average accuracy for VQA task and ROUGE-L (Lin, 2004) for textual QA task, respectively. Note that only VQA data is used for unlearning tuning in both datasets, while textual QA data is used solely for evaluation across different baselines, aligning with previous works. Please refer to Appendix B.1 and B.2 for details of the datasets and evaluation metrics.\n4.1.2 Evaluated MLLMS\nTo further verify the generalizability of our conclusions, we use two MLLMs, LLaVA-1.5-7B-hf\u00b9 and Qwen2-VL-7B-Instruct\u00b2, as our base models. The vanilla models used for unlearning are trained following the official implementations provided by MLLMU-Bench\u00b3 and CLEAR4 respectively. More details can be found in Appendix B.3.6 and B.3.1.\n4.1.3 Baselines\nFollowing Liu et al. (2024d), we compare our method with the following four baselines:\nGA (Thudi et al., 2022) applies opposite gradient updates on Forget VQA set Df.\nGA_Diff (Liu et al., 2022), an improved variant of GA, introduces joint loss to make a balance between Df and Retain VQA set Dr, as discussed in Section 3.2.\nKL_Min (Maini et al., 2024) aligns the model's predictions on Dr with those of the original model while encouraging divergence from the Forget Set, implementing by minimizing the KL Divergence.\nNPO (Zhang et al., 2024) treats Df as dispreferred data and casts unlearning into a preference optimization framework, with an oracle model fine-tuned exclusively on Dr.\nOur implementations are based on the official code from MLLMU-Bench and CLEAR, with the same pipeline. Considering that visual concepts can be stored in the vision encoder in real-world"}, {"title": "4.2 Main Result", "content": "In this section, we present the performance of MU methods on MLLMU-Bench and CLEAR dataset, offering a comprehensive comparison between four baselines and MMUNLEARNER, as detailed in Table 1. To validate the generalizability and efficiency of MMUNLEARNER, we further analyze the relationship between forget ratios and various metrics.\nOverall, our method provides a more accurate yet efficient approach to erasing visual concepts. The key observations are as follows:\nMMUNLEARNER excels in erasing visual concepts. For MLLMU-Bench, our method achieves the lowest accuracy on the Forget VQA Set for both LLaVA-7B and Qwen2-VL, demonstrating the efficiency of MMUNLEARNER. Compared to the Vanilla model, MMUNLEARNER shows a significant accuracy drop of 14.6% and 11.2%, respectively, outperforming all baseline methods. For CLEAR, our method also improves accuracy on the Forget VQA Set by 4.2% and 5.3% compared to the best baseline results. This highlights the effectiveness of MMUNLEARNER in erasing targeted visual concepts.\nMMUNLEARNER preserves untargeted visual concepts from Retain VQA and overall textual knowledge effectively. Despite its superior unlearning capability, MMUNLEARNER also demonstrates outstanding performance in preserving untargeted knowledge. Specifically, it achieves state-of-the-art results on the Retain Set and Forget QA Set in most cases, particularly for LLaVA-7B on MLLMU-Bench QA and CLEAR QA. In other tasks, such as Retain VQA and real-world VQA, MMUNLEARNER remains highly competitive, with performance gaps of no more than 2% from the best baseline results, except for a 5.8% drop behind GA on the Retain QA of CLEAR. However, considering the poor Forget VQA performance of GA on CLEAR compared to other baselines, we consider this deviation reasonable.\nExisting baselines struggle with unlearning visual concepts, although relatively better on textual knowledge removal. We find that most baseline methods effectively remove textual knowledge but struggle to erase learned visual concepts. For example, NPO achieves the best trade-off between Forget VQA and Retain VQA, performing the best on the Forget VQA Set while maintaining strong performance on the Retain VQA Set. However, even NPO shows a bias toward textual QA data, as its accuracy drop on the Forget QA Set is significantly larger than that on the Forget VQA Set for MLLMU-Bench. The success of baselines in textual knowledge removal aligns with previous findings (Liu et al., 2024d), yet their inefficacy in handling visual concepts underscores the need for dedicated MU algorithms tailored for MLLMs, rather than merely adapting LLM-oriented MU methods to VQA data."}, {"title": "4.3 Unlearning v.s. Model Utility", "content": "Previous works on LLM unlearning (Liu et al., 2024e; Zhang et al., 2024) and MLLM unlearning (Liu et al., 2024d) have discussed the trade-off between unlearning effectiveness and model utility as the forget ratio varies. However, textual utility in MLLM unlearning remains largely unexplored. In this section, we analyze the performance of different methods across three forget ratios (i.e., 5%, 10%, and 15%), as shown in Figure 4.\nMMUNLEARNER remains efficient across different forget ratios. MMUNLEARNER demonstrates remarkable forgetting performance across various forget ratios. In most cases, the difference in Forget VQA accuracy between MMUNLEARNER and the vanilla model surpasses other baselines by a significant margin, ranging from 5% to 15%. Among the four baselines, GA_Diff exhibits the strongest capability in erasing visual concepts, while NPO achieves competitive results at higher forget ratios. Notably, as the forget ratio increases, all baselines show improvements in forget quality, albeit at the cost of degraded model utility on Retain and Real-world tasks. Furthermore, the trend of MMUNLEARNER in relation to the forget ratio presents similar pattern with that of GA_Diff, but with superior forget quality and lower utility decay, as reflected in Retain VQA, Real-world VQA, Forget QA, and Retain QA.\nHigher forget ratio makes it harder to maintain Model Utility. There is a clear downward trend in model utility for VQA tasks as the forget ratio increases. When the forget ratio rises from 5% to 15%, GA_Diff experiences the most significant drop, with over a 3% decrease in Retain VQA performance compared to other baselines. However, by selectively updating the vanilla model using a weight saliency map, MMUNLEARNER effectively mitigates this issue, achieving a bet-\nter trade-off between forgetting and retention. A similar phenomenon can be observed for KL_min, NPO, and GA. Additionally, performance on Real-world VQA exhibits the smallest variation across all methods, indicating the robustness of the visual features learned by MLLMs.\nMMUNLEARNER show powerful ability on textual knowledge preservation even under high forget ratio. The scatter plots of the four baselines cluster in the lower-left region on Forget QA and Retain QA, aligning with the findings in Observation 4.2. In contrast, MMUNLEARNER exhibits a remarkable ability to preserve textual knowledge from both the Forget and Retain sets, significantly outperforming the baselines. Surprisingly, MMUNLEARNER's performance on QA tasks improves as the forget ratio increases. We attribute this phenomenon to the enhanced influence of the preserved dataset P in Eq. 6, which becomes more effective as more forget samples are considered."}, {"title": "4.4 Ablation Study", "content": "Considering real-world scenarios where visual concepts can be learned by the vision encoder through pre-training and supervised fine-tuning (Goh et al., 2021), we keep the vision encoder's parameters trainable both when obtaining the vanilla model and during the unlearning process, following previous practices (Lu et al., 2024; Wang et al., 2024). To analyze the impact of unlearning on different modules, we conduct an ablation study on LLaVA-7B using MLLMU-Bench, with the results summarized in Table 2. While there are minor differ-"}, {"title": "4.5 Case Study", "content": "In this section, we illustrate the performance of MMUNLEARNER on a given visual concepts, and compared it with GA and NPO. As shown in Table 3, MMUNLEARNER exceeds other baselines in targeted visual concept removal, textual knowledge preservation and untargeted concept retention. More detailed cases can be found in Appendix D."}, {"title": "4.6 Visualization", "content": "We visualize the parameter distribution selected by MMUNLEARNER through a heatmap, comparing it against other unlearning methods by selecting the top-n parameters with the largest deviation post-unlearning. As shown in Figure 5, which presents results on LLaVA-7B using MLLMU-Bench, GA and NPO exhibit similar update patterns, primarily affecting middle MLP, middle Attention, and shallow Attention layers. In contrast, MMUNLEARNER produces a more focused and structured distribution, peaking in the middle MLP and Attention layers. According to prior MLLM interpretability studies (Basu et al., 2024; Yu and Ananiadou, 2024), shallow Attention layers are crucial for visual information transfer, while the middle MLP layers handle information storage and aggregation. Our findings align well with previous research, providing possible insights into the distinctions among different unlearning methods for MLLMs. However, a more in-depth exploration of unlearning mechanisms is left for future work. Additional visualizations across different models and datasets are provided in the Appendix C.1."}, {"title": "5 Conclusion", "content": "In this paper, we reformulate the task of MU tailored for MLLMs, a field still in its early stages. Our proposed setting aims to erase targeted visual concepts in MLLMs while preserving untargeted knowledge. To address this challenge, we further propose a novel weight saliency-based unlearning method, MMUNLEARNER, which selectively updates parameters crucial for the forgetting objective while protecting parameters essential for retaining untargeted knowledge. Our experiments demonstrate that directly transferring LLM-oriented MU"}, {"title": "6 Limitations", "content": "Despite the contributions demonstrated in our work, several limitations remain:\n1. While we provide a detailed analysis of various unlearning methods, our experiments primarily focus on MLLMU-Bench (Liu et al., 2024d) and CLEAR (Dontsov et al., 2024), two pioneering benchmarks for MLLM MU. As this field is still in its early stages, designing more high-quality benchmarks would be beneficial for evaluating MLLM-targeted unlearning methods more comprehensively. For instance, representative LLM unlearning benchmarks such as TOFU (Maini et al., 2024) and WPU (Liu et al., 2024c) could be extended with visual information, facilitating a more thorough assessment of MLLM MU. However, we leave the enhancement and development of MLLM-oriented unlearning benchmarks for future work.\n2. Although MMUNLEARNER surpasses baseline methods in forgetting tasks, there remains a degradation in model utility after unlearning. This decline may stem from complex interactions between multimodal knowledge representations within the MLLM. Future work could further optimize MMUNLEARNER by refining dataset selection, tuning hyperparameters, and developing novel saliency score measurements to mitigate this issue.\n3. In this paper, our weight saliency-based updating strategy has proven to be both effective and robust for MLLM MU compared to baseline approaches. However, the underlying mechanisms of these methods in multimodal domains remain unexplored. Further investigation and exploration about these methods may offer valuable insights, leading to more powerful MLLM unlearning methods and revealing the knowledge storage mechanism of MLLMs."}, {"title": "A More Related Work", "content": "A.1 Multimodal Large Language Model\nThe rapid development of MLLM has attracted the attention of both the academic and industrial communities to the performance breakthroughs brought about by its architectural characteristics (Huang and Zhang, 2024; Mai et al., 2024; Yan and Lee, 2024; Yan et al., 2024c). Most MLLMs adopt a framework similar to LLaVA (Liu et al., 2024a), which proposes to project the visual embeddings extracted from a pre-trained vision encoder into the LLM's word embedding space through a connector (also known as projector or merger). The combined model is then fine-tuned with visual instruction data, as described in Eq. 1. Several open-source MLLMs have demonstrated remarkable performance on multimodal reasoning and understanding tasks, including Qwen2-VL (Wang et al., 2024), InternVL2 (Chen et al., 2024), and others (GLM et al., 2024; Li et al., 2024a). For MLLM unlearning, LLaVA-1.5 has been one of the most widely used backbones in previous studies (Dontsov et al., 2024; Li et al., 2024c; Liu et al., 2024d). To further validate our conclusions, we additionally select Qwen2-VL, one of the state-of-the-art open-source MLLMs, as another representative evaluated model.\nA.2 Machine Unlearning for Other Multimodal Models\nA brief discussion of MLLM MU is provided in Section 2.2. Despite these efforts, several pioneering studies have also explored unlearning for multimodal models with different architectures (Gao et al., 2024a,b; Li et al., 2025a; Liu et al., 2024b; Tang et al., 2024), such as CLIP (Radford et al., 2021). For example, CLIPErase (Yang et al., 2024) seeks to disentangle and selectively forget both visual and textual associations learned by CLIP, ensuring that unlearning does not compromise model performance. The motivation behind CLIPErase is therefore similar to ours. Moreover, (Kravets and Namboodiri, 2024) demonstrates class-wise unlearning in CLIP using synthetic samples. MultiDelete (Cheng and Amiri, 2024) introduces a method that separates cross-modal embeddings for the forget set of BLIP (Li et al., 2022) and AL-BEF (Li et al., 2021). While these exploratory works provide insights into multimodal MU, they do not address issues in MLLM MU."}, {"title": "B Implementation Details", "content": "B.1 Datasets\nB.1.1 MLLMU-Bench\nMLLMU-Bench (Liu et al., 2024d) is a benchmark designed to advance the understanding of multimodal machine unlearning. It consists of 500 fictitious profiles and 153 public celebrity profiles, with each profile featuring over 14 customized question-answer pairs, evaluated from both multimodal and textual perspectives. In this paper, we divide it into six subsets to comprehensively assess the efficiency, generalizability, and model utility of unlearning methods, particularly in terms of their handling of visual and textual knowledge. Compared to CLEAR, the results of MLLMU-Bench are more stable, demonstrating consistent and reliable performance across different dimensions and settings. Therefore, our further analysis of unlearning methods is primarily based on MLLMU-Bench.\nB.1.2 CLEAR\nSimilar with MLLMU-Bnech, CLEAR (Dontsov et al., 2024) is also an opensourced benchmark designed for machine unlearning in multimodal setup, which contains 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs. CLEAR is built on the top of pure-textual unlearning benchmark TOFU (Maini et al., 2024), with additional portraits for each person mentioned in QA pair. However, despite efforts to ensure consistency across different images of the same entity, the photos generated by Photomaker (Li et al., 2024e) in CLEAR still exhibit a noticeable gap from expectation. Consequently, the vision features learned by MLLMs on CLEAR can be unstable, making the unlearning process highly unpredictable. In our experiments, even minor changes in hyperparameters led to complete model collapse, resulting in 0% accuracy on both classification and generation tasks. Similar findings are also reported in the original paper of CLEAR, where the results of GA, GA_Diff, and KL_Min are all zero for both Forget and Retain Set. Given these limitations, we consider the results from CLEAR as valuable references but not as decisive evidence for our conclusions.\nB.2 Evaluation Metrics\nB.2.1 Unlearning Efficacy\nUnlearning efficacy evaluates a model's capability to eliminate specific knowledge about targeted"}, {"title": "B.2.2 Model Utility", "content": "data, ensuring it behaves as if the data were never included in the training process. In this work, we examine the task of removing visual patterns associated with particular concepts while maintaining textual knowledge. Under this framework, unlearning efficacy is assessed through the model's performance in a Visual Question Answering (VQA) setting. Specifically, the model is tested using multiple-choice questions, where it should avoid selecting the correct answer linked to a forgotten concept. Formally, given a question x and a set of possible answers Y, the model should minimize the probability of choosing the correct answer y* \u2208 Y from the Forget Set:\n\u0177 = arg $\\min_{y \\in Y}$ P(y |\u0445, Mu),\nwhere y \u2260 y* and Mu denotes the unlearned model. Ideally, the model should treat images of forgotten concepts as unknown, behaving similarly to random guessing.\nB.2.2 Model Utility\nModel utility measures the model's ability to retain valuable knowledge and sustain high performance on non-targeted data, ensuring that the unlearning process does not compromise its overall effectiveness. In our study, the preserved knowledge includes textual information related to targeted concepts, both visual and textual knowledge from the Retain Set, and general real-world understanding. We evaluate model utility using the Forget QA, Retain VQA, Retain QA, Real-world VQA, and Real-world QA datasets. For classification tasks, accuracy is determined based on multiple-choice questions associated with retained profiles. The model should sustain high accuracy without any decline due to the unlearning process. Formally, given a question & and a set of possible answers Y, the model should maximize the probability of selecting the correct answer y*:\n\u0177 = arg $\\max_{y \\in Y}$ P(y | x, Mu),\nwhere Mu represents the model after unlearning.\nB.2.3 ROUGE-L Score\nThe ROUGE-L score measures the similarity between the generated text and the reference text by evaluating the longest common subsequence (LCS). The LCS represents the longest sequence of words that appear in both the generated text P and the ground truth G in the same order, though not necessarily contiguously. Recall is calculated as the ratio of the LCS length to the length of the reference text, denoted as LG:\nRecall = $\\frac{LCS}{LG}$\nPrecision is determined by the proportion of the LCS length relative to the length of the generated text, represented as Lp:\nPrecision = $\\frac{LCS}{Lp}$\nThe final ROUGE-L score is obtained by computing the F\u2081 score of recall and precision:\nROUGE-L = 2 $\\cdot \\frac{Recall \\cdot Precision}{Recall + Precision}$\nThis approach ensures a balanced assessment of both precision and recall, providing a comprehensive evaluation metric."}, {"title": "B.3 Vanilla Fine-tuning and Baselines", "content": "B.3.1 Vanilla Model\nTo simulate a real-life scenario where unlearning algorithms are applied to a pre-trained model", "tokens": "nJ(x,y,w) = $\\frac{1}{\\|y\\|}\\sum_{i=1}^{\\|y\\|} NLL_w (y_i | [I, x, y<i"}]}