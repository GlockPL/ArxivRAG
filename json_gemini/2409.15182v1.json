{"title": "GOAL-BASED NEURAL PHYSICS VEHICLE TRAJECTORY PREDICTION MODEL", "authors": ["Rui Gan", "Haotian Shi", "Pei Li", "Keshu Wu", "Bocheng An", "Linheng Li", "Junyi Ma", "Chengyuan Ma", "Bin Ran"], "abstract": "Vehicle trajectory prediction plays a vital role in intelligent transportation systems and autonomous\ndriving, as it significantly affects vehicle behavior planning and control, thereby influencing traffic\nsafety and efficiency. Numerous studies have been conducted to predict short-term vehicle trajec-\ntories in the immediate future. However, long-term trajectory prediction remains a major challenge\ndue to accumulated errors and uncertainties. Additionally, balancing accuracy with interpretability\nin the prediction is another challenging issue in predicting vehicle trajectory. To address these\nchallenges, this paper proposes a Goal-based Neural Physics Vehicle Trajectory Prediction Model\n(GNP). The GNP model simplifies vehicle trajectory prediction into a two-stage process: deter-\nmining the vehicle's goal and then choosing the appropriate trajectory to reach this goal. The\nGNP model contains two sub-modules to achieve this process. The first sub-module employs a\nmulti-head attention mechanism to accurately predict goals. The second sub-module integrates a\ndeep learning model with a physics-based social force model to progressively predict the complete\ntrajectory using the generated goals. The GNP demonstrates state-of-the-art long-term predic-\ntion accuracy compared to four baseline models. We provide interpretable visualization results to\nhighlight the multi-modality and inherent nature of our neural physics framework. Additionally,\nablation studies are performed to validate the effectiveness of our key designs.", "sections": [{"title": "1. INTRODUCTION", "content": "Vehicle trajectory prediction is a crucial component in intelligent transportation systems (ITS)\nand autonomous driving, with significant implications for vehicle behavior planning and control.\nOn one hand, the collection of high-quality vehicle trajectory data is becoming more feasible as\nvehicles can promptly share accurate position and speed information, along with data about sur-\nrounding vehicles and road environments gathered through various sensors, using high-quality\ncommunication networks like 5G. On the other hand, for autonomous vehicles (AVs) and con-\nnected autonomous vehicles (CAVs), precise trajectory prediction is a necessary prerequisite for\nreliable decision-making, trajectory planning, and control instructions (1). Effectively leveraging\nvehicle trajectory data, as well as information about surrounding vehicles and road environments,\nto achieve high-performance prediction is an imperative task in ITS and autonomous driving.\nEarlier models for vehicle trajectory prediction relied on physical models or traditional\nmachine learning algorithms. Physics-based models employ mechanics or kinematics equations,\niterating the prediction step to forecast the trajectory (2). Traditional machine learning methods in-\ncluded Bayesian learning, hidden Markov models (HMMs), support vector machines (SVMs), and\nGaussian Processes (GP) (3). However, these methods often struggled with long-term predictions\nas they could not anticipate changes due to maneuvers or external factors and required extensive\nfeature engineering. More recently, deep learning models have revolutionized vehicle trajectory\nprediction and perform great accuracy by demonstrating the ability to extract intricate tempo-\nral and spatial features from data. Recurrent neural networks (RNNs), including long short-term\nmemory network (LSTM) and gated recurrent unit (GRU) variants (4\u20136), and transformer-based\nmodels (7, 8) excel in capturing temporal features from sequential trajectory data. For spatial\nfeatures and interactions, convolutional neural networks (CNNs) (9\u201311), and graph neural net-\nworks (GNNs) (12, 13) are employed to analyze Euclidean or non-Euclidean spatial dependencies\nbetween vehicles and the infrastructures. Additionally, to model the high uncertainty of vehicle\ntrajectories, some models adopt a multi-modality approach to predict multiple possible motion be-\nhaviors. These models include those using predefined explicit driving maneuvers or pre-clustered\nanchors, as well as generative deep learning models such as variational autoencoders (VAEs) (14)\nand generative adversarial networks (GANs) (11, 15). However, despite their high accuracy, deep\nlearning models have limitations. They require large amounts of labeled data, are computationally\nintensive, and often act as \"black boxes\" with limited interpretability, which may hinder safety-\ncritical applications in autonomous driving.\nAlthough previous trajectory prediction studies have achieved notable success, we have\nidentified and analyzed two key issues from these findings. First, vehicle trajectory prediction has\nuncertainties, largely due to the unknown intention of the vehicle. The uncertainty in a vehicle's\ntrajectory primarily arises from two main factors: the unknown driving intention of the driver\nover a specific period and the indeterminate path the vehicle will take to fulfill this intention. In\na highway environment, the former has a significant influence on trajectory prediction outcomes.\nFor instance, whether a vehicle chooses to proceed straight or change lanes leads to completely\ndifferent trajectory patterns. Additionally, the behavior of surrounding vehicles\u2014such as slowing\ndown, speeding up, and lane changes-directly influences the current vehicle's decisions. The lat-\nter is also relevant, as once the vehicle determines its intention (goal), there may be several path\noptions to execute. Previous studies primarily inferred future motion states from historical trajec-\ntories and environmental information, i.e., focusing solely on the latter factor but neglecting the\nmodeling of driving intentions. Consequently, designing a model that can accurately understand"}, {"title": "1.5 Main contributions and chapter organization of this paper", "content": "The primary contributions of this paper are as follows:\n1. We explore the general pattern of vehicle trajectories to investigate possible driving\nintentions, explicitly predicting multiple potential goals over a period of time."}, {"title": "2. METHODOLOGY", "content": "In this study, we focus on predicting the trajectory of the target vehicle in a multi-lane highway\nscenario based on the historical movement states of the target vehicle and its neighborhood vehi-\ncles. Our methodology emphasizes goal-based prediction, considering the interaction between the\ntarget vehicle and its surrounding vehicles and road marking information."}, {"title": "2.1 Problem Definition", "content": "We suppose that a sequence of vehicle state from time t = 1 to t = T as $Q_{1:T} = (q^1, q^2, ..., q^T)$,\nwhere an observation or state of the vehicle at time t is denoted as $q_t = [p_t^T, \\dot{p_t}^T]^T$, $p_t, \\dot{p_t} \\in R^2$ denotes\nthe position and velocity. For a single state $q_t^i$ of vehicle i, we also consider its neighborhood\nset $\\Omega_i \\subset R^n$ comprising its surrounding vehicle states ${q_t^j : j \\in \\Omega_i }$. Therefore, the observed\nsequence of vehicle state is $X = Q_{1:T_{obs}}$ and the future or ground truth sequence is $Y = Q_{T_{obs}+1:T}$.\nThe predicted trajectory is denoted as $\\hat{Y} = (\\hat{p}_{T_{obs}+1}, \\hat{p}_{T_{obs}+2}, ..., \\hat{p}_T)$. We aim to predict the position\ntrajectory of the target vehicle.\nThe vehicle driving process for a specific period is summarized as follows. Firstly, within\nthe time scope of a trajectory T, the drivers decide on a set of possible destinations on the high-\nway, which we call goals $g \\in {1,2,...,n_{goals}}$, where $n_{goals}$ denotes the number of goals; second,\noriented by the potential goals and considering the influence of surrounding vehicles and road\nmarkings, they choose one goal and manipulate the vehicle towards it via a route. In a 2D scene,\nthe goal g of the target vehicle is defined as the last longitudinal and lateral position of the fu-\nture time step, denoted as $H_{T+F}$. Therefore, the goal-based prediction model can be presented as\nfollows:\n$\\hat{g} = G_{\\mu}(X, \\Omega),$\n(1)\n$\\hat{Y} = F_{\\theta, \\phi} (X, \\Omega, \\hat{g}, E),$\n(2)\nwhere G and $\\mu$ are the Goal Prediction sub-module in Section 2.3 and its learnable parameters\n(neural network weights), $\\hat{g}$ is the estimated goals of the Goal Prediction sub-module. Similarly, in\nEquation 2, F is the Goal-based Neural Social Force Trajectory Prediction sub-module introduced\nin Section 2.4, $\\theta$ and $\\phi$ are interpretable parameters presented later and uninterpretable parameters\nin neural networks. Here, E represents the environmental information."}, {"title": "2.2 Model Architecture", "content": "Based on the defined problem, we introduce the architecture of our GNP model in this section. As\ndepicted in Figure 1, the GNP model comprises two sub-modules: the Goal Prediction sub-module\nand the Trajectory Prediction sub-module. The Goal Prediction sub-module employs a refined"}, {"title": "2.3 Goal Prediction", "content": "To model intentions and predict goals for vehicles, we developed an enhanced transformer encoder-\ndecoder architecture based on the design in (25). First, the mode-level transformer encoder parses\nthe intention modes and observed embeddings, capturing possible future trends in vehicle trajec-\ntories. The encoder's output, combined with information about neighboring vehicles, is fed into\nthe social interaction-level transformer decoder to explore inter-vehicle relationships. Finally, the\nmodel produces multiple possible goals, determining the vehicle's driving intention over a speci-\nfied period.\nIn contrast to other transformer-based trajectory prediction models, our model aims to pre-\ndict goals by estimating driving intentions. Previous methods typically depend only on observed\ntrajectories and neighboring vehicles, encoding temporal and spatial features from these sequences.\nOur model, however, incorporates general patterns of future vehicle behavior into the input token,\nenabling it to identify potential future driving directions and targets and thereby predict the goals.\nTrajectory modes and observed embeddings: The input token of our model combines the de-\nsigned intention mode and the observed trajectory sequence. By interpreting this token with a\nmode-level transformer encoder, the model captures the overall pattern and trend of the trajectory\nsequence. The intention mode differentiates general driving behaviors on the highway, indicating\nvarious driving purposes. For example, lane-changing trajectories differ significantly from straight-\nahead trajectories, allowing us to infer whether a vehicle intends to change lanes or go straight.\nHowever, determining intention solely based on subjective evaluations like going straight or chang-\ning lanes is insufficient. Therefore, we first perform two rigid transformations\u2014translation and ro-\ntation on the input trajectory sequence and then obtain the intention mode using a distance-based\nmethod.\nFrom a bird's eye view of the highway, the trajectory segments are distributed across var-\nious parts of the roadway and travel in two different directions. To extract the general features\nof the vehicle trajectories, it is essential to normalize the trajectories using two rigid transforma-\ntions: translation and rotation. It is important to note that trajectories exhibit rigid transformation\ninvariance. For instance, the trajectory of a vehicle moving from east to west and changing lanes\nto the right can be rotated and translated to represent the same vehicle moving from west to east\nand changing lanes to the right under identical conditions. The trajectory data in the training set\nis divided into two segments: the first segment with a length of $T_{obs}$ and the second segment with\na length of $T_{pred}$. First, we translate the entire trajectory by shifting the $T_{obs} + 1$ point of each\ntrajectory to the origin of the coordinate system, i.e., subtracting the 2D coordinates of the $T_{obs} +1$\npoint from all trajectory points. Then, considering the presence of trajectories with two travel\ndirections on the highway in some datasets, we rotate all trajectories around the coordinate ori-\ngin and uniformly convert them to trajectories traveling from west to east. For the transformed\ntrajectories, the distance between the trajectory points with similar intention behavior is smaller.\nHence, a distance-based approach can be used here to distinguish the different intention modes of\nthe trajectories.\nHere, we perform a clustering operation on the normalized future trajectories to identify L\ncenters $C \\in R^{L \\times T_{pred} \\times 2}$, with $C = {C_1,...,C_L}$ and each ${c_l | l \\in {1,...,L}}$ representing a trajec-\ntory of length $T_{pred}$. These centers C represent the general intention modes and act as one of the\ninputs for the next mode-level transformer encoder. Staying consistent with the original text, the\nclustering operation here is a preparatory step, C does not affect the efficiency of validation or test"}, {"title": "stage.", "content": "The intention mode requires two more processing steps to serve as the input token for the\nnext mode-level transformer encoder. They are first reshaped into a L \u00d7 2$T_{pred}$ features and then\nembedded by a learnable linear transformation, resulting in the input embedding $E_c$ as follows:\n$E_c = \\phi(C, W_c),$\n(3)\nwhere $\\phi(,)$ denotes a linear transformation characterized by a learnable parameter matrix $W_c \\in$\n$R^{2T_{pred} \\times D_e}$, and $E_c \\in R^{L \\times D_e}$ denotes the intention mode embedding.\nTo ensure accurate goal prediction, it is necessary to analyze the observed historical tra-\njectories in addition to the intention mode. Therefore, the intention mode and observed trajectory\n$X \\in R^{B \\times T_{obs} \\times 2}$ are combined here. Specifically, the embedded X is concatenated to the intention\nmode embedding $E_c$ as follows:\n$E_o = \\phi(X, W_o),$\n(4)\n$E_e = E_c + E_o,$\n(5)\nwhere B denotes the batch size, X denotes reshaped into B \u00d7 2$T_{obs}$ followed by a linear trans-\nformation, $W_o \\in R^{2T_{obs} \\times D_e}$ denotes the learnable parameter matrix. The dimensions of $E_c$ and\n$E_o$ are broadcast to B \u00d7 L \u00d7 $D_e$, followed by an addition operation to obtain the input embedding\n$E_e \\in R^{B \\times L \\times D_e}$. Classical Transformer models typically require positional encoding to incorporate\nsequential order information into the sequence. However, unlike tasks such as natural language\nprocessing, the input data in vehicle trajectory prediction inherently possesses temporal sequential\ninformation. Therefore, following the approach in (25), we do not add positional embedding to the\ninput embedding.\nMode-level transformer encoder: The mode-level transformer encoder is designed to analyze\nintention modes and observed trajectories to predict the vehicle's potential goals or destinations\nover time. Given the input embedding $E_e$, which represents general intention modes and the ob-\nserved trajectory, the mode-level transformer encoder employs the standard encoder architecture\nof a naive transformer. Each encoder block features a multi-head self-attention layer and a Feed-\nForward Network (FFN) with residual connections (26). Unlike the naive transformer encoder, we\ndo not add positional embedding at each encoder block as proposed in (25), but only provide the\ncombined input embedding once.\nSocial interaction-level transformer decoder: The social-level transformer decoder is designed\nto extract social interactions with neighboring entities and follows the standard decoder architec-\nture of a naive transformer, which includes an attention layer and a Feed-Forward Network (FFN).\nReferring to the work of (25), the differences from the naive transformer are threefold: 1. The\ndecoder receives masked neighboring embeddings instead of masked output embeddings. The\nmask's main role is to exclude non-existent neighbors. 2. The decoder retains the encoder-decoder\nattention mechanism but omits self-attention. 3. Positional embeddings are removed since the\npositional relationship between the vehicle and its neighbors is incorporated in the trajectory se-\nquences.\nAssume that a pedestrian has N neighbors, represented by the neighbor observed trajecto-"}, {"title": "Gan, Shi, P.Li, An, L.Li, J.Ma, C.Ma, Ran", "content": "ries $X_s \\in R^{N \\times T_{obs} \\times 2}$. Each trajectory in $X_s$ is flattened into a feature vector, leading to a feature\nmatrix $X_s \\in R^{N \\times 2T_{obs}}$. Then, we embed the feature matrix by a learnable linear transformation to\nobtain the input embeddings of the social-level transformer decoder as follows:\n$E_s = \\phi_s(X_s, W_s),$\n(6)\nwhere $E_s \\in R^{N \\times D_e}$ is the input embeddings of the decoder, $W_s \\in R^{2T_{obs} \\times D_e}$ is the learnable pa-\nrameter matrix. After that, the input embeddings $E_s$ are transformed into output embeddings with\nthe subsequent encoder-decoder attention layer and an FFN layer with the residual connection. In\nthis case, these output embeddings attend to the social interactions to forecast social-acceptable\ntrajectories and corresponding probabilities by the next dual prediction.\nOutput: To accurately describe the possible driving intentions of the vehicle, we output multiple\npotential goals and their corresponding probabilities. Based on the literature (25), we use a goal\nprediction head and a probability prediction head to output the predicted goals and their proba-\nbilities. Here, the probability prediction head is directly connected to the mode-level transformer\nencoder, while the goal prediction head is applied after the social interaction-level transformer de-\ncoder. We adopt different strategies during the training and deployment phases. In training, we\nuse a greedy strategy, assuming that the predicted goal with the highest probability originates from\nthe cluster center nearest to the ground truth goal $p^T$. Specifically, we first calculate the distance\nbetween the cluster centers $C = {c_1,...,c_L}$ and the ground truth goal $p^T$ to identify the nearest\nclustering center $c_i, i \\in {1, ...,L}$.\n$i = arg \\underset{c_i | i \\in {1,...,L}}{min} (||\\hat{Y} - c_i||_2) .$\n(7)\nThen, the soft probability p of $c_i$ can be expressed using the normalized negative distance\nas shown below.\n$p = softmax({-||\\hat{Y} - c_i||_2 | i \\in {1, ...,L}}).$\n(8)\nAs such, the predicted goal $\\hat{p}^T$ and its corresponding probability $\\hat{p}$ are derived by applying a series\nof deep transformations to $c_i$. In the testing phase, we adopt a Top-K strategy, i.e., we select the\nK highest probability goals from the model's multiple outputs to represent the final set of possible\ndriving goals."}, {"title": "2.4 Goal-based Neural Social Force Trajectory Prediction", "content": "Following to the two-stage hypothesis of vehicle trajectory execution, this sub-module is dedicated\nto accurately developing the trajectory towards the predetermined goal. According to (22), we\ndeveloped a goal-based neural social force model. As we mentioned in Section 2.1, the state $q_i$ of\nthe ith vehicle at any time t can be observed on the highway. Then a sequence of vehicle state can\nbe represented as a function of time q(t). Similarly, the sequences of neighborhood vehicle states\nis also a function of time \u03a9(t). Therefore, the vehicle dynamics in a highway in this neural social\nforce model can be formulated as follows:\n$\\frac{dq_i}{dt}(t) = f_{\\theta, \\phi}(t, q_i(t), \\Omega(t), q^T, E),$\n(9)"}, {"title": "where 0 and $ are interpretable parameters presented later and uninterpretable parameters in neural\nnetworks. The vehicle dynamics determined by function f, which is governed by time t, the current\nstate q(t), its neighboring vehicle state \u03a9(t) and the environment E.", "content": "Given the initial and final condition q(0) = $q^0$ and q(T) = $q^T$, then we have the following\nrepresentation:\n$q^T = q^0 + \\int_{t=0}^T f_{\\theta, \\phi}(t, q(t), \\Omega(t), q^T, E)dt$\n(10)\nAssuming $\\dot{p}(t)$ is second-order differentiable, we expands q(t) using Taylor's series for a\nfirst-order approximation:\n$q(t + \\Delta t) \\approx q(t) + \\dot{q}(t) \\Delta t$\n$= \\begin{bmatrix} p(t) \\\\ \\dot{p}(t) \\end{bmatrix} + \\Delta t \\begin{bmatrix} \\dot{p}(t) \\\\ \\ddot{p}(t) \\end{bmatrix},$\n(11)\nwhere $\\Delta t$ is the time step. The stochasticity $\\alpha(t, q_{t-M}:t)$ is assumed to only influence $\\ddot{p}$. Equation\n11 is general and any dynamical system with second-order differentiability can be employed here.\nFurther, we hypothesize that each vehicle behaves as a particle in a particle system and\nfollows Newton's second law of motion. The acceleration $\\ddot{p}(t)$ that we establish depends primarily\non two component forces: the Goal attraction force $F_{goal}$ and the inter-vehicle and environmental\nrepulsion $F_{rep}$.\n$\\ddot{p}(t) = F_{goal}(t, q_t, q^T) + F_{rep}(t, q_t, \\Omega_i)$\n(12)\nUnlike the traditional social force model, some parameters in our model are obtained\nthrough neural network training. We assume that the goal or destination $p^T$ of each trajectory is\ngiven, although in practice, the goal $p^T$ needs to be learned or sampled during prediction. There-\nfore, we employ the Goal Prediction sub-module described in Section 3.3 to sample $p^T$ for each\ntrajectory. For trajectory prediction, the Goal Prediction sub-module is a pre-trained model. When\nnew trajectory data is collected during testing, the Goal Prediction sub-module samples the goal\n$p^T$ for each trajectory, and the Trajectory Prediction sub-module predicts the future trajectories.\nGiven the current state and goal, we calculate $F_{goal}$ using the goal network $NN_{\\phi_1}$ as de-\nscribed in Equation 13 and $F_{rep}$ using the repulsion network $NN_{\\phi_2}$ as described in Equation 14.\nThe goal network first encodes the current state $q_i$, which is then input into a Long Short-Term\nMemory (LSTM) network to capture the dynamics. After a linear transformation, the LSTM out-\nput is concatenated with the embedded goal $p^T$. Finally, the key parameter $\u03c4$ is computed using a\nMulti-Layer Perceptron (MLP). The architecture of the collision network is similar. Each agent $q_j^i$\nin the neighborhood $\u03a9_i$ is encoded and concatenated with the encoded target vehicle state $q_i^h$. The\nparameter $k_{nj}^i$ is then computed. Through these steps, the interpretable key parameters $\u03c4$ and $k_{nj}^i$\nfor $F_{goal}$ and $F_{rep}$ are derived.\nGoal attraction: A vehicle navigates towards its destination or goal driven by its intrinsic driving\nintentions, which we abstractly model as an attraction generated by the goal. At time t, a vehicle\nhas a desired driving direction $e'$, determined by the goal $p^T$ and the current position $p^i$: $e' =$\n$\\frac{p^T - p^i}{||p^T - p^i||}$. Without any interfering forces, the vehicle adjusts its current velocity to align with the"}, {"title": "Gan, Shi, P.Li, An, L.Li, J.Ma, C.Ma, Ran", "content": "desired velocity $v_{des} = \\dot{p}^i e'$, where $\\dot{p}^i$ and $e'$ denote the magnitude and direction of the velocity,\nrespectively. Unlike the static $\\dot{p}^i$ used in previous models, we dynamically update $\\dot{p}^i$ at each time\nstep to reflect the changing desired speed as the vehicle approaches its destination: $\\dot{p}^i = \\frac{||p^T - p^i||}{(T - t)\\Delta t}$.\nThe goal attraction force $F_{goal}$ indicates the vehicle's natural tendency to adjust its current velocity\n$\\dot{p}^i$ to the desired velocity $v_{des}$ within time $\u03c4$.\n$F_{goal} = \\frac{1}{\u03c4}(v_{des} - \\dot{p}^i)$ where $\u03c4 = NN_{\\theta_1}(\\dot{q_i}, p^T)$\n(13)\nwhere $\u03c4$ is learned through a neural network (NN) parameterized by $\\theta_1$.\nInter vehicle and environment repulsion: Besides the goal attraction force, vehicles on a high-\nway are subject to certain repulsive forces. The repulsive force stems from two major components:\n(1) maintaining a safety distance from surrounding vehicles to avoid collisions, and (2) adher-\ning to lane constraints, where lane changes can be made at the dashed lines but never across the\nroad boundary lines. Given a target vehicle n, a neighboring vehicle $j \\in \\Omega_i$ at relative position\n$r_{nj} = p_n^i - p_j^i$, and lane boundaries $l \\in \\Lambda_i$ at distances $d_{nl}^i$, the repulsive force $F_{rep}^i$ exerted on\nvehicle n is based on the gradient of the total repulsive potential field $U_{total}$: \n$F_{rep}^i = -\\nabla_{r_{nj}} U_{total}$\n(14)\nUnder previous assumptions, the total repulsive potential field $U_{total}$ consists of two com-\nponents: the potential field from surrounding vehicles $U_{vehicles}$ and the potential field from lane\nboundaries $U_{lines}$. The total repulsive potential field is given by:\n$U_{total} = \\sum_{j \\in \\Omega_i} (r_{col} k_{nj} e^{\\frac{-||r_{nj}||}{\\sigma_{col}}}) + \\sum_{l \\in \\Lambda_i} U_{line,l}$\n(15)\nHere, $r_{col}$ is a scaling factor for the repulsive potential, $k_{nj}$ is a coefficient representing the\ninteraction strength between the target vehicle n and the neighboring vehicle j, and $||r_{nj}||$ is the\ndistance between these vehicles. The term $U_{line,l}$ represents the potential field contributions from\nthe lane boundaries and includes the interaction strength parameter $k_{nl}$, given by:\n$U_{line,l} = \\begin{cases} k_{nl} e^{-(\\frac{d_{nl}^i}{\\sigma_{nl}})^2} & \\text{for center lines} \\\\ \\frac{k_{nl}}{(\\frac{d_{nl}^i}{\\sigma_{nl}})^{0.5}} & \\text{for boundary lines} \\end{cases}$\n(16)\nThe selection of $U_{line,l}$ is based on the different characteristics and roles of center lines and\nboundary lines in the highway, as referenced in (27, 28). Center lines are typically crossable by\nvehicles, indicating vehicles can change lanes for overtaking or other maneuvers. Therefore, an\nexponential decay model $k_{nl} e^{-(\\frac{d_{nl}^i}{\\sigma_{nl}})^2}$ is used, which provides a decent potential field when close\nto the center line but rapidly diminishes as the distance increases, reflecting the flexibility and\nfrequent interaction with the center line. Conversely, boundary lines are generally non-crossable,\nserving as strict limits to the vehicle's movement, such as road edges or barriers. Thus, an inverse\nsquare model $\\frac{k_{nl}}{(\\frac{d_{nl}^i}{\\sigma_{nl}})^{0.5}}$ is used for boundary lines. This generates a stronger and more persistent\nrepulsive force as the vehicle approaches the boundary line, ensuring that the vehicle maintains a"}, {"title": "Gan, Shi, P.Li, An, L.Li, J.Ma, C.Ma, Ran", "content": "safe distance and avoids collisions with these non-crossable boundaries.\nMoreover, to ensure that the total repulsive potential field $U_{total}$ varies over time, we treat\n$k_{nj}$ and $k_{nl}$ as learnable dynamic variables. We define $[k_{nj}, k_{nl}] = \\alpha * sigmoid(NN_{\\phi_2} (q_n^i, q_j^i, j \\in$\n$\\Omega_i, p_{i,l}^i, l \\in \\Lambda_i))$, and adjust the hyperparameter $\\alpha$ to guarantee the learned $k_{nj}$ and $k_{nl}$ are realistic."}, {"title": "3.RESULTS AND DISCUSSION", "content": ""}, {"title": "3.1 Experimental Dataset and Setting", "content": "Our research employs two datasets. The first one, known as the Next Generation Simulation\n(NGSIM) dataset, provides comprehensive vehicle trajectory data from eastbound I-80 in the San\nFrancisco Bay area and southbound US 101 in Los Angeles. This data, collected by the U.S. De-\npartment of Transportation in 2015, captures real-world highway conditions via overhead cameras\noperating at 10 Hz. The second dataset is HighD, derived from drone video recordings at 25 Hz be-\ntween 2017 and 2018 around Cologne, Germany. It covers approximately 420 meters of two-way\nroads and documents 110,000 vehicles, including both cars and trucks, traveling a cumulative dis-\ntance of 45,000 km. We trim each trajectory to a length of 8 seconds. Given 3 seconds of trajectory\ndata, we train the model to predict the remaining 5 seconds of trajectory points. Specifically, for\nthe NGSIM dataset, 30 frames are used to predict 50 future frames, while for the HighD dataset,\n75 frames are input to predict 125 future frames."}, {"title": "3.2 Metrics", "content": "We evaluate our proposed and compared methods using three metrics: Root Mean Square Error\n(RMSE), Average Displacement Error (ADE), and Final Displacement Error (FDE). Given a true\nfuture trajectory (ground truth) ${p_t = (x_t, y_t)}_{t=T_{obs}+1}^T$ and the corresponding predicted trajectory\n${\\hat{p}_t = (\\hat{x}_t, \\hat{y}_t)}_{t=T_{obs}+1}^T$, these metrics measure the $l_2$ distance between the ground truth and the\npredicted trajectory. ADE calculates the average $l_2$ distance between the predicted trajectory and\nthe ground truth trajectory across the entire prediction period. FDE calculates the $l_2$ distance\nbetween the predicted final position and the actual final position at the end of the prediction period.\nRMSE assesses overall accuracy by computing the square root of the average squared differences\nbetween predicted and actual positions across all time steps. The metrics are defined as follows:\n$ADE = \\frac{1}{T_{pred}} \\sum_{t=T_{obs}+1}^T \\sqrt{(x_t - \\hat{x}_t)^2 + (y_t - \\hat{y}_t)^2},$\n$FDE = \\sqrt{(x_T - \\hat{x}_T)^2 + (y_T - \\hat{y}_T)^2},$\n(17)\n$RMSE = \\sqrt{\\frac{1}{T-T_{obs}}\\sum_{t=T_{obs}+1}^T ((x_t - \\hat{x}_t)^2 + (y_t - \\hat{y}_t)^2)}$"}, {"title": "3.3 Baseline models", "content": "We compare our model with the following baselines:\n\u2022 Social-LSTM (S-LSTM) (29): This model uses a shared LSTM to encode the raw tra-\njectory data for each vehicle. The extracted features from different vehicles are then\naggregated using a social pooling layer.\n\u2022 Convolutional Social-LSTM (CS-LSTM) (9): Unlike S-LSTM, this model captures"}, {"title": "Gan, Shi, P.Li, An, L.Li, J.Ma, C.Ma, Ran", "content": "social interactions by stacking convolutional and pooling layers, and it considers multi-\nmodality based on the predicted intention.\n\u2022 Planning-informed Prediction (PiP) (30): This model integrates trajectory prediction\nwith the planning of the target vehicle by conditioning on multiple candidate trajectories\nfor the target vehicle.\n\u2022 Spatial-temporal dynamic attention network (STDAN) (31): This paper introduces\na spatiotemporal dynamic attention network designed for intention-aware vehicle tra-\njectory prediction. It uses hierarchical modules to capture various levels of social and\ntemporal features, employing a multi-head attention mechanism to extract data from raw\ntrajectories and a novel feature fusion method for joint intention recognition and trajec-\ntory prediction.\n\u2022 GNP: Our proposed model integrates physics and deep learning for goal-based trajectory\nprediction in this paper."}, {"title": "3.4 Quantitative Results", "content": "Table 1 shows the comparison results with the baseline models. Our proposed models consistently\noutperform S-LSTM, CS-LSTM, PIP, and STDAN models in all 1-5 second predictions. We use\nthe RMSE metric to compare the accuracy of the models. Our proposed model is evaluated with\nthree metrics: ADE, FDE, and RMSE. The results with the best accuracy are highlighted in bold.\nThe goal-based framework achieves excellent results in terms of accuracy, effectively capturing\nthe goals even though the full trajectory prediction is derived from a physical model. Additionally,\nthe model performs better on the HighD data compared to NGSIM data. This may be because\nthe scenarios in HighD data have fewer lanes and the shorter distances required for lane changes,\nwhich make the prediction task less challenging."}, {"title": "3.5 Visualization Results", "content": "Unlike deep learning-based trajectory prediction models, our GNP model inherently provides in-\nterpretable results by analyzing the forces acting on the target vehicle. Figure 2 illustrates several\nexamples of the GNP model applied to a two-lane scenario in the HighD dataset, viewed from a"}, {"title": "Gan, Shi, P.Li, An, L.Li, J.Ma, C.Ma, Ran", "content": "right-to-left vehicle travel perspective for clarity."}]}