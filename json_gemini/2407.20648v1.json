{"title": "Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning", "authors": ["JongWoo Kim", "SeongYeub Chu", "HyeongMin Park", "Bryan Wong", "MunYong Yi"], "abstract": "Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs (HGNNs) have advanced node embeddings and relationship learning for various tasks. However, existing methods often rely on domain-specific predefined meta-paths, which are coarse-grained and focus solely on aspects like node type, limiting their ability to capture complex interactions. We introduce MF2Vec, a model that uses multi-faceted (fine-grained) paths instead of predefined meta-paths. MF2Vec extracts paths via random walks and generates multi-faceted vectors, ignoring predefined schemas. This method learns diverse aspects of nodes and their relationships, constructs a homogeneous network, and creates node embeddings for classification, link prediction, and clustering. Extensive experiments show that MF2Vec outperforms existing methods, offering a more flexible and comprehensive framework for analyzing complex networks. The code is available at https://anonymous.4open.science/r/MF2Vec-6ABC.", "sections": [{"title": "Introduction", "content": "Recent advancements in graph neural networks (GNNs) (Kipf and Welling 2016; Hamilton, Ying, and Leskovec 2017; Velickovic et al. 2017) and heterogeneous GNNs (HGNNs) (Fu and King 2024; Jin et al. 2022; Guan et al. 2023; Fu et al. 2020; Wang et al. 2019) have significantly improved the use of node paths, moving beyond traditional random walk methods to leverage node features and relationships (Dong, Chawla, and Swami 2017; Fu, Lee, and Lei 2017; Shi et al. 2018). However, the prevalent approach still relies on predefined sequences of node types and edge relations, called metapaths. As illustrated in Figure 1-(A), these metapaths guide the learning process by emphasizing specific connectivity patterns (Dong, Chawla, and Swami 2017). Recent innovations have enhanced understanding of node interactions by utilizing the unique characteristics of nodes within these metapaths (Fu et al. 2020; Fu and King 2024; Sun et al. 2022). This metapath-based approach is effective for tasks such as node classification (Wang et al. 2019), link prediction (Jin et al. 2022), and clustering (Guan et al. 2023).\nDespite this progress, much of the previous work still relies on predefined metapaths based on coarse-grained node"}, {"title": "Related Work", "content": "GNNs have become popular for extending deep neural networks to handle graph-structured data, aiming to learn low-dimensional node embeddings (Defferrard, Bresson, and Vandergheynst 2016; Kipf and Welling 2016; Hamilton, Ying, and Leskovec 2017; Velickovic et al. 2017). Heterogeneous graph embedding models, which represent node features from diverse networks, often rely on predefined meta-paths to connect nodes (Dong, Chawla, and Swami 2017; Shi et al. 2018; Fu, Lee, and Lei 2017; Chen et al. 2018; Zhang, Swami, and Chawla 2019; Wang et al. 2019; Zhang et al. 2019). For instance, Metapath2Vec (Dong, Chawla, and Swami 2017) generates node sequences using a single metapath-guided random walk, which may miss valuable information. To address this, approaches like HAN (Wang et al. 2019) and MAGNN (Fu et al. 2020) use multiple meta-paths but still face challenges with node type efficiency and redundancy. GraphHINGE (Jin et al. 2022) and HeCo (Wang et al. 2021) improve efficiency by aggregating node interactions and using contrastive learning. Meanwhile, MHNF (Sun et al. 2022) and MECCH (Fu and King 2024) employ autonomous metapath extraction and context-based aggregation, respectively, to avoid predefined schemas and redundancy. Despite these advancements, many methods are limited by predefined paths, which typically consider only a single aspect, such as node type, resulting in coarse-grained approaches. Our study, MF2Vec, overcomes these limitations by dynamically redefining paths with a multi-faceted, fine-grained approach, enabling a richer understanding of node interactions and network connectivity."}, {"title": "Multiple-Vector Network Embedding", "content": "Recent studies in graph representation learning have focused on learning multiple embeddings for individual nodes, considering the multi-faceted nature of nodes in networks. PolyDW (Liu et al. 2019) uses matrix factorization-based clustering to assign aspect distributions to nodes, sampling multi-facet independently for target and context nodes during random walk. In contrast, Splitter (Epasto and Perozzi 2019) creates multi-aspect node representations that remain closely aligned with the original node embedding. Asp2vec (Park et al. 2020) introduces a differentiable aspect selection module to assign a single aspect based on local context, along with an aspect regularization to explore interactions among multiple aspects. However, existing methods have primarily focused on exploring multiple aspects at the node level. We aim at extending this approach to the path level by"}, {"title": "Preliminary", "content": "This section provides definitions of terminologies used in the current study. The notations and their explanations are summarized in Table 1.\nDefinition 1 (Heterogeneous Graph). A heterogeneous graph G = (V, E) is characterized by a node type mapping function \u03c6: V \u2192 A and an edge type mapping function \u03c8: E \u2192 R, where A and R are sets of node and edge types, respectively, with |A| + |R| > 2.\nDefinition 2 (Metapath). A metapath is a sequence A1 R1 A2 R2... Rk Ak+1 (or A1 A2... Ak+1), representing a composite relation R = R1\u25e6 R2\u25e6\u00b7\u00b7\u00b7\u25e6 Rk between node types A1 and Ak+1.\nDefinition 3 (Multi-facet Node Representation Learning). For a graph G = (V,E) with nodes V, the goal is to learn a multi-facet node embedding E_{facet}(v_i) \u2208 R^{K\u00d7d} for each node v_i. Here, K is the number of facets, and each facet embedding E_n(v_i) \u2208 R^{1\u00d7d} aims to: 1) preserve network structure, 2) capture various aspects of v_i, and 3) represent facet interactions.\nDefinition 4 (Multi-facet Path). Unlike predefined meta-paths, a multi-facet path P = v_1 R_1 v_2 R_2... R_m v_{m+1} is extracted without manual specification. Intermediate nodes v_2,..., v_m are projected onto multi-facet vectors F_2,..., F_m, forming a multi-facet path P = v_1 \u2192 F_2 \u2192 ... \u2192 F_m \u2192 v_{m+1}. This path integrates information from intermediate nodes through facets, enhancing its descriptive power beyond node types."}, {"title": "Methodology", "content": "In this section, we describe our approach, as shown in Figure 3. We begin by extracting paths between single-type nodes using random walks, similar to traditional metapath methods. We then project the features of intermediate nodes into multiple facets and aggregate these facets to create multi-facet vector representations. These representations are used as edge features between path-guided neighbors and the target node. The model pseudocode is provided in Algorithm 1, and the notation is summarized in Table 1."}, {"title": "Extracting Paths Using Random Walk", "content": "Assuming we are predicting for a given node type A, our method constructs a homogeneous subgraph by identifying direct connections between nodes of type A, similar to traditional metapath-based models. We then apply a random walk starting from a node of type A, progressing through adjacent nodes until it returns to the same node type. This approach captures relationships up to a specified length limit, allowing us to map connections like user-user or author-author without relying on predefined metapath patterns. Our method efficiently extracts meaningful information without being constrained by the graph\u2019s size. Detailed Algorithm Random Walk is provided in the Supplementary."}, {"title": "Transforming Single Vectors into Multi-Facet", "content": "Our model captures unique information from all node types before generating a facet vector. Initially, node information is exchanged through a GNN warm-up phase. The outcome of this phase provides initial features for each node, which are then used to train the main model. The resulting node vector, E(v_i), is refined into a multi-dimensional vector E_{facet} (v_i) to capture diverse information, as detailed in Equation 1."}, {"title": "Multi-facet Feature Vector Aggregation", "content": "Aggregating Multi-Facet Feature Vectors at the Path Level To construct a multi-facet feature vector for paths consisting of multiple nodes, we employ an aggregation function that synthesizes information from the nodes along the path. The mathematical formulation for this aggregation is given by Equation 2.\nP_{facet}(v_i, v_j) = f(E_{facet}(v_s)), v_s \u2208 path_{inter} (v_i, v_j) (2)\nIn the given equation, P_{facet} (v_i, v_j) represents the aggregated multi-facet feature vector for a connection between nodes v_i and v_j. The set path_{inter}(v_i, v_j) consists of intermediate nodes along this connection, and v_s is an intermediate node within the path. The function f signifies an aggregation operation using mean pooling."}, {"title": "Message Passing", "content": "Discriminative Facet Consideration using Gumbel Softmax We utilize the Gumbel Softmax (Jang, Gu, and Poole 2016) function to introduce randomness and uncertainty into facet selection, which helps the model explore diverse facet influences in complex relationships. The temperature parameter \u03c4 allows us to control the balance between exploration and exploitation. This approach enables us to capture nuanced facet interactions and dependencies, making our model more adaptable and robust for multi-facet data. Even if the random walk algorithm extracts the paths that are little related to the end nodes, we mitigate their impact by assigning higher weights to the most crucial components using Gumbel softmax. As a result, we effectively operate within constrained environments, even without extracting meaningful paths.\n\u03b1 = \u03c3(W_2 P_n(v_i, v_j))\nP(v_i, j) = \u2211_{n=1}^K \u03b1_n P_n (v_i, v_j) (3)\nIn the given equation, \u03c3 represents Gumbel Softmax. P(v_i, v_j) is the final embedding for path(v_i, v_j) that combines these probabilities.and W_2 is a matrix with dimensions"}, {"title": "Subgraph Construction", "content": "Path-based HGNNs transform the input graph into a new homogeneous subgraph G^P = (V^P, E^P) for each node and path, where G^P varies by model. For example, in HAN, G^P represents the metapath-P-guided neighborhood, while MAGNN defines it as instances of metapath-P incident to the node. MECCH constructs subgraphs using most metapaths as illustrated in Figure 4.\nOur approach, however, avoids the need for multiple metapaths by leveraging multi-facet information from a single path between nodes. This results in a subgraph G\u2032 = (V\u2032, E\u2032) that provides richer information than traditional methods. The MF2Vec model efficiently learns complex node relationships by constructing subgraphs as described in Algorithm Sub-graph Generation in the Supplementary, incorporating multi-dimensional edge features for a more comprehensive node representation."}, {"title": "Graph Convolution Using Edge Features", "content": "To extract node embeddings, we utilize GCN. In this process, we consider interactions between nodes and utilize path facets as edge features. The formula for calculating graph node embeddings is as follows:\nh^{(l+1)} (v_i) = \u03c3 (BN (\u2211_{j\u2208N(i)} P(v_i, v_j) h^l (v_j))) (4)\nIn this equation, \u03c3 represents the activation function, which is ELU, and BN represents the batch normalization. The P(v_i, v_j) denotes the path facet feature between node i and its neighbor j, which is utilized as an edge feature."}, {"title": "Training Losses", "content": "For node classification, we apply cross-entropy loss, defined as L_{nc} = \u2211_{v\u2208V_L} y_v log(y\u2032_v), where V_L is the set of labeled nodes, y_v is the true label, and y\u2032_v is the predicted probability.\nFor link prediction, we use a loss function based on negative sampling, defined as L_{lp} = \u2211_{(v,vpos)\u2208V+} log(\u03c3(h_v \u22c5 h_{vpos})) \u2212 \u2211_{(v,vneg)\u2208V-} log(\u03c3(\u2212h_v\u22c5 h_{vneg})), where \u03c3(\u00b7) is the sigmoid function, V+ denotes positive node pairs, and V\u2212 denotes randomly sampled negative pairs."}, {"title": "Experimental Settings", "content": "Datasets We evaluate our model on five heterogeneous graph datasets: DBLP, ACM, IMDB, and Freebase for node classification and clustering, and Yelp and Movielens for link prediction. For fairness, we use the best-reported hyperparameters, a fixed random seed of 0, and a negative sample count of one. For MF2Vec, the temperature \u03c4 and number of aspects K are set to 0.5 and 5, respectively. Experiments include up to 1000 trials per node with a walk length of 5 to cover typical metapaths (e.g., APA, APCPA), using consistent splits for training, validation, and testing. Details about the datasets and target nodes are summarized in Table 2.\nBaselines. We compare MF2Vec against state-of-the-art metapath-based HGNNs (Dong, Chawla, and Swami 2017; Wang et al. 2019; Fu et al. 2020; Jin et al. 2022; Wang et al. 2021; Guan et al. 2023; Sun et al. 2022; Fu and King 2024; Lv et al. 2021; Hong et al. 2020; Busbridge et al. 2019; Schlichtkrull et al. 2018). We compare the overall performance of the models in three downstream tasks (node classification, link prediction, and node clustering).\nHyperparameters To ensure fairness, we set the node embedding dimension to 64 across all HGNNs. Hyperparameters such as batch size, multi-head attention heads, dropout rate, learning rate, and the number of epochs are based on default settings from the original papers for optimal performance. For each model-dataset pair, we use a dropout rate of 0.4, a maximum path length of 5, a learning rate typically set to 3 \u00d7 10^{-3} from the range {1,3,5} \u00d7 10^{-3}, 10^{-2}, and a weight decay of 1 \u00d7 10^{-5} from {0, 10^{-3}, 10^{-5}}. The Adam optimizer and early stopping with a patience of 20 epochs are employed."}, {"title": "Node Classification (RQ1)", "content": "In our node classification experiments, we convert nodes into low-dimensional vector embeddings and apply a Softmax function to obtain probability distributions over label categories. We evaluate the model using Micro-F1, Macro-F1, and AUC metrics, providing a balanced assessment of classification performance. These metrics are used to compare our model, MF2Vec, against baseline models on the DBLP, ACM, and IMDB datasets, with results presented in Table 3.\nOur approach consistently outperforms baseline models across all metrics, demonstrating the effectiveness of"}, {"title": "Link Prediction (RQ2)", "content": "In Movielens and Yelp, link prediction is framed as a binary classification task to determine the presence of an edge in the original graph. During training, validation, and testing, negative edges are created by replacing the destination node in positive edges with a randomly chosen node that is not connected to the source node. The link probability is computed using the dot product of node embeddings: P_{v_i, v_j} = \u03c3(v_i \u00b7 v_j), where \u03c3 is the sigmoid function."}, {"title": "Node Clustering (RQ3)", "content": "We carry out node clustering experiments using the DBLP, ACM, and IMDB datasets to assess the embedding quality produced by MF2Vec. In these experiments, similar to node classification, we utilize the embedding vectors of target nodes from the testing set as input for the K-Means model and employ NMI and ARI metrics to gauge performance. As presented in Table 5, our model surpasses the baseline models, except for MHNF (Sun et al. 2022) in ARI on the IMDB dataset, where MF2Vec closely follows it by a small margin, demonstrating our model\u2019s superior ability to generate effective node representations based on multi-facet vectors for node clustering."}, {"title": "Visualization", "content": "To intuitively evaluate our model\u2019s embedding quality, we visualize node representations from various HGNNs on the DBLP dataset. Node vectors from the entire dataset\u2014training, validation, and testing\u2014are used before the final prediction layer and mapped to a two-dimensional space using t-SNE (Van der Maaten and Hinton 2008). Figure 5 displays t-SNE plots for HAN (Wang et al. 2019),"}, {"title": "Stability of performance (RQ4)", "content": "This section highlights the key advantage of leveraging multi-facets from paths, which is central to our approach. Unlike existing metapath-based models that rely on predefined schemas (e.g., APA, APCPA), our model maintains consistent performance across various schema types. By representing multi-faceted node characteristics as vectors, our model learns complex relationships within the graph. To validate this, we conducted a comparative analysis using a single metapath across all models, including MF2Vec, on the node classification datasets: DBLP, ACM, and IMDB.\nTable 6 shows the standard deviations in model performance for node classification using only one metapath. Unlike other models, which exhibit significant performance variation based on the chosen metapath, our model remains relatively stable, demonstrating that predefined metapaths can significantly impact model performance and incur additional costs. Specifically, for the DBLP dataset, performance varies greatly with different metapaths, indicating the critical role of metapaths and the associated time and cost for analysis. In contrast, our model shows stable performance without requiring every type of metapath."}, {"title": "Time complexity (RQ5)", "content": "The time complexity of the model, considering multi-facet embedding, attention, and graph convolution, is given by O(N \u00d7 P \u00d7 K) for embedding (where P is the path length, K is the number of facets, and d is the dimension of the vectors), O(N \u00d7 t \u00d7 d \u00d7 K) for attention, and O(N \u00d7 P \u00d7 d \u00d7 t^P) for convolution, resulting in overall complexity of O(N \u00d7 P \u00d7 d \u00d7 t^P). This is lower than that of MECCH (Fu and King 2024) and HAN (Wang et al. 2019), which is O(N \u00d7 M \u00d7 t^2 \u00d7 d), and MAGNN (Fu et al. 2020), which is O(N \u00d7 M \u00d7 t^M \u00d7 d \u00d7 P), focusing on facets rather than metapaths (M). Thus, we have MF2Vec < MECCH \u2248 HAN < MAGNN in terms of time complexity.\nFigure 6 illustrates the training time (in log seconds) and Macro F1 scores for various models, including MF2Vec, on the DBLP, IMDB, and Movielens datasets. MAGNN (Fu et al. 2020) and MP2vec (Dong, Chawla, and Swami 2017) are excluded from Movielens due to their longer training times. The plot shows that MF2Vec performs best in node classification and link prediction tasks with lower training times compared to existing methods (Fu et al. 2020; Guan et al. 2023; Wang et al. 2019; Dong, Chawla, and Swami 2017), demonstrating its efficiency. This is due to MF2Vec\u2019s ability to utilize individual path characteristics effectively, avoiding the additional computational costs associated with combining multiple paths as seen in other models."}, {"title": "Conclusion", "content": "In this study, we introduce MF2Vec, a novel heterogeneous graph neural network model that generates node representations using multi-facet vectors from various paths, regardless of node types. MF2Vec captures complex latent semantics through these multi-faceted paths, offering a type-agnostic embedding approach. Evaluations across six datasets demonstrate that MF2Vec outperforms leading baseline models in both performance and time complexity. Future work will focus on automating path selection within HGNNs using learnable methods to improve upon current random pre-selection methods, highlighting the potential of dynamic multi-facet path utilization in graph networks."}]}