{"title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies", "authors": ["Sibo Ma", "Alejandro Salinas", "Peter Henderson", "Julian Nyarko"], "abstract": "We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.", "sections": [{"title": "Introduction", "content": "Generative models like large language models (LLMs) are becoming a progressively central technology across many areas of life, including healthcare [46, 32], finance [27, 60], and education [4, 8]. Users also increasingly turn to these models for advice as they navigate day-to-day challenges, such as solving homework tasks [38] or improving mental health [35]. But as the utilization of this technology proliferates, so do reservations rooted in the risks and dangers the models may pose. Among the diverse set of risks, concerns that generative models could solidify or exacerbate bias are of particular sensitivity [17, 20, 24, 53]. \u03a4\u03bf mitigate the potential for generative models to harm society, several regulatory and liability frameworks have been proposed [16, 49]. For instance, the European Union Artificial"}, {"title": "Literature Review", "content": "Recent studies have revealed pervasive biases in LLMs, showing that prompts using names associated with different demographic groups yield systematically disparate outcomes. Names linked to white individuals often receive more favorable responses in areas like product recommendations and financial opportunities compared to those associated with Black individuals, while names associated with women tend to elicit simpler language compared to those associated with men. Overall, this literature documents pervasive biases along both racial and gender lines [25, 45, 43, 14, 6]. Moreover, LLMs exhibit biases that disadvantage individuals based on geographic and social factors, disproportionately rating regions with lower socioeconomic conditions (such as parts of Africa) unfavorably across sensitive top-ics like attractiveness, morality, and intelligence, while also reinforcing societal stereotypes across race, gender, religion, and health [33, 3]. Some studies suggest that LLMs trained with reinforcement learning from human feedback (RLHF) can \u201cmorally self-correct\u201d when prompted, offering a potential pathway to mitigate harmful biases by incorporating norma-tive concepts such as fairness and discrimination [19]. Despite these advancements, biases remain a significant concern, necessitating continued research to develop more robust miti-gation strategies.\nModel localization and pruning has emerged as a crucial technique for optimizing large language models (LLMs) by selectively removing network components to improve efficiency while maintaining performance. Various pruning methods have been proposed, each target-ing different objectives such as computational efficiency [51, 50, 59, 47], interpretability [48,"}, {"title": "Methods", "content": "We consider the following generic framework: We assume two groups, a majority group Gmaj and a minority group Gmin. A language model is prompted to elicit a response, where"}, {"title": "Localization", "content": "To identify pruning targets, we employed two alternative localization methodologies fre-quently referenced in the literature. Each method focuses on a specific component of the LLM: attention head scoring inspired by Wang, Zhang, and Han [51] and Adiga, Nushi, and Chandrasekaran [1], and neuron localization inspired by the approaches outlined by Sun et al. [47] and Wei et al. [52]."}, {"title": "Scoring for Neurons", "content": "We begin by measuring the importance of individual neurons in contributing to model outputs for specific tasks, with the eventual goal of mitigating biases. Our objective is to identify and score neurons that disproportionately influence the model's responses under these prompts.\nWe adopt the WandA (Weights and Activations) method proposed by Sun et al. [47], which evaluates a neuron's importance by quantifying how its activations affect the model's final predictions.\nFormally, for a given prompt of length 1, let $A \\in R^{l \\times d}$ represent the activation matrix for d neurons in a particular layer, and let $W \\in R^{d \\times o}$ be the corresponding weight matrix that projects these activations to o output dimensions. The contribution of neuron n to the output for each token t is calculated as the product of its activation and weight:\n$S_{t,n}^{neuron} = Act_{t,n}W_{n}$                                                                                                                     (1)\nwhere $Act_{t,n}$ is the activation of neuron n for token t, and $W_{n}$ is the corresponding associated column in W. The total score of neuron n across all tokens in a given prompt is then aggregated as:\n$S_{n}^{neuron} = \\sum_{t=1}^{l} S_{t,n}^{neuron} = \\sum_{t=1}^{l} Act_{t,n}W_{n}$                                                                                                                    (2)\nThis procedure yields a neuron-level score, $S_{n}^{head}$, reflecting the aggregate contribution of each neuron to the model's final outputs under a particular prompt."}, {"title": "Scoring for Attention Heads", "content": "We hypothesize that reducing the model's focus on the tokens with the group (Gmaj or Gmaj) membership information, denoted as group tokens $T_{G \\in \\{G_{maj},G_{min}\\}}$ should reduce the bias observed in the outputs. Consistent with this hypothesis, we examine the attention that is allocated to such tokens.\nIn the Transformer architecture, each attention head computes a set of attention weights that determine how much focus each token in the input sequence places on earlier tokens. This mechanism enables the model to selectively incorporate relevant contextual information when generating each token. For a given attention head, the attention mechanism is defined as:\n$Attention_{h} (Q_{h}, K_{h}, V_{h}) = softmax(\\frac{Q_{h}K_{h}^{T}}{\\sqrt{d_{k}}}) V_{h} = A_{h}V_{h}$                                                                                                                          (3)\nwhere $Q_{h}, K_{h}, and V_{h}$ are the query, key, and value matrices derived from the input embeddings, and $d_{k}$ is the dimension of the key vectors. The matrix $A_{h} \\in R^{n \\times n}$ represents the attention weights after applying the softmax function, where n is the total number of tokens in the input sequence. The softmax function ensures that each row of $A_{h}$ forms a probability distribution over the input tokens, determining how much attention each token pays to others.\nIn decoder-only models, due to their autoregressive nature, tokens can only attend to previous tokens in the sequence. Let T denote the set of tokens that follow $T_{g}$. The submatrix $A_{T_{g}} \\in R^{|T|\\times|T_{G}|}$ is extracted from $A_{h}$, where each entry $A_{h}(i, j)$ represents the attention weight from the ith token in T to the jth token in $T_{g}$.\nThe intuition behind this focus is grounded in how attention mechanisms update to-ken representations. Specifically, multiplying Ah by V results in a weighted sum of value vectors, producing new token embeddings. For the tokens after the group tokens, a large attention weight on the group tokens would imply that the updated representations are heavily influenced by the membership information. To mitigate potential bias, we prefer these attention weights to be small, reducing their influence on subsequent tokens.\nTo quantify this influence, we define the attention score $S^{head}_{h}$ for head h as the maximum attention weight\u00b9 from any token in T to any token in $T_{g}$:\n$S_{h}^{head} = \\max_{i \\in T, j \\in T_{G}} A_{T \\leftarrow T_{g}} (i, j)$                                                                                      (4)\nA higher $S^{head}$ indicates that some token after the group-related tokens heavily rely on the group-related tokens, which could lead to biased or contextually skewed outputs. By identifying and analyzing heads with high $S^{head}$, we can better understand and mitigate the model's reliance on them, promoting more contextually balanced generation."}, {"title": "Bias Localization", "content": "We define a model component c as either an attention head h or a neuron n, where the score for each component is represented as $S_{c} \\in \\{S^{head}, S^{neuron}\\}$. To analyze systematic differences in model behavior, we extend this scoring metric across a broader set of prompts. Specifically, we evaluate $S_{c}$ using a list of majority group members $G_{maj}$ and minority group members $G_{min}$. For each model component c, this yields $|G_{maj}|$ scores associated with the majority group, denoted as $\\{S^{maj,G_{maj}}_{i}\\}_{i=1}^{|G_{maj}|}$ and $|G_{min}|$ scores associated with the minority group, denoted as $\\{S^{min,G_{min}}_{i}\\}_{i=1}^{|G_{min}|}$.\nTo summarize the influence of each component across these groups, we compute the average score for the majority and minority group separately:\n$\\bar{S}_{maj} = \\frac{1}{|G_{maj}|} \\sum_{i=1}^{|G_{maj}|} S^{maj}_{i}$, $\\bar{S}_{min} = \\frac{1}{|G_{min}|} \\sum_{i=1}^{|G_{min}|} S^{min}_{i}$                                                                                                                    (5)\nThese averages reflect the overall influence each component assigns to the prompts associated with majority and minority group membership, respectively.\nNext, we sort the components based on these average scores in descending order, gen-erating two separate rankings:\n$C_{maj} = argsort (\\{\\bar{S}_{i}^{maj}\\}_{i=1}^{C})$, $C_{min} = argsort (\\{\\bar{S}_{i}^{min}\\}_{i=1}^{C})$                                                                                                                     (6)\nwhere C is the total number of components. $C_{maj}$ and $C_{min}$ represent the ordered lists of components ranked by their average influence in processing majority and minority group membership, respectively.\nWe treat the majority group as the reference group, seeking to identify biases to the disadvantage of the minority group. To capture and isolate the components that dispropor-tionately influence outputs for minority-associated prompts, we focus on the set-difference with respect to the minority group. That is, we isolate those components that are particu-larly influential for the minority group, but not for the majority group. From each list, we select the top $\\tau_{min}$ components from $C_{min}$ and the top $\\tau_{maj}$ components from $C_{maj}$. We then compute the set difference:\n$D = \\{C_{1}^{min},..., C_{\\tau_{min}}^{min}\\} \\setminus \\{C_{1}^{maj},..., C_{\\tau_{maj}}^{maj}\\}$                                                                                       (7)\nThe set D contains the components that rank among the top $\\tau_{min}$ for the minority group but do not appear in the top $\\tau_{maj}$ for the majority group. This selection highlights compo-nents that are disproportionately influential when processing minority group membership, aligning with our goal of identifying model elements that may contribute to biased behavior."}, {"title": "Model Intervention", "content": "Having identified the sets of components that consistently exhibit disproportionate focus on minority group membership, we proceed to mitigate this bias by selectively pruning these components. The goal is to reduce the model's reliance on these influential components without disrupting the overall structure and utility of the model."}, {"title": "Evaluation Setup", "content": "Next, we assess and mitigate bias in language models, with a specific focus on disparities between Black and white racial groups. Our goal is twofold: first, to identify the model components that contribute to biased outputs; and second, to implement targeted inter-ventions that effectively reduce these disparities while preserving the specific functionality under investigation. Furthermore, we hypothesize that the presence of biased components may offer a systematic pathway for localizing and potentially generalizing bias mitigation strategies across different contexts. To achieve these objectives, we introduce a structured evaluation setup that examines bias from multiple perspectives."}, {"title": "Prompt Design", "content": "To systematically assess potential biases, we create a diverse set of prompts with three key objectives:\n\u2022 Advice-Seeking Scenarios. We follow the approach outlined in Haim, Salinas, and Nyarko [25], crafting single-turn prompts to simulate realistic requests for advice re-garding a third person. That third person's name is perceived to be a strong predictor of race [18]. The design isolates the effect of bias at the response-generation stage, offering an interpretable framework for bias detection.2\n\u2022 Quantifiable Outcomes. Each prompt yields a numeric outcome, enabling straight-forward bias quantification and reducing ambiguity or subjective interpretation.\n\u2022 Consistency Across Prompts. Ensuring structural consistency across prompts is crucial for isolating the impact of implicit race-associations. We maintain a uniform structure within each scenario, altering only the name and key contextual elements"}, {"title": "Bias and Utility", "content": "To compare model performance across different settings, we quantify disparities using the Standardized Mean Difference (SMD). The SMD measures the difference in means between two groups (Black- and white-associated names) relative to the pooled standard deviation Andrade [2]. It offers a standardized scale that enables meaningful comparisons across different prompt variations and experimental conditions. More than just measuring raw mean differences, SMD accounts for variability within each group, making it more robust to differences in scale and distribution. Additionally, SMD allows us to combine results from various variations and scenarios in a consistent manner, as its standardized nature ensures that differing units or scales do not distort the overall bias measurement. It is formally defined as\u00b9:\n$SMD = \\frac{\\bar{X}_{black} - \\bar{X}_{white}}{s_{p}}$                                                                                                                     (9)\nwhere $\\bar{X}_{black}$ and $\\bar{X}_{white}$ represent the mean outputs for prompts containing Black- and white-associated names, respectively, and $s_{p}$ is the pooled standard deviation given by:\n$s_{p} = \\sqrt{\\frac{(100\\cdot|G_{black}| - 1)s_{black}^{2} + (100 \\cdot |G_{white}| - 1)s_{white}^{2}}{100\\cdot|G_{black}| + 100 \\cdot |G_{white}| - 2}}$                                                                              (10)\nAs shown by Sun et al. [47] and Wei et al. [52], pruning may decrease model performance due to the removal of knowledge. To assess a potential reduction in model capabilities, we define a context-specific utility metric. In our application, a model without utility is one that either yields no quantitative response or yields responses of an implausible quantity (e.g. several million dollars for a car). Consequently, we define our utility metric as the Inlier Ratio, which is the fraction of predicted prices that fall within the range of the unpruned model's outputs, excluding all generated non-numeric answers. This metric captures the extent to which the pruning methods preserve the original model's output distribution."}, {"title": "Localization & Generalizability Analysis", "content": "Following Section 3.2, the set D identifies components that disproportionately focus on Black names within a single prompt variation. However, this analysis alone does not confirm whether these components consistently exhibit the same behavior across different contexts. To rigorously evaluate the generalizability and robustness of these influential components, we employ a three-step approach designed to progressively assess their stability and rele-vance.\nEach step in our evaluation progressively increases the complexity, diversity, and ab-stractness of the tested scenarios to determine whether the identified components reflect systematic, generalizable patterns of racial bias or context-specific artifacts."}, {"title": "Prompt Specific Performance", "content": "To evaluate the method's ability to identify influential components under ideal conditions, we create distinct variations of the prompt, each introducing slight contextual changes while preserving the core structure. This step allows us to assess the best possible performance of our method in detecting biased components.\nFollowing Haim, Salinas, and Nyarko [25], we adopt the Purchase scenario as our base-line, in which a user negotiates with a seller and seeks advice on an appropriate offer. The prompt varies the seller's name, introducing implicit racial associations. We select N = 10 products/variations with large baseline disparities (for details on the selection process, see Appendix A.1).\nFor each of the selected N = 10 variations, we repeat the procedure outlined in Equa-tion 7, generating the set of influential components Dk for each prompt variation k:\n$\\left\\{D_{1}, D_{2}, ..., D_{N}\\right\\}$                                                                                                             (11)\nEach set Dk aims to capture the components that are uniquely influential for Black names in the kth prompt variation. To evaluate their impact, we prune the identified components in Dk from the model and assess the pruned model's performance on the cor-responding kth prompt variation. This pruning is applied individually for each variation to observe how the removal of these components affects the model's responses within the specific context of that prompt."}, {"title": "Within-Context Generalization", "content": "To assess the stability and consistency of these influential components across similar prompt variations, we perform a leave-one-out analysis. This step evaluates whether the identified components are sensitive to minor contextual changes or if they represent a more general, albeit still within-context, pattern. For each prompt variation k, we take the intersection of the remaining N \u2212 1 = 9 sets:\n$D_{k}^{LOO} = \\bigcap_{i=1 \\\\ i \\neq k}^{N} D_{i}$                                                                                                                     (12)"}, {"title": "Cross-Context Generalization", "content": "To further challenge the generalizability of the identified components across distinct and di-verse prompt contexts, we identify 3 additional scenarios: Services, Activities, and Finance. In each, we again identify 3 variations with high baseline disparities, resulting in a dataset of M = 9 unique variations. Further details on these scenarios are provided in Appendix A.3. This step examines whether the components consistently exhibit biased behavior across a broader range of inputs. Applying the same method, we compute the intersection of the newly derived sets:\n$D^{ctx} = \\bigcap_{j=1}^{M} D_{j}^{new}$                                                                                                                             (13)\nThe resulting set $D^{ctx}$ contains components that consistently influence responses across varying contexts. To evaluate their generalizability, we prune the components in $D^{ctx}$ and analyze the model's performance on the original 10 purchase prompt variations, assess-ing whether the removal of these cross-context components affects disparities within the Purchase scenario."}, {"title": "Parameter Selection", "content": "An important consideration in our approach is the choice of the parameters $\\tau_{min}$ and $\\tau_{maj}$ defined in Section 3.2. These parameters determine the thresholds for selecting the most influential attention heads or neurons. In our analysis, we consider the minority and major-ity groups in the context of racial bias, specifically focusing on Black- and white-associated names as representatives of these groups. For neurons, we define $\\tau_{min}$ and $\\tau_{maj}$ as the percentages of the highest-scored neurons for Black-associated and white-associated names, respectively. This approach ensures that we capture the most impactful neurons in relation to each demographic group. In the case of attention heads, $\\tau_{min}$ and $\\tau_{maj}$ are defined as fixed raw counts of the top-ranking heads for Black-associated and white-associated names, reflecting a more discrete selection criterion. These parameters directly influence the con-struction of the set D, which is formally defined in Section 3.2, and play a crucial role in our bias localization and mitigation strategy.\nFor attention-head pruning, the optimal values determined through empirical evaluation are $\\tau_{min}$ = 40 and $\\tau_{maj}$ = 5, while for neuron-level pruning, the optimized parameters are $\\tau_{min}^{*}$ = 0.40 and $\\tau_{maj}^{*}$ = 0.35. These values were selected based on an evaluation"}, {"title": "Results", "content": "Overall, we find that specific context information allows for the effective reduction of bias in many scenarios, without creating a significant increase in outlier model behaviors (sec-tion 5.1). More generalized approaches, while still mitigating bias, do so to a lesser extent when compared to context-specific methods (section 5.2). At least when leveraging the methods we examine, our findings suggest that pruning might be a useful mechanism for reducing bias in narrow contexts, but there is no universal \"bias neuron\" that drives dis-parate outcomes universally."}, {"title": "Pruned Models Behavior", "content": "In Figure 2, the top panel depicts the SMDs across the ten variations of the Purchase scenario, comparing the unpruned baseline (green) against the three pruning approaches. Without any pruning strategy, the model suggests drastically higher prices for white-associated names than for Black-associated ones, making it evident that the unpruned model exhibits the largest negative shift (mean \u2248 -0.59). After applying the \u201cPrompt-Specific\u201d Neuron Pruning approach (orange), we observe that SMDs are close to zero (mean \u2248 +0.07) suggesting that it effectively reduces the targeted bias. This approach represents the best-case scenario and the method may overfit to the specific context, leading to optimal but potentially not generalizable performance. Similarly, the same approach for Attention Head Pruning mitigates bias, albeit to a smaller extent (mean \u2248 \u22120.17).\nThe \"Within-Context\" (blue) and \u201cCross-Context\" (brown) pruning approaches for both Neuron and Attention Head pruning also reduce bias, although to a lesser extent (means of ~ -0.20 and \u2248 -0.37, respectively for Neuron; and means of \u2248 -0.31 and \u2248 -0.57, correspondingly for Attention Head). These distributions show that a pruning approach tailored to a particular bias context can substantially mitigate bias, whereas more gener-alized pruning offers only partial improvement. Additionally, our findings also shows that Neuron pruning outperforms Attention Head pruning in every approach we tested.\nIn the bottom panel, the inlier ratio remains high (\u2265 0.98) for all variations and prun-ing strategies for both Neurons and Attention Heads, indicating that pruning does not drastically diminish the model's functionality that we are testing in this study. Notably, the \"Cross-Context\" approach has the highest mean inlier ratio (\u2248 1.0 for both methods), consistent with it being the strategy with the fewest alterations to the model components.\nTogether, the findings illustrate the trade-off between bias reduction and preserving the studied model functionality. They also highlight the trade-off between specificity and generalizability\u2014while all pruning methods appear to reduce biases relative to the unpruned model, the largest gains occur when the pruning is closely tailored to the particular context of the bias in question. Considering the decreases in broader, more diverse contexts, these findings also underscore the need for more adaptable bias mitigation strategies. Finally,"}, {"title": "Generalizability through the Lens of Shared Pruned Neurons", "content": "Figure 3 visualizes the overlap between the biased neurons in our Purchase scenario and from other scenarios. Specifically, the heat is defined as a fraction, with the numerator being the intersection of pruned neurons between every scenario's variation and each Pur-chase variations. The denominator is the total size of pruned neurons for the corresponding scenario's variation. The overlap ranges from around 0.12 to 0.16. Two main patterns emerge. First, there is heterogeneity in overlap, with the biased neurons identified in med-ical, tax preparation and personal cheffing consistently showing the highest similarities to the Purchase variations. These 3 variations in particular are part of the Services scenario. Second, other scenario variations (e.g., \u201cbird watching,\u201d\u201cskiing,\" and \"pottery\") which are all part of the Activities scenario share fewer pruned neurons (around 0.12-0.13). These small percentages, specially when compared to the amount of neurons shared across varia-tions within-context (see Appendix A.5) support our findings from Figure 2, where we see a decreasing effectiveness of pruning as the contextual difference increases."}, {"title": "Location of Pruned Neurons", "content": "Figure 4 presents a layer-by-layer heatmap illustrating how pruned neurons are distributed across both the network's subcomponents (q, k, v, gate, up, down) and layer indices (0-31). The heat represents the percentage of neurons pruned at each location, normalized by the total neuron count in that location. Warmer regions indicate that a higher fraction of neu-rons were pruned. The visualization demonstrates that neuron pruning is not uniform; there is a tendency for certain layers and sub-components to have consistently higher percentages of pruned neurons than others.\nIn particular, the attention sub-components (q,k,v) near the top of the Figure 4 ex-hibit relatively lighter coloration, suggesting fewer neurons pruned at those locations. This behavior is consistent throughout all the network's layer indices. In contrast, the MLP sub-component (gate, up, down) show more intense reds, indicating that a substantial fraction of pruned neurons originate from these sub-components.\nA line plot (blue) depicts the the neurons pruned at a given layer index, divided by the total number of neurons pruned. It shows a mild upwards trend, suggesting that the distribution of pruned neurons is a bit more concentrated towards the mid-to-late layers. These findings are broadly consistent with those by Adiga, Nushi, and Chandrasekaran [1], who found that bias tends to be more concentrated in the later layers. At the same time, the skew in our analysis is less pronounced than in their findings."}, {"title": "Discussion", "content": "Our results lend support to an emerging consensus (along with concurrent work [13]) that domain-specific adaptations not merely broad, \u201cone-size-fits-all\" mitigations-play an important role in effectively reducing biased outputs from generative models. As illustrated by our evaluations, these models often encode biases in ways that are heavily intertwined with particular domains (e.g., bias in financial decision-making vs. bias in commercial transactions). When pruned using training data from one domain, the improvements did not fully transfer to another domain. As far as current pruning methods are able to identify, there is no \"bias neuron\" that will affects outputs equally across scenarios. These findings speak directly to an ongoing legal and policy debate about which party should be incen-tivized to address potential mechanisms for discrimination in the model. The open question, in particular, is whether legal liability should be assigned primarily to the developers of a general-purpose model or to the deployers who adapt the model for specialized use. Two examples reflect this debate.\nFirst, our observations resonate with key provisions in the proposed EU AI Act, which adopts a risk-based approach for regulating AI systems [16]. The Act mandates that deploy-ers of models for \u201chigh-risk\u201d applications (e.g., medical devices, hiring tools) face additional duties for monitoring, auditing, and mitigating discriminatory impacts. By contrast, gen-eral purpose AI developers are subject to a different set of requirements without having"}, {"title": "Limitations", "content": "While our study provides insights into mitigating racial bias in language models, it has several limitations. The conclusions drawn from our work are closely tied to the specific pruning strategies and evaluation metrics employed. Although we observe a consistent trend of reduced effectiveness as generalization increases, the extent of this trade-off may vary de-pending on the underlying model architecture and dataset. On the measurement side, our reliance on Standardized Mean Difference (SMD) to quantify disparities introduces cer-tain limitations. While SMD effectively summarizes mean differences between demographic groups, it may not always capture critical distributional aspects such as variance, skew-ness, and the presence of outliers. These limitations can obscure important nuances in the model's outputs and make the final SMD values sensitive to extreme values, potentially impacting robustness. To address this concern, in Appendix A.6, we explore the Wasser-stein distance as an alternative measure, and find that they produce similar trends and conclusions, reinforcing the reliability of our findings. Lastly, it is important to note that our evaluation primarily focuses on a specific form of racial bias associated with names in an advice-seeking context. While the methodology we introduce is quite general, the results from the specific evaluation are thus limited in scope. Among others, we do not assess other forms of bias a language model may display, such as implicit associations [30]. Our method is also tailored to making binary comparisons between two groups. More work is needed to extend the methodology in order to allow it to capture the full and diverse breadth of identities a users may hold."}, {"title": "Appendix", "content": "A"}, {"title": "Prompt Selection for Pruning Evaluation", "content": "To systematically evaluate the impact of pruning on model disparities and utility, we first select prompt variations that exhibit significant bias. We focus on the Purchase scenario, which demonstrates the most pronounced disparities, making it an ideal test case for bias analysis. The prompt selection process consists of the following steps:\n1. We generate 30 prompt variations (e.g., different products to purchase) using a com-bination of LLM-generated suggestions and manual inspection to ensure diversity and relevance.\n2. Each variation is processed through the LLaMA 3-Instruct 8B model to measure the initial disparity in outcomes.\n3. Based on these evaluations, we select the N 10 variations with the highest disparities for further analysis.\nThese selected variations serve as the foundation for subsequent pruning experiments, allowing us to focus on cases where bias is most evident."}, {"title": "Threshold Optimization for Pruning", "content": "We conduct an extensive grid search to determine the optimal values of pruning thresholds $\\tau_{min}$ and $\\tau_{maj}$ for neurons and attention heads. The optimization process follows these steps:\n1. For each prompt variation, we explore different values of $\\tau_{min}$ and $\\tau_{maj}$ while moni-toring the size of the bias-influential set D defined in Equation 7.\n2. The thresholds are selected based on the elbow point principle, where the set size stabilizes, indicating diminishing returns. Empirical observations suggest $\\tau_{min} \\approx \\tau_{maj}$, leading to the choice of the following ranges.\nFor neuron pruning, the selected ranges were:\nWe define the parameter selection ranges for pruning as follows. Let $R^{\\tau_{min}}$ and $R^{\\tau_{maj}}$ denote the ranges for $\\tau_{min}$ and $\\tau_{maj}$, respectively.\nFor neuron pruning, the ranges are defined as:\n$R^{\\tau_{min}}_{neuron} = \\{0.05 \\cdot k \\mid k \\in \\mathbb{Z}, 1 \\leq k \\leq 10\\}$                                                                                                          (14)\n$R^{\\tau_{maj}}_{neuron} = \\{0.05 \\cdot k \\mid k \\in \\mathbb{Z}, 1 \\leq k \\leq \\frac{\\tau_{min}}{0.05}\\}$                                                                               (15)\nFor attention head pruning, the ranges are defined as:\n$R^{\\tau_{maj}}_{head} = \\{5 \\cdot k \\mid k \\in \\mathbb{Z}, 1 \\leq k \\leq 10\\}$                                                                                                               (16)\n$R^{\\tau_{maj}}_{head} = \\{5 \\cdot k \\mid k \\in \\mathbb{Z}, 1 \\leq k \\leq \\frac{\\tau_{min}}{5}\\}$                                                                                  (17)"}, {"title": "Utility definition", "content": "In our evaluation framework, utility is defined as the model's ability to generate responses within a reasonable and expected range, ensuring that pruning does not significantly al-ter the model's functional capabilities. Specifically, we consider a response to be utility-preserving if it falls within the established bounds derived from the unpruned model's outputs.\nTo achieve this, we follow a two-step process:\n1. Quantitative Filtering: Since model responses may contain both numerical and non-numerical elements, we apply regular expression (regex) rules to extract quanti-tative parts of the output. If a response cannot be successfully parsed into a numerical value (e.g., free-text responses, incomplete numbers, or ambiguous answers), it is clas-sified as a non-quantitative response and is considered out of the utility-preserving range."}, {"title": "Range-Based Filtering", "content": "We establish a reference range for acceptable numerical values based on the minimum and maximum outputs generated by the unpruned model across all prompt variations. Any response that falls outside this range is marked as a utility violation. This range-based filtering ensures that extreme devia-tions resulting from pruning do not adversely impact the model's expected behavior.\nTo mitigate the impact of outliers and ensure robustness, we apply a winsorization process to the unpruned model outputs, capping extreme values at predefined percentiles. This prevents the influence of exceptionally high or low values from skewing the utility range and provides a more stable evaluation metric. Results without winsorization are included in Appendix A.6 and are consistent with the findings in the main paper."}, {"title": "Shared Pruned Neurons", "content": "To better understand the consistency of pruning across different prompt variations, we analyze the overlap of pruned components within the Purchase scenario. Figure 7a and Figure 7b provide heatmaps that quantify the extent to which biased neurons and attention heads are shared across prompt variations. This analysis helps to assess whether certain components consistently contribute to biased behavior, thereby informing the reliability of our pruning strategy."}, {"title": "Robustness Tests", "content": "To evaluate the robustness of our findings, we conduct two additional analyses using a variant of our main metric (SMD) without winsorizing outliers and an alternative metric, Earth Mover's Distance (EMD).\nFigure 8 presents the SMD across the ten variations of the Purchase scenario without removing extreme values. As noted in Figure 2 and Appendix A.4, our main analyses win-sorize outliers to avoid double-counting them in both the bias metric (SMD) and the utility"}]}