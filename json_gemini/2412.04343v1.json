{"title": "RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse", "authors": ["Zhouyingcheng Liao", "Mingyuan Zhang", "Wenjia Wang", "Lei Yang", "Taku Komura"], "abstract": "While motion generation has made substantial progress, its practical application remains constrained by dataset diversity and scale, limiting its ability to handle out-of-distribution scenarios. To address this, we propose a simple and effective baseline, RMD, which enhances the generalization of motion generation through retrieval-augmented techniques. Unlike previous retrieval-based methods, RMD requires no additional training and offers three key advantages: (1) the external retrieval database can be flexibly replaced; (2) a hierarchical retrieval module to reuse body parts from the motion database, with an LLM (large Language Model) facilitating splitting and recombination; and (3) use a pre-trained motion diffusion model as a prior to improve the quality of motions obtained through retrieval and direct combination. Without any training, RMD achieves state-of-the-art performance, with notable advantages on out-of-distribution data.", "sections": [{"title": "1. Introduction", "content": "Motion generation has been widely used in film and gaming industries. Although text-to-motion [9, 23, 29, 32-35] has made significant progress, it still performs poorly on out-of-distribution (OOD) text inputs in real-world applications.\nCurrent methods can be divided into two primary categories. The first category of approaches [9, 29, 30, 32, 34, 35, 37] directly generates motion from text. Unlike tasks such as text or image generation, which benefit from vast training datasets, 3D human motion datasets are limited in diversity and scale. This limitation poses challenges for handling out-of-distribution (OOD) cases in motion generation. The second category [2, 4, 19, 33] retrieves motions based on the given text and incorporates them into the generation model. This approach enhances performance on specific datasets. However, they require additional training and still struggle to deliver satisfactory results for OOD scenarios.\nTo address gaps in motion data, human expertise is sometimes required. For instance, if the motion database lacks a motion of simultaneously walking and waving, a human artist might blend lower-body walking with upper-body waving, making minor adjustments to create the desired motion. Inspired by artists' effective approach to creating an unseen motion, we developed a simple, training-free baseline called RMD (Retrieval-augmented Motion Diffuse) to enhance generalized motion generation.\nRMD consists of two main stages, namely motion retrieval stage and motion diffusion stage. In the motion retrieval stage, relevant motions are selected and combined based on user input text prompt, using a customizable external motion database. In the motion diffusion stage, the combined motions are refined by a pre-trained motion diffusion model to enhance motion quality.\nTo improve generalization and flexibility, we employ an external motion database for retrieval. The key challenge lies in locating the appropriate full-body or body-part-specific motions. Given the multi-granular nature of motions, we designed a simple multi-level retrieval pipeline using an LLM. When a text input is received, the pipeline first assesses if the motion could be split by body part. For example, \"backflip\" is a complete motion, while \"walking and waving\" is a composite. The former is searched as a whole in the database, whereas the latter is divided into upper and lower body components for separate retrieval. For input requiring finer details, the LLM further decomposes the motion into specific body parts and retrieves each part individually. Examples can be seen in Figure 1. When the multiple body part motions are retrieved, they are assembled according to the LLM's decomposition. Apparently, direct combinations of different body parts can lead to misalignment issues since each retrieved part is not aware of the overall posture and semantic coherence. To address this, we use a pretrained motion diffusion model as a prior to improve motion quality and diversity. Inspired by SDEdit [16], we adopt a noise-and-denoise scheme. In the diffusion stage, RMD first adds noise to the combined motion from the retrieval stage and then leverages a trained motion diffusion model to denoise it under the guidance of input text, refining the motion for better quality.\nExperiments demonstrate that RMD achieves state-of-the-art performance with a simple, training-free pipeline, excelling particularly on out-of-distribution data. On the standard benchmark HumanML3D [8], using the training set as the motion retrieval database still yielded performance gains, suggesting that current training algorithms do not yet fully exploit the knowledge in training datasets. In addition to standard in-domain testing, we use Mixamo dataset [1] to evaluate various algorithms under cross-domain scenario. Results show that our simple retrieval-based design can outperform existing approaches. Existing algorithms exhibit low overall accuracy in cross-domain settings, highlighting significant room for improvement in generalization. Furthermore, methods that perform well on HumanML3D do not necessarily generalize to Mixamo, indicating potential overfitting in current approaches. We also designed practical prompts based on popular video content and conducted a user study, which revealed that, while accuracy on HumanML3D saturates, visualizations still fall short of user expectations, leaving a gap before achieving the ease of use seen in text or image generation applications.\nIn summary, we present a lightweight, easy-to-produce, and high-performance method, systematically benchmarking existing methods under OOD scenarios and setting a new baseline for future general motion generation."}, {"title": "2. Related Work", "content": "Text-driven motion generation. Text-to-motion is a prominent topic in conditional motion generation, requiring models to interpret text and generate corresponding motions. The advent of diffusion models, such as MotionDiffuse [32] and MDM [29], has introduced a new paradigm in motion synthesis. Models such as MotionGPT [36] and MotionLLM [5] integrate language and motion by leveraging Large Language Models (LLM), enabling contextually rich motion generation, editing, and understanding. Story-to-motion [23] uses LLMs for semantic control and enhances transitions with transformers. FineMoGen [34] improves the generation of complex, temporally coordinated motions, by explicitly modeling spatio-temporal composition constraints. STMC [20] proposes a test-time denoising method to allow users to specify a multi-track timeline of several prompts for different body parts. MoMask [9] incorporates masked modeling for text-to-motion, representing human motion as multi-layer discrete motion tokens. LMM [35] unifies mainstream motion tasks into a single generalist model. These models leverage multi-modal learning for diverse motion tasks, but their generalization to real-world out-of-distribution scenarios remains limited.\nRetrieval-augmented text-to-motion. It has been proven that Retrieval-Augmented Generation (RAG) is effective in enhancing generative models for LLMs [7, 10, 15, 25], image generation [3, 6, 27], and video generation [11],"}, {"title": "3. Method", "content": "Fig. 2 illustrates our framework. First, we construct a motion database \\(M = \\{(x_i, \\text{text}_i)\\}\\), where each motion description \\(\\text{text}_i\\), is decomposed into half-body descriptions and fine-grained descriptions. Given a text prompt \\(\\text{text}_p\\) describing a human motion, we employ a hierarchical strategy to determine whether the prompt should be used directly, decomposed into half-body motions, or further split into fine-grained motions. After retrieving motions from each body part, we recompose them to form a full-body motion, referred to as the guided motion. However, the composition might bring extra artifacts. Thus, we apply SDEdit with a pre-trained diffusion model, using original prompt \\(\\text{text}_p\\) to refine the guided motion for an optimal balance of semantic accuracy and motion quality."}, {"title": "3.1. LLM-based Motion Decomposition Agent", "content": "Human motion is inherently compositional, i.e., it is composed of distinct movement of various body parts, making it hard to a motion database containing all kinds of human movement. Our key insight is that by decomposing a full-body motion into more atomic sub-motions of different body parts and recomposing them, existing motion databases can cover a much wider variety of OOD motions. Meanwhile, for the same motion, there exist numerous descriptions of different phrasing and level of details. Even for a motion seen during training, the model could still fail to generate if the test prompt diverges from the training text distribution. Decomposing the training motion texts and the test prompt using the same LLM agent helps bridge this gap.\nFor a prompt describing a full-body motion \\(\\text{text}_{\\text{full}}\\), we use an LLM agent to decompose it into descriptions of two levels of granularity:\n1) Half-body level, decomposing the motion into upper body and lower body \\(\\text{TEXT}_{\\text{half}} = \\{\\text{text}_{\\text{upper}}, \\text{text}_{\\text{lower}}\\}\\) ;\n2) Fine-grained level, decomposing the full-body motion into sub-motions for six body parts*, \\(\\text{TEXT}_{\\text{fine}} = {\\text{text}_{\\text{head}}, \\text{text}_{\\text{torso}}, \\text{text}_{\\text{larm}}, \\text{text}_{\\text{rarm}}, \\text{text}_{\\text{lower}}, \\text{text}_{\\text{traj}}}\\).\nFor motions with multiple text prompts, we concatenate them and inform the LLM that they describe the same motion. Empirically, this approach yields better results than treating the prompts as separate items and decomposing them individually."}, {"title": "3.2. Hierarchical Motion Retrieval", "content": "To perform retrieval, we start by defining a feature set for each data entry: \\(\\{\\text{text}, l\\}\\), where \\(l\\) is the length of the motion, and \\(\\text{text} \\in {\\text{text}_{\\text{full}}, \\text{TEXT}_{\\text{half}}, \\text{TEXT}_{\\text{fine}}}\\).\nWe then collect all the training motions to construct the retrieval database. For a given query prompt, we compute its corresponding feature set and search for its best match in the database. The retrieval for each body part is handled independently.\nNaive retrieval. Our approach extracts features to measure similarities between input text descriptions and database entities. Using the pre-trained CLIP model [24], we generate text embeddings for both queries and data points. For each data entry \\((x_i, \\text{text}_i)\\), we derive the text-query feature \\(f_i = E_T(\\text{text})\\) using CLIP's text encoder \\(E_T\\). Following ReMoDiffuse [33], we also consider motion sequence length as a crucial feature for retrieval to compute the similarity score \\(s_i\\):\n\\[s_i = (f_i, f_p)e^{-\\gamma}, \\gamma = \\frac{|l_i - l_p|}{\\text{max}\\{l_i, l_p\\}}}, \\]"}, {"title": "Hierarchical retrieval", "content": "Decomposing full body motion into distinct body parts allows for greater flexibility in composing OOD motions through retrieval. However, composing motions from different sources can result in unnatural movements. Therefore, we prioritize full-body retrieval or decomposition into fewer body parts whenever possible. To determine the decomposition approach, we first evaluate if the similarity score for full-body retrieval \\(s_{full}\\) exceeds a specified threshold \\(\\tau_{full}\\). If it does, we proceed with full-body retrieval. If not, we check whether the average similarity score for half-body decomposition \\((s_{upper} + s_{lower})/2\\) is greater than the threshold \\(\\tau_{half}\\). If this criterion is met, the half-body decomposition is used. Otherwise, the fine-grained decomposition is applied.\nMotion Composition. In the case where motion decomposition is chosen instead of direct full-body retrieval, we need to re-compose all the body parts to form a full-body motion. We select the quaternion of all corresponding joints for each body part and re-combine them. If the half-body decomposition is chosen, we copy the global translation from the retrieved lower-body. Since the retrieved motions might have different lengths from the query motion length, we rescale the retrieved quaternion and trans-"}, {"title": "3.3. Retrieval-Augmented Motion Diffusion", "content": "Through retrieval and composition, we obtain a full-body motion \\(x^{(0)}\\) that already roughly matches the input prompt. However, since the motions of different body parts are retrieved from different sources, composing them together might bring extra unnaturalness. For example, the motions of the upper body and the lower body might have different movement ranges and do not coordinate well. Previous works [13, 26, 28] have shown the potential of pre-trained diffusion models as priors across various tasks. Although many methods are intricately designed, for simplicity and reproducibility, we refine the guided motion following the SDEdit [16] approach, given its widespread use.\nSDEdit leverages a key insight about reverse Stochastic Differential Equations (SDEs): they can be solved starting from any intermediate time \\(t_0\\) between 0 and 1, not just from \\(t_0 = 1\\) as in previous SDE-based generative models. More specifically, we first choose a starting time \\(t_0\\) between 0 and 1. Then we initialize SDE with a noisy input \\(x\\) by adding Gaussian noise to the guided motion \\(x^{(0)}\\): \\(x = x^{(0)} + \\sigma(t_0)z\\), where \\(z \\sim N(0; I)\\). We use DDIM to solve the reverse SDE from time \\(t = t_0\\) to \\(t = 0\\) to progressively remove the noise to obtain denoised final motion \\(x^{(0)}\\). We denote this complete process as SDEdit\\((x^{(0)}; t_0, 0)\\). Given a total denoising step \\(N\\) and the noised \\(x\\) as input, for a denosing step \\(n\\), SDEdit first samples \\(z \\sim N(0; I)\\), then it updates \\(x\\) as:\n\\[\n\\epsilon = \\sqrt{\\sigma^2(t) - \\sigma^2(t - \\Delta t)}\\\\\nx = x + \\epsilon(\\frac{\\sigma(t)}{\\sigma_s(t)}s_e(x, t) + \\epsilon z),\n\\]\nwhere \\(t = t_0\\), \\(\\Delta t = \\frac{t_0}{N}\\), \\(s_e(x, t)\\) denotes the pre-trained score model. \nThe choice of \\(t_0\\) (alongside the discretization steps used by the SDE solver) is the key hyperparameter in SDEdit. It provides the user the flexibility to balance between the retrieved and composited motion and the diffusion prior. When \\(t_0\\) is small, it maintains similarity to the guide \\(x^{(0)}\\). As it \\(t_0\\) increases, the generated motion becomes closer to the diffusion prior. When \\(t_0 = 1\\), the process is a standard diffusion sampling without the motion guide."}, {"title": "4. Experiment", "content": "4.1. Datasets and Metrics\nDatasets. Our evaluation involves two motion datasets: HumanML3D [8] and Mixamo [1]. HumanML3D [8] is a standard text-to-motion benchmark. Derived from HumanAct12 and AMASS datasets, it contains 14,616 motions with 44,970 text descriptions. To evaluate the methods under out-of-distribution scenario, we curate a testset using Mixamo [1], containing over two thousand motion sequences and corresponding text captions.\nEvaluation metrics. We adopt five standard metrics following previous works [8, 32]:\n1. R-Precision: Assesses text-motion alignment by calculating the probability of matching the correct text description within the top \\(k\\) candidates (\\(k = 1, 2, 3\\)).\n2. Frechet Inception Distance (FID): Measures generation quality by computing the distance between real and generated motion features.\n3. Diversity: Quantifies the overall variety among generated motion sequences.\n4. Multimodality: Measures the variation in motion sequences generated from a single text prompt.\n5. Multi-Modal Distance (MM Dist): Calculates the average Euclidean distance between motion and text features."}, {"title": "4.2. Implementation Details", "content": "We employ GPT-40 (2024-05-01-preview) as the LLM in this work. For measuring the semantic similarity, we use the frozen text encoder in the CLIP ViT-B/32 [24]. To construct the retrieval database, we simply use all the training data as the entries. For the diffusion model, we re-train MotionDiffuse [32], consisting of 12 transformer layers, which has better performance than the version in the original paper. During inference, we apply DDIM sampling with a total step of 50. When not specified otherwise, we choose the retrieval length coefficient \\(\\lambda = 0.05\\), the retrieval thresholds \\(\\tau_{half} = 0.96\\), \\(\\tau_{fine} = 0.96\\), and the diffusion starting time \\(t_0 = 0.96\\)."}, {"title": "4.3. Comparison", "content": "We present quantitative and qualitative comparisons between our proposed method and several state-of-the-art approaches on the HumanML3D dataset. As shown in Table 1, our approach achieves the best R Precision scores and MM Dist. Compared to our base model, MotionDiffuse, our method shows superior performance across all metrics except a slight drop in MultiModality, demonstrating that our method brings instant improvement to the existing diffusion model without training. Figure 3 provides a visual comparison, highlighting the qualitative differences between our method and previous approaches. Our generated motions"}, {"title": "5. Conclusion and Future Work", "content": "In conclusion, we propose RMD, a simple, training-free baseline towards more general motion generation. By combining a retrieval-augmented framework with a pre-trained motion diffusion model, RMD outperforms existing methods in both in-domain and cross-domain settings. Furthermore, user studies validate RMD's ability to generate natural and semantically accurate motions, establishing a new benchmark for future motion generation research.\nLimitations and future work. Currently, \\(t_0\\) in our method is either fixed or specified by users. While"}]}