{"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "authors": ["Gabriel Roccabruna", "Massimo Rizzoli", "Giuseppe Riccardi"], "abstract": "The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs' performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs' performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on ROBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "An important ability in understanding information flows such as news is to recognize the temporal relations of events, which happened, are happening, or will happen, to order them into a coherent story-line. Indeed, temporal relations are utilized in many natural language processing tasks such as narrative understanding (Song and Cohen, 1988; Mousavi et al., 2023), story generation (Han et al., 2022), summarization (Liu et al., 2009; Gung and Kalita,"}, {"title": "2 Related works", "content": "Temporal Relation Classification task Temporal Relation Classification (or Extraction) models have predominantly used encoder-based pre-trained language models, such as BERT (Devlin et al., 2019) and ROBERTa (Liu et al., 2019), as a backbone. In particular, some studies have employed graph neural networks initialised with BERT and ROBERTa embeddings to model the semantic and syntactic context surrounding the events (Mathur et al., 2021; Zhou et al., 2022; Zhang et al., 2022). More recently, Cohen and Bar (2023) have been fine-tuned a ROBERTa model for answering interval relation reasoning questions to predict a given temporal relation class. Instead, LLMs have been tasked to answer multiple choice questions (Chan et al., 2023), which challenges them to understand the semantics of the different temporal relations classes.\nTemporal QA The purpose of extracting temporal relations among events proposed in TimeML (Pustejovsky et al., 2003a) is to improve the performance of temporal Question Answering task (Llorens et al., 2015; Meng et al., 2017; Yang et al., 2023), which is to answer temporal grounded queries such as \u201cWhich is the current US president?\u201d. Recently, the temporal QA task has been used for testing and challenging the temporal reasoning ability of LLMs (Wei et al., 2023; Tan et al., 2023) by querying the LLMs parametric knowledge with time-grounded questions.\nTemporal Relations Corpora One of the first and largest corpora annotated with temporal relations is the TimeBank (Pustejovsky et al., 2003b) corpus. This corpus has been annotated using"}, {"title": "3 Task Definition", "content": "The Temporal Relation Classification (TRC) task can be defined as follows. The corpus comprises a set of documents D. A document $d \\in D$ is defined as a sequence of sentences $d = [s_1, ..., s_n]$, where a sentence is a sequence of tokens i.e. $s = [W_1, ..., w_n]$. A sentence is delimited by a full stop, exclamation or question mark. Each document in the corpus contains a set of annotated event triggers $E = {e_1, ..., e_n}$ where e is a span of tokens of a sentence of a document i.e. $e = [W_i, ..., w_j] \\in s$ with $i > 0, j \\le |s|$ and $s \\in d$. The TRC task is to assign a temporal relation r from a predefined set R to a given pair of connected events $(e_i, e_j)$, where $e_i \\neq e_j$. The set of relations R changes depending on the annotation scheme of the corpus as described in Section 5.1. Besides, the temporal relations $r \\in (e_i, e_j)$ and $r' \\in (e_j, e_i)$ are always the opposite (e.g. before and after) i.e. $r = \\overline{r}$ except when r is the relation equal, where by definition $r = r'$. Indeed, in Allen's interval algebra, the 13 temporal relations are composed of equal plus six temporal relations and their corresponding opposites."}, {"title": "4 Models", "content": "In this section, we describe the encoder-only model and LLMs with fine-tuning and prompting approaches tasked with Temporal Relation Classification (TRC)."}, {"title": "4.1 Encoder-only Architecture", "content": "In this work, we use RoBERTa (Liu et al., 2019) as an encoder-only model. This model has been pre-trained on the Masked Language Modelling (MLM) (Devlin et al., 2019) task. In this, the model is tasked to predict a masked token attending to the rest of the sequence.\nInspired by (Zhou et al., 2022), we have used the following architecture to put RoBERTa in place. The input of the models is the corresponding sentences containing the event pairs (ei, ej). The events can be in the same (intra-sentence) or in two different (inter-sentence) sentences. Formally, the input for intra-sentence events is $C = s_k \\oplus E_i, E_j \\oplus s_k$ where $e_i, e_j \\in s_k$ and for inter-sentence events is $C = s_i \\oplus s_j$, $s_i, s_j \\in d$ where $e_i \\in s_i, l_j \\in s_j$. For the latter, we concatenated the two sentences with a white space. The input C is fed into a pre-trained model to compute the word embeddings of input tokens. From these, we retrieve the embedding corresponding to the tokens of the two events. Then, the event embedding is created by aggregating all the relative sub-tokens generated by the byte pair encoding tokenizer (Sennrich et al., 2016) using the max pooling function. Aggregating all tokens is important since the verb tense is a relevant aspect of this task; therefore, the -ed sub-token (i.e. the last sub-token) can be an important feature for the event embedding. The two events embedding are then concatenated. Finally, the resulting concatenated vector is fed into a feed-forward linear layer followed by a softmax to make the prediction."}, {"title": "4.2 In-Context Learning and Fine-Tuning", "content": "Large Language Models (LLM) have been pre-trained on a large scale of data using the autoregressive language model (Roth, 2000; Brown et al., 2020) as the objective task. Differently from MLM, the model has to predict the next token $t_{k+1}$ using the previous tokens, i.e., the context sequence $t_0, ..., t_k$ only.\nTo evaluate LLMs, we experiment with in-"}, {"title": "5 Experimental settings", "content": ""}, {"title": "5.1 Datasets", "content": "We have tested our models on TimeBank(TB)-Dense (Cassidy et al., 2014) and MATRES (Ning et al., 2018), which are widely used benchmarks, and TIMELINE (Alsayyahi and Batista-Navarro, 2023), which is a newly released corpus. TB-Dense is composed of 36 news articles, a subset of the TimeBank corpus, published in 1990 and 1998. TB-Dense has been annotated with 6 temporal relations i.e. before, after, includes, is included, simultaneous, and vague. MATRES includes all the 275 news articles used in the TempEval-3 challenge. All the news articles in the train and validation sets were written and published in the time range between 1990 and 2000, while in the test set, they are all dated 2013. The corpus has been annotated with four temporal relations before, after, equal and vague. TIMELINE is composed of 48 news articles published between 2020 and 2021 and has adopted the same temporal relation scheme of MATRES. We have used official train, development and test sizes and the label distributions are shown in Appendix A."}, {"title": "5.2 Evaluation metrics", "content": "All the models are evaluated using the micro-f1 score. Following the decision made for TIMELINE (Alsayyahi and Batista-Navarro, 2023), we have completely removed from MATRES and TB-Dense the vague class. This is because we want to focus only on temporal relations. The class vague is not a temporal relation (Wen and Ji, 2021; Zhou et al., 2022) as it has been used to handle ambiguities and disagreement during the annotation process (a.k.a. catch-all class). Indeed, we have used the class vague to map the examples for which the LLMs output gibberish or produce contradictory responses.\nIn TB-Dense in the prompts QA1 and QA2 for the label simultaneous, we have used the same question for the temporal relation equal as they have the same meaning."}, {"title": "6 Evaluation and Results", "content": "We have experimented with seven Large Language Models (LLM): five open-source, namely Llama2 7B, Llama2 13B, Llama2 70B (Touvron et al., 2023), Mistral 7B (Jiang et al., 2023), Mixtral 8\u00d77B (Jiang et al., 2024), and two closed-sourced,"}, {"title": "6.1 Explainability studies", "content": "We have studied the gap in performance between LLMs and the encoder-only ROBERTa-based model using an attribution method called KernelShap (Lundberg and Lee, 2017). KernelShap is an additive feature attribution method based on Linear LIME (Mishra et al., 2017) and SHAP values (Lundberg and Lee, 2017), which gives a score to each input vector element based on its importance to the prediction.\nFrom each input sequence in three test sets, we have computed \u2075 and extracted the five tokens with the highest attribution score. Regarding Llama2 7B with prompt P, we have observed that 70% of these tokens are positioned in the few-shot context, while the remaining come from the target sequence (i.e. the context of the two events to make a prediction). To be comparable with RoBERTa model, we present the distribution of the tokens based on their position coming from the target sequence only. To do this, we have computed the relative positions, i.e. scaling into a 0 to 1 range, dividing them by the sequence length."}, {"title": "6.2 Word Embedding analysis", "content": "We have compared the performance of the word embeddings generated by LLMs and ROBERTa models in the TRC task to investigate the differences due to the pre-training strategies.\nTo do this, we have used the architecture presented in Section 4 and replaced the encoder, i.e., RoBERTa, with Llama2 7B, 13B and 70B. We have experimented with training only the classification layer, i.e. freezing the weights of the encoder, and with full fine-tuning, using LoRa for the LLMs, to attain the upper-bound performance.\nThe results of these experiments are presented in Table 3. The micro-F1 scores attained with LLMS by training the classification layer only are higher than those achieved with the same models with ICL. However, RoBERTa still achieves the highest performance on MATRES and TB-Dense. Interestingly, LLama2 70B outperforms RoBERTa on TIMELINE. The possible reason for this is that TIMELINE contains many news articles related to the recent COVID-19 pandemic. Indeed, in more than 30% of target sequences in the training and test sets, there is one of the following words covid-19, coronavirus, pandemic and vaccine. Considering that ROBERTa was pre-trained in 2019, those tokens are out-of-vocabulary tokens for the model.\nBy further training the encoder, the models generally increase the performance on all corpora with an average improvement of 4% and 9% results on MATRES and TB-Dense respectively. Conversely, on TIMELINE, the improvement of LLama2 7B and 13B is considerably contained, and we observe a worsening in the performance of Llama2 70B. While ROBERTa gains an increment of 22.2%, providing additional evidence of the initial high presence of OOV tokens for the ROBERTa model.\nOverall, the results suggest that the word embeddings yielded by RoBERTa are more effective in the TRC task, supporting the outcome of the explainability studies for which one of the probable reasons for the performance gap is in the different pre-training tasks."}, {"title": "7 Error Analysis", "content": "We have analyzed the error of the ROBERTa-based model, Llama2 70B and Llama2 13B fine-tuned by comparing the performance between intra and inter-sentences on MATRES as reported in Table 5. The encoder-only RoBERTa-based model achieves the highest intra- and inter-sentence performance. Besides, all three models underperform on the intra-sentences, where the highest difference is measured on Llama2 70B.\nTo investigate whether there is a subset of the test set with challenging examples for all the models, we have computed the intersection between the errors of the ROBERTa-based model and the correct predictions of Llama2 13B fine-tuned and Llama2 70B with ICL. The sizes of these intersections are 23.3% for Llama2 13B fine-tuned and 33.7% for Llama2 70B, which account for 2.8% and 4.0% of the test set respectively. By manually"}, {"title": "8 Conclusions", "content": "In this work, we have evaluated seven open and closed-sourced LLMs on the Temporal Relation Classification task with an in-context learning and fine-tuning approach. We have shown that the encoder-only RoBERTa-based model achieves the highest results compared to LLMs. Explainable studies suggest that one of the reasons for this gap is due to the different pre-training tasks. Finally, considering the low performance and the huge amount of computational resources needed at fine-tuning and inference time, LLMs might not be the best option for the TRC task compared to a more accurate and low-resource demanding RoBERTa-based model.\nFuture work A possible future work is to further investigate the pre-training task differences, by pre-training two models on the autoregressive and masked language model tasks using the same parameter size and training set. Another possible direction is to study a hybrid architecture to join the best performance of the RoBERTa-based models and the Large Language models such as Llama2 13B fine-tuned and Llama2 70B."}, {"title": "Limitations", "content": "We could not experiment with the largest open-source model due to limited resources. Furthermore, the choice of using an additive feature attribution method rather than a gradient-based method is mainly based on computational time. Indeed, during our tests, we estimated that the total computational time for processing MATRES using the integrated gradients (Sundararajan et al., 2017) method was three weeks compared to one day with KernelShap (Lundberg and Lee, 2017)."}, {"title": "A Appendix", "content": "We report the tables regarding the label distribution of the three corpora (Table 6), the number of relations (Table 7), and the standard deviation for each in-context learning experiment done using five different few-shot prompts (Table 8). Furthermore, we provide the schema of the three types of prompts that we used in our experiments in Table 9. Table 11 presents the F1-scores for each class achieved using the best prompt settings. In addition to this, in Table 10 we provide a couple of wrongly predicted examples by the RoBERTa-based model which is challenging also for humans.\nIn this work, we have respected the original intended uses of datasets, models and any other artefacts."}, {"title": "A.1 Hyperparameters", "content": "For MATRES and TB-Dense we have chunked the text into sentences using NLTK\u2077 library as the corpus is natively split into paragraphs. The reason for this is to have minimal context for a given event.\nRegarding the RoBERTa-based model, we experimented with different configurations and hyperparameters. In this, we have used one AdamW (Loshchilov and Hutter, 2017) optimizer for the encoder and another for the feed-forward layers (i.e. the classifier) with a learning rate of 1e-5 and 1e-4 respectively. Furthermore, we have used a linear scheduler on the optimizer of the encoder with warmup steps set to 10% of the total steps in training. In all the experiments the batch size is set to 8 event-pair data points.\nIn the in-context learning experiments (Brown et al., 2020), the number of few-shots, i.e. the number of ground truth examples used as context for the prediction, is set to one for each class of the corpus, i.e. five for TB-Dense and three examples for both MATRES and TIMELINE. The templates of the few-shot are replications of the prompt in the zero-shot version. An example of each different prompt can be found in Table 9. The examples have been extracted randomly from the training set of each corpus. To measure the impact on the performance of this selection, we have sampled and frozen five different sets to create the few-shots context for all three prompt types, i.e. P, QA1 and QA2. In the few-shot context of QA1 and QA2 and at inference time QA2, we have kept the same order of the questions for all models and datasets which is"}]}