{"title": "Learn How to Query from Unlabeled Data Streams in Federated Learning", "authors": ["Yuchang Sun", "Xinran Li", "Tao Lin", "Jun Zhang"], "abstract": "Federated learning (FL) enables collaborative learning among decentralized clients while safeguarding the privacy of their local data. Existing studies on FL typically assume offline labeled data available at each client when the training starts. Nevertheless, the training data in practice often arrive at clients in a streaming fashion without ground-truth labels. Given the expensive annotation cost, it is critical to identify a subset of informative samples for labeling on clients. However, selecting samples locally while accommodating the global training objective presents a challenge unique to FL. In this work, we tackle this conundrum by framing the data querying process in FL as a collaborative decentralized decision-making problem and proposing an effective solution named LeaDQ, which leverages multi-agent reinforcement learning algorithms. In particular, under the implicit guidance from global information, LeaDQ effectively learns the local policies for distributed clients and steers them towards selecting samples that can enhance the global model's accuracy. Extensive simulations on image and text tasks show that LeaDQ advances the model performance in various FL scenarios, outperforming the benchmarking algorithms.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) (McMahan et al. 2017) has emerged as a distributed paradigm for training machine learning (ML) models while preserving local data privacy. In FL, the model training process involves multiple clients, each possessing its own local training data with different distributions. These clients jointly optimize an ML model based on their local data and periodically upload the model updates to the server. Afterwards, the server updates the global model by applying the aggregated updates and distributes the current model to the clients for next-iteration training. This approach has gained significant attention in practical applications such as healthcare (Rieke et al. 2020) and Internet of Things (Zhang et al. 2022a), due to its potential to address privacy concerns associated with centralized data storage and processing.\nMost of existing FL studies assume that clients have a fixed pool of training data samples with ground-truth labels (Li et al. 2023; Ye et al. 2023; Yang, Xiao, and Ji 2023;"}, {"title": "2 Related Works", "content": "Active learning AL algorithms are designed to select a subset of unlabeled data samples and query their labels from an oracle. The design goal is to improve the model performance on the target dataset by selecting the most informative samples within the querying cost budget (Liu et al. 2022; Cacciarelli and Kulahci 2024). The classic data selection metrics are uncertainty-based, diversity-based or the hybrid ones. Specifically, the uncertainty-based AL algorithm (Wang and Shang 2014) selects the unlabeled data with the lowest prediction confidence, which is straightforward for improving the performance of ML models. Meanwhile, the diversity-based method (Sener and Savarese 2018) aims to reduce redundancy in the selected instances for labeling, ensuring a more efficient use of labeling resources by avoiding similar samples. Furthermore, the hybrid method attempts to combine the strengths of both uncertainty and diversity in the selection criteria. In particular, some works (Fang, Li, and Cohn 2017; Zhang et al. 2022b) leverage reinforcement learning (RL) to learn a data query policy which is able to select the impactful data samples.\nFederated active learning Recent studies have extended the investigations on data query problems to FL scenarios, termed as federated active learning (FAL) (Kim et al. 2023; Cao et al. 2023; Ahn et al. 2024; Wu et al. 2023; Kong et al. 2024). In FAL, clients select some local data samples and ask the oracles to label these samples. Kim et al. (2023) design a two-step algorithm named LoGo to select informative data samples while resolving the inter-class diversity on clients. Meanwhile, KAFAL (Cao et al. 2023) leverages the specialized knowledge by the local clients and the global model to deal with their objective mismatch. Nevertheless, the clients only have access to their own data and tend to query the samples that are beneficial for their local training objective. Because of the discrepancy between the local and"}, {"title": "3 Problem Formulation", "content": "We consider an FL system where K clients with unlabeled data streams cooperate to train an ML model by alternating data querying and model training. The goal of clients is to minimize the following objective:\n$\\min_{\\theta} E_{(x,y)\\sim p}[l(\\theta; x, y)],$ (1)\nwhere $l(\\theta; x, y)$ denotes the loss value of model $\\theta$ on data sample $(x, y)$ with data input $x$ and corresponding label $y$. Note that the target data distribution $P$ is unknown and inaccessible. We assume that the aggregated training data on all clients follow the same distribution as that of the target dataset. Nevertheless, the local data distribution $P_k$ of each client $k \\in [K]$ is typically different from the global data distribution, which is also known as the non-independent and identically distributed (non-IID) setup in FL (Kairouz et al. 2021). Formally, the assumptions on data distributions are given as: $U_{k\\in[K]}P_k = P$ and $P_k \\neq P,\\forall k \\in [K]$.\nRaw training data without ground-truth labels arrive at client k according to the underlying distribution $P_k$ (Marfoq et al. 2023). According to the data arrival status, the training process of optimizing model $\\theta$ can be divided into R rounds. At the beginning of each round $r\\in \\{1,2,..., R\\}$, some new unlabeled data samples arrive at each client. To utilize these data for model training, clients actively query the labels of some selected samples from an oracle. Afterwards, they collaboratively train the ML model with the current labeled dataset. The above process is illustrated in Fig. 2. In the following, we introduce the procedure of data querying and model training in detail.\nActive data querying In the r-th round, a set of $N_u$ unlabeled data samples arrives at client k, denoted by $U_k = \\{x_i\\}_{i=1}^{N_u}$, where $x_i$ is the feature of the i-th sample. Each"}, {"title": "4 Motivating Example", "content": "In this section, we show empirically that a data querying strategy with a global view can benefit the FL training. In particular, we explore an FL setup in which clients collaborate to classify data samples from the MNIST dataset by training a LeNet model (LeCun et al. 1998). We assume there are $N_u = 10$ unlabeled data samples arriving at each client in each round and only one data sample is selected for labeling. The classical coreset method (Sener and Savarese 2018) selects the samples such that the model learned over the selected samples is competitive for the remaining data points. For an unlabeled dataset U and given labeled dataset L, the data sample to be queried is selected as:\n$x^* = arg \\max_{x \\in U} min_{x' \\in L} ||x - x' ||_2.$ (7)\nWe first implement the coreset approach on each client locally, i.e., $U = U^k$, termed as Local Coreset. In Local Coreset, clients make selections based on the local available data samples. Besides, we consider an ideal approach termed as Global Coreset where the server can access to all clients' data $U = \\sum_{k\\in [K]} U^k$ and determine one queried sample for each client. After querying the labels, clients iteratively train the ML model based on the labeled dataset. The experimental details are referred to Appendix B.1.\nThe evaluation in Fig. 3 focuses on the effectiveness of various data querying strategies and the distribution of queried data samples. It is observed that the Global Coreset strategy improves the model accuracy compared with Local Coreset, benefiting from the utilization of global information when selecting unlabeled data. This can be verified by the distribution divergence between the queried samples in each round and the global data distribution as shown in Fig. 3 (b). In particular, Global Coreset can select critical samples that coincide with the target data distribution, since it leverages the information from a global view.\nNevertheless, the global information in Global Coreset requires computing the sample-wise distance between each unlabeled sample and labeled sample. As the data samples are distributed on different clients, it is impractical to directly implement the Global Coreset, which severely violates the data privacy of clients. Thus, it becomes crucial to devise a strategy that leverages global insights without necessitating direct access to data across all clients. To this end, we propose the LeaDQ algorithm under a centralized training decentralized execution (CTDE) paradigm, which implicitly incorporates the global objective into the querying strategies while maintaining the decentralized execution as in FL. Consequently, LeaDQ achieves comparable performance (as shown in Fig. 3) to Global Corset while preserving the data privacy."}, {"title": "5 LeaDQ: Learning the Data Querying Policies for Clients", "content": "Effective data querying strategies in FL should exhibit the ability to select unlabeled samples that are beneficial for global model training, while making the decisions in a decentralized fashion. Motivated by the capabilities of MARL algorithms in making individual decisions while optimizing a collaborative objective (Vinyals et al. 2017; Wang et al. 2021), we propose an MARL-based data querying algorithm named LeaDQ, short for Learn Data Querying. In the following, we first formulate the data querying problem in FL as a Markov decision process and elaborate on the details of system design in Section 5.1. Subsequently, we detail the process of decentralized data querying and centralized policy training in Sections 5.2 and 5.3, respectively."}, {"title": "5.1 Data Querying in FL as A Decentralized Decision-Making Process", "content": "The objective of data querying per client k in FL aims to select samples from local unlabeled dataset $U_k$ to query their labels in round r. By data querying and model training, these clients aim to collaboratively optimize the global model $\\theta^r$. However, each client only has access to its local data without the knowledge of either data distribution of other clients or the target global data distribution, i.e., the decision making process is based on partial observation in its nature.\nTo describe such a setup, we frame the data querying problem as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek and Amato 2016) denoted by a tuple $(S, A, P, R, O, K, \\gamma)$. We view each querying round as a discrete timestep and each client as an agent in this process.\nAt each discrete timestep r, a global state $s^r \\in S$ reveals the current status of global model training. We assume the server owns a held-out dataset $D_{held}$ (Fang and Ye 2022; Huang, Ye, and Du 2022) and the global state is defined as the prediction confidence of the global model $\\theta^r$ on it:\n$s^r = \\max_{y \\in Y} p(y|x; \\theta^r).$ (8)\n$x \\in D_{held}$\nWe assume the held-out dataset follows the same distribution as that of the whole dataset, and thus the global state"}, {"title": "5.2 Decentralized Policy Execution for Querying Data Samples", "content": "In round r, client k has access to its local action-observation history $T_k \\in T = (O \\times A)^*$. Based on such history, the client makes a decision of selecting unlabeled data following its data query policy. Specifically, each client feeds the local observation $o_k^r$ of arrived unlabeled data into its policy"}, {"title": "5.3 Centralized Policy Training for Learning Local Policies", "content": "By selecting unlabeled data, all clients contribute to the global model training and get a shared reward, serving as the global feedback for the joint decision of clients. However, it is difficult for clients to directly use the shared reward for local data selections. To link the global feedback and local decisions, we define the joint Q-value as a weighted sum of the local Q-values, which is given by (Rashid et al. 2018):\n$Q_{tot} = \\tau (Q_1, \\dots, Q_k, s^r)$. (12)\nHere $\\tau$ is a mixing network from a family of monotonic functions to model the complex interactions between collaborating clients. This mixing network helps to coordinate"}, {"title": "6 Experiments", "content": "6.1 Setup\nWe simulate an FL system with one server and K = 10 clients. We evaluate the algorithms on two image classification tasks, i.e., SVHN (Netzer et al. 2011) and CIFAR-100 (Krizhevsky and Hinton 2009), and one text classification task, i.e., 20Newsgroup (Lang 1995). We train a convolutional neural network (CNN) model with four convolutional layers (Kim et al. 2023) on the SVHN dataset, a ResNet-18 (He et al. 2016) on the CIFAR-100 dataset, and a DistilBERT (Sanh et al. 2020) model on the 20Newsgroup dataset. To simulate the non-IID setting, we allocate the training data to clients according to the Dirichlet distribution with concentration parameter $\\alpha = 0.5$ (Li et al. 2022). In each round, $N_u = 10$ unlabeled data samples arrive at each client independently and each client selects $N_q = 1$ data sample for label querying. The details of datasets are summarized in Table 1. More implementation details can be"}, {"title": "6.2 Main Results", "content": "We show the model accuracy in different querying rounds on several datasets in Fig. 4. We observe that the proposed LeaDQ algorithm achieves the best accuracy in all the querying rounds. On the SVHN dataset, the naive local querying strategies (i.e., Uncertainty and Coreset) attain weaker performance than other approaches, showcasing the importance of designing specific strategies for FL. On the CIFAR-100 dataset, the baselines have similar performance, while LeaDQ surpasses the best-performance baseline at the later"}, {"title": "6.3 Results With Diverse Scenarios", "content": "Results with various arrived and queried data samples\nWe show the model accuracy with different numbers of arrived samples $N_u$ and queried samples $N_q$ per round in Table 2. We observe from the results that as the ratio of queried data samples $N_q/N_u$ increases, the model achieves higher accuracy with all the data querying strategies. This is because leveraging more labeled data samples on clients effectively improves the model's performance. In most cases, LeaDQ outperforms the baselines due to its advantages in identifying critical samples. Meanwhile, when the number of queried samples is larger ($N_q > 1$), LeaDQ and KAFAL have competitive performances, surpassing other strategies.\nResults with different setups of data heterogeneity In addition, we simulate two types of data heterogeneity, i.e., the distribution-based skew and quantity skew (Li et al. 2022). The results are illustrated in Tables 3 and 4, respec-"}, {"title": "7 Conclusions", "content": "In this paper, we investigate the data querying problem in stream-based federated learning, highlighting the conflict between local data access and global querying objective. We introduce a novel MARL-based algorithm named LeaDQ, which learns local data query policies on distributed clients. LeaDQ promotes cooperative objectives among clients, ultimately leading to improved machine learning model training. Extensive experiments validate the superiority of LeaDQ over state-of-the-art baselines, demonstrating its effectiveness in querying critical data samples and enhancing the quality of the ML model. We note that LeaDQ provides a framework for data querying in FL. Future research could explore the incorporation of more advanced algorithms beyond FedAvg and QMIX to enhance performance further."}, {"title": "A Additional Details for Implementing LeaDQ", "content": "A.1 Pseudo Codes for Policy Training\nWe show the pseudo codes for policy training in LeaDQ in algorithm 2.\nA.2 A General Case for Querying Multiple Data Samples\nEach client selects a subset of $N_u$ unlabeled data samples, denoted by $U_{qry,k}$, to be labeled. In the LeaDQ framework, the querying strategy is adapted so that the clients take an action a where the size of the action, denoted by |a|, equals $N_q$. This action is aimed at maximizing the aggregate of local Q-values, which is given by:\na* = arg max $Q_k(a,o_k;\\theta_k).$ (14)\n$|a|=Nq$\nThe summation in the above equation ensures that the action selected corresponds to the highest total Q-values. Consequently, this results in identifying the most beneficial data samples for labeling according to the given policy.\nB Experimental Details\nWe implement all methods with PyTorch and run all experiments on NVIDIA RTX 3080Ti GPUs.\nB.1 Implementation Details for the Motivating Example\nWe simulate the FL system with 30 clients and evaluate the algorithms on the MNIST dataset with a LeNet with two convolutional layers (LeCun et al. 1998. The training data are allocated to clients according to the Dirichlet distribution with concentration parameter 0.5 (Li et al. 2022). The methods are implemented as follows:\n\\bullet Local Coreset: Each client selects unlabeled data locally according to Eq. 7.\n\\bullet Global Coreset: The server collects all unlabeled data samples from all clients and selects some samples for label querying. It initially selects one sample for each client and iteratively finds samples via a greedy furthest-first traversal conditioned on all labeled examples.\nMetrics The distribution divergence is compute as the Kullback-Leibler (KL) divergence between the distributions of selected unlabeled data on all clients (denoted by $U_{qry}$) and the target data (denoted by $U_{tgt}$), which is given by:\nDiv$(U_{qry}; U_{tgt}) = \\sum_{y\\in Y} P_{qry}(y) log \\frac{P_{qry}(y)}{Q_{tgt}(y)},$ (15)\nwhere $P_{qry}(y)$ is the label distribution of data in $U_{qry}$, and $Q_{tgt}(y)$ is the label distribution of data in $U_{tgt}$.\nOther implementations are same as that of the main experiments, as explained in the next subsection.\nB.2 Implementation Details for Main Experiments\nTraining task We evaluate the algorithms on two image classification tasks, i.e., SVHN (Netzer et al. 2011) and CIFAR-100 (Krizhevsky and Hinton 2009), and one text classification task, i.e., 20Newsgroup (Lang 1995), each of which contains a training set and a test set. In the default setup, the training set is allocated to clients according to the Dirichlet distribution with concentration parameter 0.5. The ML model is evaluated on the test set as the performance metric. Main experiment results are averaged over three random seeds."}, {"title": "C Supplementary Results", "content": "C.1 Results on the Tiny-ImageNet Dataset\nTable 7 shows the model accuracy after 200 querying rounds on the Tiny-ImageNet dataset. The results further verify the benefits and robustness of LeaDQ in achieving better model accuracy.\nC.2 Results with More Clients.\nWe compare the model performance when there are 50 clients in the system. The results in Table 8 show that LeaDQ consistently leads to the best model accuracy, showing its scalability.\nC.3 Hyper-parameter Analysis\nWe study the effect of episode length J on the performance of our proposed LeaDQ method. The following results on the SVHN dataset in Table 9 show that episode length has minimal impact on the final accuracy, highlighting the robustness of LeaDQ to this parameter.\nD Discussions\nDiscussions on privacy in LeaDQ The core idea of FL is to train an ML model from distributed data on clients. (a) By sharing models instead of data, FL preserves the local data privacy of clients. (b) Meanwhile, additional techniques such as secure aggregation further protect the privacy of each client's model. In this work, we study the data querying problem in FL. To preserve the same privacy as in FL, the data cannot be accessed by other clients or the server. Thus, it is hard for clients to select proper samples for labeling based on solely local data. We propose an MARL-based algorithm named LeaDQ to solve this problem. In a nutshell, LeaDQ allows clients to query data in a decentralized manner while maintaining the privacy protection in FL."}]}