{"title": "WHO'S THE MVP?\nA GAME-THEORETIC EVALUATION BENCHMARK FOR\nMODULAR ATTRIBUTION IN LLM AGENTS", "authors": ["Yingxuan Yang", "Bo Huang", "Siyuan Qi", "Chao Feng", "Haoyi Hu", "Yuxuan Zhu", "Jinbo Hu", "Haoran Zhao", "Ziyi He", "Xiao Liu", "Zongyu Wang", "Lin Qiu", "Xuezhi Cao", "Xunliang Cai", "Yong Yu", "Weinan Zhang"], "abstract": "Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating\ncomponents such as planning, reasoning, action execution, and reflection to tackle complex tasks.\nHowever, quantifying the contribution of each module to overall system performance remains a\nsignificant challenge, impeding optimization and interpretability. To address this, we introduce\nCapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in\ncooperative game theory's Shapley Value, which systematically measures the marginal impact of\nindividual modules and their interactions within an agent's architecture. By replacing default modules\nwith test variants across all possible combinations, CapaBench provides a principle method for\nattributing performance contributions. Key contributions include: (1) We are the first to propose a\nShapley Value-based methodology for quantifying the contributions of capabilities in LLM agents;\n(2) Modules with high Shapley Values consistently lead to predictable performance gains when\ncombined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500\nentries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation\nof agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic\nsystem assessment, providing actionable insights for optimizing modular LLM agents and advancing\ntheir deployment in complex, real-world scenarios.", "sections": [{"title": "Introduction", "content": "The rapid advancements in Large Language Models (LLMs) have ushered in a transformative era for artificial intelligence\nagents. These models demonstrate unprecedented capabilities in understanding, generating, and integrating natural\nlanguage across diverse domains [Brown et al., 2020, OpenAI et al., 2024]. However, LLMs still face notable challenges\nas foundational models for supporting AI agents in real-world applications. These include accurately interpreting subtle\ncontextual shifts, effectively integrating with external tools, and ensuring both the accuracy and reliability of outputs.\nTo overcome these challenges, researchers have increasingly adopted modular architectures, decomposing agents into\ndistinct components responsible for planning, reasoning, and action execution. Such modular frameworks not only\nenhance the overall performance but also improve the interpretability and maintainability of the systems. Frameworks\nsuch as ReAct [Yao et al., 2022] and AutoGPT [Tang et al., 2023] exemplify how structured workflows, achieved by\nbreaking down tasks into manageable modules, can lead to more efficient task processing. These modular architectures\nlay the groundwork for systematic evaluations of LLM agents' internal designs and effectiveness in various applications.\nDespite the impressive capabilities of LLM agents, accurately evaluating their performance remains an open challenge.\nTraditional evaluation methods have predominantly focused on task-specific benchmarks and domain-specific datasets.\nFor instance, AgentBench [Liu et al., 2023] assesses agents' abilities through specialized tasks, while ToolBench\n[Guo et al., 2024a] evaluates the effectiveness of LLM agents in leveraging external tools across diverse application"}, {"title": "Related Work", "content": "Recent advances in large language models (LLMs) have catalyzed the development of increasingly sophisticated AI\nagents. LLM agents typically employ modular architectures that decompose tasks into planning, reasoning, and action\nexecution. Early work, such as ReAct [Yao et al., 2022], highlighted the efficacy of explicit reasoning and action\nparadigms. Recent efforts, such as AutoGPT [Tang et al., 2023] pioneered autonomous task execution through iterative\nplanning and reflection. HuggingGPT [Shen et al., 2023] demonstrated advanced tool integration by orchestrating\nmultiple specialized models, while MetaGPT [Hong et al., 2024], introduced hierarchical planning strategies that enable\ndynamic task decomposition and recursive self-improvement. In addition, TRAD [Zhou et al., 2024] further advances\nthe paradigm by introducing thought-level retrieval and aligned decision-making to improve modular efficiency and\nreduce noise. These developments signify a shift from simple instruction-following to complex decision-making.\nBuilding on these works which highlight modular designs, our study systematically evaluates the marginal impact of\nindividual modules using the Shapley Value, uncovering the most suitable combinations of LLM modules for achieving\noptimal performance in different environments."}, {"title": "Agent Benchmark", "content": "The evaluation of LLM agents has evolved considerably, with early approaches primarily emphasizing task-specific\nperformance metrics. AgentBench [Liu et al., 2023] laid the groundwork by evaluating agents across diverse scenarios,\nsuch as web browsing and knowledge graph, highlighting the importance of assessing performance in diverse contexts.\nHowever, these evaluations often focused on task outcomes while overlooking the foundational skills driving these\nresults, making it difficult to analyze the root causes of failures. To address this limitation, MMAU [Yin et al., 2024]\nintroduced a novel benchmark that provides an evaluation of agent capabilities. But by combining capabilities with\npredefined tasks, MMAU risks equating task success with true capability strength, relying on limited problems that may\nnot generalize or capture complex real-world interactions.\nRecent benchmark developments have become increasingly sophisticated. OmniACT [Zhang et al., 2024] introduced\na comprehensive framework for evaluating agents in desktop environments, while AgentQuest [Yang et al., 2024a]\ndeveloped methods for assessing continuous learning and adaptation. These frameworks represent a shift toward\nunderstanding not just what agents can do, but how they handle complex, dynamic scenarios.\nBuilding on this trend, specialized benchmarks have emerged to target domain-specific skills. For example, Char-\nacterEval [Chen et al., 2024] assesses agents' ability to maintain consistent personas, while WorkBench [Liu et al.,\n2024] focuses on workplace scenarios. ToolBench [Guo et al., 2024a] evaluates tool manipulation proficiency, and\nMobile-Bench [Wang et al., 2024] tests performance across mobile platforms. These frameworks reflect the growing\nrecognition that agent evaluation must encompass both general capabilities and domain-specific competencies.\nIn contrast, CapaBench extends beyond task-level evaluations by leveraging the Shapley Value to quantitatively capture\nboth individual module contributions and interaction effects, enabling a more nuanced analysis of how each component\ninfluences overall agent performance."}, {"title": "Benchmark Design", "content": "We introduce the agent framework shown in Figure 2 as the foundation of our benchmark. This framework is specifically\ndesigned to assess LLM agents' abilities in various environments and task scenarios. It follows established agent\nprocesses and features a modular design, which supports both single-turn and multi-turn interactions. This ensures that\nour evaluations are comprehensive and adaptable."}, {"title": "Agent Capability", "content": "Building upon established agent architectures [Yao et al., 2022, Tang et al., 2023, Hong et al., 2024], our framework\nintegrates four fundamental capabilities essential for LLM agents: Planning, Reasoning, Action, and Reflection, as\nillustrated in Figure 2. These capabilities represent the core functionalities widely recognized in current agent systems,\nenabling agents to handle immediate completions and perform complex tasks.\nPlanning module initiates the agent workflow by decomposing complex instructions into structured subtasks, following\nprinciples established in hierarchical planning systems [Brown et al., 2020]. This decomposition enables effective task\nprioritization and resource allocation, particularly crucial for multi-step operations requiring strategic foresight."}, {"title": "Evaluation Methodology", "content": "To evaluate the contribution of individual capability modules within LLM agent architectures, we leverage Shapley Value\n[Hart, 1989] analysis, a principled framework grounded in cooperative game theory. This methodology quantifies the\nmarginal impact of each module on system performance by systematically evaluating all possible module configurations.\nBy capturing both independent contributions and interaction effects among modules, this approach provides a robust\nmechanism for evaluating the modular design of LLM systems, while naturally handling the nonlinear dynamics\ninherent in such architectures."}, {"title": "Shapley Value Framework", "content": "Shapley Value provides a theoretical foundation for fairly allocating the overall perfor-\nmance of a system to its individual components. For a set of N modules, Shapley Value \\(\\phi_i(v)\\) for module i is defined\nas:\n\n\\phi_i(v) = \\sum_{S \\subseteq N\\{i\\}} \\frac{|S|!(|N| - |S| \u2013 1)!}{N!} \u00b7[v(S\\cup \\{i\\}) \u2013 v(S)],\\tag{1}\n\nwhere S denotes any subset of N that excludes module i, and v(S) represents the performance(task success rate) of the\nagent when only the modules in S are active. The term v(SU {i}) \u2013 v(S) quantifies the marginal impact of adding\nmodule i to the subset S, while the weight \\frac{|S|!(|N|-|S|-1)!}{N!} ensures fair averaging across all possible subsets."}, {"title": "Evaluation Flow", "content": "CapaBench systematically evaluates the contributions of four key modules in the agent architecture:\nPlanning (P), Reasoning (R), Action (A), and Reflection (F). As shown in Figure 1, the evaluation involves testing\nall possible combinations of these modules (24 = 16 combinations) by replacing default implementations with test\nvariants provided by the target LLM model. The default \"whiteboard\" modules, implemented using Llama3-8b-instruct,\nserve as a fixed baseline to isolate the performance impact of each test module. Llama3-8b-instruct was chosen as\nthe default model implementation because it is open-source, lightweight, and easy to deploy, making it practical for\nextensive testing. While it possesses basic task completion capabilities, its moderate success rates provide an ideal\nbaseline to observe and quantify the impact of replacing modules with more advanced test models.\nFor each combination, CapaBench computes performance values to quantify the contribution of individual modules and\ntheir interactions. Diverse task benchmarks (B), including multi-step scenarios designed to simulate practical agent\napplications, are used to evaluate the system, providing insights into the optimal module configurations for various\nenvironments."}, {"title": "Capturing Synergistic Effects and Nonlinear Dynamics", "content": "Shapley Value provides a robust framework to quantify both\nthe independent contributions and synergistic interactions among modules in a modular architecture. By systematically\nevaluating all possible subsets S \u2286 N, it inherently captures the nonlinear dynamics and interdependencies between\nmodules. For instance, Planning provides structured outputs for Reasoning, while Reasoning refines these outputs\nto guide Action execution. Tasks often require at least two modules to collaborate, such as Reasoning and Action\nworking together to decompose and solve complex tasks. These collaborative effects are reflected in the marginal\ncontributions v(SU {i}) \u2013 v(S), where v(S) represents the system's performance (e.g., task success rate) with subset\nS. Shapley Value is particularly well-suited for nonlinear dynamics, as it fairly distributes contributions even when\nmodule interactions exhibit synergy or competition. Unlike linear or additive methods, it ensures unbiased attribution\nof both individual and collaborative contributions, making it ideal for evaluating modular LLM agents with complex\ninterdependencies."}, {"title": "Dataset Construction", "content": "Online Shopping Online Shopping tasks are based on the simulated online shopping platform WebShop [Yao et al.,\n2023]. The dataset includes 110 tasks, of which we modified 48 tasks to enhance the diversity and complexity of the\ninstructions. For example, the original instruction \"find me scrubs & body treatments made with tea tree and other\nnatural ingredients\u201d is rewritten as \u201cGiven my upcoming spa weekend, I'm on the lookout for scrubs & body treatments.\nCan you recommend ones specifically made with tea tree and other natural ingredients as I have sensitive skin?\" These\nmodified prompts reflect more natural and contextually rich user queries, challenging the agent to demonstrate reasoning,\npersonalization, and relevance in its recommendations. The reward model and product definitions align with WebShop,\nproviding a consistent evaluation framework for agents' performance in online shopping scenarios.\nNavigation Planning The Navigation Planning task evaluates agents' ability to collaboratively generate travel\nitineraries with a user while adapting to evolving constraints and preferences, inspired by [Lin et al., 2024]. This\ndataset's 250 tasks are designed to reflect a wide range of planning challenges. In our setup, the user provides an initial\nset of three travel requirements sampled from a pool of potential preferences, such as budget limits, preferred activities,\nor group constraints.\nTo simulate real-world planning scenarios where user preferences may evolve, the evaluation process introduces\ndynamic updates. In each iteration, there is a 50% chance that a new preference is sampled from the predefined pool.\nThis new preference will be added to the current instruction set, leading to updated instructions. If no new preference is\nintroduced (also with 50% probability), the agent's current proposal is evaluated directly.\nThe evaluation consists of two components: the first part is based on the precision derived from the experimental results,\nand the second part evaluates the rationality of the planned route, based on how well the proposal aligns with user\npreferences, considering factors such as budget adherence, inclusion of specified activities, and efficient travel distances.\nThis feedback measures the agent's ability to prioritize user needs and adaptively produce actionable travel plans.\nTicket Ordering The Ticket Ordering task, inspired by [Lin et al., 2024], evaluates an agent's ability to determine the\noptimal flight combination based on user-specified constraints. This dataset comprises 150 tasks designed to simulate\neveryday ticket ordering scenarios. In our setup, two users provide their daily calendars along with requirements such\nas the flight price.\nTo mirror real-world ticket ordering, users can choose from a wide array of flights each differing in price, duration,\narrival time, and more-which makes it challenging for agents to offer sound advice."}, {"title": "Main Results", "content": "We conducted a systematic evaluation of nine different models across five primary tasks, revealing significant perfor-\nmance disparities and distinct module contribution patterns. The following sections provide a detailed analysis of key\nfindings in each task domain, supplemented by comprehensive insights derived from the experimental results presented\nin Table 3.\nOnline Shopping Performance In the e-commerce evaluation, model performance exhibited clear hierarchical differ-\nentiation. High-performance models, specifically GLM-4-air (37.50%) and GPT-40-mini (37.43%), significantly\noutperformed the baseline model (Llama3-8B: 26.27%). This improvement is primarily attributed to effective module\nsynergy and optimized action execution. GLM-4-air demonstrated superior performance in the Planning (P: 0.1058)\nand Reasoning (R: 0.0770) modules, underscoring the importance of advanced cognitive abilities in managing complex\nshopping tasks. Additionally, Qwen2.5's notable performance in the Action module (A: 0.1557) highlights the critical\nrole of precise action selection in enhancing task success rates. The reflection capabilities of GPT-40-turbo (F:\n0.0244) further emphasize the significance of dynamic strategy adjustments in interactive scenarios.\nMath Solver Performance The mathematical problem-solving evaluation encompassed both algebra and geometry\nsub-tasks, revealing distinct performance characteristics. In algebra, Qwen2.5 achieved an impressive accuracy\nof 86.8%, marking a 65.2 percentage point improvement over the baseline. This performance is largely due to"}, {"title": "Capturing Synergistic Effects and Nonlinear Dynamics", "content": "The experimental results in Table 2 demonstrate that modules with higher Shapley\nValues consistently lead to improved task performance when combined. For instance, in the \"Online Shopping\" dataset,\nthe optimal combination achieves an accuracy of 43.31%, which is significantly higher compared to the other models,\nindicating the advantage of leveraging high-contribution modules. Similarly, in ATP, the best combination computed\nbased on Shapley Values results in an 86.79% accuracy, showcasing a marked improvement over alternatives. These\nresults demonstrate that identifying and integrating key modules with high Shapley Values enables CapaBench to\nsystematically maximize performance across tasks, validating Shapley Values as a reliable guide for module selection\nand optimization."}, {"title": "Ablation Study", "content": "In this section, we examine how changing the default model in our evaluation framework affects the Shapley\nValue results and the relative ranking of various LLMs. Specifically, we replace our original default model\n(Llama3-8B-instruct) with the model (gpt-3.5-turbo-0613) and re-run the evaluation on the same set of\nseven test LLMs over the Robot Cooperation Task. Our aim is to examine (i) whether our evaluation framework is\nrobust against different baseline capabilities, and (ii) to what extent the relative ranking of the test models is affected by\nthis change."}, {"title": "Analysis", "content": "Based on Table 3, we further enrich our analysis with the following insights:\nCross-Task Model Performance Comparison A high-level comparison of model performance across diverse tasks\nreveals distinct strengths and weaknesses. Notably, Claude-3.5 outperforms other models in most categories,\nshowing particular prowess in formal verification (e.g., Coq, Lean 4, Isabelle) and robot cooperation tasks. This\nadvantage suggests that Claude-3.5 has a robust underlying chain-of-thought reasoning mechanism and effective\nmulti-agent collaboration strategies capabilities essential for tasks that demand precise logical proof structures and\nsynchronized actions. On the other hand, open-source models like Qwen-2.5 and Mistral-8X7B exhibit moderate\ngains in more straightforward domains, such as shopping or basic Algebra, but underperform in cognitive-heavy tasks.\nTheir lag in automatic theorem proving and robot cooperation implies that while these models may be adept at handling"}, {"title": "Module Contribution Patterns", "content": "Our findings highlight that module contributions vary according to task demands,\nreflecting the distinct cognitive processes involved. Specifically:\n\u2022 Tasks with High Cognitive Complexity (e.g., Online Shopping, Robot Cooperation, and OS): Reasoning\nand Planning play pivotal roles. Online shopping requires balancing constraints (e.g., budget and preferences)\nand sequencing decisions effectively. In robot cooperation, Reasoning enables dynamic information updates\nand efficient task distribution among agents. Operation system tasks, involving troubleshooting and resource\nmanagement, rely heavily on real-time problem-solving and feedback interpretation. Across these tasks, robust\nReasoning ensures logical inference and decision-making under uncertainty.\n\u2022 Tasks Requiring Precision (e.g., Math Solvers and ATP): Action is the dominant module. In math solvers,\nparticularly geometry, precise procedural execution, such as applying theorems or constructing diagrams,\noutweighs strategic planning. Similarly, in formal verification tasks (e.g., Coq or Lean), strict adherence to\nsyntactic and semantic correctness is critical. Both scenarios demand meticulous step-by-step actions to ensure\nreliability and prevent errors.\nBy identifying module-specific dependencies, developers can target optimizations, such as enhancing Reasoning for\ndynamic decision-making or refining Action for procedural accuracy, to maximize performance across diverse domains."}, {"title": "Low Reflection Contribution", "content": "We conclude the seemingly low contribution of the Reflection module to overall task\nperformance through two main considerations. First, whether or not the reflection directly translates into a higher\nsuccess rate does not necessarily reflect the true quality or efficacy of the reflection itself. In other words, task success\nalone may not be the best measure of how well the model is \u201cthinking about\u201d its own mistakes. Second, when the model\nreflects on its own errors without extra information or guidance from a more capable model, it may fail to pinpoint the\nactual causes behind its mistakes. As a result, the lack of deeper insights into error sources means reflection often does\nnot generate meaningful improvements in task outcomes. Consequently, while the Reflection module is present, its\npractical impact on success rates remains limited."}, {"title": "Comparative Study", "content": "This experiment investigates whether Shapley Values can accurately capture model-specific\nabilities in core competencies, including planning, reasoning, and action. To this end, we conducted a capability\nevaluation experiment on a subset of 238 questions from successful trajectories in the Algebra dataset, focusing on\ncorrectly completed tasks. Using successful trajectories ensures reliable annotations for Planning, Reasoning, and\nAction modules by providing clear labels. From these trajectories, we extracted full interaction data and split it into\nsingle-step QA samples based on the three core modules. This process generated 2180 single-step samples. The\nreflection module was excluded due to its minimal impact on overall success rates and the insufficient number of\nsuccessful trajectories required to build a reliable dataset for this dimension. For each single-step sample, we asked\nthe tested models to provide responses, which were then evaluated by GPT-01-mini as an independent evaluator. The\nevaluation focused on two aspects for the Planning and Reasoning modules: semantic rationality, assessing whether the\nresponse is clear and comprehensible, and task completion degree, measuring whether the agent effectively completed"}, {"title": "Conclusion and Future Works", "content": "This paper introduced CapaBench, a game-theoretic framework that employs the Shapley Value to rigorously evaluate\nthe contributions of individual modules in LLM agents. By calculating effects among planning, reasoning, action,\nand reflection components, CapaBench enables more precise attribution, guiding targeted optimization and offering\npredictive insights into performance across diverse tasks. Moreover, our approach can potentially extend to LLM-based\nMulti-Agent Systems [Guo et al., 2024b, Yang et al., 2024b, Sun et al., 2024], where each module operates as a\nspecialized sub-agent, paving the way for future explorations in agent coordination, communication, and emergent\nbehaviors. Moving forward, we aim to expand the variety of tasks in CapaBench to improve the robustness and\ntransferability of our evaluation. Additionally, we plan to explore refined, domain-specific evaluation protocols that\nreduce computational overhead without compromising module-level insights. Ultimately, by incorporating these\nenhancements and investigating multi-agent paradigms, we hope to advance both the theoretical underpinnings and\npractical applications of modular LLM-based AI systems."}, {"title": "Online Shopping.", "content": "The Online Shopping dataset is designed to evaluate agents' planning, reasoning, and action capabilities in completing\ne-commerce tasks. The dataset consists of 110 tasks, divided into two parts: white-box tasks (62), which are from\nthe Webshop dataset, and black-box tasks (48), which are expanded using GPT-4 to enhance instruction diversity and\ncomplexity.\nDataset expansion was constructed by modifying instructions from the original dataset. GPT-4 was used to rephrase\ninstructions for greater linguistic diversity, adding context or background such as \u201cNext week is Halloween, and I need\nthemed decorations.\u201d Additionally, parameters were enriched with attributes like size, color, or material to increase task\ncomplexity. For challenging cases, explicit prompts were created to guide planning, for example, \u201cFirst search for\ndesks with wood finishes, then filter by size and price.\"\nA typical instruction in Online Shopping might be: \"I'm looking for a small portable folding desk that is already fully\nassembled; it should have a khaki wood finish, and price lower than 140 dollars, and length bigger than 40 inches.\"\nAgents are evaluated based on their ability to follow optimal trajectories, such as:\n\u2022 Ideal Trajectory 1: Search for all attributes directly (\"desk, wood, folding, khaki, 40 inches, $140\") and\nproceed to the target item.\n\u2022 Ideal Trajectory 2: Broad search (\"desk, wood, folding\"), filter by price, and then refine attributes (color,\nsize)."}, {"title": "Planning Module", "content": "prompt_system_planning =\n\"\"\"\nWelcome to the Online Shopping Challenge! Four LLM agents are working together to do\nweb-shopping tasks step by step (planning -> reasoning -> acting -> reflecting)\nThey are responsible for planning, reasoning, acting, and reflecting respectively.\nYou are the first llm agent, who is a helpful web-shopping guidance assistant in\ncharge of planning.\nYour role is to assist players by generating strategic plans based on the game's\ninstructions.\nHere is how the game is structured:\nEach round, you will be given an instruction that describes the objective need to\nachieve.\nBased on the instruction, you are to generate a clear and brief strategic plan.\nYour plan will be used to guide other agents through the shopping site efficiently.\nIf there is no response click [Buy Now] within 15 actions, the game fails.\nYour Responsibilities:\nAnalyze the original problem and break it into clear, actionable steps.\nEnsure the steps are logically ordered and comprehensive for achieving the goal.\nUse concise language, focusing only on the key actions needed to complete the task\nsuccessfully.\nOUTPUT FORMAT:\nKeep your response concise and structure:\nStrategic Plan: (A list of sequential steps to achieve the objective)\nStep 1:\nStep 2:\nStep 3:\n(Add more steps as necessary, but keep it streamlined and goal-oriented)\nEnclose the plan with three backticks\nFor example:\n\"\"\""}, {"title": "Reasoning Module Prompt", "content": "prompt_system_reasoning =\n\"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step (planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the second LLM agent, who is a helpful web-shopping guidance assistant in\ncharge of reasoning.\nYour reasoning thought will guide the acting agent in making informed decisions. You\nshould generate a thought that will be used as part of the PROMPT for acting agents"}, {"title": "Action Module Prompt", "content": "prompt_system_action =\n\"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step (planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the third LLM agent, who is a helpful web-shopping guidance assistant in\ncharge of acting.\nAs an acting agent, your role is to integrate various elements such as the instruction\nthe current state, historical actions, strategic planning, and current reasoning\nto recommend the best possible action for the next step.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. PLANNING STRATEGY\n3. HISTORICAL ACTIONS\n4. CURRENT REASONING\nYour Role:\nEach round, you will receive updated information, including the current observation,\navailable actions, strategic plan, reasoning, and past actions.\nBased on this information, decide and respond with the best possible action to move\ncloser to completing the objective.\nActions you can perform:\nSearch if a search bar is available.\nClick one of the provided clickable buttons.\nFollow the reasoning closely, but only deviate if you are confident that your choice\nis better.\nImportant Rules:\nYou must click [Buy Now] as soon as you are confident that a suitable match has been\nfound to avoid exceeding the 15-round limit.\nIf no valid action is available, perform no action and wait for the next round.\nEnsure the clicked value exactly matches the available options, including case\nsensitivity and punctuation."}, {"title": "Reflection Module Prompt", "content": "prompt_system_reflection =\n\"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step (planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the fourth llm agent in charge of reflecting. Your role is to reflect on\nwhether there was an error in the previous reasoning and action sequence.\nRemember, your clear and brief reflection will be used as part of the PROMPT for the\nlater agents to guide them to make wise decisions and succeed in the game.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. HISTORICAL REASONINGS\n3. HISTORICAL ACTIONS\nHere is your role:\nAs an LLM Agent, your role is to reflect on the recent outcomes and consider the\nfollowing points:\n1. Identify why the current result is unsatisfactory. Explore factors such as\ninadequate search queries, irrelevant clicks, or repeated useless actions.\n2. Evaluate the effectiveness of past actions and thoughts. Were there missed signals\nor incorrect assumptions?\n3. Propose improvements for the next steps. Suggest specific actions or adjustments in\nsearch strategies, clicking behaviors, or decision-making processes.\n4. Consider the overall goal of achieving successful purchases within the game's\nconstraints. How can future actions better align with this objective?\nUse these as a guide, and generate a plan for the next reasoning and action steps.\nOutline actionable insights and strategies to improve outcomes in the upcoming\nrounds.\nOUTPUT FORMAT:\nYou should carefully examine reasoning history and action history to find out where\nthings may have gone wrong, summarize where they went wrong.\nYour reflection output should provide clear and concise suggestions for the next few\nreasoning and action agents, facilitating informed decision-making and guiding the\nLLM agent towards achieving better performance in subsequent interactions.\nIdeally, it should contain:\nFlaw: One sentence that summarizes key factors causing the unsatisfactory\nresult.\nImprovement: One sentence that includes specifically how to adjust improve\nreasoning and action steps to achieve better outcomes in the future."}, {"title": "Navigation Planning.", "content": "In navigation tasks, agents are required to collaboratively generate and adapt travel itineraries based on evolving user\nconstraints and preferences.Inspired by[Lin et al., 2024], we utilized the framework to employ the evaluation. It evaluates\nthe rationality of the planned route, based on how well the proposal aligns with user preferences.\nWe enhanced the automated data generation method from [Lin et al., 2024] to construct our new dataset.The dataset\nprovides a list of locations and situations, and by randomly generating the conditions of tourist destinations for each\ninstance, it facilitates the next step of decision-making, thereby enabling significant scalability."}, {"title": "Planning Module", "content": "prompt_system_planning =\n\"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\nYour objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user's preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\nYou need to make a plan for the task. Based on the instructions provided, outline a\nstrategic travel plan that includes\nSetting the Ultimate Goal and Identifying Key Factors for Achievement\nSuggested actions for the traveler to take, such as specific search queries or\nattractions/activities to focus on.\nSplit the requirement into 3 combinations and find combinations that can be achieved\nin one place.\nComprehensive Consideration and Selection of One Approach Among Multiple Outcomes\nYour output will as follows. You should answer in one paragraph. Here is your format:\n[think] [planing] HERE IS YOUR PLAN.\nHERE IS AN EXAMPLE\nUser: [message] I'd like to see some live music, eat only takeout from Korean, kosher,\nJapanese or seafood restaurants. vegann options are a plus and I'd like to stop by\nMad Seoul. My budget is $30.I hope the minimal distance.\nYou: [think) [planing] To create a travel plan that aligns with the user's preferences\nwhich has a high score, the key objectives are to select destinations known for\nlive music, diverse takeout options (Korean, kosher, Japanese, seafood, and vegan).\nThe budget constraint of $30 will also guide the choices. I should search for these\nrequests,to see if there are places that meets the requirements. If I get the\nlocation correctly, I will give a proposal. If there is many choice, I will consider\nall proposals and give one proposal that is best.\n\"\"\""}, {"title": "Reasoning Module Prompt", "content": "prompt_system_reasoning =\n\"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\nYour objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user's preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\n[reasoning] Based on the current state of your travel plan and your information gained\nfrom previous action develop your thought process that leads to a specific\nrecommended action or to propose. If you have got many results, please take them\ninto consider.\nYou can only search for at most 3 times in one time.\nYour output will as follows. Here is your format:\n[think] [reasoning) HRER IS YOUR ANALYSE.\nAfter you give a propose you need to prepare for sending a message. Sothat next action,\nyou can send a message. If a reflection is given, you need to reasoning again and\nmessage is not allowed.\n[think] [reasoning] I will send a message to ask how does user think of it\nYou need to answer in one line. More than one line is not allowed.\nHere is an example:\nUser: [message] I'd like to see some live music, eat only takeout from Korean, kosher,\nJapanese or seafood restaurants. Vegan options are a plus and I'd like to stop by\nMad Seoul. My budget is $30.I hope the minimal distance.\nYou: [think) [planing] To create a travel plan that aligns with the user's preferences\nwhich has a high score, the key objectives are to select destinations known for\nlive music, diverse takeout options (Korean, kosher, Japanese, seafood, and vegan).\nThe budget constraint of $30 will also guide the choices. I should search for these"}, {"title": "Action Module Prompt", "content": "prompt_system_action =\n\"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\nYour objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user's preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will"}]}