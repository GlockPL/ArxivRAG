{"title": "EFFICIENTLY INTEGRATE LARGE LANGUAGE MODELS WITH VISUAL PERCEPTION: A SURVEY FROM THE TRAINING PARADIGM PERSPECTIVE", "authors": ["Xiaorui Ma", "Haoran Xie", "S. Joe Qin"], "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs.", "sections": [{"title": "1 Introduction", "content": "The study of vision-language modalities has long been a significant topic, with numerous works dedicated to utilizing transformer-based models to perform multimodal learning [1, 2]. In the era of Large Language Models (LLMs), multimodal-to-text generation tasks have experienced a paradigm shift from Vision-Language Pretrained Models (VLPMs) [3, 4, 5] to integrating LLMs with vision modalities [6, 7, 8, 9]. This shift is driven by the advantages of LLMs in terms of adaptability and reasoning ability. VLPMs require per-task fine-tuning to transfer to downstream tasks, while LLMs have strong zero-shot or few-shot adaptation abilities [6], saving the resources needed for per-task tuning. In addition, although VLPMs have visual perception abilities, enabling them to identify and caption objects in an image, they lack reasoning capabilities [10]. In contrast, LLMs can leverage their pretrained knowledge to reason with visual information [11, 12, 13], offering a deeper understanding of images. While LLMs have these advantages, leveraging off-the-shelf LLMs for VLPM is challenging due to their integrated architecture [7], where the vision encoder and text encoder are constituted as a single model. In contrast, adding a vision encoder to an LLM is more straightforward, requiring a Modality Integrator (MI) to connect the two models. The resulting model is named as Vision Large Language Models (VLLMs), and the architecture is shown in Figure 1. As LLMs scale, computational resource demands increase, making parameter efficiency critical in building VLLMs [14, 15]. This survey examines the Parameter-Efficient Adaptation (PEA) techniques for incorporating visual modalities into LLMs from the training paradigm perspective. The training paradigms are categorized into three types: Single-stage Tuning, Two-stage Tuning, and Direct Adaptation. The categorization is driven by the fact that each paradigm has distinct motivations for efficiency, and different methods are employed to achieve it.\nVLLMs adopting Single-stage Tuning first appeared in the VLPM era. From the parameter efficiency perspective, pretraining a VLPM requires multiple feedforward processes due to the simultaneous use of various learning objectives [1], resulting in the trainable parameters increasing multiplicatively as the model size increases. By adding LLMs with visual perception through a Single-stage Tuning paradigm, in most cases, only an MI is trained to bridge two modalities in one training process [7, 16, 17, 18, 19, 20]. Compared to LLM's scale, this is also a parameter-efficient strategy. For example, BLIP-2 [7] utilizes Flan-T5-XXL with 11 billion parameters, while MI accounts for 0.89% of the whole model. For downstream tasks generalization, unlike VLPMs that adopt end-to-end per-task fine-tuning, zero-shot, and few-shot learning are adopted in Single-stage Tuning to leverage the pretrained knowledge in LLMs.\nHowever, Single-stage Tuning cannot fully unlock the generalization potential and instruction-following capabilities of LLMs. For better zero-shot transfer to unseen tasks and user intentions understanding, Two-stage Tuning introduces an additional training phase, instruction tuning, that involves fully training LLMs in the second stage [8]. Due to the large size of LLMs, there are three methods to reduce trainable parameters: not training LLM but only the MI in the second stage [7, 21, 22, 23, 24, 16], training the MI while incorporating reparameterization modules into LLMs [8, 25, 26, 27, 28] such as LoRA [29], and utilizing a smaller LLMs [30, 31, 32].\nIn contrast to Two-stage Tuning aiming to improve VL performance, Direct Adaptation primarily focuses on consuming the least resources to transfer LLM to the VL domain. It skips the pretraining stage and directly finetunes the MI on downstream tasks mainly through multi-task learning without updating LLMs [33, 34, 35, 36, 37, 38, 39, 40, 41]. The design of MI achieves an excellent balance between parameter efficiency and modality fusion performance.\nExisting surveys [14, 2], however, mainly focus on the latest VLLMs adopting Two-stage Tuning paradigms. In this survey, the database used is Google Scholar [42], and the keywords are Multimodal, Large Language model, vision-language model, and parameter-efficient learning. The time period is from November 2021 to November 2024. The search results are first screened to match the idea that the model integrates vision modality into LLMs and considers parameter efficiency. Then, more papers are included from the related work of the screened literature. Finally, the quality of the papers is assessed. The inclusion criteria are that the paper needs to be published in the conference ranking A and B in the CCF Recommended List of International Conferences and Periodicals [43], or ICLR. If it does not satisfy the former requirement or it's an Arxiv paper, the annual citation should exceed 15 times as of November 1st, 2024. Based on this selection process, this review surveys 34 papers on this topic, which are shown in Figure 2. The details of the reviewed models are presented in the Table 1.\nIn addition, this review is arranged based on the steps to integrate an LLM with visual perception:"}, {"title": "2 Large Language Model", "content": "Large Language Models (MML) are mainly transformer-based [50] models with encoder [51, 52], encoder-decoder [53, 54] and decoder-only [55, 56, 57, 58, 59, 26, 60] architectures. LLMs utilized in the reviewed paper are summarized in Table 2. In the context of the three training paradigms discussed in this study, the selection of LLMs varies distinctively. Overall, the LLM release time is the key factor for the choice. For the Single-stage Tuning paradigm, the GPT series is typically adopted as this paradigm is early. In the Two-Stage Tuning paradigm, 7B and 13B LLaMA and Vicuna models are generally utilized. Exceptions are that MobileVLM [30, 31], and VLMamba [61] leverage smaller-scale LLMs to enhance efficiency. In the Direct Adaptation paradigm, T5 [53] and Bart [62] are employed as the benchmark LLM due to their manageable sizes [33, 38, 40, 41, 36]. Additionally, there is a notable trend towards adapting LLaMA models in this paradigm [41, 36, 37, 9].\nAs the preliminaries, the transformer architecture and existing PEA methods for LLMs are introduced in this section."}, {"title": "2.1 Transformer", "content": "Transformer [50] comprises an encoder and a decoder, each of which includes L transformer blocks. The basic modules for a transformer block are Multi-Head Attention (MHA) and the Feed-forward Network (FFN). After each module, there is a Layer Normalization (LN) and a residual connection. For a self-attended MHA, the input embedding X is linearly transformed h times and activated by the attention mechanism to get the head. The concatenated heads will then be projected to the model dimension d.\n$Q = XW^Q, K = XW^K, V = XW^V,$\n$Attention(Q, K, V) = softmax (\\frac{Q K^T}{\\sqrt{D}}) V,$\n$MultiHead(Q, K, V) = [head_1 ; . . . ; head_h] \\cdot W^O,$\n$where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V),$\nand the parameter size of the linear layers are $W_i^Q \\in R^{d \\times d_q}$, $W_i^K \\in R^{d \\times d_k}$, $W_i^V \\in R^{d \\times d_v}$ $W^O \\in R^{h d_v \\times d}$. The mathematical notations are summarized in the Table 3. In cross-attention, the queries are derived from the target sequence, while the keys and values come from a context sequence, which is different from self-attention, where all values are from the same input. The FFN module is an MLP composed of two linear layers and activated by RELU.\n$FFN(X) = \\sigma(XW_1)W_2,$\nwhere $\\sigma(x) = max(0,x)$.\nBased on the transformer architecture, LLaMA 1 [55] improves LM to RMSNorm, changing ReLU activation into SwiGLU activation [69], further enhancing the model's nonlinear expression. LLaMA 2 [68] replaces MHA with Grouped-Query Attention, where KV projections are shared across groups of heads instead of all heads."}, {"title": "2.2 Parameter-Efficient Adaptation", "content": "PEA is a solution that transfers LLMs to new tasks by updating a small number of parameters [15]. Typically, there are three categories of methods: Prompt-based Tuning [70, 71], Adapter-based Tuning [72], LoRA-based Tuning [29, 73, 74]. In the multimodal context, these ideas are inherited in a way that the visual feature is fed into the LLMs in the form of visual prompts or prefixes, following which the visual modality is fused with textual modality by adapters or LORA."}, {"title": "2.2.1 Prompt-based Tuning", "content": "The prompt-based method is to add trainable embeddings to the LLM input. The prompt-based method can be divided into prefix tuning and soft prompt tuning, which differs in that the soft prompt tuning only adds trainable vectors at the very first input. In contrast, the prefix tuning adds trainable queries to the input for each transformer layer. In the multimodal context, soft prompts and prefixes are the forms of visual embedding input into the LLM.\nPrefix Tuning The prefix tuning [70] adds a trainable prefix to the input layer and each transformer layer. As the subsequent generation is conditioned on the prefix, it can serve as a learnable instruction for different downstream tasks."}, {"title": "Soft Prompt Tuning", "content": "Lester et al. [71] simplifies the prefix tuning idea to add a trainable prompt only to the input of the language model as a learnable instruction to guide the model to perform different downstream tasks. The model is now maximizing $Pr_{\\theta;\\Phi}(Y | [P; X])$, where $\\theta$ is the soft prompt parameter and $\\Phi$ is the frozen LLM's parameter, and the shape of the input matrix is that $[P; X] \\in \\mathbb{R}^{(p+n)\\times d}$.\nIn addition to the textual prompt tuning, Visual Prompt Tuning (VPT) [75] proposes the visual version of prompt tuning (VPT- shallow) and prefix tuning (VPT- deep), which is to add learnable vector in the input layer and in each transformer layer respectively. For both scenarios, the input to the first transformer encoder layer is $[X_{[CLS]}; P; X_v]$, where the soft prompt is added after the $[CLS]$ token. For multimodality context, PromptFuse [35] directly adds a soft prompt at the beginning of concatenated visual embedding $X_v$ and text embedding $X_t$. The input can be formulated as $[P; X_v; X_t]$. The idea of integrating vision modality into the language model by inputting image-conditioned soft prompt originates from Frozen [6]. To avoid hurting the LM's generalization ability by a relatively small amount of multimodal training data, the LM is kept frozen, and the vision encoder and a linear layer are trainable to align the two modalities. In this way, Frozen only trains 0.56% of the total model parameters. The difference between soft prompt in LLM and in VLLMis that the former learns the difference between downstream tasks, while the visual soft prompt represents not the difference, but the image-conditioned information."}, {"title": "2.2.2 Adapter-based Tuning", "content": "Adapter-based tuning [72] is to insert a trainable parameter-efficient module into the transformer architecture. In NLP, the architecture of adapters is usually the bottleneck structure [72], while different works propose different inserting positions [76, 77] and training strategies [78]. It projects down and up the output matrices over the d dimension, which can be formulated as:\n$X = (ReLU(XW_{down}))W_{up} + X,$\n$W_{down} \\in \\mathbb{R}^{d \\times r}, W_{up} \\in \\mathbb{R}^{r \\times d}, where r << d.$\nIn a multimodal context, the In-block Modality Integrator inherits the idea of adding efficient trainable modules into LLM, but there are various structures in addition to the bottleneck structure. More details are discussed in Section 4.2."}, {"title": "2.2.3 LoRA-based Tuning", "content": "LoRA-based tuning is utilized in VLLMs involving updating the LLMs in the second training stage. LoRA [29] updates the transformer weights by decomposing the changing weights to two low-rank matrices, which can be formulated as:\n$\\Delta W = B A, W = W_o + \\Delta W,$\nwhere $B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}, r << min(d,k)$. During training, only parameters in the low-rank matrices B and A are updated, keeping the pre-trained parameters $W_o$ frozen. A set of parameters in B and A can be stored for each downstream task. QLoRA [73] advances the LoRA approach by incorporating weight quantization for the LORA adapters, reducing them to lower precision. This enhancement significantly decreases both memory usage and storage needs. Decomposed Rank Adaptation (DoRA) [74] further refines the process of model fine-tuning by decomposing the pretrained weights into two components: magnitude and direction. By leveraging LoRA to fine-tune the directional component efficiently, DoRA maintains parameter efficiency while simultaneously avoiding additional inference latency."}, {"title": "2.3 Learning Paradigm", "content": "The learning paradigms in VLLMs are adapted from LLMs, including Multi-task learning (MTL), Instruction Tuning, and Reinforcement learning (RL). This section outlines their use in LLMs and briefly covers their adaptation to multimodal settings. The learning paradigms of each model are summarized in Table 1."}, {"title": "2.3.1 Multi-task Learning", "content": "MTL refers to training a model to tackle multiple tasks concurrently during a single training phase [79], which utilizes common knowledge across multiple related tasks for better generalization performance [80]. Its complementary approache is per-task learning. MTL has become a key approach in NLP, demonstrating a wide range of applications such as information extraction, natural language understanding, and text generation [81]. In the context of LLMs, MTL has evolved to require adjustment of task-specific weights [82]. In the multimodal domain, task weights are typically determined by the size of the data for each task [33]. The corresponding loss function can be expressed as:\n$\\mathcal{L}(D; \\theta) = \\frac{1}{|D|} \\sum_{(I,T,Y) \\in D} l(I,T, Y; \\theta),$"}, {"title": "2.3.2 Instruction Tuning", "content": "VLLMs are designed to enable effective communication with humans, and instruction tuning equips VLLMs with the ability to understand user intentions and respond to commands[83]. Studies have shown that fine-tuning LLMs with instruction data significantly enhances their zero-shot performance on unseen tasks[83]. The instruction data is built by organizing original structured datasets in different ways connected by natural-language instructions. The instruction data with multimodal information is first proposed by LLaVA [8]."}, {"title": "2.3.3 Reinforcement Learning", "content": "In the LLM context, RL is a learning paradigm where an LLM learns to generate human-preferred outputs by setting the goal of maximizing rewards obtained from human feedback [84], AI feedback [85], or other reward systems [86]. During learning, Proximal Policy Optimization (PPO) [87] is a widely used RL loss function. It limits drastic policy updates through a clipping mechanism, while maximizing the cumulative reward. The standard PPO objective is defined as:\n$\\mathcal{L}^{PPO}(\\theta) = \\mathbb{E}_t [min (r_t (\\theta) A_t, clip(r_t (\\theta), 1 - \\epsilon, 1 + \\epsilon)A_t)],$\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)},$\nwhere $r_t (\\theta)$ is the ratio of the new and old policy probabilities, $A_t$ is the advantage function that estimates how much better an action is compared to the expected return, and $\\epsilon$ is a hyperparameter controlling the clipping range. The clipping function ensures that policy updates remain within a constrained range to prevent excessively large updates that destabilize training.\nReinforcement Learning with Human Feedback (RLHF) has become crucial in aligning LLMs with human preferences and ethical considerations [84]. The RLHF pipeline generally consists of three stages. First, in Supervised Fine-tuning (SFT), the LLM is initially tuned by instructions. Second, in Reward Model Training, a separate reward model is trained using human preference on multiple model outputs, ranking them based on quality. Third, the LLM is fine-tuned using the PPO algorithm, backpropagating the LLM parameter. In this setting, the model needs to optimize responses based on users' overall preferences rather than every detail. Therefore, lower computational costs are consumed because it updates the model by utilizing coarse-grained feedback, such as paragraph-level ranking or overall preferences, avoiding fine-grained backpropagation for each token [88]. This approach has been pivotal in enhancing LLMs like ChatGPT [84]. In the multi-modal domain, models like LLaVA-RLHF [46] and RLAIF [48] adopt a similar training pipeline. Recently, DeepSeek-R1 [86] has improved efficiency by reducing the number of training examples required in SFT and replacing reward model training through the application of a rule-based reward system. In the multi-modal context, ESPER [45] reduces training costs by leveraging unpaired image data and using CLIP similarity as a reward signal."}, {"title": "3 Vision Encoder", "content": "To add the visual modality, a pre-trained vision encoder is utilized to extract visual embedding $X_v$ from the input image I. The extracted $X_v$ will be further transformed to feed into LLM. The vision encoders used in the reviewed literature are summarized in Table 4. Overall, CLIP ViT L/14 [89] is the most commonly utilized vision encoder, and there is no clear preference in terms of vision encoders for the three training paradigms."}, {"title": "3.1 Architecture", "content": "There are two architectures: Vision Transformer (ViT) [90] and Residual Network (ResNet) [96], between which ViT is more frequently employed. The core idea of ViT [90] is to treat an image as a sequence of patches and regard them as tokens, similar to words in a sentence. Assuming that an image $I \\in \\mathbb{R}^{h\\times w\\times c}$, is inputted into the vision encoder, it will first be divided into N patches, where each patch $I_p \\in \\mathbb{R}^{a\\times a\\times c}$ and $N = \\frac{h}{a} \\cdot \\frac{w}{a}$. Each patch is flattened into a vector, linearly transformed and added with positional encoding, and then encoded by the transformer encoder, which is described in Sec 2.1.\nResNet [96] is a Convolutional Neural Network (CNN). It is composed of a series of residual blocks, with each block consisting of multiple convolutional layers with skip connections. For a residual block with one convolution layer, the first residual block can be formulated as:\n$X = ResBlock(I) = ReLU(W_{BatchNorm} (IW_{Conv}) + I).$"}, {"title": "3.2 Pretrained Modality", "content": "From the modality perspective, both uni-modal and multimodal vision encoders are utilized. Uni-modal vision encoder refers to encoders only pre-trained by images, while multimodal vision encoder is the vision encoder of the CLIP model [89]. CLIP is a VLPM pre-trained with image-text pairs and contrastive loss, which pushes similar text and visual representations closer while pushing the negative samples further. Most VLLMs adopt a multimodal vision encoder. Merullo et al. [97] proves that the more language supervision involved pertaining to the image encoder, the better the performance of language vision tasks. Out of data efficiency considerations, eP-ALM [34] choose a uni-modal vision encoder to avoid using multimodal encoders pretrained on huge datasets."}, {"title": "4 Modality Integrator", "content": "The modality integrator is categorized into Out-of-block Integrators and In-block Integrators, where the \"block\" refers to the LLM. The Out-of-block Integrators, as a basic component of VLLM, align the visual features extracted by vision encoders with the input of LLMs. The In-block Integrators are modules inserted into the LLM architecture, which change the computational graph of LLMs and fuse the multimodal information. The structure of the Modality Integrator is crucial for efficiently integrating the vision modality into the LLM, as it directly impacts the model's ability to process and understand multimodal information and the trainable parameter scale. The taxonomy of MI is shown in Figure 3."}, {"title": "4.1 Out-of-block Integrator", "content": "Commonly, the Out-of-block Integrator is the external connector between the vision encoder and LLM, transforming the visual features over length or dimension. Based on the architecture, the Out-of-block Integrator is classified into an Attention-based Abstractor, Dense Projector, Convolution-based Abstractor, and VSS-based Abstractor."}, {"title": "4.1.1 Attention-based Abstractor", "content": "Resampler The Resampler bridges the vision encoder and LLM by inputting visual embedding $X_v$ and fixed-length learnable prompts $P_v$ into L layers self-attention blocks and outputting the $P_v$, carrying visual information.\nIn the first transformer block of the Resampler:\n$h_1 = Block_1 ([X_v; P'])$,$\n$Q = P_v W^Q, K = [X_v; P'] W^K, V = [X_v; P'] W^V,$\n$h_1 = Attention(Q, K, V) + P',$\n$h_1 = h_1 W_{FFN} + h_1.$\nIn the last transformer layer,\n$P_v = Block_{L_1} (h_{L_1 - 1}),$\nwhere $P' \\in \\mathbb{R}^{p \\times d_v}, P_v \\in \\mathbb{R}^{p \\times d_t}$.\nClipCap [17] and Flamingo [16] utilize the Resampler as a prefix former, meaning that $P_v$ is fed into each transformer block of the language model. Whereas Meta-Mapper [19] and Qwen-VL [26] regard it as a soft prompt former, $P_v$ is prepended to the textual input for the language generator. In Meta-Mapper, the resampler is self-attended, where the input of Q, K, V are all $[X_v; P_v]$. mPLUG-Owl2 forms the Resampler as a self-attention layer with a SwiGLU activation function [69].\nThere are two advantages of the Resampler. First, it shortens the length of visual input to the language model and keeps the input length constant, irrespective of the variable length of the original visual embedding [16]. This enables flexibility in the number of images, which is significant in video input. Second, the transformer architecture is more expressive than the linear projection [17], which can capture more representative visual information.\nHowever, Cha et al. [47] argue that self-attention-based Resampler loses visual information because it puts more attention weight on the major object of an image while ignoring insignificant objects. To strengthen the spatial context-capturing capability of the Attention-based Abstractor, Deformable attention [98] is utilized in the D-Abstractor, considering the reference point and learnable offset.\nAnother drawback is the parameter inefficiency of attention-based structures. To save parameters, MAPL [18] adds down projection layers before the Resampler to reduce the input dimension of the visual embedding $X_v$ and up-project the output $P_v$ to expand its dimension, thus causing a considerable reduction in parameter consumption. The process can be formulated as:\n$X_{input} = [X_v W_{down}; P_v], X_{output} = P_v W_{up},$\n$W_{down} \\in \\mathbb{R}^{d_v \\times r}, W_{up} \\in \\mathbb{R}^{r \\times d_v}, where r < d_v.$\nQ-Former Unlike the Resampler, the Q-former proposed by BLIP-2 [7] involves visual and textual input. Each layer of the Q-former is composed of a self-attention module shared across the learned queries $P_{tv}$ and the input text $X_t$, a cross-attention layer between image embedding $X_v$ and the soft prompt $P_{tv}$, and two feed-forward layers for two modalities separately. The feed-forward process is formulated as below."}, {"title": "4.1.2 Dense Projector", "content": "A Dense Projector is either a single linear layer or a two-layer MLP, which can be formulated as:\n$P_v = X_v W,$\n$P_v = \\sigma(X_v W_1)W_2.$\nIt has been proved that a trainable linear projector is capable of translating visual semantic information to an LM-understandable language and can gain comparable performance with end-to-end trained VLMs [97]. FROMAGE [20] uses linear layers to form visual soft prompts for the image captioning tasks and form both textual and visual soft prompts for the ITM tasks. LLaVA [8] uses an MLP to connect frozen LLM and vision encoder. MiniGPT-4 [21] adds a trainable linear layer to connect the frozen vision encoder, Q-former, and LLM, achieving high parameter efficiency.\nHowever, Dense Projector lacks the flexibility to adjust the length of visual representation. To overcome this, the projector output in MemVP [36] is directly fed into the FFN of the LLM transformer block without occupying the input tokens.\nIn addition to the standalone use of Dense Projectors, its combination with In-block Integrators is frequently employed to achieve deeper modality interaction [33, 22, 37, 44, 23, 40]."}, {"title": "4.1.3 Convolution-based Abstractor", "content": "Cha et al. [47] spots the limitation of the current Dense Projectors and Attention-based Abstractors. The linear mapper is good at retaining complete visual context while having no flexibility in controlling the output number of visual tokens, which affects the inference efficiency of the MLLM. The attention-based integrator can control the visual token number, while the attention mechanism extracts only the representative subject in the image, leading to visual information loss. To retain both properties, it proposes a C-Abstractor, the architecture of which includes two modules H and Q, which are both composed of N ResNet blocks [99]. They are also connected by an adaptive average pooling layer. The architecture can be formulated as:\n$H_1 = ResBlock_1(X_v) = ReLU(W_{BatchNorm} (X_v \\cdot W_{Conv})),$\n$H_N = ResBlock_L(H_{N-1}),$\n$P = AdaptiveAvgPool(H_N),$\n$Q_1 = ResBlock_{N+1}(P) = ReLU(W_{BatchNorm} (P \\cdot W_{Conv})),$\n$Q_N = ResBlock_{2N}(Q_{N-1}).$\nMobileVLM series [30, 31] efficiently project the visual embedding $X_v$ through depthwise-convolution $W_{DepthWise}$ and average pooling $AvgPool_{2\\times 2}$. For the MobileVLM v1, the Out-of-block Integrator can be formulated as:\n$\\begin{aligned} &\\hat{P}''' = W_{PatchWise} (GELU(W_{PatchWise}(X_v))), \\\\&\\hat{P}'' = W_{LayerNorm} [W_{DepthWise} (W_{LayerNorm} (W_{DepthWise}(\\hat{P}'')))] + \\hat{P}'', \\\\&\\hat{P}' = W_{LayerNorm} [W_{DepthWise} (W_{LayerNorm} (W_{DepthWise}(\\hat{P}'')))] + \\hat{P}'', \\end{aligned} $\nwhere GELU refers to the non-linear activation function, the input and output sizes are $X_v \\in \\mathbb{R}^{n_v \\times d_v}, P_v \\in \\mathbb{R}^{(\\frac{n_v}{4}) \\times d_t}$.\nMobileVLM v2 adds a Positional Encoding Generator [100] and a residual connection, enabling the architecture to replace one Depth Convolutional layer to average pooling, which reduces over 99% parameters compared to the v1 projector.\n$\\begin{aligned} &\\hat{P}''' = W_{PatchWise} (GELU(W_{PatchWise}(X_v))), \\\\&\\hat{P}'' = AvgPool_{2\\times 2}(\\hat{P}'''), \\\\&\\hat{P}' = W_{DepthWise} (\\hat{P}'') + \\hat{P}'', \\end{aligned} $\nThe input and output sizes are $X \\in \\mathbb{R}^{n_v \\times d_v}, P_v \\in \\mathbb{R}^{\\frac{n_v}{k^2} \\times d_t}$, where k stands for the average kernel size."}, {"title": "4.1.4 Other", "content": "VL-Mamba [32] proposes a Vision Selective Scan (VSS) mechanism to capture richer information from the non-causal visual data without increasing parameters. It concatenates the feedforward, backward, horizontal, and vertical scan of image patches and uses an MLP layer or linear layer to process them."}, {"title": "4.2 In-block Integrator", "content": "The In-block Integrator here refers to tunable modules inserted into transformer blocks of LLMs. By changing the computational graph of the LLM, the In-block Integrator further fuses the two modalities or controls the degree of the introduced visual information. Commonly, In-block Integrators are adopted together with the external connector. Based on the architecture, the In-block Integrator is classified into Degree-Adaptive Prefix, Bottleneck Adapter, Attention-based Adapter, and Unimodal Linear Adapter."}, {"title": "4.2.1 Bottleneck Adapter", "content": "As described in 2.2.2, the Bottleneck Adapter [72] is a typical structure for parameter efficiency. Sung et al. [33] find that in the multimodal context, the adapter carries the information of the introduced modality instead of task-specific knowledge.\nMAGMA [44] first attempts to utilize both a Dense Projector and a Bottleneck Adapter. VL-PET [40] proposes four adapter architectures of three parameter sizes. In addition to typical bottleneck architecture, it also adopts a down-projection layer $W_{down} \\in \\mathbb{R}^{d \\times 1}$ and copies projected embeddings across the dimension N times to expand to $W_{down} \\in \\mathbb{R}^{N \\times d}$. In this way, only the parameter $W_{down}$ is tunable. To deal with mixed modality input, LaVIN [41] proposes a modality classifier to shift between single-modal and multimodal processing adapters automatically. To reduce the parameter, the two adapters share down-sampling projector weights during finetuning."}, {"title": "4.2.2 Attention-based Adapter", "content": "The base structure of the Attention-based Adapter is the transformer block as described in 2.1. In Flamingo [16", "as": "n$Q = X_t W^Q, K = P_v W^K, V = P_v W^V,$\n$h = TANH(Attention(Q, K, V)) + X_t,$\n$h_i = TANH(h W_{FFN}) + h_i,$\nwhere $TANH(x) = (e^x - e^{-x})/(e^x + e^{-x})$.\nQaP [39"}]}