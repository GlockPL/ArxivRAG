{"title": "LEVEL OF AGREEMENT BETWEEN EMOTIONS GENERATED BY ARTIFICIAL INTELLIGENCE AND HUMAN EVALUATION: A METHODOLOGICAL PROPOSAL", "authors": ["Miguel Carrasco", "Sonia Navajas-Torrente", "C\u00e9sar Gonz\u00e1lez-Mart\u00edn", "Ra\u00fal Dastres"], "abstract": "Images are capable of conveying emotions, but emotional experience is highly subjective. Advances in artificial intelligence have enabled the generation of images based on emotional descriptions. However, the level of agreement between the generative images and human emotional responses has not yet been evaluated. To address this, 20 artistic landscapes were generated using StyleGAN2-ADA. Four variants evoking positive emotions (contentment, amusement) and negative emotions (fear, sadness) were created for each image, resulting in 80 pictures. An online questionnaire was designed using this material, in which 61 observers classified the generated images. Statistical analyses were performed on the collected data to determine the level of agreement among participants, between the observers' responses, and the AI's generated emotions. A generally good level of agreement was found, with better results for negative emotions. However, the study confirms the subjectivity inherent in emotional evaluation.", "sections": [{"title": "1 Introduction", "content": "An image serves as a means of communication, conveying a message capable of evoking emotions based on the intention behind its creation Lyu et al. [2021]. To ensure the accurate interpretation of the message by the observer, it is essential to implement a well-designed visual strategy. This strategy serves as a channel to elicit both conscious and unconscious emotional reactions, which manifest physiologically Li et al. [2021a] and psychologically Hess et al. [2016], Lin and Li [2023], Sharma and Bhattacharyya [2021]. However, one of the main challenges in studying emotions is the subjective nature of emotional responses to experiences, which can vary significantly between individualsZhao et al. [2022a]. Therefore, reaching a significant agreement between individuals is complex Eser and Aksu [2022], and even more so between generative artificial intelligence and humans.\nIn addition to subjectivity, other factors affect the correct experience that produces the emotional response, such as the observer's socio-cultural context Ali et al. [2017], their experience Joshi et al. [2011], Lim [2016], Redies et al."}, {"title": "2 Background", "content": "Image generation using computational techniques has experienced significant advancements in recent years. Traditional methods, such as rule-based image processing and image synthesis techniques, have evolved into more sophisticated approaches that rely on machine learning and convolutional neural networks. This transformation is largely attributed to the rapid progress in AI, driven by the continuous generation of large-scale data. Consequently, this advancement has led to the development of considerably more accurate and reliable AI models capable of generating images, that are practically indistinguishable from authentic photographs or paintings.\nTo examine and understand the computational techniques used in image generation, this section focuses on the current state of these techniques, with particular emphasis on generating artistic images. The analysis will be conducted through a comprehensive review of scientific and technical literature, ranging from traditional methods to the most innovative approaches based on machine learning and neural networks."}, {"title": "2.1 Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN)", "content": "Recurrent Neural Networks (RNN) are one of the types of networks that have stood out in image generation. This type of network has proven useful for completing images from a section of them. The model presented by Google DeepMind in 2016, called PixelRNN van den Oord et al. [2016a], manages to understand the generality of pixel interdependence, being able to predict missing pixels in an image by receiving only a part of it. This type of network has also been used for generating images from natural language text descriptions Mansimov et al. [2015]. This study proposes an attention-based approach, where the model iteratively draws while focusing on the key words of the given description. The results obtained achieve the generation of higher resolution images than those obtained with other approaches, and generate images with a novel scene composition.\nA type of neural network perhaps more widely used than Recurrent Networks are Convolutional Neural Networks or CNNs. Using this architecture, it has been possible to generate three-dimensional images of objects from different perspectives, as in the case of the model presented by Dosovitskiy et al. [2015], where by training convolutional networks, they managed to generate images of chairs from different perspectives. Another example of the use of Convolutional Networks is seen in a study conducted by Google DeepMind van den Oord et al. [2016b], where conditional image generation is explored using convolutional networks through PixelCNN. This model is capable of generating a variety of portraits of the same person using different facial expressions, poses, and lighting conditions. Its results are on par with PixelRNN, but it achieves this at a much lower computational cost.\nThe use of RNNs and CNNs is not mutually exclusive. For instance, a study on a Recurrent Convolutional Encoder-Decoder architecture Yang et al. [2015] demonstrates this integration. In this approach, convolutional networks handle both encoding and decoding, while a recurrent network manages object rotation. This combined strategy effectively synthesizes unseen versions of three-dimensional objects, enabling the generation of images of faces or chairs from various angles."}, {"title": "2.2 Variational Auto-encoders (VAEs)", "content": "The architecture known as Variational Auto-encoders has great utility as a generative model. Numerous studies have demonstrated the use of this architecture for image generation, an example of this is the so-called Deep Recurrent Attentive Writer or DRAW Gregor et al. [2015]. This model uses a neural network that combines a spatial attention mechanism, mimicking the way human eyes move to focus on objects, with a self-encoding framework that allows the"}, {"title": "2.3 Generative Adversarial Networks (GANS)", "content": "Generative models have a long history. However, it will not be until the development of Deep Learning, that models will achieve significant advances Cao et al. [2023]. Introduced by Goodfellow et al. [2014], generative adversarial networks (GANs) have achieved important results in image processing and have attracted interest from the academic and industrial worlds in various fields of research and applications Alqahtani et al. [2021], Wang et al. [2020]. The most relevant variants for image generation in GANs are conditional (cGANs), deep convolutional (DCGANs), and recurrent adversarial networks Shahriar [2022].\nAs proposed by Mirza and Osindero [2014], cGANs allow the generation of images conditioned on an additional input, which could be a class label or a reference image. Over the years, new algorithms based on projections Miyato and Koyama [2018] have emerged, considerably improving the performance of trained generators. Odena et al. [2017] proposed a variant of GANs, called Auxiliary Classifier GANs (AC-GANs). In this new variant, each generated sample has its corresponding class label in addition to the input noise, which is used together to generate an image. The discriminator gives both inputs a probability distribution, which means that the objective function has two parts: the log probability that the source is correct, and the log probability that the class is correct. AC-GANs achieve excellent results compared with traditional cGANs.\nThe ability to condition GANs on a second input opened the door to countless possibilities for this architecture, from something as basic as training the same model to generate cats and dogs, to something that seems futuristic as generating an image from a natural-language text description. An example is the generation of more realistic images from sketches Kuriakose et al. [2020], Liu et al. [2020, 2018b], Philip and Jong [2017].\nMeanwhile, Radford et al. [2015] presented Deep Convolutional GANs (DCGANs), a class of GANs that introduce upscaling convolutional layers between the input and output images of the generator, as well as using convolutional networks in the discriminator to determine whether the image is real or fake. One of the indications for stable DCGANS is that pooling layers should be replaced by scaled convolutions or \u201cstrided convolutions\" in the discriminator and by scaled fractional convolutions or \u201cfractional-strided convolutions\u201d. This alteration of GANs considerably stabilizes the training and generates higher-quality and higher-resolution images than traditional GANs. Given the success of convolutional neural networks (CNNs) in image and video classification in recent years, DCGAN remains a suitable architecture for image-generation applications Shahriar [2022]. The works of Elgammal et al. [2017], Tian et al. [2020] using Style-based can be highlighted.\nStyle-based architectures in GANs are based on deconstructing high-level feature attributes from low-level features. An example of this type of architecture is StyleGAN Karras et al. [2021], a variant of GANs presented by NVIDIA, which is inspired by the style transfer literature. Its architecture differs from that of traditional GANs by skipping the latent code input layer instead of starting with a learned constant. Given a latent code, a nonlinear network produces a version of a generative image found in a latent space, which then controls the generator through adaptive instance normalization (AdaIN) in each convolutional layer. This revolutionized image generation is owing to its diversity and high realistic capacity Bandi et al. [2023].\nThis architecture has received updates, such as StyleGAN2 Karras et al. [2020a], implementing progressive growth and regularizing the generator to drive good conditioning in the mapping of latent codes to images. As an alternative, StyleGAN2-ADA Karras et al. [2020b] was released, where an adaptive discriminator augmentation mechanism was implemented that stabilizes training when training with a limited amount of data. These additions yield good results\""}, {"title": "2.4 Art Generation using GANS", "content": "The emergence of GANs had a significant impact on the generation of artistic works, whether transforming photographic images into paintings or generating completely new works. Nakano et al. Nakano [2019] present \"Neural Painters\", a generative model of brush strokes learned from a real, non-differentiable, and non-deterministic program. They propose a method to \"motivate\" an agent to paint using more human-like brush strokes when reconstructing digits. Huang et al. [2019] presents a method to teach machines to paint like humans, who are capable of using small brush strokes to achieve excellent results in their paintings. The goal of their model is to decompose the original image into different brush strokes and then recreate them on the canvas. To mimic the human painting process, the agent is trained to predict the next brush stroke based on the current state of the canvas and the reference image to be painted.\nA challenge that has been worked on in recent years is the generation of new artistic images or images with a different meaning from the original. The emergence of GANs Goodfellow et al. [2014] and the popularity they have gained in recent years, given their performance and good results, definitely show potential to achieve this goal. GANs have allowed the generation of new images from class labels Mirza and Osindero [2014], Odena et al. [2017] and by synthesizing text descriptions Reed et al. [2016], Xu et al. [2017], Zhang et al. [2017, 2018a], which would allow the generation of completely new artistic works that represent feelings and emotions indicated in the form of text or as classes when training new models. Zhang et al. [2018b] present an approach for generating artistic works with a specific artistic genre, based on the content text given by the user. They build an input and output system called \"AI Painting\", which consists of three parts: the content, which is an object or scene written in natural language; a word for aesthetic effect, for example, cheerful or depressive; and an artistic genre, for example, impressionism or suprematism. The workflow of this method consists of four steps: 1) generate an image based on the natural language content input using StackGAN++ Zhang et al. [2018a]; 2) modify the image to include the specified aesthetic effect; 3) transfer the image to the corresponding genre using neural style transfer; 4) illustrate the painting process in a short video.\nLi et al. [2020] present a method for generating abstract paintings. Using the WikiArt dataset and a k-means algorithm that automatically finds the optimal value of k for color segmentation, they manage to divide each painting into color blocks. Then the image segmented by color blocks is used as a real input image to the discriminator, teaching the generator to paint abstract images with color blocks.\nLisi et al. [2020] introduce a new cGAN training method, which allows the generation of samples from a sequence of distributions. The training was carried out with paintings from a series of artistic movements, which represented a different distribution. Discoveries in each distribution can be used by cGANs to predict \"future\u201d paintings. The experiments demonstrate that this training is capable of generating accurate predictions of future art, using paintings from the past as a training dataset.\n\u00d6zgen and Ekenel [2020] investigated the generation of artistic works in a varied dataset, which includes images with variations in color, shapes, and content. This variation present in the dataset provides originality, which is very important for artistic creation and its essence. One of the main characteristics of this model is that, instead of using phrases as descriptive input, keywords are used. They propose a sequential architecture of GANs, which first processes the given description and creates a base image, while the following stages focus on creating high-resolution artistic-style images without worrying about working with word vectors."}, {"title": "3 Materials and Methods", "content": "The research was divided into three stages: (1) data preparation, (2) modelling, and (3) evaluation. To provide an overview, we explain each of them in the in-depth process, which is reflected in Fig.1. The first phase consisted of data preparation. This process begins with the selection of artworks associated with the landscape category. It is important to note that owing to the type of training of the algorithm, each artwork must be associated with one or more emotions according to an emotional model (which in this case is discrete; Lang et al. [2020]. This allows the generative art algorithm to be trained using a specific output class. Second, the modelling phase consisted of 20 landscapes generated by StyleGAN2-ADA tool. Each of these images was associated with one of the four predefined emotions during the training process, corresponding to contentment, amusement, sadness, and fear. In total, 80 images (20 landscape versions of their four emotions) were generated, which were individually evaluated by 61 individuals (33 males, 28 females). Each participant classified each image into one of four emotional categories. The evaluation is blind; that is, the evaluators do not know the emotional category generated by the computer in advance, thus ensuring the independence of the experiment between the evaluator and the generative computational tool. Next, we present each stage in detail at a specific level and the intermediate steps associated with each stage (see Fig. 2)."}, {"title": "3.1 Data Preparation", "content": "This process involves image extraction and selection with certain emotions and categories. Artemis dataset Achlioptas et al. [2021] was used, which is composed of 80,031 records obtained from the WikiArt dataset Mohammad and Kiritchenko [2018]. Artemis has five records for each artwork: (1) artistic style, (2) artwork, (3) emotion declared by the annotator, (4) explanation by the annotator, and (5) number of annotators who participated in that work. Each record"}, {"title": "3.2 Modelling", "content": "As we have previously discussed, despite the existence of different style transfer tools Cai et al. [2023], we have selected StyleGAN2-ADA since related research indicates that this tool generates good results with a reduced amount of training data Karras et al. [2020a]. This tool has been configured to generate landscape images of the following four emotions: contentment, amusement, fear, and sadness. According to these categories, it is possible to group into positive emotions (contentment and amusement) and negative emotions (fear, sadness). The emotions that have been discarded are astonishment, excitement, anger, and disgust. In the case of astonishment, this can be seen positively and negatively, which would produce a certain ambiguity in its perception Lu et al. [2016]. The other discarded emotions were excitement, anger, and disgust since they could be subordinate to the selected set, therefore they presented within one of the quadrants of a continuous emotional model Russell and Mehrabian [1974]. For this reason, we have finally considered the four emotional categories described above (contentment, amusement, fear, sadness). Furthermore, from the point of view of the continuous emotional scale (CES) Mohammad and Kiritchenko [2018] we observe that"}, {"title": "3.3 Image voting by emotion", "content": "The next step of this research consists of the evaluation of each of the images generated in the previous phase. For this, a form was designed using the Google Form platform, where demographic data was collected regarding age, gender, nationality, level of education and area of knowledge. For the latter, we followed the classification of knowledge area proposed by the Organisation for Economic Co-operation and Development OECD [2015]. In the same form, the automatically generated landscapes were presented in their four emotional versions (80 in total) randomly, so as not to influence the evaluators by any pre-established order. Participants had to indicate one emotion out of the four options (contentment, amusement, fear, sadness) for each version of the landscape. The form was available in English and Spanish from October 30 to November 30, 2023. The average age of the evaluators was 30 years (std =7) with a median of 24 years, with a minimum of 18 years, and a maximum of 55 years. Regarding gender, 33 participants declared themselves as male and 28 as female. There were no participants who indicated belonging to another gender (non-binary, or no information). About the area of study, 35% of the participants declared being associated with the area of engineering and technology, 29% with the area of social sciences. The areas of humanities and natural sciences together represent only 11%. Finally, 70% of the participants declared belonging to the group of graduate or postgraduate as the highest level of education attained. The remaining 30% are grouped into students who have obtained a professional or high school degree (see indicators in Fig.4)."}, {"title": "3.4 Evaluation", "content": "Finally, with the data obtained in the previous phase, a statistical analysis has been carried out to evaluate three aspects: the agreement between evaluators, the agreement between the participants (mode) and the generative tool, and a comparative analysis of the agreement reached between different groups of observers and the generative tool. This analysis has been carried out on the 80 images in the four emotional categories (contentment, amusement, fear, sadness). Additionally, the same analysis grouped into positive (contentment and amusement) and negative (fear and sadness) categories."}, {"title": "3.4.1 Agreement between evaluators", "content": "This process consists of analyzing the inter-rater agreement among the survey participants when emotionally classifying the images generated by the generative tool to measure the agreement between them. For this, we use Krippendorff's Alpha coefficient (Krippendorff [2004]), which evaluates the level of agreement between observers or participants in assigning categories to a data set. Unlike other indicators, it can be calculated for more than two evaluators, with different types of variables and metrics, in the case of missing data and for small samples Eser and Aksu [2022], Hayes and Krippendorff [2007], Volkmann et al. [2019]. This step aims to assess whether images produced by the generative tool elicit consistent responses across all participants. This will serve as a proxy to analyze the agreement between each participant and the generative tool itself."}, {"title": "3.4.2 Agreement between mode and SG2-ADA", "content": "For the evaluation of the agreement between the participants and the generative tool, three aspects are analyzed: the inter-rater agreement, the fisher test and the confusion matrix. In this case, the inter-rater agreement is calculated"}, {"title": "3.4.3 Proportion analysis", "content": "To evaluate whether the agreement reached in the classification of the images between the participants (mode) and the AI is similar for the images that represent the same emotion, the proportion of agreement is calculated concerning the emotion chosen by the participants (mode) and the emotional label that was provided to the generative tool for each of the 80 images. First, the percentage of agreement is calculated by classifying the images into the four emotional categories, and subsequently, the proportion of agreement is calculated, coding the categories into positive and negative"}, {"title": "4 Results", "content": "The results are presented below according to each of the evaluation stages described in the methodology (Fig.2). In particular, we address the results of agreement, agreement between the IA and individuals and the proportion of agreement."}, {"title": "4.1 Agreement between evaluators", "content": "At the level of comparison on the classifications made by the study participants, the results indicate that people do not agree with each other when classifying the images into four emotional categories (contentment, amusement, fear, and sadness). However, when the emotions are dichotomized into positive and negative, the indicator slightly increases according to Krippendorff's Alpha (see results in Table 1)."}, {"title": "4.2 Agreement between mode and generative IA", "content": "This section analyzes the agreement between participant responses and the output of the generative tool (StyleGAN2-ADA). For this, we use the mode of the evaluators' classifications and the emotion used to generate the images.\nAssuming that the emotion generated by the generative tool corresponds to the actual (or true) class, we analyze the precision, recall and F1 score of the obtained data to quantify the level of agreement in the classifications for each group of evaluators and the generative tool. The results reveal important differences according to the group and the emotional category. As stated in Table 2, the best classification results were obtained for the Fear category in most groups, however, the performance changes according to the group analyzed. For example, in the female group, an F1-score=0.76 was obtained, and in the same emotional category, we achieved an F1-score=0.89 for the postgraduate group. The above indicates that by maintaining the same emotional category, different groups of segmentation obtain different performances. In the opposite direction, we observe that for the emotional category contentment, there is a lower level of classification for all groups analyzed. The above could indicate that for individuals it is more complex to classify a positive emotion over a negative one. On the other hand, when the emotions are binarized into positive and negative categories, we obtain a high performance in general. However, in some cases, it is observed that the detection of positive emotions is more difficult than negative emotions (see Table 3). To further analyze the statistical differences, we analyze this point in the following section.\nWe analyze whether there are significant differences in the agreement between the participants (mode) and the AI when classifying the images (in four categories and two categories) between participants groups: men-women, engineering- social sciences, and graduate-postgraduates. To do this, we compare the precision and recall of the confusion matrices (Table 2 and Table 3) using Fisher's Test. When the precision of the confusion matrices is compared, the results show that there are only significant differences in the case of the 'Sadness' emotion. Specifically, precision is higher for men (p-value=0.007), for individuals in the 'Engineering and Technology' area of knowledge at a 10% significance level"}, {"title": "4.3 Analysis of proportions", "content": "This section aims to investigate whether the proportion of participants agreeing with the generative tool (SD-ADA2 GAN) remains consistent across different images. Specifically, we examine if this proportion remains invariant when individuals categorize the 20 images that share the same ground truth label (generative tool). To achieve this, we first quantify the percentage of participants who agree with the AI's classifications for each of the 80 images. Subsequently, we conduct Cochran's Test to ascertain if statistically significant differences exist among the images generated with the same emotional label.\nWe represent in Fig. 10 the percentage of individuals who have agreed with the AI when classifying the image with the emotional label \u201ccontentment\u201d. As we can observe, more than 60% of the participants have selected the actual label in 10 of the 20 images. There is also diversity in the agreement, with image 15 version 4 being the one in which more individuals agree with the AI, specifically 79%. In contrast to this percentage, and at the other extreme, only 15% of"}, {"title": "5 Discussion", "content": "The technological development of artificial intelligence has been exponential in recent years, and the advances in using this tool for analyzing emotional aspects across various fields of knowledge have been significant Cao et al. [2023]. However, to the best of our knowledge, we have not found studies that analyze the level of agreement between the emotions generated by generative artificial intelligence tools and human assessments. Closely related to this issue is the research conducted by Lopatovska (Lopatovska [2016]), which focuses on works created by humans.\nFor this reason, we propose a novel methodology that begins with training a model using artworks catalogued by emotions from the Artemis dataset Achlioptas et al. [2021] to generate 20 landscape images. For each image, four emotional variants were created (contentment, amusement, sadness, and fear), which can be grouped into positive (contentment and amusement) and negative (sadness and fear) categories by dichotomizing the problem Wang [2022], Zhao et al. [2022b], resulting in a total of 80 images. Using this dataset, an online questionnaire was designed to understand human emotional appreciation, obtaining 61 responses (33 male and 28 female) from participants across different countries, educational levels, and fields of study.\nUsing the obtained data, different analyses were conducted to address the research hypothesis regarding whether images created by generative processes with a specific emotion align with human emotional responses.\nFirst, the agreement among evaluators regarding their emotional classifications of the AI-generated images was examined. For this purpose, Krippendorff's Alpha coefficient was utilized. According to Krippendorff Krippendorff [2004], the results indicate a fair level of agreement among the evaluators, with segmentation across the four emotional"}, {"title": "6 Limitations and future directions", "content": "Among the main limitations of this research, the small number of evaluators who responded to the questionnaire stands out, as it constitutes a non-representative sample that hinders the ability to draw significant conclusions regarding the level of agreement on emotions, given their inherent subjectivity.\nAdditionally, it is recognized that social and cultural context plays a crucial role in emotional appreciation. Therefore, expanding the sample to include participants from more countries would facilitate comparative analyses. Similarly, involving individuals from a broader age range would enhance the comprehensiveness of the analysis.\nAnother factor to consider is that the generated sample for classification was limited to landscapes, which restricts the number of referential elements that could aid in classifying emotions more distinctly (e.g., faces). Future research should incorporate images with varying levels of representation and different elements, enabling an examination of the level of agreement across different degrees of representation. Furthermore, it would be interesting to investigate the key visual elements influencing emotional classification decisions, following previous research that has analyzed aspects such as color, shapes, and textures.\nFinally, our study revealed that images conveying negative emotions were classified more effectively than those depicting positive emotions, suggesting that evaluators perceived negative emotions more clearly. This finding opens up new avenues for research to explore the underlying reasons for this phenomenon."}, {"title": "7 Conclusion", "content": "Given the need to validate the content generated by artificial intelligence, this research focuses on emotional validation through statistical analysis of the level of agreement between a set of artificially generated images with associated emotions and the classification of these images by humans.\nTo achieve this, a methodology was proposed that includes training StyleGAN2-ADA using the Artemis dataset to generate 20 landscape images. For each image, four emotional variants were created (Contentment, Amusement, Fear, and Sadness), which can be grouped into positive and negative emotions. The human classification was conducted through an online questionnaire. Based on the obtained data, statistical analyses were performed to evaluate the level of agreement among individuals, the mode of the responses, the emotions generated by the AI, and to analyze proportions."}]}