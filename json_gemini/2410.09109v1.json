{"title": "Compressing high-resolution data through latent representation encoding for downscaling large-scale AI weather forecast model", "authors": ["Qian Liu", "Bing Gong", "Xiaoran Zhuang", "Xiaohui Zhong", "Zhiming Kang", "Hao Li"], "abstract": "The rapid advancement of artificial intelligence (AI) in weather research has been driven by the ability to learn from large, high-dimensional datasets. However, this progress also poses significant challenges, particularly regarding the substantial costs associated with processing extensive data and the limitations of computational resources. Inspired by the Neural Image Compression (NIC) task in computer vision, this study seeks to compress weather data to address these challenges and enhance the efficiency of downstream applications. Specifically, we propose a variational autoencoder (VAE) framework tailored for compressing high-resolution datasets, specifically the High Resolution China Meteorological Administration Land Data Assimilation System (HRCLDAS) with a spatial resolution of 1 km. Our framework successfully reduced the storage size of 3 years of HRCLDAS data from 8.61 TB to just 204 GB, while preserving essential information. In addition, we demonstrated the utility of the compressed data through a downscaling task, where the model trained on the compressed dataset achieved accuracy comparable to that of the model trained on the original data. These results highlight the effectiveness and potential of the compressed data for future weather research.", "sections": [{"title": "1 Introduction", "content": "Weather forecasting is crucial for society and various industries, supporting informed decision-making in areas such as agriculture, transportation, disaster management. Traditionally, numerical weather prediction (NWP)) models have been employed for this purpose, but they are computationally intensive, requiring substantial computing resources. Recent advancements in deep learning offer promising alternatives to NWP models, potentially offering faster and equally accurate forecasts [1-3]. However, the effectiveness of deep learning applications in weather and climate relies heavily on the availability of large-scale datasets. The computational demands for acquiring, storing, and managing such data frequently exceed the capabilities of researchers with only modest setups, creating significant barriers for those lacking access to high-performance computing resources and data storage.\nMoreover, to ensure accurate weather forecasting, numerous super-computing and research centers around the world conduct operational weather and climate simulations multiple times daily. For example, the European Centre for Medium-Range Weather Forecasts (ECMWF) manages 230 petabytes (PB) of data and processes approximately 600 million Earth observations each day. This data volume is projected to quadruple over the next decade due to increasing spatial resolution in forecasting models [4]. While this data growth provides more opportunities for training deep learning models, it also poses challenges as the sheer data volume can overwhelm the existing super-computing infrastructure and complicate the distribution of weather products due to limited network bandwidth. Thus, effective data compression techniques are essential.\nCompressing weather data is similar to the Neural Image Compression (NIC) task in the computer vision domain, where the goal is to reduce the file size of images while maintaining acceptable quality. For example, Ball\u00e9 et al. [5] employed an end-to-end convolutional neural network for image compression, while Chen et al. [6] developed a framework to compress automotive stereo videos by reducing temporal redundancy. Most of the prior work has focused on using Variational Autoencoders (VAEs) for image compression [7, 8]. For instance, Cheng et al. [8] demonstrated that employing discredited Gaussian Mixture Likelihoods to parameterize the latent code distributions achieves greater accuracy than traditional entropy models."}, {"title": "", "content": "Although NIC based on VAEs shares similarities with weather data compression in exploring latent representation patterns, significant differences remain. Natural images, with high correlation among red, green, and blue (RGB) channels, allow traditional compression techniques to exploit these relationships effectively. In contrast, while weather data exhibits correlations among its variables, these relationships are not as straightforward correlated as nature images. Thus this further complicates the application of conventional methods. Additionally, the volume of weather data is substantially large. For example, the ERA5 reanalysis dataset [9] consists of hourly data at a spatial resolution of 31 km, with 640 \u00d7 1280 points, totaling approximately 226 terabytes (TB))]. In comparison, the High Resolution China Meteorological Administration Land Data Assimilation System (HRCLDAS) [10, 11] offers a regional product with a spatial resolution of 1 km, incorporating 4500 \u00d7 7000 grid points hourly. Managing such large datasets for weather applications introduces challenges for NIC in deep learning, including increased data loading times and processing requirements that can hinder training and model convergence, ultimately degrading compression performance. Thus, this study proposes a new data compression method tailored specifically to weather data to address these challenges.\nFurthermore, to evaluate the effectiveness of the proposed data compression method, we employed statistical downscaling as a proof-of-concept downstream application. Despite advancements in deep learning for downscaling, the conflict between the need for extensive data and limited computational resources remains a significant challenge. For instance, Leinonen et al. [12] utilized a generative adversarial model to stochastically downscale coarse radar data to high resolution without additional conditional variables. Harris et al. [13] divided the entire UK region into smaller patches to incorporate more conditional inputs given computational constraints. yet this approach limits the deep learning model's ability to capture global features. Similarly, Zhong et al. [14] used high-resolution assimilation data as ground truth for downscaling, but their application was confined to specific small regions in China. Thus, demonstrating the successful application of compressed data for these tasks could serve as a promising example in the field, alleviating computational constraints and promoting broader application of deep learning techniques in weather downscaling.\nIn addressing the challenges of managing large-scale weather data, we propose a NIC framework specifically designed for this purpose, compressing high-resolution weather data into a latent representation space to ease the data burden for downstream tasks. The main contributions of this research are as follows:\n\u2022 Introduction of a novel data compression framework tailor to weather data: We present a Variational Autoencoder (VAE) method for reducing high-resolution weather data. The VAE's encoder generates a quantize latent Gaussian distribution through the variational inference process. Our comparison of training strategies reveals that pre-training followed by a fine-tuning yields the best reconstruction performance, resulting in superior latent representation of the original data.\n\u2022 Proof of Concept with large HRCLDAS Data: We applied our framework to 1-km high-resolution HRCLDAS data, successfully compressing three years' worth of data from a total of 8.61 TB to a compact 204 GB data. Reconstruction results"}, {"title": "2 Methodology", "content": "demonstrate that the VAE decoder can accurately reproduce the raw data with minimal loss of information and detail, effectively recovering and preserving key properties of the original data, such as extreme values.\n\u2022 Downstream Application - statistical downscaling: We provide a downstream application - statistical downscaling the output from a deep learning based weather forecasting model, i.e. FuXi [15], over China as proof of concept. Unlike most studies that resolve the NWP outputs, our research is the first to downscale deep learning-based weather forecasting model outputs, enhancing their resolution. Our results indicate that our framework significantly reduces computational costs while maintaining model performance.\n2.1 Data sources\nTo assess the performance of the proposed compression framework, we employed downscaling as a proof-of-concept application, using HRCLDAS as the high-resolution ground truth. HRCLDAS [10, 11] is a blended dataset that integrates station observations, satellite data, and NWP data station advanced land surface and data assimilation techniques [14, 16]. In this study, the HRCLDAS data encompasses three years of data from 2019 to 2021, including hourly 2-meter temperature (\u04222\u043c), 10-meter u-component of wind (U10M), and 10-meter v-component of wind (V10M). Its original dimensions are 4384 \u00d7 6880 \u00d7 3, covering latitudes from 15\u00b0 to 55\u00b0 and longitudes from 75\u00b0 to 135\u00b0. We used the data from May 2019 to October 2019 and July 2020 to August 2021 as training dataset, and from September 2021 as testing for data compression task.\nFor the downscaling task, we used forecasts from the FuXi-2.0 model as low-resolution inputs. The FuXi-2.0 model is a cascaded machine learning weather forecasting model that provides 15-day global forecasts with a temporal resolution of 1 hour and a spatial resolution of 0.25 \u00b0[15, 17]. In this study, we selected forecasts with lead times ranging from 1 to 24 hours initialized at 00 UTC and 12 UTC for our training set.\nWe selected 40 input variables at surface and pressure levels from the FuXi forecasts for downscaling task, guided by domain knowledge (see Table 1). The dataset spans from May 2019 to October 2019 and July 2020 to August 2021, while the testing set includes data from November 2019 to June 2020 for testing the downscaling performance. The training set consists of 14,000 hours of data, whereas the testing set contains 6,346 hours, with no overlap between the two. Prior to inputting the data into the downscaling model, all input and output variables are normalized using z-score normalization."}, {"title": "2.2 Overview of Data Compression Framework", "content": "Problem statement We represent high-dimensional atmospheric data at a given time t using a tensor Xt with dimensions V \u00d7 H \u00d7 W, where H and W denote the number of latitude and longitude coordinates, respectively. For HRCLDAS, V=3, W=4384,"}, {"title": "", "content": "H=6880. The indexing scheme $X_{h,w}^v$ indicates the value of variable v at time t and latitude-longitude coordinates (h, w).\nThe first objective of this study is to compress Xt to a lower-dimensional representation space Zt using an encoder with learnable parameters within the proposed VAE compression framework. The second goal is to utilize the compressed data for the downstream task of downscaling. Here, we select input Yt, which represents a variable with a lead time of t from the FuXi forecast outputs. Our aim is to learn a mapping function f that converts the low-resolution input state Y at timestamp t to Zt.\nVAE data compression framework Fig.1 illustrates the overall compression framework based on the VAE neural network. The VAE consists of an encoder E and a decoder D. The encoder E compresses the information from the current timestamp into a low-dimensional latent vector Z. The decoder reconstructs the original high-resolution data X from latent vector Z through upsampling and convolution layers. To facilitate random sampling, E outputs a mean \u00b5 and standard deviation \u03c3, followed by a sampling function that generates the latent representation Z.\nSpecifically, the encoder E first transforms the input to a higher-dimensional feature space using a 1\u00d71\u00d7C convolutional layer, with C (the number of channels) initially set to 128. This is followed by four stages, where the first three each contain two ResNet blocks and a downsampling block. Each ResNet block utilizes two convolution layers with a kernel size of 3, followed by a Swish activation function [18] and group normalization layers. Residual connections link the input and output, mitigating the risk of vanishing gradients.\nThe first convolutional layer of each stage doubles the feature map size, while the second convolutional layer maintains the same size. At the end of each stage, a downsampling block is applied, consisting of a convolutional layer with a kernel size of 3 and a stride of 2 to reduce the volume by half, replacing the traditional pooling operation. The fourth stage differs in that it does not include a downsampling layer. Instead, it uses two additional ResNet blocks to refine the compressed features. Finally, a convolutional layer with a feature map size of 4 is used to generate the mean \u00b5 and var \u03c3,both with dimensions 548\u00d7860\u00d74.\nThe decoding process is a symmetric process with the encoding process. The decoder initially uses a convolutional layer to convert the channel dimension from 4 to"}, {"title": "", "content": "512. Bilinear interpolation method is used for upsampling at the end of each stage. Two ResNet blocks in the decoder further refine the features. The four decoding stages process the features sequentially, yielding feature map sizes of 548\u00d7860\u00d74, 548\u00d7860\u00d7512,\n548\u00d7860\u00d7512, 1096\u00d71720\u00d7256, and 2192\u00d73440\u00d7128, respectively. Finally, a convolutional layer converts the channel dimension to the output dimension, resulting in an output size of 4384\u00d76880\u00d73.\nThe loss component of the VAE consists of the reconstruction loss and the Kull-back-Leibler divergence Dkl. We utilized the charbonnie loss [19] as reconstruction loss function as described in Eq. 1 to better handle the outliers situation. In addition, the Kullback-Leibler divergence Dki regularizes the posterior distribution of the latent space P = N(\u03bc, \u03c3\u00b2) to match the prior distribution, a standard Gaussian distribution Q = N(0, 1). Those parameters are optimized with the help of the re-parametrization trick [20].\n$L(X, X') = \\sqrt{\\|X \u2013 X'\\|_2 + \\epsilon^2}$\n$KL(P||Q) = \\sum_{X} P(X)log(\\frac{P(X)}{Q(X)})$\nDownscaling Model: For the downstream task, we selected downscaling as a case study to evaluate the usability of the compact data generated by our proposed VAE framework. The primary objective of the downscaling model is to enhance the output resolution of FuXi forecasts. Once the VAE model is trained, the HRCLDAS"}, {"title": "", "content": "data are encoded by the VAE encoder to extract latent representations, which serve as ground truth for the downscaling model. We employed U-Net for this task[21]. The original FuXi forecast input has dimensions of 176\u00d7276 with a spatial resolution of 25 km. For downscaling, we interpolate the data to a resolution of 550\u00d73862\u00d73 with a spatial resolution of 8 km.\nU-Net utilizes a similar encoder-decoder architecture as the VAE. It comprises four stages, each containing two ResNet blocks in both the encoder and decoder. The decoder Dunet, which serves as an expansive path, employs upsampling layers to project the encoder output to the resolution of the compact data. The decoding part of U-Net is connected to the corresponding encoder part via skip connections. A convolution operation is applied to the concatenated output of the up-sampled output and its encoder counterpart, as illustrated in Fig. 1. Eventually, the U-Net trained on the original HRCLDAS data has 118 million trainable parameters, while the U-Net trained on compressed data has 94.8 million parameters.\n2.3 Implementation details\nWe first trained a VAE for data compression. The model was trained with a batch size of 8 and optimized using the Adam optimizer with a learning rate of 1.6e-5. Given the high resolution of the original HRCLDAS data, we split it into 1000\u00d71000 patches with overlaps in the latitude direction, resulting in 35 patches per dataset. To reduce computational costs and accelerate convergence, we further divided these 1000\u00d71000 patches into smaller 256\u00d7256 patches. We pre-trained the VAE model on these smaller patches for the first 10 epochs and subsequently fine-tuned it using the larger patches for an additional 5 epochs. Unlike natural images, where channels are highly correlated, weather variables exhibit weak inter-variable relationships. Therefore, we trained separate VAE models for T2m, and U10M, V10M, respectively.\nFor training the downscaling model, we used the compressed data Zt with a resolution of 544\u00d7856 generated from the trained VAE encoder as ground truth. We trained the U-Net model with a batch size of 16 for 50 epochs. The model was optimized using the Adam optimizer with a learning rate of 3.2e-5. All the models were implemented using the PyTorch framework and trained on 8 NVIDIA A100 GPUs.\n2.4 Experiment setup\nFour experiments were conducted to assess the impact of different modeling strategies on data reconstruction performance within the proposed VAE framework. Additionally, three experiments were performed to evaluate the model's downscaling performance using the compact data generated by the VAE.\nThe four experiments within the VAE framework are detailed in Table 2. The baseline model (resize) employs a straightforward downsampling technique to reduce dimensions and an upsampling method for data reconstruction. To achieve improved reconstruction results, we trained the two VAE models using two different loss functions: L1 loss and Charbonnier loss, on 256 \u00d7 256 patches. Since the target variables do not exhibit linear correlations with one another, we trained separate models (VAE single variable) for each target variable, rather than treating them collectively, as is"}, {"title": "", "content": "common with natural images. Finally, to obtain more accurate results, we fine-tuned the model trained on smaller patches using the full resolution data (VAE fine-tune). For the downscaling task, we used bilinear interpolation (Inter) as the baseline for comparsion. We trained the U-Net model on both the original HRCLDAS data (No-VAE) and the compact data generated by the encoder from VAE (fine-tune) model, respectively, to access the impact of the data compression method on downstream task performance.\n2.5 Evaluation metrics\nTo evaluate the performance of the reconstruction and its downscaling capabilities, we use several metrics commonly applied in the weather and climate domain [14, 22, 23]. Specifically, we calculate the mean square error (MSE), root mean square error (RMSE), and power spectrum.\nThe MSE measures the difference between the ground truth Xw,h and the reconstructed or downscaled results Xwh, where the grid corresponds to the cell center positions w and h in the zonal and meridional directions, respectively. RMSE, on the other hand, represents the average difference between the ground truth and the reconstructed data generated by the model. Both MSE and RMSE are negatively oriented, with an ideal value of 0 indicating perfect reconstruction.\n$MSE(v) = \\frac{1}{N} \\sum_{w=1}^{W} \\sum_{h=1}^{H} [X_{v,h,w} \u2013 \\hat{X}_{v,h,w}]^2$\n$RMSE(v) = \\sqrt{\\frac{1}{N} \\sum_{w=1}^{W} \\sum_{h=1}^{H} (X_{v,h,w} \u2013 \\hat{X}_{v,h,w})^2}$\nIn addition, to evaluate the local-scale variability and determine the information preserved or lost by the VAE framework for the downstream tasks, we conduct power spectrum analysis following the methodology described by Rasp et al. [22]. The power spectrum is calculated along lines of constant latitude as a function of wavenumber (unitless), frequency (km-1), and wavelength (km). The Discrete Fourier Transform (DFT), denoted as FK is computed using the following Eq.2.5:\n$F(k) = \\frac{1}{L} \\sum_{l=0}^{L-1} f_l e^{\\frac{-i2\\pi kl}{L}}$"}, {"title": "", "content": "Since our evaluation focuses on the regional rather than global scales, we do not consider the circumference in this study. The power spectrum for constant latitude is obtained using the following equation:\n$S_k = 2|F_k|^2 , k = 1, 2, ..., L/2$\nAdditionally, we approximate the average zonal power spectrum using:\n$\\int_0^{L/2} |f_l| dl \u2248 \\sum_{k=0}^{L/2} S_k$\nIn addition to the meteorological evaluation, we incorporate the structural similarity index (SSIM) [24], which was initially developed for computer vision to quantify and compare spatial variability between downscaled fields and ground truth data. This metric has recently been applied in meteorological research [14, 23].\n$SSIM(y, \\hat{y}) = \\frac{(2\\mu_{\\hat{y}}\\mu_{y} + C_1) + (2\\sigma_{\\hat{y}y} + C_2)}{(\\mu_{\\hat{y}}^2 + \\mu_{y}^2 + C_1)(\\sigma_{\\hat{y}}^2 + \\sigma_{y}^2 + C_2)}$\n3 Experiments results\n3.1 Representation performance"}, {"title": "", "content": "Table 3 presents performance results for various data compression strategies, including simple resize, VAE with L1 loss, VAE with Charbonnier loss, VAEs trained on single variables, and the VAE fine-tuning strategy. Among these methods, the VAE fine-tuning strategy demonstrates superior performance in reconstructing U10M, V10M, and T2M. Additionally, we observed that simple VAE with L1 and Charbonnier loss struggles to effectively represent the temperature and wind fields, leading to a high RMSE. Notably, training on a single variable is consistently superior to training on multiple variables. Furthermore, a comparison between the VAE trained from scratch"}, {"title": "", "content": "(single variable) and the VAE fine-tuned from pretraining with small patches to full resolution indicates that the pretraining method significantly enhances the representation performance. The VAE fine-tuning approach achieves RMSE values of 0.0181\nK, 0.0139 K, and 0.124 K for the three variables respectively, which is more than 4 times better than the linear interpretation method.\nTo further understand the VAE's performance in data compression, particularly for extreme values, we examine the differences in density distributions among the ground truth, VAE, and a simple resize baseline model, as illustrated in Fig. 3. For T2M\n(Fig. 3a), the VAE-reconstructed data closely aligns with the frequency of positive temperature values in the ground truth HRCLDAS data. In contrast, the resize method results in a slight decrease in the frequency of positive temperatures. For negative temperatures, the VAE model exhibits a lower frequency compared to the original HRCLDAS data. Regarding U10M and V10M (Fig. 3b and Fig. 3c), the VAE effectively preserves a higher frequency of both extreme high and low values.\n3.2 Downscaling performance\nIn our study, we selected the fine-tuned VAE (fine-tune) as the data compression method, subsequently applying the compressed data produced by VAE (fine-tune) for"}, {"title": "", "content": "our downstream task - downscaling. Fig. 4 and Fig 5 demonstrate the performance of the UNet model trained on compressed HCLDAS data (VAE) and original data (No-VAE) in terms of MSE (Fig. 4) and SSIM(Fig.5) respectively, compared to the baseline model - interpolation method, across lead times from 1 to 18 hours. The results indicate that the deep learning-based model significantly outperforms the simple interpolation method (Inter). Specifically, the mean square error values for linear interpolation, No-VAE, and VAE are 11.58 K2, 4.965,79K2, and 5,79K2, respectively, demonstrating that the No-VAE and VAE outperform linear interpolation by approximately 53%, 60% for temperature in terms of MSE.\nWe also employed the SSIM, commonly used in downscaling tasks to access the perceptual similarity between images. SSIM quantifies and compares mean and global spatial variability in the downscaled fields against the ground truth and also accounts for covariances. The SSIM comparisons for all models are presented in Fig 5, showing that both VAE and No-VAE consistently outperform the interpolation method across all lead times and variables.\nFurthermore, Fig. 6 presents the average zonal power spectrum plots for the No-VAE and VAE downscaling models at a 1-hour lead time, compared to the ground truth HRCLDAS data and the baseline interpolation method. The findings reveal that the baseline interpolation method fails to capture significant details across all scales for the three variables. This is expected, as interpolation tends to average out variances, leading to a smoother appearance, as visually confirmed in Fig 8 and Fig. 10. In contrast, both the VAE and No-VAE models preserve more details, with their performances being quite similar. This suggests that the compact data representation from the VAE does not lead to substantial information loss in our proof-of-concept downscaling task compared to using the original HRCLDAS data.\nIt is worth mentioning that the No-VAE model generates artifacts at small scales for\nT2M as the examples demonstrated in Fig. 8c and Fig. 8f. Sudden temperature changes create strong contrasts in specific areas, such as northeast China. This issue arises from the unpatching process, where small patches are reconstructed into the original image due to computational limitations. In contrast, this problem can be effectively mitigated by feeding the U-Net the entire latent features of HRCLDAS data directly from the VAE model, resulting in a smoother downscaling field, as demonstrated in Fig. 8d and Fig. 8g."}, {"title": "", "content": "In addition, we observed that both the VAE and No-VAE models struggle to capture fine-scale features of T2M. This limitation is likely due to the significant elevation variations in the Himalayas and certain regions of China, where topography is crucial for maintaining local temperature gradients. Since the primary focus of this study is to validate the VAE compression method, we did not incorporate topography data in the current models. However, this will be considered in future research.\nTo better understand the realism of downscaling performance, we selected three examples to compare the performance of temperature and zonal wind variables using"}, {"title": "", "content": "\"eyeball\" analysis (see Fig 8 and Fig 10). These examples correspond to 1st November 2019, at 01:00 UTC, 15th April 2020, at 18:00 UTC, and 15th January 2020, at 12:00 UTC, representing FuXi forecasts with 1-hour, 6-hour, and 12-hour lead times, respectively. As illustrated in Fig 8, it can be seen that the interpretation method shows significant biases in temperature downscaling for southwestern China, particularly within the complex mountainous terrain of the Himalayas (see topography map in Fig.7). This bias was especially pronounced in northeastern China during the cold winter period on 15th January 2020, when the No-VAE interpolation methods were applied. In contrast, the application of the VAE method with the compressed data allowed the U-Net to more effectively capture and reduce errors in this region.\n4 Conclusion and Discussion\nThis study investigates the compression of high-dimensional, high-volume weather data using the NIC method, inspired by advancements in the computer vision domain,"}, {"title": "", "content": "for downstream weather and climate applications. Specifically, we employed our proposed VAE framework to compress three years of high-resolution HRCLDAS data, reducing the data size from 8.61 TB to a compact 204 GB, an impressive 42-fold reduction in storage requirements. The results demonstrate that the VAE framework, enhanced with a fine-tuning strategy, outperforms other baseline methods, significantly reducing reconstruction errors, thereby providing more accurate reconstructions of 2-meter temperature, 10-meter U-component of wind, and 10-meter V-component"}, {"title": "Code Availability Statement.", "content": "The source code used for training and running VAE compression and downscaling models in this work is available at\nhttps://doi.org/10.5281/zenodo.13862076."}]}