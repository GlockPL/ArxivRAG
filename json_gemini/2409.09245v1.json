{"title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity", "authors": ["Chengxi Ye", "Grace Chu", "Yanfeng Liu", "Yichi Zhang", "Lukasz Lew", "Andrew Howard"], "abstract": "The discontinuous operations inherent in quantization and sparsification introduce obstacles to backpropagation. This is particularly challenging when training deep neural networks in ultra-low precision and sparse regimes. We propose a novel, robust, and universal solution: a denoising affine transform that stabilizes training under these challenging conditions. By formulating quantization and sparsification as perturbations during training, we derive a perturbation-resilient approach based on ridge regression. Our solution employs a piecewise constant backbone model to ensure a performance lower bound and features an inherent noise reduction mechanism to mitigate perturbation-induced corruption. This formulation allows existing models to be trained at arbitrarily low precision and sparsity levels with off-the-shelf recipes. Furthermore, our method provides a novel perspective on training temporal binary neural networks, contributing to ongoing efforts to narrow the gap between artificial and biological neural networks.", "sections": [{"title": "1. Introduction", "content": "The recent surge in the size and complexity of generative AI models has elevated computational efficiency to the forefront of AI research [6, 31]. Among the diverse approaches to achieving efficiency, quantization and sparsification techniques stand out as two classic and widely explored methods. Quantization and sparsity techniques can effectively reduce the computational requirements of large language models (LLMs). Quantization reduces the precision of model weights and activations, thereby decreasing storage requirements. Sparsification, on the other hand, prunes redundant weights, leading to a more compact model. These techniques enable LLMs to be deployed on resource-constrained devices, such as mobile phones and embedded systems, while also improving their speed and memory efficiency. This facilitates the widespread adoption of LLMs by a broader range of individuals and businesses. Despite their promise, quantization and sparsification introduce non-differentiable operations, such as rounding and hard thresholding, which are incompatible with the differentiable design of backpropagation, the cornerstone of neural network training. This incompatibility has plagued training algorithms for decades, hindering progress in the field of efficient neural networks [3, 24, 41].\nTo tackle the discontinuity challenges inherent in training efficient neural networks, algorithms have primarily focused on adapting gradient descent algorithms to work with non-differentiable operations. Empirical techniques such as the straight-through estimator (STE) [3] have been employed to define gradients for non-differentiable operations. However, even with the STE, the perturbation introduced by quantization has been observed to disrupt existing training recipes (Fig. 1(a)). Consequently, clipping is commonly applied to limit the signals within a small range to prevent divergence. Despite these techniques, training quantized networks remains restricted to specific precisions [7, 25, 26, 32] or models [42, 43]. When moving to low precisions, these approaches usually also make changes to model architectures and recipes, such as inserting extra normalizations[35, 42], changing learning rates [35], replacing optimizers [26], keeping several layers unquantized [1, 26] or through fine-tuning [29]. It is unclear if these techniques are effective and flexible enough in the large generative AI models.\nIn contrast to the intricate and often empirically tuned techniques of prior methods, we adopt another approach that formulates discontinuous operations as the introduction of perturbations [17]. We address this challenge directly by suppressing the effects of these perturbations, effectively denoising the signal. Our approach comprises three fundamental steps:\n1.  Affine Transform for Quantization (Sec. 4.1): An initial affine transform f scales the input signal without introducing additional operations (e.g., clipping).\n2.  Perturbation Injection (Sec. 4.2): A controlled perturbation \u03b4 is injected into the signal, precisely modeling the effect of quantization.\n3.  A Denoising Affine Transform for Reconstruction (Sec. 4.3): A key innovation of our approach is the introduction of another affine transform g that effectively reconstructs the original signal while mitigating quantization noise.\nOur method offers several key advantages: 1. Continuous control over quantization noise: This ensures stable model training and prevents divergence. 2. Graceful degradation: Under extreme noise, our approach seamlessly transitions to a lower-resolution (through averaging) model, guaranteeing a performance lower bound. 3. Compatibility: Our method works with existing architectures and training recipes, eliminating the need for extensive modifications or hyperparameter tuning.\nThese innovations enable the development of diverse, efficient neural networks for various applications. We demonstrate the effectiveness of our approach by training sparse models and temporal binary neural networks. Our key contributions include:\n\u2022\tA simple, robust, and universal solution for training quantized neural networks.\n\u2022\tA novel theoretical perspective and practical solution for training networks with discontinuous operations, based on decomposition and denoising.\n\u2022\tState-of-the-art results with ultra-low precision models, using standard architectures and training recipes.\n\u2022\tEasy adaptation to train sparse neural networks.\n\u2022\tSuccessful training of temporal binary neural networks, demonstrating the robustness of our approach and its potential for bridging the gap between artificial and biological neural networks."}, {"title": "2. Motivations", "content": ""}, {"title": "2.1. A Comprehensive Picture of Model Efficiency", "content": "While extreme quantization and sparsification can significantly reduce storage and computational requirements, they also introduce the risk of quality degradation. Existing studies on quantization and sparsity primarily focus on showcasing the effectiveness of specific implementations through meticulous tuning. While these studies provide valuable insights, they fall short of providing a comprehensive understanding of the performance trade-offs involved in applying these techniques to extreme levels. The absence of a robust, universal algorithm for training quantized and sparse neural networks has hindered an accurate comparison of different approaches and has limited the exploration of the full potential of these techniques. To address these limitations, a more comprehensive and rigorous approach is needed to evaluate the trade-off between efficiency and quality. This necessitates the development of a universal approach that can effectively manage extreme quantization and sparsity levels (Fig. 1(c,d))."}, {"title": "2.2. A Biophysical Basis for Sparse Quantization", "content": "One of the most significant achievements in 20th-century neural physiology was the development of the Hodgkin-Huxley model for modeling animal neural networks [9, 20]. This model unveiled the intriguing fact that animal neural networks primarily consist of brief electrical impulses, commonly known as spikes in their activity patterns. Despite this critical insight, the question of how animals efficiently learn and process information through these spike trains continues to pose a profound and unresolved challenge. Since the rise of regular neural networks, researchers have strived to find a more biophysically plausible approach to artificial intelligence through spiking neural networks [15, 23, 34, 39]. However, the lack of differentiability in spiking neural networks prevents the straightforward application of gradient descent algorithms, the foundation of modern neural networks. Consequently, these networks have limited application and popularity. The development of universal training algorithms for quantized and sparse neural networks could open up new avenues in the field of spiking neural networks."}, {"title": "3. Related Work", "content": "Neural network quantization has become a widely adopted technique for reducing memory footprint and accelerating inference time, enabling efficient deployment on resource-constrained devices [16]. While full-precision models typically store weights in floating-point format, quantized weights are represented as integers, typically using 8 bits [8, 21, 37], 3-4 bits [1, 8, 11, 27], or even 1 bit [7, 26, 32, 35, 42, 43]. In addition to quantizing weights, model activations can also be quantized to further enhance computational efficiency [8, 12, 21].\nAlthough 8-bit quantization is commonly used as a standard practice in industry [21], achieving lower-bit quantization remains challenging and requires specialized techniques to ensure robust training. Several common techniques include: 1. Mixed precision quantization: This approach selectively assigns different bit levels to different weights, aiming to optimize the trade-off between model size and accuracy [10, 18, 25, 36]. 2. Training recipes: These techniques compensate for the discontinuities introduced by quantization by employing strategies such as sharpness-aware minimization [14, 27], state-aware optimization [26], knowledge distillation [22], and multi-phase training [28]. 3. Quantization-friendly network architectures: This approach involves replacing original network layers with alternatives that are more amenable to quantization [42].\nIn contrast to prior work, our method explicitly models quantization discontinuities as perturbations. We decompose the perturbed signal into clean and noisy components, then apply denoising to suppress the noise. This approach leads to a closed-form solution that guarantees training convergence even at extremely low bitwidths. While previous methods have also modeled quantization noise using continuous distributions (e.g., Uniform or Gaussian) for gradient estimation [2, 10], they do not optimize the reconstruction process itself to enhance training stability.\nTo further reduce model footprint, researchers have been combining sparsity/pruning and quantization in a unified formulation to further compress neural networks [30, 40]. In this paper, we extend our noise injection and denoising reconstruction theory to sparsity and argue that instead of pruning small values to zero, moving near-mean values to mean better preserves signal fidelity."}, {"title": "4. Methods", "content": "We primarily focus on explaining our formulation for quantization, as it is more widely supported by modern hardware and offers a broader range of applications. Subsequently, we demonstrate how our formulation can be extended to sparsification. Our method's flexibility enables it to address quantization and sparsification individually or together, allowing users to apply it to model weights and activations independently or in combination."}, {"title": "4.1. Affine Transform for Quantization", "content": "We begin our study by using min-max scaling to move and scale the floating-point vector x to the desired range, e.g. [0, 2bits - 1]. We formulate with this range for simplicity. The range can be shifted if signed integers are more compatible with specific hardware. This affine transform is implemented using standard functions in neural network libraries for backpropagation training.\n\n\\(f(x) = \\frac{x - x_{min}}{x_{max} - x_{min} + \\epsilon}(2^{bits} - 1)\\)"}, {"title": "4.2. Perturbation Injection", "content": "Quantization reduces the number of bits used to represent f(x) by rounding to the nearest integer. This rounding operation introduces discontinuities at half-integer values, rendering it non-differentiable. These discontinuities can lead to training difficulties, as neural networks rely on gradients for learning. Most existing methods for training neural networks with non-smooth operations employ the straight-through estimator (STE) [3]. The STE provides a technique for defining gradients for non-differentiable operations. However, even with the STE, training algorithms may diverge when moving to lower bits (Fig. 1(a)).\nBuilding upon prior research [3, 17, 23], we model the impact of these operations as perturbations within the network, recognizing their role in causing training instability. Specifically, quantization of f(x) can be modeled as injecting a bounded perturbation:\n\n\\(q = f(x) + \\delta,\\)\n\nwhere \u03b4 = round(f(x)) \u2212 f(x), \u03b4\u2081 \u2208 [\u22120.5, 0.5]. We avoid empirical operations such as clipping, which can introduce larger perturbations and degrade model performance. By directly incorporating the quantization noise into the model, our method maintains signal fidelity and achieves superior results compared to traditional clipping-based approaches. Combining with the first affine transform (Eq. 1), our quantization can be found in Fig. 2."}, {"title": "4.3. Denoising Affine Transform for Reconstruction", "content": "A core innovation of our approach is the introduction of an additional affine transformation g, designed to be resilient to perturbations. This enhances the robustness of quantized neural networks, and remarkably, we find that standard training methods used for full-precision models can still effectively converge even when perturbations are present.\nQuantization algorithms typically introduce a one-dimensional affine or linear transform, often referred to as dequantization, to approximate the original vector x. To streamline computations, most practical implementations simply invert the scaling involved in Eq. 1 [1, 42, 43] or minimize the L2 reconstruction error [11, 32]. However, focusing solely on minimizing this approximation error may overlook the crucial challenge of training neural networks to be robust against perturbations (Fig. 1(a)).\nOur approach, counterintuitively, increases the approximation error but naturally resolves this long-standing issue. We formulate the reconstruction as a ridge regression problem, introducing a regularization factor \u03bb and solving for two parameters:\n\n\\(\\underset{a,b}{min} \\frac{1}{2N} \\sum_i |aq_i+b-x_i|^2 + \\frac{\\lambda}{2} a^2\\)\n\nHere N is the dimension/length of x. Taking the derivative with respect to b, and setting to zero yields the following equation: \u2211i(b + aqi \u2212 xi) = 0.\nSolving for b yields:\n\n\\(b = \\bar{x} - a\\bar{q}\\)\n\nAnd from setting the derivative with respect to a to zero: \u03a3i qi(aqi+b-xi)+ \u03bb\u03b1 = 0. Simplifying the equation, we obtain:\n\n\\(\\bar{q^2}a + \\bar{q}b - \\bar{xq} + \\lambda a = 0\\)\n\nSubstituting Eq. 4 into Eq. 5: \\(\\bar{q^2}a + \\bar{q}(\\bar{x} - a\\bar{q}) - \\bar{xq} + \\lambda a = 0\\).\nWe arrive at the solution for a:\n\n\\(a = \\frac{Cov_{xq}}{Var_q + \\lambda}\\)\n\nSubstituting a and b back into the ridge regression yields the reconstructed vector, representing the quantized version of x:\n\n\\(r = g(q) = a \\cdot q + b = \\frac{Cov_{xq}}{Var_q + \\lambda}(q - \\bar{q}) + \\bar{x}\\)\n\nThis affine transformation can also be seamlessly implemented using fundamental operations readily available in neural network libraries (Fig. 3). Quantization and reconstruction can be represented as a straightforward addition of \u03b4 between the two affine transform operations: r = g(f(x) + \u03b4). During actual computations, f(x) + \u03b4 is cast to the appropriate data type."}, {"title": "4.3.1 A Novel View of the Quantized Signal", "content": "The reconstruction from Eq. 7 provides a novel decomposition of the quantized signal r into a smooth component, x, and a non-smooth component, a(q - q), drawing inspiration from detail enhancement techniques in computer vision [13, 19]. Importantly, the bounded perturbation from quantization, \u03b4, is entirely contained within this non-smooth component. This observation is key, as this non-smoothness directly contributes to training instability. Consequently, we can stabilize the training by suppressing this component using the parameter \u03bb. In essence, \u03bb acts as a control knob, regulating the balance between signal and noise that enters the training process, as visualized in Fig. 1(b).\nOur design exhibits two important properties:\nProposition 1. By adjusting \u03bb, the quantized model can be trained to converge if the training algorithm converges on a smaller scale network.\nProof. As \u03bb approaches infinity, the perturbation can be completely suppressed, resulting in the mean value x serving as a fail-safe vector for the reconstruction of x. This behavior is mathematically expressed as:\n\n\\(\\underset{\\lambda \\to \\infty}{lim} a = 0, \\underset{\\lambda \\to \\infty}{lim} b = \\bar{x}\\)\n\nThe structure of this smaller scale network will be explained in Sec. 4.5. In practice, we find that a small \u03bb provides a good trade-off between preserving the original signal and suppressing quantization noise. This leads us to discuss the other extreme case of \u03bb = 0.\nProposition 2. The regularized model evolves continuously from the original model through the control parameter \u03bb.\nProof. Setting \u03bb = 0 (no regularization) and \u03b4 = 0 (eliminating perturbations) restores the network to its original form. Intuitively, this behavior arises from the fact that for unperturbed input data x, q = f(x). The scaling factor a, calculated as \\(\\frac{Cov_{xf(x)}}{Var_{f(x)}}\\) effectively inverts"}, {"title": "4.3.2 The Sensitivity of the Scaling Factor", "content": "By inverting the scaling factor in Equation 1, we can map the maximum perturbation of 0.5 to a perturbation scaled at a magnitude of \\(\\frac{0.5(x_{max} - x_{min} + \\epsilon)}{2^{bits}-1}\\) in the original training signals. This implies that the perturbation intensity doubles with each reduction in bit precision, leading to a substantially amplified impact on the training process. This heightened perturbation has disrupted traditional training recipes (Fig. 1(a)), necessitating the introduction of a regularization term to stabilize the training process.\nThe analysis of the solution for a is rooted in well-established principles of perturbation theory, as outlined in Golub's work [17] (Section 2.6).\n\n\\(\\frac{\\lambda}{Var_q + \\lambda} = \\frac{1}{Cov_{xq}}\\)\n\nWhen \u03b4 is bounded, the impact of \u03b4 on a is proportional to \\(\\kappa = \\frac{1}{Var_q + \\lambda}\\) [17] (Eq. 2.6.4). \u03bb plays a crucial role in establishing an upper bound on \u03ba. In the absence of \u03bb, the influence of \u03b4 on a may become unbounded. This aligns with our observations that strong perturbations associated with lower precision lead to corrupted training. The introduction of \u03bb effectively stabilizes the training process by mitigating the impact of quantization noise and ensuring that the solution to Eq. 9 remains well-conditioned."}, {"title": "4.4. Extension to Sparsification", "content": "Similar to quantization, sparsification introduces discontinuities through the hard thresholding operation H. Consequently, the sparsification process can also be modeled as introducing perturbations, where \u03b4 = H(x) \u2212 x. In our experiment section we show this leads to significant gain in sparsifying the model. Since the perturbation from sparsification rarely leads to divergence, we may set \u03bb = 0 to pass the entire perturbed signal when only sparsification is applied.\nWhen dealing with extreme sparsity or quantization, it is noteworthy that 100% sparsity, under the traditional definition, results in an all-zero vector. To address this limitation, we revisit Equation 7, which defines the reconstructed signal. Equation 7 involves adding back significant deviations to a mean vector of x, ensuring that the mean x remains the foundation of the signal even at extreme sparsity levels. Therefore, sparsification towards the mean presents itself as"}, {"title": "4.5. Quantized Matrix Multiplication", "content": "In matrix multiplication Y = XW, we apply sub-channel quantization by reshaping the last dimension of X and the first dimension of W, effectively splitting vectors along the contraction dimension into smaller, manageable chunks. Quantizing each chunk independently leads to a higher overall approximation quality than quantizing the entire vector or matrix at once. This finer-grained approach reduces perturbation, resulting in a more stable model. The resulting \"fail-safe\" backbone model (obtained as \u03bb \u2192 \u221e) can be visualized as a piecewise constant function (Fig. 1(b)), where each piece is represented by its mean value. This is akin to lowering the granularity of the original model, effectively providing a trainable backbone with reduced complexity.\nThis sub-channel quantization approach permits low-precision block-wise calculation of the expensive matrix multiplication, followed by summation of partial results, can be implemented through batch matrix multiplication, drastically reducing the overall computational burden. While the provided reference code (Fig. 4) utilizes \"fake-quantization\" for clarity, the actual implementation involves true quantization and match matrix multiplication to achieve performance gains.\nThe memory savings achieved depend on the chosen block size B. For instance, when a and b are stored as 16-bit floats, this results in an additional memory overhead of 32 bits per block. The effective number of bits per element is reduced to 32/B. In practice we notice that even 8-bit floats deliver similar quality results. A typical block size of 128 is employed to maintain a storage overhead of less than one bit. The block size introduces a trade-off between memory savings and model accuracy. We investigate this trade-off further in our experiments."}, {"title": "5. Experiments", "content": "To ensure consistency, we maintained a fixed block size of 128 across all experiments unless otherwise specified. Our experiments revealed that the regularization parameter \u03bb = 0.01 consistently yielded satisfactory results. Quantization"}, {"title": "5.1. Ultra-low Precision Models", "content": ""}, {"title": "5.1.1 ResNet-50 on ImageNet", "content": "We utilized the Flax framework to train ResNet-50 from scratch on ImageNet, employing stochastic gradient descent with an initial learning rate of 0.1, training the model for 100 epochs with weight decay of 0.0001. As shown in"}, {"title": "5.1.2 Transformer on WMT", "content": "To evaluate the effectiveness of our method on transformer models, we employed the Flax framework to train the transformer model on two WMT2017 datasets (EN-DE, DE-EN) and subsequently assessed its performance on the corresponding WMT2014 datasets. The training process utilized the AdamW optimizer with weight decay set to 0.1 and a cosine scheduler for 25,000 steps, employing a batch size of 1024. Recognizing the known slow convergence of transformer models, we extended the training duration to 100,000 steps (Table 3). Remarkably, our low-precision results consistently surpass the full-precision baseline.\nGiven the prevalence of transformers in large language models, extensive research has been dedicated to quantizing transformer models. We compare our findings to other works, and ours stands out as the only method that can surpass the full-precision baseline (Table 4). This achievement highlights the unique strength of our formulation, which not only preserves signal fidelity but also benefits from regularization effects. Several recent works have explored alternative quantization approaches using different datasets, which are not included in this table. One such method is AWQ, a weight-only quantization 4-bit quantization method [25], requires retaining 1% of salient weights and all activations unquantized. Their method also involves searching for an optimal scaling factor and a calibration set. Additionally, BitNet [35], presents a 1-bit quantization method for transformers. The lowest activation precision achieved in their"}, {"title": "5.1.3 Binary Transformers", "content": "In biological neural networks, information is transmitted via electrical impulses called action potentials, or spikes. The complex processes governing spike transmission within the nervous system have been extensively investigated [9]. While binary signals can effectively represent these spike trains, the learning rule for spikes remains an open question [34, 39].\nInspired by the temporal nature of the transformer model, we reduced the activation precision to 1-bit for all linear layers, transforming it into a temporal binary network, akin to a quasi-spiking neural network. However, unlike traditional spiking neural networks, our model doesn't simulate spike generation or consider spiking frequency.\nTo evaluate our approach, we assigned 1, 2, and 4 bits to the weights (Table 5). While introducing perturbations into spiking neural networks during training using backpropagation is not entirely new, our approach differs from previous attempts in several key aspects. Earlier studies have predominantly focused on introducing perturbations only at the spikes [23]. In contrast, our formulation introduces perturbations during signal quantization, irrespective of the spikes. This mirrors the inherent noisiness of the learning process and aligns with the biological reality of neural networks, where noise is an intrinsic part of neural signaling. Our results showcase that these converted binary transformers remain highly competitive with full-precision counterparts.\nA recent study attempted to binarize transformers [43]. Their approach included extra normalization layers, clipping, and progressive quantization during training. We compared our method to their A1W1 configuration (Table 1, BMT-6 in their work, trained with 200k steps), achieving significant improvements (Table 5, last column)."}, {"title": "5.2. Quantization and Sparsification", "content": "In this section, we assess the effectiveness of our sparsification through perturbation proposal by comparing it to a baseline approach that employs a multiplicative mask, Wsparse = w\u2299 1|w|>threshold, obtaining significant improvements. Additionally, we evaluate the performance of combining the proposed sparsity technique with quantization. Our findings indicate that moderate levels of sparsification perturbations introduce beneficial regularization effects during training, leading to improved BLEU scores. These findings are summarized in Table 6. Our supplementary section 8 further explores the integration of structured sparsity, demonstrating how its combination with binarization techniques yields sub-1 bit models that achieve competitive performance.\nQuantization and sparsification are both valuable approaches for compressing neural networks, but both can also introduce performance degradation. Therefore, carefully balancing these techniques is crucial to achieve the"}, {"title": "5.2.1 Block Size and \u03bb", "content": "In addition to precision and sparsity, block size also presents a trade-off between accuracy and efficiency. We observe that its influence is more pronounced at extremely low precision levels, such as 1-bit. While using a smaller block size can improve performance, the effective bits per element can easily exceed the original design. We provide some comparisons here. Considering this trade-off, lower precision models may not always be more efficient (Table 7). Firstly, the accuracy achieved with smaller blocks in an A1W1 models may not surpass the accuracy achieved using A2W2. The perturbations introduced during lower precision training often remain high, hindering the achievement of high quality. Optimal selection needs to be made based on the underlying hardware support and the problem of interest. Our work facilitates a comprehensive study of this trade-off.\nThe parameter \u03bb suppresses the impact of perturbations and prevents training explosion, especially during the transition to 1-bit quantization. Standard quantization approaches often omit the use of \u03bb in an attempt to minimize quantization error. However, this can lead to training divergence in the early stages, as demonstrated in Fig. 1(a). We compared with the classic inverse scaling-based quantization implementation in the AQT library [1] to observe this effect. In our experiments, we observe that a wide range of \u03bb values yield satisfactory results (Table 8). However, our preference set on the safer side, and use \u03bb = 0.01 for all settings. For higher precision (\u2265 4-bits), smaller \u03bb values, such as 0.0001, can be safely used to allow more signals to pass through without causing training instability. Extremely small \u03bb can cause numerical instability, particularly exacerbated by the inherent numerical noise introduced by neural network operations."}, {"title": "5.2.2 Limitations", "content": "As demonstrated in Section 5.2.1(Table 7), quantization algorithms generally achieve the highest accuracy with small sub-channel quantization blocks. However, the data reshaping required for these blocks can create a performance bottleneck, especially in low-precision settings where integer matrix multiplications are relatively fast. Furthermore, the ridge regression computations add additional overhead. To achieve an efficient implementation in practice, careful low-level optimization and consideration of the underlying hardware capabilities will be crucial."}, {"title": "6. Summary", "content": "Discontinuous operations such as quantization and sparsification pose a significant challenge to backpropagation training, as they introduce non-differentiability into the learning process. This non-differentiability has long been considered the Achilles heel of backpropagation, hindering the development of efficient and accurate neural network training algorithms. We address this challenge by re-framing these discontinuities as perturbations. This allows us to introduce a novel, continuous control mechanism that seamlessly handles non-differentiability during training. Our approach is expected to significantly facilitate the deployment of large-scale neural network models on resource-constrained devices such as mobile phones, enabling the widespread adoption of deep learning for mobile applications. Additionally, our technique holds promise for the development of biophysically plausible neural networks, which have the potential to revolutionize artificial intelligence and machine learning."}, {"title": "7. Acknowledgements", "content": "We extend our sincere gratitude to Abhijit Ogale,\nJeff Dean, Jian Li, Kyuyeuan Kim, Mark Sandler, Rasmus Larsen, Sameer Agarwal, Sanjiv Ku-mar, Tammo Splank for their insights and supports."}, {"title": "8. Ternary Weights via Structured Sparsity", "content": "Our sparsification method flexibly handles M:N structured sparsity, enabling a class of ultra-low precision models (even below 1-bit) by combining quantization and sparsity. First, we introduce perturbations to enforce an M:N sparsity constraint (for simplicity, we set N = 4 and ME 1,2,3). Non-zero values are then quantized to -1,1 by taking their sign. This structured sparsity effectively introduces an extra bin of 0, resulting in ternary weight encoding.Since model weights typically have a close to 0 mean, during weights reconstruction, we reformulating the ridge regression without the bias term b:\n\n\\( \\underset{a}{min} \\frac{1}{2N} \\sum_i |aq_i -x_i|^2 + \\frac{\\lambda}{2} a^2\\)\n\nAnd for each quantization block of 128, we solving for the scaling factor only:\n\n\\(a = \\frac{\\bar{qx}}{\\bar{q^2} + x} \\)\n\nWe applied this method to a transformer model trained on the WMT DE-EN dataset with 4-bit activations and ternary weights. Our results show that this 2:4 ternary implementation achieves comparable performance (Table 9) with dense binary weights.\nRegarding storage efficiency, encoding 1:4 structured sparsity necessitates only 2 bits for every group of 4 elements (due to one non-zero position within each group). 2:4 sparsity demands 4 bits per 4-element block, resulting in storage requirements on par with dense binary weights. Importantly, the 1:4 structured sparsity design yields a remarkably efficient 0.5-bit per element model that maintains competitive performance (as shown in Table 9). Moreover, the additional zero-bin inherent to structured sparsity contributes to both diminished storage needs and enhanced outcomes for this class of models (compared with Table 6)."}, {"title": "9. Lower Precision Floats", "content": "In practical implementations, our scaling factor and bias demonstrate resilience to lower precision representations. Empirical evidence confirms that utilizing float8 (E5M2) precision does not adversely affect the accuracy of our results. This robustness can be interpreted as introducing minor perturbations to the scaling and bias values.\nLeveraging this property, we can further reduce the sub-channel block size to 32 elements while maintaining a storage overhead of less than 1 bit per element. This approach"}, {"title": "10. Connection with the Straight-Through Estimator", "content": "Unlike traditional straight-through estimators (STEs) that rely on defining backward gradients [3, 41], our novel approach directly incorporates discontinuous operations into the forward pass. This is effectively a forward implementation of the STE. Our denoising reconstruction process explicitly mitigates the disruptive effects of these discontinuities.\nWe experimented with replacing the additive noise term (Eq. 2) with a controlled multiplicative activation function: d(f(x)) = s \u00b7 f(x) where s = stop-gradient(f(x)) represents element-wise precomputed scaling values. This approach aimed to eliminate explicit gradient scaling during backpropagation while incorporating quantization directly into the activation function. However, our experiments did not reveal any significant performance gains resulting from this modification."}, {"title": "11. Implementation through Integer Matrix Multiplication", "content": "Consider quantizing the matrix multiplication: \\(Y_{nxo} = X_{nxi} W_{ixo}\\). We use \\(\\cdot\\) for the regular dot product, and \\(\\odot\\) for the Hadamard (element-wise) product. Quantized matrix multiplication computes:"}]}