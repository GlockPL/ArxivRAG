{"title": "Personalized Dynamic Difficulty Adjustment \u2013\nImitation Learning Meets Reinforcement Learning", "authors": ["Ronja Fuchs", "Robin Gieseke", "Alexander Dockhorn"], "abstract": "Abstract-Balancing game difficulty in video games is a key task\nto create interesting gaming experiences for players. Mismatching\nthe game difficulty and a player's skill or commitment results in\nfrustration or boredom on the player's side, and hence reduces\ntime spent playing the game. In this work, we explore balancing\ngame difficulty using machine learning-based agents to challenge\nplayers based on their current behavior. This is achieved by a\ncombination of two agents, in which one learns to imitate the\nplayer, while the second is trained to beat the first. In our demo,\nwe investigate the proposed framework for personalized dynamic\ndifficulty adjustment of AI agents in the context of the fighting\ngame AI competition.\nIndex Terms-Imitation Learning, Reinforcement Learning,\nDynamic Difficulty Adjustment, Fighting Game AI", "sections": [{"title": "I. INTRODUCTION", "content": "Engaging players with an appropriate challenge is fun-damental to a rewarding gaming experience. A study by\nHagelback and Johansson [1] has shown that static difficulty\nsettings often fail to adapt to individual skill levels, leading\nto frustration for beginners or boredom for veterans. At the\nsame time, players reported games against an opponent that\nadapts to their performance to be more enjoyable. Dynamic\nDifficulty Adjustment (DDA) techniques [2] address this\nissue by adjusting difficulty based on a player's performance,\ntherefore, aiming to keep them in a constant state of flow [3].\nThis paper proposes a novel DDA technique, which aims\nto train a personalized opponent based on the player's current\nbehavior. While deep reinforcement learning (DRL) has been\nshown to result in human-compatible performance in a variety\nof gaming tasks [4], it is known to converge slowly, previously\nmaking it unsuitable for a real-time scenario. To overcome this\nissue, we implement a two-step process, in which one agent\nis asked to imitate the player's behavior, and a second agent\nis trained to compete against the first. At specific intervals,\nthe player's current opponent is replaced with the second\nagent, presenting the player with a customized challenge. The\nreplacement occurs after a specified number of observation\nsteps. For this initial study, we swapped the agents after each\nsingle step in order to be suitable for short play sessions.\nIn the following, we shortly present related work on\nDDA (Section II) before introducing our proposed agent\nmodel (Section III). A preliminary evaluation (Section IV) has\nshown promising results in terms of reported player satisfaction."}, {"title": "II. BACKGROUND AND RELATED WORK ON DDA", "content": "To achieve a state of flow [3], the developers' objective\nis to provide players with sufficiently challenging activities\nthat result in interesting and meaningful interactions [5]. DDA\napproaches often trade the designers' control of the experience\nwith the game's freedom in creating new and interesting\nchallenges on the fly.\nIn this work, we expand on previous approaches, which\nheavily focus on generating personalized levels (e.g. [6]).\nWe not only parameterize an existing AI agent but propose\na method to learn an AI opponent's behavior from scratch.\nDesigners are freed from creating flexible AI opponents but\nmust still identify player performance variables and game\nmechanics affecting difficulty, as outlined by Hendrix et al. [7]."}, {"title": "III. PERSONALIZED DYNAMIC DIFFICULTY ADJUSTMENT", "content": "To allow for a personalized dynamic difficulty adjustment\n(PDDA) experience that adapts to the player's skill, we propose\na framework combining imitation learning and reinforcement\nlearning agents. Seamless integration of our PDDA machine\nlearning-based opponent is achieved through a combination of\nthree agents:\n\u2022 Opponent Agent: The agent the player is currently\nplaying against, which is to be replaced seamlessly while\nplaying the game.\n\u2022 Imitation Learning Agent: An agent that observes the\nplayer's behavior and learns to replicate its actions.\n\u2022 Reinforcement Learning Agent: An agent trained to win\nagainst the imitation learning agent.\nStarting with a simple rule-based opponent agent, we collect\nobservations on the player's actions. The opponent agent is\nreplaced with the current reinforcement learning agent at fixed\nintervals. Longer intervals will result in less frequent changes\nin the opponent, giving the training process more time. This\ncan result in stronger agents to compete against but comes at\nthe cost of less dynamic interactions.\nOnce enough observations have been stored, the imitation\nlearning agent is trained to reproduce the player's actions. For"}, {"title": "IV. PRELIMINARY EVALUATION OF PDDA", "content": "This study serves as an initial exploration into the feasibility\nof our approach. To evaluate it, we implemented the proposed\nPDDA model in the context of the FightingICE framework [11].\nThroughout our study, the imitation learning agent received\nthe relative position of both characters and the actions they\ncurrently perform, as well as the player's input. When predict-ing player actions, we achieved an accuracy of 82-87 % on\nthe training set with the imitation learning agent (we report a\nrange of values due to the stream mining setup).\nFor the reinforcement learning agent's input, we chose to\nuse the current screen encoded as a 96 \u00d7 64 grayscale image.\nHere, we augmented the image by encoding the characters' hit\npoints and energy levels in the top left and top right corners\nof the screen using the grayscale value of two pixels each. For\nthe sake of simplicity, we used the implementation's default\nconvolutional neural network (CNN) policy 2. During training,\nwe gave the agent a positive reward for decreasing the other\ncharacter's hit points and a penalty for losing its own.\nIt's important to note that due to time constraints, our study\nwas conducted with a sample size of 5 participants, who each\nplayed three games against the MCTS agent provided by"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "The framework for personalized dynamic difficulty adjust-ment presented in this work provides the opportunity to\nchallenge players according to their individual skill levels. The\ncombination of an imitation learning agent and a reinforcement\nlearning agent results in a personalized system for DDA that\nrequires minimal setup by the designers. In the future, we aim to\nexpand on the evaluation of the proposed PDDA by increasing\nthe number of study participants and analyzing the system's\nimpact on their perceived difficulty and player satisfaction for\ndifferent opponent agents. Furthermore, the implementation\nof more complex imitation learning agents could provide a\nbetter approximation of the player's behavior and may result\nin stronger opponents to match more experienced players.\nExtending this work to high-difficulty games, as well as\nexploring the accuracy of the agent to meaningfully generalize\nto the human player is a future consideration."}]}