{"title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning", "authors": ["Zexiong Ma", "Chao Peng", "Pengfei Gao", "Xiangxin Meng", "Yanzhen Zou", "Bing Xie"], "abstract": "Mainstream issue-resolving frameworks pre- dominantly rely on commercial models, lead- ing to high costs and privacy concerns. Existing training approaches for issue resolving strug- gle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine- Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localiza- tion, line localization, and code edit genera- tion. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE- Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen- 7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to com- mercial models.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs)(OpenAI, 2023; Touvron et al., 2023) have demonstrated excep- tional performance across a wide range of complex real-world tasks (Li et al., 2022, 2024; Wu et al., 2023), particularly excelling in software develop- ment (Jimenez et al., 2024; Zhao et al., 2024; Ma et al., 2024c). The current mainstream automated software development systems mainly use commer- cial models (OpenAI, 2023; Cla, 2024). However, the API call of commercial models are costly and pose privacy leakage issues, limiting their applica- tion in development processes in the industry.\nResearch communities have attempted to fine- tune open-source LLMs (Team, 2024a; Guo et al., 2024; Cod, 2024; Touvron et al., 2023; Hui et al., 2024; Lozhkov et al., 2024) to improve their per- formance in Issue Resolving task. Existing Ap- proaches(Pan et al., 2024; Ma et al., 2024b; Xie et al., 2025a; Ma et al., 2024a) utilize LLMs to sam- ple Chain-of-Thought (CoT)(Wei et al., 2022) data, then perform Negative Sample Filtering based on ground-truth data, and fine-tune the models. While these methods improve LLMs' issue-resolving ca- pabilities, relying solely on supervised fine-tuning (SFT) can lead to poor generalization, making mod- els more susceptible to hallucinations and factual errors.\nRecent studies (Luong et al., 2024; Guo et al., 2025; Zeng et al., 2025; Xie et al., 2025b; Mu et al., 2024; Team et al., 2025) have explored rule- based reinforcement learning to enhance model per- formance on complex tasks such as mathematics."}, {"title": "2 Background", "content": "In this section, we provide a brief introduction to SWE-Bench, issue resolving framework, and the reinforcement learning algorithm.\n2.1 SWE-Bench\nSWE-Bench (Jimenez et al., 2024) is a benchmark to evaluate language models\u2019 ability to resolve real- world software issues, such as bug reports and fea- ture requests on GitHub. LLM-based programming assistants are given an issue description along with the entire repository and are expected to gener- ate code edits that resolve the issue. SWE-Bench Lite (Team, 2024b) is a curated subset of SWE- Bench, specifically focus on evaluating functional bug fixes. While SWE-Bench Verified (OpenAI, 2024c) is a human-verified subset addressing qual- ity issues in SWE-Bench, such as vague problem descriptions. All issue-resolving experiments in this paper are conducted on SWE-Bench Lite and SWE-Bench Verified.\n2.2 Issue Resolving Framework\nIssue resolving frameworks can be broadly divided into two categories: agent-based and pipeline- based. Openhands(Wang et al., 2024b) is a purely react-style (Yao et al., 2022) agent-based framework for software development tasks. Xie et al. (2025a) propose SWE-Fixer, a two-stage pipeline-based system. Ma et al. (2024a) propose SWE-SynInfer, a hybrid framework that combines pipeline design with agent-based approaches. Ad- ditionally, Xia et al. (2024) propose Agentless, a multi-stage pipeline-based framework designed to fully leverage the reasoning capabilities of LLMs for issue resolving. Notably, Agentless is the state- of-the-art (SOTA) pipeline-based framework on the SWE-Bench leaderboard and has been adopted by OpenAI (OpenAI, 2024b) and DeepSeek (Guo et al., 2025) to assess their models\u2019 performance in software engineering tasks. Constructing training data for issue-resolving framework requires sam- pling CoT data for the framework and filtering out negative samples. Assessing the accuracy of inter- mediate steps in the Agent framework is challeng-"}, {"title": "2.3 Reinforcement Learning", "content": "Reinforcement learning algorithms are widely used in the alignment phase of large language mod- els (LLMs), including proximal policy optimiza- tion (PPO) (Schulman et al., 2017), group relative policy optimization (GRPO) (Shao et al., 2024), direct preference optimization (DPO) (Rafailov et al., 2023), and Kahneman-Tversky optimization (KTO) (Ethayarajh et al., 2023, 2024). The PPO algorithm calculates the reward using Equation (1) and subsequently updates the parameters of the Policy Model.\n$r = r_{\\theta}(q, o) - \\beta \\cdot KL[\\pi_{\\theta}(\\cdot|q) || \\pi_{ref}(\\cdot|q)]$\t(1)\nwhere q represents the given question, o refers to the predicted output, $r_{\\theta}$ is the reward model, $\\pi_{\\theta}$ is the policy model, $\\pi_{ref}$ is the reference model (mostly refer to the original policy model $\\pi_{\\theta_{old}}$). As PPO is capable of maintaining stability and efficiency during policy updates, we employ PPO for RL training in this paper."}, {"title": "3 Approach", "content": "As shown in Figure 2, SoRFT contains three parts: (1) issue resolving subtask construction, (2) rejection-sampled supervised fine-tuning, and (3) rule-based reinforcement learning.\n3.1 Issue Resolving Subtasks\nTo address the challenge of constructing end-to- end training data for the Issue Resolving task, we create training data for phased subtasks. As shown in Figure 2(1), inspired by the design patterns of advanced issue-resolving frameworks (Xia et al., 2024; Ma et al., 2024a), we decompose issue re- solving into four subtasks: file localization, func- tion localization, line localization, and code edit generation. This structured decomposition enables a more targeted and effective training process for each phase of the task."}, {"title": "File Localization.", "content": "File localization training en- hances the LLMs' understanding of the high-level architecture of the repository, enabling them to per- form an initial rough localization of relevant files based on the issue description. We utilize the is- sue and repository structure as inputs, with the full names of the modified files in the pull request (PR) as outputs. To improve the quality of the training data, we excluded non-Python files and test scripts from both input and output. The relationship can be formulated as follows:\n$\\texttt{promptfile} (I, R_s) \\rightarrow F_g$\t(2)\nwhere $F_g$ represents the golden file, I represents the issue, $R_s$ represents the repository skeleton."}, {"title": "Function Localization.", "content": "Function localization training can improve the LLMs' performance on fine-grained localization based on the functional characteristics of the code. In function localization, the issue and file skeleton composed of function names are employed as inputs, while the names of modified functions from the PRs are employed as outputs. This relationship is expressed as:\n$\\texttt{promptfunction} (I, F_{g,s}) \\rightarrow FN_g$\t(3)\nwhere $FN_g$ represents the golden function name, I represents the issue, $F_{g,s}$ represents the skeleton of the golden file $F_g$."}, {"title": "Line Localization.", "content": "Line localization training en- hances the LLMs' ability to precisely identify the exact lines of code that require modification to re- solve the issue. Line localization takes the issue description and function content as inputs and out- puts the modified lines from the PR. This can be formulated as:\n$\\texttt{promptline} (I, FN_{g,c}) \\rightarrow L_g$\t(4)\nwhere $L_g$ represents the golden modified line, I represents the issue, $FN_{g,c}$ represents the content of the golden function $FN_g$."}, {"title": "Code Edit Generation.", "content": "Training code edit gener- ation can enhance the LLMs' ability to modify code snippets based on the issue. The input for code edit consists of the issue and the localized code snippet, while the output is the code edits of the correspond- ing PR. Following previous work (Xia et al., 2024; Yang et al., 2024), we employ the Search/Replace edit format. The Search/Replace format consists"}, {"title": "3.2 Rejection-sampled Supervised Fine-Tuning", "content": "We fine-tune the LLM using Rejection-sampled CoT data to enhance its understanding of the task format and reasoning process for each subtask. As shown in Figure 2(2), we sample CoT data using the LLM and then filter the CoT data based on the ground-truth answer. Specifically, for the three localization subtasks, we filter out samples that have no overlap with the ground-truth file, function or line. For the code edit generation subtask, we filter out samples that have no overlap with the lines modified by the ground-truth edits. Finally, we integrate CoT data from all subtasks to fine-tune the LLM, enabling it to comprehend both the task format and its underlying reasoning mechanisms."}, {"title": "3.3 Ruled-based Reinforcement Learning", "content": "We further enhance the reasoning ability and gen- eralization of LLMs on issue-resolving through Rule-based Reinforcement Learning. As shown in Algorithm 1, we utilize the ground-truth answer to calculate the rule-based reward for each subtask. For the localization subtask (line 1-7), we first ex- tract the localization result $O_{loc}$ from the response. If the localization result is empty or contains a target not present in the problem description, the re- ward is set to 0; otherwise, the reward is calculated as the $F_{\\beta}$ score between $O_{loc}$ and the ground-truth answer A. For the code editing subtask (line 8-15), we first extract the modification target $O_{search}$ and the modification code $O_{edit}$ from the response. If the modification code is empty or the modification target does not appear in the problem description, the reward is 0; otherwise, the reward is calculated as the $F_{\\beta}$ score between $O_{edit}$ and the ground-truth answer A. During reinforcement learning period, we replace the reward model in PPO with above rule-based reward. This approach effectively miti- gates the risk of reward hacking, ensuring that the model's learning process is guided by precise and reliable feedback.\n$F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}$\t(6)\n$\\text{Precision} = \\frac{|O \\cap A|}{|O|}$,\t$\\text{Recall} = \\frac{|O \\cap A|}{|A|}$\t(7)\nwhere O represents the outputs generated by the LLMs, A denotes the ground-truth answers for the subtask. $\\beta$ is a hyperparameter that balances the impact of precision and recall on final score. Since recall has a greater influence on the final out- come across different subtasks, $\\beta$ should be a value greater than 1. In our experiments, we set $\\beta = 3$ to prioritize recall while maintaining a reasonable trade-off with precision."}, {"title": "4 Experiments", "content": "In this section, we will introduce our evaluation benchmark, metrics, models, baselines and imple- mentation details.\n4.1 Benchmark\nWe conduct experiments on two issue resolv- ing benchmarks: SWE-Bench Verified and SWE- Bench Lite.\nSWE-Bench Verified (OpenAI, 2024c) is a man- ually verified subset of SWE-bench (Jimenez et al., 2024) with 500 instances. Each instance is a is- sue associated with test cases that can be executed in a Docker environment. Issue resolving frame- works (Xia et al., 2024; Xie et al., 2025a; Wang et al., 2024b; Ma et al., 2024a) are asked to under- stand the issue and the repository, generate patches and pass all test cases, providing a reliable evalua- tion of their issue resolving capabilities.\nSWE-Bench Lite (Team, 2024b) is the subset of SWE-Bench containing 300 instances and focuses on evaluating functional bug fixes.\n4.2 Metrics\nTo evaluate the performance of issue resolving frameworks with SoRFT-trained LLMs, we apply two metrics: % Resolved, % Applied.\n% Resolved is the proportion of samples in which applying the generated code edit success- fully passed all test cases.\n% Applied is the proportion of samples that issue resolving frameworks successfully generate valid code edits that could be applied to the repositories. This metric evaluates the framework's capability to produce practical and executable solutions.\n4.3 Framework and Model\nWe apply Agentless(Xia et al., 2024) as our is- sue resolving framework and Qwen2.5-Coder (Hui et al., 2024) series as our base model. Agent- less is an advanced open-source issue-resolving framework, used by OpenAI (OpenAI, 2024b) and DeepSeek (Guo et al., 2025) to evaluate model per- formance on software engineering tasks. For base model, we employ Qwen2.5-Coder-7B-Instruct and Qwen2.5-Coder-32B-Instruct(Hui et al., 2024), which is the SOTA open-source coder instruct mod- els (Touvron et al., 2023; Guo et al., 2024).\n4.4 Baselines\nSince issue resolving tasks necessitate the use of agent-based or pipeline-based frameworks, exist- ing fine-tuning approaches are typically designed and optimized for specific frameworks. In this work, we evaluate our method against three base- lines: (1) OpenHands with SWE-Gym-Qwen, (2) SWE-Fixer with SWE-Fixer-Qwen, and (3) SWE- SynInfer with Lingma-SWE-GPT."}, {"title": "4.5 Implementation Details", "content": "In this subsection, we will introduce the data con- struction details, fine-tuning details, and reinforce- ment learning details.\nData Construction. To construct our training dataset, we curate a collection of high-quality open-source Python projects from seart-ghs by apply- ing a set of stringent criteria. Specifically, we select repositories that satisfy the following conditions: (1) at least 1,000 issues, (2) at least 1,000 pull requests (PRs), (3) a minimum of 100 stars, (4) inclusion of an appropriate license, (5) exclusion of forked repositories, and (6) absence from the SWE-Bench test dataset to prevent data contamina- tion. Through this rigorous selection process, we identify a final set of 660 repositories. From this pool, we further select 100 repositories to gener- ate the SFT CoT data. We extract 30,000 (issue, PR) pairs from these repositories and formulate corresponding subtasks. We sample CoT data us- ing Claude-3.5-Sonnet (Cla, 2024) and filter out instances where the final answer does not align with the subtask's ground truth, retaining 60k train- ing samples. We also crawl the (issue, PR) data from the remaining repositories and construct the corresponding subtasks. During the reinforcement"}, {"title": "5 Results and Analysis", "content": "SoRFT achieves SOTA performance among open-source LLMs. Table 1 presents the per- formance of SoRFT and fine-tuned open-source LLMs on the issue-resolving task on SWE-bench Verified and SWE-bench Lite. We categorize the open-source models into two groups based on their parameter sizes: (1) 7-14B open-source LLMs, and (2) 32-72B open-source LLMs. The LLMs trained with SoRFT achieved state-of-the- art (SOTA) performance among models of the same parameter size and even slightly outperforms some larger models. On SWE-bench Verified, SoRFT-Qwen-7B outperforms SWE-Gym-Qwen- 32B (21.4 vs. 20.6). SoRFT-Qwen-32B even out- performs Lingma-SWE-GPT-72B (30.8 vs. 30.2), despite the latter having significantly more parame- ters.\nWhile OpenHands achieves optimal perfor- mance with proprietary models, the SWE-Gym model, specifically fine-tuned for OpenHands, un- derperforms compared to others. This discrepancy may arise from the challenges of constructing su- pervision signals for intermediate steps in agent"}, {"title": "6 Related Work", "content": "LLM Training for Issue Resolving. To enhance the issue resolving capabilities of open-source LLMs, several research works (Ma et al., 2024a; Xie et al., 2025a; Ma et al., 2024b; Pan et al., 2024) have attempted to use software develop- ment resources from the open-source commu- nity to construct training data and fine-tune open- source LLMs. Pan et al. (2024) crawled open- source repositories and utilized closed-source mod- els (e.g., GPT-4o (OpenAI, 2024a) and Claude-3.5- Sonnet (Cla, 2024)) to generate Openhands (Wang et al., 2024a,b) Agent trajectories, and filtered them through unit tests. Then they used the trajectories to fine-tune the Qwen (Hui et al., 2024) model, enabling it to serve as the base model for Open- hands. Ma et al. (2024a) used GPT-4o to generate Agent trajectories on open-source repository issues, and fine-tuned an open-source model with the fil- tered trajectories. Pan et al. (2024) generated CoT data for and edit generation tasks using GPT-4o, and fine-tuned an open-source model to apply it to SWE-Fixer RAG pipeline. All the above work used SFT to fine-tune models. To the best of our knowledge, we are the first work to leverage rein- forced fine-tuning (Luong et al., 2024) to enhance the issue-resolving capabilities of LLMs.\nReinforcement Learning with Rule-based Re- ward. Since OpenAI released ol (OpenAI, 2024b) model, many efforts have attempted to enhance LLMs' long-form reasoning capabili- ties through rule-based reinforcement learning. DeepSeek's R1 (Guo et al., 2025) model with rule- based GRPO (Shao et al., 2024) further demon- strates the potential of rule-based rewards. Team et al. (2025) released Kimi-k1.5, also trained with rule-based reinforcement learning. The research community (Zeng et al., 2025; Xie et al., 2025b) has also been working on replicating rule-based reinforcement learning process. Pan et al. (2025) trained a 3B model with PPO (Schulman et al., 2017) on the Countdown task and observed \u201cAha moment\u201d (Guo et al., 2025) phenomenon, aligns closely with the behavior of R1. Zeng et al. (2025) trained a 7B model using PPO on Math task and observed that response length initially decreased and then increased (similar as the ten- dency in Figure 3b). Previous work mainly fo- cused on mathematical tasks, where rewards can be straightforwardly computed based on ground truth. In this paper, we improve the performance of open-source models in issue-resolving framework through subtask-oriented rule-based reinforcement learning."}, {"title": "7 Conclusion", "content": "In this paper, we propose SoRFT, a subtask- oriented reinforced fine-tuning approach that en- hances LLMs' issue-resolving capabilities. By leveraging rule-based reinforcement learning, SoRFT improves performance while ensuring bet- ter generalization. Our results demonstrate its ef- fectiveness as a cost-efficient alternative to com- mercial models."}, {"title": "8 Limitations", "content": "The False Negatives of Rule-based Rewards. The correct resolution to an issue is often not unique. Relying solely on rule-based rewards by comparing the LLM's response to a single ground truth may incorrectly classify valid solutions as fail- ures. To address this limitation, future work could incorporate unit test execution results as a more objective and fair measure of code edit quality.\nExperiments were conducted only in the Python repositories. Due to the lack of a multilingual SWE-Bench test set, our experiments were limited to Python repositories. However, since SoRFT is a language-agnostic framework, we believe it has the potential to improve the issue-resolving capabilities of LLMs in other languages."}]}