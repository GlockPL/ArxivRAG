{"title": "Probing the Robustness of Theory of Mind in Large Language Models", "authors": ["Christian Nickel", "Laura Schrewe", "Lucie Flek"], "abstract": "With the success of ChatGPT and other similarly sized SotA LLMs, claims\nof emergent human like social reasoning capabilities, especially Theory\nof Mind (ToM), in these models have appeared in the scientific literature.\nOn the one hand those ToM-capabilities have been successfully tested us-\ning tasks styled similar to those used in psychology (Kosinski, 2023). On\nthe other hand, follow up studies showed that those capabilities vanished\nwhen the tasks were slightly altered (Ullman, 2023). In this work we intro-\nduce a novel dataset of 68 tasks for probing ToM in LLMs, including poten-\ntially challenging variations which are assigned to 10 complexity classes.\nThis way it is providing novel insights into the challenges LLMs face with\nthose task variations. We evaluate the ToM performance of four SotA open\nsource LLMs on our dataset and the dataset introduced by Kosinski (2023).\nThe overall low goal accuracy across all evaluated models indicates only\na limited degree of ToM capabilities. The LLMs' performance on simple\ncomplexity class tasks from both datasets are similar. Whereas we find a\nconsistent tendency in all tested LLMs to perform poorly on tasks that\nrequire the realization that an agent has knowledge of automatic state\nchanges in its environment, even when those are spelled out to the model.\nFor task complications that change the relationship between objects by re-\nplacing prepositions, we notice a performance drop in all models, with\nthe strongest impact on the mixture-of-experts model. With our dataset\nof tasks grouped by complexity we offer directions for further research on\nhow to stabilize and advance ToM capabilities in LLM.", "sections": [{"title": "1 Introduction", "content": "Theory of Mind (ToM) - the ability to track the concealed mental states of others, en-\ncompassing knowledge, intentions, beliefs, and desires - is considered a facet of social\nintelligence Heyes & Frith (2014); Zhang et al. (2012); Blatt et al. (2010); Swim & Bloodhart\n(2015). It might help with many applications, for instance programming or chatbot assis-\ntance. Whether, as claimed in Kosinski (2023) ToM emerged without specific training, just\nthrough increasing the size of the model and training data is an open question. Large lan-\nguage models (LLMs) seem to exhibit ToM capabilities when evaluated on simple tasks\ncommonly used in psychology, namely unexpected transfer or unexpected content tasks\n(Kosinski, 2023). But did the model really learn how other agent's minds work and what\nthey think or just linguistic patterns present in standard ToM tasks? Ullman (2023) suggests\nthat the LLMs do not exhibit real ToM capabilities, since they vanished when prompted\nwith slight variations of the original tasks. While Ullman (2023) introduces certain cate-\ngories of complications that are challenging for LLMs, besides a few examples the authors\ndo not publicly provide a large dataset that would allow more systematical research.\nThe main contribution of this work is the creation of a novel ToM benchmarking dataset\nconsisting of manually crafted unexpected content and unexpected transfer tasks based\non 10 complexity classes. Furthermore we evaluate our new dataset on four SotA LLMs."}, {"title": "1.1 Related Work", "content": "Several ToM studies and datasets have been released since the original work of Kosinski\n(2023) and Ullman (2023), yet show notable differences from our approach. Sartori & Orr\u00f9\n(2023) emphasize the potential usefulness of the investigation of LLM capabilities, like ToM,\nor their errors on related tasks for research into human cognition and biases. The newly\npublished FANTOM dataset and paper (Kim et al., 2023) focus on dynamic social interac-\ntions. Another recent work, \"ToMBench\u201d (Chen et al., 2024) is a dataset of multiple-choice\ncognition tasks, evaluated by the authors on several LLMs, leading them to the statement\nthat \"even the most advanced LLMs like GPT-4 lag behind human performance by over\n10% points, indicating that LLMs have not achieved a human-level theory of mind yet\".\nLiterature on the basics on how models might not really learn specific abilities, like ToM for\ninstance, but much rather \"only\u201d linguistic patterns present in the tasks, possibly through\n\"contaminated\u201d training data, resulting in a \"stochastic parrot\" is (Bender et al., 2021). An-\nother critical acount on the ToM abilities can be found in (Shapira et al., 2023), in which the\nauthors conduct experiments using 6 tasks probing into different aspects of ToM."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Overview", "content": "In order to measure the Theory of Mind (ToM) performance of Large Language Models on\nbasic ToM tasks as well as variations derived from the these basic tasks, we first manually\ncreate a novel dataset. Each task variation is assigned one out of 10 complexity classes per-\ntaining to similar kinds of challenges we introduced in said variation. Of course the correct\nsolutions to the tasks are also included. Besides the data necessary for the task at hand\nthe dataset also already entails the belief, which the protagonist of the tasks holds after\neach sentence. This might be of interest for further research into Chain-of-Thought (CoT)\n-reasoning and -faithfulness in the context of ToM. In the next step we prepend instructions,\nmeant to make the LLM output more compatible with machine evaluation, to the task and\nadminister the resulting prompt to four State-of-the-Art (SotA) LLMs. We apply prompt\ntuning techniques as described by Bsharat et al. (2024). Nevertheless the actual LLM output can be complex and we first have to\nextract the machine-evaluable \"final-answer\" with our extraction function. Then we are\nable to determine which task has been answered correctly. We report the overall- as well\nas the per complexity-class-performance of each model. We investigate which alterations\nof the tasks might be the most challenging to the models.\nTo understand our dataset it is easiest to imagine a stage play. Each play takes place on\nits own stage and is comprised of one or several stage settings respectively sceneries. For\nevery scene taking place in said scenery there might be several props placed on that stage.\nWhich props are placed and their positions might differ from scene to scene. Now imag-\nine the stage play has a very experimental approach. In order to make the play interactive\nand captivating after each scene the audience is asked questions about what just happened,\nwhat was where on stage and what the protagonists were thinking. This can be hard as ob-\njects containing others might be opaque and incorrectly labelled (ToM unexpected contents\ntask) or transfers of objects might have taken place without the protagonist being present\n(ToM unexpected transfer tasks). Questions about the protagonists believes are Theory\nof Mind questions, whereas the others are checking a general understanding of the scene.\nThey might serve as sanity checks whether the member of the audience truly understands\nboth, the believes of the protagonists and the real world states of the scene or is just giving\nthe \"less\" obvious answer to weirdly easy questions. This mental model is the inspiration\nfor the structure of our dataset."}, {"title": "2.1.1 Dataset Creation and Outline", "content": "We create seven different stage plays (or short stages). The basic idea is that within the\nsame stage several sceneries (tasks) and scenes (activities or arrangement of objects) can\ntake place. About each each scene we might ask ToM or general understanding or spatial\nreasoning questions (figure 1 ). For each one of the 10 complexity categories we defined be-\nforehand, we manually alter the text in the way characteristic to the respective complexity\nclass. For example, for the complexity class named \"transparent container\", we take the\ninitial stage text and replace every mention of \"intransparent paper bag\" with \"transparent\npaper bag\" and adapt other parts of the text where necessary. Each of these \"complications\"\nof a stage, including the unaltered version, is called a scenery. Note that for some stages we\nnoticed that some complexity categories are not applicable because the alterations would\nnot make sense in the context of the plot. Therefore the dataset consists of 68 sceneries in\ntotal (and not 7 x 11 sceneries). We generate 16 sub-tasks, called scenes.\nThe creation of the scenes follows the procedure by Kosinski (2023). We apply object swaps,\nadd a true belief version, a version where the protagonist gets informed about the true\nworld state and a version where the true world state in visible. Additionally to asking for\nthe protagonists belief, versions that ask for the true world state are introduced, to test\nthe LLMs understanding of the scene. As a result we obtain 68 x 16 = 1088 scenes with\ntheir corresponding correct solutions. Regarding formal requirement on the texts to avoid\nunintended hints for the LLM we followed the principles used in the instruction given to\nresearch assistants by Kosinsky. For instance we made sure, that the key words appear\nexactly the same number of times to prevent biasing the LLMs' output probabilities. Out\nof our seven stages, four represent unexpected content tasks and three are unexpected\ntransfer tasks."}, {"title": "2.1.2 Complexity Classes", "content": "Additionally to the five complexity classes introduced in Ullman (2023), we add the fol-\nlowing five new complexity classes. Examples for each class can be found in the appendix\nA.1."}, {"title": "2.2 Models and Inference", "content": "We evaluate our dataset on four open source transformer-based state-of-the-art models\nLlama-2-70-b-chat-hf (Touvron et al., 2023) (70B parameters), Vicuna-33b-v1.3 (Vic) (33B\nparameters), Yi-34B-Chat (AI et al., 2024) (34B parameters) and Mixtral-8x7B-Instruct-v0.1\n(AI, 2023), which is a mixture of experts (MoE) model consisting of 8 models with 7B pa-\nrameters each.\nWe use a temperature of 1.0 and set the number of maximum output tokens to two times\nthe number of tokens in the respective model input to scale it dynamically based on the\ninput length."}, {"title": "2.3 Evaluation Metrics", "content": "Using two recursive functions we first identify the words that constitute said answer and\nthen evaluate it against the manually crafted solution given in the dataset. This results in\na boolean value for each answer, namely whether it is \"True\" (i.e. correct) or \"False\" (i.e.\nincorrect). To account for variation in the LLM output like different spelling or spacing we\ntransform the extracted answer to a standardized form before the correctness check. We\nrelax the correctness condition from exact string matching to string inclusion, while we also\nensure the wrong solution is not present in the LLMs' answer. We call the rate of correctly\nanswered scenes turn accuracy. To get a more meaningful metric for the performance than\njust the correctness of individual sceneries (turn accuracy), we introduce the notion of goal\naccuracy. We call a scenery answered goal accurate if all scenes belonging to the scenery\nhave been answered correctly. Thus we ensure a comprehensive understanding of the\ngiven situation. We also calculate these measures for the complexity classes in order to\nfind out whether certain categories are easier or more challenging to the models tested."}, {"title": "3 Results", "content": "Generally we find a better-than-coin-toss performance on all four models tested (figure 2\nand table 1) when evaluating on a per scene basis, for which we calculate the rate of correct\nanswers in all answers given, namely the turn accuracy.\nWhereas when we group all questions asking about the same scene (goal accuracy), we\nfind that most of the time none of the models are able to answer every question about a\nsingle scene correctly (figure 3 and table 2)."}, {"title": "3.1 Llama-2-70-b-chat-hf", "content": "The largest of the Llama 2 models exhibits the overall best performance on our dataset.\nAcross all complexity classes of tasks it hits the correct answer approximately 70% of the\ntime (figure 2). With exception of the class \"automatic change knowledge\" which is an-\nswered correctly only 53.75% of the time. The overall turn accuracy is 73.71%. The best\nperformance was exhibited in \"conclusion from sentiment\" with 81.25%. When examin-\ning the performance with regards to goal accuracy, we find that the model does not once\nachieve it in most complexity classes, including the \"no complication\" class. Interestingly\nthe class \"automatic state change\u201d, which performed poorly in turn accuracy, is the best\nperforming here with 20% goal accuracy, followed by \"induction from baseline\" with a\nrate of 16.67% and \"transparent container\u201d with 14.29% (figure 3)."}, {"title": "3.2 Vicuna-33b-v1.3", "content": "Being a relative of Llama 2, but with approximately half the number of parameters we find\nVicuna-33b-v1.3 to perform worse. The overall turn accuracy is 58.00%. \"No complication\u201d\ntasks are solved in 74.00% of the cases. The performance of the complexity classes ranges\nfrom the again worst performing class \"automatic state change\u201d with 38.75% to \"Induction\nfrom baseline\" with 69.79% (figure 2). Taking a look at goal accuracy Vicuna has the lowest\nperformance of all models evaluated. For none of the sceneries Vicuna was able to answer\nall questions correctly (figure 3)."}, {"title": "3.3 Mixtral-8x7B-Instruct-v0.1", "content": "As a MoE model Mixtral is similarly sized as Llama-2-70b. The model's performance (fig-\nure 2) on our dataset is third best with an overall turn accuracy of 68.47% and a \"no compli-\ncation\" turn accuracy of 75.00%. With regards to complexity classes yet again \"automatic\nchange knowledge\u201d seems to be the most challenging with a turn accuracy of 48.75%. The\nmost consistently correctly solved class is \"induction from baseline\" with 76.04%. The only\ninstance of goal accuracy we find in the class \"induction from baseline\" in 16.67% of the\ncases. This results in an overall goal accuracy of 1.47% (figure 3)."}, {"title": "3.4 Yi-34B-Chat", "content": "The final model we evaluate on our dataset is Yi-34B-Chat. With regards to overall turn\naccuracy being 72.89% it is the second best performing model tested. As shown in figure\n2 \"no complication\u201d turn accuracy is 75.89%. The worst performing complexity class is as\nwith the previous models \"automatic change knowledge\" with 46.25% turn accuracy. The\nbest performing is \"uninformative label\" with 79.69%. Yi achieves goal accuracy in two\nclasses with each instance 14.29% frequency resulting in an overall goal accuracy of 4.41%\n(figure 3). In this regard Llama 2 and Yi are on the same level."}, {"title": "3.5 Dataset Baseline", "content": "To establish a baseline of comparison for our results we not only calculate the expected\nresults if each answer was generated by a coin toss. We also evaluated the four LLMs\non the dataset used in (Kosinski, 2023). All four models perform slightly better on our\ndataset compared to the former dataset. The highest performance difference is found for\nthe Yi-34B-Chat model with a difference of almost 10%. For the goal accuracy only the\nYi-34B-Chat model has a success rate of 14% on our dataset. The other models have a goal\naccuracy of 0%. In comparison, both Llama-2-70B and Mixtral-8x7B show some successful\nresults on the baseline dataset."}, {"title": "4 Analysis", "content": "The overall performance of the evaluated LLMs is significantly better than the expected\nbaseline for the turn accuracy of 50%, assuming a coin toss selecting the answer among\nthe two plausible objects or positions mentioned in the stories. The number of cases where\nthe LLM answer does not match one of the two plausible objects is negligible. The turn ac-\ncuracy suggests, that the tested LLMs perform slightly better on our dataset compared to\nthe dataset used by Kosinski (2023), though the difference is marginal. However to assess\nthe models \"understanding\" of a scene the measure of goal accuracy is preferable as it re-\nquires all scenes relating to the same scenery to be answered correctly. The results reported\nby Kosinski (2023) also use this measure. Barely any scene can be completely solved, thus\nthe goal accuracy is consistently low for any complexity class. This implies that the models\nlearned some linguistic patterns that make it possible to solve more than half the actual\nquestions asked, but possess no robust Theory of Mind. Due to the low rate of goal accu-\nracy, which for most complexity classes is zero, it is also hard to tell differences between\nthe difficulty of complexity classes with regards to the models performances. Nevertheless\nit should be noted that the coin toss baseline for goal accuracy, that is answering all 16\nquestions asked about a single scenery correctly, is estimated as $\\frac{2}{16} = 1.53 \\cdot 10^{-5}$, which\nLlama2-70B, Mixtral-8x7B and Yi-34B supersede with an overall goal accuracy between\n1.47% and 4.41% and a per complexity class goal accuracy up to 4.41%. As we know from\nthe previous study consisting of only \"no complication\u201d tasks, larger LLMs (late GPT-3.5\nand GPT-4) can actually achieve higher rates in goal accuracy that come close to the per-\nformance of a 7 year old child. Thus on those larger models we might see more interesting\npatterns with regards to the challenge posed by the different complexity classes of tasks.\nFurthermore we note that in this study even tasks with difficult complications sometimes\nare solved \"goal accurately\u201d. This is why we suspect that the complications suggested in\nUllman (2023) might not by their nature be unsolvable for LLMs, but might get solvable by\nmore advanced models in the future.\nEven though the models exhibit no consistent ToM abilities they still answer many ques-\ntions correctly. This is why we can still attribute different degrees of challenge to the com-\nplexity classes. While we see some variation in most complexity classes the impressive\ndrop in performance for \"automatic change knowledge\u201d is evident. We think this might\nbe due to the transfer nature of the task involving several steps of thought. First the LLM\nneeds to recognize that the protagonist holds a specific assumption about the dynamics\n(automatic changes) in the surrounding world. Next it needs to recognize that this mental\nmodel will be applied by the protagonist. Finally the model needs to compute the pre-\ndiction of that mental model of the protagonist with regards to the story. One is likely to\nproduce details in this kind of task that the LLM has not encountered during training. This\ncomplexity class might be most interesting to try out using Chain-of-Thought-Prompting\nin order to alleviate those challenges.\nAn interesting phenomenon can be observed with the two complexity classes of \"transpar-\nent container\" and \"preposition replacement\u201d. Both categories of tasks basically deal with\nthe property whether an object that is somehow placed inside or on top of a container is\nvisible or invisible to the protagonist. On the one hand, given a transparent container task\nVicuna shows a drop in turn accuracy. While Mixtral seems to recognize the transparent\nproperty of the container and consequence that the protagonist can see what is inside. Thus\nMixtral seems to be more capable in dealing with such details. On the other hand when\nadministered the similar complication of a replaced preposition, let's say an object is not\nplaced inside of a container, but rather on top of it such that the protagonist can see it, both,\nVicuna and Mixtral, exhibit a sharp decline in turn accuracy (2). The decline in Mixtral's\nability to answer correctly is intriguing. It may be an artifact of poor spatial reasoning capa-\nbilities of LLM - which is not required for the transparent container tasks -, which is more\npronounced with \"smaller\" models. As an, albeit rather large model in total, Mixtral is in\nfact a mixture of \"smaller\u201d experts, hence the label of \"8x7B\u201d in the model's name. We sup-\npose that the underperformance in spatial reasoning of smaller models can't be alleviated\nby the Mixture of Experts approach.\nConsidering the turn accuracy as well as the goal accuracy the performance of Yi-34B is\nvery close to the performance of Llama2-70B, even though it has almost twice the number\nof model parameters. The similar sized Vicuna-33B has significantly lower performance.\nThis indicates that the model size alone does not tell much about ToM capabilities."}, {"title": "5 Discussion and Future Work", "content": "The low goal accuracy across all tested models makes it hard to make meaningful con-\nclusions except that the models exhibit only very limited ToM capabilities and that the\n\"automatic state change\u201d seems to be the most challenging complexity class. As mentioned\nabove employing Chain-of-Thought prompting seems especially promising for this class.\nOur dataset can be helpful for further investigation into ToM capabilities and their weak-\nspots. However since Kosinski (2023) reported better results for their no complications\ntasks, using larger models of the GPT family, the supposed emergence of ToM capabilities\nmight only occur in models larger than the models we tested and should be evaluated\nin those. Further insights might be gained when higher goal accuracy are reached and\na more fine-grained differentiation between the performance of such LLMs on our data\nand different complexity classes can be drawn. Therefore repeating our experiments with\nlarger models and models of the GPT family seems promising.\nThe training of LLMs on data scraped from the internet poses challenges to benchmarks,\nsince LLMs might have been trained on this contaminated data. This introduces the risk\nof overestimating the models capabilities. Our new hand-written dataset might therefore\nbe valuable for benchmarking LLMs that were trained after the public release of the data\nfrom Kosinski (2023).\nAlso further experimenting with other prompting approaches might be promising to get\ndeeper insights into the LLMs ToM capabilities."}, {"title": "6 Conclusion", "content": "In this paper we contribute to the ongoing discussion about emergent ToM capabilities\nin LLMs by creating a new ToM benchmark dataset consisting of 1088 scenes grouped\ninto ten complexity classes and one \"non complexity class\u201d addressing the \"data draught\".\nBuilding on the work of Kosinski (2023), we provide a dataset to systematically evaluate\nthe complications proposed by Ullman (2023) and expand on it by introducing new com-\nplexity classes. In contrast to \"ToMBench\u201d (Chen et al., 2024) it provides extensibility as it\nalready extracts the answers from elaborate LLM outputs and can be extended to longer\nCoT outputs in the future. We use our dataset to evaluate the ToM capabilities of four SotA\nLLMs and find that none of them show robust ToM capabilities, as observed by Kosinski\n(2023), although they answer some subtasks correctly. In this regard our results align with\nthose of Shapira et al. (2023), who use a similar study design, as well as FANTOM Kim et al.\n(2023), which makes use of dynamic social interactions. Thus, in the realm of ToM, they\nmight be considered stochastic parrots after all (Bender et al., 2021). The overall low goal\naccuracy does not allow meaningful conclusions about the impact of the complications on\nthe goal accuracy, which was described to be significant by Ullman (2023). This might be\na result of our selection of tested models, which have a lower number of parameters than\nthose evaluated by Kosinski (2023) and might thus not have the same capabilities. The\nexplanatory power of our results about the hardness of different complications remains\nlimited, but the low performance on the \"automatic change knowledge\" class across all\nmodels is intriguing. Due to the nature of these tasks CoT prompting approaches might\nbe promising to improve the ToM performance in certain cases. As we propose very chal-\nlenging tasks in order to gain further insights into the robustness of ToM pertaining to our\ncomplexity classes further evaluation on even larger models is necessary and we encourage\nevery researcher to use our dataset to improve on our baselines."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Complexity Class Examples", "content": "automatic change knowledge Example: \"Charlie buys a non-transparent box of green\nmangos. There are no red mangos in the box. Green mangos ripen, change the color\nand become red mangos after a few days. Charlie does not know this.[...]\"\nadd unrelated information Example: \"Aya finds a non-transparent bottle. Next\nto it lie boxes of candy, a bag of popcorn and several unidentifiable objects.\nThere's also a spilled box of truffles. The bottle is decorated with flowery\nmotives. It has a nice brownish tone as base color. It is made from clay and\nseems to be really ancient. She considers donating it to a museum. She has never\nseen the bottle before and does not know what is inside. [...]\"\ninduction from baseline Example: \"Esther finds a non-transparent paper bag in her\nparents kitchen cabinet. She does not know what is inside the paper bag. However\nshe then thinks to herself that the bag looks exactly like the paper bags in which\nthey sell sweets in the local supermarket and use for nothing else.[...]\"\nuntrustworthy testimony Example: \"[...]Her sister comes by and tells her that\nthe bottle is filled with sweet sauce and not spicy sauce. She knows that her\nsister loves to play tricks on her and usually the opposite of what she says is\ntrue.[...]\"\nconclusion from sentiment Example: \"[...]She reads the gift card. The text says\nthat the present is from her parents and that they are sure she will like the\npresent. Previously she mentioned to her parents that she really enjoys licorice\nand no other sweets or cookies [...]\""}, {"title": "A.2 Full Prompt", "content": "### YOUR TASK ###\nBelow you will find a text describing a scene with one or more agents and one or more objects.\nObjects may have properties, for example they may be inside of one another or they may\nhave a location. Those properties can change throughout the text.\nYour task is to tell me for the describing text, where the agents in the text think the object is\nor what the agent thinks is inside of the object after the last sentence of the text. In\nother words you tell me what the agent believes to be the property of an object. Valid\nanswers are the object containing the other object or the location of the object or\n\"unknown\", if the agent does not know about the location of the object. Only tell me the\nfinal information about what the agent thinks after the events of the full text.\nYou must present this information in JSON-format, where the top level only has the entry \"final\",\nas you are required to report the final believes of the agents after the events of the full\ntext.\nOn the lower level this single entry has the fields for all asked for objects from the question,\nwhich contain the agent's belief about that object's property. For example they might have\nan assumption about it's location or another object containing it.\nYou will also be penalized if you display the actual position or objects unless what the agents\nthinks conicides with the true world state described in the text, because you are required\nto display what the agents thinks!\nNotice that unless explicitly mentioned agents do not automatically read or see objects or labels.\nI'm going to tip $100 for a better solution!"}, {"title": "#### EXAMPLE ####", "content": "\nText:\nThere is a non-tranparent box in the kitchen.\nSam is also in the kitchen and can see the box.\nThe Box is labelled \"bananas\".\nSam reads the label.\nQuestion:\nWhat does Sam think is inside the box after each sentence?\nExpected answer as JSON:\n===BEGIN JSON===\n{\n\"final\": {\n\"box\": \"bananas\"\n}\n}\n===END JSON===\n#### END OF EXAMPLE ####"}]}