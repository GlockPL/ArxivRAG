{"title": "Building Trust: Foundations of Security, Safety and Transparency in AI", "authors": ["Huzaifa Sidhpurwala", "Garth Mollett", "Emily Fox", "Mark Bestavros", "Huamin Chen"], "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Artificial intelligence and Generative AI", "content": "Generative AI, a branch of artificial intelligence focused on AI production of content such as text, images and video, has seen significant advancements since the introduction of generative adversarial networks (GANs) in 2014 (Goodfellow et al., 2014), which improved data generation but faced issues like training instability. The development of transformers and self attention mechanisms in 2017 (Vaswani et al., 2017) facilitated further improvements in natural language processing, leading to large language models (LLMs) like GPT (Radford et al., 2018) with highly advanced text generation capabilities. Diffusion models (Sohl-Dickstein et al., 2015) have also seen rapid advancement in image and video generation.\nThis rapid advancement in technology capability has been matched by an equally rapid uptake in adoption. Forbes predicts the AI market will reach a staggering $407 billion by 2027',. Additionally, organizations looking to tap into this high-growth market are driving rapid development and innovation in this space to achieve competitive advantages, streamline operations, and enhance customer engagement in an increasingly digital landscape, particularly focusing on the development of models and model systems that address emerging use cases in sectors like manufacturing, service operations, and marketing and sales\u00b2, promising cost savings as a key benefit."}, {"title": "How AI Security is different from AI Safety", "content": "AI Security and AI Safety are interrelated yet distinct aspects of managing risks in artificial intelligence systems. AI Security primarily focuses on protecting AI systems from technical threats (both internal as well as external), unauthorized access, and attacks, which traditional information technology security typically covers. For models, this encompasses safeguarding data integrity, ensuring confidentiality of the data generated by these models, and maintaining the availability of AI services and environments for training and tuning. It involves implementing robust security measures to prevent adversarial attacks, data breaches, and other forms of exploitation that could compromise the functionality or integrity of the AI models and the underlying infrastructure.\nAI Safety is typically concerned with ensuring that AI systems, LLMs, and the supporting libraries and software like PyTorch and its inference software, and their data lifecycle practices ensure the intended operation, as defined, does not cause unintentional harm to users, society, or the environment. Alignment with human-defined values and accepted outputs to mitigate this harm involves addressing issues such as algorithmic biases, ensuring reliable and predictable behavior, and designing fail-safes to mitigate potential harm, negative consequence, and any potential risks presented. While AI Security protects the system from external and insider threats, AI Safety ensures that the system and the data do not pose a threat or create harm through its operation due to its development, training, and use (intended or not).\nIn the realm of generative AI models, the line between security and safety issues often blurs significantly. For instance, a prompt injection attack, typically considered a security concern, can lead to safety issues such as the model producing toxic or harmful content, or exposing personal information such as PII and PHI. This intersection highlights the critical need for a comprehensive approach to AI risk management that addresses both security and safety concerns in tandem."}, {"title": "Effectively navigating AI Security and safety issues", "content": "It is essential that organizations developing or using AI establish appropriate processes and workflows that address the distinct aspects and considerations necessary for AI Security and for AI Safety within an overall AI Risk Management function. While there are areas of reuse and adaptation that are relative between these domains, the overall intent and outcome of effectively managing AI Security and AI Safety is to reduce the risk and subsequent impact of both on an organization, users, and society at large. Therefore, it is recommended that distinct yet parallel structures be established that allow for concentrated management of issues in each domain enabling areas of collaboration and corroboration between those areas.\nIn the context of AI Safety and AI Security, this adds another dimension to the fact that secure models may facilitate more safe models (through transparency, metadata, and other security requirements on integrity and confidentiality), just as Safe models, by nature of their practice, could be secure (applying best practices of safety - output verification, etc.) the probability of a safe and responsible model not having some semblance of security is low given the focus of \"doing the right thing\" which is instrumental to achieving safety outcomes."}, {"title": "Objectives, methodologies, and interdisciplinary expertise", "content": "AI Security focuses on protecting AI systems from external threats to confidentiality, integrity, and availability, while AI Safety deals with assuring AI behaves as intended and does not cause harm. These different objectives may require distinct methods for discovery, triage, remediation, and management reflective of their core focuses, desired outcomes, and inherent expertise required to execute and reason about those issues; suspected and reported.\nHowever, it may occur that the exploitation of a security vulnerability allows for the model's alignment and value learning to be compromised, thereby resulting in real safety hazards. This fundamental difference in approach but interconnected impact necessitates separate considerations and specialized research efforts, or perhaps as the authors suggest, exploring shared regimes and research which favor both security and safety for well orchestrated and holistically-managed risk."}, {"title": "Temporal considerations", "content": "AI Security vulnerabilities can be considered to be static because they are already present and simply require discovery and exploitation to be actualized. They exist at the time they are coded into the software or model, and only require exploitation in the context of the operational or runtime environment for the threat to be realized, requiring immediate and rapid response upon discovery and reporting to reduce impact and mitigate damage. In effect, this is no different than security vulnerabilities in software.\nSafety hazards are dynamic, in that, as the model operates, its output may stabilize or fluctuate against societal considerations and evolution. As such, they are measured on a spectrum that may evolve. Safety hazards may be realized at time of release or years later \u2013 mimicking the experience of a logic bomb, with the core difference that they are not coded in, rather derived from their training and inference as expectations and understanding of safety evolve. Therefore, safety hazards tend to be more long-term in nature.\nJust as the facets of a vulnerability (i.e. maturity, impact, complexity, etc.) determine where the measure of criticality falls on a scale, most safety hazards are measured on a similar yet bracketed spectrum which requires understanding not only of the hazard, but of any shift in human-value alignment and societal expectations since trained or inferred at the time of use or harm. Another important dimension is the multilingual abilities of an LLM and the challenge of safety around this."}, {"title": "Stakeholder involvement", "content": "AI Security vulnerabilities typically involve a narrower range of stakeholders, primarily focused on cybersecurity experts, AI developers, and organizations deploying AI systems. Comparatively, safety hazards require broader societal involvement, including ethicists, policymakers, and the general public. This difference in stakeholder composition and engagement processes supports the need for distinct resolution and management of safety hazards within a comprehensive risk management program."}, {"title": "Regulatory and governance frameworks", "content": "AI Security issues often fall under existing cybersecurity regulations and compliance frameworks, while AI Safety issues will require new or adapted governance structures. While both regulatory and governance landscapes establish a need for different policy approaches, such focused processes must roll up under AI Risk management for organizations to effectively manage comprehensive risk introduced to their business or organization as a result of AI use.\nManaging AI Security and AI Safety separately will result in unidentified risk actualization, ineffective coordination, and negatively impact organizations across their bottom line, brand, and market space, eroding or demolishing trust in one way or another, regardless of the innovation employed in constructing such governance. Frameworks such as NIST's AI Risk Management Framework\" (RMF) are a good start but lack sufficient detail in pulling AI Security and AI Safety processes and workflows together to demonstrate how each influences the overall risk AI presents. Here, the application of existing risk management frameworks fail to consider the complexity of unique challenges AI presents. While NIST's AI RMF includes multiple facets of consideration, such as intellectual property which do present risk but solely in a business context, they do not necessarily consider the externality of that"}, {"title": "Current industry trend on handling security and safety issues", "content": "While the AI industry has taken steps to address security and safety issues, several key challenges still need to be addressed. Emerging trends suggest that current efforts may need to target these issues' underlying causes fully. The following examples highlight areas where further attention is needed to ensure the effectiveness of AI Security and safety initiatives."}, {"title": "Prioritizing Speed Over Safety", "content": "In the race to develop and deploy AI technologies for market share, many companies are prioritizing speed to market over thorough safety testing and ethical considerations, particularly as frameworks to address these are still under rapid development with a few in proof-of-concept or introduced by other entities\u00b9\u00b2 but not widely adopted. This is true of any early, disruptive technology, and AI is no different\u00b3. There is generally wide public acceptance of risks associated with technology that are typically considered acceptable trade-offs for our dependence upon them. However in the matter of Artificial Intelligence, globalization and accessibility of technology, and the impact of digital content and media on daily life, we collectively lack the years of experience in this nascent technology to create reputable AI for widespread public and business acceptance of the risks, which as of yet are immeasurable to human safety. As seen with past security movements\u00b94, security is often years behind any technology, typically requiring a major"}, {"title": "Inadequate Governance and Self-Regulation", "content": "The AI industry has largely relied on voluntary self-regulation and non-binding ethical guidelines, which have proven insufficient in addressing serious security and safety concerns. This is not exclusive to AI; looking at the historical shortcomings of technology self-regulation, many governments are proposing legislation to increase the overall security of technology as can be seen with the EU's Cyber Resilience Act (CRA) among others. For AI, we see legislation proposed that doesn't align with the realities of the technology industry\u00b95 or that addresses concerns raised by industry groups to provide meaningful outcomes16. Many corporate AI ethics initiatives lack teeth and fail to address structural issues or provide meaningful accountability, being developed bespoke to the entity enacting them. The lack of industry consensus \u2013 as evidenced by over 107 different entities focused on related and overlapping areas of AI Security and AI Safety alone - further prevents regulations from being developed that provides AI developers successful implementation and consumer enforcement. The recent veto of SB 104718 highlights some, but not all of these challenges.\nRelatively successful self-governance within the tech industry does happen occasionally, and may also be helpful to study. Two notable examples within the security space are the gradual, widespread adoption of HTTPS by default\u00b9\u00ba and the rapid adoption of signing technologies in the wake of the 2020 SolarWinds software supply chain compromise20. While both phenomena were driven by various factors, a strong common link is the availability of easy-to-use tooling: with HTTPS, easily-attainable, free certificates from e.g. LetsEncrypt helped drive adoption; with software supply chain, availability of simple signing tools such as Sigstore helped signing become a simple added step instead of a laborious additional process. For the purposes of the topic at hand, as tooling for AI Safety matures and"}, {"title": "Inadequate processes for handling flaw and hazard reports, silent fixes,\ndiscouraging reporters", "content": "There exists a lack of common methods and processes of handling model flaws reported by users \u2013 no such methodology has been proposed or shared among model makers and developers to serve as a common reporting framework outside of HuggingFace's \"discussions\u201d feature or \"report model\u201d flag which lacks sufficient documentation to independently inspect the reporting process and reasoning for reporting21. It has taken the software industry decades to develop a flawed-yet-functional disclosure and reporting system for software vulnerabilities, of which many model makers have little to no exposure to or experience with. AI is a technical evolution of data science and machine learning, principally distinct from traditional software engineering and technology development due to its focus on data and math and less on building comprehensive systems for users which has established methodologies for threat modeling, user interaction, and system security. Most model makers who provide their models for free may have their own bespoke processes which are obscured and may be inadequate, or may not have any process at all. In those cases, reporting an issue may involve directly reaching out to the model maker, which can be cumbersome. It is important that reporting mechanisms align to Industry's existing expectations in reporting and management of flaws \u2013 regardless if they are unique to security or safety as both require the context of the issue's discovery, disclosure of the steps by which it was found for reproducibility, evidence or indications of impact in order to simply verify the validity of the issue beyond the individual's claim in reporting. Further, both require time to reproduce (dependent on the mutations of outputs in use), develop mitigation or fixes for, coordination in release, tracking to closure, and potential development of regression testing to prevent their further recurrence.\nBased on the above, we propose processes for handling both security issues as well as safety hazards for public models in the following parts of the paper."}, {"title": "Adapting existing processes to AI Models", "content": ""}, {"title": "Traditional application security process in brief", "content": "The Software Development Lifecycle process traditionally involves designing, writing the actual application (using some type of programming or scripting language), compiling it if required, testing, and then finally packaging and shipping the end product to its users.\nThe development processes differ significantly between open source and closed source software. Closed source applications operate under specific expectations regarding usage and security, dictated by the organization behind them. These expectations often stem from establishing a brand and meeting customer demands for security, reliability, and support. In contrast, open source projects tend to emphasize functionality and the security boundaries of the application, relying on a collaborative model where transparency and community involvement shape the development practices. This difference means that open source applications may not face the same pressure to conform to external brand expectations, allowing for a focus on innovative features and flexibility in meeting user needs.\nDespite these distinctions, there are universal industry expectations for the management of security vulnerabilities that apply to both open and closed source applications. Regardless of the development practices or the perceived robustness of the security measures in place, all software must address vulnerabilities comprehensively. The critical nature of security in software development means that organizations can face scrutiny and repercussions for failing to manage vulnerabilities effectively. Therefore, understanding and adhering to these industry standards is crucial, as users and stakeholders expect a baseline level of security from all software, irrespective of its source.\nEffective vulnerability management is essential and must be implemented using industry-accepted processes, regardless of whether the software is open or closed source.\nEach application security flaw is assigned a unique number by a CVE (Common Vulnerabilities and Exposures) naming authority (CNA) governed by a central, neutral and non-government affiliated organization, often considered a 'public good'. This organization's CVE program aims for consistency and industry alignment in how to identify, define, and catalog publicly disclosed cybersecurity vulnerabilities in software. The CVE number can be universally used to track a particular vulnerability to a particular code base, and is referred to by vendors, researchers and consumers alike. Software projects affected by the vulnerability are able to contest this assignment, and there is scope for rejection of the CVE if warranted.\nOften other types of metadata like CVSS (Common Vulnerability Scoring System) score and CWE (Common Weakness Enumeration) numbers are assigned during the security flaw"}, {"title": "Scope of AI Security flaws", "content": "Security flaws, much like the skills and expertise required to manage them, exist differently than safety, bias, ethics or any other type of issues. In order to appropriately manage and resolve these for AI, we propose the following definition of an AI Security vulnerability.\nAl Security vulnerability: A flaw in an Al system, resulting from a weakness22 that can be exploited, causing a negative impact on the confidentiality, integrity or availability of the affected component or components.\nNote: Here the word Al system is used instead of an Al model, because Al flaws tend to manifest themselves in the way the models are used. For example, in the case of a multi-modal model, it may generate unsafe images for some prompts, but if these models are never used by the application to generate images, but rather used for some other purpose, the flaw has not manifested into a vulnerability in that system.\nWhile there are many similarities to security vulnerabilities, certain nuances only exist for Al models as a result of their nature and how they work, described below."}, {"title": "", "content": "\u2022 Loss of Confidentiality: When models respond with information they should not reveal as mentioned in its model card or other documents there is a loss of confidentiality. This includes but is not limited to:\n\u039f Unauthorized PII and Intellectual property information.\n\u039f Output that can cause widespread damage where the information is not generally available on the internet or considered public.\n\u2022 Loss of Integrity: This issue can occur when an attacker can negatively influence the data within the model or is able to poison the model.\n\u039f Adversarial fine-tuning can cause malicious changes to the model weights, and result in wrong or specially-crafted output generated by the model.\n\u2022 Loss of Availability: The attacker is able to successfully perform a denial of service attack (DoS)23 on the model24 and stop all inference attempts. In most cases, this is considered a software issue rather than an AI model issue.\nThere are circumstances whereby the loss of confidentiality or integrity may result in human, social, or environmental harm, however this may only be ascertained in the context of the model's operational use. In such cases, the importance of information exchange between Al Security and Al Safety teams is paramount for fully exploring the impact of an exploited Al vulnerability so additional safeguards and security protections may be applied to protect against future occurrences of such incidents.\nAI Security vulnerabilities do not take into account the platform on which the model is run, or the support libraries which are used to load, run and train the model. Security issues in those support infrastructures are already covered by the existing CVE process detailed above."}, {"title": "Scope of AI Safety flaws", "content": "AI Safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from data generated from a generative Al model.\nAs discussed earlier, Al model safety is a novel concept as safety is not commonplace within the software development and security field, however there are a few noteworthy exceptions. Medical Device Safety and Automotive Safety are well established within their industry with their own considerations for regulatory compliance beyond any risk management program. 'Trust and Safety' or 'Privacy and Trust' teams are often aligned with but executed and managed separately from Security resulting in their activities potential exclusion from an enterprise risk management program. These concepts have traditionally been excluded from enterprise risk management because their scope was limited to user"}, {"title": "", "content": "privacy as a protected dataset due to regulatory requirements which today do not include the cognitive or mental impacts in exposure; though this is reflected in legal cases as \"intangible injury\u201d or non-economic damages for which software has no equivalent risk consideration or quantifiable measurement.\nTechnology producers that store user data, manage user information, or retain user account details often have to deal with adversarial attacks which target loss of confidentiality or loss of integrity. These attacks may include but are not limited to the compromise of user privacy (adversarial targeting of mail accounts of journalists), illicit account usage (storing and sharing explicit imagery of minors), spread of misinformation, and unauthorized sharing of sensitive content (false or true). Today, the technology industry's closest alignment with safety management is in regulatory duty to protect from harm, physically or cognitively (in the context of social platforms) and not necessarily in evaluating and quantifying the resulting impact from exposure of sensitive information, particularly from sources inappropriately perceived by the masses as trustworthy. Until recently, technology has been viewed as an enablement for harm such as that to occur but only because humans are the cause of violations in trust and safety (medical devices and automotive safety still remain the exception).\nAI is however the first time in which the technology and its development are the cause of violations in trust and safety, bringing considerations of responsible and ethical Al to the forefront of the Al Safety discussion.\nWe propose to define Al Safety issues and flaws as hazards to avoid confusion between security flaw and safety flaw. We feel hazard more readily encapsulates the potential for harm separate from flaw or issue which are currently well utilized industry terms.\nAl Safety hazard: An unexpected model behavior that is outside of the defined intent and scope of the model design. Safety hazards may result in harmful content generation, bias in decision making, or violation of social norms and ethics of groups; the impact and severity of which will vary greatly from group to group based on their culture, social, ethnic, or anthropic systems. Harm may be categorized by loss of life, injury or other physical or mental health impacts or damage, social and economic disruption or degradation, or some combination thereof."}, {"title": "Taxonomy of AI Safety risks", "content": "An important part of handling Al Safety flaws is first to define and decide on a safety taxonomy which will be used by an Al model or an Al system. Al Safety taxonomy is a structured classification system that identifies, organizes, and informs different aspects of safety and reliability in artificial intelligence systems and their uses. The taxonomy helps"}, {"title": "Challenges", "content": "Tracking safety hazards has been one of the biggest challenges of Al models and systems so far. While there have been several developments in the safety field, such as safety aligned models that can detect unsafe content, a lack of consensus and understanding in the way safety hazards should be classified, tracked and perhaps even remediated persists which delays effective tracking, coordination, and mitigation of these hazards currently present in Al today."}, {"title": "Handling AI Security and AI Safety", "content": ""}, {"title": "Adapting vulnerability disclosure and coordination for addressing AI Security flaws", "content": "We are proposing the following adaptations for tracking Al related security flaws:\n\u2022 Model providers should have a mechanism for reporting issues in a confidential manner.\n\u2022 Reporters should disclose the security issue and all necessary details first and foremost to the model provider.\n\u2022 Model providers have the primary responsibility to triage new reports and follow an established and public vulnerability disclosure process or policy. This is not without additional challenges28.\n\u2022 If the model provider is a CNA29 and they recognize this as a security issue, then they alone are responsible for assigning a CVE ID to this issue. If the model provider is not a CNA, then the CVE id can be requested directly from the CVE program partner. 30\n\u2022 If the model provider does not recognise the issue as security related or if the model provider denies assigning a CVE, the reporter may raise a formal request with the CVE Program by following the instructions outlined on their site. 31\n\u2022 Model providers issue VEX statements to convey known vulnerability presence (or lack thereof) in models.\nIt should be noted that many of the current challenges that exist in the traditional vulnerability disclosure and coordination process have parallel challenges in Al Security. While models may publicly declare their scope and intent of use, triage must still be performed to ascertain the applicability, reproducibility, and impact of the reported flaw before coordination and remediation steps may be taken."}, {"title": "Security frameworks and their relevance", "content": "To conclude this topic, we take a brief look at the various Al Security frameworks. These frameworks allow organizations to reduce the risk of the Al system and also help them meet regulatory requirements specific to the organization's industry vertical. It is usual for the industry to layer technical frameworks with those which are more conceptual.\nSome of these frameworks are high level, some help to visualize the threat landscape, some of them help to meet the local regulatory requirements. In the end one would consider the proposed security handling procedure in this paper as a low level requirement/procedure,"}, {"title": "Frameworks for Developers and Practitioners:", "content": "- Focus: Practical security implementation, threat modeling, and vulnerability management.\n- Outcome: Secure Al models, prevent data breaches, and address risks in Al applications using tools like OWASP32 and MITRE ATLAS.33"}, {"title": "Frameworks for Chief Information Security Officers (CISOs):", "content": "Focus: Integration of Al Security into broader risk management.\nOutcome: Align Al Security practices with existing cybersecurity programs and ensure compliance with regulatory standards (e.g., NIST AI RMF34, Google SAIF35)."}, {"title": "Legislature specific to a geographic area:", "content": "Examples: EU Al Act36 and Canada's Artificial Intelligence and Data Act (AIDA\u00b37).\nOutcome: Classify Al systems based on risk levels and provide governance rules to ensure safe, ethical use of Al, affecting compliance and legal responsibilities."}, {"title": "Current tracking efforts in the security community", "content": "There are several efforts ongoing in the security community in line with the above to streamline the process. Notable among them are the efforts by others for CVE/CWE for Al.38"}, {"title": "Proposal for hazards disclosure and management", "content": "Heavily drawing on the prior work done by Sven Cattell et al\u00b39 This proposal is built up of two main components."}, {"title": "Extending model/system cards", "content": "Model cards are used to document the possible use of the model, the architecture and sometimes the training data used to train the model. Today, they provide an initial set of human-generated material about the model that is beneficial in assessing its viability of use, however they have substantially more potential and applicability beyond their current usage. As organizations choose to adopt and build Al systems and capabilities built on top of models, they're choosing to do so in environments and with technologies they already have deployed. In order to facilitate better understanding of models wherever they travel or deploy to, we'd like to propose some changes to model cards.\nThe model card should serve as the introductory information about the model. In order for adopters and engineers to effectively compare models, we need a consistent set of minimum fields and content that must be present in a card. To provide for this, we suggest the development of a specification for model cards. Google's model card paper40 was a novel first step in expressing this content in a standard way, and since 2018 industry has begun to ask different questions about models that warrant updates to this non-standardized format.\nIn addition to the existing fields recommended from the paper, we propose the following be added or modified:\n\u2022 Intent and Use: While 'Intended Use' should describe the users (who) and use cases (what) of the model, it doesn't address how the model is to be used. Expanding Intended Use to be a statement for the model system that describes its usage with precise efficacy provides clarity in all three aspects of consideration (who, what, how) that is not present today. For example:\n\u039f From a text prompt, we produce images that are safe for work, safe for children, and free of demographic bias\n\u2022 Scope: The purpose of Scope is to exclude known issues that the Model producer has no intent or ability to resolve. While Intended Use should convey use cases that are out of scope, providing additional detail on specifics that are out of scope or consideration for resolution provides adopters or consumers of those models more context to make an informed decision from. This also ensures that reporters of hazards understand the purpose of the model before reporting a concern that is explicitly declared as unaddressable against its defined use. For example:\n\u039f This is an LLM with no protections; prompt injections are out of scope.\n\u2022 Evaluation Data: While an existing field in the model card, since 2018 we've developed evaluation frameworks\" that focus on different considerations that inform adopters and consumers on what the model has achieved. We propose extending"}, {"title": "", "content": "\u2022 Governance: Governing information about the model is essential to understand how an adopter or consumer can engage with the model makers or understand the methodology by which it is produced..\n\u2022 References (optional): Model manufacturers may find including references to be beneficial for potential model consumers in both understanding the model's operation, but also detailing artifacts and other content, such as an AIBOM, safety audit42, or security audit, to demonstrate the maturity and professionalism of a given model.\nSetting these and the existing fields as required elements for a model card allows for industry to begin to establish content that is essential for reasoning, decision-making, and reproducibility of models. By rendering model cards in an industry accepted format or standard, we promote the interoperability of models and their metadata across technology ecosystems.\nWe anticipate many organizations will build and develop applications and systems that embed, rely upon, and interact with models in an operational capacity. The loss of a model card as referenceable material for policy enforcement at runtime or auditing will exacerbate existing operational challenges in speedy, incident response and resolution. As we've seen with the supply chain security movement for software, the value of metadata and attestations about what has transpired to produce software is actionable for gatekeeping at deployment and in understanding where risk exists.\nThe data contained in model cards has the potential equivalent function, when stored in an widespread format, such as OC43I, to allow model cards, AIBOMs44, and other evolving metadata types to be pushed, consumed, and leveraged in existing tooling and ecosystems with maturing processes for ingestion, analysis, insights, and policy enforcement."}, {"title": "Common flaws and exposures (CFEs) for Hazard tracking", "content": "While the Common Vulnerability disclosure mechanism used to track security flaws, is effective in traditional software security, its application to Al systems face several challenges:45\n\u2022 Al models must satisfy statistical validity46 thresholds\u2074\u201dThis means that any issues or problems identified in an Al model such as biases etc must be measured and evaluated against established statistical standards to ensure they are meaningful and significant.\n\u2022 Concerns related to trustworthiness and bias often extend beyond the scope of security vulnerabilities and may not align with the accepted definition of security vulnerabilities.\nRecognizing the above limitation we propose expanding the ecosystem, with a new term called \"Common and Flaws and Exposure (CFE)\u201d, which is analogous to the CVE in the security space\nCoordinated Hazard Disclosure and Exposure (committee)\nA central, neutral body must exist to track possible safety hazards in the same way security flaws are tracked (as discussed above). Reporters who discover safety issues, are expected to coordinate with the model providers to triage and do further analysis of these issues. Once established that the issue is indeed a safety hazard, this body assigns a CFE (Common flaws and exposure) number similar to CVE. Model makers and distributors can also request CFE numbers to track safety hazards they find in their models.\nThis body is the custodian of CFE numbers. They are responsible for assigning them to safety hazards as per the process described below, for tracking them and if at some point publishing them in various forms. We propose a \u201cHEX\u201d format to publish CFE data in the section below.\nAdjunct panel\nIn accordance with the paper by Sven Cattell et al, we would also like to propose an adjunct panel to facilitate resolution of safety hazards that are contested:"}, {"title": "", "content": "If a reporter submits a safety hazard on statistical or other grounds, a vendor or model maker may reject or contest the safety hazard and the Adjunct Panel is informed. In the course of reporting, if the submitter-supplied sample/output pairs are statistically biased, and an unbiased sample set would not show a violation of the model card, the adjudicator panel may request supporting data from the vendor or model maker to validate the rejection. The adjudicator and model maker can jointly assess whether the maker-supplied data is too sensitive for the submitter and determine on further steps for resolution.\nIn the initial phase of implementation, bootstrapping the Adjunct panel with the Coordinated Hazard disclosure committee (described above) provides the initial expertise and alignment for future engagement between the two bodies. However it is important for each body to be neutral and/or have good representation from major vendors/reporters in the ecosystem.\nAdapting VEX\nThe Vulnerability Exploit eXchange format (VEX)48 is a recent format for conveying the exploitability status of a vulnerability in software. While nascent in industry adoption, we believe this style of information conveyance can be leveraged in the coordinated disclosure and necessary resolution of Al Safety hazards.\nWe propose exploration into what a potential Hazards Exposure eXchange (HEX) format would provide the industry in conveying the exposure and resulting impact a safety hazard has on the operational use of AI. The impact of those hazards on intended model use and outputs must be tracked to inform consumers how their use is impacted so they may take steps to correct any decisions or harm resulting from an impacted Al system, this includes hazards resulting from the development, training, and inference of the model and all its variants.\nMost existing VEX fields and values are adaptable to safety hazards.. Where VEX defines the subcomponent ID, we propose an unique identifier construct (such as a commit) and category (such as the model lifecycle stage and source) to define how and where the hazard is introduced. Status of the hazard is a critical field in providing consumers of models with sufficient information to understand how they have been exposed and must consider content from the model card (intended use) in detailing that status (i.e. affected, unaffected, fixed, under investigation) and justification (i.e. model_use_not_approved, guardrails_in_place, or tuned_out). Some models provide for multiple uses in which a hazard may only adversely impact a portion of them, it is important to inform model consumers if"}, {"title": "Workflow", "content": "We would like to propose a workflow similar to the security workflow above, with several marked differences to address considerations highlighted previously regarding Al Safety\n\u2022 Reporters attempting to report safety issues would do so in accordance with the specified intent and scope in the model card, reflective of safety benchmarking that has occurred. If, for example, the model is not intended for a specific use, has had a topic explicitly excluded from training or is considered beyond the scope of the model, the safety issue being reported is likely to be closed as invalid by the model maker.\n\u2022 Safety issues are reported to the model makers, following their established and discoverable safety reporting process. If they acknowledge they can approach the Coordinated Hazard Disclosure committee for a CFE number, which is then used to track that particular safety issue.\n\u2022 If model makers refuse to acknowledge the safety issue, reporters can approach the Adjudicator panel, which is responsible for resolving disputes.\n\u2022 In the end, the issue is either accepted or rejected. If accepted, the issue is published in a public database and the model maker issues a safety advisory pointing to the public record.\nAs the process evolves, several enhancements may be made to extend the safety tracking and root causes of the safety hazards to build industry knowledge of prevention, akin to data like CPEs (Common Platform Enumeration) or CWEs."}, {"title": "Current safety efforts", "content": "At this point authors would like to mention several community efforts currently underway for standardization of processes. Some of them may be orthogonal to each other, but we would like to stress the need to collaborate here, rather than compete. The list below is in no way exhaustive, and it's quite possible that new efforts would rise or the ones listed here would become inactive after this paper is published:\n\u2022 Al Alliance - Trust and Safety group 49"}, {"title": "Current challenges in the industry for implementation of this proposal", "content": "Several challenges in the current model ecosystem exist and they need to be addressed in order to implement open and effective tracking of safety hazards.\n\u2022 Model/system cards: While most bigger models have explicit model cards", "issues": "Currently there is no distinction between security and safety issues for most reporters or researchers. When reporters and researchers discover safety hazards or security vulnerabilities with an Al system, they'll look for a central reporting mechanism to submit the report to. Since these suspected issues may not be"}]}