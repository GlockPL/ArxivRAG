{"title": "Label-anticipated Event Disentanglement for Audio-Visual Video Parsing", "authors": ["Jinxing Zhou", "Dan Guo", "Yuxin Mao", "Yiran Zhong", "Xiaojun Chang", "Meng Wang"], "abstract": "Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate events within audio and visual modalities. Multiple events can overlap in the timeline, making identification challenging. While traditional methods usually focus on improving the early audio-visual encoders to embed more effective features, the decoding phase crucial for final event classification, often receives less attention. We aim to advance the decoding phase and improve its interpretability. Specifically, we introduce a new decoding paradigm, label semantic-based projection (LEAP), that employs labels texts of event categories, each bearing distinct and explicit semantics, for parsing potentially overlapping events. LEAP works by iteratively projecting encoded latent features of audio/visual segments onto semantically independent label embeddings. This process, enriched by modeling cross-modal (audio/visual-label) interactions, gradually disentangles event semantics within video segments to refine relevant label embeddings, guaranteeing a more discriminative and interpretable decoding process. To facilitate the LEAP paradigm, we propose a semantic-aware optimization strategy, which includes a novel audio-visual semantic similarity loss function. This function leverages the Intersection over Union of audio and visual events (EIoU) as a novel metric to calibrate audio-visual similarities at the feature level, accommodating the varied event densities across modalities. Extensive experiments demonstrate the superiority of our method, achieving new state-of-the-art performance for AVVP and also enhancing the relevant audio-visual event localization task.", "sections": [{"title": "1 Introduction", "content": "Human perception involves the remarkable ability to discern various types of events in real life through their intelligent auditory and visual sensors [7, 26]. We"}, {"title": "2 Related Work", "content": "Audio-Visual Learning focuses on exploring the relationships between audio and visual modalities to achieve effective audio-visual representation learning and understanding of audio-visual scenarios. Over the years, various research tasks have been proposed and investigated [26], such as the sound source localization [10, 17, 36, 37], audio-visual event localization [24, 29, 33, 38], audio-visual question answering and captioning [14, 15, 22]. While a range of sophisticated networks have been proposed for solving these tasks, most of them emphasize establishing correspondences between audio and visual signals. However, audio-visual signals are not always spatially or temporally aligned. As exemplified by the studied audio-visual video parsing task, the events contained in a video may be modality-independent and temporally independent. Consequently, it is essential to explore the semantics of events within each modality.\nAudio-Visual Video Parsing aims to recognize the event categories and their temporal locations for both audio and visual modalities. The pioneering work [23] performs this task in a weakly supervised setting and frames it as a Multi-modal Multi-Instance Learning (MMIL) problem, demanding the model to be modality-aware and temporal-aware. To tackle this challenging task, subsequent works primarily focus on designing more effective audio-visual encoders [1, 18, 30, 32]. For instance, MM-Pyr [30] utilizes a pyramid unit to constrain the unimodal and cross-modal interactions to occur in adjacent segments, improving the temporal localization. Additionally, some approaches try to generate pseudo labels for audio and visual modalities from the video level [2, 27] and segment level [31, 35]. However, prior works [5, 20, 28, 30, 31] mainly adopt the typical strategy MMIL proposed in [23] as the decoder for final event prediction. The MMIL approach directly regresses multiple classes based on the semantic-mixed hidden feature. In contrast, we further introduce the textual modality as an intermediary and disentangle the semantics of potentially overlapping events contained in audio/visual features by projecting them into semantically separate label embeddings."}, {"title": "3 Audio-Visual Video Parsing Approach", "content": ""}, {"title": "3.1 Task Definition", "content": "The AVVP task aims to recognize and temporally locate all types of events that occur within an audible video. Those events encompass audio events, visual events, and audio-visual events. Specifically, an audible video is divided into T temporal segments, each spanning one second. The audio and visual streams at t-th segment are denoted as $X_t^a$ and $X_t^v$, respectively. A video parsing model needs to classify each audio/visual segment $X_t^m$ ($m \\in \\{a, v\\}, t = 1, ..., T$) into"}, {"title": "3.2 Typical Event Decoding Paradigm MMIL", "content": "As introduced in Sec. 1, prior works [5, 11, 20, 30] usually rely on the Multi-modal Multi-Instance Learning (MMIL) [23] strategy as the late decoder used for final event prediction. We briefly outline the main steps of MMIL.\nFirst, an audio-visual encoder is employed to obtain audio and visual fea-tures: $F^a, F^v = \\Phi(X^a, X^v)$, where $F \\in \\mathbb{R}^{T \\times d}$ and d is the feature dimension. Then, a linear layer is used to transform the obtained features, and the sigmoid activation is directly used to generate the segment-wise event probabilities:\n$\\begin{cases}P^a &= sigmoid(F^a W^a), \\\\P^v &= sigmoid(F^v W^v),\n\\end{cases}$     (1)\nwhere $W^a, W^v \\in \\mathbb{R}^{d \\times C}$ are learnable parameters and $P^a, P^v \\in \\mathbb{R}^{T \\times C}$. To learn from the weak video label $y^{allv}$, the video-level event probability $p^{allv} \\in \\mathbb{R}^{1 \\times C}$ is obtained by an attentive pooling operation, which produces attention weights for both modality and temporal segments.\nTherefore, the MMIL primarily relies on simple linear transformations of audio/visual features to directly classify the multiple event classes. However, this mechanism lacks clarity in demonstrating how potentially overlapped events are disentangled from the semantically mixed hidden features. To enhance the decoding stage, we introduce all C-class label embeddings, each representing separate event semantics, and iteratively project encoded audio/visual features onto them. Through the projection process, the overlapping semantics in the hidden features are gradually disentangled to improve the distinctiveness of the corresponding label embeddings, thereby enhancing the interpretability of our event decoding process. We elaborate on our method in the next subsections."}, {"title": "3.3 Our Label Semantic-based Projection", "content": "As shown in Fig. 2(a), we propose the label semantic-based projection (LEAP) to improve the decoder for final event parsing, serving as a new decoding paradigm."}, {"title": "3.4 Audio-Visual Semantic-aware Optimization", "content": "The cross-modal attention at the last LEAP cycle, i.e., $A_N^m \\in \\mathbb{R}^{C \\times T}$, indi-cates the similarity between all C-class label embeddings and all audio/visual segments. Therefore, we directly use $A_N^m$ to generate segment-level event prob-abilities, written as,\n$P^m = sigmoid((A_N^m)^T),$     (6)\nwhere $P^m = \\{P^a, P^v\\} \\in \\mathbb{R}^{T \\times C}$. Note that $A_N^m$ in Eq. 6 is the raw attention logits without softmax operation. For video-level event prediction $p^m$, it can be produced by the obtained label embedding after LEAP, i.e., $F_N^m$, since it indicates which event classes are finally enhanced:\n$p^m = sigmoid(W(F_N^m)^T),$     (7)\nwhere $W \\in \\mathbb{R}^{1 \\times d}$ and $p^m = \\{p^a, p^v\\} \\in \\mathbb{R}^{1 \\times C}$. We use a threshold of 0.5 to iden-tify events that happen in audio and visual modalities, then the event prediction of the entire video $p^{allv}$ can be computed as follows,\n$p^{allv} = 1(p^a \\ge 0.5) \\|\\| 1(p^v \\ge 0.5),$     (8)\nwhere $1 (z)$ is a boolean function that outputs \u20181' when the $z_i \\ge 0$. '$\\|\\|$' is the logical OR operation, which computes the union of audio events and visual events.\nFor effective projection and better model optimization, we incorporate the segment-wise pseudo labels $Y^m \\in \\mathbb{R}^{T \\times C}$ ($m \\in \\{a,v\\}$) generated in recent work [31] to provide fine-grained supervision. The video-level pseudo labels $y^m \\in \\mathbb{R}^{1 \\times C}$ can also be easily obtained from $Y^m$: If one category event occurs in the temporal segment(s), this category is included in the video-level labels. The basic objective $\\mathcal{L}_{basic}$ constrains the audio and visual event predictions from both video-level and segment-level, computed by,\n$\\mathcal{L}_{basic} = \\sum_m \\mathcal{L}_{bce} (p^{allv}, y^{allv}) + \\mathcal{L}_{bce} (p^m, y^m) + \\mathcal{L}_{bce} (P^m, Y^m),$     (9)\nwhere $\\mathcal{L}_{bce}$ is the binary cross entropy loss, $m \\in \\{a, v\\}$ denotes the modalities.\n$\\mathcal{L}_{basic}$ directly acts on final event predictions and constrains the audio/visual semantic learning through uni-modal event labels. In addition, we further pro-pose a novel audio-visual semantic similarity loss function to explicitly explore the cross-modal relations, which provides extra regularization on au-diovisual representation learning. We are motivated by the observation that the audio and the visual segments often contain different numbers of events. A video example has been shown in Fig. 1(a). An AVVP model should be aware of the semantic relevance and difference between audio events and visual events to achieve a better understanding of events contained in the video.\nTo quantify the cross-modal semantic similarity, we introduce the Intersec-tion over Union of audio Events and visual events (EIoU, symbolized by r). EIoU is computed for each audio-visual segment pair, illustrating the degree of overlap between their respective event classes. For instance, consider an audio segment $a_1$ containing three events with classes $\\{c_1, c_2, c_3\\}$, a visual segment $v_1$ with events of classes $\\{c_1\\}$, and another visual segment $v_2$ with events $\\{c_1, c_2\\}$. In this scenario, the union event sets for these two audio-visual segment pairs are identical, consisting of $\\{c_1, c_2, c_3\\}$. However, the intersection event sets differ: for $a_1$ and $v_1$, the intersection set is $\\{c_1\\}$, whereas for $a_1$ and $v_2$, it is $\\{c_1, c_2\\}$. By calculating the ratio of the intersection set size to the union set size, we can obtain the EloU values for these two audio-visual pairs, i.e., $r_{11} = 1/3$ and"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setups", "content": "Dataset. We conduct experiments of AVVP on the widely used Look, Listen, and Parse (LLP) [23] dataset which comprises 11,849 YouTube videos across 25 categories, including audio/visual events related to everyday human and animal activities, vehicles, musical performances, etc. Following the standard dataset split [23], we use 10,000 videos for training, 648 for validation, and 1,200 for testing. The LLP dataset exclusively provides weak video event labels for the training set. We employ the strategy proposed in [31] to derive segment-wise audio and visual pseudo labels. For the validation and test sets, the segment-level labels are already available for model evaluation.\nEvaluation metrics. Following prior works [2, 5, 23, 27], we evaluate the model performance by using the F1-scores on all types of event parsing results, including the audio event (A), visual event (V), and audio-visual event (AV). For each"}, {"title": "4.2 Ablation Study", "content": "Ablation studies of LEAP. We begin by investigating the impacts of 1) the maximum number of LEAP blocks (N in Eq. 5) and 2) different label embed-ding generation strategies. In this part, we use the MM-Pyr [30] as the early audio-visual encoder of our LEAP-based method. 1) As shown in the upper part of Table 1, the average video parsing performance increases with the num-ber of LEAP blocks. The highest performance is 61.4%, achieved when using four LEAP blocks. This is slightly better than using two LEAP blocks but also doubles the computation cost in projection. Considering the trade-off of perfor-mance and computation cost, we finally utilize two LEAP blocks for constructing AVVP models. 2) We test three commonly used word embedding strategies, i.e., the Glove [19], Bert [4], and CLIP [21]. As shown in the lower part of Table 1, our"}, {"title": "4.3 Comparison with the Typical MMIL", "content": "We comprehensively compare our event decoding paradigm, LEAP, against the typical MMIL. Two widely employed audio-visual backbones, specifically HAN [23] and MM-Pyr [30], are used as the early encoders unless specified otherwise.\nComparison on parsing events across different modalities. 1) As shown in Table 2 (with numbers highlighted in blue), AVVP models utilizing our LEAP exhibit overall improved performances across audio, visual, and audio-visual event parsing, in contrast to models using MMIL. The improvement is more obvious when integrating with the advanced encoder MM-Pyr [30]. For exam-ple, the \"Event@AV\" metrics, indicative of the comprehensive audio and visual event parsing performance, at the segment level and event level are significantly improved by 2.2% and 3.6%, respectively. 2) Beyond this holistic dataset compar-ison, we detail the parsing performances across distinct audio and visual event categories. As shown in Fig. 3, the proposed LEAP surpasses MMIL in most of the event categories for both audio and visual modalities. In particular, the"}, {"title": "4.4 Comparison with the State-of-the-Arts", "content": "We compare our method with prior works. As shown in the upper part of Ta-ble 4, our LEAP-based model is superior to those methods developed based on HAN [23]. It is noteworthy that the most competitive work VALOR [31] also uses the segment-level pseudo labels as supervision but adopts the typical MMIL [23] for event decoding. In contrast, we combine HAN with the proposed LEAP which has better performance. Methods listed in the lower part of Table 4 primarily focus on designing stronger audio-visual encoders and we report their optimal performance. CMPAE [5] is most competitive because it additionally selects thresholds for each event class during event inference while we directly use the threshold of 0.5 as in baselines [23, 30]. Without bells and whistles, we show that the proposed LEAP equipped with the baseline encoder MM-Pyr [30] has achieved new state-of-the-art performance in all types of event parsing."}, {"title": "4.5 Generalization to AVEL Task", "content": "We finally extend our label semantic-based projection (LEAP) decoding paradigm to one related audio-visual event localization (AVEL) task, which aims to local-ize video segments containing events both audible and visible. We evaluate three typical audio-visual encoders in this task, including AVE [24], PSP [38], and CMBS [29]. We combine them with our decoding paradigm, LEAP, based on the official codes. As shown in Table 5, our LEAP is also superior to the default paradigm in this task, consistently boosting the vanilla models. The improve-ment further increases when using stronger audio-visual encoders. This indicates the generalization of our method and also verifies the benefits of introducing se-mantically independent label embeddings for the distinctions of different events."}, {"title": "5 Conclusion", "content": "Addressing the audio-visual video parsing task, this paper presents a straight-forward yet highly effective label semantic-based projection (LEAP) method to enhance the event decoding phase. LEAP disentangles the potentially over-lapping semantics by iteratively projecting the latent audio/visual features into separate label embeddings associated with distinct event classes. To facilitate the projection, we propose a semantic-aware optimization strategy, which adopts a"}, {"title": "A Parameter study of $\\lambda$", "content": "$\\lambda$ is a hyperparameter used to balance the two loss items: $\\mathcal{L}_{basic}$ and $\\mathcal{L}_{avss}$. We conduct experiments to explore its impact on our semantic-aware optimization. As shown in Table 6, the model has the highest average performance when $\\lambda$ is set to 1. Therefore, this value is adopted as the optimal configuration."}, {"title": "B Ablation study of LEAP block", "content": "In Table 1 of our main paper, we have established the optimal number (i.e., 2) of LEAP blocks, we then explore which block's output is better suited for event predictions. We assess the outputs from the first block, the last block, and the average of these two blocks. As shown in 7, the best performance is"}, {"title": "C Analysis of computational complexity", "content": "In Tables 2 and 3 of our main paper, we have demonstrated that our LEAP method can bring effective performance improvement particularly when com-bined with the advanced audio-visual encoder MM-Pyr [30]. Here, we further provide discussions on parameter overhead or computational complexity. 1) Our LEAP introduces more parameters than the typical decoding paradigm MMIL [23]. However, this increase is justified as MMIL merely utilizes several linear layers for event prediction, whereas our LEAP enhances the decoding stage with more sophisticated network designs and increases interpretability. By incorporating semantically distinct label embeddings of event classes, our LEAP involves in-creased cross-modal interactions between audio/visual and label text tokens. Consequently, our LEAP method inherently possesses more parameters than MMIL. 2) We further report the specific numbers of parameters and FLOPs of our LEAP-based model adopting the MM-Pyr as the audio-visual encoder. The total parameters of the entire model are 52.01M, while the parameters of our LEAP decoder are only 7.89M (15%). Similarly, the FLOPs of our LEAP blocks only account for 18.5% (146M v.s. 791M) of the entire model."}, {"title": "DMore qualitative examples and analyses", "content": "We provide additional qualitative video parsing examples and analyses of our method. The MM-Pyr [30] is used as the early audio-visual encoder in this part."}]}