{"title": "Label-anticipated Event Disentanglement for Audio-Visual Video Parsing", "authors": ["Jinxing Zhou", "Dan Guo", "Yuxin Mao", "Yiran Zhong", "Xiaojun Chang", "Meng Wang"], "abstract": "Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate events within audio and visual modalities. Multiple events can overlap in the timeline, making identification challenging. While traditional methods usually focus on improving the early audio-visual encoders to embed more effective features, the decoding phase crucial for final event classification, often receives less attention. We aim to advance the decoding phase and improve its interpretability. Specifically, we introduce a new decoding paradigm, label semantic-based projection (LEAP), that employs labels texts of event categories, each bearing distinct and explicit semantics, for parsing potentially overlapping events. LEAP works by iteratively projecting encoded latent features of audio/visual segments onto semantically independent label embeddings. This process, enriched by modeling cross-modal (audio/visual-label) interactions, gradually disentangles event semantics within video segments to refine relevant label embeddings, guaranteeing a more discriminative and interpretable decoding process. To facilitate the LEAP paradigm, we propose a semantic-aware optimization strategy, which includes a novel audio-visual semantic similarity loss function. This function leverages the Intersection over Union of audio and visual events (EIoU) as a novel metric to calibrate audio-visual similarities at the feature level, accommodating the varied event densities across modalities. Extensive experiments demonstrate the superiority of our method, achieving new state-of-the-art performance for AVVP and also enhancing the relevant audio-visual event localization task.", "sections": [{"title": "1 Introduction", "content": "Human perception involves the remarkable ability to discern various types of events in real life through their intelligent auditory and visual sensors [7,26]. We"}, {"title": "2 Related Work", "content": "Audio-Visual Learning focuses on exploring the relationships between audio and visual modalities to achieve effective audio-visual representation learning and understanding of audio-visual scenarios. Over the years, various research tasks have been proposed and investigated [26], such as the sound source localization [10,17,36,37], audio-visual event localization [24, 29, 33, 38], audio-visual question answering and captioning [14, 15, 22]. While a range of sophisticated networks have been proposed for solving these tasks, most of them emphasize establishing correspondences between audio and visual signals. However, audio-visual signals are not always spatially or temporally aligned. As exemplified by the studied audio-visual video parsing task, the events contained in a video may be modality-independent and temporally independent. Consequently, it is essential to explore the semantics of events within each modality.\nAudio-Visual Video Parsing aims to recognize the event categories and their temporal locations for both audio and visual modalities. The pioneering work [23] performs this task in a weakly supervised setting and frames it as a Multi-modal Multi-Instance Learning (MMIL) problem, demanding the model to be modality-aware and temporal-aware. To tackle this challenging task, subsequent works primarily focus on designing more effective audio-visual encoders [1, 18, 30, 32]. For instance, MM-Pyr [30] utilizes a pyramid unit to constrain the unimodal and cross-modal interactions to occur in adjacent segments, improving the temporal localization. Additionally, some approaches try to generate pseudo labels for audio and visual modalities from the video level [2,27] and segment level [31,35]. However, prior works [5, 20, 28, 30, 31] mainly adopt the typical strategy MMIL proposed in [23] as the decoder for final event prediction. The MMIL approach directly regresses multiple classes based on the semantic-mixed hidden feature. In contrast, we further introduce the textual modality as an intermediary and disentangle the semantics of potentially overlapping events contained in audio/visual features by projecting them into semantically separate label embeddings."}, {"title": "3 Audio-Visual Video Parsing Approach", "content": "The AVVP task aims to recognize and temporally locate all types of events that occur within an audible video. Those events encompass audio events, visual events, and audio-visual events. Specifically, an audible video is divided into \\(T\\) temporal segments, each spanning one second. The audio and visual streams at \\(t\\)-th segment are denoted as \\(X_t^a\\) and \\(X_t^v\\), respectively. A video parsing model needs to classify each audio/visual segment \\(X_t^m\\) (\\(m \\in \\{a,v\\}, t = 1, ..., T\\)) into"}, {"title": "3.2 Typical Event Decoding Paradigm MMIL", "content": "As introduced in Sec. 1, prior works [5,11,20,30] usually rely on the Multi-modal Multi-Instance Learning (MMIL) [23] strategy as the late decoder used for final event prediction. We briefly outline the main steps of MMIL.\nFirst, an audio-visual encoder is employed to obtain audio and visual fea-tures: \\(F^a, F^v = \\Phi(X^a, X^v)\\), where \\(F \\in \\mathbb{R}^{T\\times d}\\) and \\(d\\) is the feature dimension. Then, a linear layer is used to transform the obtained features, and the sigmoid activation is directly used to generate the segment-wise event probabilities:\n\\begin{equation}\\begin{cases}P^a = sigmoid(F^aW^a), \\\\P^v = sigmoid(F^vW^v),\\end{cases}\\end{equation}\nwhere \\(W^a, W^v \\in \\mathbb{R}^{d\\times C}\\) are learnable parameters and \\(P^a, P^v \\in \\mathbb{R}^{T\\times C}\\). To learn from the weak video label \\(y^{all}_v\\), the video-level event probability \\(p^{all}_{a|v} \\in \\mathbb{R}^{1\\times C}\\) is obtained by an attentive pooling operation, which produces attention weights for both modality and temporal segments.\nTherefore, the MMIL primarily relies on simple linear transformations of audio/visual features to directly classify the multiple event classes. However, this mechanism lacks clarity in demonstrating how potentially overlapped events are disentangled from the semantically mixed hidden features. To enhance the decoding stage, we introduce all \\(C\\)-class label embeddings, each representing separate event semantics, and iteratively project encoded audio/visual features onto them. Through the projection process, the overlapping semantics in the hidden features are gradually disentangled to improve the distinctiveness of the corresponding label embeddings, thereby enhancing the interpretability of our event decoding process. We elaborate on our method in the next subsections."}, {"title": "3.3 Our Label Semantic-based Projection", "content": "As shown in Fig. 2(a), we propose the label semantic-based projection (LEAP) to improve the decoder for final event parsing, serving as a new decoding paradigm."}, {"title": "3.4 Audio-Visual Semantic-aware Optimization", "content": "The cross-modal attention at the last LEAP cycle, i.e., \\(A^m \\in \\mathbb{R}^{C\\times T}\\), indi-cates the similarity between all \\(C\\)-class label embeddings and all audio/visual segments. Therefore, we directly use \\(A^m\\) to generate segment-level event prob-abilities, written as,\n\\begin{equation}P^m = sigmoid((A^m)^T),\\end{equation}"}, {"title": "4 Experiments", "content": "We conduct experiments of AVVP on the widely used Look, Listen, and Parse (LLP) [23] dataset which comprises 11,849 YouTube videos across 25 categories, including audio/visual events related to everyday human and animal activities, vehicles, musical performances, etc. Following the standard dataset split [23], we use 10,000 videos for training, 648 for validation, and 1,200 for testing. The LLP dataset exclusively provides weak video event labels for the training set. We employ the strategy proposed in [31] to derive segment-wise audio and visual pseudo labels. For the validation and test sets, the segment-level labels are already available for model evaluation."}, {"title": "4.2 Ablation Study", "content": "Ablation studies of LEAP. We begin by investigating the impacts of 1) the maximum number of LEAP blocks (\\(N\\) in Eq. 5) and 2) different label embed-ding generation strategies. In this part, we use the MM-Pyr [30] as the early audio-visual encoder of our LEAP-based method. 1) As shown in the upper part of Table 1, the average video parsing performance increases with the num-ber of LEAP blocks. The highest performance is 61.4%, achieved when using four LEAP blocks. This is slightly better than using two LEAP blocks but also doubles the computation cost in projection. Considering the trade-off of perfor-mance and computation cost, we finally utilize two LEAP blocks for constructing AVVP models. 2) We test three commonly used word embedding strategies, i.e., the Glove [19], Bert [4], and CLIP [21]. As shown in the lower part of Table 1, our"}, {"title": "4.3 Comparison with the Typical MMIL", "content": "We comprehensively compare our event decoding paradigm, LEAP, against the typical MMIL. Two widely employed audio-visual backbones, specifically HAN [23] and MM-Pyr [30], are used as the early encoders unless specified otherwise.\nComparison on parsing events across different modalities. 1) As shown in Table 2 (with numbers highlighted in blue), AVVP models utilizing our LEAP exhibit overall improved performances across audio, visual, and audio-visual event parsing, in contrast to models using MMIL. The improvement is more obvious when integrating with the advanced encoder MM-Pyr [30]. For exam-ple, the \"Event@AV\" metrics, indicative of the comprehensive audio and visual event parsing performance, at the segment level and event level are significantly improved by 2.2% and 3.6%, respectively. 2) Beyond this holistic dataset compar-ison, we detail the parsing performances across distinct audio and visual event categories. As shown in Fig. 3, the proposed LEAP surpasses MMIL in most of the event categories for both audio and visual modalities. In particular, the"}, {"title": "4.5 Generalization to AVEL Task", "content": "We finally extend our label semantic-based projection (LEAP) decoding paradigm to one related audio-visual event localization (AVEL) task, which aims to local-ize video segments containing events both audible and visible. We evaluate three typical audio-visual encoders in this task, including AVE [24], PSP [38], and CMBS [29]. We combine them with our decoding paradigm, LEAP, based on the official codes. As shown in Table 5, our LEAP is also superior to the default paradigm in this task, consistently boosting the vanilla models. The improve-ment further increases when using stronger audio-visual encoders. This indicates the generalization of our method and also verifies the benefits of introducing se-mantically independent label embeddings for the distinctions of different events."}, {"title": "5 Conclusion", "content": "Addressing the audio-visual video parsing task, this paper presents a straight-forward yet highly effective label semantic-based projection (LEAP) method to enhance the event decoding phase. LEAP disentangles the potentially over-lapping semantics by iteratively projecting the latent audio/visual features into separate label embeddings associated with distinct event classes. To facilitate the projection, we propose a semantic-aware optimization strategy, which adopts a"}]}