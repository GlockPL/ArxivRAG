[{"title": "Leaning Time-Varying Instruments for Identifying Causal Effects in Time-Series Data", "authors": ["Debo Cheng", "Ziqi Xu", "Jiuyong Li", "Lin Liu", "Thuc duy Le", "Shichao Zhang"], "abstract": "Querying causal effects from time-series data is important across various fields, including healthcare, economics, climate science, and epidemiology. However, this task becomes complex in the existence of time-varying latent confounders, which affect both treatment and outcome variables over time and can introduce bias in causal effect estimation. Traditional instrumental variable (IV) methods are limited in addressing such complexities due to the need for predefined IVs or strong assumptions that do not hold in dynamic settings. To tackle these issues, we develop a novel Time-varying Conditional Instrumental Variables (CIV) for Debiasing causal effect estimation, referred to as TDCIV. TDCIV leverages Long Short-Term Memory (LSTM) and Variational Autoencoder (VAE) models to disentangle and learn the representations of time-varying CIV and its conditioning set from proxy variables without prior knowledge. Under the assumptions of the Markov property and availability of proxy variables, we theoretically establish the validity of these learned representations for addressing the biases from time-varying latent confounders, thus enabling accurate causal effect estimation. Our proposed TDCIV is the first to effectively learn time-varying CIV and its associated conditioning set without relying on domain-specific knowledge. Extensive experiments conducted on both synthetic and real-world climate datasets showcase TDCIV's superior performance in estimating causal effects from time-series data with time-varying latent confounders.", "sections": [{"title": "I. INTRODUCTION", "content": "Estimating time-varying causal effects has attracted signif-icant attention across various fields, such as healthcare [1], economics [2], climate science [3], [4], and epidemiology [5]. For example, evaluating the causal effects of a treatment on patient outcomes over time is crucial for developing treatment guidelines in medical contexts. The gold standard for estimat-ing causal effects is to conduct a randomised controlled trial (RCT). However, RCTs are often impossible due to high costs or ethical concerns [6]\u2013[8]. Therefore, causal effect estimation using observational data has become widely accepted in real-world applications.\nFew methods focus on estimating time-varying causal ef-fects from time-series data, largely due to the inherent com-plexity of the dynamic interactions between variables over time [9]\u2013[17]. In many real-world applications, estimating time-varying causal effects makes it possible to understand how a disease or policy evolves under different treatments or interventions over time [11]. For instance, as shown in Figure 1, medical researchers analyse the time-varying causal effects of treatment adherence on HIV-related outcomes in the Women's Interagency HIV Study [12], [13]. These analyses are essential for designing targeted interventions, optimizing treatment schedules, and improving long-term outcomes for individuals living with HIV.\nThe main obstacle in estimating time-varying causal ef-fects from time-series data lies in handling time-varying con-founders, which dynamically influence both the treatment and the outcome over time, particularly when time-varying latent confounders are also present [6], [18]. Neglecting to account for latent confounders can produce biased causal estimates, potentially leading to flawed conclusions and misguided de-cisions. Following the example above of the Women's Intera-gency HIV Study, variables such as HIV disease status, mental health, and psychological well-being are often unmeasured (i.e., latent confounders) and influence both treatment adher-ence and outcomes over time. Consequently, the causal effect of treatment adherence on HIV-related outcomes becomes unidentifiable due to the causal ambiguities caused by these latent confounders [19], [20].\nIn static settings, the instrumental variable (IV) approach is commonly used to deal with the latent confounding bias due to latent confounders in causal effect estimation [21], [22], but to date, few IV methods have been developed for time-series data [12], [13], [17]. In a static setting, a valid IV must satisfy three conditions: (i) being correlated with the treatment, (ii) being independent of all latent confounders, and (iii) influencing the outcome only through its effect on the treatment [5], [23], [24]. The last two conditions are not directly testable, so many existing IV methods rely on predefined IVs, typically chosen by experts based on domain knowledge. This reliance significantly limits the scope of IV applications. To mitigate this limitation, some approaches have been proposed to decompose or recover the information of IVs from observed variables [22], [25]; but assuming static settings.\nIn time-series data, identifying a valid time-varying IV from observed variables is more complex and requires stronger assumptions than in the static setting due to the intricate temporal relationships. Thus, most existing IV methods for time-series data require a predefined time-varying IV. For instance, Michael et al. [13] developed a weighted estimator with a known time-varying IV to evaluate the effect of hospital type on neonatal survival. Cui et al. [12] extended the standard inverse probability of treatment weighted (IPTW) estimation by identifying parameters of the marginal structural Cox model using a given time-varying IV. Recently, Cheng et al. [17] proposed a Time-varying IV Factor Model (TIFM) to learn the representation of an IV without relying on a predefined time-varying IV, with the assumption that a time-varying IV (though unobserved) exists and influences all time-varying covariates at each time step. However, these IV methods do not account for time-varying Conditional IV (CIV), where a variable serves as a valid IV when conditioned on a set of variables (a conditioning set), a general scenario in many applications. Consequently, learning a time-varying CIV directly from data remains an unresolved challenge.\nIn this work, we propose a novel Time-varying Conditional Instrumental Variables (CIV) for Debiasing causal effect esti-mation (TDCIV), to disentangle and learn the latent represen-tations of a time-varying CIV and its conditioning set. With the aid of graphical causal models [19], [26], we propose the time-varying CIV within the full-time DAG $G_{full}$. Specifically, TDCIV integrates Long Short-Term Memory (LSTM) [27], Variational Autoencoder (VAE) [28] and Conditional VAE (CVAE [29]) models to disentangle and learn the representa-tions of a time-varying CIV and its conditioning set, thereby enabling the identification of causal effects in time-series data. These learned representations are then utilised in a CIV method using two stage least square (2SLS) [23] to identify time-varying causal effects in the presence of latent time-varying confounders. To the best of our knowledge, TDCIV is the first method capable of directly learning the representations of a time-varying CIV and its conditioning set from time-series data while addressing the complexities introduced by time-varying latent confounders.\nTherefore, the main contributions of our work can be summarised as follows:\n\u2022 We develop a novel time-varying CIV method for de-biasing causal effect estimation (TDCIV). Our TDCIV method aims to disentangle and learn the representations of time-varying CIV and its conditioning set in order to identify causal effects in time-series data.\n\u2022 We theoretically prove the validity of the learned repre-sentations of the time-varying CIV and its conditioning set for identifying causal effects in time-series data with time-varying latent confounders.\n\u2022 Experiments performed on synthetic and real-world cli-mate data validate the effectiveness of TDCIV in iden-tifying causal effects in time-series data influenced by time-varying latent confounders.\nSection II introduces the notations and definitions for our problem and solutions. Section III introduces our proposed TDCIV method, detailing the concept of time-varying CIVS and its conditioning set, and the architecture of the TDCIV model. In Section IV, we present the experiments on both synthetic and real-world data to validate the effectiveness of TDCIV and discuss the results. Section V discusses related work in causal inference for time-series data, highlighting the uniqueness and contributions of our method. Finally, Section VI concludes the paper and proposes potential future work."}, {"title": "II. DEFINITIONS AND PROBLEM SETTING", "content": "A. Notations and Basic Definitions\nIn this paper, uppercase letters (e.g., X) denote variables, while lowercase letters (e.g., x) represent their values. Bold-faced uppercase and lowercase letters (e.g. X and x) are used to indicate sets of variables and their corresponding values, respectively.\nWe use $\\mathbf{W}_{T} = (W_1,\\dots,W_T) \\in \\mathcal{W}^{T}$, $\\mathbf{X}_{T} = (X_1,\\dots,X_T) \\in \\mathcal{X}^{T}$ and $\\mathbf{U}^{(2)}_{T} = (U_1^{(2)},\\dots,U_T^{(2)}) \\in \\mathcal{U}^{T}$ to denote the time-varying treatment, measured covariates, and latent covariates, respectively, over the time step $t \\in \\{2,\\dots,T\\}$. Let $\\mathbf{Y}_{T+1} = (Y_2, ..., Y_{T+1}) \\in \\mathcal{V}^{T+1}$ indicate the outcome of interest from time steps over the time step $t \\in \\{2, ..., T+1\\}$. The sample index $i$ is omitted unless explicitly required. At any given time step $t$, we represent $k$ measured covariates as $X_t = [X_{t1},\\dots,X_{tp}]$ and the $k$ latent covariates as $U_t = [U_{t1},\\dots,U_{tk}]$. Let $\\mathcal{D} = (\\mathbf{X}_{T}, \\mathbf{W}_{T}, \\mathbf{Y}_{T+1})$ denote the time-series data, and $H_t = (X_t, W_{t-1}, Y_t)$ denote the historical data up to time step $t$.\nWe employ the full-time directed acyclic graph (DAG) $G_{full} = (\\mathcal{V}, \\mathcal{E})$ [19], [26], [30], [31] (i.e., an infinite DAG over time) to represent the causal relationships between variables across temporal dimensions as shown in Fig. 2 (a), where $\\mathcal{V} = \\mathcal{X}_{T} \\cup \\overline{\\mathbf{U}}_{T} \\cup \\mathbf{W}_{T} \\cup \\mathbf{Y}_{T+1}$ represents the set of variables, with $\\mathcal{E} = \\mathcal{V} \\times \\mathcal{V}$ representing the set of directed edges indicating causal relationships between variables. At each time step $t$, $W_t$ directly affects $Y_{t+1}$, represented by the directed edge $W_t \\rightarrow Y_{t+1}$ in $G$. In this context, $W_t$ is a parent node of $Y_{t+1}$ in $G$. Additionally, $X_t$ and $U_t$ each have causal effects on both $W_t$ and $Y_{t+1}$, potentially introducing time-varying confouning bias, w.r.t., estimating the causal effects of $W_t$ and $Y_{t+1}$ at time step $t$. In the full-time DAG $G$, $Y_{t+1}$ is a sink node, meaning it has no descendant nodes except its future status $Y_{t+2}$. A summary DAG $G$ as shown in Fig. 2 (b) is a simplified representation of the full-time DAG [26]. The summary DAG $G$ contains a direct edge between two nodes $V_i$ to $V_j$ if and only if there exists some $1 < k$ such that the full-time DAG $G_{full}$ includes an edge from $V_{i(t)}$ to $V_{j(t+k)}$. We can read off the conditional independencies in $\\mathcal{D}$ from the full-time DAG $G_{full}$ when the Markov property holds.\nDefinition 1. (Markov Property) A time-series data $\\mathcal{D}$ is generated by a full-time DAG $G_{full}$, such that $\\forall V \\in \\mathcal{V}$ is conditionally independent of its non-descendants given its parents in $G_{full}$.\nMarkov property implies that all dependencies within $\\mathcal{D}$ can be derived from the full-time DAG $G_{full}$.\nDefinition 2 (Faithfulness). A time-series dataset $\\mathcal{D}$ is faithful to its full-time DAG $G_{full}$ if every conditional independence in the joint probability distribution of $\\mathcal{D}$ is entailed by the Markov property of $G_{full}$, and vice versa.\nFaithfulness indicates that the full-time DAG $G_{full}$ fully encodes all independencies present in $\\mathcal{D}$.\nFurthermore, we employ a potential outcomes frame-work [6], [7] to identify the average causal effects from time-series data. Let $Y_{t+1}(W_t)$ represent the potential outcomes associated with the treatment $w_t$, where $w_t$ denotes the treat-ment applied at time step $t$, just before $Y_{t+1}$ is measured. Not all potential outcomes are measured. Using the potential outcomes, we define the average causal effect (ACE) of $W_t$ on $Y_{t+1}$, denoted as $ACE_{t}(W_t, Y_{t+1})$, from time-series data as follows:\n$\\mathbb{E}[Y_{t+1}(W_t) \\mid W_{t-1}, X_t] = \\mathbb{E}[Y_{t+1} \\mid W_t, W_{t-1}, X_t]$   (1)\nTo identify $ACE_{t}(W_t, Y_{t+1})$ in time-series data, three important assumptions are required [10], [11], [16]. The rela-tionship between the observed data and the potential outcomes is established through the consistency.\nAssumption 1 (Consistency). If $W_t = w_t$, then $Y_{t+1}(W_t) = Y_{t+1}$, i.e., the potential outcome under $w_t$ is consistent with the observed outcome $Y_{t+1}$.\nMost existing works [10], [14]\u2013[16], [24], [32], [33] rely on the Sequential Randomisation Assumption (SRA) and positivity, introduced by Robins [6], [31]:\nAssumption 2 (SRA). $Y_{t+1}(W) \\perp W_t \\mid W_{t-1}, X_t$, where $\\perp$ denotes the statistical independence.\nSRA means that there are no latent time-varying con-founders, i.e., all variables affecting both $W_t$ and $Y_{t+1}(W)$ are measured and included in the time-series data $\\mathcal{D}$.\nAssumption 3 (Positivity). If $P(W_{t-1} = w_{t-1}, X_t = x_t) \\neq 0$, then we have $P(W_t = w_t \\mid W_{t-1} = w_{t-1}, X_t = x_t) > 0$ for all $w_t$.\nThe positivity assumption ensures that every possible treat-ment assignment has a non-zero probability of occurring for any given combination of measured covariates.\nB. Problem Definition\nIn this work, we consider a more general scenario that involves time-varying latent confounders, denoted by $U_t$, which influences both $W_t$ and $Y_{t+1}$. Consequently, using Equation (1) will result in a biased estimate, as $U_t$ induces a spurious association between $W_t$ and $Y_{t+1}$ at each time step and cannot be measured directly, i.e., $\\mathbb{E}[Y_{t+1}(W_t) \\mid W_{t-1}, X_t] \\neq \\mathbb{E}[Y_{t+1} \\mid W_t, W_{t-1}, X_t]$.\nTo address the bias caused by time-varying latent con-founders $U_t$, we rewrite Assumption 2 (SRA) by splitting the covariates as measured covariates $X_t$ and latent covariates $U_t$ as Latent SRA (LSRA):\nAssumption 4 (LSRA). $Y_{t+1}(W_t) \\perp W_t \\mid W_{t-1}, X_t, \\overline{U}_t$.\nThe assumption states that $U_t$ captures all latent confound-ing between $W_t$ and $Y_{t+1}$ such that the SRA holds when"}, {"title": "III. THE TDCIV METHOD", "content": "In this section", "19": [26], "31": ".", "23": "and DeepIV [21", "22": [24], "conditions": "nDefinition 3. (The concept of a time-varying CIV in a full-time DAG) Given a full-time DAG $G_{full"}, "mathcal{V}, \\mathcal{E})$ with $\\mathcal{V} = \\mathcal{X}_{T} \\cup \\overline{\\mathbf{U}}_{T} \\cup \\mathbf{W}_{T} \\cup \\mathbf{Y}_{T+1}$, and $W_t \\rightarrow Y_{t+1}$ in $\\mathcal{E}$ at each time-step $t$, a variable $S_t \\in X_t$ is a time-varying CIV relative to $W_t \\rightarrow Y_{t+1}$ if there exists a set of variables $Z_t \\in X_t \\setminus \\{S_t\\}$ such that:\ni  $S_t \\perp W_t \\mid S_{t-1}, W_{t-1}, Y_t, Z_t$, where $\\mathcal{Z}_{t} = (Z_1,\\dots, Z_t)$,\nii  $S_t \\perp Y_{t+1} \\mid S_{t-1}, W_{t-1}, Y_t, Z_t$ in $G_{W_{t}}$, where $G_{W_{t}}$ is the graph obtained by removing $W_t \\rightarrow Y_{t+1}$ at time step $t$ from $G_{full}$, and\niii $\\forall Z_t \\in Z_t$ is not a descendant of $Y_{t+1}$.\nThe third condition in Definition 3 is satisfied in our problem setting because $Y_{t+1}$ is a sink node and has no descendant nodes other than its future statuses (e.g., $Y_{t+2}$ and $Y_{t+3}$). The first condition specifies that $S_t$ is correlated with $W_t$, conditional on the historical values of $S_{t-1}, W_{t-1}, Y_t$ and $Z_t$. This requirement for a time-varying CIV differs from that of a static CIV, as it necessitates consideration of past states, i.e., the historical data [26"], "components": "an LSTM", "follows": "n$H_t = LSTM(\\xi);\\newline H_t = LSTM(H_{t-1"}, {"follows": "n$p(S_t) = \\mathcal{N"}, {"follows": "n$p(W_t \\mid Z_t", "distribution": "n$p(Y_{t+1"}, {"distribution": "n$p(Y_{t+1"}, {"by": "n$\\mathcal{M} = \\mathbb{E}_q [log p(H_t \\mid"}]