{"title": "Leaning Time-Varying Instruments for Identifying Causal Effects in Time-Series Data", "authors": ["Debo Cheng", "Ziqi Xu", "Jiuyong Li", "Lin Liu", "Thuc duy Le", "Shichao Zhang"], "abstract": "Querying causal effects from time-series data is important across various fields, including healthcare, economics, climate science, and epidemiology. However, this task becomes complex in the existence of time-varying latent confounders, which affect both treatment and outcome variables over time and can introduce bias in causal effect estimation. Traditional instrumental variable (IV) methods are limited in addressing such complexities due to the need for predefined IVs or strong assumptions that do not hold in dynamic settings. To tackle these issues, we develop a novel Time-varying Conditional Instrumental Variables (CIV) for Debiasing causal effect estimation, referred to as TDCIV. TDCIV leverages Long Short-Term Memory (LSTM) and Variational Autoencoder (VAE) models to disentangle and learn the representations of time-varying CIV and its conditioning set from proxy variables without prior knowledge. Under the assumptions of the Markov property and availability of proxy variables, we theoretically establish the validity of these learned representations for addressing the biases from time-varying latent confounders, thus enabling accurate causal effect estimation. Our proposed TDCIV is the first to effectively learn time-varying CIV and its associated conditioning set without relying on domain-specific knowledge. Extensive experiments conducted on both synthetic and real-world climate datasets showcase TDCIV's superior performance in estimating causal effects from time-series data with time-varying latent confounders.", "sections": [{"title": "I. INTRODUCTION", "content": "Estimating time-varying causal effects has attracted significant attention across various fields, such as healthcare [1], economics [2], climate science [3], [4], and epidemiology [5]. For example, evaluating the causal effects of a treatment on patient outcomes over time is crucial for developing treatment guidelines in medical contexts. The gold standard for estimating causal effects is to conduct a randomised controlled trial (RCT). However, RCTs are often impossible due to high costs or ethical concerns [6]-[8]. Therefore, causal effect estimation using observational data has become widely accepted in real-world applications.\nFew methods focus on estimating time-varying causal effects from time-series data, largely due to the inherent complexity of the dynamic interactions between variables over time [9]-[17]. In many real-world applications, estimating time-varying causal effects makes it possible to understand how a disease or policy evolves under different treatments or interventions over time [11]. For instance, as shown in Figure 1, medical researchers analyse the time-varying causal effects of treatment adherence on HIV-related outcomes in the Women's Interagency HIV Study [12], [13]. These analyses are essential for designing targeted interventions, optimizing treatment schedules, and improving long-term outcomes for individuals living with HIV.\nThe main obstacle in estimating time-varying causal effects from time-series data lies in handling time-varying confounders, which dynamically influence both the treatment and the outcome over time, particularly when time-varying latent confounders are also present [6], [18]. Neglecting to account for latent confounders can produce biased causal estimates, potentially leading to flawed conclusions and misguided decisions. Following the example above of the Women's Interagency HIV Study, variables such as HIV disease status, mental health, and psychological well-being are often unmeasured (i.e., latent confounders) and influence both treatment adherence and outcomes over time. Consequently, the causal effect of treatment adherence on HIV-related outcomes becomes unidentifiable due to the causal ambiguities caused by these latent confounders [19], [20].\nIn static settings, the instrumental variable (IV) approach is commonly used to deal with the latent confounding bias due to latent confounders in causal effect estimation [21], [22], but to date, few IV methods have been developed for time-series data [12], [13], [17]. In a static setting, a valid IV must satisfy three conditions: (i) being correlated with the treatment, (ii) being independent of all latent confounders, and (iii) influencing the outcome only through its effect on the treatment [5], [23], [24]. The last two conditions are not directly testable, so many existing IV methods rely on predefined IVs, typically chosen by experts based on domain knowledge. This reliance significantly limits the scope of IV applications. To mitigate this limitation, some approaches have been proposed to decompose or recover the information of IVs from observed variables [22], [25]; but assuming static settings.\nIn time-series data, identifying a valid time-varying IV from observed variables is more complex and requires stronger assumptions than in the static setting due to the intricate temporal relationships. Thus, most existing IV methods for time-series data require a predefined time-varying IV. For instance, Michael et al. [13] developed a weighted estimator with a known time-varying IV to evaluate the effect of hospital type on neonatal survival. Cui et al. [12] extended the standard inverse probability of treatment weighted (IPTW) estimation by identifying parameters of the marginal structural Cox model using a given time-varying IV. Recently, Cheng et al. [17] proposed a Time-varying IV Factor Model (TIFM) to learn the representation of an IV without relying on a predefined time-varying IV, with the assumption that a time-varying IV (though unobserved) exists and influences all time-varying covariates at each time step. However, these IV methods do not account for time-varying Conditional IV (CIV), where a variable serves as a valid IV when conditioned on a set of variables (a conditioning set), a general scenario in many applications. Consequently, learning a time-varying CIV directly from data remains an unresolved challenge.\nIn this work, we propose a novel Time-varying Conditional Instrumental Variables (CIV) for Debiasing causal effect estimation (TDCIV), to disentangle and learn the latent representations of a time-varying CIV and its conditioning set. With the aid of graphical causal models [19], [26], we propose the time-varying CIV within the full-time DAG Gfull. Specifically, TDCIV integrates Long Short-Term Memory (LSTM) [27], Variational Autoencoder (VAE) [28] and Conditional VAE (CVAE [29]) models to disentangle and learn the representations of a time-varying CIV and its conditioning set, thereby enabling the identification of causal effects in time-series data. These learned representations are then utilised in a CIV method using two stage least square (2SLS) [23] to identify time-varying causal effects in the presence of latent time-varying confounders. To the best of our knowledge, TDCIV is the first method capable of directly learning the representations of a time-varying CIV and its conditioning set from time-series data while addressing the complexities introduced by time-varying latent confounders.\nTherefore, the main contributions of our work can be summarised as follows:\n\u2022 We develop a novel time-varying CIV method for debiasing causal effect estimation (TDCIV). Our TDCIV method aims to disentangle and learn the representations of time-varying CIV and its conditioning set in order to identify causal effects in time-series data.\n\u2022 We theoretically prove the validity of the learned representations of the time-varying CIV and its conditioning set for identifying causal effects in time-series data with time-varying latent confounders.\n\u2022 Experiments performed on synthetic and real-world climate data validate the effectiveness of TDCIV in identifying causal effects in time-series data influenced by time-varying latent confounders.\nSection II introduces the notations and definitions for our problem and solutions. Section III introduces our proposed TDCIV method, detailing the concept of time-varying CIVs and its conditioning set, and the architecture of the TDCIV model. In Section IV, we present the experiments on both synthetic and real-world data to validate the effectiveness of TDCIV and discuss the results. Section V discusses related work in causal inference for time-series data, highlighting the uniqueness and contributions of our method. Finally, Section VI concludes the paper and proposes potential future work."}, {"title": "II. DEFINITIONS AND PROBLEM SETTING", "content": "In this paper, uppercase letters (e.g., X) denote variables, while lowercase letters (e.g., x) represent their values. Bold-faced uppercase and lowercase letters (e.g. X and x) are used to indicate sets of variables and their corresponding values, respectively.\nWe use  W(1)t=(W1t,...,Wpt)\u2208 WT, Xt=(X1t,...,Xpt)\u2208 Xt and U(2)t=(U1t,...,Upt)\u2208 Ur to denote the time-varying treatment, measured covariates, and latent covariates, respectively, over the time step t \u2208 {2,...,T}. Let Y(i)t+1=(Y2,...,Yi+1) \u2208 VT+1 indicate the outcome of interest from time steps over the time step t \u2208 {2, ...,T+1}. The sample index i is omitted unless explicitly required. At any given time step t, we represent k measured covariates as Xt = [Xt1,...,Xtp] and the k latent covariates as Ut = [Ut1,...,Utk].. Let D = (\u0425\u0442, W\u0442, \u04ae\u0442T+1) denote"}, {"title": "A. Notations and Basic Definitions", "content": "To identify ACEt(Wt, Yt+1) in time-series data, three important assumptions are required [10], [11], [16]. The relationship between the observed data and the potential outcomes is established through the consistency.\nAssumption 1 (Consistency). If Wt = wt, then Yt+1(Wt) = Yt+1, i.e., the potential outcome under wt is consistent with the observed outcome Yt+1."}, {"title": "B. Problem Definition", "content": "In this work, we consider a more general scenario that involves time-varying latent confounders, denoted by Ut, which influences both Wt and Yt+1. Consequently, using Equation (1) will result in a biased estimate, as Ut induces a spurious association between Wt and Yt+1 at each time step and cannot be measured directly, i.e., E[Yt+1(Wt) | Wt\u22121, Xt] \u2260 E[Yt+1 | Wt, Wt\u22121, Xt].\nTo address the bias caused by time-varying latent confounders Ut, we rewrite Assumption 2 (SRA) by splitting the covariates as measured covariates Xt and latent covariates Ut as Latent SRA (LSRA):\nAssumption 4 (LSRA). Yt+1(Wt) || Wt | Wt\u22121, Xt, \u016at.\nThe assumption states that Ut captures all latent confounding between Wt and Yt+1 such that the SRA holds when"}, {"title": "III. THE TDCIV METHOD", "content": "In this section, we introduce the TDCIV to identify causal effects in time-series data where latent confounders evolve over time. First, we present the time-varying CIV, along with its corresponding conditioning set in the full-time DAG Gfull. Next, we conduct a theoretical analysis of using CIV and representation learning for causal inference in time-series data. Finally, we detail our TDCIV for learning latent representations of a time-varying CIV St and the corresponding conditioning set Zt using proxy variables in time-series data."}, {"title": "A. Conditional IV (CIV) in a full-time DAG", "content": "The causal effects of ACEt (Wt, Yt+1) are not identifiable due to latent confounding bias introduced by \u016at [19], [26], [31]. IV approach is widely applied for mitigating the latent confounding bias with the aid of a pre-defined IV. However, most IV methods, such as Two-Stage Least Squares (2SLS) [23] and DeepIV [21], do not account for the temporal structure of time-series data, leading to biased estimates when directly applied to time-series data to identify causal effects. In this work, we aim to develop a novel time-varying CIV methodology for identifying the causal effects from time-series with time-varying latent confounders.\nFirst, we extend the concept of CIV from a static causal DAG [22], [24] to a full-time DAG Gfull, introducing the notion of a time-varying CIV. In the full-time DAG Gfull, the time-varying CIV St and its corresponding conditioning set Zt must satisfy the following three conditions:\nDefinition 3. (The concept of a time-varying CIV in a full-time DAG) Given a full-time DAG Gfull = (V,E) with V = X\u0141U\u016a\u0141U WtUYt+1, and Wt \u2192 Yt+1 in E at each time-step t, a variable St \u2208 Xt is a time-varying CIV relative to Wt \u2192 Yt+1 if there exists a set of variables Zt \u2208 Xt \\ {St} such that:\ni St \u22a5 Wt | St\u22121, Wt\u22121, Yt, zt, where Zt = (Z1,..., Zt),\nii St || Yt+1 | St\u22121, Wt\u22121, Yt, Zt in Gw\u2081, where Gw\u2081 is the graph obtained by removing Wt \u2192 Yt+1 at time step t from Gfull, and\niii \u2200Zt \u2208 Zt is not a descendant of Yt+1.\nThe third condition in Definition 3 is satisfied in our problem setting because Yt+1 is a sink node and has no descendant nodes other than its future statuses (e.g., Yt+2 and Yt+3). The first condition specifies that St is correlated with Wt, conditional on the historical values of St\u22121, Wt-1, Yt and Zt. This requirement for a time-varying CIV differs from that of a static CIV, as it necessitates consideration of past states, i.e., the historical data [26], [34]. Similarly, the condition (ii) also requires conditioning on the past states of St\u22121, Wt\u22121, Yt and Zt. We illustrate the time-varying CIV with the full-time DAG as shown in Figure 3.\nTo determine a time-varying CIV St, we need to identify a conditioning set Zt that, along with the past states of St-1, Wt-1, Yt and Zt-1, blocks all paths between Wt and Yt+1 in the manipulated causal DAG Gw\u2081. Note that the second condition of the time-varying CIV is not testable from time-series data since it is in a manipulated causal graph.\nThus, discovering or determining a valid time-varying CIV St and the corresponding conditioning set Zt from time-series data remains an unsolved problem. In this work, we explore how to learn the time-varying CIV St and its corresponding conditioning set Zt directly from time-series data, with as minimal domain knowledge as possible.\nTo achieve this goal, we extend the concept of proxy variables, originally used in static settings to recover latent variables [35], [36], to time-series data. In causal inference, it is commonly assumed that measurement errors of latent variables can be captured by proxy variables [37], thereby facilitating the recovery of the underlying latent variables from these proxies. Building on this concept, we assume that the time-varying CIV is a latent factor but can be approximated through the measurement errors present in the observed covariates, Xt, at each time step. Specifically, we assume that"}, {"title": "B. The scheme for learning CIV and its conditioning set", "content": "We aim to learn the time-varying CIV St from proxy variables and generate the conditioning set Zt based on the observed covariates Xt. The time-series data D is assumed to be generated by an underlying full-time DAG Gfull, as illustrated in Figure 3. In Gfull, St represents a latent CIV that is proxied by one of the observed variables in Xt. The conditioning set Zt is generated by leveraging Xt along with the historical information.\nIf we have obtained the two representations St and Zt from the observed covariates Xt, the following theorem ensures that the conditioning set Zt along with the historical data {St-1, Wt-1, Yt, Zt-1} instrumentalises St.\nTheorem 1. Given a full-time DAG Gfull = (V,E), where V = X\u0141 U \u016a\u0141 UWtUYt+1, and Wt \u2192 Yt+1 in E at each time-step t, and \u016a\u0165 representing the set of latent confounders affecting both Wt and Yt+1. Suppose that there exists at least one proxy variable of the latent CIV St in Xt. If the representations of St and Zt as shown in the full-time DAG in Figure 3 can be disentangled and learned from time-series data D = {X+, Wt, Yt}, then Zt along with the historical data {St-1, Wt-1, Yt, Zt\u22121} instrumentalises St relative to Wt\u2192 Yt+1 over time.\nProof. We prove that the representation of St is a time-varying CIV when conditioned on the representation Zt along with the historical data {St-1, Wt-1, Yt, Zt\u22121} relative to Wt \u2192 Yt+1, as specified by the proposed full-time DAG shown in Figure 3. In the full-time DAG Gfull, St is a cause of Wt, meaning that St and Wt are dependent given any set of covariates, i.e., St \u22a5 Wt | St\u22121, Wt-1, Yt, Zt holds. Thus, St satisfies the first condition of time-varying CIV in Definition 3. Next, in the manipulated causal DAG Gw\u2081, all arrows out of Wt are removed. Consequently, the back-door paths between St and Yt+1 include paths, such as Xt \u2192 Zt \u2192 Yt+1, as well as all past statuses of St\u22121, Wt-1, Yt and Zt-1, which are also directed into Yt+1. All paths through Zt into Yt+1 are blocked by Zt because these paths end in the sub-path Xt \u2192 Zt \u2192 Yt+1, forming a chain in Gw\u2081. These paths consisting of all past statuses of St-1, Wt-1 and Yt, which are also directed into Yt+1, are blocked by the historical data {St-1, Wt-1, Yt} since these paths terminated at a direct edge, i.e., St\u22121 \u2192 Yt+1 [34]. Therefore, St is d-separated from Yt+1 by conditioning on St\u22121, Wt\u22121, Yt and Zt, which blocks all back-door paths between St and Yt+1. Thus, St || Yt+1 | St\u22121, Wt\u22121, Yt, Zt holds, and the representations of St and Zt satisfy the second condition of time-varying CIV in Definition 3. Finally, since Yt+1 is a sink node in G, Zt does not contain any descendants of Yt+1. Thus, the representations of St and Zt satisfy the third condition of time-varying CIV in Definition 3.\nTherefore, we conclude that Zt along with the historical data {St-1, Wt-1, Yt, Zt-1} instrumentalises St relative to Wt \u2192 Yt+1 for unbiased ACEt (Wt, Yt+1) estimation from time-series data with time-varying latent confounders."}, {"title": "C. The Proposed TDCIV Method", "content": "Inspiration from the effective use of proxy variables in causal inference [22], [35]-[37], [39], leverage the strengths of deep generative models [28], [38] to directly capture the sequential representations of CIV St and its conditioning set Zt from time-series data at each time step. We explore the use of VAEs and LSTM networks [27] to disentangle and learn latent representations of time-varying CIV and its corresponding conditioning set.\nTo capture the historical information inherent in Xt, Wt-1 and Yt (i.e., Ht), we utilise LSTM as a core component of the TDCIV method. By integrating LSTM at each time step, the method effectively models the historical dependencies present in time-series data. To disentangle and learn St and Zt from Xt based on the causal DAG as shown in Figure 3, we also employ VAE and CVAE models.\nWe demonstrate our architecture of the TDCIV in Figure 4. Our TDCIV consists of three components: an LSTM, an inference network, and a generative network. Specifically, the inference and generative networks are combined to form the posterior distributions p(St | St\u22121, Ht) and p(Zt | Zt\u22121, Ht). As depicted in the full-time causal DAG in Figure 3, these distributions for the two latent representations, St and Zt, at time step t rely on the historical information Ht, as well as their respective previous states, St-1 and Zt-1. These latent representations capture the time-varying CIV and its conditioning set at time step t.\nAs discussed previously, we use the LSTM component as a core part of our TDCIV to capture the historical information between Xt, Wt-1 and Yt. The LSTM is defined as follows:\nHt = LSTM(\u03be);\nHt = LSTM(Ht-1), (2)\nwhere \u03be represents the randomly initialised parameter at the initial step, which is then trained jointly with the other parameters of the LSTM. Then, Ht is passed forward to subsequent time steps and is used alongside the inference and generative networks to model the latent CIV and its conditioning set. The LSTM's capability to retain long-term associations in the sequence ensures that the historical context is effectively integrated into the learning process of our TDCIV. Upon training the LSTM, we extract Ht and use it as an input for the subsequent representation learning process."}, {"title": "Algorithm 1 TDCIV (Time-varying CIV using Deep generative model)", "content": "Input: Time-series data D = {Xt, Wt, Yt} for time steps\nt = 1,2,..., T\nOutput: Estimated ACEt(Wt, Yt+1)\nInitialise LSTM model with hidden states Ht to capture\ntime dependencies in D\nfor each time step t do\nCompute hidden state Ht using Equation 2;\nInference Step:\nUse Ht in VAE and CVAE: q(Zt | Ht, Zt\u22121) and q(St |\nHt, St-1) are employed to learn the representations of\nthe time-varying CIV St and its conditioning set Zt;\nGenerate Step:\nReconstruction distributions: p(Ht | \u017dt, St), p(Wt |\nZt, St, Ht) and p(Yt+1 | Zt, Ht);\nUpdate parameters of LTDCIV;\nPrediction Step:\nObtain St and Z\u0141, and use them in 2SLS to estimate the\naverage causal effect ACEt(Wt, Yt+1) at time step t;\nend for\nReturn: ACE (Wt, Yt+1)\nTo ensure that St captures as much information as possible about the CIV, and that Zt captures more confounding information between St and Yt+1, we add two predictors into our ELBO to predict Wt and Yt+1, respectively, as done in previous work [22], [37], [39]. In our TDCIV, Wt is predicted by Zt, St, and Ht, while the outcome Yt+1 is predicted by Zt, and Ht. Thus, our final objective function is expressed as:\nLTDCIV = -M+aEq [log p(Wt | Zt, St, Ht)]\n+ BEq [logp(Yt+1 | Zt, Ht)], (10)\nwhere a and \u1e9e are hyper-parameters that balance the two additional predictors.\nTo estimate ACEt(Wt, Yt+1), we extract the time-varying representations St and Z\u0142 from our TDCIV model at each time step t. The representation St captures the information of time-varying CIV, while Zt, along with Ht, forms the conditioning set. Once both representations are obtained, we employ the CIV method [23], [34] to estimate ACEt(Wt, Yt+1) at time step t. In the linear case, ACEt(Wt, Yt+1) is calculated as:\nACEt (Wt, Yt+1) = St*Yt+1*(Zt, Ht) / St*Wt*(Zt, Ht) (11)\nwhere St*Yt+1*(Zt, Ht) and St*Wt*(Z\u0141,H+) represent the esti-\nmated causal effect of St on Yt+1 and on Wt, conditioned on\nZt and Ht based on Theorem 1.\nWe present the pseudo-code for the TDCIV method in Algorithm 1. In detail, our TDCIV begins by initialising an LSTM model to capture relevant historical information and encodes the sequential information into Ht. Then, Ht is used in VAE and CVAE to disentangle and learn the latent representations of time-varying CIV St and the corresponding conditioning set Zt. Next, TDCIV uses the learned latent representations to reconstruct the observational time-series data and update the parameters of LTDCIV. Finally, we extract St and Zt at each time step t and use them in 2SLS to estimate ACE+(Wt, Yt+1). The output of our TDCIV is a list of ACEt (Wt, Yt+1) over time.\nTime Complexity: The time complexity of TDCIV consists of three components: the computational costs associated with the LSTM model, the VAE/CVAE models, and the 2SLS estimation. A detailed analysis of each component is provided below: (1) The time complexity of LSTM model: The LSTM captures temporal dependencies with a LSTM layer that requires O(Tm\u00b2) operations per layer, where m is the number of hidden units. Thus, the total time complexity of the LSTM is O(L*T*m\u00b2), where L is the number of LSTM layers. (2) The time complexity of VAE and CVAE Models: The VAE and CVAE learn latent representations of St and Zt from time-series data D. The encoding and decoding for each time step have a complexity O(\u03c6\u00b2 +\u03c6\u03c6), leading to O(n\u00b7T\u00b7 (\u03c6\u00b2 +\u03c6\u03c6)), where 4 is the dimensionality of the latent space and $ is the number of neurons per dense layer. (3) The time complexity of 2SLS Estimation: The 2SLS step estimates the treatment effect ACE+(Wt, Yt+1) with complexity O(nn\u00b2), where \u03b7 is the number of Xt.\nTherefore, the total time complexity of TDCIV can be expressed as O(n \u00b7 T \u00b7 (m\u00b2 + \u00a2\u00b2 + \u03c6\u03c6) + n\u03b7\u00b2).\nThe time complexity analysis of TDCIV highlights that its effectiveness is influenced by T, m, \u03c6, and 7. This also indicates that the TDCIV method is well-suited for high-dimensional, long-sequence time-series data, particularly in scenarios where time-varying latent confounders are present.\nLimitations: The performance of TDCIV relies on the assumption regarding the measurement errors of the time-varying CIV present in Xt. If this assumption is violated, TDCIV is likely to produce estimations with high variance. Additionally, he TDCIV requires that the latent representations learned by the VAE and CVAE correct. Failure to adequately learn these representations due to insufficient data, suboptimal hyperparameter settings, or overly complex models, may negatively impact performance. In such scenarios, we recommend that users conduct sensitivity analyses."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we examine the effectiveness of TDCIV in accurately estimating ACEt(Wt, Yt+1) in time-series data D. The experiments are divided into two parts: evaluation using synthetic datasets and a real-world case study involving climate data. In the first part, we construct synthetic datasets based on the methodologies proposed in [11], [40]. These datasets provide access to ground truth, enabling precise computation of the true causal effects over time. Additionally, we perform sensitivity analyses to evaluate TDCIV's robustness under various parameter configurations. In the second part, we validate the practical applicability of TDCIV by applying it to climate data. This case study demonstrates the method's effectiveness in uncovering causal relationships in complex, real-world scenarios."}, {"title": "A. Experiment Setup", "content": "a) Data Generation: To enable fair comparisons, we conduct simulation studies using multiple synthetic datasets. To emulate real-world scenarios, we adopt the approach outlined in [11], employing p-order autoregressive processes to generate synthetic data. At each time step t, the observed covariates Xt and latent covariates Ut are defined as follows:\nXt = 1/p \u03a3 (aixt-i + WiWt-i) + \u00a3x, (12)\nUt = 1/p \u03a3 (BiUti + diWt-i) + \u03b5\u03c5\u00b7\nThe parameters in Equation 12 are sampled as ai, di ~ \u039d(0,0.52), \u03c9\u03af, \u03b2\u03af ~ N(1 \u2212 (i/p), (i/p)2), and noise terms \u03b5\u03c7, \u03b5\u03c5 ~ N(0,0.012). The latent time-varying CIV St is generated as follows:\nSt = 1/p \u03a3 (St-i) + Es (13)\nThe conditioning set Zt is generated as:\nZt = 1/p \u03a3 (Zt-i + Xt) + \u03b5\u03c5, (14)\nThe time-varying treatment variable Wt depends on the latent CIV St, its conditioning set Zt, latent confounders Ut and covariates Xt:\n\u03b8t = \u03bcxXt + \u03bc\u03c5\u00dbt + \u00b5sSt + pz2t,\nWt | \u03b8t ~ Bernoulli(\u03c3(c\u00b7 \u03b8t)), (15)\nwhere Xt, \u00dbt, St, 2t denote the sum for the measured covariates, latent covariates, latent CIV, and its conditioning set, respectively, over the last p time steps. The function \u03c3(\u00b7) denotes the sigmoid function, and \u03bcx\u2081, \u03bc\u03c5\u1e6d, \u00b5St, \u03bczt, c ~ N(0, 12).\nThe time-varying outcome variable Yt+1 is generated by a function with the time-varying treatment variable Wt, the conditioning set Zt and latent variables Ut:\nYt+1 = pwWt + pzZt + puUt, (16)\nwhere pw, pz, \u03c1\u03c5 = 0.5.\nWe generate synthetic time-series datasets, D, with sample sizes of 2k, 4k, 6k, and 8k. To minimise potential bias from the data generation process, we produce 30 datasets for each sample size. Time dependencies are introduced by configuring p to 1 and 3. Both the observed covariates, Xt, and the latent covariates are assigned a dimensionality of 3.\nb) Methods for Comparison: We evaluate our TDCIV against several state-of-the-art causal effect estimators to highlight its effectiveness. These methods include:\n1) LinearDML (LDML) [41], which addresses reverse causal metric bias using a cross-fitting strategy.\n2) NonParamDML (NPDML) [41], a non-parametric version of Double ML estimators that allows for arbitrary machine learning models in the final stage.\n3) SparseLinearDML (SLDML) [32], an adaptation of LinearDML where the loss function is modified by incorporating L\u2081 regularisation.\n4) CausalForestDML (CFDML) [42], which employs two random forests to estimate causal effects by predicting two potential outcomes.\n5) KernelDML (KDML) [43], which integrates dimensionality reduction techniques with kernel methods.\n6) Meta-learner [44], specifically the X-learner (XLer) and the T-learner (TLer).\nIt is important to note that methods designed for causal inference in static settings may lead to biased estimates when applied to time-series data, as they fail to account for the temporal dependencies between covariates. In our experiments, we do not compare with IV-based or CIV-based methods developed for static settings, as these approaches assume the IV or CIV is explicitly known and included in the dataset. However, in our problem setting, the IV or CIV is unmeasured and instead proxied by observed variables.\nSeveral methods exist for estimating time-varying causal effects in time-series data, including Standard Marginal Structural Models [18], [45], Recurrent Marginal Structural Networks [46], and time-series Deconfounder [11]. However, these methods are completely different from the tasks we focus on; they produce predictions of potential outcomes, while our TDCIV method focuses on the estimation of ACEt for each time step.\nThere are two works that are similar to our setting. CIV.VAE [22] learns the representation of CIV and its conditioning set in a static setting. We do not include this method as TDCIV works with time-series data. TIFM [17] is proposed for handling time-varying latent confounders in time-series data using standard IV. We involve this method as a comparison in our experiment. However, please note that TIFM works with IV rather than CIV, which imposes a more restrictive assumption compared to TDCIV.\nc) Evaluation Criterion: To assess the performance of TDCIV and the baseline models, we employ the absolute error, |A\u0108Et - ACE\u2081| as the evaluation metric, where A\u0108Et represents the estimated value, and ACEt denotes the ground truth."}, {"title": "V. RELATED WORK", "content": "In this section, we reviewed works related to our TDCIV, including methods using static IVs, methods for causal effect estimation in time-series data, and methods leveraging time-varying IVs.\na) Methods using static IVs: Most existing methods for causal effect estimation from observational data assume that all confounders, i.e., variables that affect both the treatment variable and the outcome variable, are static [7], meaning they do not vary over time. Numerous approaches have been developed to estimate causal effects in static environments, where all variables are assumed to remain constant [22], [37], [39], [52]-[55]. However, this assumption is unrealistic in many real-world scenarios and limits their ability to discover and utilise causal relationships in temporal settings.\nIn contrast, our TDCIV specifically addresses the challenge of latent time-dependent confounders in single treatment-outcome relationships by developing a novel CIV-based approach for time-series data.\nc) Methods leveraging time-varying IVs: Recently, several time-varying IV methods have been developed for estimating time-varying causal effects, particularly in the presence of time-to-event outcomes with latent time-varying confounders. For instance, Martinussen et al. [58] proposed a time-varying IV estimator within a semiparametric structural cumulative model, utilising a structural accelerated failure model. Furthermore, Michael et al. [13] explored the identification and estimation of marginal structural mean models (MSMMs) for time-varying treatments, incorporating the use of time-varying IVs. Cui et al. [12] extended the work of Michael et al. by providing sufficient conditions for parameter identification in MSMs using temporal data, leveraging time-varying IVs derived from domain knowledge. However, these time-varying IV-based methods rely on external domain expertise to define a valid time-varying IV.\nIn contrast, our TDCIV aims to disentangle and learn the representations of time-varying CIV and its conditioning set from time-series data, thereby minimising reliance on pre-existing domain knowledge."}, {"title": "VI. CONCLUSION", "content": "In our paper, we develop a novel debiased method, TDCIV, for learning representations of a time-varying CIV and its conditioning set to estimate causal effect in time-series data. TDCIV aims to address confounding bias caused by time-varying latent confounders. To achieve this, we generalise the concept of CIV in causal DAG from static to time-varying settings. Our TDCIV integrates VAEs with LSTM models to capture the complex temporal relationships between latent factors and measured variables in time-series data. By disentangling and reconstructing the latent representations St and Zt, and integrating them with historical data, TDCIV effectively mitigates the confounding bias introduced by time-varying latent confounders. Experimental evaluations on both synthetic and real-world datasets validate the effectiveness of TDCIV in estimating causal effects in time-series data influenced by latent confounders. In future work, we aim to explore broader applications of the time-varying CIV in more complex scenarios and extend its utility to address additional challenges in time-series causal inference."}]}