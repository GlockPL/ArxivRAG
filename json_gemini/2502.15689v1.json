{"title": "Knowledge Graphs: The Future of Data Integration and Insightful Discovery", "authors": ["Saher Mohamed", "Kirollos Farah", "Abdelrahman Lotfy", "Kareem Rizk", "Abdelrahman Saeed", "Shahenda Mohamed", "Ghada Khouriba", "Tamer Arafa"], "abstract": "Knowledge graphs are now used as an eficient way of repre-senting and connecting information on various concepts. It can be applied to reasoning, question answering or knowledge base completion tasks. Knowledge graphs are an organized way of presenting data, with one feature being that data points are linked to each other. There are also several advantages to using knowledge graphs for academic researchers and scientists working with big and diverse data in various fields. The ability to put together a large amount of information from various fields into a single database enables researchers to come up with new questions that do not have to belong to a single discipline. For instance, an aca-demic in medicine who focuses on analyzing the social consequences of an illness might use data from journals, patients' records, census data and economic statistics to identify new information on vulnerable com-munities. Making these connections explicit can be useful in coming up with new ideas for research or in identifying potential interdisciplinary projects. It is imperative that academia incorporate the use of knowl-edge graphs in order to propel studies in times of big and complex data. A knowledge graph creates a web of data points which are nodes and edges that join the nodes together. Nodes represent single entities, while edges depict the connections between those things. This structure of the network enables knowledge graphs to create relationships between the different data points and unveil them to be easily navigated, compre-hended and utilized for diverse purposes. This graph structure also en-ables representation of relations and connections between different nodes or vertices such as, persons, companies, ideas or occasions. Traditional databases and knowledge representations struggle to effectively capture the wide range of relationships inherent in unstructured data sources like text. Knowledge graphs address this challenge by providing an intercon-nected semantic framework that can represent diverse types of entities and their associated attributes and interrelationships. There are several strategies that organizations can adopt when developing their knowl-edge graphs, and this depends on the types of data sources to be used, the kind of entities to be incorporated in the graph and the number of users and uses of the graph. One approach is to begin with such care-fully selected seed data as knowledge bases, and then use techniques such as named entity recognition, relationship extraction, and entity disam-biguation on free text sources to identify new nodes and edge for the graph. Knowledge graphs can also be used to aggregate different", "sections": [{"title": "Introduction", "content": "The goal of artificial intelligence (AI) is to mimic human thinking, which involves more than just learning from specialized books. It's similar to how a doctor combines textbook knowledge with real-life practice. While AI models have made progress in learning from experience, they often miss out on the valuable insights found in structured literature, affecting their explainability and performance. Knowledge graphs (KGs) have become an important tool in this area. They organize human knowledge in a way that's similar to how books are structured, making it easy for Al to access and use specific information. KGs provide a strong framework to represent various entities and their relationships, making them useful for many fields like social interactions, biological processes, academic references, and transport systems. These graphs are essential for Al systems to understand and operate effectively Hogan et al. (2021).\nThere are two main types of knowledge graphs: static and dynamic Liang et al. (2022). Static KGs offer a stable foundation of information for AI, much like the base of a building. In contrast, dynamic KGs can adapt and grow with new data and changing contexts, similar to an ever-changing landscape. Building and"}, {"title": "Background and Terminology", "content": "The history of knowledge graphs dates back to the development of semantic networks in artificial intelligence during the early 1960s. These networks repre-sented knowledge as nodes (concepts) and edges (relationships), creating a graph that illustrates the interrelation of concepts. Ontologies emerged in the 1990s, formalizing the representation of a set of concepts within a domain and the re-lationships between them to facilitate knowledge sharing and domain reasoning. At the turn of the millennium, Tim Berners-Lee Berners-Lee et al. (2001)in-troduced the concept of the Semantic Web, an extension of the current web pro-viding well-defined meaning to information, allowing both computers and people to work in cooperation. Linked Data principles were crucial to this vision, linking\ndata for easy machine processing.\nThe term \"knowledge graph\" gained popularity when Google launched its Knowledge Graph in 2012. This vast network of interlinked descriptions of en-tities aimed to deliver more relevant search results by understanding entity se-mantics.The essence of a knowledge graph, according to a cited work, can be captured as:\n\"A knowledge graph is a multi-relational graph composed of entities and re-lations that are regarded as nodes and different types of edges, respectively.\"\nillustrates the evolution of technologies and concepts in the field of knowl-edge graphs from the 1960s to the present. It shows a timeline with key mile-stones, starting with semantic networks in the 1960s, followed by ontologies in the 1970s, the introduction of Linked Data principles around the 1980s, and the development of knowledge graphs, with a specific emphasis on the Semantic Web and dynamic knowledge graphs.\nSince then, knowledge graphs utilize a static structure that provides a snap-shot of knowledge at a single point in time. However, this approach has limita-tions in representing knowledge that is constantly evolving. Dynamic knowledge graphs address this by enabling the graph structure and information to change over time as new data is incorporated. This adaptive capability of dynamic knowledge graphs allows for more robust and reasoned modeling of knowledge domains that are rapidly changing and interconnected. As additional related data is acquired, the knowledge graph can automatically restructure and up-date itself to represent the most current understanding. Outdated information can be removed or modified while newly discovered relationships and concepts"}, {"title": "Motivation", "content": "There is a significant academic motivation behind the development of dynamic knowledge graphs. The reasoning capability in static knowledge graphs does not change quickly with the addition of new information or the discovery of new linkage. Since knowledge domains online are evolving at such high speed, static graphs do not easily upgrade with updates that understand new relationships or discover them. Dynamic knowledge graphs tackle this challenge by providing an avenue for continual knowledge improvement. Unlike static structures with"}, {"title": "How can we develop a data integration methodology that effectively combines heterogeneous data sources to con-struct static knowledge graphs, ensuring data reliability and accuracy?(RQ1)", "content": ""}, {"title": "Data Source Diversity and Complexity", "content": "Knowledge graphs (KGs) can be categorized into two types: generic knowledge graphs, which cover multiple domains with encyclopedic content such as Wiki-data, YAGO Suchanek et al. (2007), Freebase Bollacker et al. (2008), and DB-pedia Lehmann et al. (2015), and domain-specific knowledge graphs, focused on narrower domains for specific problems or industries Abu-Salih (2021).\nKG development typically follows either a top-down or a bottom-up ap-proach. The top-down approach involves defining the ontology or data schema first, followed by knowledge extraction based on this ontology Li et al. (2020a). In contrast, the bottom-up approach starts with knowledge extraction from data, with the ontology of the KG being defined based on the extracted information Li et al. (2020a).\nThe definition of a knowledge graph's domain is instrumental in the effective identification of data sources and the subsequent determination of data extrac-tion methodologies Yan et al. (2018). This domain delineation can range from broad, encompassing areas such as education Chen et al. (2018), healthcare Li et al. (2020b), and agriculture Chen et al. (2019), to more specific sectors like so-cial media analysis Lian et al. (2017) or autonomous vehicle technologies Luettin et al. (2022).\nUpon establishing the domain, the next critical step is the identification of suitable data sources. These sources, varying in structure and format, signifi-cantly influence both the overall development process of the knowledge graph and the selection of appropriate knowledge extraction techniques. Typically, data may be categorized as structured, semi-structured, or unstructured. Structured data, such as those found in relational databases or tabular formats, are char-acterized by a clear and defined structure. Semi-structured data, like XML files,"}, {"title": "Knowledge Extraction Techniques from Unstructured data", "content": "This section focuses only on unstructured data, specifically text."}, {"title": "Large Language Model in Knowledge Extraction", "content": "Large language models have become invaluable tools for performing knowledge extraction from unstructured text sources. Due to their pre-training on vast amounts of text, LLMs like BERT, GPT-3 and LLMAs have developed strong abilities to analyze language, identify patterns and relationships within text. One of the primary ways LLMs facilitate knowledge extraction is through named entity recognition. By leveraging their linguistic understanding, LLMs can efi-ciently identify entities mentioned in text like people, organizations, locations, products and more. This extracted entity information can be structured into a knowledge base or graph. In addition, LLMs are also capable of performing"}, {"title": "Natural Language Processing in Knowledge extraction", "content": "Sophisticated natural language processing methods are needed to build knowl-edge graphs from unstructured text sources. A significant preliminary process is the pretreatment of input data using linguistic analysis tools. This makes it pos-sible to extract entities and relationships from raw sentences based on syntactic patterns. Many popular NLP libraries, such as spaCy Srinivasa-Desikan (2018), offer strong neural network models that are pre-trained on large corpora and allow the identification of parts of speech, noun phrases, etc., which is necessary for data extraction purposes. After extracting entities and relations, they must be organized into a graph representation with nodes connected to show seman-tic relationships. NetworkX is a great graph library capable of producing the"}, {"title": "How can we transform a static knowledge graph into a dynamic knowledge graph? (RQ2)", "content": ""}, {"title": "Clustering Techniques in Link Prediction and Adding New En-tities", "content": ""}, {"title": "Traditional Clustering", "content": "Clustering the entities in the knowledge graph allows groups of similar nodes to be identified, helping to understand the underlying themes or topics repre-sented. A number of algorithms were tested to cluster the entities derived from"}, {"title": "Graph Neural Network in Link Prediction and Adding New En-tities", "content": "One of the major challenges faced with knowledge graphs is disconnected sub-graphs, where entities are not fully connected within the graph. When subgraphs are disconnected, the entities contained within them cannot benefit from poten-tial relationships and links to other parts of the knowledge graph. This fragmen-tation of information limits the insights that can be discovered from the graph. One effective approach for addressing disconnected subgraphs is link prediction using graph neural networks (GNNs). GNNs have the advantage of being able to learn representations of nodes by aggregating features from their neighbor-"}, {"title": "Traditional Embedding Techniques in Link Predictions and Adding New Entities", "content": "Traditional embedding techniques like TransE learn embeddings for entities and relations in knowledge graphs to predict missing links. These models learn low-dimensional vector representations of entities and relationships where similar entities and relationships are closer in the embedding space. The models are rel-atively simple and eficient to train. However, these techniques also have several"}, {"title": "Extract triples and predict relations using pre-trained ontology", "content": "there are three distinct methods for extracting triples and predicting relations between entities: DBpediaOntology class connected to the API of DBpedia Spot-light, ConceptNetOntology connecting with an API from ConceptNet, and on top of that, Rebel model retrieved from Babelscape assemblage. Every approach targets the problem of information retrieval from unstructured text; however, they vary in the applied approaches and purposes."}, {"title": "How can retrieved information from knowledge graphs play an important role in different domains? (RQ3)", "content": ""}, {"title": "Knowledge graph in Explainable AI", "content": "KG explainability can be utilized at different stages in the AI development pro-cess. Typically, KG explainability is performed before (pre-modeling explainabil-ity), during (explainable modeling), or after (post-modeling explainability) the AI modeling stage Rajabi and Etminani (2022).\nPre-modeling Explainability: This method works independently of the model and usually involves using a KG before selecting a model, as it applies only to the data itself. Pre-modeling explainability can include constructing KGs from a dataset or standardizing a dataset with KGs. For instance, in one study, a KG was used to transfer features in a zero-shot learning model and generate explanations for unseen classes in an image classification problem Geng et al. (2021).\nIn-modeling Explainability: This approach focuses on the model's inner workings, such as its mathematical aspects, and uses KGs to generate expla-nations during the training phase. For example, a KG was used to improve a deep learning model's performance on an image classification task by training the model to predict every node in a knowledge graph and then propagating information between the nodes to refine predictions Daniels et al. (2020).\nPost-modeling Explainability: These techniques describe the application of a KG after a model has been trained. They enhance the explainability of AI models by using KGs to provide insights into what the trained model has learned without altering the underlying model. For example, in one study, a KG was applied to a graph neural network model to capture important features and relationships in a set of news articles, using the relevance scores of entities to guide the embedding of the articles Cui et al. (2020)."}, {"title": "Knowledge Graph for Automatic Coding", "content": "Knowledge graphs have been proven extremely useful in powering diverse ap-plications in various applications such as program search, code understanding, bug detection, and code automation. GraphGen4Code Abdelaziz et al. (2021) is therefore designed as a toolkit to build knowledge graphs for program code. This toolkit can construct code representations that represent actual program flow along with natural language descriptions of API calls when they exist to enhance code representations.\nCOCOMIC Ding et al. (2022) is a framework on top of existing code lan-guage models that jointly learns in-file and cross-file context for code comple-tion.Authors also build CCFINDER. The tool has two main steps: (1) Analyze the program dependencies within the project and parse the source code to ex-tract both the bird's-eye view of the whole project and the code details of each module. With that, CCFINDER builds the project context graph: graph nodes"}, {"title": "Knowledge Graph for Self Driving Car", "content": "Zheng and Kordjamshidi (2022) propose the Dynamic Relevance Graph Network (DRGN) model to tackle the challenging problem of commonsense question an-swering using external knowledge from a knowledge graph. Existing approaches have dificulty reasoning over knowledge graphs when edges are missing in the extracted subgraph needed for reasoning chains, or when handling questions with negative words. DRGN operates on a knowledge graph subgraph contain-ing entities mentioned in the question and answers. It adds the question as a node to provide contextual information. Unlike prior models, DRGN com-putes dynamic relevance matrices between node representations at each layer of a relational graph network. This allows it to establish direct connections and potentially recover missing edges by learning new edges based on chang-ing relevance scores. Computing relevance between the question node and entity nodes also helps leverage contextual cues better [1]. Experiments on the Com-monsenseQA and OpenbookQA benchmarks demonstrate that DRGN achieves state-of-the-art performance, obtaining particularly strong results on negatively worded questions through its use of question-entity relevance. Ablation studies validate that component such as the knowledge graph, relational edges, question node, and dynamic relevance computation contribute to DRGN's effectiveness at capturing relationships for improved commonsense question answering.\nKrause (2022) proposes the Navi Approach as a method to enable dynamic knowledge graph embeddings. Currently, most existing embedding methods as-sume a static knowledge graph structure. When updates occur through additions or deletions of nodes and edges over time, the embeddings need to be retrained from scratch. However, retraining for large graphs is computationally expensive and can affect downstream machine learning models that utilize the embeddings. The Navi Approach works to address this issue by reconstructing embeddings locally based only on the neighborhood of each node, rather than requiring a full retraining of the embedding model. It leverages existing static embeddings as a starting point and independently derives new embeddings for nodes based on the embeddings of neighboring nodes. Two different types of \"Navi Layers\" are presented - the Connectivity Layer which takes the average of neighbor embed-dings, and the Translational Layer which applies transformations like TransE. Composite reconstructions can be obtained by combining multiple Navi Lay-ers. Preliminary evaluation on benchmark datasets shows the reconstructed embeddings with the Navi Approach can maintain or outperform the accuracy of retraining embeddings from scratch for entity classification, indicating the hypotheses around enabling dynamic embeddings may be answered positively. The approach is evaluated in tasks like link prediction and entity classification"}, {"title": "Knowledge Graph for Climate Change", "content": "Wu et al. (2022a) proposed a climate knowledge graph to integrate multiple climate data sources into a linked data platform for facilitating cross-domain analysis between climate data and other domains. The knowledge graph primar-ily uses data from NOAA climate summaries, OpenStreetMap, and Wikipedia data, and supports joint SPARQL queries across these datasets. A workflow is presented to automatically retrieve and transform NOAA climate data into RDF graphs aligned with a custom ontology, and publish it as linked open data. Ge-ographic data is integrated from OpenStreetMap and linked to Wikipedia data to provide contextual information. The resulting linked data knowledge graph is deployed in an SPARQL endpoint and URIs are dereferenceable for browsing. A web interface and tutorial assist climate researchers in exploring the knowledge graph and trying sample queries. Usability testing indicates the platform suc-ceeds in providing understandable access. Key benefits include easy multi-source integration, open data enrichment, dynamic updates, and added explain ability via semantically structured data."}, {"title": "Conclusion", "content": "Knowledge graphs are really improving how data is used these days. By con-necting different types of information together and learning from examples, it helps integrate, understand, and apply data in many fields much better. Systems that use knowledge graphs can adapt more easily to new situations because they combine rules about specific topics with patterns found in data. This allows knowledge graphs to build very thorough models of things and how they relate to each other.Finding ways for knowledge graphs to eficiently manage and link together even larger sets of data with more complex webs of connections is a big focus. In addition, ensuring these models are understandable will help en-sure they can be trusted for serious applications. By continuing advancements, knowledge graphs will play an increasingly important job in helping many in-dustries gain benefits from massive amounts of information. As knowledge graph technologies continue to evolve, they will further enhance our ability to leverage data, ultimately driving innovation and eficiency across various domains."}]}