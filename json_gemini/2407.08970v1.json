{"title": "Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions", "authors": ["Tingwei Zhang", "Collin Zhang", "John X. Morris", "Eugene Bagdasaryan", "Vitaly Shmatikov"], "abstract": "We introduce a new type of indirect injection vulnerabilities in language models that operate on images: hidden \u201cmeta-instructions\u201d that influence how the model interprets the image and steer the model's outputs to express an adversary-chosen style, sentiment, or point of view.\nWe explain how to create meta-instructions by generating images that act as soft prompts. Unlike jailbreaking attacks and adversarial examples, outputs resulting from these images are plausible and based on the visual content of the image, yet follow the adversary's (meta-)instructions. We describe the risks of these attacks, including spam, misinformation, and spin, evaluate their efficacy for multiple visual language models and adversarial meta-objectives, and demonstrate how they can \u201cunlock\u201d the capabilities of the underlying language models that are unavailable via explicit text instructions. Finally, we discuss defenses against these attacks.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) operating on third-party content\u2014webpages, wikis, forums, social media, emails and messages, and user-generated content in general\u2014are vulnerable to indirect prompt injection attacks [13]. By adding prompts to the text content under their control, adversaries can try to influence outputs and actions generated by LLMs when processing this content.\nMany modern LLMs accept inputs in multiple modalities, in particular images. We refer to LLMs that operate on images as Visual Language Models (VLMs). Like their text-only counterparts, VLMs are vulnerable to various direct and indirect prompt injection attacks. For example, jailbreaking attacks use image perturbations to cause models to generate toxic or unsafe outputs, even if the same models refuse to generate such outputs in response to text prompts. Adversarial examples and related attacks cause VLMs to generate outputs chosen by the adversary that are unrelated to the visual content of input images. We discuss these attacks in Sections 2.3 and 2.4.\nConceptual contributions. We introduce and evaluate a new class of indirect attacks on visual language models. Adversarial meta-instructions are stealthy perturbations that steer outputs produced by a VLM in response to an image so that these outputs satisfy some adversarial meta-objective. Meta-instructions preserve the visual content of the image, as interpreted by the VLM. The resulting responses are thus"}, {"title": "2. Background and Related Work", "content": "We focus on visual language models (VLMs) that accept text and image inputs. These models typically combine a pre-trained generative language model such as Llama [34] with two encoder inputs: a text encoder and an image (visual) encoder [17].\nVLMs are intended to accurately respond to prompts about their input images and maintain a conversation with the user regarding the image.\nLet $\\theta$ be a VLM that contains the text encoder $\\theta_{text}^{enc}$, the image encoder $\\theta_{image}^{enc}$, and the language decoder $\\theta_{dec}$. The text of the prompt $p \\in P$, e.g., \"describe the image\", is fed into the text encoder $\\theta_{text}^{enc}$, and the image $x \\in X$ is fed into the image encoder. Their respective embeddings produced by the encoders are concatenated and fed into the language decoder:\n$\\theta(p, x) = \\theta_{dec}(\\theta_{text}^{enc}(p) + \\theta_{image}^{enc}(x)) = y$ (1)\nAn instruction-tuned VLM performs the task of matching instruction prompts and images to text outputs, i.e., $(P, X) \\rightarrow Y$.\nBrown et al. [6] demonstrate that prompt design can significantly impact the behavior of language models. However, creating effective prompts requires substantial human effort, making the process costly. Furthermore, automatically optimizing prompts is inefficient because text prompts are discrete.\nLester et al. [16] introduce the concept of a \"soft prompt\" as a parameter-efficient fine-tuning method. In Equation 1, the language model takes prompts $p$ and encodes them into $\\theta_{text}^{enc}(p)$. The text of $p$ is the \u201chard prompt", "soft prompt": "Hard prompts are discrete and thus challenging to fine-tune with gradient descent, but soft prompts are continuous. Lester et al. [16] show that $\\theta_{text}^{enc}(p)$ can be treated as model parameters and optimized via gradient descent; they find that even with a small number of parameters, soft prompt tuning is competitive with full parameter-tuning in models with billions of parameters.\nThere is prior research that explored prompt tuning from an adversarial perspective. Although attackers typically only control discrete prompts, Qi et al. [23] observe that image inputs in Equation 1 are projected and fed into the VLM as a soft prompt. Our work also explores using images as adversarial soft prompts, but we search for a much broader and more powerful category of adversarial perturbations\u2014see the discussion in Section 2.3.\nThere are multiple examples\u00b2 of adversarial images that \u201cjailbreak", "describe the image\") and (2) fixed text sequences drawn from a dataset of known harmful outputs. The resulting images cause the model to generate a harmful response in the first turn, but the rest of the conversation does not appear to be affected. While the induced responses are harmful (they satisfy the \\\"toxicity\\\" meta-objective, in our parlance), they tend to be unrelated to the input image.\nSchwinn et al. [29] generate jailbreak images by targeting soft prompts in the embedding space. They maximize the similarity between (1) the model's output given the embedding of input tokens and the adversarial embedding perturbation (i.e., soft prompt), and (2) fixed harmful text sequences, similar to [23]. The resulting images evade safety alignment in open-sourced LLMs.\nIn general, training soft prompts on a dataset of fixed text sequences induces VLM responses that may satisfy a given meta-objective (such as toxicity), but these responses\"\n    },\n    {\n      \"title\": \"2.4. Indirect Prompt Injection\",\n      \"content\": \"Indirect prompt injection attacks were introduced in [13]. In an indirect injection attack, the attacker does not prompt the LLM directly. Instead, the attacker adds his prompt to some content (e.g., a webpage or an email) that another user, the victim of the attack, uses as part of their prompt (e.g., they may ask the LLM a question about the attacker's webpage). The attacker's prompt then controls the LLM's responses to the victim.\nThere are several proof-of-concept examples of hiding prompts in images\u00b3 that add pixels explicitly spelling out the prompt to the original image, typically in an imperceptible shade or color that is not noticeable to a human. This approach only works against VLMs that are capable of optical character recognition (OCR). In our experiments, this technique did not work against MiniGPT-4 and LLaVa, the two VLMs considered in this paper, because they fail to recognize words in input images even when these words are not stealthy (e.g., black texts on a white background). By contrast, the soft-prompt method introduced in this paper works regardless of the target model's OCR capabilities.\nThe closest related work is a proof of concept by Bagdasaryan et al. [2]. They give several examples, without systematic evaluation, of adversarial images that cause multi-modal LLMs to generate arbitrary fixed strings chosen by the attacker. These strings may contain instructions. If and only if the string output by the LLM is consumed by the same LLM as part of its context for subsequent autoregressive generation, the LLM follows the instruction contained in the string. This attack is not stealthy because the adversary's instruction is always visible in the target model's first text output generated from the adversarial image. In this paper, we design and systematically evaluate a different method for injecting instructions into images. It does not rely on forcing the VLM to output a fixed text string, nor does it assume that the VLM adds its own outputs to the generation context.\"\n    },\n    {\n      \"title\": \"2.5. Model Spinning\",\n      \"content\": \"Meta-instructions are an inference-time equivalent of training-time \\\"model spinning\\\" attacks introduced by Bagdasaryan and Shmatikov [3]. In those attacks, an adversary re-trains or fine-tunes a language model so that its outputs satisfy some adversarial meta-objective (conditionally, only if the input contains certain words chosen by the adversary).\"\n    },\n    {\n      \"title\": \"3. Threat Model\",\n      \"content\": \"The main proposed application of visual language models is to answer questions about images [17]. For example, a user may ask the model to explain the contents of an image or analyze the depicted scene. Visual language models can also be deployed as components of content-processing and content-generation systems, where their outputs are used to summarize and/or present information to human users.\nIn many cases, images on which VLMs operate come from websites, social media, and messaging apps. Their sources are not always trusted. User-generated content can originate from anywhere, including adversaries pursuing a particular agenda or objective (we use the term \\\"meta-objective\\\" to distinguish from training objectives in machine learning). Such an adversary could attempt to craft an image that will cause VLMs to generate outputs reflecting their agenda or satisfying their meta-objective.\nIt is possible to create an image perturbation that forces the VLM to respond with a predefined text sequence [2, 4]. In general, however, the adversary does not know the context in which the VLM will be queried about the image, nor the specific prompts that the VLM users will use. The fixed sequence is likely to be incorrect, implausible, or inappropriate in a given context. Again, note the difference with jailbreaking, where the adversary's goal is to produce harmful or toxic outputs regardless of the context or visual content of the image.\nWe consider adversaries who aim to steer models to generate contextually appropriate outputs that satisfy their meta-objectives [25]. To this end, an adversary can exploit the following observation. Unlike in classification tasks, where there is a single correct answer for a given input,\"\n    },\n    {\n      \"title\": \"4. Images as Soft Prompts\",\n      \"content\": \"We start by creating a synthetic dataset. We used the public API of OpenAI's ChatGPT, specifically the GPT-4 models (GPT-4 Turbo and GPT-40), for the experiments conducted between February 2024 and June 2024. Given an image $x$ and its label $l$, we provide them to ChatGPT and prompt it to \u201cgenerate N questions about $l$ in the image.": "et $P$ be the resulting set of prompts, simulating natural user queries.\nWe then provide a meta-instruction $t^*$ and request ChatGPT to answer each query $p \\in P$ according to this meta-instruction with the prompt \"answer the following questions about the $l$ in the image with a $z$ spin\". In general, $z$ could be any adversarial meta-objective. For the purposes of this paper, we use $z \\in \\{Positive, Negative, Neutral, Formal, Informal, Spam, URL injection\\}$. A variant of this prompt is \"answer the following questions about the $l$ in the image in $z$\", where $z \\in \\{English, Spanish, French\\}$, or \u201canswer the following questions about the $l$ in the image with $z$"}, {"title": "5. Evaluation", "content": "We evaluated our method on MiniGPT-4 [40] and LLaVA [18], two commonly used, open-source, multi-modal, instruction-following language models that were publicly available at the time we performed these"}, {"title": "5.2. Satisfying Meta-objectives", "content": "Table 1 reports our attack success rates, i.e., how well the responses induced by our images follow the corresponding meta-instructions\u2014against LLaVA and MiniGPT-4. These results show that all ten meta-instructions achieve results comparable to explicit instructions.\nFor some meta-objectives, such as political bias and informal text, spam, and URL injection, even explicit text instructions do not achieve a high success rate. We attribute this to the limitations of our target VLMs in following certain instructions.\nInterestingly, in some cases (indicated in bold in Table 1), images with hidden meta-instructions achieve significantly higher success than explicit instructions. For example, both MiniGPT-4 and LLaVA do not follow explicit instructions to produce outputs that contain adversary-chosen spam or specific URLs, yet when equivalent meta-instructions are added to images trained as soft prompts, Minigpt-4 includes spam (respectively, adversary's URLs) in the outputs for 56% (respectively 30%) of the images. LLaVA includes spam (respectively, adversary's URLs) in the outputs for 91% (respectively 67%) of the images. As mentioned in Section 1, we conjecture that instruction-tuning of these models on image-description prompts sup-"}, {"title": "5.3. Preserving Image Semantics", "content": "In Table 2, we measure the similarity between clean and perturbed images using the cosine similarity of the image-encoder embeddings and SSIM.\nFirst, we calculate the average similarity between unrelated images randomly selected from the training dataset. This is the lower-bound baseline for the similarity metrics. Second, we compute the average similarity of an image to its augmented versions (which we assume have the same visual semantics) using various techniques: JPEG compression, Gaussian Blur, Random Affine, Color Jitter, Random Horizontal Flip, and Random Perspective. Third, we compute the similarity between a clean image and its perturbed version produced by the jailbreaking method [23], as described in Section 2.3. This method aims to maximize the similarity between LLM outputs and a set of harmful outputs, irrespective of the image content.\nResults in Table 2 show that our method preserves image semantics, whereas the jailbreaking method does not.\nCosine similarity results show that similarities between the embeddings of clean and perturbed images (MiniGPT-4: 0.601, LLaVA: 0.332) are slightly lower than those between clean and augmented images (MiniGPT-4: 0.809, LLaVA: 0.362). This suggests that our perturbations lose some of the semantic content of the images. For comparison, we also include similarities between clean images and visual jailbreaking images, as well as clean images and unrelated images, all of which are lower than perturbed images.\nSSIM is an independent metric that measures similarity between images at the pixel level. SSIM results are similar to embedding similarity. SSIM values for perturbed images (MiniGPT-4: 0.316, LLaVA: 0.337) are close to those of augmented images (MiniGPT-4: 0.432, LLaVA: 0.432), vs. 0 for unrelated image pairs and those for visual-jailbreaking images (MiniGPT-4: 0.173, LLaVA: 0.188), further confirming that our perturbations maintain visual quality and structural integrity of images.\nTable 3 shows the results of LLM-based measurement of image preservation. The first and fourth columns of the table show how often the target VLM responds that the label accurately represents the content of the perturbed images, as described in Section 5.1. For MiniGPT-4, this value averages 46%, compared to 88% for LLaVA. These values are similar to those for clean images (43% and 100%, respectively). We attribute this to the differences in the models' respective inherent capabilities to describe images.\nThe other columns in Table 3 show the percentage of responses deemed by the oracle LLM as relevant to the prompts and corresponding clean and perturbed images, respectively. For both MiniGPT-4 and LLaVA, these values are very high, averaging 96%. This indicates that the models' outputs are contextually accurate for our perturbed images.\nBy contrast, visual-jailbreaking images force the model to generate harmful outputs that are irrelevant to the content of the image. As a result, none of these outputs are related to either clean or perturbed images\u2014even though they use the same e as our perturbations and appear visually similar to clean images. This demonstrates that small e is insufficient to preserve the visual semantics of the image and highlights the necessity to train with text sequences that answer questions about the image, as described in Section 4.\nOverall, Tables 2 and 3 suggest that while there are some variations in how VLMs interpret images, our method creates image soft prompts that preserve the visual content of the corresponding clean images."}, {"title": "5.4. Making Perturbations Stealthy", "content": "Table 4 shows the results for the sentiment meta-instruction under different perturbation norms: $L_\\infty$ ($\\epsilon$ ="}, {"title": "5.5. Transferability", "content": "Table 5 shows the results of image soft prompts trained with MiniGPT-4 (Vicuna V0 13B) against different target VLMs, including different versions and sizes of MiniGPT-4 and LLaVA.\nThese results show that the attack transfers to a smaller version of the same model. Specifically, image soft prompts generated using MiniGPT-4 (Vicuna V0 13B) are effective against MiniGPT-4 (Vicuna V0 7B), with positive, negative, and neutral sentiment meta-objective following rates of 40%, 30%, and 69%, respectively.\nTransferring to different model architectures or significantly different versions significantly decreases effectiveness. Images trained on MiniGPT-4 (Vicuna V0 13B) are ineffective against MiniGPT-4 (Llama2 7B) and LLaVA (Llama2 13B): generated outputs have similar sentiment scores to outputs generated from clean images."}, {"title": "6. Defenses", "content": "There is a large body of research on training adversarially robust models [22, 30]. For better or for worse, little of this research has found its way to real-world LLMs, whether production models or available research prototypes. Implementors of LLMs have not been interested in adversarial robustness, with a few exceptions, such as protecting models from jailbreaking [9, 10, 26] and prompt injection [35]. One of the reasons could be the negative impact of adversarial robustness on model performance, which is especially pronounced for multi-modal models. For example, adversarially robust contrastive learning significantly reduces accuracy even on basic tasks such as CIFAR [37].\nIn addition to training-time defenses, inference-time de-"}, {"title": "6.1. Feature Distillation", "content": "Defenses of this type apply transformations that preserve visual features of the image while destroying adversarial features [20]. JPEG compression is an example of such a transformation. In our case, adding a JPEG compression layer before encoding input images significantly reduces the efficacy of meta-instructions hidden in image perturbations.\nTable 6 shows that when JPEG compression is applied to the perturbed images, success of the attack, i.e., percentage of outputs that satisfy the adversary's meta-objective (sentiment, in this case) drops significantly. This indicates that JPEG compression disrupts adversarial features while maintaining the visual content of the image. Note that attack success rates are non-zero even on clean images because responses to clean images occasionally satisfy the meta-objective without any instructions.\nThis aligns with findings from prior research, which demonstrated that applying JPEG compression can significantly lower the effectiveness of adversarial perturbations against multi-modal encoders [38].\nDefenses of this type can usually be evaded by an adaptive adversary who incorporates the defense into the perturbation generation process. For example, Zhang et al. demonstrate JPEG-evading multi-modal embedding attacks [38]. We follow the same technique and add a differentiable approximation of JPEG compression [33] to our perturbation method, aiming to train a more robust image soft prompt that could evade JPEG defenses.\nIn our case, this evasion failed. Even in the absence of the defense, images trained using this method induce VLM outputs that do not follow the meta-instruction, thus failing the primary (meta-)objective of the attack. This finding is consistent with our transferability results (see Section 5.5), indicating that image soft prompts are somewhat brittle and difficult to train robustly. We leave evasion of feature-distillation defenses and countermeasures to future work."}, {"title": "6.2. Anomaly Detection", "content": "By design, image embeddings are intended to preserve essential visual features of images. These features are also preserved by various augmentations (flips, jitter, etc.).\nTherefore, a plausible defense is to compare the embedding of an input image with the embeddings of its augmentations. For normal images, the embeddings should be similar; for images with adversarial perturbations, there may be significant differences.\nTable 7 shows our evaluation of this defense. We use all ten meta-instructions for this evaluation.\nFor MiniGPT-4, the average cosine similarity between the embeddings of unperturbed images and their augmentations is 0.839, whereas for perturbed images, it is lower at 0.651. For LLaVA, however, the average cosine similarity between the unperturbed (respectively, perturbed) images and their augmentations is 0.443 (respectively, 0.424). The confidence intervals of these values overlap, indicating that the defense may not be effective for LLaVA."}, {"title": "7. Discussion and Future Research", "content": "We introduced a new type of attack that enables adversaries to add stealthy \u201cmeta-instructions\" to images that influence how visual language models respond to queries about these images. Meta-instructions keep responses contextually appropriate and relevant to the visual content of the image while steering them to satisfy some adversary-chosen meta-objective or \"spin\" (e.g., positive or negative sentiment or political bias or spam). In instruction-tuned visual language models such as LLaVA, meta-instructions can be more powerful than explicit instructions and unlock"}, {"title": "Societal Impact", "content": "Visual Language Models have been proposed for applications, e.g., personal assistants, that mediate users' access to information by explaining images, figures, and articles. Understanding how an adversary could attempt to influence users by manipulating inputs to VLMs and how to protect users from these threats are important steps toward safely deploying these models in the real world."}]}