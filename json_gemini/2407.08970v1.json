{"title": "Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions", "authors": ["Tingwei Zhang", "Collin Zhang", "John X. Morris", "Eugene Bagdasaryan", "Vitaly Shmatikov"], "abstract": "We introduce a new type of indirect injection vulnerabilities in language models that operate on images: hidden \u201cmeta-instructions\u201d that influence how the model interprets the image and steer the model's outputs to express an adversary-chosen style, sentiment, or point of view. We explain how to create meta-instructions by generating images that act as soft prompts. Unlike jailbreaking attacks and adversarial examples, outputs resulting from these images are plausible and based on the visual content of the image, yet follow the adversary's (meta-)instructions. We describe the risks of these attacks, including spam, misinformation, and spin, evaluate their efficacy for multiple visual language models and adversarial meta-objectives, and demonstrate how they can \u201cunlock\u201d the capabilities of the underlying language models that are unavailable via explicit text instructions. Finally, we discuss defenses against these attacks.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) operating on third-party content\u2014webpages, wikis, forums, social media, emails and messages, and user-generated content in general\u2014are vulnerable to indirect prompt injection attacks [13]. By adding prompts to the text content under their control, adversaries can try to influence outputs and actions generated by LLMs when processing this content. Many modern LLMs accept inputs in multiple modalities, in particular images. We refer to LLMs that operate on images as Visual Language Models (VLMs). Like their text-only counterparts, VLMs are vulnerable to various direct and indirect prompt injection attacks. For example, jailbreaking attacks use image perturbations to cause models to generate toxic or unsafe outputs, even if the same models refuse to generate such outputs in response to text prompts. Adversarial examples and related attacks cause VLMs to generate outputs chosen by the adversary that are unrelated to the visual content of input images. We discuss these attacks in Sections 2.3 and 2.4. Conceptual contributions. We introduce and evaluate a new class of indirect attacks on visual language models. Adversarial meta-instructions are stealthy perturbations that steer outputs produced by a VLM in response to an image so that these outputs satisfy some adversarial meta-objective. Meta-instructions preserve the visual content of the image, as interpreted by the VLM. The resulting responses are thus \u201ccorrect\u201d with respect to the image and plausible in the context of the conversation between a human user and the VLM. In this sense, meta-instructions are the opposite of jailbreaking prompts and adversarial examples, which aim to produce outputs unrelated to the human-perceived visual content of the image. For example, a meta-instruction may steer the VLM into generating outputs that express a style, sentiment, or point of view chosen by the adversary. See an example in Figure 1: meta-instructions hidden in image perturbations change how the VLM answers the question about a stock performance chart depicted in the image. In all cases, the answer is based on the image, but, depending on the meta-instruction, the interpretation changes to positive or negative, or includes adversary-chosen spam, or specific URLs. Figure 2 is another example\u2014motivated by our prior experience with conference reviews obviously generated with the help of an LLM\u2014where we steer the model's interpretation of an image of our own paper to positive or negative, depending on our choice of the meta-instruction. Meta-instructions are an indirect attack. An adversary applies a perturbation with a hidden meta-instruction to a legitimate image and then plants the modified image in a webpage, social media post, or personal message. When the user asks a VLM about the image, the VLM's entire conversation with the user follows the meta-instruction and satisfies the adversary's meta-objective. In contrast to jailbreaking scenarios, users of VLMs are victims of the attack rather than perpetrators. Adversarial meta-instructions can be \u201cweaponized\u201d to produce misinformation, propaganda, or spin [3] when untrusted images are processed by LLM-augmented search engines, news and social-media summarizers, or personal assistants. There is already evidence that real-world adversaries use generative AI to rewrite legitimate news with explicit instructions to express certain political stances or slanted interpretations [25]. Hidden meta-instructions increase this attack surface. They enable the creation of \u201cself-interpreting\u201d images that automatically generate misinformation when processed by VLM-based systems\u2014see an example in Figure 3. Technical contributions. We design, implement, and evaluate a method for creating a new type of image perturbations that act as soft prompts for a language model while preserving the visual semantics of the image."}, {"title": "2. Background and Related Work", "content": "2.1. Visual Language Models\nWe focus on visual language models (VLMs) that accept text and image inputs. These models typically combine a pre-trained generative language model such as Llama [34] with two encoder inputs: a text encoder and an image (visual) encoder [17]. VLMs are intended to accurately respond to prompts about their input images and maintain a conversation with the user regarding the image. Let \u03b8 be a VLM that contains the text encoder \u03b8enc\ntxt, the image encoder \u03b8enc\nimg, and the language decoder \u03b8dec. The text of the prompt p \u2208 P, e.g., \"describe the image\", is fed into the text encoder \u03b8enc\ntxt, and the image x \u2208 X is fed into the image encoder. Their respective embeddings produced by the encoders are concatenated and fed into the language decoder:\n\u03b8(p, x) = \u03b8dec(\u03b8enc\ntxt(p) + \u03b8enc\nimg(x)) = y (1)\nAn instruction-tuned VLM performs the task of matching instruction prompts and images to text outputs, i.e., (P, X) \u2192 Y."}, {"title": "2.2. Soft Prompts", "content": "Brown et al. [6] demonstrate that prompt design can significantly impact the behavior of language models. However, creating effective prompts requires substantial human effort, making the process costly. Furthermore, automatically optimizing prompts is inefficient because text prompts are discrete. Lester et al. [16] introduce the concept of a \"soft prompt\" as a parameter-efficient fine-tuning method. In Equation 1, the language model takes prompts p and encodes them into \u03b8enc\ntxt(p). The text of p is the \u201chard prompt\", and its embedding \u03b8enc\ntxt(p) is the \u201csoft prompt\u201d. Hard prompts are discrete and thus challenging to fine-tune with gradient descent, but soft prompts are continuous. Lester et al. [16] show that \u03b8enc\ntxt(p) can be treated as model parameters and optimized via gradient descent; they find that even with a small number of parameters, soft prompt tuning is competitive with full parameter-tuning in models with billions of parameters. There is prior research that explored prompt tuning from an adversarial perspective. Although attackers typically only control discrete prompts, Qi et al. [23] observe that image inputs in Equation 1 are projected and fed into the VLM as a soft prompt. Our work also explores using images as adversarial soft prompts, but we search for a much broader and more powerful category of adversarial perturbations\u2014see the discussion in Section 2.3."}, {"title": "2.3. Jailbreaking and Adversarial Examples in Visual Language Models", "content": "There are multiple examples\u00b2 of adversarial images that \u201cjailbreak\" LLMs by causing them to generate outputs that violate their safety guardrails, e.g., toxic text. Shayegani et al. [32] generate adversarial images that look like noise and have no semantics. Qi et al. [23] generate jailbreak images by maximizing the similarity between (1) the model's output given the image and a fixed text prompt (e.g., \u201cdescribe the image\") and (2) fixed text sequences drawn from a dataset of known harmful outputs. The resulting images cause the model to generate a harmful response in the first turn, but the rest of the conversation does not appear to be affected. While the induced responses are harmful (they satisfy the \"toxicity\" meta-objective, in our parlance), they tend to be unrelated to the input image. Schwinn et al. [29] generate jailbreak images by targeting soft prompts in the embedding space. They maximize the similarity between (1) the model's output given the embedding of input tokens and the adversarial embedding perturbation (i.e., soft prompt), and (2) fixed harmful text sequences, similar to [23]. The resulting images evade safety alignment in open-sourced LLMs. In general, training soft prompts on a dataset of fixed text sequences induces VLM responses that may satisfy a given meta-objective (such as toxicity), but these responses do not match the context of the conversation, i.e., the user's prompts and visual semantics of the image. Such responses are implausible, not stealthy, and do not meet the requirements of the threat model we discuss in Section 3."}, {"title": "2.4. Indirect Prompt Injection", "content": "Indirect prompt injection attacks were introduced in [13]. In an indirect injection attack, the attacker does not prompt the LLM directly. Instead, the attacker adds his prompt to some content (e.g., a webpage or an email) that another user, the victim of the attack, uses as part of their prompt (e.g., they may ask the LLM a question about the attacker's webpage). The attacker's prompt then controls the LLM's responses to the victim. There are several proof-of-concept examples of hiding prompts in images\u00b3 that add pixels explicitly spelling out the prompt to the original image, typically in an imperceptible shade or color that is not noticeable to a human. This approach only works against VLMs that are capable of optical character recognition (OCR). In our experiments, this technique did not work against MiniGPT-4 and LLaVa, the two VLMs considered in this paper, because they fail to recognize words in input images even when these words are not stealthy (e.g., black texts on a white background). By contrast, the soft-prompt method introduced in this paper works regardless of the target model's OCR capabilities. The closest related work is a proof of concept by Bagdasaryan et al. [2]. They give several examples, without systematic evaluation, of adversarial images that cause multi-modal LLMs to generate arbitrary fixed strings chosen by the attacker. These strings may contain instructions. If and only if the string output by the LLM is consumed by the same LLM as part of its context for subsequent autoregressive generation, the LLM follows the instruction contained in the string. This attack is not stealthy because the adversary's instruction is always visible in the target model's first text output generated from the adversarial image. In this paper, we design and systematically evaluate a different method for injecting instructions into images. It does not rely on forcing the VLM to output a fixed text string, nor does it assume that the VLM adds its own outputs to the generation context."}, {"title": "2.5. Model Spinning", "content": "Meta-instructions are an inference-time equivalent of training-time \"model spinning\" attacks introduced by Bagdasaryan and Shmatikov [3]. In those attacks, an adversary re-trains or fine-tunes a language model so that its outputs satisfy some adversarial meta-objective (conditionally, only if the input contains certain words chosen by the adversary). The meta-objectives in our work are similar: for example, adding an adversary-chosen sentiment, style, or spin to the outputs of a language model. They are achieved, however, not via training but via instructions hidden in inputs that unlock the adversary-chosen behavior in unmodified models at inference time."}, {"title": "3. Threat Model", "content": "The main proposed application of visual language models is to answer questions about images [17]. For example, a user may ask the model to explain the contents of an image or analyze the depicted scene. Visual language models can also be deployed as components of content-processing and content-generation systems, where their outputs are used to summarize and/or present information to human users. In many cases, images on which VLMs operate come from websites, social media, and messaging apps. Their sources are not always trusted. User-generated content can originate from anywhere, including adversaries pursuing a particular agenda or objective (we use the term \"meta-objective\" to distinguish from training objectives in machine learning). Such an adversary could attempt to craft an image that will cause VLMs to generate outputs reflecting their agenda or satisfying their meta-objective. It is possible to create an image perturbation that forces the VLM to respond with a predefined text sequence [2, 4]. In general, however, the adversary does not know the context in which the VLM will be queried about the image, nor the specific prompts that the VLM users will use. The fixed sequence is likely to be incorrect, implausible, or inappropriate in a given context. Again, note the difference with jailbreaking, where the adversary's goal is to produce harmful or toxic outputs regardless of the context or visual content of the image. We consider adversaries who aim to steer models to generate contextually appropriate outputs that satisfy their meta-objectives [25]. To this end, an adversary can exploit the following observation. Unlike in classification tasks, where there is a single correct answer for a given input, there is a large range of \"correct\" or at least plausible answers that a generative model can produce in response to a given prompt. The model can thus be steered to generate a response that is contextually appropriate (i.e., plausible and based on the visual content of the image) but also has some property or \"spin\u201d chosen by the adversary [3]. Examples include positive or negative sentiment and political bias. Meta-instructions. We say that t\u2217 is a meta-instruction if it causes the model to generate output text y \u2208 Y that satisfies an adversary-chosen meta-objective z \u2208 Z or \u201cspin\u201d [12] (we use meta-objective and spin interchangeably). For example, suppose an adversary chooses a meta-instruction that adds positive sentiment. This instruction tells the model to produce outputs that (a) respond to the user's prompts about the image and (b) are positive. It is important that the output y preserve input semantics, i.e., actually responds to the user's question about the image, otherwise it will affect the model's performance and damage the user's trust in the model. Formally, we define a predicate \u03b1: Y \u00d7 Z\u2192{0, 1} that holds when output y\u2208Y satisfies the meta-objective z\u2208Z. We also define a \"image semantics preservation\" predicate \u03b2: P \u00d7 X \u00d7 Y\u2192{0, 1} that holds when an output y \u2208 Y is an appropriate response to question p about image x. The output of the model follows the meta-instruction t\u2217 and answers question p about image x if \u03b1(\u03b8(p, x), z) = \u03b2(p, x, \u03b8(p, x)) = 1. In practice, evaluating whether the model's output satisfies either predicate can be done using a separate evaluator model or an oracle language model. We describe the details in Section 5. Adversary's capabilities. Figure 4 schematically depicts our threat model. The adversary controls and can modify an image. We assume that the victim obtains the adversary's image (e.g., from a website, messaging application, or another channel) and submits it to the VLM either directly or via some application with its own prompt. We additionally assume that the adversary knows which VLM the victim uses (we relax this assumption in Section 5.5). They can query the model either in a white-box (with access to the model's gradients) or black-box (only using API access) fashion but cannot modify it. The adversary does not know the victim's text prompt, other than it will involve a query about the image. The image is provided to the model as an actual input in a modality supported by the model (i.e., the adversary cannot directly or indirectly submit embedding vectors). Adversary's goals. The adversary perturbs an image x by creating xs = x + \u03b4, where the perturbation \u03b4 encodes a meta-instruction t\u2217. The adversary's first goal is that the VLM's output \u03b8(p, xs) = y\u2217 on this image satisfy the adversary's meta-objective, i.e., \u03b1(\u03b8(p, xs), z) = 1. The adversary's second goal is that the output correctly responds to the user's question, i.e., \u03b2(p, xs, \u03b8(p, xs)) = 1. The adversary's third goal is stealthiness: the perturbed image xs should be within some distance \u03f5 of the original image x, i.e., \u2225x - xs\u2225 < \u03f5. Many metrics are available for \u03f5; full discussion is outside the scope of this paper. Furthermore, the perturbed image should appear similar to the original image to a human."}, {"title": "4. Images as Soft Prompts", "content": "Figure 7 schematically depicts our method for generating images that act as soft prompts. Generating question-answer pairs. We start by creating a synthetic dataset. We used the public API of OpenAI's ChatGPT, specifically the GPT-4 models (GPT-4 Turbo and GPT-4o), for the experiments conducted between February 2024 and June 2024. Given an image x and its label l, we provide them to ChatGPT and prompt it to \u201cgenerate N questions about l in the image.\u201d Let P be the resulting set of prompts, simulating natural user queries. We then provide a meta-instruction t\u2217 and request ChatGPT to answer each query p \u2208 P according to this meta-instruction with the prompt \"answer the following questions about the l in the image with a z spin\". In general, z could be any adversarial meta-objective. For the purposes of this paper, we use z \u2208 {Positive, Negative, Neutral, Formal, Informal, Spam, URL injection}. A variant of this prompt is \"answer the following questions about the l in the image in z\", where z \u2208 {English, Spanish, French}, or \u201canswer the following questions about the l in the image with z\u201d, where z \u2208 {Republican bias, Democratic bias}. For z \u2208 {Spam, URL injection}, we append the sentence \"Explore Bitconnect for potential investment opportunities.\" or \"To find out more about this image, visit <maliciousurl>\" to the output generated when z = English. Let the resulting answers be the Y\u2217 = {y}. We use evaluator models (see Section 5.1) to check if y follows the meta-instruction (except for the spam and URL-injection meta-instructions). If answers don't follow the meta-instruction, we repeat generation until at least 80% of the answers pass the evaluator check. By construction, text sequences in Y\u2217 answer the prompt p about the image, with a spin prescribed by the meta-instruction. As a result, the perturbed image generated by our method preserves the semantics of the original image. By contrast, jailbreak perturbations [23] are tuned to produce toxic outputs, which have no relation to the original images. Consequently, they do not preserve image semantics. We measure the preservation of image semantics for both methods in Section 5.3. Our method for synthesizing question-answer pairs simulates a natural distribution of user queries and the corresponding responses, creating a realistic dataset for both training and evaluation. We use the entire set, including answers that fail the evaluator check described above. We use some pairs for training the adversarial mask, while the remaining pairs are used to evaluate whether the outputs follow the injected meta-instructions. More details can be found in Section 5.1. Training image soft prompts. We use a standard technique from the adversarial examples literature, Projected Gradient Descent (PGD) [22], to search for a constrained perturbation of \u03b4 < \u03f5 to the input x that, when combined with Pi, will make the model output Y\u2217:\nmin\n\u03b4 L (\u03b8 (\u03b8enc\ntxt(P) | \u03b8enc\nimg(x + \u03b4)), Y\u2217)\nWe use cross-entropy for L to compare the model's output with the target y\u2217. We employ PGD in L\u221e norm for most training and also consider PGD in L2 norm when discussing stealthiness of perturbations in Section 5.4."}, {"title": "5. Evaluation", "content": "5.1. Experimental Setup\nTarget models. We evaluated our method on MiniGPT-4 [40] and LLaVA [18], two commonly used, open-source, multi-modal, instruction-following language models that were publicly available at the time we performed these experiments. The underlying VLMs are Vicuna 13B and Llama-2 13B, respectively. We consider different versions and model sizes in our transferability experiments (see Section 5.5). Meta-objectives. We selected the following 10 meta-objectives: 1) Sentiment: Positive, negative, neutral 2) Formality: Formal, informal 3) Language: English, French, Spanish 4) Political bias: Republican bias, Democratic bias 5) Attack: Spam, URL injection We picked these meta-objectives because they are amenable to systematic evaluation. For each objective from this list, it is possible to automatically check whether a given output satisfies it, using either an evaluator model or another LLM. We employ the following models for our evaluation. Sentiment analysis. We use the \"twitter-roberta-base-sentiment-latest\u201d library, a pre-trained sentiment analysis model used in recent research [8, 21] to capture sentiment-specific nuances in tweets. This model was trained on an extensive dataset of approximately 124 million tweets and fine-tuned for sentiment analysis with the TweetEval benchmark [5]. Formality classification. We use the \"roberta-base-formality-ranker\" library, a pre-trained classifier that determines whether English sentences are formal or informal. This model was trained on the biggest available dataset, Grammarly's Yahoo Answers Formality Corpus (GYAFC) [24], and its quality was evaluated by an accompanying research paper [1]. Language detection. We use the \"xlm-roberta-base-language-detection\" library, a version of the XLM-"}, {"title": "5.2. Satisfying Meta-objectives", "content": "Table 1 reports our attack success rates, i.e., how well the responses induced by our images follow the corresponding meta-instructions\u2014against LLaVA and MiniGPT-4. These results show that all ten meta-instructions achieve results comparable to explicit instructions. For some meta-objectives, such as political bias and informal text, spam, and URL injection, even explicit text instructions do not achieve a high success rate. We attribute this to the limitations of our target VLMs in following certain instructions. Interestingly, in some cases (indicated in bold in Table 1), images with hidden meta-instructions achieve significantly higher success than explicit instructions. For example, both MiniGPT-4 and LLaVA do not follow explicit instructions to produce outputs that contain adversary-chosen spam or specific URLs, yet when equivalent meta-instructions are added to images trained as soft prompts, Minigpt-4 includes spam (respectively, adversary's URLs) in the outputs for 56% (respectively 30%) of the images. LLaVA includes spam (respectively, adversary's URLs) in the outputs for 91% (respectively 67%) of the images. As mentioned in Section 1, we conjecture that instruction-tuning of these models on image-description prompts suppressed some of the instruction-following capabilities of the underlying LLM. Our images, acting as soft prompts, \"unlock\" these capabilities."}, {"title": "5.3. Preserving Image Semantics", "content": "In Table 2, we measure the similarity between clean and perturbed images using the cosine similarity of the image-encoder embeddings and SSIM. First, we calculate the average similarity between unrelated images randomly selected from the training dataset. This is the lower-bound baseline for the similarity metrics. Second, we compute the average similarity of an image to its augmented versions (which we assume have the same visual semantics) using various techniques: JPEG compression, Gaussian Blur, Random Affine, Color Jitter, Random Horizontal Flip, and Random Perspective. Third, we compute the similarity between a clean image and its perturbed version produced by the jailbreaking method [23], as described in Section 2.3. This method aims to maximize the similarity between LLM outputs and a set of harmful outputs, irrespective of the image content. Results in Table 2 show that our method preserves image semantics, whereas the jailbreaking method does not. Cosine similarity results show that similarities between the embeddings of clean and perturbed images (MiniGPT-4: 0.601, LLaVA: 0.332) are slightly lower than those between clean and augmented images (MiniGPT-4: 0.809, LLaVA: 0.362). This suggests that our perturbations lose some of the semantic content of the images. For comparison, we also include similarities between clean images and visual jailbreaking images, as well as clean images and unrelated images, all of which are lower than perturbed images. SSIM is an independent metric that measures similarity between images at the pixel level. SSIM results are similar to embedding similarity. SSIM values for perturbed images (MiniGPT-4: 0.316, LLaVA: 0.337) are close to those of augmented images (MiniGPT-4: 0.432, LLaVA: 0.432), vs. 0 for unrelated image pairs and those for visual-jailbreaking images (MiniGPT-4: 0.173, LLaVA: 0.188), further confirming that our perturbations maintain visual quality and structural integrity of images. Table 3 shows the results of LLM-based measurement of image preservation. The first and fourth columns of the table show how often the target VLM responds that the label accurately represents the content of the perturbed images, as described in Section 5.1. For MiniGPT-4, this value averages 46%, compared to 88% for LLaVA. These values are similar to those for clean images (43% and 100%, respectively). We attribute this to the differences in the models' respective inherent capabilities to describe images. The other columns in Table 3 show the percentage of responses deemed by the oracle LLM as relevant to the prompts and corresponding clean and perturbed images, respectively. For both MiniGPT-4 and LLaVA, these values are very high, averaging 96%. This indicates that the models' outputs are contextually accurate for our perturbed images. By contrast, visual-jailbreaking images force the model to generate harmful outputs that are irrelevant to the content of the image. As a result, none of these outputs are related to either clean or perturbed images\u2014even though they use the same \u03f5 as our perturbations and appear visually similar to clean images. This demonstrates that small \u03f5 is insufficient to preserve the visual semantics of the image and highlights the necessity to train with text sequences that answer questions about the image, as described in Section 4. Overall, Tables 2 and 3 suggest that while there are some variations in how VLMs interpret images, our method creates image soft prompts that preserve the visual content of the corresponding clean images."}, {"title": "5.4. Making Perturbations Stealthy", "content": "Table 4 shows the results for the sentiment meta-instruction under different perturbation norms: L\u221e (\u03f5 = 16/255, 32/255) and L2 (\u03f5 = 6, 12, 24). Figure 8 shows examples of image soft prompts with different perturbations. Sharif et al. [31] demonstrated that perturbations with L2 norm of 6 are less noticeable to humans than perturbations with L\u221e norm (16/255). This suggests that L2 perturbations are more stealthy, making them preferable for tasks requiring minimal perceptual alteration. Results in Table 4 show that applying perturbations with L2 norm or lower L\u221e norms (e.g., 16/255) creates less-perceptible changes while still steering the model to follow the meta-instruction. Meta-objectives following rate for L2 perturbations with \u03f5 = 6 (Positive: 41%, Negative: 22%, Neutral: 77%) is similar to perturbations with \u03f5 = 12 (Positive: 49%, Negative: 18%, Neutral: 72%). Although there is a slight drop in meta-instruction following (i.e., satisfying the meta-objective) compared to explicit instructions and image soft prompts generated with L\u221e norm and \u03f5 = 32 (Positive: 62%, Negative: 34%, Neutral: 69%), there is a good balance between stealthiness of the perturbation and inducing outputs that satisfy the meta-objective."}, {"title": "5.5. Transferability", "content": "Table 5 shows the results of image soft prompts trained with MiniGPT-4 (Vicuna V0 13B) against different target VLMs, including different versions and sizes of MiniGPT-4 and LLaVA. These results show that the attack transfers to a smaller version of the same model. Specifically, image soft prompts generated using MiniGPT-4 (Vicuna V0 13B) are effective against MiniGPT-4 (Vicuna V0 7B), with positive, negative, and neutral sentiment meta-objective following rates of 40%, 30%, and 69%, respectively. Transferring to different model architectures or significantly different versions significantly decreases effectiveness. Images trained on MiniGPT-4 (Vicuna V0 13B) are ineffective against MiniGPT-4 (Llama2 7B) and LLaVA (Llama2 13B): generated outputs have similar sentiment scores to outputs generated from clean images."}, {"title": "6. Defenses", "content": "There is a large body of research on training adversarially robust models [22, 30]. For better or for worse, little of this research has found its way to real-world LLMs, whether production models or available research prototypes. Implementors of LLMs have not been interested in adversarial robustness, with a few exceptions, such as protecting models from jailbreaking [9, 10, 26] and prompt injection [35]. One of the reasons could be the negative impact of adversarial robustness on model performance, which is especially pronounced for multi-modal models. For example, adversarially robust contrastive learning significantly reduces accuracy even on basic tasks such as CIFAR [37]. In addition to training-time defenses, inference-time defenses aim to filter adversarial inputs and/or outputs. Llama Guard [14] is an LLM-based model that detects unsafe content in LLM inputs and outputs. Lakera [15] provides an API service to detect malicious inputs to LLMs. These defenses are independent of the model and don't affect LLM performance. The types of adversarial inputs and outputs tackled by these defenses are different from those considered in this paper. We, too, focus on inference-time defenses that can be implemented as wrappers around existing models, primarily via input pre-processing. 6.1. Feature Distillation Defenses of this type apply transformations that preserve visual features of the image while destroying adversarial features [20]. JPEG compression is an example of such a transformation. In our case, adding a JPEG compression layer before encoding input images significantly reduces the efficacy of meta-instructions hidden in image perturbations. Table 6 shows that when JPEG compression is applied to the perturbed images, success of the attack, i.e., percentage of outputs that satisfy the adversary's meta-objective (sentiment, in this case) drops significantly. This indicates that JPEG compression disrupts adversarial features while maintaining the visual content of the image. Note that attack success rates are non-zero even on clean images because responses to clean images occasionally satisfy the meta-objective without any instructions. This aligns with findings from prior research, which demonstrated that applying JPEG compression can significantly lower the effectiveness of adversarial perturbations against multi-modal encoders [38]. Defenses of this type can usually be evaded by an adaptive adversary who incorporates the defense into the perturbation generation process. For example, Zhang et al. demonstrate JPEG-evading multi-modal embedding attacks [38]. We follow the same technique and add a differentiable approximation of JPEG compression [33] to our perturbation method, aiming to train a more robust image soft prompt that could evade JPEG defenses. In our case, this evasion failed. Even in the absence of the defense, images trained using this method induce VLM outputs that do not follow the meta-instruction, thus failing the primary (meta-)objective of the attack. This finding is consistent with our transferability results (see Section 5.5), indicating that image soft prompts are somewhat brittle and difficult to train robustly. We leave evasion of feature-distillation defenses and countermeasures to future work. 6.2. Anomaly Detection By design, image embeddings are intended to preserve essential visual features of images. These features are also preserved by various augmentations (flips, jitter, etc.). Therefore, a plausible defense is to compare the embedding of an input image with the embeddings of its augmentations. For normal images, the embeddings should be similar; for images with adversarial perturbations, there may be significant differences. Table 7 shows our evaluation of this defense. We use all ten meta-instructions for this evaluation. For MiniGPT-4, the average cosine similarity between the embeddings of unperturbed images and their augmentations is 0.839, whereas for perturbed images, it is lower at 0.651. For LLaVA, however, the average cosine similarity between the unperturbed (respectively, perturbed) images and their augmentations is 0.443 (respectively, 0.424). The confidence intervals of these values overlap, indicating that the defense may not be effective for LLaVA."}, {"title": "7. Discussion and Future Research", "content": "We introduced a new type of attack that enables adversaries to add stealthy \u201cmeta-instructions\" to images that influence how visual language models respond to queries about these images. Meta-instructions keep responses contextually appropriate and relevant to the visual content of the image while steering them to satisfy some adversary-chosen meta-objective or \"spin\" (e.g., positive or negative sentiment or political bias or spam). In instruction-tuned visual language models such as LLaVA, meta-instructions can be more powerful than explicit instructions and unlock capabilities of the base LLM that are not available via explicit prompts in the VLM. We designed, implemented, and evaluated a novel method for creating images with meta-instructions. This method generates adversarial perturbations that act as \"soft prompts\" for the target model. The efficacy of meta-instructions is limited by the capabilities of the target VLM's decoder model. Since the attack is fundamentally based on soft prompts, it does not transfer well across model families. It is unclear how to generate image soft prompts with black-box, query-only access to the target VLM. Smaller, stealthier perturbations reduce the efficacy of meta-instructions. Furthermore, the current version of the attack is defeated by simple defenses such as JPEG compression. An interesting direction for future research is to investigate whether it is possible to create local soft-prompt perturbations, akin to adversarial patches [7], that can be applied to any image. Another question for future research is measuring, with various prompts about the original and perturbed images, how much semantic information about the image is lost due to applying soft-prompt perturbations. Future user-oriented research can study whether humans find responses generated by VLMs in response to meta-instructions plausible and persuasive for various adversarial meta-objectives."}, {"title": "Societal Impact", "content": "Visual Language Models have been proposed for applications, e.g., personal assistants, that mediate users' access to information by explaining images, figures, and articles. Understanding how an adversary could attempt to influence users by manipulating inputs to VLMs and how to protect users from these threats are important steps toward safely deploying these models in the real world."}, {"title": "Acknowledgments", "content": "This work was performed at Cornell Tech and partially supported by the NSF grant 1916717."}]}