{"title": "LLM-Select: Feature Selection with Large Language Models", "authors": ["Daniel P. Jeong", "Zachary C. Lipton", "Pradeep Ravikumar"], "abstract": "In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., \"blood pressure\") in predicting an outcome of interest (e.g., \"heart failure\"), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could potentially benefit practitioners in domains like healthcare, where collecting high-quality data comes at a high cost.", "sections": [{"title": "Introduction", "content": "Transformer-based large language models (LLMs) pretrained on massive text corpora for next-word prediction exhibit the remarkable capability to generalize to unseen tasks, simply by conditioning on an input prompt that contains task-relevant instructions and a small number of examples (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020). With sufficient model scale and an appropriate prompting strategy, these models demonstrate strong performance on various commonsense, symbolic, and arithmetic reasoning tasks (Lewkowycz et al., 2022; Wei et al., 2022b; Kojima et al., 2022; Suzgun et al., 2023; Anil et al., 2023) and complex question-answering and prediction tasks that require real-world knowledge (Petroni et al., 2019; Li\u00e9vin et al., 2022; Singhal et al., 2023; Manikandan et al., 2023). Such findings suggest that by pretraining on vast amounts of text from various domains, LLMs encode rich knowledge about real-world relationships, which they can leverage for performing various downstream tasks (Choi et al., 2022; Moor et al., 2023). In this paper, we demonstrate that LLMs are capable of performing feature selection for supervised learning tasks. Given that we are often aware of the real-world semantics associated with the input features (e.g., \"blood pressure\") and the target outcome (e.g., \"heart failure\") in a downstream training dataset, we investigate effective ways of prompting an LLM to identify the most informative features for predicting the outcome (Figure 1(a)). For example, we prompt the LLM with \u201cRank the following features by their importance for"}, {"title": "Related Work", "content": "2.1 Prompting LLMs\nPrompting is a computationally efficient and effective method for adapting pretrained LLMs to perform new tasks unseen during training (Radford et al., 2019; Liu et al., 2023). In a standard prompting setup, an output is autoregressively sampled from an LLM, conditional on text descriptions of a desired task and optionally a set of in-context input-output examples (Brown et al., 2020), and used as a solution for the given task. Even without task-specific fine-tuning, such a zero-/few-shot in-context learning approach can be surprisingly effective for adapting pretrained LLMs to solve a wide range of natural-language tasks (Hendrycks et al., 2021; Lin et al., 2022; Patel & Pavlick, 2022; Srivastava et al., 2023), given a language model of sufficient scale (Wei et al., 2022a). Meanwhile, recent works show that LLM outputs can be highly sensitive to the specifics of the input prompt and the decoding strategy (Jiang et al., 2020; Schick & Sch\u00fctze, 2021; Zhao et al., 2021) and that choosing an appropriate prompting strategy is crucial, especially for challenging reasoning tasks. In our experiments, we consider two prompting techniques\u2014chain-of-thought prompting (CoT; Wei et al., 2022b) and self-consistency decoding (Wang et al., 2023). We focus on these methods as they often dramatically boost performance on tasks that require multi-step reasoning (Kojima et al., 2022; Lewkowycz et al., 2022; Chen et al., 2023), which we hypothesized to be important for feature selection. Chain-of-thought prompting (CoT). Chain-of-thought prompting (CoT; Wei et al., 2022b) is a few-shot prompting method that augments each input-output example with a chain-of-thought a coherent series of natural-language reasoning steps that leads to the correct answer. Wang et al. (2023) show that given a large-enough model and an appropriately designed input prompt, CoT prompting can elicit logically consistent step-by-step solutions to unseen examples from the LLM, and substantially boost performance over standard few-shot prompting (Brown et al., 2020) on tasks such as solving math problems (Lewkowycz et al., 2022; Imani et al., 2023), answering commonsense reasoning questions (Suzgun et al., 2023; Anil et al., 2023), and answering expert-level medical questions (Li\u00e9vin et al., 2022; Singhal et al., 2023) Self-consistency decoding. A common sampling strategy used for LLMs is greedy decoding (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022), where at each token-generation step, the token with the highest probability is taken as the output. In self-consistency decoding (Wang et al., 2023) on the other hand, multiple outputs are randomly sampled from the LLM (via e.g., temperature sampling (Ackley et al., 1985)) and marginalized to generate the final prediction. Prior works suggest that for CoT prompting, self-consistency decoding can significantly boost performance and that it is especially beneficial when a diverse set of reasoning paths are possible for solving a given task (Wang et al., 2023; Lewkowycz et al., 2022).\n2.2 Feature Selection\nFeature selection is a classical machine learning problem, where given a set of candidate features, the goal is to select the most informative feature subset that is predictive of an outcome of interest (Blum & Langley, 1997; Guyon & Elisseeff, 2003; Chandrashekar & Sahin, 2014; Li et al., 2017). Feature selection methods can generally be grouped into three categories: filter, wrapper, and embedded methods. Filter methods (Lazar et al., 2012) select features by ranking them according to a statistical or information-theoretic criterion (e.g., mutual information (Lewis, 1992; Ding & Peng, 2005), Fisher score (Duda et al., 2001; Gu et al., 2011), maximum mean discrepancy (Song et al., 2012)) and choosing the top ranked features, independent of the downstream learning algorithm. Wrapper methods (Kohavi & John, 1997) identify a locally optimal feature subset that maximizes the performance of the downstream prediction model using a heuristic search strategy (e.g., sequential selection (Ferri et al., 1994; Luo & Chen, 2014), recursive feature elimination (RFE; Guyon et al., 2002)). Embedded methods select features as part of the model learning process, and are often based on regularization techniques that encourage feature sparsity (Tibshirani, 1996; Yuan & Lin, 2006; Feng & Simon, 2017; Lemhadri et al., 2021). In our experiments, we compare our methods against traditional feature selection baselines from all three categories."}, {"title": "Selecting Features with LLMs", "content": "We address the standard supervised learning setup where, given labeled data $\\mathcal{D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^n$ with $x^{(i)} \\in \\mathbb{R}^d$ and $y^{(i)} \\in \\mathcal{Y}$, our goal is to learn a prediction model $f \\in \\mathcal{F}$ such that $f = \\arg \\min_{f\\in \\mathcal{F}} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{L}(f, \\mathcal{D})]$ for some model class $\\mathcal{F}$ and loss function $\\mathcal{L}$. We assume access to concepts $c = [c_1, ..., c_d]$ for the input features and $c_y$ for the prediction target, which are text descriptions that capture their real-world semantics. For example, when predicting heart failure (1 if positive, 0 otherwise) given a patient's blood pressure and weight measurements, $c = [\\text{``blood pressure''}, \\text{``weight''}]$, $c_y = \\text{``heart failure''}$, and $x = [x_1,x_2]$ denotes the numerical measurements used to learn $f$. Concept annotations are widely available in many practical settings, e.g., as column names in tabular data or via datasheets that contain auxiliary metadata. For feature selection, our goal is to find a subset $S \\subset \\{1,...,d\\}$ of size $k < d$ such that a model $f_S$ trained on $\\mathcal{D}_S = \\{(x^{(i)}_S,y^{(i)})\\}_{i=1}^n$, where $x^{(i)}_S = [x^{(i)}_{s_1},...,x^{(i)}_{s_k}]$, achieves strong performance under a budget on $k$.\n3.1 Feature Selection with LLMs\nTo leverage a pretrained LLM $\\mathcal{M}$ for feature selection, we prompt $\\mathcal{M}$ with the input concepts $c$ and target concept $c_y$, and select features based on the generated output. We consider the following three approaches: (i) selecting features based on LLM-generated feature importance scores, (ii) selecting features based on an LLM-generated ranking, and (iii) sequentially selecting features in a dialogue with an LLM. We design separate prompt templates for each approach and denote them by $prompt_{score}$, $prompt_{rank}$, and $prompt_{seq}$, respectively. Each prompt template can be viewed as a function of the input and target concepts which outputs a set of natural-language instructions tailored to the corresponding selection strategy. While generally, text outputs generated from an LLM need to be processed further to extract the relevant information, we omit such steps in the notation below for simplicity. Selection based on LLM-generated feature importance scores (LLM-SCORE). In this approach, we prompt $\\mathcal{M}$ for a set of numerical feature importance scores $s = [s_1, ..., s_d]$ with $s_j \\in [0, 1] \\forall j \\in \\{1, ...,d\\}$, where a high $s_j$ indicates that an input concept $c_j$ is closely related to $c_y$. Formally, we can represent this as\n$s_j = \\mathcal{M}(prompt_{score}(c_j, c_y)), \\forall j \\in \\{1,...,d\\}.$  (1)\nWe then define $S$ to be the set of indices of the top-k concepts with the highest importance scores and use $x_S$ for learning $f_S$. Given that the LLM is only given a single input concept $c_j$ and the target concept $c_y$ each time it is prompted, we hypothesize that $s_j$ captures the marginal importance of each feature for predicting the target, as informed by the knowledge encoded in $\\mathcal{M}$. We note that the feature importance scores $s$ are directly parsed from the text output and do not correspond to the token probabilities associated with generating the text output. We also note that the score range of [0, 1] is an arbitrary choice and therefore evaluate the sensitivity of LLM-SCORE to different choices in Appendix A.3.4. Selection based on an LLM-generated feature ranking (LLM-RANK). In this approach, we prompt $\\mathcal{M}$ for a ranking $r = [c_1',..., c_d']$ of all input concepts, where the input concepts $c$ are ordered by their conceptual relevance to $c_y$. Formally, we can represent this as\n$r = \\mathcal{M}(prompt_{rank}(c, c_y)).$ (2)"}, {"title": "Experiments", "content": "We define $S$ to be the set of indices of the top-k highest ranked concepts and use $x_S$ for learning $f_S$. We hypothesize that the rank of each input concept reflects its relative importance for predicting the target, with respect to all of the other input concepts in $c$. Sequential selection in a dialogue with an LLM (LLM-SEQ). In this approach, we consider a selection strategy analogous to sequential selection methods (Ferri et al., 1994; Luo & Chen, 2014). We start with an empty set of concepts and iteratively add a new concept by prompting the LLM to select a candidate concept that would maximally improve cross-validation performance, until we have selected k concepts. Formally, at each iteration $t = 1, ..., d$, we have\n$c^{(t)} = \\mathcal{M}(prompt_{seq}(C_{S_{t-1}}, c_y)),$ (3)\nwhere $c^{(t)}$ denotes the t-th selected input concept, $S_t \\subseteq \\{1,...,d\\}$ denotes the subset of concept indices selected up to the t-th iteration, and $S_0 = \\emptyset$. As another setup, we consider starting with $S_1 = \\{ \\arg \\max_j s_j\\}$ containing the concept with the highest score from Equation (1) and iterating over $t = 2, ..., d$. In the main text, we focus on the former, as it empirically performs better than the latter (we compare the two approaches in Appendix A.3.5). At each t, we then use $x_S$ to learn $f_S$. We hypothesize that this approach encourages selecting a feature that is maximally informative with respect to the feature subset already selected. For instantiating each method, we use LLMs that have been fine-tuned via instruction tuning and reinforce-ment learning from human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2022; Ouyang et al., 2022), which are generally better at following instructions and capable of handling conversational contexts. However, we emphasize that the LLMs are not fine-tuned in any way on the downstream dataset $\\mathcal{D}$.\n4 Experiments\nIn this section, we demonstrate the effectiveness of the three LLM-based feature selection methods introduced in Section 3 on various real-world prediction tasks. For all of our experiments, we use the following LLMs:\n1. GPT-4 (OpenAI, 2023): ~1.7T parameters,\n2. GPT-3.5 (Brown et al., 2020): ~175B parameters,\n3. Llama-2 (Touvron et al., 2023): 70B parameters,\n4. Llama-2 (Touvron et al., 2023): 13B parameters,\n5. Llama-2 (Touvron et al., 2023): 7B parameters. For GPT-4 and GPT-3.5, we use the gpt-4-0613 and gpt-3.5-turbo models available via the OpenAI API. We clarify that for both models, the official parameter counts have not been disclosed by OpenAI, and that the approximate (~) number of parameters listed here are rumored estimates. For Llama-2, we use the HuggingFace checkpoints llama-2-70b-chat-hf, llama-2-13b-chat-hf, and llama-2-7b-chat-hf and use the vLLM framework (Kwon et al., 2023) to increase throughput and speed up output generation. Prompt design. We include all of the prompt templates that we found to work well in Appendices C-E. In the default template, we only include (i) the main system prompt (e.g., \u201cYour task is to provide a feature importance score between 0 and 1 for predicting (target concept) and a reasoning behind how the importance score was assigned.", "Output your answer in a JSON format.\"), and (iii) the main user prompt (e.g., \"Provide a score and reasoning for (concept).\"). We emphasize that the default prompts are not \"fine-tuned\" for each dataset in any way, as they only embed the input and target concepts and no other dataset-specific information. Meanwhile, we examine how the following changes to the input prompt affect feature selection performance": "n1. Adding dataset-specific context: When prompting the LLM to select features for dataset $\\mathcal{D}$"}, {"title": "Discussion and Conclusion", "content": "In this work, we demonstrated that LLMs are capable of performing feature selection for supervised learning tasks, even without access to the downstream training data. We showed that with sufficient LLM scale, selecting features by zero-shot prompting an LLM leads to strong downstream predictive performance, com-"}]}