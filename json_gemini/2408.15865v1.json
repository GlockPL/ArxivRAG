{"title": "\u03bcYOLO: Towards Single-Shot Object Detection\non Microcontrollers", "authors": ["Mark Deutel", "Christopher Mutschler", "J\u00fcrgen Teich"], "abstract": "This work-in-progress paper presents results on the feasi-\nbility of single-shot object detection on microcontrollers using YOLO.\nSingle-shot object detectors like YOLO are widely used, however due\nto their complexity mainly on larger GPU-based platforms. We present\n\u03bcYOLO, which can be used on Cortex-M based microcontrollers, such as\nthe OpenMV H7 R2, achieving about 3.5 FPS when classifying 128x128\nRGB images while using less than 800 KB Flash and less than 350 KB\nRAM. Furthermore, we share experimental results for three different ob-\nject detection tasks, analyzing the accuracy of YOLO on them.", "sections": [{"title": "1 Introduction", "content": "Object detection in computer vision describes the task of first finding, i.e., re-\ngressing, a bounding box, and then classifying objects in scenes. Early approaches\nto this problem use a sliding window to look at evenly spaced locations of the\nimage [4] or, like regional CNNs (R-CNNs) [5], solve the two problems sepa-\nrately by first using some region proposal algorithm and then classifying the\ngenerated set of proposals by re-purposing a standard CNN. Nowadays, these\napproaches are mostly dominated by single-shot detectors (SSDs), with the most\nprominent one being YOLO [8]. They are capable of solving the detection task\nextremely efficient using only a single CNN, and thus do not require a separate\nregion proposal stage. However, even though SSDs are efficient and therefore offer\nhuge potential for energy-efficient real-time object detection, their deployment\non edge devices has so far been achieved mainly for larger embedded platforms\nsuch as the Jetson Xavier [9,1], while the feasibility of object detection on more\nresource-constrained microcontrollers is still an open topic.\nWe present \u03bcYOLO, optimized for use on Cortex-M based microcontrollers,\nand show first results from work in progress. With our work, we aim to evaluate\nthe feasibility and limitations of object detection on microcontrollers, despite\nsevere resource constraints (typically less than 1-2 MB of Flash and less than 1\nMB of RAM). To meet these constraints, we lower the input image resolution\nto 128 x 128 pixels, significantly reduce the number of trainable parameters of\nthe backbone network, and reduce the grid size and bounding box predictors per\ncell compared to the original YOLO architecture. In the following we present a\nsolution deploying \u03bcYOLO on a Cortex-M7 based OpenMV H7 R2 microcon-\ntroller, while achieving a framerate of 3.5 frames per second at 480 MHz while\nrequiring less than 800 KB of Flash and less than 350 KB of RAM.\nThe remainder of this paper is structured as follows: First, in Section 2, we\ndiscuss the architecture of \u03bcYOLO. Second, in Section 3, we present results from\nongoing work on three object detection tasks, two derived from subsets of the\nCOCO dataset [7] and one using a self-recorded dataset. Furthermore, based on\nthe results obtained so far, we analyze the performance of \u03bcYOLO, provide an\nerror analysis and discuss the limitations of our approach. Section 4 concludes."}, {"title": "2 \u03bcYOLO", "content": "The idea of YOLO is to solve object detection tasks using a single DNN and\nby looking at images only once, i.e. in a single forward pass. To achieve this,\nYOLO consists of a CNN backbone followed by a detection head, which typically\ncontains linear layers that make predictions in a $S\\times S$ grid-based manner. For\neach grid cell, the detection head proposes a class prediction vector of size N\nand a set of bounding boxes B, each defined by a confidence score, a center point\nrelative to the upper left corner of its cell, and a size relative to the image size.\nThe backbone CNN of \u03bcYOLO consists of a single convolution, followed by\nseven depth-wise separable convolutions [2], see Table 1 for a detailed description\nof the architecture. We choose depth-wise separable convolutions to minimize\ntrainable parameters. \u03bcYOLO's classification head has two linear layers and has\nan output similar to the original YOLO paper [8] with a shape of $S\\times S\\times N+B*5$.\nOur experiments so far have shown that, given the resource constraints of our\ntarget platform and the low spatial resolution of the input images (128), $B = 2$\nand $S = 5$ is the best compromise. We use ReLU as the activation, but use batch\nnormalization instead of a dropout layer before the last linear layer as described\nin the original YOLO paper.\nWe pre-trained our backbone model on the Caltech-256 dataset [6], which is\na 256 class image classification problem. We achieved a top-1 accuracy of 38%\nand a top-5 accuracy of 61% with \u00b5YOLO's backbone, which while below other\nlarge-scale classification models, is still reasonable given the small capacity of\nour backbone and the large number of classes in Caltech-256.\nTo further reduce the size of \u00b5YOLO, we apply network pruning to all con-\nvolutional and linear layers during training using an iterative gradual pruning\nschedule [10] with a L1 norm heuristic, as well as 8-bit quantization afterwards.\nWe did not use quantization-aware training in order to minimize the overhead\nrequired for training, as we realized early on that for our application it did\nnot provide an improvement over post-training quantization. To automate these\nsteps and to convert the resulting model to deployable C-code, we apply the\ncompression and deployment pipeline described in [3]."}, {"title": "3 Evaluation", "content": "We evaluated \u03bcYOLO on three detection tasks: (1) human detection, (2) vehicle\ndetection (truck, bus, car, bicycle, motorcycle), both derived from the Microsoft\nCOCO dataset [7], and (3) toy groceries detection in a mini-fridge (water, milk,\nchocolate milk, orange juice) using a dataset we recorded ourselves. For all tasks,\nwe used an input resolution of 3 \u00d7 128 \u00d7 128 and applied affine transformations\nas well as brightness, saturation, and hue adjustments to all training samples\nduring training to prevent overfitting. We trained all models for 400 epochs using\nstochastic gradient descent (SGD) with learning rate of 0.001, momentum of 0.9,\nand weight decay of 0.005. During the last 100 epochs of training, we applied\niterative pruning every 20 epochs. Furthermore, due to the low resolution of the\ninput, we discarded ground truth boxes that became smaller than 12 \u00d7 12 pixels."}, {"title": "3.1\nExperimental Results", "content": "We present the mean average precision with an intersection over union (IoU)\nthreshold of 0.5 (mAP0.5) over the course of training for all three detection tasks,\nsee Fig. 1. We repeated the training for all tasks three times with different seeds.\nOverall, we observed that \u00b5YOLO achieved a significantly higher mAP0.5 score\non the refrigerator detection task (56.4% on average) than on the other two tasks\n(27.7% on average on the human detection task and 12.3% on average on the\nvehicles detection task). We observed a slight degradation of mAP0.5 especially\nfor the fridge task (blue curve) during the last 100 epochs of training due to\npruning, while we did not notice any significant degradation after quantization.\nWe argue that the strong contrast in performance observed in the three\ndifferent tasks shown in Fig. 1 is mainly the result of the different scene depth\nand complexity present in the datasets, combined with the low input resolution,\nwhich makes it difficult to detect and distinguish particularly small objects in\nthe background. To test this hypothesis, we trained \u03bcYOLO on a simplified\nversion of the vehicle task and at different input image resolutions, considering\nas ground truth only a maximum of three largest bounding boxes per image,\nsee Fig. 2. To handle larger input image resolutions, we increased the kernel\nsize in the first convolution of \u03bcYOLO to 7 and the padding of the following\ntwo depthwise-separable convolutions to 2. For smaller image resolutions, we\ndecreased the number of neurons in the linear layers of the detection head.\nLooking at Fig. 2, \u03bcYOLO was able to achieve a significantly higher mAP0.5\non the simplified vehicles detection task than on the unrestricted version (red\ncurve in Fig. 1) for all tested input image resolutions. Interestingly, while choos-\ning an extremely small input resolution had a negative impact on the achieved\nprecision, see the blue curve compared to the other three curves, increasing the\nimage resolution did not affect the observed mAP0.5, which always converged to"}, {"title": "3.2 Error Analysis", "content": "To deeper investigate made observations, we analyzed the detection errors of\nYOLO by computing the confusion matrices over the validation datasets for\nthe fridge and vehicles tasks, see Fig. 4. Each of the confusion matrices represent\nthe predicted bounding boxes by class, i.e. rows, compared to the actual ground\ntruth bounding boxes provided by the validation set, i.e. columns. Thereby, a\npredicted bounding box is \"not background\" if its confidence score is higher than\n50%, and it is a correct prediction if it overlaps its corresponding ground truth\nwith an IoU higher than 50%. This means that, the diagonals of the matrices\ncontain the correctly predicted bounding boxes, while the triangular matrices\nabove and below contain errors. More precisely, the last row of each matrix\ndenotes false negatives, the last columns denotes false positives, and all other\nfields denote misclassifications of correctly detected bounding boxes.\nComparing Fig.4a with Fig. 4b and Fig. 4c, the most common observable er-\nrors of \u03bcYOLO on the validation sets were false negatives, i.e. \u03bcYOLO \"missing\"\npresent objects. Other errors, such as misclassifying correctly detected objects or\nmistakenly detecting an object where none exists, i.e. false positives, were much\nless common in comparison. However, we found that these effects were much\nmore pronounced in the unrestricted vehicles detection task, see Fig. 4b, than\nin the restricted vehicles and unrestricted fridge detection tasks, see Fig. 4a and\nFig. 4c. Based on these observations, we argue that the high number of false neg-\natives we found in Fig. 4b is mainly due to the low input resolution of \u03bcYOLO\ncombined with the high complexity of multi-layered scenes with object overlap\nand partial occlusion, as they are predominantly present in the COCO dataset.\nIn comparison, the fridge task or the simplified vehicles task are less complex,\nwhich as a result seems to allow \u03bcYOLO to achieve higher mAP0.5 scores, mainly\ndue to false negatives being less common."}, {"title": "3.3 Deployment", "content": "To deploy YOLO on the OpenMV H7 microcontroller we used the pipeline\ndescribed in [3], see Table 2 and Table 3 for a detailed breakdown of memory\nconsumption and performance. The difference in Flash consumption and slight\nchange in performance that can be observed in Table 2 is due to the size of the\noutput feature map being dependent on the number of classes, thereby changing\nthe number of neurons in the output layer. Furthermore, in Table 3 it can be seen\nthat changing the size of input images processed by \u00b5YOLO has a big impact on\nRAM consumption and performance. In particular, the large increase in RAM\nconsumption makes the use of YOLO with larger input image resolutions on\nthe OpenMV H7 microcontroller quickly infeasible.\nBesides changing the input image resolution, the trade-off between FPS and\nmAP0.5 can also be controlled via pruning. In initial experiments and by using\naggressive pruning schedules, we were able to achieve up to 8 FPS, but at the\ncost of a significantly degraded mAP0.5. Since 3.5 FPS might be too low for some\ndetection applications, we consider further exploration of the trade-off between\nmodel performance and precision as future work."}, {"title": "4 Conclusion", "content": "We presented \u00b5YOLO, a novel single-shot object detection model based on\nYOLO that can be deployed on microcontroller platforms without hardware\nacceleration with a memory footprint less than 800 Kb of Flash and less than\n350 Kb of RAM. We presented work in progress results on three different ob-\nject detection tasks and provide an in-depth analysis of the accuracy YOLO\ncan achieve on these tasks. We also shared results from deploying \u00b5YOLO on\na Cortex-M7 based \"OpenMV H7 R2\" microcontroller, where we achieved a\nperformance of about 3.5 frames per second for all three tasks."}]}