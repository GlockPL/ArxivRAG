{"title": "\u03bcYOLO: Towards Single-Shot Object Detection on Microcontrollers", "authors": ["Mark Deutel", "Christopher Mutschler", "J\u00fcrgen Teich"], "abstract": "This work-in-progress paper presents results on the feasibility of single-shot object detection on microcontrollers using YOLO. Single-shot object detectors like YOLO are widely used, however due to their complexity mainly on larger GPU-based platforms. We present \u00b5YOLO, which can be used on Cortex-M based microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when classifying 128x128 RGB images while using less than 800 KB Flash and less than 350 KB RAM. Furthermore, we share experimental results for three different object detection tasks, analyzing the accuracy of YOLO on them.", "sections": [{"title": "1 Introduction", "content": "Object detection in computer vision describes the task of first finding, i.e., regressing, a bounding box, and then classifying objects in scenes. Early approaches to this problem use a sliding window to look at evenly spaced locations of the image [4] or, like regional CNNs (R-CNNs) [5], solve the two problems separately by first using some region proposal algorithm and then classifying the generated set of proposals by re-purposing a standard CNN. Nowadays, these approaches are mostly dominated by single-shot detectors (SSDs), with the most prominent one being YOLO [8]. They are capable of solving the detection task extremely efficient using only a single CNN, and thus do not require a separate region proposal stage. However, even though SSDs are efficient and therefore offer huge potential for energy-efficient real-time object detection, their deployment on edge devices has so far been achieved mainly for larger embedded platforms such as the Jetson Xavier [9,1], while the feasibility of object detection on more resource-constrained microcontrollers is still an open topic.\nWe present \u03bcYOLO, optimized for use on Cortex-M based microcontrollers, and show first results from work in progress. With our work, we aim to evaluate the feasibility and limitations of object detection on microcontrollers, despite severe resource constraints (typically less than 1-2 MB of Flash and less than 1 MB of RAM). To meet these constraints, we lower the input image resolution"}, {"title": "2 \u03bcYOLO", "content": "The idea of YOLO is to solve object detection tasks using a single DNN and by looking at images only once, i.e. in a single forward pass. To achieve this, YOLO consists of a CNN backbone followed by a detection head, which typically contains linear layers that make predictions in a $S \\times S$ grid-based manner. For each grid cell, the detection head proposes a class prediction vector of size N and a set of bounding boxes B, each defined by a confidence score, a center point relative to the upper left corner of its cell, and a size relative to the image size. The backbone CNN of \u03bcYOLO consists of a single convolution, followed by seven depth-wise separable convolutions [2], see Table 1 for a detailed description of the architecture. We choose depth-wise separable convolutions to minimize trainable parameters. \u03bcYOLO's classification head has two linear layers and has an output similar to the original YOLO paper [8] with a shape of $S \\times S \\times N + B*5$. Our experiments so far have shown that, given the resource constraints of our target platform and the low spatial resolution of the input images (128), $B = 2$ and $S = 5$ is the best compromise. We use ReLU as the activation, but use batch normalization instead of a dropout layer before the last linear layer as described in the original YOLO paper.\nWe pre-trained our backbone model on the Caltech-256 dataset [6], which is a 256 class image classification problem. We achieved a top-1 accuracy of 38% and a top-5 accuracy of 61% with \u00b5YOLO's backbone, which while below other large-scale classification models, is still reasonable given the small capacity of our backbone and the large number of classes in Caltech-256.\nTo further reduce the size of \u00b5YOLO, we apply network pruning to all convolutional and linear layers during training using an iterative gradual pruning schedule [10] with a L1 norm heuristic, as well as 8-bit quantization afterwards. We did not use quantization-aware training in order to minimize the overhead required for training, as we realized early on that for our application it did not provide an improvement over post-training quantization. To automate these steps and to convert the resulting model to deployable C-code, we apply the compression and deployment pipeline described in [3]."}, {"title": "3 Evaluation", "content": "We evaluated \u00b5YOLO on three detection tasks: (1) human detection, (2) vehicle detection (truck, bus, car, bicycle, motorcycle), both derived from the Microsoft COCO dataset [7], and (3) toy groceries detection in a mini-fridge (water, milk, chocolate milk, orange juice) using a dataset we recorded ourselves. For all tasks, we used an input resolution of $3 \\times 128 \\times 128$ and applied affine transformations as well as brightness, saturation, and hue adjustments to all training samples during training to prevent overfitting. We trained all models for 400 epochs using stochastic gradient descent (SGD) with learning rate of 0.001, momentum of 0.9, and weight decay of 0.005. During the last 100 epochs of training, we applied iterative pruning every 20 epochs. Furthermore, due to the low resolution of the input, we discarded ground truth boxes that became smaller than 12 \u00d7 12 pixels."}, {"title": "3.1 Experimental Results", "content": "We present the mean average precision with an intersection over union (IoU) threshold of 0.5 (mAP0.5) over the course of training for all three detection tasks, see Fig. 1. We repeated the training for all tasks three times with different seeds. Overall, we observed that \u00b5YOLO achieved a significantly higher mAP0.5 score on the refrigerator detection task (56.4% on average) than on the other two tasks (27.7% on average on the human detection task and 12.3% on average on the vehicles detection task). We observed a slight degradation of mAP0.5 especially for the fridge task (blue curve) during the last 100 epochs of training due to pruning, while we did not notice any significant degradation after quantization.\nWe argue that the strong contrast in performance observed in the three different tasks shown in Fig. 1 is mainly the result of the different scene depth and complexity present in the datasets, combined with the low input resolution, which makes it difficult to detect and distinguish particularly small objects in the background. To test this hypothesis, we trained \u03bcYOLO on a simplified version of the vehicle task and at different input image resolutions, considering as ground truth only a maximum of three largest bounding boxes per image, see Fig. 2. To handle larger input image resolutions, we increased the kernel size in the first convolution of \u03bcYOLO to 7 and the padding of the following two depthwise-separable convolutions to 2. For smaller image resolutions, we decreased the number of neurons in the linear layers of the detection head.\nLooking at Fig. 2, \u03bcYOLO was able to achieve a significantly higher mAP0.5 on the simplified vehicles detection task than on the unrestricted version (red curve in Fig. 1) for all tested input image resolutions. Interestingly, while choosing an extremely small input resolution had a negative impact on the achieved precision, see the blue curve compared to the other three curves, increasing the image resolution did not affect the observed mAP0.5, which always converged to"}, {"title": "3.2 Error Analysis", "content": "To deeper investigate made observations, we analyzed the detection errors of YOLO by computing the confusion matrices over the validation datasets for the fridge and vehicles tasks, see Fig. 4. Each of the confusion matrices represent the predicted bounding boxes by class, i.e. rows, compared to the actual ground truth bounding boxes provided by the validation set, i.e. columns. Thereby, a predicted bounding box is \"not background\" if its confidence score is higher than 50%, and it is a correct prediction if it overlaps its corresponding ground truth with an IoU higher than 50%. This means that, the diagonals of the matrices contain the correctly predicted bounding boxes, while the triangular matrices above and below contain errors. More precisely, the last row of each matrix denotes false negatives, the last columns denotes false positives, and all other fields denote misclassifications of correctly detected bounding boxes.\nComparing Fig.4a with Fig. 4b and Fig. 4c, the most common observable errors of \u03bcYOLO on the validation sets were false negatives, i.e. \u03bcYOLO \"missing\" present objects. Other errors, such as misclassifying correctly detected objects or mistakenly detecting an object where none exists, i.e. false positives, were much less common in comparison. However, we found that these effects were much more pronounced in the unrestricted vehicles detection task, see Fig. 4b, than in the restricted vehicles and unrestricted fridge detection tasks, see Fig. 4a and Fig. 4c. Based on these observations, we argue that the high number of false negatives we found in Fig. 4b is mainly due to the low input resolution of \u03bcYOLO combined with the high complexity of multi-layered scenes with object overlap and partial occlusion, as they are predominantly present in the COCO dataset. In comparison, the fridge task or the simplified vehicles task are less complex, which as a result seems to allow \u03bcYOLO to achieve higher mAP0.5 scores, mainly due to false negatives being less common."}, {"title": "3.3 Deployment", "content": "To deploy YOLO on the OpenMV H7 microcontroller we used the pipeline described in [3], see Table 2 and Table 3 for a detailed breakdown of memory consumption and performance. The difference in Flash consumption and slight change in performance that can be observed in Table 2 is due to the size of the output feature map being dependent on the number of classes, thereby changing"}, {"title": "4 Conclusion", "content": "We presented \u00b5YOLO, a novel single-shot object detection model based on YOLO that can be deployed on microcontroller platforms without hardware acceleration with a memory footprint less than 800 Kb of Flash and less than 350 Kb of RAM. We presented work in progress results on three different object detection tasks and provide an in-depth analysis of the accuracy YOLO can achieve on these tasks. We also shared results from deploying \u00b5YOLO on a Cortex-M7 based \"OpenMV H7 R2\" microcontroller, where we achieved a performance of about 3.5 frames per second for all three tasks."}]}