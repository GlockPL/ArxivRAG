{"title": "ShapeICP: Iterative Category-level Object Pose and Shape Estimation from Depth", "authors": ["Yihao Zhang", "John J. Leonard"], "abstract": "Category-level object pose and shape estimation from a single depth image has recently drawn research attention due to its wide applications in robotics and self-driving. The task is particularly challenging because the three unknowns, object pose, object shape, and model-to-measurement correspondences, are compounded together but only a single view of depth measurements is provided. The vast majority of the prior work heavily relies on data-driven approaches to obtain solutions to at least one of the unknowns and typically two, running with the risk of failing to generalize to unseen domains. The shape representations used in the prior work also mainly focus on point cloud and signed distance field (SDF). In stark contrast to the prior work, we approach the problem using an iterative estimation method that does not require learning from any pose-annotated data. In addition, we adopt a novel mesh-based object active shape model that has not been explored by the previous literature. Our algorithm, named ShapeICP, has its foundation in the iterative closest point (ICP) algorithm but is equipped with additional features for the category-level pose and shape estimation task. The results show that even without using any pose-annotated data, ShapeICPsurpasses many data-driven approaches that rely on the pose data for training, opening up a new solution space for researchers to consider.", "sections": [{"title": "I. INTRODUCTION", "content": "We investigate the task of estimating the pose and shape of an object given a single depth image of it. The object instance is only known up to its semantic category. Its exact geometry (i.e. CAD model) is unavailable to us. Such a problem is commonly encountered in many scenarios ranging from self-driving to home robots. For example, self-driving may require estimation of the poses and shapes of other vehicles on the street without knowing the detailed CAD models of them apriori. Similarly, home robots may require estimation of the poses and shapes of table top objects in order to manipulate them. Because of the wide applications and the significance of this task, it has drawn substantial research attention in recent years.\nThe category-level object pose and shape estimation prob-lem is typically set up such that the instance segmentation and semantic classification of the object is done on the accompanied RGB image with an off-the-shelf method [1]\u2013[20]. However, even after factoring out the object detection and segmentation problem, inferring the object pose and shape from the segmented depth image remains challenging due to the three entangled unknowns, object pose, object shape and model-to-measurement correspondences. It requires us to know two of the three unknowns to reduce the problem to an easily solvable one. Specifically, if we know the object shape and how the measurements correspond to the points on the surface of the shape, it is reduced to the problem of aligning two point sets which the Umeyama algorithm [21] can solve. If the object pose and the corre-spondences between the measurements and a category-level shape template are known, one can transform the template to the same pose as the measurements and translate the template points onto the corresponding measurements to obtain the shape. Researchers often recognize the correspondences as an unknown and deform the template to fit the measurements [22]-[25] using Chamfer loss where the closest points are the correspondences [22]-[24], [26], [27]. However, with the three variables all left unknown, the problem is extremely difficult.\nMost prior methods exploit neural networks to learn a mapping from inputs such as the RGB image, the depth image, and the categorical shape prior, to one or two of the three unknowns. For example, in [1], a deformation field is learned to deform the shape prior to obtain the object shape and a correspondence matrix is predicted to draw correspondences between the model shape and the back-projected depth points for pose estimation with Umeyama [21]. Although these learning methods greatly factor away the complexities in the problem by leveraging data and have achieved excellent performance on benchmarks, they are faced with potential failures caused by any domain change and tedious data curation process for training when they are deployed to a new environment. In the object pose and shape estimation task, the ground truth pose of an object is especially painstaking to annotate since so far there has been no scalable and easily accessible approach to measure the pose of an object. On the other hand, virtually all the prior methods focus on either point cloud or signed distance field (SDF) as the shape representation. Point clouds are flexible but they do not have the notion of surface, while SDF entails unfavorable computational complexity which is often scaled cubically with the resolution.\nIn this work, we take a radically different approach. First, in contrast to prior methods, we adopt a novel mesh-based active shape model (ASM) [28] as the object shape representation. Meshes incorporate the notion of surface, being superior to point clouds. They are also inherently a geometric representation, unlike SDF or occupancy grid where the geometry is represented implicitly and some pro-cesses such as finding the zero level set (SDF) or marching cubes [29] (occupancy grid) are required to extract the geometry. The use of meshes in category-level object pose"}, {"title": "II. RELATED WORK", "content": "We will review the related work in terms of which of the three unknowns, object pose, object shape, and model-to-measurement correspondences are predicted by neural networks. We will also discuss shape representations used in the literature.\nPredicting shape and correspondences. One of the most popular approaches is predicting the object shape as well as the correspondences between the shape and the measured depth points. The seminal work NOCS [32] belongs to this category. The predicted NOCS map encodes the depth profile (i.e. shape) of the observed object in its canonical pose. The NOCS map also corresponds pixel-to-pixel to the measured depth image, being essentially a combined prediction of the object shape and the correspondences. The Umeyama algorithm [21] can solve the transformation between the NOCS map and the depth image. Another work is [18] where the authors first use Mesh R-CNN [26] to predict a metric-scale mesh of the object shape from the image and then predict the NOCS map to solve for the pose. Among the methods that predict both shape and correspondences, a notable line of work [1]\u2013[6] attempts to predict the de-formation applied to a categorical shape prior to obtain the object shape and a correspondence matrix to associate the model (i.e. the deformed prior) points to the measured depth points. The Umeyama algorithm [21] is then engaged to solve the transformation. One representative work is Shape Prior [1] where a network takes in the RGB image patch, back-projected depth points, and the categorical shape prior to predict the deformation field and the correspondence matrix.\nPredicting shape and pose. A more straightforward ap-proach is predicting the pose and shape directly. Methods in this category often have an iterative closest point (ICP) [30] post-processing step to refine the predicted pose. One highly recognized work is CASS [7]. It incorporates a variational auto-encoder (VAE) [33] to learn a point cloud based shape embedding space. During inference, the shape latent code is predicted given the RGB-D image patch. The predicted latent code is both decoded to the shape in its canonical pose and fed to the module for pose prediction. There are various other solutions in this category of methods. For example, CenterSnap [34] and ShAPO [35] forgo the use of the region of interest (RoI) based Mask R-CNN [36] for object detection. Instead they build a YOLO-style [37] one-shot estimation pipeline. The object poses and shape codes along with the object detection are predicted in one shot on a heat map. iCaps [9] selects the rotation by comparing the depth image features to the features in a pre-computed code book and predicts the DeepSDF [38] latent vector as the shape representation. SDFEst [8] refines the predicted pose and shape by comparing the rendered depth image to the actual depth image. Other methods that directly regress shape and pose to various extent include [10]-[12], [39].\nPredicting pose. Another set of methods bypasses the shape estimation and only estimates the pose. FS-Net [40] extracts rotation-aware features through a scale and shift-invariant 3D graph convolution network. It also decouples the rotation into two perpendicular vectors for prediction. SAR-Net [13] transforms a categorical template to the same rotation as the observed point cloud by neural networks. The transformed template is aligned with the original template by Umeyama [21] to find the rotation. The translation and size are predicted based on a mirroring-completed point cloud which is the measured point cloud concatenated with the mirrored version of the point cloud with respect to the plane of symmetry. Both the template transforming step and mirroring-completion step are performed by neural networks which have to implicitly learn the object pose prediction task since both the transformation and the plane of symmetry are pose dependent. MH6D [14] transforms the measured point cloud randomly three times, regresses poses for the three point clouds and enforces consistency among the regressed results.\nPredicting correspondences. Several methods use key-point or descriptor matching as the base to solve the prob-lem. [41] assumes known model-to-measurement correspon-dences provided by neural keypoint matching and solves an alignment problem between the model keypoints and the measurement keypoints. Instead of only estimating the SIM(3) pose as in Umeyama [21], the method additionally estimates the parameters for an active shape model (ASM) [28] of the object. Fortunately, the ASM is linear and can be treated similarly as the translation. The method also features a graph-theoretic approach and an outlier-robust solver to handle false correspondences. [42] is an earlier work that essentially shares the same problem formulation as [41] but with a different solver that does not consider the outlier issue. [15] computes semantic primitives using a part segmentation network. It derives a SIM(3)-invariant shape descriptor from the primitives and minimizes the discrepancy between the"}, {"title": "III. METHOD", "content": "Our task is to estimate the SIM(3) pose and shape of an object which is only known up to its semantic category given a single depth image of it. We assume that instance segmentation of the object and its class label are provided by an off-the-shelf method such as Mask RCNN [36] as typically used in the prior work [1]\u2013[17]. In order to estimate the shape, the shape representation of the object has to be malleable to the shapes within the category. We hence adopt a mesh-based object active shape model (ASM) [28] for its flexibility and surface representability. In order to have consistent number of vertices and vertex connectivity across all the object models in the database for Principal Component Analysis (PCA) which is required to build the ASM, we deform a spherical template mesh to wrap around each object model and run PCA on the deformed templates. After obtaining the ASM offline, we estimate the pose and shape by transforming and fitting the shape model to the observed depth points. The optimization is achieved by an augmented iterative closest point (ICP) algorithm which in addition to the existing pose estimation step has a shape deformation step for shape estimation. We thus name our algorithm ShapeICP. However, ICP is a local solver and so is the base form of ShapeICP. This becomes a significant challenge due to the lack of an initial guess. To overcome it, ShapeICP is outfitted with multi-hypothesis tracking for ro-tation estimation, expectation maximization (EM) for robust correspondence handling, and shape classification for shape initialization. The multi-hypothesis estimation is paramount to the success of the algorithm. It has hypothesis scoring functions based on closest point distance, symmetry, and rendering. The full pseudo-algorithm of ShapeICP can be found in Alg. 1.\nTemplate deformation. Mesh-based ASMs have been applied to human faces [46], [47] but not yet in the task of object pose and shape estimation. The bottleneck is that object models from a database such as ShapeNet [48] have different numbers of vertices and different vertex connec-tivity even within the same category (see Fig. 1), making it difficult to draw corresponding vertices across models for PCA. To solve this challenge, we deform a template mesh to wrap around each object model and run PCA on the vertices of the deformed templates for all the object models within a category. More specifically, we denote a mesh_by_ $M = (V,E,F)$, where $V = \\{v_i\\}_{i=1}^{V}$ are the V vertices in the mesh, $E = \\{e_i\\}_{i=1}^{E}$ is the set of E tuples that each store the indices of a pair of vertices connected by an edge, $F = \\{f_i\\}_{i=1}^{F}$ is the set of F tuples of indices of vertices on the same faces. To deform the template mesh $M_s$ to a target mesh $M_t$, we first randomly sample points on the mesh surfaces with N points from the template mesh $p_n \\sim M_s$ and M points from the target mesh $q_m \\sim M_t$ (here N = M). The deformation is done by minimizing the"}, {"title": "C. Alternating Pose and Shape Optimization", "content": "Formulation. To introduce our ShapeICP formulation, we need sample points on the ASM. Each sampled point can be expressed as\n$p_n = \\sum_{i \\in F(n)} \\mu_{n,i}v_i \\quad s.t. \\sum_{i \\in F(n)} \\mu_{n,i} = 1$ (6)\nwhere F(n) is a function that maps the sample index n to a randomly selected face, and $\\mu_{n,i}$ are randomly generated interpolation weights for this sample. These interpolation weights are fixed during the optimization. Combining with (5), $p_n = \\sum_{i \\in F(n)} \\mu_{n,i} (b_{o,i} + \\sum_{k=1}^{K} c_k b_{k,i})$. We now intro-duce the ShapeICP formulation:\n$Res_O (\\Theta) = \\underset{\\substack{R \\in SO(3), t \\in \\mathbb{R}^3, s \\in \\mathbb{R}^{++} \\\\ c = [c_1 ... c_K] \\in \\mathbb{R}^K}}{\\min} \\sum_{m=1}^{M} \\underset{n}{\\min} ||sRp_n + t - q_m||_3^2$ (7)\nwhere $\\{q_m\\}_{m=1}^{M}$ (overloaded notation) are now the object depth points segmented from the depth image and back-projected to 3D using the camera intrinsics, R, t, and s are the object rotation, translation, and scale to be estimated, c which enters the objective function through $p_n$ is the ASM shape code also to be estimated. For each measurement $q_m$, (7) encourages the transformed model points to be close to the measurements, which is similar to the ICP objective function [30] but with the addition of the shape parameter c. We also remark that formulation (7) equivalently works for a point-based ASM where $p_n$ is simply a point in the ASM point cloud. However, mesh-based ASM has the notion of surface and thus is a better geometric representation. Fur-thermore, a mesh model enables the possibility of rendering to be used in Section III-D.\nCore algorithm. We make two observations of (7). First, if the shape code c is given, the problem shares the same"}, {"title": "D. Coping with Local Minima", "content": "Category-level object pose and shape estimation given only a single-view depth image naturally comes with large ambiguity (Fig. 4) since many configurations of the variables may fit equally well to the observed partial object point cloud. In other words, the optimization landscape exhibits many local minima, which hurt the performance of a local solver such as ShapeICP. In this section, we develop several strategies to cope with the local minima.\nExpectation maximization (EM). The correspondences between the measured depth points and the model points can be treated as latent variables. Instead of hard one-to-one associations, EM allows softer associations [51], reducing the chance of being stuck at a local minimum. We integrate EM into our ShapeICP formulation following a similar approach as in [52]. Each measurement m is now accompanied by a latent variable $z_m$ that models the association of this measurement. Specifically,\n$P(z_m = n) = \\begin{cases}\n\\frac{1}{Q} & \\text{if } n \\in N_Q(q_m) \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (8)\nwhere $P(z_m = n)$ is the probability of $q_m$ associated to $p_n$ and $N_Q(q_m)$ finds the Q closest transformed model points to $q_m$ (i.e. the top Q solutions to $\\underset{n}{arg \\min} ||sRp_n + t - q_m||_3^2$). We further model the conditional data likelihood as\n$P(q_m|z_m = n) = N(sRp_n + t - q_m; 0, \\Sigma_m)$ (9)\nwhere $N(\\cdot; 0, \\Sigma_m)$ is a zero-mean Gaussian distribution with covariance $\\Sigma_m$ (i.e. conditioned on $q_m$ associated to $p_n$, the residual follows a Gaussian distribution). Assuming indepen-dence $q_m|z_m \\perp z_{m'}$ , $q_m|z_m \\perp q_{m'} \\forall m' \\neq m$ and $z_m \\perp Z_{m'}$ , and isotropic covariance $\\Sigma_m = diag(\\sigma_m^2)$, the objective function $L_{em}$ derived from EM is (explanations about the probabilistic models and the independence assumptions, and the derivation of EM can be found in the appendix):\n$\\underset{\\substack{R \\in SO(3), t \\in \\mathbb{R}^3, s \\in \\mathbb{R}^{++} \\\\ c = [c_1 ... c_K] \\in \\mathbb{R}^K}}{\\min} \\sum_{m=1}^{M} \\sum_{n \\in N_Q(q_m)} w_{mn}||sRp_n + t - q_m||_3^2$ (10)\nwhere\n$w_{mn} = \\frac{N(sRp_n + t - q_m; 0, \\sigma_m)}{\\sum_{n' \\in N_Q(q_m)} N(sRp_{n'} + t - q_m; 0, \\sigma_m)}$ (11)\nwhere the hatted quantities $\\hat{s}$, $\\hat{R}$, $\\hat{t}$, and $\\hat{p_n}$ ($\\hat{p_n}$ as a result of $\\hat{c}$) are the estimates from the last step and do not participate in the current optimization. EM yields an intuitive formula-tion, which rather than one hard correspondence, considers likely correspondences weighted by their probabilities computed from the last-step estimates. This increases the field of view of the model-measurement association step and can help move out of local minima. Both the pose and shape steps can use (10).\nMulti-hypothesis estimation. Initialization is critical in avoiding local minima. The translation can be initialized as"}, {"title": "IV. EXPERIMENTS", "content": "Implementation. We implement our ShapeICP method with PyTorch [56] and PyTorch3D [49]. For the segmented depth image of an object, we perform statistical outlier removal on the back-projected point cloud as in [10], [57]. Moreover, we discard object detections that have points fewer than a threshold. All the hyperparameter values are summarized in the appendix.\nDatasets. ShapeNetCore [48] is used to compute the ASM for the six categories in the NOCS REAL data [32]. For each category, we exclude weird-looking shapes, which is similarly done in [10]. As our model does not train on any pose-annotated data, synthetic benchmarks are irrelevant. We thus evaluate our method on the NOCS REAL test set. For the shape classification network, we use the NOCS REAL train set for training.\nBaselines. We benchmark our ShapeICP method against several interesting baselines. NOCS [32] is the seminal work in this research area. Neural Analysis-by-Synthesis [16] is an optimization-based method that is not learning-based and does not rely on image sequences or strong application-specific constraints (see Section II-A for more details). Metric Scale [18] is a rare case of adopting a mesh-based shape representation. ASM-Net [10] also uses ASM but it"}, {"title": "V. CONCLUSION", "content": "We proposed the ShapeICP algorithm for category-level object pose and shape estimation from a single depth im-age. Our method implements a novel mesh-based active shape model (ASM) to represent categorical object shape. Remarkably the method does not require any pose-annotated data, unlike the vast majority of the methods in the field. ShapeICP achieves surpassing or on-par performance to many learning-based methods which rely on pose-annotated data. Future directions include how to better disambiguate the problem potentially by exploring RGB-image-based con-straints and how to better initialize the algorithm closer to the convergence basin of a global minimum."}, {"title": "APPENDIX", "content": "We start off by explaining the probability models (8)(9). Since our sensor is a depth camera, the correct association for a depth measurement at a pixel should be made by ray casting instead of looking for the closest point to $q_m$. Therefore, ideally $N_Q(q_m)$ in (8) should find the Q closest points near the point of the first hit of the cast ray (Q points instead of one to account for association uncertainties caused by the errors in the object pose and shape). However, the approximation of the Euclidean closest points near $q_m$ greatly simplifies the computation and has shown robustness in ICP. Likewise, (9) is also an approximation to simplify the computation. When the association is given for a pixel in a depth image, the uncertainty should only be along the ray through the camera optical center, the pixel, and the associated point (1D uncertainty). By using the 3D Gaussian approximation as in (9), we are assuming the sensor is not a depth camera but rather the following generative process that directly makes the spatial measurement:\n$q_m = sRp_{z_m} + t + \\epsilon_m$ (14)\nwhere $\\epsilon$ is Gaussian noise following $N(0, \\Sigma_m)$.\nThe independence assumption $q_m|z_m \\perp z_{m'}$ (i.e. $P(q_m|z_m, z_{m'}) = P(q_m|z_m)$) is reasonable. If the measure-ment association is already given, the association of another measurement should provide no extra information. Similarly, $q_m|z_m \\perp q_{m'} \\forall m' \\neq m$ is also reasonable. Given the associations, two measurements are assumed to be independent (unless there is some systematic bias of the sensor). The assumption of all the latent variables ($z_{m's}$) being independent of each other is not totally valid for a depth camera because the association at one pixel gives away the ray orientation (i.e. the bearing of the camera along that ray) and the association of the next pixel can only be made on a cone with its center line being the ray given by the first association. Nevertheless, for the generative model (14), it is reasonable to assume that the association of one measurement does not provide information to another association ($z_{m's}$ are independent of each other. i.e. $z_m \\perp Z_m$) and it also greatly simplifies the derivation of EM.\nTo derive EM, we begin with the data likelihood:\n$P(q_m) = \\sum_{n} P(q_m|z_m = n) P(z_m = n)$ (15)\n$= \\sum_{n \\in N_Q(q_m)} \\frac{1}{Q} N(sRp_n + t - q_m; 0, \\Sigma_m)$\nMaking use of the independence assumptions and denoting\n$Z = \\{z_m\\}_{m=1}^{M}$,\n$P(\\{q_m\\}_{m=1}^{M}|Z) = \\prod_{m=1}^{M} P(q_m|Z)$\n$= \\prod_{m=1}^{M} P(q_m|z_m)$ (16)\n$= \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)$\nwhere the first equality assumes $P(q_m|z_m) \\perp P(q_{m'}|z_{m'})$ and the second equality assumes $P(q_m|z_m, z_{m'}) = P(q_m|z_m)$. Since we have further assumed all the latent variables ($z_{m's}$) are independent of each other, it leads to\n$P(Z) = \\begin{cases}\n\\frac{1}{Q^M} & \\text{if } z_m \\in N_Q(q_m) \\forall m \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (17)\nWe can now compute the posterior probability of the latent variables:\n$P(Z|\\{q_m\\}_{m=1}^{M}) = \\frac{P(Z, \\{q_m\\}_{m=1}^{M})}{P(\\{q_m\\}_{m=1}^{M})}$\\n$= \\frac{P(\\{q_m\\}_{m=1}^{M}|Z)P(Z)}{\\sum_Z P(\\{q_m\\}_{m=1}^{M}|Z)P(Z)}$ (18)\nwhose result is\n$\\begin{cases}\n\\frac{\\frac{1}{Q^M} \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)}{\\sum_{Z} \\frac{1}{Q^M} \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)} & \\text{if } z_m \\in N_Q(q_m) \\forall m \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (19)\nIn the expectation step, we need to compute $\\sum_Z P(Z|\\{q_m\\}_{m=1}^{M}) ln P(Z, \\{q_m\\}_{m=1}^{M})$, where $P(Z|\\{q_m\\}_{m=1}^{M})$ is given a hat notation to indicate that it is computed from the last available estimates [51]:\n$\\sum_Z P(Z|\\{q_m\\}_{m=1}^{M}) ln P(Z, \\{q_m\\}_{m=1}^{M})$\n$= \\sum_Z \\frac{\\frac{1}{Q^M} \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)}{\\sum_Z \\frac{1}{Q^M} \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)}$\n$\\left\\{\\ln{\\frac{1}{Q^M}} + \\ln{\\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)}\\right\\}$ (20)\nwhere $\\sum_Z$ is over all possible combinations of $\\{z_m \\in N_Q(q_m)\\}_{m=1}^M$. To simplify (20), we examine the pattern in a simple case. Suppose we have three measurements (M = 3) and we use a, b, c to denote the three $N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)$ terms. We then use a', b', c' to denote the three $\\ln{\\frac{1}{Q}} + \\ln{N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)}$ terms. Suppose for each measurement, we have two possible associations (i.e. 2 realizations of $z_m$ and 8 realizations of Z for 3 mea-surements) which are denoted by subscript 1 and 2. The nor-malization term $\\sum_Z \\frac{1}{Q^M} \\prod_{m=1}^{M} N(sRp_{z_m} + t - q_m; 0, \\Sigma_m)$"}]}