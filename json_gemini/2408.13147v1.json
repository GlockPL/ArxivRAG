{"title": "ShapeICP: Iterative Category-level Object Pose and Shape Estimation from Depth", "authors": ["Yihao Zhang", "John J. Leonard"], "abstract": "Category-level object pose and shape estimation from a single depth image has recently drawn research attention due to its wide applications in robotics and self-driving. The task is particularly challenging because the three unknowns, object pose, object shape, and model-to-measurement correspondences, are compounded together but only a single view of depth measurements is provided. The vast majority of the prior work heavily relies on data-driven approaches to obtain solutions to at least one of the unknowns and typically two, running with the risk of failing to generalize to unseen domains. The shape representations used in the prior work also mainly focus on point cloud and signed distance field (SDF). In stark contrast to the prior work, we approach the problem using an iterative estimation method that does not require learning from any pose-annotated data. In addition, we adopt a novel mesh-based object active shape model that has not been explored by the previous literature. Our algorithm, named ShapeICP, has its foundation in the iterative closest point (ICP) algorithm but is equipped with additional features for the category-level pose and shape estimation task. The results show that even without using any pose-annotated data, ShapeICPsurpasses many data-driven approaches that rely on the pose data for training, opening up a new solution space for researchers to consider.", "sections": [{"title": "I. INTRODUCTION", "content": "We investigate the task of estimating the pose and shape of an object given a single depth image of it. The object instance is only known up to its semantic category. Its exact geometry (i.e. CAD model) is unavailable to us. Such a problem is commonly encountered in many scenarios ranging from self-driving to home robots. For example, self-driving may require estimation of the poses and shapes of other vehicles on the street without knowing the detailed CAD models of them apriori. Similarly, home robots may require estimation of the poses and shapes of table top objects in order to manipulate them. Because of the wide applications and the significance of this task, it has drawn substantial research attention in recent years.\nThe category-level object pose and shape estimation prob-lem is typically set up such that the instance segmentation and semantic classification of the object is done on the accompanied RGB image with an off-the-shelf method [1]\u2013[20]. However, even after factoring out the object detection and segmentation problem, inferring the object pose and shape from the segmented depth image remains challenging due to the three entangled unknowns, object pose, object shape and model-to-measurement correspondences. It re-quires us to know two of the three unknowns to reduce the problem to an easily solvable one. Specifically, if we know the object shape and how the measurements correspond to the points on the surface of the shape, it is reduced to the problem of aligning two point sets which the Umeyama algorithm [21] can solve. If the object pose and the corre-spondences between the measurements and a category-level shape template are known, one can transform the template to the same pose as the measurements and translate the template points onto the corresponding measurements to obtain the shape. Researchers often recognize the correspondences as an unknown and deform the template to fit the measurements [22]\u2013[25] using Chamfer loss where the closest points are the correspondences [22]\u2013[24], [26], [27]. However, with the three variables all left unknown, the problem is extremely difficult.\nMost prior methods exploit neural networks to learn a mapping from inputs such as the RGB image, the depth image, and the categorical shape prior, to one or two of the three unknowns. For example, in [1], a deformation field is learned to deform the shape prior to obtain the object shape and a correspondence matrix is predicted to draw correspondences between the model shape and the back-projected depth points for pose estimation with Umeyama [21]. Although these learning methods greatly factor away the complexities in the problem by leveraging data and have achieved excellent performance on benchmarks, they are faced with potential failures caused by any domain change and tedious data curation process for training when they are deployed to a new environment. In the object pose and shape estimation task, the ground truth pose of an object is especially painstaking to annotate since so far there has been no scalable and easily accessible approach to measure the pose of an object. On the other hand, virtually all the prior methods focus on either point cloud or signed distance field (SDF) as the shape representation. Point clouds are flexible but they do not have the notion of surface, while SDF entails unfavorable computational complexity which is often scaled cubically with the resolution.\nIn this work, we take a radically different approach. First, in contrast to prior methods, we adopt a novel mesh-based active shape model (ASM) [28] as the object shape representation. Meshes incorporate the notion of surface, being superior to point clouds. They are also inherently a geometric representation, unlike SDF or occupancy grid where the geometry is represented implicitly and some pro-cesses such as finding the zero level set (SDF) or marching cubes [29] (occupancy grid) are required to extract the geometry. The use of meshes in category-level object pose and shape estimation is also a relatively untapped terrain in the literature. It is worth being explored more. Second, we devise an iterative estimation algorithm based on the iterative closest point (ICP) algorithm [30] and many other classical estimation techniques without training any module on the pose-annotated data. The method has an option to use an image-based shape classification network for shape initial-ization to boost the estimation performance. We consider that in practice object ground-truth shape is more accessible than its ground-truth pose since object scanning and depth fusion techniques [31] are mature.\nWe evaluate our method on the NOCS REAL dataset [32] and find better or comparable performance to many learning-based methods even though our method is at the disadvantage that no pose-annotated data are used. Besides the performance comparison, the evaluation of our method also reveals what can be already accomplished without learning and why learning may be needed at some places in this estimation task. These insights are traditionally difficult to draw from the evaluation of black-box style learning-based methods."}, {"title": "II. RELATED WORK", "content": "We will review the related work in terms of which of the three unknowns, object pose, object shape, and model-to-measurement correspondences are predicted by neural networks. We will also discuss shape representations used in the literature.\n**A. Category-Level Pose and Shape Estimation**\n*Predicting shape and correspondences*. One of the most popular approaches is predicting the object shape as well as the correspondences between the shape and the measured depth points. The seminal work NOCS [32] belongs to this category. The predicted NOCS map encodes the depth profile (i.e. shape) of the observed object in its canonical pose. The NOCS map also corresponds pixel-to-pixel to the measured depth image, being essentially a combined prediction of the object shape and the correspondences. The Umeyama algorithm [21] can solve the transformation between the NOCS map and the depth image. Another work is [18] where the authors first use Mesh R-CNN [26] to predict a metric-scale mesh of the object shape from the image and then predict the NOCS map to solve for the pose. Among the methods that predict both shape and correspondences, a notable line of work [1]\u2013[6] attempts to predict the de-formation applied to a categorical shape prior to obtain the object shape and a correspondence matrix to associate the model (i.e. the deformed prior) points to the measured depth points. The Umeyama algorithm [21] is then engaged to solve the transformation. One representative work is Shape Prior [1] where a network takes in the RGB image patch, back-projected depth points, and the categorical shape prior to predict the deformation field and the correspondence matrix.\n*Predicting shape and pose*. A more straightforward ap-proach is predicting the pose and shape directly. Methods in this category often have an iterative closest point (ICP) [30] post-processing step to refine the predicted pose. One highly recognized work is CASS [7]. It incorporates a variational auto-encoder (VAE) [33] to learn a point cloud based shape embedding space. During inference, the shape latent code is predicted given the RGB-D image patch. The predicted latent code is both decoded to the shape in its canonical pose and fed to the module for pose prediction. There are various other solutions in this category of methods. For example, CenterSnap [34] and ShAPO [35] forgo the use of the region of interest (RoI) based Mask R-CNN [36] for object detection. Instead they build a YOLO-style [37] one-shot estimation pipeline. The object poses and shape codes along with the object detection are predicted in one shot on a heat map. iCaps [9] selects the rotation by comparing the depth image features to the features in a pre-computed code book and predicts the DeepSDF [38] latent vector as the shape representation. SDFEst [8] refines the predicted pose and shape by comparing the rendered depth image to the actual depth image. Other methods that directly regress shape and pose to various extent include [10]\u2013[12], [39].\n*Predicting pose*. Another set of methods bypasses the shape estimation and only estimates the pose. FS-Net [40] extracts rotation-aware features through a scale and shift-invariant 3D graph convolution network. It also decouples the rotation into two perpendicular vectors for prediction. SAR-Net [13] transforms a categorical template to the same rotation as the observed point cloud by neural networks. The transformed template is aligned with the original template by Umeyama [21] to find the rotation. The translation and size are predicted based on a mirroring-completed point cloud which is the measured point cloud concatenated with the mirrored version of the point cloud with respect to the plane of symmetry. Both the template transforming step and mirroring-completion step are performed by neural networks which have to implicitly learn the object pose prediction task since both the transformation and the plane of symmetry are pose dependent. MH6D [14] transforms the measured point cloud randomly three times, regresses poses for the three point clouds and enforces consistency among the regressed results.\n*Predicting correspondences*. Several methods use key-point or descriptor matching as the base to solve the prob-lem. [41] assumes known model-to-measurement correspon-dences provided by neural keypoint matching and solves an alignment problem between the model keypoints and the measurement keypoints. Instead of only estimating the SIM(3) pose as in Umeyama [21], the method additionally estimates the parameters for an active shape model (ASM) [28] of the object. Fortunately, the ASM is linear and can be treated similarly as the translation. The method also features a graph-theoretic approach and an outlier-robust solver to handle false correspondences. [42] is an earlier work that essentially shares the same problem formulation as [41] but with a different solver that does not consider the outlier issue. [15] computes semantic primitives using a part segmentation network. It derives a SIM(3)-invariant shape descriptor from the primitives and minimizes the discrepancy between the observed descriptor and the model descriptor to optimize the shape. Once the shape is optimized, Umeyama [21] is performed on the primitive centers and the measured point cloud to estimate the pose.\n*Optimization-based*. A few methods resort to optimiza-tion for the pose and shape estimation avoiding direct re-gression. Neural Analysis-by-Synthesis [16] leverages neural rendering to render an image from the object shape code and pose. The rendered image is compared against the actual image for the shape code and pose optimization. It starts from multiple initial states in parallel to avoid local minima. DirectShape [17] sets up the optimization losses by silhouette matching and left-right stereo photo consistency using an ASM of a signed distance field (SDF) representation for the shape. As the work is for vehicle estimation only, it engages strong regularization such as the ground plane constraint to alleviate the optimization difficulty. TransPoser [19] directly outputs a rendered depth image using a neural network given the object pose and shape code to compare with the actual depth image. As the method is applied on RGB-D image sequences, it sequentially updates the pose and shape code with a Transformer [43]. ELLIPSDF [20] employs a bi-level shape representation which returns two levels of shape, an SDF and an ellipsoid, from the latent code. It estimates an initial pose by fitting the ellipsoid to the object segmentation. The optimization of the pose and shape is performed to minimize the signed distance of the measured depth points computed from both the SDF representation and the ellipsoid representation.\nAs we have seen, previous methods commonly adopt data-driven solutions. Pose-annotated data are exploited in almost all the prior work either explicitly or implicitly. Even the optimization-based methods may actually require extra information such as an RGB-D sequence instead of a single image [19], [20] or application-specific constraints [17]. Our method works completely without pose-annotated data. The addition of an optional shape classification network merely requires shape-annotated data, which also falls into the rare category of predicting only the shape, a direction not well explored by the prior work.\n**B. Shape Representation**\nThe active shape model (ASM) [28] has been employed in the category-level object pose and shape estimation task [10], [17], [41] and also the 3D object shape reconstruction task [44], [45]. The ASM has the advantage of being a simple linear model which is more efficient to back-propagate through during optimization compared to a neural network. The object ASM is typically point-based [10], [41], [42] or SDF-based [17], [44], [45]. In this work, we explore a mesh-based ASM. Methods in the literature for category-level object pose and shape estimation primarily use SDF and point cloud as the shape representation, it is interesting to explore more in the mesh representation."}, {"title": "III. METHOD", "content": "**A. Overview**\nOur task is to estimate the SIM(3) pose and shape of an object which is only known up to its semantic category given a single depth image of it. We assume that instance segmentation of the object and its class label are provided by an off-the-shelf method such as Mask RCNN [36] as typically used in the prior work [1]\u2013[17]. In order to estimate the shape, the shape representation of the object has to be malleable to the shapes within the category. We hence adopt a mesh-based object active shape model (ASM) [28] for its flexibility and surface representability. In order to have consistent number of vertices and vertex connectivity across all the object models in the database for Principal Component Analysis (PCA) which is required to build the ASM, we deform a spherical template mesh to wrap around each object model and run PCA on the deformed templates. After obtaining the ASM offline, we estimate the pose and shape by transforming and fitting the shape model to the observed depth points. The optimization is achieved by an augmented iterative closest point (ICP) algorithm which in addition to the existing pose estimation step has a shape deformation step for shape estimation. We thus name our algorithm ShapeICP. However, ICP is a local solver and so is the base form of ShapeICP. This becomes a significant challenge due to the lack of an initial guess. To overcome it, ShapeICP is outfitted with multi-hypothesis tracking for ro-tation estimation, expectation maximization (EM) for robust correspondence handling, and shape classification for shape initialization. The multi-hypothesis estimation is paramount to the success of the algorithm. It has hypothesis scoring functions based on closest point distance, symmetry, and rendering. The full pseudo-algorithm of ShapeICP can be found in Alg. 1.\n**B. Mesh-based Object Active Shape Model**\n*Template deformation*. Mesh-based ASMs have been applied to human faces [46], [47] but not yet in the task of object pose and shape estimation. The bottleneck is that object models from a database such as ShapeNet [48] have different numbers of vertices and different vertex connec-tivity even within the same category (see Fig. 1), making it difficult to draw corresponding vertices across models for PCA. To solve this challenge, we deform a template mesh to wrap around each object model and run PCA on the vertices of the deformed templates for all the object models within a category. More specifically, we denote a mesh by $\\mathcal{M} = (V, E, F)$, where $V = \\{v_i\\}_{i=1}^V$ are the $V$ vertices in the mesh, $E = \\{e_i\\}_{i=1}^E$ is the set of $E$ tuples that each store the indices of a pair of vertices connected by an edge, $F = \\{f_i\\}_{i=1}^F$ is the set of $F$ tuples of indices of vertices on the same faces. To deform the template mesh $\\mathcal{M}_s$ to a target mesh $\\mathcal{M}_t$, we first randomly sample points on the mesh surfaces with $N$ points from the template mesh $p_n \\sim \\mathcal{M}_s$ and $M$ points from the target mesh $q_m \\sim \\mathcal{M}_t$ (here $N = M$). The deformation is done by minimizing the similar losses as in [26], [27], [49]:\n$L_c = \\frac{1}{N} \\sum_n \\min_m ||p_n - q_m||_2 + \\frac{1}{M} \\sum_m \\min_n ||p_n - q_m||_2$ (1)\nwhich is the Chamfer distance between the two sets of sampled points. To regularize the template mesh during the deformation process, a normal consistency loss is imposed on the template mesh:\n$L_n = \\frac{1}{E} \\sum_e 1 - \\cos(\\hat{n}_{f_{e+}}, \\hat{n}_{f_{e-}})$ (2)\nwhere $\\hat{n}_{f_{e+}}$ and $\\hat{n}_{f_{e-}}$ are the normals of the two faces that share edge $e$, and cos is the cosine distance. This loss regularizes the surface normals to be smooth. To avoid overly long outlier edges in the template mesh, an edge length loss is added:\n$L_e = \\frac{1}{E} \\sum_e ||v_{e+} - v_{e-}||^2$ (3)\nwhere $v_{e+}$ and $v_{e-}$ are the vertices connected by the edge $e$. A Laplacian smoothing loss is also enforced on the template mesh to encourage the vertices to move along with their neighbors and potentially avoid mesh self-intersection:\n$L_l = \\frac{1}{V} \\sum_i ||v_i - \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j \\in \\mathcal{N}(i)} v_j||^2$ (4)\nwhere $\\mathcal{N}(i) = \\{j\\} \\forall (i, j) \\in E$ is the set of neighbor-ing vertices of vertex $v_i$. The overall objective function is $\\min_v \\lambda_c L_c + \\lambda_n L_n + \\lambda_e L_e + \\lambda_l L_l$, where $\\lambda$'s are the weights for the loss terms. This optimization is carried out by stochastic gradient descent (SGD) [50] starting from a spherical template mesh. We remark that category-specific templates can be used to better fit the topology of each class but we choose a spherical template for its generality. An example of the deformation process is visualized in Fig. 2.\n*PCA*. Now that we have acquired a set of deformed templates that take on the model shapes across a category and have consistent number of vertices and connectivity, we can build a feature vector for each model by concatenating the vertices of its deformed template, i.e. $[v_1...v_V]$, and then run PCA on all the feature vectors to obtain our category-level mesh-based ASM. A vertex in the final ASM with $K$ principal components is expressed as\n$v_i = \\bar{b}_{o,i} + \\sum_{k=1}^K c_k b_{k,i}$ (5)\nwhere $\\bar{b}_{o,i}$ is the vertex in the mean shape corresponding to $v_i$, $b_{k,i}$ is the corresponding vertex in basis $k$, and $\\{c_k\\}_{k=1}^K$ are the weights for the bases. These weights parameterize the shape. Fig. 3 visualizes the vertices of the bases and the final shape in an example ASM. The edges and faces of the mesh-based ASM are inherited from the template mesh.\n**C. Alternating Pose and Shape Optimization**\n*Formulation*. To introduce our ShapeICP formulation, we need sample points on the ASM. Each sampled point can be expressed as\n$p_n = \\sum_{i \\in F(n)} \\mu_{n,i} v_i \\quad s.t. \\sum_{i \\in F(n)} \\mu_{n,i} = 1$ (6)\nwhere $F(n)$ is a function that maps the sample index $n$ to a randomly selected face, and $\\mu_{n,i}$ are randomly generated interpolation weights for this sample. These interpolation weights are fixed during the optimization. Combining with (5), $p_n = \\sum_{i \\in F(n)} \\mu_{n,i} (\\bar{b}_{o,i} + \\sum_{k=1}^K c_k b_{k,i})$. We now intro-duce the ShapeICP formulation:\n$\\mathop{\\text{Res}}_{O(3)} \\text{min} \\mathop{\\text{min}}_{c=[c_1...c_K] \\in \\mathbb{R}^K} \\frac{1}{M} \\sum_{m=1}^M ||sRp_n + t - q_m||_2$ (7)\nwhere $\\{q_m\\}_{m=1}^M$ (overloaded notation) are now the object depth points segmented from the depth image and back-projected to 3D using the camera intrinsics, $R, t$, and $s$ are the object rotation, translation, and scale to be estimated, $c$ which enters the objective function through $p_n$ is the ASM shape code also to be estimated. For each measurement $q_m$, (7) encourages the transformed model points to be close to the measurements, which is similar to the ICP objective function [30] but with the addition of the shape parameter $c$. We also remark that formulation (7) equivalently works for a point-based ASM where $p_n$ is simply a point in the ASM point cloud. However, mesh-based ASM has the notion of surface and thus is a better geometric representation. Fur-thermore, a mesh model enables the possibility of rendering to be used in Section III-D.\n*Core algorithm*. We make two observations of (7). First, if the shape code $c$ is given, the problem shares the same objective function with ICP. Second, if the SIM(3) pose is given, the problem mimics the mesh deformation problem introduced in Section III-B except that (7) is only one-sided Chamfer distance and the optimization variable is the shape code instead of the vertices. These two observations lead to an intuitive alternating minimization algorithm to solve (7):\n*   Pose step*: Associate each measurement $q_m$ to the clos-est transformed model point $sRp_n + t$ (i.e. solving $\\min_n$ in (7)) using the last estimates $\\hat{c}, \\hat{R}, \\hat{t}$, and $\\hat{s}$. Solve for the incremental $R, t$, and $s$ with Umeyama [21] to further align the model points with the measurements while keeping $c$ fixed.\n*   Shape step*: Re-associate each $q_m$ to the closest model point using the updated $R, t$, and $s$. Solve for $c$ as if it is a mesh deformation problem while keeping $R, t$, and $s$ fixed.\nAfter the pose step, the incremental $R, t$, and $s$ are accumu-lated onto the estimates $R, t$, and $\\hat{s}$.\n*Shape step*. In the shape step, we include the regulariza-tion losses (2) \u2013 (4). The final objective function $L_{sc}$ (sc: shape code) in the shape step is $\\min_c L_{sc} = \\min_c L_{ps} + \\lambda_n L_n + \\lambda_e L_e + \\lambda_l L_l$, where $L_{ps}$ (ps: pose shape) refers to the objective function in (7) and $\\lambda$'s are the weights. These weights can be reused from Section III-B if we center and normalize the observed point cloud, and shift and normalize the transformed model points by the same amount (because the ShapeNet [48] objects in Section III-B are all centered and normalized).\nSince this is an iterative coordinate-descent style algo-rithm, at each iteration the shape step does not have to solve to the optimum. Only a few steps of gradient descent is performed on $c$ starting from the last estimate $\\hat{c}$. We also stop running the shape step in the last few iterations for better empirical performance.\n**D. Coping with Local Minima**\nCategory-level object pose and shape estimation given only a single-view depth image naturally comes with large ambiguity (Fig. 4) since many configurations of the variables may fit equally well to the observed partial object point cloud. In other words, the optimization landscape exhibits many local minima, which hurt the performance of a local solver such as ShapeICP. In this section, we develop several strategies to cope with the local minima.\n*Expectation maximization (EM)*. The correspondences between the measured depth points and the model points can be treated as latent variables. Instead of hard one-to-one associations, EM allows softer associations [51], reducing the chance of being stuck at a local minimum. We integrate EM into our ShapeICP formulation following a similar approach as in [52]. Each measurement $m$ is now accompanied by a latent variable $z_m$ that models the association of this measurement. Specifically,\n$P(z_m = n) = \\{\\begin{array}{ll} \\frac{1}{Q} & \\text{if } n \\in \\mathcal{N}_Q(q_m) \\\\ 0 & \\text{otherwise} \\end{array}$ (8)\nwhere $P(z_m = n)$ is the probability of $q_m$ associated to $p_n$ and $\\mathcal{N}_Q(q_m)$ finds the $Q$ closest transformed model points to $q_m$ (i.e. the top $Q$ solutions to $arg \\min_n ||sRp_n + t - q_m||_2$). We further model the conditional data likelihood as\n$P(q_m | z_m = n) = N(sRp_n + t - q_m; 0, \\Sigma_m)$ (9)\nwhere $N(\\cdot; 0, \\Sigma_m)$ is a zero-mean Gaussian distribution with covariance $\\Sigma_m$ (i.e. conditioned on $q_m$ associated to $p_n$, the residual follows a Gaussian distribution). Assuming indepen-dence $q_m | z_m \\perp z_{m'}$ , $q_m | z_m \\perp q_{m'} \\quad \\forall m' \\neq m$ and $z_m \\perp z_{m'} | \\forall m' $ , and isotropic covariance $\\Sigma_m = diag(\\sigma_m^2)$, the objective function $L_{em}$ derived from EM is (explanations about the probabilistic models and the independence assumptions, and the derivation of EM can be found in the appendix):\n$\\mathop{\\text{min}}_{R \\in SO(3), t \\in \\mathbb{R}^3, s \\in \\mathbb{R}^{++}} \\mathop{\\text{min}}_{c=[c_1...c_K] \\in \\mathbb{R}^K} \\sum_{m=1}^M \\sum_{n \\in \\mathcal{N}_Q(q_m)} w_{mn}||sRp_n + t - q_m||_2$ (10)\nwhere\n$w_{mn} = \\frac{N(sRp_n + t - q_m; 0, \\sigma_m)}{ \\sum_{n' \\in \\mathcal{N}_Q(q_m)} N(sRp_{n'} + t - q_m; 0, \\sigma_m)}$ (11)\nwhere the hatted quantities $\\hat{s}, \\hat{R}, \\hat{t}$, and $\\hat{p}_n$ ($\\hat{p}_n$ as a result of $\\hat{c}$) are the estimates from the last step and do not participate in the current optimization. EM yields an intuitive formula-tion, which rather than one hard correspondence, considers likely correspondences weighted by their probabilities computed from the last-step estimates. This increases the field of view of the model-measurement association step and can help move out of local minima. Both the pose and shape steps can use (10).\n*Multi-hypothesis estimation*. Initialization is critical in avoiding local minima. The translation can be initialized as the centroid of the observed point cloud $\\bar{q} = \\sum_m q_m$. We find it better to begin with a small initial scale such as the average distance to the center $\\frac{1}{M} \\sum_m ||q_m - \\bar{q}||$. Without any neural network, the shape code is initialized to be the mean code for the category $\\bar{c} = \\frac{1}{U} \\sum_u c_u$ where $U$ is the total number of database object models for the category. However, rotation initial guess is not easily available. We thus track multiple initial rotation hypotheses in parallel and drop unpromising hypotheses quickly to save computation. Specifically, we start with the base SO(3) grid from [53] as also used by [8]. The grid has 2304 discrete rotations uni-formly covering SO(3). During the course of optimization, we drop hypotheses according to the following three score functions:\n*   Objective function*. The mean residual in (7) $S_r = \\frac{1}{M} \\sum_m \\min_n ||sRp_n + t - q_m||_2$ plus the standard deviation of it $S_\\sigma = std(\\{r_m\\}_{m=1}^M)$ is used. We find the addition of the standard deviation improves the results as we hope to have relatively uniform residuals over all the depth points.\n*   Symmetry check*. Man-made objects often have mirror symmetry (e.g. laptop and bowl) and rotation symmetry (e.g. bottle and can). If the estimated pose and shape are correct, mirroring and rotating the observed point cloud with respect to the estimated plane and axis of symmetry should still result in low residual. Let $T = [\\hat{s} \\hat{R} \\hat{t}; 0 \\; 0 \\; 0 \\; 1]$ be the SIM(3) pose estimate, and $p_{h,n} = [p_n; 1]$ and $q_{h,m} = [q_m; 1]$ be the homogeneous coordinates of $p_n$ and $q_m$. The symmetry score $S_{\\psi,r}$ is\n$\\frac{1}{M} \\sum_m \\min_{\\psi} \\frac{1}{M} \\sum_m \\min_n ||p_{h,n} - T \\psi T^{-1} q_{h,m}||^2$ (12)\nwhere $T_\\psi = [R_\\psi \\; 0; 0 \\; 0 \\; 0 \\; 1]$ is a symmetry operation such as rotation around the axis of symmetry or reflection with respect to the plane of symmetry (in which case $det(R_\\psi) = -1$). (12) essentially transforms the observed point cloud to the object canonical pose using the current estimate $T$, applies $R_\\psi$, and transforms back so that the order of magnitude of $S_{\\psi,r}$ is similar to $S_r$. The standard deviation score is $S_{\\psi,\\sigma} = std(\\{r_{\\psi,m}\\}_{m=1}^M)$ and the total symmetry score is $S_\\psi = \\frac{1}{4} \\sum_\\psi S_{\\psi,r} + S_{\\psi,\\sigma}$ for a total of 4 different symmetry operations (multiple rotation symmetry operations of different angles are possible).\n*   Depth rendering*. Taking advantage of the mesh-based representation, we render the current estimates to a depth image and compare with the observed depth image of the object (after applying the segmentation mask). The depth rendering score is\n$S_{dr} = \\frac{1}{|\\Omega(D)|} \\sum_{d \\in \\Omega(D)} [\\mathcal{R}(d, \\mathcal{M}(c), K_c) - D_d]^2$ (13)\nwhere $D$ is the masked observed depth image, $d \\in \\Omega(D)$ indexes pixels in the image space, $\\mathcal{R}$ is the renderer, $\\mathcal{M}(c)$ is the mesh model built from $c$, and $K_c$ includes the known camera intrinsics and other camera parameters (such as image size). We use the renderer from [49] and assign the same background value to the rendered image and the masked observed depth image. The depth rendering helps to identify over-sized model (which may still give low $S_r$ if a portion of it fits the measurements well) [54]. To avoid computation overflow, we start depth rendering after the number of hypotheses is low.\nThe total score to be minimized is $S_{tot} = S_r + S_\\sigma + \\lambda_\\psi S_\\psi + \\lambda_{dr} S_{dr}$ where $\\lambda$'s are the weights. When picking top hypotheses, we greedily select the next hypothesis at least 20 degrees apart from the last chosen hypothesis to alleviate duplicated or very close hypotheses.\n*Shape classification*. To better initialize the shape code $c$, we build a shape classification network whose input is a color-coded normal vector image of the segmented object and the output is the index of the closest database model $u^*$. We choose ResNet-50 [55] for the network. The normal vector image is computed from the observed depth image and converted from $x, y, z \\in [0, 1]$ (assuming unit vector) to RGB values. Only the image patch that contains the object (i.e. 2D bounding box) is resized to a constant size and fed to the network. We find the ground-truth closest model by computing Chamfer distance (1) between the ground-truth shape and all the models in the database (ShapeNet [48]). The final initial guess is $\\bar{c}_{u^*}$ for the shape code. Note that if a model in the database is not matched to any ground-truth shape, it does not appear as a class in the network output. Therefore, the number of classes is smaller than or equal to the number of ground-truth shapes.\n**E. Summary**\nThe full estimation algorithm is given in Alg. 1."}, {"title": "IV. EXPERIMENTS", "content": "**A. Experiment Setup**\n*Implementation*. We implement our ShapeICP method with PyTorch [56", "49": ".", "10": [57], "48": "is used to compute the ASM for the six categories in the NOCS REAL data [32"}, {"10": ".", "32": "is the seminal work in this research area. Neural Analysis-by-Synthesis [16"}]}