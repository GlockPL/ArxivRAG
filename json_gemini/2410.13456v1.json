{"title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland", "authors": ["Luca Rolshoven", "Vishvaksenan Rasiah", "Srinanda Br\u00fcgger Bose", "Matthias St\u00fcrmer", "Joel Niklaus"], "abstract": "Legal research is a time-consuming task that most lawyers face on a daily basis. A large part of legal research entails looking up relevant caselaw and bringing it in relation to the case at hand. Lawyers heavily rely on summaries (also called headnotes) to find the right cases quickly. However, not all decisions are annotated with headnotes and writing them is time-consuming. Automated headnote creation has the potential to make hundreds of thousands of decisions more accessible for legal research in Switzerland alone. To kickstart this, we introduce the Swiss Leading Decision Summarization (SLDS) dataset, a novel cross-lingual resource featuring 18K court rulings from the Swiss Federal Supreme Court (SFSC), in German, French, and Italian, along with German headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary models. Our analysis highlights that while proprietary models perform well in zero-shot and one-shot settings, fine-tuned smaller models still provide a strong competitive edge. We publicly release the dataset to facilitate further research in multilingual legal summarization and the development of assistive technologies for legal professionals.", "sections": [{"title": "1 Introduction", "content": "A significant part of legal work involves research, where lawyers must find similar cases and navigate numerous judicial decisions, especially when interpreting laws with room for debate. Due to the time-intensive nature of this task, they usually rely on judgment summaries. However, creating these summaries is labor-intensive and requires expertise from judges and clerks, who are already burdened with heavy caseloads (Bieri, 2015) and time-pressure (Ludewig and Lallave, 2013).\nTo alleviate this increasing need for efficient ways to navigate vast amounts of legal documents, legal document summarization has become a critical area of interest in NLP (Jain et al., 2021). Over the years, researchers have made significant strides in both extractive and abstractive summarization of legal texts. Earlier works focused on extracting key sentences to create concise summaries (Grover et al., 2004; Hachey and Grover, 2006; Kim et al., 2013; Bhattacharya et al., 2021), while recent advancements have turned towards abstractive methods, which generate condensed paraphrases of the most important information in a document (Shukla et al., 2022; Niklaus and Giofr\u00e9, 2022; Moro et al., 2023; Jain et al., 2024; Niklaus et al., 2024).\nDatasets with legal documents and their corresponding summaries have been instrumental in enabling these advancements, yet they primarily focus on monolingual corpora. Therefore, existing datasets do not adequately address the unique challenges posed by multilingual jurisdictions, such as Switzerland, where legal decisions are written in multiple languages and need to be summarized consistently. This gap is significant because most legal NLP tools and models are trained on English-centric datasets, which often do not generalize well to cross-lingual environments.\nTo fill this gap, we introduce the Swiss Leading Decision Summarization (SLDS) dataset, a large collection of Swiss legal cases in German, French, and Italian, created for legal summarization. In Switzerland, lawyers often rely on leading decisions Leading Decisions (LDs) from the Swiss Federal Supreme Court (SFSC), which come with headnotes - short summaries that highlight key legal points and important laws. The SLDS dataset focuses on these LDs, which explain how Swiss laws are interpreted and applied. It is designed to help develop tools that assist legal professionals working in multilingual environments. It also enables training language models to handle the complexities of legal text in different languages, making it a valuable resource for research in cross-lingual legal summarization. The dataset is available on Hugging Face under a CC BY 4.0 license."}, {"title": "2 Related Work", "content": "Recently, research on legal text summarization has increasingly shifted towards abstractive summarization, with a particular emphasis on creating datasets that facilitate fine-tuning of pre-trained language models for this task. In the following section, we will discuss the most relevant contributions."}, {"title": "2.1 Monolingual Datasets", "content": "English Two English legal text summarization datasets with a focus on U.S. legislative particularly stand out. The first one is BillSum (Kornilova and Eidelman, 2019) comprising 22K bills from the U.S. Congress and the State of California. The authors evaluated different extractive algorithms on their dataset. They trained two binary classifiers, a Random Forest (Breiman, 2001) based on hand-crafted features and TF-IDF (Luhn, 1957; Sparck Jones, 1972) scores, and a BERT (Kenton and Toutanova, 2019) encoder model, whose purpose is to predict whether a sentence should be included in the summary. To create the summary, they use the Maximal Marginal Relevance (Goldstein et al., 2000) algorithm. The authors evaluate both classifiers, an ensemble of both classifiers, along with two other unsupervised baselines on their dataset using ROUGE scores (Lin, 2004a). They find that the ensemble performs best, with most of the performance coming from BERT. Moreover, they applied transfer learning in summarization from federal to state laws.\nThe second dataset is Multi-LexSum (Shen et al., 2022), which targets long civil rights lawsuits, with an average length of over 75K words. The corpus consists of 9K documents and allows for in-depth study at different summary lengths: short (25 words), medium (130 words), and long (650 words), a unique feature of the Multi-LexSum dataset. Models based on BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020a) were evaluated on this dataset. There are also larger datasets. A significant contribution in the recent past comes from Bauer et al. (2023) who focused on extracting key passages from 430K U.S. court opinions to create concise summaries. Interestingly, they find that their reinforcement-learning-based MemSum model out-performs two transformer-based models that were also evaluated. However, due to licensing issues, the authors were not able to release the dataset their models were trained on.\nPortuguese RulingBR (de Vargas Feij\u00f3 and Moreira, 2018) is another monolingual dataset for legal document summarization. It consists of over 10K rulings from the Brazilian Federal Surpreme Court, along with the corresponding summaries in Portuguese. The summaries are divided into four parts that represent different types of information, with one of them being the reference summary that can be used to train a summarization model. The"}, {"title": "2.2 Multilingual Datasets", "content": "There exists also multilingual corpora. Below, we briefly describe two of these datasets.\nEUR-Lex-Sum Derived from EUR-Lex, the official online repository of European Union law, EUR-Lex-Sum (Aumiller et al., 2022) encompasses the 24 official languages of the European Union and features more than 1,500 document/summary pairs per language. A subset of 375 legal acts is cross-lingually aligned and available in all 24 languages. Although legal acts tend to be dense and technical, they generally adhere to a more structured format compared to court decisions, which are characterized by complex reasoning, factual narratives, and often involve multiple legal perspectives.\nA significant distinction between our work and EUR-Lex-Sum lies in our focus on court cases within a single jurisdiction, as opposed to their focus on legal acts from a supranational entity. While summarizing international laws is valuable, understanding their interaction with national laws and their application to specific cases is just as important, if not more so. Our dataset prioritizes understanding how laws are applied in a national context, as opposed to simply detailing what the laws are. Additionally, SLDS provides over 13 times more samples for French-to-German cross-lingual summarization and more than double the number of Italian-to-German samples. This enables a more comprehensive analysis of cross-lingual summarization for these specific languages, which hold particular relevance in the Swiss context.\nMILDSum The dataset, introduced by Datta et al. (2023), was designed to address language barriers in the Indian legal system, focusing on cross-lingual translation of English legal documents into Hindi. MILDSum comprises 3,000 Indian legal case judgments in English, along with their corresponding summaries in both English and Hindi. A key finding by the authors is that the Summarize-then-Translate approach outperforms direct cross-lingual summarization. The average lengths of the case judgments and summaries are 4,696 tokens for the judgments, and 724 and 695 tokens for the English and Hindi summaries, respectively.\nWhile MILDSum and our dataset share a focus on summarizing court decisions and cross-linguality, there are notable differences. Our dataset includes three instead of two languages, and none of them is English. This likely makes the summarization task in SLDS more challenging, given that English dominates most pre-training datasets. Another distinction lies in the structure of the summaries: our dataset contains headnotes, which include citations, keywords, and summarized decisions, whereas MILDSum does not mention any similar structure for their summaries."}, {"title": "3 Data", "content": "We present SLDS, a novel dataset for cross-lingual summarization in the legal domain. The dataset consists of 18,175 leading decisions in German, French or Italian published by the SFSC, along with their corresponding summaries in German written by clerks and judges. It serves as an important resource to study cross-lingual summarization, which is relatively underexplored in the legal domain. In contrast to datasets like EUR-Lex-Sum that focus on legislation, our dataset contains judicial decisions, making it especially relevant for developing tools aimed at supporting legal practitioners and researchers working with court rulings."}, {"title": "3.1 Data Collection", "content": "The original data was published on the official website of the SFSC. These leading decisions are then scraped by the platform Entscheidsuche, from where we downloaded them."}, {"title": "3.2 Fields", "content": "The dataset contains the following fields:\n\u2022 id: identifier for a specific decision\n\u2022 header: a short header containing metadata about the case such as the case citation, the division that handled the case, date of judgment, parties involved, and the type of complaint\n\u2022 text: the decision\n\u2022 summary: headnote (Regeste in German) that summarizes the decision. Consist of three components: i) important citations to laws and prior cases, ii) thematic keywords derived from a legal thesaurus, and iii) a free-form summary of key considerations.\n\u2022 language: the language of the decision, i.e. the column text\n\u2022 chamber: an identifier for one out of seven chambers of the SFSC"}, {"title": "3.3 Descriptive Statistics", "content": "Dataset Splits The dataset is divided into train, validation, and test subsets. The training set contains 16.5K samples, while the validation and test sets have 500 and 1000 samples, respectively. We partitioned the data by year: training includes decisions from 1954 to 2015, validation from 2016 to 2017, and testing from 2018 to 2022. This was done to prevent data leakage and ensures consistency with current summarization styles.\nLanguages The decisions, which are written by clerks and judges in the language of the proceedings, are either in German, French or Italian. There are roughly 12K German decisions, almost 5K in French, and 835 in Italian. The decisions are stored under the column text. The headnotes are in the column summary, all written in German.\nText Length The distributions of the number of words and tokens, when using a pre-trained BERT tokenizer, for the decisions and their headnotes are shown in Figure 2. To save space, we removed the tail of the distribution by truncating all texts to a maximum token and word length of 8000, resulting in a peak at the end of the x-axis.\nDecision Year The judgments in this dataset were made between 1954 and 2022, with only little data available for the years 2001 to 2006. The number of decisions per year ranges from 2 to 420, with a median of 278 decisions per year. The exact distribution can be seen in Appendix C."}, {"title": "3.4 Licensing", "content": "We release the data under the CC-BY-4.0 license, which complies with the SFSC licensing."}, {"title": "3.5 Ethical Considerations", "content": "Due to the sensitive nature of court cases and their corresponding rulings, the SFSC anonymizes personal or sensitive information according to their guidelines before publishing them online."}, {"title": "4 Experimental Setup", "content": "To establish baselines, we fine-tuned the two variants small and base of mT5 (Xue, 2020), a multilingual variant of the T5 model (Raffel et al., 2020), on the training subset of our dataset. We then evaluated the models using BERTScore (Zhang et al., 2020b), BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004b). Since each individual metric has inherent weaknesses (Zhang et al., 2020b), it is necessary to employ multiple metrics for a more comprehensive assessment. Besides this quantitative evaluation, we suggest evaluating predictions with trained lawyers as future work.\nWe also compared the two fine-tuned models against GPT-3.5-Turbo, GPT-4, Claude 2, and Claude Instant in a zero-shot and one-shot setting. The original prompt and a translated version can be found in Appendix F. We used a German prompt because the headnotes are always in German in our dataset, and we found that models tend to generate in the prompt's language, regardless of the input's language. The decisions in our dataset consist of 3081 tokens on average, while the headnotes average 168 tokens. The maximum sequence length for the decisions is 38K, and the 90th percentile is 5646 tokens. For our main experiment, we truncated the inputs to 4096 tokens and the output to 256 tokens, preserving the full output in 82% of cases. Additionally, we performed an extended version of the experiment with input sequence lengths ranging from 512 to 16K tokens. In this extended version, we also evaluated the large variant of mT5."}, {"title": "5 Results", "content": "Overall Results The proprietary models perform well on SLDS in the zero-shot and one-shot setting, at least in terms of BLEU and METEOR (see Table 1). According to the ROUGE scores, the fine-tuned mT5 models outperform the pre-trained proprietary models, while BERT-Score does not discriminate clearly. We assume that summarization is a large portion of internal instruction tuning datasets used for optimizing these models.\nIn general, the generated summaries are of high quality. They demonstrate a good stylistic imitation of legal language and consistent logical coherence. To see examples of three generated summaries by our fine-tuned mT5Base model, refer to Appendix G. Summaries generated by the proprietary models can be found in Appendix I. GPT-3.5-Turbo (zero-shot) offered a narrative-style summary, while others adhered to the traditional format of headnotes in Switzerland. Notably, GPT-3.5-Turbo made a factual error by negating a crucial element, and Claude 2 referenced an outdated legal provision. We conduct a detailed error analysis in Section 6."}, {"title": "5.1 Main Experiment", "content": "The proprietary models perform well on SLDS in the zero-shot and one-shot setting, at least in terms of BLEU and METEOR (see Table 1). According to the ROUGE scores, the fine-tuned mT5 models outperform the pre-trained proprietary models, while BERT-Score does not discriminate clearly. We assume that summarization is a large portion of internal instruction tuning datasets used for optimizing these models.\nIn general, the generated summaries are of high quality. They demonstrate a good stylistic imitation of legal language and consistent logical coherence. To see examples of three generated summaries by our fine-tuned mT5Base model, refer to Appendix G. Summaries generated by the proprietary models can be found in Appendix I. GPT-3.5-Turbo (zero-shot) offered a narrative-style summary, while others adhered to the traditional format of headnotes in Switzerland. Notably, GPT-3.5-Turbo made a factual error by negating a crucial element, and Claude 2 referenced an outdated legal provision. We conduct a detailed error analysis in Section 6."}, {"title": "5.2 Extended Experiment", "content": "Table 2 presents the extended experiment results, revealing two key trends: First, longer inputs generally improve scores across models, as truncated inputs miss information, leading to lower evaluation scores. Claude Instant was an exception, showing worse performance with longer inputs in both zero-shot and one-shot settings. Second, larger mT5 models outperform smaller ones, though the difference between base and large is subtle. For proprietary models, one-shot settings yielded slightly better scores than zero-shot, as anticipated."}, {"title": "6 Error Analysis", "content": "Automatic metrics have limits in evaluating text generation tasks (Schluter, 2017; Zhang et al., 2020b). To address this and gain clearer insights, a legal expert on this project conducted an error analysis. This analysis, based on a reference sample (Figure 4) and generated summaries from proprietary models like GPT-4 (Figure 5), and others (see Appendix I), looked at a case where the SFSC ruled against a wife's appeal on her husband's reduced maintenance, as she couldn't prove his unemployment was meant to harm her.\nAll models demonstrated an authentic stylistic representation. GPT-3.5-Turbo provides a more narrative-style summary, while the other models stick more closely to a classic Swiss headnote format. Claude 2 cited an outdated legal provision, Article 137 of the ZGB, inactive since 2011, even though the case decision was from 2017 and no such reference was in the input. Despite this, the article cited remains relevant to the context. Meanwhile, GPT-4's reference to BGE 137 III 118 and GPT-3.5's mention of BGE 143 III 233 S. 235 were accurate and existed in the input."}, {"title": "7 Conclusions and Future Work", "content": "In this article, we introduce SLDS, a cross-lingual dataset for summarizing judicial decisions to advance cross-lingual summarization and support legal practitioners in Switzerland. Our experiments demonstrate that fine-tuned mT5 models can compete with proprietary models like GPT-4 in summarizing judicial decisions, achieving similar or better performance while offering significant computational efficiency. Our results highlight the effectiveness of domain-specific fine-tuning for multilingual legal tasks.\nFuture research may address two key areas. First, finetuning larger models or exploring advanced summarization techniques, such as multi-step summarization. Second, expanding the error analysis with additional input from more legal professionals or collecting quantitative preference ratings at scale could help evaluate models in more detail by uncovering deeper issues."}, {"title": "Limitations", "content": "We tried as carefully as possible to design the experiment in a way that the evaluation is meaningful, which is why we included BERTScore to measure semantic similarity and multiple other metrics that measure lexicographic similarity. We also performed a qualitative error analysis to further improve the evaluation. Due to resource constraints, we were not able to involve additional legal experts into the evaluation process. We leave this up to future work, along with the incorporation of a LLM-based evaluation such as LLM-as-a-Judge (Zheng et al., 2023).\nAlthough the mT5 model family might be considered somewhat outdated given today's wide range of LLMs, we use this model to show that fine-tuning is still competitive compared to prompting larger proprietary models. Subsequent work should investigate fine-tuning larger LMs that are currently considered state-of-the-art."}, {"title": "A Potential Risks", "content": "We believe the release of SLDS poses minimal risk. On the contrary, we expect our dataset to foster further research and encourage the development of assistive technologies that can make the work of lawyers, judges, and clerks more efficient. However, it is crucial not to rely on these summaries blindly. We recommend using such systems as tools to enhance efficiency, rather than as substitutes for human oversight. Users must ensure that the generated summaries accurately reflect the decisions and do not introduce any misleading content, since lawyers will rely on these summaries to find relevant cases faster."}, {"title": "B Use of AI Assistants", "content": "We used ChatGPT to improve the content of this article. It was used to rephrase certain passages, as well as condense them to make the text less redundant and easier to understand. We carefully checked that the generated paraphrases corresponded to our own ideas and that no errors were introduced during this process."}, {"title": "C Number of Leading Decisions by Year", "content": "In Figure 6, we provide a distribution of LDs over the years."}, {"title": "D Resources Used", "content": "For the experiments, we used two NVIDIA A100 80GB GPUs. We used 10 GPU days for our experiments."}, {"title": "E Hyperparameters", "content": "We used gradient accumulation in our fine-tuning experiments to achieve an effective batch size of 16. The mT5 models were trained for 10 epochs and evaluations were performed every 1000 steps."}, {"title": "F Prompts", "content": "We used Prompt 2 for the few-shot experiments. To avoid language barriers, we included Prompt 1, the English translation of the original German prompt, in the main body of this article."}, {"title": "G Generated Summaries", "content": "Figure 7 shows three different samples, along with generated summaries, ground truths, and corresponding evaluations. The headnotes were generated with our fine-tuned mT5Base model, and they are ordered according to high, average, and low evaluation scores."}, {"title": "H Few-shot results per language", "content": "We provide the detailed results for the cross-lingual evaluations in our experiment in Table 3. The results are grouped by few-shot setting, either zero-shot or one-shot, and language."}, {"title": "I Qualitative Analysis", "content": "The reference sample that was used in our qualitative analysis can be seen in Figure 4. The generated summaries along with their evaluation scores are shown in Figures 5 to 10."}]}