{"title": "DisenSemi: Semi-supervised Graph Classification via Disentangled Representation Learning", "authors": ["Yifan Wang", "Xiao Luo", "Chong Chen", "Xian-Sheng Hua", "Ming Zhang", "Wei Ju"], "abstract": "Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in real-world scenarios, labeled graph data can be limited or scarce. To address this issue, we focus on the problem of semi-supervised graph classification, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. In this paper, we propose a novel framework named DisenSemi, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factor-wise graph representations for both supervised and unsupervised models. Then we train two models via supervised objective and mutual information (MI)-based constraints respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experimental results on a range of publicly accessible datasets reveal the effectiveness of our DisenSemi.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPHS are among the most versatile data structures in many real-world multimedia applications across various domains, e.g., biology networks [1], molecule structures [2], citation networks [3], social networks [4] and recommender system [5], etc. For graph-structured data, one fundamental task is graph classification, which endeavors to capture the property of the entire graph and has become an important research hot spot in numerous multimedia tasks, including chemical compound prediction [2], text categorization [6] and social network analysis [4].\nTo tackle the graph classification problem, many machine learning-based methods are proposed. Graph kernel methods leverage structural patterns such as shortest paths [7], subtrees [8], subgraphs [9] and graphlets [10] to measure the similarity among graphs and pass the similarity matrix to a kernel-based supervised algorithm such as the Support Vector Machine (SVM) to perform classification. Instead of hand-crafted feature extraction in graph kernel methods, more recently, graph neural networks (GNNs) can extract graph structural features in a supervised end-to-end manner. Despite the better performance, GNNs always require a large amount of labeled data, which is highly expensive and time-consuming in the real world [11]. For example, as demonstrated in Fig. 1, the label of a chemical compound is typically produced with a costly Density Functional Theory (DFT) calculation or several complicated experiments.\nIn practice, there always exist massive unlabeled graph samples. Although labels of these graphs are unavailable, the inherent knowledge of the graph may help to enhance GNNs' encoding more expressive and discriminative. Semi-supervised classification methods combine both supervised and unsupervised models where the unsupervised model learns from unlabeled data and can serve as a regularizer. Indeed, there are a handful of semi-supervised works proposed for graph classification, which can be divided into several categories, i.e., pseudo-labeling [12]\u2013[14], consistency learning [2], [15]\u2013[20]. Especially for consistency learning, methods like InfoGraph [16] and GraphSpa [20] jointly learn supervised and unsupervised models from labeled and unlabeled data. Meanwhile, a popular fashion (i.e., GraphCL [15] and JOAO [17]) has been developed recently which first pre-trains the unsupervised model with unlabeled data and then fine-tunes the model with labeled data.\nHowever, these existing semi-supervised graph classification methods still remain unsatisfactory due to the following limitations. (1) Neglect the entanglement of latent factors. The formation of graphs in the real world typically stems from the highly complex interaction of different latent factors. For example, a molecular graph may consist of various groups of atoms and bonds representing different functional units [21]. Existing works often neglect the entanglement of latent factors and the extracted features are holistic, which harms interpretability and leads to sub-optimal performance for the graph classification task. (2) Mismatch semantics between supervised and unsupervised tasks. Supervised and unsupervised tasks on labeled and unlabeled graph data may capture different information or occupy separate semantic spaces, corresponding to distinct graph factors. Simply combining two tasks may lead to a \"negative transfer\" [22]. Therefore, we argue that a more efficient way is to disentangle the graph representation learned from unsupervised tasks into distinct latent factors and transfer the corresponding factor-wise information well aligned to semantically enhance the supervised task for semi-supervised graph classification.\nRecently, disentangled representation learning has gained much attention, aiming at learning factorized representations that are capable of uncovering information about the salient (or explanatory) properties of the data. Moreover, disentangled representations have been demonstrated to be more generalizable and resilient to complex variants, i.e., the learned factors are supposed to be independent and less sensitive to the noises from other factors in the limited observed training data [23]. For graph-structured data, where multiple heterogeneous relations are always mixed and collapsed into one graph, disentangled representation learning tails to decompose underlying relations and is mainly focused on the supervised link prediction tasks (i.e., recommender systems [5], [24]), supervised graph classification tasks [25], [26] and unsupervised graph generation/identification process [27], [28]. However, the semi-supervised method of bridging the supervised and unsupervised tasks remains largely unexplored.\nToward this end, in this paper, we propose DisenSemi, a novel Disentangled graph representation learning framework for Semi-supervised graph classification, which is capable of enhancing the supervised prediction task with the unsupervised representation learning tasks semantically. In contrast to existing works that directly utilize an entangled representation learned from a large amount of unlabeled data for prediction, our proposed approach emphasizes regularizing the rationale to explicitly exploit transferable factor information between supervised and unsupervised tasks. In particular, we design a disentangled graph encoder by characterizing the global-level topological semantics, during which the whole graph is first decomposed into factor graphs. Then the factor-wise interpretable graph representation is produced via the multi-channel message-passing layer, where each channel is tailored to propagate features under one factor graph and a separate readout operation in each channel summarizes the specific aspect of the graph. For labeled data, we train the model using a supervised objective function while for unlabeled data, the model is trained with MI-based constraints for the input factorized graph and its corresponding features to ensure the disentanglement. Next we conduct MI maximization between the supervised and unsupervised models under each latent factor instead of in the whole feature space for disentangled consistency regularization. Compared with the existing works, this novel factor-wise MI estimation strategy can ensure the regularized factor is best pertinent to the aspect bridging supervised and unsupervised models for the current semi-supervised graph classification task. More importantly, we demonstrate that our framework can be formalized as a problem of maximizing the log-likelihood solved by Expectation Maximization (EM).\nTo summarize, in this paper we make the following contributions:\n\u2022 Conceptual: We propose a novel disentangled representation learning framework for semi-supervised graph classification, which explicitly models the rationale factor that fits well between supervised and unsupervised learning tasks. And the proposed mechanism can be potentially generalized to other semi-supervised learning tasks.\n\u2022 Methodological: We propose a graph disentangled encoder that produces factor-wise graph representations under decomposed factor graphs. Moreover, different MI-based constraints and consistency regularization are proposed to capture the characteristic differences and the connections between supervised and unsupervised learning models.\n\u2022 Experimental: We conduct extensive experiments on a variety of public datasets to evaluate the DisenSemi. Experimental results demonstrate the efficiency and outstanding interpretability of our proposed framework for semi-supervised graph classification tasks."}, {"title": "II. RELATED WORK", "content": "A. Semi-supervised Graph Classification\nSemi-supervised learning has received growing attention in recent years. It is associated with a learning paradigm that uses both labeled and unlabeled data and encompasses two prominent techniques. The first line is the consistency regularization methods, which are based on the manifold or smoothness assumption and posit that realistic perturbations of the data points should not change the output of the model. The most common structure is Teacher-Student, consisting of two models called student and teacher and applying a consistency constraint between the two predictions of student and teacher [29]\u2013[31]. The second line is pseudo-labeling methods, which predict the label distribution of unlabeled samples and select confident samples to provide further guidance during the training process [32], [33].\nFor graph-structured data, there are also attempts using consistency regularization [14]\u2013[16], [18], pseudo-labeling [12], [13], [34], [35] or other approaches [20], [36]\u2013[39] for semi-supervised learning. Typically, InfoGraph [16] learns supervised and unsupervised model respectively and estimates the MI between two models. GraphCL [15] and GLA [14] use contrastive learning to get graph representations for semi-supervised graph classification. DualGraph [13] and UGNN [35] jointly learn a prediction and a retrieval module via posterior regularization during the pseudo-labeling process. GraphSpa [20] actively selects informative graphs for subsequent model training using a hybrid selection strategy. However, these approaches fail to disentangle the diverse underlying factors behind the graph data, making it challenging to identify suitable alignments between supervised and unsupervised models.\nB. Disentangled Representation Learning\nDisentangled representation learning aims to acquire factorized representations capable of discerning and separating the latent explanatory factors concealed within the observed data [23]. By achieving such a disentangled representation, the model gains a deeper understanding of the intricate dependencies within the data, facilitating more insightful and meaningful analysis or manipulation of the underlying factors. Existing efforts about disentangled representation learning are mainly on computer vision [40], [41], natural language processing [3], [42] and recommendation [24], [43], [44].\nRecently, there has been a notable surge of interest in applying disentangled representation learning techniques to graph-structured data [5], [25]\u2013[27]. DisenGCN [25] learns disentangled node representation through a neighborhood routing mechanism to divide the neighborhood of the node into several mutually exclusive parts. DisenHAN [5] learns disentangled representation in Heterogeneous Information Network (HIN) to iteratively identify the major aspect of the relation between node pairs and propagate corresponding information semantically. FactorGCN [26] takes the whole graph as input and produces block-wise interpretable graph-level features for classification. DGCL [27] proposes a self-supervised graph disentangled representation framework via contrastive learning method. UMGRL [28] provides a self-supervised disentangled representation learning method for the multiplex graph to capture common and private graph information. Our work focuses on learning disentangled representation for semi-supervised graph classification. And our primary objective is to design a framework where a supervised model can learn from an unsupervised model through the latent semantic space that aligns well with the current graph classification task."}, {"title": "III. PROBLEM DEFINITION AND PRELIMINARIES", "content": "A. Problem Definition\nDefinition 1 (Graph). A graph is formally defined as G = (V, E, X), where V and E \u2208 V \u00d7 V are the node set and edge set of the graph respectively. We denote the node feature matrix as X \u2208 R^{|V|\u00d7d'}, where each row x_i \u2208 R^{d'} represents the feature vector of node i and d' is the dimension of node feature. A graph is typically labeled if it is associated with a class label y \u2208 {0,1}^C, where C is the number of classes. A graph is unlabeled if its class label is unknown.\nDefinition 2 (Semi-supervised Graph Classification). Given a set of graphs G = {G^L,G^U}, where G^L = {G_1,...,G_{|G^L|}\\} and G^U = {G_{|G^L|+1},\u2026\u2026\u2026,G_{|G^L|+|G^U|}\\} represent labeled and unlabeled graphs respectively. Let Y^L = {Y_1,...,Y_{|G^L|}\\} represents the label corresponding to G^L. Semi-supervised graph classification seeks to learn a prediction function that can assign class labels to unlabeled graph data in G^U based on the available class labels in G^L.\nB. Preliminaries\nPreliminary 1 (MI Estimation). MI is a fundamental measurement of the dependence between two random variables, which has been applied to a wide range of tasks [41], [45]. However, estimation MI is intractable especially in high dimensional continuous settings. Since earlier non-parametric binning methods perform poor, a neural estimator MINE [46] formates MI between x and y as Kullback-Leibler (KL)-divergence between the joint distribution P_{xy} and the product of the marginals p_x & P_y,\nI_{KL}(x,y) := D_{KL}(P_{xy}||P_xP_y),\nand derive a lower bound of MI with dual representations of KL-divergence. Following previous works [16], [45], we can further rely on non-KL divergence (i.e., Jensen-Shannon divergences) as an alternative for the MI estimator.\nI_{JS}(x, y) = E_{p_{xy}} [-T(x, y)] - E_{p_xp_y} [log(1 \u2013 T(x,y))], \nwhere T(,) is a critic (or score) function approximated by a neural network. In this way, the loss can be seen as a standard binary cross-entropy (BCE) loss between the samples from the joint (a.k.a. positive examples) and the product of marginals (a.k.a. negative examples).\nPreliminary 2 (MI and Contrastive Learning). More recently, InfoNCE [47] propose a Noise Contrastive Estimation (NCE [48])-based MI lower bound, defined as:\nI_{NCE}(x, y) := E[log \\frac{expT(x_i,Y_i)}{\\sum_{j=1}^N expT(x_i,y_j)}],\nwhere the expectation is over N samples {(x_i, Y_i)}_{i=1}^N drawn from the joint distribution P_{xy}. And contrastive learning encourages learning the model where representations between positive samples are pulled closer (e.g., graph representations corresponding to the same factor bridging supervised and unsupervised tasks) and representations among negative samples are pushed apart."}, {"title": "IV. THE PROPOSED MODEL", "content": "A. Overview\nFor semi-supervised graph learning, the objective is to smooth the label information over graph data with regularization. And a straightforward way for the objective is to combine the purely supervised loss and the unsupervised objective function. Formally, previous semi-supervised learning frameworks try to minimize the following objective function:\nL_{total} = \\sum_{i=1}^{|G_L|} L_S(G_i, Y_i) + \\lambda \\sum_{j=1}^{|G|} L_U (G_j),\nwhere L_S denotes the supervised loss that measures the discrepancy between the supervised model prediction and the real label of the graph data, L_U denotes the unsupervised loss acting as a regularization term and \\lambda is the relative weight between two losses.\nHowever, we argue that supervised and unsupervised tasks have different optimization targets, which correspond to different semantic spaces of the graph data. Thus, we propose DisenSemi for semi-supervised graph classification. The basic idea is to explicitly infer the latent factors underlying a large amount of unlabeled graph data and transfer the well-aligned knowledge to enhance the supervised task in a factor-wise manner. As shown in Fig. 2, our framework is composed of a supervised model and an unsupervised model, where graph-level representations of both models are learned via the GNN-based encoder. Given the graph data, the GNN-based encoder first factorizes the input graph into several factor graphs, then gets graph representation by propagating and aggregating features on the corresponding factor graph. For the supervised model, we merge all the extracted factor graph representations to predict graph labels. For the unsupervised model, we introduce several MI-based constraints among factorized graphs and their corresponding extracted representations. Finally, a disentangled consistency regularization is conducted to explicitly identify the rationale between the two models and transfer the learned knowledge semantically via factor-wise MI maximization. The process is equivalent to a Variational EM algorithm maximizing the log-likelihood.\nB. GNN-based Encoder\nGNNs have recently emerged as effective approaches for learning graph-structured data. Prevailing GNNs for learning graph representations are based on the neural message passing among nodes to update their features and utilize a readout function to get the graph-level representation. However, we argue that these methods are holistic and neglect the hidden factors stemming from different aspects. In fact, multiple heterogeneous relations between nodes are mixed together and collapsed into one single edge, and these relations correspond to multiple aspects of discriminant features of the graph. Thus, given input graph G_i, we factorize it into K factor graphs {G_{ik}}_{k=1}^K to separately encode their features. Specifically, for each edge e_m = {x_u, x_v} in edge set E(G_i) = {e_1,..., e_M}, where M is the total edge of graph G_i, we project the node feature x \u2208 R^{d'} with a transformation matrix W \u2208 R^{d \u00d7 d'} and generate the factor coefficients as follows:\nE_{uvk}(G_i) = \\frac{1}{1+ exp-\\Psi_k(Wx_u,Wx_v)},\nwhere \\Psi_k is the function that takes the features of node u and v as input to compute the attention score of edge e_m under factor k, and the function can be implemented via a multi-layer perceptron (MLP). Following that, the attention score is normalized to get \\hat{E}_{uvk}(G_i), representing the coefficient of edge e_m contained in the factor graph G_{ik}.\nWe represent each factor graph by its own edge coefficient \\hat{E}_{uvk} (G_i). By stacking L message passing layers, the node embedding of each node v in G_{ik} is updated by recursively aggregating and combining its neighborhood features. Formally, the embedding of node v in l-th layer is calculated as follows:\nh_v^{(l)}(G_i) = A^{(l)}({h_u^{(l-1)}(G_i), u \u2208 \\mathcal{N}(v)}, \\hat{E}_{uvk}(G_i)),\nh_v^{(0)}(G_i) = W_kx_v,\nh^{(l)}(G_i) = C^{(l)} (h_v^{(l-1)}(G_i), h_v^{(l)}(G_i)),\nwhere \\mathcal{N}(v) denotes the neighborhood of node v, A^{(l)} represents the aggregation operation corresponding to the coefficient \\hat{E}_{uvk} (G_i) as edge weight for factor graph G_{ik}, and C^{(l)} represents the combination operations at layer l. In this paper, we adopt GraphConv [49] as our message passing layer due to its powerful expression ability and initialize the node embedding of each factor graph via a factor-specific transformation matrix W_k \u2208 R^{d \u00d7 d'}.\nFinally, for each factor graph G_{ik}, the graph-level representations can be attained by aggregating all node embeddings of the last L layer with a readout function. Formally,\nz_k(G_i) = READOUT({h_v^{(L)}(G_i)}_{v\u2208V(G_i)}),\nwhere V(G_i) is the node set of G_i, READOUT can be a simple permutation invariant function such as the mean function. By considering all the hidden factors of graph data, we can get a factor-wise graph representation, namely, Z_{G_i} = [z_1(G_i),..., z_k (G_i)].\nC. Supervised Loss\nWith the GNN-based encoder in the supervised task, a factor-wise representation Z_{G_i}^S = [z_1^S(G_i),...,z_k^S(G_i)] is extracted from labeled data, where different factor graphs will result in different features of the graph. Towards this end, we introduce K learnable factor prototypes C = {c_k}_{k=1}^K to obtain the attention weight \\alpha_k(G_i) over latent factors for the input G_i:\n\\alpha_k(G_i) = Softmax(\\phi(z_k^S(G_i), c_k)),\nwhere \\phi is the similarity function, i.e., cosine similarity. After that, we weighted sum all latent factors to get the graph representation and fed it into a label prediction model to get the corresponding output, as shown in Fig. 3(b).\nZ'_{G_i}^S = \\sum_{k=1}^K \\alpha_k(G_i) z_k^S(G_i),\n\\hat{y}_i = Softmax(MLP(Z'_{G_i}^S)),\nwhere we adopt a two-layer MLP to map the graph representation extracted from different factor graphs to label predictions. For the supervised loss, we adopt cross-entropy, which can be defined as follows:\nL_S = - \\sum_{c=1}^C Y_{ic} log \\hat{Y}_{ic},\nwhere y_i \u2208 R^C is the label with the one-hot encoding of the input graph data G_i, and C corresponds to the number of graph categories in the dataset.\nD. MI-based Representation Disentanglement\nFor unsupervised learning, we can also get a factor-wise graph representation Z_{G_i}^U = [z_1^U (G_i),..., z_k^U(G_i)] via the GNN-based encoder. Since some of the decomposed factor graphs may contain a similar structure without any other constraint, we propose several MI-based constraints to effectively disentangle the graph representation, as shown in Fig. 3(a).\n1) Intra-factor MI: As each factor graph corresponds to an interpretable relation between nodes, we seek to obtain the discriminant representation that captures the global content of the graph. Inspired by the deep InfoMax [45], we maximize the MI between the decomposed factor graph and the corresponding graph-level representation. For factor k, we define the intra-factor MI on global-local pairs, maximizing the estimated MI over all the nodes in factor graph G_{ik}, which is shown as:\nL_{intra} = - \\frac{1}{|V|} \\sum_{v \u2208V(G_i)} I(z_k^U(G_i), h_v^{(L)}(G_i)),\nwhere I(z_k^U(G_i), h_v^{(L)}(G_i)) is the MI estimator modeled by discriminator D_k and parameterized by a neural network. We use the binary cross-entropy (BCE) loss between samples from the joint (a.k.a., positive example) and product of marginals (a.k.a., negative examples) as the objective:\n-I(z_k^U(G_i), h_v^{(L)}(G_i)) =logo(D_k(z_k^U(G_i), h_v^{(L)}(G_i)))\n+E_{p \u2297 p}[log(1 \u2013 \u03c3(D_k(z_k^U(G_i), \\hat{h}_v^{(L)}(\\hat{G}_i))))],\nwhere \\hat{h}_v^{(L)}(\\hat{G}_i) is generated by the negative input graph \\hat{G}_i sampled from a distribution p = p, which is identical to the empirical probability distribution of the input space. In practice, we could sample negative samples by an explicit (stochastic) corruption function or directly sample negative graph instances in a batch.\n2) Inter-factor MI: For graph representations Z_{G_i}^U extracted from different factor graphs, the MI between every two of them reaches its minimum value zero when p(z_k^U(G_i), z_{k'}^U(G_i)) = p(z_k^U(G_i))p(z_{k'}^U(G_i)), which means that z_k^U(G_i) and z_{k'}^U(G_i) are independent to each other. Thus, MI minimization among them encourages the factor-wise graph representation to learn different aspects of information for the graph. Recently, several MI upper bounds [50], [51] have been introduced for MI minimization. However, the estimation MI upper bound among these K graph representations requires K(K \u2013 1) times estimation, resulting in far more costs especially when K is large. To alleviate this problem, as orthogonality can be seen as a special case of linear independence among representations, we loosen the constraint of minimization MI to orthogonality and the method has also been demonstrated to be effective by many previous studies [52]:\nL_{inter} = || Z_{G_i}^U (Z_{G_i}^U)^T - I ||,\nZ_{G_i}^U = z_1^U(G_i)||...||z_K^U(G_i),\nwhere || is the L\u2081 norm, I is the identity matrix, || is the concatenation operation.\nRemark: In our model, we have also tested other MI minimization methods, such as Contrastive Log-ratio Upper Bound (CLUB) [51], which estimates MI by the difference of log-ratio of conditional distribution between positive and negative sample pairs. And these methods do not bring much change to the performance.\nWe maximize the MI between the factor graph and its corresponding graph representation to characterize each hidden factor while minimizing the MI among different factor graph representations for one input graph to enforce representation disentanglement. Formally, unsupervised loss of DisenSemi can be formulated as:\nL_U = \\sum_{k=1}^K L_{intra} + L_{inter}.\nE. Disentangled Consistency Regularization\nFor transferring the learned disentangled graph representations from the unsupervised encoder to the supervised encoder, we encourage representations extracted from two encoders with high MI when they correspond to the same factor of one graph data bridging supervised and unsupervised tasks. Specifically, we maximize NCE-based MI lower bound [47], which aims at making similar representations closer and different instances far away from each other. We treat each representation of a graph data instance as a distinct class and distinguish it from representations extracted from other graph data instances. Formally, given graph dataset G = {G_i}_{i=|G^L|+|G^U|}, we assign each graph G_i with a unique surrogate label s_i = i. Different from the ground-truth label y_i of the labeled graph data, s_i can be seen as the ID of the corresponding graph in the dataset G. And in this way, the MI can be defined as:\nI_{NCE}(Z_{G_i}^S, Z_{G_i}^U) := log p(s_i|G_i)\n= log E_{p(k|G_i)} [P(s_i|G_i, k)],\nwhere p(s_i| G_i) corresponding to the identify discrimination over dataset, p(k |G_i) is the probability distribution of the k-th latent factors reflected in G_i, namely, can be \\alpha_k (G_i). And p(s_i |G_i,k) corresponding to the identify discrimination subtask under k-th latent factor defined as:\np(s_i |G_i, k) = \\frac{exp \\phi (z_k^S(G_i), z_k^U(G_i))}{\\sum_{j=1}^{|G|} exp \\phi (z_k^S(G_i), z_k^U(G_j))},\nwhere the subtask is over all graphs in the dataset. \\phi is the similarity function, adopting cosine similarity in our paper.\nIn effect, it is non-trivial to maximize the log-likelihood over the whole graph dataset due to the latent factors, where we need to calculate the posterior distribution defined with Bayes' theorem as follows:\np(k |G_i, s_i) = \\frac{p(k |G_i)p(s_i |G_i,k)}{\\sum_{k'=1}^K p(k |G_i)p(s_i|G_i,k)},\nCompared with prior distribution p(k |G_i) that inferred only given the graph G_i, p(k |G_i,s_i) is the probability of the k-th latent factor effectively aligning both the supervised and unsupervised models when applied to the same graph data. However, it is prohibitive to compute posterior probability due to the term p(s_i |G_i, k) in Eq. 16, which needs all the instances in the dataset for computing the denominator. Therefore, we alternatively maximize the evidence lower bound (ELBO) of the log-likelihood, which is defined as:\nlogp(s_i|G_i) \u2265 L_{ELBO} := E_{q(k|G_i,s_i)} [logp(s_i|G_i, k)]\n-D_{KL}(q(k | G_i, s_i), p(k|G_i)),\nwhere q(k | G_i, s_i) is a variational distribution to approximate the posterior probability p(k |G_i, s_i). Here, we formalize the variational distribution by:\nq(k | G_i, s_i) = \\frac{p(k |G_i)p(s_i |G_i, k)}{\\sum_{k=1}^K p(k |G_i)p(s_i|G_i, k)},\nwhere p(s_i |G_i,k) can be defined with NT-Xent loss [53] on a minibatch \\mathcal{B}_{G_i} of graph data:\np(s_i |G_i,k) = \\frac{exp \\phi (z_k^S(G_i), z_k^U(G_i))}{\\sum_{j\u2208\\mathcal{B}_{G_i}, j\u2260i} exp \\phi (z_k^S(G_i), z_k^U(G_j))},\nNotice that the process is a variation of the Variational EM algorithm, where we infer q(k|G_i, s_i) at the E-step and optimize the ELBO at the M-step.\nF. Optimization and Complexity Analysis\n1) Optimization: We combine the supervised classification loss L_S, unsupervised loss L_U as well as consistency regularization loss L_{ELBO} together. And the overall objective for our semi-supervised graph classification is defined as:\nL_{total} = \\sum_{i=1}^{|G_L|} L_S +\\lambda \\sum_{j=1}^{|G|} L_U - \u03b3\\sum_{j=1}^{|G|}L_{ELBO},\nwhere \\lambda is the relative weight between losses, \u03b3 is a tunable weight for consistency regularization loss.\n2) Complexity Analysis: To ensure scalability with large-scale datasets, we leverage mini-batch with size B = B_L+B_U to compute gradients. The computational consumption is mainly composed of three parts: (i) the unsupervised module; (ii) the supervised module; (iii) the disentangled consistency regularization. Given the graph with an average number of nodes |V| and edges |E|, the number of GNN layer and factor graphs is L and K, and the representation dimension is d. For (i) and (ii), the time complexity of the GNN-based encoder O(|E|Ld). For (i), the additional computational complexity of intra-factor and inter-factor MI constraints is O(|V|d) and O((K \u2212 1)d/2) respectively. For (ii), the additional computational complexity of the label prediction model is O(d + (d/K)\u00b2 + Cd/K) with C classes in the task. For (iii), we perform disentangled consistency regularization within minibatch, which takes O(Bd) for each graph. To sum up, we have the overall complexity of DisenSemi, O((2|E|L + |V| + (K + 1)/2 + d/K\u00b2 + C/K + B)d), which scales linearly in terms of the number of nodes and edges in each graph instance. Moreover, DisenSemi requires K factor graphs, prototypes and coreresponding GNN-based encoder in both supervised and unsupervised module, the space complexity is O((|V| + |E|)Ld + d + K|E|)."}, {"title": "V. EXPERIMENT", "content": "We introduce the experimental settings and conduct extensive experiments on ten real-world benchmark datasets to validate the effectiveness of our proposed DisenSemi. We focus on answering the following research questions:\n\u2022 RQ1: How does our proposed model DisenSemi perform compared with other state-of-the-art models for semi-supervised graph classification?\n\u2022 RQ2: Is it necessary to keep key components of DisenSemi (i.e. GNN-based encoder", "RQ3": "How do different hyper-parameters in DisenSemi impact the performance?\n\u2022 RQ4: Can DisenSemi ensure the regularized factor pertinent to the aspect bridging supervised and unsupervised tasks semantically?\nA. Experimental Settings\n1) Datasets and Baselines: We apply our model to ten public accessible graph classification benchmark datasets\u00b9 following the previous works [14", "16": "which includes four molecule datasets: MUTAG", "dataset": "PROTEINS and five social network datasets: IMDB-BINARY (IMDB-B)", "15": [16], "approaches": "GK [10", "7": "WL [8", "9": "DGK [54", "55": "and Graph2Vec [56", "methods": "MVGRL [57"}, {"15": "JOAO [17", "27": "RGCL [58", "16": "GLA [14", "18": "GraphSpa [20", "Protocol": "We evaluate the models with 10-fold cross-validation. Following the recent works [14"}, {"16": "we randomly shuffle and split each dataset into 10 parts. Each fold corresponds to one part of data as the test set", "Details": "We implement our DisenSemi model in Pytorch. For all datasets, the embedding size of DisenSemi and other baselines (except GK, SP, WL, MLG) is fixed to 128. Since the ground-truth number of the latent factors is unknown, we search the number of factor graphs K amongst {2^0, 2^1, 2^2, 2^3, 2^4}, corresponding embedding di-mensions are {128,64, 32, 16,8} per factor graph. We set three message passing layers for our method and the hyperparameters \\lambda = 0.001,\u03b3 = 0.001. We optimize DisenSemi with Adam optimizer by setting the learning rate to 0.005 and the number of epochs is set to 200 for all the methods. For all the traditional graph classification approaches, we compute similarities between graphs and feed them into a downstream Support Vector Machines (SVM) classifier to evaluate the classification accuracy. For MVGRL, GraphCL, JOVO, DGCL and RGCL, we pre-train the model with unlabeled graphs first and fine-tune the model with the labeled graphs. For InfoGraph, GLA, TGNN, GraphSpa and our proposed DisenSemi, we integrate the pre-training and fine-tuning phases together. The source code of DisenSemi is available"}]}