{"title": "DisenSemi: Semi-supervised Graph Classification via Disentangled Representation Learning", "authors": ["Yifan Wang", "Xiao Luo", "Chong Chen", "Xian-Sheng Hua", "Ming Zhang", "Wei Ju"], "abstract": "Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in real-world scenarios, labeled graph data can be limited or scarce. To address this issue, we focus on the problem of semi-supervised graph classification, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. In this paper, we propose a novel framework named DisenSemi, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factor-wise graph representations for both supervised and unsupervised models. Then we train two models via supervised objective and mutual information (MI)-based constraints respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experimental results on a range of publicly accessible datasets reveal the effectiveness of our DisenSemi.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPHS are among the most versatile data structures in many real-world multimedia applications across various domains, e.g., biology networks [1], molecule structures [2], citation networks [3], social networks [4] and recommender system [5], etc. For graph-structured data, one fundamental task is graph classification, which endeavors to capture the property of the entire graph and has become an important research hot spot in numerous multimedia tasks, including chemical compound prediction [2], text categorization [6] and social network analysis [4].\nTo tackle the graph classification problem, many machine learning-based methods are proposed. Graph kernel methods leverage structural patterns such as shortest paths [7], subtrees [8], subgraphs [9] and graphlets [10] to measure the similarity among graphs and pass the similarity matrix to a kernel-based supervised algorithm such as the Support Vector Machine (SVM) to perform classification. Instead of hand-crafted feature extraction in graph kernel methods, more recently, graph neural networks (GNNs) can extract graph structural features in a supervised end-to-end manner. Despite the better performance, GNNs always require a large amount of labeled data, which is highly expensive and time-consuming in the real world [11]. For example, as demonstrated in Fig. 1, the label of a chemical compound is typically produced with a costly Density Functional Theory (DFT) calculation or several complicated experiments.\nIn practice, there always exist massive unlabeled graph samples. Although labels of these graphs are unavailable, the inherent knowledge of the graph may help to enhance GNNs' encoding more expressive and discriminative. Semi-supervised classification methods combine both supervised and unsupervised models where the unsupervised model learns from unlabeled data and can serve as a regularizer. Indeed, there are a handful of semi-supervised works proposed for graph classification, which can be divided into several categories, i.e., pseudo-labeling [12]\u2013[14], consistency learning [2], [15]\u2013[20]. Especially for consistency learning, methods like InfoGraph [16] and GraphSpa [20] jointly learn supervised and unsupervised models from labeled and unlabeled data. Meanwhile, a popular fashion (i.e., GraphCL [15] and JOAO [17]) has been developed recently which first pre-trains the unsupervised model with unlabeled data and then fine-tunes the model with labeled data.\nHowever, these existing semi-supervised graph classification methods still remain unsatisfactory due to the following limitations. (1) Neglect the entanglement of latent factors. The formation of graphs in the real world typically stems from the highly complex interaction of different latent factors. For example, a molecular graph may consist of various groups of atoms and bonds representing different functional units [21]. Existing works often neglect the entanglement of latent factors and the extracted features are holistic, which harms interpretability and leads to sub-optimal performance for the graph classification task. (2) Mismatch semantics between supervised and unsupervised tasks. Supervised and unsupervised tasks on labeled and unlabeled graph data may capture different information or occupy separate semantic spaces, corresponding to distinct graph factors. Simply combining two tasks may lead to a \"negative transfer\" [22]. Therefore, we argue that a more efficient way is to disentangle the graph representation learned from unsupervised tasks into distinct latent factors and transfer the corresponding factor-wise information well aligned to semantically enhance the supervised task for semi-supervised graph classification.\nRecently, disentangled representation learning has gained much attention, aiming at learning factorized representations that are capable of uncovering information about the salient (or explanatory) properties of the data. Moreover, disentangled representations have been demonstrated to be more generalizable and resilient to complex variants, i.e., the learned factors are supposed to be independent and less sensitive to the noises from other factors in the limited observed training data [23]. For graph-structured data, where multiple heterogeneous relations are always mixed and collapsed into one graph, disentangled representation learning tails to decompose underlying relations and is mainly focused on the supervised link prediction tasks (i.e., recommender systems [5], [24]), supervised graph classification tasks [25], [26] and unsupervised graph generation/identification process [27], [28]. However, the semi-supervised method of bridging the supervised and unsupervised tasks remains largely unexplored.\nToward this end, in this paper, we propose DisenSemi, a novel Disentangled graph representation learning framework for Semi-supervised graph classification, which is capable of enhancing the supervised prediction task with the unsupervised representation learning tasks semantically. In contrast to existing works that directly utilize an entangled representation learned from a large amount of unlabeled data for prediction, our proposed approach emphasizes regularizing the rationale to explicitly exploit transferable factor information between supervised and unsupervised tasks. In particular, we design a disentangled graph encoder by characterizing the global-level topological semantics, during which the whole graph is first decomposed into factor graphs. Then the factor-wise interpretable graph representation is produced via the multi-channel message-passing layer, where each channel is tailored to propagate features under one factor graph and a separate readout operation in each channel summarizes the specific aspect of the graph. For labeled data, we train the model using a supervised objective function while for unlabeled data, the model is trained with MI-based constraints for the input factorized graph and its corresponding features to ensure the disentanglement. Next we conduct MI maximization between the supervised and unsupervised models under each latent factor instead of in the whole feature space for disentangled consistency regularization. Compared with the existing works, this novel factor-wise MI estimation strategy can ensure the regularized factor is best pertinent to the aspect bridging supervised and unsupervised models for the current semi-supervised graph classification task. More importantly, we demonstrate that our framework can be formalized as a problem of maximizing the log-likelihood solved by Expectation Maximization (EM).\nTo summarize, in this paper we make the following contributions:\n\u2022 Conceptual: We propose a novel disentangled representation learning framework for semi-supervised graph classification, which explicitly models the rationale factor that fits well between supervised and unsupervised learning tasks. And the proposed mechanism can be potentially generalized to other semi-supervised learning tasks.\n\u2022 Methodological: We propose a graph disentangled encoder that produces factor-wise graph representations under decomposed factor graphs. Moreover, different MI-based constraints and consistency regularization are proposed to capture the characteristic differences and the connections between supervised and unsupervised learning models.\n\u2022 Experimental: We conduct extensive experiments on a variety of public datasets to evaluate the DisenSemi. Experimental results demonstrate the efficiency and outstanding interpretability of our proposed framework for semi-supervised graph classification tasks."}, {"title": "II. RELATED WORK", "content": "Semi-supervised learning has received growing attention in recent years. It is associated with a learning paradigm that uses both labeled and unlabeled data and encompasses two prominent techniques. The first line is the consistency regularization methods, which are based on the manifold or smoothness assumption and posit that realistic perturbations of the data points should not change the output of the model. The most common structure is Teacher-Student, consisting of two models called student and teacher and applying a consistency constraint between the two predictions of student and teacher [29]\u2013[31]. The second line is pseudo-labeling methods, which predict the label distribution of unlabeled samples and select confident samples to provide further guidance during the training process [32], [33].\nFor graph-structured data, there are also attempts using consistency regularization [14]\u2013[16], [18], pseudo-labeling [12], [13], [34], [35] or other approaches [20], [36]\u2013[39] for semi-supervised learning. Typically, InfoGraph [16] learns supervised and unsupervised model respectively and estimates the MI between two models. GraphCL [15] and GLA [14] use contrastive learning to get graph representations for semi-supervised graph classification. DualGraph [13] and\nA. Semi-supervised Graph Classification"}, {"title": "III. PROBLEM DEFINITION AND PRELIMINARIES", "content": "Definition 1 (Graph). A graph is formally defined as G = (V, E, X), where V and E \u2208 V \u00d7 V are the node set and edge set of the graph respectively. We denote the node feature matrix as X \u2208 R^{|V|\u00d7d'}, where each row x_i \u2208 \\mathbb{R}^{d'} represents the feature vector of node i and d' is the dimension of node feature. A graph is typically labeled if it is associated with a class label y \u2208 {0,1}^C, where C is the number of classes. A graph is unlabeled if its class label is unknown.\nDefinition 2 (Semi-supervised Graph Classification). Given a set of graphs G = {G^L,G^U}, where G^L = {G_1,...,G_{|G^L|}\\} and G^U = {G_{|G^L|+1},\u2026\u2026\u2026,G_{|G^L|+|G^U|}\\} represent labeled and unlabeled graphs respectively. Let Y^L = {Y_1,...,Y_{|G^L|}\\} represents the label corresponding to G^L. Semi-supervised graph classification seeks to learn a prediction function that can assign class labels to unlabeled graph data in G^U based on the available class labels in G^L.\nPreliminary 1 (MI Estimation). MI is a fundamental measurement of the dependence between two random variables, which has been applied to a wide range of tasks [41], [45]. However, estimation MI is intractable especially in high dimensional continuous settings. Since earlier non-parametric binning methods perform poor, a neural estimator MINE [46] formates MI between x and y as Kullback-Leibler (KL)-divergence between the joint distribution P_{xy} and the product of the marginals p_x \\& P_y,\nI_{KL}(x,y) := D_{KL}(P_{xy}||P_xP_y),                                                        (1)\nand derive a lower bound of MI with dual representations of KL-divergence. Following previous works [16], [45], we can further rely on non-KL divergence (i.e., Jensen-Shannon divergences) as an alternative for the MI estimator.\nI_{JS}(x, y) = E_{p_{xy}} [-T(x, y)] - E_{p_{x}p_{y}} [log(1 \u2013 T(x,y))],(2)\nwhere T(,) is a critic (or score) function approximated by a neural network. In this way, the loss can be seen as a standard binary cross-entropy (BCE) loss between the samples from the joint (a.k.a. positive examples) and the product of marginals (a.k.a. negative examples).\nPreliminary 2 (MI and Contrastive Learning). More recently, InfoNCE [47] propose a Noise Contrastive Estimation (NCE [48])-based MI lower bound, defined as:\nI_{NCE}(x, y) := E[log \\frac{expT(x_i,Y_i)}{\\sum_{j=1}^N expT(x_i,Y_j)} ],                                                                 (3)\nwhere the expectation is over N samples {(x_i, Y_i)}_{i=1}^N drawn from the joint distribution P_{xy}. And contrastive learning encourages learning the model where representations between positive samples are pulled closer (e.g., graph representations corresponding to the same factor bridging supervised and unsupervised tasks) and representations among negative samples are pushed apart.\nFor semi-supervised graph learning, the objective is to smooth the label information over graph data with regularization. And a straightforward way for the objective is to combine the purely supervised loss and the unsupervised objective function. Formally, previous semi-supervised learning frameworks try to minimize the following objective function:\n\\mathcal{L}_{total} = \\sum_{i=1}^{G^L} L_S(G_i, Y_i) + \\lambda \\sum_{j=1}^{G} L_U (G_j),                                              (4)\nwhere L_S denotes the supervised loss that measures the discrepancy between the supervised model prediction and the real label of the graph data, L_U denotes the unsupervised loss\nA. Problem Definition"}, {"title": "IV. THE PROPOSED MODEL", "content": "feature x_u \u2208 \\mathbb{R}^{d'} with a transformation matrix W \u2208 \\mathbb{R}^{d \\times d'} and generate the factor coefficients as follows:\nE_{uvk}(G_i) = \\frac{1}{1+ exp-\\Psi_k(Wx_u,Wx_v)},                                                   (5)\nwhere \\Psi_k is the function that takes the features of node u and v as input to compute the attention score of edge e_m under factor k, and the function can be implemented via a multi-layer perceptron (MLP). Following that, the attention score is normalized to get E_{uvk}(G_i), representing the coefficient of edge e_m contained in the factor graph G_{ik}.\nWe represent each factor graph by its own edge coefficient E_{uvk}(G_i). By stacking L message passing layers, the node embedding of each node v in G_{ik} is updated by recursively aggregating and combining its neighborhood features. Formally, the embedding of node v in l-th layer is calculated as follows:\nh_v^{(l)}(G_i) = \\mathcal{A} (\\{h_u^{(l-1)}(G_i), u \u2208 \\mathcal{N}(v)\\}, E_{uvk}(G_i)),\nh_v^{(0)}(G_i)=W_kx_v, \\\nh_v^{(l)}(G_i) = \\mathcal{C} (h_v^{(l-1)}(G_i), h_v^{(l)}(G_i)),\n(6)\nwhere \\mathcal{N}(v) denotes the neighborhood of node v, \\mathcal{A} represents the aggregation operation corresponding to the coefficient E_{uvk}(G_i) as edge weight for factor graph G_{ik}, and \\mathcal{C} represents the combination operations at layer l. In this paper, we adopt GraphConv [49] as our message passing layer due to its powerful expression ability and initialize the node embedding of each factor graph via a factor-specific transformation matrix W_k \u2208 \\mathbb{R}^{d' \\times d'}.\nFinally, for each factor graph G_{ik}, the graph-level representations can be attained by aggregating all node embeddings of the last L layer with a readout function. Formally,\nz_k(G_i) = READOUT(\\h_v^{(L)}(G_i)\\}_{v\u2208V(G_i)}),\n(7)\nwhere V(G_i) is the node set of G_i, READOUT can be a simple permutation invariant function such as the mean function. By considering all the hidden factors of graph data, we can get a factor-wise graph representation, namely, Z_{G_i} = [z_1(G_i),..., z_k (G_i)].\nWith the GNN-based encoder in the supervised task, a factor-wise representation Z_{G_i}^S = [z_1^S(G_i),...,z_k^S(G_i)] is extracted from labeled data, where different factor graphs will result in different features of the graph. Towards this end, we introduce K learnable factor prototypes C = \\{C_k\\}_{k=1}^K to obtain the attention weight \\alpha_k over latent factors for the input G_i:\n\u03b1_k(G_i) = Softmax(\u03c6(z_k^S(G_i), C_k)),                                                                              (8)\nwhere \u03c6 is the similarity function, i.e., cosine similarity. After that, we weighted sum all latent factors to get the graph representation and fed it into a label prediction model to get the corresponding output, as shown in Fig. 3(b).\nZ_{G_i}'^S = \\sum_{k=1}^K \u03b1_k(G_i) z_k^S(G_i),\n\u0177_i = Softmax(MLP(Z_{G_i}'^S)),                                                                                            (9)\nwhere we adopt a two-layer MLP to map the graph representation extracted from different factor graphs to label predictions. For the supervised loss, we adopt cross-entropy, which can be defined as follows:\nL_S = - \\sum_{c=1}^C Y_{ic} log \u0177_{ic},                                                                                 (10)\nwhere y_i \u2208 \\mathbb{R}^C is the label with the one-hot encoding of the input graph data G_i, and C corresponds to the number of graph categories in the dataset.\nFor unsupervised learning, we can also get a factor-wise graph representation Z_{G_i}^U = [z_1^U(G_i),..., z_K^U(G_i)] via the GNN-based encoder. Since some of the decomposed factor graphs may contain a similar structure without any other constraint, we propose several MI-based constraints to effectively disentangle the graph representation, as shown in Fig. 3(a).\n1) Intra-factor MI: As each factor graph corresponds to an interpretable relation between nodes, we seek to obtain the discriminant representation that captures the global content of the graph. Inspired by the deep InfoMax [45], we maximize the MI between the decomposed factor graph and the corresponding graph-level representation. For factor k, we define the intra-factor MI on global-local pairs, maximizing the estimated MI over all the nodes in factor graph G_{ik}, which is shown as:\n\\mathcal{L}_{intra} = - \\frac{1}{|V|} \\sum_{v\u2208V(G_i)} I(z_k^U(G_i), h_v^{(L)}(G_i)),                                                          (11)\nwhere I(z_k^U(G_i), h_v^{(L)}(G_i)) is the MI estimator modeled by discriminator D_k and parameterized by a neural network. We use the binary cross-entropy (BCE) loss between samples from\nthe joint (a.k.a., positive example) and product of marginals (a.k.a., negative examples) as the objective:\n-I(z_k^U(G_i), h_v^{(L)}(G_i)) =log\u03c3(D_k(z_k^U(G_i), h_v^{(L)}(G_i)))\n+E_{p \\& p'}[log(1 \u2013 \u03c3(D_k(z_k^U(G_i), h_v^{(L)}(G_i')))],                              (12)\nwhere h_v^{(L)}(G_i') is generated by the negative input graph G_i' sampled from a distribution p' = p, which is identical to the empirical probability distribution of the input space. In practice, we could sample negative samples by an explicit (stochastic) corruption function or directly sample negative graph instances in a batch.\n2) Inter-factor MI: For graph representations Z_{G_i}^U extracted from different factor graphs, the MI between every two of them reaches its minimum value zero when p(z_k^U(G_i), z_{k'}^U(G_i)) = P(z_k^U(G_i))P(z_{k'}^U(G_i)), which means that z_k^U(G_i) and z_{k'}^U(G_i) are independent to each other. Thus, MI minimization among them encourages the factor-wise graph representation to learn different aspects of information for the graph. Recently, several MI upper bounds [50], [51] have been introduced for MI minimization. However, the estimation MI upper bound among these K graph representations requires K(K \u2212 1) times estimation, resulting in far more costs especially when K is large. To alleviate this problem, as orthogonality can be seen as a special case of linear independence among representations, we loosen the constraint of minimization MI to orthogonality and the method has also been demonstrated to be effective by many previous studies [52]:\n\\mathcal{L}_{inter} = ||Z_{G_i}^{UT} Z_{G_i}^U - I||_1,\nZ_{G_i}^U = z_1^U(G_i)||...||z_K^U(G_i),                                                                   (13)\nwhere || is the L\u2081 norm, I is the identity matrix, || is the concatenation operation.\nRemark: In our model, we have also tested other MI minimization methods, such as Contrastive Log-ratio Upper Bound (CLUB) [51], which estimates MI by the difference of log-ratio of conditional distribution between positive and negative sample pairs. And these methods do not bring much change to the performance.\nWe maximize the MI between the factor graph and its corresponding graph representation to characterize each hidden factor while minimizing the MI among different factor graph representations for one input graph to enforce representation disentanglement. Formally, unsupervised loss of DisenSemi can be formulated as:\n\\mathcal{L}_{U} = \\sum_{k=1}^K \\mathcal{L}_{intra} + \\mathcal{L}_{inter}.                                                                                                              (14)\nFor transferring the learned disentangled graph representations from the unsupervised encoder to the supervised encoder, we encourage representations extracted from two encoders with high MI when they correspond to the same factor of one graph data bridging supervised and unsupervised tasks. Specifically, we maximize NCE-based MI lower bound [47], which aims at making similar representations closer and different in-stances far away from each other. We treat each representation of a graph data instance as a distinct class and distinguish it from representations extracted from other graph data instances. Formally, given graph dataset G = \\{G_i\\}_{i=|G^L|+G^U}, we assign each graph G_i with a unique surrogate label s_i = i. Different from the ground-truth label y_i of the labeled graph data, s_i can be seen as the ID of the corresponding graph in the dataset G. And in this way, the MI can be defined as:\nI_{NCE}(Z_{G_i}^S, Z_{G_i}^U) := log p(s_i|G_i)\n= log E_{p(k/G_i)} [p(s_i|G_i, k)],                                                                   (15)\nwhere p(s_i G_i) corresponding to the identify discrimination over dataset, p(k G_i) is the probability distribution of the k-th latent factors reflected in G_i, namely, can be \u03b1_k(G_i). And p(s_i G_i,k) corresponding to the identify discrimination subtask under k-th latent factor defined as:\np(s_i|G_i, k) = \\frac{exp (z_k^S(G_i), z_k^U(G_i))}{\\sum_{j=1}^{|G|} exp (z_k^S(G_i), z_k^U(G_j))},                                                       (16)\nwhere the subtask is over all graphs in the dataset. \u03c6 is the similarity function, adopting cosine similarity in our paper.\nIn effect, it is non-trivial to maximize the log-likelihood over the whole graph dataset due to the latent factors, where we need to calculate the posterior distribution defined with Bayes' theorem as follows:\np(k|G_i)p(s_i|G_i,k)\np(k|G_i, s_i) = \\frac{}{\\sum_{k=1}^K p(k|G_i)p(s_i|G_i,k)},                                                                              (17)\nCompared with prior distribution p(k G_i) that inferred only given the graph G_i, p(k G_i,s_i) is the probability of the k-th latent factor effectively aligning both the supervised and unsupervised models when applied to the same graph data. However, it is prohibitive to compute posterior probability due\nto the term p(s_i|G_i, k) in Eq. 16, which needs all the instances in the dataset for computing the denominator. Therefore, we alternatively maximize the evidence lower bound (ELBO) of the log-likelihood, which is defined as:\nlogp(s_i|G_i) \u2265 L_{ELBO} := E_{q(k|G_i,s_i)} [logp(s_i|G_i, k)]\n-D_{KL}(q(k|G_i, s_i), p(k|G_i)),                                                                                   (18)\nwhere q(k G_i, s_i) is a variational distribution to approximate the posterior probability p(k G_i, s_i). Here, we formalize the variational distribution by:\nq(k|G_i, s_i) = \\frac{p(k|G_i)p(s_i|G_i, k)}{\\sum_{k=1}^K p(k|G_i)p(s_i|G_i, k)},                                                                                 (19)\nwhere p(s_i G_i,k) can be defined with NT-Xent loss [53] on a minibatch B \u2286 G of graph data:\np(s_i|G_i,k) = \\frac{exp \u03c6(z_k^S(G_i), z_k^U(G_i))}{\\sum_{j\u2208B,j\u2260i} exp \u03c6(z_k^S(G_i), z_k^U(G_j))},                                                                  (20)\nNotice that the process is a variation of the Variational EM algorithm, where we infer q(k G_i, s_i) at the E-step and optimize the ELBO at the M-step.\nWe combine the supervised classification loss L_S, unsupervised loss L_U as well as consistency regularization loss L_{ELBO} together. And the overall objective for our semi-supervised graph classification is defined as:\n\\mathcal{L}_{total} = \\sum_{i=1}^{G^L}L_S +\\lambda\\sum_{j=1}^{|G|} \\mathcal{L}_{U} - \u03b3 \\sum_{j=1}^{|G|} L_{ELBO},                                                                       (21)\nwhere \u03bb is the relative weight between losses, \u03b3 is a tunable weight for consistency regularization loss.\n2) Complexity Analysis: To ensure scalability with large-scale datasets, we leverage mini-batch with size B = B_L+B_U to compute gradients. The computational consumption is mainly composed of three parts: (i) the unsupervised module; (ii) the supervised module; (iii) the disentangled consistency regularization. Given the graph with an average number of nodes V and edges E, the number of GNN layer and factor graphs is L and K, and the representation dimension is d. For (i) and (ii), the time complexity of the GNN-based encoder O(|E|Ld). For (i), the additional computational complexity of intra-factor and inter-factor MI constraints is O(|V|d) and O((K \u2212 1)d/2) respectively. For (ii), the additional computational complexity of the label prediction model is O(d + (d/K)\u00b2 + Cd/K) with C classes in the task. For (iii), we perform disentangled consistency regularization within minibatch, which takes O(Bd) for each graph. To sum up, we have the overall complexity of DisenSemi, O((2|E|L + |V| + (K + 1)/2 + d/K\u00b2 + C/K + B)d), which scales linearly in terms of the number of nodes and edges in each graph instance. Moreover, DisenSemi requires K factor graphs, prototypes and coreresponding GNN-based encoder in both supervised and unsupervised module, the space complexity is O((|V| + |E|)Ld + d + K|E|).\nF. Optimization and Complexity Analysis"}, {"title": "V. EXPERIMENT", "content": "We introduce the experimental settings and conduct extensive experiments on ten real-world benchmark datasets to validate the effectiveness of our proposed DisenSemi. We focus on answering the following research questions:\n\u2022 RQ1: How does our proposed model DisenSemi perform compared with other state-of-the-art models for semi-supervised graph classification?\n\u2022 RQ2: Is it necessary to keep key components of DisenSemi (i.e. GNN-based encoder, MI-based representation disentanglement and consistency regularization)?\n\u2022 RQ3: How do different hyper-parameters in DisenSemi impact the performance?\n\u2022 RQ4: Can DisenSemi ensure the regularized factor pertinent to the aspect bridging supervised and unsupervised tasks semantically?\n1) Datasets and Baselines: We apply our model to ten public accessible graph classification benchmark datasets\u00b9 following the previous works [14], [16], which includes four molecule datasets: MUTAG, PTC-MR, NCI1 and OGB-HIV, one bioinformatic dataset: PROTEINS and five social network datasets: IMDB-BINARY (IMDB-B), IMDB-MULTI (IMDB-M), REDDIT-BINARY (REDDIT-B), REDDIT-MULTI-5K (REDDIT-M5K) and COLLAB. Following recent works [15], [16], we adopt one-hot degree and centrality features instead when node attributes are not available in the datasets. We compare our proposed DisenSemi with sixteen recent proposed baselines including seven traditional graph classification approaches: GK [10], SP [7], WL [8], MLG [9], DGK [54], Sub2Vec [55] and Graph2Vec [56] and nine GNN-based semi-supervised graph classification methods: MVGRL [57], GraphCL [15], JOAO [17], DGCL [27], RGCL [58], Info-Graph [16], GLA [14], TGNN [18], GraphSpa [20].\n2) Evaluation Protocol: We evaluate the models with 10-fold cross-validation. Following the recent works [14]\u2013[16], we randomly shuffle and split each dataset into 10 parts. Each fold corresponds to one part of data as the test set, another part as the validation set and the rest parts as train set. Then we select 30% graphs from the training set as labeled graphs for each fold and conduct semi-supervised learning. The results are computed by the mean accuracy (%) with standard variation after ten repeated runs.\n3) Implementation Details: We implement our DisenSemi model in Pytorch. For all datasets, the embedding size of DisenSemi and other baselines (except GK, SP, WL, MLG) is fixed to 128. Since the ground-truth number of the latent factors is unknown, we search the number of factor graphs K amongst {2^0, 2^1, 2^2, 2^3, 2^4}, corresponding embedding di-mensions are {128,64, 32, 16,8} per factor graph. We set three message passing layers for our method and the hyperparameters \u03bb = 0.001,\u03b3 = 0.001. We optimize DisenSemi with Adam optimizer by setting the learning rate to 0.005 and\nhttps://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets;\nhttps://ogb.stanford.edu/docs/graphprop/\nthe number of epochs is set to 200 for all the methods. For all the traditional graph classification approaches, we compute similarities between graphs and feed them into a downstream Support Vector Machines (SVM) classifier to evaluate the classification accuracy. For MVGRL, GraphCL, JOVO, DGCL and RGCL, we pre-train the model with unlabeled graphs first and fine-tune the model with the labeled graphs. For InfoGraph, GLA, TGNN, GraphSpa and our proposed Dis-enSemi, we integrate the pre-training and fine-tuning phases together. The source code of DisenSemi is available at https://github.com/jamesyifan/DisenSemi.\n1) Overall Comparison: We present the quantitative results of semi-supervised graph classification with 30% label ratio in Table I. According to the results, the following observations can be derived.\n\u2022 Traditional graph classification approaches, which include graph kernel and unsupervised graph-level representation learning methods, perform worse than GNN-based semi-supervised graph classification methods. This indicates that hand-crafted features extracted by traditional methods suffer from poor generalization. Instead, GNN-based methods are more powerful to extract features from graph-structured data for the semi-supervised graph classification task.\n\u2022 Among GNN-based semi-supervised graph classification methods, DGCL and RGCL, which leverage disentangled representation for graph classification, always achieve better performance than other baselines on most datasets. It shows that explicitly considering the entanglement of factors for a graph helps to learn better representation for semi-supervised graph classification.\n\u2022 Our model DisenSemi achieves the best performance on all ten datasets, which significantly demonstrates the effectiveness of our proposed model. From the results, we deem that the improvement over state-of-the-art methods not only from learning disentangled representation of the graph, but also from explicitly regularizing the rationale that fits well between supervised and unsupervised learning models for semi-supervised graph classification.\n2) Performance on Different Labeling Ratio: We vary the labeling ratio of training data to show the performance of different models. Here we omit the traditional methods since they do not have competitive performance. Fig. 4 illustrates the performance w.r.t. different labeling ratios of data on four datasets. We find that:\n\u2022 The performance of all models generally improves as the number of available labeled data increases. This demonstrates that the labeling ratio of training data is an important factor of the model. Meanwhile, the growth is not linear, which means unsupervised data also plays a key role to boost the model performance.\n\u2022 Disentangled graph representation model, i.e., DGCL and RGCL, though explicitly considering the entanglement of factors for graph, can not always guarantee the improvement over other baselines among different labeling ratios. This is attributed to the fact that supervised and unsupervised\nB. Performance Comparison (RQ1)\nA. Experimental Settings"}, {"title": "VI. CONCLUSION", "content": "In this paper, we study semi-supervised graph classification, which is a fundamental problem in graph-structured data mining. For transferring suitable knowledge from the unsupervised model to the supervised model, we propose a novel framework termed DisenSemi, which learns disentangled representation to capture multiple graph characteristics stemming from different aspects. Specifically, our DisenSemi consists of a supervised model and an unsupervised model. For both models, we design a disentangled graph encoder to extract factor-wise graph representation and train two models with supervised objective and MI-based constraints, respectively. Then we propose an MI-based disentangled consistency regularization to identify the rationale that aligns well between two models for the current graph classification task and transfer corresponding knowledge semantically. Extensive experiments on ten benchmark graph classification datasets demonstrate the efficacy of our DisenSemi. However, the number of factor graphs varies among datasets and needs to be searched to find the best value. So in future work, we will apply our DisenSemi to more realistic semi-supervised classification scenarios and automatically select factor graph numbers in a bi-level optimization framework."}, {"title": "C. Ablation Studies (RQ2)", "content": "To deeply understand our proposed DisenSemi"}, {"title": "VI. CONCLUSION", "content": "In this paper, we study semi-supervised graph classification, which is a fundamental problem in graph-structured data mining. For transferring suitable knowledge from the unsupervised model to the supervised model, we propose a novel framework termed DisenSemi, which learns disentangled representation to capture multiple graph characteristics stemming from different aspects. Specifically, our DisenSemi consists of a supervised model and an unsupervised model. For both models, we design a disentangled graph encoder to extract factor-wise graph representation and train two models with supervised objective and MI-based constraints, respectively. Then we propose an MI-based disentangled consistency regularization to identify the rationale that aligns well between two models for the current graph classification task and transfer corresponding knowledge semantically. Extensive experiments on ten benchmark graph classification datasets demonstrate the efficacy of our DisenSemi. However, the number of factor graphs varies among datasets and needs to be searched to find the best value. So in future work, we will apply our DisenSemi to more realistic semi-supervised classification scenarios and automatically select factor graph numbers in a bi-level optimization framework."}, {"title": "C. Ablation Studies (RQ2)", "content": "To deeply understand our proposed DisenSemi, we conduct ablation studies over the key components of the model.\n1) Effect of GNN-based Encoder: In DisenSemi, we encode each factor graph via the proposed GNN with its own edge coefficient. To explore whether DisenSemi can derive benefits from the proposed GNN-based encoder, we compare five types of well-known GNN operators, namely SAGEConv [59], GATConv [60], GINConv [61], GCNConv [62] and Graph-Conv [49]. Notice that for SAGEConv*, GATConv* and GIN-Conv*, we ignore the edge coefficient and randomly remove 15% edges to obtain different factor graphs. For GraphConv*, we retain the edge coefficients and randomly remove edges separately. The performance of different GNN operators on all ten datasets is shown in Table II. We make the following observations from the results.\n\u2022 In most cases, the performance of GCNConv is better than other variants, which ignore the edge coefficient and randomly remove edges to obtain different factor graphs (i.e., SAGEConv*, GATConv*, GINConv*). This indicates that disentangling the graph with heterogeneous relations into different factor graphs can learn more effective and discriminative graph-level representation for the downstream classification task.\n\u2022 GraphConv* considers the high-order structure of the factor graph but still exhibits poor performance in cases where edges are randomly removed. Instead, GraphConv consistently outperforms other models when considering the edge coefficient. This outcome further validates the effectiveness of the learned edge coefficient within each factor graph for capturing higher-order graph structures.\n2) Effect of MI-based Representation Disentanglement: As disentangled graph representation is implemented via MI estimation in our DisenSemi, we investigate its impact on the performance with the following three variants:\n\u2022 w/o Intra-MI-It removes the intra-factor MI maximization on global-local pairs for unsupervised objective function (i.e., L_U = L_{inter}).\n\u2022 w/o Inter-MI-It removes the inter-factor MI minimization between every two factor graphs for unsupervised objective function (i.e., L_U = \\sum_{k=1}^K L_{intra}).\n\u2022 w/o MI-It removes the whole MI-based constraint for unsupervised objective function (i.e., w/o L_U).\nTable III presents the results of different model variants on all ten datasets and we can observe that the best results have been attained by considering both intra- and inter-MI estimation, which indicates that both intra-factor MI maximization on global-local pairs and inter-factor MI minimization between every two factor graphs play a key role to learn disentangled representation for semi-supervised graph classification.\n3) Effect of Consistency Regularization: Disentangled consistency regularization helps to transfer the learned disentangled graph representations from the unsupervised encoder to the supervised encoder. To further investigate how the consistency regularization facilitates the model performance, we compare DisenSemi with the following two variants:\n\u2022 Variant 1-It set p(k|G_i) = 1/K, a uniform distribution over K latent factors, which means each k-th latent factors reflected in G_i is same.\n\u2022 Variant 2-It set p(k G_i) with a random distribution over K latent factors. We implement it by randomly choosing a latent factor reflected in G_i.\nThe result of DisenSemi and its variants are shown in Table III and we have the following observations:\n\u2022 The performance decreases when p(k|G_i) of the model are set with uniform and random distribution in Variant 1 and Variant 2 respectively, which demonstrates that inferring the latent factor of a graph to explicitly identify and regularize the rationale between supervised and unsupervised learning model is important.\n\u2022 Variant 1 performs worse than variant 2 in most datasets. This may be that setting p(k|G_i) with a random distribution can also specify a rationale between supervised and unsupervised learning models. Instead, inferring the latent factor of a graph uniformly might introduce rationale's complement to the representation learning."}, {"title": "D. Parameter Sensitivity (RQ3)", "content": "We also examine the sensitivity of the proposed DisenSemi to various hyper-parameters.\n1) Effect of the Number of Factor Graphs: To analyze whether DisenSemi can benefit from disentangled representation, we study the performance of the model with varying numbers of factor graphs. In particular, we search the number of factor graphs in the range of {2^0,2^1,...,2^4}. Fig. 5 summarizes the experimental results w.r.t. different numbers of factor graphs on four datasets and we find that:\n\u2022 When the number of factor graphs K = 1, the model can be degraded into an entangled representation-based semi-supervised graph classification model with poor performance. This indicates that explicitly modeling multiple aspects of discriminant features for the graph can greatly facilitate the model performance.\n\u2022 Increasing the number of factor graphs can substantially enhance the model performance. DisenSemi achieves the best performance at K = 4 in MUTAG, PROTEINS and REDDIT-B and K = 8 in IMDB-B, which represents the optimal aspects of discriminant features for the graph.\n\u2022 However, when the number of factor graphs is too large (i.e., K \u2265 16), the model performance will degrade slowly. This might be caused by applying a too complex semantic structure for the graph.\n2) Effect of the Number of Message Passing Layers: For each disentangled factor graph, we investigate how the depth of DisenSemi affects the performance. In particular, we stack varying numbers of message passing layers in the range of [0,5]. Fig. 6 shows the experimental results w.r.t. different number of message passing layers in four datasets (i.e., MUTAG, PROTEINS, IMDB-BINARY and REDDIT-BINARY) and we have the following findings:\n\u2022 When the number of message passing layers L = 0, the model only takes a linear transformation into consideration and suffers from the degenerating issue. Hence, the result verifies the rationality and effectiveness of GNNs for learning graph-level representation.\n\u2022 More message passing layers will yield influence from high-order neighbors in the graph and increase the depth of the model. Clearly, two layers of a model can achieve better performance than one layer. It illustrates the importance of messages passing between nodes to capture the structure of each factor graph.\n\u2022 Too many message passing layers may hurt the model performance. When stacking more than three message passing layers, the model may introduce noise and suffer from the over-smoothing issue.\n3) Effect of Relative Weight between Losses: We investigate the sensitivity of relative weight between losses, namely \u03bb and \u03b3. In particular, we vary the value of \u03bb and \u03b3 from 0 to 1 on PROTEINS and COLLAB. As shown in Fig. 7, unsupervised module and disentangled consistency regularization play an important role in the objective and there is an evident performance drop when \u03bb, \u03b3 = 0. As \u03bb and \u03b3 get larger, the performance is relatively stable, and we can get the best results when \u03bb, \u03b3 = 1e - 3."}, {"title": "E. Visualization and Case Study (RQ4)", "content": "To further investigate how the disentangled representation facilitates the semi-supervised graph classification task, we conduct two qualitative assessments (visualization and case study) of the proposed DisenSemi and baselines.\n1) Visualization of Representation Correlation: Besides the quantitative evaluation, we also plot the absolute values of the correlations between the elements of the 128-dimensional graph representations. Fig. 8 shows the correlations analysis of representation obtained from GraphCL, DGCL and our proposed DisenSemi with four factor graphs in the MUTAG and PROTEINS dataset. We find that:\n\u2022 Compared with the more highly independent representation of DGCL, the learned representation of GraphCL is entangled and the performance is degraded. This indicates that disentangled representation can achieve high performance in the graph classification task.\n\u2022 The representation generated from DisenSemi presents a more block-wise correlation pattern, indicating that four factor graphs of DisenSemi are likely capturing mutually exclusive information to provide discriminant learned features for the target task.\n2) Case Study: For a more intuitive understanding of the disentanglement, we provide examples of the generated factor graphs in DisenSemi. Fig. 9 presents the original graph and those disentangled factor graphs. The edges of the visualized disentangled factor graphs are highlighted with different colors after we set the coefficient threshold in the original graph. For example, in the MUTAG dataset, the task is to predict the molecule's mutagenicity on Salmonella typhimurium for a collection of nitroaromatic compounds. We can see different parts of a molecule graph playing different roles in the prediction task. This also justifies the reliability of our generated factor graphs in getting the disentangled graph representation."}, {"title": "ACKNOWLEDGEMENT", "content": "The authors are grateful to the anonymous reviewers for critically reading this article and for giving important suggestions to improve this article."}]}