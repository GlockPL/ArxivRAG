{"title": "STRATEGIC ARMS WITH SIDE COMMUNICATION PREVAIL OVER LOW-REGRET MAB ALGORITHMS", "authors": ["Ahmed Ben Yahmed", "Cl\u00e9ment Calauz\u00e8nes", "Vianney Perchet"], "abstract": "In the strategic multi-armed bandit setting, when arms possess perfect information about the player's behavior, they can establish an equilibrium where: 1. they retain almost all of their value, 2. they leave the player with a substantial (linear) regret. This study illustrates that, even if complete information is not publicly available to all arms but is shared among them, it is possible to achieve a similar equilibrium. The primary challenge lies in designing a communication protocol that incentivizes the arms to communicate truthfully.", "sections": [{"title": "1. INTRODUCTION", "content": "The concept of the strategic multi-armed bandit extends the traditional multi-armed bandit (MAB) problem by incorporating the utility aspect of the arms. In this context, arms have the ability to report values that differ from the observed rewards. Formally, we consider a set of K stochastic arms, each arm k is characterized by its own reward distribution $D_k$ with a mean denoted as $\u03bc_k = E[D_k]$. To maintain clarity while broadening our perspective, we assume an order such that $1 \u2265 \u03bc_1 \u2265 \u03bc_2 \u2265 ... \u2265 \u03bc_K \u2265 0$. During each round t, the player pulls an arm $k_t$. Subsequently, the chosen arm observes a reward $r_{k_t,t} ~ D_{k_t}$, and it has the possibility to report a value $X_{k_t,t} \\neq r_{k_t,t}$ to the player while retaining $r_{k_t,t} - X_{k_t,t}$ as its own utility. Importantly, only arm $k_t$ possesses knowledge of the actual observed reward $r_{k_t,t}$, whereas the player is only aware of $X_{k_t,t}$ and remains unaware of the withheld portion. Therefore, the player's decision is based on the information collected up to time t, which can be formally encoded in the filtration $F_{P,t} = {k_1,X_{k_1,1},..., k_t, X_{k_t,t}}$. We define $x_k$ (and $r_k$) as the concatenation of reported values (and rewards, respectively) over T rounds. Accordingly, the utility associated with arm k can be expressed as:\n$$U_k (X_k, X_{-k}) = E [\\sum_{t=1}^{T} (r_{k_t,t}-X_{k_t,t}) 1 [k_t=k]]$$"}, {"title": "1.1. Related Work", "content": "Previous studies, such as [4, 5, 6], have already examined scenarios involving connected and communicating arms. In these scenarios, pulling an arm k at time step t not only provides information about arm k itself but also reveals information about some related arms. A typical example of such a situation is advertising on social networks, where a decision-maker targets individual users of an online platform with promotions, hoping to maximize purchases. However, in this context, the arms (i.e users) are connected and capable of communication. Conceptually, pulling an arm k triggers instantaneous communication through the arms, revealing aggregated information about all arms related to k and itself [4]. The concept of a strategic arm in the MAB setting was first introduced in the groundbreaking work by [7]. This notion highlights the challenge of dealing with arms that are not limited to providing their true rewards but can instead manipulate the player to maximize their own utilities. To achieve this, authors present a equilibrium strategy for such arms especially when $\u03bc_1 - \u03bc_2 \u2264 \u03b5$, enabling them to leave the player with only a minimal reward. This strategic approach poses a significant obstacle for any low-regret algorithm employed by the player."}, {"title": "1.2. Contributions", "content": "This study addresses the limitation of [7] where arms need full information to be able to have an equilibrium strategy in which they can extract (almost) all the value. The new strategy introduced here, which includes a proper communication protocol, enables them to achieve equilibrium for full surplus extraction and prevents the player from generating high revenue, regardless of the low-regret MAB algorithm the player chooses. We build upon the strategy presented in [7], which serves as a foundation for our work. The main challenge in constructing an equilibrium strategy that doesn't rely on public knowledge of the entire history is to guarantee that the communication scheme doesn't encourage arms to convey false information. Indeed, from a game-theoretic perspective, both the report $X_{k_t,t}$ to the player and the information they share with other arms are components of strategic behavior. We support our claims with theoretical analysis and experimental results."}, {"title": "2. MODELING COMMUNICATION", "content": "We assume that the arms are interconnected and form a network modeled by a graph. This setting is widely used in distributed learning and multi-agent communication [8] as it provides a stronger privacy protection and reduced risk of communication bottleneck. Arms will be linked by some graph topology (see Fig. 1) that allows neighboring arms to share information. At any point in time, no single arm will have access to all the information that is available across the graph. Additionally, all types of message passing occur simultaneously throughout the graph. This means that all information at all arms is updated instantaneously and equivalently at the same time (i.e the network has a synchronized clock)."}, {"title": "3. LOW-REGRET MAB ALGORITHMS", "content": "The efficiency of MAB algorithms is most commonly compared with the notion of regret. Regret measures the cumulative loss incurred over T rounds by choosing arm $k_t$ at time t instead of the best arm $k^*$. Ideally, we aim for a low-regret, which implies convergence to the best arm, at least asymptotically. Formally, let $k_t$ be the strategic arm selected by the player at round t using algorithm A. The selected arm observes a reward $r_{k_t,t}$ that we consider in this work in [0, 1]. Then, it reports, in an adversarial fashion, a value $X_{k_t,t}$. The regret of the algorithm A is the random variable:\n$$R_T(A) = max_k \\sum_{t=1}^{T} X_{k,t} - \\sum_{t=1}^{T} X_{k_t,t}$$\nWe follow the $(\u03c1, \u03b4)$-low-regret definition given in [7], i.e an algorithm A is a $(\u03c1, \u03b4)$-low-regret for the MAB problem if with probability 1 \u2212 \u03c1,\n$$R_T(A) \u2264 \u03b4$$\nThe majority of MAB algorithms, particularly in the adversarial scenario - which is relevant here, considering that the reporting protocol is widely regarded as an adversarial setting \u2013 assign a probability $p_{k,t}$ to each arm $k \u2208 {1,\u2026\u2026, K}$ to be pulled at round t. The algorithm then selects the arm according to these probabilities. The probability $p_{k,t}$ is primarily determined by the parameters of the algorithm A, an intrinsic information $I_k$ of arm k, and the average"}, {"title": "4. ARMS' STRATEGY THAT PREVAILS OVER LOW-REGRET MAB ALGORITHMS", "content": "In this section, we extend the work presented in [7] and introduce a strategy that enables strategic arms with restricted communication to reach an \u03b5-Nash equilibrium while providing only a marginal utility to the player. The Strategy 1 adopts market sharing techniques, where arms select their actions in a way that ensures they are chosen an equal number of times. As a result, the player receives only minimal revenue, as he is unable to commit to selecting the best arm consistently, deviating from the traditional bandit setting. This strategy does not assume that the arms possess prior knowledge of their own distributions or the history of selected arms. In other words, their respective information available at time t is less complex compared to the setting presented in [7]. The strategy is presented as follow: let A be the low-regret MAB algorithm used by the player , $N_k (t)$ be the number of times arm k has been pulled up to time t and A = $[a_{kl}]$ the combination matrix describing the communication graph topology. Set $B = 7\\sqrt{KT\u03b4}$ and $\u03b8 = \\frac{K\u03b4}{T}$. Then the strategy that the arms shall use is Strategy 1."}, {"title": "Strategy 1: Equilibrium strategy", "content": "1 for t = 1,..., T do\n2 Update $I_{k,t}$ the intrinsic information available for all\n3 arm k at time t. Initiate $I_{k,0} = I_{k,t}, \u2200k$.\n4 for n = 1,..., $\\frac{K\u03b4}{T}$ do\n5 for k = 1,..., K do\n6 $I_{k,n} = \\sum_{l\u2208N_k} \u03b1_{lk}I_{l,n-1}$\n7 end\n8 end\n9 for k = 1,..., K do\n10 If at any time s \u2264 t in the past $N_k(s) < -B$ then arm k defects and offers its full value\n11 $X_{k,t} = r_{k,t}$.\n12 Else arm k computes the probability $p_{k,t} = Pr(A, I_k, I_K,T)$ and offers $X_{k,t} = \\frac{\u03b4}{\u03b8}(1 - p_{k,t})$.\n13 end\nend\nThis strategy consists of two parts. In the first part, each arm updates its information individually. For example, if an arm is pulled, it updates its reward information based on the received reward. If it is not chosen, the information remains the same as in the previous time step t \u2212 1. Then, iteratively, the arms use the communication"}, {"title": "5. THEORETICAL ANALYSIS", "content": "To facilitate the analysis, we suppose that $K \u2264 \\frac{T}{1000} \\log(T)$, $P < \\frac{1}{2}$ and $\u03b4 \u2265 \\sqrt{T \\log(T)}$."}, {"title": "5.1. Reliable approximation of pulling probabilities", "content": "First, we will demonstrate the utility of the communication steps and how, after a sufficient number of iterations \u03c4, the local values $\\hat{I}_{k,T}$ serve as accurate estimates of the true average $I_t$ which allows each arm k to compute an accurate estimate $\\hat{p}_{k,t}$ of $p_{k,t}$. To do so, we start by introducing some assumptions that are commonly used in the literature [8, 11, 9, 12].\nAssumption 1 (Doubly-stochastic combination matrix). The combination matrix A = $[a_{kl}]$ representing the graph topology is doubly-stochastic and symmetric. This means that the matrix has non-negative elements and satisfies:\n$$A1_K = 1_K, A^T = A$$\nWe also assume that the matrix A is primitive. This implies that there exist paths, in both directions, between any two distinct nodes with nonzero scaling weights. Additionally, there is at least one nontrivial self-loop present, meaning that $a_{kk} > 0$ for at least one node k.\nBy applying the Perron-Frobenius theorem, Assumption 1 states that the mixing rate \u03bb of the combination matrix (i.e., the spectral radius of $A - \\frac{1_K1_K^T}{K}$) is strictly less than 1:\n$$\u03bb < 1$$\nAssumption 2 (Lipschitz mapping). The mapping $\\hat{I} \u2192 Pr(., ., \\hat{I})$ is Lipschitz, namely: $\u2203L \u2208 R_+$ such that $\u2200\\hat{I}, \\hat{I}'$:\n$$|| Pr(., ., \\hat{I}) \u2013 Pr(., ., \\hat{I}')|| \u2264 L||\\hat{I} \u2013 \\hat{I}'||$$\nAssumption 2 is valid since we are considering a finite horizon T.\nTheorem 1 (Network disagreement). Under Assumption 1, the network disagreement between the true average $I_t$ and the local estimates $\\hat{I}_{k,\u03c4}$, converges to zero.\n$$\\frac{1}{K} \u03a3_{k=1}^{K} ||\\hat{I}_{k,\u03c4} \u2013 I_t||^2 < \u03b1_\u03c4 \u03bb^{2\u03c4}, with \u03b1_\u03c4 > 0$$\n$$\\xrightarrow{T\u2192+\u221e}$$\nwhere $1_K$ is a vector of length K consisting of ones."}, {"title": "Proof:", "content": "for generality we suppose that the dimension of variables is M, i.e dim($I_{k,t}$) = dim($I_t$) = dim($\\hat{I}_{k,t}$) = M. We begin by defining the following variable that aggregates the local variables of each arm into a single variable:\n$$\\hat{I}_n = col{\\hat{I}_{1,n},.., \\hat{I}_{K,n}}$$\n$$\\hat{I}_n \u2208 R^{K \\cdot M}$$\nWhere $\\hat{I}_n$ is a vector of length K \u00d7 M, obtained by vertically concatenating vectors enclosed in brackets. We express the update in the communication scheme using a more concise notation:\n$$\\hat{I}_n = A \\hat{I}_{n-1}$$\n$$\\implies (I - \\frac{1_K1_K^T}{K} \u2297 I_M) \\hat{I}_n = (A - \\frac{1_K1_K^T}{K} \u2297 I_M) \\hat{I}_{n-1}$$\nIt should be noted that due to the nature of the combination matrix A, the true average $I_t$ is equivalent to the average of the variables $I_{k,n}$ for any given n. So we write:\n$$I_t = \\frac{1}{K} \\sum_{k=1}^{K} I_{k,n} = (\\frac{1_K1_K^T}{K} \u2297 I_M) \\hat{I}_n$$\nand we define the extended average as a vector of length K \u00d7 M:\n$$\\hat{I}_t = \\frac{1_K1_K^T}{K} \u2297 I_t = (\\frac{1_K1_K^T}{K} \u2297 1_M) \\hat{I}_n$$\nWe get:\n$$\\hat{I}_n - \\hat{I}_t = (A - \\frac{1_K1_K^T}{K} \u2297 I_M) \\hat{I}_{n-1}$$\n$$(A - \\frac{1_K1_K^T}{K} \u2297 I_M) \\hat{I}_t = (\\frac{1_K1_K^T}{K} \u2297 I_M) (\\hat{I}_{n-1} - \\hat{I}_t)$$\n$$||\\hat{I}_n - \\hat{I}_t|| = ||(A - \\frac{1_K1_K^T}{K} \u2297 I_M) (\\hat{I}_{n-1} - \\hat{I}_t)||$$\nTaking the square norm:\n$$||\\hat{I}_n - \\hat{I}_t||^2 = ||(A - \\frac{1_K1_K^T}{K} \u2297 I_M) (\\hat{I}_{n-1} - \\hat{I}_t)||^2$$\n$$\u2264 \u03bb^2 ||\\hat{I}_{n-1} - \\hat{I}_t||^2$$\nIterating from \u03c4 to 0:\n$$||\\hat{I}_\u03c4 - \\hat{I}_t||^2 \u2264 \u03bb^{2\u03c4} ||\\hat{I}_{0} \u2013 \\hat{I}_t||^2$$\nTaking $\u03b1_\u03c4 = ||\\hat{I}_{0} \u2013 \\hat{I}_t||^2$ finishes the proof.\nCorollary 1. Under Assumption 2 and using Theorem 1, we have:\n$$|p_{k,t} - \\hat{p}_{k,t}| = |Pr(A, I_{k,t}, I_t) \u2013 Pr(A, I_{k,t}, \\hat{I}_{k,t})|$$\n$$< L \u03b1_\u03c4 \u03bb^{2\u03c4}$$\nTherefore, we have demonstrated that after a sufficient number of iterations \u03c4, $\\hat{p}_{k,t}$ provides a reliable approximation for $p_{k,t}$. This allows the arms to calibrate their rewards as if they have access to the complete information available to the player."}, {"title": "5.2. Equilibrium resulting from Strategy 1", "content": "Following Strategy 1, arms won't defect and will achieve a market sharing situation where each arm is pulled approximately an equal number of times. This renders the utilized low-regret MAB algorithm inefficient. This observation is formalized as follows:\nTheorem 2. If arms use Strategy 1, then with high probability $(1 - \\frac{1}{T})$, $N_k(t) \u2265 \\frac{T}{K} - B,\u2200t \u2208 [T],k \u2208 [K]$ and they will be in an O($\\sqrt{KT\u03b4}$)-Nash equilibrium.\nProof sketch: if arms faithfully adhere to the Strategy 1 denoted as S*, then by employing Corollary 1 and similar arguments as in [7], we can demonstrate that with high probability (1 - $\\frac{1}{T}$), $N_k(t) \u2265 \\frac{T}{K} - B$ for all $t \u2208 [T]$ and $k \u2208 [K]$. This implies that arms do not defect, and step 9 of the strategy is never executed. To establish equilibrium, we will evaluate the utility of arm k when all arms are adhering to S*, while the player employs a low-regret MAB algorithm. We show that:\n$$U_k (S_k, S_{-k}) \u2265 \\frac{\u03bc_k T}{K} - O(\\sqrt{KT\u03b4})$$\nOn the other hand, if arm k plays any strategy S other than S*, we can demonstrate that:\n$$U_k (S_k, S_{-k}) < \\frac{\u03bc_k T}{K} + O(\\sqrt{KT\u03b4})$$\nTherefore, for all strategies S different from S*, we can derive the following inequality:\n$$U_k(S_k, S_{-k}) \u2013 U_k(S_k, S_{-k}^*) \u2264 O(\\sqrt{KT\u03b4})$$\nshowing that $(S_1^*, ..., S_K^*)$ is an O($\\sqrt{KT\u03b4}$)-Nash equilibrium for all arms. Therefore, it becomes evident that the equilibrium is primarily determined by the number of times each arm is pulled. At equilibrium, we observe that each arm is pulled approximately the same number of times and receives in average $\\frac{\u03bc_k}{K}$ per round. If arms choose to deviate from this strategy by dishonestly reporting either their values to the player or the values communicated to their neighbors, one of two scenarios will unfold. In the first scenario, this deviation will not impact the number of times each arm is pulled, thus failing to activate the defection step 9. Consequently, the utility of the arms remains unaffected, and the equilibrium remains intact. In the second case, the defection step 9 is triggered, resulting in arm 1, which has the highest real mean value, emerging as the winner. In this case, it gains at maximum $\u03bc_1 - \u03bc_2$ per round. However, it's important to note that $\u03bc_1 - \u03bc_2 \u2264 \\frac{\u03b4}{\u03b8}$ by assumption. Given this condition, there is no incentive for arms to deviate from the strategy, as the potential gain from defection is less than what they can achieve by adhering to the equilibrium strategy.\nThis equilibrium proves detrimental to the player, resulting in constrained revenue, irrespective of the low-regret algorithm employed:\nCorollary 2. If arms follow Strategy 1, the player gets at most O($\\sqrt{KT\u03b4}$) revenue.\nProof: given that playing according to Strategy 1 implies that with high probability $\u2200t \u2208 [T], k \u2208 [K], N_k(t) \u2265 \\frac{T}{K} - B$, arms won't defect and the player gets $T \\cdot 0 = O(\\sqrt{KT\u03b4})$. In the case of the low probable event the player will get at most T. So the player revenue is:\n$$Player-revenue \u2264 (1 - \\frac{1}{T})O(\\sqrt{KT\u03b4}) + \\frac{T}{T}$$\n$$\\implies Player-revenue < O(\\sqrt{KT\u03b4})$$"}, {"title": "6. EXPERIMENT", "content": "In this section we test Strategy 1 against an adapted version of EXP3.P [13] as it is done in [4] to take into consideration the existence of the side communication. It is a ($\u03c1, \u039f(/T \\log(K\u03c1^{-1})))$-\nlow-regret MAB algorithm. The intrinsic information for arm k is the exponential of its weighted estimated cumulative gain. We create a random Erdos-R\u00e9nyi graph over K = 10 nodes, where each pair of nodes are linked independently with probability p = 0.6. Arms are modeled as Bernoulli random variables. For most arms, mean is set at 0.4, while three specific arms have different means: 0.8, 0.85, and 0.9. The combination matrix A is generated using the Metropolis rule [14]. We run the experiment for T = 5.105 rounds and set \u03c4 = 50."}, {"title": "7. CONCLUSION", "content": "In scenarios involving repeated interactions, converting a single-step collusion scenario into an equilibrium within the cumulative game necessitates the ability of each participant to identify instances where others deviate from the collusive behavior. This study illustrates that even when not all historical information is publicly accessible, the arms can implement a communication strategy enabling each of them to detect deviations, whether they involve falsifying player reports or manipulating shared information. The established equilibrium surpasses any low-regret MAB algorithm, resulting in reduced player revenues. Future research can focus on developing mechanisms that encompass not only traditional sequential learning in the classical MAB style but also integrate incentive mechanisms to effectively address the challenges highlighted in this paper."}]}