{"title": "Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures", "authors": ["Ziyuan Huang", "Mark Newman", "Maria Vaida", "Srikar Bellur", "Roozbeh Sadeghian", "Andrew Siu", "Hui Wang", "Kevin Huggins"], "abstract": "This study examined the viability of enhancing the prediction accuracy of artificial neural networks (ANNs) in image classification tasks by developing ANNs with evolution patterns similar to those of biological neural networks. ResNet is a widely used family of neural networks with both deep and wide variants; therefore, it was selected as the base model for our investigation. The aim of this study is to improve the image classification performance of ANNs via a novel approach inspired by the biological nervous system architecture of planarians, which comprises a brain and two nerve cords. We believe that the unique neural architecture of planarians offers valuable insights into the performance enhancement of ANNs. The proposed planarian neural architecture-based neural network was evaluated on the CIFAR-10 and CIFAR-100 datasets. Our results indicate that the proposed method exhibits higher prediction accuracy than the baseline neural network models in image classification tasks. These findings demonstrate the significant potential of biologically inspired neural network architectures in improving the performance of ANNs in a wide range of applications.", "sections": [{"title": "I. INTRODUCTION", "content": "THE use of multistep or modular neural networks to solve complex problems is widespread in several scientific fields, e.g., biomedical science, geography, and railroad track management [1-3]. Multistep solutions to a given research problem involve implementing and linking multiple neural networks sequentially in a series of steps and decomposing the overall task into constituent subtasks to be solved via different networks. However, when such architectures do not include cross-network communication, forward and backward propagation is possible only within each individual neural network.\nIn this study, we considered a planarian neural network (PNN) framework based on the biological nervous system of planarians. A PNN comprises a brain, two nerve cords, and the communication mechanisms between them. To optimize the training process, we introduced the concept of a patience gate that regulates the frequency of communication between the brain and the nerve cords. Our experiments revealed a correlation between the value of the patience gate in the PNN framework and the test accuracy, indicating that the patience gate plays a crucial role in optimizing PNN training. The best-performing PNNs on CIFAR-10 and CIFAR-100 were those with patience gate values of 15 (PNN15) and 20 (PNN20), respectively.\nWe adopted a standardized methodology to evaluate the performance of the proposed model on CIFAR-10 and CIFAR-100. In particular, we recorded the mean and median measurements over five runs for CIFAR-10 and seven runs for CIFAR-100. The performances of pairs of models and multiple models were compared via the Mann-Whitney U test and the Kruskal-Wallis H test, respectively."}, {"title": "II. MATERIALS AND METHODS", "content": "Data Augmentation: Data augmentation was performed by adding four pixels to each side of each image with values of 0 and then randomly cropping it to a 32 \u00d7 32 size, including a horizontal flip and pixel mean subtraction, during training [4-7]. During validation, only a horizontal flip was implemented. Therefore, no data augmentation was performed on the test data. This strategy was adopted to ensure that the proposed PNN algorithm can enhance model performance on the basis of only moderate data augmentation.\nCIFAR-10: CIFAR-10 is a public dataset intended for computer vision and machine learning tasks [8]. It contains 60,000 32 \u00d7 32 color images categorized uniformly into 10 classes. These images were divided into 50,000 training images and 10,000 test images, with 5,000 and 1,000 corresponding to training and testing in each class, respectively. In this study, 10% of the training images were used as validation images\u2014the validation data loader randomly loaded 10% of the training images in each epoch. Testing was not initiated until the training was complete.\nCIFAR-100: CIFAR-100 is derived from the same source as CIFAR-10 [8]. CIFAR-100 contains 60,000 32 \u00d7 32 color images classified into 100 classes, with 600 images in each class. These images were categorized into 50,000 training and 10,000 test images, with 500 and 100 images belonging to each class, respectively. During k-fold cross-validation, 10% of the training images were used as validation data. Testing was performed only after the training was completed.\nProposed PNN Architecture: This section presents the proposed dual-nerve cord PNN adapted from the structure of the planarian nervous system. It is based on the guiding principle that the expansion, integration, and fusion of nerve integration centers facilitate nervous system evolution [9]. The PNN was designed to improve the prediction performance of artificial neural networks (ANNs). The number of artificial nerve cords was varied depending on the requirements of each experiment.\nA conceptual diagram of the PNN model, comprising an artificial brain and two or more artificial nerve cords, is depicted in Fig. 1. The artificial brain is considered an artificial apical nervous system (AANS) that controls global activities. The nerve cords are represented by an artificial blastoporal nervous system (ABNS), which controls local and specialized functions. The artificial brain balanced global optimization on the basis of knowledge learned from the ABNS. The ABNS comprises multiple nerve cords that follow the principle of nervous system specialization, i.e., each nerve cord serves a specific purpose."}, {"title": "B. Experimental Design", "content": "We conducted experiments on the CIFAR datasets using Python 3.8.11 and PyTorch 1.12.0 on a Ubuntu 20.04 workstation with four RTX 3090 graphics cards. The proposed PNN framework was evaluated using two sets of experiments\u2014on CIFAR-10 and CIFAR-100. Each experimental set included 12 models, with five random seeds for CIFAR-10 and seven random seeds for CIFAR-100. Each experiment corresponded to the execution of one model with one seed, and the mean result of each model over the different seeds was considered to be its final result on each dataset.\nThrough systematic random sampling, we selected five and seven initial seeds from the seed lists for CIFAR-10 and CIFAR-100, respectively. Systematic random sampling was employed owing to two advantages: 1) it covers the population more evenly, and 2) it is easier to perform than other alternatives [10]. For the seed population, we used the lower and upper value boundaries of NumPy's seed generator, which ranged between 0 and 2**32\u20131. The interval, k, was calculated by dividing 2**32\u20131 by 60. Each kth integer represented the expected seed number for our experiments. On CIFAR-10, five of the 60 seeds were used for each model, as both the first and second ResNet publications reported model performance in terms of five-run averages [4,5]. For CIFAR-100, seven seeds were used to increase the precision of the experimental results, as CIFAR-100 is considered to be significantly more complex than CIFAR0-10."}, {"title": "III. MODEL SELECTION AND ARCHITECTURE CONSTRUCTION", "content": "At the time of writing, ResNet is one of the most well-established deep neural network families and provides a systematic and robust development foundation for both wide and deep network architectures compared with other deep architectures for learning algorithm families. Consequently, we considered ResNet and its variants as reference deep learning algorithms in this study, inspired by a study conducted by Google Research on deep and wide neural networks [11]. According to Eguyen et al., deep networks perform better on consumer goods, whereas wide networks perform better on classes that represent scenes. Nevertheless, both network types achieved similar levels of accuracy in our study.\nIn this study, we constructed deep and wide neural networks with similar numbers of neurons to evaluate the performance of the proposed models and demonstrated that the improvements in test accuracy were induced by the inclusion of cross-network communication rather than an increase in the number of neurons, which is already known to improve neural network performance [12-15]."}, {"title": "A. Brain, Nerve Cord, and Interaction between Them", "content": "The brain and nerve cords are the most critical components of the proposed PNN. Our experiments involved an artificial brain that enabled nerve cords to exchange weights during training. Most modern ANNs, e.g., ResNet, DenseNet, VGG, Inception, and PeleeNet, implement StemBlocks initially. The PeleeNet study specifically discussed StemBlocks, which enhance the feature expression abilities of a neural network without significantly increasing the computational burden [16]. StemBlocks are usually independent of complicated ConvBlocks in the aforementioned neural networks and have a relatively simple architecture. These characteristics make the StemBlock an ideal weight-exchange portal between the brain and nerve cords in the PNN architecture."}, {"title": "B. Weight Swap Condition", "content": "The weight-swapping mechanism was managed using a patience gate, which is a crucial concept in the PNN framework (Fig. 2.). Better optimization was achieved by controlling weight swapping during training. Algorithm 2 (TABLE II.) describes the interaction of the weight-swapping conditions with the brain and nerve cords."}, {"title": "C. Models used in Experiments on CIFAR-10", "content": "ResNet20, WideResNet14, Ensemble (ResNet20 + WideResNet14), and a PNN (ResNet20+ WideResNet14) were designed, proposed, trained, and evaluated on CIFAR-10. The loss function, optimizer, and scheduler used were cross-entropy, stochastic gradient descent, and cosine annealing, respectively. The weight decay, momentum, initial learning rate, and batch size were 0.0001, 0.9, 0.1, and 128, respectively. Two hundred epochs were considered instead of 64,000 iterations, as reported in the first and second ResNet publications. Among the aforementioned models, ResNet20, WideResNet14, and Ensemble (ResNet20 + WideResNet14) are non-PNN models, whereas PNN (ResNet20 + WideResNet14) is selected as the PNN model. The location of the network within the PNN framework did not affect its performance. The models are described below:\nResNet20: ResNet20 was originally introduced by He et al. [4] in their first ResNet study using the CIFAR-10 dataset. As a baseline, we replicated the ResNet20 model and its results on the basis of the description in the first ResNet paper [4]. The constructed ResNet20 consisted of 6n+2 weighted layers, where n = {3}. Each n represents three basic blocks, each containing two convolution layers. The first convolution layer comprised a 3 \u00d7 3 convolution with a 1 \u00d7 1 stride and 1 \u00d71 padding. A stack of 6n 3 \u00d7 3 convolution layers was subsequently constructed with feature maps of sizes 32, 16, and 8. Each feature map size corresponded to 2n layers. Therefore, 16, 32, and 64 filters were used, respectively. Subsampling in ResNet20 used a stride size of 2 \u00d7 2 pixels. Subsequently, global average pooling was implemented at the end of the neural network with a 10-way fully connected layer. The total number of neurons in ResNet20 was 272,474.\nWideResNet14: We adopted the concept of wide neural networks in this study in the form of wide residual networks (WRNs) [17]. On the basis of WRNs and ResNet20, we developed WideResNet14, which comprises 4n+2 weighted layers, where n = {3}. Each n represents two basic blocks, each containing two convolution layers. The number of filters was set to 32 and 64 to widen the network. The other configurations of WideResNet14 were identical to those of ResNet20. Consequently, the total number of neurons in WideResNet14 was 258,458.\nEnsemble (ResNet20 + WideResNet14): An ensemble-stacking technique was implemented in the ensemble experiment (Fig. 3.). ResNet20 and WideResNet14 were allowed to operate independently. The two models were stacked in parallel, and a Softmax classifier was used to generate the results. Finally, using soft voting with prediction probabilities, the two sets of results were merged and integrated into one set of predictions. The Ensemble model included 530,932 neurons in aggregate."}, {"title": "D. Models used in Experiments on CIFAR-100", "content": "Four types of networks were used on CIFAR-100\u2014ResNet164, WideResNet110, Ensemble (ResNet164 + WideResNet110), and PNN (ResNet164 + WideResNet110). The hyperparameters used in these experiments were identical to those used in the case of CIFAR-10. ResNet164, WideResNet110, and Ensemble (ResNet164 + WideResNet110) were independently used for non-PNN models, whereas the stacked ResNet164 + WideResNet110 combination was considered in the case of PNN to be examined, compared, and analyzed. The first nerve cord in the PNN model was ResNet164, and the second was WideResNet110. The locations of the nerve cords were not significant.\nResNet164: We replicated ResNet164 [4,5] based on the first and second ResNet studies. Its model architecture was nearly identical to that of ResNet20\u2014the only exception was that n = 18 with three bottleneck blocks and the number of classes was changed to 100. Owing to the inclusion of three convolution layers in each bottleneck block, ResNet164 contained 9n+2 weighted layers with 1,727,284 neurons.\nWideResNet110: We constructed WideResNet110 based on design concepts presented in the first and second ResNet studies and the WideResNet study [4,5,17]. WideResNet110 contained six 6n+2 weighted layers, with n = {18}. Each n represents three bottleneck blocks, and each bottleneck block contained two convolutional layers. In addition, the number of filters was set to 32 and 64 to widen the network. The other configurations of WideResNet110 were identical to those of WideResNet14. In aggregate, WideResNet110 contained 1,637,428 neurons.\nEnsemble (ResNet164 + WideResNet110): We constructed the Ensemble (ResNet164 + WideResNet110) network using the neural network architectural methodologies of ResNet, WideResNet, and ensemble learning. The ensemble models, ResNet164 and WideResNet110, were trained independently. After training and validation, ResNet164 and WideResNet110 were stacked as ensemble models. The soft voting technique of Ensemble (ResNet164 + WideResNet110) is identical to that of Ensemble (ResNet20 + WideResNet14). On the basis of the prediction probabilities, Ensemble (ResNet164 + WideResNet110) produced one set of predictions. This Ensemble model comprised 3,364,712 neurons.\nPNN (ResNet164 + WideResNet110): PNN (ResNet164 + WideResNet108) was constructed using a similar configuration as PNN (ResNet20 + WideResNet12). It exhibited identical global epochs, subepochs, and weight-swapping principles but was deeper than the latter. ResNet164 and WideResNet108 were stacked together as two nerve cords with brain interactions using StemBlocks. Each nerve cord generated its own results, and soft voting was used to merge the two sets of predicted results into one. The PNN model contained 3,364,712 neurons."}, {"title": "IV. RESULTS", "content": "For CIFAR-10 and CIFAR-100, five and seven seeds were used per model, respectively, and one seed was used per run. The models were evaluated via methods discussed in the first and second ResNet publications, with modifications presented in this section [4,5]. For CIFAR-10 and CIFAR-100, He et al. [4,5] suggested that the mean and median over five runs, respectively, adequately represented model performance. In this study, the number of CIFAR-100 runs per model was increased to seven to increase the precision of the results. The results obtained on CIFAR-10 and CIFAR-100 are presented in Tables III, IV, V, and VI."}, {"title": "A. Experimental Results on CIFAR-10", "content": "The experimental results on the CIFAR-10 dataset are presented in Tables III and IV. These tables list the neural networks and the number of networks used in each model, indicating that each model was trained and tested using five different seeds. Table III lists the test error rates of single networks with five seeds, where each model comprises only one neural network. Table IV lists the test error rates of dual networks with five seeds, where each model comprises two neural networks.\nSingle Network Results: Eight models were used in a single network. The baseline models were ResNet20 and WideResNet14. The remaining six models were trained via the PNN framework, in which each PNN comprised two neural networks. Each PNN was considered in terms of two independent models to observe the effect of the PNN architecture on the prediction performance of individual neural networks. On CIFAR-10, after the PNN models were fine-tuned, we presented the PNN5, PNN10, and PNN15 models, each containing ResNet20 and WideResNet14, with patience gate values of 5, 10, and 15, respectively. The five-run averages for ResNet20, WideResNet14, PNN5 ResNet20, PNN5 WideResNet14, PNN10 ResNet20, PNN10 WideResNet14, PNN15 ResNet20, and PNN15 WideResNet14 are listed in TABLE III.\nIn TABLE III, the underlined numbers represent averages over the five seeds in experiments on CIFAR-10. The average error rate of PNN15's ResNet20 was observed to be 7.27, which was the best among the eight models. The average error rate of PNN15's WideResNet14 was the second best. As the patience gate value increased, the five-run error average decreased steadily."}, {"title": "Dual Network Results", "content": "Each dual network comprised four models. The baseline model was an Ensemble (ResNet20 + WideResNet14) model. In addition to the baseline model, PNN5 (ResNet20 + WideResNet14), PNN10 (ResNet20 + WideResNet14), and PNN15 (ResNet20+WideResNet14) were included, with fine-tuned patience gate values of 5, 10, and 15, respectively. The five-run average errors of Ensemble (ResNet20 + WideResNet14), PNN5 (ResNet20 + WideResNet14), PNN10 (ResNet20 + WideResNet14), and PNN15 (ResNet20 + WideResNet14) were 5.98, 6.01, 5.94, and 5.81, respectively.\nAmong the four dual networks listed in TABLE IV, PNN15 (ResNet20 + WideResNet14) exhibited the best five-run average error of 5.81 compared with the baseline model's 5.98. We also observed a decreasing trend in the five-run average test error as the patience gate value increased."}, {"title": "B. Experimental Results on CIFAR-100", "content": "The results on CIFAR-100 are presented in Tables V and VI, which list the neural networks used in each model and the number of networks involved in each model, indicating that each model was trained and tested using seven different seeds. We evaluated the models in terms of three measures-the mean, trimmed mean, and median. The trimmed mean was calculated after removing the highest and lowest values from the original test results. TABLE V lists the test error rates of the eight single networks for seven seeds. TABLE VI presents the test error rates of the four dual networks-one ensemble and three PNN methods for seven seeds."}, {"title": "Single Network Results", "content": "Eight models were used in the single-network architecture. The two baseline models were ResNet164 and WideResNet110. Three pairs of ResNet164 and WideResNet110 were trained within the PNN framework with patience gate values of 10 (henceforth referred to as PNN10), 15 (henceforth PNN15), and 20 (henceforth PNN20), respectively. We treated each neural network embedded in the PNN framework as an independent model to observe the effect of the PNN architecture on the prediction performance of individual neural networks. TABLE V presents the seven-run errors for ResNet164, WideResNet110, PNN10 ResNet164, PNN10 WideResNet110, PNN15 ResNet164, PNN15 WideResNet110, PNN20 ResNet164, and PNN20 WideResNet110.\nPNN15's WideResNet110 performed the best among the eight single-network models in terms of the mean, trimmed mean, and median. On CIFAR-100, fine-tuning the patience gate values was observed to improve the test error rates."}, {"title": "Dual Network Results", "content": "TABLE VI lists the test error rates of the four dual network models with seven seeds on CIFAR-100. The baseline model was an ensemble (ResNet164+WideResNet110), and the proposed models were PNN10 (ResNet164+WideResNet110), PNN15 (ResNet164+WideResNet110), and PNN20 (ResNet164+WideResNet110). The PNN models were fine-tuned using patient gate values of 10 (PNN10), 15 (PNN15), and 20 (PNN20).\nThe seven-run average errors of the Ensemble (ResNet164+WideResNet110), PNN10 (ResNet164 + WideResNet110), PNN15 (ResNet164 + WideResNet110), and PNN20 (ResNet164+WideResNet110) models were 20.79, 21.36, 20.82, and 20.67, respectively. Their seven-run trimmed mean values were 20.71, 20.92, 20.78, and 20.55, respectively, and their seven-run medians were 20.67, 20.77, 20.64, and 20.46, respectively. In terms of these metrics, PNN20 (ResNet164 + WideResNet110) performed the best among the four models. The results presented in TABLE VI demonstrate that fine-tuning the PNN framework reduced test error rates, which correlated with an increase in the patient gate value."}, {"title": "V. DISCUSSION", "content": "The experimental results obtained on the CIFAR-10 and CIFAR-100 datasets were analyzed via the nonparametric Kruskal-Wallis H test (h test) to define statistically significant differences between multiple models and the Mann-Whitney U test (u test) to determine statistically significant differences between pairs of models. Owing to greater data complexity, the results corresponding to CIFAR-100 were more likely to not be normally distributed. The prediction performances on CIFAR-10 and CIFAR-100 were measured in terms of five-run medians, as in the second ResNet publication [5]. In this study, we utilized nonparametric tests of medians, which were deemed appropriate for this type of data.\nThe H test assumes ordinal or continuous variables, independent samples, and sufficient data [18]. Assumption 1 required ordinal or continuous observations of hierarchical relationships. This was satisfied-in experiments on both CIFAR-10 and CIFAR-100, the test errors were continuous observations with hierarchical relationships. In this hierarchy, lower outputs corresponded to higher quality. Assumption 2 requires that observations be independent samples. This was also satisfied as the sample data were derived from the models introduced in this study. Each model generated its own results independently of the others. The final assumption was the availability of sufficient data. For the experiments on CIFAR-10, five samples were generated per group, whereas for those on CIFAR-100, seven samples were generated per group.\nThe assumptions of the U test were that the two groups must be independent and the dependent variable must be ordinal or continuous [19]. These were satisfied by the experimental design on both CIFAR-10 and CIFAR-100, as each model generated its own data to be used in the U tests, thus ensuring independence. Further, the sample data generated in the experiments were continuous because they were test errors that measured model performance.\nHence, all the experimental results satisfied the assumptions of the H and U tests. The following discussion focuses on the results of visual observations, H tests, and U tests."}, {"title": "A. Discussion of Results on CIFAR-10", "content": "Visual Observation: The experimental results on CIFAR-10 were plotted jointly to demonstrate the error rates of all the models, with both single and dual networks. The eight models with a single-network architecture presented a mean error rate of 7.49, whereas the four models with a dual-network architecture presented a mean error rate of 5.93. Because of the obvious mean differences between the single and dual networks described below (Fig. 4.), we decided to analyze the two cases separately to determine the impact of the PNN framework on model performance."}, {"title": "Single Network Architecture", "content": "The single-network PNN models exhibited certain advantages over the baseline models (Fig. 5.). To verify the visual observations, H tests and U tests were conducted.\nThe H-test hypotheses on single network models during experiments on CIFAR-10 are listed below:\n$H_o: X_1 = X_2 = X_3 = X_4 = X_5 = X_6 = X_7 = X_8$\n$H_1: at least one population median value is different from those of other models$\nThe population medians of the eight experimental models tested on CIFAR-10 are denoted by $X_1, X_2, X_3, X_4, X_4, X_6, X_7$, and $X_8$ in the H test null hypothesis.\nThe H statistic is given by:\n$H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N + 1)$                                                                                                (1)\nwhere N denotes the total number of observations, k denotes the number of models or groups in the CIFAR-10 experiments, $R_i$ denotes sample i's sum of ranks per group, and $n_i$ denotes the number of observations per group. Using SciPy's statistical function for the H test, the H statistic was calculated to be 8.27, with a p value of 0.31. Given a = 0.05, which was less than the p value of 0.31, there was not enough evidence to reject the null hypothesis and declare the differences between the test errors of the eight models to be significant.\nHowever, the differences between the best-performing PNN and baseline models still had to be investigated. Hence, we performed U tests on two pairs of comparisons-ResNet20 vs. PNN15's ResNet20 and Wide ResNet14 VS. PNN15's WideResNet14-to evaluate whether the PNN framework improves model performance. The hypotheses of the U test are listed below, where $x_b$ represents the population median of the baseline models (ResNet20 or WideResNet14) and $x_p$ represents the population median of the proposed models (PNN15's ResNet20 or PNN15's WideResNet14). Therefore, the hypothesis testing statements were as follows:\n$H_o: X_b = X_p$, two population medians are identical\n$H_1: X_b \\neq X_p$, two population medians are different\nFor the first pair of U tests, we considered a sample of nb observations {7.5, 7.38, 7.57, 7.49, 7.24} for ResNet20 and a sample of np observations {7.38, 7.02, 7.22, 7.24, 7.48} for PNN15's ResNet20. Using SciPy's statistical function for the U test, we generated a statistic of 21.00 with a p value = 0.09. Given a = 0.05, the null hypothesis that ResNet20 and PNN15's ResNet20 are identical could not be rejected. Thus, ResNet20's test errors were deemed to not be significantly different from those of PNN15's ResNet20.\nFor the second pair of U tests, we considered a sample of $n_b$ observations {7.51, 7.66, 7.25, 7.48, 7.89} for WideResNet14 and a sample of $n_p$ observations {7.49, 7.26, 7.22, 7.63, 7.24} for PNN15's WideResNet14. Using SciPy's statistical function for the U test, we generated a U statistic of 19.00 with a p value = 0.22. Given a = 0.05, the null hypothesis that WideResNet14 and PNN15's WideResNet14 are identical could not be rejected. Therefore, the difference between WideResNet14 and PNN15's WideResNet14 was judged to be insignificant with respect to test errors."}, {"title": "Dual Network Architecture", "content": "The four dual network models depicted in Fig. 6. exhibited interesting distinctions. H and U tests were performed to measure the statistical significance of the differences between them on CIFAR-10.\nThe H test hypotheses for dual network models on CIFAR-10 are listed below:\n$H_o: X_1 = X_2 = X_3 = X_4$\n$H_1: at least one population median is different from those of the other models$\nBased on (1) and using SciPy's statistical function for the H test, we generated a statistic of 9.45 with a p value of 0.02. Given a = 0.05, we rejected $H_o$ to conclude that at least one population median of a model was statistically different from those of other models.\nNext, we evaluated the significance of the differences between the best fine-tuned model, PNN15, and the baseline Ensemble model on CIFAR-10 using the U test. The hypotheses of the U test are listed below, where $x_b$ represents the population median of the baseline Ensemble (ResNet20 + WideResNet14) model and $x_p$ represents the population median of the proposed PNN15 (ResNet20 + WideResNet14) model.\n$H_o: X_b = X_p$, two population medians are identical\n$H_1: X_b \\neq X_p$, two population medians are different"}, {"title": "B. Discussion of Results on CIFAR-100", "content": "Visual Observation: The experimental results on CIFAR-100 were plotted jointly to illustrate the error rates generated by all 12 models with both single and dual network architectures (Fig. 7.). The eight models with a single-network architecture exhibited a mean error rate of 23.53, whereas the four with a dual-network architecture presented a mean error rate of 20.91. Owing to the obvious error differences between the mean errors of the single and dual network models, we decided to analyze the two cases separately."}, {"title": "Single Network Architecture", "content": "Based on the data visualization presented in Fig. 8.", "following": "n$H_o: X_1 = X_2 = X_3 = X_4 = X_5 = X_6 = X_7 = X_8$\n$H_1: at least one population median is different from those of the other models\nThe null hypothesis stated that all the single networks have identical population medians. By contrast, the alternative hypothesis claimed that at least one population median was significantly different from the others. Using SciPy's H test function in (1), we generated an H statistic of 4.82 with a p value of 0.68. Given a = 0.05, there was not sufficient evidence to reject the null hypothesis.\nHowever, we observed a moderate performance improvement in the proposed models. As discussed in TABLE V, each baseline model contained three comparable PNNs. First, ResNet164 was compared with PNN10's ResNet164, PNN15's ResNet164, and PNN20's ResNet164. Among the four ResNet164 models, PNN10's ResNet164 exhibited the lowest test error mean (23.35), the second-best trimmed error mean (23.60), and the best test error median (23.56). In comparison, ResNet164 exhibited a test error mean of 23.49, a trimmed error mean of 23.56, and a test error median of 23.64.\nNext, WideResNet110 was compared with PNN10's WideResNet110, PNN15's WideResNet110, and PNN20's WideResNet110. Among these four WideResNet110 models, PNN15's WideResNet110 exhibited the lowest test error mean (22.23), the lowest trimmed error mean (23.01), and the lowest error median (22.58). In comparison, WideResNet110 exhibited a test error mean of 23.64, a test trimmed error mean of 23.53, and a test error median of 23"}]}