{"title": "IoT-LM: Large Multisensory Language Models\nfor the Internet of Things", "authors": ["Shentong Mo", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Paul Pu Liang"], "abstract": "The Internet of Things (IOT) network integrating billions of smart physical devices\nembedded with sensors, software, and communication technologies is a critical and\nrapidly expanding component of our modern world. The IOT ecosystem provides a\nrich source of real-world modalities such as motion, thermal, geolocation, imaging,\ndepth, sensors, and audio to recognize the states of humans and physical objects.\nData-driven tools present a rich opportunity to automatically process IoT data at\nscale, enabling efficient inference for understanding human wellbeing, controlling\nphysical devices, and interconnecting smart cities. To realize this potential, we\nintroduce IOT-LM, an open-source large multisensory language model tailored for\nthe IOT ecosystem. IOT-LM is enabled by two technical contributions: the first\nis MULTIIOT, the most expansive unified IOT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory pre-\ntraining and instruction-tuning. The second is a new multisensory multitask adapter\nlayer to condition pre-trained large language models on multiple multisensory\nIOT tasks simultaneously, enabling the sharing of information across modalities\nand tasks for better generalization. Not only does IoT-LM yield substantial\nimprovements on 8 supervised IOT classification tasks, but it also demonstrates\nnew interactive question-answering, reasoning, and dialog capabilities conditioned\non IOT sensors. We release IOT-LM's data sources and new multisensory language\nmodeling framework at the repository 1.", "sections": [{"title": "Introduction", "content": "The digital world is witnessing an unprecedented surge in the realm of the Internet of Things\n(IOT): the ever-growing system of interlinked devices from individual households to vast industrial\ncomplexes [8]. These devices are often embedded with sensors, software, and communication\ntechnologies that can safely and privately analyze the human and physical world [36, 50]. For\nexample, high-fidelity sensors are able to recognize physical activities to inform us of our daily\nphysical wellness [47, 55]; vision, depth, and lidar sensors are able to navigate self-driving cars\nand connect them with traffic lights for efficient transport management [27, 31]; and wifi, depth,\ncamera sensors can detect if the elderly require assistance in hospitals [2, 34]. As a result, there\nhas been substantial interest in building machine learning systems that can efficiently process these\nIOT sensors and make predictions, which has great potential for understanding human wellbeing,\ncontrolling physical devices, and interconnecting smart cities [1, 20, 52, 56].\nHowever, most existing machine learning approaches in the IOT domain have largely focused on\nsupervised models, trained only on a single input sensory modality and for a single output prediction\ntask [3, 6, 23, 28, 33]. We extend the existing ML for IOT paradigms in two directions. Firstly,\nin the input space, we investigate how to learn from multiple heterogeneous and interacting IOT\nsensory modalities simultaneously. This is important since practical IOT scenarios often see multiple"}, {"title": "Related Work", "content": "We cover related work in the design and applications of IOT sensors, how machine learning can be\nused to accelerate IOT perception, and related background work in multisensory machine learning\nand foundation models.\nInternet of Things (IOT): The pursuit of extracting meaningful insights from IOT data [7, 30, 35, 13]\nhas led to various innovative approaches that focus on individual modalities and specific tasks [17, 10,\n4] and resource-constrained devices [15, 32, 24]. For instance, DIP-IMU [23] effectively fuses depth\nsensing with IMU data for enhanced pose estimation, demonstrating the potential of multimodal\nintegration. Similarly, EyeMU [33] utilizes time-series neural networks to process IMU sensor data\nfor accurate gaze tracking on mobile devices. TouchPose [3] explores tactile data for pose recognition,\nwhile LLVIP [28] focuses on leveraging visual data from IOT environments for dynamic real-world\napplications. Furthermore, RGBDGaze [6] integrates RGBD data for gaze estimation, highlighting\nthe diversity of sensory data applications. However, these approaches generally remain confined to\nsingle-task models and lack the capability to generalize across multiple IOT modalities and tasks,\nan area where our work with IoT-LM introduces a significant advancement by enabling statistical\nsharing and generalization across a broad spectrum of IOT data.\nMultisensory machine learning: The field of multimodal machine learning has seen substantial\ngrowth, with models designed to integrate inputs from various sensory channels to perform more\ncomplex interpretation and interaction with the environment [9, 42]. Notable works include multi-\nmodal transformers [54, 44] that fuse visual, auditory, and textual data to improve learning efficacy.\nThese multimodal transformers have also been scaled for self-supervised pre-training across an\nincreasing range of modalities including time-series and sensors [25, 41, 49]. While these models\noffer a foundation for integrating diverse data types, they often do not address the unique challenges\nposed by IOT environments, such as the integration of non-traditional sensor data (e.g., IMU, thermal\nsensors) and the need for models to interact dynamically with physical environments.\nFoundation models: Recently, the concept of foundation models [11, 12, 5], pre-trained on large-\nscale datasets and adaptable to a wide range of tasks [57, 29, 21], has gained traction. These models,\nexemplified by works such as GPT-4 [46] and LLaMA [53], offer a robust starting point for further\nfine-tuning on individual natural-language tasks. There has also been a recent drive towards large\nmultimodal models, using either LLMs as a starting point and training adapters from other modalities\nto LLM input space [16, 57], or training multimodal transformers from scratch (sometimes called\n'natively') with interleaved language tokens, image frames, audio frames, and other modalities [21, 46].\nOur approach extends these paradigms into the IOT domain, where IOT-LM adapts a large language\nmodel framework to not only understand textual information but also effectively process and reason"}, {"title": "IOT-LM: A New Multisensory IOT Foundation Model", "content": "In this section, we describe the architecture, data curation, and training innovations in IoT-LM."}, {"title": "IOT-LM Architecture: Multisensory Multitask Adapter", "content": "IOT-LM's backbone consists of a variety of IOT sensory inputs, a general-purpose multisensory\nmultitask encoder that fuses the information from multiple sensors and across multiple tasks, a\nmultisensory multitask adapter that transforms encoded representations into pretrained LLM input\nspace, and the pretrained LLM itself. We show an overview of the IOT-LM architecture in Figure 1.\nMultisensory multitask encoder for heterogeneous IOT signals The encoder is trained using\na novel approach that effectively manages the heterogeneity of IoT data by combining supervised\nlearning with unsupervised feature extraction techniques. This training involves task-specific adapta-\ntions, where the encoder learns to map raw sensor data to an intermediate feature space conducive for\nlanguage model processing. The encoder handles the multisensory data through a combination of\nmultimodal fusion methods, including early, late, and model-based fusion, which are chosen based\non the data characteristics and the specific requirements of each task.\nSpecifically, the adapters transform the sensor data into a representation that is more conducive for\nthe language model to process. For each sensor modality xi, we employ a dedicated encoder Ei\nthat maps raw sensor data to an intermediate feature space. The encoded features from different\nmodalities are then combined using a fusion mechanism within the adapter, allowing the model to\nharness information from all available sensors. We employ late fusion methods that are extracted from\neach sensor modality and are independently processed and only combined at decision-making layers,\nfacilitating specialization in feature extraction while leveraging multimodal data for final predictions.\nThese fusion techniques are selected and fine-tuned based on the specific characteristics of the\ndata and the tasks at hand, ensuring optimal performance. Our fusion methods innovations involve\ndeveloping new model-based fusion techniques that adaptively adjust the weighting of different\nsensor inputs based on their predictive value for specific tasks, a novel approach to handling IoT data\nheterogeneity. The training of these components is conducted through a multitask supervised learning\nframework that emphasizes individual task accuracy and enhances generalizability across tasks by\nsharing representations and leveraging common patterns found in the diverse IoT data landscape.\nThis method strengthens the model's ability to perform well across various conditions and tasks,\nmaking it highly effective for real-world IoT applications."}, {"title": "IOT-Language Data Collection", "content": "To train IOT-LM, we aggregate and release the largest IOT dataset for machine learning research,\ncomprising 1.15 million samples from 12 different sensory modalities for 8 different real-world IOT\nprediction tasks related to personal wellness, healthcare, smart cities, and more. These modalities\ninclude Inertial Measurement Units (IMU), thermal sensors, GPS, LiDAR, gaze, pose, capacitance\nsensors, and traditional modalities such as images, audio, and video. Tasks supported include:\n1. Gaze estimation: For human-computer interaction, driver monitoring, and virtual reality appli-\ncations, our dataset includes RGB images of faces, depth data, and IMU outputs. The model is\ntasked with predicting X/Y coordinates for gaze tracking, necessitating a deep understanding of\nthe interactions between these modalities.\n2. Depth estimation: This task involves estimating the distance from cameras to objects in images,\nvital for AR/VR, robotics, and object detection. Our dataset includes RGB images combined with\ncamera parameters, GPS coordinates, and IMU data to create depth maps for various scenarios,\nincluding street scenes and robotic hand interactions.\n3. Gesture classification: Key to enhancing human-machine interfaces, this task uses data from\ngaze tracking and IMU sensors (accelerometer, gyroscope, and orientation) to classify human\ngestures. The challenge here is to accurately interpret the nuanced cross-modal interactions.\n4. Pose estimation: This task determines the arrangement of human joints, using RGB images and\nIMU data to predict poses of the human body, including 24 joints with three angles each (yaw,\npitch, roll). This requires the model to fuse data from IMUs and visual inputs.\n5. Touch contact: To improve touch-based device interactions, this task classifies the type of touch\non capacitive surfaces using RGB and capacitive images, depth maps, and hand poses."}, {"title": "Multisensory Multitask Encoder Pretraining", "content": "Building on the IOT-LM architecture and data resources, we now describe the pre-training and\ninstruction tuning stages in IOT-LM. The pre-training stage aims to learn a general-purpose multisen-\nsory multitask encoder that fuses the information from multiple sensors and across multiple tasks.\nDuring pretraining, the multisensory multitask encoder is trained on a combination of supervised\nlearning tasks to extract and fuse features from diverse sensor data effectively. This stage is pivotal in\npreparing the model to process and understand complex sensory inputs, which can be mathematically\ndescribed as follows:\n$\\Theta_{\\text{pre}} = \\underset{\\Theta}{\\text{arg min}} \\sum_{k=1}^{K} \\sum_{(x_k,Y_k) \\in D_k} L_k (M_{\\Theta}(E_k(x_k)),Y_k),$ (2)\nwhere @ represents the parameters of the entire network including the adapters, Dk is the dataset\nconsisting of input-output pairs (xk, Yk), Ek(xk) represents the encoded inputs from various sensors\nfor task k, and Lk denotes the loss function used to measure the discrepancy between the model's\npredictions and the true outputs."}, {"title": "IOT-LM Instruction Tuning", "content": "During the instruction-tuning phase, IOT-LM is trained to understand and interpret multisensory IoT\ndata contexts and perform specific tasks based on directed language inputs. These tasks can include\nmaking a prediction, answering a question, holding a dialog, reasoning about physical properties,\nand so on. This stage is crucial for refining the model's ability to follow complex and varied human\ninstructions. The instruction tuning phase is guided by the following optimization:\n$\\Theta_{\\text{tune}} = \\underset{\\Theta}{\\text{arg min}} \\sum_{k=1}^{K} \\sum_{(x_k,C_k,Y_k) \\in T_k} L_k (M_{\\Theta}(E_k(x), C_k), Y_k),$ (3)\nwhere Te represents the task-specific dataset with inputs xk, context or commands ck, and outputs Yk\nfor task k. Here, Ek(xk) denotes the encoded sensor data for task k, ck provides specific instructions\nor task directives, and Me is the model parameterized by @ including the tuned adapters. This phases\nensure that IOT-LM not only learns to process a wide array of sensor data but also understands and\nexecutes complex commands pertinent to IOT applications, thus achieving high performance across\nvaried IOT tasks."}, {"title": "Experiments", "content": "Our experiments aim to benchmark the performance of IOT-LM on supervised IOT classification\ntasks, as well as their reasoning, dialog, interaction, and zero-shot and few-shot transfer abilities\nacross different modalities and tasks."}, {"title": "Experimental Setup", "content": "Experiments were conducted using NVIDIA A100 GPUs, ensuring high-performance computation\nfor our deep learning models. The models were trained for 30 epochs using the Adam optimizer with\na learning rate of 1e-4 and a batch size of 16. We compare to the following baselines:\n\u2022 Unimodal models [33, 3, 45] processed data from single sensor types using tailored neural\narchitectures for each IOT domain data,\n\u2022 Unimodal Adapter models [16] utilized deep architectures such as LLaMA-adpater [16] with\nadapter layers, fine-tuning only the adapter modules with a learning rate of 0.0005,\n\u2022 Unimodal multi-task models [39] employed shared encoder layers and task-specific decoders,\nensuring balanced gradients among tasks,\n\u2022 Multisensory models [26], where data fusion can occur at varying levels, from input to decision,\nand models ensured balanced data representation from each modality,\n\u2022 Multisensory multitask models [41] utilized modality-specific encoders followed by task-specific\ndecoders, balancing both modalities and tasks during training. Each method's efficacy was validated\non respective datasets.\nTo evaluate performance, we employ task-specific metrics. For gaze and pose estimation, we measure\nthe mean euclidean error in centimeters between predictions and ground truth. Depth estimation\nutilizes mean absolute error in millimeters, while gesture classification, touch contact classification,\nand activity recognition rely on accuracy metrics. Event detection employs the F1 score for confident\nthreshold predictions, and 3D pose reconstruction is assessed using the End-point-error in millimeters\nfor joint discrepancies."}, {"title": "Main quantitative results", "content": "Overall performance: Table 1 reports the quantitative results of IOT-LM compared to state-of-the-\nart domain-specific, single modality, single task, multimodal multitask, and adapter and alignment\nmodels. As seen in Table 1, the IoT-LM consistently outperforms the single modality and single task\nmodels across all tasks. This can be attributed to their ability to integrate textual information across\nmodalities and tasks, which is especially crucial when one modality might have noisy or incomplete\ndata. While the adapter and multimodal multitask models show commendable performance due\nto their ability to adapt to new tasks, they often fall short in scenarios where multiple modalities\nhave to be processed simultaneously. IoT-LM improves upon models that only perform multimodal\nmultitask supervised learning, since it inherits the prediction and reasoning capabilities from the\npretrained large language model component.\nPerformance across increasing modalities: In this section, we study how adding additional modali-\nties to IOT-LM impacts performance. From Table 2, we find significant performance improvements"}, {"title": "Qualitative analysis", "content": "In this section, we qualitatively demonstrate the dialog and interactive capabilities of IOT-LM on\nprocessing audio and Inertial Measurement Unit (IMU) data.\nAudio: For audio data, in Figure 4, IOT-LM was presented with an audio waveform and instructed\nto identify the corresponding activity from a list of categories. The model was able to discern that\nthe audio signal most closely resembled the category \"Coughing.\" The choice was justified by the\nsudden and sporadic nature of the spikes in the waveform, which align with the acoustic signature of\na cough. This example highlights the model's capability to process temporal acoustic features and\ncorrectly categorize and reason about it.\nIMU: For IMU data, in Figure 5, IoT-LM analyzed a graph with multiple lines, each representing\ndifferent sensor readings from the IMU. The task was to select an activity from a list that best matched\nthe IMU data pattern. The model identified \"Knocking\" as the most likely activity, reasoning that the\nregular intervals of peaks were indicative of a repetitive action with varying force, which is consistent\nwith the pattern of knocking.\nThese examples demonstrate the multisensory multi-task adapter's analytical proficiency in interpret-\ning and classifying data from different IOT modalities based on their temporal characteristics. Such\ncapability is instrumental in realizing the full potential of IOT systems, where understanding and\nacting upon such heterogeneous data in real time is essential for various applications, from smart\nhomes to industrial automation. The examples provided also show the model's potential in bridging\nthe gap between raw sensor data and meaningful insights, enabling non-expert users to interact with\nand benefit from complex IOT systems."}, {"title": "Conclusion and Broader Impacts", "content": "This paper presents IOT-LM, a new large multisensory language model with multisensory perception\nand natural language interaction capabilities over a spectrum of IOT modalities and applications. Key\ninnovations in IoT-LM include a new multisensory multitask adapter to simultaneously condition\npretrained LLMs on multiple multisensory IoT tasks for better generalization, as well as a new\nresource of 1.15 million IoT sensor - natural language paired samples covering 12 modalities and 8\nreal-world tasks. Overall, IOT-LM not only advances the state-of-the-art in IOT predictive learning\nbut also enables new question-answering, reasoning, and interactive dialog capabilities on physical\nsensor data. Future work should focus on expanding the model's capabilities to more IOT modalities\nand tasks, improving its reasoning and robustness, and exploring the implications of its deployment\nin real-world IOT systems. We hope that IOT-LM will inspire further innovations at the intersection\nof machine learning and IOT, contributing to smarter and more responsive technologies that can\nunderstand and interact with their environments."}, {"title": "Details Regarding IOT Datasets", "content": "We collected diverse data from IoT devices, such as Inertial Measurement Units (IMU), Thermal\nsensors, and Global Positioning Systems (GPS). Furthermore, we include challenging modalities,\nsuch as capacitance, depth, gaze, and pose. Finally, we collect common and widely used image, audio,\nand video modalities. These modalities bring unique challenges since they typically involve noisy\nreal-world sensor measurements, that lack explicit tokenization and alignment with other modalities\nthat we typically expect from conventional multimodal image-text research.\nIMU: Inertial Measurement Units capture 3D motion and orientation. This data is fundamental for\nvarious applications, including motion tracking and navigation. We collected 2,940 IMU samples from\nEyeMU [33] for gaze estimation and motion gesture classification, where they used the accelerometer\nand gyroscope raw values sampled at 60 Hz as the IMU values per-axis. 28,400 IMU instances are\nincluded from SAMOSA [45] to save synchronized streams of the 9-axis IMU data (accelerometer,\ngyroscope and orientation) at 50 Hz by using a Fossil Gen 5 smartwatch running Google Android\nwearOS 2.23. Further, we sampled 160,120 IMU samples (9-axis) recorded by the device motion\nsensor using an iOS application [6] on Apple iPhone X for gaze tracking. For human bodies, 330,178\nIMU orientation recordings [23] from 17 sensors on different body parts are saved for pose estimation\nand activity recognition. For first-person videos in Ego4D [22], we used 510,142 timestamps-based\nIMU samples with the normalized accelerometer and gyroscope values in each video for activity\nrecognition. For IMU data on self-driving cars, we collected 41,000 samples from KITTI [18] for\ndepth estimation.\nThermal: Thermal modality data provide temperature radiance insights, crucial in surveillance. For\ncollection, we used 12,025 Thermal samples from LLVIP [28] containing many pedestrians and\ncyclists from different locations on the street between 6 and 10 o'clock in the evening. They used\nHIKVISION DS-2TD8166BJZFY-75H2F/V2 as the camera equipment, a binocular camera platform\nconsisting of an infrared camera with a wavelength of 8 ~ 14um.\nGPS: Global Positioning Systems offer location data with high precision. This data is invaluable for\ntasks like location-based services, asset tracking, and navigation. For GPS data on self-driving cars, we\ncollected 41,000 samples from KITTI [18] using OXTS RT3003 inertial and GPS navigation system\nfor depth estimation. The geographic coordinates include global orientation, altitude, velocities,\naccelerations, angular rates, and satellite information. Following the original dataset, we applied two\nspecified 3-axis coordinates as accelerations for the vehicle and angular rates to describe the tangent\nplane of the earth's surface corresponding to the geographic location.\nCamera: Cameras provide visual data, capturing the environment in rich detail. They serve as the\nbackbone for countless computer vision tasks. For Camera data, we collected 41,000 instances from\nKITTI [18] using a Velodyne laser scanner installed on a vehicle car for depth estimation. We stored\nits 3-axis coordinate and an additional reflectance value for each point. The timestamp-based points\ncan be considered according to the scanner's continuous rotation on its vertical axis, which provides\na complementary context to GPS/IMU systems for auto-driving.\nCapacitance: Capacitive sensors measure changes in capacitance to detect nearby objects or changes.\nThis is foundational for touchscreen technologies and proximity sensing. For capacitance data, we\nused 65,374 samples from TouchPose [3] using a 39.6 cm capacitive Crystal Touch panel (Ocular\nTouch, Dallas, TX), 16-bit touch digitizer, and cameras to record ground-truth data. When fingers\napproach the lines on the mutual-capacitance touch sensor, it causes a capacitance drop between lines,\nresulting in the mutual-capacitance image.\nDepth: Depth sensors measure distances between the sensor and objects, providing a 3D view of the\nenvironment. They play a significant role in tasks like object detection and scene reconstruction. For\ndepth data, we collected 160,120 samples from RGBGaze [6] using Apple iPhone X with a TrueDepth\ncamera (640 x 480 depth map interpolated from a 170 \u00d7 170 IR dot pattern). The participants were\nasked to look at a target (red dot) that was moving on the screen. While the user gazed at the target,\nthe depth imagery was logged at approximately 8 Hz, along with the ARKit gaze prediction. For\ntouch depth data, we used 65,374 samples from TouchPose [3] recorded by a ToF depth camera"}, {"title": "Eight Well-defined and Challenging Tasks", "content": "Our benchmark includes tasks that reflect real-world IoT challenges and that will drive the community\ntowards solutions with tangible societal impacts.\nGaze estimation: This task is pivotal for human-computer interaction, driver monitoring, and virtual\nreality. Given RGB images of faces, depth and IMUs, our goal is to predict the location (X/Y) for"}, {"title": "Experimental Setup", "content": "\u2022 Data Preparation: Each modality, e.g., RGB images, capacitive images, or hand pose, is pre-\nprocessed independently. The data undergo normalization and any specific transformations\ntailored to that modality.\n\u2022 Network Architecture: Distinct neural architectures optimized for each modality type,\nsuch as CNNs for images and RNNs for sequential data.\n\u2022 Training Details: Models are trained using a batch size of 128, employing the Adam\noptimizer with a learning rate of 0.001. Early stopping with a patience of 10 epochs ensures\nprevention from overfitting.\n\u2022 Evaluation: Each unimodal model is evaluated on its respective validation dataset to gauge\nperformance."}, {"title": "Setup for Adapter Models", "content": "\u2022 Data Preparation: Data is fed through a pre-trained network, where only the adapter\nmodules are trainable.\n\u2022 Network Architecture: Utilizing deep architectures like LLaMA [16], but with adapter\nlayers inserted in-between the pre-defined layers.\n\u2022 Training Details: Since only the adapter layers are trainable, fewer parameters are updated,\nallowing for a larger batch size of 256. The training uses the Adam optimizer with a learning\nrate of 0.0005.\n\u2022 Evaluation: Model performance is assessed by evaluating the fine-tuned model on the\ntargeted task's validation set."}, {"title": "Setup for Unimodal Multi-task Models", "content": "\u2022 Data Preparation: Data from different tasks, but the same modality, are concatenated or\npaired.\n\u2022 Network Architecture: Shared encoder layers process the input data, followed by task-\nspecific decoders.\n\u2022 Training Details: Gradient balancing techniques are employed to prevent one task from\ndominating the training process. Training leverages a batch size of 128 and the Adam\noptimizer with a learning rate of 0.001.\n\u2022 Evaluation: Performance is evaluated separately for each task on their respective validation\nsets."}, {"title": "Setup for Multisensory Models", "content": "\u2022 Data Preparation: Data from different modalities are fused either at the input, feature, or\ndecision level.\n\u2022 Network Architecture: Modality-specific encoders process each input type. Fusion layers\nthen combine features from all encoders.\n\u2022 Training Details: Models are trained with a batch size of 128 using the Adam optimizer\nand a learning rate of 0.001. Data balancing techniques ensure equal representation from\neach modality.\n\u2022 Evaluation: The combined model's efficacy is evaluated using a validation dataset that\nincludes all modalities."}, {"title": "Setup for Multisensory Multitask Models", "content": "\u2022 Data Preparation: Data from different modalities and tasks are paired or concatenated as\nrequired.\n\u2022 Network Architecture: Shared modality-specific encoders are followed by task-specific\ndecoders.\n\u2022 Training Details: Gradient balancing techniques are applied, along with modality balancing,\nto ensure fairness in learning. The model trains using a batch size of 128 and the Adam\noptimizer at a learning rate of 0.001.\n\u2022 Evaluation: Each task's performance is assessed on their respective validation datasets.\nFor all the methods, the experimental environment remains consistent. All models are trained and\nevaluated on NVIDIA V100 GPUs, ensuring uniformity in computational power and performance."}, {"title": "Evaluation Metrics", "content": "To measure performance, we utilize a combination of metrics following prior work on each specific\ntask. For gaze estimation, we use mean euclidean error in centimeters to measure the positional\ndistance between the predicted gaze and the ground-truth gaze. For depth estimation, we apply mean"}, {"title": "More analysis", "content": "Testing long-range interactions: Long-range interactions are critical to many problems in machine\nlearning, particularly in fields like time series forecasting, natural language processing, and signal\nanalysis. Recognizing patterns and relationships over vast sequences or across multiple modalities\noften requires models to understand and leverage these long-range dependencies. However, capturing\nthese interactions remains a challenge for many conventional models.\nIn a controlled experiment, we truncated sequences to various lengths and observed how conventional\nmodels performed. As the sequence lengths increased, representing longer durations of time or more\nextensive contexts, there was a marked decline in performance. This showcased the models' inability\nto effectively encapsulate and understand interactions beyond a certain range. Multimodal setups\nfurther complicate this. The long-range dependencies aren't just within a modality but can also be\nacross modalities. This inter-modality long-range interaction is a largely uncharted territory, and our\nexperiments showed that it's an area where even advanced models can falter.\nExploring architectures that inherently focus on long-range interactions, potentially leveraging self-\nattention mechanisms but with modifications to handle extremely long sequences. Employing models\nthat operate at different temporal scales, allowing them to summarize information at various levels\nand potentially capture longer-range interactions more effectively. Techniques that allow models\nto allocate more computational resources when faced with potential long-range dependencies, thus\nemphasizing critical parts of a sequence or modality. For multimodal problems, mechanisms that\nfacilitate better cross-modal attention can be crucial. This will enable models to recognize and act\nupon dependencies that span across different modalities, even if they are separated by considerable\ntemporal or sequential gaps.\nTesting heterogeneity in structure and noise: Heterogeneity in data, both in terms of structure and\nnoise, is a pervasive challenge in machine learning. As datasets grow more complex, encompassing a\nwider variety of sources, the inherent differences in data structure and the presence of various types\nof noise can significantly hamper the performance of models. Understanding how models grapple\nwith such heterogeneity is vital for real-world applications.\nWe exposed models to datasets that combined structured data (such as GPS, IMU) with unstructured\ndata (such as images or raw audio). Unimodal baselines often struggled to reconcile these different\ndata forms, leading to a significant drop in accuracy compared to when dealing with homogenous\ndata types. We also introduced varying degrees of noise into datasets, Gaussian noise in numerical\ndata. Currrent methods saw a rapid decline in performance as the noise levels increased, unable to\nfilter out irrelevant information effectively. Heterogeneity challenges underline the importance of\nrobustness in model design. Our experiments highlighted that many models, even those considered\nstate-of-the-art, have vulnerabilities when exposed to unexpected data structures or noise patterns.\nExploring architectures and training techniques that are inherently more robust to noise and hetero-\ngeneity. This might include noise injection during training or techniques like dropout that encourage\nmodel generalization. Leveraging advanced data augmentation techniques, both for structured and\nunstructured data, to simulate and thus prepare the model for varied data structures and noise patterns.\nUsing meta-learning approaches where models are trained to quickly adapt to new data structures or\nnoise patterns with minimal fine-tuning. Research into advanced denoising mechanisms, especially\nones that can handle structured noise, can be invaluable. This includes both pre-processing methods\nand in-model techniques."}, {"title": "Analysis of information sharing", "content": "Finally, we show examples of how information is shared across modalities and tasks, based on two\npotential sources of sharing: low-level modality features and high-level semantic concepts.\nLow-level modality features: Different sensory modalities often contain unique low-level perceptual\nfeatures that complement those in other modalities. We illustrate this information sharing across 3\nmodalities: IMU, video, and pose data for predicting 2 common activities: walking and dancing.\nWalking is a common activity with distinctive rhythmic characteristics. Using IMU features, the\nmodel learns that rhythmic patterns, particularly in acceleration and deceleration, correspond to each\nwalking step. The cadence, stability, and any irregularities in the walking pattern can also be inferred.\nVideo features capture the holistic visual representation of walking, presenting details such as gait,\narm swing, speed, stride length, and frequency. Finally, pose features highlight the specific posture\nchanges during walking, emphasizing leg movement, foot placement, and body alignment.\nDancing requires complex and expressive motions with varying styles and dynamics. IMU provides\ndynamic, often non-linear patterns in IMU data, reflecting the dance's tempo, vigor, and style\nvariations; video captures the dance form, style, synchronization, and expressiveness; and pose\ndata captures the alignment and configuration of body parts, offering insights into dance postures,\ntransitions, and intricate footwork or hand movements.\nHigh-level semantic concepts encapsulate a more general conceptual understanding and reasoning\nabout the environment. We show two examples showing how the audio and IMU modalities share\ninformation about two high-level semantic concepts, focusing on 'body pose' and 'hand pose'.\nBody pose represents the spatial arrangement and posture of the entire human body. This can involve\nstances like standing, sitting, lying down, or even dynamic movements like jumping or running. For\nAudio, indirect cues such as the sound of footsteps, a person sitting down on a chair, or even the echo\nin a room (indicating a certain body pose affecting sound propagation) can provide hints about the\nbody's posture. For IMU, accelerometers capture the directional movement while gyroscopes provide\nrotational dynamics to distinguish if a person is upright, moving rapidly, or stationary.\nHand pose looks at the orientation, gesture, and spatial arrangement of just the hands, ranging from\ngestures like waving, gripping, to more intricate signs in sign language. In audio, sounds like clapping,\nsnapping, or even the subtle rustling of hands moving through the air can be detected. The distinct\nsounds made by hang interactions with objects can also hint at specific hand poses. When IMU\nsensors are placed on the wrist or back of the hand, they can capture detailed dynamics of hand\nmovements, tilting, rotation, or swift movements that indicate hand poses."}, {"title": "More examples", "content": "In this section, we present more examples of diverse IoT modalities, emphasizing their heterogeneity\nand the implications of their temporal interactions. Each modality contributes uniquely to the\nunderstanding and processing of environmental data, pivotal for applications in IoT networks. These\nexamples underscore the heterogeneity in sensor types and data characteristics in IoT systems.\nMoreover, they highlight the importance of temporal interaction in data processing and application\nresponsiveness."}, {"title": "IMU", "content": "The Inertial Measurement Unit (IMU) is critical for capturing dynamic motion and orientation. An\nIMU typically combines accelerometers, gyroscopes, and sometimes magnetometers to provide\ncomprehensive motion tracking. For example, in a smartwatch, the IMU captures temporal data on\nuser movement patterns, crucial for activity recognition and health monitoring applications. This\nmodality's high sampling rate allows for detailed temporal analysis, capturing minute fluctuations in\nmotion, as shown in Figure 6."}, {"title": "Audio", "content": "Audio"}]}