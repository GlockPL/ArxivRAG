{"title": "IoT-LM: Large Multisensory Language Models for the Internet of Things", "authors": ["Shentong Mo", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Paul Pu Liang"], "abstract": "The Internet of Things (IOT) network integrating billions of smart physical devices embedded with sensors, software, and communication technologies is a critical and rapidly expanding component of our modern world. The IOT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, and audio to recognize the states of humans and physical objects. Data-driven tools present a rich opportunity to automatically process IoT data at scale, enabling efficient inference for understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To realize this potential, we introduce IOT-LM, an open-source large multisensory language model tailored for the IOT ecosystem. IOT-LM is enabled by two technical contributions: the first is MULTIIOT, the most expansive unified IOT dataset to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks prepared for multisensory pre-training and instruction-tuning. The second is a new multisensory multitask adapter layer to condition pre-trained large language models on multiple multisensory IOT tasks simultaneously, enabling the sharing of information across modalities and tasks for better generalization. Not only does IoT-LM yield substantial improvements on 8 supervised IOT classification tasks, but it also demonstrates new interactive question-answering, reasoning, and dialog capabilities conditioned on IOT sensors. We release IOT-LM's data sources and new multisensory language modeling framework at the repository 1.", "sections": [{"title": "1 Introduction", "content": "The digital world is witnessing an unprecedented surge in the realm of the Internet of Things (IOT): the ever-growing system of interlinked devices from individual households to vast industrial complexes [8]. These devices are often embedded with sensors, software, and communication technologies that can safely and privately analyze the human and physical world [36, 50]. For example, high-fidelity sensors are able to recognize physical activities to inform us of our daily physical wellness [47, 55]; vision, depth, and lidar sensors are able to navigate self-driving cars and connect them with traffic lights for efficient transport management [27, 31]; and wifi, depth, camera sensors can detect if the elderly require assistance in hospitals [2, 34]. As a result, there has been substantial interest in building machine learning systems that can efficiently process these IOT sensors and make predictions, which has great potential for understanding human wellbeing, controlling physical devices, and interconnecting smart cities [1, 20, 52, 56].\nHowever, most existing machine learning approaches in the IOT domain have largely focused on supervised models, trained only on a single input sensory modality and for a single output prediction task [3, 6, 23, 28, 33]. We extend the existing ML for IOT paradigms in two directions. Firstly, in the input space, we investigate how to learn from multiple heterogeneous and interacting IOT sensory modalities simultaneously. This is important since practical IOT scenarios often see multiple"}, {"title": "2 Related Work", "content": "We cover related work in the design and applications of IOT sensors, how machine learning can be used to accelerate IOT perception, and related background work in multisensory machine learning and foundation models.\nInternet of Things (IOT): The pursuit of extracting meaningful insights from IOT data [7, 30, 35, 13] has led to various innovative approaches that focus on individual modalities and specific tasks [17, 10, 4] and resource-constrained devices [15, 32, 24]. For instance, DIP-IMU [23] effectively fuses depth sensing with IMU data for enhanced pose estimation, demonstrating the potential of multimodal integration. Similarly, EyeMU [33] utilizes time-series neural networks to process IMU sensor data for accurate gaze tracking on mobile devices. TouchPose [3] explores tactile data for pose recognition, while LLVIP [28] focuses on leveraging visual data from IOT environments for dynamic real-world applications. Furthermore, RGBDGaze [6] integrates RGBD data for gaze estimation, highlighting the diversity of sensory data applications. However, these approaches generally remain confined to single-task models and lack the capability to generalize across multiple IOT modalities and tasks, an area where our work with IoT-LM introduces a significant advancement by enabling statistical sharing and generalization across a broad spectrum of IOT data.\nMultisensory machine learning: The field of multimodal machine learning has seen substantial growth, with models designed to integrate inputs from various sensory channels to perform more complex interpretation and interaction with the environment [9, 42]. Notable works include multi-modal transformers [54, 44] that fuse visual, auditory, and textual data to improve learning efficacy. These multimodal transformers have also been scaled for self-supervised pre-training across an increasing range of modalities including time-series and sensors [25, 41, 49]. While these models offer a foundation for integrating diverse data types, they often do not address the unique challenges posed by IOT environments, such as the integration of non-traditional sensor data (e.g., IMU, thermal sensors) and the need for models to interact dynamically with physical environments.\nFoundation models: Recently, the concept of foundation models [11, 12, 5], pre-trained on large-scale datasets and adaptable to a wide range of tasks [57, 29, 21], has gained traction. These models, exemplified by works such as GPT-4 [46] and LLaMA [53], offer a robust starting point for further fine-tuning on individual natural-language tasks. There has also been a recent drive towards large multimodal models, using either LLMs as a starting point and training adapters from other modalities to LLM input space [16, 57], or training multimodal transformers from scratch (sometimes called 'natively') with interleaved language tokens, image frames, audio frames, and other modalities [21, 46]. Our approach extends these paradigms into the IOT domain, where IOT-LM adapts a large language model framework to not only understand textual information but also effectively process and reason"}, {"title": "3 IOT-LM: A New Multisensory IOT Foundation Model", "content": "In this section, we describe the architecture, data curation, and training innovations in IoT-LM."}, {"title": "3.1 IOT-LM Architecture: Multisensory Multitask Adapter", "content": "IOT-LM's backbone consists of a variety of IOT sensory inputs, a general-purpose multisensory multitask encoder that fuses the information from multiple sensors and across multiple tasks, a multisensory multitask adapter that transforms encoded representations into pretrained LLM input space, and the pretrained LLM itself. We show an overview of the IOT-LM architecture in Figure 1.\nMultisensory multitask encoder for heterogeneous IOT signals The encoder is trained using a novel approach that effectively manages the heterogeneity of IoT data by combining supervised learning with unsupervised feature extraction techniques. This training involves task-specific adapta-tions, where the encoder learns to map raw sensor data to an intermediate feature space conducive for language model processing. The encoder handles the multisensory data through a combination of multimodal fusion methods, including early, late, and model-based fusion, which are chosen based on the data characteristics and the specific requirements of each task.\nSpecifically, the adapters transform the sensor data into a representation that is more conducive for the language model to process. For each sensor modality xi, we employ a dedicated encoder Ei that maps raw sensor data to an intermediate feature space. The encoded features from different modalities are then combined using a fusion mechanism within the adapter, allowing the model to harness information from all available sensors. We employ late fusion methods that are extracted from each sensor modality and are independently processed and only combined at decision-making layers, facilitating specialization in feature extraction while leveraging multimodal data for final predictions. These fusion techniques are selected and fine-tuned based on the specific characteristics of the data and the tasks at hand, ensuring optimal performance. Our fusion methods innovations involve developing new model-based fusion techniques that adaptively adjust the weighting of different sensor inputs based on their predictive value for specific tasks, a novel approach to handling IoT data heterogeneity. The training of these components is conducted through a multitask supervised learning framework that emphasizes individual task accuracy and enhances generalizability across tasks by sharing representations and leveraging common patterns found in the diverse IoT data landscape. This method strengthens the model's ability to perform well across various conditions and tasks, making it highly effective for real-world IoT applications."}, {"title": "Multisensory multitask adapter layer", "content": "We extend the typical use of adapter modules, which are compact, trainable layers inserted between the existing layers of a pre-trained language model [53]. These adapters are designed to fine-tune the pre-existing model to new tasks and modalities without significant modifications to the original model's weights, thus preserving its general linguistic capabilities while extending its functionality to new IoT-specific domains.\nThe key difference in IoT-LM is that the multisensory multitask adapter layer conditions pretrained LLMs on multiple multisensory IoT tasks simultaneously, enabling the sharing of information across modalities and tasks for better generalization and holistic understanding of IOT environments. We show an overview of the multisensory multitask adapter layer in Figure 2. The architecture is formalized as follows:\n$y = M_{w+A}(f(E_1(x_1) \\oplus E_2(x_2) \\oplus ... \\oplus E_m(x_m)) = M_w (A_{w_a}(E_1(x_1) \\oplus E_2(x_2) \\oplus ... \\oplus E_m(x_m)))$\nwhere $M_{w+A}(\u00b7)$ and $M_w (\u00b7)$ denote the models with combined and original weights, respectively, and $E_1, E_2, ..., E_m$ represent the encoders for each sensor modality. This structure allows IOT-LM to maintain the foundational knowledge of the pre-trained model while adapting to the specific nuances of IOT data."}, {"title": "3.2 IoT-Language Data Collection", "content": "To train IOT-LM, we aggregate and release the largest IOT dataset for machine learning research, comprising 1.15 million samples from 12 different sensory modalities for 8 different real-world IOT prediction tasks related to personal wellness, healthcare, smart cities, and more. These modalities include Inertial Measurement Units (IMU), thermal sensors, GPS, LiDAR, gaze, pose, capacitance sensors, and traditional modalities such as images, audio, and video. Tasks supported include:\n1.  Gaze estimation: For human-computer interaction, driver monitoring, and virtual reality appli-cations, our dataset includes RGB images of faces, depth data, and IMU outputs. The model is tasked with predicting X/Y coordinates for gaze tracking, necessitating a deep understanding of the interactions between these modalities.\n2.  Depth estimation: This task involves estimating the distance from cameras to objects in images, vital for AR/VR, robotics, and object detection. Our dataset includes RGB images combined with camera parameters, GPS coordinates, and IMU data to create depth maps for various scenarios, including street scenes and robotic hand interactions.\n3.  Gesture classification: Key to enhancing human-machine interfaces, this task uses data from gaze tracking and IMU sensors (accelerometer, gyroscope, and orientation) to classify human gestures. The challenge here is to accurately interpret the nuanced cross-modal interactions.\n4.  Pose estimation: This task determines the arrangement of human joints, using RGB images and IMU data to predict poses of the human body, including 24 joints with three angles each (yaw, pitch, roll). This requires the model to fuse data from IMUs and visual inputs.\n5.  Touch contact: To improve touch-based device interactions, this task classifies the type of touch on capacitive surfaces using RGB and capacitive images, depth maps, and hand poses.\n6.  Event detection: In applications ranging from healthcare to smart homes, this task identifies specific occurrences or anomalies in data streams, using audio spectrograms and IMU data to categorize events over time.\n7.  Activity recognition: Central to applications in fitness and healthcare, this task uses RGB images, pose data, and IMU outputs to recognize human activities such as walking, running, or jumping.\n8.  3D reconstruction: Significant in gaming, film, and AR/VR, this task involves creating three-dimensional models from RGB images, capacitance images, and depth maps, aimed at reconstruct-ing 3D poses of objects and environments.\nThese diverse tasks not only prepare IOT-LM to handle complex sensor data but also ensure it can perform a broad range of functions, from classification to complex reasoning and dialogue involving multiple IOT devices."}, {"title": "3.3 Multisensory Multitask Encoder Pretraining", "content": "Building on the IOT-LM architecture and data resources, we now describe the pre-training and instruction tuning stages in IOT-LM. The pre-training stage aims to learn a general-purpose multisen-sory multitask encoder that fuses the information from multiple sensors and across multiple tasks. During pretraining, the multisensory multitask encoder is trained on a combination of supervised learning tasks to extract and fuse features from diverse sensor data effectively. This stage is pivotal in preparing the model to process and understand complex sensory inputs, which can be mathematically described as follows:\n$\\Theta_{pre} = arg \\min_{\\theta} \\sum_{k=1}^{K} \\sum_{(x_k,Y_k)\\in D_k} L_k (M_{\\theta}(E_k(x_k)),Y_k),$\nwhere \u0398 represents the parameters of the entire network including the adapters, $D_k$ is the dataset consisting of input-output pairs $(x_k, Y_k)$, $E_k(x_k)$ represents the encoded inputs from various sensors for task k, and $L_k$ denotes the loss function used to measure the discrepancy between the model's predictions and the true outputs."}, {"title": "3.4 IOT-LM Instruction Tuning", "content": "During the instruction-tuning phase, IOT-LM is trained to understand and interpret multisensory IoT data contexts and perform specific tasks based on directed language inputs. These tasks can include making a prediction, answering a question, holding a dialog, reasoning about physical properties, and so on. This stage is crucial for refining the model's ability to follow complex and varied human instructions. The instruction tuning phase is guided by the following optimization:\n$\\Theta_{tune} = arg \\min_{\\theta} \\sum_{k=1}^{K} \\sum_{(x_k,C_k,Y_k) \\in T_k} L_k (M_{\\theta}(E_k(x), C_k), Y_k),$\nwhere Te represents the task-specific dataset with inputs xk, context or commands ck, and outputs Yk for task k. Here, $E_k(x_k)$ denotes the encoded sensor data for task k, ck provides specific instructions or task directives, and Me is the model parameterized by \u0398 including the tuned adapters. This phases ensure that IOT-LM not only learns to process a wide array of sensor data but also understands and executes complex commands pertinent to IOT applications, thus achieving high performance across varied IOT tasks."}, {"title": "4 Experiments", "content": "Our experiments aim to benchmark the performance of IOT-LM on supervised IOT classification tasks, as well as their reasoning, dialog, interaction, and zero-shot and few-shot transfer abilities across different modalities and tasks."}, {"title": "4.1 Experimental Setup", "content": "Experiments were conducted using NVIDIA A100 GPUs, ensuring high-performance computation for our deep learning models. The models were trained for 30 epochs using the Adam optimizer with a learning rate of 1e-4 and a batch size of 16. We compare to the following baselines:\n\u2022 Unimodal models [33, 3, 45] processed data from single sensor types using tailored neural architectures for each IOT domain data,\n\u2022 Unimodal Adapter models [16] utilized deep architectures such as LLaMA-adpater [16] with adapter layers, fine-tuning only the adapter modules with a learning rate of 0.0005,\n\u2022 Unimodal multi-task models [39] employed shared encoder layers and task-specific decoders, ensuring balanced gradients among tasks,\n\u2022 Multisensory models [26], where data fusion can occur at varying levels, from input to decision, and models ensured balanced data representation from each modality,\n\u2022 Multisensory multitask models [41] utilized modality-specific encoders followed by task-specific decoders, balancing both modalities and tasks during training. Each method's efficacy was validated on respective datasets.\nTo evaluate performance, we employ task-specific metrics. For gaze and pose estimation, we measure the mean euclidean error in centimeters between predictions and ground truth. Depth estimation utilizes mean absolute error in millimeters, while gesture classification, touch contact classification, and activity recognition rely on accuracy metrics. Event detection employs the F1 score for confident threshold predictions, and 3D pose reconstruction is assessed using the End-point-error in millimeters for joint discrepancies."}, {"title": "4.2 Main quantitative results", "content": "Overall performance: Table 1 reports the quantitative results of IOT-LM compared to state-of-the-art domain-specific, single modality, single task, multimodal multitask, and adapter and alignment models. As seen in Table 1, the IoT-LM consistently outperforms the single modality and single task models across all tasks. This can be attributed to their ability to integrate textual information across modalities and tasks, which is especially crucial when one modality might have noisy or incomplete data. While the adapter and multimodal multitask models show commendable performance due to their ability to adapt to new tasks, they often fall short in scenarios where multiple modalities have to be processed simultaneously. IoT-LM improves upon models that only perform multimodal multitask supervised learning, since it inherits the prediction and reasoning capabilities from the pretrained large language model component.\nPerformance across increasing modalities: In this section, we study how adding additional modali-ties to IOT-LM impacts performance. From Table 2, we find significant performance improvements observed when adding multimodal datapoints of increasing ratios (25%, 50%, all) as compared to unimodal models. This can be attributed to the IoT-LM's ability to tap into complementary information present in different modalities, especially in scenarios where one modality might be ambiguous or noisy.\nPerformance across increasing tasks: We also analyzed model performance when trained on increasing numbers of tasks, while keeping the same modality inputs constant. From Table 3, we see that IOT-LM's performance steadily increased as we increased the number of tasks during training. This suggests that the multitask instruction tuning in IOT-LM was beneficial since the model learns more general features while also improving computational efficiency.\nZero-shot and few-shot transfer: Furthermore, we study whether IOT-LM trained on certain modalities or tasks can transfer to a new set of target modalities or tasks they have never seen during training (zero-shot) or have seen with only very few examples (few-shot). We chose the fix-8 dataset as the target, primarily because of its diverse representation of modalities (IMU, capacitance, depth, image) and its challenging task (gaze estimation and touch contact classification). From Table 4, we find that across the board, even a few examples significantly boosted performance compared to the zero-shot setting, which highlights the model's ability to quickly adapt to new information. Using IOT-LM as a base model for zero-shot and few-shot experiments consistently outperformed other types of models, such as supervised unimodal, multimodal, single-task, and multitask variants. These gains were most pronounced in the 20-shot setting but were noticeably beneficial even in the 5-shot scenario. Our results suggest that IOT-LM excels at zero-shot and few-shot learning not seen in single-modality, single-task, and supervised models, and is a promising approach to deal with limited labeled data often seen in real-world IOT systems.\nScaling law of IOT-LM: Finally, we systematically increased the model size of IoT-LM to observe the impact of size on performance and representation learning. We evaluated three configurations of the multisensory multi-task adapter: small (7 billion parameters), medium (13 billion parameters), and large (70 billion parameters), and the results are reported in Table 5. Each configuration was trained using a consistent training regime on a curated dataset comprising various IOT sensory inputs, including visual, auditory, and tactile data. We focused on a range of tasks, such as anomaly detection, predictive maintenance, and activity recognition, to test the adaptability and efficiency of the models at different scales. Our experiments demonstrate a clear scaling law: as the number of parameters increases, the models exhibit improved performance across all tasks. This is quantified not only in terms of accuracy, but also in how effectively the models generalize to unseen data, indicating better learning of underlying representations. The observed scaling law suggests that larger models are more adept at integrating and processing multisensory data, leading to more robust and general representations. This supports the hypothesis that model capacity plays a crucial role in multisensory learning environments typical of IOT applications."}, {"title": "4.3 Qualitative analysis", "content": "In this section, we qualitatively demonstrate the dialog and interactive capabilities of IOT-LM on processing audio and Inertial Measurement Unit (IMU) data.\nAudio: For audio data, in Figure 4, IOT-LM was presented with an audio waveform and instructed to identify the corresponding activity from a list of categories. The model was able to discern that the audio signal most closely resembled the category \"Coughing.\" The choice was justified by the sudden and sporadic nature of the spikes in the waveform, which align with the acoustic signature of a cough. This example highlights the model's capability to process temporal acoustic features and correctly categorize and reason about it.\nIMU: For IMU data, in Figure 5, IoT-LM analyzed a graph with multiple lines, each representing different sensor readings from the IMU. The task was to select an activity from a list that best matched the IMU data pattern. The model identified \"Knocking\" as the most likely activity, reasoning that the regular intervals of peaks were indicative of a repetitive action with varying force, which is consistent with the pattern of knocking.\nThese examples demonstrate the multisensory multi-task adapter's analytical proficiency in interpret-ing and classifying data from different IOT modalities based on their temporal characteristics. Such capability is instrumental in realizing the full potential of IOT systems, where understanding and acting upon such heterogeneous data in real time is essential for various applications, from smart homes to industrial automation. The examples provided also show the model's potential in bridging the gap between raw sensor data and meaningful insights, enabling non-expert users to interact with and benefit from complex IOT systems."}, {"title": "5 Conclusion and Broader Impacts", "content": "This paper presents IOT-LM, a new large multisensory language model with multisensory perception and natural language interaction capabilities over a spectrum of IOT modalities and applications. Key innovations in IoT-LM include a new multisensory multitask adapter to simultaneously condition pretrained LLMs on multiple multisensory IoT tasks for better generalization, as well as a new resource of 1.15 million IoT sensor - natural language paired samples covering 12 modalities and 8 real-world tasks. Overall, IOT-LM not only advances the state-of-the-art in IOT predictive learning but also enables new question-answering, reasoning, and interactive dialog capabilities on physical sensor data. Future work should focus on expanding the model's capabilities to more IOT modalities and tasks, improving its reasoning and robustness, and exploring the implications of its deployment in real-world IOT systems. We hope that IOT-LM will inspire further innovations at the intersection of machine learning and IOT, contributing to smarter and more responsive technologies that can understand and interact with their environments."}, {"title": "Appendix", "content": "A Details Regarding IOT Datasets\nA.1 Twelve Rich Modalities\nWe collected diverse data from IoT devices, such as Inertial Measurement Units (IMU), Thermal sensors, and Global Positioning Systems (GPS). Furthermore, we include challenging modalities, such as capacitance, depth, gaze, and pose. Finally, we collect common and widely used image, audio, and video modalities. These modalities bring unique challenges since they typically involve noisy real-world sensor measurements, that lack explicit tokenization and alignment with other modalities that we typically expect from conventional multimodal image-text research.\nIMU: Inertial Measurement Units capture 3D motion and orientation. This data is fundamental for various applications, including motion tracking and navigation. We collected 2,940 IMU samples from EyeMU [33] for gaze estimation and motion gesture classification, where they used the accelerometer and gyroscope raw values sampled at 60 Hz as the IMU values per-axis. 28,400 IMU instances are included from SAMOSA [45] to save synchronized streams of the 9-axis IMU data (accelerometer, gyroscope and orientation) at 50 Hz by using a Fossil Gen 5 smartwatch running Google Android wearOS 2.23. Further, we sampled 160,120 IMU samples (9-axis) recorded by the device motion sensor using an iOS application [6] on Apple iPhone X for gaze tracking. For human bodies, 330,178 IMU orientation recordings [23] from 17 sensors on different body parts are saved for pose estimation and activity recognition. For first-person videos in Ego4D [22], we used 510,142 timestamps-based IMU samples with the normalized accelerometer and gyroscope values in each video for activity recognition. For IMU data on self-driving cars, we collected 41,000 samples from KITTI [18] for depth estimation.\nThermal: Thermal modality data provide temperature radiance insights, crucial in surveillance. For collection, we used 12,025 Thermal samples from LLVIP [28] containing many pedestrians and cyclists from different locations on the street between 6 and 10 o'clock in the evening. They used HIKVISION DS-2TD8166BJZFY-75H2F/V2 as the camera equipment, a binocular camera platform consisting of an infrared camera with a wavelength of 8 ~ 14um.\nGPS: Global Positioning Systems offer location data with high precision. This data is invaluable for tasks like location-based services, asset tracking, and navigation. For GPS data on self-driving cars, we collected 41,000 samples from KITTI [18] using OXTS RT3003 inertial and GPS navigation system for depth estimation. The geographic coordinates include global orientation, altitude, velocities, accelerations, angular rates, and satellite information. Following the original dataset, we applied two specified 3-axis coordinates as accelerations for the vehicle and angular rates to describe the tangent plane of the earth's surface corresponding to the geographic location.\nCamera: Cameras provide visual data, capturing the environment in rich detail. They serve as the backbone for countless computer vision tasks. For Camera data, we collected 41,000 instances from KITTI [18] using a Velodyne laser scanner installed on a vehicle car for depth estimation. We stored its 3-axis coordinate and an additional reflectance value for each point. The timestamp-based points can be considered according to the scanner's continuous rotation on its vertical axis, which provides a complementary context to GPS/IMU systems for auto-driving.\nCapacitance: Capacitive sensors measure changes in capacitance to detect nearby objects or changes. This is foundational for touchscreen technologies and proximity sensing. For capacitance data, we used 65,374 samples from TouchPose [3] using a 39.6 cm capacitive Crystal Touch panel (Ocular Touch, Dallas, TX), 16-bit touch digitizer, and cameras to record ground-truth data. When fingers approach the lines on the mutual-capacitance touch sensor, it causes a capacitance drop between lines, resulting in the mutual-capacitance image.\nDepth: Depth sensors measure distances between the sensor and objects, providing a 3D view of the environment. They play a significant role in tasks like object detection and scene reconstruction. For depth data, we collected 160,120 samples from RGBGaze [6] using Apple iPhone X with a TrueDepth camera (640 x 480 depth map interpolated from a 170 \u00d7 170 IR dot pattern). The participants were asked to look at a target (red dot) that was moving on the screen. While the user gazed at the target, the depth imagery was logged at approximately 8 Hz, along with the ARKit gaze prediction. For touch depth data, we used 65,374 samples from TouchPose [3] recorded by a ToF depth camera\nA.2 Eight Well-defined and Challenging Tasks\nOur benchmark includes tasks that reflect real-world IoT challenges and that will drive the community towards solutions with tangible societal impacts.\nGaze estimation: This task is pivotal for human-computer interaction, driver monitoring, and virtual reality. Given RGB images of faces, depth and IMUs, our goal is to predict the location (X/Y) for tracking gazes of the person. This regression task requires multisensory understanding on long-range interactions between RGB images and depth and heterogeneity in IMUs.\nDepth estimation: A cornerstone for AR/VR applications, robotics, and object detection, depth estimation involves predicting the distance between the camera and each pixel in the image. Given RGB images, camera parameters, GPS coordinates, and IMU, we are expected to predict the depth maps of objects, such as cars and pedestrian on the streets. In the touch robots case, given RGB images, capacitive image, and hand poses, our target is to estimate the depth maps of hands. This regression problem requires multisensory understanding on long-range interactions between RGB images and capacitance and heterogeneity in poses.\nGesture classification: Crucial for intuitive human-machine interfaces, gesture classification aims to recognize specific hand or body movements. Given gaze locations and IMU data on accelerom-eter, gyroscope and orientation, the task is defined to classify the gesture of human heads. This classification problem requires the cross-model perception on heterogeneity in gaze and IMUS.\nPose estimation: With applications in AR/VR, gaming, and health, pose estimation focuses on determining the spatial arrangement of human joints. Given RGB images and measured IMU data, our goal is to predict the poses of human body including 24 joints with three joint angles (yaw, pitch, roll). This regression problem requires a deeper cross-modal understanding on the heterogeneity in IMUs and RGB pixels.\nTouch contact classification: Vital for enhancing user experiences on touch-based devices, this task involves determining the type or nature of touch on capacitive surfaces. Given RGB images, capacitive images, depth maps, and hand poses, we are expected to classify touch contact using diverse modalities. This classification task requires a multimodal understanding on the long-range interactions between RGB images and capacitance and heterogeneity in depth maps and poses.\nEvent detection: A broad area with applications in surveillance, smart homes, and industrial setups, event detection involves identifying specific occurrences or anomalies in the data stream. Given audio spectrograms and IMU data on accelerometer, gyroscope and orientation, our goal is to predict the categories of events across different timestamps. This classification problem requires a cross-modal understanding on the long-range interactions between audio and IMU. If a predicted activity is above a confidence threshold, we consider it an event. Othwise, if it's below a confidence threshold, or belongs to the Other class, we do not consider it an event.\nActivity recognition: Central to fitness, health, and elder care applications, activity recognition aims to discern human activities like walking, running, or jumping. Given RGB images, poses with three joint angles (yaw, pitch, roll), and IMU data, we are expected to classify the class of actions for the human body. For ego-centric cases, we are given video frames and IMU orientation recordings on from different sensors to predict the category of activity in the videos. This classification task requires a cross-modal understanding on the heterogeneity in poses, videos and IMU.\n3D reconstruction: With significance in gaming, film, and AR/VR, 3D reconstruction involves creating a three-dimensional model of an environment or object from 2D data. Given RGB images, capacitance image, and depth maps, our target is to reconstruct the 3D poses. This regression problem requires a multimodal understanding of both capacitance images and depth maps.\nB Experimental Setup\nB.1 Setup for Unimodal Models\n\u2022 Data Preparation: Each modality, e.g., RGB images, capacitive images, or hand pose, is pre-processed independently. The data undergo normalization and any specific transformations tailored to that modality.\n\u2022 Network Architecture: Distinct neural architectures optimized for each modality type, such as CNNs for images and RNNs for sequential data.\n\u2022 Training Details: Models are trained using a batch size of 128, employing the Adam optimizer with a learning rate of 0.001. Early stopping with a patience of 10 epochs ensures prevention from overfitting.\n\u2022 Evaluation: Each unimodal model is evaluated on its respective validation dataset to gauge performance.\nB.2 Setup for Adapter Models\n\u2022 Data Preparation: Data is fed through a pre-trained network, where only the adapter modules are trainable.\n\u2022 Network Architecture: Utilizing deep architectures like LLaMA [16], but with adapter layers inserted in-between the pre-defined layers.\n\u2022 Training Details: Since only the adapter layers are trainable, fewer parameters are updated, allowing for a larger batch size of 256. The training uses the Adam optimizer with a learning rate of 0.0005.\n\u2022 Evaluation: Model performance is assessed by evaluating the fine-tuned model on the targeted task's validation set.\nB.3 Setup for Unimodal Multi-task Models\n\u2022 Data Preparation: Data from different tasks, but the same modality, are concatenated or paired.\n\u2022 Network Architecture: Shared encoder layers process the input data, followed by task-specific decoders.\n\u2022 Training Details: Gradient balancing techniques are employed to prevent one task from dominating the training process. Training leverages a batch size of 128 and the Adam optimizer with a learning rate of 0.001.\n\u2022 Evaluation: Performance is evaluated separately for each task on their respective validation sets.\nB.4 Setup for Multisensory Models\n\u2022 Data Preparation: Data from different modalities are fused either at the input, feature, or decision level.\n\u2022 Network Architecture: Modality-specific encoders process each input type. Fusion layers then combine features from all encoders.\n\u2022 Training Details: Models are trained with a batch size of 128 using the Adam optimizer and a learning rate of 0.001. Data balancing techniques ensure equal representation from each modality.\n\u2022 Evaluation: The combined model's efficacy is evaluated using a validation dataset that includes all modalities.\nB.5 Setup for Multisensory Multitask Models\n\u2022 Data Preparation: Data from different modalities and tasks are paired or concatenated as required.\n\u2022 Network Architecture: Shared modality-specific encoders are followed by task-specific decoders.\n\u2022 Training Details: Gradient balancing techniques are applied, along with modality balancing, to ensure fairness in learning. The model trains using a batch size of 128 and the Adam optimizer at a learning rate of 0.001.\n\u2022 Evaluation: Each task's performance is assessed on their respective validation datasets.\nFor all the methods, the experimental environment remains consistent. All models are trained and evaluated on NVIDIA V100 GPUs, ensuring uniformity in computational power and performance.\nC Evaluation Metrics\nTo measure performance, we utilize a combination of metrics following prior work on each specific task. For gaze estimation, we use mean euclidean error in centimeters to measure the positional distance between the predicted gaze and the ground-truth gaze. For depth estimation, we apply mean\nD More analysis\nTesting long-range interactions: Long-range interactions are critical to many problems in machine learning, particularly in fields like time series forecasting, natural language processing, and signal analysis. Recognizing patterns and relationships over vast sequences or across multiple modalities often requires models to understand and leverage these long-range dependencies. However, capturing these interactions remains a challenge for many conventional models.\nIn a controlled experiment, we truncated sequences to various lengths and observed how conventional models performed. As the sequence lengths increased, representing longer durations of time or more extensive contexts, there was a marked decline in performance. This showcased the models' inability to effectively encapsulate and understand interactions beyond a certain range. Multimodal setups further complicate this. The long-range dependencies aren't just within a modality but can also be across modalities. This inter-modality long-range interaction is a largely uncharted territory, and our experiments showed that it's an area where even advanced models can falter.\nExploring architectures that inherently focus on long-range interactions, potentially leveraging self-attention mechanisms but with modifications to handle extremely long sequences. Employing models that operate at different temporal scales, allowing them to summarize information at various levels and potentially capture longer-range interactions more effectively. Techniques that allow models to allocate more computational resources when faced with potential long-range dependencies, thus emphasizing critical parts ofa sequence or modality. For multimodal problems, mechanisms that facilitate better cross-modal attention can be crucial. This will enable models to recognize and act upon dependencies that span across different modalities, even if they are separated by considerable temporal or sequential gaps.\nTesting heterogeneity in structure and noise: Heterogeneity in data, both in terms of structure and noise, is a pervasive challenge in machine learning. As datasets grow more complex, encompassing a wider variety of sources, the inherent differences in data structure and the presence of various types of noise can significantly hamper the performance of models. Understanding how models grapple with such heterogeneity is vital for real-world applications.\nWe exposed models to datasets that combined structured data (such as GPS, IMU) with unstructured data (such as images or raw audio). Unimodal baselines often struggled to reconcile these different data forms, leading to a significant drop in accuracy compared to when dealing with homogenous data types. We also introduced varying degrees of noise into datasets, Gaussian noise in numerical data. Currrent methods saw a rapid decline in performance as the noise levels increased, unable to filter out irrelevant information effectively. Heterogeneity challenges underline the importance of robustness in model design. Our experiments highlighted that many models, even those considered state-of-the-art, have vulnerabilities when exposed to unexpected data structures or noise patterns.\nExploring architectures and training techniques that are inherently more robust to noise and hetero-geneity. This might include noise injection during training or techniques like dropout that encourage model generalization. Leveraging advanced data augmentation techniques, both for structured and unstructured data, to simulate and thus prepare the model for varied data structures and noise patterns. Using meta-learning approaches where models are trained to quickly adapt to new data structures or noise patterns with minimal fine-tuning. Research into advanced denoising mechanisms, especially ones that can handle structured noise, can be invaluable. This includes both pre-processing methods and in-model techniques.\nD.1 Analysis of information sharing\nFinally, we show examples of how information is shared across modalities and tasks, based on two potential sources of sharing: low-level modality features and high-level semantic concepts.\nLow-level modality features: Different sensory modalities often contain unique low-level perceptual features that complement those in other modalities. We illustrate this information sharing across 3 modalities: IMU, video, and pose data for predicting 2 common activities: walking and dancing.\nWalking is a common activity with distinctive rhythmic characteristics. Using IMU features, the model learns that rhythmic patterns, particularly in acceleration and deceleration, correspond to each walking step. The cadence, stability, and any irregularities in the walking pattern can also be inferred. Video features capture the holistic visual representation of walking, presenting details such as gait, arm swing, speed, stride length, and frequency. Finally, pose features highlight the specific posture changes during walking, emphasizing leg movement, foot placement, and body alignment.\nDancing requires complex and expressive motions with varying styles and dynamics. IMU provides dynamic, often non-linear patterns in IMU data, reflecting the dance's tempo, vigor, and style variations; video captures the dance form, style, synchronization, and expressiveness; and pose data captures the alignment and configuration of body parts, offering insights into dance postures, transitions, and intricate footwork or hand movements.\nHigh-level semantic concepts encapsulate a more general conceptual understanding and reasoning about the environment. We show two examples showing how the audio and IMU modalities share information about two high-level semantic concepts, focusing on 'body pose' and 'hand pose'.\nBody pose represents the spatial arrangement and posture of the entire human body. This can involve stances like standing, sitting, lying down, or even dynamic movements like jumping or running. For Audio, indirect cues such as the sound of footsteps, a person sitting down on a chair, or even the echo in a room (indicating a certain body pose affecting sound propagation) can provide hints about the body's posture. For IMU, accelerometers capture the directional movement while gyroscopes provide rotational dynamics to distinguish if a person is upright, moving rapidly, or stationary.\nHand pose looks at the orientation, gesture, and spatial arrangement of just the hands, ranging from gestures like waving, gripping, to more intricate signs in sign language. In audio, sounds like clapping, snapping, or even the subtle rustling of hands moving through the air can be detected. The distinct sounds made by hang interactions with objects can also hint at specific hand poses. When IMU sensors are placed on the wrist or back of the hand, they can capture detailed dynamics of hand movements, tilting, rotation, or swift movements that indicate hand poses.\nE More examples\nIn this section, we present more examples of diverse IoT modalities, emphasizing their heterogeneity and the implications of their temporal interactions. Each modality contributes uniquely to the understanding and processing of environmental data, pivotal for applications in IoT networks. These examples underscore the heterogeneity in sensor types and data characteristics in IoT systems. Moreover, they highlight the importance of temporal interaction in data processing and application responsiveness.\nE.1 IMU\nThe Inertial Measurement Unit (IMU) is critical for capturing dynamic motion and orientation. An IMU typically combines accelerometers, gyroscopes, and sometimes magnetometers to provide comprehensive motion tracking. For example, in a smartwatch, the IMU captures temporal data on user movement patterns, crucial for activity recognition and health monitoring applications. This modality's high sampling rate allows for detailed temporal analysis, capturing minute fluctuations in motion, as shown in Figure 6.\nE.2 Audio\nAudio sensors capture sound waves, converting them into digital signals that represent the acoustic environment. In smart homes, audio sensors can detect various sounds, from spoken commands to the"}]}