{"title": "SongCreator: Lyrics-based Universal Song Generation", "authors": ["Shun Lei", "Yixuan Zhou", "Boshi Tang", "Max W. Y. Lam", "Feng Liu", "Hangyu Liu", "Jingcheng Wu", "Shiyin Kang", "Zhiyong Wu", "Helen Meng"], "abstract": "Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and an additional attention mask strategy for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different prompts, exhibiting its potential applicability. Our samples are available at https://songcreator.github.io/.", "sections": [{"title": "1 Introduction", "content": "Music is an integral part of human culture, embodying human intelligence and creativity. Songs combining vocals and accompaniment compose an essential part of it, whose generation has been a hotspot in both academia and industry in recent years. Although with the rapid advancements in generative models, communities have witnessed the applications of Artificial Intelligence Generated Content (AIGC) models in the generation of texts [1-3], images [4-6] and speeches [7-11], it still remains a big question whether we can replicate the successes in song generation, which demands coordination among various complex elements such as instruments, rhythm, melody and vocals. Currently creating high-level songs with both vocals and accompaniment still requires substantial human effort in composition, instrument arrangement, singing, and so on, a process requiring a great deal of time and expertise. Lyrics-to-song generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.\nPrevious works mostly explored specific aspects of song generation, as listed in Table 1. Although they exhibit abilities in vocal composition, instrumental arrangement and harmonious generation, none of them is able to combine these three for high-quality lyrics-to-song generation. To this end, Jukebox [12] can be seen as the first and only attempt from published literature so far to simultaneously generate vocals and accompaniment in a song from lyrics using a single model."}, {"title": "2 Related Work", "content": "Singing voice synthesis Singing Voice Synthesis (SVS) [15\u201320] aims at synthesize vocals given scores, has made great progress in recent years. Several works attempt to adopt transformer models [15], generative adversarial networks [16] and conditional variational autoencoder [17, 18] for SVS. Recently, research [19, 20] focuses on enhancing the quality of synthesized vocals through diffusion models, demonstrating state-of-the-art (SOTA) performance. Similarly, SongCreator also employs a"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Overview", "content": "Let $x \\in X$ represent a song audio. A song generation process can be defined as $f : C \\rightarrow X$,\nwhere $C$ is the set of conditioning signals. In this work, we consider a flexibly conditioned song\ngeneration system $f$ with $C \\in C$, accepting a variety of optional inputs including lyrics, vocal prompt,\naccompaniment prompt, pre-determined vocal track and pre-determined accompaniment track. The\nhigh flexibility of conditions empowers the controllability of our model, so that different elements\nwithin the generated songs can be customized as needed.\nHowever, end-to-end generating a high-fidelity song $x$ from $C$ with a neural network $f$ remains\nchallenging to date. In the same spirit as previous works [23, 25, 36], we introduce a language-alike\ndiscrete sequence (a.k.a., semantic tokens), denoted as $S = (S_1,..., S_N)$, to capture significant\nstructural information in song and to embody LMs as the \u201cbrain\u201d of our system for writing songs."}, {"title": "3.2 Dual-sequence language model", "content": "Formally speaking, the proposed dual-sequence language model (DSLM) is tasked with the gen-\neration of $(S_s, S_v, S_a)$ given $C$. An overview of the proposed architecture is presented in Figure\n2. Concerning the quadratic complexity of Transformer with respect to sequence length, instead of\nprocessing the concatenated sequences of multiple target sequences token-by-token as in [29], in\nDSLM we utilize different decoders to model the semantic tokens of vocals $S_v$ and accompaniment\n$S_a$ and harmoniously combine them to generate the semantic tokens of song $S_s$.\nThe proposed DSLM consists of a lyrics encoder, two decoders (one for vocals and one for accom-\npaniment) inter-connected through a bidirectional cross-attention module, and a final song decoder.\nThe lyrics encoder is built upon a stack of Transformer encoder layers, which, as a architecture\nwidely adopted in speech synthesis [7, 10], extracts critical information related to the pronunciation\nof the lyrics $C_{\\text{lyrics}}$. On the other hand, the vocal decoder and accompaniment decoder are together\ncomposed of multiple DSLM blocks. Each DSLM block is composed of a self-attention (SA) layer,\na cross-attention (CA) layer, a bidirectional cross-attention (BCA) layer and a feed-forward layer.\nThe cross-attention layer is applied on the embeddings extracted by lyrics encoder and the decoder\nfeatures to model the alignment between the lyrics and vocals and to extract semantic information\nfrom the lyrics for generating accompaniment, in vocal and accompaniment decoders respectively,\nwhich has been widely applied in previous works on speech synthesis [58, 59] and audio genera-\ntion [60, 61]. Moreover, in a complete song, the vocal and accompaniment parts have a complex\ninterrelationship. The accompaniment must complement the vocal track without overshadowing\nthem, ensuring that both parts work together to highlight the song's expressive and artistic intents. To\nunderstand and model this interrelationship, we introduce a bidirectional cross-attention (BCA) layer,\nwhich consists of two symmetrical cross-attention mechanisms. For example, in the vocal decoder,\nthe BCA allows the model to attend to the generated parts of accompaniment while generating vocals,"}, {"title": "3.3 Attention mask strategy for universal song generation", "content": "In both self-attention (SA) layer and bidirectional cross-attention (BCA) layer, we employ the mask\nmatrix M as shown in Equation 2 to control the access of the semantic tokens to be predicted. As\nshown in Figure 2, we implement multiple mask strategies for SA and BCA using different M.\nSpecifically, we employ two different masking strategies for the SA to control each semantic token's\naccess to the context within the same sequence. One strategy is the causal attention mask, where the\nrepresentation of each token can only access the leftward context tokens and itself. This approach\npredicts a token conditioned on its historical (left) context, thereby learning generation and continua-\ntion capabilities, but it is difficult to fully capture the dependencies between the context. The other\nstrategy is the non-causal attention mask, where all token can attend to each other within the same\nsequence. It incorporates contextual information from the entire sequence, and can generate more\ncomprehensive and enriched context representations than the causal approach.\nFor BCA, we design four masking strategies to control the mutual attention between the semantic\ntoken sequences representing vocals and accompaniment. The bidirectional mask (BR) allows\nrepresentations in both the vocal sequence and accompaniment sequence to attend to representations\nin the other sequence. However, when predicting the token at time step t, it can only attend to the\nrepresentation of tokens in the other sequence at time step less than or equal to t. For example, the\nrepresentation $H_{v,t}$ of semantic token $S_{v,t}$ can only pay attention to $H_{a,\\leq t}$, but not to $H_{a,>t}$. It\nattempts to capture the relationships between vocals and accompaniment, but does not consider the\nfull context of the other sequence, leading to certain limitations when one sequence is pre-determined.\nAs a supplement, the accompaniment-to-vocals (A2V) and vocals-to-accompaniment (V2A) strategies\nallow one sequence to attend to all tokens in the other sequence. Take the A2V as an example, the"}, {"title": "3.4 Training Setup", "content": "We investigate a multi-task training setup, in which the model is trained on several tasks to enhance\nits composition, arrangement, and comprehension abilities. We consider the following three tasks:\nSong generation from lyrics In this task, the SA in both the vocals decoder and the accompaniment\ndecoder employs the causal attention mask to simultaneously generate vocal and accompaniment\nsemantic tokens. For BCA, 80% of the time we use the bidirectional attention mask to learn how to\ngenerate harmoniously coordinated vocals and accompaniment. In the remaining 20% of the time,\nwe use the None strategy to independently generate vocals and accompaniment.\nSong generation from pre-determined accompaniment or vocals Take the accompaniment is\ndetermined as an example, in this task, the SA in the vocals decoder maintains the causal mask to\ngenerate vocals, while the SA in the accompaniment decoder employs the non-causal mask, with the\nBCA using the A2V strategy. Note that for the non-causal mask, we randomly mask 20% of tokens\nin the input sequence, to encourage the model to learn the relationships between context tokens.\nFurthermore, for the above two training tasks, we provide the model with a vocal and accompaniment\nprompt to encourage the model to learn to control the acoustic conditions of the generated audio.\nSong editing The song editing task combines the above two tasks. The difference is that we\nrandomly select a span of tokens from the end of the target sequence to replace the audio prompt,\nusing a special token <EDIT> in between to distinguish the editing task from the generation task.\nIn all training tasks, the vocal decoder and accompaniment decoder are trained using the next token\nprediction objective, and the song decoder predicts the semantic tokens of the complete song based\non the embeddings extracted from the vocal decoder and accompaniment decoder. After that, we\ncalculate the cross-entropy loss for vocals, accompaniment and song, and optimize the DSLM with\nthe sum of these losses. Note that we follow previous works [54, 62] and calculate the loss on all\ntokens, not just the masked tokens, for non-causal strategy. Moreover, we also mask the lyrics 20%\nof the time to encourage the model to attempt unconditional generation."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental setup", "content": "Data and model DSLM and all baselines are trained on around 8,500 hours of song data with lyrics\n(approximately 270,000 songs). We preprocess this dataset by segmenting it into 1.7M clips, each no\nlonger than 30 seconds, based on the lyrics and Voice Activity Detection (VAD) results. Each clip is"}, {"title": "4.2 The results of tasks", "content": "Lyrics-to-song As shown in Table 3, our proposed SongCreator significantly outperforms the base-\nlines across all three metrics, confirming the effectiveness of SongCreator. The difference between\nSongCreator and Ground Truth is merely 0.05 and 0.01 for musicality and quality, respectively.\nSongCreator (Single) and GPT (Vocals & Song) perform better than other baselines, demonstrating\nthe difficulty of directly modeling the complete song. Additionally, we use the same lyrics from the\ndemos of the previous SOTA model Jukebox [12] and conduct the ABX preference test. As shown in\nTable 14, SongCreator is preferred over Jukebox 60% of the time.\nTo investigate the ability of SongCreator to maintain acoustic conditions from prompts, we compared\nit with MusicGen. The results are shown in Table 5. SongCreator achieved scores of 4.01 in musicality\nand 3.82 in similarity, considerably improving upon MusicGen's scores of 3.46 and 3.27, with only a\nslightly lower score of 0.16 in FAD. In addition, SongCreator can independently control the acoustic\nconditions of the vocals and accompaniment in the generated song. This capability is lacking in\nprevious methods and results can be found on the demo page.\nLyrics-to-vocals SongCreator provides two inference methods for lyrics-to-vocals. One is similar to\nlyrics-to-song, where the model considers the relationship with the accompaniment when generating\nvocals. The other doesn't use BCA and the accompaniment decoder, relying solely on the vocal\ndecoder to generate the vocals, named SongCreator (Vocal Only). As shown in Table 4, SongCreator\n(Vocal Only) achieves scores of 3.68 in musicality and 3.63 in quality, comparable to the performance\nof SongCreator (Single) and GPT. However, after considering the relationship between vocals and\naccompaniment, SongCreator surpasses these models with a substantially higher score of 3.98 in\nmusically. In this study, we also conduct a zero-shot evaluation of the vocals between our proposed\nmodel and VALL-E. Table 6 presents the results. From the performance evaluated by MOS and\nSECS, our proposed model outperforms VALL-E, especially in terms of similarity, demonstrating\nSongCreator's robust zero-shot clone ability for generating vocals.\nVocals-to-song and accompaniment-to-song As shown in Table 7 and Table 8, our proposed\nSongCreator gets comparable results with recent SOTA models in terms of musicality and harmony.\nFor the FAD score, our model reaches 1.88 and 1.24 on the two tasks, respectively, outperforming\nSingSong. A possible reason is that our model considers the complete song, rather than just the\npartially separated vocals considered in SingSong. In addition, we used the same vocals (6 samples)\nin SingSong's demos to generate songs with our model, and asked subjects to choose their preferred\nsongs. As shown in Table 15, SingSong gets an extra preference (54.1%) over SongCreator (30%).\nWe speculate one of the reasons is that SingSong uses a large-scale high-quality dataset (46k hours).\nMusic Continuation For the music continuation task,\nwe compare different models by generating 10s music\nbased on a 5s instrumental music prompt. As illustrated\nin Table 9, we can see that SongCreator achieves com-\nparable results with AudioLM and GPT. This indicates\nthat SongCreator can effectively continue the musical\nelements in the prompt, providing the capability to con-\ntrol the accompaniment in song generation.\nEditing tasks To evaluate the performance on editing tasks, we manually constructed a dataset of\n30 song editing examples, as shown in Appendix D. Table 10 presents the results of song editing.\nWe can see that SongCreator gets comparable performance in terms of naturalness to the baselines.\nHowever, benefiting from its strong ability to generate song, SongCreator surpasses these baselines\nin musicality, achieving a score of 4.01. In the vocal editing, as shown in Table 11, all three models\nachieve relatively close performance in both subjective and objective evaluations. To demonstrate\nthe editing ability of SongCreator, we further conduct the ABX preference test on three tasks: song\nediting, vocals editing, and vocals editing in song. In each task, SongCreator restores the masked"}, {"title": "4.3 Ablation Studies", "content": "The influence of multi-task training Through previous experiments, we can find that multi-task\ntraining significant improves most tasks, especially in lyrics-to-song. This indicates that the DSLM\neffectively capture the shared information between different tasks, such as composition, arrangement\nand the relationship between vocals and accompaniment.\nThe influence of attention mask strategy in\nself-attention layer To validate our designed\nSA mask strategy, we disable the non-causal\nmask of SA during training and conduct an\nABX preference test to compare this version\nwith SongCreator on three tasks: lyrics-to-song,\nvocals-to-song, and accompaniment-to-song. As\nshown in Figure 3, the performance on all three\ntasks showed significant degradation, especially\nfor vocals-to-song. These results indicate that\nincorporating the non-causal attention mask as-\nsists the learning of the relationships within the\ncontext and provides additional contextual infor-\nmation for generation.\nThe influence of bidirectional cross-attention\nlayer We evaluate the SongCreator and the\nmodel without using BCA on lyrics-to-song and\nlyrics-to-vocals. Figure 4 shows the results.\nWhen the BCA is removed from the DSLM, the\nperformance on lyrics-to-song exhibit a marked\ndeterioration, suggesting utilizing BCA is help-\nful for the model generate harmonious vocals\nand accompaniment. Interestingly, the perfor-\nmance also declined on the lyrics-to-vocals task,\ndemonstrating that learning the relationships be-\ntween vocals and accompaniment is also beneficial for generating vocals."}, {"title": "5 Conclusion and Discussion", "content": "Conclusion In this paper, we propose SongCreator, a system designed for lyrics-based song\ngeneration. We introduce a dual-sequence language model (DSLM) to separately model vocals and\naccompaniment information, and employs a dynamic bidirectional cross-attention module to capture\nthe influences between these two sequences, with designing a special attention mask strategy for\nDSLM. In experiments, the proposed SongCreator provides competitive performance on all eight\ntasks.\nLimitations We acknowledge the limitations of our proposed SongCreator. Due to the challenges in\ncollecting data, SongCreator currently cannot control the genre and style of the output songs through\ntext descriptions. Besides, the interference from accompaniment in the song makes it difficult for\nBEST-RQ to fully encode the vocal information, imposing a limited clarity of the synthesized vocals\nin further work, we hope to extract better semantic representations for songs.\nBroader Impact We believe that our work has huge potential to develop into a song creation tool\nfor content creators or novices to seamlessly express their creative pursuits with a low entry barrier,\nwhile also streamline and improve the workflow of experienced music producers. However, the"}, {"title": "A Training and Implementation Details", "content": ""}, {"title": "A.1 Dual-sequence language model", "content": "Our DSLM consists of a lyrics encoder and three decoders. The lyrics encoder is a 4-layer Transformer\n[65] with 1024 hidden size. The vocal decoder and accompaniment decoder have a similar architecture\nthat contains 8 DSLM layers with 1024 hidden size. The song decoder also consists of 4 feed-forward\nTransformer layers with 1024 hidden size. We provide detailed hyper-parameter settings about this\nmodel configuration in Table 12. We collected approximately 8500 hours of songs with lyrics from\nthe internet for model training, comprising part of the DISCO-10M [68] dataset and some in-house\ndatasets."}, {"title": "A.2 BEST-RQ with vector quantization", "content": "BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) [57] is a simple\nand effective self-supervised learning model that learns representations from audio data without\nmanually labeled annotations. This self-supervised algorithm helps alleviate the scarcity of song data\nwith lyrics and provides a robust foundation for the entire generation system.\nOur implementation of BEST-RQ was based on an open-source library. In particular, our implementa-\ntion follows the same architecture as of BEST-RQ [57], but with a codebook's vocabulary size of 1024.\nFor feature extraction, 80-dimensional log Mel-spectrograms are extracted with 24kHz sampling\nrate with a hop size of 480 and fed into the model to obtain a 50Hz sequence of 1024-dimensional\nlatent representations. We train this model, which has approximately 0.6 billion parameters, using\nour prepared 100k hours of audio data in the self-supervised learning (SSL) manner described in [57].\nFurthermore, as we aim to achieve universal song generation, our training dataset includes not only\ncomplete songs with vocals and accompaniment but also separate instrumental music and vocals.\nThis diverse dataset ensures that our model gains a comprehensive understanding of different music\nelements and their interactions, enhancing its ability to generate a wide array of musical and vocal\noutputs.\nNext, we train a Vector Quantization (VQ) module to quantize the 1024-dimensional latent representa-\ntions extracted from the 14th layer of the Conformer within the BEST-RQ model. Our implementation\nof the VQ module was based on an open-source library,\u2074 with codebook size of 16384 and codebook\ndimensional with 32. By combining BEST-RQ and the VQ module, we can extract 50Hz semantic\ntoken sequences from the audio."}, {"title": "A.3 Latent diffusion model", "content": "As shown in Figure 1, we train a latent diffusion model (LDM) as a renderer, which converts a\n50Hz semantic token sequence into a 44.1kHz audio, such as songs, vocals, and instrumental music.\nIn contrast to DDPM [38], which directly models the raw training data, LDM operates on a low-\ndimensional latent space to significantly reduce the computational cost and improve the generation\nstability. Our implementation of the latent diffusion model was based on the open-source Stable\nAudio. The reproduced latent diffusion model is composed of a VAE and a U-NET-based conditioned\ndiffusion model.\nIn particular, for the VAE, we use the same encoder-decoder network architecture as in DAC. 6 For\nthe network configurations, the encoder (downsampler) uses strides of [4, 4, 8, 8], d_model of 128\nand latent_dim of 64, where the 64-dim matrix is employed as the mean and variance of VAE latents\n(in 32-dim). Besides, the decoder (upsampler) uses strides of [8, 8, 4, 4] and hidden channels of\n1536 to transform the 64-dim latents back to 44.1kHz audio. Based on this pre-trained VAE, we\nsubsequently train a diffusion model in a way similar to Stable Audio 1.0, except having 32-dim\nlatents as targets and semantic tokens as conditions."}, {"title": "B The details of all tasks supported by SongCreator", "content": "Benefiting from our specially designed attention mask strategy and multi-task training approach,\nSongCreator can effecively support the following eight tasks:\nLyrics-to-song This task aims to generate a complete song that includes harmoniously integrated\nvocal and accompaniment from lyrics. SongCreator supports to control various acoustic conditions\nin the generated song by providing optional prompts. The vocal prompt can control speaker, vocal\nmelody, and tempo, while the accompaniment prompt can control instruments, musical melody, and\nrhythm. The vocal prompt and accompaniment prompt can either be present simultaneously, exist\nindividually, or be absent altogether.\nLyrics-to-vocals This task aims to generate the vocals without accompaniment based on the given\nlyrics. In this case, the vocal prompt can be provided to control the speaker, melody, and tempo of\nthe generated vocals.\nAccompaniment-to-song The purpose of this task is to supplement the vocal track of a song\nbased on the given lyrics for a pre-determined accompaniment track. The vocal track in generated\nsong should complement the input accompaniment track to create a coherent song. Similar to\nlyrics-to-vocals, the generated vocals can also be controlled using the vocal prompt.\nVocals-to-song Contrary to the accompaniment-to-song task, the purpose of this task is to generate\nharmonious accompaniment for the input vocal track and combine them to create a coherent song.\nMusic continuation This task is expected to generate instrumental music, which is coherent with\nthe accompaniment prompt in terms of instruments, melody, harmony and rhythm.\nSong editing This task requires a model to alter a segment within a song to match a target lyrics.\nThe modified segment must be coherent with the unedited parts of the original song, i.e., maintaining\nthe speaker, instruments, melody and rhythm.\nVocals editing This task is similar to song editing, but the modification target is changed from the\ncomplete song to the vocals without accompaniment.\nVocals editing in song This is a unique capability of SongCreator, which modifies the content of\nthe vocal track in a song while keeping the original accompaniment track unchanged. It means that\nthe modified vocal segment not only maintains coherence with the unedited vocal track of the original\nsong but also harmonizes with the accompaniment in the original song."}, {"title": "C Detailed baseline settings", "content": "SongCreator (Single) Our proposed SongCreator is trained on multiple tasks. For comparison, we\nkeep the model's structure and hyperparameters and train it on different specific tasks, resulting in\nSongCreator (Single) for each task.\nGPT Inspired by UniAudio [51], we set up this baseline model, treating each task as a conditional\nlanguage modeling task. For each task, we first tokenize both the conditional and target audio\nusing BEST-RQ. Then, we concatenate the source-target pair as a single sequence and perform the\nnext-token prediction task using GPT [67]. Our implementation of GPT was based on an open-\nsource library, that contains 24 Transformer layers with 1024 hidden size and 4096 feed-forward\ndimensional. Finally, we convert the predicted semantic token sequence into audio by the pre-trained\nlatent diffusion model.\nMusicLM MusicLM [25] has demonstrated excellent performance in text-to-music generation.\nInspired by this, we attempted to employ its methods to lyrics-to-song and lyrics-to-vocals. Specif-\nically, to achieve this, we make some modifications to the open-source library. First, we replace\nthe MuLan in MusicLM with a lyrics encoder to better encode phoneme information, and replace\nw2v-BERT with BEST-RQ to more effectively extract semantic tokens from songs. Additionally,\nsince SoundStream [69] is not open-source, we used the widely adopted Encodec [70] as a substitute.\nOur reproduced MusicLM follows the same hyperparameters as [25], using 24 layers of decoder-only\nTransformers for both the semantic stage and acoustic stage.\nMusicGen In addition to MusicLM, MusicGen [22] is another SOTA model in text-to-music\ngeneration. Our implementation of MusicGen for lyrics-to-song is based on the official open-source\nlibrary. It directly predicts the acoustic tokens extracted by Encodec from the lyrics, without\nadditional semantic tokens. Similar to other baselines, we also use 24 Transformer layers to ensure\nthis model has approximately 0.6B parameters. Moreover, considering that MusicGen allows control\nthe generated output through prompts, we also compared it with our proposed SongCreator for the\nprompt-based lyrics-to-song evaluation.\nVALL-E Recently, language model-based text-to-speech models (e.g., VALL-E) have shown the\ncapability of generating high-quality personalized speech with a 3s acoustic prompt. Considering the\nsimilarity between text-to-speech and lyrics-to-vocals tasks, we attempted to directly apply VALL-E\nto the lyrics-to-vocals. Our implementation is based on the open-source library.10 To ensure a fair\ncomparison, both the autoregressive transformer decoder and the non-autoregressive transformer\ndecoder in VALL-E are composed of 24 layers, 16 attention heads, an embedding dimension of 1024,\nand feed-forward layers of dimensionality 4096. And we also compared the zero-shot voice cloning\nabilities of SongCreator and VALL-E.\nAudioLM To validate the performance of SongCreator in music continuation, we implement\nAudioLM [36] based on the open-source code.11 Similar to our settings with MusicLM, we replace\nw2v-BERT with BEST-RQ and Soundstream with Encodec in AudioLM. Additionally, we also used\na 24-layer decoder-only transformer structure for both the semantic and acoustic stages.\nSingSong SingSong [29] has demonstrated excellent performance in vocals-to-accompaniment\ngeneration. In this work, we reproduce SingSong based on our previous implementation of AudioLM\nand utilize it as a baseline for the vocals-to-song task. We follow the same setup as SingSong\n[29], which generates the accompaniment based on the vocals first and then mixes the vocals and\naccompaniment to produce the complete song. To eliminate the influence of the pre-trained latent\ndiffusion model, we also directly use it to convert the semantic tokens predicted by SingSong into\naudio without requiring an additional acoustic modeling stage. This new baseline is named SingSong\n(BEST-RQ)."}, {"title": "D The editing dataset", "content": "We performed insertion, deletion or substitution operations on the original lyrics, with editing spans\nranging from 1 to 15 words. Examples of the song editing dataset are shown in table 13."}, {"title": "E Results of the ABX preference test", "content": ""}], "equations": ["Q_v = H_v W_Q^v, K_a = H_a W_K^a, V_a = H_a W_V^a", "M_{ij} = \\begin{cases} 1,\\quad  \\text{allow to attend} \\\\ -\\infty,\\quad \\text{prevent from attending} \\end{cases}", "A_v = \\text{softmax} \\left(\\frac{Q_v K_a^\\top}{\\sqrt{d_k}} + M\\right)", "p(S_v | C_{\\text{lyrics}}, \\hat{S}_{v; \\text{vocal}}) = \\prod_{t=0}^T p(S_{v,t} | S_{v,<t}, S_{a, <t}, C_{\\text{lyrics}}, \\hat{S}_{v; \\text{vocal}})", "p(S_s | E_v, E_a; \\theta_{\\text{song}}) = \\prod_{t=0}^T p(S_{s,t} | S_v, S_a; \\theta_{\\text{song}})"]}