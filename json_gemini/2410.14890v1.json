{"title": "Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games", "authors": ["Yunhao Yang", "Leonard Berthellemy", "Ufuk Topcu"], "abstract": "We develop a method that integrates the tree of thoughts and multi-agent framework to enhance the capability of pre-trained language models in solving complex, unfamiliar games. The method decomposes game-solving into four incremental tasks-game summarization, area selection, action extraction, and action validation-each assigned to a specific language-model agent. By constructing a tree of thoughts, the method simulates reasoning paths and allows agents to collaboratively distill game representations and tactics, mitigating the limitations of language models in reasoning and long-term memorization. Additionally, an automated fine-tuning process further optimizes the agents' performance by ranking query-response pairs based on game outcomes, e.g., winning or losing. We apply the method to a non-cooperative game and demonstrate a 65 percent winning rate against benchmark algorithms, with an additional 10 percent improvement after fine-tuning. In contrast to existing deep learning algorithms for game solving that require millions of training samples, the proposed method consumes approximately 1000 training samples, highlighting its efficiency and scalability.", "sections": [{"title": "1 INTRODUCTION", "content": "While existing deep learning algorithms have demonstrated remarkable efficacy in tacking complex games such as chess and Go, these algorithms are computationally expensive. Existing algorithms typically rely on deep neural networks and reinforcement learning techniques to master these board games' intricate strategies and nuanced gameplay and require massive human-labeled data [16, 18, 22].\nThe emergence of pre-trained language models obviates the necessity of training deep learning models for game playing, but their constraints on knowledge [12], long-term memorization [2, 6, 9, 19], and reasoning make them inadequate for playing games beyond their knowledge domain [15, 24, 26]. Although several works address constraints [5, 10, 13, 20, 25] through constructing abstractions or representations of the language outputs, they are inadequate or exceedingly inefficient in the phase of encapsulating the exponential growth of the size of game states.\nWe develop a method that integrates tree of thoughts [27] and multi-agent framework [3, 21] to distill game representations and tactics from language models. The method decomposes the distillation process into four smaller, incremental tasks, with four language models designated as agents, each responsible for a specific task."}, {"title": "2 RELATED WORKS", "content": "Existing deep learning methods, such as AlphaGo [22], AlphaZero [18] and DeepChess [4] have showcased the ability to surpass human experts through extensive self-play and neural network training. However, these algorithms require extensive training and large computational power.\nExisting approaches [1, 8, 11, 23] ask language models to decide the tactics in a game. They have shown the capability of language models in playing games such as Tik-Tac-Toe and chess. However, they rely on the language model containing prior knowledge of the game, hence inadequate to newly designed games beyond the language model's knowledge."}, {"title": "3 PRELIMINARY", "content": "Non-Cooperative Game. The extensive form of a non-cooperative game a tuple $G = (P, Q, F, I, A, T, U)$, where\n\u2022 $P = {1, ..., p}$ is a set of $p$ rational Players.\n\u2022 $Q = {q_1, ..., q_k}$ is a set of Game States.\n\u2022 $F = {q_f, ..., q_{fa}} \\subset Q$ is a set of Termination States.\n\u2022$I: P \\times Q \\rightarrow Q$ is a function that maps a player and a game state to another game state. It indicates the game state from the player's perspective. In full-information games, the function maps each state to itself. In partial-information games, it may map a state to other states due to the player's misinformation.\n\u2022 $A = {a_1, ..., a_m}$ is a set of Actions that a player can take.\n\u2022$T: Q \\times P \\times A \\rightarrow Q$ is a transition function. It transits from one state to another based on which player is taking which action.\n\u2022 $U : F \\rightarrow R^P$ is a Utility function that returns a list of $p$ numerical values indicating the utility of $p$ players at the termination state.\nGoedendag. A variant of the game Stratego that is under development. It is a two-player board game where each player commands an army of pieces, each with a specific rank, and the objective is to capture the opponent's flag (a special piece). We present the game rules in Section 4 and more detailed descriptions of this game in the supplementary.\nIt is a non-cooperative game $G = (P, Q, F, I, A, T, U)$ with the following components:\n\u2022 $P = {p_1, p_2}$, there are two players in the game.\n\u2022 $Q = {q_1, ..., q_k}$, each state corresponds to a layout of a game board, including the location and rank of each piece.\n\u2022 $F \\subset Q$, each terminate state indicates a player wins or draws the game.\n\u2022$I: P \\times Q \\rightarrow Q$, each player does not know all the ranks of the opponent's pieces.\n\u2022 $A = {a_1, ..., a_m}$, each action indicates a player's move of a piece.\n\u2022$T: Q \\times P \\times A \\rightarrow Q$.\n\u2022 $U : F \\rightarrow {-1, 0, 1}^P$, a player can win, lose, or draw."}, {"title": "Language Model for Game Solving", "content": "A language model takes a text-based input prompt and outputs a text-based response. By formulating the input prompts, we can query the language model to generate tactics (actions), understand game states, or predict utilities. However, directly querying the language model to solve games requires the language model containing prior knowledge of the game, e.g., well-known games such as chess and tic-tac-toe.\nWe aim to use language models to solve unseen games whose rules are beyond their knowledge domain. In this work, we choose Goedendag-a variant of an existing game that is not released publicly-as the unseen game and use language models to extract game tactics. At a high level, we propose a method that iteratively sends game-relevant information to language models and queries for tactics/actions until reaching a termination state.\nThe proposed method formulates task-specific prompts to guide the language models such that the action a returns from the language model satisfies: (1) For a player $p \\in P$ at a state $q \\in Q$, $a \\in R(q, p)$, i.e., the action complies with the game rule. (2) Maximize the player's utility."}, {"title": "4 METHODOLOGY", "content": "Consider a non-cooperative game $G = (P, Q, F, I, A, T, U)$ with a written description of the game rules R. We design a method that enhances pre-trained language models' ability to solve complex, unfamiliar games by integrating the tree of thoughts framework with a multi-agent system. The method decomposes the game-solving process into four tasks: game summarization, area selection, action selection, and action validation. We assign each task to a separate pre-trained language model, denoted a language-model agent, allowing for a collaborative and structured approach to game representation and decision-making.\nThe method employs the tree of thoughts to simulate multiple reasoning paths, enabling agents to extract game representations and tactics (actions). In particular, the summarization and area selection tasks aim to extract and estimate a game representation from the current state. The tree of thoughts passes the game representation to the agent for area selection. Then, the action selection task aims to extract a game tactic for the current player at the current state. The validation task checks whether the extracted tactic complies with the rules, provided by the written descriptions, and signals to other agents if the tactic fails to meet the rules. We demonstrate the method in Figure 2.\nIn this section, we use the Goedendag as a running example to clarify the method.\nOverview. Given the game $G = (P, Q, F, I, A, T, U)$, the method starts from a player $p\\in P$ at a state $q \\in Q$. The method guides four language-model agents working collaboratively to extract a game tactic, i.e., action, $a \\in A$. If the action a complies with the rules: $a \\in R(q, p)$, the game will transit to a new state $q' = T(p, q, a)$.\nIf the new state $q'$ is one of the termination states: $q' \\in F$, the method stops the game and computes the utility for the player: $U (q')_p$. Otherwise, the method extracts a new action given the new state and repeats this procedure until reaching a termination state.\nPrompt Format. Recall that we assign four language-model agents to four tasks and each language model $M_i$ takes the rule descriptions and a task-specific prompt as inputs, and returns text-based responses. Using the tree of thoughts to integrate the four agents, they collaboratively return an action $a \\in A$ for the current player $p\\in P$. We send the same rule description and different task-specific prompts to each language-model agent, in the following format:\n<Written Descriptions of Rules>.\nYou are player <player_number>.\n<Task-Specific Prompt>.\nThe written descriptions of rules are global information included in the prompts to all the language-model agents, while the task-specific prompts are different for each agent.\nThe written description of rules R are text-based game manuals provided by the game designer. In the Goedendag example, a written description of rules includes the description of the categories of pieces, the rules of moving the pieces, and the winning conditions:\n1. Players: player 0 and player 1. Each player will play turn by turn.\n2. Types of pieces: Rank 1, ......, Rank 10, Bomb, and Flag.\n3. Moving pieces: All pieces are movable except the bombs and the flag.\n4. Attacking pieces: The piece moving into the tile is the Attacker and the piece that was already occupying that tile is the Defender......\n5. Winning conditions: One of the player's pieces moves to the tile of the opponent's flag.\nWe attached the complete description in the supplementary.\nA task-specific prompt includes instructions to the language-model agent on what information we want to retrieve and the"}, {"title": "4.1 Tasks", "content": "The method breaks the game-solving procedure into four tasks and assigns four language-model agents to these tasks. The method builds a tree of thoughts that formulates task-specific prompts to guide the agents in completing their tasks.\nSummarization. The first task is using a language-model agent M\u2081 to extract a text-based game representation of the current game state. We denote this extraction procedure as summarization.\nRecall that every agent takes the rule description R and a task-specific prompt as inputs. We present the task-specific prompt for M\u2081 below.\nThe game state description is a text-based description of the current game state. In the Goedendag example, a game state description consists of the size of the game board, the positions of both players' pieces, and their categories:\nM\u2081 returns a text-based game representation rep(G), which encodes the current game state $q \\in Q$, the current player p, a subset of allowable actions $A' \\subset A$, and a set of all possible state transitions ${T(q, p, a): a \\in A'}$. Note that the game state descriptions can be varied depending on the game state. In contrast, the text-based game representation is in a formalized, consistent structure across the entire game. We present a sample game representation in the Goedendag game below.\nArea Selection. Define area Q' as a subset of the game states: $Q' \\subset Q$. Recall that the game states transit based on the current state, the current player, and the action: $T: Q\\times P\\times A \\rightarrow Q$. Given the current game state $q \\in Q$ and the current player $p \\in P$, the area selection task aims to produce a subset of states $Q' \\subset Q$ such that\n$\\forall q'\\in Q' \\exists a\\in A T(q, p, a) = q'$.\nBy fixing the size of the area, i.e., the number of states in Q', the method avoids the need to memorize the exponentially grown game states. Instead, the method only memorizes the selected area, which results in a linear growth in the memory.\nIn the area selection task, the method formulates a task-specific prompt that consists of the text-based game representation rep(G) obtained from M\u2081 and an instruction on selecting a subset of game states.\nA language-model agent M2 takes the descriptions of rules R and the task-specific prompt as inputs. It returns a text-based representation rep(G') that encodes the information (e.g., states, player, actions, possible transitions) of G', where G' = (p, Q', F, I, A, T, U), $p\\in P$ is the current player and $Q' \\subset Q$ is the set of selected states satisfying Equation 1. We show an example of the representation with a selected area in the next blue text box.\nIn the Goedendag game, game states correspond to the layouts of the game board. Hence, we can choose a subset of states by choosing a sub-area on the board. The task-specific prompt is:\nAction Selection. Given a game representation rep(G') for the selected area returned from M2, where $G' = (p, Q', F, I, A, T, U)$, the method then extracts an action a such that T(q, p, a) = q' and q, q' \u2208 Q'.\nIn particular, a language-model agent M3 takes the description of rules R and a task-specific prompt as inputs, then outputs an action a. The task-specific prompt consists of the representation rep(G') (e.g., the previous blue text box) and a short instruction \"choose an action from the selected area.\"\nIn the Goedendag example, the method selects an action that moves a piece located within the red circle in Figure 3.\nAction Validation. Given the game representation rep(G) from M\u2081 (summarization task), the representation rep(G') with the selected area from M2, and the selected action a from M3, the method uses a language-model agent M4 to determine whether the action complies with the rules described in R.\nThe task-specific prompt for the action validation task consists of rep(G), rep(G'), a, and instruction \"Does this action comply with all the rules of the game?\"\nThe agent M4 takes the description of rules and the task-specific prompt as input, then returns a signal indicating whether the action complies with the rules. The agent returns one of the following three signals:\n(1) If M4 returns \"approval,\" the player can take this action and proceed to the next game state.\n(2) If M4 returns \"area rejection,\" indicating the selected area does not include any valid action, hence the method passes the rejection signal to M2 and requests a new round of area selection.\n(3) The \"action rejection\" signal indicates valid actions exist in the selected area but the selected action violates the rules. If M4 rejects the action, the method passes the rejection signal to M3 and queries for a different action a'."}, {"title": "5 EXPERIMENT", "content": "We apply the proposed method to play the Goedendag game against several benchmark algorithms. Recall that the Goedendag game requires two players. One player uses the proposed method to decide actions and the other uses the benchmark algorithm. We first evaluate the method by the two metrics described in Section 5.1 and demonstrate how the proposed method outperforms the benchmarks.\nThen, we perform a set of ablation studies to demonstrate the necessity of the four agents by removing one of the agents and showing performance degradation.\nLastly, we fine-tune the language models and show that the fine-tuned models further improve the proposed method."}, {"title": "5.1 Experiment Setting", "content": "Evaluation Metric. We use two metrics to evaluate the proposed method and other benchmarks.\n(1) Win Rate: Let $p_i$ and $p_j$ be the two players, $w_1$ be the number of games $p_1$ wins and $w_2$ be the number of games $p_2$ wins, the win rate $r_w(p_i)$ of player $p_i$ is\n$r_w(p_i) = \\frac{w_1}{w_1 + w_2}$\nWhen we compute the win rates, we only count the games where a winning player exists. If a player returns actions that do not comply with the rules, we randomly select an action that meets the rules for the player.\n(2) Error Rate: Let $\u00c3_i = \\bar{a}_1\\bar{a}_2\\bar{a}_3......$ be a sequence of actions player pi takes in the game, from the initial state to the termination state. Let $\\epsilon_i$ be the number of actions $\\bar{a}_j \\in \u00c3_i$ that violate the rules described in R, |P| be the number of players, and |\u00c3\u00a1| be the total number of actions pi have taken to complete the game, the error rate of one game $\\varepsilon$ is the number of actions violating the rules over the total number of actions:\n$\\varepsilon = \\sum_i^{|P|} \\frac{\\epsilon_i}{\\|\u00c3_i\\|}$\nIf the proposed method or the benchmark returns actions that violate the rules, we replace these actions with randomly generated actions that comply with the rules and proceed to the next state, avoiding the game being interrupted by the rule violations."}, {"title": "5.2 Benchmark Comparison", "content": "We select three benchmark methods to determine actions for a player. For a game $G = (P, Q, F, I, A, T, U)$, all the benchmark methods return an action $a \\in A$ for the player $p\\in P$ based on the current state $q \\in Q$.\n(1) Direct Query: This method directly sends the written descriptions of rules R and the game state description (defined in the summarization task in Section 4.1) into a language model and queries for an action:\nThe language model returns a text-based description to a selected action $a \\in A$, in the same format with M3's outputs in the action selection task.\n(2) Chain-of-Thought (CoT) Query: This benchmark method builds an input prompt based on the chain-of-thought technique [14]. The prompt format is\nA language model takes the descriptions and instructions as inputs, then outputs an action $a \\in A$ in natural language, e.g., \"move the Rank 1 piece from (0,0) to (1,0).\"\nThe CoT query method uses a single zero-shot language model, as opposed to multiple language models in our method, hence it is less costly. We compare it against our method to show the performance-cost trade-off. In contrast to the direct query method, the CoT query enables the language model to reason through the game before determining the action.\n(3) Random: This method first extracts all the valid actions at the current game state and then randomly selects an action a from the set of valid actions. If there are n valid actions, then the probability of each action being chosen is 1/n.\nWe evaluate our proposed method against the three benchmarks over the win and error rates. In the experiments, we use GPT-40-mini in our method and benchmarks that require language models. We conduct pair-wise evaluations, where each evaluation consists of 5 sets of games, and each set includes 40 games. We split each"}, {"title": "5.3 Ablation Study", "content": "We perform a set of ablation studies to indicate the necessity of the tasks in our proposed method. Recall that the method consists of four tasks: summarization, area selection, action selection, and validation. The former two tasks extract game representations and the latter two tasks decide actions. While the tasks for deciding actions are mandatory, we remove the summarization and area selection tasks and observe how these two tasks impact the performance.\nWe revise the tree of thoughts in the proposed method and formulate two variances of our method, denoted as ours w/o area and ours w/o summary.\nOurs w/o summary skips the summarization task and directly passes the game state description into the area selection task. The rest of the tasks proceed as normal.\nOurs w/o area skips the area selection task. The method passes the text-based game representation directly to the action selection task and removes the backward communication from action validation to area selection.\nWe evaluate the two variances of the method against several benchmarks and present the results in Table 1 and 2. From Table 1, we observe that removing the summarization task decreases the win rate by approximately 5 percent while removing the area selection task decreases the win rate by 2 percent. From the perspective of winning the game, the summarization task distills a complete game representation that helps the understanding of the current game state, leading to a significant improvement in the win rate.\nOn the other hand, Table 2 shows that our method without area selection results in a 5x higher error rate compared to our method with area selection. The area selection task distills a game representation for a subset of states (i.e., a sub-area of a board), significantly lowering the number of actions to choose from. Hence, the language-model agent for the action selection task can easily determine an action that complies with the rules from a much smaller set of actions. Therefore, we have demonstrated the necessity of both tasks in our method."}, {"title": "5.4 Fine-Tuning", "content": "To further refine our approach, we implement a self-play strategy, where our method competes against itself to gather input-output pairs from the four tasks performed by the winning agent. These collected pairs are subsequently used to fine-tune the language-model agents. Due to the limitation of the computing resources, we only collect data and fine-tune the agents for area selection and action selection tasks. In all the fine-tuning procedures, we use the default supervised fine-tuning algorithm provided by OpenAI [17] with early stopping (at convergence) [7] to fine-tune the GPT-40-mini we used in our experiments.\nWe collect 1060 input/output pairs from each of the two tasks (area and action selection) across 100 games from the 4x4 board. Then, we fine-tune three language-model agents:\n\u2022 Area agent: a language model fine-tuned by the data collected from the area selection task.\n\u2022 Action agent: a language model fine-tuned by the data collected from the action selection task.\n\u2022 End-to-end agent: a language model fine-tuned by all the collected data.\nWe present the fine-tuning losses for the three agents in Figure 5. Subsequently, we substitute the original agents in our method with the fine-tuned agents in various configurations to evaluate how the fine-tuning influences the win rates. We assess four replacement strategies:\n(1) replacing only the original agent for the area selection task with the fine-tuned area agent,\n(2) replacing only the agent for the action selection task with the fine-tuned action agent,\n(3) replacing agents for both tasks with their respective fine-tuned agents,\n(4) replacing agents for both tasks with the end-to-end fine-tuned agent.\nFigure 6 shows the win rate of our method, employed with the four strategies, against the benchmarks and our method before refinement. We fine-tune the agents over 4x4 boards and validate them over 10x10 boards. We show an average of 10 percent improvement over the win rates, demonstrating the effectiveness of our fine-tuning procedure. Additionally, fine-tuning each agent independently achieves significantly higher win rates compared with using a single end-to-end model for multiple tasks, showcasing the necessity of our multi-agent approach."}, {"title": "6 CONCLUSION", "content": "We develop a method that integrates the tree of thoughts with a multi-agent framework, effectively enhancing pre-trained language models' ability to solve complex and unfamiliar games. The method alleviates the limitation of language models on long-term memorization and reasoning by breaking down the game-solving process into incremental tasks. The results, including a 65 percent winning rate with an additional 10 percent improvement post-fine-tuning, demonstrate the method's potential, especially given its efficiency in requiring only a fraction of the training samples compared to traditional deep learning approaches.\nFor future research, this method can be extended to a wider variety of game types, such as cooperative games. Additionally, constructing a memory bank to retain past game states could further enhance the method's long-term memorization capabilities. Another avenue for improvement lies in refining the fine-tuning process by implementing more advanced ranking mechanisms based on detailed game metrics, which may result in further performance enhancements."}]}