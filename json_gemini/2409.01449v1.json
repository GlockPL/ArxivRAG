{"title": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning", "authors": ["Esraa Elelimy", "Adam White", "Michael Bowling", "Martha White"], "abstract": "Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform other recurrent architectures across several partially observable environments while using significantly less computation.", "sections": [{"title": "1 Introduction", "content": "Agents, animals, and people perceive their surrounding environment through imperfect sensory observations. When the state of the environment is partially observable, agents must construct and maintain their own state from the stream of observations. The constructed agent state summarizes past environment-agent interactions in a form that is useful to predict and control future interactions [40]. Recurrent Neural Networks (RNNs) provide a flexible architecture for constructing agent state [18, 20, 12, 5, 9].\nWhile standard RNN architectures have been mainly supplanted by Transformers [43], in online reinforcement learning settings where the agent learns while interacting with the environment, RNNs remain a promising direction to pursue [16, 10]. There are two main issues that limit the use of self-attention mechanisms from Transformers in online learning. First, calculating the similarity between each pair of points results in a computational complexity that is a function of $k^2$, where k is the sequence length. Moreover, calculating the similarity between all pairs ignores the temporal order of the data points, which limits the usefulness of self-attention when the data is temporally correlated [46]. Second, we need access to the whole sequence of observations before taking an action or updating the learnable parameters, which is impractical in continual learning. While recent works have reduced the complexity of transformers from quadratic in the sequence length to linear [14, 15, 36], the full sequence length is still needed to train such architectures. Gated Transformer-XL is an attempt to overcome this issue by keeping a moving window of previous observations [34]. A window of past observations does not scale well to long sequences-the computation is quadratic in the sequence length and a window is one particular fixed function of history. The simpler recursive form in RNNs, on the other hand, can learn a great variety of functions of history from the data and is well suited for updating the state online for a sequential data stream and have been shown to outperform transformers in such settings [22]."}, {"title": "2 Background", "content": "We formalize the problem setting as a Partially Observable Markov Decision Process (POMDP). At each time step t = 0, 1, 2, \u2026, the agent perceives an observation $x_t$, a limited view of the state $s_t \u2208 S$, and takes an action $A_t \u2208 A(s_t)$. Depending on the action taken, the agent finds itself in a new state $s_{t+1} \u2208 S$, observes the corresponding observation $x_{t+1}$ and a reward $R_{t+1} \u2208 R$. In the online control setting, the agent's goal is to maximize the sum of received rewards. It may also make predictions about its environment, such as future observations' outcomes.\nFor prediction and control in a partially observable environment, the agent should use the stream of observations to construct its agent state. The agent state summarizes information from the history of the agent-environment interactions that are useful for prediction and control [40]. We could use the whole history up to t, namely $(x_0, A_1, R_1, x_1, A_2, R_2, . . . x_t)$, as the agent state. Though the history preserves all the information, it is not feasible to use directly. We want the agent to have constant memory and computation per time step and storing the whole history causes the memory and the computation to grow with time. Instead, the agent needs to compress this history into a concise representation. We refer to the agent's internal representation of the history at time t as its agent state or its hidden state $h_t$. The agent constructs its current agent state $h_t \u2208 R^n$ from its previous agent state $h_{t\u22121} \u2208 R^n$ and the recent observation $x_t \u2208 R^d$ using a state-update function $g: R^n \u00d7 R^d \u2192 R^n$: $h_t = g(h_{t\u22121}, x_t)$.\nOne way to learn this state-update function g is with a recurrent neural network (RNN). A simple form is a linear recurrent layer, where $g(h_{t\u22121}, x_t) = W_xx_t + W_hh_{t\u22121}$ for weight matrices $W_x$ and $W_h$. We can also add a nonlinear activation, such as ReLU."}, {"title": "3 Recurrent Trace Units", "content": "In this section, we introduce Recurrent Trace Units (RTUs). We start by revisiting why complex- valued diagonals represent dense recurrent layers, and why using real-valued diagonals is insufficient. We then introduce the specific form for RTUs that leverages this relationship. We then provide the RTRL update for RTUs, highlighting that it is simple to implement and linear in the hidden dimension. We finally contrast RTUs to LRUs and motivate why this small extension beyond LRUs is worthwhile."}, {"title": "3.1 Revisiting Complex-valued Diagonal Recurrence", "content": "Assume we have the recurrence relationship, with learnable parameters $W_h \u2208 R^{n\u00d7n}$ and $W_x \u2208 R^{n\u00d7d}$, $h_t = W_hh_{t\u22121} + W_x u(x_t)$, where u can be any transformation of the inputs $x_t$ before they are inputted into the recurrent layer. We can rewrite the square matrix $W_h$ using an eigenvalue decomposition $W_h = PAP^{-1}$, where P contains the n linearly independent eigenvectors and $\u039b \u2208 C^{n\u00d7n}$ is a diagonal matrix with the corresponding eigenvalues. Then we have that\n$h_t = P(\u039bP^{\u22121}h_{t\u22121} + P^{\u22121}W_x u(x_t)) \u21d2 P^{\u22121}h_t = \u039bP^{\u22121} h_{t\u22121} + P^{\u22121} W_x u(x_t)$\nBy defining $h_t = P^{\u22121} h_t \u2208 C^n$ and $W_x = P^{\u22121}W_x \u2208 C^{n\u00d7d}$, we get a new recurrence\n$h_t = \u039bh_{t\u22121} + W_x u(x_t)$.\nWe can see $h_t$ and $h_t$ are representationally equivalent: they are linearly weighted for downstream predictions, and so the linear transformation on $h_t$ can fold into this downstream linear weighting. But it is more computationally efficient to use $h_t$ with a diagonal weight matrix $\u039b$, meaning each hidden unit only has one recurrent relation instead of n. LRUs precisely leverage this equivalence [33]. Specifically, they learn a complex-valued $h_t$, and use $Re(Wh_t)$ as an input to an MLP for downstream nonlinearity.\nSince we did not impose constraints on the matrix $W_h$, other than being diagonalizable, the eigenval- ues of $W_h$ can be complex or real numbers. Previous diagonal RNNs such as eLSTM [16], Columnar networks [17], and IndRNN [19] use only real-valued diagonal matrices. Having only real-valued diagonals assumes that the matrix $W_h$ is symmetric. We provide a small experiment in Appendix A.3"}, {"title": "3.2 The RTU Parameterization", "content": "A complex number can be represented in three ways: a + bi (the real representation), $rexp(i\u03b8)$ (the exponential representation), and $r(cos(\u03b8) + i sin(\u03b8))$ (the cosine representation). Mathematically, these three representations are equivalent, but do they affect learning differently? Orvieto et al. [33] empirically showed that using the exponential representation resulted in a better-behaved loss function than the real representation on a simple task; we provide some discussion in Appendix D.1 further motivating why the real representation is less stable. We chose instead to pursue the cosine representation, because it allows us to represent the complex hidden vector as two real-valued vectors. The remainder of this section outlines RTUs, with and without nonlinearity in the recurrence.\nOur goal is to learn a complex-valued diagonal matrix with weights $\u03bb_k = r_k(cos(\u03b8_k) + i sin(\u03b8_k))$ on the diagonal, for k = 1,...,n. Multiplying by a complex number is equivalent to multiplying by a 2x2 block matrix with a rescaling. We can use this rotational form to avoid explicitly using complex numbers, and instead use two real-values for each complex-valued hidden node. We write this real-valued matrix $\u039b \u2208 R^{2n\u00d72n}$ as blocks of rotation matrices\u00b9\n$\u039b =\\begin{bmatrix} C_1 & &\\\\ & ... & \\\\ & & C_n\\end{bmatrix}$ where $C_k = r_k\\begin{bmatrix} cos(\u03b8_k) & -sin(\u03b8_k) \\\\ sin(\u03b8_k) & cos(\u03b8_k)\\end{bmatrix}$ (1)\nEach element of $h_t = \u039bh_{t\u22121} + W_x x_t \u2208 R^{2n}$ has two components $h_t^1, h_t^2$, updated recursively\n$h_t^1 = r cos(\u03b8) h_{t\u22121}^1 \u2212 r sin(\u03b8) h_{t\u22121}^2 + W_1x_t,$\n$h_t^2 = r cos(\u03b8) h_{t\u22121}^2 + r sin(\u03b8) h_{t\u22121}^1 + W_2 x_t$.\nWe finally combine the new recurrent states into one state $h_t = [f(h_t^1); f (h_t^2)]$, potentially using a nonlinearity f after the recurrence.\nWe also adopt two parameterization choices made in LRUs that showed improved performance. The first is learning logarithmic representations of the parameters rather than learning them directly: instead of learning r and \u03b8, the network learns $v_{log}$ and $\u03b8_{log}$, where $r = exp(\u2212v), v = exp(v_{log})$, and $\u03b8_{log} = log(\u03b8)$. This re-parametrization restricts the r to be $\u2208 (0, 1]$, required for stability. We found these modifications to improve stability of RTUs (see Appendix C). The second parameterization choice we adopt from LRUs is to multiply by the input $(W_xx_t)_k$ with a normalization factor of $\u03b3_k = (1 \u2212 r)^{1/2}$. Putting this all together, the final formulation of RTUs is\n$h_t^1 = g(v_{log}, \u03b8_{log}) \u2299 h_{t\u22121}^1 \u2212 \u03c6(v_{log}, \u03b8_{log}) \u2299 h_{t\u22121}^2 + \u03b3 \u2299 W_1x_t,$\n$h_t^2 = g(v_{log}, \u03b8_{log}) \u2299 h_{t\u22121}^2 + \u03c6(v_{log}, \u03b8_{log}) \u2299 h_{t\u22121}^1 + \u03b3 \u2299 W_2x_t,$\n$h_t = [f(h_t^1); f(h_t^2)]$. (2)\nwhere $\u03b3 \u2208 R^n$ is the vector composed of $\u03b3_k = (1 - exp(-exp(v_{log}))^{2})^{1/2}$ and\n$g(v_k, \u03b8_k) = exp(\u2212exp(v_{log})) cos(exp(\u03b8_{log})),$\n$\u03c6(v_k, \u03b8_k) = exp(\u2212exp(v_{log})) sin(exp(\u03b8_{log}))$. (3)\nNote that \u03b3 can be absorbed by W, and so does not change representation capacity."}, {"title": "3.3 The RTRL Update for RTUs", "content": "This section shows the RTRL updates for RTUs with more in-depth derivations in Appendix C. To keep notation simpler, we write the updates as if we are directly updating r and \u03b8; the updates for $v_{log}$ and $\u03b8_{log}$ are easily obtained then using the chain rule. The full derivation is in Appendix C.2.\nConsider the partial derivative with respect to $r_1$ for the first RTU with input $x_1 = (W_1x_t)_1$\nThen\n$\\frac{\u2202L_t}{\u2202r_1} = \\frac{\u2202L_t}{\u2202h^{c_1}} \\frac{\u2202h^{c_1}}{\u2202r_1} +  \\frac{\u2202L_t}{\u2202h^{c_2}} \\frac{\u2202h^{c_2}}{\u2202r_1}$\nbecause $r_1$ only impacts the two units in the first RTU, and derivatives w.r.t. the remaining hidden units are zero. Therefore, we just need to keep track of the vector of partial derivatives for these two components, $e^{r,c_1}_t = \\left[\\frac{\u2202h^{c_1}}{\u2202r_1}, ..., \\frac{\u2202h^{c_1}}{\u2202r_n}\\right]$ and $e^{r,c_2}_t = \\left[\\frac{\u2202h^{c_2}}{\u2202r_1}, ..., \\frac{\u2202h^{c_2}}{\u2202r_n}\\right]$ with recursive formulas\n$e^{r,c_1}_t = cos(\u03b8) \u2299 h_{t-1} + r cos(\u03b8) \u2299 e^{r,c_1}_{t-1} - \\frac{1}{2} \\frac{\\sqrt{1 - r^2}}{r} \\sqrt{1 - r^2} W_1x_t$\n$e^{r,c_2}_t = cos(\u03b8) \u2299 h_{t-1} + r cos(\u03b8) \u2299 e^{r,c_2}_{t-1} + \\frac{1}{2} \\frac{\\sqrt{1 - r^2}}{r} \\sqrt{1 - r^2} W_2x_t$\nWe can similarly derive such traces for \u03b8. The update to r involves first computing $\\frac{\u2202L}{\u2202h}$, using backpropagation to compute gradients back from the output layer to the hidden layer; this step involves no gradients back-in-time. Then r is updated using the gradient on $L e^{r,c_1}_t + L e^{r,c_2}_t$, which is linear in the size of $r \u2208 R^n$, as the vectors $e^{r,c_1}_t$, $e^{r,c_2}_t \u2208 R^n$ can be updated with linear computation in the above recursion. This update is the RTRL update, with no approximation."}, {"title": "3.4 Contrasting to LRUS", "content": "RTUs are similar to LRUs, with two small differences. First, RTUs have real-valued hidden units, because the cosine representation is used instead of the exponential representation. Second, RTUs use nonlinear activations in the recurrence, making them no longer linear. Though again a minor difference, we find that incorporating nonlinearity in the recurrence can be beneficial. RTUs can be seen as a small generalization of LRUs, moving away from strict linearity\u2014and thus motivating the name change\u2014but nonetheless a generalization we find performs notably better in practice.\nLet us now motivate the utility of moving to a cosine representation and real-valued traces. LRUs parameterize each hidden unit with $\u03bb_k = r_k exp(i\u03b8_k) = exp(\u2212 exp(v_{log}) + i exp(\u03b8_{log}))$ and directly work with complex numbers. Consequently, the hidden layer cannot be directly used to predict real- values. It would be biased to take $Re(h_t)$ (see Appendix D.2), and instead an additional weight matrix $W\u2208 C^{n\u00d7n}$ must be learned, to get $Re(W h_t)$. To understand why this works, assume that we took the original $h_t$ from the dense NN, and handed it to an MLP. This would involve multiplying $Wh_t$ for some W. If we set $W = WP$, then $Wh_t = WPP^{\u22121}h_t = Wh_t$ and we did not introduce any bias. In fact, if W is set this way, we do not need to take the real-valued part, because the output of $Wh_t$ is real-valued. Of course, learning does not force this equivalence\u2014in fact this parameterization is more flexible than the original\u2014and so it is necessary to take the real-part.\nRTUs avoid some of these complications by explicitly writing the recurrence and updates with real- valued hidden states. Implicitly, the relationship between the two real-valued hidden vectors forces"}, {"title": "4 Online Prediction Learning", "content": "In this section, we explore different architectural variants of RTUs and LRUs in a online prediction task and then move on to study the tradeoffs between computational resources and performance when using RTUs with RTRL compared to GRUs and LRUs with T-BPTT."}, {"title": "4.1 Ablation Study on Architectural Choices for RTUs and LRUS", "content": "In this first experiment, we investigate the impact of several architectural choices on the performance of RTUS and LRUs varying where nonlinearity is applied. We use a simple multi-step prediction task called Trace conditioning [38] inspired by experiments in animal learning. The agent's objective is to predict a signal-called the US-conditioned on an earlier signal--the CS. The prediction is formulated as a return, $G_t = \u03a3_{k=0} \u03b3^kUS_{t+k+1}$, where the agent's goal is to estimate the value function for this return. More details on this environment and experimental settings are in Appendix E. Figure 1 summarizes the results."}, {"title": "4.2 Learning under resources constraints", "content": "In this section, we investigate the tradeoffs between computational resources and performance when using RTUs with RTRL compared to GRUs and LRUs with T-BPTT.\nIn the following experiments, all agents consist of a recurrent layer followed by a linear layer generating the prediction. We measure performance of the agents online by calculating the Mean Square Return Errors (MSRE), which is the mean squared error between the agent's prediction at time t and Gt. In all the experiments, we used the Adam optimizer. We first ran each agent with different step sizes for 5 runs and 2 million steps. We then averaged the MSRE over the 2 million steps and selected each agent's best step size value. Finally, we ran the agents with the best step size value for 10 runs, which we report here. We also report all agents' step size sensitivity curves in Appendix E.3.\nLearning under computational constraints: We first investigate: how well do different agents exploit the available computational resources? We specified a fixed computational budget of 15000"}, {"title": "5 Real-Time Recurrent Policy Gradient", "content": "This section first highlights some differences in using linear RTRL methods, i.e., RTRL with linear complexity, in incremental and batch settings. We then investigate different ways of integrating linear"}, {"title": "5.1 Linear RTRL Methods in Incremental and Batch Settings", "content": "The benefits of linear RTRL methods over T-BPTT are more evident in the incremental rather than the batch setting. In the incremental learning setting, where the agent updates its parameters after each interaction step, linear RTRL methods have a constant computational complexity per update that depends only on the number of parameters. In contrast, T-BPTT methods have a complexity proportional to the truncation length T since T-BPTT methods require storing a sequence of past T activations to perform one gradient update. Figure 3 shows the time it takes to make one update with linear RTRL and T-BPTT given the same number of parameters. For T-BPTT, the time to make one update scales with the truncation length T, while for linear RTRL, it is constant.\nThe computational analysis for the batch setting is different than the incremental setting. In the batch setting, linear RTRL still have a constant cost per update and provide an untruncated yet stale gradient for all the samples. When using T-BPTT in the batch setting, there are two possibilities for the gradient updates. The first option, the typical strategy, is to divide the batch into non-overlapping sequences, each of length T, and perform T-BPTT on each sequence. In this case, the cost of one gradient update per sequence is a function of T, resulting in an effective update cost per sample independent of T. As a result, T-BPTT is computationally efficient in this case, albeit at the expense of a worse gradient estimate; in each sequence, only the last sample has a gradient estimate with T steps [24]. Figure 3 shows the time it takes to make one batch update with linear RTRL and T-BPTT given the same number of parameters. In this case, both methods use similar time per update. The second option is to divide the batch into overlapping sequences, where each gradient uses a sequence of T steps [24]. This approach increases the cost of updates per sample to be proportional to T, as in the incremental setting, with the benefit of better gradient estimates. However, all standard implementations of RL methods with T-BPTT use the computationally efficient option [37, 13, 23].\nIntegrating Linear RTRL Methods with PPO When performing batch updates, as with PPO, the RTRL gradients used to update the recurrent parameters will be stale, as they were calculated during the interaction with the environment w.r.t old policy and value parameters. One solution to mitigate the gradient staleness is to go through the whole trajectory after each epoch update and re-compute the gradient traces. However, this can be computationally expensive. In Appendix F, Algorithm 1, we provide the pseudocode for integrating RTRL methods with PPO with optional steps for re-running the network to update the RTRL gradient traces, the value targets, and the advantage estimates. We also performed an ablation study to investigate the effect of the gradient staleness in RTRL when combined with PPO, Appendix F. The results from the ablation study show that using a stale gradient results in better performance with RTUs and suggests that the staleness might help PPO maintain the trust region."}, {"title": "6 Experiments in Memory-Based Control", "content": "In this section, we evaluate the memory capabilities of RTUs when solving challenging RL control problems. We divide the problems in this section based on the source of partial observability. 1) Missing sensory data, where we mask out parts of the agent's observation. The agent must accumulate and integrate the sensory observations over time to account for the missing information. 2) Remembering important cues, where the agent must remember an essential cue about the environment that happened many steps in advance."}, {"title": "7 Conclusion and Limitations", "content": "In this work, we investigated using complex-valued diagonal RNNs for online RL. We built on LRUs, to provide a small modification (RTUs) that we found performed significantly better in online RL across various partially observable prediction and control settings. We also found RTUs performed better than the more computationally intensive GRUs. Overall, RTUs are a promising, lightweight approach to online learning in partially observable RL environments.\nA primary limitation of RTUs is the extension to multilayer recurrence. This limitation is inherent to all RTRL approaches; with multilayers, we need to save the gradient traces of the hidden state w.r.t the weights from all the preceding layers [16]. Previous work [16, 47] showed that using stop gradient operations between the layers and not tracing the time dependencies across layers is a viable choice. However, we need a more principled approach for tracing the gradient across layers.\nOne advantage of the linearity restriction in LRUs is that it allows the use of parallel scans for training [25]. However, recent works have shown the possibility of employing parallel scans to non-linear RNNs [7, 21]. A future direction is to investigate the use of parallel scans for training RTUs."}, {"title": "A More Details on Representability with Complex-valued Diagonal Recurrence", "content": "This section explains why we need complex-valued diagonals to represent dense recurrent layers. We first show when it is equivalent to use complex-valued diagonal and a dense recurrent layer. We highlight that using a real-valued diagonal is like restricting the weights to be symmetric-because the (complex) diagonal corresponds to the eigenvalues of the weight matrix\u2014which can severely limit representability. We provide a small experiment to show that complex eigenvalues naturally arise when training both a dense linear and nonlinear RNN, further motivating the utility of moving towards complex-valued diagonals."}, {"title": "A.1 Representability with Complex-valued Diagonals", "content": "Let us first consider when we can perfectly represent a dense, linear recurrent layer with a complex-valued diagonal recurrent layer. Assume we have the recurrence relationship, with learnable parameters $W_h\u2208 R^{n\u00d7n}$ and $W_x \u2208 R^{n\u00d7d}$,\n$h_t = f(W_hh_{t\u22121} + W_xu(x_t))$ (5)\nwhere f is a potentially nonlinear function that inputs a vector and outputs the same-sized vector and u can be any transformation of the inputs xt before they are inputted into the recurrent layer. The following equivalence result is straightforward but worthwhile formalizing."}, {"title": "A.3 Complex Eigenvalues in Vanilla RNNS", "content": "We empirically investigate whether complex eigenvalues appear when training dense RNNs in a simple task. The goal is to show that Wh is not a symmetric matrix, even in the simplest tasks, so having only real-valued diagonals is too restrictive. We used the Three State POMDP[41] for this experiment.\nThe Three State MDP is depicted in Figure 7. In this task, the agent needs to remember one cue from the previous time-step ago, to make a prediction about the next time-step. The MDP has three states, $s_1$, $s_2$, and $s_3$, and no actions. If the agent is in either $s_1$ or $s_2$, it transitions to any of the three states with equal probability. However, if the agent is in $s_3$, it transitions to the state preceded by $s_3$. A sequence of observations would look like 1, 3, 1, 2, 2, 3, 2,..., and we ask the agent to predict the next observation.\nWe trained a vanilla RNN with 3 hidden states with T-BPTT with truncation length 2, which is a sufficient history length in this problem to predict the next observation. Since we have 3 hidden states, the matrix Wh is \u2208 $R^{3\u00d73}$ and could have at most 2 complex eigenvalues.\nWe measured the performance in terms of the percentage of correct predictions made in $S_3$. We recorded the number of complex eigenvalues of Wh after each parameter update, shown in Figure 8. This agent reaches 100% accuracy in this problem relatively quickly. We can also see that the agent oscillates between having two complex eigenvalues and zero eigenvalues. The average number of complex eigenvalues across 30 run is above 1.5, which means that on more than of the steps, the RNN has two complex eigenvalues. The primary point is that we see complex eigenvalues appear frequently."}, {"title": "B Background on BackPropagation Through Time and Real-Time Recurrent Learning", "content": "This section provides a brief background on BackPropagation Through Time (BPTT) and Real-Time Recurrent Learning (RTRL) algorithms."}, {"title": "B.1 BackPropagation Through Time", "content": "BPTT calculates the gradient, $\u2207_\u03c8L$, by unfolding the recurrent dynamics through time and incorporating the impact of the parameters on the loss from all observed time steps. Formally, we can write $\u2207_\u03c8L$ as:\n$\u2207_\u03c8L = \\sum^{t-1}_{i=0}\u2207_\u03c8L_i$. (13)\nApplying the chain rule, we re-write Eq.13 as:\n$\u2207_\u03c8L = \\sum^{t-1}_{i=0} \\frac{\u2202L_i}{\u2202h_i} \\frac{\u2202h_i}{\u2202\u03c8}$.  (14)\nWhen calculating $\\frac{\u2202h_i}{\u2202\u03c8}$, we need to consider the effect of \u03c8 from all the time steps. To illustrate this effect, consider unrolling the last 2 steps of the RNN dynamics:\n$h_t = f(h_{t-1}, x_t, \u03c8)$\nRe-write $h_{t-1}$ as $f(h_{t-2}, x_{t-1},\u03c8)$\n$= f(f(h_{t-2}, x_{t-1}, \u03c8), x_t, \u03c8)$\nRe-write $h_{t-2}$ as $f(h_{t-3}, x_{t-2}, \u03c8)$\n$= f(f(f(h_{t-3}, x_{t-2}, \u03c8), x_{t-1}, \u03c8), x_t,\u03c8)$.  (15)\nEquation 15 shows that the network parameters \u03c8 affect the construction of the recurrent state $h_t$ through two pathways: a direct pathway, i.e., using \u03c8 to evaluate $f (h_{t-1}, x_t, \u03c8)$, and an implicit pathway, i.e., \u03c8 affected constructing all previous recurrent states, $h_{t-1},..., h_1$, and all those recurrent states affected $h_t$ construction. Thus, to calculate $\\frac{\u2202h_t}{\u2202\u03c8}$, we need to consider those two pathways:\n$\\frac{\u2202h_t}{\u2202\u03c8}$ = $\\frac{\u2202f (h_{t-1}, x_t, \u03c8)}{\u2202\u03c8}$ + $\\frac{\u2202f (h_{t-1}, x_t, \u03c8)}{\u2202h_{t-1}} \\frac{\u2202h_{t-1}}{\u2202\u03c8}$. (16)"}]}