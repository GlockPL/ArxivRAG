{"title": "Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration", "authors": ["Zijian Wang", "Bin Wang", "Haifeng Jing", "Huayu Li", "Hongbo Dou"], "abstract": "Recent years, multi-hop reasoning has been widely stud-ied for knowledge graph (KG) reasoning due to its efficacy and interpretability. However, previous multi-hop reasoning approaches are subject to two primary shortcomings. First, agents struggle to learn effective and robust policies at the early phase due to sparse rewards. Second, these approaches often falter on specific datasets like sparse knowledge graphs, where agents are required to traverse lengthy reasoning paths. To address these problems, we propose a multi-hop reasoning model with dual agents based on hierarchical reinforcement learning (HRL), which is named FULORA. FULORA tackles the above reasoning challenges by efficient Guidance-ExpLORAtion between dual agents. The high-level agent walks on the simplified knowledge graph to provide stage-wise hints for the low-level agent walking on the original knowledge graph. In this framework, the low-level agent optimizes a value function that balances two objectives: (1) maximizing return, and (2) integrating efficient guidance from the high-level agent. Experiments conducted on three real-word knowledge graph datasets demonstrate that FULORA outperforms RL-based baselines, especially in the case of long-distance reasoning.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) are designed to represent the world knowledge in a structured way. There are various downstream NLP tasks especially knowledge-driven services, such as query answering (Guu, Miller, and Liang 2015; Cui et al. 2019), relation extraction (Mintz et al. 2009; Reiplinger, Wiegand, and Klakow 2014) and dialogue generation (He et al. 2017). However, a significant proportion of KGs are severely incomplete, which constrains their efficacy in numerous tasks. Consequently, this study concentrates on automatic knowledge graph (KG) reasoning, also as known knowledge graph completion (KGC).\nOver the past few years, several embedding-based models (Bordes et al. 2013; Lin et al. 2015) are proposed to focus on preserving the structural information in the KG and are effective for single-hop reasoning, but lack the necessary interpretability. To solve this problem, some works (Xiong, Hoang, and Wang 2017; Das et al. 2018) have considered introducing reinforcement learning framework to KG reasoning tasks, these approaches compose single-hop triplets"}, {"title": "Multi-hop Reasoning", "content": "The advancement of deep reinforcement learning (DRL) has led to an increased interest in utilizing DRL to address path-finding issues. The first groundbreaking work combining DRL and KG reasoning is DeepPath (Xiong, Hoang, and Wang 2017), which has an important inspiration for the subsequent derivative models. However, the method is constrained by the requirement to know the target entity in advance. MINERVA (Das et al. 2018), by contrast, does not need to know the target entity in progress but instead causes the agent to walk on the knowledge graph until it finds answer.\nIn recent years, many powerful neural networks have been developed. Some works consider the potential of utilizing these networks to generate walking policies. M-Walk (Shen et al. 2018) utilizes RNN to record the sequence trajectories obtained by the agent traversal and then optimize the discounted reward by a RL algorithm called Monte Carlo Tree Search (MCTS), to tackle the problem of sparse rewards. GRL (Wang et al. 2020) employs both generative adversarial net (GAN) and long short-term memory (LSTM) to generate new trajectory sequences if needed, which means the agent cannot only reason in the original graph, but sub-graphs can be automatically generated to extend the relational path until the target entities are found. SQUIRE (Bai et al. 2022) utilizes an encoder-decoder Transformer structure to map query and previous path sequence to a contextualized representation, and further use such representation to autoregressively decode the output path, token by token. These models have demonstrated satisfactory performance in short-distance reasoning scenarios. However, their performance in long-distance reasoning tasks has consistently been suboptimal.\nThe proposal of hierarchical reinforcement learning (HRL) (Li et al. 2018; Wen et al. 2020; Liu et al. 2023) has brought attention to the fact that mupltiple agents can walk on the graph simultaneously. CURL (Zhang et al. 2022) aligns with the RL formulation of MINERVA (Das et al. 2018), i.e., learning to walk and search for answer entities of a particular KG query in an end-to-end fashion. Nevertheless, previous multi-agent based KG reasoning models don't strike a satisfying balance between guidance from other agents and self-exploration. Moreover, these models don't utilize enough of entity embedding and relation embedding, which wastes some structural information of the KG. In contrast to the preceding methodologies, FULORA employs a distinctive training approach that seamlessly integrates supervised learning and reinforcement learning. This innovative approach effectively reconciles the inherent trade-off between self-exploration and path-guidance, a challenge commonly encountered by DWARF."}, {"title": "Preliminary", "content": "In this section, we commence with the problem definition of our work, followed by the introduction of environment representation."}, {"title": "Knowledge Graph Embedding", "content": "Knowledge graph embedding (KGE) methods map entities to vectors in low-dimensional embedding space, and model relations as transformations between entity embeddings (Bai et al. 2022). Once we map them to low-dimensional dense vector space, we can use modeling methods to perform calculations and reasoning (Ma et al. 2018). Prominent examples include TransE (Bordes et al. 2013), TransR (Lin et al. 2015), ConvE (Dettmers et al. 2018) and TuckER (Balazevic, Allen, and Hospedales 2019). These models are equipped with a scoring function that maps any triplet $(e_s, r_q, e_o)$ to a scalar score. For instance, TransE (Bordes et al. 2013) first encoded the entities and relations into latent vectors by following translational principle in pointwise Euclidean space. The translation assumption in TransE implies the equation that $||h + r \u2013 t|| \u2248 0$. The embeddings of entities and relations are learnt by maximizing the score function so that the likelihood score is high for true triplets while low for false triplets."}, {"title": "Problem Definition", "content": "We formally define the research problem of this paper in the following part. The knowledge graph is defined as a directed graph G = {E, R}, where E is a set of all entities and R is a set of all relations. A link triplet l consists of the source entity $e_s \u2208 E$, the target entity $e_o \u2208 E$ and the relation $r \u2208 R$, i.e., l = $(e_s,r,e_o)$. In the real world, a link triplet corresponds to a fact tuple. For instance, we can represent the fact whale is a mammal as (whale, is a, mammal). We follow the definition in prior graph walking models, that is, query answering (Das et al. 2018; Zhang et al. 2022). In the applications such as searching and query answering, most problems are to infer another entity when we only know the source entity $e_s$ and the query relation $r_q$, which can be formed by an incomplete link triplet $l_m = (e_s, r_q, ?)$, where \u201c?\u201d indicates the target entity $e_o$ is unknown and needs to be found in the KG."}, {"title": "Environment Representation", "content": "As in the standard reinforcement learning paradigm, we divide the environment which is modeled as a Markov decision process (MDP) into four elements: state, action, transition and reward.\nState. The state consists of two entities and query relation. Specifically, the two entities consist of a current entity $e_t$ and a source entity $e_s$. The current entity visited by the agent at time step t can be considered as state-dependent information, it reflects the reasoning condition of the agent. The source entity and query relation are the global context shared by all states. Concretely, $s_t = (e_t, e_s, r_q) \u2208 S$.\nAction. In reinforcement learning, the agent interacts with the environment by taking action. Specific to the graph walking task, the agent makes an action $a_t \u2208 A$ by selecting an outgoing edge of graph G. This process can be formed as $a_t = {(r_{t+1},e_{t+1})|(e_t, r_{t+1},e_{t+1}) \u2208 G}$. In order to grant the agent an option to terminate a search, the agent can choose to stay at each entity by adding a self-loop edge to every action.\nTransition. Choosing a new action entails a transition of state. A transition function $\u03b4: S \u00d7 A \u2192 S$ is defined by $\u03b4(s_t, a_t) = \u03b4((e_t, e_s, r_q), a_t)$, which means that performing an action $a_t$ at a given state produces a new state. Most multi-hop reasoning picks a neighboring entity at random because it's common for an entity to have multiple neighbors that are all connected to it via the same relation (Xiong, Hoang, and Wang 2017; Das et al. 2018).\nReward. In the default formulation, after the agent takes an action under a certain state if the corresponding entity is a correct target, a favorable reward 1 is obtained, and conversely, an unfavorable reward 0 is obtained."}, {"title": "Methodology", "content": "In this section, we propose FULORA, an extended dual-agent framework for knowledge graph reasoning via efficient guidance-exploration. As illustrated in Figure 4, FULORA is able to improve the exploration efficiency of GIANT via dynamic feedback mechanism, so as to provide more reliable guidance for DWARF. In a corresponding manner, DWARF employs a supervised learning approach to achieve a balance between guidance and exploration. Subsequently, it aggregates messages emitted by all neighbors of the current entity via an attention mechanism. Next, we will delve into the specifics of each aforementioned components."}, {"title": "Entity Embeddings and Cluster Embeddings", "content": "Consistent with (Zhang et al. 2022), we employ TransE (Bordes et al. 2013) due to its efficiency in encoding the structural proximity information of KG to generate pre-trained entity embeddings, then we divide the original KG into N clusters by utilizing K-means. In order for GIANT can easily walk on the cluster-level graph $G_c$ while preserving the relation information of the original KG G, we add a link to two clusters if there is at least one entity-level edge between them, named cluster mapping. An example of cluster mapping is shown in Figure 2."}, {"title": "Policy Networks For GIANT and DWARF", "content": "In our model, we utilize a three-layer LSTM, enabling the agent to memorize and learn from the actions taken before. In contrast to previous models, the necessity arises to design the network separately in this case, given that two agents are walking on the KG. An agent based on LSTM encodes the recursive sequence as a continuous vector $h_t$. Specifically, the hidden state embedding of GIANT is $h_t^G$ while the hidden state embedding of DWARF is $h_t^D$. Their initial hidden state is O. In addition, we define an information sharing vector $I_t = [h_t^G; h_t^D]$ for GIANT and DWARF to share path information. To a certain extent, cluster-level paths are complementary to entity-level paths, as they ensure the sharing of essential path information from GIANT to DWARF.\nFor GIANT. We denote the current cluster embedding at time step t by $c_t \u2208 R^{2d}$. The action representation $a_t^G$ is given by the cluster embedding itself, i.e., $a_t^G = c_t \u2208 R^{2d}$ because the action corresponds to the next outgoing cluster. The history embedding is updated according to LSTM dynamics:\n$h_t^G = LSTM(W_c[h_{t-1}^G; I_{t-1}], a_{t-1}^G)$   (1)\nwhere $W_c \u2208 R^{2d\u00d74d}$ is a projection matrix to maintain shape."}, {"title": "Efficient Guidance-Exploration", "content": "As mentioned above, in order to alleviate the issue of sparse rewards and huge action space, we hope GIANT will guide DWARF to shrink the action space. Nevertheless, the guidance provided by GIANT is not always beneficial, primarily due to two key issues: (1) Poor guidance will force DWARF to get wrong answer, and (2) even if GIANT is capable of generating fair guidance, mapping from entity to cluster results in a distribution shift, that is, a appropriate policy for GIANT may not be fully appropriate for DWARF. Considering the two issues, we propose an efficient guidance-exploration approach, which gives DWARF a constraint reward to balance guidance and exploration via supervised learning approaches.\nConstraint Reward. Considering the constraint, that is, receiving high-quality guidance from GIANT, we introduce a metric for the state distance between GIANT and DWARF, denoted as $D(s_t^G, s_t^D)$. It is calculated as the cosine similarity between the pre-trained embeddings of the current entity and the current cluster:\n$D(s_t^G, s_t^D) = \\frac{c_t^T e_t}{||c_t||_2||e_t||_2}$  (8)\nWe solve the optimization problem by the following formulas,\n$\\begin{aligned}\n&\\underset{\\theta_{\\pi^D}}{\\text{maximize}} \\mathbb{E}_{a_{\\theta_{\\pi^D}}}[\\sum_{t=0}^{T-1} \\delta \\frac{r_c(s_t) + \\epsilon}{r_e(s_t) + \\epsilon} ] \\\\\n&\\text{subject to } D(s_t^G, s_t^D) \\geq \\delta\n\\end{aligned}$  (9)\nwhere $r_e(s_t)$ and $r_c(s_t)$ are default rewards for DWARF and GIANT respectively, the agent obtains a favorable reward 1 if the corresponding entity or cluster is a correct target and unfavorable reward 0 otherwise. It is only when GIANT reaches the correct cluster that the constraint in Equation 9 is operative.\nPractical Algorithm. The objective in Equation 9 can be optimized with any reinforcement learning algorithm that implements generalized policy iteration. Here we use REIN-FORCE (Williams 1992) and the method of Lagrange multipliers as described by (Abdolmaleki et al. 2018; Grillotti"}, {"title": "Dynamic Path Feedback", "content": "A further challenge is to enhance the search efficiency of GIANT in order to provide DWARF with the requisite guidance as expeditiously as possible, given that the search is limited to a fixed number of step T. In the default reward, GIANT will receive 1 reward only when it reaches the correct cluster, and the rest will be 0, which makes GIANT adopt random policy at the early phase. This phenomenon is not conducive to stable learning outcomes for two agents. On the basis of the theory of reward shaping (Ng, Harada, and Russell 1999; Harutyunyan et al. 2015), we rewrite the reward function of GIANT, named dynamic path feedback. In particular, we write it in the form of an objective function $J(\u03b8_{\u03c0^G})$,\n$J(\u03b8_{\u03c0\u03bf}) = \\sum_{t=0}^{T-1}[r_c(s_t) \u2013 \u03b1\u2206(s_t, s_{t+1})]$,   (12)\nwhere $\u2206(s_t, s_{t+1}) = D(s_t, s_{target}) \u2013 D(s_{t+1}, s_{target})$\n$s_{t+1}$ comes from the next state generated by the policy network. In contrast to the default reward, dynamic path feedback uses the reward function to score the GIANT's path in a rollout rather than simply identifying whether it has reached the correct target cluster. Even if GIANT does not reach the correct target cluster in a rollout, it will evaluate the quality of the path, thereby accelerating the learning process. In Appendix D, we prove that GIANT learns optimal policy in dynamic path feedback is consistent with default rewards."}, {"title": "Algorithm 1: FULORA Training Algorithm", "content": "Require:\nKG G; Initial policy networks parameters $\u03b8_{\u03c0^G}$ and $\u03b8_{\u03c0^D}$; Initial Lagrange multiplier parameter $\u03b8_\u03bb$;Source entity and cluster nodes $e_s$ and $c_s$; Entity-level query $r_q$; Target entity and cluster nodes $e_o$ and $c_o$; Maximum path length T\nEnsure:\nparameters $\u03b8_{\u03c0^G}, \u03b8_{\u03c0^D}, \u03b8_\u03bb$\n1: for t = 0, ..., T \u2013 1 do\n2: Set default cluster-level reward $r_c$ = 1 if $C_t = C_o$ otherwise $r_c$ = 0\n3: Set default entity-level reward $r_e$ = 1 if $e_t = e_o$. otherwise $r_e$ = 0\n4: Predict the action $a_t^G$ and $a_t^D$ for GIANT and DWARF based on policy networks parameters $\u03b8_{\u03c0^G}$ and $\u03b8_{\u03c0^D}$\n5: Compute $J(\u03b8_{\u03c0^G})$, $J(\u03b8_{\u03c0^D})$, $J(\u03b8_\u03bb)$ based on Eq. (10)-(12).\n6: end for\n7: Update model parameters:\n$\u03b8_{\u03c0^G} \u2190 \u03b8_{\u03c0^G} + \u03b1_{\u03c0^G}\u2207_{\u03b8_{\u03c0^G}}J(\u03b8_{\u03c0^G})$\n$\u03b8_{\u03c0^D} \u2190 \u03b8_{\u03c0^D} + \u03b1_{\u03c0^D}\u2207_{\u03b8_{\u03c0^D}}J(\u03b8_{\u03c0^D})$\n$\u03b8_\u03bb \u2190 \u03b8_\u03bb \u2013 \u03b1_\u03bb\u2207_{\u03b8_\u03bb}J(\u03b8_\u03bb)$\n8: return $\u03b8_{\u03c0^G}, \u03b8_{\u03c0^D}, \u03b8_\u03bb$"}, {"title": "Experiments", "content": "In this section, we evaluate the efficacy of FULORA on three real-world KG datasets: NELL-995 (Xiong, Hoang, and Wang 2017), WN18RR (Dettmers et al. 2018) and FB15K-237 (Toutanova et al. 2015). The datasets statistics are listed in Table 1. As can be seen from the statistical indicators, these three datasets represent standard KG, sparse KG and Dense KG respectively. In our selection of the baseline, we are not only evaluating it against the state-of-the-art multi-hop reasoning methods (DeepPath (Xiong, Hoang, and Wang 2017), MINERVA (Das et al. 2018), M-Walk (Shen et al. 2018), AttnPath (Wang et al. 2019), SQUIRE"}, {"title": "Link Prediction Results", "content": "For each triplet $(e_s, r_q, e_o)$ in the test set, we convert it to a triplet query $(e_s, r_q, ?)$, and then use embedding-based models or multi-hop based models and a beam search width of 50 to get the ranking list of tail entity. Following the previous work (Bordes et al. 2013), here we use two metrics: (1) the mean reciprocal rank of all correct tail entities (MRR), and (2) the proportion of correct tail entities ranking in the top K (Hits@K) for evaluation. As shown in Table 2, we first display the performances of FULORA and all baselines on NELL-995, WN18RR and FB15K-237. From the table, we can observe that our model outperforms all previous multi-hop reasoning models on standard KG (NELL-995) and sparse KG (WN18RR). Especially on NELL-995, where our model gains marked improvements compared with the best multi-hop reasoning baseline SQUIRE and dual-agent model CURL. In addition, FULORA scores similarly to the best embedding-based model LMKE. Since FB15K-237 contains much larger number of 1-to-M than the M-to-1 relation instances (Wan et al. 2020; Zhang et al. 2022), multi-hop reasoning methods is prone to be stuck in the local entity nodes with high-degree centrality, renders it hard to reach the correct entity. Compared with strigent multi-hop reasoning models (MINERVA, M-Walk and CURL), FULORA's attention mechanism alleviates this issue by forcing the agent to focus on the neighbor which is highly relative to the query. After averaging results on three datasets, we observe that FULORA leads to overall improvements relative to the dual-agent model (CURL) by 2.5%, 2.3%, 1.3%, 3.2%, in terms of MRR and Hits @1, 3, 10."}, {"title": "Fact Prediction Results", "content": "As opposed to link prediction, fact prediction task is concerned with verifying the veracity of an unknown fact, the true test triplets are ranked with some generated false triplets. Since we share a similar query-answering mechanism as CURL (Zhang et al. 2022), FULORA is capable of identifying the most appropriate entity for a given query and eliminates the need to evaluate negative samples of any particular relation. In the experiments, the dual agents try to infer and walk through the cluster-level and entity-level respectively to reach the correct target under removing all links of groundtruth relations in the original KG. Here, we report Mean Average Precision (MAP) scores for various relation tasks of NELL-995. We reuse the results of TransE, TransR, PRA, DeepPath, MINERVA, M-Walk on seven tasks already reported in (Zhang et al. 2022). As demonstrated in Table 3, FULORA produces a satisfying result in most tasks, contributing an average gain of 9.1% relative to the multi-hop based reasoning approaches (PRA, DeepPath, MINERVA, M-Walk and CURL) and 16.1% gain compared to the embedding-based approaches (TransE and TransR)."}, {"title": "Long-Distance Reasoning Ability", "content": "Recall the motivation of FULORA, we tend to address the issue of multi-hop reasoning models being unable to infer correct answer due to the lack of short direct path by improving long-distance reasoning ability. This issue is significant on standard KG and sparse KG, so we conduct the following experiments on NELL-995 and WN18RR. For effective evaluation, we compare our model with MINERVA and CURL in NELL-995 and WN18RR, where we remove the most frequently-visited short paths found by the bi-directional search (Xiong, Hoang, and Wang 2017) inside KGs.\nCluster State Similarity. One of the most significant contributions of GIANT is the implementation of dynamic path feedback, which has been shown to enhance the learning efficiency. To visually demonstrate the efficacy of dynamic path feedback, we initially focus on GIANT's reasoning ability on cluster-level, which directly impacts DWARF's reasoning results. Here, we record the cluster state similarity (CSS) $D(s_t^G, s_{target})$ under each epoch. As shown in Figure 5, our model outperforms CURL in scores and stability in most cases. This demonstrates that dynamic path feedback does improve GIANT's learning efficiency. See Appendix B for a comparison of the remaining tasks.\nEntity Reasoning Accuracy. The preceding analysis has demonstrated that FULORA enables GIANT to provide high-quality guidance, and next, we will demonstrate that efficient guidance-exploration gives DWARF a superior performance in long-distance reasoning against other multi-hop baselines. Figure 6 plots the MRR scores on varying path lengths on NELL-995 and WN18RR. As can be seen, FULORA outperforms CURL, MINERVA and SQUIRE with different path lengths. In particular, FULORA is much more prominent in the long-distance reasoning accuracy of WN18RR (sparse KG). It explains our motivation: FULORA can alleviate the poor performance of multi-hop reasoning approaches in the absence of a short direct path (typically found in standard KG and sparse KG) by improving long-distance reasoning ability. Overall, FULORA has a more robust performance under the demands of long-distance reasoning, and also shows minimal performance degradation for long-path settings. It is mainly due to the dynamic path feedback and efficient guidance-exploration proposed by us, which enable more efficient information sharing between GIANT and DWARF, and ensure that DWARF maintains an excellent self-exploration ability."}, {"title": "Ablation Study", "content": "We utilize \u03b4 and \u03b1 to control the degree of efficient guidance-exploration and dynamic path feedback. To comprehensively explore the efficacy of efficient guidance-exploration and dynamic path feedback, we conduct link prediction on NELL-995 and WN18RR by varying the values of \u03b4 and \u03b1 as {0.20, 0.30, 0.40,0.50} and {0.05, 0.10, 0.15, 0.20}, respectively. The averaged results are shown in Figure 7. In accordance with our previous analysis, it is evident that DWARF cannot rely excessively on GIANT for guidance. Furthermore, the level of path feedback that GIANT receives should not be unduly strong. In addition, in Appendix B, we have carefully discussed the influence of three main components of FULORA on knowledge graph reasoning efficiency."}, {"title": "Conclusion", "content": "We present FULORA, an efficient guidance-exploration model built on dual-agent KG reasoning framework to enhance the agent's long-distance reasoning ability on standard KG and sparse KG. The key insight behind our approach is balancing the self-exploration of DWARF and the guidance from GIANT. Specifically, on the one hand, we leverage the attention mechanism to make DWARF pay attention to the neighbouring entities that are close to the query. On the other hand, we propose that dynamic path feedback enables GIANT to have better learning efficiency, thus providing DWARF with high-quality guidance, making the DWARF to have a favourable global vision while having excellent local reasoning ability. Experiments on three real-world datasets show that FULORA outperforms the recent state-of-the-art multi-hop reasoning methods in most cases. Further analysis of these results reveals that FULORA's Long-distance reasoning ability on standard KG and sparse KG has significant advantages compared to multi-hop reasoning methods."}, {"title": "A. Experiment Setup", "content": "Data Statistics\nWe adopt three knowledge graph datasets with different scales: NELL-995, WN18RR and FB15K-237. We give a brief overview of these datasets.\nNELL-995 (Xiong, Hoang, and Wang 2017). NELL995 is an open source machine learning dataset developed by the OpenAI research group that contains more than 950,000 pieces of entity relationship data collected from the network to help machine learning systems make inferences. NELL995 can be used to train machine learning models, such as natural language processing models, machine translation models, question answering systems, and semantic search systems.\nWN18RR (Dettmers et al. 2018). WN18RR is a subset of WordNet that describes the association characteristics between English words. It preserves the symmetry, asymmetry, and composition relationships of the WordNet, and removes the inversion relationships. WN18RR contains some relational information about words. It consists of 40,945 entities and 11 relations.\nFB15K-237 (Toutanova et al. 2015). FB15K-237 is a common knowledge graph dataset, which is a subset extracted from Freebase knowledge graph. It contains 14,505 entities and 237 relations, and the data is carefully processed to remove reversible relational data and trivial triples, ensuring that entities in the training set are not directly connected to the verification or test set, thus avoiding information leakage issues.\nBaseline Methods\nWe compare the performance of FULORA with the following baselines, including embedding-based and multi-hop-based knowledge graph reasoning methods:\nTransE (Bordes et al. 2013) embeds entities and relations into low-dimensional vector space so that complex relationships in graphs can be represented and reasoned by vector operations.\nDistMult (Yang et al. 2015) uses triples (source entity, relation, tail entity) to train the model, where each entity and relation is represented as a vector. The target is to maximize the score of the correct triples while minimizing the score of the wrong triples.\nComplex (Trouillon et al. 2016) is based on tensor decomposition and uses complex vectors to represent entities and relationships in order to capture complex relationships in the knowledge graph.\nDeepPath (Xiong, Hoang, and Wang 2017) utilizes a knowledge graph-based embedded policy-based agent with continuous states, which extends its path in a KG vector space by sampling the most promising relation-ships.\nMINERVA (Das et al. 2018) formulates the query task as a reinforcement learning (RL) problem where the goal is to take the best sequence of decisions (choice of relation edges) to maximize the expected reward (reaching the correct answer node)."}, {"title": "Implementation Details", "content": "We use the following software versions:\n\u2022 Ubuntu 24.04 LTS\n\u2022 Python 3.10\n\u2022 Pytorch 2.0.1\nWe conduct all experiments with a single NVIDIA GeForce 3080Ti GPU.\nOur experiments of all of 10 baselines are conducted on their official implementation provided by their respective authors. To reproduce the results of our model in Table 2 and Table 3, we report the empirically optimal crucial hyperparameters as shown in Table 4. On all datasets, the quantities of path rollouts in training and testing are 20 and 100, separately."}, {"title": "B. Additional Results and Analysis", "content": "Case Stuides\nIn this part, we take WorksFor, AthletePlaysSport and TeamPlaysinLeague from NELL-995 as examples to analyaze these paths found by AttnPath (Wang et al."}, {"title": "Cluster State Similarity", "content": "To assess the exploration efficiency of GIANT, we introduce cluster state similarity (CSS) as a metric, and in the main content we compare the performance of CURL and FULORA on only three tasks due to space constraints. Figure 8 also compare the model performance on remaining six tasks. For the same task, CURL experiences significant oscillations as path length increases, while FULORA remains stable. As in our previous analysis, CURL can only judge path quality by whether it has gone to the correct target cluster, while FULORA utilizes dynamic path feedback to promote the GIANT to converge to a high-quality path.\nFigure 9 further visualizes performances of FULORA and CURL on these tasks with varied path length. We utilize the ratio of the mean to the variance of CSS as an evaluation metric for the mean measures inference accuracy and the variance measures inference stability. In the vast majority of cases, FULORA outperforms CURL. In the case of long-distance reasoning for complex tasks like PersonBorninLocation-Path9 (PBL-9), AthletePlaysinLeague-Path7 (APL-9), AthletePlaysSport-Path9 (APS-9), and TeamPlaysinLeague (TPL), the performance gap between FULORA and CURL is significant."}, {"title": "Additional Ablation Studies", "content": "Here, we conduct a series of ablation experiments designed to answer the following three questions:\nQ1: Can attention mechanism enhance DWARF's reasoning ability?\nQ2: Can dynamic path feedback accelerate DWARF'S learning speed via improving GIANT'S reasoning ability?\nQ3: Can the efficient guidance-exploration approach make the guidance-exploration trade-off?\nWith a slight abuse of abbreviation, we use ATTN, DPF and GE to denote attention mechanism, dynamic path feedback and the efficient guide-exploration approach respectively."}, {"title": "Attention Mechanism", "content": "We first focus on ablation experiments on the attention mechanism. As shown in Figure 10, the absence of the attention mechanism has little effect on WN18RR (sparse KG), while the effect of the attention mechanism increases with increasing density. In particular, on FB15K-237 (dense KG), the effect of the attention mechanism on the average ESS reaches 1/3. It is consistent with the purpose of introducing the attention mechanism, that is, enhancing the reasoning ability of DWARF in the case of multi-neighbor and multi-relation. The reason lies in DWARF are forced to prioritize relations and neighbors that are highly correlated with the query relations, which plays an important role in reasoning on dense KG with complex relations, but because sparse KG neighbor relations are not sophisticated, the attention mechanism does not have a substantial effect."}, {"title": "Dynamic Path Feedback", "content": "We have demonstrated in previous experiments that dynamic path feedback can improve GIANT's reasoning ability, next we concentrate on the impact to DWARF's learning speed of dynamic path feedback. To illustrate the speed and stability of the training, we utilize kdeplot in Figure 10. The distribution maps on the three datasets have a common feature: compared with FULORA, the learning speed and learning accuracy decrease in the absence of dynamic path feedback (corresponding to the widening and downward movement of the distribution maps). In FB15K-237 (dense KG), the lack of dynamic path feedback has a great impact on the learning speed and accuracy, because the error of cluster mapping escalates with the complexity of neighbor relations when the number of clusters maintains. At this time, a large deviation between cluster-level KG and entity-level KG requires GIANT to make a correct evaluation of the path timely to provide exact guidance for DWARF."}, {"title": "Efficient Guidance-Exploration", "content": "The core of FULORA is the efficient guidance-exploration method, which contributes to make the guidance-exploration trade-off. In the previous HRL-based reasoning method like CURL, there is a high degree of coupling between the two agents. We treat ESS and CSS as two time series because they change with training progress. Next, we use Granger Causality test to examine the degree of coupling between DWARF and GIANT.\nGranger Causality test is used to study the causal relationship between two sets of data, that is, to test whether one set of time series causes changes in another set of time series (Diks and Panchenko 2006). It is worth noting that Granger Causality test requires stationary time series, otherwise false regression issues may occur, so it is necessary to detect the stationarity of time series through ADF test. If the pair-to-pair time series is non-stationary and satisfies the homogeneity of order, the Granger Causality test can be carried out only after the cointegration test"}]}