{"title": "Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration", "authors": ["Zijian Wang", "Bin Wang", "Haifeng Jing", "Huayu Li", "Hongbo Dou"], "abstract": "Recent years, multi-hop reasoning has been widely studied for knowledge graph (KG) reasoning due to its efficacy and interpretability. However, previous multi-hop reasoning approaches are subject to two primary shortcomings. First, agents struggle to learn effective and robust policies at the early phase due to sparse rewards. Second, these approaches often falter on specific datasets like sparse knowledge graphs, where agents are required to traverse lengthy reasoning paths. To address these problems, we propose a multi-hop reasoning model with dual agents based on hierarchical reinforcement learning (HRL), which is named FULORA. FULORA tackles the above reasoning challenges by efficient Guidance-ExpLORAtion between dual agents. The high-level agent walks on the simplified knowledge graph to provide stage-wise hints for the low-level agent walking on the original knowledge graph. In this framework, the low-level agent optimizes a value function that balances two objectives: (1) maximizing return, and (2) integrating efficient guidance from the high-level agent. Experiments conducted on three real-word knowledge graph datasets demonstrate that FULORA outperforms RL-based baselines, especially in the case of long-distance reasoning.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) are designed to represent the world knowledge in a structured way. There are various downstream NLP tasks especially knowledge-driven services, such as query answering (Guu, Miller, and Liang 2015; Cui et al. 2019), relation extraction (Mintz et al. 2009; Reiplinger, Wiegand, and Klakow 2014) and dialogue generation (He et al. 2017). However, a significant proportion of KGs are severely incomplete, which constrains their efficacy in numerous tasks. Consequently, this study concentrates on automatic knowledge graph (KG) reasoning, also as known knowledge graph completion (KGC).\nOver the past few years, several embedding-based models (Bordes et al. 2013; Lin et al. 2015) are proposed to focus on preserving the structural information in the KG and are effective for single-hop reasoning, but lack the necessary interpretability. To solve this problem, some works (Xiong, Hoang, and Wang 2017; Das et al. 2018) have considered introducing reinforcement learning framework to KG reasoning tasks, these approaches compose single-hop triplets into multi-hop reasoning chains. With the development of deep learning (DL) and reinforcement learning (RL), recent multi-hop works (Wang et al. 2019; Lv et al. 2020; Nikopensius et al. 2023; Xu et al. 2024) incorporate more advanced DL and RL techniques. For example, AttnPath (Wang et al. 2019) makes use of attention mechanism to force an agent to walk forward every step to avoid the agent stalling at the same entity node constantly.\nAlthough existing multi-hop reasoning models have achieved impressive results, there is a noteworthy issue with these models that they only perform well when the agent's reasoning path length is short. The drawback that the agent relies heavily on short reasoning chains is fatal in certain datasets, such as sparse KG. Figure 1 is an illustration of short direct path and long indirect path. It's challenging for an agent to identify a short direct path just like (a) due to the sparsity which makes the relation $r_2$ do not exist. The optimal path for agent is $e_1 \\rightarrow e_2 \\rightarrow c_4 \\rightarrow e_3$ we called long indirect path instead of $e_1 \\rightarrow e_2 \\rightarrow e_3$. From this point, enhancing the agent's long-distance reasoning ability is one solution to alleviate the multi-hop models' poor performance in the sparse KG.\nHowever, the dimension of the discrete action space at each space is large (Das et al. 2018). As the path length increases, the choices the agent faced will grow exponentially. The most relevant to our work are CURL (Zhang et al. 2022) which proposed a dual-agent framework and mutual reinforcement rewards to assign one of the agents (GIANT) searching on cluster-level paths quickly and providing stage-wise hints for another agent (DWARF). Compared to the classic multi-hop model MINERVA (Das et al. 2018), CURL does improve long-distance reasoning ability. But CURL also has two drawbacks, which make CURL's performance inconsistent. (1) Mutual reinforcement reward mechanism forces the low-level agent DWARF to adopt similar policies to the high-level agent GIANT, even if GIANT's policies is not well enough. This may result in false-negative rewards to the intermediate actions which are reasonable. (2) At the early phase, DWARF and GIANT adopt a near-random policy which makes low training efficacy. For briefness, we call the high-level agent as GIANT and the low-level agent as DWARF like (Bai et al. 2022).\nA number of efforts have been made to justify Transformer in the RL architecture.\nIn light of these challenges, we propose FULORA, a robust dual-agent framework for KG reasoning with taking full advantage of entity embedding and relation embedding in the KG. FULORA seamlessly makes the self-exploration and path-reliance trade-off of DWARF through a supervised learning method. This unique mechanism will enable DWARF to make its own decisions while receiving meaningful guidance from GIANT, instead of relying entirely on the command of GIANT. Moreover, with the aim to make better use of the entity embedding and relation embedding, FULORA introduces attention mechanism and dynamic path feedback to DWARF and GIANT respectively. Intuitively, GIANT searches a feasible path as soon as possible to guide DWARF to reduce the search space, while DWARF adopts diversified exploration policies in the constrictive search space to prevent over-dependence on GI-ANT. In this way, DWARF walking on the original KG has both excellent long distance reasoning ability and short direct path utilization ability."}, {"title": "Related Work", "content": "In line with the focus of our work, we provide a brief overview of the background and related work on knowledge graph embedding and multi-hop reasoning."}, {"title": "Knowledge Graph Embedding", "content": "Knowledge graph embedding (KGE) methods map entities to vectors in low-dimensional embedding space, and model relations as transformations between entity embeddings (Bai et al. 2022). Once we map them to low-dimensional dense vector space, we can use modeling methods to perform calculations and reasoning (Ma et al. 2018). Prominent examples include TransE (Bordes et al. 2013), TransR (Lin et al. 2015), ConvE (Dettmers et al. 2018) and TuckER (Balazevic, Allen, and Hospedales 2019). These models are equipped with a scoring function that maps any triplet $(e_s, r_q, e_o)$ to a scalar score. For instance, TransE (Bordes et al. 2013) first encoded the entities and relations into latent vectors by following translational principle in pointwise Euclidean space. The translation assumption in TransE implies the equation that $||h + r \u2013 t|| \\approx 0$. The embeddings of entities and relations are learnt by maximizing the score function so that the likelihood score is high for true triplets while low for false triplets."}, {"title": "Multi-hop Reasoning", "content": "The advancement of deep reinforcement learning (DRL) has led to an increased interest in utilizing DRL to address path-finding issues. The first groundbreaking work combining DRL and KG reasoning is DeepPath (Xiong, Hoang, and Wang 2017), which has an important inspiration for the subsequent derivative models. However, the method is constrained by the requirement to know the target entity in advance. MINERVA (Das et al. 2018), by contrast, does not need to know the target entity in progress but instead causes the agent to walk on the knowledge graph until it finds answer.\nIn recent years, many powerful neural networks have been developed. Some works consider the potential of utilizing these networks to generate walking policies. M-Walk (Shen et al. 2018) utilizes RNN to record the sequence trajectories obtained by the agent traversal and then optimize the discounted reward by a RL algorithm called Monte Carlo Tree Search (MCTS), to tackle the problem of sparse rewards. GRL (Wang et al. 2020) employs both generative adversarial net (GAN) and long short-term memory (LSTM) to generate new trajectory sequences if needed, which means the agent cannot only reason in the original graph, but subgraphs can be automatically generated to extend the relational path until the target entities are found. SQUIRE (Bai et al. 2022) utilizes an encoder-decoder Transformer structure to map query and previous path sequence to a contextualized representation, and further use such representation to autoregressively decode the output path, token by token. These models have demonstrated satisfactory performance in short-distance reasoning scenarios. However, their performance in long-distance reasoning tasks has consistently been suboptimal.\nThe proposal of hierarchical reinforcement learning (HRL) (Li et al. 2018; Wen et al. 2020; Liu et al. 2023) has brought attention to the fact that mupltiple agents can walk on the graph simultaneously. CURL (Zhang et al. 2022) aligns with the RL formulation of MINERVA (Das et al. 2018), i.e., learning to walk and search for answer entities of a particular KG query in an end-to-end fashion. Nevertheless, previous multi-agent based KG reasoning models don't strike a satisfying balance between guidance from other agents and self-exploration. Moreover, these models don't utilize enough of entity embedding and relation embedding, which wastes some structural information of the KG. In contrast to the preceding methodologies, FULORA employs a distinctive training approach that seamlessly integrates supervised learning and reinforcement learning. This innovative approach effectively reconciles the inherent trade-off between self-exploration and path-guidance, a challenge commonly encountered by DWARF."}, {"title": "Preliminary", "content": "In this section, we commence with the problem definition of our work, followed by the introduction of environment representation."}, {"title": "Problem Definition", "content": "We formally define the research problem of this paper in the following part. The knowledge graph is defined as a directed graph $G = \\{E, R\\}$, where E is a set of all entities and R is a set of all relations. A link triplet $l$ consists of the source entity $e_s \\in E$, the target entity $e_o \\in E$ and the relation $r \\in R$, i.e., $l = (e_s,r,e_o)$. In the real world, a link triplet corresponds to a fact tuple. For instance, we can represent the fact whale is a mammal as (whale, is a, mammal). We follow the definition in prior graph walking models, that is, query answering (Das et al. 2018; Zhang et al. 2022). In the applications such as searching and query answering, most problems are to infer another entity when we only know the source entity $e_s$ and the query relation $r_q$, which can be formed by an incomplete link triplet $l_m = (e_s, r_q, ?)$, where \"?\" indicates the target entity $e_o$ is unknown and needs to be found in the KG."}, {"title": "Environment Representation", "content": "As in the standard reinforcement learning paradigm, we divide the environment which is modeled as a Markov decision process (MDP) into four elements: state, action, transition and reward.\nState. The state consists of two entities and query relation. Specifically, the two entities consist of a current entity $e_t$ and a source entity $e_s$. The current entity visited by the agent at time step $t$ can be considered as state-dependent information, it reflects the reasoning condition of the agent. The source entity and query relation are the global context shared by all states. Concretely, $s_t = (e_t, e_s, r_q) \\in S$.\nAction. In reinforcement learning, the agent interacts with the environment by taking action. Specific to the graph walking task, the agent makes an action $a_t \\in A$ by selecting an outgoing edge of graph G. This process can be formed as $a_t = \\{(r_{t+1},e_{t+1})|(e_t, r_{t+1},e_{t+1}) \\in G\\}$. In order to grant the agent an option to terminate a search, the agent can choose to stay at each entity by adding a self-loop edge to every action.\nTransition. Choosing a new action entails a transition of state. A transition function $\\delta: S \\times A \\rightarrow S$ is defined by $\\delta(s_t, a_t) = \\delta((e_t, e_s, r_q), a_t)$, which means that performing an action at a given state produces a new state. Most multi-hop reasoning picks a neighboring entity at random because it's common for an entity to have multiple neighbors that are all connected to it via the same relation (Xiong, Hoang, and Wang 2017; Das et al. 2018).\nReward. In the default formulation, after the agent takes an action under a certain state if the corresponding entity is a correct target, a favorable reward 1 is obtained, and conversely, an unfavorable reward 0 is obtained."}, {"title": "Methodology", "content": "In this section, we propose FULORA, an extended dual-agent framework for knowledge graph reasoning via efficient guidance-exploration. As illustrated in Figure 4, FULORA is able to improve the exploration efficiency of GI-ANT via dynamic feedback mechanism, so as to provide more reliable guidance for DWARF. In a corresponding manner, DWARF employs a supervised learning approach to achieve a balance between guidance and exploration. Subsequently, it aggregates messages emitted by all neighbors of the current entity via an attention mechanism. Next, we will delve into the specifics of each aforementioned components."}, {"title": "Entity Embeddings and Cluster Embeddings", "content": "Consistent with (Zhang et al. 2022), we employ TransE (Bordes et al. 2013) due to its efficiency in encoding the structural proximity information of KG to generate pre-trained entity embeddings, then we divide the original KG into N clusters by utilizing K-means. In order for GIANT can easily walk on the cluster-level graph $G_c$ while preserving the relation information of the original KG $G$, we add a link to two clusters if there is at least one entity-level edge between them, named cluster mapping. An example of cluster mapping is shown in Figure 2."}, {"title": "Policy Networks For GIANT and DWARF", "content": "In our model, we utilize a three-layer LSTM, enabling the agent to memorize and learn from the actions taken before. In contrast to previous models, the necessity arises to design the network separately in this case, given that two agents are walking on the KG. An agent based on LSTM encodes the recursive sequence as a continuous vector $h_t$. Specifically, the hidden state embedding of GIANT is $h_t^G$ while the hidden state embedding of DWARF is $h_t^D$. Their initial hidden state is 0. In addition, we define an information sharing vector $I_t = [h_t^D; h_t^G]$ for GIANT and DWARF to share path information. To a certain extent, cluster-level paths are complementary to entity-level paths, as they ensure the sharing of essential path information from GIANT to DWARF.\nFor GIANT. We denote the current cluster embedding at time step t by $c_t \\in \\mathbb{R}^{2d}$. The action representation $a_t^G$ is given by the cluster embedding itself, i.e., $a_t^G = c_t \\in \\mathbb{R}^{2d}$ because the action corresponds to the next outgoing cluster. The history embedding is updated according to LSTM dynamics:\n$h_t^G = LSTM(W^G_e[h_{t-1}^G; I_{t-1}], a_{t-1}^G), \\qquad (1)$\nwhere $W^G_e \\in \\mathbb{R}^{2d \\times 4d}$ is a projection matrix to maintain shape.\nFor DWARF. Different from GIANT walking on the cluster KG, DWARF encounters greater difficulties on the original KG. Firstly, an entity has several different aspects. For example, a university professor may be linked with professional relations like worksForUniversity or worksInState, and also family relations like spouse or father/mother. Secondly, It can be observed that cluster-level paths are typically shorter than entity-level paths. Consequently, DWARF must possess enhanced long-distance reasoning capabilities to identify an accurate target. Furthermore, for diverse query relations, it's advisable for DWARF to prioritize relations and neighbors that are highly correlated with the query relations. So we introduce Graph Attention mechanism (Velickovic et al. 2017) into DWARF.\nA single-layer feedforward neural network is employed to compute attention weight. Consider that the current entity is $e_i$, a collection of all neighbors is $N(i)$. The attention weight from $e_i$ and its neighbor $e_j \\in N(i)$ is calculated as\n$A_{ij} = LeakyReLu(\\textbf{a}^T[W e_i; W e_j]), \\qquad (2)$\nwhere $W \\in \\mathbb{R}^{d \\times d}$ is a linear transformation matrix and $\\textbf{a} \\in \\mathbb{R}^{2d}$ is a weight vector shared across all entities. For $e_i$, we normalize the attention weights with SoftMax:\n$\\alpha_{ij} = \\frac{exp(A_{ij})}{\\sum_{k \\in N(i)} exp(A_{ik})}. \\qquad (3)$\nNow we can compute the graph attention for the state:\n$attn_t = \\sum_{k \\in N(i)} \\alpha_{ik} W e_k . \\qquad (4)$\nDWARF's history embedding $h_t^D$ can be obtained from\n$h_t^D = LSTM(W_e^D[h_{t-1}^D; attn_{t-1}^i; I_{t-1}], a_{t-1}^D), \\qquad (5)$\nwhere $W_e^D \\in \\mathbb{R}^{2d \\times 5d}$ is a projection matrix, while the action representation $a_t^D$ is the concatenation of the relation embedding $r_t \\in \\mathbb{R}^{d}$ and the end node embedding $e_t \\in \\mathbb{R}^{d}$, i.e., $a_t^D = [r_t; e_t] \\in \\mathbb{R}^{2d}$.\nPolicy Generation. To predict the next cluster for GIANT and the next entity for DWARF, we apply a two-layer feedforward network on the concatenation of their last LSTM states and current RL state embeddings,\n$d^G = SoftMax(A_G \\times W \\text{ReLU}(W_1[c_t; h_t^G])), \\qquad (6)$\n$\\tilde{a}^G \\sim \\text{Categorical}(d^G)$,\n$d^D = SoftMax(A_D \\times W \\text{ReLU}(W_1[e_t; r_q; h_t^D])), \\qquad (7)$\n$\\tilde{a}^D \\sim \\text{Categorical}(d^D)$,\nwhere $W_1, W_1' \\in \\mathbb{R}^{4d \\times 4d}$ and $W_1'', W_1''' \\in \\mathbb{R}^{6d \\times 6d}$ are the matrices of learnable weights to maintain dimension of history embedding. While $A_G \\in \\mathbb{R}^{\\vert A_G \\vert \\times 4d}$, $A_D \\in \\mathbb{R}^{\\vert A_D \\vert \\times 6d}$ represent the embeddings of all next possible actions for GIANT and DWARF respectively."}, {"title": "Efficient Guidance-Exploration", "content": "As mentioned above, in order to alleviate the issue of sparse rewards and huge action space, we hope GIANT will guide DWARF to shrink the action space. Nevertheless, the guidance provided by GIANT is not always beneficial, primarily due to two key issues: (1) Poor guidance will force DWARF to get wrong answer, and (2) even if GIANT is capable of generating fair guidance, mapping from entity to cluster results in a distribution shift, that is, a appropriate policy for GIANT may not be fully appropriate for DWARF. Considering the two issues, we propose an efficient guidance-exploration approach, which gives DWARF a constraint reward to balance guidance and exploration via supervised learning approaches.\nConstraint Reward. Considering the constraint, that is, receiving high-quality guidance from GIANT, we introduce a metric for the state distance between GIANT and DWARF, denoted as $D(s_t, s_t')$. It is calculated as the cosine similarity between the pre-trained embeddings of the current entity and the current cluster:\n$D(s_t, s_t') = \\frac{c_t \\cdot e_t}{\\Vert c_t \\Vert_2 \\Vert e_t \\Vert_2}. \\qquad (8)$\nWe solve the optimization problem by the following formulas,\n$\\begin{aligned} &\\underset{\\{\\lambda_t\\}_{t=0}^{T-1}}{\\text{maximize}} \\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\tilde{r}_e(s_t)\\right] \\\\ &\\text{subject to } D(s_t, s_t') \\ge \\frac{\\delta}{r_c(s_t)+\\epsilon} \\end{aligned} \\qquad (9)$\nwhere $r_e(s_t)$ and $r_c(s_t)$ are default rewards for DWARF and GIANT respectively, the agent obtains a favorable reward 1 if the corresponding entity or cluster is a correct target and unfavorable reward 0 otherwise. It is only when GIANT reaches the correct cluster that the constraint in Equation 9 is operative.\nPractical Algorithm. The objective in Equation 9 can be optimized with any reinforcement learning algorithm that implements generalized policy iteration. Here we use REIN-FORCE (Williams 1992) and the method of Lagrange multipliers as described by (Abdolmaleki et al. 2018; Grillotti et al. 2024). For all DWARF state $s_t$, we maximize the Lagrangian function, subject to $0 \\le \\lambda(s_t) \\le 1$,\n$J(\\theta_{\\pi_e}) = \\sum_{t=0}^{T-1} [(1 - \\lambda(s_t))r_e(s_t) + \\lambda(s_t)D(s_t, s_t')], \\qquad (10)$\nthe Lagrange multiplier is updated to make the guidance-exploration trade-off. When the state similarity metric between GIANT and DWARF $D(s_t, s_t')$ is less than the threshold, the parameter $\\theta_{\\lambda}$ are optimized so that $\\lambda(s_t)$ increases to encourage GIANT to provide more guidance for DWARF. Conversely, the parameter $\\theta_{\\lambda}$ are optimized so that $\\lambda(s_t)$ decreases to encourage DWARF to explore in the constrictive space when the state similarity metric between GIANT and DWARF $D(s_t, s_t')$ is greater than the threshold. In practice, we utilize a cross-entropy loss to optimize $\\theta_{\\lambda}$:\n$J(\\theta_\\lambda) = \\sum_{t=0}^{T-1} [-(1-y)\\log(1 - \\lambda(s_t)) - y\\log(\\lambda(s_t))], \\qquad (11)$\n$\\begin{cases} 0 & \\text{if }D(s_t, s_t') \\ge \\frac{\\delta}{r_c(s_t) + \\epsilon} \\\\ 1 & \\text{otherwise} \\end{cases}$\nDynamic Path Feedback\nA further challenge is to enhance the search efficiency of GIANT in order to provide DWARF with the requisite guidance as expeditiously as possible, given that the search is limited to a fixed number of step T. In the default reward, GIANT will receive 1 reward only when it reaches the correct cluster, and the rest will be 0, which makes GIANT adopt random policy at the early phase. This phenomenon is not conducive to stable learning outcomes for two agents. On the basis of the theory of reward shaping (Ng, Harada, and Russell 1999; Harutyunyan et al. 2015), we rewrite the reward function of GIANT, named dynamic path feedback. In particular, we write it in the form of an objective function $J(\\theta_{\\pi_c})$,\n$J(\\theta_{\\pi_c}) = \\sum_{t=0}^{T-1}[r_c(s_t) - \\alpha\\Delta(s_t, s_{t+1})], \\qquad (12)$\nwhere $\\Delta(s_t, s_{t+1}) = D(s_t, s_{target}) - D(s_{t+1}, s_{target})$,$s_{t+1}$ comes from the next state generated by the policy network. In contrast to the default reward, dynamic path feedback uses the reward function to score the GIANT's path in a rollout rather than simply identifying whether it has reached the correct target cluster. Even if GIANT does not reach the correct target cluster in a rollout, it will evaluate the quality of the path, thereby accelerating the learning process. In Appendix D, we prove that GIANT learns optimal policy in dynamic path feedback is consistent with default rewards."}, {"title": "Experiments", "content": "In this section, we evaluate the efficacy of FULORA on three real-world KG datasets: NELL-995 (Xiong, Hoang, and Wang 2017), WN18RR (Dettmers et al. 2018) and FB15K-237 (Toutanova et al. 2015). The datasets statistics are listed in Table 1. As can be seen from the statistical indicators, these three datasets represent standard KG, sparse KG and Dense KG respectively. In our selection of the baseline, we are not only evaluating it against the state-of-the-art multi-hop reasoning methods (DeepPath (Xiong, Hoang, and Wang 2017), MINERVA (Das et al. 2018), M-Walk (Shen et al. 2018), AttnPath (Wang et al. 2019), SQUIRE (Bai et al. 2022) and CURL (Zhang et al. 2022)), but also against with other embedding-based KG reasoning methods (TransE (Bordes et al. 2013), DistMult (Yang et al. 2015), ComplEx (Trouillon et al. 2016) and LMKE (Wang et al. 2022)). FULORA and all baselines are implemented under the Pytorch framework and run on the NVIDIA 3080Ti GPU. We set the default path length to 3, and all the results are the average of the results in five experiments.\nLink Prediction Results\nFor each triplet $(e_s, r_q, e_o)$ in the test set, we convert it to a triplet query $(e_s, r_q, ?)$, and then use embedding-based models or multi-hop based models and a beam search width of 50 to get the ranking list of tail entity. Following the previous work (Bordes et al. 2013), here we use two metrics: (1) the mean reciprocal rank of all correct tail entities (MRR), and (2) the proportion of correct tail entities ranking in the top K (Hits@K) for evaluation. As shown in Table 2, we first display the performances of FULORA and all baselines on NELL-995, WN18RR and FB15K-237. From the table, we can observe that our model outperforms all previous multi-hop reasoning models on standard KG (NELL-995) and sparse KG (WN18RR). Especially on NELL-995, where our model gains marked improvements compared with the best multi-hop reasoning baseline SQUIRE and dual-agent model CURL. In addition, FULORA scores similarly to the best embedding-based model LMKE. Since FB15K-237 contains much larger number of 1-to-M than the M-to-1 relation instances (Wan et al. 2020; Zhang et al. 2022), multi-hop reasoning methods is prone to be stuck in the local entity nodes with high-degree centrality, renders it hard to reach the correct entity. Compared with strigent multi-hop reasoning models (MINERVA, M-Walk and CURL), FULORA's attention mechanism alleviates this issue by forcing the agent to focus on the neighbor which is highly relative to the query. After averaging results on three datasets, we observe that FULORA leads to overall improvements relative to the dual-agent model (CURL) by 2.5%, 2.3%, 1.3%, 3.2%, in terms of MRR and Hits @1, 3, 10.\nFact Prediction Results\nAs opposed to link prediction, fact prediction task is concerned with verifying the veracity of an unknown fact, the true test triplets are ranked with some generated false triplets. Since we share a similar query-answering mechanism as CURL (Zhang et al. 2022), FULORA is capable of identifying the most appropriate entity for a given query and eliminates the need to evaluate negative samples of any particular relation. In the experiments, the dual agents try to infer and walk through the cluster-level and entity-level respectively to reach the correct target under removing all links of groundtruth relations in the original KG. Here, we report Mean Average Precision (MAP) scores for various relation tasks of NELL-995. We reuse the results of TransE, TransR, PRA, DeepPath, MINERVA, M-Walk on seven tasks already reported in (Zhang et al. 2022). As demonstrated in Table 3, FULORA produces a satisfying result in most tasks, contributing an average gain of 9.1% relative to the multi-hop based reasoning approaches (PRA, DeepPath, MINERVA, M-Walk and CURL) and 16.1% gain compared to the embedding-based approaches (TransE and TransR).\nLong-Distance Reasoning Ability\nRecall the motivation of FULORA, we tend to address the issue of multi-hop reasoning models being unable to infer correct answer due to the lack of short direct path by improving long-distance reasoning ability. This issue is significant on standard KG and sparse KG, so we conduct the following experiments on NELL-995 and WN18RR. For effective evaluation, we compare our model with MINERVA and CURL in NELL-995 and WN18RR, where we remove the most frequently-visited short paths found by the bi-directional search (Xiong, Hoang, and Wang 2017) inside KGs.\nCluster State Similarity. One of the most significant contributions of GIANT is the implementation of dynamic path feedback, which has been shown to enhance the learning efficiency. To visually demonstrate the efficacy of dynamic path feedback, we initially focus on GIANT's reasoning ability on cluster-level, which directly impacts DWARF's reasoning results. Here, we record the cluster state similarity (CSS) $D(s_t, s_{target})$ under each epoch. As shown in Figure 5, our model outperforms CURL in scores and stability in most cases. This demonstrates that dynamic path feedback does improve GIANT's learning efficiency. See Appendix B for a comparison of the remaining tasks.\nEntity Reasoning Accuracy. The preceding analysis has demonstrated that FULORA enables GIANT to provide high-quality guidance, and next, we will demonstrate that efficient guidance-exploration gives DWARF a superior performance in long-distance reasoning against other multi-hop baselines. Figure 6 plots the MRR scores on varying path lengths on NELL-995 and WN18RR. As can be seen, FULORA outperforms CURL, MINERVA and SQUIRE with different path lengths. In particular, FULORA is much more prominent in the long-distance reasoning accuracy of WN18RR (sparse KG). It explains our motivation: FULORA can alleviate the poor performance of multi-hop reasoning approaches in the absence of a short direct path (typically found in standard KG and sparse KG) by improving long-distance reasoning ability. Overall, FULORA has a more robust performance under the demands of long-distance reasoning, and also shows minimal performance degradation for long-path settings. It is mainly due to the dynamic path feedback and efficient guidance-exploration proposed by us, which enable more efficient information sharing between GIANT and DWARF, and ensure that DWARF maintains an excellent self-exploration ability."}, {"title": "Ablation Study", "content": "We utilize $\\delta$ and $\\alpha$ to control the degree of efficient guidance-exploration and dynamic path feedback. To comprehensively explore the efficacy of efficient guidance-exploration and dynamic path feedback, we conduct link prediction on NELL-995 and WN18RR by varying the values of $\\delta$ and $\\alpha$ as $\\{0.20, 0.30, 0.40,0.50\\}$ and $\\{0.05, 0.10, 0.15, 0.20\\}$, respectively. The averaged results are shown in Figure 7. In accordance with our previous analysis, it is evident that DWARF cannot rely excessively on GIANT for guidance. Furthermore, the level of path feedback that GIANT receives should not be unduly strong. In addition, in Appendix B, we have carefully discussed the influence of three main components of FULORA on knowledge graph reasoning efficiency."}, {"title": "Conclusion", "content": "We present FULORA, an efficient guidance-exploration model built on dual-agent KG reasoning framework to enhance the agent's long-distance reasoning ability on standard KG and sparse KG. The key insight behind our approach is balancing the self-exploration of DWARF and the guidance from GIANT. Specifically, on the one hand, we leverage the attention mechanism to make DWARF pay attention to the neighbouring entities that are close to the query. On the other hand, we propose that dynamic path feedback enables GIANT to have better learning efficiency, thus providing DWARF with high-quality guidance, making the DWARF to have a favourable global vision while having excellent local reasoning ability. Experiments on three real-world datasets show that FULORA outperforms the recent state-of-the-art multi-hop reasoning methods in most cases. Further analysis of these results reveals that FULORA's Long-distance reasoning ability on standard KG and sparse KG has significant advantages compared to multi-hop reasoning methods."}, {"title": "A. Experiment Setup", "content": "Data Statistics\nWe adopt three knowledge graph datasets with different scales: NELL-995, WN18RR and FB15K-237. We give a brief overview of these datasets.\nNELL-995 (Xiong, Hoang, and Wang 2017). NELL995 is an open source machine learning dataset developed by the OpenAI research group that contains more than 950,000 pieces of entity relationship data collected from the network to help machine learning systems make inferences. NELL995 can be used to train machine learning models, such as natural language processing models, machine translation models"}]}