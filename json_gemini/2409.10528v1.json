{"title": "From Latent to Engine Manifolds: Analyzing ImageBind's Multimodal Embedding Space", "authors": ["Andrew Hamara", "Pablo Rivas"], "abstract": "This study investigates ImageBind's ability to generate meaningful fused multimodal embeddings for online auto parts listings. We propose a simplistic embedding fusion workflow that aims to capture the overlapping information of image/text pairs, ultimately combining the semantics of a post into a joint embedding. After storing such fused embeddings in a vector database, we experiment with dimensionality reduction and provide empirical evidence to convey the semantic quality of the joint embeddings by clustering and examining the posts nearest to each cluster centroid. Additionally, our initial findings with Image-Bind's emergent zero-shot cross-modal retrieval suggest that pure audio embeddings can correlate with semantically similar marketplace listings, indicating potential avenues for future research.", "sections": [{"title": "1 Introduction", "content": "The growth of online marketplaces has yielded a massive dataset of image/text pairs that loosely describe the same item but often contain noise (e.g. contact information, payment preference, URLs). Concurrently, the introduction of the transformer architecture [32] has led to significant advancements [5, 12, 13, 26, 28] in deep learning feature representation. More specifically, models trained on large corpora have proven [5,15,20] to capture semantics by encoding inputs into embedding vectors (embeddings).\nResearch of such models has shifted toward multimodality [1, 17, 21, 27, 31, 33], encoding several modalities into the same embedding space, often using a contrastive loss function. Studies on these models [12,17] have demonstrated that robust embedding spaces preserve addition and subtraction, and average word embeddings (AWEs) have been widely adopted [7,30] to represent the general semantics of a sentence, particularly when weighted [2,3,11,14]. Exploration and discussion of averaging cross-modal embeddings, however, remains limited in the literature, even within sections of embedding arithmetic [17].\nOur research focuses on leveraging ImageBind [17] to fuse image/text embeddings into a joint representation to capture overlapping information between the modalities. The main contribution of this research is empirical evidence that"}, {"title": "2 ImageBind Overview", "content": "This section summarizes ImageBind's architecture as developed by its authors, aiming to briefly capture the essence of their design and contextualize its utility in our research."}, {"title": "2.1 Modalities", "content": "ImageBind combines six different data types: text, image/video, thermal, depth, audio, and IMU into a unified embedding space. With images as an anchor, this space allows for representing semantic meaning across different modalities, even those that are not typically paired together in datasets. We utilize the text, image, and audio modalities in our study. While we acknowledge that our results could potentially be replicated with a simpler model (e.g. [18]), we chose ImageBind to maintain as much flexibility as possible in future research."}, {"title": "2.2 Encoders", "content": "ImageBind uses separate encoders for all six modalities. The Vision Transformer (ViT) is used for images and video. Audio, depth, and thermal data are also processed with ViT, where audio is first converted into spectrograms, and depth and thermal data are treated as single-channel images. Finally, they follow the text encoder design from CLIP [27]. A linear projection head is added to each encoder, yielding a fixed size d-dimensional embedding across all modalities."}, {"title": "2.3 Training", "content": "ImageBind was trained to create a joint embedding space using pairs of modalities, specifically images I and another modality M, leveraging large-scale web datasets covering a broad semantic spectrum. It also incorporates self-supervised pairings of images with other modalities including audio, depth, thermal, and Inertial Measurement Unit (IMU) data.\nFor each image $I_i$ and its corresponding observation in another modality $M_i$, ImageBind generates normalized embeddings $q_i = f(I_i)$ and $k_i = g(M_i)$, respectively, where $f$ and $g$ are deep networks. The encoders are then optimized via the InfoNCE [24] loss function:\n$L_{I,M} = -log \\left(\\frac{e^{q_i \\cdot k_i / \\tau}}{e^{q_i \\cdot k_i / \\tau} + \\sum_{j \\neq i} e^{q_i \\cdot k_j / \\tau}}\\right)$\nwhere $\\tau$ is a scalar affecting the softmax distribution, and $j$ refers to an unrelated observation. InfoNCE aims to maximize the similarity between the related embeddings (positives) $q_i$ and $k_i$ and minimize the similarity between $q_i$ and a set of unrelated observations (negatives).\nImageBind demonstrates emergent behavior where it aligns embeddings of two different non-image modalities M1, M2 that were not directly paired during"}, {"title": "3 Methodology", "content": "Our dataset comprises text and image data extracted from online C2C auto parts listings featuring textual descriptions and at least one corresponding image of the item(s) for sale. In cases where listings contain multiple images, we assume that each image is equally relevant to the listing. Additionally, we consider the images and text to hold equal weight in representing the post. We processed a total of 50k posts with a total of 220k images. Since the dataset contains personally identifiable information, it will remain closed to ensure privacy."}, {"title": "3.1 Embedding Fusion", "content": "Denote $e_{image}^{(i)}$ as the embedding vector of the i-th image of a post, with i ranging from 1-n, and n being the number of images within the post. The text content of the post is represented by the embedding vector $e_{text}$. Taking advantage of ImageBind's embedding arithmetic, we average embeddings to preserve general semantic similarities [4,9]. We calculate a mean image embedding as the arithmetic mean of the individual image embeddings, i.e., $e_{avg \\: image} = \\frac{1}{n}\\sum_{i=1}^{n} e_{image}^{(i)}$. To ensure a balanced contribution of the text embedding and mean image embedding in the fused result, the combined embedding is scaled by a factor of 0.5 [17]. Thus, the fused multimodal embedding is formally expressed as:\n$e_{multimodal} = \\frac{1}{2}\\left( \\frac{1}{n} \\sum_{i=1}^{n} e_{image}^{(i)} + e_{text} \\right)$"}, {"title": "3.2 Clustering", "content": "After storing the fused embeddings in a vector database, we apply Principal Component Analysis (PCA) to reduce their dimensionality, accelerating the processes of clustering and retrieval. We explore dimensionality reductions to 8, 16, 32, 64, and 128 dimensions, running k-means clustering on each reduced set of embeddings. The resulting centroids are evaluated against the original high-dimensional embeddings using the Silhouette score [29], Calinski-Harabasz index [6], and Davies-Bouldin index [10]. As shown in Table 1, our results indicate that reducing to 32 dimensions is marginally optimal for this dataset, assuming equal weight of the metrics."}, {"title": "4 Results and Discussion", "content": "We apply Uniform Manifold Approximation and Projection (UMAP) [23] to the original high-dimensional embeddings to reduce them to 2D for visualization, using the labels from the 32-dimensional clusters for colorization. This visual representation aids in the subsequent analysis of the clusters, where we explore the patterns that define the structure of our data."}, {"title": "4.1 Cluster Analysis", "content": "The clusters revealed from our analysis showcase distinct patterns and group characteristics within the dataset. Most notably, cluster 0, depicted in the upper-left corner of Fig. 3, is outlying."}, {"title": "4.2 Cross-Modal Retrieval", "content": "Further exploring ImageBind's set of modalities, we generate pure audio embeddings for sounds related to auto parts, e.g. car doors closing, car collisions, or engines revving. We then applied k-NN to map these audio embeddings to their nearest counterparts in our database of fused listing embeddings. Image-Bind successfully aligned a wide range of such sounds with their corresponding image/text counterparts demonstrating the model's ability to create meaningful cross-modal associations and potentially have applications in audio or video based recommendation."}, {"title": "5 Conclusions", "content": "Our analysis demonstrates that ImageBind is a powerful tool for interpreting online C2C auto parts listings. By examining clusters and the corresponding k-NN posts for each k-means centroid, we found that meaningful fused image/text embeddings could be created through simple averaging of the corresponding text/image embeddings for each listing. This research not only proves the quality of ImageBind's embedding space and the efficacy of its embedding arithmetic but also indicates potential for applications in filtering and recommendation systems within C2C marketplaces. Our exploration of audio embeddings further proves ImageBind's cross-modal capabilities, presenting a promising avenue for future research."}]}