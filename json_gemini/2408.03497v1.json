{"title": "Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN", "authors": ["Chang Yu", "Yixin Jin", "Qianwen Xing", "Ye Zhang", "Shaobo Guo", "Shuchen Meng"], "abstract": "Bank credit risk is a significant challenge in modern financial transactions, and the ability to identify qualified credit card holders among a large number of applicants is crucial for the profitability of a bank's credit card business. In the past, screening applicants' conditions often required a significant amount of manual labor, which was time-consuming and labor-intensive. Although the accuracy and reliability of previously used ML models have been continuously improving, the pursuit of more reliable and powerful AI intelligent models is undoubtedly the unremitting pursuit by major banks in the financial industry. In this study, we used a dataset of over 40,000 records provided by a commercial bank as the research object. We compared various dimensionality reduction techniques such as PCA and T-SNE for preprocessing high-dimensional datasets and performed in-depth adaptation and tuning of distributed models such as LightGBM and XGBoost, as well as deep models like Tabnet. After a series of research and processing, we obtained excellent research results by combining SMOTEENN with these techniques. The experiments demonstrated that LightGBM com- bined with PCA and SMOTEENN techniques can assist banks in accurately predicting potential high-quality customers, showing relatively outstanding performance compared to other models.", "sections": [{"title": "I. INTRODUCTION", "content": "Bank operations face many risks, among which credit risk is the primary concern. Credit risk caused by the possibility of loss to creditors due to the debtor's failure to fulfill obligations as per the contract or changes in credit quality [46], [52]. Credit risk can cause direct or indirect economic losses to banks, increase management costs, affect the utilization of bank funds, and seriously hinder the bank's lending operations. Based on the above reasons, establishing a credit risk prediction model can help customers predict whether they will default based on their data information, which can assist banks in controlling risks, reducing losses [13], [45], and increasing potential revenue.\nIn this paper, we compare various classical models used in major banks, such as the prediction model based on Random Forest and Logistic Regression used by Milad Malkipirbazar on social credit platforms, which achieved a risk prediction accuracy of 88% for Random Forest and 49% for Logistic Regression [37]. Later models, such as Neural Networks [18], also achieved an accuracy of over 80% for the fraud data they used [12]. Ke, G., and Meng, in their paper \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree. Advances in Neural Information Processing Systems,\u201d mentioned the theoretical application of using LightGBM for high-performance classification and processing [20]. Based on the principles they proposed, we combined PCA dimensionality reduction and SMOTEENN composite sampling methods and achieved very promising success through a series of data processing steps [34].\nThis study focuses on building more powerful models to predict User Credit Risk. We conducted multiple model training sessions based on the original data [17]. First, we tuned a series of algorithms, including LightGBM, XGBoost, CatBoost, Tabnet, and Neural Network, on the original data. After the initial screening, we combined PCA dimensionality reduction techniques with these models to optimize their performance [49]. On this basis, we incorporated the advanced SMOTEENN technique, which greatly improved the perfor- mance of XGBoost, LightBoost, CatBoost, and Tabnet, with LightGBM showing particularly outstanding performance.\nTo fully present the structure of our research, the fol- lowing parts are organized as follows: Section II showcases the theoretical work and preparatory work related to our research, including the sources of the various techniques we used. Section III will illustrate the technology details for the LightGBM, Tabnet, and XGBoost algorithms we employed, as well as the technical and theoretical details of PCA and SMOTEENN used in data processing. In Section IV, we will detail the evaluation parameters we used, and the experiments"}, {"title": "II. RELATED WORK", "content": "To successfully implement this research, we studied and explored a large amount of relevant literature and summarized the work of previous researchers.\nGuolin Ke conducted a study on using machine learning for credit risk prediction, proposing the necessity for finan- cial institutions to utilize artificial intelligence and machine learning to assess credit risk and analyze vast amounts of information. The author raised research questions regarding the algorithms, evaluation metrics, results, datasets, variables, and related limitations used in credit risk prediction, and found 52 relevant studies in the microcredit industry through searching well-known databases [20], [23].\nMilad Malekipirbazari's studies have explored the complex behavioral dynamics in social lending, such as the impact of adverse selection and herding effects on the lending pro- cess. Some studies have proposed purely rational investment guidelines to decrease default risk, such as investing only in borrowers with no delinquent accounts, low debt-to-income ratio, and no recent credit inquiries. Empirical analyses have shown that lender decisions on platforms like Prosper were not made purely rationally and were affected by previous lender decisions, exhibiting a herding behavior [36].\nAbdou, H. A., and Pointon, J.'s article summarizes the current state of research in the field of credit scoring. It provides a comprehensive introduction and discussion of commonly used statistical techniques, evaluation criteria, and data processing methods. Their article serves as an excellent reference for researchers in the field of credit scoring and offers valuable guidance for financial institutions in developing and optimizing credit scoring models [2], [51].\nLi et al. [28] introduced a novel calibration method for es- timating the extrinsic parameters of multiple non-overlapping cameras on the fly, which can be integrated into state-of-the- art implicit SLAMs to enhance reconstruction stability during exploration. This work has provided valuable insights into how we can improve our data processing by incorporating online calibration techniques to handle multi-sensor data more effectively [5].\nTraditional deception detection methods usually use phys- iological sensors, including blood volume pulse and skin conductance, which are invasive and not convenient for large- scale use at border entry points. Li et al. [24] pioneered the use of multimodal data, including linguistic scripts, and deep learning for deception detection with limited data.\nFor model prediction, Yiluan Xing's finding highlights the innovative integration of volatility modeling with traditional time series analysis, underscoring the potential for hybrid mod- els to enhance forecasting precision in dynamic and volatile financial markets [45].\nLiu et al. [33] proposed a local navigation system us- ing Deep Q-Network (DQN) and Double Deep Q-Network (DDQN) to address the frequent deceleration of unmanned vehicles when approaching obstacles. We also the similar method to help optimize our data processing [6].\nLi et al. [27] proposed using self-supervised local features and quantization to improve the tracking stability and real-time performance of feature-based VSLAM under illumination and viewpoint changes. Their approach to feature quantization has inspired us to explore similar techniques for optimizing our data processing pipeline, leading to more efficient and robust data analysis.we also find [17] that visual prompting can help models find better sparse networks and propose a novel data- model co-design sparsification method.\nRecently, work [4] proposed a novel framework to integrate pre-trained vision language models (VLM) with large language models (LLM). This research aims to extend VLM with rea- soning capabilities to address challenging task-oriented object detection problems. The task-agnostic and open vocabulary nature of the framework presented in work makes it promising for application to other reasoning tasks.\nVan Der Maaten, Postma, and Van den Herik (2009) provide a detailed analysis of both linear and non-linear dimensionality reduction techniques, evaluating their theoretical foundations, advantages, and performance under various conditions. The article discusses methods such as PCA, LDA, t-SNE, and Isomap, highlighting differences in computational efficiency, scalability, and ability to preserve data structure. The authors also explore practical applications in fields like image pro- cessing, bioinformatics, and text mining, and suggest potential future research directions [42]."}, {"title": "III. METHOLOGY", "content": "In the Data Processing phase, our primary task was to filter and process the existing data to enhance the performance of subsequent training. This work is not just a process, but a significant step that directly impacts the efficiency and accuracy of the model training. By analyzing and calculating the Information Value (IV) of each feature, we were able to make informed decisions that directly improved the model's performance.\nOur approach involved a detailed assessment of the IV values associated with various features. IV is a metric used to quantify the predictive power of a variable in relation to the target variable. It helps identify the most significant features of the model. Features with higher IV values are considered more predictive and, thus, more important for the model's performance. The work [8], [16] rethought and designed a new paradigm for data preprocessing that also significantly improves the model efficiency with neglectable performance degradation.\nBy calculating the IV for each feature, we were able to pri- oritize them based on their contributions to the target variable."}, {"title": "B. Random UnderSamping", "content": "In our original experimental dataset, the number of \"not approve\" versus \"approve\" instances was 45,318 to 667. Given this extreme imbalance, we applied Random UnderSampling to balance the sample sizes of each class. By randomly under- sampling the majority class, we were able to equalize the number of samples across classes, enabling the model to learn the features of each class more fairly [7], [50]. This reduction in dataset size also accelerated the training process and conserved computational resources.\nRandom UnderSampling involves multiple iterations of sampling and training. By repeatedly creating different subsets of the data and training the model on these subsets, we obtained a set of models based on various data samples. We then employed ensemble learning techniques to combine these models, thereby enhancing overall performance. Inspired by the work of Liu et al. [32], we have adopted a similar approach to efficiently analyze and process our data. Their method has significantly optimized our processing pipeline, laying a solid foundation for subsequent program processes.\nThis approach not only helped in balancing the dataset but also improved the correlation among the features we trained, thus laying a solid foundation for subsequent experiments as"}, {"title": "C. Dimensionality Reduction using Principal Component Analysis", "content": "We utilized Principal Component Analysis (PCA), our dimensionality reduction technique, in this study. PCA is a widely used statistical technology that changes high- dimensional data into a lower-dimensional space while retain- ing the essential information [19]. The primary goal of PCA is to identify the principal components, which are linear com- binations of the original features that capture the maximum variance in the data [1].\nWe can effectively compress high-dimensional data by ap- plying PCA into a lower-dimensional representation, reducing storage and computational requirements. This is particularly valuable when dealing with large-scale datasets, as it sig- nificantly saves storage space and improves data processing efficiency. Moreover, PCA helps to remove noise and re- dundant information from the data by identifying the main patterns and directions. By retaining the principal components and discarding the less significant ones, PCA extracts the essential features of the data, minimizing the impact of noise on subsequent analyses.\nFurthermore, PCA enables data visualization and ex- ploratory analysis by projecting high-dimensional data into a lower-dimensional space, typically two or three dimensions. Visualizing the PCA results allows for a more intuitive under- standing of the data's intrinsic structure, clustering patterns, and relationships between different categories [42]. This fa- cilitates data interpretation and aids in gaining insights from complex datasets.\nIn machine learning tasks, high-dimensional data is often affected by the \"curse of dimensionality,\u201d where the perfor- mance of learning algorithms deteriorates as the number of dimensions increases. By applying PCA as a preprocessing step, we can reduce the feature dimensionality, alleviate the curse of dimensionality, and improve the performance and generalization ability of learning algorithms [3]. PCA can also significantly enhance computational efficiency and simplify models by reducing the number of features, accelerating the training and prediction processes, and reducing model complexity and parameter count, as we have gotten in"}, {"title": "D. Synthetic Minority Over-sampling Technique and Edited Nearest Neighbors", "content": "Building upon the PCA aggregation, we further optimized the data with very cool technology: the Synthetic Minor- ity Over-sampling Technique and Edited Nearest Neighbors (SMOTEENN). This technique combines the strengths of both SMOTE and ENN, enabling our AI model to perform more effectively.\nThe SMOTEENN technique involves several processing steps:\nMinority Class Over-sampling: For each sample x in the minority class, a random sample y is selected from its k- nearest neighbors. A new synthetic sample is then generated between x and y. This process is repeated until the desired over-sampling ratio is achieved.\nSynthetic sample = x + \\lambda (y \u2212 x), \\lambda ~ U(0,1)\nMajority Class Under-sampling: For each sample x, the number of samples from different classes within its k-nearest neighbors is calculated. If the class of x is in the majority among its k-nearest neighbors, the sample is retained; other- wise, it is removed.\nIterative Optimization: Steps 1 and 2 are repeated until the desired class balance and sample quality are achieved. In each iteration, SMOTE generates new synthetic samples, and ENN removes noise and boundary samples, gradually optimizing the dataset.\nClass Balance: By adjusting the over-sampling ratio of SMOTE and the under-sampling ratio of ENN, the number of samples in each class in the final dataset can be controlled to achieve class balance.\nSMOTEENN has the following characteristics:\nCombination of Over-sampling and Under-sampling: SMO- TEENN combines SMOTE over-sampling and ENN under-sampling to balance the dataset by increasing the number of minority class samples and decreasing the number of majority class samples [22].\nSynthetic Sample Generation: SMOTE creates new syn- thetic samples by interpolating between minority-class sam- ples, increasing the number of minority-class samples. This helps to expand the decision boundary of the minority class and improve the classifier's ability to recognize minority class samples.\nNoise and Boundary Sample Removal: ENN reduces the number of majority class samples by removing some samples from the majority class. The removed samples are usually noise or boundary samples that may degrade the classifier's performance, and the result will act as shown in Fig 5. Iterative Optimization: SMOTEENN iteratively applies SMOTE and ENN to gradually optimize the balance and quality of the dataset until the desired class balance and sample quality are achieved."}, {"title": "E. Light Gradient Boosting Machine", "content": "Light Gradient Boosting Machine (LightGBM), devel- oped by Microsoft Research, is an advanced gradient boost- ing framework optimized for efficient and scalable deci- sion tree algorithms. It excels in handling large-scale and high-dimensional datasets, often outperforming other gradient boosting frameworks in terms of speed and accuracy. Key features include Gradient-based One-Side Sampling (GOSS) for reducing computation time, Exclusive Feature Bundling (EFB) for lowering data dimensionality, a histogram-based algorithm for improved memory efficiency, and a leaf-wise growth approach for deeper, more complex trees."}, {"title": "F. XGBoost", "content": "XGBoost is an optimized, highly efficient, portable, and flexible distributed gradient boosting system. This algorithm operates under the gradient-boosting framework, including generalized linear models and gradient-boosted decision trees. XGBoost excels in quickly and accurately solving various data science problems by integrating multiple weak learners (CART regression trees) into a strong learner. It optimizes a regularized objective function that balances prediction error and model complexity to prevent overfitting. Using a second- order Taylor expansion approximation, XGBoost simplifies the objective function into a quadratic form, allowing efficient tree structure determination and leaf weight calculation. Notable features include support for custom loss functions, handling of missing values, and parallel computing, making it superior to traditional gradient-boosted decision trees in accuracy and efficiency. XGBoost is extensively used in data mining com- petitions and industrial applications."}, {"title": "G. TabnNet", "content": "TabNet, introduced by Google, is a neural network model designed for tabular data. It uses a sequential attention mecha- nism for feature selection and an encoder-decoder framework for self-supervised learning. Compared to traditional tree- based models like Random Forest and XGBoost, TabNet offers advantages in performance, interpretability, and generalization. The core idea involves sequentially applying an Attentive Transformer for feature selection and a Feature Transformer for converting selected features into useful representations.\nThis process mimics the decision-making of trees and is repeated for multiple steps, controlled by a hyperparameter. TabNet also incorporates sparsity and feature reuse to enhance performance and prevent overfitting, using prior scales to control feature selection frequency and entropy-based loss functions to make attention masks sparser. The model provides both local and global interpretability, allowing visualization of feature importance and overall feature contributions. Through these mechanisms, TabNet constructs a complex nonlinear transformation in the feature space, enhancing expressive power while maintaining interpretability."}, {"title": "IV. EVALUATION", "content": "The following sections will comprehensively overview the various model evaluation metrics utilized in our experiments and conduct detailed model performance comparisons under different conditions. Through iterative tuning and continuous exploration, we ultimately identified the most suitable model for processing our specific data type."}, {"title": "A. Evaluation Metrics", "content": "To comprehensively evaluate these models' performance, we employed a set of widely adopted evaluation metrics, including Precision, Recall, F1 Score, and ROC AUC Score. These metrics provide a multi-faceted perspective on the model's effectiveness and help identify its strengths and weak- nesses in various scenarios.\nPrecision will measure the proportion of actual positive instances among those predicted as positive by the model [29], [39]. It reflects the model's accuracy in correctly identifying positive instances, calculated as:\nPrecision = \\frac{TP}{TP+FP}\nwhere TP denotes true positives and FP denotes false positives. A higher Precision indicates that the model can strongly minimize false positives.\nOn the other hand, Recall represents the proportion of actual positive instances correctly identified by the model [15], [21]. It captures the model's ability to identify all relevant instances, defined as:\nRecall = \\frac{TP}{TP+FN}\nwhere FN denotes false negatives. A higher Recall suggests that the model effectively covers the majority of true positive instances.\nThe F1 Score is generated by the harmonic mean of Precision and Recall, which provides a balanced measure that considers the accuracy and the coverage of the model [31], [40]. It is calculated as:\nF1 Score = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}\nThe F1 Score is particularly useful when seeking a balance between Precision and Recall, as it ensures that both metrics are relatively high.\nIn addition to the above metrics, we utilized the ROC AUC Score to evaluate the model's performance, especially in the context of imbalanced datasets [14]. The value under the ROC curve (AUC) provides a very useful measure of the classifier's ability to discriminate between classes, with a higher AUC indicating better performance. The AUC is calculated as:\nAUC = \\int_{0}^{1} ROC(t) dt\nBy employing this comprehensive set of evaluation met- rics, we established a rigorous framework for assessing the"}, {"title": "B. Experiment", "content": "We began by directly utilizing the raw data to train all of our current models. We employed a variety of commonly used models, including Logistic Regression, Random Forest, XGBoost, CatBoost, LightGBM, NGBoost, and TabNet. The training results are presented in"}, {"title": "1) Origin Data", "content": "We began by directly utilizing the raw data to train all of our current models. We employed a variety of commonly used models, including Logistic Regression, Random Forest, XGBoost, CatBoost, LightGBM, NGBoost, and TabNet. The training results are presented in"}, {"title": "2) Model Performance with PCA", "content": "In the preliminary pro- cessing, we observed that the accuracy of the models was gen- erally low. To improve the model performance, we employed Principal Component Analysis for dimensionality reduction of the data [11], [48]. After applying PCA, the models exhibited exceptional performance, and the results were remarkable, as shown in"}, {"title": "3) Further Optimization with SMOTEENN", "content": "To further op- timize our models and push the results to the limit, we applied the SMOTEENN technique to the PCA-transformed data. The SMOTEENN method is designed to handle imbal- anced datasets by combining oversampling and undersampling techniques. The results after applying SMOTEENN were also impressive, with the models exhibiting unprecedented perfor- mance, as shown in"}, {"title": "CONCLUSION", "content": "The research work on User Risk Prediction has always been a top priority in the financial domain [29]. Through a series of experiments, we discovered that LightGBM, Random Forest, and CatBoost perform exceptionally well in predicting credit risk. Among these models, LightGBM demonstrated outstanding performance, showcasing its vast potential for applications in this field.\nOur study investigated different data processing methods and their effects on model performance [47]. We discov- ered that combining Principal Component Analysis (PCA) for dimensionality reduction with SMOTEENN for address- ing imbalanced datasets significantly enhanced the models' predictive capabilities. By implementing these techniques, we achieved notably high accuracy in User Risk Prediction.\nThe outcomes of our experiments underscore the critical role of data preprocessing and feature engineering in develop- ing precise predictive models. PCA enabled us to extract es-sential features and patterns from the data, while SMOTEENN balanced the class distribution, resulting in improved model performance.\nAdditionally, the outstanding performance of LightGBM in all scenarios highlights its robustness and suitability for credit risk prediction tasks. Its capacity to manage complex datasets and deliver high-quality predictions makes it a valuable asset in the financial sector.\nOur findings have significant implications for financial institutions and businesses involved in credit risk assessment [43]. By leveraging machine learning models like LightGBM, Random Forest, and CatBoost, together with appropriate data processing techniques, organizations can enhance their risk management strategies and make more informed decisions [30].\nHowever, it is crucial to emphasize that the success of these models depends on the quality and representativeness of the training data. Continuous monitoring and updating of the models with new data are essential to maintain their effectiveness over time.\nIn conclusion, our research demonstrates the immense po- tential of machine learning techniques, particularly Light- GBM, in User Risk Prediction [26], [44]. The combination of advanced data processing methods and powerful predictive models opens up a wide range of applications in the financial domain. By embracing these approaches, organizations can significantly improve their risk assessment capabilities and make data-driven decisions to mitigate potential losses and optimize their operations."}]}