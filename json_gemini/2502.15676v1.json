{"title": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind", "authors": ["Zhining Zhang", "Chuanyang Jin", "Mung Yao Jia", "Tianmin Shu"], "abstract": "Theory of Mind (ToM), the ability to understand people's mental variables based on their behavior, is key to developing socially intelligent agents. Current approaches to Theory of Mind reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use rigid, handcrafted Bayesian Theory of Mind (BToM) models, which are more robust but cannot generalize across different domains. In this work, we introduce AutoToM, an automated Bayesian Theory of Mind method for achieving open-ended machine Theory of Mind. AutoToM can operate in any domain, infer any mental variable, and conduct robust Theory of Mind reasoning of any order. Given a Theory of Mind inference problem, AutoToM first proposes an initial BToM model. It then conducts automated Bayesian inverse planning based on the proposed model, leveraging an LLM as the backend. Based on the uncertainty of the inference, it iteratively refines the model, by introducing additional mental variables and/or incorporating more timesteps in the context. Empirical evaluations across multiple Theory of Mind benchmarks demonstrate that AutoToM consistently achieves state-of-the-art performance, offering a scalable, robust, and interpretable approach to machine Theory of Mind.", "sections": [{"title": "1 Introduction", "content": "To successfully engage in rich and complex social interactions such as cooperation, communication, and social learning, humans must adequately understand one another's mental states (e.g., goals, beliefs, desires). This ability is termed Theory of Mind (ToM) (Wimmer and Perner, 1983). Prior works have demonstrated that like human interactions, Theory of Mind is also crucial for the success of human-AI interactions (e.g., Dautenhahn, 2007; Hadfield-Menell et al., 2016; Liu et al., 2018). In particular, to safely and productively interact with humans in an open-ended manner, AI systems need to interpret humans' mental states from observed human behavior (e.g., Chandra et al., 2020; Wang et al., 2021; Wan et al., 2022; Patel and Chernova, 2022; Puig et al., 2023; Zhi-Xuan et al., 2024; Ying et al., 2024).\nThere are two primary approaches to developing machine Theory of Mind in recent works. First, with the rapid progress of large language models (LLMs), there has been an increasing interest in directly applying LLMs to reason about people's mental states with prompting strategies such as perspective-taking (Wilf et al., 2023; Sclar et al., 2023; Jung et al., 2024), change-tracking (Huang et al., 2024), and temporal-spatial reasoning (Hou et al., 2024). However, even with these advanced prompting techniques, state-of-the-art LLMs still make systematic errors in complex scenarios (Jin et al., 2024). Second, cognitive studies have demonstrated that model-based inference, in particular, Bayesian inverse planning (BIP), can reverse engineer human-like theory of Mind reasoning (Baker et al., 2009; Ullman et al., 2009; Baker et al., 2017; Zhi-Xuan et al., 2020). BIP relies on Bayesian Theory of Mind (BTOM) models (Baker et al., 2017) to approximate rational agent behaviors. Inspired by this, recent works have proposed to combine BIP and LLMs to achieve scalable yet robust model-based ToM inference (Jin et al., 2024; Shi et al., 2024). While these methods significantly outperform LLMs in specific domains, they typically require manual specification of BTOM models, including necessary mental variables (e.g., goals, beliefs) for answering a given ToM question. Therefore, they lack the required generalizability for open-ended Theory of Mind."}, {"title": "2 Related Works", "content": "Enhancing LLMs' Theory of Mind. There has been systematic evaluation that revealed LLMs' limitations in achieving robust Theory of Mind inference (Ullman, 2023; Shapira et al., 2023). To enhance LLMs' Theory of Mind capacity, recent works have proposed various prompting techniques. For instance, SimToM (Wilf et al., 2023) encourages LLMs to adopt perspective-taking, PercepToM (Jung et al., 2024) improves perception-to-belief inference by extracting relevant contextual details, and Huang et al. (2024) utilize an LLM as a world model to track environmental changes and refine prompts. Explicit symbolic modules also seem to improve LLM's accuracy through dynamic updates based on inputs. Specifically, TimeToM (Hou et al., 2024) constructs a temporal reasoning framework to support inference, while Symbolic-TOM (Sclar et al., 2023) uses graphical representations to track characters' beliefs. Additionally, Wagner et al. (2024) investigates ToM's necessity and the level of recursion required for specific tasks. However, these approaches continue to exhibit systematic errors in long contexts, complex behaviors, and recursive reasoning due to inherent limitations in inference and modeling (Jin et al., 2024; Shi et al., 2024). Most of them rely on domain-specific designs, lacking open-endedness.\nModel-based Theory of Mind inference. Model-based Theory of Mind inference, in particular, Bayesian inverse planning (BIP) (Baker et al., 2009; Ullman et al., 2009; Baker et al., 2017; Zhi-Xuan et al., 2020), explicitly constructs representations of agents' mental states and how mental states guide agents' behavior via Bayesian Theory of Mind (BTOM) models. These methods can reverse engineer human ToM inference in simple domains (e.g., Baker et al., 2017; Netanyahu et al., 2021; Shu et al., 2021). Recent works have proposed to combine BIP with LLMs to achieve robust ToM inference in more realistic settings (Ying et al., 2023; Jin et al., 2024; Shi et al., 2024). However, these methods require manual specification of the BTOM models as well as rigid, domain-specific implementations of Bayesian inference, limiting their adaptability to open-ended scenarios. To overcome this limitation, we propose AutoToM, a method capable of automatically modeling mental variables across diverse conditions and conducting automated BIP without domain-specific knowledge or implementations.\nAutomated Modeling with LLMs. There has been an increasing interest in integrating LLMs with inductive reasoning and probabilistic inference for automated modeling. Piriyakulkij et al. (2024) combine LLMs with Sequential Monte Carlo to perform probabilistic inference about underlying rules. Iterative hypothesis refinement techniques (Qiu et al., 2023) further enhance LLM-based inductive reasoning by iteratively proposing, selecting, and refining textual hypotheses of rules. Beyond rule-based hypotheses, Wang et al. (2023) prompt LLMs to generate natural language hypotheses that are then implemented as verifiable programs, while Li et al. (2024) propose a method in which LLMs construct, critique, and refine statistical models represented as probabilistic programs for data modeling. Cross et al. (2024) leverage LLMs to propose and evaluate agent strategies for multi-agent planning but do not specifically infer individual mental variables. Our method also aims to achieve automated modeling with LLMs. Unlike prior works, we propose a novel automated model discovery approach for Bayesian inverse planning, where the objective is to confidently infer any mental variable given any context via constructing a suitable Bayesian Theory of Mind model."}, {"title": "3 AutoToM", "content": "3.1 Preliminaries\nBayesian Inverse Planning (BIP) is a computational framework that models how observers infer unobservable mental states such as beliefs and goals-from an agent's behavior (Baker et al., 2009). It assumes that the agent acts rationally according to a generative model, a Bayesian Theory of Mind (BTOM) model (Baker et al., 2017), which specifies how internal variables lead to observable actions in a Bayesian network (e.g., the example models on the bottom panels in Figure 2). Using"}, {"title": "3.2 Overview of AutoToM", "content": "As shown in Figure 1, AutoToM aim to construct a suitable BToM model for Bayesian inverse planning to confidently infer any target variable. There are several key challenges in achieving this: First, different ToM inference problems require different BTOM models (as illustrated in Figure 2); our model does not know which is most suitable a priori. Second, in a given context, our method must determine which time steps are relevant. Third, there is no predefined hypothesis space for each mental variable, and each space could be infinite. Last, to infer mental variables in any context, our method must flexibly represent them without assuming specific types of representations.\nAutoToM addresses these challenges in the two key components: (1) automated Bayesian inverse planning which conducted Bayesian inverse planning given a specified BTOM model and (2) automated model discovery which proposes and adjusts the BTOM model based on the question and the inference results. These two components form a self-improvement loop to iteratively update the BTOM model and corresponding inference result as summarized in Algorithm 1. We discuss these two components in Section 3.3 and Section 3.4 respectively. More details are provided in Appendix B."}, {"title": "3.3 Automated Bayesian Inverse Planning", "content": "Given a BTOM model, M, including the necessary latent mental variables $V_{t_s:t}$ and the observable variables $X_{t_s:t}$, we integrate LLMs as the computational backend to implement every aspect of the Bayesian inverse planning (Line 2-6 in Algorithm 1). In particular, the hypothesis sampling module suggests a small set of possible values of latent variables. The Bayesian inference module then computes the posterior distribution of the target variable in the query based on Eqn. (3) or Eqn.( 4).\nHypothesis Sampling. Conventional BIP assumes a manually defined hypothesis space and hypothesis representation for each latent mental variable. Our hypothesis sampling module instead leverages an LLM to propose only a small set of quality hypotheses for each latent variable in $V_{t_s:t}$. This is similar to amortized inference (Ritchie et al., 2016; Jha et al., 2024) but does not require learning a data-driven proposal distribution. To ensure that the sampled hypotheses are relevant to the ToM inference problem, we guide the sampling process with both the question and the observable variables"}, {"title": "3.4 Automated Model Discovery", "content": "Prior works on Bayesian inverse planning rely on manually designed BTOM models, which limits their applicability to domain-specific scenarios. In contrast, the Automated Model Discovery component automatically proposes a model and dynamically adjusts it to ensure both the effectiveness of"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nWe evaluated our method on multiple Theory of Mind benchmarks, including ToMi (Le et al., 2019), BigToM (Gandhi et al., 2024), MMToM-QA (Jin et al., 2024), MuMA-ToM (Shi et al., 2024), and Hi-ToM (He et al., 2023). The diversity and complexity of these benchmarks pose significant reasoning challenges. For instance, MMTOM-QA and MuMA-ToM incorporate both visual and textual input, while MuMA-ToM and Hi-ToM require higher-order inference. Additionally, MMTOM-QA features exceptionally long contexts, and BigToM presents open-ended scenarios.\nBesides the full AutoToM method, we additionally evaluated AutoToM given manually specified models (AutoToM w/ Model Spec.).\nWe compared AutoToM against state-of-the-art baselines: LLMs: Llama 3.1 70B (Dubey et al., 2024), Gemini 2.0 Flash, Gemini 2.0 Pro (Team et al., 2023) and GPT-40 (Achiam et al., 2023); ToM prompting for LLMs: SymbolicToM (Sclar et al., 2023), SimToM (Wilf et al., 2023), TimeToM (Hou et al., 2024), and PercepToM (Jung et al., 2024); Model-based inference: BIP-ALM (Jin et al., 2024) and LIMP (Shi et al., 2024).\nFor multimodal benchmarks, MMTOM-QA and MuMA-TOM, we adopt the information fusion methods proposed by Jin et al. (2024) and Shi et al. (2024) to fuse information from visual and text inputs respectively. The fused information is in text form. We ensure that all methods use the same fused information as their input.\nWe use GPT-40 as the LLM backend for AutoToM and all ToM prompting and model-based inference baselines to ensure a fair comparison-except for TimeToM, which relies on GPT-4 and is not open-sourced."}, {"title": "4.2 Results", "content": "The main results are summarized in Table 1. Unlike AutoToM, many recent ToM baselines can only be applied to specific benchmarks. Among general methods, AutoToM achieves state-of-the-art results across all benchmarks. In particular, it outperforms its LLM backend, GPT-40, by a large margin. This is because Bayesian inverse planning is more robust for inferring mental states given long contexts with complex environments and agent behavior. It is also more adept at recursive reasoning"}, {"title": "4.3 Ablated Study", "content": "We evaluated the following variants of AutoToM for an ablation study: no hypothesis reduction (w/o hypo. reduction); always using POMDP (w/ POMDP); always using the initial model proposal without variable adjustment (w/o variable adj.); only considering the last timestep (w/ last timestep); and considering all timesteps without timestep adjustment (w/ all timesteps).\nThe results in Figure 5 show that the full AutoToM method constructs a suitable BTOM model, enabling rich ToM inferences while reducing compute. We analyze key model components below:\nHypothesis reduction. Compared to the full method, AutoToM w/o hypo. reduction has a similar accuracy but consumes 53% more tokens on average, demonstrating that hypothesis reduction optimizes efficiency without sacrificing performance.\nVariable adjustment. AutoToM dynamically identifies relevant variables for ToM inference, generalizing domain-specific BIP approaches to open-ended scenarios. Compared to its variant without variable adjustment, AutoToM improves performance with minimal additional compute. The variant that always uses POMDP performs well in scenarios aligned with the POMDP assumption (e.g., MMTOM-QA) but generalizes poorly elsewhere and incurs much higher computational costs.\nTimestep adjustment. By selecting relevant steps for inference, timestep adjustment enhances performance by focusing on essential information. In contrast, the variant using only the last timestep misses crucial details, significantly lowering performance. The variant incorporating all timesteps suffers from higher computational costs and reduced accuracy due to conditioning on unnecessary, potentially distracting information.\nFull ablation results are provided in Appendix A.3."}, {"title": "5 Conclusion", "content": "We have proposed AutoToM, a novel framework for open-ended Theory of Mind. Given any ToM inference problem, AutoToM can automatically construct a suitable BTOM model and conduct automated Bayesian inverse planning with an LLM backend. Our experimental results demonstrated that AutoToM can answer different Theory of Mind questions in diverse scenarios, significantly outperforming baselines. AutoToM suggests a promising direction toward cognitively grounded Theory of"}, {"title": "Limitations", "content": "AutoToM still makes mistakes in several aspects of the inference and model discovery. First, it sometimes proposes hypotheses unrelated to the ToM inference problem, particularly in questions where the definitions of certain mental variables are more ambiguous. Second, the LLM backend may also produce inaccurate likelihood estimation when there are multiple similar hypotheses for a latent variable. Last, model adjustment may fail to recognize the relevance of certain mental variables, resulting in an insufficient model. In addition, while AutoToM can balance accuracy and cost to a certain degree, it still requires multiple API calls. For applications with a strict computational budget, there is a need for further reducing the cost."}, {"title": "Ethics Statement", "content": "Engineering machine Theory of Mind is an important step toward building socially intelligent AI systems that can safely and productively interact with humans in the real world. Our work provides a novel framework for achieving open-ended and reliable machine Theory of Mind, which may serve as a component of any AI systems designed to interact with humans. The explicit BToM model discovered by AutoToM offers an interpretable explanation of the model results, enabling human users to examine and diagnose the model inference. While we do not foresee any negative impact or risk of our work, we acknowledge the importance of robust and trustworthy machine Theory of Mind. Interpretable and cognitively grounded machine Theory of Mind methods such as AutoToM may help mitigate the negative effects of LLMs, including hallucinations and biases. Additionally, current Theory of Mind benchmarks are typically constructed using procedurally generated stories and questions. There is a need to carefully examine the potential biases in these benchmarks, to ensure that the models evaluated on these benchmarks are fair and unbiased."}, {"title": "3.1 Preliminaries", "content": "P(V_t\\X^t) = \\frac{P(V_t, X^t)}{\\Sigma_{V} P(V, X^t)}."}, {"title": "3.1 Preliminaries", "content": "P(V_{t_s:t}|X_{t_s:t}) \\propto P(V_{t_s:t}, X_{t_s:t})."}, {"title": "3.1 Preliminaries", "content": "P(v|X_{t_s:t}) \\propto \\Sigma_{V_{t_s:t}} P(v, V_{t_s:t}, X_{t_s:t})."}, {"title": "3.1 Preliminaries", "content": "P(x_{t+1}|x_{t_s:t}) \\propto \\Sigma_{V_{t_s:t}} P(V_{t_s:t}, x_{t+1}, x_{t_s:t})."}, {"title": "3.1 Preliminaries", "content": "P(b^t | s^t, a^t) \\propto\\Sigma_{b^{t-1}} \\Sigma_{o^t} \\Sigma_{g} P(a^t | b^t, g)\\n\\bullet P(b^t | b^{t-1}, o^t)P(o^t | s^t)\n        P(b^{t-1})P(g)."}, {"title": "3.4 Automated Model Discovery", "content": "U(M,q) = R(M, q) \u2013 C(M),"}, {"title": "3.4 Automated Model Discovery", "content": "R(M,q) = -H(P(q|X_{t_s:t}))"}]}