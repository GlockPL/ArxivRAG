{"title": "MODELING AUTONOMOUS SHIFTS BETWEEN FOCUS STATE AND MIND-WANDERING USING A PREDICTIVE-CODING-INSPIRED VARIATIONAL RNN MODEL", "authors": ["Henrique Oyama", "Jun Tani"], "abstract": "The current study investigates possible neural mechanisms underling autonomous shifts between focus state and mind-wandering by conducting model simulation experiments. On this purpose, we modeled perception processes of continuous sensory sequences using our previous proposed variational RNN model which was developed based on the free energy principle. The current study extended this model by introducing an adaptation mechanism of a meta-level parameter, referred to as the meta-prior w, which regulates the complexity term in the free energy. Our simulation experiments demonstrated that autonomous shifts between focused perception and mind-wandering take place when w switches between low and high values associated with decrease and increase of the average reconstruction error over the past window. In particular, high w prioritized top-down predictions while low w emphasized bottom-up sensations. This paper explores how our experiment results align with existing studies and highlights their potential for future research.", "sections": [{"title": "1 Introduction", "content": "During mindfulness practice, such as focusing on sensations like breathing, our attention sometimes spontaneously deviates to mental imagery or thoughts about the past and future, a phenomenon known as mind-wandering [1, 2, 3]. This shift from a focused state to mind-wandering can occur not only during meditation but also in everyday activities, such as driving, listening to music, or tasting food.\nMind-wandering tends to occur more frequently during tasks that are either too easy or too difficult. When tasks are less demanding, such as simply attending to breathing, instances of mind-wandering increase. Conversely, during more challenging tasks, like reading complex material, our minds are more prone to wander because maintaining focus becomes difficult over extended periods [4, 5].\nAn interesting aspect is that the transition from the focused state (FS) to the mind-wandering state (MW) often happens without conscious awareness, whereas the shift from MW back to FS involves recognizing the mind-wandering episode consciously [6]. Various studies have investigated the psychological and systematic mechanisms underlying these shifts. For example, [7] argued that the transition from FS to MW is gradual, as evidenced by increasing response times during focused tasks. In contrast, [8] suggested that the shift is abrupt, triggered by sudden internal or external stimuli."}, {"title": "2 Materials and Methods", "content": ""}, {"title": "2.1 Overview", "content": "This study investigates autonomous shifts between the focused state (FS) and mind-wandering (MW) during a perception task using sequential sensory input patterns. The predictive coding framework is employed to model this perception process. Predictive coding assumes a generative model that predicts sensory sequences by learning both the latent state transition function and the likelihood mapping from latent states to sensory observations. Additionally, this generative model infers the current latent state through continuous sensory sequence observations.\nBoth learning and inference processes are achieved by minimizing prediction error or, more specifically, free energy. We hypothesize that FS is enhanced by strengthening bottom-up inference, while MW becomes more likely by emphasizing top-down sensory pattern generation. It is also hypothesized that shifts between FS and MW take place autonomously incorporating with adaptation of meta-level states with response to particular system variables. To test this, we propose an extended version of a variational recurrent neural network model, referred to as the Predictive Coding Inspired Variational RNN (PV-RNN) [18]. Details of the original PV-RNN and its extensions are provided in the following sections."}, {"title": "2.2 Predictive Coding Inspired Variational RNN Model (PV-RNN)", "content": "The PV-RNN is based on the free energy principle [13], where learning and inference are achieved by minimizing free energy (Equation 1) in accordance with Bayes' theorem:\n$F = DKL [q_\\phi(z|X)||p_\\theta(z)] \u2013 E_{q_\\phi (z|x)} [log p_\\theta(X|z)]$\ncomplexity accuracy\n(1)\nHere, $p_\\theta (X)$ is the marginal likelihood of the sensory observation X, given the generative model $p_\\theta$ parameterized by \u03b8. The latent variables z and inference model $q_\\phi$, parameterized by \u03c6, allow for posterior inference through minimization of free energy. Free energy consists of two terms: the complexity term (a measure of divergence between prior and posterior distributions) and the accuracy term (log-likelihood of sensory observations) [22]. PV-RNN serves as both a generative model and an inference model. The generative model predicts future sensory inputs via top-down processes, while the inference model estimates the approximate posterior from observed sensory sequences through free energy minimization as bottom-up processes.\nThe following subsections describe the PV-RNN implementation and the use of the meta-prior w."}, {"title": "2.2.1 Model Implementation", "content": "The free energy F for PV-RNN predicting a time series of T steps is given by:\n$F = w \\sum_{t=1}^T E_{q_\\phi (z_{1:t-1} d_{t-1},X_{t:T})} [DKL[q_\\phi(z_t|d_{t-1}, X_{t:T})||p_\\theta(z_t|d_{t-1})]] \\\\- \\sum_{t=1}^T E_{q_\\phi (z_{1:t-1} d_{t-1},X_{t:T})} [log p_\\theta (X_t|d_t)]$\n(2)\nPV-RNN introduces two types of latent variables: probabilistic latent variables (z) governed by Gaussian distributions, and deterministic latent variables (d). Their relationships are shown in Figure 1. In equation 2, a meta-level parameter, named meta-prior w, is introduced to balance the complexity and accuracy terms during this process. This regulation is particularly important when the limited amount of training data prevents reliable estimation of latent variable distributions. Also, dynamic behavior of PV-RNN is largely affected by setting of the meta-prior. It was shown that high setting of meta-prior w enhances generation of the top-down imagery while low setting of it enhances the bottom-up sensory perception [19, 20, 21, 12]."}, {"title": "2.2.2 Learning and Inference", "content": "The free energy F of PV-RNN can be computed as follows by adapting the original equation 2. Given a PV-RNN with L layers, predicting a T time series sensory inputs, F can be written as\n$F=\\sum_{t=1}^T \\sum_{l=1}^L w^lDKL[q_\\phi(z|d_{t-1}, X_{t:T})||p_\\theta(z|d_{t-1})] - \\sum_{t=1}^T||X_t - \\hat{X_t}||^2$\n(6)\nwhere $w^l$ is w specific to lth layer, and X denotes the prediction output of the PV-RNN. In equation 6, we approximate the expectation with respect to the approximate posterior by iterative sampling. Also, the accuracy term is replaced by the squared error, which can be regarded a special case of computation of log-likelihood wherein each dimension of X and $\\hat{X}$ is independent and follows a Gaussian distribution with standard deviation 1. Since the Kullback-Leibler (KL) divergence between two one-dimensional Gaussian distributions takes a simple expression, equation 6 is reduced to\n$F=\\sum_{t=1}^T \\sum_{l=1}^L \\frac{w^l}{R^l} \\sum_{r=1}^{R} \\delta(l,r,t) - \\frac{1}{R_X}\\sum_{t=1}^T ||X_t - \\hat{X_t}||^2$\n(7)\nwhere\n$\\delta(l, r, t) = log\\frac{\\sigma_{p,l,r}^2}{\\sigma_{q,l,r}^2}+\\frac{\\mu_{q,l,r}-\\mu_{p,l,r}^2+\\sigma_{q,l,r}^2}{2\\sigma_{p,l,r}^2}-\\frac{1}{2}$\n(8)\n$\u00b5_{p,l,r}^t$ represents rth element of \u00b5 of the prior, and the same notation is applied to $\u00b5_{q,l,r}^t$, $\u03c3_{p,l,r}^t$, and $\u03c3_{q,l,r}^t$. $R$ denotes the dimension of $z^l$. Given that the complexity term is summed over all the dimension of z, which is arbitrary to the network design, and the accuracy term is to all the data dimension, which varies among data, the free energy is normalized with respect to the dimension of z and the data dimension. Therefore, introducing such normalization, the free energy of PV-RNN in the study is computed by\n$F=\\sum_{t=1}^T \\frac{1}{Rx} \\sum_{t=1}^T \\frac{w^l}{R^l} \\sum_{r=1}^{R} \\delta(l,r,t) - \\frac{1}{Rx}||X_t - \\hat{X_t}||^2$\n(9)"}, {"title": "2.2.3 Adaptation of Meta-Prior", "content": "The meta-prior w is dynamically adapted based on the average prediction error ($err_{sum}$) over a fixed length time window in the past. When the error decreases below a predefined threshold ($Thr_l$), w transitions to a high value ($w^h$), prioritizing top-down generation, which leads to generating MW. This can be intuitively understood from analogy that continuing easy or predictable tasks tends to initiate MW [5, 4]. Conversely, when the average prediction error exceeds an upper threshold ($Thr_h$), w transitions to a low value ($w^l$), enhancing bottom-up inference. The implementation strategy for autonomous meta-prior switching between FS and MW is described in Algorithm 1. Specifically, the probabilistic shifting between the two modes is given by equations 10-11, where Temp is the temperature, a tunable parameter that can reflect how stochastic or deterministic the system is (see Section 3.2). It is highly speculated that this dynamic adaptation should enable autonomous transitions between FS and MW, as will be validated in the simulation experiments detailed in subsequent sections.\n$P(FS \u2192 MW) = sigmoid (\\frac{-(err_{sum} - Thr_l)}{Temp})$\n(10)\n$P(MW \u2192 FS) = sigmoid (\\frac{err_{sum} - Thr_h}{Temp})$\n(11)"}, {"title": "3 Experiments and Results", "content": ""}, {"title": "3.1 Model Training", "content": "First, we trained a PV-RNN with 2-dimensional sensory sequence data. The training data comprised 80 sequences, each containing 2160 time steps. For preparing those trajectories, we designed 2 different 2-dimensional cyclic patterns, one with periodicity of 40 time steps and the other with periodicity of 27 time steps. Each trajectory was made of probabilistic switching among these 2 cyclic patterns wherein after one cycle of a particular pattern the same pattern repeats with a probability of 60% and the pattern transits to the other pattern with a probability of 40% equally. Noise has been added to individual points at randomly spaced intervals. The intervals between noise points are determined by drawing from a normal distribution (mean of 1, standard deviation of 10), providing a variable time step size. At each noise interval, Gaussian noise (mean of 0, standard deviation of 0.001) is added to the current data point, slightly perturbing its coordinates to simulate natural fluctuations without disrupting the cyclic structure. A part of the training trajectory is shown in Figure 2."}, {"title": "3.2 Testing of perception task", "content": "The trained PV-RNN was tested by performing the perception task. In the test, the inference process was performed within the inference window, while one of the trained patterns was used as the target sensory sequence for the inference of the latent variables. The length of the inference window was set to 400 time steps. The adaptation of meta-prior, w, during inference with the monitoring of the average prediction error over 300 time steps was carried out using the parameters listed in Table 2."}, {"title": "4 Discussion", "content": "This study explored the neural mechanisms underlying autonomous shifts between the focused state (FS) and mind-wandering (MW) through simulation experiments using a newly proposed model based on the free energy principle. The proposed model, an extension of PV-RNN, introduces an adaptation mechanism for a meta-level parameter, the meta-prior w, which is modulated based on the average reconstruction error over a fixed-size past window. Specifically, w probabilistically switches to a high value when the average reconstruction error decreases close to a minimal threshold and to a low value when the average reconstruction error increases near a maximal threshold.\nIn the simulation experiments, the PV-RNN was first trained to generate probabilistic transitions between two distinct cyclic patterns. In the perception task phase, latent variables within the inference window were inferred to minimize the reconstruction error for given target sensory sequence while adapting w. One of the trained cyclic patterns was used as the target.\nWhen w shifted to a low value, stronger bottom-up sensory perception dominated, regenerating the observed sensory sequence in the outputs with minimal reconstruction error while allowing larger Kullback-Leibler divergence between the prior and the approximate posterior. This leads to a focused state. Conversely, when w shifted to a high value, the approximated posterior is attracted toward the prior by stronger mean of minimizing the Kullback-Leibler divergence between the prior and the approximated posterior. This allowed stronger top-down processing while less attending to sensation, generating relatively large reconstruction error in the inference window. This results in a state resembling mind-wandering.\nOne limitation of the current study is that the proposed model does not account for the phenomenon of becoming consciously aware of MW, which enables redirection of attention back to FS. [11] hypothesize that inferring a \"true meta-state\" by asking, \"How aware am I of where my attention is?\" could trigger self-awareness of MW. While the dynamically changing meta-prior in the current model modulates the balance between top-down and bottom-up information flow, leading to shifts between FS and MW, it may correspond to the meta-state proposed in [11]. However, the current model lacks a mechanism for explicitly inferring such a meta-state, making it unable to account for self-\nawareness of it. Future studies should address this limitation by extending the model to include an inference mechanism for a meta-state.\nHow do the current results relate to either the two-stage model [9] or the multiple sub-event model [10] described previously? The two-stage model suggests that the probability of remaining in FS decreases over time during an FS-MW episode, which concludes with conscious awareness of MW. In contrast, the multiple sub-event model posits a lesser decrease in this probability, speculating that multiple unconscious shifts between FS and MW occur before MW is consciously noticed. Since the current model does not account for self-awareness of MW, as discussed earlier, it is challenging to directly align its results with either of these models.\nFinally, numerous studies have indicated that MW during the resting state is intricately linked to the functional organization and dynamics of brain networks, particularly the default network (DN), central executive network (CEN), and salience network (SN) [28, 29, 30]. The current study does not model interactions between such distinct networks. Extending the model to incorporate dynamic interactions among these networks would provide a tighter connection to established neuroscientific findings on resting-state phenomena and offer deeper insights into mind-wandering."}]}