{"title": "StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization", "authors": ["Kaiyuan Tang", "Chaoli Wang"], "abstract": "In volume visualization, visualization synthesis has attracted much attention due to its ability to generate novel visualizations without following the conventional rendering pipeline. However, existing solutions based on generative adversarial networks often require many training images and take significant training time. Still, issues such as low quality, consistency, and flexibility persist. This paper introduces StyleRF-VolVis, an innovative style transfer framework for expressive volume visualization (VolVis) via neural radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its ability to accurately separate the underlying scene geometry (i.e., content) and color appearance (i.e., style), conveniently modify color, opacity, and lighting of the original rendering while maintaining visual content consistency across the views, and effectively transfer arbitrary styles from reference images to the reconstructed 3D scene. To achieve these, we design a base NeRF model for scene geometry extraction, a palette color network to classify regions of the radiance field for photorealistic editing, and an unrestricted color network to lift the color palette constraint via knowledge distillation for non-photorealistic editing. We demonstrate the superior quality, consistency, and flexibility of StyleRF-VolVis by experimenting with various volume rendering scenes and reference images and comparing StyleRF-VolVis against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF and SNERF) style rendering solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the early 1990s, volume visualization (VolVis) has been a central topic in scientific visualization research. One of the most popular techniques for VolVis is direct volume rendering (DVR). It works by casting rays from the image plane to the 3D volume, gathering samples along each ray, mapping them to visual quantities (i.e., colors and opacities) via transfer function (TF) lookups, and compositing the final color for each pixel in the rendering image. Thanks to the astonishing advances in graphics hardware, DVR can be efficiently implemented to achieve interactive framerates and high-quality visualizations, providing superior capability for users to explore volumetric datasets interactively. This practice has been the norm for the past two decades.\nThe recent surge of deep learning for scientific visualization research [55] has sprouted new opportunities for VolVis. Leveraging the capabilities of generative adversarial networks (GANs), one can train a network to synthesize rendering results under novel viewpoints, TFs, or other parameters, eliminating the need to access the original volumetric data and bypassing the conventional rendering pipeline [5, 23]. These seemingly impossible advances could have shocked many researchers just several years ago but are widely understood by the research community nowadays. After all, generative AI has swept across many fields, culminating in its extraordinary power to synthesize novel images and videos from text prompts and level the playing field for the masses.\nInspired by this fantastic trend, in this paper, we aim to design a deep learning framework to accomplish expressive visualization via style transfer for VolVis. Similar to prior work [5,17,21,23], we are supposed to be given a set of 2D images captured from the sample viewpoints and rendered with a fixed, reasonably good, yet unknown TF. Also, we assume no access to the 3D volumetric dataset during training or inference. We aspire to develop a new model that achieves expressive visualization by meeting the following goals (G1 to G3) for end users. First, they can freely explore the DVR scene in excellent visual quality from previously unseen viewpoints (G1). Second, they can further flexibly recolor the rendering results photo-realistically by editing the underlying colors, opacities, and lighting effects (G2). Third, and most importantly, they can even intuitively modify the rendering results non-photorealistically by transferring styles (e.g., salient brushstrokes) from any image or painting of interest to the DVR scene (G3).\nRealizing G1 is relatively straightforward. Unlike GAN-based techniques, state-of-the-art solutions based on neural radiance field (NeRF) can deliver high-resolution, high-quality novel view synthesis results using a set of sample images. Nevertheless, significant challenges exist for achieving G2 and G3 due to the considerable gaps between the given 2D images and the 3D scene we aim to reconstruct and edit. For G2, the main challenge lies in accurately extracting the contributing color, opacity, and lighting information for faithful downstream edits to ensure visual content consistency. For G3, the grand challenge is to move beyond a fixed number of colors decoded from the original rendering images and adapt to abundantly rich color patterns or textures in the reference images.\nWe introduce StyleRF-VolVis, achieving style transfer of NeRFs for expressive VolVis. For large volumetric data, NeRF-based scene representations are space efficient and thus have implications for altering renderings in a compressive manner. At its core, we highlight three key components of StyleRF-VolVis to realize the stated goals (G1 to G3)."}, {"title": "2 RELATED WORK", "content": "This section discusses related work on deep learning for VolVis, style transfer, NeRF for stylization, and knowledge distillation.\nDeep learning for VolVis. Over the past few years, deep learning has emerged as a promising solution for improving the DVR process. One application is to use deep learning models to replace the traditional DVR pipeline. For instance, Berger et al. [5] constructed a GAN to synthesize volume-rendered images by investigating the image space of volume rendering under various TFs and viewing parameters. Hong et al. [23] proposed a GAN to synthesize high-resolution images from volume data under the desired rendering effect without knowing the TF. He et al. [21] developed InsituNet, a surrogate model that correlates simulation and visualization parameters with visualization outcomes, allowing users to preview visualization results under different simulation settings with a trained model. Shi et al. [46] built a view-dependent neural-network-latent-based surrogate model that supports producing high-resolution visualization results. Han and Wang [17] presented CoordNet, a coordinate-based neural network to synthesize rendering images under novel viewpoints given a set of DVR images for training. Another application employs a scene representation network (SRN) to represent volumetric data, minimizing disk storage requirements and enabling interactive neural rendering without direct access to volume data. For example, Weiss et al. [59] designed a dense-grid encoding method fV-SRN that directly renders from compressed representation with no additional memory for storing volume data during rendering. Wu et al. [60] leveraged multiresolution hash grid encoding and achieved fast volume encoding and real-time rendering. Wurster et al. [61] developed APMGSRN, an adaptively placed multi-grid SRN with a domain decomposition training approach for efficient VolVis. Other deep learning works in the context of VolVis focus on data or visualization generation [14, 15, 19,51,58], compression [13,36,37,50], translation [20, 62], and completion [16]. Our work represents the first step in applying style transfer techniques to volume rendering results using a NeRF model. Compared with existing works [5, 17, 21, 23], StyleRF-VolVis takes a smaller set of DVR images for training and infers higher-quality results, maintaining content consistency across different viewpoints and supporting flexible style transfer.\nStyle transfer. Ever since Gatys et al. [11] demonstrated the capability of convolutional neural networks (CNN) in applying diverse artistic styles to natural images, neural style transfer [27] has emerged as a trending topic in both academia and industry. Numerous techniques have been developed to improve or extend the original algorithm. Johnson et al. [28] introduced a feed-forward network to solve the slow optimization problem and achieved real-time style transfer. Huang et al. [25] further maintained speed comparable to [28] without sacrificing the flexibility of transferring inputs to arbitrary new styles. This was realized using an adaptive instance normalization layer that aligns the statistics information of the content and style features. Ruder et al. [45] extended style transfer to video sequences by computing optical flow to achieve consistent style transfer across frames. Wang et al. [57] presented ReReVST that relaxes the objective function of style loss to make the transfer more robust to motions in content video.\nIn VolVis, Bruckner and Gr\u00f6ller [6] introduced a style TF using the lit sphere, which assigns the optical properties of voxel values with rendering styles instead of simple colors and opacities. However, extracting or designing the lit sphere requires artist involvement, and the rendering relies on a traditional pipeline. Our StyleRF-VolVis adopts an advanced NeRF representation, supporting end-to-end style transfer and rendering without accessing the original volume.\nNeRF for stylization. Since the groundbreaking work of Mildenhall et al. [39] in 2020, NeRF has been widely used for novel view synthesis. We refer readers to recent survey papers [7,53] to follow the roadmap of NeRF-based applications. While extensive studies [3, 10, 24, 29,40] emphasize the quality or speed enhancement of NeRF, there is also a growing body of work focusing on editing a base NeRF to perform 3D style transfer of a scene. Generally, editing NeRF for style transfer can be classified into photorealistic and non-photorealistic. One main task for photorealistic editing is recoloring the scene. For example, Kuang et al. [32] and Gong et al. [12] utilized optimizable base colors in a palette to fit the scene and modify the base colors to recolor the scene during inference. Lee et al. [33] modified the essential weights in the color multilayer perceptron (MLP) of a trained NeRF to perform local recoloring. Unlike photorealistic editing, non-photorealistic editing transfers more abstract information, such as textures or brushstrokes of an artistic work. For instance, Chiang et al. [8] utilized a hypernetwork to transfer style features into the NeRF representation. Huang et al. [26] proposed a mutual learning strategy to integrate 2D stylization with NeRF geometry consistency. Zhang et al. [63] designed a nearest neighbor-based loss to stylize a trained NeRF, which performs better stylization quality compared with standard Gram matrix-based loss. Liu et al. [34] transformed the grid features within RFs to match a reference style. Even though the quality is less impressive than [63], it supports zero-shot transfer to an unseen reference image. StyleRF-VolVis differs from all these works in that it supports photorealistic and non-photorealistic editing in a single framework. We will show that conducting photorealistic editing before non-photorealistic editing yields better quality and flexibility for stylization results.\nKnowledge distillation. Hinton et al. [22] introduced knowledge distillation (KD), which optimizes a small network to match the predictions of a large model. KD has been used in model compression and optimization [18, 22, 38]. In the NeRF space, Reiser et al. [44] leveraged a large global pre-trained NeRF to speed up the training of multiple small local NeRFs. Barron et al. [4] improved the rendering quality with an online distillation strategy. Wang et al. [56] designed R2L to learn the light fields of a pre-trained NeRF model effectively. Fang et al. [9] proposed progressive volume distillation, which provides a systematic KD method that allows the distillation between explicit and implicit NeRF architectures. Instead of applying KD to optimize a small network for model compression or optimization purposes, we utilize KD to transfer the knowledge of a photo-realistically edited RF to improve the quality of later non-photorealistic editing."}, {"title": "3 STYLERF-VOLVIS", "content": "The objective of StyleRF-VolVis is to edit the style (i.e., appearance) of the DVR scene represented by a NeRF model without losing the original content (i.e., geometry) information. The style can be photorealistic (such as adjusting the original rendering's color, opacity, and lighting attributes) or non-photorealistic (such as altering the texture to resemble"}, {"title": "3.1 Base NeRF Model", "content": "StyleRF-VolVis is built upon the scene representation from a base NeRF model. In general, a NeRF model [39] represents the scene with two neural functions: a density function \\(\\sigma(x)\\) that maps any 3D position x to a density value \\(\\sigma\\) and a color function \\(c(x, d)\\) that outputs RGB color c given an arbitrary 3D position x and viewing direction d. NeRF samples rays from the camera origin o to the rendering pixels for each training camera view d. For one sample ray r = (o, d), if we sample M points along ray r with 3D positions \\(X_1...M\\) at depths \\(t_1...M\\). The predicted pixel color \\(\\hat{c}(r)\\) of ray r is computed as\n\\[\\hat{c}(r) = \\sum_{i=1}^{M} \\alpha_i (1 - w_i) c(x_i, d),\\]\nwhere \\(w_i = exp(-(t_i - t_{i-1})\\sigma(x_i))\\) represents the transmittance along the ray between \\(x_i\\) and \\(x_{i-1}\\). \\(\\alpha_i = \\prod_{j=1}^{i-1} w_j\\) denotes the attenuation of the ray from its origin o to \\(x_i\\). We select the advanced architecture from Instant-NGP [40] to optimize the RF, ensuring fast convergence and efficient rendering. Specifically, Instant-NGP comprises a multiresolution hash-grid encoding and a tiny MLP for efficient optimization. During the optimization of the base NeRF, we optimize with a loss \\(L_{BASE}\\) defined as\n\\[L_{BASE} = ||c(r) - \\hat{c}(r)||^2,\\]\nwhere c(r) and \\(\\hat{c}(r)\\) are the ground-truth (GT) and predicted pixel colors corresponding to ray r. After optimizing the base NeRF, we freeze its model parameters in the subsequent stages to avoid style editing influencing the scene geometry. Note that the colors of the base NeRF will not be used in rendering the stylized RF; we employ the PCN for PSE and the UCN for NPSE."}, {"title": "3.2 Palette Color Network (PCN)", "content": "After training the base NeRF model for accurate density representation, we optimize a PCN that represents the scene appearance with colors from a palette. The trained PCN can classify any 3D position within the RF with its palette color. The classification of the PCN preserves region classification from the user-defined TF, ensuring visual content consistency of the downstream style editing. Figure 3 shows the PCN architecture. The network is composed of two branches: one branch uses the lighting MLP to predict the view-dependent specular color \\(c_s\\) and the other branch uses the palette MLP to output the view-independent diffuse color \\(c_d\\).\nFor the specular color branch, we use a structure similar to the Instant-NGP's color function. The lighting MLP also ingests geometry features output from the density MLP and spherical harmonics (SH) encoded view direction as input. The only difference is that the output of the lighting MLP is a grayscale color instead of the original RGB color. Before optimization, we leverage the hidden parameters of the base NeRF's color function to initialize the shallow layers of the palette MLP to improve the prediction of \\(c_s\\) at the early stage.\nFor the diffuse color branch, we design a palette MLP that captures the weights \\(\\omega\\) of palette colors P for any points within the RF. This way, we can use \\(\\omega\\) to classify points in the RF according to their primary colors, preserving visual content consistency across different views during style editing. Similar to the recent NeRF's palette-based recoloring strategy [12,32,54], we apply the RGB convex hull method [49] to extract colors \\(P_{CONVEX}\\) from training images. However, unlike previous palette recoloring methods that directly employ \\(P_{CONVEX}\\) to initialize the palette, we further refine \\(P_{CONVEX}\\) as it usually contains many similar colors that may significantly hinder the classification. We remove similar colors when the distance between two colors is below a certain threshold. This distance is measured by the hue component of colors in the hue-saturation-brightness (HSB) space. The saturation and brightness components of palette colors are not considered in measuring the distance as they can be easily optimized during training. Refer to the appendix for more details about the refinement algorithm.\nOnce the palette colors are initialized and the number of palette colors \\(N_p\\) is determined, we construct and optimize the PCN. During the optimization, the density MLP and hash-grid encoder \\(E_r\\) learned in the first stage are fixed for invariant density representation. The palette MLP takes features output from the palette hash-grid encoder \\(E_p\\) as input and outputs three values: an intensity value I shared by all palette colors to counteract the range shift caused by the RGB convex hull color normalization, a set of weights \\(\\omega\\) that indicate the contribution of each palette color to \\(c_d\\), and a color offset vector \\(\\delta\\) to enhance the expressiveness of palette colors. Given the refined palette colors P and specular color \\(c_s\\), the output color c associated with position x and view direction d is computed as\n\\[c(x,d) = c_s(x,d) + I(x) \\sum_{i=1}^{N_p} \\omega_i(x) (P_i + \\delta_i(x)),\\]\nwhere I(x) is the intensity value at position x, \\(\\omega_i(x)\\) and \\(\\delta_i(x)\\) denote the weight and color offset for palette color \\(P_i\\) at x, respectively. We then predict pixel color \\(\\hat{c}(r)\\) using Equation 1 for c(x, d), and optimize network parameters and palette colors with loss \\(L_{PALETTE}\\) defined as\n\\[L_{PALETTE} = ||c(r) - \\hat{c}(r)||^2 + \\sum_{i=1}^{M} \\sum_{j=1}^{N_p} \\lambda_{\\delta} ||\\delta_j(x_i)||,\\]\nThe first term of \\(L_{PALETTE}\\) is the MSE loss between the GT pixel color c(r) and predicted pixel color \\(\\hat{c}(r)\\). The second term is the offset regularization loss for sample points along ray r, aiming to suppress the magnitudes of color offsets and avoid significant palette color shiftings. We set \\(\\lambda_{\\delta} = 0.1\\) to control the regularization strength."}, {"title": "3.3 Photorealistic Style Editing (PSE)", "content": "When inferring the PCN, the output palette weights \\(\\omega(x)\\) can classify any 3D position within the RF according to its palette color. We then tune the network values to support PSE. Interactive PSE is achieved using Instant-NGP's fast inference ability.\nWe use the HSB space for recoloring following [32]. Given the target palette colors \\(P'\\), we compute the difference \\(\\Delta P\\) between the original palette colors P and \\(P'\\) in the HSB space. \\(\\Delta P_i\\) are then added to \\(P_i + \\delta_i(x)\\) in Equation 3. To achieve opacity or lighting editing, we multiply \\(\\sigma(x_p)\\) or \\(c_s(x_p, d)\\) by a scalar value, where \\(x_p\\) denotes a point position receiving its primary color contribution from \\(P_i\\)."}, {"title": "3.4 Unrestricted Color Network (UCN)", "content": "Although the PCN can represent the colors of the RF with PSE, it prohibits using diverse colors in NPSE. To address this issue, we utilize KD to optimize a UCN (student model) that produces similar diffuse colors within the PCN (teacher model) with no color palette constraint. Before distillation, we initially extract the average palette colors \\(\\bar{P}\\) from each reference style to replace the original palette colors P. This color transfer step ensures the distilled student network can match with styles, reducing optimization effort in NPSE. Our UCN comprises one hash-grid encoder \\(E_u\\) following an unrestricted MLP that outputs color \\(c_u\\). During distillation, we first randomly sample camera views from the scene. Then we optimize \\(E_u\\) with volume-aligned loss \\(L_{VOLUME}\\) adopted from [9] for each view. \\(L_{VOLUME}\\) is defined as\n\\[L_{VOLUME} = ||E_p(x) - E_u(x)||^2,\\]\nwhere \\(E_p(x)\\) and \\(E_u(x)\\) are the output features from the palette and unrestricted encoders for each sample point x. We optimize \\(L_{VOLUME}\\) for 150 iterations to initialize \\(E_u(x)\\) to accelerate the subsequent distillation. For the following 500 iterations, we optimize both \\(E_u(x)\\) and unrestricted MLP with color loss \\(L_{COLOR}\\), which is defined as\n\\[L_{COLOR} = ||\\bar{c}_d(x) - c_u(x)||^2,\\]\nwhere \\(\\bar{c}_d(x)\\) and \\(c_u(x)\\) are the diffuse colors output from the PCN with the average palette colors \\(\\bar{P}\\) and the UCN at x."}, {"title": "3.5 Non-Photorealistic Style Editing (NPSE)", "content": "Given one or multiple reference images, we first extract \\(N_p\\) desired reference styles \\(S = \\{S_1, S_2,..., S_{N_p}\\}\\) for \\(N_p\\) regions of the RF. When the number of reference images is less than \\(N_p\\), or users are only interested in local patterns of one image, we can leverage unsupervised segmentation techniques such as the segment anything model (SAM) [30] to automatically or manually (with point prompt selection) divide a reference image into localized style regions for flexible stylization.\nWe use the optimizable unrestricted color \\(c_u(x)\\) (Section 3.4) and frozen density \\(\\sigma(x)\\) of the base NeRF (Section 3.1) to construct a new RF. During training, we first compute \\(N_p\\) rendering images"}, {"title": "3.6 Interactive interface", "content": "shows our visual interface for users to conduct PSE and NPSE for a given DVR scene. For PSE, users select a palette ID and interactively modify the corresponding scene region's color, opacity, and lighting. For NPSE, users first specify the target style for each scene region from single or multiple reference images. They then start optimizating UCN. After NPSE, users can still change the opacity and lighting of the stylized DVR scene but cannot modify the color as the stylized scene is represented with UCN. Refer to the accompanying video for the recorded interaction with the interface."}, {"title": "4 RESULTS AND DISCUSSION", "content": ""}, {"title": "4.1 Datasets, Training, Baselines, and Metrics", "content": "Datasets and network training. To show the quality, consistency, and flexibility of StyleRF-VolVis, we evaluate it using the datasets listed in Table 1. The visible ranges are the opacity bumps in the TF specification corresponding to distinct visual contents for generating the original rendering. All reference images used are copyright-free, coming from WikiArt [2, 42] and Pexels [1]. We implemented StyleRF-VolVis using PyTorch and ran experiments on an NVIDIA RTX A4000 graphics card with 16 GB memory. We trained the base NeRF model with 30,000 iterations and PCN with 2,500 iterations using 92 DVR images with cameras evenly placed along a sphere enclosing the volume. The KD of UCN and NPSE does not need GT images. For KD, we optimized UCN with 624 iterations, where each iteration trains on one of the randomly sampled 312 camera views. For NPSE, we optimized UCN using the NNFM loss with 210 iterations, where each iteration trains on one of the uniformly sampled 42 camera views. For all three stages (base NeRF, PCN, and UCN), we applied the Adam optimizer with a learning rate of 0.01 and \\(\\beta_1 = 0.9, \\beta_2 = 0.999\\) for training and set the batch size to 4,096 rays. For the base NeRF model, PCN, and UCN during KD, we decayed the learning rate exponentially for every iteration until it"}, {"title": "4.2 Qualitative Comparison", "content": "We compare StyleRF-VolVis against baseline methods using DVR scenes of vortex, supernova, combustion, and five jets. Each scene showcases two stylization results using distinct reference images. Figure 8 shows the qualitative comparison. Unlike all baseline methods, StyleRF-VolVis supports customized style selection and mapping to distinct visual contents, which we highlight in the DVR scene.\nOverall, StyleRF-VolVis produces clearer and more consistent stylization results than other methods. In contrast to video- and image-based stylization methods (ReReVST and AdaIN), NeRF-based stylization methods (StyleRF-VolVis, ARF, and SNeRF) preserve better geometry consistency and are capable of separating foreground objects from the background during stylization. Among NeRF-based methods, SNERF captures the overall style of the reference image but cannot assign distinct styles to different visual regions of the original scene. Thus, SNeRF's stylization is blurry and mixed, making it hard to identify the difference between individual visual contents in the original DVR scene. For the vortex dataset, ARF shows better stylization than SNERF in preserving visual content distinction. However, the uncontrollable ARF style selection process could let the model choose a negligible local style that does not match the overall style (e.g., the WikiArt-1 case) or select one style for multiple visual regions (e.g., the WikiArt-8 case). Both drawbacks could lead to poor stylization results. In addition, in the stylization process, ARF and SNERF discard view directions in their input to ensure cross-view stylization consistency. Consequently, they cannot preserve the view-dependent lighting of the DVR scene. Unlike ARF and SNERF, StyleRF-VolVis utilizes the lighting MLP to preserve lighting information of visual content while ensuring cross-view consistency during NPSE by removing view direction input in UCN optimization. Moreover, StyleRF-VolVis allows explicit assigning of different styles to different DVR regions. Via color transfer, we ensure that the region color always aligns with the overall style color, avoiding the disadvantage of NNFM loss, which focuses on negligible local style."}, {"title": "4.3 Quantitative Comparison", "content": "Training and inference time. We report the training and inference time for all NeRF-based methods in Table 2. The training of StyleRF-VolVis is slower than ARF but faster than SNeRF. Unlike ARF, StyleRF-VolVis requires additional steps to optimize PCN and apply KD to UCN before NPSE. Moreover, optimizing on a single camera view during NPSE requires rendering different regions independently. This process maps a style to a region, slowing the stylization process. Although the alternative training process of SNeRF is relatively straightforward, it demands significant time to train the image stylization module, leading to the longest training time. For inference, StyleRF-VolVis needs additional feedforward steps of the lighting MLP to provide lighting information, which takes longer than ARF or SNeRF.\nShort- and long-range consistency. In Table 3, we compare shortand long-range cross-view consistency for different methods. We use"}, {"title": "4.4 Flexibility of NPSE", "content": "Our NPSE strategy allows users to define which style should be applied to which part of a DVR scene (analogous to using the TF), which"}, {"title": "4.5 PCN Classification and PSE", "content": "As discussed in Section 4.4, the PCN classification outcomes are crucial for style editing. Visual contents in a DVR scene are often nested with each other. However, as demonstrated in Figure 12, even for a complex scene like the earthquake, PCN can still produce region classification closely resembling GT, even though only DVR images are used for training. Although the colors and lighting of PCN classification results are not perfect, the boundary of each region is clear, and even fine details maintain consistency with GT. This ensures precise PSE and NPSE of each region in subsequent stages.\nIn , we showcase the PSE outcomes for various DVR scenes. With the assistance of PCN classification, we can perform a"}, {"title": "4.6 Ablation Study", "content": "Benefit of PSE to NPSE. At the beginning of UCN optimization, we perform the color transfer in PCN (teacher network) to ensure that the color representation of UCN (student network) matches the selected styles before NPSE. This PSE step is essential for speeding up the convergence and improving the stylization quality of UCN during NPSE optimization. Figure 14 compares the style transfer process without and with color transfer. Comparing the stylization results of 42 and 168 iterations, we can see that NPSE without color transfer suffers from inaccurate style color matching at an early stage and shows unclear stylization (i.e., black borders, see arrows) at 168 iterations. In contrast, NPSE with color transfer matches the overall style color well at the early stage and reveals well-stylized texture at 168 iterations.\nView-dependent lighting in NPSE. Recent NeRF-based stylization methods [8, 26, 34, 41, 63] discard view directions in the input to ensure cross-view stylization consistency. However, they ignore that view-dependent lighting of the original scene is essential for maintaining consistency between the original content and the stylized scene. In contrast, StyleRF-VolVis utilizes the additional lighting MLP optimized in the PCN training stage to preserve the DVR lighting during NPSE. In Figure 15, we show examples of vortex and supernova datasets to compare the effect of DVR lighting on NPSE outcomes. The results show that the stylized scene with lighting is more consistent with the original DVR scene than that without."}, {"title": "4.7 Limitations", "content": "Although StyleRF-VolVis achieves flexible, high-quality stylization of DVR scenes, it has the following limitations. First, given an initial TF, NeRF cannot represent the value ranges where opacity equals"}, {"title": "5 CONCLUSIONS AND FUTURE WORK", "content": "We have presented StyleRF-VolVis, the first work in VolVis that targets style transfer in the NeRF space. The crux of our approach lies in the accurate extraction of content and appearance information separately from the given DVR scene and the bridging between photorealistic and non-photorealistic style editing via knowledge distillation. With these innovations, StyleRF-VolVis achieves high-quality, consistent, and flexible 3D style transfer outcomes with novel view synthesis. The efficacy of StyleRF-VolVis is demonstrated with various combinations of DVR scenes and reference images. Moreover, we compare StyleRF-VolVis against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF and SNeRF) solutions via objective and subjective evaluation to showcase its superior quality performance.\nIn the future, we will extend StyleRF-VolVis to handle dynamic DVR scenes produced from time-varying datasets. The challenge is maintaining a consistent appearance over timesteps to achieve temporally coherent stylization. We will also explore StyleRF-VolVis for multivariate or ensemble VolVis, where different variables or ensembles could be mapped to visually distinct styles beyond colors for better differentiation. It remains to be seen what the appropriate number of styles and their mixing should be to leverage the human's visual capacity best while maintaining observation clarity.\nTo ease the difficulty for non-professionals using StyleRF-VolVis, we will explore integrating natural language interaction into the current graphical user interface and broaden the selection of reference images from the WikiArt collection to images created by generative AI. The success of StyleRF-VolVis will unfold exciting opportunities for VolVis beyond the originated scientific domain. We envision that such a solution will enable citizen science by fusing diverse disciplines, such as science and art, for the general public's exploration, understanding, and appreciation, which we would like to pursue."}, {"title": "APPENDIX", "content": ""}, {"title": "1 IMPLEMENTATION DETAILS", "content": "Palette refinement algorithm. When initializing palette colors P before PCN training, we refine \\(P_{CONVEX}\\) extracted from the RGB convex hull method to eliminate similar colors within the palette. Specifically, for any two colors in \\(P_{CONVEX}\\), if they are similar (i.e., their L1 distance of hue value in the HSB space is below a threshold \\(T_h\\)), we remove one of them. Furthermore, there is a small chance that \\(P_{CONVEX}\\) may contain gray colors corresponding to the light color in the DVR scene. Such gray colors are unnecessary in P as the lighting MLP has represented lighting components. Therefore, we remove any color in \\(P_{CONVEX}\\) if its brightness value in the HSB space is less than a threshold \\(T_l\\). For normalized color component values in the HSB space, we empirically set \\(T_h = 0.1\\) and \\(T_l = 0.2\\) to obtain refined P.\nLuminance background. Before NPSE, we calculate each style's luminance value L to compute the NNFM loss using VGG-16. Following the calculation steps given in [48], for each average RGB color of the selected style, we normalize each component, such as R, and convert it into the linear-scale counterpart \\(R_{lin}\\)\n\\[R_{lin} = R^{2.2}.\\]\nL of the selected style can be computed as\n\\[L = 0.2126 \\times R_{lin} + 0.7152 \\times G_{lin} + 0.0722 \\times B_{lin},\\]\nwhere the RGB component coefficients reflect the average spectral sensitivity of lighting perceived by humans. We use L as the corresponding background color for each style rendering. This way, the stylization of StyleRF-VolVis can better match the overall brightness of the selected style."}, {"title": "2 ADDITIONAL RESULTS", "content": "PSE after NPSE. After NPSE, users can still apply further PSE to the stylized scene. When doing so, one limitation is that if a region of the NPSE scene utilizes the UCN to represent the color term, users cannot apply a color PSE to the region as the PCN does not represent the color. However, users can modify the opacity and lighting of each region without restriction.\nChoice of \\(\\lambda_{\\delta}\\). When optimizing the PCN to avoid palette color shiftings, we include an offset regularization loss and use \\(\\lambda_{\\delta}\\) to control"}]}