{"title": "Privacy Threats and Countermeasures in Federated Learning for Internet of Things: A Systematic Review", "authors": ["Adel ElZemity", "Budi Arief"], "abstract": "Federated Learning (FL) in the Internet of Things (IoT) environments can enhance machine learning by utilising decentralised data, but at the same time, it might introduce significant privacy and security concerns due to the constrained nature of IoT devices. This represents a research challenge that we aim to address in this paper. We systematically analysed recent literature to identify privacy threats in FL within IoT environments, and evaluate the defensive measures that can be employed to mitigate these threats. Using a Systematic Literature Review (SLR) approach, we searched five publication databases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating relevant papers published between 2017 and April 2024, a period which spans from the introduction of FL until now. Guided by the PRISMA protocol, we selected 49 papers to focus our systematic review on. We analysed these papers, paying special attention to the privacy threats and defensive measures specifically within the context of IoT - using inclusion and exclusion criteria tailored to highlight recent advances and critical insights. We identified various privacy threats, including inference attacks, poisoning attacks, and eavesdropping, along with defensive measures such as Differential Privacy and Secure Multi-Party Computation. These defences were evaluated for their effectiveness in protecting privacy without compromising the functional integrity of FL in IoT settings. Our review underscores the necessity for robust and efficient privacy-preserving strategies tailored for IoT environments. Notably, there is a need for strategies against replay, evasion, and model stealing attacks. Exploring lightweight defensive measures and emerging technologies such as blockchain may help improve the privacy of FL in IoT, leading to the creation of FL models that can operate under variable network conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "The Internet of Things (IoT) consists of interconnected devices that communicate and exchange data, enhancing real- time data collection and analysis across sectors [1]. This connectivity introduces privacy and security challenges, neces- sitating solutions such as Federated Learning (FL) that train models on decentralised data. FL improves traditional machine learning by addressing issues of accuracy, efficiency, and pri- vacy [2]. However, FL in IoT faces challenges such as resource limitations, data heterogeneity, communication overheads, and privacy issues [3], [4]. These challenges are amplified by the limited computational power and energy resources of IoT devices, increasing potential risks to privacy [5]. Protecting FL data privacy on IoT devices is critical, and it requires robust defensive measures against threats such as inference attacks and data leakage. This review addresses the research gap by systematically analysing privacy threats and evaluating defensive measures within the IoT domain.\nIn order to address the identified research gap, this review systematically examines recent and pertinent literature. This review advances the knowledge regarding FL's applicabil- ity and privacy implications in IoT contexts by developing research questions centred on identifying privacy risks and defensive measures in such settings.\nEnhancing the taxonomy for FL privacy in IoT, the system- atic classification of existing publications offers insights into privacy properties, potential threats, and defence mechanisms. The importance of this review stems from its comprehensive evaluation of FL's privacy implications in the IoT domain. It is a valuable resource for practitioners and researchers who aim to understand and manage the complex interactions between privacy and AI technologies in the IoT environments, which typically have very limited resources. Other published literature reviews tend to focus only on specific facets of FL or IoT privacy. In comparison, this review is notable for its thorough analysis of FL privacy in the IoT environments.\nContributions. The key contributions of our paper are:\n\u2022 Comprehensive systematic review and analysis of privacy threats in Federated Learning (FL) within the Internet of Things (IoT) environments, including inference attacks, poisoning attacks, and eavesdropping.\n\u2022 Evaluation of various defensive measures such as Differ- ential Privacy and Secure Multi-Party Computation, as- sessing their effectiveness in safeguarding privacy without undermining the operational integrity of FL in IoT.\n\u2022 Identification of critical research gaps, particularly high- lighting the need for robust strategies against replay, eva- sion, and model stealing attacks, to enhance the privacy posture of FL in IoT.\nThe rest of the paper is organised as follows. Section II in- troduces the important background of FL, especially in relation to privacy. Section III outlines our methodology, including a detailed explanation of the review's scope. Section IV presents our findings, while Section V discusses the implications of these findings. Finally, Section VI concludes our systematic review and suggests several areas for future research."}, {"title": "II. BACKGROUND", "content": "Privacy in the IoT domain faces complex challenges due to the ubiquitous nature of the devices involved, and the vast amount of data they collect. Privacy threats are ex- acerbated by the diversity and scale of IoT environments, making effective privacy protections crucial, yet difficult to achieve. Various studies highlight the need for robust privacy- preserving measures tailored to IoT's unique constraints, such as device heterogeneity and extensive data generation [6]. Moreover, emerging solutions need to focus on enhancing privacy without compromising the functionality and scalability of IoT systems [7].\nUsers of wearable and smart IoT devices are more worried than ever about how the personal data they collect is used and shared across services. Because of its volume and diversity, pervasive user data are beneficial for state-of-the-art machine learning and deep learning algorithms, which are being used in these applications more and more. To facilitate learning over a distributed network without transferring the data from each device, Federated Averaging (FedAvg) [8] was presented as a foundational schema. FedAvg literature \u2013 and the broader FL literature - examine communication constraints and suggest enhanced learning frameworks, but do not investigate FL in severely constrained IoT environments with limited computing and storage capacity on the device [9].\nFL is a distributed machine learning technique where clients train locally without sharing personal data with the server [2]. Devices iteratively update a shared global model by aggre- gating information from each client model. Figure 1 depicts the high-level architecture of the FL process, which usually consists of three phases [4]:\n1) Data Collection and Local Model Update: The target application and task requirements are determined by the central server during the first phase. The server initialises a global model (W&) and transmits it to the chosen local clients, called participants. Every participant uses their local data to create a model. Each client k updates its model parameters (W) to find the optimal parameters that minimise the local loss function (Fk(W)) after receiving the global model (W) (where t denotes the tth iteration). The local optimal models are then shared with the FL server.\n2) Global Aggregation: The FL server aggregates the local models provided by the participants to create an updated global model (Wt+1).\n3) Model Deployment: All of the new participants are given access to the most recent global model. Phases 2 and 3 are repeated until the central server reaches a convergence by minimising the global loss function (F(WG)), which can be expressed as follows [10]: $(min_w f(w) = \\sum_1^N P_kF_k(w))$ where N is the total number of devices available, $F_k(w)$ is the expected prediction loss on a sample input of the $k^{th}$ device on parameter w, $P_k (\\geq 0)$ indicates the relative impact of each device k while satisfying $\\Sigma_k P_k = 1$, and each device k has $n_k$ samples (where $n = \\Sigma_k n_k$). $P_k = (n_k/n)$ is the expression that can be used to show the relative impact of each local device.\nAs this section has shown, current implementations of FL still face significant challenges, even though they offer promising paths for privacy-preserving ML, particularly within the IoT. These include protecting against sophisticated cyber threats that take advantage of the particular weaknesses of distributed architectures, managing resource constraints on IoT devices, and guaranteeing data privacy during model training. Significant gaps in privacy have come up from the inadequacies of existing strategies in effectively addressing these concerns, which our research attempts to address. The sections that follow will go into more detail about these issues and provide a new angle on privacy risks and the efficiency of modern defences in IoT environments."}, {"title": "III. METHODOLOGY AND SCOPE OF REVIEW", "content": "Our systematic literature review follows the PRISMA proto- col [11] for a rigorous and transparent approach. We defined research questions to identify privacy threats and defensive measures in FL within IoT contexts. Using a comprehensive search strategy and strict inclusion and exclusion criteria, we systematically analysed recent advances in the field.\nTo analyse the literature and compare the proposed tech- niques systematically, we established the following research questions to guide our assessment:\n\u2022 RQ1: What are the privacy threats present in federated learning within IoT environments?\n\u2022 RQ2: What are the defensive measures to mitigate these risks without compromising data integrity, user privacy, and confidentiality?"}, {"title": "A. Paper Selection and Data Collection", "content": "We selected keywords aligned with our research ques- tions, such as \"Federated Learning\u201d, \u201cFL\u201d, \u201cDecentralised Machine Learning\u201d, \u201cPrivacy-Preserving Machine Learning\", \"Resource\u201d, \u201cEnergy\u201d, \u201cPower\u201d, \u201cLimited\u201d, \u201cConstrain\u201d, \u201cPri- vacy\u201d, and \u201cThreat\". The search query was: (\u201cFederated Learning\u201d OR \u201cFL\u201d OR \u201cDecentralised Machine Learning\") AND (\u201cIoT\u201d OR \u201cInternet of Things\") AND (\u201cResource\u201d OR \"Energy", "Power": "AND (\u201cLimited\u201d OR \u201cConstrain\u201d) AND (\"Privacy\u201d OR \u201cThreat\").\nWe used Scopus, IEEE Xplore, Wiley, ACM, and Science Direct to filter articles based on inclusion and exclusion criteria: articles from 2017 to April 2024, written in English, incorporating \"federated learning"}, {"title": "B. Summary of Selected Papers", "content": "Following the PRISMA protocol, 980 papers were identi- fied through database searching. Additionally, we employed citation chaining, identifying 30 additional papers through backward and forward snowballing from our core articles to ensure thorough coverage of the literature. After removing duplicates, 970 papers remained, but 715 of which were then excluded after initial title and abstract filtering. Subsequently, the full-text of the remaining articles were subjected to further screening based on the inclusion and exclusion criteria by the researchers involved. In the event of disagreement between the researchers, a third researcher served as a mediator to resolve the selection conflict. Finally, 49 articles were selected for subsequent analysis in this systematic literature review."}, {"title": "IV. RESULTS", "content": "Existing reviews offer insights into the challenges and limitations of FL in IoT. Hosseinzadeh et al. [12] discuss com- munication efficiency, resource allocation, and client selection in FL, focusing on its advantages without balancing potential drawbacks. Mothukuri et al. [13] highlight security threats such as communication bottlenecks and backdoor attacks in FL, but their review lacks comprehensive coverage of all privacy-related threats and limitations, particularly in resource- constrained IoT environments. Nguyen et al. [14] emphasise security and privacy in FL for IoT networks but do not provide a comprehensive risk analysis. Similarly, Khan et al. [15] discuss privacy challenges such as edge-cloud server inference and malicious user threats but lack an in-depth examination of IoT-specific vulnerabilities. Ferrag et al. [16] focus on attack vectors such as model poisoning and inference attacks but do not extensively evaluate privacy challenges in the IoT context. These reviews highlight the need for a comprehensive understanding of privacy concerns in FL within resource- constrained IoT environments. FL in IoT faces various threats across its phases.\n\n\n\n\n\n\n\n\n\n1) Inference Attacks: Membership inference attacks pose a significant risk to users' privacy in resource-constrained IoT environments, determining if a specific data record was used in training a model. This can reveal sensitive information about the data subjects. Zhang et al. [17] discuss a member- ship inference attack using Generative Adversarial Networks (GANs) in FL, highlighting significant privacy leakages. This attack particularly affects FL models in IoT environments. Chen et al. [18] propose a novel user-level inference attack mechanism in FL, which is a critical concern for privacy in IoT implementations. Nguyen et al. [19] explore an active membership inference attack in FL under local differential privacy settings, demonstrating vulnerabilities in IoT data privacy. Zhao et al. [20] analyse membership inference attacks at a user level within a FL framework deployed in a wireless IoT network. Model inversion attacks use model outputs to infer sensitive features of the input data. Salim et al. [21] discuss FL's vulnerability to model inversion attacks in IoT- based social networks and propose a differential privacy-based framework to counter these threats. Xie et al. [22] explore the challenges of resisting model inversion and extraction attacks in IoT using FL, proposing a lightweight privacy protection protocol for edge computing. Zhang et al. [23] address privacy threats, including model inversion, using cryptographic meth- ods within IoT-based healthcare systems employing FL. Zhou et al. [24] discuss protecting against model inversion attacks within a fog computing scenario using FL, focusing on the IoT context. Property inference attacks infer properties that hold over the entire training dataset or its subsets, which were not intended to be shared. A study by Shen et al. [25] explores property inference attacks in blockchain-assisted FL within intelligent edge computing, specifically targeting unintended property leakages from model updates. Wang et al. [26] present novel methodologies for carrying out a poisoning- assisted property inference attack that specifically targets FL systems, aiming to infer properties of training data that are unrelated to the learning objective.\n2) Poisoning Attacks: Adversaries intentionally manipulate the training data or the model updates to corrupt the learning process, leading to incorrect model outputs or leaking specific data characteristics. Sun et al. [27] discuss data poisoning at- tacks in FL within IoT systems, highlighting the vulnerability of federated models to such attacks and proposing a novel systems-aware optimisation method to derive optimal attack strategies. Li et al. [28] explore adaptive poisoning attacks in the context of software-defined Industrial IoT (IIoT). They propose a framework that uses a tentacle distribution-based detection algorithm and a stochastic tentacle data exchanging protocol to minimise the impact of poisoned data. Zhang et al. [29] introduce PoisonGAN, a generative poisoning attack model for FL in edge computing. They demonstrate how this model can efficiently reduce attack assumptions and make at- tacks feasible in practice. Zhang et al. [30] propose RobustFL, a robust FL method for defending against poisoning attacks in IIoT, using an adversarial training framework. This method improves the resistance of the FL model to such attacks.\n3) Eavesdropping: Unauthorised interception of data dur- ing transmission between IoT devices and the central server or amongst the devices themselves, potentially exposing sensitive data. Zheng et al. [31] explore FL as a method to preserve data training privacy from eavesdropping attacks in mobile- edge computing-based IoT. They propose a framework for op- timising resource allocation to balance learning accuracy and energy consumption while protecting privacy. Ruzafa-Alcazar et al. [32] discuss the use of FL with differential privacy techniques to safeguard against intrusion and eavesdropping in IIoT environments. Matheu et al. [33] propose an FL approach to detect cyberattacks in IoT-enabled smart cities, integrating it with manufacturer usage descriptions to address eavesdropping and other attacks.\n4) Sybil Attacks: Attackers create multiple fake identities to influence the training process maliciously or to gain a disproportionate influence over the model. Xiao et al. [34] propose a novel approach for Sybil-based collusion attacks in IIoT FL systems, demonstrating how malicious participants can manipulate model aggregation through Sybil identities. Jiang et al. [35] address Sybil attacks in the context of dif- ferential privacy-enhanced FL, proposing defence mechanisms that monitor training loss for anomalies to detect and mitigate such attacks. Fung et al. [36] introduce \u201cFoolsGold\", a defence against Sybil-based poisoning attacks in FL, which identifies malicious Sybils by examining the diversity of client updates.\n5) Backdoor Attacks: Embedding hidden malicious func- tionality in the FL model, which can be activated to cause intended misbehaviour or to extract data. Hou et al. [37] discuss a defence mechanism against backdoor attacks in IIoT applications using FL, incorporating federated backdoor filters with explainable AI models. Ranjan et al. [38] propose graph- theoretic algorithms to identify and isolate backdoor attackers in FL systems, improving the robustness of the system. Yang et al. [39] explore clean-label poisoning attacks on FL in IoT environments, focusing on stealth and robustness of the attacks. Liu et al. [40] enhance the effectiveness of early-stage backdoor attacks in FL by leveraging information leakage about the whole population's data distribution.\n6) Gradient Leakage: Even though raw data does not leave local devices, sharing model gradients can still leak information about the original data. Zhu et al. [41] focus on defending against inference attacks in FL within IoT, using parameter compression to mitigate the risk of gradient leakage.\n7) Reconstruction: Attackers use the gradients or model parameters shared during FL updates to reconstruct the inputs used in training. Techniques might involve solving optimisa- tion problems that aim to find data points that would produce similar gradients. Li et al. [42] discuss the vulnerabilities of FL models to gradient-based reconstruction attacks, particularly in complex IoT environments. They propose a defence strategy suitable for resource-constrained IoT devices, emphasising adaptive communication to ensure model security and decrease communication overhead. Na et al. [43] reevaluate the ef- fectiveness of current privacy-preserving techniques against reconstruction attacks in FL, proposing a new lightweight solution called Fragmented Federated Learning (FFL).\""}, {"title": "B. Defensive Measures", "content": "We also systematically evaluated the measures used within the literature to protect FL processes in IoT environments. Based on the specific privacy threats they address, we group seven defensive measures into three key categories: (i) Encryp- tion and Obfuscation, (ii) Differential Privacy and Noise Injec- tion, and (iii) Secure Multi-Party Computation and Anonymi- sation. These are detailed below.\n1) Encryption and Obfuscation: These measures encrypt or alter data to prevent direct access or interpretation by unauthorised parties. Gradient obfuscation conceals sensitive data by altering gradient samples within FL processes to prevent direct inference attacks without sacrificing model performance. It protects data by making it difficult to reverse- engineer or identify sensitive information from gradients [44]. Yue et al. [63] present an analysis of how gradient obfuscation, including quantisation and perturbation, provides a false sense of security in FL by demonstrating the feasibility of data reconstruction attacks despite these privacy measures. Fu et al. [61] propose VFL, a verifiable FL framework for big data in the IIoT, enhancing privacy through Lagrange interpolation and blinding technology to safeguard gradient privacy. Gade et al. [60] introduce a privacy-preserving distributed learning method using obfuscated stochastic gradients to enhance pri- vacy against honest-but-curious adversaries in an FL setup. Parameter compression reduces detailed information shar- ing in FL, preventing attackers from reconstructing private data from model parameters. Zhu et al. [41] address privacy inference attacks in FL for IoT via parameter compression, preserving privacy and model accuracy. Chen et al. [45] discuss an adaptive federated optimisation algorithm that balances computation, communication, and precision in IoT environments using parameter compression.\nCompressed sensing as encryption uses compressed sens- ing as a dual method for data compression and encryption, safeguarding gradients and labels against inference attacks. Miao et al. [46] design an efficient privacy-preserving FL scheme based on compressed sensing, which serves both as a compression and encryption method. This approach ensures that gradients do not disclose private information, making it suitable for IoT scenarios. Li et al. [47] propose FL algorithms based on compressed sensing, enhancing communication effi- ciency in IoT environments. These algorithms allow for model updates between IoT clients and a central server, improving performance over traditional methods.\n2) Differential Privacy and Noise Injection: These mea- sures use noise to mask data, adhering to differential privacy standards to ensure individual data points remain indiscernible. Differential privacy-injected noise incorporates artificial noise based on differential privacy to protect local parameters, balancing privacy with model accuracy. Shen et al. [48] have developed a performance-enhanced DP-based FL algorithm for IoT, introducing a classifier-perturbation regularisation method to improve the robustness of the trained model against DP- injected noise. Cui et al. [49] have designed an improved differentially private FL system for anomaly detection in IoT infrastructures, optimising data utility throughout the training process. Ruzafa-Alcazar et al. [32] provide a comprehensive evaluation of differential privacy techniques in the training of an FL-enabled intrusion detection system for IIoT. Yin et al. [50] propose a new hybrid privacy-preserving method for federal learning that employs sparse differential gradient to improve transmission efficiency in social IoT scenarios. He et al. [51] introduce adaptive local differential privacy mecha- nisms in FL for heterogeneous IoT data, focusing on balancing the trade-off between privacy and utility.While differential privacy techniques have shown promising results in controlled environments, their practical application in real-world IoT scenarios often faces challenges such as maintaining utility while ensuring privacy. Recent studies [70] and [49] have highlighted the need for adaptive mechanisms that balance this trade-off effectively.\nDecentralised perturbation techniques distribute the task of injecting noise across federated nodes to protect privacy, enhancing the scalability and robustness of privacy mea- sures [52]. Mothukuri et al. [53] propose an FL-based anomaly detection for IoT security, utilising decentralised data pro- cessing to enhance privacy and model accuracy. Mantey et al. [54] introduce a Secure Recommendation and Training Technique (SERTT) that leverages both FL and blockchain for privacy-preserved data management in the Internet of Medical Things (IoMT). Alotaibi [55] proposes a biserial correlative Miyaguchi-Preneel blockchain-based Ruzicka-indexed deep multi-layer perceptive learning (BCMPB-RIDMPL) method for improving malware detection in IoMT. Alamleh et al. [56] have developed a standardisation and bench-marking frame- work for machine-learning based intrusion detection systems using FL in IoMT environments.\n3) Secure Multi-party Computation and Anonymisation: These measures focus on collaborative techniques that en- able secure and private computations among multiple parties without revealing individual data inputs. Secure multiparty computing employs secure multiparty computing to enable private information exchange between FL participants, en- hancing data privacy through complex protocols [57]. Liu et al. [58] propose a privacy-preserving FL scheme for Internet of Medical Things, which includes secure authentication and aggregation to protect data during model training. Lu et al. [59] have designed a blockchain and FL-based architecture for secure data sharing in IIoT, maintaining data privacy by sharing the data model instead of the actual data. Zhou et al. [24] present an FL scheme in fog computing that enhances privacy and efficiency by integrating secure multi-party com- puting techniques. Anonymisation and Siamese networks use anonymisation strategies along with advanced network architectures to protect client identity and data during the training process, making re-identification challenging. Song et al. [62] propose a framework incorporating GANs with a multi-task discriminator to analyse user-level privacy leakage in FL, developing a siamese network to re-identify anonymised updates and measuring the similarity of representatives effec- tively in IoT scenarios."}, {"title": "V. DISCUSSION", "content": "This comprehensive review systematically explores the landscape of privacy threats in FL within IoT environments and evaluates the effectiveness of various defensive measures. We identify common threats such as inference and poisoning attacks and discuss lesser-covered threats such as Sybil and backdoor attacks in the context of IoT devices. The defen- sive measure of Differential Privacy is prominently featured, highlighting its critical role across various phases of the FL process. Additionally, we extend the current understanding by contrasting our findings with previous reviews which often fo- cus on a narrower range of threats or do not address the unique challenges posed by the IoT environment. Notable papers by Mothukuri et al. [13] and Ferrag et al. [16] primarily highlight security aspects without delving into the nuanced impacts of these environments on privacy and security strategies.\nIn the reviewed papers, inference attacks are extensively studied, while gradient leakage and reconstruction are notably less addressed, indicating significant research gaps within federated learning in IoT . Replay, evasion, and model stealing attacks also emerge as critical yet under-researched threats. The lack of focus on these vulnerabilities is concerning due to their potential to disrupt federated models' integrity and effectiveness. We emphasise the need for IoT system designers to incorporate robust defences early in the design phase. Defensive strategies such as differential privacy and secure multi-party computation, though promising, must be tailored to IoT constraints such as limited computational power and energy resources. Addressing these gaps is crucial for safeguarding systems against sophisticated cyber threats, ensuring reliability and trustworthiness in applications such as autonomous driving and medical diagnostics. In practical applications, secure multiparty computation in IoT has shown varying success. For instance, Liu et al. [58] demonstrate its feasibility in medical IoT systems but noted significant computational overhead. Similarly, Lu et al. [59] highlight integrating blockchain with FL to enhance data integrity and privacy in IIoT, though it requires substantial computational resources that may not be available in all IoT settings."}, {"title": "B. Advances and Innovations", "content": "There is a critical need for developing lightweight privacy- preserving algorithms optimised for the IoT contexts. Our findings suggest that while differential privacy offers a bal- anced approach to privacy and efficiency, secure multi-party computation and other high-cost measures may require signif- icant optimisation to be feasible in IoT contexts. Furthermore, emerging technologies like blockchain could offer scalable solutions but need thorough evaluation in real-world IoT settings to determine their operational viability. Additionally, empirical studies assessing the real-world applicability and resilience of proposed defensive mechanisms under varied IoT conditions and attack scenarios would greatly benefit the field. The expansion of IoT devices in sensitive areas (such as healthcare and smart cities) underscores the urgency of addressing privacy in FL. As IoT devices become more pervasive, ensuring the privacy and security of FL systems will be crucial in maintaining user trust and regulatory compliance."}, {"title": "C. Limitations", "content": "There are several limitations to our research, starting with the exclusion of gray literature and non-English publications, which might contain relevant data and insights. Additionally, the rapid evolution of both threats and technologies in this domain means that our findings might require continuous updates to remain relevant."}, {"title": "VI. CONCLUSION", "content": "In conclusion, this systematic review critically assesses privacy threats and defensive measures in Federated Learning (FL) within IoT environments. Analysing literature from 2017 to April 2024, we identified persistent challenges such as infer- ence and poisoning attacks that compromise FL model robust- ness. The review highlights the need for innovative defensive strategies tailored to IoT constraints, balancing computational efficiency with privacy safeguards. Our findings emphasise integrating advanced measures such as Differential Privacy and Secure Multi-Party Computation to mitigate privacy risks. However, under-explored threats \u2013 such as replay, evasion, and model stealing attacks pose significant risks, necessitating further research. Practical implementation of defensive mea- sures in IoT settings reveals some potential but also exposes gaps requiring further research. Effective deployment demands addressing computational constraints and ensuring robust per- formance under variable network conditions, as shown in recent studies [58], [70]. Future research should prioritise de- veloping lightweight, optimised privacy-preserving algorithms and explore emerging technologies such as blockchain to enhance FL privacy. Additionally, developing FL models that operate under variable network conditions while maintaining edge device privacy is crucial. Further exploration into FL adaptations for edge computing to reduce latency and improve response times in privacy-critical applications is also essential."}]}