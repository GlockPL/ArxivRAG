{"title": "REASONING WITH LATENT THOUGHTS: ON THE POWER OF LOOPED TRANSFORMERS", "authors": ["Nikunj Saunshi", "Nishanth Dikkala", "Zhiyuan Li", "Sanjiv Kumar", "Sashank J. Reddi"], "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the pri-mary driver. In this work, we make a stronger claim  many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many syn-thetic reasoning problems like addition, p-hop induction, and math problems, a k-layer transformer looped L times nearly matches the performance of a kL-layer non-looped model, and is significantly better than a k-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling  on many downstream rea-soning tasks, a language model with k-layers looped L times can be competitive to, if not better than, a kL-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scal-ing of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate T steps of CoT with T loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.", "sections": [{"title": "INTRODUCTION", "content": "Language models have shown a lot of promise in solving problems that require strong reasoning abil-ities like math, coding, common sense reasoning and logical puzzles (Brown et al., 2020; Team et al., 2023). This has sparked interest in developing techniques to improve reasoning on harder problems (Wei et al., 2022b) and has inspired theoretical studies on how Transformers are able to perform reasoning (Feng et al., 2024; Sanford et al., 2024a). Reasoning abilities are often emergent in larger language models (Wei et al., 2022a)  this aligns with various scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Allen-Zhu & Li, 2024) that show that the performance of language models is very strongly dependent on the model size (i.e., number of parameters) and much lesser on other archi-tectural design choices. However, recent works have started to question this view. Ye et al. (2024) argue that scaling laws for reasoning are more subtle, and depth is very important in addition to parameter count  at the same parameter count, deeper but shallower models are better. This is a de-viation from the conventional scaling law wisdom, but it intuitively makes sense because reasoning problems often requires multi-step compositional thinking, and thus, depth can play a crucial role.\nIn this work, we make a stronger claim  while depth is important, many reasoning problems do not necessarily require a lot of parameters. How does one solve reasoning problems with large depth but few parameters? We argue that looped models are perfectly suited for this, where the same"}, {"title": "Claim 1:", "content": "Many reasoning problems require depth but not necessarily parameters. That is, they can be solved via looped models"}, {"title": "Claim 2:", "content": "For language modeling, looped models have an inductive bias towards good reasoning despite having worse perplexity and memorization to an iso-flop non-looped model"}, {"title": "Claim 3:", "content": "Looped models generate latent thoughts and can, in theory, simulate CoT reasoning\nNote that CoT reasoning gives the model more time and compute by generating multiple thought tokens before the answer, and it has powered the recent paradigm of inference-time scaling for \"thinking\" models like O1 and DeepSeek's R1 (Guo et al., 2025). We make an observation about CoT reasoning  it is essentially a looped model that generates 1 thought token in each iteration. However, looped models seem to be much more powerful, since they can generate multiple latent thoughts in each iteration. We translate this intuition into a theoretical result about how looped models can simulate CoT reasoning.\nMotivated by these findings, we propose a regularization scheme that aims to tap into the inductive bias of looped models towards reasoning. This leads us to our final claim:"}, {"title": "Claim 4:", "content": "Looping-inspired regularization can leverage this inductive bias towards better reasoning\nWith the backdrop of these claims, we concretely present the contributions of the paper below:\n In this paper we study looped models  multilayer models with weight sharing  and their role in reasoning. In particular, we compare a k-layer model looped L times, denoted by $(k \\otimes L)$, with an iso-param $(k \\otimes 1)$ non-looped model with k layers and an iso-flop $(kL \\otimes 1)$ model with kL layers and L times more parameters.\n We conduct experiments on synthetic reasoning tasks like addition, p-hop induction and GSM-style math word problems in Section 2. For these tasks, we surprisingly find that iso-flop looped models, despite having way fewer parameters, can nearly match or outperform a non-looped model. Supporting these experiments, in Section 5 we present theoretical results for why looped models can solve such problems with almost optimal depth.\n In Section 3, we train looped models on causal language modeling at 1B parameter scale. Here, we show that looped models have an inductive bias towards doing well on reasoning benchmarks, despite having much worse perplexity. This finding is novel, since most prior work on looping focused more on perplexity metrics rather than downstream reasoning tasks. We validate this in-ductive bias by visualizing the perplexity vs downstream performance plots as training process. Additionally, we show that looped models demonstrate good scaling behavior on various bench-marks as the number of loops are increased, akin to CoT reasoning. Finally, we show that looped models, along with a scratchpad, can simulate chain-of-thought reasoning.\n Inspired by this inductive bias, in Section 4, we propose a regularization that encourages layers to be more similar to each other. We find that training with such a regularization inherits the inductive bias of looped models towards reasoning without affecting perplexity."}, {"title": "2 LOOPED MODELS ON SIMPLE REASONING TASKS", "content": "We first explore our hypothesis of looped models helping reasoning tasks on a set of tasks con-structed in a procedural manner. The illustrative reasoning tasks we consider are: n-ary addition, p-hop induction head that tests the model's ability to track back for p steps, and i-GSM which con-sists of synthetically constructed grade-school math problems. While these obviously do not cover the whole spectrum of reasoning problems, they provide useful insights into looped models and provide a basis for the theoretical results in Section 5.\nLooped models. While many variants of looped model have been proposed (Lan et al., 2020; Dehghani et al., 2018; Giannou et al., 2023; Yang et al., 2023; Mohtashami et al., 2023), we use the vanilla version for simplicity of our exploration. For any sequence-to-sequence function $f$, we denote $f^{(L)} = f \\circ f \\cdots \\circ f$ to be the function that is $f$ looped $L$ times. In general, the looping mechanism is independent of architectural choices for $f$. For the rest of the paper, we typically use $f$ to denote a $k$-layer Transformer backbone of a model. Thus, $f$ looped $L$ times is the same as a $kL$ layer model with weight sharing between all $L$ blocks of $k$ consecutive layers. We denote such looped models with the notation $(k \\otimes L)$. Please refer to Figure 1 for a succinct illustration of the looping mechanism. Section 5.1 provides a more formal definition of looped transformers that is used for theoretical analysis."}, {"title": "2.1 EXPERIMENTS WITH SIMPLE REASONING PROBLEMS", "content": "n-ary addition. We consider the problem of adding n numbers with 3 digits each. Addition is pop-ular in the literature to study aspects of reasoning with Transformers, such as use of scratchpad (Nye et al., 2021), chain of thought reasoning (Lee et al., 2024; Li et al., 2024) and length generalization (Cho et al., 2024). One reason for its popularity is that addition can have algorithmic solutions, which is a feature of many reasoning problems. For our experiments, we train on a uniform mixture on numbers of operands $n \\in \\{2,4,8, 16, 32\\}$ and sample each 3-digit operand uniformly at random between [0, 999]. We train all models directly on the input-output pair, without any chain-of-thought steps. Following is an example for $n = 4$:\nInput: \"315 + 120+045 + 824 =\" ; Output = \"1304\".\nWe train a standard Transformer-based baseline $(12 \\otimes 1)$ model with 12 layers. Please refer to Appendix A.1 for details on the training setup. We also train $(k \\otimes 12/k)$ looped model and an iso-param $(k \\otimes 1)$ baseline models for comparison, and vary $k \\in \\{2,3,4,6\\}$. All trained models are finally evaluated separately on each of $n \\in \\{8, 16, 24, 32\\}$ to measure accuracy on increasingly difficult problems. Results are presented in Table 1.\nWe find that, while the shallower baselines $(k\\otimes1)$ degrade with lower k, the looped model $(k\\otimes12/k)$ performs very well, and nearly matches the iso-flop $(12\\otimes1)$ baseline. In fact, even a 1-layer network looped 12 times is able to solve this, despite using merely 1/12th of the parameters of the baseline. This suggests the addition problem primarily requires depth, but not necessarily more parameters.\np-hop induction. The p-hop problem is a synthetic induction task studied in Sanford et al. (2024b), who were inspired by the analysis of induction heads from Elhage et al. (2021). Specif-ically, given a sequence of letters $v = (v_1 ...v_n)$ from an alphabet $\\Sigma$, an induction head tries to find the penultimate occurrence of $u_n$ (from left to right) in the sequence and output the character immediately succeeding it. The p-hop problem generalizes this idea to sequentially hop p times. Intuitively, the p-hop problem tests a model's ability to recursively backtrack and retrieve the answer for a given query. This is reminiscent of the reasoning abilities required to solve reading compre-hension kind of problems. We present the formal definition of the p-hop problem in Definition A.1. We perform experiments with looped and non-looped models on the p-hop problem, with alphabet size set to 4 and sequences of length 256, and vary p between 16 and 32 to control the problem difficulty. Our observations are presented in Table 1. Similarly to our findings on the addition task, reasonably deep looped models perform as well as the baseline using much fewer parameters."}, {"title": "3 LANGUAGE MODELING WITH LOOPED MODELS", "content": "In this section, we pretrain and evaluate looped models for causal language models. We train models on 250B tokens of the Pile dataset (Gao et al., 2020) and use a 24-layer 1B parameter model for most experiments, motivated by the setting in Tay et al. (2022) (refer to Appendix A.2 for more details)."}, {"title": "3.1 EXPERIMENTS WITH 1B LANGUAGE MODELING", "content": "For causal language modeling, we pretrain various looped models on the standard GPT2-style next token prediction objective (Radford et al., 2019). We train models with different parameter budgets to make sure that the findings are robust. We remind the reader that the notation $(k \\otimes L)$ corresponds to a k layer model looped L times. For each setting, we compare 3 models: (a) $(24 \\otimes 1)$: 24-layer 1B model, (b) $(k \\otimes 1)$: k-layer model with the same configuration as the 24-layer model for other dimensions, (c) $(k \\otimes 24/k)$: k-layer model looped 24/k times to match the parameter count of (b) and match the effective depth/FLOPs of (a). We run experiments for $k \\in \\{4,6,8,12\\}$ to ensure that the findings are robust. After pretraining on Pile, we evaluate the models on validation perplexity and on downstream benchmarks using k-shot evaluations. Results are summarized in Table 3\nEvaluation metrics. We evaluate the models on perplexity metric after training is completed. Since there is growing evidence that perplexity, although very useful for training, is a narrow measure of model quality, we also track more holistic downstream evaluations (Liang et al., 2023). Thus, we evaluate the model on 4 important slices: closed book QA, open book QA, math word problems and reasoning primitives. These comprise of 19 different tasks in total."}, {"title": "3.2 INDUCTIVE BIAS TOWARDS REASONING", "content": "In this section, we formalize the inductive bias by plotting the perplexity vs downstream metric iso-plots, as introduced in Saunshi et al. (2022). Section 3.1 showed that looped models have higher than expected performance on reasoning problems. However, since looped models are worse on perplexity, it is hard to make a direct comparison between various models. One way to bring parity between models is to look at their downstream performances at the same validation pretraining loss (Liu et al., 2023). Saunshi et al. (2024) proposed plotting pretraining loss vs downstream metrics as training proceeds, as a way to study the inductive bias of various methods. For each model, we evaluate the log perplexity and downstream metrics at every 20k steps, starting from 120k steps. We plot these values in a scatter plot and fit a linear function with log perplexity and the corresponding downstream metric being input and output respectively. Please refer to Figures 2 and 7 for two sets of isoplots.\nFindings. For all values of k, we observe the following:\n The isoplots for $(k\\otimes L)$ looped model and $(k \\otimes 1)$ baseline are very aligned for closed book QA tasks (if extrapolated). This suggests that log perplexity is a very strong indicator of downstream performance on memorization based tasks.\n For open book QA and math word problems, the isoplot line for the looped model is always higher than the baseline model. This suggests that at the same log perplexity, looped models will tend to have higher evaluation on these tasks that require more reasoning.\n For reasoning primitives, there is a stark difference between looped and baseline models. The looped model seems to have good performance at most points in training.\nOverall this suggests a strong inductive bias of looped models towards improving reasoning."}, {"title": "3.3 MIDDLE LOOPING VARIANT AND RELATIONSHIP WITH GRADUAL STACKING", "content": "Recently Saunshi et al. (2024) introduced a gradual stacking (Gong et al., 2019; Reddi et al., 2023) approach for training language models called MidAS. This approach gradually grows the model depth as training proceeds by duplicating certain layers of the model in each stacking operation. Surprisingly, they found that MidAS not only speeds up pretraining, but also improves reasoning in the same sense as Figure 2  better reasoning at the same perplexity. Furthermore, the paper established a strong connection between stacking via MidAS and looped models, owing to the layer duplication operation, and conjectured that this is the reason for such an inductive bias. Our results from the previous section provides a compelling evidence for this conjecture by showing that looped models also show a very similar inductive bias, thus, further strengthening the connection between stacking and looped models. Why such an inductive bias occurs is still an open question, and we believe that understanding this is an important future direction.\nFurthermore, inspired by their findings, we explore middle looping (see Figure 1 for an illustration) a variant of looping which maintains independent layers at the start and the end of the network, and perform looping on the middle block of layers. The high-level intuition from Saunshi et al. (2024) is that the first and last layers play a special role in the model and thus, should be treated differently from the middle layers. In Table 3, we report results for a version of middle looping that is iso-param with a $(12 \\otimes 1)$ baseline and iso-flop with a $(24 \\otimes 1)$ baseline, just like the $(12 \\otimes 2)$ model. Overall, we find that middle looping has better perplexity and more uniform improvements than the default looping of $(12 \\otimes 2)$ (except for math word problems), and thus, might be a more practical looping approach. We leave the exploration of the best looping strategies for future work."}, {"title": "3.4 SCALING BEHAVIOR OF LOOPING", "content": "In this section, we discuss an intriguing scaling behavior of looping, specifically the impact of the number of loops on various evaluation metrics. In particular, we are interested in scaling behavior of: (a) accuracy as a function of number of loops and (b) comparison of looped and non-looped baseline of the same effective depth. To this end, we pretrain various looped language models of the form $(4 \\otimes L)$, i.e., 4-layer model looped L times, for $L \\in \\{1,2, 3, 6, 9, 12\\}$. To enable iso-FLOPs comparison, we also train baseline models of the form $(4L\\otimes 1)$, which has L times more parameters. For each task group, we plot the average accuracy as a function of the effective depth, i.e. $D = 4L$. From the results presented in Figure 3, we observe the following.\n1. In both cases, we find that the accuracies for all task groups continue to increase with more loops/depth, although, unsurprisingly, the returns are diminishing with depth for both looped and non-looped models. Interestingly, for both looped and non-looped models, we found that one can fit a simple scaling law of the following form:\n$Acc \\propto a \\log(D) + \\beta$,\nwhere $D$ is the effective depth and $a$ measures the impact of depth on downstream performance.\n2. Furthermore, for each task group, we compute $A_{loop}/A_{base}$ to assess the relative impact of \u201cdepth via looping\" compared to \"depth via additional parameters\". We find that more loops continue to help, and the relative benefit of loops is higher for reasoning tasks like open book QA and math problems. Remarkably, the impact of loops is even higher (1.19x) than impact of depth for reasoning primitives, which further consolidates the benefit of looped models for reasoning.\nLatent thoughts and connections to CoT reasoning. We end this discussion with an important question: why should we expect this interesting scaling behavior of looped models? We argue that looped models have a strong connection to chain-of-thought (CoT) reasoning. Such a connection is insightful because recent works on thinking have shown CoT can demonstrate inference-time scaling behavior, where the accuracy on reasoning datasets continues to improve as the length of the models chain of thought increases; i.e., generating more thoughts leads to better reasoning.\nTo establish this connection, we make a simple observation about CoT reasoning  it is essentially a looped model that generates a single thought token in each iteration. However, looped models can be more powerful since they can generate multiple \"latent thoughts\" in each iteration. This can be visualized in Figure 4. We further translate this intuition into a theoretical result (see Section 5.4)"}, {"title": "4 LOOPING-INSPIRED REGULARIZATION", "content": "In the previous section, we observed the looped models can improve reasoning with worse perplex-ity. Can we leverage this observation to improve reasoning without affecting perplexity? Here, we propose a simple approach: regularize the weights of the model to encourage them to be close to a looped model. This could have two advantages, (a) the model still has free parameters to improve perplexity, (b) the closeness to looped model can inherit the desirable inductive bias. In particular, if an L-layer model is denoted as $f_0 \\circ f_1 \\cdots \\circ f_{L/k-1}$, where each $f_i$ is a block of $k$ layers, we add a regularization term that makes all the weights of the block $f_i$ close to $f_{i+1}$ in terms of cosine similarity. For a parameter group $G$ (e.g. first feed-forward layer, or query matrix in Transformer), we use $\\Theta_G^{(0)}, \\Theta_G^{(1)},..., \\Theta_G^{(k-1)}$ to denotes the weights in all layers. Then the regularization term is\n$R_{G}(k)=\\frac{1}{L^{-2k-1}}\\sum_{i=0}^{\\frac{L}{k}-1}\\sum_{j=0}^{k-1}Cosine(\\Theta_G^{(ik+j)}, \\Theta_G^{((i+1)k+j)})$\n(3)\nThe final loss function is a sum of the standard cross-entropy loss and the regularization term aver-aged over all groups, multiplied by a scalar hyperparameter. Let $G$ denote the set of all parameter groups; $G= \\{Attn-Q, Attn-K, . . ., FFN-W2\\}$\n$L = L_{Xent} + \\lambda_{reg} |G|^{-1}\\sum_{G \\in G}R_{G}(k)$\n(4)\nIn the above formulation, $\\lambda_{reg}=0$ would recover standard training and $\\lambda_{reg} \\to \\infty$ would converge to a fully looped model. Intermediate values of $\\lambda_{reg}$ will lead to \u201capproximately\u201d looped models. For instance, to emulate the $(4 \\otimes 6)$ looped model setting, we use pick $k = 4, L = 24$ and a large regularization strength like $\\lambda_{reg} = 10$. All other hyperparameters are kept the same as baseline training for a fair comparison. We tried options other than cosine similarity, like $l2$ norm, to bring the weights closer but found that cosine was more robust and effective.\nCosine similarities. Firstly, we check if the regularization had the right effect by measuring the cosine similarities between the successive blocks of $k$ layers at the end of training. We, indeed, find that for all parameter groups, the cosine similarity around 0.98 or higher (see Figure 5).\nInductive bias. To confirm the inductive bias of the regularizer, we visualize the log perplexity vs downstream isoplots for $\\lambda_{reg} = 10$ and baseline models in Figure 2. While the plots are similar for closed book QA, a strong inductive bias shows up for open book QA and reasoning problems. Crucially, the regularized model does well on reasoning without hurting perplexity (see Table 4)."}, {"title": "5 THEORETICAL ANALYSIS FOR LOOPED MODELS", "content": "In this section, we present theoretical results to understand the phenomenon from the previous sections  why can looped model with few parameters match an iso-flops non-looped baseline on reasoning problems? While a complete theory is challenging, since \u201creasoning\" is a very broad concept, the goal is to provide some intuition and formalization for the expressive power of looped models. First, we show that looped Transformers can effectively solve group composition (a generalization of the addition problem). Then we show a very general result on how a non-looped model with very few distinct layers can be simulated by a looped model with a small blowup in model size. This result is then used to solve the p-hop problem using a one-layer looped transformer. Our construction for group composition and p-hop problem are nearly optimal in terms of depth and much more efficient in terms of parameters compared to non-looped models."}, {"title": "5.1 PRELIMINARIES AND NOTATIONS", "content": "We first define the standard transformer architecture. Throughout the paper we will fix the dimension of the embedding to be $d \\in \\mathbb{N+}$, the vocabulary to be $V$ and maximum sequence length to be $n_{max}$. We will use id to denote the identity mapping. Here, we describe the high-level notation. Please refer to Appendix B.1 for detailed notations. We use FF and MHA to denote the feed-forward and attention layers respectively, and $\\theta_{MHA}$ and $\\theta_{FF}$ denote the parameters in these layers.\nDefinition 5.1 (Transformer Block). Given number of layers $L \\in \\mathbb{N+}$ and parameter $\\Theta_{TB} = {(\\theta_{FF}^{(l)}, \\theta_{MHA}^{(l)})_{l=0}^{L-1}}$, L-layer transformer block $TB_{\\Theta_{TB}}: (\\mathbb{R}^d)^{n} \\rightarrow (\\mathbb{R}^d)^{n}$ for any $n \\in \\mathbb{N+}$ is defined as\n$TB_{\\Theta_{TB}} (\\cdot) \\triangleq (id + FF_{\\theta_{FF}^{(L-1)}}) \\circ (id + MHA_{\\theta_{MHA}^{(L-1)}}) \\circ \\cdots \\circ (id + FF_{\\theta_{FF}^{(0)}}) \\circ (id + MHA_{\\theta_{MHA}^{(0)}}),\n(5)\nWe also denote EMBED and OUTPUT to be the input embedding and output softmax layers respec-tively. Please refer to Appendix B.1 for precise definitions. Finally, we define the entire transformer model that maps a sequence of tokens to a distribution over tokens: $p_{\\theta}: \\bigcup_{n \\leq n_{max}} V^{n} \\rightarrow \\Delta^{|V|-1}$.\n$p_{\\theta} \\triangleq OUTPUT_{\\theta_{OUTPUT}} \\circ TB_{\\Theta_{TB}} \\circ EMBED_{\\theta_{\\varepsilon}, \\theta_{P_{\\varepsilon}}}$,\n(6)\nwhere $\\theta = (\\Theta_{TB}, \\theta_{\\varepsilon}, \\theta_{P_{\\varepsilon}}, \\theta_{OUTPUT})$ denote all the transformer parameter. In particular, we use $TF_{\\theta}(v_1,..., v_n) \\equiv arg \\max_{v \\in V} p_{\\theta}(v | v_1, ..., v_n)$ to denote the deterministic version of the trans-former model. We now define a looped Transformer model that also subsumes a non-looped model.\nDefinition 5.2 (($L \\otimes T$) Looped Transformer). Given the number of loops $T \\in \\mathbb{N+}$, parameters $\\theta = (\\Theta_{TB}, \\theta_{\\varepsilon}, \\theta_{P_{\\varepsilon}}, \\theta_{OUTPUT})$, where $\\Theta_{TB} = {(\\theta_{FF}^{(l)}, \\theta_{MHA}^{(l)})_{l=0}^{L-1}}$, we define a ($L \\otimes T$) looped Transformer as $p_{\\theta, T} \\triangleq OUTPUT_{\\theta_{OUTPUT}} \\circ (TB_{\\Theta_{TB}})^T \\circ EMBED_{\\theta_{\\varepsilon}, \\theta_{P_{\\varepsilon}}}$."}, {"title": "5.2 GROUP COMPOSITION PROBLEM", "content": "We consider the problem of composing $n$ elements from a group, and prove that a standard 1-layer transformer looped $O(\\log(n))$ times can solve this problem. This is a generalization of the modular addition problem and has a long history (see Appendix B.2.2). Recently, Liu et al. (2022) show that transformers with $\\log_2 n$ depth can compute composition over $n$ group elements, regardless of whether the group is solvable or not. However, the construction in Liu et al. (2022) uses different attention parameter for each layer. Here, we provide a more parameter efficient construction where we solve this problem by looping a one-layer transformer $\\log(n)$ times.\nTheorem 5.1. For any finite group $G$ and every $n \\in \\mathbb{N+}$, there exists a constant-precision looped transformer $TF_{\\theta, T}$ computing the composition of $n$ elements from $G$ with a 1-layer transformer block, $T = [\\log_2 n]$ loops, $G \\cup \\{#\\}$ being the vocabulary, $d = 3 ([\\log_2 |G|] + [\\log_2 n + 1])$ embedding size, $d_{FF} = |G|^2 + 6[\\log_2 |G|]$ hidden dimension in MLP, $d_{attn} = [\\log_2 n]$ hid-den attention dimension, and 2 attention heads. More specifically, for any $g_1,...,g_n \\in G$, $TF_{\\theta}(#,g_1,..., g_n) = g_1 \\circ \\cdots g_n$.\nThe above result matches the depth upper bound shown for non-looped models by Liu et al. (2022), and is very close to the super constant lower bound shown there. Thus looped models can solve the problem with best known depth."}, {"title": "5.3 LOOPED MODELS CAN SIMULATE NON-LOOPED MODELS", "content": "Our second theoretical result shows that a non-looped transformer with repeated layers can be sim-ulated by a looped transformer with fewer parameters and same depth.\nTheorem 5.2. For any transformer $p_{\\theta}$ with $L$ layers, $d$ embedding size, $d_{FF}$ hidden dimension for MLP, $H$ attention heads with $d_{attn}$ hidden dimension, at most $R$ distinct transformer layers and bounded activation value, there is a looped transformer $p_{\\theta',L}$ working on the same vocabulary $V$ plus a dummy token $\\#$, which loops a 1-layer transformer block for $L$ times, with $d + R + 2$ embedding size, $Rd_{FF} + O(L)$ hidden dimension for MLP, $RH$ attention heads with $d_{attn}$ hidden dimension, such that for any string $v_1, ..., v_n \\in V$, $p_{\\theta}(v_1, ..., v_n) = p_{\\theta',L}(\\#, v_1, ..., v_n)$.\nWe can also use the above theorem to convert the $O(\\log_2 n)$ depth transformer that simulates group composition into a 1-layer looped transformer, although it is less parameter efficient than the result from the previous section. Please refer to Appendix B.2.1 for the full proof.\nTheory for p-hop. The experiments on p-hop induction from Section 2.1 surprisingly show that a small model looped multiple times can solve it very effectively. We establish a theoretical basis for this finding. More specifically, we show that a constant layer transformer with $\\log(p)$ loops suffices to solve p-hop induction problem. This result, in fact, matches the lower bound for layers required for non-looped models proved in Sanford et al. (2024b). The result follows from Theorem 5.2 and the construction for non-looped models from Sanford et al. (2024b) (restated in Theorem B.12).\nCorollary 5.3. p-hop problem (Definition A.1) can be solved by looping a 1-layer transformer $[\\log_2 p] + 2$ times, which has $O(\\log n)$ bits of precision, $d = d_{FF} = d_{attn} = O(1)$ embedding size, hidden dimension for MLP and attention, and at most 3 attention heads."}, {"title": "5.4 LOOPED MODELS CAN SIMULATE COT REASONING", "content": "In Section 3.4, we discussed how looped models can be viewed as generating multiple latent thoughts in each iteration. The following theorem shows that one can use looped transformer with L loops to simulate L steps CoT of another transformer with similar sizes.\nTheorem 5.4 (Looped transformer simulates CoT). For any L-layer non-looped transformer $TF_{\\theta}$ with fixed input length $n$ and the number of CoT steps $m$, there exists a looped transformer $TF_{\\theta'}$ with $L + O(1)$ layers, $\\Omega(\\log(n + m))$, more embedding dimension and constantly many more attention heads, such that for any input $v = (v_i)_{i=1}^{n}$, the output of non-looped transformer after $m$ steps of CoT, $TF_{\\theta}^{m}(v)$, is the same as that of the looped transformer on input $x$ concatenated by $m$ dummy tokens with $m$ loops, $TF_{\\theta',m}(v,\\#^m)$.\nBelow we sketch the high-level idea behind Theorem 5.4; the full proof can be found in Ap-pendix B.3.\n (Masking all-but-one) We maintain a counter $t$ (Lemma B.10) at each position for the current number of loop and for the $i$th position, we will only \"update\" the embedding if $i-n > t$ and reset the embedding to the input to the looped transformer layer otherwise. This can be implemented by MLP. So similar to CoT, the first $n + i - 1$ embedding won't be changed in the $i$th loop.\n (Shift by one) We can use attention to obtain the output of the current loop at the previous position and use that as the input of the next loop at current position. (Lemma B.9)\n (Token decoding) We show that we can use MLP with ReLU activation to simulate the encoding and decoding process of CoT. (Lemma B.7)"}, {"title": "6 RELATED WORK", "content": "Reasoning is recognized as a core ability for intelligent and robustly model and has thus seen significant focus over the last few years. The synthetic reasoning problems we consider in this work have all been used in the prior works of Sanford et al. (2024b); Ye et al. (2024); Sanford et al. (2024a); Nogueira et al. (2021) to theoretically analyze the strengths and limitations of Transformers. There is also interest in the representation power of Transformers for computational"}, {"title": "7 CONCLUSIONS, LIMITATIONS AND FUTURE WORK", "content": "This work explores a new direction of \"looped models for reasoning\u201d. Not only are looped models able to solve many reasoning problems with very fewer parameters, they also have an inductive bias towards disproportionately improving the reasoning performance of language models, over memorization. The theoretical results on the expressivity of looped models start to provide some hints into their depth-optimality. While we test looped models on a subset of reasoning problems, a natural question is whether the results hold for many other forms of reasoning (e.g. multimodal and common-sense reasoning). In particular, a succinct formalization of reasoning problems itself is an interesting future direction. Furthermore, the inductive bias towards improved reasoning performance at the same perplexity is very intriguing and deserves further exploration. We find the scaling behavior of looped models very fascinating, and the connections to latent thoughts and CoT"}, {"title": "A EXPERIMENTS", "content": "A.1 SIMPLE REASONING SETUP DETAILS\nn-ary addition. All experiments"}]}