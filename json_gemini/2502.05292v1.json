{"title": "Drone Detection and Tracking with YOLO and a Rule-based Method", "authors": ["Purbaditya Bhattacharya", "Patrick Nowak"], "abstract": "Drones or unmanned aerial vehicles are traditionally used for military missions, warfare, and espionage. However, the usage of drones has significantly increased due to multiple industrial applications involving security and inspection, transportation, research purposes, and recreational drone flying. Such an increased volume of drone activity in public spaces requires regulatory actions for purposes of privacy protection and safety. Hence, detection of illegal drone activities such as boundary encroachment becomes a necessity. Such detection tasks are usually automated and performed by deep learning models which are trained on annotated image datasets. This paper builds on a previous work and extends an already published open source dataset. A description and analysis of the entire dataset is provided. The dataset is used to train the YOLOv7 deep learning model and some of its minor variants and the results are provided. Since the detection models are based on a single image input, a simple cross-correlation based tracker is used to reduce detection drops and improve tracking performance in videos. Finally, the entire drone detection system is summarized.", "sections": [{"title": "I. INTRODUCTION", "content": "Drones or unmanned aerial vehicles (UAV) are primarily used in military operations, industrial and commercial appli- cations. But recreational drone flying has also increased in urban areas which can be concerning for public safety and privacy. Hence, regulation in terms of legal zones and other requirements are usually put in place. At the same time, it is necessary to monitor if any violation of such regulations have taken place. An automated drone detection system can therefore generate an alert based on any illegal activity and enable a capturing system to act. Such detection systems can operate based on data processed by different sensors. In this context, infrared and visual cameras are used as two of many sensors which might provide better visibility under different environmental conditions.\nA part of this work builds on our previous work [1] on a published dataset of infrared images. The previous dataset is a collection of annotated infrared images captured by a FLIR and an InfraTec Camera at two distinct locations - the university campus and one of the city harbours. In this work, a set of annotated color images, also captured in the same locations, are added to the existing dataset. Additionally, more videos are recorded at the harbour location with additional backgrounds and the corresponding frames are annotated and added to the dataset.\nWith the incremental improvement in deep learning based methods, they are primarily used for multiple object detection tasks [2]-[5]. More commercial multi-sensor or vision systems now use a trained and properly scaled deep learning model for improved object detection. YOLO is one of the frontrunners in object detection and is widely used for these tasks. Hence, such models are used for drone detection [6], [7]. The larger drone dataset is created with the objective of developing and training the YOLO convolutional neural network (CNN) models and integrate them in a drone detection system. The base model and models modified by the addition of smaller modules are trained and compared.\nThe following sections provide an overview of the drone detection system followed by an analysis of the datasets. A brief description of the modified YOLO models are provided in the next section along with an evaluation of their perfor- mance. Finally, a simple ensemble method for improving the detection performance and tracking is introduced followed by a conclusion."}, {"title": "II. SYSTEM OVERVIEW", "content": "In this section, the system overview as illustrated in Fig. 1 is described. The camera placed in the campus area covered by 5G outdoor network records a flying drone and the pro- cessed video stream is transmitted to the indoor network. The transmission is then received by the computer which runs the drone detection algorithm. After detection, the computer can display the results or transmit it further to another machine. In order to send or recieve videos, the GStreamer multimedia application framework [8] is used. The constructed GStreamer pipelines primarily include data transmission and reception via UDP, processing and conversion, encoding and decoding, and buffering. The received stream is processed by a customized pre-trained YOLOv7 model in a Docker [9] container which has Python OpenCV built with GStreamer support. Finally, the result is displayed."}, {"title": "III. DATASET", "content": "To create the dataset, videos of flying drones are recorded with three different cameras, followed by the extraction of frames and their annotation by various methods. The following subsections provide a brief description of the process."}, {"title": "A. Video Recording", "content": "The original dataset is constructed from drone videos cap- tured at the university football field and the Wilhelmsburg harbour using a FLIR Scion OTM366 infrared camera and an InfraTec VarioCAM HD Z equipped with a zoom lens (25-150 mm) with image resolutions of 640 \u00d7 480 pixels and 1024 x 768 pixels, respectively. The color videos are recorded at the aforementioned locations with a Sony a6000 camera with an image resolution of 1920 x 1080 pixels. The initial videos are recorded independently with each cameras from multiple perspectives with some videos being recorded on different days.\nFinally, all the cameras are mounted on a steerable tripod and additional videos are recorded at the Harbour. The new videos have more background information compared to the initial recordings and all cameras have very similar perspec- tives while recording the drone. Figure 2 shows some example images captured by the cameras. The camera setup on the tripod is shown in Fig. 3. Example images from each location are also shown in Fig. 4"}, {"title": "B. Dataset Labelling", "content": "The LabelImg [10] graphical annotation tool is used to man- ually annotate the drones in the color images extracted from the video recordings. The annotations are done in the Pascal VOC [11] format. As described in [1], an automated annotation script based on a simple bounding box tracking method is used to accelerate the process followed by intermediate manual correction of any wrong detections. Drones that are difficult to spot are either not annotated or labelled as a difficult example. After the manual annotation of the initial infrared and color datasets the base YOLO model is used to train them.\nThis trained model is used to detect drones in the second dateset constructed with the FLIR infrared and color images. Based on the detection results, initial annotations are generated and incorrect annotations are manually corrected. Since feature matching proved to be very challenging and inconsistent, automatic registration methods yielded poor results. Instead a transformation matrix is manually generated based on multiple trials, followed by a simpler cross-correlation based matching of small local regions around the drone. Image pairs are discarded when a registration yields poor results. Backgrounds in the image pairs are shifted with respect to each other in the multi-channel image but for a single class classification task, the impact of such a problem should be less."}, {"title": "IV. DATASET ANALYSIS", "content": "The previous dataset already consists of 66,438 images and 71,520 annotated drones. In 2,617 images there were no drones. In 10,130 cases, the annotated drone is marked as difficult because it is barely distinguishable from the back- ground or mostly outside the image. The dataset prepared with infrared images is initially divided into normal and difficult images depending on the visibility of drones due to clutter, occlusion, or heavy blurring. In this work, the relatively easy images are selected where the drones are mostly visible by the naked eye. It is however noteworthy that the set of normal images do contain a substantial number of challenging images where the drones are partially occluded/ blurred or the contrast between the drones and background is not big. Additionally, the total number of final images used during training of a model is further reduced due to some images being blurry or the drones being very tiny.\nThe color image dataset contains a total of 37,508 images with 37,842 annotated drones. Instances where the drone is difficult to perceive are not annotated at all in this case. All of the images are used to train a model.\nFinally, the additional dataset generated from the final recordings at the harbour consists of 40,059 infrared and color images each. However, this dataset is not used for training any model. Rather, an attempt is made to register the infrared and color images based on detections from a pre-trained model, so that the location of drones coincide relatively well and the images can be merged as 4-channel image data. Traditaional image processing and registration algorithms failed to reliably find the correct transformations and projections due to the difference in features between the infrared and color images as well as the quality of the images. The attempt to register the images manually is also made difficult due to imperfect synchronization and small relative movements of cameras on the tripod while following the drones. This has lead to background mismatches in most images. Table I provides an overview of all datasets.\nFigures 5 and 6 show heatmaps of the average positions of annotated drones, used for training, on the individual pixels of the infrared and color images separately. Black pixels in the heatmaps represent positions that are not reached by any drone. As can be seen in Fig. 5, the average position of drones is more prominent around the center of the images compared to the boundaries particularly due to the hand held FLIR camera following the drone comfortably. The occurance of drones near the boundaries can be attributed to the heavy InfraTec camera which is difficult to steer at the required speed. The smallest bounding box has a size of 128 pixels while the largest bounding box has a size of 230886 pixels. The average bounding box size is approximately 7472 pixels.\nThe color images have the drone appearing more around the center of the image compared to the boundaries. Similar to the FLIR camera the Sony a6000 is also easy to maneuver and follow the drone. The smallest bounding box has a size of 44 pixels while the largest bounding box has a size of 368954 pixels. The average bounding box size is approximately 9794 pixels. All of the above sizes are calculated with respect to a reference image size of 1280 \u00d7 1280 pixels which is the default input dimension to the YOLO models used in the later section."}, {"title": "V. DEEP LEARNING BASED DRONE DETECTION", "content": "After an initial application of multiple CNN architectures from the YOLO family [12], [13], [14] and one based on EfficientDet [4], it was concluded that YOLOv7 was the best performing model, particularly in terms of inference speed. The YOLOv7 model is pre-trained on the COCO dataset and the pre-trained weights are used to initialize during training. Similar to previous YOLO models, YOLOv7's architecture can be divided into a backbone, neck, and head structure.\nApart from the basis YOLOv7 architecture, a set of sim- ple modules and a couple of transformer modules are also experimented with, to see if an improvement in detection accuracy is significant enough to compromise with increased computation. The additional modules are illustrated in Fig. 7. The first module is a channel-attention (CAT) module which is used in many computer vision tasks [15], [16] where each input feature map is weighted with a scalar value or an importance score, learned during training. The pixel attention (PxAT) module [15] weighs each pixel of a feature map instead of the entire feature map, which performs better in particular computer vision tasks such as image denoising.\nThe patch attention (PAT) module divides each feature map into patches of a certain size and learns an importance score for a region in each feature map of the layer. Such modules attempt to decorrelate the feature maps to reduce redundancy and improve their relative variation. Transformer modules are particularly popular in natural language processing and large language models [17] due to their efficiency of pattern recognition in sequential data. Their usage in deep learning for computer vision tasks has shown significant improvements in multiple applications [18], [19] as well, because they are usually better at improving the detection of more salient features or redundant structures on a feature map. The classical transformer module with multihead attention structure was initially used for such tasks. These modules with multiple heads are usually quite computationally expensive, particularly in models for vision applications. Hence, the shifted window transformer (SWIN) [20] which is faster than a vanilla trans- former, is experimented with. All the above modules are added in pyramidal backbone at multiple stages where the feature"}, {"title": "A. Rule-based Tracking Method", "content": "Although YOLOv7 performs relatively well in terms of successful detection, there are certain frames in between where the drone detection fails either due to a sudden drop of the confidence score or some degree of occlusion. In order to improve the detection drop due to a sudden change in con- fidence, a simple rule or condition based method is proposed. The method can be described in the following steps:\n\u2022 A high ($conf_h$) and a low confidence threshold ($conf_l$) are selected.\n\u2022 The bounding boxes above $conf_h$ are preserved while only the bounding boxes above $conf_l$ are considered as valid. The method waits for the first valid detection or detections.\n\u2022 The first valid detection or detections are used as refer- ence. These bounding boxes and slightly larger regions around these boxes are selected for use in a cross- correlation function. This frame becomes the previous frame.\n\u2022 The bounding boxes from the new or current frame are initially registered to the previous valid detections and the corresponding ones (bounding box or boxes based on proximity, IOU, and size) are selected.\n\u2022 If new bounding boxes of the current frame are found and their confidence scores are above $conf_h$, no other step is required.\n\u2022 If the confidence scores of the new boxes are lower than $conf_h$ but higher than $conf_l$, then bounding boxes are also predicted. This prediction is done with a cross-correlation method from the corresponding bounding boxes in the previous frame and the region around that boxes cropped from the new frame. Based on the estimated shift, the predicted bounding boxes for the current frame are gen- erated and intersection over union (IOU) values with the new bounding boxes are calculated. If the IOU values are above a certain threshold, the new bounding boxes are considered as valid and their confidence scores are replaced by the previous high confidence score.\n\u2022 If the IOU values calculated in the previous step are lower than a threshold, then the predicted bounding boxes can be considered as valid if the cross-correlation values are higher than a given threshold.\n\u2022 If the above does not happen, the objects are either occluded or not detected. The confidence score of any corresponding bounding box is replaced with 0. The process waits for the next valid detections.\nIn the above method, the different threshold values can be experimented with. The proposed method running on top of YOLOv7 successfully detects more drones compared to the usage of only YOLOv7 and reduces detection drops. The amount of improvement depends on the choice of different thresholds and usually ranges from 2 to 10% more drones detected, depending on the video. Figure 9 shows an example of the method which makes additional detections compared to only YOLOv7. However, there are still many instances where detection is unsuccessful due to very low confidence scores. While the system is dependent on YOLO making an initial detection per frame, any confidence score below the low threshold ($conf_l$) will result in a discontinuation of tracking. The method is based on one prior frame and can be improved by the inclusion of multiple prior frames for a more accurate prediction of the drone's position and movement.\nA simple and fast test is performed where an indoor computer runs a video of a flying drone and the displayed video on a monitor is recorded by a USB camera placed at a certain distance. The recording is simultaneously transmitted from the indoor network to the outdoor network and is received by a laptop connected to the 5G network and running the containerized model. The model detects the drone and displays it on the screen, as shown in Fig. 10. This test is done for multiple times during a day. On the laptop, a 15 fps frame rate is achieved without any visible stutter. An approximate latency of 350 ms is observed which is primarily contributed by the USB camera, the parameter settings in GStreamer pipeline (jitter buffer), and the detection model running on the laptop with a Nvidia RTX3050 processing unit. However, a proper test setup needs to be built with a real use-case scenario and accurate measurement setup."}, {"title": "VI. CONCLUSION", "content": "This work is a continuation of our previous work related to a drone detection system based on YOLOv7. The data required"}]}