{"title": "Enhancing Relation Extraction via Supervised Rationale Verification and Feedback", "authors": ["Yongqi Li", "Xin Miao", "Shen Zhou", "Mayi Xu", "Yuyang Ren", "Tieyun Qian"], "abstract": "Despite the rapid progress that existing automated feedback methods have made in correcting the output of large language models (LLMs), these methods cannot be well applied to the relation extraction (RE) task due to their designated feedback objectives and correction manner. To address this problem, we propose a novel automated feedback framework for RE, which presents a rationale supervisor to verify the rationale and provides re-selected demonstrations as feedback to correct the initial prediction. Specifically, we first design a causal intervention and observation method to collect biased/unbiased rationales for contrastive training the rationale supervisor. Then, we present a verification-feedback-correction procedure to iteratively enhance LLMs' capability of handling the RE task. Extensive experiments prove that our proposed framework significantly outperforms existing methods.", "sections": [{"title": "Introduction", "content": "The relation extraction (RE) task aims to extract the semantic relation between entities in the text, which is an important task in information extraction. Unlike previous fine-tuning strategies based on small language models (Wu and He 2019), recent studies (Wan et al. 2023; Ma et al. 2023) leverage the strong instruction understanding abilities and rich intrinsic knowledge of large language models (LLMs) (Ouyang et al. 2022; Touvron et al. 2023; Bai et al. 2022) to enhance the performance of RE.\nDespite their significant progress, LLM based methods may suffer from relation bias when performing relation extraction. For example, given a sentence \"data is derived from a study\", where \"data\" and \"study\u201d form the \u201cEntity-Origin\u201d relation, LLMs may be influenced by the pre-trained knowledge and have the stereotype that \"data is the product that someone produces\u201d, thus making a biased relation prediction \u201cProduct-Producer\", which ignores that the real producer is investigators (producer of the study). Furthermore, existing LLM based RE methods focus on the pre-selection of in-context demonstrations (Wan et al. 2023; Ma, Li, and Zhang 2023) or instruction design (Zhang, Guti\u00e9rrez, and Su 2023) to improve the performance. The verification and feedback mechanism for correcting the biased prediction is still missing from current LLM based RE research.\nTo fill this gap, in this study, we focus on exploring the verification and feedback mechanism (Pan et al. 2023) of LLMs for RE. Specifically, we aim to examine whether the relation prediction of LLMs is biased by verifying the rationale (the generated explanation when LLMs perform RE) and providing feedback for correction. However, the current verification and feedback mechanism faces the following two problems when being applied to RE.\nFirstly, existing methods are mainly designed for other tasks, e.g., the reasoning task. The objectives of their feedback are also tailored for those tasks, e.g., correcting code, factual, or calculation errors in initial responses (Zhang et al. 2023; Gou et al. 2023), or choosing an optimal prefix for the next step in multi-step reasoning (Khalifa et al. 2023), as shown in Fig. 1 (a). For example, for the mathematical reasoning task, Self-Refine (Madaan et al. 2023) utilizes the LLM agent to find calculation errors in the initial answer and provide error information as feedback to correct the answer. However, such feedback objectives are based on the logical properties of reasoning tasks, which are not available for RE.\nSecondly, existing methods (Madaan et al. 2023; Nathani et al. 2023) do not include demonstrations in their feedback. However, the demonstrations are essential for RE even at the correction stage. This is because without demonstrations in the feedback, the RE task would degrade to zero-shot RE and is harder than the initial few-shot one. Moreover, the demonstrations in initial few-shot RE cannot be directly used in feedback since they will mislead the model back to the initial one, and thus the impact of feedback is discarded.\nTo address the above problems, we propose a novel automated feedback framework for RE, which trains a rationale supervisor based on a BERT-like small model and utilizes it to not only verify the prediction but also provide new demonstration improved feedback for correction during the inference. As shown in Fig. 1 (b), our rationale supervisor provides re-selected demonstrations as feedback for correcting the initial prediction of LLMs.\nIn order to train a rationale supervisor, we need to collect both unbiased and biased rationales, i.e., positive and negative samples. Though several verification methods have been proposed to collect positive and negative rationales in other tasks, both their purpose and the collection method are not suitable for our RE task. (1) Firstly, their collected positive and negative rationales are used for training the verifier, which only needs to discriminate the positive predictions from negative ones. In contrast, the rationale supervisor in our framework is designed to correct biased predictions, thus needing to further discriminate different negative rationales. (2) Secondly, the way of collecting rationales in current verification methods relies on the manually annotated golden reasoning steps as positive samples and perform rule-based perturbation (Paul et al. 2023; Golovneva et al. 2023) or error step alignment (Khalifa et al. 2023; Li et al. 2023b) to obtain negative samples. Unfortunately, such annotated samples and rules for perturbation are not available in RE.\nIn view of this, we propose a causal intervention and observation method to address the lack of annotated rationales and collect biased rationales for training the supervisor. Specifically, we first present a label-guided intervention strategy to collect unbiased rationales, and we also present a diversified intervention strategy to collect biased rationales. In addition, during the inference, we utilize the rationale supervisor to retrieve new demonstrations from the labeled samples and include them in the feedback, which are then used by the LLM for re-generating predictions. Since the supervisor has learned the difference among various biased rationales, the LLM gets the signal to adjust its direction for correction. This verification-feedback-correction procedure iterates until the output rationale is verified as unbiased.\nOverall, we make three major contributions. 1) We extend the LLM based RE research to the automated feedback paradigm, which equips LLM with the ability of correcting the biased prediction. 2) We propose a novel supervised rationale verification and feedback framework, which first collects rationales with a causal intervention and observation method for training the supervisor, and then employs the supervisor to retrieve sample-related demonstrations as feedback for guiding the LLM in correction. 3) Extensive experiments prove that our proposed method can improve the performance of LLM based RE methods and is superior to existing automated feedback methods."}, {"title": "Related Work", "content": "LLMs for Relation Extraction Recently, many studies (Xu et al. 2023; Li et al. 2023a; Wei et al. 2023; Wadhwa, Amir, and Wallace 2023; Li, Wang, and Ke 2023) have explored how to unlock the potential of LLMs for the RE task, including designing the in-context demonstration selection strategy (Wan et al. 2023; Ma, Li, and Zhang 2023; Pang et al. 2023) and optimizing instruction patterns (Zhang, Guti\u00e9rrez, and Su 2023; Wang et al. 2023a; Ma et al. 2023). Despite great success, these methods rely solely on optimizing the initial prompt to improve performance. However, we find that due to the relation bias, LLMs may still confuse certain relations with similar entities and thus make biased predictions. To alleviate this issue, we introduce the idea of automated feedback to RE for the first time, expecting to correct biased predictions via the provided feedback.\nLLMs with Automated Feedback Some researchers have exploited the automated feedback for correcting the undesirable output of LLMs (Pan et al. 2023; Kamoi et al. 2024). However, the feedbacks in existing methods are designed for correcting various reasoning mistakes, e.g., code errors (Zhang et al. 2023), factual errors (Gou et al. 2023), calculation errors (Nathani et al. 2023; Madaan et al. 2023; Paul et al. 2023), or as an optimal prefix for the next step in multi-step reasoning (Khalifa et al. 2023; Li et al. 2023b). These feedbacks are dependent on the reasoning task and unavailable for RE. Moreover, they do not include the demonstrations which are essential for RE. To address this issue, we propose a novel automated feedback framework which provides re-selected demonstrations as feedbacks to help LLMs correct the biased prediction."}, {"title": "Method", "content": "This section presents our proposed supervised rationale verification and feedback (SRVF) framework for the RE task.\nTask Formulation Given a set of pre-defined relation types $Y_D$, the relation extraction (RE) task aims to predict the relation type $y \\in Y_D$ between the head entity $e_h$ and the tail entity $e_t$ of each test example $x = \\{s, e_h, e_t\\}$, where $s$ denotes the sentence. In this study, we adopt in-context learning (ICL) with the rationale to prompt LLMs for the RE task. Specifically, for each test example $x$, we need to randomly select or retrieve $m$ initial in-context demonstrations $D_{icl} = \\{\\{x_1, r, y_1\\},..., \\{x_m, r_m, y_m\\}\\}$ related to $x$ from the labeled dataset $D_l$. Then, the LLM $f_\\theta$ with parameters $\\theta$ is expected to output the relation type $y \\in Y_D$ between $e_h$ and $e_t$, along with the rationale $r$, denoted as $\\{r,y\\} = f_\\theta(D_{icl}, x)$.\nOverview In this paper, we propose a rationale verification and feedback framework to guide LLMs towards better predictions for RE iteratively. Generally, this framework consists of three phases: 1) causal intervention and observation for rationale collection, 2) contrastive training rationale supervisor, and 3) rationale verification and feedback.\nSpecifically, we first adopt the causal intervention and observation method to collect unbiased and biased rationales,"}, {"title": "Causal Intervention and Observation for Rationale Collection", "content": "Generally, during this phase, for each labeled sample $\\{x_i, y_i\\}$, we aim to collect the unbiased rationale corresponding with the golden label $\\{r_u, y_i\\}$, as well as the biased rationale with corresponding biased relation prediction $\\{r_b, \\hat{y}\\}$. This process consists of two steps: 1) induce unbiased rationale, and 2) observe biased rationale. As shown in Fig. 2, we use the structural causal model (SCM) in causal inference (Pearl et al. 2000) to illustrate the strategy.\nPreliminary of SCM As shown in Fig. 2, the SCMs show the relationships among the input (X), the relation prediction (Y), the rationale for prediction (R), the certain bias of LLMs (B) and in-context demonstration I. The arrows between nodes indicate causal directions. For example, \"X \u2192 R\" means that the LLM generates the rationale R related to the prediction for the sample X. \u201cX \u2192 B \u2192 R\u201d indicates that the LLM activates some biased knowledge B related to the sample X and generates a rationale R influenced by the biased knowledge B. Besides, in Fig. 2 (b), the \u201cdo(Y)\u201d indicates that cutting off all factors that could influence the value of Y and assigning Y a certain value as needed.\nInduce Unbiased Rationale Previous methods rely on the human-annotated rationales, e.g., golden reasoning steps in mathematical tasks (Khalifa et al. 2023), which are not available in the RE dataset. To address this issue, we propose a label-guided intervention strategy to obtain the unbiased rationale for each labeled sample, which explains why the sample $x_i$ should be predicted as the golden label $y_i$.\nAs shown in Fig. 2 (b), this strategy consists of two steps: 1) cut causal directions that could make bias (B) influence the prediction (Y), and let the golden label guide the rationale (R) generation, formally denoted as $do(Y = y_i)$ and $do(Y) \u2192 R$. The observed generated rationale is $R = r^u_i$; 2) conduct similar do-operation to the rationale R and let $do(R)$ point to Y, i.e., $do(R = r^u_i)$, $do(R) \u2192 Y$. If the observed value of Y is equal to the golden label $y_i$, we treat $\\{r^u_i, y_i\\}$ as the unbiased one and add it to $R_u$.\nObserve Biased Rationale In previous methods, incorrect rationales are synthesized from golden ones using perturbation or error step alignment based on certain rules (Golovneva et al. 2023; Khalifa et al. 2023). However, these rules are designed based on the logical properties of reasoning tasks, which are not available in RE. To tackle this problem, we propose a diversified intervention strategy for collecting the biased rationales."}, {"title": "Contrastive Training Rationale Supervisor", "content": "We expect the rationale supervisor to 1) verify whether the output rationale is biased, and 2) provide different feedbacks for different bias situations to correct the initial prediction. To reach this, we adopt contrastive learning to train the rationale supervisor to acquire two abilities: 1) discriminating biased and unbiased rationales, and 2) learning the difference of various biased rationales.\nWe design two kinds of positive and negative pairs for contrastive training.\nFor positive pairs, we treat \"unbiased rationales with the same golden label\", and \"biased rationales under the same bias situation\" as the two kinds of positive pairs. For example, if samples $s_1$ and $s_2$, which have the same label, are also predicted as the same wrong relation, we call \"samples $s_1$ and $s_2$ are in the same bias situation\". Thus, the biased rationales ($r^b_1$ and $r^b_2$) of $s_1$ and $s_2$, are treated as a positive pair and should be pulled together in the rationale representation space, i.e., $r^b_1 \\rightarrow \\leftarrow r^b_2$.\nFor negative pairs, we first consider the \u201cbiased and unbiased rationales from the same sample\" as a negative pair. This is designed to train the rationale supervisor to distinguish between biased and unbiased rationales. For example, a sample $s_1 = \\{r^u, y_1\\}$ where $y_1$ is the golden label and $r^u_1$ is the corresponding unbiased rationale, is wrongly predicted as relation $y_2$ and corresponding biased rationale is $r^b_1$. Thus, $r^b_1$ and $r^u_1$ are treated as a negative pair and should be pushed away in the rationale representation space, i.e., $r^b_1 \\nleftrightarrow r^u_1$. Second, we also treat \"biased rationales under different bias situations\" as a negative pair to train the rationale supervisor, which can distinguish different bias situations and provide feedback based on the biased rationale in"}, {"title": "Rationale Verification and Feedback", "content": "As shown in Fig. 3, in the inference time, the trained rationale supervisor $R_\\gamma$ first verifies whether the prediction is biased. If the prediction is biased, the rationale supervisor will retrieve a feedback demonstration set, which then guides LLMs toward refined predictions. In this subsection, we will elaborate on the \u201cRationale Verification\u201d and \u201cFeedback Demonstration Retrieval\u201d in Fig. 3 in detail. Here we denote the test example, output rationale, and relation prediction of LLMs as x, r, and y, respectively.\nRationale Verification For verification, we need to select the subsets $S_b$ and $S_u$ related to the prediction y from R and $R_u$, respectively, which are then used as anchors to determine whether the current output rationale is close to the biased or unbiased groups. $S_b$ and $S_u$ are defined as follows:\n$S_b = \\{\\{r^b, y^b\\} | \\{r^b, y^b\\} \\in R_b, \\hat{y} = y\\}, \\qquad(3)$\n$S_u = \\{\\{r^u, y^u\\} | \\{r^u, y^u\\} \\in R_u, y^u = y\\}, \\qquad(4)$\nThen, the indicator score to judge whether r is a biased rationale is calculated as follows:\n$p_b = \\frac{max_\\{\\{r^b,y^b\\}\\in S_b\\} sim(r, r^b) - max_\\{\\{r^u,y^u\\}\\in S_u\\} sim(r, r^u)}{2}, \\qquad (5)$\nwhere the similarity function $sim()$ is defined in Eq. 2. When $p_b$ is greater than 0, it implies that the feature of r is closer to the feature field of $S_b$ than that of $S_u$, which means r and corresponding relation prediction y should be regarded as biased, and feedback is needed to correct them.\nFeedback Demonstration Retrieval Once the output rationale r is verified as biased, we need to retrieve a new set of in-context demonstrations based on the feature of r for guiding LLMs toward correct predictions. Specifically, we first select the k most similar biased rationales to r in $S_b$, denoted as $S_{b^{topk}}$, which is defined as:\n$S_{b^{topk}} = \\{\\{r^b,y\\} | rank\\{\\{r^b,y\\}\\in s_1\\} (sim(r, r')) \\leq k\\},\\qquad (6)$\nThen, we select the labeled samples corresponding to the biased rationales in $S_{b^{topk}}$ from $D_l$ as the feedback demonstrations $D_{fb}$, which is defined as:\n$D_{fb} = \\{\\{x_i, r, y_i\\} | \\{x_i, r, y_i\\} \\in D_1, \\{r_i, y\\} \\in S_{b^{topk}}\\},\\qquad (7)$\nwhere the biased $\\{r, y\\}$ and unbiased $\\{r^u_i, y_i\\}$ correspond to the same labeled sample $\\{x_i, y_i\\}$.\nCorrection via In-context Learning After the feedback demonstrations $D_{fb}$ are selected, we re-generate r and y using the LLM $f_\\theta$, i.e., $\\{r, y\\} = f_\\theta(D_{fb}, x)$. This process will be iteratively performed until r is verified as unbiased, and the corresponding prediction y will be finally output."}, {"title": "Experiments", "content": "Table 1 reports the experimental results with various initial demonstration selection strategies on Llama-2-7b-chat on the SemEval, TACRED, and Re-TACRED datasets. From Table 1, we can draw the following conclusions: 1) Our proposed SRVF framework yields significant enhancements upon various backbones with different demonstration selection strategies. Specifically, the improvement is most significant when randomly selecting the initial demonstrations, getting a 10.65% absolute micro-F1 score increase on average. Besides, when using SimCSE and task-specific retriever as backbones to carefully select initial in-context demonstrations, there are also 6.49% and 3.24% absolute micro-F1 score boosts on average, respectively. 2) Our proposed method exhibits significant superiority over existing verification and feedback methods under all settings. The multi-agent based Self-Refine method is the worst, which is mainly due to its unsuitable feedback objectives and correction manner. Existing methods for verifying the output of LLMs, i.e., Self-Consistency and GRACE, can enhance the performance of in-context learning to some extent. However,"}, {"title": "Ablation Study", "content": "To validate the effectiveness of components in our method, we introduce the following variants for ablation studies:\n\u2022 w/o label-guided intervention (LGI), where the labels do not guide the collecting of unbiased rationales.\n\u2022 w/o diversified intervention (DI), which replaces the DI with random sampling for collecting biased rationales.\n\u2022 w/o rational contrastive training (RCT), which trains the rationale supervisor with cross-entropy loss.\n\u2022 w/o feedback demonstration retrieval (FDR), which removes the FDR strategy and uses the initially selected demonstrations as the feedback.\n\u2022 w/o RG, which skips the re-generation process and directly adopts the label of the top-1 retrieved demonstration as the final prediction.\nThe results of the ablation study are shown in Table 2. From the table, we make the following observations. 1) Removing LGI and DI strategies significantly degrades performance, indicating that LLMs struggle to collect unbiased rationales based solely on generation without causal intervention. 2) Eliminating RCT also reduces performance, demonstrating its effectiveness in helping the rationale supervisor distinguish between unbiased and various biased situations. 3) Omitting FDR significantly decreases performance, highlighting its crucial role in guiding LLMs toward corrected predictions despite iterative verification. 4) Removing the re-generation process results in a substantial performance drop, showcasing that simple assignment of retrieved top-1 demonstrations isn't sufficient and that in-context feedback for re-generation adds robustness to the correction process."}, {"title": "Analysis", "content": "Effectiveness on Various-scale LLMs To examine whether the proposed method remains effective for various-scale LLMs, we conduct experiments on various sizes of LLMs from the Llama-2-chat (Touvron et al. 2023), Meta-Llama-3-Instruct (AI@Meta 2024), and GPT-3.5 (Ouyang et al. 2022), and present their results in Table 3.\nFrom Table 3, it can be seen that our rationale supervisor can boost the performance of LLMs with various sizes.\nSpecifically, even with the most powerful Meta-Llama-3-70B-Instruct, there is still a 2.47% micro-F1 score improvement over the original in-context learning. The experimental results indicate that the \u201crelation bias\u201d issue exists in LLMs of various scales, and our proposed method can function as a plug-in module for various LLMs to effectively mitigate this problem.\nComparision with Well-designed Few-shot Methods for RE As shown in Table 3, we include two established supervised fine-tuning methods for RE as baselines: 1) R-BERT (Wu and He 2019), which fine-tunes a BERT for the RE task, and 2) KnowPrompt (Chen et al. 2022), which is tailored for few-shot scenarios and has shown good few-shot performance. As we can see from the results, with the help of our proposed SRVF, even the relatively weak Llama-2-7b-chat can outperform KnowPrompt by 1.80% averagely. Moreover, when deploying our SRVF on the most powerful Meta-Llama-3-70B-Instruct, there is an average performance improvement of 11.27% compared to KnowPrompt.\nAnalysis on Successfully Corrected Samples To visualize which samples are successfully corrected by the proposed method, we compare the error matrix on the SemEval dataset before and after correction. The results are obtained by summing the number of error predictions of all settings in Table 1. The results are shown in Fig. 4.\nFrom Fig. 4 (a), we observe that LLMs struggle to distinguish between relations that share similar entities, e.g., 687 samples labeled as \u201cEntity-Destination\u201d are incorrectly predicted as \"Content-Container\". Such error can arise when, for example, given sentences \"please move the eggs into the"}, {"title": "Analysis on Method Efficiency", "content": "Considering possible concerns on the inference efficiency due to the iterative feedbacks, we compare the inference time on the SemEval dataset of different methods. Besides, we also evaluate the pre-inference time of each method, e.g., the time to obtain biased/unbiased data and train the rationale supervisor in our SRVF. The comparison results are shown in Fig. 5.\nFrom Fig. 5, we can observe that:\n1) Basic in-context learning (ICL) is the most efficient. 2) Self-Refine does not require pre-inference time, but its inference time is more than the sum of our pre-inference time and inference time. Moreover, Self-Refine has the worst performance among all methods (Table 1). 3) Self-Consistency and GRACE have much higher computational costs than our SRVF, especially in terms of inference time. This is mainly because the proposed rationale supervisor can verify whether the LLM prediction is biased. Only the test samples verified as biased by the rationale supervisor will proceed to the correction round for regeneration. This greatly reduces the time cost of our method in inference time after correction.\nOverall, our SRVF is the second-best in computational efficiency while achieving the best performance (Table 1).\nExperiments on Document-level RE To explore the effectiveness of our method for document-level RE, we apply SRVF on three backbones and conduct experiments on two commonly used document-level RE datasets, DocRED (Yao et al. 2019) and Re-DocRED (Tan et al. 2022). The random and SimCSE backbones are kept the same as before. For the task-specific backbone, we borrow the idea from RE-PLM (Ozyurt, Feuerriegel, and Zhang 2024), which obtains the final prediction by aggregating the predictions based on multiple retrieved demonstrations. The experimental results are reported in Table 4.\nFrom Table 4, we can observe that: 1) LLM performs poorly on document-level RE, which is consistent with empirical observations in Li, Jia, and Zheng (2023); Sun et al. (2024). This is due to the difficulty LLMs face in selecting entity pairs that have certain relations from a vast space of candidate entity pairs. Besides, the large number of candidate relation labels (96 in DocRED and Re-DocRED) further increases the difficulty in assigning each entity pair a relation. 2) Our proposed SRVF effectively enhances the performance of LLM under various settings on DocRED and Re-DocRED, indicating that our method remains to be effective in such challenging scenarios."}, {"title": "Conclusion", "content": "In this paper, we propose a novel automated feedback framework for LLM based relation extraction (RE), which includes a rationale supervisor to iteratively correct the biased relation prediction of LLMs. Specifically, we first present a causal intervention and observation method to collect unbiased and biased rationales, which are then used to train the rationale supervisor. Then, we develop a verification-feedback-correction procedure to iteratively enhance LLMs' ability to correct the biased prediction. Extensive experiments demonstrate the superiority of our framework over existing methods. In the future, we will try to extend the proposed framework to other NLP tasks."}, {"title": "Experimental Details", "content": "Table 5 shows the statistics of datasets used in our experiments and the number of training samples under various few-shot settings.\nPrompt Design for LLM based RE Table 6 presents an example of the designed prompt with the corresponding expected response. The prompt consists of four parts, i.e., {Instruction, Demonstrations, Hint, Inference}. When conducting relation extraction using LLMs, we input the prompt into LLMs and parse the response for the predicted relation. Besides, due to the limited context size of LLMs 3, we set the number of demonstrations as 10, 4, and 4 for the SemEval, TACRED, and Re-TACRED datasets, respectively.\nDemonstration Selection Strategy\nRandom For the \"Random\" strategy, we randomly select m in-context demonstrations from the few-shot labeled sample set.\nSimCSE For the \"SimCSE\" strategy, we retrieve the top-k nearest samples as in-context demonstrations measured by the sentence embedding. Specifically, we adopt the embeddings corresponding to the [CLS] tokens for retrieval. In our experiments, we adopt the sup-simcse-bert-base-uncased (Gao, Yao, and Chen 2021) as the encoder to obtain the embeddings.\nTask-specific For the \"Task-specific\" strategy, similar to the \"SimCSE\" strategy, we retrieve the top-k nearest samples as in-context demonstrations based on certain embeddings. Different from the \u201cSimCSE\u201d strategy, here we adopt a task-specific encoder to obtain the task-aware embeddings, which is proposed by Wan et al. (2023) and is the state-of-the-art strategy to select in-context demonstrations for LLM based RE. Specifically, we first concat each sample with special tokens, i.e., {[CLS], sentence-part1, $, head-entity, sentence-part2, #, tail-entity, sentence-part3, [SEP]}. We average the embeddings corresponding to [CLS], head-entity and tail-entity tokens as the embedding for retrieval. The embeddings are obtained by a task-specific encoder, which is initialized as BERT (Devlin et al. 2018) and is trained via the cross-entropy loss for classification using the few-shot labeled sample set. For the training of the task-specific encoder, the batch size is set to 16, the learning rate is set to 2e-5, and the epoch number is set to 100.\nBaselines\nSelf-Refine (Madaan et al. 2023) is proposed to improve the initial response from LLMs through iterative feedback and refinement. Since it is designed for various reasoning tasks, e.g., math reasoning, code optimization, or acronym generation, we cannot directly adopt it for the RE task. Thus, to adapt it for RE, we design the \"inconsistency issue\" as the verification objective, which means that the given rationale is not consistent with the final relation prediction. Specifically, this framework consists of three LLM based agents, i.e., RE agent, verifier agent, and refiner agent. First, we prompt the RE agent using the designed prompt in Appendix for relation extraction. Second, we ask the verifier agent to find errors like the \"inconsistency issue\u201d in the initial response. If the verifier agent finds certain errors, it will output the corresponding reasons for the found error, which will be treated as feedback for refinement. Finally, the error feedback given by the verifier agent will be fed to the refiner agent to correct the initial prediction. To show how this approach is reproduced, we show an example of the prompt used for the verification and refinement in Table 7.\nSelf-Consistency (Wang et al. 2023b) is proposed to conduct verification for the multiple candidate responses. Specifically, it employs a simple majority voting strategy to select the final prediction from the candidate predictions. For example, if there are 5 candidate relation predictions {y_A, y_B, y_A, y_A, y_A}, the final prediction is y_A, which is the majority one.\nGRACE (Khalifa et al. 2023) is proposed to conduct verification and feedback for multi-step reasoning tasks. It selects the best one for each intermediate reasoning step, where the selected reasoning step is used as the feedback for LLMs to generate the next step. However, the reasoning procedure (rationale) in the RE task usually consists of only one step, making such feedback unsuitable for RE. Thus, in the experiments, we only use it for the verification stage, i.e., selecting the best relation prediction via selecting the corresponding best rationale. Specifically, we follow its official code to implement the training of the discriminator, which"}, {"title": "Supplementary Analysis", "content": "Event Detection Task\nTo explore the extensibility of our proposed approach to other NLP tasks, we select another important task in information extraction, i.e., event detection. The event detection (ED) task aims to detect event trigger words in a given sentence and categorize the event trigger words. For example, given the sentence: \"Shaunie O'Neal gave birth to the couple 's third child at 1:52.\u201d, we aim to extract that \"{birth} is the trigger word for a {Life:Be-Born} event\u201d. We select the widely used dataset ACE05 (LDC 2005) for a preliminary study, which has 33 event types. The experimental results are shown in Table 11.\n\u2022 Step 1: Initial Scoring As shown in Table 14, we input the evaluated rationale, task instruction, and evaluation criteria to GPT-4 to score the rationale on a scale from 1 to 5. Besides, we also prompt GPT-4 to generate a corresponding reason for the given score, which describes the strengths and weaknesses of the evaluated rationale from multiple perspectives.\n\u2022 Step 2: Expert Review We then ask human experts familiar with the relation extraction task, to review the reason and score provided by GPT-4. The human experts may adjust the scores by increasing, maintaining, or decreasing them as necessary.\n\u2022 Step 3: Final Scoring Finally, we calculate the average of the adjusted scores from the three human experts as the final quality score.\nQuality Analysis of Unbiased Rationale We conduct a quality analysis of the unbiased rationale generated by our proposed SRVF. Specifically, the set for evaluation is composed of samples from SemEval, TACRED, and Re-TACRED under the 5-shot setting, with 50, 210, and 200 samples respectively. Following Liu et al. (2023), we first input the rationale, task introduction, and evaluation criteria to GPT-4, and ask it to generate a quality score with detailed reason. Then, we ask three human experts to raise or lower the initial score when they disagree with the reason given by GPT-4. The agreement rate between GPT-4 and the human experts is 91.3%. As shown in Fig. 6, the overall process for evaluating the quality of the generated unbiased rationale is as follows:\nAnalysis of Iterative Feedback To explore the required iteration number in the automated feedback procedure, we visualize the performance corresponding to the number of iterations in Fig. 7."}]}