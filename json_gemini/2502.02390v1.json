{"title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning", "authors": ["Jianfeng Pan", "Senyou Deng", "Shaomang Huang"], "abstract": "Research on LLM technologies is rapidly emerg-ing, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-01, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (COAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have rapidly become a cornerstone in natural language processing, powering applications ranging from conversational agents to complex decision-making systems. Central to their operation is the process of inference, where LLMs generate contents based on learned patterns from massive datasets by auto-regressive learning algorithm in pre-trained stage. Most LLMS (GPTS [Achiam et al., 2023], Llamas [Dubey et al., 2024], and Qwens [Yang et al., 2024] et al.) employ a 'fast thinking' approach to inference which relies heavily on the pre-trained reasoning capabilities of LLM models. This approach processes a single query to produce the final result. Although effective for many tasks, this approach often struggles with problems requiring nuanced, iterative reasoning, or adaptation to new information.\nRecent advances have begun to explore alternatives to 'fast thinking', introducing 'slow thinking' methodologies that align more closely with human thinking processes. This idea emphasize deliberate, iterative reasoning, and the integration of historical contents or external knowledge during inference. OpenAI-01 [Jaech et al., 2024], a notable project, has sparked significant interest in this domain, showcasing the potential of 'slow thinking' frameworks to improve reasoning capabilities. However, slow thinking merely subdivides the reasoning process into smaller steps and involves rethinking what has already been generated. Throughout the process, reliance is still placed on the initial input information and the logical reasoning abilities of the LLM itself."}, {"title": "2 Realted Work", "content": "Inspired by the human ability to constantly associate and replenish knowledge during thinking, we propose the Chain-of-Associated-Thoughts (CoAT) framework. To our knowledge, associative memory mechanisms were first applied to simulate human thought in LLM processes. The associative memory mechanism empowers CoAT to dynamically incorporate new key information during inference, mimicking the human ability to associate and update knowledge iteratively. Furthermore, we optimize the routing strategy in the MCTS algorithm to ensure that each addition of associative memory will provide additional key information for subsequent content generation. This synergy between structured search and adaptive learning enables CoAT to expand its reasoning scope while maintaining contextual coherence, overcoming limitations of conventional LLMs.\nThe effectiveness of our framework is validated through extensive experiments across a diverse range of generative and reasoning tasks. The results demonstrate that our framework significantly outperforms traditional models in terms of accuracy, coherence, and diversity. By iteratively refining inferences and integrating evolving information, our CoAT framework achieves outputs that are both precise and comprehensive, advancing the state-of-the-art in reasoning-oriented LLM frameworks.\nIn summary, the main contributions of our work are as follows.\n\u2022 We propose the CoAT framework to enhance LLM reasoning. Our framework expands the LLM reasoning space to search for a high-quality solution using the optimized MCTS algorithm.\n\u2022 We endow the LLM reasoning process with human-like associative and adaptive self-refinement capabilities to effectively address complex reasoning tasks.\n\u2022 We optimized the routing strategy to identify the optimal content generation path within our framework, and extensive qualitative and quantitative experimental results demonstrate its superior performance compared to other inference approaches.\nThis paper is structured as follows: Section 2 reviews related work on LLM inference strategies. Section 3 details the design and implementation of our CoAT framework. Section 4 presents the experimental results, and Section 5 concludes with insights into the implications of this research and potential future directions.\nThe development of Large Language Models (LLMs) has witnessed significant advances in recent years, with a particular focus on improving reasoning capabilities. This section reviews key research on LLM inference strategies, the integration of iterative reasoning frameworks, and associative memory mechanisms, all of which inform the design of our Chain-of-Associated-Thoughts (COAT).\nLLM Inference Strategies Traditional LLMs, including BERT [Devlin, 2018], GPT-3 [Brown et al., 2020] and its successors (like GPT-4 [Achiam et al., 2023]) rely on a single-shot or few-shot inference paradigm. These methods emphasize the model's ability to provide accurate responses using fixed prompts, often resulting in outputs that lack robustness in scenarios that require deeper reasoning. To address these limitations, researchers have explored chain-of-thought (CoT) prompting [Wei et al., 2022] and interleaving retrieval with chain-of-thought (IRCoT) [Trivedi et al., 2022], which enable LLMs to decompose complex problems into smaller sequential steps. Although this improves reasoning quality, it remains inherently static as the model cannot revisit or refine previous inferences during the reasoning process.\nMore recently, the variants of CoT, such as self-consistency chain-of-thought (CoT-SC) [Wang et al., 2022] have introduced diversity in reasoning by sampling multiple outputs and selecting the most consistent solution, Graph-of-thought (GoT) [Besta et al., 2024] has been improved with search algorithms that can search solution paths more effectively, and Tree-of-thought (ToT) [Yao et al., 2024] prompting uses DFS or BFS search guided by LLMs. However, these methods do not fundamentally alter the underlying inference mechanism, leaving room for further exploration of dynamic and iterative reasoning processes.\nThe concept of 'slow thinking' has gained traction as an alternative to traditional inference paradigms, inspired by the human ability to deliberate and refine thoughts over time. OpenAI-01 [Jaech et al., 2024] has been a pioneering framework in this space, demonstrating the benefits of iterative reasoning for tasks involving complex problem solving and decision making. By allowing LLMs to reassess previous steps and integrate new information, slow thinking frameworks improve adaptability and output quality. These advancements highlight the potential of moving beyond static reasoning toward more dynamic, context-aware methodologies.\nMonte Carlo Tree Search in Inference MCTS has a long history of success in domains requiring decision making under uncertainty, such as game playing [Silver et al., 2016] and planning [Coulom, 2006]. Its ability to balance exploration and exploitation makes it a compelling candidate for enhancing LLM reasoning. Existing works, like LLM-MCTS [Zhao et al., 2024], LLM agent tree search (LATS) [Zhou et al., 2023] and reasoning via planning (RAP) [Hao et al., 2023] have integrated MCTS into specific AI systems to improve search space exploration, but its application in LLMs remains limited. Our CoAT extends this approach by leveraging MCTS not only for structured exploration but also as a means to iteratively refine reasoning pathways by inserting associative memory during inference.\nAug-mented knowledge, an external information retrieval process that enables humans to form and retrieve connections between related concepts when thinking, has inspired various machine learning models. Memory-augmented neural networks [Santoro et al., 2016] and recurrent memory-based architectures [Zaremba, 2014] have demonstrated their effectiveness in tasks requiring long-term context retention. However, these systems often lack the flexibility to adapt to evolving information during LLM inference. Recent advancements, such as native Retrieval Augmented Generation (NativeRAG) [Lewis et al., 2020], Knowledge Augmented"}, {"title": "3 Method", "content": "Inspired by the human ability to form associations during cognitive processes and the demonstrated effectiveness of MCTS algorithm in enhancing the reasoning capability of LLMs, we propose the CoAT reasoning framework. The framework leverages the association mechanism to enable LLMs to perform real-time retrieval of relevant information and self-augmentation during the reasoning process. The realization of this functionality is underpinned by our optimized MCTS algorithm, which systematically integrates associative content and generated content through tree node search. By assigning precise values to each node based on our predefined rules, the algorithm facilitates the automatic association process, thereby completing the reasoning task. To further enhance the reasoning quality of CoAT framework, we have designed a flexible mechanism for sourcing associative content. This mechanism allows the model to either perform self-association or retrieve associative information through external knowledge sources, referred to as an \"External Brain (EB).\" The external brain encompasses commonly used resources such as knowledge graph, vector database, LLM agents, and web search engines. A detailed search process of the CoAT framework when query \"How should we view the role of artificial intelligence in contemporary interna-tional competition? Which countries hold the leading advantages in this field?\" is shown in Figure 3. In the following subsections, we provide a detailed explanation of the association memory mechanism and the optimized MCTS algorithm."}, {"title": "3.1 Associative Memory Mechanism", "content": "We introduce associative memory mechanism in the CoAT framework, can be regarded as a novel external knowledge augmentation mechanism, which enables the reasoning process of LLMs to dynamically update and integrate newly retrieved information in real time according to the generated content of each node. Existing methods primarily focus on incorporating extended knowledge into the reasoning process at its initial stage. However, this approach may lead to incorporation of overly broad knowledge, which introduces two significant drawbacks: (a) an excess of irrelevant information that compromises inference efficiency, and (b) insufficient inclusion of critical content, ultimately degrading inference quality. In contrast, our proposed real-time association mechanism, integrated into the inference process, effectively addresses these issues by dynamically aligning relevant knowledge with the ongoing inference.\nThe associative memory mechanism generates content that is beneficial for reasoning and has not been previously mentioned in historical contents. The associative content should exhibit minimal redundancy with existing generated contents and should be concise enough to avoid interfering with the reasoning process. Furthermore, the subject of associative content must maintain a strong relevance to the overall reasoning framework. If these conditions are not satisfied, the associative content for the node can be left empty. The above principle will be applied in evaluation stage for evaluating the quality of associative memory.\nWhen generating the associative memory of a node ni, the \"External Brain\" can serve as an alternative approach to enhance the quality of inference results. However, this approach may reduce the inference efficiency. This process can be summarized as follows:\n$AM(n_i) = EB \\rightarrow LLM(Q | G(n_i)).$ (1)\nwhere $G(n_i)$ denotes the generated content of node ni and EB is the External Brain of target LLM.\nThen, a node can reference both the historical content and the associative memories derived from all of its ancestral nodes. Their historical content and associative content together constitute the comprehensive thinking process of the target LLM. The generation process of each node $n_{i+1}$ is formulated as follows:\n$G(n_{i+1}) = LLM(Q | G(n_i) | AM(N_{1:i})).$ (2)\nwhere Q is the input query and $AM(n_{1:i})$ denotes the associative memories of nodes $n_1 \\sim n_i$ in the reasoning trajectory."}, {"title": "3.2 Optimized MCTS", "content": "The standard process of the MCTS algorithm consists of four stages: Selection, Expansion, Simulation, and Backpropagation. In the selection stage, MCTS applies UCT algorithm (Upper Confidence bounds applied to Trees) [Kocsis and Szepesv\u00e1ri, 2006] to choose the best node and then adds it to the trajectory. The UCT of a node n is calculated as follows:\n$UCT(n) = V(n) + w \\sqrt{\\frac{lnN(p)}{N(n)}}$ (3)\nwhere N(n) is the number of visits to the node n, V(n) is the score value of node n, w is the exploration weight, and p is the parent node of n. When the end of an episode is reached, a back-propagation is carried out to update the value of node n and its parent nodes.\nThe traditional MCTS algorithm has demonstrated significant success across various decision-making domains. Recently, with the advancements in LLM, numerous novel variants of MCTS have been proposed to enable more effective integration with LLMs. The work of LATS [Zhou et al., 2023] introduces an Evaluation stage after Expansion and a Reflection stage at the end of the process. The evaluation stage assesses the quality of the content generated during the expansion stage, while the reflection stage determines whether the outputs correctly address the inputs. Building on these improvements, we propose an Association stage to simulate the human associative mechanism between the expansion and evaluation stages. The optimized MCTS process is shown in Figure 4. Consequently, the quality of the associative content is also assessed during the evaluation stage. The evaluation criteria encompass both the quality of the associative content and its correlation with the content generated during the expansion stage, with the goal of preventing excessive associations and mitigating hallucinations. Now, the evaluation value of each node n has two components: the generated content value and the associative content value. And the node value is calculated as follows:\n$V(n) = F_g(Q, G(n)) + \\beta * F_a(G(n), AM(n)).$ (4)\nwhere G(n), AM(n) denotes the generated content and the associative content at node n, respectively. $F_g$ is the evaluation function for generation and association. \u03b2 is the weight to balance the impact factor of the associative content.\nIn the backpropagation stage, we update the visit counts and quality evaluations for every node along the trajectory based on the outcomes of the simulation stage from the leaf node to the root node. The calculation of visit counts is formalized as C(ni+1) = C(ni)+1. And the quality evaluation value of a parent node np will be updated with its children nodes nc as follows:\n$V(n_p)^* = \\frac{V(n_p) * C(n_p) + \\Sigma_c V(n_c)}{C(n_p) + K}$ (5)\nwhere K is number of candidate nodes of each parent node, C(np) is the original visit counts of np. The updated node value $V(np)^*$ is used in the UCT algorithm (Eq. 3.2) to choose trajectory node in the next selection stage.\nTo more precisely determine when to terminate the MCTS search process, we trained a specialized Reward Model (RM) to evaluate the content generated at the leaf node of the search trajectory. In certain extreme cases, the search process may enter an ambiguous state, leading to inefficiencies. To mitigate this issue, we introduce a hyper-parameter (D) to constrain the maximum depth of the tree search. When the search depth surpasses D, the process halts, and the best inference result obtained up to that point is returned. Notably, setting D = -1 removes any depth limitation, allowing the search to continue until the optimal result is identified. The above algorithm flow can be summarized as Algorithm 1."}, {"title": "4 Experiments", "content": "The implementation of our CoAT framework is built upon the LangChain project, which provides a robust foundation for developing language model pipelines. To quantitatively evaluate the performance of our framework on various tasks, we leverage the OpenCompass [Contributors, 2023] project, a comprehensive benchmarking initiative for language model evaluation. The compared models are accessed via OpenAI-compatible APIs, with the vLLM [Kwon et al., 2023] framework enabling efficient API integration and execution. To assess the effectiveness of our CoAT framework, we designed two series of validation experiments: (a) evaluating the qualitative performance of our framework when integrated with LLMs; (b) analyzing the quantitative performance of our framework with other related enhancing reasoning methods."}, {"title": "4.1 Qualitative Performance Evaluation", "content": "In this section, we conduct a set of complex query questions that require more extensive knowledge in order to fully answer them.\nAs shown in the results, the output content of Qwen2.5-32B with our COAT framework has the richest content both in terms of text volume and text coverage. Compared to the contents of Qwen2.5-32B and chatGPT, our generated content has the supernumerary category \"Ethical and Regulatory Frameworks\" which is important for AI research. Meanwhile, the content of our framework in each category is more abundant than the other two models."}, {"title": "4.2 Quantitative Performance Evaluation", "content": "In this section, we will verify the validity of our CoAT framework in two aspects. (a) We compare the base models reasoning through the CoAT framework with other retrieval-augmented methods. (b) We compare a base model reasoning through the CoAT framework with its fine-tuned model in an explicit field."}, {"title": "Quantitative Performance in RAG Generation", "content": "In this section, we enhance the quality of content generated by the associative mechanism through the integration of extended knowledge, and demonstrate that improving the quality of associative content leads to enhanced reasoning ability in our framework. To validate the effectiveness of CoAT framework for the knowledge-intensive question-answering task, we conduct a series of quantitative comparative experiments based on retrieval-augmented generation.\nThe compared methods are NativeRAG [Lewis et al., 2020], IRCOT [Trivedi et al., 2022], HippoRAG [Guti\u00e9rrez et al., 2024], LATS [Zhou et al., 2023], and KAG [Liang et al., 2024]. And two widely-used multi-hop QA datasets are HotpotQA [Groeneveld et al., 2020], which consists of 113k Wikipedia-based question-answer pairs, and 2WikiMultiHopQA [Ho et al., 2020], which introduces evidence information and contains reasoning paths for multi-hop problems.\nSettings. To evaluate the QA performance, we adopt two widely used metrics: Exact Match (EM), which measures the percentage of exact matches between predicted and ground-truth answers, and F1 scores, which capture the harmonic mean of precision and recall. Furthermore, the associative memory is influenced not only by the inherent capabilities of the LLM but also by the quality of retrieval results from external knowledge sources. The evaluation framework is built on the Github project hotpot\u00b9 and 2wikimultihop\u00b2, while the parameters of them are all the default values."}, {"title": "Quantitative Performance in Code Generation", "content": "In this section, we compare the base models (Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct) reasoning through the COAT framework with its fine-tuned models (Qwen2.5-Coder-7B-Instruct, Qwen2.5-Coder-14B-Instruct) in code generation task on some open-source datasets, such as HumanEval [Chen et al., 2021], MBPP [Austin et al., 2021], and HumanEval-X [Zheng et al., 2023]."}, {"title": "5 Conclusion", "content": "In this paper, we proposed the Chain-of-Associated-Thoughts (COAT) framework, which advances LLM reasoning by integrating an optimized Monte Carlo Tree Search (MCTS) algorithm and a dynamic associative memory mechanism. These innovations enable structured exploration of reasoning pathways and adaptive knowledge updating, addressing limitations of traditional LLMs. Extensive experiments demonstrated that our CoAT outperforms conventional approaches in accuracy, coherence, and diversity. Our work highlights the potential of combining structured search and adaptive associative memory in LLMs, offering a new exploration for future research on integrating external real-time knowledge for real-world applications."}]}