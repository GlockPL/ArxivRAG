{"title": "Is Your LLM Secretly a World Model of the Internet? MODEL-BASED PLANNING FOR WEB AGENTS", "authors": ["Yu Gu", "Boyuan Zheng", "Boyu Gou", "Kai Zhang", "Cheng Chang", "Sanjari Srivastava", "Yanan Xie", "Peng Qi", "Huan Sun", "Yu Su"], "abstract": "Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents' performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WEBDREAMER, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WEBDREAMER uses LLMs to simulate outcomes for each candidate action (e.g., \"what would happen if I click this button?\") using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with on-line interaction\u2014VisualWebArena and Mind2Web-live\u2014demonstrate that WEBDREAMER achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents.", "sections": [{"title": "1 INTRODUCTION", "content": "Planning (Mattar & Lengyel, 2022)\u2014the strategic search for optimal action sequences to achieve goals from initial states-has been fundamental to artificial intelligence since its inception, driving remarkable breakthroughs including superhuman performance in games like Go (Feng et al., 2023; Silver et al., 2016). Recent advances have demonstrated that integrating large language models (LLMs) with advanced planning algorithms (e.g., Yao et al. (2023a); Hao et al. (2023); Gu et al. (2023); Wang et al. (2024); Feng et al. (2023); Brown et al. (2024)) substantially enhances their performance on complex reasoning tasks beyond chain-of-thought (CoT) (Wei et al., 2022) approaches, with OpenAI's o1 (OpenAI, 2024b) serving as a prominent example. These methods effectively scale inference-time compute and enable LLMs to explore multiple potential solution paths, which ultimately lead to more accurate outcomes.\nAlongside these developments, research into generalist web agents capable of planning and executing a sequence of actions to complete complex tasks across diverse websites has garnered significant interest (Deng et al., 2023; Zhou et al., 2023; Zheng et al., 2024; Koh et al., 2024a), partly due to the web's potential as a complex yet realistic environment for driving agent research and development. However, applying existing planning algorithms to the online web environment presents formidable challenges. Chief among these challenges are the inherent safety risks associated with live website"}, {"title": "2 RELATED WORK", "content": "2.1 WEB AGENTS\nDriven by the goal of automating tedious and repetitive web-based tasks, web agents powered by (multimodal) language models have made substantial progress in various aspects. Benchmarks have evolved from MiniWoB++ (Shi et al., 2017; Liu et al., 2018) to WebShop (Yao et al., 2022) and WebArena (Zhou et al., 2023), offering increasingly realistic website simulations. VisualWebArena (Koh et al., 2024a) and Mind2Web (Deng et al., 2023) challenge models' ability to handle visual information and generalize across diverse tasks, websites, and domains.\nReactive Agents. Reactive agents make decisions based on immediate observations from the environment without performing any search or simulation of future actions, typically implemented with the ReAct framework (Yao et al., 2023b). Much progress has been made to enhance the fundamental capabilities of reactive web agents through both prompting closed-source models (Zheng et al., 2024; He et al., 2024; Deng et al., 2023) and training models using HTML and webpage screenshots (Lee et al., 2023; Gur et al., 2023; Furuta et al., 2023; Hong et al., 2024; Baechler et al., 2024). Additionally, models' abilities to ground web agent actions to elements have been improved through training on action-coordinate pair data (You et al., 2024; Cheng et al., 2024). Further advancements have been achieved by training on web agent trajectories, utilizing both human-annotated trajectories (Shaw et al., 2023; Hong et al., 2024; Deng et al., 2023; Lai et al., 2024) and synthesized exploration trajectories (Furuta et al., 2023; Song et al., 2024; Patel et al., 2024). However, reactive agents inherently suffer from short-sightedness, which can often lead to suboptimal performance in multi-step decision making.\nAgents with tree search. Pan et al. (2024a) introduces a reward model based on GPT-4V, designed to provide both step-wise and trajectory-level rewards to guide inference-time search. Search Agent (Koh et al., 2024b) investigates inference-time search algorithms in interactive web environments, enabling explicit exploration and multi-step planning. In contrast to Search Agent, which employs a variant of best-first tree search, AgentQ (Putta et al., 2024) and WebPilot (Zhang et al., 2024) utilize Monte Carlo Tree Search (MCTS) as their primary search strategy.\nWhile tree search on websites has demonstrated significant improvements, it still presents several limitations. First, the search process substantially increases inference time due to the need for extensive exploration, which is difficult to parallelize given its inherently sequential nature. Backtracking to previous states is essential for search-based methods but impractical on real-world websites. Koh et al. (2024b) addressed this in sandbox environments by storing action sequences to resume states after resetting the environment. However, resetting the environment or undoing action sequences is not feasible on live websites. Finally, the extra explorations introduced by search algorithms substantially amplify the risk of destructive actions that may irreversibly alter the website's state, potentially causing harmful side effects.", "2.2 WORLD MODELS": "World models, a cornerstone of model-based reinforcement learning (Moerland et al., 2023) since the introduction of Dyna by Sutton (1991), are typically trained on observed state transitions to predict future states and rewards. These world models enable efficient training through simulated experiences, reducing environmental interactions and improving sample efficiency (Ha & Schmidhuber, 2018). Beyond their role in training, researchers have explored the use of world models to"}, {"title": "3 PRELIMINARY", "content": "3.1 TASK FORMULATION\nWeb agents tasked with automating activities in live websites confront vast and complex search spaces. Formally, each task with a task instruction I can be framed as a partially observable Markov decision process (POMDP): (S, A, O, T, R, \u03a9), where S represents the set of all possible states of the environment, A represents all possible actions the agent can take, O represents the set of possible observations from the environment, T : S \u00d7 A \u2192 S represents the state transition function, R is a binary reward denoting whether the task specified in I has been completed or not, and \u03a9 : S \u2192 O is a deterministic function that projects a state to an observation. The goal of the task is to execute a sequence of actions that achieves a reward of 1.\nIn practical scenarios, the environment is partially observable due to the complexity of web environ-\n3.2 PLANNING THROUGH SIMULATION\nPlanning an optimal action sequence through tree search using real interactions governed by T is costly and risks irreversible actions. Model-based planning addresses these challenges by using a computational representation of the environment to simulate interaction outcomes. Instead of executing actions in the real environment, the agent leverages an approximate model to predict state transitions, enabling efficient exploration and evaluation of action sequences without real-world interactions. While offline planning can compute entire action sequences before execution in deterministic environments like BlocksWorld (Hao et al., 2023), web environments are too complex for such long-term prediction. This necessitates online planning approaches that interleave planning and execution, computing one action at a time.\nOne prominent approach is Model Predictive Control (MPC; Garcia et al. (1989)), which iteratively simulates future trajectories to select actions. At each state s, MPC simulates trajectories over a"}, {"title": "4 WEBDREAMER: MODEL-BASED PLANNING FOR WEB AGENTS", "content": "In this paper, we propose WEBDREAMER, a pioneering approach leveraging LLMs as world models to enable efficient planning in complex digital environments. Our approach is motivated by the observation that web interfaces, despite their complexity, are designed to be predictable for human users. When browsing websites, humans can effectively anticipate action outcomes based on visual cues and common design patterns-clicking a \u201cSubmit\u201d button leads to form submission, selecting a product image navigates to its detail page. Given that LLMs are trained on vast amounts of web-related data, we hypothesize that they have acquired sufficient knowledge to simulate the consequences of user actions, potentially serving as effective world models for planning.\n4.1 CORE DESIGN\nWEBDREAMER follows the planning through simulation paradigm introduced in Section 3.2. Figure 2 illustrates this process with three candidate actions, where WEBDREAMER simulates two-step trajectories for each action, selects the trajectory with the highest score, and executes its corresponding initial action. At its core, WEBDREAMER leverages an LLM to implement both the simulation function sim and the scoring function score.\nImplementation for sim: Our implementation of sim consists of two modules: one predicts state changes after action execution, approximating T, while the other imagines a possible action based on the predicted state. Together, these two modules generate trajectories of length H, where H"}, {"title": "5 EXPERIMENTS", "content": "5.1 SETUP\nTo properly test our planning framework's real-world performance, we use benchmarks with on-line evaluation, capturing the dynamic nature of web interactions. We focus on two representative benchmarks: VisualWebArena (VWA; Koh et al. (2024a)), which emphasizes a multimodal setting, and Mind2Web-live (Pan et al., 2024b), which operates with HTML by default. VWA comprises 910 tasks across three locally hosted websites: Shopping, Classifieds, and Reddit. In contrast, Mind2Web-live includes 104 tasks spanning 69 real-world websites. We adhere to the default settings of both benchmarks: for VWA, we use screenshots with Set-of-Marks prompting as the observation space, while for Mind2Web-live, we use HTML. For our LLM, we choose the most advanced multimodal LLM available, GPT-4o, as it best serves our aim to pioneer model-based planning with LLMs and explore the full potential of this envisioned paradigm. In our experiments, we empirically set the planning horizon H to 1. A comprehensive analysis of this parameter is presented in Section 6.1.\nTo demonstrate the effectiveness of our proposal, we primarily compare our approach with two major baselines: the reactive agent and the tree search agent with real interactions. While we can readily implement our own method for both benchmarks, for the tree search baseline (Koh et al., 2024b), we can only compare with it on VWA, because of the infeasibility of doing tree search in real-world websites in Mind2Web-live. Specifically, in VWA, Koh et al. (2024b) keeps track of the sequences of actions to get to states in previous trajectories. During backtracking, they reset the sandbox and re-execute the action sequence to restore the state. However, resetting the environment to undo effects is not always feasible in real-world websites featured in Mind2Web-live.\n5.2 MAIN RESULTS\nEffectiveness. We present the overall performance results in Table 2. WEBDREAMER demonstrates substantial improvements over the reactive agent on both VWA and Mind2Web-live datasets. Notably, on the VWA dataset, our proposed method achieves a 33.3% relative performance gain. Meanwhile, our proposal still underperforms the tree search baseline in terms of overall success rate. Despite these improvements, our approach still falls short of the tree search baseline in terms of overall success rate. However, it is crucial to emphasize that tree search is not a practical option for real-world websites, whereas WEBDREAMER provides a more flexible and adaptive alternative. On Mind2Web-live, WEBDREAMER outperforms the reactive baseline by 2.9% (a relative gain of 13.1%), which is less significant than the improvement on VWA. However, it is worth noting that the Mind2Web-live dataset does not offer as much discriminative power, as evidenced by the minimal performance differences across multiple base LLMs shown in Table 2. The strong results on both VWA and Mind2Web-live indicate the effectiveness of our method across different observation settings.\nWe further conduct a more granular analysis comparing our proposed method to the reactive baseline on the VWA dataset across multiple dimensions. Table 3 demonstrates that our model-based planning approach consistently outperforms the reactive baseline across all websites and task difficulty levels. On tasks of medium difficulty according to the official annotation by VWA, model-based planning even surpasses the performance of tree search (i.e., 22.2% vs. 24.1%). Despite its promise, model-based planning still struggles with hard tasks in VWA that necessitate multistep simulations."}, {"title": "6 ANALYSES", "content": "6.1 STATE REPRESENTATION AND PLANNING HORIZON\nOur model-based planning approach relies on two critical dimensions for simulation: the state representation and the planning horizon (i.e., the simulation depth). To gain deeper insights into its effectiveness and limitations, we investigate how various configurations affect the final performance. Given the high computational cost of these experiments, we conduct this analysis using a subset of the VWA dataset, comprising 100 shopping tasks with officially annotated human trajectories.\nIn addition to the state change description used in our primary experiments, we explore alternative approaches where GPT-4o predicts either the HTML code or the accessibility tree of the resulting webpage within the simulation. For each of these state representations, we evaluate planning horizons of 1, 2, and 3 steps. As depicted in Figure 4, all three state representations significantly outperform the reactive baseline. However, their effectiveness diminishes as the planning horizon extends to 3 steps, indicating a common limitation in long-horizon simulation across these approaches. Specifically, the action proposal within the simulation tends to hallucinate relevant actions for task completion, even when such actions may not exist in the current state predicted by the LLM. Notably, the state change representation exhibits the most pronounced performance degradation as planning horizons extend. This decline is particularly severe with a planning horizon of 3, where performance falls below that of the reactive baseline. This vulnerability stems from its implicit specification of available interactive elements on the current webpage, requiring the model to infer these elements by applying changes to the initial state. In contrast, HTML and accessibility tree representations provide explicit element information. Consequently, the state change approach is more susceptible to hallucination during extended simulations. Despite this limitation, the state change approach remains a viable choice given the current capabilities of LLMs. It matches the performance of HTML and accessibility tree representations for planning horizons less than 3 while consuming fewer output tokens.\n6.2 ABLATION STUDY\nTo determine if the observed improvements come from specific parts of our model-based planning approach, we perform ablation studies on the simulation and self-refinement stages, using the same subset from Section 6.1. We pay special attention to the"}, {"title": "7 CONCLUSION", "content": "In this paper, we demonstrate the strong potential of using LLMs as world models to support planning in complex environments. Specifically, our model-based planning approach, WEBDREAMER, shows substantial improvement over reactive baselines and offers greater flexibility than tree search, which is often impossible in real-world websites. As a pioneering effort in this area, our work opens new avenues for model-based planning with LLM-simulated world models. Future work can focus on further optimizing LLMs as world models for complex environments and developing more robust model-based planning algorithms for long-horizon planning.\nLIMITATIONS\nOur study, as a pioneering exploration of MPC-based planning with LLMs for web navigation, naturally comes with several limitations, which are also exciting future research directions:\nSimplicity of Planning Algorithm. In this preliminary work, we deliberately employed a straightforward planning algorithm to demonstrate the core potential of our approach. While effective, this simplicity leaves ample room for future enhancements. More sophisticated planning techniques, such as Monte Carlo Tree Search (MCTS), could be integrated to further improve performance. As a foundational study, our focus was on establishing the viability of the concept rather than optimizing every aspect of the system. This strategic choice allows future research to build upon our findings and explore more advanced planning strategies within the framework we've established.\nComputational Cost. Our current implementation, utilizing state-of-the-art models like GPT-4o, incurs non-trivial API costs (approximately $1 per task on VWA). This cost reflects our prioritization of exploring the full potential of LLM-based planning without immediate constraints. For practical applications, future work could investigate cost-effective alternatives such as fine-tuning specialized models for simulation tasks. This sets a benchmark for future optimizations that balance performance and efficiency.\nThese limitations underscore the nature of our work as a proof of concept, opening up numerous avenues for future research and optimization. By establishing the foundational potential of MPC-based planning with LLMs, we have laid the groundwork for a new planning paradigm for LLM-based language agents, inviting further innovations that can refine and extend model-based planning."}, {"title": "A PROMPTS FOR FOUR STAGES IN MPC-BASED PLANNING", "content": "A.1 ACTION PROPOSAL\nAction Proposal\nYou are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\nHere's the information you'll have: {Web Information}\nThe user's objective: Task Objective} This is the task you're trying to complete.\nThe current web page screenshot: Web Page Screenshot Image} This is a screenshot of the webpage, with each interactable element assigned a unique numerical id. Each bounding box and its respective id shares the same color.\nThe observation, which lists the IDs of all interactable elements on the current web page with their text content if any, in the format [id] [tagType] [text content). tagType is the type of the element, such as button, link, or textbox. text content is the text content of the element. For example, [1234] [button] ['Add to Cart'] means that there is a button with id 1234 and text content 'Add to Cart' on the current web page. [ ] [StaticText] [text] means that the element is of some text that is not interactable.\nThe current web page's URL: {Web URL} This is the page you're currently navigating.\nThe open tabs: {Previous Tabs} These are the tabs you have open.\nThe previous action: {Previous Action} This is the action you just performed. It may be helpful to track your progress.\nThe actions you can perform fall into several categories:\nPage Operation Actions:\n- click [id]: This action clicks on an element with a specific id on the webpage.\n type [id] (content): Use this to type the content into the field with id. By default, the Enter key is pressed after typing unless press_enter after is set to 0, i.e., type [id] [content] [0].\nhover [id]: Hover over an element with id.\n-press [key_comb]: Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+V)\n-scroll [down] or scroll [up]: Scroll the page up or down.\nTab Management Actions:\nnew_tab: Open a new, empty browser tab.\ntab_focus (tab_index): Switch the browser's focus to a specific tab using its index.\nclose_tab: Close the currently active tab.\nURL Navigation Actions:\ngoto [url]: Navigate to a specific URL.\ngo_back: Navigate to the previously viewed page.\n- go_forward: Navigate to the next page (if a previous go_back action was performed).\nCompletion Action:\n- stop [answer]: Issue this action when you believe the task is complete. If the objective is to find a text-based answer, provide the answer in the bracket.\nHomepage:\nIf you want to visit other websites, check out the homepage at http://homepage.com. It has a list of websites you can visit. http://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites.\nTo be successful, it is very important to follow the following rules:\n1. You should only issue an action that is valid given the current observation\n2. You should only issue one action at a time.\n3. You should follow the examples to reason step by step and then issue the next action.\n4. Generate the action in the correct format. Start with a \"In summary, the next action I will perform is\" phrase, followed by action. For example, In summary, the next action I will perform is click [1234].\n5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop."}, {"title": "A.2 SELF-REFINEMENT", "content": "Self-Refinement\nYou are assiting a web navigation agent to help a human user navigate a website to complete a task. Given the user's intent, the action history, and the current state of the webpage, the agent has proposed a set of candidate actions to take at the current step.\nYour role is not to determine a best action for the agent at this step, but to filter out the actions that are very likely not relevant or helpful for the agent to accomplish the task.\nPlease select all actions that you think that could possibly lead the agent to accomplish the task. It's important to note that to accomplish a task, the agent will execute a sequence of actions. So the action to take at this step does not have to immediately lead to the completion of the task. You should select any action that could be relevant for the agent to take in the current state of the webpage. Try to be as thoughtful and comprehensive as you can! Don't miss any possible action. If there is one action that is clearly the best, and all other actions are clearly not very relevant, you can only select one action. Please do this sparely, since some actions may be helpful in a longer horizon.\nA action should be included as long as it could be relevant to the task, even if it may not be the most direct action to take at this step!! Some relevant actions might seem indirect at the first glance, but could be helpful in a longer horizon. Please also include those actions.\nPlease at least select one action.\n*IMPORTANT*\nFormat your response into two lines as shown below:\nThoughts: <your thoughts and reasoning process>. You must explicitly evaluate each action one by one and imagine whether it could be relevant to the task following the format:\naction:... rationale:...\nSelected actions: id0; id1;aid2;... (please return the index of the action in the candidate actions list, starting from 0. Don't output the action description itself. Separate the indices with semicolons. Do not add spaces or any other characters between after the semicolons.)\nAction History: {last_actions_str}\nCurrent URL: {current_url}\nThe images corresponding to the user intent are shown in the FIRST {len (intent_images)} images (before the User Intent).\nThe last {len (screenshots)} snapshots of the agent's trajectory are shown in the LAST {len(screenshots)} images. The LAST IMAGE represents the current state of the webpage.\nProposed Action: {action_descriptions}"}, {"title": "A.3 WORLD MODEL", "content": "World Model\nYou are an agent that predicts the effect of an action on a webpage. You will be given a screenshot of a webpage, a sequence of actions and state changes applied to the initial screenshot, and an operation to perform on the webpage. You are required to predict the new changes that will occur on the webpage after the operation is performed, such as the appearance of new elements, the disappearance of existing elements, or changes in the content of existing elements. The operation type and the element to operate will be provided in the prompt. Directly output State changes: ... and don't output anything else. Try to be as comprehensive and detailed as possible.\nBased on the initial screenshot and the changes to the webpage, please predict the changes after action:"}, {"title": "A.4 REWARD MODEL", "content": "Reward Model\nYou are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human user navigate a website to complete a task. Given the user's intent, the agent's action history, the current state of the webpage, your goal is to decide **whether the simulated steps by the agent indicate a successful execution of the user intent**. In particular, if the predicted state (i.e., the current state represented by the last image plus all the predicted changes so far) corresponds to a successful final state. If it is a failure but it looks like the simulated steps are on the right track towards success, you should also output as such. Note that, in the simulated steps, all the state changes are predicted by the agent's world model, and they may not actually be faithful to the real website interactions (e.g., some proposed actions may not be avaiable in a realistic website). You should also account for this in your evaluation (e.g., if the predicted state changes are not reasonable then it's probably a failure).\n*IMPORTANT*\nFormat your response into two lines as shown below:\nThoughts: <your thoughts and reasoning process>\nStatus: \"success\" or \"failure\"\nOn the right track to success: \"yes\" or \"no\""}]}