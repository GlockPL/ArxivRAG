{"title": "LaRA: Benchmarking Retrieval-Augmented Generation\nand Long-Context LLMs - No Silver Bullet for LC or RAG Routing", "authors": ["Kuan Li", "Liwen Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Shuai Wang", "Minhao Cheng"], "abstract": "Effectively incorporating external knowledge into\nLarge Language Models (LLMs) is crucial for\nenhancing their capabilities and addressing real-\nworld needs. Retrieval-Augmented Generation\n(RAG) offers an effective method for achieving\nthis by retrieving the most relevant fragments\ninto LLMs. However, the advancements in\ncontext window size for LLMs offer an alternative\napproach, raising the question of whether\nRAG remains necessary for effectively handling\nexternal knowledge. Several existing studies\nprovide inconclusive comparisons between RAG\nand long-context (LC) LLMs, largely due to\nlimitations in the benchmark designs. In this\npaper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG\nand LC LLMs. LaRA encompasses 2,326 test\ncases across four practical QA task categories\nand three types of naturally occurring long\ntexts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we\nfind that the optimal choice between RAG\nand LC depends on a complex interplay of\nfactors, including the model's parameter size,\nlong-text capabilities, context length, task type,\nand the characteristics of the retrieved chunks.\nOur findings provide actionable guidelines for\npractitioners to effectively leverage both RAG and\nLC approaches in developing and deploying LLM\napplications. Our code and dataset is provided at:\nhttps://github.com/likuanppd/LaRA.", "sections": [{"title": "1. Introducion", "content": "While large language models (LLMs) excel across various\ndomains, the dynamic nature of information poses\nsignificant challenges to their ability to acquire new\nknowledge effectively. Current studies reveal several\nlimitations of LLMs, including high computational costs\nwhen processing long texts, a tendency to produce factual\nerrors and hallucinations, difficulty adapting to specialized\ndomains, and a propensity for generating overly generic\nresponses (Gao et al., 2023). To address these limitations,\nresearchers have explored retrieval-augmented generation\n(RAG) (Guu et al., 2020; Lewis et al., 2020). RAG\nenables LLMs to efficiently utilize external knowledge\nby retrieving the most relevant fragments from uploaded\ndocuments, knowledge bases, or websites. However, recent\nadvancements in LLMs, such as GPT-40 (OpenAI, 2024),\nLlama 3.2 (Meta, 2024a), Claude 3.5 (Anthropic, 2024),\nand Qwen 2.5 (Yang et al., 2024), now support input lengths\nof up to 128k tokens, offering an alternative by directly\nfeeding the full context of relevant information into the\nmodel. This raises questions about the continued necessity\nof RAG, which was initially crucial for handling long texts,\nsince these models can now potentially access and process\nthe necessary information directly. Therefore, it is essential\nto systematically compare the strengths and weaknesses of\nRAG and long-context (LC) LLMs.\nNumerous studies have investigated the performance\ndifferences between RAG and providing LLMs with full\nlong contexts. For example, Xu et al. find that RAG\noutperforms LC on several traditional QA datasets. More\nrecently, Li et al. argued that LC consistently outperforms\nRAG in almost all settings. However, Yu et al. subsequently\nconducted experiments demonstrating that RAG is not\ninherently weaker than LC. This lack of consensus likely\nstems from several flaws in the evaluation pipeline design\nof existing benchmarks, including issues with the corpus\n(e.g., excessively short text lengths, failed replacements),\nevaluation metrics, and impractical task design.\nTo address these issues and facilitate a robust comparison\nbetween RAG and LC, we propose LaRA, a benchmark\nfor evaluating Long-context LLMs competing againt RAG.\nIn constructing LaRA, we adhere to the following criteria:\n(1) Context length should be maximized within the LLMs'\ninput limits to avoid truncation that could obscure the true\ncapabilities of the models. (2) The context should consist"}, {"title": "2. Revisiting RAG vs. Long Context\nBenchmarks", "content": "Desipte a lot of benchmarks has been used to compare\nRAG with feeding lanuage model with long context, there\nlacks a clear guidelines and conclusion on when and where\nthe RAG will be a better choice than long context. Xu\net al. and Bai et al. draw opposing conclusions on whether\nRAG or LC performs better on traditional QA datasets.\nRecently, Li et al. argued that LC consistently outperforms\nRAG in almost all settings, and Yu et al. subsequently\nclaim RAG can defeat LC on the same benchmark. In\nthis section, we conduct a detailed analysis on the existing\nbenchmark and analysis, and find the key issues are stem\nfrom some significant flaws with their evaluation pipeline.\nFor simplicity, our analysis is mainly limited to question\nanswering tasks based on long contexts."}, {"title": "2.1. Issues with External Contexts", "content": "Insufficent context lengths. As LLM base models\ncontinue to evolve, the definition of \"long context\" has\nalso shifted, expanding from the early limit of 4k tokens to\nthe now commonly supported 128k context length. Some\nearly work utilize datasets such as Qasper (QASP) (Dasigi\net al., 2021), NarrativeQA (NQA) (Kocisk\u00fd et al., 2018),\nand QUALITY (QLTY) (Pang et al., 2022) to compare\nRAG and LC. For instance, Xu et al. conduct experiments\non these datasets and find that RAG can strengthen large\nmodels, such as Llama2-70B and GPT-4-3B. Similarly, Lee\net al. combine these datasets to create a new benchmark for\nfurther evaluations. However, such datasets no longer align\nwith the current definition of long context. For example,\nQASP and QLTY have average context lengths of only\n4,912 and 6,592 tokens, respectively, which are far below\nthe context length capabilities of modern LLMs. Moreover,\nRAG typically uses chunk sizes of 300-600 tokens, and\nwith 5-20 retrieved chunks, the total context length in RAG\nbecomes comparable to that of full-context input, reducing\nthe distinction between the two approaches.\nData Leakage. Since LLMs use more and more datasets\nin the training procedure, the problem of data leakage\nbecomes more serious. At the same time, it is challenging to"}, {"title": "2.2. Inaccurate Evaluation", "content": "Unreasonable metrics. Many previous evaluations use\nautomated metrics such as F1-score and exact match"}, {"title": "3. LaRA", "content": "In this section, we introduce the construction of LaRA and\nhow it addresses the issues present in previous benchmarks,\nas mentioned in Section 3. The statistics of LaRA are\nprovided in Appendix A."}, {"title": "3.1. Long Context Data Collection", "content": "In our context selection process, we adhere to the following\nprinciples: (1) Timeliness: We select recent high-quality\nlong contexts to prevent data leakage issues, ensuring that\nthey are less likely to have been included in the LLM's\ntraining data. (2) Appropriate Length: Considering that\nmainstream commercial and open-weight models typically\nsupport context length of 32k and 128k, we choose contexts\nthat are as close to these window sizes as possible without\nexceeding them. (3) Naturalness: The chosen contexts are\nnaturally occurring long documents, rather than artificially\nconstructed or assembled from unrelated short texts, to\nensure the benchmark reflects the complexity and diversity\nof real-world use. (4) Authoritativeness: All contexts are\nconsidered reliable and credible sources of information due\nto expertise, reputation, and qualifications of the authors or\ninstitutions behind them.\nTo ensure a diverse range of contexts, we select novels2,\nfinancial statements\u00b3, and academic papers as the context.\nFor novels, we choose the txt format of novelettes and\nnovels to serve as the 32k and 128k contexts, respectively.\nFinancial statements include the latest quarterly reports\n(32k) and annual reports (128k) from publicly listed\ncompanies in the United States for the year 2024. To create\ncontexts of appropriate length for academic papers, we\ncombine several papers published on arXiv in 2024 that\nare related through citations.\nEntity Replacement. To mitigate the risk of data leakage\nfrom novels, which are likely present in LLMs' training\ndata, we perform entity replacement. Previous work has\nemployed similar strategies (Zhang et al., 2024a; Li et al.,\n2022), but we find that many entity replacements were\nincorrect or inconsistent, leading to inaccurate evaluations.\nTo address this, we use GPT-40 to accurately identify and\nreplace character entities as well as formulating questions\ntargeting the replaced entities, ensuring consistency between\nthe novel text and the questions. Details are provided in\nAppendix B."}, {"title": "3.2. Tasks in LaRA", "content": "To comprehensively evaluate the capabilities of LC LLMs\nand RAG, LaRA includes four major task categories:\nlocation, reasoning, comparison, and hallucination\ndetection, which are designed to assess distinct aspects of\nLLM performance, motivated by the need to assess both\nthe strengths and weaknesses of RAG and LC in handling\ncomplex, real-world information needs. Below, we will\nintroduce each task in detail and further elaborate on the\nmotivation behind them. Examples of each task are provided\nin Appendix F."}, {"title": "Location.", "content": "The location task, the most fundamental task\nin LaRA, evaluates an LLM's ability to locate specific\ninformation within a long context. In this task, the answer\nresides in a single sentence or paragraph within a long\ncontext, and no additional reasoning or computation is\nrequired to formulate a correct response, such as identifying\na character's name or a specific value mentioned in the text.\nIt is worth noting that the location task differs from the\n\"Needle in a Haystack\u201d problem (Kamradt., 2023), which\nfocuses on verbatim matching. In contrast, the location task\nallows for paraphrasing, as long as the underlying meaning\nis preserved. This task is crucial for assessing an LLM's\nbasic comprehension and information retrieval capabilities\nwithin a long context."}, {"title": "Reasoning.", "content": "The reasoning task in LaRA involve questions\nthat require logical deduction, inference, or calculation\nbased on the information provided in the long context.\nInstead of directly extracting answers from the text, these\ntasks demand a deeper understanding and processing of the\ninformation to derive the correct answer, such as inferring\nthe relationship between two characters or calculating\nrelevant data in financial statements. These tasks evaluate\nthe ability of LC and RAG to handle complex questions,\nparticularly in scenarios where the long context contains a\nsignificant amount of noise irrelevant to the question. The\nspecific questions vary significantly depending on the type\nof context involved. Instead of explicitly defining sub-task\ntypes, we adopt different seed questions tailored to specific\ntext types. These seed questions are used to generate similar\nQA pairs through in-context learning. For example, in\nfinancial statements, which contain a significant amount\nof statistical data, we focus on computational questions, and\nfor novels, the questions involve reasoning about the plot or\ncharacter traits."}, {"title": "Comparison.", "content": "The comparison task in LaRA evaluates\nthe ability of RAG and LC to synthesize information from\nmultiple parts of a long context, comparing their content or\nnumerical values to arrive at the final answer. Crucially, the\ncomparison task also involves manually designing different\nseed questions tailored to various text types. This approach\nensures that the generated questions are not only relevant\nbut also reflect the nuances and complexities of the specific\ncontext. For instance, in academic papers, the questions\nmay focus on comparing different explanations of the\nsame phenomenon, while in novels, they may compare\nchanges in a character's traits or appearance over time. This\ntask is essential for assessing an LLM's ability to extract\ninformation from different parts."}, {"title": "Hallucination detection.", "content": "Hallucination, a common issue\nin LLMs, occurs when the model generates inaccurate\nor irrelevant information (Huang et al., 2023). The\nhallucination detection task aims to test the model's ability\nto decline answering questions that are not mentioned in"}, {"title": "3.3. Data Annotation", "content": "The annotation process for different tasks follows a similar\nframework, starting with the manual creation of seed\nquestions and answers. We then utilize GPT-40 to generate\nnew QA pairs through in-context learning. A subset of\nnewly generated QAs is sampled for manual validation to\nensure correctness and practicality. If the pass rate does not\nmeet a predefined threshold, the seed QAs and prompts are\nrefined, followed by re-generation and re-validation. We\nprovide the annotation prompt in Appendix E.\nAnnotating long texts presents a unique challenge due to\nthe inherent difficulty of long context processing. One\neffective approach to improve generation quality is to\nconvert annotations for long texts into annotations for\nshorter texts. To achieve this, we employ various strategies\ntailored to different context types and tasks. Specifically, for\nlocation and reasoning tasks, we split the long context into\nmultiple segments, each approximately 10k tokens in length,\nand input them individually into GPT-40 to generate QAs.\nThis approach serves multiple purposes: First, it reduces the\ncognitive load on the annotator (GPT-40 here) and improves\nthe focus and accuracy of the generated QA pairs. Second,\nit ensures that the answers are evenly distributed across the\nentire context, as we observe that providing the full context\nto the LLM often results in answers being concentrated\nat the beginning and end of the context. Third, it allows\nus to examine the relationship between answer accuracy\nand answer location, enabling us to investigate whether the\nLLM suffers from the \"lost in the middle\" issue, where\nperformance declines for information located in the middle\nsections of long documents (Liu et al., 2024a). For the\ncomparison task, we split the context into smaller segments\nand then sample two segments to generate comparison\nquestions similar to the seed questions. Meanwhile, our\nsegmentation strategies are tailored to the specific context\ntype to preserve the inherent structure and coherence of the\ndocuments. For research papers, we separate concatenated\npapers to maintain the integrity of each individual paper.\nFor novels and financial statements, we directly split the\ntext into multiple segments based on token count."}, {"title": "3.4. Evaluation", "content": "Metrics. Automated evaluation metrics, such as F1-score\nand ROUGE, can often produce lower scores, while LLM\nevaluations for QA tasks with definitive answers have\ndemonstrated high precision (Chiang & Lee, 2023; Liu\net al., 2023). Therefore, we provide GPT-40 with the query,\nthe ground-truth answer, and the prediction, enabling it to\nassess the correctness of the response (details and prompt\nare provided in Appendix C). Since LaRA consists solely\nof QA pairs with clearly defined answers and no open-\nended questions, using LLM as a judge is highly effective in\nensuring accuracy and consistency in the evaluation process.\nManual Verification. To ensure the quality and reliability\nof LaRA, we incorporate manual verification at two stages\nof the construction process. First, during the generation\nprocess, we employ a pipeline involving sampling, prompt\nrefinement, and seed QA selection as manual adjustments.\nWe find that the choice of seed questions has the most\nsignificant impact, possibly because LLMs perform much\nbetter in in-context learning compared to zero-shot question\ngeneration, demonstrating the importance of providing\nrelevant examples for guiding the generation process (For\ndetails, see Appendix E). Second, we calculate the Cohen's\nKappa coefficient between the evaluation from LLM and\nhuman to quantitatively assess the agreement between the\nLLM and human evaluations, ensuring consistency and\nreliability in the judgment process. We provide the details\nin Appendix C."}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nBaselines. To investigate the impact of various factors\non RAG and LC performance, we evaluate a diverse set of\n11 LLMs, encompassing both open-source and proprietary\nmodels. This includes seven open-source LLMs: Llama-3.2-\n3B-Instruct (Meta, 2024a), Llama-3.1-8B-Instruct (Meta,\n2024b), Llama-3.3-70B-Instruct (Meta, 2024c), Llama-3.3-\n70B-Instruct-Q8 (Meta, 2024c) (utilizing FP8 quantization),\nQwen-2.5-7B-Instruct (Yang et al., 2024), Qwen-2.5-72B-\nInstruct (Yang et al., 2024), and Mistral-Nemo-12B (AI,\n2024). We also evaluate four advanced proprietary LLMs:\nGPT-40 (OpenAI, 2024), GPT-40-mini (OpenAI, 2024),\nClaude-3.5-Sonnet (Anthropic, 2024), and Gemini-1.5-\nPro (Reid et al., 2024).\nImplementation of RAG. Our evaluation employs a\nstandardized configuration with a chunk size of 600 tokens,\n5 chunks per document, and an overlap of 100 tokens\nbetween chunks. We utilize GTE-large-en-v1.5 (Zhang\net al., 2024b; Li et al., 2023) for embedding extraction\nand employ a hybrid search strategy combining embedding"}, {"title": "4.2. Main Results and Analysis", "content": "Overall performance. Our analysis reveals a\ncomplex relationship between model architecture, context\nlength, and performance. For open-source models at a\n32k context length, LC generally outperforms RAG, with\nthe exception of Llama-3.2-3B-Instruct and Mistral-Nemo-\n12B. However, this trend reverses at a 128k context length,\nwhere RAG demonstrates superior performance across\nmost models. In contrast, proprietary models consistently\nfavor LC at both context lengths, likely due to their larger\nparameter sizes and enhanced ability to process long-\ncontext inputs. The inherent self-attention mechanism in\nthese models appears more effective at handling extended\ncontexts compared to the sparse attention employed in\nRAG. Notably, at a 128k context length, the top three\nperforming models (GPT-40, Gemini-1.5-Pro, and Claude-\n3.5-Sonnet) all utilize LC, while the bottom three (Llama-\n3.2-3B-Instruct, Qwen-2.5-7B-Instruct, and Mistral-Nemo-\n12B) are also LC-based. This observation underscores the\nabsence of a universal \"winner\" between RAG and LC, as\nperformance is highly dependent on the specific LLM and\ncontext length."}, {"title": "Scaling law holds in LC.", "content": "Our experimental results\nconfirm the established scaling law in LC (Kaplan et al.,\n2020): larger models consistently outperform smaller\ncounterparts. For example, GPT-40 and Qwen-2.5-72B-\nInstruct show significant performance gains ranging from\n7.35% to 16.2% over their smaller versions, GPT-40-mini\nand Qwen-2.5-7B-Instruct, respectively. This advantage is\nfurther amplified at a 128k context length. While all models\nexperience a performance decline with longer contexts, the\ndrop is more pronounced for smaller models, highlighting\ntheir limitations in processing extensive textual input. This\nobservation challenges the purported ability of smaller\nmodels to handle extremely long contexts effectively."}, {"title": "RAG empowers models to handle extremely long\ncontext.", "content": "At a 128k context length, RAG consistently\noutperforms LC across nearly all open-source models.\nFor example, Llama-3.1-8B-Instruct and Qwen-2.5-7B-\nInstruct demonstrate improvements of 0.15% and 7.39%,\nrespectively, when using RAG instead of LC. All models\nexhibit a performance decline at a 128k context length\ncompared to 32k. However, LC experiences a more\nsignificant drop than RAG, indicating that as context length\napproaches its limit, RAG is less affected by the increase\nin context length. Furthermore, RAG enables models with\nweaker long-context capabilities, such as Mistral-Nemo-\n13B and Llama-3.2-3B-Instruct, to achieve performance\ncomparable to other models. These findings highlight that\nwhile larger models excel at long-context processing, RAG\noffers an effective alternative for smaller or weaker models,\nensuring competitive performance even with extended\ncontext lengths."}, {"title": "4.3. Task Analysis", "content": "Table 2 presents a detailed breakdown of LC and RAG\nperformance across four distinct tasks. To provide a clear\noverview of the relative strengths of each task, we also\ncalculate the average performance gap (avg gap) for each\ntask, representing the mean difference in accuracy between\nLC and RAG across all evaluated models."}, {"title": "Location.", "content": "The location task proves to be the easiest among\nthe four, with both RAG and LC achieving high accuracy.\nThe average performance gap between RAG and LC is 0.08%\nat a 32k context length and -5.25% at 128k, indicating a\nslight advantage for LC with shorter contexts and a more\npronounced advantage for RAG with longer contexts. For\nopen-source models at 32k, the performance difference\nbetween RAG and LC is minimal, while at 128k, RAG\ndemonstrates superior performance. This suggests that\nwhen models struggle to process long texts, retrieval acts\nas a valuable tool for location-based questions. Conversely,\nfor proprietary models, LC consistently outperforms RAG,\nindicating that with sufficient model capacity, LLMs can\noutperform RAG in handling such simple tasks on their own."}, {"title": "Reasoning.", "content": "The performance trends for reasoning tasks\nbasically mirror those observed in the location tasks,\nparticularly for smaller models. With a 32k context, RAG\nexhibits slightly lower accuracy compared to LC, while\nat 128k, this trend reverses. However, for larger models\nlike GPT-40 and Claude-3.5-Sonnet, the advantage of LC\nbecomes more pronounced. At a 128k context length,\nGPT-40 and Claude-3.5-Sonnet outperform RAG by 9.09%\nand 8.98% in accuracy, respectively. We speculate that\nwhile reasoning tasks often rely on specific text segments\nfor answers, other parts of the document may contain\nsupplementary information that aids in inference. Models\nwith stronger long-context capabilities are better equipped\nto leverage this global knowledge, leading to improved\nperformance in reasoning tasks."}, {"title": "Comparison.", "content": "Comparative tasks pose the greatest\nchallenge for RAG, showing the largest performance gap\ncompared to LC. The average gap reaches 15.22% at 32k\nand 14.30% at 128k. A deeper analysis of problematic\ncases reveals two primary reasons for the struggle of RAG.\nFirst, some comparative questions emphasize one aspect\nof the comparison while providing limited information\nabout the other, making it difficult for RAG to retrieve\nboth necessary chunks for a correct answer. Second, certain\nqueries describe the comparison in abstract terms rather than\nconcrete details, hindering the effectiveness of similarity-\nbased retrieval. This abstraction makes it challenging for\nRAG to locate all the relevant chunks for comparison.\nIn contrast to location tasks, which involve pinpointing\na single piece of information, comparative tasks require\nthe accurate retrieval and comparison of multiple distinct\nchunks, significantly increasing the complexity for RAG."}, {"title": "Hallucination detection.", "content": "This is the only task where RAG\ndemonstrates a clear advantage in both small and large models.\nLC tends to generate more hallucinated or incorrect answers,\nlikely due to the increased noise introduced by feeding\nthe entire text to the model. This makes the model more\nsusceptible to errors and distractions, leading to fabricated\nresponses. Interestingly, larger models do not exhibit an\nevident advantage in this task. While GPT-40 achieves top\nperformance on other tasks, it only attains an accuracy of\n56.34% on hallucination detection. This suggests that even\nmodels capable of handling long contexts can be overwhelmed\nby the sheer volume of information and generate flawed\nconclusions. In contrast, RAG's selective retrieval of relevant\ninformation helps mitigate this issue, enabling both small\nand large models to better identify situations where they lack\nsufficient knowledge to provide an accurate answer."}, {"title": "4.4. Context Type Analysis", "content": "We present the influence of different context types on the\nperformance of RAG and LC in Figure 1. Novel-related\nquestions pose the greatest challenge, while paper-related\nquestions are the easiest. This disparity likely stems from\nthe repetitive sentence structures common in novels, which\ncan hinder precise information localization. Conversely,\nacademic papers typically exhibit a stronger logical flow\nand higher information density, facilitating easier distinction\nbetween questions and answers. At a 32k context length, LC\noutperforms RAG for nearly all models. However, at 128k,\nweaker models demonstrate better performance with RAG.\nInterestingly, the performance gap between RAG and LC is\nsmaller for novel-related tasks compared to paper-related or\nfinancial statements tasks, regardless of context length. This\nsuggests that for less structured contexts, RAG presents a\nviable alternative for reducing computational cost. On the\nother hand, for highly structured texts like academic papers\nand financial statements, LC demonstrates a clear advantage."}, {"title": "4.5. Impact of Chunk Quantity and Size", "content": "We explore the impact of the length of retrieved information\nlength on RAG performance through two aspects: the\nnumber of chunks and the size of each chunk. We conduct\nexperiments on Qwen-2.5-72B-Instruct and Qwen-2.5-7B-\nInstruct to observe the impact of chunk size and quantity on\nboth large and small models. As shown in Figure 2, for the\n72B model, performance improves consistently as the number\nof retrieved chunks increases, benefiting from its stronger\nlong-context processing capability. In contrast, the 7B\nmodel exhibits a peak performance at an intermediate chunk\nquantity, after which excessive retrieval introduces noise\nthat outweighs the information gain. Regarding chunk size,\nboth excessively large and small chunks lead to performance\ndegradation. Within a reasonable range, increasing chunk\nsize provides some improvement, though its effect is less\nsignificant than increasing the number of chunks."}, {"title": "4.6. Lost in The Middle", "content": "By controlling the location of the information required to\nanswer the questions, we find that LC LLMs exhibit a\ndecrease in accuracy when the answer is closer to the center\nof the context, indicating a susceptibility to the \"lost in the\nmiddle\" phenomenon. In contrast, we do not observe a clear\ncorrelation between RAG performance and the position of\nthe answer, suggesting that RAG models are more robust to\nthis issue. These findings highlight a potential advantage\nof RAG models in handling long contexts, particularly for\ntasks that require accessing information from various parts\nof the document. For detailed results, see Appendix D."}, {"title": "5. Conclusion", "content": "This study addressed the critical question of whether RAG or\nLC is more effective for incorporating external knowledge\ninto LLMs. Through the development and evaluation\nof LaRA, a novel benchmark, we demonstrated that the\noptimal choice depends on a complex interplay of factors,\nincluding model size, context length, and task type. Our\nfindings challenge previous inconclusive comparisons and\noffer actionable guidelines for practitioners. LaRA serves\nas a valuable resource for evaluating and comparing RAG\nand LC models, facilitating further research in this rapidly\nevolving field."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here."}, {"title": "A. Statistics of LaRA", "content": "LaRA consists of approximately 2300 test cases, encompassing three context types and four task categories. To ensure that\nthe token count of the context is as close as possible to 32k and 128k without exceeding these limits, the primary token\nranges for these two lengths are 20-30k and 80-120k, respectively. The average token counts and numbers of tasks are\nprovided in Table 3."}, {"title": "B. Named Entity Recognition and Replacement for Novels", "content": "We initially tried using some traditional Named Entity Recognition (NER) methods (Wang et al., 2021; 2022), but found that\ntheir performance was poor. First, traditional sequence labeling models struggle with long-text processing due to fixed-length\ncontext windows, failing to capture cross-paragraph entity associations. Second, performance degrades significantly on\nout-of-distribution data, particularly when handling domain-specific or stylistically unique texts (Li et al., 2022). These\nconstraints prove especially problematic for literary analysis, where novels exhibit both long-range narrative dependencies\nand rich variations in entity references (e.g., honorifics, epithets, and contextual substitutions).\nThe emergence of LLM presents new opportunities to overcome these limitations through their superior contextual\nunderstanding and robust generalization capabilities. Hence, we leverage GPT-40 to perform entity extraction and\nreplacement in full-length novels through a three-stage pipeline:\n1. We partition input texts into coherent segments averaging 500 tokens, preserving complete sentence/paragraph\nboundaries. This chunk size optimizes the balance between LLMs' context window constraints and narrative continuity\nrequirements.\n2. Each text chunk undergoes parallel entity recognition through GPT-40 processing. Our extraction protocol captures\nboth original names and contextual variants (e.g., \"The Dark Lord\u201d \u2192 \u201cVoldemort\" in Harry Potter). After feeding each\nchunk for extraction, we merge all the entities and remove duplicates to obtain a final list of entities while preserving\nlegitimate aliases.\n3. For alias disambiguation, we prompt GPT-40 with the entity list and the novel's title to determine if multiple names\nindicate the same character, and if they do, we replace them with the same fake name."}, {"title": "C. LLM as A Judge", "content": "Given the unreliability of rule-based evaluations and the high costs associated with human evaluation, the use of LLM\nfor assessment has gained increasing popularity (Liu et al., 2024b; Wang et al., 2024). In LaRA, we prompt GPT-40 to\ndetermine whether a model correctly answer a question, using the query, the ground-truth answer, and the model's prediction\nas inputs. The specific prompt is shown in Figure 5.\nTo verify the consistency between GPT-4o evaluation and human evaluation, we compute the Cohen's Kappa coefficient,\nwhich is defined as:\n$\\kappa = \\frac{P_o - P_e}{1 - P_e}$ (1)\nwhere $P_o$ represents the proportion of agreement between the two evaluators on the positive and negative classes, and $P_e$\ndenotes the probability of agreement between evaluators under the assumption of independent and random classification.:\n$P_o = \\frac{TP + TN}{TP + FP + TN + FN}$ (2)\n$P_e = \\frac{(TP + FN)(TP + FP) + (TN + FN)(TN + FP)}{(TP + FP+ TN + FN)^2}$, (3)\nwhere TP and TN refer to cases where both the human evaluator and the model agree that the answer is correct, while FP\nand FN refer to cases where only one of them judges the answer as correct."}, {"title": "D. Lost in The Middle", "content": "To assess the presence of \u201clost in the middle\" in our LaRA benchmark, we ensure a uniform distribution of answers across\ndifferent positions within the context during the annotation of location and reasoning tasks. Specifically, for location tasks"}, {"title": "E. Annotation Details", "content": "Recent advancements in LLMs have demonstrated their ability to generate high-quality text comparable to human output,\npositioning synthetic data generated by LLMs as a viable alternative or complement to human-generated data (Long et al.,\n2024). Benefiting from this, we use GPT-4o to generate highly diverse question-answer pairs across multiple scenarios and\ntasks.\nSpecifically, for each context type and task category, we design different prompts and seed questions based on distinct\nguiding principles. To better understand how we generate QA pairs in LaRA, we present the prompts used for generating\nfinancial statement QAs as an example. The prompts for the Location, Reasoning, Comparison, and Hallucination tasks are\nshown in Figures 7"}]}