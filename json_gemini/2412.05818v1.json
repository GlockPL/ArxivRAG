{"title": "SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation", "authors": ["Leigang Qu", "Haochuan Li", "Wenjie Wang", "Xiang Liu", "Juncheng Li", "Liqiang Nie", "Tat-Seng Chua"], "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in multimodal understanding and generation, pushing forward advancements in text-to-image generation. However, achieving accurate text-image alignment for LMMs, particularly in compositional scenarios, remains challenging. Existing approaches, such as layout planning for multi-step generation and learning from human feedback or Al feedback, depend heavily on prompt engineering, costly human annotations, and continual upgrading, limiting flexibility and scalability. In this work, we introduce a model-agnostic iterative self-improvement framework (SILMM) that can enable LMMs to provide helpful and scalable self-feedback and optimize text-image alignment via Direct Preference Optimization (DPO). DPO can readily applied to LMMs that use discrete visual tokens as intermediate image representations; while it is less suitable for LMMs with continuous visual features, as obtaining generation probabilities is challenging. To adapt SILMM to LMMs with continuous features, we propose a diversity mechanism to obtain diverse representations and a kernel-based continuous DPO for alignment. Extensive experiments on three compositional text-to-image generation benchmarks validate the effectiveness and superiority of SILMM, showing improvements exceeding 30% on T2I-CompBench++ and around 20% on DPG-Bench.", "sections": [{"title": "1. Introduction", "content": "Large Multimodal Models (LMMs) are advancing rapidly, surpassing Large Language Models (LLMs) by embracing multimodal capabilities for multimodal content perception, understanding, and generation [19, 61]. In particular, LMMs demonstrate promising abilities in interpreting user input prompts for text-to-image generation (T2I) [52, 54], producing vivid and photorealistic images. However, as shown in Fig. 1(a), achieving precise text-image alignment between generated images and complex prompts remains challenging, especially for compositional prompts involving multiple objects, attributes, counting, and complex relationships [6, 16, 46].\nTo enhance text-image alignment, existing work falls into two primary research lines. One line focuses on decomposing the T2I task into multiple stages. For example,"}, {"title": "2. Related Work", "content": "Compositional Text-to-Image Generation. Diffusion models [52, 54] have marked a significant advancement in T2I generation due to their stability and scalability. However, they still struggle with text-image alignment, such as attribute binding, counting error, and relation confusion [16, 48]. To enhance compositional T2I, some approaches intervene in language structures [16] or cross-attention mechanisms [6]. Other methods [17, 35, 36, 46] incorporate layout planning by LLMs or use multi-agent collaboration [44, 67]. Inspired by alignment successes in LLMs, recent work [5, 15, 64] applies RLHF [41] to optimize diffusion models. Despite the progress, they rely on inductive biases, extensive prompt engineering, or labor-intensive annotations, limiting flexibility and scalability.\nLarge Multimodal Models. The pioneering LMMs [37, 76] integrate a visual encoder, e.g., CLIP [49], with LLMs as the foundation, showing impressive multimodal understanding capabilities. To extend LMMs to visual generation, recent approaches align diffusion models [13, 19, 69] with LLMs or train a single transformer [59, 65, 71, 73]. According to the form of output visual features, they can be divided into discrete visual tokenization methods [19, 47, 59, 65] and continuous visual representation methods [13, 61, 69]. While LLM integration enhances language understanding and supports flexible applications (e.g., interleaved multimodal generation [61]), compositional T2I in the context of LMMs remains underexplored.\nLearning from AI Feedback. The high cost of collecting human preference has spurred research into RLAIF [3]. Benefiting from the convenience and scalability, there have been a series of studies adopting RLAIF to tackle a range of NLP tasks [10, 32, 75] and vision-language understanding [66, 74]. Despite the thrilling success, they only focus on text generation, overlooking the potential of RLAIF in other modalities. In contrast, we explore self-improving LMMs by activating multimodal understanding abilities for T2I. Particularly, we propose continuous strategies meticulously tailored to continuous visual features."}, {"title": "3. Methodology", "content": "In this section, we elaborate on the proposed method, including the SILMM framework with five steps and the iteration strategy (Sec. 3.1), as illustrated in Fig. 2. Afterward, we introduce the continuous KC-DPO applied to LMMs with continuous visual features in Sec. 3.2."}, {"title": "3.1. Self-Improving Large Multimodal Models", "content": "Step 1: Compositional Prompt Generation. We first divide compositional scenarios into four categories: Attribute (color, shape, texture), Layout (counting, spatial relation), Semantic Relation, and Complex Composition. Complex composition includes any possible composition of the first three. For attribute and layout, we prompt the LMM to separately generate common objects, attributes, numbers, and spatial relations, and then use templates to compose these concepts. For semantic relation and complex composition, we adopt in-context learning [12] to generate prompts. More details can be found in App. 6.\nStep 2: Diverse Representation and Image generation. The purpose of this step is to sample diverse intermediate visual representations from the LLM backbone \u03c0 of an LMM, given a text prompt x, which would be decoded into images with different qualities. These representations are denoted as $Z = \\{z_i, ..., z_M\\}$, where $z_i \\sim \\pi(z|x)$. For discrete LMMs [19], $z_i$ is a discrete visual sequence. We follow the common practice [41, 50] in language generation to obtain Z, by sampling with different random seeds during auto-regressive decoding. For continuous LMMs [13], the LLM can only output a fixed continuous visual feature, without diversity. To tackle this issue, we propose DropDiv. First, we insert the dropout operations in the last few MLP layers of LLMs, which introduces randomness and enables LLMs for sampling. During inference, we activate these dropout operations to output diverse representations by sampling: $z_i \\sim \\pi'(z|x)$, where $z_i$ denotes a continuous visual feature and \u03c0' represents the LLM with activated dropout operations. Afterward, these diverse visual representations Z are decoded into images as $Y = \\{y_1, ..., y_M\\}$.\nDiscussion. Unlike prior work [5, 15] focused on tuning diffusion models, our approach resorts to LLM backbones in LLMs to control image decoders (e.g., diffusion models) for better text-image alignment, centering on LLM backbone optimization. Our approach offers three key advantages: 1) LLMs demonstrate superior proficiency in prompt comprehension over text encoders [49, 51] commonly employed in diffusion models. Tuning LLM backbones may unlock their enormous potential for compositional T2I, especially in complex scenarios. 2) Tuning diffusion models is often constrained by efficiency challenges inherent to iterative likelihood estimation, whereas there have been well-established technologies [1, 38, 50] for LLM alignment. 3) Our method is orthogonal to existing methods to tune diffusion models, combining them may get further gains.\nStep 3: Decompositional Self-Questioning. To provide helpful feedback to the generated images, the LMM should first accurately assess text-image alignment, which requires strong compositional reasoning abilities. However, current advanced LMMs still suffer from compositional reasoning [40], such as spatial relation understanding [7] and counting [53]. To improve compositional reasoning, we introduce a divide-and-conquer strategy [74] for self-questioning. Specifically, the LMM first divides the given prompt x into atomic concepts (e.g., \"a white harp\") and relations (e.g., \u201ca pancake is on the left of a pasta\u201d), and then generates questions $Q = \\{q_1,..., q_n\\}$, each $q_i$ corresponding to a concept or relation. For simplicity, the generated questions are constrained to be yes/no questions (e.g., \"Is there a while harp?\u201d, \u201cIs the pancake on the left of the pasta?\"). Refer to App. 7 for more details on prompt templates of self-questioning.\nStep 4: VQA-based Self-Feedback. Taking a generated image $y \\in Y$ and all the questions Q as input, the LMM conducts the VQA task, and the average difference between the probabilities of answering \u201cyes\u201d and \u201cno\u201d serves as the text-image alignment score:\n$s(x,y) = \\frac{1}{N} \\sum_{i=1}^{N} [p(\"yes\" |y, q_i) \u2013 p(\u201cno\u201d |y, q_i)].$ (1)\nHere we adopt the vision-language understanding abilities of LMMs via VQA to provide feedback to the images generated by themselves, thus this step is named VQA-based self-feedback. We carry out this step through all the sampled images prompted by x and get all the scores $S = \\{s(x, y_j)|y_j \\in Y\\}$.\nStep 5: Learning from Self-Feedback. Based on the self-feedback alignment scores, we sample representation pairs $(z_w, z_l)$ from Q, where $z_w$ and $z_l$ denote the chosen and the rejected representations and their corresponding decoded images should satisfy $s(x, y_w) > s(x, y_l)$. With the preference data, we optimize the LLM backbone with DPO [50]:\n$\\mathcal{L}_{DPO} = -\\mathbb{E}_{(x,z_w,z_l)\\sim D}\\left[\\log \\sigma\\left(\\beta\\log \\frac{\\pi_{\\theta}(z_w|x)}{\\pi_{ref}(z_w|x)} - \\beta\\log \\frac{\\pi_{\\theta}(z_l|x)}{\\pi_{ref}(z_l|x)}\\right)\\right],$ (2)\nwhere D denotes the training set, and $\\pi_{\\theta}$ and $\\pi_{ref}$ represent the policy and reference models, respectively. $\\sigma$ is the sigmoid function, and $\\beta$ is a hyperparameter controlling the deviation from the reference model.\nIterative Self-Improvement. After learning from self-feedback, the updated LMM becomes more likely to generate preferred representations that are decoded into images better aligned with the prompt. This improvement in overall text-image alignment motivates us to iterate the above five steps with the updated LMM as the new reference model. The iteration mechanism continues until the alignment performance converges. As the process is independent of human annotations and external models, it is cost-effective and scalable. More importantly, it showcases the potential for self-improvement in LMMs by harmonizing their understanding and generation capabilities."}, {"title": "3.2. Continuous Direct Preference Optimization", "content": "At the step of learning from self-feedback, LMMs are optimized using the DPO objective as shown in Eqn. (2). The difference between discrete and continuous LMMs in this learning process lies in the calculation of the likelihood $\\pi(z|x)$. For discrete LMMs, $\\pi(z|x)$ can be straightforwardly obtained by the softmax categorical distribution. However, for continuous LMMs with unknown distribution modeling, calculating $\\pi(z|x)$ is intractable.\nPredictive Distribution with MC Dropout. MC Dropout [18] enables predictive distribution estimation via Monte Carlo simulation to calculate $\\pi(z|x)$. Specifically, the dropout layers in an LMM are activated during inference and the LMM performs forward propagation multiple times to get multiple outputs. Assuming a Gaussian distribution, we can estimate its parameters and calculate the likelihood $\\pi(z|x)$ based on these outputs. However, such multi-forward estimation imposes a significant computational burden during training, making this approach insufficient and impractical.\nSimplified Kernel-based Continuous DPO. Inspired by MC Dropout and motivated by its insufficiency issue, we propose a simplified method to achieve continuous DPO. Concretely, the intermediate representation z often performs as a feature matrix $H \\in \\mathbb{R}^{L \\times D}$ where L and D denote the sequence length and dimension. H can be attained by a Q-Former [13, 20] or from the last layer of the LMM in an autoregressive way [60, 61]. To estimate $\\pi(H|x)$, we first make a decomposition as:\n$\\pi(H|x) = \\prod_{i=1}^{L} \\pi(h_i | H_{<i}, x),$ (3)\nwhere $h_i \\in \\mathbb{R}^D$ denotes the i-th feature vector. Based on the Gaussian assumption, we have:\n$\\pi(h_i | H_{<i}, x) = \\frac{\\exp [-(h_i \u2013 \\mu_i)^T \\Sigma_i^{-1} (h_i \u2013 \\mu_i)]}{\\sqrt{(2\\pi)^D |\\Sigma_i|}},$ (4)\nwhere $\\mu_i$ and $\\Sigma_i$ denote the mean vector and the covariance matrix, respectively. Furthermore, we further simplify and approximate this formula: 1) the mean vector is estimated by the direct output of the continuous LMM, i.e., $\\mu_i \\approx LMM(x)[i]$, and 2) the Gaussian distribution is isotropic and all dimensions share the same variance value $\\bar{\\sigma}$, i.e., $\\Sigma_i \\approx diag(\\sigma_1, ..., \\sigma_D)$ and $\\sigma_1 = ... = \\sigma_D = \\bar{\\sigma}$, and $\\bar{\\sigma}$ can be learnable or viewed as a hyperparameter.\nWe compute the simplified likelihood with Eqn. (4), obtain the joint one with Eqn. (3), and finally derive the continuous DPO based on Eqn. (2):\n$\\mathcal{L}_{C-DPO} = -\\mathbb{E}_{(x,H_w, H_l)\\sim D}\\left[ \\log \\sigma \\left(\\frac{\\beta}{2\\bar{\\sigma}^2} (-\\||H \u2013 H_w||_F^2 + \\||H \u2013 H_l||_F^2)\\right)\\right],$ (5)\n$=-\\mathbb{E}_{(x,H_w, H_l)\\sim D}\\left[\\log \\sigma \\left(\\frac{\\beta}{2\\bar{\\sigma}^2} (-\\||H_r \u2013 H_w||_F^2 + \\||H_r \u2013 H_l||_F^2)\\right)\\right],$ (5)\nwhere $||\\cdot||_F$ denotes the Frobenius norm, and H and $H_r$ represent the continuous feature matrices from the policy and reference LMMs, respectively. $H_w$ and $H_l$ refer to the chosen and rejected feature matrices, respectively. Compared with the MC dropout method, this objective only requires one forward pass, which is more efficient. We relegate more details of the derivation to App. 8. From Eqn. (5), we can see that this objective aims to adjust the relative distances within the quadruple $(H, H_r, H_w, H_l)$ and the distance metric is the Euclidean distance between two matrices. To further improve the flexibility, we generalize the continuous DPO objective to,\n$\\mathcal{L}_{KC-DPO} = -\\mathbb{E}_{(x,H_w, H_l)\\sim D}\\left[\\log \\sigma \\left(\\gamma(\\cdot k(H, H_w) + k(H_r, H_w) + k(H, H_l) \u2013 k(H_r, H_l))\\right)\\right],$ (6)\nwhere $\\gamma = \\frac{2}{\\beta^2}$ controls the degree of adherence to the reference model, k(,) denotes a generalized distance measurement function. Considering it is similar to kernel methods [22, 57], we name the objective Kernel-based Continuous DPO (KC-DPO). In the following experiments section, we will discuss different distance functions and their influences on alignment performance."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nBase Model Settings. We implement our method on DreamLLM (continuous LMM) [13] and SEED-LLaMA (discrete LMM) [19] for all experiments. Details on DPO training are provided in App. 9.\nDatasets. We curated a dataset of 16,000 prompts across four categories using LMM. In each DPO training iteration, images generated by the model in the previous iteration served as the training data for next DPO iteration, allowing for iterative self-improvement. Details on data creation are provided in App. 10.\nBenchmarks. We evaluate our method on three text-to-image alignment benchmarks and follow their default settings. T2I-CompBench++ [28] consists of 8,000 compositional text prompts organized into 4 main categories: attribute, layout, non-spatial and complex compositions, further divided into 8 subcategories, including color binding, shape binding, texture binding, 2D/3D-spatial relationships, non-spatial relationships, numeracy, and complex compositions. TIFA [27] uses pre-generated question-answer pairs and a VQA model to evaluate generation results based on 4,081 diverse text prompts and 25,829 questions across 12 categories. DPG-Bench [26] comprises 1,065 densely descriptive prompts with an average token length of 83.91, presenting more complex scenarios with varied objects and rich adjectives."}, {"title": "4.2. Performance Comparison", "content": "As shown in Tab. 1, we evaluate alignment performance of our method against T2I generative models and base LMMs on three compositional T2I benchmarks, including T2I-CompBench++ [28], DPG-Bench [26], and TIFA [27]. Key observations are as follows: 1) Although LMMs enable more flexible settings (e.g., in-context learning and interleaved multimodal generation) for image generation, they still underperform compared to specialized T2I models in terms of the basic alignment ability to follow prompts. It demonstrates that current LMMs may ignore the compositional text-image alignment during multimodal pre-training and fine-tuning. 2) Without human annotations or external models, the proposed SILMM method enhances alignment performance across all categories in three benchmarks over the base LMMs, improving both the discrete SEED-LLaMA and the continuous DreamLLM, verifying the effectiveness and the generalization of SILMM. 3) SEED-LLaMA shows greater self-improvement than DreamLLM, possibly due to its weaker baseline alignment and the stability of discrete DPO over continuous KC-DPO induced by a series of simplification, as discussed in Sec. 3.2. And 4) improvements are more challenging in layout, relation, and complex categories than in attribute categories. This difficulty arises partly because the basic generative ability in these categories is weak, making it difficult to obtain high-quality chosen samples. Besides, understanding compositional concepts remains a challenge for LMMs [7, 53]."}, {"title": "4.3. In-depth Analysis", "content": "To explore the efficacy of SILMM, we conduct extensive ablation studies and hyperparameter analyses. We first investigate the iteration process and data scaling, followed by an in-depth study of key components, including diversity strategies, decompositional self-questioning and answering for self-feedback, and KC-DPO."}, {"title": "5. Conclusion", "content": "In this work, we present a self-improvement approach named SILMM to enhance text-image alignment within LMMs, introducing an iterative model-agnostic framework comprising five stages to enable high-quality self-feedback and alignment learning. For continuous LMMs, we propose a dropout-based strategy to diversify image representations and a continuous DPO method, KC-DPO, for optimizing LMMs with preference representation pairs. Extensive experiments on three compositional T2I benchmarks validate the effectiveness and superiority of our SILMM framework."}, {"title": "6. Details of Compositional Prompt Generation", "content": "For attribute and layout prompt generation, we first leverage the world knowledge of LMMs to generate common objects spanning various categories, including animals, plants, fruits, household items, clothing, vehicles, food, musical instruments, and electronic devices. Attributes such as color, shape, texture, and 2D/3D spatial relations are also incorporated. Using predefined templates, we systematically combine objects with attributes, numeracy, and spatial relations to construct compositional prompts. The templates are detailed below:\nAttribute.\n\u2022 A {adj} {noun}\n\u2022 A {adj1} {noun1} and a {adj2} {noun2}\nLayout\n\u2022 A {noun1} {spatial_2d/spatial_3d} a {noun2}\n\u2022 {quantity} {object_singular/object_plural}\n\u2022 {quantity} {object_singular/object_plural} and {quantity} {object_singular/object_plural}\nFor non-spatial and complex relations, we adopt in-context learning to generate diverse prompts based on LMMs:"}, {"title": "7. Details of Self-Questioning Prompt", "content": "We follow a divide-and-conquer strategy, where the LMM first extracts the atomic concepts from the given prompt. These atomic concepts are then transformed into simple yes-or-no questions. The specific instructions are shown in the following:"}, {"title": "8. Derivation of KC-DPO", "content": "8.1. Preliminary\nReinforcement Learning from Feedback with Reward Model. With collected preference pairs D = {(xi, yw, yi)}i=1 from human feedback [41] or AI feed-\nN"}]}