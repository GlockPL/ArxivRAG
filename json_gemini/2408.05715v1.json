{"title": "Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking", "authors": ["Zhi-Cun Lyu", "Xin-Ye Li", "Zheng Xie", "Ming Li"], "abstract": "Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently. Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work. However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates. Otherwise, the system would be unhelpful. In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. Top Pass directly optimizes the pass@kloss function, enhancing the quality at the top of the candidate list. This enables the user to find the correct solution within as few tries as possible. Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method.", "sections": [{"title": "1 Introduction", "content": "Code generation aims to generate programs automatically based on the requirements described in natural language. It is important as programmers could save great time and efforts if correct programs are generated automatically. Large Language Models (LLMs) such as ChatGPT have brought the breakthroughs to the field of code generation. Specifically, LLMs pre-trained on code corpus substantially enhance their capacities, achieving unprecedented performance in solving problems [1-4]. This capacity has served as the foundation for tools like Github Copilot, which capitalizes on the concept of \u201cnatural language programming\u201d by these LLMs. Previous works have confirmed that these tools improve the coding efficiency of programmers greatly.\nDespite the rapid improvement in code generation system, when dealing with real-world complex problems, it is still a long way for LLM-based code generation system to generate the correct solution within a few tries. To deal with this problem, the prevailing strategy is to sample a huge number of program candidates, usually 100 [5], 200 [1], or even 1000 [6, 7], with the hope of any one in them could work. However, when applying code generation system for improving development efficiency, the correct program should be provided within a small candidate set for the generation system to be helpful. Otherwise, the users will spend too much time on reviewing or testing the wrong code candidates and the time loss will outweigh the gain. Such a goal of generating correct program within as few tries as possible is also reflected in the performance measure of the code generation systems, i.e., (average) pass@k. It measures the possibility of a user can find a correct program by testing k candidates. Typically, the parameter k is set to 1, 5, or at most 10, as it is usually unacceptable to test more than 10 candidates to find the correct program. However, ensuring the provision of a correct solution within a small number of candidate programs still remains challenging for current LLM-based code generation system.\nOther than fine-tuning the LLM itself for better code generation performance, an effective yet economy way to improve pass@k is to rank the candidates according to their possibilities to be correct. CodeRanker [8] has demonstrated the effectiveness of such a strategy by training a neural ranker with a binary classification objective to determine which candidates are more likely to be correct. Such an approach improves the performance measures (pass@k) on multiple code generation benchmarks with the help of Codex for generating candidates. However, it models the code ranking as binary classification and ignores the goal is ranking any positive sample to the top. As shown in fig. 2, classification based rankers could achieve high classification accuracy but fail to rank positive samples to the top. How to effectively minimize user effort in utilizing code generation systems still remains a challenging problem currently.\nIn this paper, we propose to optimize the metric 'pass@k' directly. By its definition, this optimization objective emphasizes that ranker model should rank the first positive program before the k-th negative program. However, directly optimizing this formulation faces some challenges. Solely relying on a single positive sample can disrupt the generalization capabilities of the ranker model. Instead"}, {"title": "2 Related Work", "content": "Recently, LLM-based approaches built upon Transformer architecture [9] have gained prominence for code generation [10-14]. [7] introduces Codex, a decoder-only language model fine-tuned on public available code and demonstrates impressive code writing ability. Concurrently, [15] presents CodeT5, an encoder-decoder architecture model based on T5 [16] with incorporation of an identifier aware pre-training task. In addition to the pre-training paradigm, [17] employs reinforcement learning to train AlphaCode and leverages supervision derived from the execution results of test cases. This approach achieves beginner-level performance on the Codeforces platform. Open-source code large language models LLMs [1, 3, 18-22] also make significant strides in performance on code generation benchmarks. Other than fine-tuning code LLM, [23] enforce the code LLM perform rubber duck debugging by explaining the generated code through few-shot demonstrations. Generating the extra test cases is validated useful in practice [5, 24, 25].\nCode ranking heavily depends on good code understanding capacities. Typical code understanding tasks include functionality classification, clone detection and code search [26-29]. Prior works [30-35] focus on enhancing code representation. They treat code as sequence of tokens and use pre-trained langugage models to capture the representation for the sequence. A notable work in this domain is CodeBERT [31], which uses encoder-only architecture and utilizes replace token detection objective [36] for modeling both natural language and programming language. [32] extends it to Graph-CodeBERT by introducing two new structure-aware pre-training tasks to learn the semantics from source code and data flow. Additionally, [34] proposes a one-to-one mapping function which elegantly transforms the Abstract Syntax Trees (AST) into the sequences, thus enabling processing AST information in the same manner as natural language and programming language. [33] proposes a novel cross-modal language model, UniXcoder, which leverages multi-modal information including the AST, the comment and the code fragment to enrich the representation learning during training."}, {"title": "3 The Top Pass Method", "content": "To alleviate the time cost of users in reviewing or testing the tremendous candidate programs, we propose a novel post-process method named Top Pass focusing on ranking the positive samples to the top. We first formalize the problem and then introduce the pass@kloss, which serves as the basis of Top Pass. Last, we describe the overall loss function for training the ranker.\nFor a programming task Q described in natural language, the code generation system is asked to generate n code candidates C = {C_i}_{i=1}^n. We use the symbol y_i = 1 to denote the candidate is a correct solution (positive), and y_i = 0 otherwise (negative). Confirming the correctness of the candidates requires manual review or unit test, thus at most k candidates can be reviewed or tested by the user to find a correct solution. The typical value of k is usually no more than 10, which is significantly smaller than n, the total number of candidates. Since the order of the candidates is random, it is doubtful whether the correct solution can be found within k attempts, even if it exists. Suppose the set of the k tested candidates is C^*, whether the solution of the programming task is found within the k candidates is denoted by 'pass@k'.\nThe problem-level pass@k is defined as if there exist any candidate passes all the test cases in the tested k candidates C^*:\n\\(pass@k = \\mathbb{I}[\\exists C_i \\in C^*, y_i = 1]\\)\nGenerally, the average pass@k across a group of programming tasks reflects the performance of a code generation system. It indicates how many tasks could be solved if the user can merely afford a maximum of k attempts for each task.\nThe expectation of pass@k for random candidate list [7]:\n\\(estimated\\ pass@k = 1 - \\frac{{\\binom{n-c}{k}}}{\\binom{n}{k}}\\)\nIt can be inferred from eq. (2) that even if the code generation system provides a correct solution, the likelihood of the user finding it is still low. Besides, for a given set of code candidates, the pass@k also fluctuates hugely due to the random order. It yields a variance of\n\\(Var = \\Big(1 - \\frac{{\\binom{n-c}{k}}}{\\binom{n}{k}}\\Big)\\Big(\\frac{{\\binom{n-c}{k}}}{\\binom{n}{k}}\\Big)\\)\nTo mitigate the unreliability of the code generation system, we introduce code ranking, which involves incorporating a ranker model f to discern codes with a higher likelihood of correctness. The ranker model assigns a correctness score to each candidate s_i = f(Q,C_i). By sorting the candidate list based on these scores, the user has a higher probability of finding the correct solution among the k candidates with the highest scores.\nAs described above, ranking the candidates properly can significantly enhance the performance and reliability of the code generation system. Previous study [8] formulates the ranking problem as a binary classification problem. However, such an approach neglects the ranking nature of the task, resulting in an unsatisfactory quality of the selected candidate set, particularly those at the top.\nIn this paper, we propose to directly optimize the pass@k, which is the ultimate goal of the code generation systems. Let C_+ = \\{C_+\\,1, C_+\\,2,\\cdots\\} and C_- = \\{C_-\\,1, C_-\\,2,\\cdots\\} denote the sets of positive and negative candidates, respectively, sorted in descending order based on their correctness scores. Derived from eq. (1), pass@k can be reformulated as:\n\\(pass@k = \\mathbb{I}[f(Q, C_+\\,1) > f(Q, C_-\\,k)]\\)\nThat is, the model should rank the first positive program before the k-th negative program. Based on the above formulation, the learning problem of ranking the code candidates can be immediately formulated as:\n\\(min\\quad 1 - \\mathbb{I}[f(Q,C_+\\,1) > f(Q, C_-\\,k)]\\)\nThis optimization problem exactly reflects the objective of 'pass@k' metric, which involves ranking any one of the positive candidate codes to the top k of the list. However, directly solving the above maximization problem encounters several practical challenges. Next, we will describe how to address these challenges.\nThe above formulation only requires the model to identify any one of the correct programs. However, due to the randomness of the candidates, training the model with only one positive program diminishes the model's ability to identify the correct code and consequently lowers its generalization performance. In practice, we relax the limitation of using only one correct program with the maximum score and allow the use of multiple correct programs with top scores. Let C_+^\\circ = \\{C_+\\,1, C_+\\,2,\\cdots,C_+\\,m\\}, where we select top proportion p positive candidates with highest confidence. We introduce a hyper-parameter to filter certain percentage of candidates. Ideally, the positive snippets with higher scores are the ones that are easily identified and are potentially the snippets written with commonly used algorithms and high-quality codes. On the other hand, the positive snippets with lower scores are those that are difficult to identify, often implemented with suboptimal algorithms, low-quality codes, or even false positive snippets due to weak test cases.\nInstead of using a single k-th top negative snippet for a specific k, we opt to use a portion of the top negative candidates as the negative set. Let C_-^\\circ = \\{C_-\\,1, C_-\\,2,...,C_-\\,n\\}, where we select top proportion q negative candidates with highest score. The proportion is usually larger than the typical value of k, allowing the ranker model to maximize pass@k for different common k values simultaneously. The selected negative programs with high scores are typically those that are challenging to distinguish from the positive programs. These programs are also typically high-quality codes but do not solve the given problem. Training with these negative programs enhances the discriminative abilities of the ranker model. On the other hand, the negative ones with lower scores are mostly irrelevant or low quality codes that are easily identified. By not training with those programs, we prevent the model from excessively focusing on irrelevant codes.\nBy combining the aforementioned techniques, we propose our loss function for maximizing pass@k with the following formulation:\n\\[\\sum_{C^+_\\iota \\in C^+\\circ} \\sum_{C^-_\\iota \\in C^-\\circ} \\mathbb{I}[f(Q, C^+_\\iota) > f(Q, C^-_\\iota)]\\]\nIt can be equivalently rewritten with the 0-1 loss:\n\\[\\sum_{C^+_\\iota \\in C^+\\circ} \\sum_{C^-_\\iota \\in C^-\\circ} l_{01} (f(Q, C^+_\\iota) - f(Q, C^-_\\iota)),\\]\nwhere \\(l_{01}(z) = \\frac{1}{2}(1 - sign(z))\\). In practice, we use hinge square as a convex and smooth surrogate loss function for better optimization:\n\\[L_{pass@k} = \\sum_{C^+_\\iota \\in C^+\\circ} \\sum_{C^-_\\iota \\in C^-\\circ} l(f(Q, C^+_\\iota) - f(Q, C^-_\\iota)),\\]\nwhere the surrogate loss \\(l(z) = (1 - z)^2\\).\nThe fine-tuning of the ranker model is based on the minimization of the following loss function:\n\\(L = L_{pass@k} + \\lambda L_{cls}\\)\nwhere \\(L_{pass@k}\\) is our pass@k loss described previously, \\(L_{cls}\\) is a standard cross entropy loss, and \u03bb is a coefficient of \\(L_{cls}\\), typically set to 0.3."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to verify the effectiveness of our Top Pass method. The experiment results shows that (1) Our approach, Top Pass, improves pass@k on four code generation benchmarks. (2) the improvement brought by our Top Pass method is significant across datasets with different false positive rates. (3) the impact of hyper-parameters on the performance of Top Pass is relatively smooth.\nWe consider four code generation benchmarks for evaluation. (1) CodeContests [17] is created for fine-tuning and evaluating AlphaCode and consists of competitive programming problems with augmented test cases. The training set contains 13328 tasks, the validation set 117 tasks and the test set 165 tasks. (2) APPS [37] consists of 5000 training and 5000 test programming tasks collected from popular programming competition platforms such as LeetCode. Programming tasks are split into three difficulty levels, introductory, interview and competition. (3) MBPP [38] consists of 974 mostly basic python programming tasks, designed to be solvable by entry level programmers. (4) HumanEval [7] consists of 164 human-written test tasks and most of the tasks can be solved with a few lines of python code. On first three benchmarks, we train ranker model on the train split and evaluate it on the test split of the dataset. To measure the transfer ability of our method between two different datasets, we evaluate the pass@k metrics on HumanEval using the ranker model trained on CodeContests without additional training.\nWe construct the dataset for training the ranker model and testing followings 2 steps. (1) For each problem description, we use LLM to generate 200 candidate python programs. (2) Executing the python candidate programs against test cases provided in the dataset and collect the execution information such as return value and execution time. If the program passes all the test cases within time limit, it is considered correct (positive). Otherwise, it is considered wrong (negative). If the program fail for one test case, we immediately label it as negative and do not execute the left test cases. In this way, we label all candidate programs as positive or negative according to test cases.\nWe use pass@k (k=1,2,3,5,10) for evaluation. This metric is first computed for each individual problem, then averaged across multiple problems. The computation of pass@k depends on whether the methods include a ranking step or not. For those methods without a ranking step, we report the expected value of pass@k, as defined in eq. (2). For methods that do include a ranking step, pass@k is either 0 or 1 for each problem, depending on whether any of the top-k candidates pass all the test cases, as detailed in eq. (4).\nOur ranker model consists of a pre-trained transformer based encoder and a classification head. We use CodeBERT [31] as the base encoder. The input of the model is the concatenation of programming task description and python code, separated by a special token, namely [CLS], Q, [SEP], C, [EOS]. The hidden representation of the first token [CLS] is used for representing the whole sequence. Subsequently, the classification head is applied to the representation to determine the score s_i.\nThe overall process is that we use a code LLM to generate candidates programs and then fine-tune a light-weight neural ranker model with our proposed pass@k loss. For sampling code candidates from LLMs, we sample N = 200 candidates per programming task in four datasets mentioned above since N = 200 is sufficient to estimate pass@k. We use nucleus sampling with top-p = 0.95 and temperature T = 0.8, consistent with the sampling setting used in Codex experiments [7]. For training ranker model, we tuned the hyper parameter p and q on each benchmark, where p represents the proportion of positive and negative sample selection separately. In CodeContests we keep top 90% positives and top 50% negatives, while in APPS we keep top 70% positives and top 60% negatives separately. For MBPP we keep top 60% positives and top 70% negatives separately. We calculate the pass@k loss according to eq. (8). And we choose \u03bb = 0.3 in eq. (9) to calculate the final loss. We fine-tuned the ranker models on different datasets. We used AdamW with a learning rate of 5e-5 and no weight decay. All the experiments are conducted on A100-40GB GPUs.\nWe compare our method with several baselines:\n\u2022 Standalone LLM: We compare Codex [7], ChatGPT [39], WizardCoder [18], StarCoder [1], AlphaCode [17], Code LLaMa [3] and DeepSeek-Coder 33B [20]. It reflects the average case when sampling from LLMs.\n\u2022 LLM with generated test cases: CodeT [5] and ALGO [6] generate more test cases to filter candidate programs, leading higher probability of selecting the correct solutions.\n\u2022 LLM with ranker: CodeRanker [8] models the code selection as a binary classification and sort the candidates according to predicted scores. The Coder-Reviewer [40] proposes a novel code ranking method according to the product \\(p(x|y)p(y|x)\\)."}, {"title": "4.3 Main Results", "content": "Our experiments are summarized in table 1, with the best performance highlighted in bold. Our approach, Top Pass, is highlighted with a shaded background. The results, as depicted in table 1, reveal that our approach Top Pass outperforms various baselines over all metric pass@k, including standalone LLMs, LLMs with test cases and prior ranker-based approaches. (1) With ChatGPT as generator, Top Pass attains a notable 7.3% pass@1, 12.7% pass@3 and 13.3% pass@5. (2) Using the DeepSeek-Coder as generator, our method Top Pass also achieves remarkable improvement over all metrics. It attains a outstanding 9.7% pass@1, 13.3% pass@3 and 14.5% pass@5, bringing a 32.9% relative improvement in pass@1, 29.1% in pass@3 compared with CodeRanker. Also, we achieve remarkable nearly twice (1.9\u00d7) pass@1 metric compared with standalone DeepSeek-Coder. In conclusion, our Top Pass method improves the quality of ranking at the top on the CodeContests dataset regardless of the code generation LLM. This substantial improvement further enhances the practical usage of code generation system. The additional time cost during test includes scoring each candidate program and finding the top-K the candidates with the highest scores. It is relatively small compared with the time cost of LLM inference. On CodeContests the additional time cost for each problem is about 1.8s when we want to select top-5 from 200 candidates. Thus, our method remains promising for practicality in real-world applications.\nWe evaluate our method on APPS benchmarks. The experimental results for APPS are presented in table 2. From the table we can see that our method demonstrate a significant enhancement in pass@k against various baselines. Compared with the standalone DeepSeek-Coder, our method Top Pass improves pass@1 from 17.3% to 20.4% and pass@5 from 30.7% to 32.0%. When compared with the previous CodeRanker, our method achieves relative 6.8% improvement for pass@1 and 2.6% improvement for pass@5, outperforming baselines by a significant margin. Table 2 also illustrates the pass@k improvement across various difficulty levels in the APPS benchmark.\nWe also conduct experiments on MBPP and HumanEval dataset. Experiment results are shown in table 3. On HumanEval dataset Top Pass outperforms both standalone DeepSeek-Coder and CodeRanker, achieving a remarkable 64.6% pass@1 and 76.2% pass@5. On MBPP dataset Top Pass achieves 69.2% pass@1, outperforming both standalone code LLMs and CodeRanker. It also outperforms other code ranking method such as Normalized Coder-Reviewer with Codex002."}, {"title": "4.4 Discussions", "content": "As illustrated in table 4, the removal of pass@k loss leads to a substantial performance downgrade in pass@k across various values of k. This observation not only validates the significance of pass@k loss in our method but also highlights its contribution to overall performance.\nEvaluation of the code highly depends on the quality of test cases. However, crafting high-quality test cases demands meticulous consideration. The insufficient test cases result in labeling buggy code as correct, commonly known as the false positives, in training datasets. Previous study has shown that the existence of such false positives could adversely affect the training of the model [17]. To demonstrate the robustness of Top Pass against the false positives, we introduces additional false positive samples by removing different percentage of test cases, yielding three training datasets with distinct false-positive rates: 30%, 40%, and 70%. We then evaluate ranker models trained on three generate dataset. As shown in fig. 4, the ranking quality of Top Pass remains minimally effected while pass@k metrics of CodeRanker dramatically drop to slightly better than standalone ChatGPT. This observation aligns with our expectation as Top Pass is robust to low quality code by design.\nWe consider the impact of different sample numbers during test and fig. 5 illustrates the change in pass@1 with an increasing number of samples. As depicted in the figure, Top Pass consistently outperforms CodeRanker across all sample numbers, with the lead in pass@1 metric still continuing to increase. Previous ranking method, CodeRanker, ignores the ranking quality at the top of candidate lists. So when the candidate programs get more and more, the negative programs overwhelm the positive ones at the top, thus leading to the poor pass@1 performance. However, our method is dedicated to improve the ranking quality of the top section and achieves outstanding pass@1 performance.\nWe also analyze the impact of hyper-parameters p and q by training ranker model on CodeContests dataset with different values of p and q. From fig. 6 we observe that the variations of two hyper-parameters influence pass@1 smoothly. And it is evident that removing bottom negatives brings better performance, which demonstrates that giving more significance to the top positive/negative programs is beneficial. More detailed results are presented in Appendix."}, {"title": "5 Conclusion", "content": "In this paper, we propose an approach named Top Pass, which elaborates to improve the ranking quality at the top section of the candidate list by designing and directly optimizing a novel pass@k loss. Experimental results demonstrate that the Top Pass outperforms previous methods by a substantial margin in term of pass@k metrics across various LLMs and code generation benchmarks with different difficulty levels.\nOur approach improves the usability of the code generation systems and allows users to find correct code within a few tries, thus reducing the burden of handful testing and reviewing. Applying pass@k loss in training reward model in the reinforcement learning process thus improving code LLMs' capacity will be investigated in the future."}, {"title": "Appendixes", "content": "Here we analyze an example online judge programming task of CodeContests in detail. For CodeForces 1619B task, the problem description is listed as following.\nAfter sampling 200 candidate programs, the program ranked as the top choice by Top Pass is displayed in table 7. We can see that the program not only successfully passes all the test cases in CodeForces but also implements the algorithm in a efficient and elegent code style. The program ranked as the top choice by CodeRanker is also displayed in table 8. The program fails for test cases in CodeForces, thus it is viewed as buggy program. It is not only difficult to read and but also implemented with inefficient algorithm.\nWe also investigate the influence of different sampling temperature. The results is displayed in fig. 7. From the figure we can see that our method, Top Pass, exhibits stable performance under different sampling temperatures, outperforming random case by a large margin.\nWe consider the influence of the hyper-parameter \u03bb. Table 5 shows the detailed results. From the table we see that different choices of hyper-parameter \u03bb has smooth influence on the performance of Top Pass ranker, peaking at 0.3 approximately."}]}