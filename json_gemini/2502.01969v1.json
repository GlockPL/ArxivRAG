{"title": "Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration", "authors": ["Younan Zhu", "Linwei Tao", "Minjing Dong", "Chang Xu"], "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent works attribute this issue to an inherent bias of LVLMs where vision token attention map has a fixed correlation with spatial position, and propose to mitigate this issue by reordering visual tokens. However, we find that different LVLMs exhibit different correlations between attention and spatial position, which makes the existing solution difficult to generalize to other LVLMs. To address this issue, we first introduce a training-free solution, Uniform Attention Calibration (UAC), that estimates the bias from single meaningless input image and applies a calibration matrix to rectify attention imbalances. To further alleviate the bias, we relax the assumption of single meaningless input in UAC and introduce a fine-tuning solution, Dynamic Attention Calibration (DAC), that enforces the consistent outputs wherever the object locates in the image via a plug-and-play module. Comprehensive experiments across multiple benchmarks demonstrate that UAC and DAC significantly reduce object hallucination while improving general multimodal alignment. Our methods achieve state-of-the-art performance across diverse LVLM architectures on various metrics.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (LVLMs) have garnered significant attention in the AI research community for their remarkable ability to comprehend the visual world and engage in conversational interactions with humans. Despite these advances, LVLMs continue to face critical challenges, particularly in the form of object hallucination, a phenomenon where models generate responses that are not factually aligned with the visual content. This issue undermines the reliability of LVLMs, posing a significant barrier to their deployment in real-world applications.\nA variety of approaches have been proposed to mitigate object hallucination in LVLMs. One common strategy involves post-hoc correction using revisor models, which aim to reduce hallucinated responses by refining outputs. Another approach improves supervised fine-tuning through diversified instruction tuning data or aligns model responses with human preferences. Recently, several studies have explored training-free methods for mitigating object hallucination by addressing issues in the autoregressive decoding process of LVLMs.\nA recent study reveals that LVLMs' perception varies with object positions due to the inherent processing order in autoregressive models. As 2D vision tokens are concatenated with text tokens and flattened into a raster-scan sequence (top-to-bottom, left-to-right), the model develops a bias, prioritizing tokens in the bottom-right region closer to the instruction tokens, termed as Spatial Perception Bias (SPB). This spatial bias skews perception capabilities. To mitigate this, propose a position alignment technique that reorders the perception sequence, reducing spatial bias.\nHowever, this approach has two major limitations. First, the method is based on the assumption that the model assigns greater attention to tokens that are relatively nearby. As demonstrated in Figure 1(a-c), our analysis reveals that the attention distributions of vision tokens vary significantly across different LVLM models and unexpectedly high attentions are assigned to arbitrary locations. This observation challenges the generalization of the heuristic reordering strategy proposed by, highlighting the need for a more dynamic and adaptable solution. Second,\nthe proposed technique requires retraining the entire network, which is computationally expensive and often impractical for large-scale LVLMs, underscoring the necessity of developing a lightweight alternative.\nBuilding on this analysis, we aim to rectify the inherent SPB in vision token attention distributions. To achieve this, we introduce two attention calibration methods: Uniform Attention Calibration (UAC) and Dynamic Attention Calibration (DAC). UAC provides a simple training-free solution with competitive performance by calibrating biased attention through bias estimation on a meaningless input. Though effective and efficient, the performance of UAC could be limited due to the assumption of single meaningless input. Thus, we further relax the assumption in UAC and introduce DAC to fine-tune LVLMs for better performance. Specifically, DAC consists of a learnable plug-and-play module integrated into the self-attention mechanism. With a simple yet effective data augmentation technique, the module is then fine-tuned via contrastive learning to encourage consistent outputs with different object positions in the image, which dynamically adjusts vision token attention map to tackle object hallucination.\nComprehensive experiments confirm the effectiveness of UAC and DAC, revealing substantial improvements across multiple object hallucination benchmarks and a range of LVLMs, including LLaVA-1.5, mPLUG-Owl2, and LLaVA-NeXT. Additionally, our approach strengthens the overall perception capabilities of LVLMs, as demonstrated by its strong performance on MME and LLaVA-Bench, emphasizing its utility beyond mitigating object hallucination. To summarize, our main contributions are as follows:\n1. We systematically investigate Spatial Perception Bias(SPB) in vision token attention within LVLMs, revealing its strong correlation with object hallucination and its persistence across different models.\n2. Based on these findings, we propose Uniform Attention Calibration (UAC), a simple yet effective training-free bias correction module, and Dynamic Attention Calibration (DAC), a learnable plug-and-play module that dynamically adjusts vision token attention.\n3. Extensive experiments confirm that both UAC and DAC significantly reduce object hallucination and enhance overall perception. Our methods achieve notable improvements across various LVLMs."}, {"title": "2. Related Work", "content": "2.1. Visual-Language Models\nLarge Vision-Language Models (LVLMs) have evolved from early BERT-based architectures to models that integrate Large Language Models (LLMs). Early vision-language models, such as ViL-BERT and LXMERT, fused visual and textual features through transformer-based architectures. The introduction of LLMs enabled contrastive learning approaches like CLIP and ALIGN, improving multimodal adaptability. Recent LVLMs, such as LLaVA and InstructBLIP, leverage visual instruction tuning for improved context-aware generation. Advances have further enabled referential dialogues, interleaved image-text processing, and visual prompts, broadening LVLM applications in interactive AI systems. These developments highlight a growing shift toward task-specific fine-tuning and multimodal interaction.\n2.2. Hallucination in VLMs\nObject hallucination arises when Large Vision-Language Models (LVLMs) generate textual descriptions containing objects or attributes not present in the accompanying image. This phenomenon is frequently observed in tasks such as image captioning and visual question answering, where maintaining an accurate alignment between visual and textual content is critical. A range of methods has been proposed to address hallucination, from post-hoc correction using external or self-correcting models to enhanced instruction tuning that diversifies training data or aligns outputs with human feedback. Recently, training-free approaches that rely on model-based distribution comparisons were proposed. As LVLMS grow more sophisticated and versatile, understanding and mitigating object hallucination remains a key focus in multimodal learning research. From a unique perspective, our design is rooted in the correlation between vision tokens attention and object hallucination."}, {"title": "3. Preliminary", "content": "In this section, we provide a brief overview of the widely adopted LVLMs architecture and explain how vision tokens"}, {"title": "4. Method", "content": "A valid approach to mitigating bias is to calibrate the attention values. Since the attention map essentially forms a discrete probability distribution that sums to one, this problem closely resembles uncertainty calibration. A common strategy in uncertainty calibration literature is to adjust the probability distribution by modifying the output logits. Inspired by calibration literature, we propose two attention calibration techniques to meet different needs: Uniform Attention Calibration (UAC) and Dynamic Attention Calibration (DAC). UAC is a training-free method that removes SPB estimated from a meaningless input, offering a simple yet effective solution with competitive performance. In addition, DAC is a learnable plug-and-play module within the self-attention mechanism, providing state-of-the-art performance with minimal computational overhead.\n4.1. Uniform Attention Calibration\nWe hypothesize that LVLMs should assign uniform attention to meaningless images, such as blank white or random noise images. Therefore, we introduce UAC, which enforces uniform attention by first estimating SPB from a meaningless input (defaulting to a blank white image) and computing a calibration matrix W that adjusts vision tokens attention map to be uniform across all positions. This matrix is then applied as an affine transformation to vision tokens attention during inference.\nSpecifically, we first obtain the attention map $\\mathbf{A}_{img}$ for a meaningless input and compute calibration matrix W as:\n$W = \\frac{avg(\\mathbf{A}_{img})}{\\mathbf{A}_{img}}$\nwhere $avg(\\cdot)$ denotes the average value over all elements.\nDuring inference, the calibrated attention matrix $\\mathbf{A}_{img}^{cal}$ for any input image is obtained as:\n$\\mathbf{A}_{img}^{cal} = W \\circ \\mathbf{A}_{img}$\nwhere $\\mathbf{A}_{img}$ represents the original attention weights computed from the input image, $\\circ$ denotes the element-wise (Hadamard) product. Notably, W can be calcaulated and applied to any layer of the LVLM decoder, making it a flexible and model-agnostic calibration method.\n4.2. Dynamic Attention Calibration\nUAC removes SPB by assuming a single meaningless reference, but this one-size-fits-all approach does not guarantee optimal attention calibration across diverse inputs. To overcome this limitation, we seek an trainable solution that allows LVLMs to dynamically adjust their attention distributions while ensuring invariance to an object's spatial position within an image. To this end, we introduce Dynamic Attention Calibration (DAC), which involves plug-and-play learnable module to dynamically calibrate the SPB via contrastive learning.\nDAC Design\nMotivated by the superior calibration performance of affine transformation in the field of uncertainty calibration, we introduce a lightweight trainable transformation f to calibrate unreliable vision token attention weights before SoftMax function as $\\mathbf{A}_{img} = f(\\mathbf{A}_{img})$, where $\\mathbf{A}_{img}$ denotes the calibrated vision token attention weights. Specifically, the transformation f operates within the self-attention mechanism of the transformer decoder layers and consists of a small stack of linear transformations with ReLU activations. The details about building blocks can be found in the Appendix. The forward pass of DAC module can be defined as\n$\\mathbf{A}_{img} = f(\\mathbf{A}_{img}) = g_{L-1}W_L + b_L, \\\\g_i = ReLU(g_{i-1}W_i + b_i), \\text{for } i \\in \\{1, ..., N - 1\\},$\nwhere L denotes the layer number in DAC module, $W_i \\in \\mathbb{R}^{D_i \\times D_i}$ denotes the weight matrix of layer i, $b_i \\in \\mathbb{R}^{D_i}$ denotes the bias vector, $g_i$ represents the output of the i-th layer, and $g_0 = \\mathbf{A}_{img}$. The DAC module can be applied to any layer of the language model decoder, targeting the layers responsible for vision tokens processing.\nDAC Optimization\nWith the DAC module in Eq. 5, a much stronger constraint can be imposed on vison token attention weights of LVLMs to alleviate the bias. Instead of the uniform constraint in UAC, we further propose to force the consistent outputs wherever the object locates in the image. The key idea is to ensure that the model maintains the same capability of identifying an object regardless of its position within the image. However, to impose such a constraint, it could be challenging to obtain sufficient training data variants with different object positions. Thus, we introduce a simple yet effective data augmentation technique inspired by the concept of instant discrimination.\nFormally, we select a small portion of the validation set, 20% of the total validation data, as our calibration set, denoted as $\\mathcal{D}_{cal}$. Each image $V \\in \\mathcal{D}_{cal}$ is paired with ground-truth annotations and their corresponding bounding boxes. The calibration set $\\mathcal{D}_{cal}$ undergoes an augmentation process to produce the augmented dataset $\\mathcal{D}_{aug}$. Specifically, we crop the ground truth objects from the images using the annotations and bounding boxes provided, then apply random resizing and paste the cropped objects onto a pure white background as $V_{crop}$. For each $V_{crop}$, we generate balanced positive and negative query-label pairs, ensuring a well-balanced dataset. Additionally, we include annotations for\nthe cropped images $V_{crop}$ to be utilized in instance discrimination tasks, as discussed later in the paper. The detailed augmentation process is summarized in the Appendix.\nWith sufficient augmented data from $\\mathcal{D}_{aug}$, we propose leveraging contrastive learning to encourage LVLMs to focus on objects themselves rather than their absolute positions in the image. This approach ensures consistent outputs regardless of object position. By reducing reliance on positional cues, the model learns to robustly identify objects despite spatial transformations. Specifically, contrastive learning is formulated to increase the similarity between embeddings of the same object at different spatial locations while pushing apart the embeddings of different objects. We begin with an $\\mathcal{D}_{aug}$ dataset and randomly sample a minibatch of B examples. Each example then undergoes an additional augmentation process, resulting in a total of 2B augmented data points. Following the approach of, for each positive pair, we consider the remaining 2(B \u2013 1) augmented examples within the minibatch as negative examples. Given the embeddings $z_i$ and $z_j$ of the positive augmented pair $v_i$ and $v_j$, the contrastive loss can be expressed as:\n$\\ell_{CL}(i, j) = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{2B} 1_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k) / \\tau)}$\nwhere B denotes the number of examples in a minibatch, $\\text{sim}(\\cdot, \\cdot)$ represents the cosine similarity, $1_{[k \\neq i]}$ is an indicator function, and $\\tau$ is the temperature parameter. Combined with a cross-entropy (CE) loss, the final loss function is formulated as\n$\\mathcal{L} = \\mathcal{L}_{CE}(F(T_{crop}, V_{crop}), Y_{crop}) + \\lambda \\mathcal{L}_{CL},$\nwhere F represents the model, $T_{crop}$ and $V_{crop}$ are the query and cropped image, $Y_{crop}$ is the corresponding label, and $\\lambda$ is a hyperparameter balancing the two losses. We optimize our DAC using Eq. 7 alongside instruction tuning, while keeping all other components frozen. The overall training process is summarized in Algorithm 1."}, {"title": "5. Experiment", "content": "5.1. Setup\nModels and Baselines We implement three representative LVLMs for evaluation: LLaVA-1.5 and mPLUG-Owl2 at the 7B scale, and LLaVA-NeXT at the 8B scale. Our method is compared against five methods. Baseline responses are generated using the original LVLMs, while other techniques such as Visual Contrastive Decoding (VCD), OPERA, Self-Introspective Decoding (SID), and Concentric Causal Attention (CCA) are included for comparative analysis. Additional details on the compared methods are provided in the Appendix."}, {"title": "6. Conclusion and Limitation", "content": "In this paper, we investigated object hallucination in LVLMs and identified an inherent imbalance in vision token attention within these models, particularly in spatial positioning, as a key contributing factor. Our findings highlight that such imbalances amplify SPB, thereby increasing the likelihood of hallucinations. To address this challenge, we introduced Uniform Attention Calibration (UAC) and Dynamic Attention Calibration (DAC), two efficient techniques designed for vision token attention correction. UAC offers a training-free solution that estimates SPB using a meaningless input and applies a calibration matrix to mitigate attention imbalances. DAC, on the other hand, is a learnable module that dynamically refines attention weights within the self-attention mechanism. Through extensive evaluations on multiple benchmarks and LVLM architectures, our methods consistently reduce hallucination while improving general perception. These results emphasize the significance of attention calibration in enhancing LVLM reliability and performance, providing a promising direction for future research in mitigating hallucination-related biases.\nLimitation Although this study employs a data-efficient approach to introducing SPB, its effectiveness may be limited when validation data is scarce. A promising direction for future work is the development of fine-grained, data-free calibration techniques and extend the idea of attention calibration to improve spatial perception in LVLMs."}]}