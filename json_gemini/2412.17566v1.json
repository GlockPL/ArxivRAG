{"title": "The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning", "authors": ["Shentong Mo"], "abstract": "Masked autoencoders (MAE) have recently succeeded in self-supervised vision representation learning. Previous work mainly applied custom-designed (e.g., random, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets. However, they ignore the potential role of the self-training (student) model in giving feedback to the teacher for masking and targets. In this work, we present to integrate Collaborative Masking and Targets for boosting Masked AutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple collaborative masking mechanism through linear aggregation across attentions from both teacher and student models. We further propose using the output features from those two models as the collaborative target of the decoder. Our simple and effective framework pre-trained on ImageNet-1K achieves state-of-the-art linear probing and fine-tuning performance. In particular, using ViT-base, we improve the fine-tuning results of the vanilla MAE from 83.6% to 85.7%.", "sections": [{"title": "Introduction", "content": "Masked autoencoders (MAE) (He et al. 2021) have recently achieved advanced success in learning meaningful visual representations for many downstream tasks, e.g., image classification, object detection, and semantic segmentation. Meanwhile, researchers also introduced diverse masking pipelines to show the effectiveness of masked model-ing in learning meaningful representations from video (Tong et al. 2022; Feichtenhofer et al. 2022), audio (Huang et al. 2022), and MRI/CT scans (Chen et al. 2023).\nExploring masking strategies (i.e., pretext tasks) and supervision targets is critical for MAE (He et al. 2021) to capture meaningful features during pre-training. In this work, we aim to simultaneously improve the powerfulness of the pre-text task and target in MAE-based pre-training on images for self-supervised vision representation learning, which boosts the performance of several downstream tasks compared to MAE (He et al. 2021) and DINO (Caron et al. 2021), as shown in Figure 1.\nEarly works (Bao, Dong, and Wei 2021; Atito, Awais, and Kittler 2021; He et al. 2021) on masked image modeling (MIM) mainly applied custom-designed (i.e., random,\nblock-wise, square-wise) masking during pre-training. For instance, BEIT (Bao, Dong, and Wei 2021) used a block-wise masking strategy to reconstruct discrete tokens of masked image patches for pre-training transferrable visual representations. To simplify masked image encoding, the seminal work, MAE (He et al. 2021) directly reconstructed missing pixels of 75% masked patches. MaskFeat (Wei et al. 2022a) studied five different types of features as supervision targets and observed that Histograms of Oriented Gradients (HOG) with local contrast normalization achieved the best performance. SimMIM (Xie et al. 2022) applied a large masked patch size to randomly mask the input image, where RGB values of raw pixels are recovered by a one-layer pre-diction head.\nRecent researchers (Li et al. 2021b; Shi et al. 2022; Wei et al. 2022b; Li et al. 2022) started to leverage a teacher network or adversarial learning to generate the mask and supervision target. Benefiting from CLIP (Radford et al. 2021) pre-trained on 400 million image-language pairs, MVP (Wei et al. 2022b) introduced knowledge from CLIP as guidance to achieve impressive gains for MIM-based self-supervised visual pre-training. AttMask (Kakogeorgiou et al. 2022) applied an attention map generated from a teacher transformer encoder, i.e., iBoT (Zhou et al. 2022) to guide masking for the student pre-training. Similarly, SemMAE (Li et al. 2022) started to integrate semantic-guided masks from a self-supervised part-learning module with diversity constraints on attention.\nWhile the aforementioned methods achieve promising results, they ignore the potential role of the self-training (student) model in cooperating with the teacher for collaborative masking and targets. The main challenge is that the teacher and students have different knowledge levels. The teacher network (e.g., CLIP) masters the knowledge at a higher level than students at initial training, while the student network starts to increase its level across the training. To address the aforementioned challenge, our key idea is to simultaneously incorporate two different knowledge-level models (i.e., teacher and student) to guide masking and generate re-construction targets. During training, we aim to leverage students with self-training knowledge to help the teacher with fixed knowledge to guide MIM-based image pre-training dynamically and powerfully.\nTo this end, we propose a novel masked autoencoder that can integrate the student and teacher networks for collaborative masking and targets, namely CMT-MAE. In particular, we introduce a simple collaborative masking mechanism through linear aggregation across attention maps from both teacher and student models, which improves the powerfulness of guided masks. To further boost the performance of downstream tasks, the proposed framework selects representations generated from those two models as the collaborative target for the decoder during pre-training.\nOur pre-training process is composed of two stages: In the first stage, a teacher transformer encoder (i.e., CLIP) takes an input image to extract an attention map from the last attention layer to guide masking. The student encoder generates features from unmasked patches, which are concatenated with masked tokens to feed into a decoder for recovering the teacher features of masked patches. In the second stage, we apply a student momentum encoder to generate a student-guided attention map and linearly aggregate it with a teacher-guided attention map to produce the collaborative attention map with a collaborative ratio for collaborative masking. Then masked tokens concatenate with features of unmasked patches from the student encoder to feed into the decoder. Finally, two predicted heads are linearly applied to reconstruct the teacher and student features of masked patches for collaborative targets. It should be noted that the collaborative ratio is also applied to calculate collaborative losses from the teacher and student targets.\nExperimental results on ImageNet-1K, MS-COCO, ADE-20K, and DAVIS 2017 demonstrate the state-of-the-art performance of our CMT-MAE. In particular, using the backbone of the ViT-base, we improve the fine-tuning results of the vanilla MAE from 83.6% to 85.7%, and linear probing from 68.0% to 79.8%. Our method also achieves +4.8 mIoU (i.e., 48.1 \u2192 52.9) on ADE20K semantic segmentation, +6.6 (J&F)m (i.e., 51.0 \u2192 57.6) on DAVIS video segmentation, +2.5 APbox (i.e., 50.3 \u2192 52.8) on COCO object detection, and +0.8 Apmask (i.e., 44.9 \u2192 45.7) on COCO instance segmentation. In addition, qualitative visualizations of collaborative attention vividly showcase the effectiveness of our CMT-MAE in learning meaningful representations. Extensive ablation studies also demonstrate the importance of collaborative masking and collaborative targets in learning masked autoencoders for improving downstream performance.\nOur main contributions can be summarized as follows:\n\u2022 We present a simple yet effective masked autoencoder that can achieve collaborative masking and targets, called CMT-MAE, for boosting MIM-based visual pre-training.\n\u2022 We propose a novel collaborative masking mechanism through linear aggregation across attention maps from both teacher and student networks to achieve powerful guidance.\n\u2022 Extensive experiments comprehensively demonstrate the state-of-the-art superiority of our CMT-MAE over previous baselines on downstream tasks."}, {"title": "Related Work", "content": "Self-supervised Visual Learning. Self-supervised visual learning aims to mine the internal characteristics from images without annotations by applying well-designed pretext tasks. Early non-transformer researchers introduced instance-level (Wu et al. 2018; Chen et al. 2020a,b; Grill et al. 2020; He et al. 2020; Chen et al. 2020c; Chen and He 2021; Zbontar et al. 2021; Wu et al. 2023a; Mo, Sun, and Li 2023c,a; Wu et al. 2024b) and cluster-based (Caron et al. 2020; Li et al. 2021a; Wang, Liu, and Yu 2021; Mo, Sun, and Li 2021, 2022) contrastive learning to pull representations from positive samples closer while pushing away features from negative pairs. Recently, contrastive learning has been widely used in self-supervised vision transformers (Chen, Xie, and He 2021; Xie et al. 2021; Caron et al. 2021; Mo, Sun, and Li 2023b; Mo and Yun 2024; Mo and Tong 2024) to achieve promising performance on visual downstream tasks. Typically, MoCov3 (Chen, Xie, and He 2021) introduced a momentum encoder in ViT (Dosovitskiy et al. 2021) to minimize the distance between representations of two augmented views from the base encoder and momentum one. To capture the local-to-global alignment, DINO (Caron et al. 2021) used a momentum encoder with multi-crop training to achieve knowledge distillation in the vision transformer. In this work, our main focus is to learn meaningful visual representations in self-supervised transformers through another acclaimed technique, i.e., masked image modeling.\nMasked Image Modeling. Masked image modeling (MIM) has been explored in many previous works (Bao, Dong, and Wei 2021; Atito, Awais, and Kittler 2021; He et al. 2021; Wei et al. 2022a; Xie et al. 2022; Wu and Mo 2022; Wu et al. 2023b, 2024a) to reconstruct the masked image patch given the unmasked counterpart as clues. Early MIM approaches (Bao, Dong, and Wei 2021; Atito, Awais, and"}, {"title": "Method", "content": "Given an image with masked and unmasked patches, our target is to train a masked autoencoder framework with an encoder and a decoder to recover the masked patches using unmasked counterparts. We present a simple yet effective masked autoencoder with collaborative masking and targets, named CMT-MAE, which mainly consists of two modules, Collaborative Masking and Collaborative Targets."}, {"title": "Preliminaries", "content": "In this section, we first describe the problem setup and notations, and then revisit the masked image modeling in MAE (He et al. 2021) and teacher-guided MAE for self-supervised visual pre-training."}, {"title": "Problem Setup and Notations", "content": "Given an image with a dimension of 3 \u00d7 H \u00d7 W and a patch resolution of P, our goal is to learn a masked autoencoder framework with an encoder fe(\u00b7) and a decoder fd(\u00b7) to recover the masked"}, {"title": "Collaborative Masking", "content": "In order to explicitly achieve collaborative masking guided by both the teacher and student, we propose a two-stage training paradigm with a simple collaborative masking mechanism through linear aggregation across attention maps from both models to improve the powerfulness of generated masks. Specifically, in the first stage, we leverage a teacher transformer encoder (i.e., CLIP (Radford et al. 2021)) takes an input image to extract an attention map  from the last attention layer to guide masking. The student encoder generates features from unmasked patches, which are concatenated with masked tokens to feed into a decoder for recovering the teacher features  of masked patches.\nSince momentum encoders (Tarvainen and Valpola 2017; He et al. 2020) is a technique often used in self-supervised and semi-supervised learning to obtain slow-moving target representations, leading to more stable self-training and enhanced representations. In the second stage, as shown in Figure 2, we leverage a student momentum encoder to take the input image to generate a student-guided attention map . Following (Tarvainen and Valpola 2017; ?), we update the student momentum encoder using an exponential moving average of the corresponding online encoders with coefficient m.1 The student  and teacher  attention maps are then linearly aggregated into a final collaborative map  of the form as\n$A^{c} = \\alpha * A^{s} + (1 - \\alpha) * A^{t}$        (3)\nwhere \u03b1 denotes the collaborative ratio. The collaborative map  is followingly applied to generate a masking set  to split the input image into masked and unmasked patches for the second training stage."}, {"title": "Collaborative Targets", "content": "Beyond collaborative masking, we introduce collaborative targets as the training objective in the second stage to dynamically select representations from both the teacher and student for simultaneous optimization. With a masking set  guided by the collaborative attention map , we concatenate masked tokens with features of unmasked patches from the student encoder to feed into the decoder. Since updates to the student encoders are slowly incorporated into the momentum encoders, the target representations display smoother behavior during the training process. In the end, we apply two predicted heads to linearly reconstruct the teacher features  and student features  of masked patches for collaborative targets.\nThe overall objective of our model in the second stage is simply optimized in an end-to-end manner as:\n$L_{cmt-mae} = \\frac{1}{|M^{c}|} \\sum_{i \\in M^{c}} \\alpha*||f^{s}_{i} -\\hat{f}^{s}_{i}||^{2}_{2} + (1-\\alpha) *||f^{t}_{i} - \\hat{f}^{t}_{i}||^{2}_{2}$       (4)\nwhere  denotes the total number of masked patches in the collaborative masking set . ,  denote the target and prediction of student features, and ,  for teacher features. \u03b1 is the collaborative ratio. Note that the collaborative ratio \u03b1 is applied to calculate collaborative losses from the teacher and student targets."}, {"title": "Experimental setup", "content": "Datasets. Following previous methods (He et al. 2021), we use ImageNet-1K (Deng et al. 2009) for image classi"}, {"title": "Comparison to prior work", "content": "In this work, we propose a novel and effective framework with MAE pre-training for downstream tasks, i.e., linear probing, fine-tuning, object detection, instance segmenta-"}, {"title": "Image classification", "content": "Table 1 reports the quantitative comparison results of linear probing and fine-tuning on pre-trained ViT-B/16 and ViT-L/16 models. As can be seen, we achieve the best performance in terms of all metrics for both models. In particular, the proposed CMT-MAE significantly outperforms MAE (He et al. 2021), the original masked image modeling baseline, by 11.8% & 2.1% and 6.5% & 1.3% relative top-1 accuracies in terms of linear probing & fine-tuning on ViT-B/16 and ViT-L/16 models. Moreover, we achieve superior performance gains compared to SemMAE (Li et al. 2022), the recent MIM-based pre-training approach that applied semantic-guided masks and diversity constraints on attention. Meanwhile, our CMT-MAE outperforms iBoT (Zhou et al. 2022) by 1.7% and 2.4% relative top-1 accuracies in terms of fine-tuning on ViT-B/16 and ViT-L/16 models. The proposed CMT-MAE also achieves better results than DINO (Caron et al. 2021), a strong contrastive-based pre-training baseline. These significant improvements demonstrate the superiority of our method in learning better representations during pre-training for image classification."}, {"title": "Object detection & instance segmentation", "content": "For the COCO object detection & instance segmentation benchmarks, we report the quantitative comparison results of COCO object detection and instance segmentation on the pre-trained ViT-B/16 model in Table 2. We can observe that the proposed CMT-MAE achieves the best results in terms of all metrics. Compared to MAE (He et al. 2021) trained on 1600 epochs, we achieve performance gains of 2.5@Apbox and 0.8@Apmask. We also achieve highly better results than other contrastive-based (Caron et al. 2021) and MIM-based (Bao, Dong, and Wei 2021; Li et al. 2022) pre-training approaches."}, {"title": "Semantic segmentation", "content": "Table 2 also shows the quantitative comparison results of ADE20K semantic segmentation on the ViT-B/16 model pre-trained on ImageNet-1K. Our CMT-MAE significantly outperforms MAE (He et al. 2021) by 4.8@mIoU and also achieves better performance than MVP (Wei et al. 2022b), the strong baseline using CLIP (Radford et al. 2021) knowledge pre-trained on 400"}, {"title": "Video object segmentation", "content": "We also present additional video object segmentation on the DAVIS 2017 benchmark using ImageNet-1K pre-trained ViT-B/16 and ViT-L/16 models, as shown in Table 3. We achieve superior performance gains of 6.6@(J&F)m, 7.3@Im, 5.9@Fm, and 7.1@(J&F)m, 7.2@Im, 7.0@Fm in terms of pre-trained ViT-B/16 and ViT-L/16, compared to MAE (He et al. 2021). We also achieve better results than I-JEPA (Assran et al. 2023), the recent joint embedding predictive architecture for image self-supervised learning. These qualitative results also showcase the effectiveness of our CMT-MAE in generating much more accurate and high-quality segmentation masks on video objects compared to MAE (He et al. 2021), as shown in Figure 3."}, {"title": "Experimental analysis", "content": "In this section, we performed ablation studies to demonstrate the benefit of introducing Collaborative Masking (CM) and Collaborative Targets (CT) modules. We also conducted extensive experiments to explore the impact of collaboration ratio a and learned meaningful collaborative attention maps.\nCollaborative Masking & Collaborative Targets. In order to validate the effectiveness of the introduced collaborative masking (CM) and collaborative targets (CT), we ablate the necessity of each module and report the quantitative results in Table 4. We can observe that adding CM to the vanilla baseline highly increases the results of 5.5@Linear Probing, 0.6@Fine-tuning, 1.9@APbox, 0.9@Apmask, 2.2@mIoU, 2.8@(J&F)m, 2.4@Jm, 3.2@Fm, which demonstrates the benefit of collaborative masking in generating meaningful representations guided by both the teacher and student during pre-training. Meanwhile, introducing only CT in the baseline also increases the downstream performance in terms of all metrics. More importantly, incorporating CM and CT together into the baseline significantly raises the performance by 11.8 @Linear Probing, 2.1@Fine-tuning, 4.4@APbox, 3.1@Apmask, 6.8@mIoU, 6.6@(J&F)m, 7.3@Jm,5.9@Fm. These improving results validate the importance of collaborative masking and targets in learning collaborative representations from both teacher and student for masked autoencoders.\nTrade-off on Collaborative Ratio. The number of collaborative ratios in the proposed collaborative masking and targets affects the pre-trained representations for diverse downstream tasks. To explore such effects more comprehensively, we varied the number of ratios from {0%, 10%, 30%, 50%, 70%, 90%, 100%}. The comparison results of all downstream tasks using a ViT-B/16 model pre-trained on ImageNet-1K are reported in Table 5. When the number of collaboration ratio a is 30%, we achieve the best downstream performance in terms of all metrics. With the increase of collaboration ratio from 0% to 30%, the proposed CMT-MAE consistently raises results, which shows the importance of collaborative masking and collaborative targets in masked autoencoders for learning discriminative representations. However, increasing the collaboration ratio from 30% to 90% will not continually improve the result since there might be a trade-off between the teacher and student to learn different representations during pre-training."}, {"title": "Conclusion", "content": "In this work, we present CMT-MAE, a simple yet effective masked autoencoder that can simultaneously achieve collaborative masking and targets. We leverage a novel collaborative masking mechanism through linear aggregation across attentions from both teacher and student models. We further use their representations as the collaborative target of the decoder for reconstruction. Experimental results on ImageNet-1K, MS-COCO, ADE-20K, and DAVIS 2017 validate the state-of-the-art superiority of the proposed framework. In addition, qualitative visualizations vividly showcase the effectiveness of our CMT-MAE in capturing meaningful representations for downstream tasks. Extensive ablation studies also demonstrate the importance of collaborative masking and collaborative targets in learning masked autoencoders for improving downstream performance."}]}