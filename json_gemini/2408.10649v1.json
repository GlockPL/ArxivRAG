{"title": "Inferring Underwater Topography with FINN", "authors": ["Co\u015fku Can Horuz", "Matthias Karlbauer", "Timothy Praditia", "Sergey Oladyshkin", "Wolfgang Nowak", "Sebastian Otte"], "abstract": "Spatiotemporal partial differential equations (PDEs) find extensive application across various scientific and engineering fields. While numerous models have emerged from both physics and machine learning (ML) communities, there is a growing trend towards integrating these approaches to develop hybrid architectures known as physics-aware machine learning models. Among these, the finite volume neural network (FINN) has emerged as a recent addition. FINN has proven to be particularly efficient in uncovering latent structures in data. In this study, we explore the capabilities of FINN in tackling the shallow-water equations, which simulates wave dynamics in coastal regions. Specifically, we investigate FINN's efficacy to reconstruct underwater topography based on these particular wave equations. Our findings reveal that FINN exhibits a remarkable capacity to infer topography solely from wave dynamics, distinguishing itself from both conventional ML and physics-aware ML models. Our results underscore the potential of FINN in advancing our understanding of spatiotemporal phenomena and enhancing parametrization capabilities in related domains.", "sections": [{"title": "1 Introduction", "content": "Machine learning models that incorporate physical principles follow the governing rules about the problem at hand. Such a physics-aware model outperform pure ML models when tackling physical phenomena [10, 17]. The success and enhanced performance of physics-aware ML models have been demonstrated across a variety of applications, underscoring their power [3, 12, 13, 19, 20]. Nevertheless, as in every emerging field, they have not reached their full potential. Especially the physics-aware models lack the ability to structurally incorporate explicitly defined physical equations as well as the capacity to reconstruct underlying physical structures from the data. Recent introduction of the finite volume neural network (FINN) marks a significant step forward in addressing these limitations [15, 16, 9]. FINN combines the well-studied physical models with the learning capabilities of artificial neural networks and models partial differential equations (PDEs) in a mathematically compositional manner. A notable feature of FINN is its ability to directly incorporate boundary conditions (BC) demonstrating an example of explicitly embedded physical structure [9]. Excitingly, FINN can also handle the BC that were not considered during training phase and infer unknown Dirichlet boundary conditions, which is an example of hidden information inference from data [5,6]. In this study, we explore the FINN concept to reconstruct underwater topography using the shallowwater equations (SWE), a set of PDEs that model wave dynamics. By adapting FINN to solve these equations, we open new pathway for a broader area of application of the model. We compare the quality of the parameterized topography with two models, DISTANA [8] and PhyDNet [3]. Our results indicate that the models are able to reconstruct the physical processes and predict the wave patterns, but FINN's embedded robust physical structure makes it stand out."}, {"title": "2 Models", "content": "This section provides an overview of the models under comparison: PhyDNet, DISTANA, and FINN. The descriptions of DISTANA and PhyDNet are presented with less detail in comparison to FINN, as the model of interest."}, {"title": "2.1 FINN", "content": "The finite volume neural network (FINN) introduced in [9, 15] represents a novel fusion of the finite volume method (FVM), a well-established numerical technique in computational physics, with the learning capabilities of deep neural networks. The FVM spatially discretizes continuous PDEs, transforming them into algebraic (linear) equations defined over a finite number of control volumes. These control volumes possess distinct states and exchange fluxes following the conservation laws. By embedding physical principles within the FVM structure, FINN is inherently constrained to enforce (partially) known laws of physics. Consequently, this hybrid approach yields an interpretable, highly generalizable, and robust computational methodology. FINN solves PDEs that convey non-linear spatiotemporal advection-diffusion-reaction processes. It solves the equations in the following form (as formulated in [9]): \n\n$$\\frac{\\partial u}{\\partial t}=D(u) \\frac{\\partial^{2} u}{\\partial x^{2}}-v(u) \\frac{\\partial u}{\\partial x}+q(u)$$\n\nwhere $u$ is the unknown function of time $t$ and spatial coordinate $x$, which encodes a state. The objective of a PDE solver (if the PDE was fully known) is to find the value of $u$ in all time steps and spatial locations. However, Equation 1 is composed of three, often unknown functions, which modify $u$, i.e. $D, v$, and $q . D$ is the diffusion coefficient, which manages the equilibration between high and low concentrations, $v$ is the advection velocity, which represents the movement of concentration due to the bulk motion of a fluid, and $q$ is the source/sink term, which increases or decreases the quantity of $u$ locally. FINN is capable to model the physical systems corresponding to Equation 1 in both one and two dimensional problems. \n\nFigure 1 and Equation 2 illustrate how FINN approach models the PDE for a single control volume $i$. The first- and second-order spatial derivatives $\\left(\\frac{\\partial u}{\\partial x}, \\frac{\\partial^{2} u}{\\partial x^{2}}\\right)$, are approximated by a linear layer, $\\varphi_{\\mathcal{N}}$, aiming to learn the FVM stencil, i.e., the exchange terms between adjacent volumes. $\\varphi_{\\mathcal{D}}, \\varphi_{\\mathcal{A}}, \\Phi_{\\psi}$ are also linear layers approximating $D(u), v(u)$ and $q(u)$, respectively. $\\mathcal{R}(\\cdot)$ is a point-wise mapping, which manipulates the ReLU activation function to ensure the correct flux direction (i.e. the sign of advective values). The estimated values are combined together as in Equation 1 in two different kernels, called flux and state kernel. Flux kernel computes the values in the adjacent volumes wheres state kernel receives these fluxes and combines them with the source/sink term which simply decides if there is an extra incoming or outgoing flux in that particular volume. As can be seen in Figure 1, all modules' outputs are summed up to conclude in $\\frac{\\partial u}{\\partial t}$, which results in a system of ordinary differential equation (ODE) solvable over time, e.g. via Euler method. On a high level, FINN operates as a data-driven ODE maker, utilizing spatial data (PDE response or measurements) to resolve the spatial dimensions through FVM, thereby reducing the problem to a temporal ODE. The resulting ODE is fed into a solver, which produces a prediction for the next time step. This process is applied to all control volumes in the FVM mesh and the prediction is fed back to the model in a closed-loop fashion until the end of the sequence is reached. The error is computed via $\\mathcal{L}\\left(\\hat{a}_{i}^{(1: T)}, u_{i}^{(1: T)}\\right)$, where $(1: T)$ corresponds to the entire sequence and $i$ is the discretized spatial control volume index. Mean squared error is used as loss function. For a comprehensive understanding of FINN's underlying mechanisms, including its kernel functions and different neural network structures, readers are referred to [9]."}, {"title": "2.2 DISTANA", "content": "The distributed spatiotemporal graph artificial neural network architecture (DISTANA) is a hidden state inference model for time series prediction. It encodes two different kernels in a graph structure. First, the prediction kernel (PK) network predicts the dynamics at each spatial position while being applied to each node of the underlying mesh. Second, the transition kernel (TK) network coordinates the lateral information flow between PKs. Thus allowing the model to process spatiotemporal data."}, {"title": "2.3 PhyDNet", "content": "PhyDNet is a physics-aware encoder-decoder model introduced in [3]. First, it encodes the input at time step $t$. Afterward, the encoded information is disentangled into two separate networks, PhyCell and ConvLSTM-Cell. Inspired by physics, PhyCell implements spatial derivatives up to a desired order and can approximate solutions of a wide range of PDEs, e.g., heat equation, wave equation, and the advection-diffusion equation. Moreover, the model covers the residual information that is not subsumed by the physical norms using neural networks. Concretely, the ConvLSTMCell complements the PhyCell and approximates the residual information in a convolutional deep learning fashion. The outputs of the networks are combined and fed into a decoder in order to generate a prediction of the unknown function at the next time step. PhyDNet is a state-of-the-art physics-aware neural network, which applies to advection-diffusion equations and was therefore selected as a physics-aware baseline in this study."}, {"title": "3 Shallow-Waters Equations", "content": "Shallow water equations (SWE) represent fluid dynamics where the wavelength $\\lambda$ (distance between consecutive waves) is much larger than the depth of the fluid $H$ : \n\n$$\\frac{H}{\\lambda} \\ll 1$$\n\nThe corresponding fluid does not have to be water but SWE are predominantly applied to aquatic environments. This is due to the common occurrence of the shallow-water assumption given in Equation 3 in modelling the dynamics of stream beds, such as those found in rivers and seas. SWE are essentially a simplified version of the Navier-Stokes equations, derived from the principles of continuity and momentum conservation, specifically tailored to account for the properties of water. In this study, the nonlinear form of the continuity equation is considered, whereas momentum equations are taken to be linear. The specific form of the SWE employed in this study are taken from [11] and derived in Appendix A. SWE are defined as follows (see Figure 2 for an illustration of SWE with the respective terms from the equations below): \n\n$$\\begin{aligned} & \\frac{\\partial \\eta}{\\partial t}+\\frac{\\partial}{\\partial x}[u(H+\\eta)]+\\frac{\\partial}{\\partial y}[v(H+\\eta)]=0 \\\\ & \\frac{\\partial u}{\\partial t}=-g \\frac{\\partial \\eta}{\\partial x} \\\\ & \\frac{\\partial v}{\\partial t}=-g \\frac{\\partial \\eta}{\\partial y}\\end{aligned}$$\n\nwhere $u$ (not to confuse with the unknown state in Equations 1 and 2) and $v$ are the velocity vectors in $x$ - and $y$-directions, respectively. $H$ is the topography of the fluid represented by the depth length of each grid in meters. The term topography is interchangeably used with depth highlighting that $H$ represents the depth in case of a flat surface whereas it represents a topography when it is not flat. $\\eta$ is the displacement of the free surface, essentially capturing the vertical deviation of the fluid surface from the mean depth as waves propagate. Lastly, $g$ expresses the gravitational force that arises from the gravitational field of the earth. A thorough derivation and the visualization of SWE are provided in Appendix A."}, {"title": "4 Underwater Topography Reconstruction", "content": "Following the previous work in [6], this study delves into FINN's capacity to infer hidden information in data. Nevertheless, there are substantial differences. First of all, SWE is two-dimensional and is a combination of three equations. Second, the aim is to reconstruct the topography from $\\eta$ but $H$ does not have a direct influence on $\\eta$. It also does not change the physical domain drastically as in the case of boundary conditions such that a change in $H$ causes only a small and indirect change in $\\eta$ through velocities. Therefore, it would be fair to say that the topography $H$ in SWE is literally the underlying structure in data. It is closely coupled to the velocities. The relationship between velocity and topography is mathematically expressed through the eigenvalues of the velocity tensors $u \\pm \\sqrt{g \\cdot H}$ and $v \\pm \\sqrt{g \\cdot H}$ (cf. [1]). Through this tied relation between velocity and topography, it is possible for FINN to infer $H$. As SWE is different than the other equations in FINN's repertoire, a corresponding model modification was necessary. The visualization of this modification can be seen in Figure 3. Each FINN module, corresponds to the architecture in Figure 1 without the solver (it is specifically given in Figure 3). Different FINN modules approximate different unknowns (i.e. $\\eta$ and the velocities $u$ and $v$ ). The adaptation process involves sequentially computing the unknown state $\\eta$ in Equation 4 and also the velocities $u$ and $v . \\eta^{(t=0)}$ is the initial condition which is fed into the model as in the standard FINN way. This first model, called FINN $_{\\text {velo }}$, receives the initial state as input. For the initial state, the following Gaussian bump is used: \n\n$$\\eta_{x, y}^{(t=0)}=\\exp \\left(-\\left(\\frac{\\left(x-x_{0}\\right)^{2}}{2 \\sigma^{2}}+\\frac{\\left(y-y_{0}\\right)^{2}}{2 \\sigma^{2}}\\right)\\right)$$\n\n$x_{0}$ and $y_{0}$ are randomly chosen points on the grid for each sequence and standard deviation $\\sigma$ is set to $5 \\cdot 10^{4}$. The Gaussian bump simulates the effect of a huge rock dropped into the water to create waves. Depending on the initial state, FINN $_{\\text {velo }}$ approximates the time derivatives of the velocities at the next time step $(t+1)$ and produces the ODEs $\\frac{\\partial \\hat{u}^{(t+1)}}{\\partial t}$ and $\\frac{\\partial \\hat{v}^{(t+1)}}{\\partial t}$. These predictions are sent to the solver to compute $\\hat{u}^{(t+1)}$ and $\\hat{v}^{(t+1)}$. Euler solver is used in our study, although it is possible to use finer-grained solvers. In the preliminary studies, the Euler method worked sufficiently well and thus the method with the least computational cost is chosen. The resulting velocities for the next step along with $\\eta^{(t)}$ are provided to the second model, which is called FINN $_{\\eta}$. To estimate $\\frac{\\partial \\eta^{(t+1)}}{\\partial t}$, the velocities $u$ and $v$ evaluated at $\\left(H_{i, j}+\\dot{\\eta}_{i, j}^{(t)}\\right)$, where $i$ and $j$ correspond to the spatial coordinates, must be approximated (cf. Equation 4). These intermediate values are computed as a combination of the neural network predictions inside FINN $_{\\eta}$. Afterwards the spatial derivative of the intermediate values is determined via the finite difference method (FDM), a simplified version of FVM. This procedure results in $\\frac{\\partial \\eta^{(t+1)}}{\\partial t}$, which is again an ODE that is solved by the solver. The resulting state $\\dot{\\eta}^{(t+1)}$ is fed back into FINN $_{\\text {velo }}$ in a closed-loop fashion until the end of the sequence is reached. The above mentioned adaptation of FINN to solve multiple equations necessitated significant changes, not only in its operational framework but also in its neural network architecture. In standard FINN, $\\varphi_{\\mathcal{N},-}$ and $\\varphi_{\\mathcal{N},+}$approximate the stencils between two adjacent volumes. This allows FINN to be adapted to both unstructured and structured grids. In current work, $\\varphi_{\\mathcal{N}}$ estimates the spatial derivatives such as $-g \\frac{\\partial \\eta}{\\partial x}$ and $-g \\frac{\\partial \\eta}{\\partial y}$ in continuity equations which are equal to the time derivatives $\\frac{\\partial u}{\\partial t}$ and $\\frac{\\partial v}{\\partial t}$, respectively. $g$ is the gravitational constant and the network weights can effectively generalize for $g$ alongside the spatial derivatives. These derivatives are approximated by multilayer perceptrons (MLP) with an extra inductive bias. It is implemented in MLP through the explicit presentation of adjacent cells similar to FVM/FDM. The tensors $\\eta_{i, j}^{(t)}$ and $\\eta_{i+1, j}^{(t)} \\forall i \\leq(X-1)$, where $X$ corresponds to the width of the domain, are stacked on top of each other and provided to the model jointly. Accordingly, tensors $\\eta_{i, j}^{(t)}$ and $\\eta_{i, j+1}^{(t)}$ used for the horizontal direction in the same way. Thus the network receives each time two horizontally or vertically adjacent cells as input. This configuration enables the network to adjust its weights according to the value changes between adjacent cells. An important point worth considering in such implementation of feedforward neural networks is the utilization of boundary conditions. Specifically, when the cells at locations $(i, j)$ and $(i+1, j)$ are stacked, it is not possible to take the last cell on the edge as $i$ because the cell $(i+1)$ would simply not exist. Therefore $i$ has to be less than or equal to $(X-1)$. This means that when two adjacent cells are stacked together, the resulting tensor will have a size that is one unit smaller than the spatial domain size. Hence, when adjacent cells are stacked together, the boundaries should be handled carefully. In this study, the boundary conditions are applied to the output of the networks explicitly. For velocity tensors, boundary values are set to zero to adhere to the no-slip condition, a fundamental principle in fluid dynamics. For $\\eta$-intermediate tensors, which are estimated in FINN $_{\\eta}$ module, the boundary values of $\\eta$ are used such that the value leaving the domain from one side is fed back into the domain in the same side again. This is done to create the wall effect on the edges. Changes have also been made to the comparative models in order to reconstruct the topography. In DISTANA, lateral information is shared across the prediction kernel on the domain. This is accomplished by convolution kernels applied to data (the unknown state). To embed the topography in the model, we stacked the topography grid onto data, allowing kernels to adjust themselves via gradient descent. In the convolutional sense, basically, another extra input channel was added through stacking operation. PhyDNet adopts a similar strategy, with the primary implementation being in the ConvLSTM encoder branch of the model. Training and inference processes follow the same scheme in each model and are worth explaining as they are different than in previous studies. In [5] and [6], it was shown that a multi-BC training scheme improves performance at inference. In these previous studies, models received several sequences simultaneously in training (mini-batch learning scheme), each with a different set of boundary conditions. This allowed models to learn the effect of different BCs such that the trained model could infer the unknown BCs more accurately. With this insight, we trained models in multiH scheme. Different sequences (in total 512) each with a different topography were fed into the model. The ground-truth topography in data was calculated by the inverse of the tangent function such that the depth increases with distance and it has a smooth surface (see left plot in Figure 4). It is situated on a $1000 \\mathrm{~km}^{2}$ structured mesh with an average depth of 100 meters. Moreover, $H$ is rotated with a random angle $\\phi \\in[0,2 \\pi]$ and the depth of $H$ is scaled with a random number $\\beta \\in[0.5,1.0]$ such that the average depth is between $50-100 \\mathrm{~m}$ in each sequence. Additionally, the initial conditions, represented by a starting location of the Gaussian bump, are also randomly chosen. This vast diversity in topography data, with $H$ explicitly provided during training, is significantly beneficial for the models, exposing them to a wide range of topographies. After training on various sequences and topographies, the task is to infer a particular topography from data. However, accomplishing this task with a single sequence could cause problems because there may be artifacts in the data. Especially the Gaussian initialization could interfere the process and cause a gradient-induced peak in the topography. Consequently, relying on limited data could inadvertently result in models overfitting to these artifacts. Therefore different sequences with the same topography were generated (in total 256). Topography used in inference data is bumpy, and more non-linear but has an average depth inside of the training depth range (see right plot in Figure 4). Identical to the training data, the topography was scaled with a randomly sampled number from $[0.5,1]$ (the scale factor $\\beta=0.68$ ). Note that as the same topography is used for each inference sequence, they are also scaled by the same scale factor. Similar to the previous experiments in [5] and [6], $H$ is set as learnable parameter. Hence the number of total parameters is the same as the number of grid cells (in this study $32 \\times 32$ grid). Given the gradient-based optimization framework and the use of mean squared error for loss calculation, this approach does not inherently consider the spatial relationships between grid cells of H. In preliminary studies, we realized that this approach leads to an unrealistic, distorted topography reconstruction. It was also not possible to apply active tuning [14] as the topography does not change over time. To address this, a smoothness-constraint was introduced, penalizing significant deviations between adjacent cells to enhance the smoothness of surface. The regularization constant $\\lambda$ is set to a small value (i.e. $5 \\cdot 10^{-3}$ for PhyDNet and $5 \\cdot 10^{-7}$ for the others) such that it was still possible to represent the non-linearity of the topography. Another problem we faced in preliminary studies was the reconstruction of the edges. As stated before, $H$ is coupled with the velocity tensors and these have zero velocity on the edges due to the no-slip condition. As a result, the values of neighboring cells showed significant differences at the edges, leading to a deformed reconstruction. Another constraint was applied explicitly on the edges such that the values on the edges move closer to the ones towards the inside ( $\\lambda_{\\text {edge }}=5 \\cdot 10^{-7}$ ). It is an unconventional employment of loss constraint but it basically regularizes edges especially because of their specific situation. The use of the $\\lambda_{\\text {edge }}$ regularization resulted in performance improvements across all models. However, both PhyDNet and DISTANA still struggled to produce accurate reconstructions along the edges. Results with and without edge reconstruction are provided for a detailed comparison in the following section."}, {"title": "5 Experiments", "content": "Throughout the experiments, the primary objective is to achieve optimal reconstruction results for each model. The findings are reported in Table 1. It is noteworthy that DISTANA demonstrated performance comparable to that of FINN. Notably, given the wide range of values observed within the inference topography, falling within the interval $[63,74]$, the errors attained by these two models carry particular significance. All three models successfully learned SWE and accurately predicted subsequent wave states with varying levels of accuracy. However, it is important to acknowledge the computational demands associated with PhyDNet, which imposes a significant computational burden. The slightly higher inference error exhibited by PhyDNet can be attributed to the application of a stricter grid regularizer aimed at enhancing reconstruction quality. Using the same $\\lambda$ value for PhyDNet as other two modules has tripled the reconstruction error and the inferred topographies were predominantly characterized by spikes. The most accurate topography reconstructions for each model are depicted in Figure 5 and Figure 6. While Figure 5 offers a realistic and intuitive portrayal of the topographies, Figure 6 provides a clearer visualization of discrepancies. The topography inference process of FINN is also given in much detail in Appendix B. These visualizations confirm that FINN and DISTANA manage to reconstruct the topography although FINN's precision is superior. In particular, DISTANA exhibits notable deviations along the edges, attributable to inadequacies in boundary condition implementation. As it was also stated in the previous works [9, 6], FINN rigorously applies boundary conditions based on the governing equations. However, DISTANA and PhyDNet rely on convolution operations for this purpose, leading to poorer edge reconstruction. This distinction underscores FINN's advantage in handling boundary conditions. Therefore, to ensure a fair comparison, we show two different reconstruction errors in Table 1. Full error encompasses the error of the entire spatial domain, i.e. the grid size is $32 \\times 32$. Inner domain error, on the other hand, omits 5 data points on each boundary for a fair comparison, i.e. the grid size is 28 . Indeed, the error difference between DISTANA and FINN is closer within the inner domain, however, DISTANA's error remains approximately double of FINN's error. PhyDNet, on the contrary, fails to produce meaningful reconstruction. The high standard deviation of PhyDNet's prediction is also another argument that it does not reliably perform on this task. One important point that could catch the eyes of the reader is the difference between training and test error by FINN. Normally it is expected to have smaller training error. Nevertheless, the conventional error computation is not used in this experiment due to the distinct topographies present in the training and testing datasets. Consequently, this leads to different error regimes. Although the topography of the training data is smooth and mildly non-linear, it spans from 55 to 82 with a range of $82-55=27$. This range is 2.45 times bigger than the topography values of the inference/test set which has a range of $74-63=11$. Thus, the training set has a larger error regime. This case has been verified numerically as well. We have observed that a model trained without inference on both datasets exhibited an error scheme similar to the results in Table 1, confirming the impact of topography on the training process."}, {"title": "6 Discussion", "content": "The aim of this study was to assess the capabilities of DISTANA, PhyDNet, and FINN in inferring the underwater topography depending on the shallow water equations. Compared with the previous investigations[15, 16, 9, 5, 6], SWE differ in two aspects. Firstly, behaviors of SWE are characterized not by a single PDE but by a system of three coupled PDEs. Secondly, SWE is a dynamic equation such that the waves move until the end of the sequence. All the previously examined systems by FINN reach a sort of equilibrium state after a while. To adapt FINN to these novelties, it was modified in different ways which are explained in section 3 and section 4. These developments on FINN are not only useful in reaching the goal of this study; they also pave the way for a broader application of the model across various domains. FINN produced remarkable results. Along with the low test error, it produced accurate reconstruction of the underwater topography (see Figures 5 and 6). Although inferior to FINN, DISTANA managed to successfully solve the SWE system and inferred the underlying topographical structure. This aligns findings from the previous study by [7], which demonstrated DISTANA's capabilities in inferring latent land-sea mask. PhyDNet, despite its physics-aware concept, managed to model the SWE but struggled to accurately reconstruct a topography. PhyDNet is a large model, compared to the other models in this study. This causes an overfitting scenario to the training data such that the model is too complex for the problem and cannot give enough importance to the embedded topography."}, {"title": "7 Conclusion", "content": "In addressing physical problems, pure ML models often stay too universal especially in inferring unknown structures in data. This study underscores the crucial need for a physically structured model, encapsulating application-specific inductive bias, to complement the learning abilities of neural networks. FINN encompasses these two aspects by implementing multiple MLP modules through mathematical composition to impose physical constraints. This architecture allows FINN to establish unknown structures in data at inference, setting it apart from other physics-aware ML models. Wide range of spatiotemporal problems can be characterized by advection-diffusion processes and FINN can solve them. In addition to that, FINN can also generalize to certain unknown structures in data with high precision. The current study has demonstrated the potential of FINN to accurately infer and generalize unknown data structures representing the underwater topography. However, the potential applications and scope for FINN-based architectures stretch far beyond current achievements, revealing a vast directions for possible research. The long-term aim could be the application of FINN to a variety of real-world scenarios of a larger scale as an extension to the previous work by [16]. Especially considering the SWE as highly relevant in modeling wave dynamics in coastal regions including tsunamis [2]. The prospect of FINN reconstructing coastal underwater topography of coastal regions from real-world data presents a relevant pathway for exploration. We do hope, that this article opens a road towards real-world data inference. Furthermore, given that SWE represents a simplified version of Navier-Stokes equations, further research efforts could be directed towards integrating more complex versions of these equations into FINN's framework. This advancement will significantly enhance FINN's utility and broaden its application in accurately understanding and predicting natural phenomena."}]}