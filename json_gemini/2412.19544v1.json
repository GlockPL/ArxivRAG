{"title": "TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data", "authors": ["Xiang Huang", "Jiayu Shen", "Shanshan Huang", "Sitao Cheng", "Xiaxia Wang", "Yuzhong Qu"], "abstract": "Semantic parsing, which converts natural language questions into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (TARGA), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning. Experiments on multiple knowledge base question answering (KBQA) datasets demonstrate that TARGA, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA (+7.7) and KBQA-Agent (+12.2). Furthermore, TARGA also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.", "sections": [{"title": "1 Introduction", "content": "Reasoning over structured environments, such as Knowledge Base (KB), Database, and Web, has emerged as a crucial ability for large language models (LLMs) (Liu et al., 2024; Gu et al., 2023). Among various methods for structural reasoning, semantic parsing stands out as a mainstream and has garnered increasing attention from researchers. By translating natural language questions (NLQ) into logic forms, semantic parsing enables seamless interaction with structured environments, thereby enhancing user experience and accessibility."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Few-shot KBQA with LLMs", "content": "With the advancement of large language models, recent works have adopted LLMs as the backend for KBQA. In particular, In-Context Learning (Brown et al., 2020) requires dozens of demonstrations to guide the model's responses. To achieve competitive performance, existing ICL-based KBQA works (Li et al., 2023b; Nie et al., 2023) typically retrieve the most similar examples from a manually annotated training set as demonstrations. However, this strategy often results in performance degradation on non-I.I.D. questions involving unseen structures or KB items. For example, Nie et al. (2023); Li et al. (2023b) reported that the performance in zero-shot settings can be up to 20% lower compared to I.I.D. settings.\nAnother line of KBQA methods, agent-based methods (Liu et al., 2024; Huang et al., 2024; Gu et al., 2024), decomposes questions into individual steps to solve. While step-by-step solving aligns with human intuition and demonstrates remarkable generalization ability, it incurs high computational costs and presents challenges in constructing trajectories. Moreover, the effectiveness of the agent-based paradigm relies heavily on the planning and generalization abilities of advanced LLMs, leading to subpar performance when using weaker models, such as some open-source variants. Such dependency underscores the limitation of agent-based approaches when superior LLMs are unavailable or impractical to use due to resource constraints."}, {"title": "2.2 Synthetic Data Generation", "content": "Instead of relying solely on human annotation for training data, recent works have leveraged LLMs to generate synthetic data, thereby reducing the burden on human annotators. For instance, Chiang et al. (2023); Taori et al. (2023) use instructions to generate training data as a supplement to manual annotation via self-instruct techniques (Wang et al., 2023). However, this approach still requires human-annotated seed examples to ensure high-quality demonstrations, which entails significant demand for LLM usage. Rather than directly prompting LLMs to generate training data, Cao et al. (2022); Huang et al. (2023) address this problem by first sampling structured queries from the environment and then converting these queries into natural language using LLMs. Nevertheless, obtaining meaningful structured queries remains a non-trivial task."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Overview", "content": "As shown in Figure 2, our targeted data synthesis framework, TARGA, consists of four parts. Given a natural language question nlq, we first collect candidate KB items such as entities Enlq and relations Rnlg as initialization. Then, we explore valid query graphs from simple to complex structures to construct synthetic queries Q. Next, we filter high-relevance candidate queries by ranking. Finally, we use these high-quality synthetic data as demonstrations for QA reasoning."}, {"title": "3.2 Candidate KB Items Retrieval", "content": "For candidate entities, we adopt the linking result provided by Gu et al. (2023) for a fair comparison (detail in Appendix E.2). For candidate relations, we compute the similarity of question and freebase relations based on text-embedding-ada-002 and retain the top 20 most similar candidates. Different from previous fine-tuned methods (Ye et al., 2022; Hu et al., 2022), which typically require training a relation linking model for higher precision and recall, our method does not rely on the precision of these items. This is because, although all retrieved KB items are relevant to the question, they do not necessarily form valid combinations within a specific graph structure. In this way, the subsequent query construction steps in Section 3.3 can be viewed as a joint entity-relation disambiguation process, thus significantly reducing the number of invalid queries."}, {"title": "3.3 Synthetic Query Construction", "content": "This stage aims to construct question-targeted queries to facilitate subsequent QA reasoning. Given a knowledge base G and the set of retrieved KB items relevant to the question nlq, we explore the possible query structures Q that are valid (i.e., yield non-empty execution results). However, enumerating all possible structures may lead to an unmanageable combinatorial explosion. To mitigate this, our exploration of candidate queries follows a simple-to-complex manner, where we only further explore new structures that are derived from the sub-structures already verified as valid. Starting from the simplest structure (L\u2081 in Figure 2), we progressively search for more complex query structures through Layer-wise Expansion (for multi-hop structures) and Cross-layer Combination (for multi-constraint structures), gradually extending the obtained query graphs until a desired complexity is achieved.\nLayer-wise Expansion is utilized to model multi-hop structures (the depth of the query graph), which are chain-like, non-branching query structures originating from a single entity. We define Lk as the set of queries in which the distance from the entity to the farthest variable in the chain-like query structure is k. Specifically, we first identify all possible connections between Enlq and Rnlq, forming the simplest query structure L1, where an entity s is connected to a variable o through a single relation p. EXEC(q, G) indicates the execution results of query q against G.\n$L_{1} = \\{(s, p, o) |s \\in E_{nlq}, p \\in R_{nlq}, \\\\ \\qquad EXEC((s, p, o), G) \\neq 0\\}.$\nWe then progressively expand outward from the terminal variable nodes by connecting them to a new variable through another relation to construct L2, and so forth. Generally, Lk+1 is formed by expanding the valid queries from the previous layer (Lk) with an additional edge:\n$L_{k+1} = \\{q \\cup (o_{i}, p', o_{j}) | q \\in L_{k}, O_{i} \\in O(q), \\\\ \\qquad p' \\in R_{nlq}, EXEC (q \\cup (o_{i}, p', o_{j}), G) \\neq 0\\}.$\nwhere O(q) represents the set of variables in q and oj is a newly introduced variable. The expansion process stops when the complexity threshold (e.g., 3 hops) is reached, since for a coherent and reasonable question, the distance between a specific entity and the final answer typically does not exceed three hops."}, {"title": "3.4 Synthetic Query Re-ranking", "content": "To obtain the most relevant examples for the subsequent QA task, we re-rank all valid queries using the bge-reranker-v2-m3 model (Chen et al., 2024) based on their similarity to the question. Additionally, we employ a process called Query Textification, where the synthesized query is transformed into a format closer to natural language through heuristic rules. This step helps bridge the gap between the text embedding model and the query, further improving the quality of the ranking. Detail and examples of textification process are provided in Appendix E.4 and Table 12.\nTo address the imbalance caused by the exponential growth of complex queries, we implement a Hierarchical Ranking strategy. For all queries derived from the same parent query (the sub-query that this query is derived from), we retain only the top n candidates. The final candidate query pool is the union of all top tanked candidates:\n$Q_{ranked} = \\bigcup_{a \\in A} \\mathop{\\text{ARGMAX}}\\limits_{q \\in Q, PARENT(q)=a}^{(n)} SCORE(QT(q), nlq).$\nwhere Q denotes the set of queries generated during query construction, SCORE measures similarity and QT is Query Textification. PARENT(q) indicates the parent query of q, and A refers to the set of parent queries that have child queries. This approach ensures that the size of the candidate pool grows at a manageable rate, while preserving high-quality queries for downstream processing."}, {"title": "3.5 Question Answering", "content": "To help the LLM understand the semantics of the provided query, we equip each generated"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Setup", "content": "We experiment with four complex KBQA datasets, i.e., GrailQA (Gu et al., 2021), GraphQ (Su et al., 2016), KBQA-Agent (Gu et al., 2024), MetaQA (Zhang et al., 2018) and a Text2SQL dataset, i.e., WikiSQL (Zhong et al., 2017). We use the F1 scores as the evaluation metric for KBQA and denotation accuracy for Text2SQL. We compare TARGA with various paradigms of baselines, including fine-tuning, ICL, and Agent, where we report performance in the original paper. For experiments with other settings, we copy the re-implemented result from Gu et al. (2024). By default, we use Qwen-2.5-7B-Instruct as the base LLM in our experiments with 10 demonstrations for all datasets."}, {"title": "4.2 Main Result", "content": "Table 1 illustrates the main result for KBQA. we compare TARGA with methods that require different amounts of annotation. For the relatively challenging datasets, i.e., GrailQA, GraphQ, and KBQA-Agent, based on a 7B model, TARGA achieves the best performance among all non-fine-tuned methods which are based on advanced close-sourced LLMs. On GrailQA and KBQA-Agent, TARGA surpasses previous SOTA non-fine-tuned methods by 8.7 and 13.0 F1. On GraphQ, TARGA even beats some fine-tuned methods and achieves similar performance with the best non-fine-tuned method.\nWhen compared to methods with a similar paradigm (ICL-based), TARGA outperform previous methods by 14.0 and 28.4 in F1 on GraphQ and KBQA-Agent, respectively. It is worth noting that our method requires neither any manually annotated corpus nor the expensive close-sourced model. Besides, we have not incorporated self-consistency to boost the performance. This can be attributed to the high quality of the synthetic data, which has led to a reduction in task difficulty and a decreased reliance on the capabilities of strong LLMs. Moreover, compared with other ICL-based methods which include 40-100 demonstrations, TARGA uses only 10 demonstrations but still achieves the best performance, demonstrating notable data efficiency.\nCompared to BYOKG which also works without annotated data, TARGA achieves approximately 1.5\u00d7 performance on GrailQA and MetaQA-3Hop. More importantly, TARGA dynamically synthesizes the most relevant data for different questions, enabling seamless adaptation to questions from any distribution. Besides, the synthetic data by TARGA is generated online, eliminating the need for a time-consuming offline data collection phase."}, {"title": "4.3 Detailed Analyses", "content": "To gain more insights, we conduct detailed experiments to illustrate some favorable practical characteristics of TARGA on: sample efficiency, robustness, generalization ability, efficiency, model size requirements, and transferability."}, {"title": "4.3.1 Sample Efficiency", "content": "In this section, we analyze how the number of demonstrations impacts the performance. We experiment on GrailQA, with the number of demonstrations ranging from 1 to 20. Based on our QA framework, we compare three distinct sampling settings: Random, Similarity-based, and Ours (synthetic), corresponding to examples randomly sampled from the training set, retrieved by similarity from the training set, and retrieved by similarity from the synthetic data by TARGA, respectively. The random and similarity settings can be viewed as reflections of the previous ICL-based and the retrieval-augmented ICL-based methods. Results are illustrated in Figure 3. With only one demonstration, our synthetic setting significantly outperforms"}, {"title": "4.3.2 Robustness Analyses", "content": "To further validate the robustness of our approach in real-world scenarios, we conduct an adversarial experiment designed to simulate conditions of poor synthetic data quality. Specifically, the attack involves randomly replacing one relation in a candidate query. We compare the same three settings as in Section 4.3.1. As in Figure 4, our method exhibits significantly stronger robustness under adversarial conditions. Even when all demonstrations were compromised, the performance degradation of TARGA was only around 25%. In contrast, the other setups experience a sharp decline: the F1 scores of similarity-based setup drop by about 40%, and the random setting even falls by approximately 75%. This further demonstrates the superior robustness of our method compared to other approaches. We also provide another analysis about when corrupt entities in demonstrations in Appendix F.2"}, {"title": "4.3.3 Performance on Different Generalization Levels", "content": "We experiment on GrailQA to compare the performance on different generalization levels (preliminary in Appendix D). To make a fair comparison with the \u201c-R\u201d setting of previous methods, we also implemented a \u201c-R\" version of TARGA where the entire training set was incorporated into our demonstration sampling pool. Specifically, we retain the top 5 most similar examples from the training set"}, {"title": "4.3.4 Efficiency analysis", "content": "For a practical QA system, high efficiency is also a key characteristic. Following Huang et al. (2024), we analyzed three efficiency metrics: TPQ, QPQ, and CPQ, as shown in Table 3. Regarding TPQ, our method significantly outperforms previous methods with only 4.5 seconds response time on GrailQA (detailed in Appendix F.4). Regarding QPQ, Agent-based methods have an inherent advantage. However, comparatively speaking, the overhead of QPQ remains relatively inexpensive than the other two metrics. Compared to KB-Binder, which also employs the ICL paradigm, our approach demonstrates a marked superiority on QPQ. This is primarily because our synthetic demonstrations are highly aligned with the target question, enabling the generated logic forms to be"}, {"title": "4.3.5 Performance on different sizes of LLM", "content": "In real-world applications, large and powerful LLMs are not always accessible or affordable. Therefore, we further analyze the performance of various methods across different model sizes. We compare the Agent method (QueryAgent), ICL method (KB-Binder), and the retrieval-augmented ICL method (KB-BINDER-R). As shown in Table 4, our approach demonstrates remarkable adaptability from qwen-1.5B-instruct to gpt-4o-mini. With just the 1.5B model, our method already surpasses the previous best-performing method, while at 7B, it only slightly lags behind the closed-source model. The Agent method has strong generalization capabilities but is heavily reliant on the planning and self-correction abilities of the most advanced LLM, which smaller models do not excel at. For models below 72B, the performance of the QueryAgent is essentially unusable. For the 72B model, the performance of QueryAgent is still inferior to that of"}, {"title": "4.3.6 Ablation Study", "content": "Table 5 presents the impact of distinct components on model performance across three datasets. Compared to the full model, removing the query textification component leads to a noticeable drop, particularly on KBQA-Agent (-6.9), highlighting the importance of bridging the gap between the text embedding model and logic form. The removal of the re-ranking component results in the largest performance decrease, with reductions of 9.1, 11.9, and 27.2 on GrailQA, GraphQ, and KBQA-Agent, respectively, underscoring the importance of the re-ranking step. In contrast, excluding synthetic question generation yields more modest declines, suggesting it is less critical than the other components but still beneficial for KBQA-Agent. It is unexpected, but from another aspect, it indicates that even only using the synthetic query as the demonstration the performance is also competitive."}, {"title": "4.3.7 Transferability to Text2SQL", "content": "We adapted our framework to the Text2SQL task to demonstrate the generality of our approach in other semantic parsing tasks. Employing the WikiSQL dataset, we compare TARGA with both the fine-tuned and non-fine-tuned methods. Among them, StructGPT and Readi are 32-shot and 7-shot methods, respectively. AgentBench and QueryAgent both use 1 shot. As shown in Table 6, with merely 10 synthetic examples as demonstrations, our method surpasses prior methods with 32 manually annotated examples and also outperforms the best 1-shot method, all while incurring a lower cost and smaller model. Besides, TARGA can even surpass a fine-tuned method with 3B model."}, {"title": "5 Conclusion", "content": "In this paper, we explore two critical challenges in the semantic parsing task: reliance on annotated data and poor generalization on non-I.I.D. cases. We proposed a novel method called TARGA, which automatically synthesizes examples that are most relevant to the test data and utilizes them as demonstrations for in-context learning. Remarkably, TARGA achieves the best performance among all non-fine-tuned methods across three complex KBQA datasets and one Text2SQL dataset, especially on GrailQA and KBQA-Agent (7.7 and 12.2 F1 points, respectively). While achieving impressive performance, TARGA also exhibits the following practical properties: 1) It does not require any annotated data. 2) It is effective even with a model size of just 7B parameters. 3) The synthetic data is generated online. 4) It exhibits superior generalization, robustness, and speed. This work highlights the potential of leveraging synthetic data in semantic parsing, and we hope that TARGA can serve as a valuable foundation for developing more practical systems in this field."}, {"title": "Limitations", "content": "We would like to discuss some limitations of our work. First, in this paper, we validate TARGA on two specific semantic parsing tasks: KBQA and Text2SQL. While these tasks demonstrate the potential of our approach, further exploration across a broader range of tasks that involve transforming natural language into logical forms could strengthen the generalizability of TARGA. Additionally, we have not yet investigated the feasibility of our synthetic data generation method in other paradigms, such as agent-based or fine-tuned models. We would like to adapt TARGA to these paradigms in future work."}, {"title": "A Preliminary", "content": ""}, {"title": "A.1 Knowledge Base Question Answering", "content": "We introduce related concepts and the task of knowledge base question answering as follows.\nLet E be a set of entities, P be a set of relations, C be a set of classes, and I be a set of literals. A knowledge base K \u2286 E \u00d7 P \u00d7 (EUCUI) is a set of triples (s, p, o), where s \u2208 E is a subject entity, p\u2208 P is a predicate, and o \u2208 (EU CUI) is an object entity, class or literal value.\nThe task of knowledge base question answering can be formalized as learning a function f that takes a natural language question q as input, and outputs a structured query q' = f(q) in a formal language such as SPARQL (Gregory, 2013). The structured query q' should ideally encode the entities, relations, and constraints specified by the input question q, such that executing q' over the knowledge base K yields the correct answer of q."}, {"title": "A.2 In-Context Learning", "content": "In-context learning (Brown et al., 2020) allows LLMs to perform new tasks by simply observing examples provided within the input, without updating their internal parameters. Intuitively, the model \"learns\" from the context and uses it to generate appropriate responses for similar tasks, relying on patterns it recognizes from the given examples.\nMore precisely, let [X1,X2,\u2026\u2026, In] be a sequence of input tokens representing natural language texts, and let [Y1, Y2, , Yn] be a corresponding sequence of output tokens representing the desired task, which in our context are structured queries. A LLM, denoted as fe, is a function parameterized by 0, which takes an input sequence and predicts the next token or sequence of tokens. In-context learning refers to the ability of a pretrained LLM to learn and adapt to a specific task purely by conditioning on a sequence of examples S = [(x1, y1), (X2,Y2), \u2026\u2026\u2026, (Xn, Yn)], provided as part of the input context, without updating the model parameter 0. In contrast to traditional learning paradigms that require parameter updates via gradient descent, the LLM uses the provided examples to infer the underlying task and generate predictions for a new input Xn+1."}, {"title": "B Datasets", "content": "We conduct experiments on four KBQA datasets and one Text2SQL dataset, their statistics are shown in Table 7."}, {"title": "C Baselines", "content": ""}, {"title": "C.1 Seq2Seq Fine-tuning Methods", "content": "\u2022 ArcaneQA (Gu and Su, 2022) is a generation-based method that incrementally synthesizes a program by dynamically predicting a sequence of subprograms. It prunes the search space by constrained decoding.\n\u2022 Pangu (Gu et al., 2023) leverages the discrimination ability of language models to build queries in an incremental manner. It consists of a symbolic agent to collect valid candidate plans and an LM to select the most likely one."}, {"title": "C.2 ICL-based Method", "content": "\u2022 Pangu-ICL (Gu et al., 2023) is a ICL version of Pangu. The result is based on output distribution of the LLMs, which require access to model parameters."}, {"title": "D Levels of Generalization", "content": "In the context of KBQA, the three levels of generalization\u2014I.I.D. generalization, compositional generalization, and zero-shot generalization\u2014refer to the capability of models to handle increasingly challenging and diverse types of questions (Gu et al., 2021). In particular, I.I.D. generalization refers to the model's ability to correctly answer questions that are sampled from the same distribution as the training set, which assumes that the test data follow similar patterns and schema. On top of that, compositional generalization refers to the model's ability to handle novel combinations of KB items (e.g., entities and relations) that were seen during training, but in configurations that the model has not encountered before. Finally, zero-shot generalization refers to the model's ability to answer questions involving entirely new KB items, such as unseen entities and relations that were never presented in the training set.\nGu et al. (2021) argue that relying solely on datasets under I.I.D. settings limits the practical use of KBQA models, as real-world questions often involve unfamiliar entities or require novel reasoning. This limitation is evident in the degraded performance of existing KBQA methods under compositional and zero-shot settings compared to I.I.D. settings. Therefore, practical KBQA models should be equipped with built-in generalization capabilities across all three levels to better handle diverse, real-world questions."}, {"title": "E More Details", "content": ""}, {"title": "E.1 Other Experiment Settings", "content": "TARGA and Pangu used 10 demonstrations across all datasets. KB-Binder and KB-Coder used 40 shots for GrailQA, 100 shots for GraphQ, and 5 shots for MetaQA, with KB-Binder employing 20 shots for KBQA-Agent. AgentBench, FUXI, and QueryAgent are all 1-shot methods. The experiments using models with 1.5B, 7B, 32B, and 72B parameters were conducted on 1, 1, 4, and 8 A100 GPUs, respectively."}, {"title": "E.2 Entity Linking Detail", "content": "The detailed entity linking results of all compared methods are listed in Table 8. For KBQA-Agent and MetaQA-3Hop, all compared methods use golden linking result. For GrailQA and GraphQ, most compared methods use the entity linking re-"}, {"title": "E.3 Logic Form Design", "content": "Inspired by PyQL (Huang et al., 2023), we designed a simplified logical form for constructing queries, which uses a series of functions for semantic expression, making it easier for LLMs to learn and enabling seamless translation into SPARQL. Table 10 shows all functions that we used."}, {"title": "E.4 Query Textification Detail", "content": "To improve the performance of the text embedding model, we transform the synthesized query into a format closer to natural language by implementing a simple yet effective rule-based parsing program. An example of query textification is provided in Table 12.\nSpecifically, we first replace the entities and relations in the triples with their labels, concatenating the subject, predicate, and object to create a description. For two nested descriptions (multi-hop), we represent them in a hierarchical form using \"a have b.\". For two conjunction descriptions (multi-constraint), we connect the two parts with \u201cand.\" We also applied some simple processing to the filters, such as using \"more than\" and \"less than\" to connect comparison objects."}, {"title": "E.5 Statistics of Query Construction", "content": "As shown in Table 9, we present the statistics of the synthetic query in Section 3.3."}, {"title": "E.6 Natural Language Question Generation Detail", "content": "We directly use the pseudo-questions generated by query textification for building (query, question) pairs as demonstrations in QA phase, and the performance is good enough. We also experimented with using an LLM (GPT or Qwen) to generate questions during this phase, but the downstream QA performance remained nearly unchanged, with a difference of less than 0.5 F1. Moreover, leveraging an LLM to generate corresponding questions requires labeling a few examples as demonstrations and incurs an extra time cost (less than 1 second). In fact, even without generating questions and using only the queries as demonstrations, the performance showed only a slight decline (see the ablation study in Table 6). We recommend choosing between using rules or an LLM based on specific scenarios:\n\u2022 If you want to use TARGA for ICL-style QA, the quality of question generation at this stage does not need to be particularly high. Heuristic rules are sufficient enough.\n\u2022 If you want to construct standard parallel corpora for fine-tuning or other purposes, we recommend you use an LLM along with some demonstrations to generate higher-quality natural language questions."}, {"title": "E.7 Prompt for Question Answering", "content": "We provide example prompts for TARGA on the KBQA and Text2SQL tasks In Tables 16 and 17. It consists of some synthetic demonstrations (NLQ-Query pairs) and the test question. For the KBQA task, we provide the entity linking results. For the Text2SQL task, we provide the headers of the table."}, {"title": "F Further Analysis", "content": ""}, {"title": "F.1 The performance difference between Compositional and zero-shot setting", "content": "In Table 2, we notice that the performance of the compositional setting is lower than the zero-shot setting for TARGA, but this trend is reversed for other compared methods. Our perspective is outlined as follows:\n\u2022 This is a matter of relative performance: If the performance of TARGA on zero-shot performance is above the normal level, then compositional will appear lower than zero-shot, and vice versa for comparative methods. If the performance of other methods on zero-shot setting is very poor, the performance of compositional setting will appear relatively high.\n\u2022 Setting aside whether the test questions are seen in the training set or not, there are significant inherent difficulty differences among the"}, {"title": "F.2 Further Analysis on Robustness", "content": "In Section 4.3.2, we have analyzed the effect of randomly replacing relations in demonstrations. Here, we present the performance of randomly replacing entities in the demonstrations. Similarly to 4.3.2, we corrupt N demonstrations by replacing one en-"}, {"title": "E.3 Further analysis on ranking", "content": "Considering that a poor ranking might incorrectly identify the top 1 or top 2 results, while the top 10 overall might still be generally correct. Therefore, we conducted a more convincing experiment using only the top 1 data as the demonstraion to better illustrate the impact of query textification on ranking quality.\nThe results are shown in Table 13: we found that in this setting, the decline without query textifica-"}, {"title": "F.4 Runtime Breakdown", "content": "We present a detailed runtime breakdown to provide a clearer understanding of the time costs associated with each stage, using GrailQA as an example.\n(1). Entity linking: nearly negligible (0 s): We use the cache provided by previous work for fair comparison. Most previous non-finetuning methods either rely on cached results or the golden linking results. Details are provided in Section E.2.\n(2). Relation linking: very fast (0.60 s): Relation linking consists of three steps:\na). Obtain the relation embedding (0 s, cached in advance). Note that the relations in Freebase are fixed and limited, so they only need to be cached once in advance.\nb). Obtain the question embedding (0.52 s).\nc). Calculate the embedding similarity between question embedding and all relation embeddings (0.08s). We utilized FAISS (Johnson et al., 2019) to accelerate similarity computations.\n(3). Query construction: the slowest part (2.99 s for GrailQA):\nThis is the most time-consuming stage, which is described in detail in Section 3.3.\n(4). Reranking: very fast (0.11 s): This is implemented by bge-reranker. Since we carefully controlled the number of candidate queries during the construction process, the final set for ranking is relatively small. Details about the size of the candidate queries are provided in Table 9. For instance, only an average of 17.4 candidate queries were involved in the reranking stage, which contributed to"}, {"title": "(5). Question generation", "content": "nearly negligible (0 s): Since we use the pseudo-questions generated by query textification (implemented by simple rules) as the corresponding question of a candidate query, this time cost is negligible."}, {"title": "(6). In-Context Learning QA", "content": "(0.78 s): This is a simple ICL request with about 700 input tokens; a normal inference speed is generally less than 1 second. The time difference between using locally deployed open-source models (Qwen) and online closed-source models (GPT) is not significant. For closed-source models (GPT), a normal request speed is generally on the order of seconds. For open-source models, the specific speed depends on your hardware configuration. We deployed a"}]}