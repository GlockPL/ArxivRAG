{"title": "Contrastive Visual Data Augmentation", "authors": ["Yu Zhou", "Bingxuan Li", "Mohan Tang", "Xiaomeng Jin", "Te-Lin Wu", "Kuan-Hao Huang", "Heng Ji", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in multimodal pre-training (OpenAI, 2023; Google, 2023; Hurst et al., 2024) and visual instruction tuning (Liu et al., 2023b;a; 2024c) have enabled impressive LMM abilities. However, as shown in Fig.1, it still remains a challenge for current state-of-the-art proprietary and open-source models to robustly recognize novel visual concepts (e.g. \u201cClouded Tiger Cat\" Fig.1a) and confusing / low-resource / commonly misrepresented visual concepts (e.g. \"Resupply Base\" Fig.1b).\nIn order to help models better acquire new visual concepts and distinguish confusable concepts, existing approaches straightforwardly 1). Fine-tune text decoder on new textual corpora to expand the concept base; and 2). Fine-tune both vision and text components on new web image-text pairs for visual concept acquisition. These approaches are ineffective due to scarcity of data for certain concepts and inefficiency caused by not knowing what precisely confused the models (3.1). As depicted in Fig.1, it can be difficult to obtain ample high-quality real images for novel concepts such as new animal species. While for confusing concepts, the problem usually lies with biased concept representation in web image-text data. For example: online images of \"Resupply Base\" mostly only consist of exterior views of the architecture without the interior details, which may cause the models to confuse it with a \"Wholesale Store\" that shares some interior features.\nTo help LMMs recognize and reason about novel and confusing concepts more robustly and efficiently, we propose CODA, a Contrastive Visual Data Augmentation technique. For each target concept, CoDA first identifies a \"confusable concept\" that the LMM finds most similar to the target. Then, it extracts contrastive textual and visual features of the target concept with respect to the confusable concept. The extracted features go through a filtering process based on discriminability and generability to make sure that: 1). The features are possessed by the target concept but not the confusable concept; and 2). The feature can be reliably generated by the text-to-image generative model and recognized by the LMM. Afterwards, the features are passed to the text-to-image generative model to produce augmented visual instances of the target concept. To make sure that the features are indeed generated and recognizable by the LMM, CoDA again uses the LMM's zero-shot inference to rank and filter the augmented images. Finally, the resulting augmented images can be used to update the LMM via low-rank adaptation, basic fine-tuning, in-context learning, or any other method of choice.\nIn addition to evaluating on existing datasets INaturalist and SUN, we create NovelSpecies, an annotated image dataset of newly discovered animal species in recent years. NovelSpecies allows the simple selection of species discovered after any model's latest knowledge cutoff date, ensuring the selected species were never seen by the model. Therefore, NovelSpecies is the perfect testbed for methods aimed at improving LMMs' novel concept recognition ability.\nComprehensive experiments with LLaVA-NeXT on the 3 datasets show CoDA performs surprisingly well in teaching LMMs novel and confusing concepts, significantly improving data and compute efficiency compared to existing methods. In additional experiments, we show that CoDA is also able to improve novel concept recognition for traditional classifiers like ViT and proprietary LMMs such as GPT40-mini. Finally, ablation experiments show that CoDA can be significantly improved by simply replacing its off-the-shelf components such as the text-to-image generation model with superior versions of similar models."}, {"title": "2. Related Works", "content": "Few-shot image recognition is a long-standing problem in the vision community. Early works in this area focused on improving traditional image classifiers on classifying existing concepts (Vinyals et al., 2016; Finn et al., 2017; Nichol, 2018; Dhillon et al., 2019; Tian et al., 2020; Bhagat et al., 2023; Afrasiyabi et al., 2022). On the other hand, while recent advancements in the training of vision language models (VLMs) and large multimodal models (LMMs) (OpenAI, 2023; Google, 2023; Hurst et al., 2024; Liu et al., 2023b;a; 2024c) have shown great promise and extensibility, they still severely lag behind traditional models in image classification, especially for low-resource, novel, and confusing concepts (Zhang et al., 2024; Cooper et al., 2024).\nWhile commonly used text-side VLM data augmentation strategies (Yuksekgonul et al., 2022; Yang et al., 2023; Liu et al., 2024e; Sharifzadeh et al., 2024) have little effect on this issue, a more promising technique to solve this is through visual data augmentation. This includes basic visual manipulations such as cropping, flipping, and rotation (Yang et al., 2022; Kumar et al., 2024); and more advanced model-based augmentation such as style transfer (Zheng et al., 2019; Chun & Park, 2021) and image mixing (Uddin et al., 2020; Xie et al., 2021; Hao et al., 2023). More recently, with the rise of controllable and promptable visual generative models, knowledge and feature editing-based augmentation methods (Liu et al., 2022; Wu et al., 2023; Jin et al., 2024) have gained in popularity. Such methods generally focus on using multimodal data and general knowledge bases to guide image-editing models in creating augmented visual data based on existing images.\nOne main issue with current methods is that the augmented images they produce must be closely based on existing real images, which makes them unhelpful for novel concepts where real images are extremely rare, and mis-represented concepts where existing real images do not accurately depict the concept. Additionally, due to their close connection to existing images, such augmented images usually lack visual frame structure and view variation. In contrast, our method CoDA can extract accurate and meaningful features from extremely limited multimodal data, and use text-to-image generative models to produce diverse high-quality augmented data for LMM updating."}, {"title": "3. Methods", "content": "As shown in Fig.2, CoDA consists of 4 major steps including contrastive textual and visual feature extraction, feature filtering, feature-controlled image generation, and augmented image filtering. Together these steps ensure CoDA reliably generates informative and high-quality augmented images that help LMMs recognize novel and confusing concepts."}, {"title": "3.1. Feature Extraction", "content": "Textual Feature Extraction In our exploratory experiments, we find that significant mis-recognition errors occur on low-resource or commonly mis-represented concepts in vision-language instruction fine-tuning and multimodal pre-training datasets, which the LMMs are trained on. For example, the LLaVA 1.6 (34B) model (Liu et al., 2024c), mainly tuned on LAION-GPT-4V(LAION, 2024) and ShareGPT-4V (Chen et al., 2023) datasets, has a strong tendency to mis-recognize interior images of \u201cResupply Base\u201d as \"Wholesale Store\" (Fig.1). Unsurprisingly, we find that all related references of \"Resupply Base\" across the 3 instruction-tuning datasets only depict exterior views of the concept rather than interior views. While the concept itself is not a low-resource concept in existing text corpora, it is severely low-resource and also commonly mis-represented in vision-language instruction fine-tuning datasets.\nTo address this issue, we prompt LLMs to directly generate feature attributes of the target concept based on their existing knowledge, focusing on visual appearance, and avoiding hallucination for unfamiliar concepts. For this task, we use the cost-efficient GPT40-mini model with chain of thought reasoning. Generally, textual feature extraction is most applicable for concepts that are high-resource in existing textual corpora, yet low-resource and/or commonly mis-represented in vision-language instruction-tuning and pre-training datasets. Here we do not try to classify which concepts fall under this criteria, but rather apply this step for all concepts. To ensure extracted feature quality and filter out hallucinated and/or non-visually-recognizable features, we pass all extracted features through an automatic filtering step, as described in 3.2.\nWe also considered other methods for textual feature extraction, including using knowledge bases (Jin et al., 2024), retrieval augmented generation, and LLMs with internet search. However, we believe currently the advantages brought by these methods do not out-weigh their complexity overhead, thus we opt for simplicity.\nVisual Feature Extraction While textual feature extraction generally works well for pre-existing and non-hyper-domain-specific concepts that are prevalent in textual data sources, it tends to fail when either of the conditions are not met. For example, a large language model with a knowledge cutoff prior to June 2023 would not be able to provide meaningful features regarding the Apple Vision Pro device announced in July, or the new animal species \"Clouded Tiger Cat (L. pardinoides)\" first described by scientists in April 2024 (1). In addition to this weakness, LLM-based textual feature extraction is also unreliable when asked to provide detailed information regarding hyper-domain-specific concepts like the \"Mazda MX-5 Miata RF\" or the \"Lear's Macaw (Anodorhynchus Leari)\". In practice, we observe that for novel and hyper-domain-specific concepts, most of the LLM extracted textual features end up being filtered out by our automatic feature filtering module.\nTo address this weakness, we implement an additional visual feature extraction module based on VLMs. Given a single image of the target concept, the VLM is asked to extract its key visual features. When there is more than one image containing the target concept available, we use a LM to de-duplicate and summarize the combined extracted visual features from all images. For simplicity and cost-efficiency, we use the GPT4o-mini model for both visual feature extraction and feature de-duplication.\nIn contrast to textual feature extraction, visual feature extraction is most effective for hyper-domain-specific and novel concepts that are very rare or non-existent in textual corpora but have a limited number of visual examples. Thus, it well-complements textual feature extraction. Similarly, we do not attempt to classify which concepts fall under this criterion; instead, we apply this step to all concepts and rely on automatic filtering (3.2) to remove low-quality features.\nContrastive Feature Extraction While basic textual and visual feature extraction both aim to exhaustively list identifying features of the target concept, this is essentially an intractable task for complex concepts as it usually requires a huge number of features to fully describe them. For novel or low-resource concepts the LMM has likely never seen before, it is extremely difficult to teach the LLM the new concept using an incomplete description.\nThere are two potential solutions to this problem: (1). Leveraging hierarchical information to narrow down concept category and reduce descriptional features. (2). Illustrating the new concept based on contrastive differences from a similar existing concept the LMM already understands. Previous works in language and visual data augmentation (Jin et al., 2024) tend to use solution (1). However, its feasibility is contingent on the existence of a comprehensive textual knowledge base or tree-like structure that already includes the target concept. As discussed in Section 3.1, this is often not the case for novel concepts such as new electronic products (e.g. Apple Vision Pro) or new animal species (eg. Clouded Tiger Cat).\nTo enable the handling of novel concepts and remove the need for external databases, we adopt solution (2) and perform contrastive multimodal feature extraction for all target concepts. First, we use the LMM's zero-shot inference on the target concept $C_T$ to obtain the misidentified concept $C_M$. Then, we perform contrastive textual and visual feature extraction by querying LLMs and VLMs for visually identifying features that belong to $C_T$ but not $C_M$."}, {"title": "3.2. Feature Filtering", "content": "Automatic Feature Filtering After obtaining visually identifying features from contrastive textual and visual feature extraction, we filter them based on two key criteria:\n1.  Discriminability ($D(f, C_T, C_M)$): measures whether a feature $f$ can indeed be used to differentiate the target class $C_T$ from the misidentified concept $C_M$ ($f$ must first be a valid feature of $C_T$).\n2.  Generability ($G(f, C_T, C_M)$): measures whether a feature $f$ can be properly generated by the text-to-image generative model.\nTo calculate the Discriminability of a feature $f$ given the target concept $C_T$ and misidentified concept $C_M$, we compute the likelihood that CLIP (Radford et al., 2021) associates this feature with real images of the target concept compared to the likelihood that it is associated with real images of the misidentified class:\n$D(f, C_T, C_M) = \\sum_{i_{real} \\in I_{real}} \\frac{CLIP(f, i_{real})}{CLIP(f, i_{real}) + CLIP(f, i^{M}_{real})}$\nHere we use an equal number of images of the target and misidentified concepts. A score below 0.5 indicates that the feature is more likely to be associated with the misidentified class rather then the target class. To ensure that selected features are more strongly associated with the target class, we filter out all features with Discriminability below 0.6. This method avoids the CLIP score bias against smaller features by only comparing feature association with the two classes and not relying on the absolute CLIP score.\nGenerability is calculated in a similar manner, comparing the average CLIP similarity between $f$ and synthetic images of the target concept against the average CLIP similarity between $f$ and real images of the misidentified concept:\n$G(f, C_T, C_M) = \\sum_{i_{real} \\in I_{real}} \\frac{CLIP(f, i_{synthetic})}{CLIP(f, i_{synthetic}) + CLIP(f, i^{M}_{real})}$\nHere we rank all remaining features by their Generability score and select the top 5 features to be passed to the text-to-image generative model (as current diffusion models usually have limited text encoder attention span). This step identifies features that not only help distinguish the target concept, but also can be effectively rendered by the text-to-image generative model in synthetic images, which is critical to the success of synthetic data augmentation.\nOur automatic feature filtering module based on Discriminability and Generability ensures feature quality and limits the information loss between features and the generated augmented images. The remaining features are used for image generation and improving in-context recognition ability in inference prompts. We further verify the quality of remaining features with human evaluation in Sec.3.4."}, {"title": "3.3. Image Generation and Verification", "content": "Image Generation After feature extraction and filtering based on Discriminability and Generability, we pass the selected features to a text-to-image generative model to generate augmented visual data. We experiment with both SOTA open-weights (Esser et al., 2024; Stability AI, 2024) and proprietary (Recraft.AI, 2024) models.\nVerification To ensure final images for augmentation contain our extracted and filtered target concept features, we propose a simple automatic verification metric that checks whether desired features are recognized in the augmented images by the LMM we want to update: Given the vanilla LMM $M$, a set of features $F$, and an augmented images $i^{synthetic}$, the feature satisfaction rate $S(i^{synthetic}, F, M)$ for each augmented image:\n$S(i^{synthetic}, F, M) = \\frac{\\sum_{f \\in F} 1\\{M(f, i^{synthetic})\\}}{|F|}$\nHere $M(f, i^{synthetic})$} returns true if the feature f is recognized in the image $i^{synthetic}$. Afterwards, we filter out all images with $S(i^{synthetic}, F, M) <1.0$, keeping only augmented images that fully match all target concept features."}, {"title": "3.5. In-Context Inference for Enhanced Recognition", "content": "In addition to updating the LMM with augmented data, we can further boost performance by integrating the extracted features into the inference prompt. For each query, we can append a concise list of the most discriminative and generable features of the target and confusable classes. These features serve as an in-context guide, focusing the LMM's attention on critical distinguishing attributes. By explicitly highlighting what to look for (and what not to mistake it for), the model more reliably identifies the correct concept."}, {"title": "4. NovelSpecies Dataset", "content": "Proprietary LMMs like GPT40 (Hurst et al., 2024) and Gemini (Google, 2023) are trained on vast online text-image data and proprietary data, both non-public and impossible to inspect. Some open-source and open-data LMMs such as LLaVA (Liu et al., 2024b;d) are trained on publicly available image-text datasets. However, the text encoders used by such models are often not open-data, for example LLaVA-1.6 34B uses the closed-data Yi-34B model as its language backbone. Even in the rare cases where both image-text training data and text encoder training data are publicly available, it is still difficult to ascertain whether concepts in your benchmark were seen by your LMM through indirect data leakage (i.e. partial / paraphrased mentions). Due to the above issues, it is difficult to evaluate true novel concept recognition ability with existing datasets.\nOne way to bypass this problem with 100% guaranteed success is to use a dataset that only contains concepts created / discovered after the LMM's knowledge cutoff, i.e. the latest knowledge cutoff date among all of its textual / visual sub-components. Based on this idea, we curate NovelSpecies, a dataset of novel animal species discovered in each recent year, starting with 2023 and 2024. We provide detailed information for each species, including time of discovery, latin name, common name, family category, textual description, and more. Data will be released upon publication.\nTo create NovelSpecies, we start by collecting the list of species first described in each year by Wikidata (Wikidata, 2024). Then, to make sure we can curate a visual benchmark of novel species, we manually annotate and filter out extinct species and species with too few publicly available images. After filtering, we end up with a dataset of 64 new species, each consisting of 35 human-verified image instances, thus a total of 2240 images. The images are split into training, validation, and test sets. For each specie, there are 5 training images, 15 validation images, and 15 test images. This data split is consistent with our goal of creating a benchmark dataset for novel concept recognition, where the maximum number of training instances for a completely unseen concept can range from 1 to 5."}, {"title": "5. Experiments", "content": "To evaluate CoDA's ability to improve novel and confusing concept recognition in LMMs, we experimented with CoDA and other relevant baselines on three different datasets:\n1.  The iNaturalist Dataset (Van Horn et al., 2018) is a challenging natural world concept recognition benchmark for LMMs due to its extensive highly domain-specific and fine-grained species categories and inclusion of rare and low-resource species classes.\n2.  The SUN Dataset (Xiao et al., 2010) is a widely used large-scale scene recognition dataset that contains rich and confusing visual scenes. Correctly recognizing the scenes requires fine-grained visual reasoning and understanding of the scenes.\n3.  NovelSpecies Dataset (Sec.4) is our new dataset consisting only of novel animal species concepts that LMMs are guaranteed to have never encountered in their training or fine-tuning.\nFor each dataset, we use an automatic data selection strategy A.2 to find a subset of challenging concepts that the model fails to recognize. Then, we apply CoDA along with 3 other visual data augmentation baselines:\n1.  All Real uses an all real augmented image set. In the Fixed Real Data setting, this means using the 5 real images provided. In the Fixed Compute setting, this means using unlimited real images to match the total number of real + synthetic images in other settings.\n2.  Cropping and Flipping are widely used traditional visual data augmentation strategies. We include them here for direct comparison with CoDA and other existing feature-based augmentation methods.\n3.  ARMADA (Jin et al., 2024) is the current state-of-the-art feature-based visual data augmentation strategy for concept recognition and image classification.\nIn additional to these 3 baselines, we also include ablations of CoDA with non-contrastive textual and visual features, i.e. w/o contrastive guidance from confusable concepts (3.1) nor discriminability-based feature filtering (3.2)."}, {"title": "5.2. Main Experiment", "content": "For our main experiment, we consider two different resource settings that correspond to common real-world scenarios:\nFixed Real Data Under the fixed real data setting, we only have access to 5 real images for each concept. Each data augmentation strategy may generate 1-5 synthetic images. Then, the model is LoRA-adapted on the combined real and synthetic images. This setting simulates real-world scenarios, where there isn't sufficient real training data for certain concepts. This is common for novel concepts, hyper-domain-specific concepts, and long-tail distributed datasets. In these scenarios, the quality and effectiveness of synthetic augmented data is especially instrumental to the updated model's performance.\nExperiment results across the 3 datasets show that CoDA consistently outperforms existing traditional and feature-based data augmentation methods in the Fixed Real Data setting. When augmenting the training set with just a single synthetic image, CoDA is able to achieve 11.8% (NovelSpecies), 10.0% (SUN), and 17.8% (iNat) absolute gains in accuracy compared using all real images. It further outperforms the best existing baseline augmentation methods by 5-12% absolute gains. We also observe that ablated performance of CoDA (w/o contrastive) is still significantly above traditional and image-editing-based augmentation baselines while being almost consistently below CoDA's performance. This shows the benefits of text-to-image generative augmentation methods compared to existing methods, as well as the benefits of fine-grained textual features during inference. This also highlights the need for contrastive feature selection and discriminability-based feature filtering. We find that increasing the number of augmented synthetic images does not necessarily improve updated model performance, this may be attributed to the fact that all generated images are ranked and selected from the same pool, with the first image being of the highest quality. Finally, the largest improvement over existing baselines can be seen in NovelSpecies, where CoDA methods involving visual features achieve the highest performance. This makes sense as the visual feature extraction method is designed to be robust to novel concepts with little textual documentation."}, {"title": "5.3. Additional Experiments", "content": "For additional experiments, we focus on NovelSpecies as it most closely resembles real-world scenarios, where over time, models are required to learn novel concepts without access to sufficient real training data.\nAdvanced T2I Model As explained in Sec.3, off-the-shelf model components used in CoDA can be easily swapped for superior versions of similar models to improve performance. To demonstrate this, we replace the open-weight Stable Diffusion 3.5 Large Turbo model (Stability AI, 2024) with the SOTA proprietary Recraft V3 Model (Recraft.AI, 2024) and run the same LLaVA-updating experiments as in Tab.2. Here we note that Recraft V3 has better instruction-following ability as well as better image generation quality compared to Stable Diffusion 3.5 Large Turbo. More details on these differences can be found in Sec.A.1. Our experiment results in Tab.3 show a significant performance boost when LoRA fine-tuning LLaVA with Recraft V3 produced synthetic images compared to fine-tuning on all-real data (28.7%) and also compared to fine-tuning on Stable Diffusion 3.5 Large Turbo produced synthetic data (7.9%). This demonstrates the potential increase of CoDA's effectiveness along with improvements in Text-to-Image generative models. We believe it is also possible to achieve similar improvements by replacing the LLM/VLM components of CODA with superior models in the future.\nProprietary LMM While proprietary LMMs like GPT40-mini (Hurst et al., 2024) tend to have relatively strong 0-shot performance on existing datasets such as SUN and iNaturalist, their performance significantly degrades on NovelSpecies due to having never encountered the novel concepts. To test whether CoDA can effectively improve novel concept recognition performance for such Proprietary LMMs, we fine-tune the gpt-4o-mini-2024-07-18 model using CoDA and relevant augmentation baselines. Results in Tab.3 demonstrate a significant performance gain (9.5%) for GPT40-mini after being fine-tuned on CoDA augmented synthetic images. While this improvement is not as significant compared to the LLaVA-1.6 model (20.3%), it is due to GPT40-mini's better base performance.\nTraditional Classifier In addition to evaluating CoDA on LMMs which take image-text input and produces text output, we also test whether it can help traditional image classifiers recognize novel concepts. We run the widely-used ViT classifier (Alexey, 2020) on NovelSpecies with CoDA and other augmentation baselines. Results in Tab.3 show that CoDA is able to achieve a consistent performance gain over existing baselines for ViT-base (9.1% for single-shot augmentation). The ViT classifier provides stronger base performance compared to general VLMs, thus offering less room for improvement. However, we note here that our main focusing on improving LMMs instead of such traditional classifiers stems from LMMs' superior extensibility and generalizability to other related tasks such as recognition-based reasoning and explanation."}, {"title": "6. Conclusion", "content": "In this work, we propose CoDA, a contrastive visual data augmentation approach that helps LMMs recognize novel, confusing, and low-resource concepts through efficient and effective model updating. CoDA is a plug-and-play method which utilizes off-the-shelf models for contrastive feature extraction, feature filtering, text-to-image generation, and image filtering. We evaluate CoDA against four existing baselines and self-ablations on three datasets: INaturalist, SUN, and NovelSpecies, which we created in this work. Consisting only of animal species discovered in recent years, NovelSpecies offers an ideal testbed for LMMs' novel concept recognition. We provide comprehensive additional experiments demonstrating CoDA's effectiveness for traditional classifiers and proprietary LMMs. Finally, we show that CoDA can be easily improved by replacing off-the-shelf components, such as text-to-image generation model with superior versions of similar models in the future."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Qualitative Comparison", "content": "Figure 3 illustrates the qualitative comparison of the quality of generated images using CoDA and other approaches.\nThe Crop method, while useful for localized feature emphasis, often results in the loss of crucial visual details necessary for species identification. For instance, in the Phyllobates Samperi images, cropping removes the black spots on the frog's skin, which are an essential distinguishing feature. Without these patterns, the cropped images lack key identity cues, potentially leading to misclassification. Similarly, in the Tail-Spot Wrasse images, cropping reduces visibility of the distinct horizontal striping pattern along the fish's body, making it difficult to recognize key species attributes.\nARMADA is capable of retaining some structural features, but it struggles with precise reproduction due to the limited image editing capabilities. This limitation is particularly evident in its generated images, where critical patterns such as the Phyllobates Samperi's orange stripes are missing. The generated frog appears to have a distorted pattern, failing to fully capture the contrast between black skin and bright orange lines, which are key species identifiers. Similarly, in the case of the Tail-Spot Wrasse, the image generated by ARMADA loses the feature of its vibrant horizontal stripes, leading to a visually inconsistent and less biologically accurate representation.\nIn contrast, CoDA successfully captures all species-specific features by leveraging contrastive textual and visual attributes through different image generation models. CoDA (SD-3.5) produces a high-fidelity image of Phyllobates Samperi, accurately preserving the orange stripes, dark skin, and black spots. However, slight variations in texture suggest that this model, while effective, may not fully match the real-world skin reflectivity of the species. Meanwhile, CoDA (Recraft V3) generates an even more realistic image, successfully capturing the frog's signature features with improved color richness and anatomical precision, making it nearly indistinguishable from real-world references.\nGenerally, CoDA (SD-3.5) and CoDA (Recraft V3) both perform significantly better than previous methods. Take Tail-Spot Wrasse as another example, the horizontal stripes, which were previously distorted or missing in previous methods, are now clearly visible. CoDA (Recraft V3), in particular, produces a more vivid and structurally accurate representation, ensuring the preservation of both color gradients and fin structure.\nNote that the quality of CoDA-generated images is inherently dependent on the capability of the underlying image generation model, meaning that limitations in the base model, such as resolution constraints or texture inaccuracies, may impact the fidelity of the final augmented data."}, {"title": "A.2. Data Selection Strategy", "content": "For each dataset, we focus on a randomly selected subset of concepts that the model is unable to recognize. The data selection strategy is as follows: In each iteration, we select a random subset of 15 species across different supercategories, including \"Birds,\" \"Mammals,\" and \"Reptiles.\u201d This strategy allows us to identify confusing pairs without overloading the system, progressively building a collection of challenging cases from each subset. For each species within a subset, we create prompts in a multiple-choice format, incorporating the image and a randomized list of options from all species in the subset. Based on the response from the LMM, we are able to highlight specific species that are commonly mistaken for each other, guiding us in selecting pairs for further analysis. In particular, misclassification happens when an image of one species is identified by the LLM to be an image of another species. A pair (A, B) is considered as a confusing pair if rate of misclassification on either direction is above the threshold 0.2. The process is repeated across new subsets, incrementally building an ample dataset of concepts the model has difficulty recognizing."}, {"title": "A.3. Experiment Details", "content": null}, {"title": "A.3.1. FEATURE EXTRACTION", "content": "For textual feature extraction, we use GPT-40-mini with chain-of-thought reasoning, running with OpenAI API calls. Each API call processes up to 2048 tokens, costing approximately 0.0025 per 1K input tokens and 0.005 per 1K output tokens. Given an average of 500 tokens per query and 10 queries per concept, the estimated cost per concept is around $0.0375.\nFor visual feature extraction, we utilize GPT-40-mini running with OpenAI API calls. Images are preprocessed to a resolution of 336x336 pixels and normalized before feature embedding extraction. Each image query incurs a cost similar to textual feature extraction. With an estimated 5 images processed per concept, the cost per concept amounts to approximately 0.1875.\nWith the rapid advancement of open-weights large language models and vision language models including DeepSeekV3 (Liu et al., 2024a), DeepSeekVL2 (Wu et al., 2024), Llama 3.2 (Dubey et al., 2024), and more; we expect that feature extraction LLMs and VLMs can be replaced with these models with none or minimal impact to performance. We plan to perform experiments on some of these models and provide comparison results in the next updated version of our work."}, {"title": "A.3.2. FEATURE FILTERING", "content": "We employ CLIP for automatic feature filtering, evaluating Discriminability and Generability scores. Discriminability is computed using cosine similarity between feature embeddings of target and misidentified concepts, with a threshold of 0.6. Generability is assessed by comparing feature presence in synthetic images using an ensemble of Stable Diffusion 3.5 Large and RecraftV3 models. The feature selection step is executed on an NVIDIA A100 GPU, processing features in approximately 2 hours. Top 5 ranked features are selected per concept."}, {"title": "A.3.3. IMAGE GENERATION AND VERIFICATION", "content": "For synthetic image generation, we employ Stable Diffusion 3.5 Large, running on a single A100 GPU. Additionally, we also integrate the RecraftV3 model through an API call. Image generation is performed at a resolution of 512x512 pixels with a guidance scale of 7.5. The pipeline generates 50 images per concept in approximately 1.2 seconds per image.\nPost-generation, we perform automated verification using LLaVA V1.6-34b, running on an A6000 GPU. Each image would takes approximately 1 minutes to run for feature presence using a feature-matching confidence threshold of 0.85. Images"}, {"title": "A.3.4. MODEL UPDATING", "content": "We train V1.6-34b with supervised fine-tuning (SFT) using LoRA with rank 128 and alpha 256, optimizing memory efficiency while maintaining model expressiveness. The training runs on two"}]}