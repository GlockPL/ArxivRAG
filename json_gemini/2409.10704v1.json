{"title": "SELF-SUPERVISED SPEECH MODELS\nFOR WORD-LEVEL STUTTERED SPEECH DETECTION", "authors": ["Yi-Jen Shih", "Zoi Gkalitsiou", "Alexandros G. Dimakis", "David Harwath"], "abstract": "Clinical diagnosis of stuttering requires an assessment by a\nlicensed speech-language pathologist. However, this process\nis time-consuming and requires clinicians with training and\nexperience in stuttering and fluency disorders. Unfortunately,\nonly a small percentage of speech-language pathologists re-\nport being comfortable working with individuals who stutter,\nwhich is inadequate to accommodate for the 80 million indi-\nviduals who stutter worldwide. Developing machine learning\nmodels for detecting stuttered speech would enable univer-\nsal and automated screening for stuttering, enabling speech\npathologists to identify and follow up with patients who are\nmost likely to be diagnosed with a stuttering speech disorder.\nPrevious research in this area has predominantly focused on\nutterance-level detection, which is not sufficient for clini-\ncal settings where word-level annotation of stuttering is the\nnorm. In this study, we curated a stuttered speech dataset with\nword-level annotations and introduced a word-level stuttering\nspeech detection model leveraging self-supervised speech\nmodels. Our evaluation demonstrates that our model sur-\npasses previous approaches in word-level stuttering speech\ndetection. Additionally, we conducted an extensive ablation\nanalysis of our method, providing insight into the most im-\nportant aspects of adapting self-supervised speech models for\nstuttered speech detection.", "sections": [{"title": "1. INTRODUCTION", "content": "Stuttering is a complex, multifactorial disorder characterized\nby atypical disruptions in the forward flow of speech [1]. It af-\nfects approximately 1 percent of the population and has a sig-\nnificant negative impact on all aspects of an individual's life\nincluding social, educational, emotional, and vocational [2,\n3]. A stuttering assessment is conducted by a licensed speech-\nlanguage pathologist. It includes measurement of the impact\nof stuttering on the person's quality of life as well as anal-\nysis of the individual's speech to determine stuttering sever-\nity. Unfortunately, over the past decades, speech-language\npathologists have consistently reported limited competence\nin working with individuals who stutter (e.g., [4, 5, 6, 7],\nwith less than 5 percent of licensed professionals in the United\nStates reporting expertise working with individuals who stut-\nter [8, 6]. Another challenge is that the process of speech\ntranscription and disfluency annotation is labor-intensive, as\nspeech-language pathologists primarily transcribe and anno-\ntate speech disfluencies manually.\nA potential approach to address the above challenges is\nto use deep learning models to serve as an initial screening\nfor stuttering, which could be deployed on edge devices such\nas smartphones or laptops. Early work [9, 10] has used sim-\nple deep learning models to detect stuttering behavior from\nspeech utterances. However, the main disadvantage is that\nthe system is highly dependent on the scale of training data.\nUnfortunately, the largest publicly available stuttering dataset\nis SEP-28K [11], which only contains 15.6 hours of utter-\nance level labeled data, which is relatively small compared to\nstandard datasets like LibriSpeech [12], which has a total 960\nhours for training.\nA promising recent approach in the speech community\nto deal with limited labeled data is Self-Supervised Learning\n(SSL). Speech SSL models are first pretrained on a large\namount of untranscribed speech and then finetuned on a\nsmaller amount of transcribed/annotated speech for specific\ndownstream tasks. Due to the task-agnostic nature of the\npretraining, speech SSL models demonstrate high generaliz-\nability across different speech processing tasks [13]. Hence,\nspeech SSL models are regarded as foundation models in\nmany applications nowadays, such as Automatic Speech\nRecognition [14, 15] and Speaker Verification [16]. Due to\nthe advantage of reducing the need for disfluency-labeled\ndata, speech SSL models are a promising approach to tack-\nling stuttered speech detection problems. Specifically, prior\nworks [17, 18, 19] utilize Wav2vec 2.0 [20] as their model\nbackbone for stutterred speech detection.\nDespite progress in this field, previous work has mainly\nfocused on utterance-level stuttering detection. However, this\nis too coarse for clinical application as stuttering-like disflu-\nencies are also present in typically fluent individuals who are\nnot diagnosed as a Person who Stutters (PWS). Clinically,\nstuttering is diagnosed based on meeting a certain frequency\nthreshold of stuttering-like disfluencies. Hence, we are inter-\nested in developing a more fine-grained stuttering detection\nmodel. To our best knowledge, [21] is the only previous"}, {"title": "2. RELATED WORK", "content": "In the field of stuttered speech detection, there are several\nproposed datasets, such as UClass [23], KSoF [24], and Sep-\n28K [11]. UClass is an unlabeled dataset while the others\nare labeled. KSoF and Sep-28K share the same format. Both\nof them consist of 3-second clips with utterance-level anno-\ntations. Specifically, Sep-28K also includes a subset of ut-\nterance level annotation from Fluency Bank. Not only the\nsize of the dataset are relatively small, but they only provide\nutterance-level labels, which we believe is one of the main\nobstacles in this field.\nSeveral recent works have used Speech SSL models in\nstuttering detection. In [18], they adopted Wav2vec 2.0 as\ntheir backbone and trained different models using different\nupstream layers of Wav2vec 2.0 for stuttering detection. They\nalso proposed to employ gender prediction as an auxiliary\ntask for enhancing performance. In [19], they pointed out that\nstuttering types do not happen independently, so they framed\nthe detection problem as a multi-label classification problem.\nIn [17], they improved the quality of Wav2vec 2.0 embed-\ndings for stuttered speech detection using Siamese network\nand contrastive training. Despite these impressive results,\ntheir models predict at the utterance level, which is insuffi-\ncient for clinical usage. Furthermore, the optimal strategies\nfor utilizing Speech SSL models for stuttering detection are\nstill not fully understood. Several prior works also incorpo-\nrate Speech SSL models into their model design, such as for\ndetecting dysarthria [25] and aphasia [26, 27], but they focus\non other types of speech disorders and are not directly appli-\ncable to our task.\nTo the best of our knowledge, [21] is the only study that\nfocused on fine-grained stuttering detection, more specifically\nframe-level stuttering detection. Hence, we chose it as our"}, {"title": "3. METHOD", "content": "Following prior work using Wav2vec 2.0 on stuttered speech\ndetection, we chose another Speech SSL model, WavLM\nLarge [22] as our backbone since it significantly outperforms\nWav2vec 2.0 on most SUPERB [13] benchmark tasks. Our\nmodel structure is shown in Figure 1. Furthermore, we adopt\nthe recent Hierarchical Convolution Interface (HConv.) as\nour method for utilizing the WavLM backbone according to\nrecent work [28], which was shown to improve overtaking a\nlearnable weighted sum of layer activations. As pointed out\nin [18], the information for stuttering detection exists in mul-\ntiple layers, so we decided to use HConv. due to its ability to\naggregate information across multiple layers. After that, we\nadd a few non-linear layers as the prediction head to classify\nindividual frames as stuttered or non-stuttered.\nIn [18], it was shown that phoneme recognition and stut-\ntering detection tasks utilize the same set of layers in the\nupstream Speech SSL model. Motivated by this, we de-\ncided to add an auxiliary Connectionist Temporal Classifica-\ntion (CTC) Loss to predict the phoneme sequences. The CTC\nloss is only applied in the pretraining stage where we have\nthe ground truth phoneme sequences.\nWe roughly follow the training pipeline in [21], our\nframework consists of two stages: Pretrain on synthetic aug-\nmentation of LibriSpeech 360 and Finetune on SEP-28K.\nPretrain on LibriSpeech: Different from previous works,\nwe add our synthetic augmentations between the convolu-\ntion layers and the transformer layers of WavLM rather than\nadding augmentation directly on the input waveform. We\nkeep the augmentation types the same as [21], which includes\nartificial prolongation, and word/sound repetition. By do-\ning so, we significantly save on computation and speed up\nthe online augmentation. Suppose the outputs of the predic-\ntion head for each input utterance are o = [o_1, o_2, . . . o_T],\nwhere $o_i$ = [o_i^0, o_i^1], corresponding to the probability of pos-\nitive and negative for frame i respectively. The labels are\nl = [l_1,l_2,...l_T]. The loss is calculated as a cross entropy\nbetween the prediction head outputs and the label sequence."}, {"title": "3.1. Model Architecture", "content": "Following prior work using Wav2vec 2.0 on stuttered speech\ndetection, we chose another Speech SSL model, WavLM\nLarge [22] as our backbone since it significantly outperforms\nWav2vec 2.0 on most SUPERB [13] benchmark tasks. Our\nmodel structure is shown in Figure 1. Furthermore, we adopt\nthe recent Hierarchical Convolution Interface (HConv.) as\nour method for utilizing the WavLM backbone according to\nrecent work [28], which was shown to improve overtaking a\nlearnable weighted sum of layer activations. As pointed out\nin [18], the information for stuttering detection exists in mul-\ntiple layers, so we decided to use HConv. due to its ability to\naggregate information across multiple layers. After that, we\nadd a few non-linear layers as the prediction head to classify\nindividual frames as stuttered or non-stuttered.\nIn [18], it was shown that phoneme recognition and stut-\ntering detection tasks utilize the same set of layers in the\nupstream Speech SSL model. Motivated by this, we de-\ncided to add an auxiliary Connectionist Temporal Classifica-\ntion (CTC) Loss to predict the phoneme sequences. The CTC\nloss is only applied in the pretraining stage where we have\nthe ground truth phoneme sequences.\nWe roughly follow the training pipeline in [21], our\nframework consists of two stages: Pretrain on synthetic aug-\nmentation of LibriSpeech 360 and Finetune on SEP-28K.\nPretrain on LibriSpeech: Different from previous works,\nwe add our synthetic augmentations between the convolu-\ntion layers and the transformer layers of WavLM rather than\nadding augmentation directly on the input waveform. We\nkeep the augmentation types the same as [21], which includes\nartificial prolongation, and word/sound repetition. By do-\ning so, we significantly save on computation and speed up\nthe online augmentation. Suppose the outputs of the predic-\ntion head for each input utterance are o = [o_1, o_2, . . . o_T],\nwhere $o_i$ = [$o_i^0$, $o_i^1$], corresponding to the probability of pos-\nitive and negative for frame i respectively. The labels are\nl = [$l_1$,$l_2$,...$l_T$]. The loss is calculated as a cross entropy\nbetween the prediction head outputs and the label sequence.\n$$L_{dis} = \\frac{1}{T} \\sum_{t=1}^{T} - l_t^0 \\cdot log(o_t^0) - l_t^1 \\cdot log(o_t^1),$$"}, {"title": "3.2. Implementation Details", "content": "Model and training details: The scalar hyperparameter $w_{ctc}$\nis set to 0.3 empirically to balance the magnitude between the\ntwo losses. For the finetuning stage, we feed the entire audio\nwithout augmentation and calculate a global level loss with its\ncorresponding label. In both stages, the HConv. interface and\nthe stuttering detection head are trainable. We use Adam opti-\nmizer with a learning rate of 1e \u2013 4. We select the checkpoint\nwith the lowest validation loss for the pretraining stage and\nfor the finetuneing stage, we select the one with the highest\nF1 score on the validation set.\nDataset: For pretraining, we use LibriSpeech 360hr and for\nSEP-28K, we follow the split specified in [21] for SEP-28K\nas they make the dataset balanced across positive and negative\nclasses."}, {"title": "4. WORD LEVEL STUTTERING SPEECH DATASET", "content": "To make our system clinically applicable, students in speech-\nlanguage pathology, trained in fluency disorders and stutter-\ning, transcribed and annotated all the speech samples using\nCHAT, a transcription program that is part of CLAN and the\nTalk-Bank initiative [29]. We used a set of codes developed\nto be used with CHAT [30] to annotate stuttering disfluencies\nand typical disfluencies. Our dataset comes from two sources:\nFluencyBank: This dataset included interview data from 36\nadult individuals who stutter from the FluencyBank [31] En-\nglish Voices-AWS Corpus.\nBilinguals Speech: This dataset included narrative samples\nproduced in English by 62 bilingual adults, 6 of whom stut-\ntered. Participants were asked to generate a story based on a\nwordless picture book. In our evaluation, we have two par-\ntitions for this dataset: Stuttering Bilinguals Speech, Non-\nstuttering Bilinguals Speech.\nTo conduct a word-level stuttering evaluation on the\nCHAT annotations, we need timestamp information. To this\nend, we first leverage Whisper Large [32] to transcribe the\ncorresponding audio file and get the transcripts as well as the"}, {"title": "5. EVALUATION", "content": "While we aim to evaluate the performance of our model\non word-level stuttered speech detection, we also want to\ncompare it with previous works that studied utterance-level\nstuttering detection. Hence, we evaluate under both the\nutterance-level and word-level stuttering detection condi-\ntions. For the former, we evaluate on SEP-28K testing set\nand also report the F1 score for each stuttering type on Flu-\nency Bank (a subset of SEP-28K). For the latter, we evaluate\nmodels on our word-level stuttering dataset and report F1,\nprecision, and recall on each partition separately. Addition-\nally, we report the F1 score and Average precision on the\nentire dataset. We choose to report F1 score instead of other\nmetrics such as accuracy due to the fact that the data is highly\nimbalanced (There are many more non-stuttered speech seg-\nments than stuttered segments). We sweep the detection\nthreshold from 0 to 1 with a step of 0.05 to find the optimal\nthreshold for all settings. Throughout our evaluation, we\nfound out that the word-level dataset is very sensitive to the\nthreshold selection. Hence, we also report Average Precision\non the entire word-level stuttered speech dataset. F1 score on\nthe one hand is more intuitive but sensitive to threshold selec-\ntion especially on this dataset, while Average Precision is a\nthreshold-independent metric that evaluates the classification\nmodel under all possible thresholds. With these two evalu-\nation benchmarks, we further conduct ablation experiments\non the effect of CTC loss, the SSL interface/model selection,\nand the size of the finetuning dataset. Finally, we show some\nqualitative examples of our model's outputs."}, {"title": "5.1. Utterance Level stuttered speech detection", "content": "Since our model is only trained to detect stuttering events and\nnot to distinguish between different types of stuttering, we\nreport the type-specific stuttering detection F1 score by se-\nlecting different subsets of the testing dataset. For example,"}, {"title": "5.2. Word Level stuttered speech detection", "content": "In Table 2, overall \"WavLMLg + HConv. + CTC\" outper-\nforms the baseline model and other variations. In terms of\ndifferent partitions, Fluency Bank has the highest F1 scores\nregardless of method. The reason might be the difference be-\ntween native speakers and bilingual speakers. For Bilinguals\nSpeech, stuttering bilinguals tend to have higher F1 scores\ncompared to non-stuttering bilinguals. The reason is that our\nmodel tends to have a high recall and a low precision in most\ncases, which is also true for the baseline model. Compared to\nthe baseline method, our model improves more on recall than\nprecision. One avenue for future work would be to incorpo-\nrate an improved prior over where stuttered speech is likely\nto occur within an utterance or a better way to condition the\nmodel on different patient demographics."}, {"title": "5.3. The effect of CTC loss and SSL interface selection", "content": "To study the effect of CTC loss and HConv. Interface we\ndesign several ablation experiments. For the utilization of\nWavLM, we design models using weighted sum (which is\na set of learnable weights that is multiplied layer-wisely on\nWavLM Large hidden layers and sum up) or simply select a\nsingle layer. Due to the extensive computing, we select every\n4 layers of WavLM Large for our experiment. For HConv.\nand Weighted Sum, we also try adding CTC loss or not.\nOn the utterance level (See Table 2), we see that with the\nhelp of CTC loss, the performance with both HConv. and\nWeighted Sum layer aggregation strategies improved. Inter-\nestingly, without CTC loss, both interfaces perform roughly\nthe same overall. We hypothesize that HConv. could ben-\nefit more from the guidance of CTC loss in terms of word-\nlevel stuttered speech detection. The same trend can also be"}, {"title": "5.4. The effect of different Speech SSL models", "content": "In addition to the interface selection, we also tried using\ndifferent sizes and types of upstream Speech SSL models.\nAs shown in Table 3, overall, the Speech SSL models out-\nperform the baseline model, indicating the effectiveness of\nself-supervised training. Furthermore, Large size models\nsurpass Base size models in all cases. Last but not least,\nWavLM performs the best compared to Wav2vec2 [20] and\nData2vec [33]. From our results, we believe that WavLM is\na more suitable choice for stuttering detection and that previ-\nous works [17, 18, 19] might also benefit if they change their"}, {"title": "5.5. The effect of fine-tuning dataset size", "content": "To test our model on the effect of fine-tuning dataset size,\nwe tried using different proportions of our fine-tuning data:\n0%, 25%, 50%, 75%, 100%. Notice that we keep the ratio of\npositive and negative examples in the training set the same\nwhile using different proportions of the data. We report both\nutterance and word level results on Table 1 and Table 2. On\nboth datasets, we observe a large improvement from 0% to\n50%. Surprisingly, there is a little drop in the performance\nwhen using 75% of the data, but there's a large leap when\nusing 100% of the data. Overall, this again corroborates\nthe problem of data scarcity in stuttered speech detection and\nshows our model's scalability."}, {"title": "5.6. Qualitative results of Stuttering Detection", "content": "To visualize our detection results, we show some of the ex-\namples in our word-level stuttered speech detection dataset.\nAs shown in Figure 3, both the baseline and our model tend\nto predict stuttering events more frequently than the speech\npathologist. However, compared with the baseline model, our\nmodel's predictions tend to be of higher precision."}, {"title": "6. CONCLUSIONS", "content": "In this paper, we shed light onto a more fine-grained evalua-\ntion of stuttered speech detection which is also closer to clin-\nical usage. We curated a clinical word-level stuttering speech\ndataset and further proposed a word-level stuttered speech de-\ntection model which exhibits significant improvements com-\npared to prior work. Additionally, through our comprehen-\nsive ablation studies, we investigated the utilization of Speech\nSSL models on stuttered speech detection and demonstrated\nthe potential scaling of our method.\nFor future work, we think that increasing the dataset size\nand diversity would bring additional improvements for this\nfield. Finally, we would also like to incorporate word-level\nstuttering type classification in our model, to better assist\nspeech pathologists for screening and diagnosis."}]}