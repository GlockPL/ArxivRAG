{"title": "KV PREDICTION FOR IMPROVED TIME TO FIRST TOKEN", "authors": ["Maxwell Horton", "Qingqing Cao", "Chenfan Sun", "Yanzi Jin", "Sachin Mehta", "Mohammad Rastegari", "Moin Nabi"], "abstract": "Inference with transformer-based language models begins with a prompt process-ing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the \"time to first token\", or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/ apple/corenet/tree/main/projects/kv-prediction.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated impressive capabilities on many downstream tasks (Gunter et al., 2024; Achiam et al., 2023; Chowdhery et al., 2022; Abdin et al., 2024). However, the high computational cost of running large language models results in limited capabilities for on-device inference. On-device inference is essential for privacy, latency, energy efficiency, and performance in limited-connectivity areas (Frantar et al., 2022; Alizadeh-Vahid et al., 2023; Stojkovic et al., 2024). For these reasons, LLM efficiency remains an important and active area of research.\nLLM inference with the popular transformer (Vaswani et al., 2017) architecture consists of two phases. First, in the prompt processing phase, the model processes an input prompt to populate the KV cache. Next, in the generation phase, the model autoregressively generates output tokens. Many recent works focus on improving generation time through approximations of the KV cache (Wu &\nTu, 2024; Jiang et al., 2023a; Ge et al., 2023a;b; Li et al., 2023), but only a few recent works have explored improving the prompt processing time (Fu et al., 2024).\nImproving prompt processing time allows an application using the LLM to begin sending outputs to the user earlier. The \"time to first token\" (TTFT) refers to the length of time between a user's input query and the production of the first output token. In scenarios such as chatting with a user, the TTFT may be a more important runtime experience metric than autoregressive generation time, since the user can begin consuming outputs after the first token is produced. For on-device models, prompt processing times can be intolerably slow (up to 10s of seconds, Fig. 1). Reducing TTFT in these cases enables a better user experience."}, {"title": "2 RELATED WORK", "content": "On-Device TTFT Efficiency: Few works have explored our problem domain of improving on-device TTFT. LazyLLM (Fu et al., 2024) uses an attention-based token dropping strategy to drop unneeded tokens at inference time. These tokens can be revived later during the generation phase. Their evaluations are on GPU, but their technique is applicable to on-device inference. Random token dropping (Yao et al., 2022) and static token pruning are both studied in Fu et al. (2024) as methods for improving TTFT. These methods are our baselines.\nServer-Side TTFT Efficiency: Cachegen (Liu et al., 2023) compresses KV caches for faster TTFT, but their setup assumes that a precomputed, pre-compressed KV cache is stored on a server that is available over network. Another similar server-based approach, CritiPrefill (Lv et al., 2024), performs attention-based pruning of KV cache segments on a per-query basis. Our investigation differs from these in that we focus on on-device TTFT improvements.\nContext Compression: Previous works have investigated compressing the KV cache for improving generation efficiency. PyramidKV (Cai et al., 2024), StreamingLLM (Xiao et al., 2023), SnapKV\n(Li et al., 2024), and Model Tells You What To Discard (Ge et al., 2023a) all compress the KV cache along the token dimension by observing attention scores and pruning irrelevant tokens. However, their methods compute a forward pass before pruning. Thus, they improve generation time, but not TTFT. Layer-Condensed KV Cache (Wu & Tu, 2024) develops a method for compressing an N-layer KV cache into a 1-layer KV cache, but requires 9 prompt processing steps. Thus, their method improves generation time and memory usage but negatively impacts TTFT.\nA few recent works have explored compressing the input context using a separate network (Jiang et al., 2023a;b; Ge et al., 2023b; Li et al., 2023). However, their purpose is to improve genera-tion time, not TTFT. As shown in Fu et al. (2024), such methods can increase TTFT due to more expensive prompt processing.\nSharing Activations Between Models: Similar to our method, Tandem Transformers (AishwaryaP et al., 2024) uses two different models that share hidden activations. However, their focus is on im-proving the performance of Speculative Decoding (Leviathan et al., 2022), not TTFT. Other methods explore using a larger teacher model to distill knowledge into a smaller student model to improve efficiency (Bagherinezhad et al., 2018; Gou et al., 2020; Xu et al., 2024). After training, the teacher model is discarded. Our method differs in that both models are retained."}, {"title": "3 MOTIVATION: TIME TO FIRST TOKEN (TTFT)", "content": "We observe that the TTFT of running a language model on an edge device can be prohibitively high, negatively impacting user experience. We visualize total prompt processing time of OpenELM (Mehta et al., 2024) models on an M2 Pro CPU (EveryMac, 2024) with 32GB of RAM in Fig. 1. As prompt lengths and batch sizes increase, the TTFT transitions from noticeable to intolerable. Most users are not willing to wait more than a few seconds for a response from a chat bot (Dashly, 2023), but on-device prompt processing times can far exceed this. This long processing time would be further exacerbated by running the model on low-end hardware instead of a high-end laptop CPU.\nIn addition to the negative user experience created by high TTFTs, we note that the ratio of prompt processing time to generation time is of particular significance to applications that generate short responses. For example, in a question-answering system, the model may only generate a few tokens. Thus, the fractional runtime spent during prompt processing can dominate the overall runtime. In Fig. 1, we visualize the ratio of TTFT to the autoregressive generation time for an OpenELM 3B model. As prompt length and batch size increases, the process becomes compute-bound and the cost of prompt processing substantially increases relative to the cost of a generation step.\nTo illustrate the impact of this high ratio of prompt processing time to generation time, consider a question-answering system with 1024 tokens of context, operating at batch size 1, and generating a 2 token response. In this case, the model will spend 4.8 seconds processing the prompt and generating the first token, and only 4.8/12 = 0.4 seconds generating the second token (since prompt processing takes 12 times as long as generation, see Fig. 1). In this case, 5.2 seconds are spent generating the output, of which 4.8/5.2 = 92.3% is spent in prompt processing.\nIn summary, large prompt processing times can negatively impact practical usage of a model in 2 ways. First, they can result in a large TTFT, negatively impacting user experience. Second, they also can represent a large fraction of the total compute used in inference. Our goal is to reduce on-device TTFT through a novel modeling strategy which we describe in the next section."}, {"title": "4 KV PREDICTION", "content": "Recent works have shown that the KV cache can be compressed with little or no loss in accuracy at generation time (Cai et al., 2024; Li et al., 2024; Xiao et al., 2023; Ge et al., 2023a). Thus, we"}, {"title": "4.1 PREDICTING KV CACHES", "content": "Our model contains a frozen pretrained base network B, a learned auxiliary network A, and a learned\nKV predictor P. The auxiliary and predictor networks are used to generate the predicted KV cache efficiently. Afterwards, inference proceeds with the base model.\nInference: During inference (Fig. 2b) the prompt is passed to the auxiliary model and the auxiliary\nKV cache $KVA$ is computed. Then, the predicted base KV cache $KVB = P(KVA)$ is computed. At this point, A and P are no longer required to continue inference. To generate the first token, a single-token generation step of B is run, but with KVB being used as the keys and values in the attention operations (instead of using the keys and values from the base model's QKV projections). The logits produced from this generation step are used to produce the first output token. Now that the first token has been produced, generation continues autoregressively in the standard fashion, with new KV cache entries being added as new tokens are processed.\nTraining: During training (Fig. 2c), a sequence of tokens I are fed to the auxiliary model A to produce output logits $A(I)$ and a KV cache $KVA$. Then, the predicted KV cache $KVB = P(KVA)$ is computed. Finally, a forward pass of B is computed using the predicted KV cache KVB (instead of using the keys and values from the base model's QKV projections) to produce output logits\n$B(I)$. In other words, the base model computes a forward pass using masked cross-attention with the predicted KV cache to produce output logits. Errors in the base model's logits backpropagate through the frozen base model and into A and P."}, {"title": "4.2 ARCHITECTURAL DETAILS", "content": "Our KV Prediction models are fully specified by our base, auxiliary, and predictor networks. Our base network always consists of a standard pretrained frozen transformer network. Here we describe the architectural details of our auxiliary and predictor networks."}, {"title": "4.3 LOSS FUNCTION", "content": "In this subsection we describe our training loss. It consists of three components: the base loss $LB$,\nthe auxiliary loss $LA$, and the consistency loss $Lc$."}, {"title": "4.4 IMPROVEMENTS IN TTFT FLOPS", "content": "We analyze the improvement in TTFT of our method. The FLOPs-per-token compute cost of trans-formers inference can be estimated as $2P$, where P is the number of parameters in the model (Kaplan et al., 2020). The total FLOPs required for prompt processing for N tokens is NP. Thus, the ratio of the FLOPs of prompt processing to the ratio of FLOPs for a single generation step is N.\nLet $t(N)$ denote the forward pass FLOPs of network N with N input tokens. As described in Section 4, producing the first output token using KV Prediction requires an N-token forward pass of A, followed by an N-token forward pass of P, then a single-token forward pass of B (to generate the first output token using the predicted KV cache). The FLOPs taken to process the prompt and generate the first token is $ta(N) + tp(N) + tb(1) = Nta(1) + tp(N) + tb(1)$. The computational cost of $tp(N)$ is negligible compared to $Nta(1)$, as it contains only a linear layer of smaller dimensionality than the transformer's FFN layers. For sufficient N, $Nta(1) \\gg tb(1)$, and the FLOPs of a single inference are dominated by $Nta(1)$. Thus, the relative improvement in prompt processing FLOPs over the standard inference setup can be approximated as $ta(1)/tb(1)$."}, {"title": "5 EXPERIMENTAL SETUP", "content": "We experiment with our KV Prediction method to evaluate the improvement in TTFT and the accu-racy retention. Here we present details of our experimental setup.\nKV Prediction Models: We experiment with KV Prediction using OpenELM (Mehta et al., 2024), as (1) it provides a suitable efficient architecture for on-device execution, and (2) its layers have variable KV cache sizes, allowing us to test our method's robustness to this challenging scenario."}, {"title": "6 RESULTS", "content": "We analyze our method's performance on language modeling benchmarks in the following sections. Our main results are generative evaluations, which allow us to assess the compatibility of the initial predicted KV cache with the autoregressively generated KV cache. In Section 6.1, we investigate the"}, {"title": "6.1 QUESTION-ANSWERING ON TRIVIAQA", "content": "We investigate our method's efficiency-accuracy trade-off for OpenELM 1.1B and OpenELM 3B in Fig. 3 (we also present these results in table format in Appendix B). We measure accuracy on TriviaQA using LLMEvalHarness (Gao et al., 2021), and use the FLOPs speedup approximation developed in Section 4.4.\nOur method produces the strongest efficiency-accuracy trade-off, tracing a pareto-optimal curve. At a fixed FLOPs reduction, our method achieves the highest accuracy. Equivalently, at fixed accuracy, our method obtains the highest FLOPs reduction. The KVP-C strategy outperforms the KVP-LP method, but the KVP-LP method is able to achieve higher accuracy retention because larger models can be obtained. For instance, OE3B-KVP-LP-0.75 (Fig. 3, right side, top left point in the KVP-LP curve) has roughly 2\u00d7 as many parameters as OE3B-KVP-C-1.1B (Fig. 3, right side, top left point in the KVP-C curve). In all cases except OE3B-KVP-LP-0.25, our model produces higher accuracy at equivalent TTFT FLOPs compared to baselines.\nOur OE baselines correspond to only using the auxiliary model architecture from the KVP-C experi-ment (but with different weights). Directly comparing our KVP-C method to these baselines, we see that our method strongly increases accuracy. A similar observation can be made when comparing OE-LP and KVP-LP models.\nOur naive token-pruning baselines of random pruning (RP) and static pruning (SP) do not perform very well. As explored in Fu et al. (2024), SP can serve as a strong baseline when contexts are extremely long (~ 4k) tokens. We conjecture that performance is worse here because there are fewer redundant tokens in TriviaQA evaluations. LazyLLM provides much higher accuracy retention than naive token-pruning, but accuracy decreases as we push to more aggressive FLOP reductions."}, {"title": "6.2 CODE COMPLETION RESULTS", "content": "We investigate our method's efficiency-accuracy trade-off for code completion with a base model of OpenELM 1.1B in Fig. 4 (we also present these results in table format in Appendix B). We train"}, {"title": "6.3 TIMING EXPERIMENTS", "content": "We perform timing experiments to analyze the runtime improvement of our method. We measure the reduction in TTFT on an M2 Pro CPU (EveryMac, 2024) with 32GB of RAM using the average query length of TriviaQA 1-shot (59 tokens) and a batch size of 64. We plot this TTFT reduction (relative to the Base model) and the accuracy retention (relative to the base model) in Fig. 5a. We find that our method traces the pareto-optimal frontier. The TTFT of OE baseline models exceeds that of KVP-C models, but the stronger accuracy of the KVP-C models keeps them on the pareto-optimal frontier. A similar observation can be made for OE-LP and KVP-LP models. We also present these comparisons in table format in Appendix B.\nNext, we measure the TTFT of a KV Prediction model compared to its base and auxiliary models. To do this, we perform a comparison of the runtime characteristics of OpenELM 1.1B, OE1.1B-KVP-C-450M, and OpenELM 450M in Fig. 5b. We set the batch size to 8 and show the change in TTFT as a function of prompt length. We find that the TTFT of OE1.1B-KVP-C-450M is within 10 - 20% of the OpenELM 450M baseline, demonstrating that KV prediction has small overhead on-device relative to only running the auxiliary model. Both OE1.1B-KVP-C-450M and OE450M have a TTFT far superior to OpenELM 1.1B. This indicates that our method is competitive with the auxiliary-only baseline in terms of speed, while being much more accurate (as discussed in Section 6.1, Section 6.2)."}, {"title": "7 ANALYSIS", "content": "To better understand the performance of our KV cache prediction method, and to motivate future optimizations to improve performance, we analyze the quality of the KV cache predictions."}, {"title": "8 CONCLUSION", "content": "We present a method for improving time to first token (TTFT) called KV Prediction. Our method uses a small auxiliary model to efficiently predict the KV cache needed by a larger base model. We analyze the theoretical and actual speedup of our model, as well as the accuracy retention. We find that our model maintains a strong efficiency-accuracy trade-off, creating a pareto-optimal trade-off in terms of accuracy retention and TTFT."}]}