{"title": "A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio Applications", "authors": ["Sriniketh Vangaru", "Daniel Rosen", "Dylan Green", "Raphael Rodriguez", "Maxwell Wiecek", "Amos Johnson", "Alyse M. Jones", "William C. Headley"], "abstract": "Technological trends show that Radio Frequency Reinforce-\nment Learning (RFRL) will play a prominent role in the\nwireless communication systems of the future. Applications\nof RFRL range from military communications jamming to en-\nhancing WiFi networks. Before deploying algorithms for these\npurposes, they must be trained in a simulation environment to\nensure adequate performance. For this reason, we previously\ncreated the RFRL Gym: a standardized, accessible tool for\nthe development and testing of reinforcement learning (RL)\nalgorithms in the wireless communications space. This envi-\nronment leveraged the OpenAI Gym framework and featured\ncustomizable simulation scenarios within the RF spectrum.\nHowever, the RFRL Gym was limited to training a single RL\nagent per simulation; this is not ideal, as most real-world RF\nscenarios will contain multiple intelligent agents in coopera-\ntive, competitive, or mixed settings, which is a natural conse-\nquence of spectrum congestion. Therefore, through integration\nwith Ray RLlib, multi-agent reinforcement learning (MARL)\nfunctionality for training and assessment has been added to\nthe RFRL Gym, making it even more of a robust tool for RF\nspectrum simulation. This paper provides an overview of the\nupdated RFRL Gym environment. In this work, the general\nframework of the tool is described relative to comparable\nexisting resources, highlighting the significant additions and\nrefactoring we have applied to the Gym. Afterward, results\nfrom testing various RF scenarios in the MARL environment\nand future additions are discussed.", "sections": [{"title": "II. INTRODUCTION", "content": "In the wireless spectrum, the pre-allocation of specific\nfrequencies by the FCC [1] and the increasing usage of the\nradio frequency (RF) range of the spectrum, such as in mobile\nnetworks [2], have motivated the use of cognitive radios\n(CR). CRs use dynamic spectrum access (DSA) to adaptively\naccommodate the quantity of signals continuously being sent\nand received. However, to enable these technologies to better\nmanage the constantly fluctuating nature of the RF spectrum,\nRadio Frequency Reinforcement Learning (RFRL) technolo-\ngies have been developed [3]. These make prediction-based\ndecisions regarding the frequencies a CR will transmit on as\nopposed to being purely reactive, and they have significantly\nimproved the ability of a CR to avoid interference in the\nspectrum [4]-[6].\nWith the expansion of 5G and the early development of\n6G wireless communication, demand for deployment-ready\nRFRL algorithms has increased dramatically; the RFRL Gym\nwas created as a simulation environment and testbed to ac-\ncelerate their development [7]. However, the original version\nof the RFRL Gym included exclusively single-agent training\nscenarios. In the majority of real-world scenarios, multiple\nsignals will exist in a wireless spectrum at one time, such as in\nmilitary contexts where spectrum sharing and radio jamming\nare an essential concern [8] or in everyday products (e.g.,\nmobile phones) whose ubiquity and usage of the spectrum\nare continuously increasing [9]. CRs that act in the same\nspace as other \u201cintelligent\u201d radios are dependent upon Multi-\nAgent Reinforcement Learning (MARL) methodologies to\ndependably achieve optimal results, as using a single-agent\nmodel will not properly encapsulate all of the dynamics of\na given real-world situation; as a result, these are currently\nbeing rapidly researched and developed [10]\u2013[12]. Therefore,\nto increase the fidelity and realism of the training process,\nMARL scenarios were deemed a necessary addition to the\ntestbed. This paper describes this primary addition to the\nRFRL Gym along with IQ data generation and rendering\nimprovements for a more realistic, friendly user experience."}, {"title": "B. Multi-Agent Reinforcement Learning", "content": "Multi-Agent Reinforcement Learning (MARL) involves re-\ninforcement learning scenarios in which multiple RL agents\nare acting and/or being trained simultaneously. In the context\nof the RFRL Gym, each agent is analogous to a CR device.\nConsider Fig. 1, in which 4 distinct co-channel entities can be\nseen during the same time period. Assume these signals are\neach being transmitted by an independent CR (a frequently-\nencountered scenario). In response to additional transient or\npersistent signals in the spectrum, the CRs may have to alter\ntheir transmission frequencies to avoid interference. Under the\nassumption that these CRs utilize RL to determine subsequent\nfrequencies, this scenario is considered MARL. Such problems\nwill become even more common as the expansion of IoT\nbrings an increase in the number of devices that utilize the\nwireless spectrum, further emphasizing the need for programs\nsuch as our RFRL Gym to play a pivotal role in solving these\nproblems.\nIn MARL, the reward functions of these agents are related\nto the actions of the other agents, since those actions would\ncontribute to modifying the environment and make it non-\nstationary [13]. Agents can be encouraged to act cooperatively,\ncompetitively, or a mixture of the two. Independently from that\ncategorization of situations, the MARL algorithms themselves\ncan be centralized or decentralized. In a centralized context,\na single model is used to determine the actions of multiple\nagents, whereas in a decentralized context, each agent is\nassociated with its own model and determines its action exclu-\nsively. Optionally, decentralized agents can communicate with\neach other in some manner\u2014such as by sharing observations,\njoint rewards, or joint actions\u2014which is a quality known as\n\"networked agents\" [13]. Our RFRL Gym is able to support all\ncombinations of cooperative and competitive situations due to\neasy modification of scenarios, allowing for MARL research\nin RFRL to be further explored."}, {"title": "III. BACKGROUND", "content": "Machine learning techniques have appeared in various facets\nof the RF Field [14]\u2013[16]. In particular, DSA has become a\nfocus of RL research, both with singular agents [17] and for\nmultiple agents [18]. MARL methods have shown excellent\nperformance in both cooperative [19] and adversarial [20] sce-\nnarios. In recognition of these achievements, research efforts\nfor MARL in RF have swelled [11], [12], [21], [22]. Therefore,\nthe addition of MARL functionality to the RFRL Gym was\nessential.\nPrior works [7], [23]-[25] describe tools for training RL\nalgorithms for the wireless communications space, including\nthe multi-agent RFRL Gym that we are introducing. Table I\ndisplays the differences between these environments.\nFlexible Scenario Design: Ability to easily customize\nentities and agents present in the training environment. [23]\nscenarios can be customized, but each requires a non-trivial\nimplementation of a scenario class for each testing scenario.\nSimilarly, [24] requires a custom C++ interface to be written\nfor each scenario. [25] provides a wide variety of premade\nscenarios of incumbents (similar to entities). Though not\ncustomizable, they are relatively comprehensive in conjunc-\ntion. The RFRL Gym's scenarios can be easily and finely\ncustomized with JSON scenario configuration files.\nRL Package Compatibility: Ability to interface with ex-\nternal RL libraries for algorithms and optimization methods.\n[23], [24], and the RFRL Gym utilize OpenAI's Gym API\n[26]. Because of this, these environments interface with vari-\nous libraries including Stable-Baselines [27], PettingZoo [28],\netc. [25] is not similarly integrated, as models are primarily\nintended to be trained offline on data collected from the\nenvironment.\nSpectrum Sensing Capabilities: Various methods of ob-\nserving the state of the spectrum. This includes methods of\ndetecting and classifying entities.\nMulti-Agent Capability: The newest version of the RFRL\nGym environment features the addition of multi-agent capa-\nbilities, meaning that it can support the concurrent training of\nmultiple intelligent agents in one scenario. This also includes\ncentralized and decentralized information structures, as well\nas competitive, cooperative, and mixed settings for scenarios.\nEase of Use: Tool is designed to simplify the training\nand analysis process of RL algorithms, especially if this\nfunctionality is accessible to a wide audience. [23] and [24]\nboth require understanding of an external resource (GNU\nRadio and ns-3 respectively). GNU Radio is not officially\nsupported on Windows and [23] does not offer rendering.\n[24] requires C++ implementation to interface with the ns-\n3 environment. [25] does not offer rendering and access is\nlimited to researchers affiliated with the US Department of\nDefense. The RFRL Gym is designed to be accessible to\nboth those new to wireless communications and those with\nextensive experience in the field. The RFRL Gym features a\nGUI for easy scenario generation and a variety of rendering\noptions for data visualization.\nHardware Compatibility: The training environment's abil-\nity to interface with radio hardware for real-time information\ncollection. [23] and [24] both utilize external resources for\nwireless spectrum simulation, GNU Radio and ns-3 respec-\ntively. Both of these resources can interface with hardware for\nreal-time spectrum sensing or simulate the wireless spectrum\nwith software. [25] uses fiber optics to emulate the wireless\nspectrum. So, although its resources are accessed remotely\nby users, no real radios are involved. However, because [25]\nadopts the OpenRAN Gym framework [29], containerized\napplications can easily be ported from Colosseum to hardware-\ninterfacing training environments. Currently, all of the RFRL\nGym's spectrum simulation is software-driven using open-\nsource Python modules but the addition of hardware compat-\nibility is in progress.\nRelative to comparable tools, the RFRL Gym offers\na uniquely accessible workflow. The environment features\ndeeply customizable scenarios for bespoke training executions.\nWith human-readable information from these scenario files, IQ\ndata is generated using abstracted information so a limited\nunderstanding of wireless communications is necessary for\nusers. These scenario files can be produced using an easy-\nto-use graphical user interface, as shown in [7]. Overall,\nwithout the burden of the need for extensive knowledge of\nwireless communications, researchers can use this tool to\nincrementally develop an understanding of concepts such as\nspectrum sensing, DSA, and signal modulation. The robust\nvisualization and efficient abstraction make the RFRL Gym\nan effective educational tool for those new to the field of\nRF. These features distinguish our tool from others as an\naccessible resource for experimentation and testing. However,\nit shares its more technical features and functionality with\npowerful spectrum simulation tools, such as GNU Radio and\nns-3, making the RFRL Gym an appropriate tool for experts\nas well."}, {"title": "IV. THE RFRL GYM MARL FRAMEWORK", "content": "In order to execute training simulations, users will first\ncreate a JSON scenario file. This file contains information\nabout the agents and environment (detailed below in Section\nIV-D). Fig. 2 shows the exchange of information that occurs\nduring a training simulation. Entities and agents are prompted\nby the gym to select actions at each timestep; entities follow\nthe action pattern corresponding to the entity type provided in\nthe scenario, and learning agents take actions based on their\npolicies that were refined through a learning algorithm. This\ndata is optionally converted into IQ data if using the single-agent environment. Then, the environment determines the\noccupancy of each channel and displays its results using the\nselected rendering method. Investigators can use this displayed\ninformation to assess the performance of their algorithms.\nAfterward, the agents' rewards and the environment's state\nare calculated, and these are recorded by the gym before being\nsent to the agents to begin the next timestep."}, {"title": "B. Comparison to Original RFRL Gym", "content": "This paper expands upon the existing single-agent version\nof the RFRL Gym, defined in [7]. Structurally, the new version\nof the RFRL Gym remains similar to that of the initial\nversion. As can be seen in Fig. 2, information transfer between\ncomponents remains consistent in the MARL environment,\nabiding by the classic reinforcement learning cycle.\nThe original single-agent environment will still exist in the\npackage with its previous functionality. One addition made\nto this environment is a separate single-agent environment\nclass that incorporates software-generated IQ data to better\nconsider real spectrum dynamics, described further in Section\nIV-F. Another primary modification made to the original\nenvironment is the upgrade to the Farama Foundation's Gym-\nnasium [30], which is an extension to OpenAI's Gym API\n[26]. This interface is compatible with external RL libraries\nsuch as Stable-Baselines3 [27] and PettingZoo [28]. This\nAPI is also leveraged by our MARL environment through\ninheritance from Ray RLlib's MultiAgentEnv class [31].\nOn top of compatibility with external libraries, Ray RLlib\nprovides several high-powered functions such as remote cluster\ncomputation and cloud interfacing, whose computing power\ncan be helpful when running larger-scale MARL simulations.\nIn the new multi-agent environment, the movement patterns\nthat the non-learning RF entities have available in the RL\nenvironment have not been modified. Their functions and\napplications can be found in [7]. Each agent can track the\nlocations of the entities with either the detect or classify\nobservation mode, as in the original Gym. The observation\nand action histories of each agent are stored independently\nand provided to the user at each timestep, facilitating easy\nanalysis of individual agents. The primary differences between\nthe MARL RFRL Gym environment and the single-agent\nRFRL Gym environment exist in reward calculation, scenario\nfile construction, data structures for observation and action\nhistories, and rendering performance. The rest of this section\nenumerates and explains these features in further detail."}, {"title": "C. MARL Reward Calculation", "content": "Ray RLlib's RL training framework includes user-defined\nagent groups, which can, for instance, be defined explicitly us-\ning RLlib's MultiAgentEnv.with_agent_groups()\nfunction or implicitly using the policy_mapping_fn con-\nfiguration property depending on the algorithm being used.\nThe members of an agent group can view fellow members'\nactions at each timestep. The rewards for each agent in a group\nare correlated, and so these features permit centralized RL\nalgorithms to be trained in our environment. This is useful\nas many algorithms use a very common approach involving\ncentralized training with decentralized execution (CTDE) [32].\nA centralized critic can even be implemented manually, such\nas by creating a function that shares observations [31].\nAlternatively, agent groupings with one member per group\nmay simulate a decentralized reward system. A standard use\ncase for this method is decentralized training with decentral-\nized execution (DTDE), a step up from independent Q-learning\n(IQL) in a range of scenarios, including mixed cooperative-\ncompetitive settings [33], [34].\nThe individual reward functions are given in Equations 1\nand 2, where a is an agent's action and g is the set of\nactions from the agents in a group. The dynamic spectrum\naccess (DSA) function rewards the agent for avoiding channels\ncontaining other entities or agents, and the jamming function\nrewards the agent for occupying the same channel as a\nparticular entity in the environment, which is selected by the\nuser. The reward function for each agent group is calculated\nas the sum of the individual rewards of each agent [31]. The\nmean reward, attributed to each agent in an agent group, can be\naccessed in the results dictionary returned by RLlib during\ntraining but its usage (compared to the usage of the group's\nreward) depends on the algorithm used. This is shown in\nEquation 3. As a result of this customizability of agent groups,\nthe RFRL Gym is suitable for cooperative, competitive, and\nmixed training scenarios."}, {"title": "D. MARL Scenario Files", "content": "The definition of a scenario and its separate categories\nare stated in [7]. The only category that has changed is the\nenvironment dictionary in the JSON file, which controls\nall specific constraints that the multiple agents will learn\nunder. Whereas our previous implementation only included the\nconfigurations for one agent, the environment section now\ncontains an agents dictionary, which defines the number of\nagents in the environment and the observation mode, reward\nfunction, and target entity (if applicable) for each agent. For\nease of use, we have also added a comments key-value pair\nto allow the user to add a description of the scenario within\nthe JSON itself. This ensures that when the scenario is given\nto another analyst, it helps them understand the scenario's\ncontents and purpose."}, {"title": "E. Rendering Enhancement", "content": "Previous rendering in the RFRL Gym only supported a\nsingle-agent environment for both PyQt rendering and terminal\nrendering, as explained in the previous paper [7]. With the\nenhancement of the RFRL Gym to a multi-agent environment,\nthe major changes in rendering include visuals of cumula-\ntive reward changes for each agent at each time step in a\ntraining episode (or at each episode, depending on the user's\nconfiguration) and different colors to represent each learning\nagent or non-learning entity as it occupies a space in the\nradio frequency spectrum, as shown in Fig. 1. The same\ncolor that identifies a specific agent in the depiction of the\nspectrum will also be used for the corresponding agent on the\ncumulative reward graphs. The primary justification behind\ngiving a different color scheme to each individual agent,\nsimilarly to how multiple entities in the single-agent gym\nwould have their own color, is to easily identify which agents\nare learning their action policy effectively. The subclasses\nof our MultiAgentRenderer class can be modified to\nvisually distinguish between different types of collisions (e.g.,\nagent with agent, agent with entity, entity with entity) by\nchanging the colors that represent such events."}, {"title": "F. IQ Data Generation", "content": "The RFRL Gym also now includes another single-agent\nenvironment in the form of a Python class (RFRLGymIQEnv)\nthat uses software-generated IQ data to represent the locations\nof entities and the agent in the spectrum. In this environment,\nat each time step, the discrete actions selected by each\nentity and the agent are used to produce simulated IQ data.\nModulation methods are available to users out of the box in\nboth LDAPM and Tone classes. Once the data are created, the\nenvironment infers occupancy of each channel based on an\nenergy threshold being met. This method improves the realism\nof scenarios as the non-deterministic nature of the wireless\nspectrum is included in the training."}, {"title": "V. SIMULATION RESULTS AND DISCUSSION", "content": "Four multi-agent reinforcement learning algorithms were\ninvestigated in our evaluations. They were chosen due\nto being RLlib's only available algorithms with multi-\nagent functionality at the time of testing. For each, the\npolicy_mapping_fn function in the algorithm's configu-\nration maps each agent to its own policy so that, as mentioned\nearlier in Section IV-C, the agent groups simulate a decentral-\nized setting. The algorithms are listed below."}, {"title": "A. Algorithms", "content": "DQN is a widely-used RL\nalgorithm that uses a neural network to approximate Q-Values\n[35]. The DQN implementation used for simulations uses\nan experience replay buffer to minimize recency bias, which\nprioritizes selecting more recent experiences. Though DQN\nis originally a single-agent learning algorithm, RLlib's multi-\nagent extension of DQN uses a prioritized replay buffer,\ndesigned specially for multi-agent environments, that allows\nthe user to optimize how the prior sampled experiences are\npicked. DQN is typically well-suited for discrete action spaces\n[36].\n2) Proximal Policy Optimization (PPO): PPO is another\nwell-known, standard learning algorithm in single-agent RL\nsystems, using an objective function and a surrogate objective\nfunction where the policy updates for the latter are constrained\nto remain relatively small [37]. This tends to ensure that\nlearning is stable. Similarly to DQN, we used RLlib's im-\nplementation of multi-agent PPO.\n3) Asynchronous Proximal Policy Optimization (APPO):\nAPPO is an RLlib-implemented variant of PPO with asyn-\nchronous sampling of experiences, which allows it to be\nquicker (measured in real-world time) than PPO, though its\nperformance is not always better than standard PPO [31].\n4) Importance Weighted Actor-Learner Architecture (IM-"}, {"title": "B. Evaluation Methods", "content": "In order to test our scenarios, we wrote a testing script for\neach algorithm to run and evaluate the effectiveness of the\nlearned policies for the agents in any given scenario. Due to\neach algorithm containing a different number of episodes per\ntraining iteration, the method of algorithm evaluation is based\non training episodes to maintain uniformity. The collective\nreward accrued by the agents in an episode will be plotted\nagainst the episode count for each algorithm, with the results\nof the algorithms' performances shown and discussed below\nin Section V-C."}, {"title": "C. Scenarios Tested", "content": "Here, we present a variety of scenarios which represent\ndifferent states that learning agents and non-learning entities\ncan take on in wireless communications situations, before\ndiscussing the results from training and evaluating the learning\nalgorithms for the agents. For each scenario, the environment\nassumes a standard of 10 discrete frequency channels, as well\nas 100 sampled timesteps (i.e., calls of env.step(), where\nenv is an instance of our multi-agent environment class) per\nepisode of training. It should be noted that during training,\ndepending on the algorithm used, the number of times that\neach sampled step() call is actually trained on varies and,\nas mentioned earlier, the number of episodes per iteration also\nvaries. Additionally, as mentioned in Section IV-C, we use\na fully decentralized system for every scenario, so for each\nalgorithm, each agent is mapped to its own policy.\n1) Scenario 1: Jamming Constant-Frequency Entities:\nAs a baseline test of the ability of our multi-agent RFRL\nGym environment to support learning algorithms that train\nthe agents' policies, a simple scenario was created where\neach agent was given an entity to jam (i.e. transmit on the\nsame channel as). There are four agents, where two have\nthe detect observation mode and the other two use the\nclassify, and all of them are using the jam reward mode.\nThere are also 2 entities which constantly remain on one\nfrequency channel each, separate from each other.\nThe environment successfully interfaced with the various\naforementioned algorithms implemented by RLlib, and the\nresults are displayed in Fig. 3. An episode reward of 400\nindicates that all 4 agents consistently picked the optimal\nchannel to transmit on; this is because each optimal action\nobtains a reward of 1 for that agent, and the rewards are simply\nsummed up across all 4 agents and 100 timesteps in an episode\nto form the episode reward. (Note: For the same reason,\nScenario 5 also has a maximum episode reward of 400.)\nAs shown, all algorithms converged to an optimal policy for\nall agents, demonstrating the basic capabilities of our RFRL\nGym environment involving having multiple agents effectively\nlearn to complete a task in the RF spectrum. Evidently, DQN\ndemonstrated faster convergence than the other algorithms,\nwhich is likely due to its general aptitude for discrete action\nspaces, with the benefit from this being compounded by only\nhaving a small number of actions\u2014which are the 10 frequency\nchannels to transmit on available for each agent [36].\n2) Scenario 2: Fixed-Hop Frequency Jamming: To incorpo-\nrate slightly more realistic non-learning, transmitting entities\ncompared to those in Scenario 1, we created a scenario which\ninvolves two agents trying to both jam a single entity that\nuses a linear hopping pattern. The results are shown in Fig. 4.\nDue to only having 2 agents, the maximum episode reward\nattainable in this scenario is 200. Note: Due to the same\nreason, Scenarios 3 and 4 also have a maximum episode\nreward of 200.\nThis scenario revealed more interesting results than Scenario\n1, with the most noticeable difference being that APPO and\nIMPALA converged to an episode reward value of 0. After\nobserving the agents' actions, it became clear that the two\nagents attempting to follow the fixed hopper entities were\nnot able to learn the optimal actions to take regardless of\ntheir observation mode and instead opted to not transmit\non any channel, giving a reward of 0, instead of repeatedly\npicking the wrong channel, which would give a reward of -1. A\nprobable reason for this, considering that APPO and IMPALA\nshare the same asynchronous sampling architecture for their\nreplay buffer of experiences, is that the nature of the fixed-\nhoping entity requires the learning algorithm to be aware of\nthe hopping pattern (which changed the transmission channel\nevery step in time), and the asynchronous data collection in\nthese algorithms may not be capable of that [38].\nAdditionally, DQN demonstrated rather quick learning to\nan optimal convergence, before dropping its performance at\naround 600 episodes; this is conventionally known as catas-\ntrophic forgetting [39]. Upon further investigation, a highly\nlikely cause of this is that 600 episodes, or 60,000 steps, is\nequal to the size of the experience replay buffer capacity of this\nDQN implementation which was set as a hyperparameter, so\nthe drop in episode rewards coincides with the point at which\nthe replay buffer begins to filter out previous experiences\nin a first-in first-out (FIFO) cyclic manner [35]. Allowing\nprevious experiences to be removed from the sampling process\ncan destabilize learning due to, for instance, specific valuable\nexperiences being lost or the policy being learned from new\nexperiences differing from the policy attained from the initial\n60,000 experiences [40]. DQN did appear to relearn after\nthis, but in this scenario, PPO demonstrated more robustness\nagainst catastrophic forgetting.\n3) Scenario 3: Dynamic Spectrum Access with Constant\nFrequency Entities: An extremely common setting in wireless\ncommunications is when each cognitive radio possesses the\ngoal of transmitting on a free channel. This scenario was\ndesigned to simulate how RF devices would learn to act in\na simple variation of this setting, by using the dsa reward\nfunction. Due to the non-learning entities only transmitting\non constant frequencies in this scenario, it is comparable to\nScenario 1 when each agent learned to target one set channel,\nexcept in this case multiple agents targeting the same channel\nwould negatively reward both of them. It was expected that\nlearning would be fairly fast, similarly to Scenario 1, but with\nless stability due to agents being able to interfere with each\nother's learning goals, and these predictions were reflected in\nthe results. They are displayed in Fig. 5. One notable outcome\nwas that DQN once again demonstrated catastrophic forgetting\nafter maintaining high rewards consistently.\n4) Scenario 4: Dynamic Spectrum Access with Fixed-Hop\nFrequency Entities: In this scenario, we wished to demonstrate\nagents learning to find free channels when there are moving\nentities in the environment. However, there are fewer entities\nthan in the previous scenario to make it more feasible for the\nagents to identify patterns in the channels that are occupied.\nNaturally, there was significantly more variability in the re-\nwards due to the agents having to learn a more complicated\npolicy and continuously unintentionally running into collisions\n(e.g., with each other) in the convoluted spectrum; the results\nare in Fig. 6.\n5) Scenario 5: A Mixed Setting: With each scenario so far,\nwe have attempted to incorporate more realism to provide\nsimulations of these algorithms working in useful contexts,\nso Scenario 5 emulates a situation with some agents having\ngoals that allow for cooperation with others being directly\nadversarial to other agents in the spectrum. To support this,\nthere is variance in observation modes, reward modes, and\ntarget entities among the 4 agents. The results are shown in\nFig. 7.\nEvidently, none of the algorithms learned to approximate a\ncompletely optimal function. DQN learned an almost optimal\npolicy, although there are continuous dips in performance\nwhich were due to the DSA agents taking actions that in-\nterfered with the fixed-hop frequency entities. Similarly, with\neach of PPO, APPO, and IMPALA, a subset of the agents\nlearned a policy without issue and the remaining agents either\ndecided to not transmit to obtain the 0 reward or continuously\nchose suboptimal actions that averaged out to a positive\nreward. Similar to the previous scenarios for these algorithms,\nthe agents jamming the constant-frequency entities tended to\nlearn perfectly, the agents jamming the fixed-hop frequency\nentities tended to not transmit due to difficulty following the\nentity, and the agents finding free channels did transmit but had\nhigh variability in their rewards. A primary goal in this tool's\ndevelopment is to empower future RF and RL researchers\nwho are using it to explore exactly these types of nuanced\ntrade-offs, as understanding them is crucial for developing\nfuture algorithms that reach optimal policies in situations with\ncompeting intelligences."}, {"title": "VI. CONCLUSION", "content": "This paper introduces the addition of multi-agent reinforce-\nment learning functionality to the RFRL Gym. This new\nfunctionality is key for accurately representing and simulating\nthe real-world wireless spectrum. In conjunction with this\nupgrade, the RFRL Gym was integrated with Ray RLlib. This\nnew interface leverages the new industry-standard Gymnasium\nAPI. As a result, the RFRL Gym now supports countless ad-\nditional integrations including Stable-Baselines3, PettingZoo,\nand others.\nFrom experimentation, our results conclusively demonstrate\nthat the environment properly enables multiple agents to learn\noptimal policies, despite having complex variations across the\nreward functions used. Additionally, it has been shown that\na variety of multi-agent learning algorithms can be used in\ntandem with the new multi-agent RFRL Gym environment.\nFuture work includes the addition of multi-channel agents\n(which can represent real-world devices that transmit signals\non multiple frequency bands at once), the use of real hardware\nto integrate with and represent the state of the environment,\nmore customizable reward functions (e.g., functions that in-\ncentivize agents to jam other learning agents or that allow\nmultiple agents to collaborate such that only a single one\nof them jams an entity) within the scenario file, and testing\ndifferent variations of scenarios with full centralization / partial\ncentralization of the agents' observations, actions, and rewards."}], "equations": ["r_{DSA}(a)=\\begin{cases}1, & \\text{no collision}\\\\0, & \\text{no transmission}\\\\-1, & \\text{collision}\\end{cases}", "r_{jamming}(a) =\\begin{cases}1, & \\text{collision with target entity}\\\\0, & \\text{no transmission}\\\\-1, & \\text{transmitting elsewhere}\\end{cases}", "r_{mean}(g)=\\frac{\\Sigma_{a \\in g}r(a)}{|g|}"], "keywords": ["multi-agent reinforcement learning", "wireless communications", "dynamic spectrum access", "OpenAI Gym"]}