{"title": "Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects", "authors": ["Abdurrahman Zeybey", "Mehmet Ergezer", "Tommy Nguyen"], "abstract": "3D Gaussian Splatting has advanced radiance field reconstruction, enabling high-quality view synthesis and fast rendering in 3D modeling. While adversarial attacks on object detection models are well-studied for 2D images, their impact on 3D models remains underexplored. This work introduces the Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate adversarial noise targeting the CLIP vision-language model. M-IFGSM specifically alters the object of interest by focusing perturbations on masked regions, degrading the performance of CLIP's zero-shot object detection capability when applied to 3D models. Using eight objects from the Common Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces the accuracy and confidence of the model, with adversarial noise being nearly imperceptible to human observers. The top-1 accuracy in original model renders drops from 95.4% to 12.5% for train images and from 91.2% to 35.4% for test images, with confidence levels reflecting this shift from true classification to misclassification, underscoring the risks of adversarial attacks on 3D models in applications such as autonomous driving, robotics, and surveillance. The significance of this research lies in its potential to expose vulnerabilities in modern 3D vision models, including radiance fields, prompting the development of more robust defenses and security measures in critical real-world applications.", "sections": [{"title": "1 Introduction", "content": "Computer vision has rapidly advanced with the development of large-scale datasets and vision-language models such as CLIP Li u. a. (2022). These advancements have expanded the application of computer vision algorithms into areas critical to modern life, such as humanoid robots, autonomous driving, and surveillance systems Kim u. a. (2024)Zhou u. a. (2024). Ensuring the accuracy and robustness of these systems is crucial, as errors could lead to severe consequences, from accidents in autonomous vehicles to failures in surveillance systems.\nWhile adversarial attacks on 2D vision models have been widely studied Goodfellow u. a. (2015);\nSzegedy u. a. (2014), their impact on 3D models remains underexplored Li u. a. (2024); Song u. a.\n(2024). This gap is significant, as 3D models are increasingly used in real-world applications, including robotics, augmented reality, and autonomous systemsZhu u. a. (2023); Zhao u. a. (2019).\nRecent progress in 3D model rendering techniques, such as 3D Gaussian Splatting (3DGS) Kerbl u. a.\n(2023), has enabled high-quality view synthesis and efficient rendering in 3D modeling, presenting\nnew opportunities, and vulnerabilities, for adversarial attacksZeng u. a. (2019). However, there is"}, {"title": "2 Method", "content": "In this section, we introduce our proposed method, the Masked Iterative Fast Gradient Sign Method\n(M-IFGSM), designed to generate adversarial perturbations specifically targeting 3D models in a\nvision-language context. While most adversarial methods perturb the entire image uniformly, this can\nlead to suboptimal attacks and degrade downstream tasks, such as 3D reconstruction. Our approach\nenhances traditional adversarial attack methods by focusing perturbations solely on the object of\ninterest within input images. We detail the components of our pipeline, including segmentation using\nthe Segment Anything Model (SAM), adversarial perturbation generation with M-IFGSM, and 3D\nmodel reconstruction using Gaussian Splatting.\nOur method consists of three main stages: i) Mask Generation: Utilizing the Segment Anything\nModel (SAM) to extract object masks from input images. ii) Adversarial Perturbation Generation:\nApplying M-IFGSM to generate noise focused on masked regions. iii) 3D Model Reconstruction:\nBuilding 3D models from adversarial images using Gaussian Splatting. Figure 1 illustrates the overall\npipeline."}, {"title": "2.1 Mask Generation with SAM", "content": "We employ the Segment Anything Model (SAM) Kirillov u. a. (2023) to generate accurate segmenta-tion masks for the target objects within the input images. Utilizing the pre-trained 'sam-vit-h-4b899'\ncheckpoint, SAM provides high-quality masks without the need for additional training. The mask\ngeneration process outputs the segmentation mask $M_i$, bounding boxes, and quality metrics."}, {"title": "2.2 Adversarial Perturbation Generation with M-IFGSM", "content": "Traditional FGSM applies uniform perturbations across the entire image, often leading to undesirable\nartifacts in 3D reconstructions due to background noise. To address this, we propose M-IFGSM, which enhances FGSM by concentrating perturbations solely on the segmented object regions. This targeted approach improves attack effectiveness and preserves background integrity.\nThe untargeted attack is formulated as:"}, {"title": "2.3 3D Model Reconstruction", "content": "After generating adversarial images with M-IFGSM, we reconstruct 3D models using the Gaussian\nSplatting method Kerbl u. a. (2023). We use an 85-15% train-test split, with 35 images for training and six for testing.\nThe reconstruction process involves four stages. i) Initialization: Using a sparse point cloud from Structure from Motion Schonberger und Frahm (2016) to initialize 3D Gaussians. ii) Optimization: Adjusting the Gaussians' parameters (position, covariance, opacity, color) via gradient descent to match the adversarial images. iii) Adaptive Density Control: Refining the model by adding or splitting Gaussians in under- or over-reconstructed areas. iiii) Rendering: Employing a tile-based rasterizer for efficient real-time visualization.\nThis process allows us to assess how adversarial perturbations affect 3D model accuracy and the performance of the CLIP model."}, {"title": "2.4 Experimental Setup", "content": "We refer to the attacked model as the victim. We target the CLIP ViT-B/16 model Dosovitskiy u. a.\n(2021), which combines vision and language understanding. The model divides input images into\n16 \u00d7 16 patches and processes them using transformer layers.\nFor our dataset, we employed the Common Objects in 3D (CO3D) dataset Reizenstein u. a. (2021), selecting one dataset for each of the eight object classes. Each dataset contains approximately 200 images from different angles. We reduce the number to 41 images per class by selecting every fifth image and resizing them to 224 \u00d7 224 pixels to meet the input requirements of CLIP. Our experiments are conducted on a system with dual NVIDIA RTX 3090 GPUs."}, {"title": "3 Results", "content": "In this section, we present and analyze the outcomes of our experiments conducted to evaluate\nthe efficacy of M-IFGSM for generating adversarial noise in the context of 3D Gaussian Splatting\n(3DGS). Our primary objective was to assess the impact of adversarial attacks on the classification performance of the object detection model when subjected to adversarially perturbed 3D models.\nFirst, we discuss the results of perturbed images generated by the M-IFGSM method, followed by the rendered images generated by the 3DGS technique."}, {"title": "3.1 Perturbed Images Results", "content": "Table 1 presents the classification confidence and accuracy for perturbed images generated using\nM-IFGSM. The \u201cOriginal Images\" column contains the confidence and top-1/top-5 accuracy for unmodified images, serving as baseline performance. The \u201cAdversarial Images\" column shows the same statistics for perturbed images, highlighting the degradation in the model's confidence and\naccuracy due to the adversarial perturbations."}, {"title": "3.2 Rendered Images Results", "content": "The additional results on the rendered images further highlight the impact of adversarial perturbations on 3D models. In Table 2, we present the results of the classification confidence and accuracy of\nrendered images from the 3D Gaussian Splatting (3DGS) models. The original 3DGS model was created with a clean image dataset, while the adversarial 3DGS model was created with perturbed\nimages. At this point, we split the images into 85% train and 15% test sets, meaning 35 out of 41\nimages were used in the 3D modeling process, while 6 out of 41 images' camera positions were used\nas test renders after the models were created."}, {"title": "4 Conclusions", "content": "In this paper, we investigated adversarial attacks targeting vision-language models, specifically\nfocusing on CLIP's object detection capabilities, and explored the transferability of these attacks to 3D models. Our study primarily centered on the application of the Masked Iterative Fast Gradient\nSign Method (M-IFGSM), and we demonstrated how this adversarial noise could be effectively\nintegrated into 3D Gaussian Splatting models. By employing the Common Objects 3D (CO3D)\ndataset, we conducted experiments on eight distinct object classes, creating noisy 3D models with\nadversarial noise from 35 images captured from different angles.\nThe significance of this work lies in its extension of adversarial attacks from 2D vision systems to\n3D object detection models, which presents a previously unexplored area of research. Our findings\nindicate that, with the addition of segmentation, the adversarial noise designed for 2D models not\nonly successfully transfers to the 3D domain but also leads to a substantial degradation in model\nperformance. While the target class was consistently identified as the top-1 prediction in the original 3D models, under adversarial conditions, the model's top-1 accuracy sharply decreased. For the training images, the average top-1 accuracy fell from 95.4% to 12.5%, and for the test images, it\ndropped from 91.2% to 35.4%. This demonstrates that adversarial perturbations in a 2D context can profoundly impact the accuracy of 3D models, including those constructed from Gaussian Splatting techniques.\nThis work represents a novel contribution to the adversarial machine learning field by bridging the gap between 2D adversarial attacks and their effects in 3D environments. Additionally, our approach highlights the vulnerabilities of cutting-edge models like CLIP when tasked with multi-view object"}]}