{"title": "Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "abstract": "Artificial intelligence (AI) is expected to significantly enhance radio resource management (RRM) in sixth-generation (6G) networks. However, the lack of explainability in complex deep learning (DL) models poses a challenge for practical implementation. This paper proposes a novel explainable AI (\u03a7\u0391\u0399)-based framework for feature selection and model complexity reduction in a model-agnostic manner. Applied to a multi-agent deep reinforcement learning (MADRL) setting, our approach addresses the joint sub-band assignment and power allocation problem in cellular vehicle-to-everything (V2X) communications. We propose a novel two-stage systematic explainability framework leveraging feature relevance-oriented XAI to simplify the DRL agents. While the former stage generates a state feature importance ranking of the trained models using Shapley additive explanations (SHAP)-based importance scores, the latter stage exploits these importance-based rankings to simplify the state space of the agents by removing the least important features from the model's input. Simulation results demonstrate that the XAI-assisted methodology achieves ~97% of the original MADRL sum-rate performance while reducing optimal state features by ~28%, average training time by ~11%, and trainable weight parameters by ~46% in a network with eight vehicular pairs.", "sections": [{"title": "I. INTRODUCTION", "content": "CELLULAR vehicle-to-everything (C-V2X) networks have emerged as key enablers for intelligent transportation systems, leveraging existing cellular infrastructure to improve road safety, traffic efficiency, and in-vehicle entertainment. C-V2X technology promises widespread coverage, high reliability, and efficient spectrum use, even in high-mobility scenarios [1]. Advanced C-V2X applications demand transmission latencies of a few milliseconds and 99.999% reliability [2]. As vehicular networks expand and radio resource management becomes increasingly complex, existing centralized resource allocation approaches in cellular networks struggle to meet diverse quality-of-service (QoS) requirements, particularly ultra-reliable and low-latency needs.\n The recent decade has witnessed a surge in publications unveiling the merits of artificial intelligence (AI) and deep learning (DL) methods, suggesting their potential to mitigate the aforementioned challenges of model-based approaches [3]. In particular, deep reinforcement learning (DRL) has been shown effective in solving hard-to-optimize non-convex combinatorial problems efficiently and has widely been adopted to deal with high-dimensional state-action spaces of vehicular communications applications [4]. Nevertheless, DRL-based resource allocation relies on complex deep neural networks (DNNs) with multiple layers, making them difficult to understand due to their black-box nature and lack of explainability [5]. This complexity obscures the influence of input state features on the model's output and the logical reasoning behind decisions. This lack of explainability hinders the adoption of DRL-based resource allocation in safety-critical vehicular applications, as the decision-making processes are not easily understandable."}, {"title": "A. Relevant works", "content": "Resource sharing in C-V2X applications requires a judicious allocation for mitigating interference and optimizing resource utilization for efficient vehicular communications. In centralized resource management in C-V2X, the BS collects the channel statistics to monitor the quality of each link and formulate the optimal resource allocation objective function [6]\u2013[9]. For instance, centralized graph theory-based proposals [6] and hierarchical optimization theory-based solutions [7] require executing iterative algorithms at the central controller with up-to-date global channel state information (CSI) availability. To alleviate the CSI acquisition overhead, low-complexity algorithms based on only large-scale fading information [8] or imperfect CSI assumptions [9] have been proposed. However, the V2X resource allocation problem is often modeled as a combinatorial optimization problem with nonlinear constraints, which are difficult to solve using traditional optimization algorithms in real time. Further, these works [6]-[9] consider Shannon rate-based communication, ignoring the reliability aspect of V2V links in terms of the decoding error probability, implicitly assuming infinite block-length availability, making them unsuitable for the low-latency, high-reliability requirements of V2X scenarios addressed in this study.\n Many existing studies have considered different DRL-based solutions for solving resource allocation problems in V2X"}, {"title": "B. Contributions", "content": "In this work, we present a distributed multi-agent deep reinforcement learning (MADRL) algorithm to jointly optimize transmit power and spectrum sub-band assignment, enhancing the performance of vehicle-to-network (V2N) and vehicle-to-vehicle (V2V) links in ultra-reliable and low-latency communications (URLLC)-enabled V2X communications network. Utilizing the SHAP method from XAI, we design a post-hoc and model-agnostic explainability pipeline that improves understanding of DRL agent inferences and simplifies the agent's inputs through feature selection based on importance rankings. The contributions of this paper are summarized as follows:\n\u2022 We formulate the joint optimization problem of spectrum and transmit power allocation with the objective of maximizing the sum-rate of V2N and V2V links under reliability, latency, and transmit power constraints in a URLLC-enabled vehicular network. We propose a MADRL algorithm with centralized learning and decentralized execution to solve the formulated optimization problem. Specifically, multiple DQNs are distributively executed at the V2V transmitters, whereas the weight parameters are centrally trained at the base station (BS) using a common reward function for all agents to ease implementation and improve stability.\n\u2022 We propose a novel two-stage systematic explainability framework leveraging feature relevance-oriented XAI to simplify the DRL agents. The former stage involves generating state feature importance ranking of the trained models using the SHAP-based importance scores. The latter stage exploits these importance-based rankings to simplify the state space of the agents, whereby the least important features are removed from the model's input, i.e., by masking feature values and observing the effect on the model's performance using an automated novel variable feature selection method.\n\u2022 We quantitatively validate the effectiveness of the proposed XAI-based methodology. Via extensive simulations, we demonstrate that our proposed methodology can simplify the model in terms of the average training time, the number of broadcast parameters, and the number of optimal state features required for training for different network configurations without significant performance loss."}, {"title": "C. Notations and Paper Organization", "content": "The boldface lowercase letter is used to represent a column vector. denotes the assignment operation. ||x|| denotes the length of x. || is the absolute function operator and f = g denotes that function f is equivalent to g. Key mathematical notations are summarized in Table I.\n The rest of the paper is organized as follows. Section II describes the vehicular URLLC system model and assumptions used in the paper. Section II-B presents the mathematical formulation of the joint transmit power and spectrum sub-band allocation in vehicular URLLC system. Section III provides a brief background on the design of the DRL framework and describes the proposed multi-agent DRL-based algorithm for the transmit power and spectrum sub-band allocation problem. Section IV explains the proposed systematic SHAP-based XAI methodology for generating the feature importance ranking and describes the proposed feature selection algorithm."}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "This section first provides the system model of the C-V2X communications network. Then, it formulates a resource allocation problem that aims to maximize the sum-rate for the V2N links as well as to ensure reliable information transmission for the V2V links."}, {"title": "A. System Model", "content": "We consider a single-cell C-V2X communications network covered by a Road-Side Unit (RSU) acting as a single-antenna BS. Each vehicle is equipped with a single-antenna On-Board Unit (OBU), supporting high data rate uplink services and reliable safety message sharing. C-V2X includes two operation modes to support diverse vehicular applications: V2N and V2V mode. V2N links support high data rate applications (e.g., video streaming, location tracking, map updates) via cellular interfaces using the Uu interface of 5G new radio (NR) [2]. V2V links transmit safety-related information through device-to-device (D2D) communications, requiring low latency and high reliability. We represent the sets of V2N and V2V links as N and K, with cardinalities N and K, respectively. Each V2N link occupies a unique sub-band with fixed transmission power. In cases where N > K, we introduce N - K virtual V2V connections. Our focus is on Mode 4 in the cellular V2X architecture, where vehicles autonomously select radio resources for V2V communications"}, {"title": "1) Channel Models:", "content": "Consider a slotted communication system with scheduling time slots indexed by t, where each slot has a duration of \u03c4. We consider a quasi-static block fading channel model where orthogonal frequency division multiplexing (OFDM) technology is utilized to group consecutive subcarriers to form a spectrum sub-band. The channel coherence time is given by Tc = $\\frac{9}{16\u03c0f_c}\\sqrt{\\frac{c}{v_s}}$, where fp = $f_c\\frac{v_s}{c}$ is the Doppler frequency, fc is the carrier frequency, vs is the vehicle velocity, and c is the speed of light [32]. T > Tc holds for moderate vehicular speeds, and the channel state information (CSI) can be regarded as constant throughout the slot duration. Then, the direct channel gain of the k-th V2V link over the n-th sub-band in coherence time slot t is expressed as $g_k^n[n] = \\alpha_k^n |h_k^n[n]|$, where $h_k^n[n] \u223c \\mathcal{CN}(0, 1)$ is a complex Gaussian variable representing the Rayleigh fading, and $\\alpha_k^n$ captures the large-scale fading effect, including path loss and shadowing, which is typically frequency-independent and changes slowly since it primarily depends on the locations of the transmitter and receiver of the V2V pair. The channel gain between the n-th V2N transmitter and the BS over the n-th sub-band at slot t is defined as $g_{n\\rightarrow B}^n[n] = \\alpha_{n\\rightarrow B}^n |h_{n\\rightarrow B}^n[n]|$, where subscript n \u2192 B denotes the n-th V2N link traversing the BS.\n In the case of n-th sub-band's reuse, the interfering channel gains between the i-th and the j-th V2V/V2N links are defined as $g_{ij}^n[n] = \\alpha_{ij}^n |h_{ij}^n[n]|$, where i \u2260 j, i \u2208 {k, n}, j \u2208 {k\u2032, B}, $\\alpha_{ij}^n$ represents the large-scale fading, and $h_{ij}^n[n]$ denotes the small-scale fading."}, {"title": "2) Achievable Rates:", "content": "The achievable data rate of the n-th V2N link at time slot t is expressed as\n$R_n^{\\text{V2N}}(t)[n] = B_w \\log_2(1 + \\gamma_n^{\\text{V2N}}(t)[n])$, (1)\nwhere $B_w$ indicates the channel bandwidth occupied by each V2N link and $\\gamma_n^{\\text{V2N}}(t)[n]$ is the received signal-to-interference-plus-noise ratio (SINR) of the n-th V2N link over the n-th sub-band at time slot t, which is defined as\n$\\gamma_n^{\\text{V2N}}(t)[n] = \\frac{P_n^{\\text{V2N}}(t) g_{n\\rightarrow B}^n[n]}{\\sigma^2 + \\sum_{k \\in \\mathcal{K}} \\eta_k^n[n] P_k^{\\text{V2V}}(t)g_{k\\rightarrow B}^n[n]}$, (2)\nwhere $P_n^{\\text{V2N}}(t)$ and $P_k^{\\text{V2V}}(t)[n]$ denote the transmit powers of the n-th V2N link and the k-th V2V link over the n-th sub-band, respectively; \u03c32 represents the variance of additive white Gaussian noise; and $\u03b7_k^n[n] \u2208 {0, 1}$ is the resource allocation indicator with $\u03b7_k^n[n] = 1$ implying the k-th V2V link uses the spectrum of the n-th V2N link and $\u03b7_k^n[n] = 0$ otherwise. We assume that each V2V link can only use one sub-band at the same time, i.e., $\\sum_{n \\in \\mathcal{N}}\\eta_k^n[n] = 1$, while the orthogonal sub-band of each V2N link can be reused by multiple V2V pairs.\n URLLC-enabled V2V communications are mainly responsible for the reliable dissemination of safety-critical messages using short code block lengths, rendering the classic Shannon capacity no longer appropriate to describe the maximum achievable data rate. In the context of finite block length regime, the maximum achievable information rate of the k-th V2V link at time slot t over the n-th sub-band, which can be decoded with block error probability no greater than \u03b5k is given by the normal approximation [33]\n$R_k^{\\text{V2V}}(t)[n] = B_w\\left[ C(\\gamma_k^{\\text{V2V}}(t)[n]) - \\sqrt{\\frac{V(\\gamma_k^{\\text{V2V}}(t)[n])}{m}}Q^{-1}(\\epsilon_k[n]) \\frac{1}{\\ln 2} \\right]$, (3)\nwhere $C(\\gamma_k^{\\text{V2V}}(t)[n]) = \\log_2(1 + \\gamma_k^{\\text{V2V}}(t)[n])$ is the Shannon rate, $V(\\gamma_k^{\\text{V2V}}(t)[n]) = \\frac{\\gamma_k^{\\text{V2V}}(t)[n]}{(1+\\gamma_k^{\\text{V2V}}(t)[n])^2}$ is the channel dispersion, m is the blocklength allocated to the k-th V2V link, $\u03b5_k[n] \u2208 (0, 1)$ is the decoding error probability at the k-th V2V receiver over n-th sub-band, $Q^{-1}(.)$ is the inverse Gaussian tail function $Q(x) = \\int_x^{\\infty} \\frac{1}{2\u03c0} e^{-t^2/2} dt$, and $\\gamma_k^{\\text{V2V}}(t)[n]$ is the SINR of the k-th V2V link over the n-th sub-band at time slot t defined as\n$\\gamma_k^{\\text{V2V}}(t)[n] = \\frac{P_k^{\\text{V2V}}(t)[n] g_k^k[n]}{\\sigma^2 + I_k^n[n]}$, (4)\nwhere $I_k^n[n] = \\sum_{n \\in \\mathcal{N}} P_n^{\\text{V2N}}(t) g_{n\\rightarrow k}^n[n] + \\sum_{k' \\neq k \\in \\mathcal{K}} \\eta_{k'}^n[n] P_{k'}^{\\text{V2V}}(t)[n]g_{k'\\rightarrow k}^n[n]$ is the collective interference at the k-th V2V receiver from the n-th V2N link and the k\u2032-th V2V links sharing the n-th sub-band. The code block length can be expressed by m = Bw\u0394T, where \u0394T represents the packet transmission latency defined as the time taken for an entire packet of L bits to be transmitted. In our work, packet transmission latency is fixed to 1 milliseconds to ensure the latency requirement defined in 3GPP TR 36.885 [34]. Then, for a given payload size of L bits per vehicle, the coding/data rate for the k-th V2V link, $r\u0302 = \\frac{L}{\u0394T B_w}$. By rearranging Eq. (3), the decoding error probability at the k-th V2V receiver can be expressed as\n$\\epsilon_k[n] = Q\\left( \\sqrt{\\frac{L}{\\ln 2} \\left(C(\\gamma_k^{\\text{V2V}}(t)[n])-\\frac{L}{T B_w} \\right)}\\right)$. (5)\nThen, based on the joint coding theory [35], the achievable"}, {"title": "B. Problem Formulation", "content": "To maximize the sum-rate for the V2N and V2V links while ensuring reliable information transmission for the V2V links, we jointly optimize the spectrum allocation $\u03b7_k^n[n]$ and the V2V transmission power $P_k^{\\text{V2V}}(t)[n]$, \u2200k \u2208 K and \u2200n \u2208 N. Specifically, the sum-rate performance metric measures the total data rate achieved by all V2V and V2N communication links in the network. This metric is crucial in V2X communications because it directly reflects the network's ability to efficiently utilize the available spectrum while supporting multiple simultaneous links. A higher sum-rate indicates better spectral efficiency for data-intensive applications and reliable transmission of safety-critical messages. Considering these QoS requirements in vehicular networks, the objective function of our work is to jointly optimize the sum rate for the V2N links and the V2V links. The dynamic multi-objective resource allocation problem at time slot t is formulated as\n$\\begin{aligned} &\\underset{\\substack{\\eta(t), p(t)}}{\\text{maximize }} w_1 \\sum_{n=1}^{N} R_n^{\\text{V2N}}(t) + w_2 \\sum_{k=1}^{K} R_k^{\\text{V2V}}(t),\\\\ &\\text{subject to } \\epsilon_k[n] \\leq \\epsilon_{\\text{max}, k} \\quad \\forall k \\in \\mathcal{K}, \\forall n \\in \\mathcal{N}  \\\\ &0 \\leq P_k^{\\text{V2V}}(t)[n] \\leq P_{\\text{max}} \\quad \\forall k \\in \\mathcal{K}, \\forall n \\in \\mathcal{N}  \\\\ &\\sum_{n \\in \\mathcal{N}} \\eta_k^n[n] = 1, \\quad \\forall k \\in \\mathcal{K} \\\\ &\\eta_k^n[n] \\in \\{0, 1\\}. \\end{aligned}$ (7)\nwhere w\u2081 and w\u2082 represent the weights to balance the sum-rate of V2N and V2V link, respectively; and $\\eta^{(t)} = [\u03b7_1^1[1], ..., \u03b7_k^n[n], ..., \u03b7_K^N[N]]$, $P^{(t)} = [P_1^{\\text{V2V}}[1], ..., P_k^{\\text{V2V}}[n], ..., P_K^{\\text{V2V}}[N]]$ are the vectors for the sub-band selection and the power allocations at time slot t, respectively. Then, in (7b), \u03b5max,k is the maximum error probability constraint for the k-th V2V link to ensure a predefined quality of service (QoS). (7c) is the maximum power constraint for the k-th V2V transmitter. (7d) indicates that each V2V link can only use one sub-band at a time.\n The optimization problem (7) is a non-convex mixed integer nonlinear programming (MINLP) problem and involves sequential decision-making over the time slots. Due to the dynamic nature of the C-V2X networks and the associated short coherence time interval, centralized solutions are inadequate due to difficulties in acquiring the instantaneous CSI of all links [36]. To address these issues, we propose to exploit DRL to solve the problem (7) in a decentralized fashion. Specifically, the sequential decision-making over time can be encapsulated within a Markov decision process (MDP) framework, and solved using a DRL-based strategy by treating each V2V transmitter as an agent interacting with the unknown environment to maximize the reward. In what follows, we transform the problem of joint spectrum and power allocation into a multi-agent DRL problem."}, {"title": "III. MULTI-AGENT REINFORCEMENT LEARNING-BASED SOLUTION", "content": "In this section, we design a DRL-based algorithm to solve the resource allocation problem in (7). To build the foundations for the proposed DRL-based strategy, the description of key components of the RL framework in terms of the state space, action space, and reward function is first provided, followed by the proposed MADRL algorithm."}, {"title": "A. Problem Transformation into DRL Framework", "content": "DRL effectively addresses complex decision-making problems by training an agent to interact with its environment and learn optimal behaviors over time through continuous feedback and adjustment. The agent refines its actions to achieve optimal outcomes within a MDP framework [37]. An MDP is encapsulated by a tuple (S, A, P, R), where S is a set of states, A is a set of actions, P is the transition probability from states to actions, and R is the reward for taking action in a state. At each discrete time step t, the agent observes the state s(t), takes action a(t) according to policy \u03c0(s(t), a(t)), transitions to state s(t+1), and receives a reward r(t). The DRL-based system design aims to learn an optimal policy that maximizes the expected return [38], [39]. Therefore, we cast the optimization problem (7) as an MDP by mapping the key elements from the DRL framework to the resource allocation problem.\n In the multi-agent scenario, each V2V transmitter acts as an agent and interacts with the unknown communication environment. The joint action A(t) = (a1t, . . . , akt, . . . , aKt), with akt as the k-th V2V agent\u2019s action in the time slot t, collectively influences the common environment and directs the agents towards the optimal policy. Hence, in our MADRL framework, different agents compete for limited power and spectrum resources, and the resource-sharing problem is shifted to a fully cooperative one by providing all agents with the same common reward. Next, we introduce the key components of the DRL framework in detail in terms of the state space, action space, and reward function."}, {"title": "1) State Space:", "content": "The state of the k-th V2V agent at time slot t, s(t), includes the channel information and the received interference from other links. Specifically, the channel information for the k-th V2V agent includes the instantaneous channel gain of its own link gkk[n], \u2200n \u2208 N, the interference channel gain from the transmitter of the n-th V2N link and the k\u2032-th V2V link, gkn[n] and gkk\u2032[n], \u2200n \u2208 N, (k\u2032 \u2260 k), and the interference channel gain from its transmitter to the BS gkB[n], \u2200n \u2208 N. The channel information gkk[n], gkn[n], gkk\u2032[n], and gkB[n], can be accurately estimated by the receiver of the k-th V2V link at the beginning of each scheduling slot, and we assume it is also available instantaneously at the transmitter through delay-free feedback. The CSI for gkB[n] is estimated at the BS and broadcast to all vehicles within the BS coverage at the beginning of each scheduling slot [11], [38].\n Additionally, the aggregate interference from the V2N links and the V2V links over the n-th sub-band in the past time slot"}, {"title": "2) Action Space:", "content": "The action of each agent at time slot t corresponds to the spectrum sub-band and transmission power selection. While N disjoint sub-bands are preoccupied by the N V2N links and all V2V links share these sub-bands, each V2V agent can only pick at most one spectrum sub-band during the same time slot ($\\sum_{n \\in \\mathcal{N}} \u03b7_k^n[n] = 1, \u2200k \u2208 \\mathcal{K}$). The power allocation is discretized in Lo levels given by $P = \\{\\frac{P_{\\text{max}}}{L_o}, \\frac{2P_{\\text{max}}}{L_o},..., P_{\\text{max}}\\}$, with P representing the power action set. Then, the action executed by the k-th V2V agent in the time slot t can be expressed as $a_k(t) = [\u03b7_k^n[n], P_k^{\\text{V2V}}(t)[n]]$, which includes $\u03b7_k^n[n]$ and $P_k^{\\text{V2V}}(t)[n]$ for spectrum sub-band and power action decision, respectively. Accordingly, each agent has an action vector with the dimension of Lo \u00d7 N."}, {"title": "3) Reward Function:", "content": "A common reward is designed for all agents to encourage cooperation among the multiple V2V agents. In order to reflect the performance of the decision taken by the agents, the immediate reward function is formulated as\n$r^{(t)}=\\lambda_1 \\sum_{n=1}^{N} R_n^{\\text{V2N}}(t) + \\lambda_2 \\sum_{k=1}^{K} T_k^{N} -\\lambda_3 \\sum_{k=1}^{K} (max (\\epsilon_k [n]) - \\epsilon_{\\text{max},k}, 0))$, (9)\nwhere the summation terms respectively represent the sum-rate of V2N links, the sum-rate of V2V links, and the reliability requirements of the V2V links; and \u03bbi, i \u2208 {1,2,3}, are the positive weights to balance the utility and the penalty cost in terms of constraint violations. To align the designed reward function at each step with the network objective (7a), the first two terms in (9) are included as positive reward elements. Additionally, the reward function incorporates a penalty for failing to satisfy the reliability constraint related to the decoding error probability. The selection of weight coefficients is critical, as they significantly affect the learning efficiency and convergence of the algorithm. In practical scenarios, these coefficients, denoted as \u03bbi, are considered hyperparameters and require empirical tuning. Setting equal positive weights for all reward components prioritizes maximizing the sum rate of V2V and V2N links. However, our tuning experience suggests such a design will impede the algorithm\u2019s convergence as the agents struggle to learn reliability aspects. This is because the penalty term, based on target error probabilities, is much smaller than other reward components. The reward function components are normalized by setting $\\delta_1 = \\frac{\\lambda_1}{R^{\\prime}}$, $\\delta_2 = \\frac{\\lambda_2}{R^{\\prime}}$, where $R^{\\prime} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} C(\\gamma_k^{\\text{V2V}}(t)[n])$ is the sum V2V Shannon rate, and $\\delta_3 = \\lambda_3$. This hyperparameter selection approach balances the contribution of utility and constraint violation cost while enabling stable updates of the Q-values for improved convergence."}, {"title": "B. Proposed Multi-Agent DRL-based Algorithm", "content": "We propose an MADRL scheme with each V2V transmitter as an agent. The multi-agent scenario breaks the non-stationarity assumption of the MDP as the environment may not be stationary from the agent\u2019s perspective as other learning agents update their policies. To effectively handle the non-stationarity issue in multi-agent settings, we propose the centralized training and decentralized implementation approach."}, {"title": "IV. XAI Guided Feature Selection and Model Simplification", "content": "In this section, we introduce our XAI-based approach to simplify the trained DRL models\u2019 complexity by quantifying the relevance of input state features to the output actions for our proposed DRL-based V2X communication system. A novel post-hoc and model-agnostic framework is proposed to explain and reduce the complexity of the trained networks by eliminating some of the input variables using the state feature importance ranking.\n As the basis of our methodology, we leverage the SHAP [40], a unified approach that offers a strong theoretically grounded framework for computing local and global explanations of a machine learning model. In what follows, we first introduce Shapley values, followed by a brief description"}, {"title": "A. Preliminaries on Shapley Values and SHAP", "content": "Feature importance rankings necessitate the computation of Shapley values which can be interpreted as the average expected marginal contribution of each input feature xi \u2208 x to a model\u2019s prediction f(x), where x is the vector of original feature values. The Shapley value for xi is computed by considering all possible coalitions of features and calculating the weighted average difference in predictions with and without the feature xi. For a prediction model f(\u00b7) with input x, the Shapley value for feature xi is given by:\n$\\phi_i(f,x) = \\sum_{z\u2019 \\subseteq x\\prime \\setminus \\{x_i\\}} \\frac{|z\u2019|!(L - |z\u2019| - 1)!}{L!}[f(z\u2019) \u2212 f(z\u2019 \\cup \\{x_i\\})]$, (11)\nwhere L is the total number of original features, z\u2032 is the subset of features used in the model, f(z\u2032) is the model\u2019s prediction on feature subset z\u2032, $f(z\u2032 \\cup \\{x_i\\})$ is the model\u2019s prediction on feature subset z\u2032 excluding feature xi. When calculating f(z\u2032), the xi-th feature is masked out and then simulated by drawing random instances of the xi-th feature from the background dataset. The Shapley value \u03a6i(f, x) signifies the impact of feature xi on transitioning from the reference output value, i.e., the expected value without knowledge of the xi-th feature values to the actual output provided by the prediction model. These contributions carry both magnitude and sign, allowing to assess a feature\u2019s significance. However, exactly calculating Shapley values requires searching through all possible 2L feature combinations while retraining models, which is computationally prohibitive. As a remedy, SHAP provides a computationally efficient way to calculate Shapley values by iterating over a small subset of possible feature permutations using sampling approximations [40]. It combines previously proposed explainability methods, such as LIME [22] and Deep Learning Important Features (DeepLIFT) [41], to approximate the Shapley values. As an additive feature attribution method, SHAP approximates the output f(x) of the original complex model by summing the scores attributed to each feature, \u03c6i, as follows:\n$g(z\u2019) = \\phi_0 + \\sum_{l=1}^{L} \\phi_l z_l\u2019$, (12)\nwhere g(z\u2032) \u2248 f(x) is the explanation model, and \u03c6o corresponds to the average or expected output of the model seen during training that can be interpreted as the initial output of the model before the impact of any features is considered. Moreover, Deep Shapley additive explanation (Deep-SHAP) is an extended version designed specifically for DNNs and adapts DeepLIFT algorithm to approximate Shapley values by linearizing the non-linear components of DNNs [40]. Deep-SHAP adds explainability to the DNNs by decomposing the output prediction of DNNs on a specific input by backpropagating the contributions of all neurons to every input feature. Instead of using a single reference value, Deep-SHAP exploits a distribution of background samples such that the resulting Shapley values sum up to the difference between the expected model output on the passed background samples and the current model output."}, {"title": "B. Proposed Systematic XAI Methodology", "content": "The systematic XAI methodology offers two key benefits: First, it simplifies the input size by eliminating non-influential features through an iterative SHAP-based feature selection algorithm. Second, it reduces the DNN model\u2019s complexity by adjusting the architecture based on the refined input size, thereby lowering computational load and minimizing parameter updates for the distributed implementation. Our methodology can be divided into two stages. The first stage involves generating state feature importance ranking of the trained models using the Deep-SHAP explainer. The second stage uses importance-based rankings to simplify the state space of the agents using a novel variable feature selection method.\n For notation consistency, we denote the k-th trained V2V agent as the prediction model, i.e., $f_k(x) = Q_{agent}^k(x)$, which takes real-valued state feature vector x = [x1, . . . , xL] as an input, where L = (K + 2) \u00d7 N is the number of input neurons. The prediction model fk(x) outputs the Q-value of each possible action denoted by qk = [qk,1, . . . , qk,m, . . . , qk,M ], where M = Lo \u00d7 N is the number of output layer neurons and equal to the cardinality of the action set described in Section III-A. The trained model uses argmax operator to select the action maximizing the model\u2019s output.\n In the first stage of generating state feature importance ranking using the trained agents, we instantiate a DeepExplainer from the SHAP library [21] and pass the trained agent\u2019s DQNS along with the background dataset to it. The background dataset XBG is generated during the training process and serves as a prior expectation for the sample instances to be explained [42]. To compute the SHAP_values, a hold-out dataset X is used, which contains the state features and a copy of all possible actions, i.e., Q-values. This dataset is generated by evaluating the performance of the well-trained agents over a set of test episodes. Deep-SHAP produces local explanations in the form of importance score or \"SHAP_values,\" \u03a6k(x) = [\u03c6k,1, . . . , \u03c6k,L], where \u03c6k,l is the SHAP_value associated with feature xl \u2208 x and probable action qk,m of the k-th prediction model. To obtain global explanations, SHAP_values are obtained by repeating this across the entire database, resulting in a SHAP matrix for each sample instance in X. The process for calculating the SHAP_values for the prediction model fk(x) using Deep-SHAP is delineated in Fig. 3, wherein Deep-SHAP first computes the SHAP matrix Sk for state sample x \u2208 X with one row for each action value qk,m and one column per state feature xl \u2208 x. Then, the SHAP_values corresponding to the maximizing action index are selected from Sk. Afterward, the absolute mean of the feature column of Sk is calculated across all data instances in X. The resulting vector of mean absolute SHAP_values |\u03a6k(x)| = [[\u03c6k,1| , . . . , |\u03c6k,L|] is sorted in descending order. The first position of the resulting vector contains the most important feature, the second position contains the second most important, and so on. To reveal the meaning behind the SHAP_values in terms of their contribution to"}, {"title": "C. Post-hoc SHAP-based Input State Selection Algorithm", "content": "We devise an iterative state feature selection algorithm based on SHAP-based state feature importance rankings. The post-hoc SHAP-based state feature selection strategy is summarized in Algorithm 2, whose inputs are the K trained agents, hold-out dataset X"}]}