{"title": "Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness", "authors": ["Srija Mukhopadhyay", "Adnan Qidwai", "Aparna Garimella", "Pritika Ramu", "Vivek Gupta", "Dan Roth"], "abstract": "Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally, we identify areas for improvement and propose future research directions to build more robust and reliable CQA systems. This study sheds light on the limitations of current models and paves the way for future advancements in the field.", "sections": [{"title": "1 Introduction", "content": "Chart question answering (CQA) (Masry et al., 2022; Chaudhry et al., 2020) has emerged as a critical area within the field of Visual Language Understanding (VLU) (Lee et al., 2023; Ghosh et al., 2024), aiming to equip machines with the ability to comprehend and answer questions based on data visualizations. While recent advancements in Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have yielded impressive performance improvements in CQA (Liu et al., 2023b; Masry et al., 2023; Xia et al., 2024; Xu et al., 2024; Team et al., 2023; Achiam et al., 2023; Meng et al., 2024), their true capabilities remain obscure in uncertainty. This paper delves into an insightful analysis of the robustness and consistency of state-of-the-art CQA models, exposing their limitations and guiding future research directions.\nWe address several key questions regarding the current state of CQA: Are existing models truly effective, or do their impressive average scores mask significant weaknesses? How do models perform on specific aspects of chart understanding, such as question complexity and chart type?\nFurthermore, is the robustness of these models, their ability to generalize across diverse variations, adequately explored? Exploring the effect of these variations could provide deeper insights into the data and enhance the comprehensibility of the visualizations for models.\nTo answer these questions, we present a rigorous evaluation of leading CQA models on a meticulously curated dataset. This dataset encompasses diverse chart types and question categories, allowing for a thorough assessment of model performance across varying levels of complexity. We examine how well the models generalize across diverse visual representations of identical data, assessing their robustness against perturbations. Our findings reveal significant performance discrepancies, particularly when transitioning from simple to complex chart-question combinations. Moreover, we demonstrate that even the highest-performing models exhibit a substantial drop in accuracy when subjected to diverse perturbations, highlighting the critical need for improved robustness in CQA. This paper makes the following contributions:\n\u2022 Providing a thorough analysis of the strengths and weaknesses of current VLMs and MLLMs for chart understanding.\n\u2022 Introducing a new evaluation set with fine-grained splits across chart types and question complexities, facilitating a deeper understanding of model performance.\n\u2022 Performing a detailed robustness analysis to uncover the shortcomings of current models, emphasizing the necessity for additional research in this domain.\nOur research sheds light on the current state of CQA, offering crucial insights."}, {"title": "2 Initial Dataset", "content": "This section highlights the dataset preparation process employed to analyze the performance of CQA models across a spectrum of chart types and question complexities."}, {"title": "2.1 Dataset Selection", "content": "To ensure a comprehensive evaluation of CQA models, we selected the ChartQA dataset (Masry et al., 2022) as our primary benchmark. This dataset is widely used in CQA benchmarking, covering diverse domains from sources like Our World in Data, Statista, OECD, and Pew Research."}, {"title": "2.2 Chart and Question Labelling", "content": "To facilitate a more granular analysis of model performance, we categorized both charts and questions according to their complexity levels. This categorization was applied to the entire ChartQA test set, resulting in a modified evaluation dataset tailored for our experiments.\nChart Categorization. Charts were classified into two categories using a code-based approach.\nSimple Charts: These charts represent a single entity over a dataframe with two columns, exhibiting no overlaps or complex visual elements.\nComplex Charts: These charts feature more than two columns, often encompassing multiple entities, leading to increased visual complexity.\nQuestion Categorization. Human annotators cleaned and categorized the questions from the ChartQA dataset into two categories based on their complexity:\nSimple Questions: These questions primarily focus on data extraction, and typically involve a single step of reasoning.\nComplex Questions: These questions require multi-step reasoning and data extraction, often involving comparisons and logical inferences.\nWe introduced these categorizations while preserving the existing division of question generation types (human-generated and augmented questions), resulting in eight categories."}, {"title": "3 Experiments", "content": "To rigorously assess the performance of CQA models, we selected a diverse range of state-of-the-art models, varying in architecture, size, and training setup. All models were evaluated using a zero-shot Chain-of-Thought (Wei et al., 2022) prompting approach, with prompts tailored for each model to maximize performance. Importantly, no additional reasoning aids were provided to any of the models. For the sake of clarity and analysis, we grouped the models into three broad categories:\nChart-based VLMs. This category contains open-source VLMs specifically adapted for chart reasoning. MatCha (282M) (Liu et al., 2023b) is a transformer based model which enhances the capabilities of Pix2Struct (Lee et al., 2023) models through pre-training on mathematical reasoning and chart derendering tasks. UniChart (201M) (Masry et al., 2023) is another similar model which achieves chart understanding by leveraging pre-training on tasks such as data table generation, numerical and visual reasoning, and open-ended question answering. DePlot (282M) (Liu et al., 2023a) is a model which specializes on extracting tabular data from a given chart. The extracted table is subsequently passed to a Language Model (LM), e.g. Flan UL2 (20B) (Tay et al., 2022), for reasoning via Chain-of-Thought prompting (Wei et al., 2022).\nGeneralist VLMs. This category comprises open-source VLMs trained on general visual comprehension tasks. Notably, these models were not specifically trained or adapted for chart reasoning. QwenVL (Bai et al., 2023b) is a generalist 7-billion-parameter VLM built on top of Qwen-LM (Bai et al., 2023a) through the integration of visual encoders and the use of general and multi-task pre-training. CogAgent VQA (Hong et al., 2024) is an 18-billion-parameter VLM specializing in Graphical User Interface (GUI) understanding and navigation. InternLM-XComposer2 (8B) (Dong et al., 2024) is an adaptation of InternLM2-7B (Cai et al., 2024), excelling in producing high-quality long-text multi-modal content and reasoning within visual-language understanding contexts.\nLarge MLLMs. This category features state-of-the-art closed-source Multimodal Large Language Models (MLLMs) pre-trained on extensive visual and language data. For this category, we utilized Gemini 1.5 Flash (Team et al., 2023), and GPT-4O (Achiam et al., 2023), renowned for their capabilities in reasoning and visual understanding.\nEvaluation\nTo improve on the Relaxed Accuracy metric, we introduce a new evaluation metric that includes extra checks for precise answer matching. This metric, similar to Relaxed Accuracy, provides a 5% leverage for numerical answer matching. However, it includes the following checks:\n\u2022 Alphanumeric String Matching: Removing comma and spaces from the during answer matching to ensure an exact alphanumeric string comparison.\n\u2022 Strict Year Matching: For questions specifically asking for a \"Year\" as an answer, the 5% relaxation is disabled, forcing a strict string match. This ensures that the model accurately identifies the correct year.\n\u2022 Unordered Exact List Matching: For questions requiring multiple answers, an unordered exact list matching is applied, which ensure model correctly identifies all the elements in answer list, regardless of their order.\nTo validate the accuracy of our proposed evaluation metric, we manually verified the answers obtained using this metric.\nSmaller VLMs. Smaller models (QwenVL, CogAgent, InternLM) struggled to produce answers in the correct format. We addressed this by employing an \"LLM as an Extractor\" approach, using Gemini 1.5 Flash to extract answers from their outputs. Manual verification of 150 samples confirmed that Gemini primarily acted as a formatting tool, preserving the original model's answer in 149 cases and performing rounding in the one remaining instance. This demonstrates Gemini's effectiveness in enhancing the usability of smaller models without significantly altering their intent."}, {"title": "4 Can VLMs reasons consistently?", "content": "This section presents our findings and analysis on the performance of various chart question answering (CQA) models across different chart types and question complexities."}, {"title": "4.1 Results and Discussion", "content": "Table 2 gives an overview of all results obtained for this section.\n(Q1) Does any model excel across all categories? While no single model dominates all categories, GPT-40 and Gemini 1.5 Flash consistently demonstrate impressive performance, with GPT-40 leading in most cases. Among open-source models, InternLM stands out as the top performer.\nInterestingly, models specifically trained on chart tasks (MatCha, UniChart) excel on augmented questions. This likely stems from their exposure to similar question formats during training. This is particularly evident in simple questions from the augmented set, where MatCha achieves a high accuracy of 91.40%, followed by UniChart at 87.20%. However, they struggle significantly with reasoning-based questions, achieving as low as 25% accuracy for complex chart and complex question pairs, highlighting the need for enhancement in the reasoning abilities of such models.\n(Q2) How do models perform across various chart types? Across all models, a consistent trend emerged: performance was consistently better on simple charts compared to complex charts, regardless of the question type. This behavior is likely attributable to the inherent difficulty in understanding and extracting values from complex charts. Factors like overlapping data points and complex color resolution contributes to challenges in data extraction, increasing the difficulty of reasoning on such charts.\n(Q3) How do models perform across various question types? For the same chart type, models consistently perform better on simple questions compared to complex questions. This significant difference in scores highlights the limitations of certain models in fine-grained data extraction and reasoning. GPT-40 and Gemini 1.5 Flash exhibit the smallest decrease in scores, indicating strong data extraction and reasoning capabilities. Smaller models, particularly those specifically trained on charts, struggle with questions requiring mathematical reasoning, despite their competence in basic data extraction.\n(Q4) Do models struggle more with complex charts or complex questions? To assess model capabilities, we compared performance on two categories: \"Simple Charts, Complex Questions\" and \"Complex Charts, Simple Questions.\" This analysis reveals whether a model excels at visual data extraction (complex charts) or reasoning (complex questions).\nOur results show that LLMs like GPT-4 demonstrate strong reasoning skills, excelling on complex questions even with simple charts. Conversely, Gemini 1.5 Flash performs consistently across both categories. Generalist and chart-based VLMs tend to favor complex charts over complex questions, suggesting limitations in complex reasoning. This insight allows for targeted model fine-tuning to enhance specific domains where they lack dexterity.\n(Q5) Are there charts and questions where all models consistently fail to answer accurately? We focused on identifying patterns of model failure across all categories. Given below are a few recurring difficulties for models:\n- Charts containing similar colours: Models struggled with charts which required discrimination between slightly different colors. The issue extends further to recognizing specific colors by their names accurately.\nTight pie charts: In some instances, models incorrectly assigned labels to categories in pie charts with narrow slices. Thus, failing to identify the correct association.\n- Charts containing summary statistics: Models failed to interpret such charts, recalculating metrics like mean or sum even though these values were explicitly provided within the chart itself.\n- Questions involving counting: Models consistently struggled to accurately count objects when the number exceeded ten.\n(Q6) How well do the models attend to the provided image for reasoning? To investigate the extent to which models rely on visual information versus their internal knowledge base, we conducted an experiment using blank images and irrelevant charts. We sampled 100 questions from each category and tested the top-performing models on their reasoning skills.\nSurprisingly, even when presented with irrelevant or blank images, some models successfully answered the questions, indicating a reliance on their pre-existing knowledge. This observation suggests potential leaks in testing data, as models even provided factually incorrect answers, highlighting the need for masked evaluation sets for visual reasoning tasks."}, {"title": "5 Are VLMs robust on CQA?", "content": "Another crucial aspect of our analysis involves investigating the robustness and consistency of these models across different visual representations of the same underlying data. Through the help of this probing, we aim to understand if model performance remains stable when presented with variations in chart types, styles, or aesthetics while conveying the same information.\nExamining these variations can offer deeper insights into the data and improve the clarity of the visualizations."}, {"title": "5.1 Our RobustCQA Dataset", "content": "Following the initial dataset preparation, a perturbation dataset was created to rigorously assess the robustness of the top-performing models across diverse chart variations. We refer to this dataset as the RobustCQA dataset, which systematically manipulates various chart elements while preserving the underlying data.\nCreation We identified 75 unique perturbation types for both simple and complex charts. These perturbations cover a broad spectrum of visual variations, including:\n\u2022 Color Scheme Changes: Modifying color palettes, gradients and hues.\n\u2022 Chart Type Variations: Experimenting with line plots, bar plots, stair plots, stem plots and other less commonly used chart types.\n\u2022 Legend and Axis Modification: Altering label position, formatting, and positioning of legend and axis elements.\nA detailed section showcasing all perturbation types has been presented in the Appendix.\nThe perturbed charts were generated using Matplotlib, ensuring that only the perturbed element changed while maintaining consistency in all other chart elements. The tables from the ChartQA dataset served as the source for the underlying data.\nHuman Verification To ensure the quality and relevance of our dataset, a rigorous manual annotation process was employed. Expert evaluators meticulously verified each perturbed chart, assessing its comprehensibility and answerability by humans. They also evaluated the relevance of each perturbation to the specific chart type, refining the perturbation set to include only meaningful variations. The underlying tables were also thoroughly verified to confirm that the generated questions remained answerable based on the chart data. This comprehensive evaluation was facilitated by a custom-built annotation platform, specifically designed to streamline the manual annotation process and ensure high-quality data.\nFinal Dataset. The finalized perturbations were then grouped into related categories to create the final dataset. This set comprises 22 unique perturbation categories for simple charts and 25 such categories for complex charts, encompassing a wide range of visual variations."}, {"title": "5.2 Methodology", "content": "To delve deeper into the performance and limitations of leading chart question answering models, we evaluated Qwen-VL, CogAgent-VQA, InternLM-XComposer2 (open-source VLMs) and Gemini 1.5 Flash, GPT-40 (closed-source MLLMs) using our RobustCQA dataset. We employed a similar evaluation metric as before, leveraging an LLM extractor for smaller models to ensure consistent output format, and analyzed all models through Zero-Shot Chain-of-Thought prompting."}, {"title": "5.3 Results and Discussion", "content": "The results obtained for perturbations on complex charts have been highlighted in table 4. For simple charts, a similar table has been presented in the Appendix.\n(Q1) Does model performance stay consistent with perturbed charts? The results reveal a significant performance degradation for most models when confronted with perturbations. While performance generally decreases across all models, some exhibit more drastic drops.\nAmong open-source models, InternLM-XComposer2 proves most resilient, demonstrating consistency across various perturbations. However, CogAgent-VQA and Qwen-VL struggle significantly with most perturbations, exhibiting low accuracy. Surprisingly, GPT-40, a closed-source model, displays relatively low accuracy with most perturbations, highlighting a potential lack of robust data extraction skills, particularly with non-annotated charts. In contrast, Gemini 1.5 Flash demonstrates notable consistency with minimal variations across perturbations, showcasing strong reasoning and data extraction capabilities.\nManual analysis of model performances suggests that Gemini 1.5 Flash's success stems from its approximation skills, allowing it to accurately estimate values even in non-annotated charts where other models struggle. This highlights the importance of improving data extraction for non-annotated charts to enhance model robustness in chart-based tasks.\n(Q2) Are there certain perturbations which help enhance the model performance? Our experiments highlighted several perturbations that unexpectedly improved model performance. Notably, across all models, annotated data points consistently boosted accuracy. While the most beneficial plot type varied across models and question/chart categories, annotated bar graphs emerged as a consistently positive influence.\nFurthermore, the addition of a grid and altering tick orientation also yielded significant performance improvements. Grids provide models with precise reference points for data estimation, while clear tick labels enhance accurate data point interpretation. Other beneficial perturbations included replacing legends with labels on lines and adjusting legend positioning. Replacing legends reduces the complexity of color resolution, which often leads to accuracy drops.\nOptimal legend placement ensures that labels do not obscure crucial data points. Initial experimentation also indicated that increasing font size, particularly for smaller models, often led to improved performance.\n(Q3) Are there perturbations which are always detrimental to model performance? Our analysis reveals that while models demonstrate promising performance on standard chart datasets, they struggle with robustness when faced with visual perturbations. While annotations generally improve performance, most other perturbations negatively impact model accuracy.\nLogarithmic scales pose the most significant challenge, likely due to the inherent difficulty in data retrieval even for humans. Models also struggle significantly with horizontal chart variations, particularly horizontal stacked charts, though they perform better with vertical stacked charts.\nIt should however be noted that models do also struggle with normal stacked plots as well as area plots. This observation can be attributed to the complex process of accurate data extraction for these plots, requiring additional mathematical reasoning to get the precise value of a point which is challenging for humans as well.\nStair plots, where models struggle to identify the precise data point to refer to, and changes in chart scale, which seem to disrupt model attention, also contribute to performance decline.\nThese findings emphasize the need to develop more robust models that can effectively interpret visual information beyond simple visual cues.\n(Q4) Are there certain perturbations which are more effective for certain question types? Our analysis suggests that certain chart types might be more effective than others for different question types. For instance, line charts excel at revealing trends and correlation. Stacked bar charts are less suitable for almost any question, unless it explicitly asks for data aggregation, as extracting individual data points from stacked bars can be challenging. Bar charts, while useful for comparing individual values within a group, prove to not be good for showcasing correlations across different groups or entities. These observations highlight the importance of selecting correct chart types for accurate and efficient question answering, particularly in domain-specific applications.\n(Q5) Does the effect of each perturbation type vary across models? The impact of each perturbation on model performance exhibits significant variation. While question and chart type play a role, for a given model, certain perturbations consistently prove more helpful or harmful. This nuanced effect is detailed in Tables , allowing us to identify specific areas for model enhancement, ultimately leading to a deeper understanding and extraction of insights from chart data.\nAnnotations, for both bar and point, consistently ranked among the top performing perturbations across most models. Secondly, chart elements in random colors surprisingly proved to be beneficial, indicating that models are capable of effectively resolving visual information when provided with colours far apart from each other. Furthermore, replacing legends with element names placed alongside or within the chart resulted in improved performance compared to traditional legend-based techniques. On the other hand, we observed that stacked chart elements, particularly horizontally stacked bars, significantly hindered model performance because of tougher data extraction. Similarly, logarithmic scales, known to be challenging for human interpretation, also negatively impacted model accuracy. This analysis provides valuable information for targeted model fine-tuning, addressing specific weaknesses and improving overall performance."}, {"title": "6 Related Work", "content": "Chart datasets Chart comprehension and question answering (CQA) (Hoque et al., 2022) are highly important domains with a substantial body of research. While existing CQA datasets have advanced reasoning capabilities over charts, most suffer from limitations in terms of size (Kafle et al., 2018), template-based questions (Methani et al., 2020; Chaudhry et al., 2020), synthetically generated charts (Methani et al., 2020; Han et al., 2023; Chaudhry et al., 2020) or being limited to a specific domain (Methani et al., 2020; Ahmed et al., 2023; Li and Tajbakhsh, 2023), or have only open domain question answers (Kantharaj et al., 2022). Additionally, even the current dataset used for state of the art bench-marking, Chart QA (Masry et al., 2022) has limitations in the form of not having classifications for a more meaningful analysis. These datasets also have limited variations in the kinds of charts used.\nAdditionally, ChartX (Xia et al., 2024) incorporates different chart types in the analysis performed, while ChartBench (Xu et al., 2024) and MMC (Liu et al., 2024) focuses on creating a large scale dataset with varied chart types.\nFinally, a very recent work, CharXiv (Wang et al., 2024) provides a extensive and comprehensive evaluations on a range of charts as well as questions ranging from those requiring reasoning to descriptive ones. They also perform ablation analysis by modifying charts and questions.\nHowever, to the best of our knowledge, RobustCQA is the first dataset to systematically perturb all elements within a chart, enabling a fine-grained analysis of factors affecting model performance. Additionally, we do fine grained analysis on the Chart QA dataset as well, based on question and chart complexity, which has not been done prior to this.\nModeling approaches for charts Various approaches have been developed for chart modeling. This includes models primarily made for chart comprehension and reasoning, constructed with the end to end goal of reasoning on charts (Liu et al., 2023b; Masry et al., 2023; Singh and Shekhar, 2020) as well as models which focus on converting the chart to an intermediate table format (Liu et al., 2023a) followed by reasoning by a generalized LLM through Chain of Thought (Wei et al., 2022) or Program of Thought (Chen et al., 2023) prompting."}, {"title": "7 Conclusion", "content": "This research introduces ChartQA-Split and RobustCQA, the first datasets dedicated to understanding model consistency across complexities and robustness to visual perturbations in chart question answering. Our evaluation of SOTA models, including baselines and VLMs/MLLMs, using a zero-shot chain-of-thought setting, reveals significant challenges in both areas. We perform an in-depth analysis of model weaknesses and identify key areas for improvement, such as enhancing data extraction for non-annotated charts and developing models that can effectively interpret complex visual information, taking every possible visual cue into consideration. Our work provides a foundation for future research in developing more robust and reliable chart question answering systems.\nFuture Directions. Perturbation analysis provides a nuanced understanding of model performance by revealing both universal and model-specific vulnerabilities and strengths. This insight drives targeted improvements: Model Pretraining focusing on perturbations that affect models allows for effective fine-tuning to address weaknesses. Perturbation-Aware Training Integrating specific perturbations during training enhances overall robustness, helping models develop resilience against challenges. Interpretable Models Understanding the impact of perturbations aids in debugging and explainability, fostering the development of reliable and transparent chart understanding systems.\nLimitations\nThe presented work exhibits several limitations. First, our data was obtained from a singular dataset, and we used only one plotting software for testing the perturbations. Expanding the dataset to include diverse sources and exploring various plotting libraries would strengthen the findings and improve generalizability. Second, the dataset is limited to English, while models are developed and evaluated on a wide variety of languages. Future research is required to expand the domain beyond English. Third, we were not able to cover a few chart types in the course of our analysis in order to make a more generalized perturbation set."}, {"title": "Ethics Statement", "content": "This research adheres to the ACL code of ethics, acknowledging and addressing potential ethical implications. While LLMs assisted in writing and presentation, all ideas and conclusions are solely attributed to the authors. The research promotes responsible and fair use of methodologies, ensuring transparency and reproducibility. We plan to release all scripts, resources, comprehensive documentation, evaluation metrics, datasets, model specifications, and prompting methods to enable others to build upon our work. We strive to present our findings clearly and accurately, avoiding exaggerated claims or misinterpretations."}, {"title": "Effect of Font-size on Models", "content": "Table 7 demonstrates the significant impact of font size on model performance. Increasing font size has a positive effect on the OCR capabilities of visual language models (VLMs). This finding suggests that increasing font size can be a beneficial preprocessing step for improving model performance in tasks involving chart comprehension through such models."}, {"title": "Different chart perturbations", "content": "This section details the construction and structure of the RobustCQA dataset.\nInitially, we generated an extensive set of perturbed chart images, creating 85 unique perturbations applied to both simple and complex chart types. This process ensured that every chart element, including those unique to specific chart types, was isolated and perturbed. For example, we perturbed markers for scatter and line plots and varied line styles for line, stem, and step plots.\nFollowing the initial generation, we performed a rigorous analysis of the perturbations, manually categorizing them into distinct groups based on their visual characteristics and impact on chart interpretation. This categorization allowed us to identify and retain only the most relevant perturbations for each chart type.\nDuring the refinement process, we carefully considered the relevance and interpretability of specific perturbations for different chart types. For example, \"stack plots\" were not considered for simple charts due to the absence of stackable elements. Similarly, \"overlapping area plots\" were excluded from complex charts due to their inherent ambiguity and complexity even for human annotators.\nFollowing the process, we ended up with 22 unique categories for simple charts and 25 unique categories for complex charts as have been highlighted through the results table.\nTo illustrate the final perturbation categories, we provide representative images showcasing all subplots within each category. These visual examples provide a clear understanding of the types of perturbations included in RobustCQA and have been depicted in the section following the tables."}]}