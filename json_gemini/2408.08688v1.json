{"title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation", "authors": ["Samee Arif", "Sualeha Farid", "Abdul Hameed Azeemi", "Awais Athar", "Agha Ali Raza"], "abstract": "This paper presents synthetic Preference Optimization (PO) datasets generated using multi-agent workflows and evaluates the effectiveness and potential of these workflows in the dataset generation process. PO dataset generation requires two modules: (1) response evaluation, and (2) response generation. In the response evaluation module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we use inter-rater agreement using Cohen's Kappa between human annotators and LLMs. For the response generation module, we compare different configurations for the LLM Feedback Loop using the identified LLM evaluator configuration. We use the win rate (the fraction of times a generation framework is selected as the best by an LLM evaluator) to determine the best multi-agent configuration for generation. After identifying the best configurations for both modules, we use models from the GPT, Gemma, and Llama families to generate our PO datasets using the above pipeline. We generate two types of PO datasets, one to improve the generation capabilities of individual LLM and the other to improve the multi-agent workflow. Our evaluation shows that GPT-40-as-a-Judge is more consistent across datasets when the candidate responses do not include responses from the GPT family. Additionally, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate a range of Natural Language Processing (NLP) capabilities, including text generation, question answering, and language understanding. However, LLMs can sometimes deviate from user instructions and exhibit unintended behaviors (Tamkin et al. 2021). To mitigate this problem and align the LLM outputs more closely with human preferences, techniques like Reinforcement Learning from Human Feedback (RLHF) are used, which involves fine-tuning LLMs using the reward signal from human preferences (Christiano et al. 2017). Improved methods like Direct Preference Optimization (DPO) (Rafailov et al. 2024) eliminate the need for fitting the reward model and are more stable and performant. In DPO, the preference optimization dataset requires a pair of accepted and rejected responses for each prompt. The accepted response is one that better aligns with the desired human preferences. Other techniques like Kahneman-Tversky Optimization (KTO) (Ethayarajh et al. 2024) require each response to indicate whether it is good or bad (i.e., as a binary classification task) instead of pairwise preferences.\nIn the process of constructing the dataset of human preferences, the evaluation and ranking of the outputs generated by LLMs are typically done by human annotators, who assess these outputs based on various criteria such as instruction following, helpfulness, relevance, accuracy, depth, and creativity. The PO dataset generation process is divided into two modules: response evaluation and response generation. The response evaluation module involves assessing and ranking responses generated by LLMs, while the response generation module focuses on creating responses that align with the identified preferences. This manual process, while effective, is labor-intensive, time-consuming, inconsistent, and subject to human biases. In this work, we thus ask the question, \"Can we use LLM agents to automate and improve response evaluation and generation for constructing preference optimization (PO) datasets?\".\nFor the response evaluation step, we leverage LLMs as evaluators and compare several configurations including LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate to pick the best evaluation strategy. Previously, single agents have been used to generate the responses for PO datasets. However, we use a multi-agent framework for response generation, which allows us to generate more refined and higher-quality responses. The multi-agent approach uses the collaboration between multiple LLMs, where one agent can provide suggestions for improvements, and the other can revise the response based on the feedback. This iterative process leads to a thorough refinement of the generated content, ensuring that the final output better aligns with human preferences and expectations.\nIn this paper, we present multiple DPO and KTO datasets. Our focus is on generating two separate categories of datasets: one aimed at improving the performance of individual LLMs and the other to enhance the effectiveness of multi-agent workflows. The primary aim of the first dataset is to enhance the performance and capabilities of individual LLMs by providing high-quality PO training data that better aligns with human judgment and expectations. The goal of the second category of the dataset is to improve the multi-agent frameworks of LLM Feedback Loop generation approach, enabling better feedback provision, response refinement, and decision-making. Our contributions can be summarized as follows:\n1.  We generate synthetic PO datasets for single-agent improvement by combining the best configuration for the evaluation and generation module. We also generate PO datasets for multi-agent improvement.\n2.  We present a comprehensive evaluation of using LLMs as evaluators on the task of selecting the better response among the candidate responses. We specifically compare the performance of three distinct approaches: LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate.\n3.  We present an evaluation of the LLM Feedback Loop workflow for the response generation module, specifically testing different configurations using Llama-3.1-8 and Gemma-2-9b models."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Preference Optimization", "content": "Preference Optimization has emerged as a pivotal technique for aligning model outputs with human preferences. Rafailov et al. 2024 introduce DPO, a method that simplifies solving the standard RLHF (Reinforcement Learning from Human Feedback) problem by converting it into a classification task, enabling the extraction of the optimal policy in a straightforward way. Hong, Lee, and Thorne 2024 introduce ORPO algorithm that combines the traditional supervised fine-tuning and preference alignment stages into a single process. The dataset for DPO and ORPO require annotated preference pairs, where each pair consists of two model outputs labeled according to which one better aligns with human preferences. Ethayarajh et al. 2024 introduce KTO, a cost-effective approach to align Large Language Models (LLMs) with human feedback, improving performance without the need for preference pairs. Argilla Distilabel (\u00c1lvaro Bartolom\u00e9 Del Canto et al. 2024) uses LLM to judge between the responses of two models to create synthetic PO datasets. The datasets are available on Hugging Face. To our knowledge, no one has yet explored the use of Multi-Agent workflows for the generation of PO datasets. However, multi-agent frameworks have been utilized for other tasks which we discuss below."}, {"title": "2.2 Agent Frameworks", "content": "Recently, there has been a growing interest in using LLM multi-agent frameworks for different tasks. Zheng et al. 2023a presents an evaluation of LLM-as-a-Judge on the MT-Bench (Zheng et al. 2023b) and Chatbot Arena (Li et al. 2024). Their results reveal that strong LLM judges like GPT-4 can match both controlled and crowd-sourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Additionally, they evaluate several variants of Llama and Vicuna on the dataset. They study the limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability. Verga et al. 2024 explore the use of LLMs-as-a-Jury. Their approach, a Panel of LLM evaluators (POLL), composed of a larger number of smaller models outperforms a single large judge. They also show that the POLL approach exhibits less intra-model bias as compared to LLM-as-a-Judge. They use Command-R, GPT, Claude-3, and Mistral families for their study. Additionally, they compare two prompting strategies: (1) reference-based scoring where they provide the LLM with a reference answer, and (2) candidate answer and pair-wise scoring where they ask the LLM to pick the better response from the candidate responses. PoLL outperforms single-agents on KILT (Petroni et al. 2021) and Chatbot Arena.\nLiang et al. 2024 introduce Multi-Agent Debate (MAD) to encourage divergent thinking in LLMs. They mitigate the Degeneration-of-Thought (DoT) problem, which is that once the LLM has established confidence in its solutions, it is unable to generate novel thoughts. In their approach, the affirmative LLM and the negative LLM debate on the answer while the LLM judge evaluates both arguments after each round of debate. They evaluate the approach on the Commonsense Machine Translation Dataset (Chinese to English) (He et al. 2020) and their Counter-Intuitive Arithmetic Reasoning (CIAR) dataset. MAD was able to achieve a 37% accuracy on the CIAR dataset using GPT-3.5-Turbo which outperforms Chain-of-Thought, Self-Consistency, and Self-Reflection prompting. They also show that using the MAD approach decreases bias and increases response diversity. Du et al. 2023 evaluates a different variant of multi-agent debate where multiple models generate their own responses, and each model receives the opinions of the other models, then updates its response if necessary. This is done for multiple rounds. Du et al. 2023 evaluates the approach on the following tasks: Biography generation, MMLU, Chess move validity, Arithmetic, Grade school math, and Chess move optimality. Their approach using ChatGPT and Bard outperforms single-agent on all the tasks. To evaluate LLM responses Chan et al. 2023 presents another variant of multi-agent debate. Their architecture involves assigning agents different roles such as General Public, Critic, Psychologist, News Author, and Scientist. They used ChatGPT and GPT-4 for their evaluation on FairEval (Wang et al. 2023a) dataset and achieved a Cohen's Kappa score of 0.40 using LLM Debate, 0.03 more than the single agent."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Experimental Setup", "content": "In this study, we perform experiments on the three categories of models given in Table 1. For the evaluation module, we evaluate single agents and multi-agent frameworks on four datasets, Alpaca Eval (Li et al. 2023), FairEval (Wang et al. 2023a), PandaLM-Eval (Wang et al. 2024, 2023b) and MT-Bench (Zheng et al. 2023b). For the generation module, we compare the multi-agent frameworks using win rate - the ratio of times a generation framework is selected as the best by an LLM evaluator when comparing outputs from all generation workflows. After the extensive evaluation of both modules, we used the picked strategies to generate synthetic PO datasets. We set the temperature to 0 in all our evaluations to ensure reproducibility."}, {"title": "3.2 LLM-as-Evaluator", "content": "With the aim of automating the evaluation component of PO dataset generation, we assess the performance of LLMs in the role of evaluators using the Alpaca Eval, FairEval, PandaLM-Eval, and MT-Bench datasets. Our goal is to determine whether multi-agent workflows work better than a single agent for LLM evaluation. The system prompts for this task are modified version of the prompts used by Zheng et al. 2023a and are given in Appendix A.\nWe evaluate six different LLMs on the Alpaca Eval dataset, calculating Cohen's Kappa with the human annotations. Our evaluation involved three distinct prompting strategies for the LLM-as-a-Judge:\n1.  Direct Comparison: The Judge-LLM is provided with the user question and the responses generated by different LLMs. It is asked to pick the best response among the given options.\n2.  Independent Scoring: The Judge-LLM is given the user question and each response in separate conversations. It is asked to score each response independently.\n3.  Combined Scoring: The Judge-LLM is provided with the user question and all the responses in a single conversation thread. It is asked to assign a score to each response within the same conversation context. To observe if the scoring range influences the LLM's scoring consistency and its alignment with human annotations, we test three different scoring totals: 5, 10, and 100.\nFor each of these prompting strategy, we systematically analyze the performance of the LLMs by calculating Cohen's Kappa, against the human annotations. The system prompts are given in Table 7 in Appendix A.\nWe extend the evaluation from the LLM-as-a-Judge approach by forming juries composed of multiple LLMs. We test all possible permutations of the jury configurations. We use three datasets: FairEval, PandaLM-Eval and MT-Bench datasets for a more comprehensive analysis. We systematically analyze the performance of each jury configuration, focusing on how the size and combination of the LLMs affect their judgment accuracy. The Combined Scoring system prompt in Table 7 in Appendix A is used for all the jurors because it performed the best in our previous evaluation.\nWe also evaluate the LLM Debate framework following the implementation described by Chan et al. 2023. In this approach, we assign three distinct roles-Psychologist, General Public, and Critic and the three agents debate the scores that should be assigned to candidate responses. After the debate, each agent gives its final score which is used to determine which candidate response they vote for. These votes are then used to pick the best response. This strategy is evaluated using the FairEval, PandaLM-Eval, and MT-Bench benchmarks. The system prompt, the user message structure and the prompts for the roles used are given in Table 8 and Table 9 in Appendix A."}, {"title": "3.3 LLM-as-Generator", "content": "To evaluate the LLM Feedback Loop workflow for the generation module, we test different configurations using Llama-3.1-8b (Meta 2024) and Gemma-2-9b (Google 2024) models. In this framework, a generator LLM produces a response, which is then evaluated by a feedback LLM that provides improvement suggestions. The generator revises the response based on these suggestions, and the process repeats for multiple iterations. The system prompt for the generator and reviewer is given in Table 10 and 11 in Appendix A. We calculate the win rate against single-agent GPT-40 (OpenAI 2024), Llama-3.1-8b and Gemma-2-9b baseline outputs on a subset of 500 prompts from the Argilla Capybara DPO dataset to identify the best configuration. We test the following configuration:\n1.  Same Model as Both Agents: Gemma-2-9b or Llama-3.1-8b as both the feedback and generation agent.\n2.  Different Models for Each Agent: Gemma-2-9b as the feedback agent and Llama-3.1-8b as the generation agent, or vice versa.\n3.  Both Models for Feedback, One for Generation: Gemma-2-9b or Llama-3.1-8b as the generation agent, with both models as feedback agents."}, {"title": "3.4 Preference Optimization Dataset", "content": "We use Llama-3.1-8b and Gemma-2-9b in the generation module and GPT-40 in the evaluation module to generate multiple DPO datasets and KTO datasets for single-agent improvement and multi-agent improvement. The prompts used for single-agent improvement dataset generation are given in Table 7, 10 and 11 in Appendix A. The prompt used for multi-agent improvement dataset generation is given in Table 12 in Appendix A. The evaluation code, all the evaluation outputs and the generated datasets are publicly available on GitHub.\nSingle-Agent Improvement. We use the prompts from Argilla Capybara DPO dataset. The Feedback Loop framework generates N responses (where N is the number of iterations). LLM-as-Evaluator picks the best response from the candidates to create the DPO and KTO dataset.\nMulti-Agent Improvement. We use the prompts and the human-generated responses from the No Robots dataset (Rajani et al. 2023). The evaluator is given the human response (reference response) and LLM response and is asked to generate feedback based on the reference response. The generated feedback and the human response are used to create the PO dataset. The goal of this dataset is to improve both response generation and response evaluation. The structure of the dataset is given in Figure 3."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 LLM-as-Evaluator", "content": "Prompting Strategies. Table 2 shows the results of LLM-as-a-Judge approach on the three prompting strategies. The Independent Scoring prompt strategy consistently under-performs compared to the Direct Comparison and Combined Scoring approaches across all evaluated LLMs. This result is reflected in lower Cohen's Kappa values in Table 2. In evaluating responses in isolation the LLM has to re-calibrate its scoring mechanism for every new response. This can lead to inconsistencies, especially when multiple responses are closely matched in quality. Due to the low Kappa values observed, we opted not to conduct experiments with the scoring-out-of-5 and 100 scales for Independent Scoring.\nThe Direct Comparison Strategy performs better than the Independent Scoring approach across most LLMs, with a notable improvement for GPT-40 (0.372 vs. 0.249) and GPT-40-mini (0.342 vs. 0.254). However, it generally falls short when compared to the Combined Scoring method, where GPT-4o achieves a score of 0.401 using the scoring-out-of-100 scale. The higher Cohen's Kappa values indicate that the Direct Comparison and Combined Scoring strategy benefits from providing the LLM with a side-by-side evaluation of responses, allowing for more accurate and consistent judgments.\nThe Combined Scoring strategy, as presented in Table 2, shows consistent performance using all the scoring scales. It outperforms both the other prompts. The scoring scales of 5, 10, and 100 show variability across different models, with certain scales performing better for some models than others. For example, GPT-40 performs the best in scoring-out-of-10 scale with a Kappa score of 0.382 while Gemma-2-9b performs best under scoring-out-of-5 scale. Given these results, we selected the scoring-out-of-10 scale as the most effective option for the Combined Scoring approach. We use this prompt for all our further evaluations.\n The LLM-as-Judge evaluations indicate that GPT-40 outperforms all the models on PandaLM-Eval and MT-Bench achieving a Cohen's Kappa score of 0.688 and 0.410 respectively. Additionally, GPT-40 consistently ranks in second position across all three datasets. This consistent top-tier performance underscores GPT's effectiveness as a reliable judge in evaluating LLM responses. Gemma-2-27b outperforms all other models on the Fair-Eval dataset, achieving the highest score in this particular evaluation. However, it's important to note that the Fair-Eval dataset is relatively small, consisting of only 80 samples. Furthermore, the Fair-Eval dataset primarily compares GPT-3.5-Turbo with Vicuna-13b, which might introduce a bias in favor of GPT models when GPT is the evaluator. shows that GPT-40 selects GPT-3.5-Turbo as the better agent 50 times and Vicuna-13b 30 times. This indicates a potential bias in favor of GPT responses when GPT-40 is the evaluator. Additionally, we can observe in the figure that Llama models also display a similar bias towards GPT responses, whereas Gemma models do not exhibit this bias, suggesting that Gemma is more impartial in its evaluations.\nIn evaluating of LLMs-as-a-Jury, we analyze the top three juries from each dataset as shown in Table 4. Notably, the scores exhibit considerable variation across the different datasets. On the Fair-Eval and MT-Bench datasets, the jury approach outperformed the judge approach, indicating a potential advantage in using multiple models for evaluation. For instance, on Fair-Eval, the highest-performing jury achieves a Cohen's Kappa of 0.428 while the judge achieves Kappa of 0.389, suggesting a relatively strong agreement with human judgments compared to individual judges. This configuration, however, shows a drop in performance on other datasets with a kappa of 0.604 on PandaLM-Eval and 0.395 on MT-Bench, underscoring the challenge of generalizing a single jury setup across varied datasets. However, the judge approach outperforms the jury on the PandaLM-Eval dataset, where the best judge attained a kappa of 0.688, surpassing the top jury's kappa of 0.673. The best jury on MT-Bench, with a kappa of 0.429, also demonstrates variability in its performance across datasets as well, with a kappa of 0.636 on PandaLM-Eval and only 0.273 on Fair-Eval.\nThe jury approach, by incorporating diverse models, mitigates the biases that occur in LLM-as-a-Judge approach when bench-marking on the Fair-Eval dataset. However while the jury approach can offer robustness through diversity, in evaluation task, it does not universally outperform single judges. The decision to employ a jury versus a judge should consider whether the candidate responses being evaluated include output from the judge itself, which can introduce bias in the results. Additionally, scalability should be taken into account, as the jury approach might require more computational resources. Another critical consideration is the variability in performance across different datasets, which poses a challenge for generalization.\nThe LLM Debate approach, as summarized in Table 5, showcases varying degrees of effectiveness across three different datasets: Fair-Eval, PandaLM-Eval, and MT-Bench. GPT-40 performs the best across all datasets, with Cohen's Kappa scores of 0.404, 0.654, and 0.402 respectively. LLM Debate outperforms LLM-as-a-Judge on Fair-Eval only and does not surpass the LLMs-as-a-Jury approach on any dataset. On Fair-Eval using the Debate framework increases the Kappa score of GPT-40 from 0.327 to 0.404 and of GPT-40-mini from 0.333 to 0.360. It shows that the debate approach decreases the bias of GPT-40 and GPT-40-mini towards the responses of it's family.\nThere is a significant variance in the performance of LLM Debate across the models and the datasets. For instance, as seen in Table 5 Gemma-2-27b in debate architecture outperforms Gemma-as-a-Judge on PandaLM-Eval and MT-Bench but on Fair-Eval judge performers better. Gemma-2-9b in debate architecture has a Kappa score of 0.323 on Fair-Eval, outperforming 0.279 of Gemma-as-a-Judge. However on PandaLM-Eval and MT-Bench Gemma-2-9b in debate framework achieves a Kappa score of 0.520 and 0.326, respectively. Both scores lower as compared to Gemma-as-a-Judge scores of 0.595 and 0.354. In case of Llama, Llama-3.1-8b in judge configuration outperforms itself in debate configuration. Llama-3.1-70b in debate framework only outperforms Llama-as-a-judge on Fair-Eval. Figure 5 shows a comparison of Cohen's Kappa of LLM Debate and LLM-as-a-Judge across the three datasets and all the models."}, {"title": "Evaluation Framework for PO Dataset", "content": "Based on the comparative evaluation scores across the three datasets and the advantages and disadvantages associated with each multi-agent framework, we have chosen to use the LLM-as-a-Judge approach with GPT-40 as our primary evaluator for generating the PO dataset. This decision is driven by multiple factors:\n1.  In our context, the task involves generating a PO dataset using Llama-3.1-8b and Gemma-2-9b. Therefore there will be no bias in the evaluation when using GPT-40 as the judge.\n2.  The performance of GPT-4o-as-a-Judge has been consistently high across various evaluations, indicating its reliability as a judge. While the LLMs-as-a-Jury and LLM Debate approaches have a high variance in Cohen's Kappa score across different datasets.\n3.  The computational resources required for managing the LLM Debate and LLM Jury frameworks are considerably higher than those needed for a single-judge setup. The LLM-as-a-Judge method is simpler to implement and scale."}, {"title": "4.2 LLM-as-Generator", "content": "We compare the performance of Multi-Agent Feedback Loop with the baseline Single-Agents (GPT-40, Llama-3.1-8b, Gemma-2-9b) using win rate as shown in Table 6. We utilize GPT-40-as-a-judge in this evaluation process. For the baseline we find the win rate of Gemma and Llama against GPT-40 and each other. Both smaller models have similar win rate of 38.6% and 39.2% against GPT, while Gemma has a win rate of 66.6% against Llama.\nIn the Multi-Agent setting, all variations outperform the single-agents against GPT-40, with the highest win rate of 49.0% for Llama as a generator and Gemma as a reviewer. This configuration performs the best against Llama and Gemma too, with 71.8% and 73.8% win rate respectively. We observe that using Llama as the generator improves the performance as compared to using Gemma as the generator because this configuration leads to a better win rate against all three baselines.\nLlama's strengths in generating responses may be enhanced by Gemma's ability to fine-tune and correct the errors, leading to more polished outputs. The results underscore the importance of assigning appropriate roles based on the specific strengths of each model. Llama, when set as the generator, appears to leverage its capabilities more effectively than Gemma in this role. The use of diverse models in the feedback loop likely helps mitigate biases that any single model might introduce. This diversity ensures a broader range of perspectives while answer a question. In conclusion, the demonstrated efficacy of the Multi-Agent Feedback Loop, especially with Llama as the generator and Gemma as the reviewer, validates the concept of collaborative AI systems."}, {"title": "4.3 Preference Optimization Dataset", "content": "Single-Agent Improvement. For the Single-Agent improvement dataset generation we use GPT-4o-as-a-Judge in the evaluation module. In the generation module, we use LLM Feedback Loop with Llama-3.1-8b as the generator and Gemma-2-9b as the reviewer. The framework is shown in Figure 6. For the dataset generation, we use N = 3 iterations. We pick the best response (judged by GPT-40) as accepted and the other 2 responses as rejected and generate two datasets, one for DPO and one for KTO.\nMulti-Agent Improvement. For the Multi-Agent improvement dataset generation we use Gemma-2-9b because the win rate for Gemma against Llama-3.1-8b is 66.6%. We give Gemma user prompt, human response and it's own response and ask it to generate feedback based on the human reference. For the rejected responses we use the feedback without human reference. We generate two datasets, one for DPO and one for KTO."}, {"title": "5 Conclusion", "content": "This paper presents PO datasets generated using multi-agent frameworks, and evaluates these frameworks by highlighting the advantages, drawbacks, and challenges of each approach. In the response evaluation module, our comparative analysis of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate shows the suitability of each setup depending on the context of use. For the response generation module, we evaluate the LLM Feedback loop using Llama-3.1-8b and Gemma-2-9b in various configurations. LLM-as-a-Judge proved to be highly effective when candidate responses don't have a response from the Judge LLM. Whereas LLMs-as-a-Jury and LLM Debate demonstrated robustness, particularly useful in reducing evaluator bias. However, Cohen's Kappa for both of these approaches has a high variance making them less suitable for novel applications.\nOur experiments with LLM Feedback Loop using Llama-3.1-8b and Gemma-2-9b configurations show the potential of multi-agent frameworks in refined content generation. Configurations where Llama-3.1-8b served as the generator and Gemma-2-9b as the reviewer consistently delivered better results, demonstrating the benefits of leveraging complementary strengths of different models to refine output quality. These findings indicate the effectiveness of multi-agent frameworks for varied AI applications, showing promise for moving towards systems requiring minimal human intervention - however, this method is computationally expensive in comparison.\nWe also generate multiple DPO and KPO datasets using LLM Feedback Loop with Llama-3.1-8b as the generator and Gemma-2-9b as the evaluator and GPT-40-as-a-Judge. The aim of these datasets is to improve single-agent capabilities for better response generation and multi-agent capabilities including better communication and improved feedback.\nIn order to facilitate further research and ensure transparency, all code, LLM responses, and generated datasets have been made public."}, {"title": "6 Future Work", "content": "In terms of future work, there are three avenues of investigation: (1) Performance comparison of models fine-tuned on our PO dataset versus widely-used LLMs to investigate the impact of our generated datasets through a series of experiments. (2) Using larger models such as Llama-3.1-70b and Gemma-2-27b for dataset generation as this may provide more diverse and higher-quality training data, potentially leading to further advancements in model performance and generalizability. (3) Experimenting with the number of iterations used in the Feedback Loop framework and including other LLM families in the dataset generation process."}]}