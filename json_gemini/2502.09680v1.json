{"title": "OBJECT-CENTRIC LATENT ACTION LEARNING", "authors": ["Albina Klepach", "Alexander Nikulin", "Ilya Zisman", "Denis Tarasov", "Alexander Derevyagin", "Andrei Polubarov", "Nikita Lyubaykin", "Vladislav Kurenkov"], "abstract": "Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by x2.7 and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by x2.6 on average.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the scaling of model and data sizes has led to the creation of powerful and general foundation models that have enabled many breakthroughs in understanding and generation of natural language and images . On the other hand, the field of embodied AI has generally remained behind in terms of generalization and emergent abilities , being mostly limited by the lack of diverse data for pre-training . The vast amount of video data on the Internet, covering a wide variety of human-related activities, can potentially fulfill the current data needs . Unfortunately, videos cannot be used directly since they do not have action labels, which is necessary for imitation and reinforcement learning algorithms.\nIn order to compensate for the lack of action labels, approaches based on Latent Action Models (LAM) , aim to infer latent proxy-actions between consecutive observations. Such actions can be later used for imitation learning to obtain behavioral policy prior from large unlabeled datasets. A significant challenge in this endeavor is the presence of action-correlated distractors-dynamic backgrounds, incidental object motions, camera shifts, and other nuisances that falsely correlate with agent actions, and may lead models to overfit to non-causal patterns . Existing methods for learning latent actions from videos, such as Latent Action Pretraining (LAPA) , often assume curated, distractor-free datasets or rely on costly annotations. Although effective in controlled settings, this reliance on clean data limits their scalability and applicability in real-world scenarios.\nIn this preliminary work, we propose object-centric latent action learning in order to improve applicability to real-world data. By decomposing scenes into spatio-temporal object slots , our method provides the structural priors needed to disentangle causal agent-object"}, {"title": "2 BACKGROUND", "content": "Learning from observations. Learning solely from observations (LfO) has emerged recently to mimic the success of large-scale pretraining in different domains, like text, as a pathway for scalable embodied foundation models. Early efforts like Video PreTraining (VPT) demonstrated the potential of pretraining on internet-scale video data (e.g., Minecraft gameplay) to recover latent policies. However, VPT requires costly action labeling via human demonstrations, limiting its scalability. In Ghosh et al. (2023) the authors proposed modeling latent intentions, representations of various outcomes an agent might aim to achieve, to learn useful features from observations data.\nSubsequent work shifted to Latent Action Policies (LAPO) , a combination of forward-dynamics model (FDM) $f_{FDM}(o_{t+1}|o_t, a_t)$, which predict future states from current observations, and inverse-dynamics model (IDM) $f_{IDM}(a_t|o_t, o_{t+1})$, which infer latent actions from state transitions. FDM and IDM are jointly learned to minimize the next state prediction and further used to label the trajectory of observations with latent actions. As Schmidt & Jiang (2024) show, obtained latent actions can recover ground true actions, however, they assume distractor-free environments, a brittle assumption for real-world video data.\nLearning in noisy setting. Recent work by Wang et al. (2023) argues that optimal World Models should suppress such distractors by isolating controllable, reward-relevant factors. There are papers proving the discovery of true latent states from offline trajectories of observations and actions. Real-world videos inherently contain action-correlated distractors: environmental dynamics (e.g., moving backgrounds, camera jitter) that spuriously correlate with agent actions. However, existing LfO methods lack mechanisms to disentangle distractors , leading to overfitting in noisy settings.\nObject-Centric Pretraining for Videos. Object-centric learning aims to decompose visual scenes into structured, entity-level representations that capture independent objects, i.e. slots. During learning, slots compete with each other in describing the image. Such procedure is named"}, {"title": "3 METHOD", "content": "Object-Centric Representation Learning. We employ the VideoSAUR (Zadaianchuk et al., 2023) to decompose input video frames into spatio-temporal object slots. This self-supervised model isolates individual entities within a scene, providing structured representations that are less susceptible to background noise and incidental motion. In the end of this step we obtain an encoder Fs, which directly maps a trajectory of observations O into the trajectory of slots S. The number of slots K for each observation $o_i$ in St is hyperparameter, fixed at the beginning of the training. The resulting representation St of the observation $o_t$ is a combination of slot vectors $s_i$, i.e. $F_s (O) = S$. Due to having a transformer-based slot decoder, slots $s_i$ can be projected to initial observation $o_t$ utilizing attention maps as alpha masks, to obtain object masks $m_{ij}$. Further we will denote them as masks (see Figures 1, 4 and 5 for a visualization).\nSlot Selection. From the generated object slots, we identify and select those relevant to the agent's interactions. We plan to address the automatic selection of control-related slots in the future. In the DCS's the control-related objects are the main agent (cheetah, walker, hopper or humanoid) and the floor. Depending on the number of slots and the environment, they can arise in the same or different slots, so if needed, slots can be combined during this stage by concatenation over selected slots or taking mean over selected masks. In the current pipeline slot selection among slot vectors $s_{i,j}$ is performed based on the corresponding masks $m_{ij}$. Due to fixed slots initialization (see details in Appendix D) it can be done only once for the whole dataset.\nLatent Action Modeling. Utilizing the selected object-centric representations, we train a latent action model inspired by the LAPO (Schmidt & Jiang, 2024). The inverse-dynamics model $z_t ~ f_{IDM}(s_t, a_{t+1})$ and the forward-dynamics model $\\hat{s}_{t+q} ~ F_{FDM} (s_t, z_t)$ are trained to reconstruct the trajectory of slots $||\\hat{s}_{t+1} \u2013 s_{t+1}||^2$ (or masks). We denote this as lapo-slots and, accordingly, lapo-masks for masks representations. Thus, lapo-slots reconstruct next observations in latent space, while lapo-masks reconstruct images, which were filtered based on selected slots masks (see Figures 1 and 5).\nBehavior Cloning and Finetuning. The inferred latent actions serve as proxies for actual action labels. We train a behavior cloning (BC) agent to predict these latent actions, using the same dataset as for latent action learning. To evaluate the pre-training effectiveness, as a final stage, we fine-tune the resulting agent on a limited set of trajectories with ground-truth action labels, in line with (Schmidt & Jiang, 2024; Ye et al., 2024). The scores of the BC agent depending on number of the labeled trajectories are presented on the Figure 3."}, {"title": "4 EXPERIMENTS", "content": "Task formulation. To address the robustness to action-correlated distractors, a critical challenge in learning from raw video data, we evaluate on Distracting Control Suite (DCS) (Stone et al., 2021) environment. DSC is an extension of DM Control with three types of real-world visual noise: (1) dynamic backgrounds: 60 diverse videos from DAVIS 2017 , simulating realistic environmental clutter; (2) color variations: hue/saturation shifts that preserve object semantics but degrade low-level features; (3) camera perturbations: randomized pose adjustments mimicking handheld camera noise. The scale of color and camera variations is 0.1. As for the tasks: we selected 4 (in the order of increasing complexity): cheetah-run, walker-run, hopper-hop, humanoid-walk. We trained expert policy optimization agents on trained on privileged state information and collected the dataset with observation-action tuples. Importantly, the level of distractions is tuned so the behavior"}, {"title": "4.1 LATENT ACTION QUALITY", "content": "To quantify the quality of obtaining latent actions we employed linear probing of latent actions against ground-truth expert actions. Such probe quantifies how well latent actions capture causal"}, {"title": "4.2 DOWNSTREAM PERFORMANCE", "content": "To evaluate the quality of the resulting agents we measure episodic return of a behavior cloning agent, trained on the obtained latent actions and fine-tuned on 32-128 trajectories (1k transitions) with ground-truth actions. Small size of the action-labeled sample is critical for real-world deployment. Each agent is evaluated over 25 episodes in the environment. The scores on the Figure 3 are normalized by the performance of BC trained on the full datasets with all action labels revealed (see Table 3 in the Appendix). The scores on the Table 2 are scaled by 100. The quantitative results of evaluation retuns are presented on the Figure 3 and in the Table 2. The result are averaged among 3 random seeds.\nLAPO struggle in the precense of distractors. The gap between LAPO trained in distracted and non-distracted setting is x3.9 among all tasks, which shows the importance a technique, that can filter out the distraction.\nObject-slots reduce the gap caused by distractors. Following the results in the Section 4.1 slots and masks outperform baseline lapo for all 4 tasks: by at least x2.1 for slots (x2.6 on the average), and at least x1.7 for masks (x2.0 on the average). The maximal improvement os observed on humanoid-walk: x7 for both slots and masks, while improvement between non-distracted and distracted lapo is x4.5.\nFor easy tasks slots show higher sample efficiency. Horizontal lines on Figure 3 denote the evaluation score of an average corresponding BC agent finetuned on the whole training dataset of trajec-"}, {"title": "5 CONCLUSION & LIMITATIONS", "content": "Our preliminary results demonstrate that object-centric representations significantly mitigate the impact of distractors for learning latent actions from videos. By disentangling scene elements into meaningful, interpretable slots, latent actions focus on causal dynamics rather than spurious correlations. This could provide a strong inductive bias for more effective Latent Action Models in noisy environments.\nHowever, challenges remain. While object slots reduce noise, selecting task-relevant slots is still ambiguous. Without explicit supervision, models may focus on irrelevant elements. This points to a fundamental limitation in current object-centric methods: the difficulty in dynamically attending to the \"right\" objects across varying tasks and environments. Another limitation is the dependency on the quality of training data. Cleaner, well-curated datasets naturally lead to more robust representations, whereas large-scale uncurated video data necessitates additional mechanisms for filtering out noise. This presents a trade-off between data volume, model complexity, and data cleaning efforts. While curated data simplifies training, it limits generalization and scalability. On the other hand, noisier datasets require more sophisticated models but unlock broader generalization capabilities for embodied AI."}, {"title": "D FIXED INITIALIZATION FOR SLOT STABILITY", "content": "To mitigate slot permutation variance across predictions, we introduce a fixed slot initialization scheme that learns deterministic initial slot vectors while preserving robustness. Unlike standard Gaussian initialization, which samples slots stochastically at each step, our approach learns per-slot parameters (mean $\u03bc_\u03ba \u2208 R^d$ and variance $\u03c3_\u03ba \u2208 R^d$) during training. During training, we inject controlled noise scaled by the learned variance into the slot initializations, acting as a regularizer to encourage robust feature disentanglement. At inference, slots are initialized deterministically using the learned means, ensuring consistent slot-object assignments. This hybrid strategy bridges the gap between training stability and inference consistency: the noise-augmented training phase prevents overfitting to fixed initializations, while the deterministic inference phase enables efficient object-wise slot selection via decoder masks as visual priors.\nTrain: $s^{(init)}_k  \u03bc_\u03ba + \u03c3_\u03ba\u03f5, \u03f5~ N(0, I)$, Inference: $s^{(inference)}_k = \u03bc_\u03ba$."}]}