{"title": "BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark", "authors": ["Nikita Chernyadev", "Nicholas Backshall", "Xiao Ma", "Yunfan Lu", "Younggyo Seo", "Stephen James"], "abstract": "We introduce BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set in home environments, ranging from simple target reaching to complex kitchen cleaning. To capture the real-world performance accurately, we provide human-collected demonstrations for each task, reflecting the diverse modalities found in real-world robot trajectories. BiGym supports a variety of observations, including proprioceptive data and visual inputs such as RGB, and depth from 3 camera views. To validate the usability of BiGym, we thoroughly benchmark the state-of-the-art imitation learning algorithms and demo-driven reinforcement learning algorithms within the environment and discuss the future opportunities.", "sections": [{"title": "1 Introduction", "content": "Machine learning benchmarks are of significant importance for measuring and understanding the progress of research algorithms. Examples of notable benchmarks include ImageNet [1] for image understanding, KITTI [2] for autonomous driving, and SQUAD for language-based question answering [3]. In robotics, prior benchmarks have greatly reduced the cost of iterating and developing algorithms. Examples include OpenAI Gym [4], DeepMind Control Suite [5], and MetaWorld [6]. However, all of these benchmarks focus on pure reinforcement learning (RL) with dense shaped rewards, limiting their application in long-horizon manipulation tasks where accurately defining reward functions is challenging.\nWhile crafting reward is difficult, obtaining expert trajectories, such as those from human demonstrations, is relatively straightforward. This advantage has boosted the popularity of demonstration-driven methods within the robot learning community, manifesting as both imitation learning (IL) [7, 8, 9, 10, 11, 12, 13] and demo-driven RL [14, 15, 16, 17]. To support the research of building demo-driven agents, RLBench [18] was created with a wide range of single-arm fixed manipulation tasks with expert demonstrations generated by motion planners. Using motion planners allows RLBench to generate a large amount of demonstration data purely in simulation, however, the output trajectories are often either unnatural due to the inherent randomness in sampling-based planners, or have an unrealistically narrow trajectory distribution when compared to noisy real-world human demonstrations. Moreover, the community-progress is beginning to plateau on a large number of RLBench tasks, in particular with recent 3D next-best pose agents [10, 11, 16, 17, 19, 20, 21].\nThese limitation highlights the need for a new benchmark which provides: (1) more natural demonstrations like those seen in real-world robot data and (2) a set of new challenging tasks where state-of-the-art algorithms are likely to perform poorly. To this end, we present BiGym, a demo-driven mobile bi-manual manipulation benchmark with a humanoid embodiment. BiGym covers 40 visual mobile manipulation tasks, ranging from simple tasks like moving plates between drainers to interacting with articulated objects such as dishwashers (see Figure 1). Unlike prior humanoid benchmarks [22, 23] that focus only on RL with dense shaped reward functions, which may lead to undesired behaviors [24], we provide for each task only sparse rewards but with 50 demonstrations, allowing evaluation of both IL and RL algorithms. Additionally, compared to previous benchmarks that rely on expert demonstrations generated by planners [18], the human-collected demonstrations in BiGym are much more realistic and multi-modal (see Figure 3), better reflecting the trajectories of real-robot movements. Finally, BiGym considers locomotion and mobile bi-manual manipulation challenges separately; specifically, BiGym allows users to switch between the whole-body mode, which jointly considers locomotion and manipulation, and a bi-manual mode, which focuses on upper-body mobile manipulation while controlling the lower body with fixed controllers (see Figure 2). This separation of action modes enables researchers to better investigate and benchmark the capability of various algorithms with different focuses, i.e., locomotion control and mobile bi-manual manipulation solely. Code for BiGym is available on our project website."}, {"title": "2 Related Works", "content": "With the rapid progress in robot learning algorithms, the role of benchmarks has become crucial as a tool to understand the effect of various algorithmic design choices and compare algorithms in the same setup. There have been a series of benchmarks for complex manipulation tasks. Most of the existing benchmarks consider a single-arm manipulation scenario. The IKEA furniture assembly environment [25], BEHAVIOUR [26], and Habitat [27] provide a wide range of long-horizon household object (mobile) manipulation tasks. They emphasise long-horizon planning capabilities but employ abstract low-level actions that overlook physical interactions. Some benchmarks focus on more realistic settings with physics interactions [18, 28, 29, 30, 31, 32] and mainly support training RL agents. Notably, James et al. [18] provide APIs to generate expert demonstrations with motion planners. As a result, it is widely used for benchmarking IL [10, 11, 21] and demo-driven RL algorithms [16, 17, 19, 20]. Concurrent to our work, RoboCasa [33] constructs realistic environments"}, {"title": "3 BiGym", "content": "We present BiGym, a demo-driven mobile bi-manual manipulation benchmark. BiGym consists of 40 mobile bi-manual manipulation tasks, ranging from simple target reaching to complex dishwasher cleaning tasks. To evaluate IL and demo-driven RL algorithms in a realistic scenario with noisy, multi-modal demonstrations, BiGym provides human-collected demonstrations for all tasks. We describe which challenges BiGym presents (see Section 3.1), the simulation platform (see Section 3.2), and details on human demonstration datasets (see Section 3.3) and tasks (see Section 3.4)."}, {"title": "3.1 Challenges of BiGym", "content": "We design BiGym to pose the following challenges:\nPartial Observability. BiGym tasks are formulated as a partially observable Markov decision process (POMDP) [37] with discrete time $t = 1, 2, ..., T$, continuous action $a_t$, hybrid-observations, which include visual observations $o_t$ and robot low-level states $s_t$, and reward $r_t$. To achieve the task, the agent is required to learn a belief $b_t$, i.e., a distribution over the environment states, given past partial"}, {"title": "3.2 Simulation Platform", "content": "We build BiGym simulation environments based on MuJoCo [42] (see Figure 2(a) for the illustration of the whole system). We brief the core design choices below and more details are in Appendix A.\nHumanoid Body Configurations. We implement the platform with the Unitree H1 robot given its publicly available model\u00b9"}, {"title": "3.3 BiGym Human Demonstration Datasets", "content": "One of the key design choices in BiGym is to provide a fixed number of human-collected demonstrations for each task. This allows BiGym benchmark to better reflect the challenges of real-world robot learning, which involves dealing with noisy, multi-modal demonstrations in contrast to synthetic demonstrations generated by motion planners [18]. We describe BiGym's human demonstration dataset collection and management system as below and more details are in the appendix.\nDemo Collection Pipeline. We use VR\u00b2 to collect demonstrations by virtually tele-operating the H1 robot in simulation (using the 6 DoF poses of the headset and controllers). The headset pose controls the position and orientation of the H1 body, and the 6 DoF poses of the controllers are used to operate the arms. We solve the inverse kinematics for each arm and then reorient the grippers to match the orientation of the respective controller.\nDown-Sampling Demonstrations. The control frequency of demonstrations can significantly affect the horizon length of tasks and the ability to capture fine grained control, both of which can greatly affect task success [44]. To provide users with the flexibility in selecting action frequencies, we capture demonstrations at the frequency of physics calculation (500 Hz) and provide the functionality to down-sample the demonstrations to a desired frequency (20-500 Hz).\nDemonstration Management. To minimise the use of storage for saving demonstrations, we save lightweight demonstrations that only contain control signals (actions). We then provide a tool that enables users to pull such lightweight demonstrations and replay them to obtain full demonstrations with user-specified observations such as RGB or depth images. These demonstrations are cached in users' local storage so that users do not have to re-download or replay the same demonstrations again.\nTools. BiGym provides two tools for demonstration management. The demo_player tool provides various demonstration-related functionalities such as downloading, deleting, verifying, replaying at different frequencies, converting, and re-recording demonstrations. The demo_recorder enables users to easily collect data by streamlining the process of recording demonstrations with VR."}, {"title": "3.4 BiGym Tasks", "content": "BiGym provides 40 high-quality tasks with human-collected demonstrations to support the research on household mobile bi-manual manipulation. We describe the tasks and their configurations below.\nReach Target Tasks. Different from the standard reach target tasks in single-arm settings, we consider three variants of reach target tasks:\n(1) reach_target_single: The robot must use a specified wrist to reach a coloured target.\n(2) reach_target_multi_modal: The robot must reach the target with either left or right wrist. This induces a multi-modal demonstration distribution of reaching the target with different hands. We expect the policy to understand this multi-modality during training.\n(3) reach_target_dual: In this task, the robot must reach two targets, one with each arm. The success criteria require both wrists to be aligned with corresponding targets. Once this criteria is met, the targets are highlighted to provide visual feedback.\nTable-Top Manipulation Tasks. We then consider table-top manipulation which requires the robot to interact with rigid-body objects, e.g., plates and cups. The challenge here is that the robot should be"}, {"title": "4 Experiments", "content": "The contribution of the paper is BiGym. However, in this section, we aim to validate that current algorithms can attain some degree of performance on all BiGym tasks, even if minimal. To this end, we conduct experiments with both state-of-the-art IL and demo-driven RL algorithms. Specifically, we focus on the following seven general robot learning algorithms:\nIL Algorithms. We aim to investigate how different policy representations contribute to the final performance of the algorithms on BiGym, which provides highly noisy and multi-modal demonstrations. In pursuit of this goal, we consider the following algorithms: standard Behaviour Cloning (BC), Action Chunking Transformers (ACT) [9] which trains a transformer model [45] to predict a sequence of actions, and Diffusion Policies [8] which trains a diffusion model to approximate the expert action distribution. In particular, we do not benchmark against the popular 3D next-best pose agents [10, 11, 16, 17, 19, 20, 21] since they reply on heuristic-based key-frame extraction methods which only apply to single fixed arms [17]; thus, they are not currently applicable to the mobile bi-manual manipulation morphology.\nRL Algorithms. We mainly consider demo-driven RL algorithms which support training with expert demonstrations. Specifically, we focus on off-policy algorithms and offline RL algorithms that have demonstrated good capabilities in online settings. We consider the following algorithms: DrQV2 [46], Advantage Weighted Actor-Critic (AWAC) [47], Implicit Q-Learning (IQL) [48], and Coarse-to-fine Deep Q-Network (CQN) [49]. We note that BiGym tasks can be extremely challenging for RL algorithms due to their sparse reward, partial observations, and complex dynamics. To provide a reference for future studies, we provide the results of all the methods as-is with the common set of hyperparameters, instead of tuning their performance for individual BiGym tasks.\nWe provide experimental results and discussions in Appendix C."}, {"title": "5 Discussions", "content": "Opportunities and Future Works. BiGym presents various future research opportunities, including but not limited to: (1) exploring better network architectures for approximating multi-modal noisy human demonstrations; (2) studying better belief estimation mechanisms for the POMDP in the mobile manipulation context; (3) investigating better collaboration modes between arms on mobile platforms; (4) whole-body motion planning which improve the efficiency and performance of mobile agents while navigating in cluttered environments; (5) better methodology for both locomotion and manipulation control with humanoids. We aim to actively maintain and continually improve BiGym to push for the advances of the field.\nConclusion. We introduce BiGym, a new and challenging benchmark for demo-driven mobile bi-manual manipulation. BiGym covers 40 challenging tasks of mobile bi-manual manipulation, ranging from simple target reaching to complex dishwasher manipulation tasks. Built upon the humanoid embodiment of Unitree H1 robot, BiGym allows users to flexibly customise the action modes: the whole-body mode and the bi-manual mode. Furthermore, we provide multi-modal and noisy human-collected demonstrations for all BiGym tasks, exhibiting realistic trajectories compared to synthetic ones generated by motion planners. In our experiments, we validate the usability of BiGym by benchmarking state-of-the-art IL and RL algorithms."}, {"title": "A Additional Simulation Details", "content": "In this section, we provide additional details about BiGym.\nObservation Spaces. For image observations, we allow users to specify the resolution of the images, where the default resolution is 84\u00d784. Higher resolution may allow learning better policies, but we find the default value works across tasks. In the whole-body mode, the proprioception state $s_{proprio} \\in R^{76} = {s_{qpos}, s_{qvel}, s_{grip}}$, where $s_{qpos} \\in R^{37}$ is the joint angle positions of the robot, $s_{qvel} \\in R^{37}$ is the corresponding velocities, and $s_{grip} \\in R^{2}$ is the gripper opening amount of both grippers. On the contrary, the bi-manual mode greatly simplifies the locomotion by replacing the lower-body control with a predefined controller, i.e., a floating base. This reduces the dimension of $s_{qpos}^{wb}$ and $s_{qvel}^{wb}$ to $s_{qpos}^{bm} \\in R^{29}$ and $s_{qvel}^{bm} \\in R^{29}$. Furthermore, an additional state $s_{base} = (x, y, z, \\theta) \\in R^{4}$ is included in $s_{proprio}$ to indicate the position and orientation of the floating base. As a result, $s_{proprio}^{bm} \\in R^{64} = {s_{qpos}^{bm}, s_{qvel}^{bm}, s_{base}, s_{grip}}$\nAction Spaces. In the whole-body mode where the agent has the full control over the body, an action space $A_{wb} \\in R^{23}$ is defined as $A_{wb} = {A_{arms}, A_{legs}, A_{torso}, A_{grip}}$, where $A_{arms} \\in R^{10}$ controls both arms, $A_{legs} \\in R^{10}$ controls the legs, $A_{torso} \\in R^{1}$ controls the main torso joints, and $A_{grip} \\in R^{2}$ controls the opening amount of grippers. In bi-manual mode, the user controls the floating base instead of the leg joints. Therefore the action space becomes $A_{bm} \\in R^{16} = {A_{arms}, A_{base}, A_{grip}}$, with $A_{base} \\in R^{4}$ controlling the delta actions $(\\delta x, \\delta y, \\delta z, \\delta \\theta)$ of the base.\nSimulation Performance. We present the simulation speed in Figure 5. The benchmark was done on a headless server of NVIDIA L4 GPU and Intel Xeon Gold 6438Y+ CPU, in a single process. Benefiting from the highly optimised MoJoCo engine, BiGym runs at around 400FPS to 1400FPS depending on the number of cameras. The performance could be further improved by using parallel environments or MuJoCo XLA, which speeds up the execution with XLA just-in-time compilation."}, {"title": "B Details of Task Success Detectors", "content": "In this section, we detail the definitions of all task success detectors.\nReach Target Tasks.\n(1) reach_target_single: The distance from the robot left wrist to the target is smaller than a tolerance value. The default tolerance value is 0.1.\n(2) reach_target_multi_modal: The distance from either the robot left wrist or the right wrist is smaller than a tolerance value. The default tolerance value is 0.1.\n(3) reach_target_dual: The distance from the left wrist and the right wrist to their corresponding goals are smaller than a tolerance value. The default tolerance value is 0.1."}, {"title": "C Experiments", "content": "C.1 Implementation Details\nWe implemented all algorithms using PyTorch [50]."}, {"title": "C.2 Results and Discussions", "content": "In Table 2, we provide the performance of IL and demo-driven RL methods on 40 BiGym tasks. Overall, we observe that BiGym tasks are challenging and pose a variety of unique and interesting challenges for future researches. We outline our observations as below:\nThe mobile manipulation of articulated or rigid-body objects is challenging for the state-of-the-art algorithms. BiGym has presented a series of tasks that involve interactions with articulated or rigid-body objects, which typically require high-precision manipulation, e.g., move_two_plates, cupboards_open_all, and stack_blocks. When coupled with the mobile base, these tasks become more challenging because (i) accurately measuring the grasping poses while moving is hard, and (ii) correctly estimating the posterior distribution of the states given partial history information of a POMDP is difficult. For instance, while we observe that ACT and Diffusion Policy achieve the overall best performance across all tasks, they still struggle in the seemingly simple tasks, e.g., stack_blocks, which requires the agent to pick 3 cubes, and stack them to a target region located on the other side of the table. We believe a more robust system with stronger memory mechanisms to track the \u201cbeliefs\", i.e., estimating the posterior distributions of the states, is necessary to solve such challenging BiGym tasks.\nThe long-horizon tasks in BiGym requires both task and motion planning of the agent. BiGym introduces a series of long-horizon tasks, e.g., dishwasher_unload_cups_long and put-cups. All algorithms fail on these tasks. Intuitively, these tasks are composed of multiple sub-tasks, and the difficulty level of achieving these long-horizon tasks grows exponentially at the same time. As model-free agents, our baselines are not capable of performing task-level reasoning. Hierarchical methods [12] could work as a better policy representation for these tasks. We leave it for future study.\nThe complex policy space of BiGym requires carefully designed agent architectures. We observe that almost on all tasks, ACT and Diffusion Policy achieves superior performance to BC and demo-driven RL baselines. We hypothesize this is because both ACT and Diffusion Policy utilise powerful policy classes based on generative representation learning, i.e., CVAE and Diffusion models, and they also use expressive network architecture such as transformers or UNets. In contrast, BC and all demo-driven RL approaches use simple CNN + MLP architectures. It is likely that these weaker architectures struggle to deal with the complex multi-modal noisy demonstrations introduced in BiGym. We believe this can motivate future research on finding appropriate policy representations for mobile bi-manual manipulation.\nDemo-driven RL approaches struggle with the complex task space and sparse reward in BiGym. We observe that demo-driven RL algorithms fail on most of the BiGym tasks. For instance, CQN [49], which exhibits strong performance on fixed single-arm demo-driven RL setups, fails to solve most of the BiGym tasks. It is notable that all RL algorithms only achieve non-zero success rates on simple\"\n    }"}]}