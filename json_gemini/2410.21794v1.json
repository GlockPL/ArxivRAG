{"title": "Inverse Attention Agent for Multi-Agent System", "authors": ["Qian Long", "Ruoyan Li", "Minglu Zhao", "Tao Gao", "Demetri Terzopoulos"], "abstract": "A major challenge for Multi-Agent Systems is enabling agents to adapt dynamically to diverse environments in which opponents and teammates may continually change. Agents trained using conventional methods tend to excel only within the confines of their training cohorts; their performance drops significantly when confronting unfamiliar agents. To address this shortcoming, we introduce Inverse Attention Agents that adopt concepts from the Theory of Mind, implemented algorithmically using an attention mechanism and trained in an end-to-end manner. Crucial to determining the final actions of these agents, the weights in their attention model explicitly represent attention to different goals. We furthermore propose an inverse attention network that deduces the ToM of agents based on observations and prior actions. The network infers the attentional states of other agents, thereby refining the attention weights to adjust the agent's final action. We conduct experiments in a continuous environment, tackling demanding tasks encompassing cooperation, competition, and a blend of both. They demonstrate that the inverse attention network successfully infers the attention of other agents, and that this information improves agent performance. Additional human experiments show that, compared to baseline agent models, our inverse attention agents exhibit superior cooperation with humans and better emulate human behaviors. The code is available here.", "sections": [{"title": "1 Introduction", "content": "Multi-Agent Reinforcement Learning (MARL) has significantly advanced the study of complex, interactive behaviors in multi-agent systems, allowing intricate modeling of scenarios involving multiple autonomous agents. However, creating an ad-hoc agent that excels with various types of teammates and opponents poses a significant challenge. Current methods suffer a limitation: while agents trained together demonstrate proficient coordination, their performance deteriorates markedly when paired with unfamiliar agents.\nTo address this challenge, we delve deeper into multi-agent collaboration by incorporating a cognitive perspective, specifically through the modeling of attention and Theory of Mind (ToM) [Bratman, 1987] within the MARL framework. Unlike classical ToM research, which focuses on attributing mental states such as beliefs and desires, our model shifts to the crucial yet less-emphasized component of attention. Our methodology adopts a mentalistic approach, explicitly modeling the internal attentional state of agents using an attention recognition neural network that can be trained end-to-end in combination with components of MARL. Contrary to traditional ToM modeling approaches that rely heavily on Bayesian inference to handle mental state transitions [Baker et al., 2009, Kleiman-Weiner et al., 2016, Shum et al., 2019, Gao et al., 2020, Tang et al., 2022], our method maintains the ontology of these states while focusing on the direct modeling of agents' attentional mechanisms. This shift from Bayesian methods to more direct, attention-based modeling opens new pathways for understanding and enhancing agent interactions in complex environments."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Theory of Mind and Attention", "content": "Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to oneself and others [Bratman, 1987]. It allows for tailored strategies to be generated in multi-agent scenarios, where one can reason about what other players are doing and determine one's action accordingly. ToM has been an active area of research in multi-agent systems, with the goal of designing agents that coordinate more like humans. Previous work has primarily utilized Bayesian approaches to explicitly model beliefs, desires, and intentions [Shum et al., 2019, Kleiman-Weiner et al., 2016, Wu et al., 2021], providing a principled framework for inferring and updating beliefs about other agents' mental states based on observed behavior and actions. To avoid the complexity of recurrently reasoning about each others' mental state, Tang et al. [2020] and Stacy et al. [2021] proposed frameworks based on the idea of shared agency, relying on coordinating group-level mental states and establishing a shared understanding of the task and goals among agents. Departing from Bayesian methods, Rabinowitz et al. [2018] proposed to integrate ToM reasoning directly into the neural network architecture, aiming to learn representations of other agents' mental states in a data-driven manner.\nBuilding upon previous frameworks, our work addresses three key aspects: First, we develop our model around the idea that ToM is not merely concerned with beliefs, desires, and intentions, but is a human-unique ability to be sensitive to what others are attending to. While attention as a critical mental state is often neglected in ToM research, our work aims to address this gap by explicitly modeling attention mechanisms. Second, we model cooperation from an individualistic perspective to encourage more flexible behavior generation. In this way, instead of shared agency, our agents maintain an individualistic perspective while coordinating with teammates. Third, we deviate from traditional Bayesian approaches to modeling ToM and instead develop an end-to-end neural network-based training, thus allowing for more flexibility and generalizability."}, {"title": "2.2 Ad-Hoc Teaming", "content": "Ad-hoc collaboration is defined as the challenge of enabling autonomous agents to effectively coordinate with previously unknown teammates, without any prior opportunities for coordination or agreement on strategies [Stone et al., 2010]. Addressing this problem necessitates agents to model the behavior of their teammates and subsequently select actions that facilitate effective collaboration, while simultaneously adapting to changes or new information that emerges during the interaction. A prominent approach to modeling teammate behavior is type-based reasoning, which represents teammates as belonging to hypothesized behavior types [Barrett et al., 2017]. Alternatively, neural network-based techniques have been proposed to infer teammate types from observations [Rabinowitz et al., 2018, Rahman et al., 2021, Xie et al., 2021]. Once teammate models are obtained, agents perform downstream action planning with techniques such as Monte Carlo tree search [Barrett et al., 2014, Albrecht and Stone, 2019] and meta-learning action selection [Zintgraf et al., 2021]. Adapting the agent's behavior based on new information about teammates is also crucial in sustaining the collaboration. Addressing this, Macke et al. [2021] employed communication between agents and Lupu et al. [2021] proposed a method to generate diverse training trajectories to improve generalization to novel teammates.\nIn our work, we tackle the ad-hoc problem by leveraging the attention mechanism. We argue that the instability issues in ad-hoc settings typically arise because agents fail to comprehend unseen states, consequently making it difficult to generalize the trained policies. However, by implementing the attention mechanism, our agents are capable of selectively focusing on relevant aspects of the environment. This focused attention helps maintain consistency in decision-making across different scenarios [Cheng et al., 2023]. Thus, the attention mechanism serves as a method for filtering information, which is crucial for achieving generalizability when interacting with previously unseen teammates."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Markov Games", "content": "We consider multi-agent Markov Decision Processes (MDPs) [Littman, 1994], where the state transitions and rewards depend on the actions of all agents. Formally, a Markov game for N agents is defined by a set of states S, a set of actions $A_i$ for each agent i, and a transition function $T: S \\times A_1 \\times \\dots \\times A_N \\rightarrow \\Delta(S)$, where $\\Delta(S)$ denotes the set of probability distributions over S. Each agent i receives a reward as a function of the state and action of all agents, denoted by $R_i: S \\times A_1\\times \\dots \\times A_N \\rightarrow \\mathbb{R}$. The goal of each agent is to maximize its expected discounted reward $\\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t R_i(s_t, a_{1,t},..., a_{n,t})]$, where $\\gamma \\in [0, 1]$ is the discount factor and $s_t$ denotes the state at time t. In the multi-agent reinforcement learning (MARL) context, each agent typically learns a policy $\\pi_i: S \\rightarrow \\Delta(A_i)$ that specifies the action distribution given the current state, aiming to optimize its own long-term payoff in the environment defined by the Markov game."}, {"title": "3.2 Gradient Field Representation in Multi-Agent Systems", "content": "The Gradient Field (GF) representation in multi-agent systems has proven to yield better policy learning [Wu et al., 2022, Long et al., 2024]. Instead of using raw observation of the environment, the GF is a higher level representation which enhances the ability of agents in the environment. The key idea is that a Denoised Score Matching (DSM) [Song et al., 2021] generative model aims to learn the gradient field of a log-data-density; i.e., the score function. Given samples $\\{x_i\\}_{i=1}^N$ from an unknown data distribution $\\{x_i \\sim P_{data}(x)\\}$, the goal is to learn a score function to approximate $\\nabla_x \\log P_{data}(x)$ via a score network $s_\\theta(x) : \\mathbb{R}^{|x|} \\rightarrow \\mathbb{R}^{|x|}$, by adopting an extension of DSM that estimates a time-dependent score network $s_\\theta(x, t) : \\mathbb{R}^{|x|} \\times \\mathbb{R}^1 \\rightarrow \\mathbb{R}^{|x|}$ to denoise the perturbed data from different noise levels simultaneously:\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\mathcal{U}(\\epsilon,T)} \\mathbb{E}_{x \\sim q_{\\sigma(t)}(x|x_0)} \\Big[ \\Big\\|s_\\theta(x_t, t) - \\frac{1}{\\sigma^2(t)} (x - x_0) \\Big\\|^2 \\Big]$,  (1)\nwhere $T, \\epsilon$, $x(t) = \\sigma^2(t)$, $\\sigma(t) = \\sigma_\\epsilon^t$, and $\\sigma_0$ are hyper-parameters. The optimal time-dependent score network holds $s_\\theta(x, t) = \\nabla_x \\log q_{\\sigma(t)}(x)$ where $q_{\\sigma(t)}(x)$ is the perturbed data distribution:\n$q_{\\sigma(t)}(x) = \\int q_{\\sigma(t)}(x|x) p_{data}(x)dx$.  (2)\nWhen learning from different offline datasets $D_{N}$, N gradient field (GF) representation functions $s_{\\theta_i}$ are learned, resulting in the observation representations $s_{\\theta_1}(o)$, $s_{\\theta_2}(o)$,..., $s_{\\theta_N}(o)$. These GF"}, {"title": "4 Problem Statement", "content": "In the context of a fully observable Multi-agent environment $E(N, \\{A_i\\})$ under the MDP settings, where agents types are fixed and can make observations (previous action included) of itself and also of other agents, we aim to learn a decentralized policy $\\pi$. This policy, trained with a single group of agents, should achieve optimal long-term pay off not only with the trained agents but also when interacting with previously unseen agents."}, {"title": "5 Method", "content": "In this section, we introduce the inverse-attention agent, which acts based on its attention weights of goals and updates the weights based on the inferred goal weights of other agents. The agent is trained in three phases. The first phase involves applying the self-attention mechanism to the policy function [Vaswani et al., 2017] so that an agent acts based on the weights of the attentions. The second phase is to infer the attention of other agents of the same type using the inverse attention network. By placing itself in the position of the other agents, the agent infers their attention weights, gaining insights into their attention and behavior. The final phase involves training the inverse attention agent. The agent uses the inferred attention weights from the previous phase to update its own attention weights, consequently leading to changes in its final actions. The following subsections present the details of our algorithm. The overall pipeline is shown in Figure 1. The architecture of the inverse attention agent policy network is shown in Figure 2."}, {"title": "5.1 Self-Attention Structure", "content": "We incorporate a self-attention mechanism into the policy network in order to explicitly model the agent's mental states through attention weights assigned to different goals. These mental states are crucial, as they influence the agent's final action, and altering the attention weights will lead to changes in the action. Additionally, during this phase, we prepare the data required for training the inverse attention network. The agent trained with this structure is referred to as the Self-Att agent."}, {"title": "5.2 Attention Inference", "content": "We train the inverse attention network using the attention inference dataset D. This network operates inversely to the self-attention network: It outputs the predicted attention weights based on the given goals and past actions. An agent then applies the inverse attention network to other agents of the same type to infer their attention weights. This allows the current agent to infer the attentions of other agents without knowing the ground truth values of their attention weights.\nInverse Attention Network: We propose the inverse attention network, designed to infer the attention of agents of the same type from their perspective. This function has the following structure:\n$\\{W_{i,1}, W_{i,2}, ..., W_{i,N}\\} = IW_i(o_i) = attention(o_{i,i}, f_i(O_{i,j})), \\forall j \\neq i$.  (5)\nSimilar to the attention weight function, the $IW_i$ function predicts the attention weight by applying self-attention between the embedding of self-information $o_{i,i}$ and the embedding of other goals $f_i(O_{i,j})$. Using the dataset D, which is agent's own data collected during the training process of Phase 1, we train the $IW_i$ network by minimizing the following loss function:\n$Loss = L_2(\\{W_{i,1}, W_{i,2}, ..., W_{i, N} \\}, IW_i(o_i))$.  (6)"}, {"title": "5.3 Inverse Attention Agent", "content": "We construct the inverse attention agent (Inverse-Att) by concatenating its original attention weights with the inferred weights. This concatenated vector is then passed through a fully connected layer, $UW_i$, to update the attention weights. This mechanism enables an inverse attention agent to adapt its attention weights and consequently adjust its actions based on the attentions of other agents. The process is represented by the following equation:\n$\\{W_{i,1}, W_{i,2}, ..., W_{i,n}\\} = UW_i(W_i(f_i(o_i)), IW_i(o_j)), \\forall j\\neq i \\text{ and } a_j \\text{ is the same type as } a_i$.  (8)\nwhere $o_j$ is the observation from the other agent in the previous step, $IW_i(o_j)$ outputs the inverse attention estimation of agent j from agent i, and $UW_i$ is the weight updating model, which concate- nates the estimated attention weights from other agents of the same type with the original weights $W_i(f_i(x))$. The concatenated weights are then passed through a one-layer fully connected network to update the attention weights to w. Given that the attention weight embedding is a high-level representation, only a shallow network is required.\nFor fast convergence, the $UW_i$ are initialized to 1 when connected to the original attention weight and 0 when connected to the other agents' attention weights. This initialization guarantees that the initial output aligns with that of the Self-attention agent, maintaining unchanged attention weights. Commencing the optimization process from this point enhances effectiveness.\nThe complete policy network module for the inverse attention agent is outlined as follows:\n$\\pi_i(o_i) = h_i(UW_i(W_i(f_i(o_i)), IW_i(o_j)), V_i(f_i(x)))), \\forall j \\neq i \\text{ and } a_j \\text{ is the same type as } a_i$.  (9)"}, {"title": "6 Experiments", "content": "We adopted MAPPO [Yu et al., 2022] as our MARL training scheme. We evaluated the adaptive performance of the policies in collaboration with human participants across all these tasks and found that the inverse attention agent performed well in these environments. Additionally, we compared our agents' behavior with human behavior. Our results indicate that the inverse attention agent is more adaptable to a wider variety of agents."}, {"title": "6.3 Quantitative Results", "content": "In this section, we delve into the quantitative results across all five games. We conducted training for MAPPO, IPPO, MAA2C, TOM2C* and Self-Att agents over 40 million steps across all scales. For training the Inverse-Att agent, Phase 1 encompassed 20 million steps, followed by another 20 million steps for Phase 3. Phase 2 training involved offline learning, obviating the need for interaction with the environments. Subsequently, evaluations were carried out over $2 \\times 10^6$ steps of mix-and-match for all five games.\nWe tested all the methods across all environments, with the full results presented in Table 1. The Inverse-Att method demonstrates significant improvement over all the other methods across the five games in cooperative, competitive, and mixed settings, as well as with both individual and team rewards. This highlights the superior adaptability and generality of Inverse-Att when dealing with unseen agents. Self-Att ranks second, while the remaining baselines perform similarly and rank below both Inverse-Att and Self-Att. Due to the similar performance of MAPPO, IPPO, MAA2C, and ToM2C*, and computational limitations, in the following sections we will focus on MAPPO, Self-Att, and Inverse-Att in the Spread, Adversary, and Grassland games."}, {"title": "6.4 Impact of Different Population Scales", "content": "We tested the scalability of Inverse-Att by comparing with MAPPO and Self-Att in the Spread, Adversary, and Grassland games in the MPE environment.\nIn the Spread game, evaluations were conducted across three scales: 2, 3, and 4 agents. The results are detailed in Table 2. Meanwhile, in the Adversary and Grassland games, evaluations were conducted across scales of 2-2, 3-3, and 4-4 for both sheep and wolves. The outcomes are presented in Table 3 and Table 4, respectively.\nAcross all three games, notable enhancements were evident when comparing the outcomes of MAPPO and Self-Att, underscoring the efficacy of the self-attention network. The Inverse-Att method exhibited superior performance across the tested environments, particularly in cooperative-related games such as Spread and Grassland. This superiority likely stems from the attention inference being exclusively applied to agents of the same type (teammates), a factor more critical in cooperative settings than in fully competitive games."}, {"title": "6.5 Human Experiments", "content": "To further assess the adaptive capabilities of the Inverse-Att agents, we conducted human experiments to evaluate their performance in collaboration with human players. Five participants engaged in Spread, Grassland and Adversary games across the following scales: Spread (3 agents), Adversary (3- 3), and Grassland (3-3), assuming five distinct roles (agent for Spread; sheep and wolf for Adversary and Grassland). In each role, participants cooperated with teammate agents of the same type trained"}, {"title": "6.6 Impact of Multiple Inverse-Att Agents", "content": "In multi-agent systems, incorporating Theory of Mind (ToM) can introduce intricate dynamic interac- tions. This complexity is amplified when the inference is reciprocal, potentially creating a cognitive loop that exacerbates uncertainty and leads to unpredictable or suboptimal decision-making outcomes. Our objective is to explore the impact of increasing the number of Inverse-Att agents on emergent behavior patterns.\nWe initiated the investigation by replacing randomly sampled MAPPO agents, trained with different seeds, with Inverse-Att agents gradually. Evaluations were conducted over $2 \\times 10^6$steps per group in the Spread game, across three different scales: 2, 3, and 4 agents. The total rewards earned by the teams are summarized in Table 6. We observe a nonlinear marginal return pattern as more Inverse-Att agents are introduced into the game at all three scales. This observation underscores the effectiveness of our Inverse-Att agent in cooperating effectively with other attention-aware agents."}, {"title": "6.7 Inverse Attention Network Prediction Accuracy", "content": "We collected $2 \\times 10^6$ steps of weights-observation pair from one Self-Att agent per environment, considered as the attention ground truth of those agents. We then inputed the observation into our inverse attention network and compared the predicted weights with the ground truth across the spread, adversarial, and grassland environments at scales \\{spread: 3, adversarial: 3 - 3 and grassland: 3 -3\\}."}, {"title": "7 Conclusion", "content": "We have introduced the Inverse Attention Agent, which operates by leveraging attention weights to infer the attention of other agents. It then utilizes these inferred weights to adjust its own attention weights, thereby fine-tuning its final actions. The Inverse Attention Agent demonstrates superior adaptability to unseen agents, as evidenced by interactions with policies trained using various methods, as well as with human participants.\nLimitations and future work: Currently, the attention inference is limited to the same type of agents. In future work, we would like to model the ToM of agents of different types. We will also develop an attention model for the $UW_i$ network that can accommodate an arbitrary number of inferred attention weights."}, {"title": "8 Broad Impact", "content": "The board impact of this work is that it provides a new and promising approach to MARL. Inverse- Attention can be used to train agents that can adapt to changing teammates. This work has the potential to make a significant impact on the development of autonomous systems that can interact with other agents in complex and dynamic environments, such as for robotics and autonomous driving applications."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Environmental Details", "content": "Reward: In the Spread scenario, during training, agents are awarded +100 reward for occupying a landmark at every timestep. For reward engineering, the agent's reward will be deducted proportional to the minimum distance to the landmarks $R_{distance} = 0.2 min(Distance(self, Landmark_{all}))$ In the Adversary scenario, wolves are awarded with +100 reward for every sheep it catches, while sheep are awarded with -100 reward for every time it gets caught. The wolves reward will be deducted proportional to the minimum distance to the sheep $R_{distance} = 0.2 * min(Distance(self, Sheep_{all}))$. In the Grassland scenario, wolves are awarded a +5 reward for every sheep they catch, while sheep are awarded a -5 reward for every time they get caught and +2 reward for every landmark they occupy. The wolves' reward will be deducted proportional to the minimum distance to the sheep $R_{distance} = 0.2 min(Distance(self, Sheep_{all}))$. On the other hand, the sheep's reward will be deducted propor- tional to the minimum distance to the landmarks $R_{distance} = 0.2 min(Distance(self, Landmark_{all}))$. For Navigation environment, the group reward is defined as +5 reward for every landmark an agent occupied. For Tag environment, when any wolf catches a sheep, all wolves received +5 points and all sheep receive -5.\nObservation: In the Spread scenario, MAPPO and Self-Att agents receive information regarding their own position, velocity, other agents' positions, and landmarks' positions. They use these information to generate gradient field \\{velocity, gf \\} as observation. Inverse-Att agents also receive the past actions and past observations of their teammates and have \\{ teammate past action, teammate past observation \\} in addition to the gradient fields. In the Adversary scenario, the observation is the same except for the omission of $gf_{landmarks}$, and Grassland, Navigation and Tag have exactly the same observation space.\nAction: The agent's action is represented by a two-dimensional continuous vector, which describes the force applied to an entity, considering both magnitude and angular direction."}, {"title": "A.2 Training Details", "content": "We present the hyperparameters for gradient field as well as for all agent training."}, {"title": "A.3 Gradient Field Synthetic Data Generation", "content": "We use synthetically generated data to train the Agent Gradient Field as well as the Boundary Gradient Field. Detailed instructions for synthetic data generation are as follows:\nEntity Gradient Field: We randomly generated 10,000 two dimensional points within a 2 \u00d7 2 grid as one entity's position. We then randomly generated another entity's position close to the previous position such that the $L_1$ distance is less than 10-5. We mark this gf representation as $gf_e$. This gf function takes in the agent's location and a relative position of another entity.\nBoundary Gradient Field: We randomly generated 10000 positions $(x,y) \\in [-0.8,0.8] \\times [-0.8,0.8]$. We mark this gf representation as $gf_{wall}$. This gf function only takes in the position of the agent's position."}, {"title": "A.4 Gradient Field Representation of the Environment", "content": "For the spread, adversary and grassland environments, we applied entity gradient field for all the other entities around that agent as entity attentions, as well as a boundary gradient field used as the environment attention. Thus we transform the observation from $\\{o_{i,1}, o_{i,2}, ..., o_{i,n}\\}$ which is the information of N entities to $\\{gf_e(o_{i,1}), gf_e(o_{i,2}),..., gf_e(o_{i,n}), gf_{wall}(o_{i,i})\\}$."}, {"title": "A.5 Qualitative Results", "content": "Figure 5 provides example screenshots of a representative match involving Inverse-Att agents in the Spread scenario. In these images, the blue balls represent Inverse-Att agents, while the small black balls are landmarks. The left figure shows the initial state of all agents. The middle figure demonstrates the Inverse-Att agents successfully navigating to their respective landmarks. The right"}, {"title": "A.6 Human Experiment Details", "content": "There are five experiments in total: Spread, Adversary (human plays wolf), Adversary (human plays sheep), Grassland (human plays wolf), Grassland (human plays sheep). In each experiment, human players will cooperate with three types of agents, MAPPO, Self-Att, and Inverse-Att, for five rounds. The episode rewards are averaged for these rounds. Human players are not informed about the type of agents that they are playing with. In the Adversary and Cooperate games, the opponents are always MAPPO agents.\nFive participants (4 males and 1 females) participated in this experiment. All were between the ages of 21 and 28 with normal or corrected-to-normal visual acuity. All participants were given the Experiment Instructions below and received rewards (free food) for their participation. All participants were instructed to use the UP, DOWN, LEFT, and RIGHT keys on the keyboard to control their movements."}, {"title": "A.7 Computational Resources", "content": "Hardware specifications: All experiments are run on servers/workstations with the following configurations:\n\u2022 128 CPU cores, 692GB RAM\n\u2022 128 CPU cores, 1.0TB RAM\n\u2022 32 CPU cores, 120GB RAM\n\u2022 32 CPU cores, 120GB RAM\n\u2022 32 CPU cores, 120GB RAM\n\u2022 24 CPU cores, 80GB RAM, 1 NVIDIA 3090 GPU.\n\u2022 24 CPU cores, 80GB RAM, 1 NVIDIA 3090 GPU.\n\u2022 24 CPU cores, 80GB RAM, 1 NVIDIA 3090 GPU.\n\u2022 24 CPU cores, 80GB RAM, 1 NVIDIA 3090 GPU.\nAll experiments can be run on a single server with 24 CPU cores, 80GB RAM, 1 NVIDIA 3090 GPU."}]}