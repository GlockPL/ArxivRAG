{"title": "RegNLP in Action: Facilitating Compliance Through Automated Information Retrieval and Answer Generation", "authors": ["Tuba Gokhan", "Kexin Wang", "Iryna Gurevych", "Ted Briscoe", "Mohamed Bin Zayed"], "abstract": "Regulatory documents, issued by governmental regulatory bodies, establish rules, guidelines, and standards that organizations must adhere to for legal compliance. These documents, characterized by their length, complexity and frequent updates, are challenging to interpret, requiring significant allocation of time and expertise on the part of organizations to ensure ongoing compliance.Regulatory Natural Language Processing (RegNLP) is a multidisciplinary subfield aimed at simplifying access to and interpretation of regulatory rules and obligations. We define an Automated Question-Passage Generation task for RegNLP, create the ObliQA dataset containing 27,869 questions derived from the Abu Dhabi Global Markets (ADGM) financial regulation document collection, design a baseline Regulatory Information Retrieval and Answer Generation system, and evaluate it with RePASs, a novel evaluation metric that tests whether generated answers accurately capture all relevant obligations and avoid contradictions.", "sections": [{"title": "1 Introduction", "content": "Regulatory documents are formal quasi-legal texts issued by governmental regulatory bodies that set out rules, guidelines, and standards that organizations must follow to ensure compliance. These documents typically cover a wide range of topics, from environmental and financial compliance to workplace safety and data protection. Characterized by their complex and precise language, they are designed to cover all potential scenarios, but can be challenging to interpret without specialized knowledge. Additionally, regulatory documents are frequently updated to reflect new laws, technological changes, or shifts in societal norms, necessitating continuous monitoring and adaptation by organizations to meet new compliance requirements. Compliance with these regulations requires considerable investment in terms of time and expertise. For example, between 1980 and 2020, the U.S. public and private sectors spent an estimated 292.1 billion hours complying with 36,702 regulations, accounting for approximately 3.2% of total annual working hours (Kalmenovitz, 2023). Moreover, compliance errors can lead to severe penalties, as, for example, demonstrated by the recent C$7.5 million fine levied against the Royal Bank of Canada by FINTRAC for failing to report suspicious transactions(Bloomberg, 2023). Regulatory Natural Language Processing (RegNLP) is a recent multidisciplinary subfield designed to simplify access to and interpretation of regulatory rules and obligations, reducing errors and enhancing operational efficiency.\nThis paper makes several significant contributions to RegNLP:\n1. Automated Question-Passages Generation:\nWe introduce a framework that employs Large Language Model (LLM) agents to automatically generate questions, with Natural Language Inference (NLI) integrated at the validation stage. This framework can be applied to specialized regulatory document collections to create datasets for the development and evaluation of question-answering and passage retrieval systems.\n2. ObliQA:\nWe have developed a multi-document, multi-passage QA dataset \u00b9 specifically developed to facilitate research in Reg-NLP. It consists of 27,869 questions and their associated source passages, all derived from the full collection of regulatory documentation provided by Abu Dhabi Global Markets (ADGM)2, the authority overseeing financial services in the UAE's free economic zones."}, {"title": "2 Related Work", "content": "Recent RegNLP research has advanced significantly using various methodologies and datasets. Lau et al. (2005) utilized information retrieval and extraction on accessibility regulations from the US and Europe, formatted into unified XML for semi-structured data handling. Kiyavitskaya et al. (2008) employed the Cerno framework for semantic annotation to automate extracting rights and obligations from U.S. HIPAA and Italian laws. Chalkidis et al. (2018) introduced a hierarchical BiLSTM model for extracting obligations from legal contracts, demonstrating superior performance over traditional models. Nair et al. (2018) developed a pipeline for annotating global import-export regulations, enhancing compliance workflows. Chalkidis et al. (2021) proposed a two-step document retrieval process for EU/UK legislative compliance using BERT-based models, handling long document queries. Abualhaija et al. (2022) presented an automated question-answering system using BERT, achieving high accuracy in identifying relevant texts from European regulations, including the General Data Protection Regulation.\nThese studies highlight the potential of recent NLP techniques and tools for improving the retrieval and extraction of regulatory information. Our work extends RegNLP by firstly developing a novel dataset in the financial domain from the complete document collection of a single regulatory authority, and secondly defining a novel regulatory question-answering (QA) task and evaluation metric."}, {"title": "2.2 Synthetic Data Generation with LLMS", "content": "Recent research studies have demonstrated the versatile capabilities of LLMs in enriching QA datasets across diverse domains. Alberti et al. (2019) pioneered methods for generating synthetic QA corpora, using roundtrip consistency for validation and achieving state-of-the-art performance on tasks such as SQuAD2 and NQ. Maatouk et al. (2023) introduced a zero-shot learning approach for neural passage retrieval, using synthetic question generation and hybrid term-neural models to enhance retrieval performance without extensive domain-specific data. Abdullin et al. (2023) focused on synthetic dialogue dataset generation, automating dialogues for linear programming problems using LLMs, thereby aiding in training conversational agents. Maatouk et al. (2023) developed TeleQnA, specifically evaluating LLMs' telecommunications knowledge, while Tihanyi et al. (2024) introduced CyberMetric for assessing cybersecurity knowledge. These datasets enable evaluation and further training of LLMs to enhance their domain-specific capabilities. Additionally, multilingual question generation for health education, as explored by Ackerman and Balyan (2024), highlights the effectiveness of LLMs in diverse linguistic contexts. Collectively, these studies underscore the transformative potential of LLMs in refining QA datasets tailored to specific domains, fostering the development of more specialized and effective AI tools."}, {"title": "2.3 Retrieval-Augmented Generation", "content": "RegNLP can benefit from advances in retrieval-augmented generation (RAG) (Lewis et al., 2020) and other related technologies to simplify access to regulatory documents and improve compliance. Regulatory documents, known for their complexity and frequent updates, pose unique challenges that can be mitigated through methods that leverage the strengths of NLP and information retrieval systems. RAG has significantly improved the accuracy, efficiency, and trustworthiness of LLMs by integrating external, contextually relevant and up-to-date information. Notable approaches include: Self-RAG (Asai et al., 2023) enhances response quality by incorporating self-reflection mechanisms; RAFT (Zhang et al., 2024) optimizes domain-specific knowledge retrieval by filtering out irrelevant data and improving reasoning chains; RobustRAG (Xiang et al., 2024) and RQ-RAG (Chan et al., 2024) tackle challenges such as retrieval corruption and ambiguous queries; PipeRAG (Jiang et al., 2024a) improves performance by reducing retrieval latency. However, these studies do not consider regulatory documents so we are interested in testing the ability of RAG methods for solving the QA task for regulatory questions."}, {"title": "3 ObliQA: Obligation Based Question Answering Dataset", "content": "The ObliQA - The Obligation Based Question Answering Dataset creation process involves three main steps: Data Collection, Question Generation, and Question-Passages Validation via NLI, as illustrated in Figure 1."}, {"title": "3.1 Data Collection", "content": "The regulatory documents provided by ADGM are in .docx format, each ranging from 30 to 100 pages and written in a broadly legalistic style. These documents feature complex internal structures, including numerous (sub)subsections, numbered clauses, and cross-references within and across documents. In total, there are 40 documents comprising approximately 640,000 words.\nDocuments Standardization: To ensure the integrity of the dataset, the documents underwent a meticulous semi-automatic restructuring process. This step is critical due to the diverse formats employed by different departments in the preparation of regulations. Each document is parsed and transformed into .txt format. Tables within the documents are delineated with \\Table Start and \\Table End tags, while graphical content is transcribed into text using manually-corrected GPT-4 output and encapsulated with \\Figure Start and \\Figure End tags.\nStructuring the Data: Subsequently, the standardized .txt files are converted into a structured JSON format for data manipulation and analysis. Below is an example of the JSON structure:"}, {"title": "3.1.1 Question Generation", "content": "Regulatory documents contain extensive descriptions and titles, which are filtered out in the question generation process to focus solely on essential content.\nInput Definition:\nInputs for Single Passage Questions: Inputs for single passage questions isolate one specific source passage per question, simplifying the context and enhancing the relevance of the generated question.\nInputs for Multi-Passage Questions: For broader queries, inputs encompass multiple source passages. Sixty-nine distinct topics related to finance and regulatory compliance (see subsection A.1 for a sample) guide the aggregation of related regulations into meaningful clusters. These clusters then form the basis for multi-passage questions by randomly grouping a specified number of rules per question.\nGPT-4 Generator: The gpt-4-turbo-1106 model is employed to generate both questions and answers.\nFor single passage questions, the prompt is:\nFor multi-passage questions, the complexity increases, and the prompt is adjusted accordingly:"}, {"title": "3.2 Question-Passages Validation via NLI", "content": "Validating the generated question-passage pairs using Natural Language Inference (NLI) involves assessing the semantic relationship between each question and its corresponding source passages to ensure alignment. The validation process uses the nli-deberta-v3-xsmall model with the passage set as the premise and the question set as the hypothesis. For each passage associated with a question, NLI is applied to evaluate the relationship. The validation results are categorized as follows.\nEntailment: The passages entail the questions being retained.\nContradictions: Passages that contradict the questions are eliminated.\nNeutral: Passages with neutral scores are retained if they are closer to the entailment score but eliminated if they are closer to the contradiction score.\nThe validation process ensures that only passages with strong semantic alignment to the questions are included. This step results in a dataset where questions are linked to passages that provide relevant and accurate information.\nThe final dataset consists of approximately 21k questions linked to single passages and 5k questions linked to multiple passages."}, {"title": "4 RePASs: Regulatory Passage Answer Stability Score", "content": "We propose a reference-free evaluation metric, RePASs, to evaluate generated answers in regulatory compliance contexts. Inspired by recent work on answer attribution to specific sources (Bohnet et al., 2023; Yue et al., 2023), this metric evaluates answers based on three key aspects:\n\u2022 Every answer sentence must be supported by a sentence in the source passage(s).\n\u2022 The answer must not contain any sentences that contradict the information in the source passage(s).\n\u2022 The answer must cover all the obligations present in the source passages, meaning that all critical regulatory obligations should be reflected in the answer.\nThis metric prefers answers that are not only accurate but also comprehensive in covering all relevant regulatory obligations. The metric includes the following steps:\nStep 1: Generation of Entailment and Contradiction NLI Pair Matrices\nWe generate an NLI pair matrix by comparing each passage sentence (premise) with each answer sentence (hypothesis) using the cross-encoder/nli-deberta-v3-xsmall (He et al., 2021a). The model provides three outputs for each pair: probabilities for entailment, contradiction, and neutrality. These probabilities are organized into two matrices: the entailment matrix, which contains the entailment probabilities for each passage-answer sentence pair, and the contradiction matrix, which stores the contradiction probabilities for each sentence pair.\nStep 2: Calculation of Entailment and Contradiction Scores\nWe reduce the size of the entailment and contradiction matrices by selecting the highest probability for each answer sentence across the passage sentences. Matrix reduction ensures that only the most relevant support from or conflict with the source passage is considered. The entailment score is computed by averaging the highest entailment probabilities across all answer sentences. Similarly, the contradiction score is obtained by averaging the highest contradiction probabilities.\n$E_s = \\frac{1}{N} \\sum_{i=1}^N max_j P_{\\text{entailment}}(p_j, a_i)$ (1)\n$C_s = \\frac{1}{N} \\sum_{i=1}^N max_j P_{\\text{contradiction}}(p_j, a_i)$ (2)\nWhere $E_s$ is the Entailment Score, and $C_s$ is the Contradiction Score. N represents the total number of sentences in the generated answer. $P_{\\text{entailment}}(p_j, a_i)$ is the probability that the i-th sentence in the answer ($a_i$) is entailed by the j-th sentence in the source passage ($p_j$). The $\\text{max}$ function selects the highest entailment probability for each answer sentence ($a_i$) across all sentences in the source passage ($p_j$). Similarly, $max_j$ selects the highest probability for each answer sentence across all sentences in the source passage.\nStep 3: Calculation of Obligation Coverage Score\nThe obligation coverage score assesses how accurately the generated answer reflects the obligations present in the source passage(s). First, we create a dataset using an automated pipeline powered by the GPT-4-turbo-1106 model. This pipeline extracted both obligation and non-obligation sentences from regulatory documents. The sentences are verified and labeled as obligations or non-obligations. The dataset was subsequently used to fine-tune a classifier model based on LegalBERT.(Chalkidis et al., 2020) to define the obligatory sentences. Both the source passages and the generated answers are tokenized into individual sentences, enabling sentence-level evaluation.\nFor each obligation sentence detected in the source passage, the system verifies its coverage in the generated answer. This is done by comparing the obligation sentences with those in the answer using the NLI model microsoft/deberta-large-mnli (He et al., 2021b). If any sentence in the answer demonstrates an entailment score exceeding 0.7 when compared to an obligation sentence from the source passage(s), the obligation is considered covered. The obligation coverage score is calculated as the ratio of the number of covered obligations to the total number of obligations present in the source passage(s).\nThe formula for the Obligation Coverage Score is given by:\n$OCS = \\frac{1}{M} \\sum_{k=1}^M (max_l P_{\\text{entailment}}(o_k, a_l) > 0.7)$ (3)\nWhere $OC_S$ represents the Obligation Coverage Score, and M is the total number of obligation sentences in the source passage. $P_{\\text{entailment}}(o_k, a_l)$ denotes the probability that the k-th obligation sentence in the source passage ($o_k$) is entailed by the l-th sentence in the answer ($a_l$). The $max_l$ function selects the highest entailment probability for each obligation sentence across all sentences in the answer. The indicator function returns 1 if the highest entailment score exceeds 0.7, meaning the obligation is considered covered.\nStep 4: Calculation of the Regulatory Passage Answer Stability Score\nThe final step integrates the three components into a single composite score. The score penalizes contradictions and normalizes the result between 0 and 1.\n$RePASS = \\frac{E_s - C_s + OC_s + 1}{3}$ (4)"}, {"title": "5 Regulatory Information Retrieval and Answer Generation Task", "content": "The Regulatory Information Retrieval and Answer Generation (RIRAG) task we have defined breaks the task down into two distinct steps which are evaluated independently. It begins with a passage retrieval step that locates all obligations relevant to a given question within the regulatory documents. This is followed by the answer generation step, which aims to synthesize the retrieved data into precise and informative responses that comprehensively address the regulatory query. In this section, we describe a baseline approach for RIRAG, setting a standard that will support future advances and improvements in the field."}, {"title": "5.1 Passage Retrieval", "content": "We experiment with six retrieval models, which are (1) BM25 (Robertson et al., 1994): the traditional lexical-based model; (2) DRAGON+ (Lin et al., 2023): the State-of-the-Art (SotA) single-vector dense retriever model fine-tuned on MS MARCO (Nguyen et al., 2016); (3) SPLADEv2 (Formal et al., 2021): the SotA neural sparse retriever fine-tuned on MS MARCO; (4) ColBERTv2: the SotA multi-vector dense retriever model finetuned on MS MARCO; (5) NV-Embed-v2 (Lee et al., 2024): the SotA single-vector dense retriever model fine-tuned on multiple text-embedding datasets, achieving the best\u00b9 performance on Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023); (6) BGE-EN-ICL (Xiao et al., 2023): the SotA single-vector dense retrieval model fine-tuned on multiple text-embedding datasets, achieving the second-best5 performance on MTEB.\nDRAGON+, SPLADEv2 and ColBERTv2 represents the three main architectures for neural retrieval, achieving SotA effectiveness with limited training resources and parameters (all around 110M parameters). NV-Embed-v2 and BGE-EN-ICL are the best-performing dense retrievers when the training resources are extensive and the parameters are largely scaled up (7.1B and 7.9B parameters, respectively). Additionally, BGE-EN-ICL supports In-Context Learning (ICL) (Brown et al., 2020) for text embeddings, providing potentials in obtaining task-adapted model with a low cost via adding example demonstration in the model input.\nSince the retrieval task in RIRAG asks to retrieve passages from a corpus of long documents, we also consider it as a Document-Aware Passage Retrieval (DAPR) (Wang et al., 2024) task to jointly model the document context and passage retrieval. To this end, we apply rank fusion to linearly fuse the passage ranking by the neural or BM25 retrievers and the document ranking by the BM25 retriever. The counterpart without rank fusion is denoted as passage-only.\nFor evaluating the retrieval module in RIRAG, we use recall@10 as the main evaluation metric, as we rely on the retrieval module to cover the relevant information as much as possible while leaving the burden of noise filtering to the answer-"}, {"title": "5.2 Answer Generation", "content": "The answer generation process begins once 10 relevant passages have been retrieved for each query from the passage retrieval task. In our post-retrieval stage, we implement a score-Based filtering approach with a threshold of 0.2 to identify significant drops in relevance between consecutive passages. We also enforce a minimum score criterion of 0.7, ensuring that only the most relevant passages are considered for generating answers. Using these filtered passages, we use the gpt-4-turbo-1106 model to generate comprehensive answers. The model follows a tailored prompt designed to simulate the role of a regulatory compliance assistant, integrating all relevant obligations and best practices from the passages into a cohesive response."}, {"title": "5.3 Results", "content": "Our method for the regulatory information retrieval and answer generation task is evaluated using a dataset of 446 question-passage pairs, which have been validated by ADGM experts."}, {"title": "5.3.2 Passage Retrieval Results", "content": "The results of the retrieval task are shown in Table 5. We find the models of DRAGON+, SPLADE and ColBERTv2, which are fine-tuned with limited training data (i.e. MS MARCO only) and limited parameters generalize poorly, achieving worse recall@10 and MAP@10 than simple BM25 most of the time. ColBERTv2, the best model in this group, outperforms BM25 by 1.8 points MAP@10 and 0.3 points recall@10. On the other hand, the models of NV-Embed-v2 and BGE-EN-ICL, which are fine-tuned with more training data and more parameters, outperform BM25 significantly in all the settings. Compared with BM25, recall@10 and MAP@10 are improved by up to 4.9 points and up to 2.9 points, respectively. These results show that scaling the retrievers with more training data and parameters is necessary for good generalization to the retrieval task in RIRAG. We find ICL is important for adapting the neural models to this new task, while the advantage of including more examples in the demonstration is very marginal. Lastly, we find rank fusion, which considers the document context during passage retrieval, improves the performance only marginally. We assume this is because the query generation process only considers the target passage as the input, resulting in fairly self contained query evidence pairs."}, {"title": "5.3.3 Answer Generation Results", "content": "The Table 6 illustrates the performance differences between the methodologies BM25(passage-only)+GPT-4 and BM25(rank fusion)+GPT-4 used in our answer generation task. Both methods show comparable performance in terms of Es and Cs. However, BM25(passage-only)+GPT-4 exhibits a slight advantage in the OCs, leading to a higher overall RePASS."}, {"title": "6 Conclusion", "content": "In conclusion, this paper advances the field of Reg-NLP by introducing innovative tools and methodologies aimed at improving the overall efficiency and precision of regulatory compliance. The automated question-passage generation framework and the ObliQA dataset significantly contribute to refining the retrieval and understanding of regulatory information. Furthermore, the regulatory information retrieval and answer generation task, supported by the novel RePASs evaluation metric, provides a robust framework for improving on our baseline approach.\nIn the future, the field of RegNLP could expand with the integration of new tasks. For instance, Summarization and Simplification methods could be used to develop tools to make complex regulatory texts simpler and more accessible to non-experts. Automated Compliance Checking could leverage cross-document analysis techniques to improve regulatory adherence by comparing an organization's internal compliance documents with regulatory documents. Regulatory Gap Analysis could focus on identifying vagueness, ambiguity or contradictions in regulations to aid in their refinement and ongoing compliance when regulations are added or updated. The continued development of these tasks will equip RegNLP to better serve the needs of regulatory bodies and regulated organizations alike, making compliance a more streamlined, reliable and efficient process."}, {"title": "Limitations", "content": "One significant limitation we encountered is the scarcity of publicly available datasets. Regulatory compliance often involves handling highly sensitive and confidential information, such as personal data or financial records, which presents challenges in terms of data privacy and security. To address this, we created a semi-synthetic dataset\u2014ObliQA\u2014using our Automated Question-Passage Generation Framework. During the validation phase, we faced difficulties stemming from multidisciplinary complexity and shortage of experts. Another key limitation is the lack of standardized formats for regulatory documents. Regulatory documents vary significantly across industries, regions, and even within the same organization, making it difficult to develop models capable of generalization. Furthermore, the absence of comprehensive benchmarks is another limitation. Although we proposed the RePASs metric, we were unable to compare it with other models due to the lack of obligation coverage in existing benchmarks. Nevertheless, on genuine gold standard data from ADGM, RePASS demonstrated its suitability for RIRAG tasks."}, {"title": "A.4 Rank Fusion", "content": "Rank fusion fuses the relevance scores from a BM25 retriever and a neural retriever. We compute the fusion as the convex combination of the normalized relevance scores (Wang et al., 2021):\n$S_{convex} (q, p, d) = \\alpha \\hat{S}_{BM25}(q, p) + (1 - \\alpha) \\hat{S}_{neural}(q, d)$,\nwhere $\\alpha \\in [0,1]$ is the fusion weight, and $\\hat{S}_{BM25} / \\hat{S}_{neural}$ represents the normalized BM25/neural-retrieval relevance score, respectively. The normalization for a relevance score s is calculated as:\n$\\hat{s}(q, c) = \\frac{s(q, c) - m_q}{M_q - m_q}$, (5)\nwhere c represents the passage/document candidate, $m_q$ and $M_q$ are the min. and max. relevance scores"}]}