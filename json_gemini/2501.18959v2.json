{"title": "Enhancing Neural Function Approximation: The XNet Outperforming KAN", "authors": ["Xin Li", "Xiaotao Zheng", "Zhihong Xia"], "abstract": "XNet is a single-layer neural network architecture that leverages Cauchy integral-based activation functions for high-order function approximation. Through theoretical analysis, we show that the Cauchy activation functions used in XNet can achieve arbitrary-order polynomial convergence, fundamentally outperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely on increased depth or B-spline activations. Our extensive experiments on function approximation, PDE solving, and reinforcement learning demonstrate XNet's superior performance - reducing approximation error by up to 50000 times and accelerating training by up to 10 times compared to existing approaches. These results establish XNet as a highly efficient architecture for both scientific computing and AI applications.", "sections": [{"title": "1 Introduction", "content": "A novel method for constructing real networks from the complex domain using the Cauchy integral formula was introduced in Li et al. (2024), utilizing Cauchy kernels as basis functions. This approach enables a significant extension of traditional neural network architectures. This work comprehensively compares these networks with Kolmogorov-Arnold Networks (KANs), which use B-splines as basis functions Liu et al. (2024), and Multi-layer Perceptrons (MLPs), highlighting substantial improvements in various tasks.\nMulti-layer perceptrons (MLPs) (Haykin (1994); Cybenko (1989); Hornik et al. (1989)), recognized as fundamental building blocks in deep learning, have their limitations despite their wide use, particularly in their accuracy, and the large number of parameters needed in structures such as transformers (Vaswani et al. (2017)), as well as their lack of interpretability without post-analysis tools (Cunningham et al. (2023)). Kolmogorov-Arnold Networks (KANs) were introduced as a potential alternative, drawing on the Kolmogorov-Arnold representation theorem (Kolmogorov (1956); Braun & Griebel (2009)), and have demonstrated efficiency and accuracy in computational tasks, especially in solving PDEs and function approximation (Sprecher & Draghici (2002); K\u00f6ppen (2002); Lin & Unbehauen (1993); Lai & Shen (2021); Leni et al. (2013); Fakhoury et al. (2022)).\nIn the swiftly advancing domain of deep learning, the continuous search for novel neural network designs that deliver superior accuracy and efficiency is pivotal. While traditional activation functions such as the Rectified Linear Unit (ReLU) (Nair & Hinton (2010)) have been widely adopted due to their straightforwardness and efficacy in diverse applications, their shortcomings become evident as the complexity of"}, {"title": "Principal Contributions", "content": "Our study advances the field of neural network architectures through several key contributions:\n(i) Enhanced Function Approximation Capabilities: Through comprehensive comparative analysis, we demonstrate XNet's superior performance over KAN in function approximation, particularly excelling in handling the Heaviside step function, complex high-dimensional scenarios, and noisy data. Sections 3.1.3 through 3.1.5 provide detailed empirical validations of these capabilities.\n(ii) Superiority in Physics-Informed Neural Networks: Using the Poisson equation and Heat equation as a benchmark, we establish XNet's enhanced efficacy within the PINN framework. Our results, detailed in Section 3.2.2, demonstrate significant performance improvements over both MLPs and KANs, setting new standards for accuracy and efficiency in PDE solving."}, {"title": "2 Theoretical Comparison Between Cauchy Kernels and B-splines", "content": "Theorem 1. Cauchy Approximation Theorem (from Li et al. (2024)). Let $f(z^1,...,z^d)$ be an analytic function on an open set $U \\subset \\mathbb{C}^d$. It was shown in Li et al. (2024) that for any desired accuracy $\\epsilon > 0$, $f$ can be approximated in the $L^{\\infty}$-norm using a finite sum of Cauchy kernels:\n$\\sup_{z \\in U} |f(z^1,..., z^d) - \\sum_{i=1}^{N} \\frac{\\lambda_i}{(z-z_i^1)...(z^d-z_i^d)} | < \\epsilon$.\\newline\nFurthermore, the approximation error satisfies $\\epsilon = O(N^{-p})$ for any fixed integer p and sufficient smoothness of functions f.\nCauchy activation functions are derived from the above approximation theorem. This demonstrates that XNet can approximate analytic functions with error decreasing at an arbitrary polynomial rate, whereas traditional neural networks typically require increased width and depth to achieve comparable accuracy. Structurally, XNet can be interpreted as a single-layer MLP with Cauchy activation functions, while KAN utilizes B-splines as basis functions.\nCauchy kernels and B-splines provide distinct strategies for function approximation. The following section explores their differences from three perspectives: 1. Approximation Power; 2. Computational Efficiency; 3. Numerical Properties."}, {"title": "Approximation Rate", "content": "Given an analytic function f, the Cauchy kernel approximation satisfies the following error bound:\n$||f - f_n|| = O(N^{-p}), \\forall p > 0$. (2)"}, {"title": "Computational Efficiency", "content": "The Cauchy Approximation Theorem indicates that to achieve a specified approximation error $\\epsilon$, the number of required basis functions for Cauchy kernels is given by:\n$N_{Cauchy} = O(\\epsilon^{-1/p}),$ (4)\nwhere p is any large positive integer. This flexibility allows for the convergence rate to be tailored very aggressively, making it possible to enhance the precision of approximation significantly by increasing p.\nContrastingly, the use of B-splines involves a fixed degree k, which generally does not exceed practical limits due to computational and numerical stability considerations:\n$N_{B-spline} = O(\\epsilon^{-1/k}).$ (5)\nWhile B-splines are powerful and flexible within their operational scope, they are inherently limited by the highest degree k that can be practically utilized, which may not offer the same level of aggressive error reduction as Cauchy kernels.\nFurthermore, the structural simplicity of Cauchy-based systems, which can be efficiently implemented using a single-layer network architecture, provides significant advantages in terms of computational speed and resource usage. This simplicity allows for rapid adjustments and learning, particularly advantageous in scenarios requiring real-time processing and high responsiveness. In contrast, networks based on B-splines or other more complex architectures often require multiple layers or more intricate setups, which can increase computational time and delay convergence.\nExample in Practice: In real-world applications, especially those requiring quick response times and high precision, the single-layer Cauchy network can outperform more complex multi-layer networks. For instance, in tasks involving real-time image or signal processing, the ability to compute approximations quickly and accurately is crucial, and the Cauchy kernel's properties directly contribute to superior performance in these cases."}, {"title": "Numerical Characteristics", "content": "For function approximation with Cauchy kernels and B-splines, the following aspects are crucial for assessing their practical application:\n\u2022 Matrix Structure and Stability: Cauchy kernel interpolation results in dense matrices, which might lead to large condition numbers as the problem size increases. This is analyzed using Gershgorin's Circle Theorem:\n$|\\lambda - A_{ii}| \\le \\sum_{j \\neq i} |A_{ij}| = \\sum_{j \\neq i} \\frac{1}{|z_i - z_j|},$ (6)\nindicating that the condition number $\\kappa(A) = O(N)$ as N increases. Although this suggests potential issues in numerical stability, the single-layer structure of networks using Cauchy kernels allows for rapid computation and effective management of these matrices, particularly when combined with modern computational techniques to mitigate numerical instability.\nB-splines, benefiting from local support, generate sparse banded matrices that inherently have better stability and lower condition numbers, typically O(1).\n\u2022 Derivative Properties: Cauchy kernels allow precise and straightforward derivative computations due to their explicit formulas:\n$\\frac{d}{d(d^2)} (\\frac{1}{x^2 + d^2}) = -\\frac{1}{(x^2 + d^2)^2}$ (7)"}, {"title": "3 Experimental Setup and Results", "content": "To comprehensively evaluate XNet's performance compared to KAN and traditional MLPs, we conduct experiments across three domains: function approximation, PDE solving, and reinforcement learning. Each domain is designed to test different aspects of the networks' capabilities."}, {"title": "3.1 Function Approximation Experiments", "content": "We evaluate the networks on three categories of functions with increasing complexity:\n\u2022 Low-Dimensional Tasks:\nDiscontinuous: Heaviside step function\n\u2022 High-Dimensional Tasks:\n4D nonlinear: exp (\\frac{1}{2} (sin (\u03c0(x1 + x2)) + x3x4))\n100D periodic: exp (\\sum_{i=1}^{100} sin\u00b2 (\\frac{xi}{2}))\n\u2022 Noisy Data Tasks:\nTime series system with varying noise levels (0%, 5%, 10%)\nThe experimental comparison between XNet, B-spline, and KAN demonstrates XNet's superior approximation ability. Except for the first function example and the final task, all other examples are from the referenced article, with KAN settings matching those from the original experiments. This ensures a fair comparison, fully proving that XNet has stronger approximation capabilities in various benchmarks."}, {"title": "3.1.2 Experimental Protocol", "content": "\u2022 Network Configurations\nXNet: 64 or 5000 basis functions\n- KAN: Hierarchical structure with optimal grid points\n- MLP: Comparable parameter count\n\u2022 Training Setup\n- Low-D: 1000 training / 1000 test points\nHigh-D: 8000 training / 1000 test points\nOptimizer: Adam (XNet), L-BFGS (KAN)"}, {"title": "3.1.4 Approximation with high-dimensional functions", "content": "We continue to compare the approximation capabilities of KAN and XNet for high-dimensional functions. Following the procedure described in the article Liu et al. (2024), 8000 points were used for training and another 1000 points for testing. XNet is trained with adam, while KAN is initialized to have G = 3, trained with LBFGS, with increasing number of grid points every 200 steps to cover G = 3, 5, 10, 20, 50.\nFirst, we consider the four-dimensional function $exp (\\frac{1}{2} (sin (\u03c0(x_1 + x_2)) + x_3x_4))$. For this case, the KAN structure is configured as [4,4,2,1], while XNet is equipped with 5000 basis functions. Under the"}, {"title": "3.1.5 Functions with noise", "content": "The system is generated by the following equations:\n$x^{i}_{t} = 0.1x^{i-1}_{t}x^{i-1}_{t-1} + 0.5sin(x^{i-1}_{t}x^{i-1}_{t-1}) + sin(x^{i-1}_{t+1}) + \\mu^{i}$,\nwhere i = 1, 2, ..., \u03b7, and\n$x^{0}_{t} = x^{t}_{1}, x^{1}_{t} = x^{t}_{1}, x^{2}_{t} = x^{t}_{1}, x^{3}_{t} = x^{t-1}_{1}$,\nwhere the initial conditions $x^{0}_{t}, x^{1}_{t}, x^{2}_{t}, x^{3}_{t}, x^{4}_{t}$ are randomly sampled in the range [0, 0.2], and the noise term $\\mu^{i}$ is sampled from a normal distribution, $\\mu^{i} \\sim N(0, noise)$. This generates a series ${f_i = x^{i}_t}_{i=1,...,n}$, with n = 300. The first 80% of the data contains noise, while the last 20% is noise-free. Clearly, the system is governed by relatively simple functions. The task of predicting the sixth data point using the first five data points becomes a high-dimensional function approximation problem.\nIn this example of a simple function-driven time series, XNet clearly outperforms KAN, particularly in noisy and noise-free environments. Given these results, we hypothesize that XNet will also exhibit superior performance in highly noisy, real-world datasets. The [5,64,1] KAN model, however, shows signs of overfitting, with excellent performance on the training set but noticeable degradation on the test set."}, {"title": "3.2 PDE Solving Capability", "content": "We focus on the Heat equation as a benchmark PDE problem:\n$\\frac{\\partial u}{\\partial t} = \\nu \\frac{\\partial^2 u}{\\partial x^2}, (x, t) \\in [0, 1] \\times [0, 1],$\n$u(x, 0) = sin(\\pi x), x \\in [0, 1],$\n$u(0, t) = u(1, t) = 0,$\nwith known analytical solution $u(x, t) = u(x, t) = e^{-\\nu \\pi^2 t} sin(\\pi x)$ (Figure 3)."}, {"title": "3.2.1 Implementation Details", "content": "\u2022 Network Settings\n- MLP: [2, 20, 20, 1] architecture\nXNet: 20 and 200 basis functions\n- KAN: [2, 10, 1] structure\n\u2022 Training Protocol\nViscosity coefficient \u03bd = .001\nInterior points: 2500\nBoundary points: 150\n- Loss weight: a = 0.1"}, {"title": "3.2.2 Heat function", "content": "We use the framework of physics-informed neural networks (PINNs) to solve this PDE (8), with the loss function given by\n$loss = \u03b1 loss_i + loss_b = \\frac{\u03b1}{N_i}\\sum_{i=1}^{N_i} |v_{xx}(z_i) - f(z_i)|^2 + \\frac{\u03b1}{N_b}\\sum_{i=1}^{N_i} |v_{xx}(z_i) - v_{tra}(z_i)|^2$,\nwhere $loss_i$, referred to as the interior loss, is evaluated by discretizing the domain into $n_i$ uniformly sampled points $z_i = (x_i, t_i)$. Similarly, $loss_b$, representing the initial and boundary loss, is computed using $n_o$ uniformly sampled points on both the initial and boundary conditions. The hyperparameter a balances the contributions of the interior and boundary losses."}, {"title": "3.3 Reinforcement Learning Applications", "content": "We evaluate XNet, KAN, and MLP as function approximators within the Proximal Policy Optimization (PPO) framework on two continuous control tasks from the DeepMind Control Suite: HalfCheetah-v4 and Swimmer-v4. Table 9 summarizes the network configurations for each model.\nProximal Policy Optimization (PPO) optimizes the stochastic policy \u03c0\u03b8 using a clipped surrogate objective to ensure stable updates:\n$L^{PPO}(\\theta) = E_{t}[min(r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)],$ (10)\nwhere $r_t(\\theta)$ is the policy ratio and $\\epsilon$ is a clipping parameter (set to 0.2). This objective is combined with a value function loss and entropy regularization to balance exploration and exploitation."}, {"title": "Theoretical Analysis of XNet's Advantage", "content": "The superior performance of XNet in reinforcement learning can be attributed to its unique Cauchy kernel activation functions, which possess three critical advantages:\n1. Localized Response with Fast Decay: Cauchy kernels exhibit a localized response in their basis functions, which improves the network's ability to adapt to complex policy and value functions with sharp variations in state-action spaces. This property is especially advantageous in continuous control tasks, where such variations are common.\n2. High-Order Approximation Capability: As established in Theorem 1, XNet achieves arbitrarily high-order approximation for analytic functions, allowing it to represent policies and value functions with higher precision compared to MLPs and KANS.\n3. Reduced Parameter Complexity: Unlike KAN, which requires a large number of parameters to achieve similar accuracy, XNet leverages its basis function design to achieve better performance with fewer parameters. This reduced complexity directly translates to faster convergence in RL tasks."}, {"title": "4 Summary and Outlook", "content": "Through comprehensive theoretical and empirical studies, we have thoroughly evaluated XNet's capabilities across function approximation, physics-informed learning, and reinforcement learning. Our key findings and future directions are summarized below."}, {"title": "4.1 Key Findings", "content": "\u2022 Superior Function Approximation Our experiments demonstrate that XNet significantly outperforms the recently proposed KAN in function approximation tasks. Notably, XNet maintains high computational efficiency while achieving superior accuracy, particularly for discontinuous and high-dimensional functions. Across all evaluation metrics (MSE, RMSE, MAE), XNet consistently surpasses KAN while reducing computational time. In high-dimensional function tests, XNet achieves up to a 1000-fold improvement in MSE compared to KAN.\n\u2022 Enhanced Physics-Informed Learning Within the PINN framework, XNet exhibits two key advantages over traditional neural networks and KAN: (i) Higher solution accuracy, reducing MSE by a factor of 50 when solving the Heat equation. (ii) Lower computational cost, requiring only 40% of KAN's training time. These results highlight XNet's potential to improve the efficiency of physics-informed neural networks.\n\u2022 Effective Reinforcement Learning Integration Integrating XNet into the PPO framework, we observe superior performance in reinforcement learning tasks on DeepMind Control Suite environments (HalfCheetah-v4 and Swimmer-v4). XNet-based PPO models achieve faster convergence and higher final rewards, reaching scores of 3298.52 and 100.38, respectively, substantially outperforming MLP- and KAN-based implementations. These findings indicate XNet's effectiveness in learning complex control strategies."}, {"title": "4.2 Future Directions", "content": "While XNet demonstrates substantial advantages over existing architectures, several research challenges remain:\n\u2022 Extending XNet for Large-Scale Architectures Investigating its integration with transformer-based models to improve sequence modeling and time-series analysis."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Mathematical Analysis", "content": "Derivation of Approximation Rate. For any fixed integer p, if f is CP smooth, the error bound follows from Cauchy approximations theorem:\n$||f - fv|| \\le C(p)N^{-P}, \\forall p > 0,$ (11)"}]}