{"title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations", "authors": ["Leonardo Ranaldi", "Marco Valentino", "Andr\u00e9 Freitas"], "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms brings its inherent challenges, as LLMs need to deal with potentially noisy contexts. Recent studies have shown that LLMs still struggle to critically analyse RAG-based in-context information, a limitation that may lead to incorrect inferences and hallucinations. In this paper, we investigate how to elicit critical reasoning in RAG via contrastive explanations. In particular, we propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant documents given a query, (ii) selects and exemplifies relevant passages, and (iii) generates explanations that explicitly contrast the relevance of the passages to (iv) support the final answer. We show the impact of C-RAG building contrastive reasoning demonstrations from LLMs to instruct smaller models for retrieval-augmented tasks. Extensive experiments demonstrate that C-RAG improves state-of-the-art RAG models while (a) requiring significantly fewer prompts and demonstrations and (b) being robust to perturbations in the retrieved documents.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) aims to improve the factuality and memory access of Large Language Models (LLMs) by systematically integrating relevant knowledge from external sources via retrieval mechanisms (Lewis et al., 2020). In particular, RAG is designed to mitigate some of the well-known limitations of LLMs, including the tendency for hallucinations and the lack of specific domain knowledge in the training data (Siriwardhana et al., 2023; Zhang et al., 2023; Kandpal et al., 2023).\nDespite the benefits of RAG, contemporary studies have identified a range of persisting limitations emerging from noisy retrieval, where irrelevant or contradictory in-context passages can introduce biases in the models, particularly on smaller LMs (Petroni et al., 2020; Shi et al., 2023). These shortcomings stem from the inability of RAG to systematically and critically assess the retrieved passages provided as context. While an emerging line of research attempted to improve the RAG pipeline by incorporating multi-step reasoning strategies to determine the relevance of in-context passages (Li et al., 2023; Yoran et al., 2024), this usually comes at the cost of significantly increasing the computational resources required for training and inference (Xia et al., 2024).\nTo tackle such limitations, this paper proposes Contrastive-RAG (C-RAG), a novel end-to-end framework designed to elicit a critical reasoning process in retrieval-augmented language models in the form of contrastive explanations \u2013 i.e., ex-"}, {"title": "2 Contrastive Explanations", "content": "Contrastive explanations have been identified as fundamental modes of explanation in epistemology, artificial intelligence, and cognitive science (Lipton, 1990; Miller, 2019). A contrastive explanation is a type of inference aimed at answering why-questions of the form \u201cWhy P rather than Q?", "Why is Pq relevant to q rather than Qq?\", where the set of relevant documents Pq corresponds to the fact, and the set of irrelevant documents Qq corresponds to the foil.\"\n    },\n    {\n      \"title\"": "3 Contrastive-RAG"}, {"content": "To elicit contrastive explanations in RAG, we present a framework composed of four inference stages: i) collecting step (\u00a73.1), where, given a query, the retrieved documents are collected and an LLM-based model extracts relevant passages; ii) contrastive reasoning (\u00a73.2), where an LLM-based model generates critical and contrastive explanations about the relevance of the extracted passages, by exemplifying and contrasting relevant and irrelevant aspects; iii) explanation (\u00a73.3), where the arguments constructed in ii) are summarised into a single explanation; iv) answering (\u00a73.3), where a final short answer to the query is generated. The prompt adopted for C-RAG is illustrated in Table 6. In this paper, we are interested in employing C-RAG to generate synthetic reasoning demonstrations (\u00a73.4) to enhance RAG models (\u00a73.5) and to equip them with the capability of critically analysing the questions and the retrieved documents and generate a contrastive explanation to arrive at the answer (Figure 2)."}, {"title": "3.1 Collecting Step", "content": "RAG models leverage retrieved knowledge to improve accuracy and reduce hallucinations in LLMs. Therefore, the first step in the proposed pipeline involves retrieving relevant documents from a given reference document base D. In this paper, we use DPR (Karpukhin et al., 2020) and Contriever (Izacard et al., 2021) as the default retriever models R. Subsequently, we instruct the LLM to analyse the question and extract the most relevant passages from the set of retrieved documents (i.e., '#Reference Evidence'). We collect the retrieved documents and relevant passages for all the queries and refer to this step as 01."}, {"title": "3.2 Contrastive Reasoning", "content": "Recent work has shown that exemplifying passages from the retrieved documents and directly using them as in-context knowledge for RAG can improve LLMs' accuracy (Asai et al., 2023). However, each passage may still contain irrelevant or contradictory knowledge that can mislead the model. Hence, after collecting the passages from the top-k documents (\u00a73.1), we instruct the LLM to generate contrastive explanatory arguments to identify and compare relevant and irrelevant points in the passages with respect to the question (\u00a73.1). We perform this step by setting out the instructions as reported in Table 6. Hence, we collect the outcomes by defining this phase as a2."}, {"title": "3.3 Explanation & Answer", "content": "Finally, we leverage the arguments in the previous steps to generate a final contrastive explanation to derive the answer. In particular, we instruct the LLM to explicitly consider the contrastive rationales and summarise the main points into a single explanation. We define this step as 03. Subsequently, we instruct the model to generate the final answer following the pattern '#Answer:'. This final step is defined as 04."}, {"title": "3.4 C-RAG Operability", "content": "C-RAG leverages a set of structured instructions to deliver multi-step explanations via contrastive reasoning. Thus, the operability of C-RAG is two-"}, {"title": "3.4.1 C-RAG as a Prompting Strategy", "content": "By operating through the instructions in Table 6, we adopt C-RAG to prompt GPT-4 (OpenAI, 2023). Specifically, we instruct GPT-4 to extract the most crucial passages from the retrieved documents (\u00a73.1), explaining the relevant and irrelevant points to answer the given question by exemplifying the main passages (\u00a73.2), provide a single exhaustive explanation that best describes the critical points (\u00a73.3); and finally, generate the final answer in a strict format, in order to have a more straightforward and less ambiguous downstream assessment. However, although the sequence of instructions is well-structured and defined, the ability to perform sequential and complex reasoning tasks is limited to larger LLMs (such as GPT-4, as discussed in the experiments). Therefore, to transfer contrastive reasoning to smaller models, we use C-RAG to generate synthetic annotations as training demonstrations."}, {"title": "3.4.2 C-RAG as a Demonstration Strategy", "content": "Following recent work (Xia et al., 2024; Asai et al., 2023), we instruct smaller models via demonstrations produced by high-performing LLMs that are capable of following structured instructions. In contrast to previous methods, however, our approach is based on a single prompt composed of a series of sequential instructions (Figure 2). Although GPT-4 have demonstrated the ability to follow sequential instructions (Peng et al., 2023), we cannot formally guarantee that the generated demonstrations are correct. Therefore, we follow the method proposed by Xia et al. (2024), which computes the citation precision for the considered documents as a proxy for the quality of the demonstrations. However, since C-RAG uses a different annotation mechanism, our heuristics firstly filter out the final correct answers through a strict, exact match; then, after the filtering (cutting off about 50% of the demonstrations), it verifies that each retrieved document along the reference evidence has been considered. The starting annotations consist of approximately 10,000 training samples delivered with GPT-4, which, after the first filtering strategy, are reduced to 4,500 and finally, through quality control, are reduced to 2,000 (see Appendix C)."}, {"title": "3.5 Training", "content": "Thanks to the operability of C-RAG (3.4), a Language Model \u03c0 can be trained using the generated annotations\u00b9, which are augmented with reasoning demonstrations \u03b1 using the standard language modeling objective, maximizing likelihood:\nmax E(q,a,y)~DA log p\u3160(y | a, q)p\u03c0(\u03b1|q) (1)\n\u03c0\nwhere \u03b1 = A1 A2 A3 A4 is the combination of the multiple reasoning steps performed by the model,  is the concatenation operator, and 01, 02, are the respective annotations generated by the above processes. q is the provided question, and y is the model output, including the intermediate steps and the final answer. DA is the training corpus augmented with contrastive reasoning demonstrations."}, {"title": "4 Experiments", "content": "We evaluate C-RAG on four open-domain question-answering tasks (\u00a74.1). We perform the retrieval and evaluation phases by following standard approaches used to assess the RAG pipeline (\u00a74.2) and perform the tuning phase by using the setup presented in \u00a74.3."}, {"title": "4.1 Tasks & Datasets", "content": "We conduct an extensive experimental evaluation on the following question-answering (QA) tasks: (i) NaturalQuestion (NQ) (Kwiatkowski et al., 2019), (ii) PopQA (Mallen et al., 2023), (iii) TriviaQA (Joshi et al., 2017) and (iv) FEVER (Thorne et al., 2018). Appendix D describes the composition of each dataset."}, {"title": "4.2 Experimental Setup", "content": "Retriever We use DPR (Karpukhin et al., 2020) and Contriever-MS MARCO (Izacard et al., 2021) to retrieve the top top-k documents from the document base. We chose k = 5 to have a fair comparison to related RAG approaches using the same value for k. By default, we operate via DPR on NQ, as DPR has been fine-tuned on the dataset. On PopQA, where question and answer pairs are created based on Wikipedia, we use the Wikipedia_corpus as background knowledge as proposed in (Xia et al., 2024)."}, {"title": "4.3 Models Setup", "content": "To get a comprehensive evaluation of existing RAG pipelines and the impact of C-RAG, three different LLMs are used: GPT-4 (OpenAI, 2023), Llama-2-7 and Llama-2-13 (Touvron et al., 2023) along with their instruction-tuned chat version Llama-2-7-chat and Llama-2-13-chat. The models were chosen to have a common ground for comparison with state-of-the-art approaches.\nInference Settings We use greedy decoding in all experiments to ensure a more deterministic genera-"}, {"title": "4.3.1 Training Setup", "content": "To evaluate the impact of C-RAG contrastive reasoning demonstrations on smaller models (\u00a73), we use the annotations produced following the C-RAG strategy (\u00a73.4.2). Additionally, for a fair comparison, we produce annotations using Llama-2-SFT, where Llama-2 is fine-tuned on training samples without C-RAG. We compare our models with several related RAG approaches trained with demonstrations generated by GPT-4 to establish strong baselines (detailed in the last column of Table 1). We fine-tune the Llama-2 models for 3 epochs with a batch size of 32 and a learning rate equal to 3e-5 with a 0.001 weight decay. We use the cosine learning rate scheduler with a warmup ratio of 0.03. We conducted our experiments on a workstation equipped with four Nvidia RTX A6000 with 48GB of VRAM."}, {"title": "4.4 Prompting", "content": "We systematically prompt the models using two main settings:"}, {"title": "5 Results & Discussions", "content": "The results of the empirical evaluation are reported in Table 1. Overall, the experiments confirm that C-RAG can improve the capabilities of LLMs to handle retrieved documents for QA and fact verification tasks demonstrating the impact of contrastive reasoning and explanations on RAG models. We found that C-RAG is particularly effective as a demonstration strategy to improve the performance of smaller Llama-2 models, achieving state-of-the-art performance when compared to related fine-tuning approaches in the literature (Xu et al., 2023; Asai et al., 2023; Xia et al., 2024). In the following sections, we analyse the impact of C-RAG when adopted as both a prompting strategy (\u00a75.1) and as a framework for generating annotations to instruct LLMs (\u00a75.2). Finally, we analyse the role of the contrastive explanations (\u00a75.3) and provide evidence of robustness on challenging perturbations and low-resource settings (\u00a75.4)."}, {"title": "5.1 C-RAG as a Prompting Strategy", "content": "The middle part of Table 1 reports the results of C-RAG when adopted as a prompting strategy for different models. While we can observe an overall improvement over the baseline models without"}, {"title": "5.2 C-RAG as a Demonstration Strategy", "content": "The lower part of Table 1 reports the results of C-RAG when adopted as a demonstration strategy for different models. Here, the results show that C-RAG is highly effective in improving the performance of Llama-2 models when used to generate reasoning demonstrations via GPT-4. In particular, we found that C-RAG can outperform state-of-the-art approaches, including RECOMP (+1.8%), Self-RAG (+7.2%), and Self-Reasoning (+1.9%). Moreover, as shown in Table 1, C-RAG can achieve such results using a fraction of the reasoning demonstrations used by related approaches, also requiring fewer prompts and annotation steps. Specifically, our approach uses significantly fewer demonstrations when compared to RECOMP (Xu et al., 2023) and Self-RAG (Asai et al., 2023), (2k demonstrations vs. 150k and 190k). Similarly, in contrast to Self-Reasoning (Xia et al., 2024), C-RAG operates via a single-step prompting. This experimental setting significantly reduces the use of resources and simplifies the training process. In addition, we observe that Llama-2 models fine-tuned via C-RAG can outperform GPT4-o by 6.1% and substantially improve the performance of all the evaluated Llama-2 models (with and without RAG). These results strongly demonstrate the impact of the training signal provided by contrastive explanations and their ability to efficiently elicit critical arguments in smaller LMs (as shown in an inference example in Appendix G)."}, {"title": "5.3 The Role of Contrastive Explanations", "content": "Table 2 evaluates the impact of the end-to-end contrastive reasoning framework on the final performance. In particular, the table shows the effect of eliminating one of the steps or randomly shuffling them to produce the demonstrations.\nThe results demonstrate the importance of each stage in the multi-step reasoning process intro-"}, {"title": "5.4 Robustness & Ablation Analysis", "content": "The C-RAG framework instructs models to reason on the retrieved documents independently of their order of occurrence (i.e., invariance to documents' permutations) and attempts to elicit critical explanations to identify irrelevant or contradictory knowledge in the extracted passages (i.e., robustness to the bias) as revealed in Figure 3. In addition, the benefits of C-RAG also emerge when reducing the number of demonstrations, showing that contrastive explanations can improve data efficiency (Figure 4). To assess such properties in more detail, we performed a robustness analysis along with an evaluation of how scaling the training demonstrations affects models' behaviours.\nRobustness to Perturbations Since noisy retrieval can negatively affect the performance of LLMs (Petroni et al., 2020), we follow the methodology introduced in (Asai et al., 2023; Xia et al., 2024) to evaluate robustness. Specifically, we shuffled the order of the retrieved documents (Random Shuffle) and inserted two irrelevant documents (Random Noise). Figure 3 reports the experimental results. We found that the C-RAG framework consistently outperforms the baseline model (Llama-2-13b), as well as Self-RAG and Self-Reasoning, finding that perturbations have a lower impact on the final performance. In particular, the random shuffling of retrieved documents has a minimal impact on performance, demonstrating the permutation invariance property of C-RAG. Moreover, when noisy documents are added, all the evaluated models suffer a higher performance drop. However, the drop for C-RAG is typically lower than"}, {"title": "6 Related Work", "content": "Previous research investigated the advantages of augmenting Large Language Models (LLMs) through retrieved text passages, a technique known as Retrieval-augmented Generative (RAG) (Lewis et al., 2020; Ram et al., 2023). However, recent work showed that the benefits of RAG can be undermined by noisy retrieval, thus decreasing consistency and reliability (Liu et al., 2023; Petroni et al., 2020; Shi et al., 2023). Hence, several works proposed techniques to improve RAG reliability by enabling the models to elicit the critical steps to arrive at the final answer (Menick et al., 2022; Gao et al., 2023b). Similarly, recent work focused on improving the retrieval phase by fine-tuning LLMs to perform dynamic retrieval vi adaptive reasoning strategies Jiang et al. (2023); Yao et al. (2023); Gao et al. (2023a); Zhang et al. (2024). Nevertheless, the use of multi-step reasoning strategies usually comes at the cost of increasing the computational resources in terms of number of prompts and training examples(Fan et al., 2024; Gao et al., 2024).\nFor instance, Asai et al. (2023) instructed models to retrieve information using special reflection tokens. However, this solution requires the training of two external models, requiring tens of thousands of additional training samples. Xu et al. (2023) attempted to lower the computational cost for multi-step RAG pipelines, but their approach still requires additional models to summarise the retrieved documents. Finally, Xia et al. (2024) eliminated the dependence on special tokens and external components by introducing reasoning trajectories that are employed to boost the performance of LLMs directly. Although the solution improves efficiency, the framework operates through a multi-step mechanism that requires multiple annotation phases.\nSimilarly to recent work investigating the impact of natural language explanations on LLMs (He et al., 2024; Dalal et al., 2024; Ye and Durrett, 2022; Quan et al., 2024b,a; Ranaldi and Freitas, 2024a,b), we propose a method to integrate via multi-step explanations into RAG. To the best of our knowledge, however, this is the first work to investigate the impact of contrastive explanations on RAG and demonstrate how contrastive reasoning demonstrations can boost the performance of smaller LMs."}, {"title": "7 Conclusion", "content": "RAG has shown great potential in boosting the performance of LLMs on knowledge-intensive tasks. Despite the success of RAG, noisy retrieval represents a major limitation. To tackle such challenges, we introduce a new framework called C-RAG, designed to improve RAG-based models by leveraging contrastive explanations. We demonstrate that C-RAG can outperform state-of-the-art models while requiring fewer prompts and demonstrations and being robust to perturbations in the retrieved documents, laying the foundation for future research exploring the impact of natural language explanations on RAG-based models' efficiency, consistency and reliability."}]}