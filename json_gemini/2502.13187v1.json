{"title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models", "authors": ["Longchao Da", "Justin Turnau", "Thirulogasankar Pranav Kutralingam", "Alvaro Velasquez", "Paulo Shakarian", "Hua Wei"], "abstract": "Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) algorithms are showing potential in multiple domains for their promising sequential decision-making abilities. In addition to gaming scenarios, the solutions are getting closer to real-world problems such as robotic control [111], recommender systems [3, 38], healthcare [76, 241], and transportation [87, 229], etc.\nDespite the frontier explorations of the RL-based methods, deploying the RL-learned policies in the real world is still challenging [39, 218], especially in high-risk scenarios like autonomous driving [110] and disease diagnosis or chronic treatment [139]. These real-world problems are struggling to benefit from the RL methods due to the gap between the simulator (used for policy learning) and reality (used for policy deployment), known as 'Sim-to-Real' gap.\nThe Sim-to-Real gap is introduced in the policy training process and magnified in the deployment execution. Consequently, the well-trained RL policy suffers from severe real-world performance drop. In the worst case, there even exists a potential safety hazard given the unpredictable decisions under unseen scenarios. Some researchers attributed this to the transition gaps between the sim- and real- environments and proposed several aspects to tackle this gap from transition dynamics [217], such as Domain Randomization and Domain Adaptation, etc. There was also literature discussing the gap introduced during the perception or execution period and proposed ways of Grounded Learning [188]. It is a pleasure to witness more and more attention cast on sim-to-rea RL, however, we find that different researchers are working on specific domains respectively [46, 82, 91, 232], and some great insights should be unified while specialties should be discussed in domain-specific aspects. Besides, with the explosive development of Large Foundation Models [138, 231], various effective approaches are proposed to integrate the foundation model's inference ability to downstream tasks [137], and we observe its great potential to benefit sim-to-real transfer for RL methods.\nIn this paper, we tend to unify the commonly adopted, classic techniques from several sim-to-real domains, we provide a taxonomy that frames the majority of sim-to-real techniques based on the four elements of MDPs: Observation, Action, Transition, and Reward, and we also include the development of sim-to-real research from the classic to the most emerging techniques with foundation models. Then we discuss the challenges and solutions in domain-specific categories. After the introduction of the sim-to-real solutions in the training aspect, we categorize the evaluations into sim-to-real validation and Policy Evaluation, providing ways to effectively understand the policy performance.\nIn summary, compared to existing survey papers as in Table. 1, our main contributions are \u2460 Taxonomy: We propose a formal taxonomy for sim-to-real RL from issues, techniques, domains specialties and evaluations, and specifically, we categorize the technique solutions into the four foundation elements of MDPs; \u2461 Comprehensive Review: We conduct comprehensive literature review that covers the most of related works, we reflect on the cause of sim-to-real gap and stem from classic, we introduce how large foundation models can benefit this research direction. \u2462 Domain specific discussion: Except for the techniques, we identify the unique challenges related to different real-world domains, and discuss potential prospects."}, {"title": "2 The RL and Sim-to-Real Issue", "content": "In this section, we will start with a brief overview of the key concepts in Reinforcement Learning, including the MDP, and policy learning. And then, we will formally introduce the sim-to-real issue in RL. To avoid confusion, we include a notation explanation table to summarize key terms in Table 5.\n2.1 Definition\n2.1.1 Reinforcement Learning (RL). Reinforcement Learning is a special machine-learning paradigm that empowers the learning of decision policy from the agent's interaction in an environment, the learning is directed by receiving feedback (reward) that comes along with the action. To maximize the accumulated reward, the policy is iteratively improved using various learning algorithms.\nIn general, the above RL learning procedure is often defined on a Markov Decision Process (MDP) M that satisfies formal mathematical modeling [70], where M = (S, A, T,R, \u03b3):\n\u2022 S is the state space that covers all possible situations that happen in the environment, either discrete or continuous.\n\u2022 A is the action space that encompasses all possible actions an agent can take, either discrete or continuous.\n\u2022 T is the transition function that defines the probability distribution of moving from state st to state st+1 given that action at is taken, i.e., T(St+1|st, a). The transition T is defined on SXAXS\u2192 R.\n\u2022 R: reward specifies the feedback an agent could obtain by taking action a at state st and moving to st+1.\n\u2022 y is the discount factor that determines the importance of future rewards.\nThe MDP starts with an initial state \u00b5o that is sampled from S, an action at will be output from current policy n every time needs an action to further interact with the environment. The \u03c0 can be taken as a mapping from states to actions, \u03c0 : S \u2192 A, and the quality of a is measured by value functions. To be concise, the RL's goal is to learn a policy that maximizes the accumulated expected return on the basis of rewards:\n$J(\u03c0) := E(s,a)~* (s,a) [\u2211rre (St, at, St+1 ~ p(st, at)]$ (1)\nwhere pt (s, a) is the stationary state-action distribution under \u03c0, at = \u03c0(st) and p(st, at) = T (St+1 St, a).\nThere are multiple branches of research on RL algorithms for faster and more stable learning processes, like value-based methods: Deep Q learning (DQN) [89], State-Action-Reward-State-Action (SARSA) [75], and policy gradient-based methods such as REIN-FORCE [204], Proximal Policy Optimization (PPO) [192], and Deep Deterministic Policy Gradient (DDPG) [130], etc. This paper focuses on the sim-to-real problems and solutions, so it will not step into detail on the methods above.\n2.1.2 Sim-to-Real of Reinforcement Learning. In the context that the RL algorithm learns upon a MDP M, the Sim-to-Real issue can be described as the policy n learned from Ms can not be well generalized to Mr, where Ms represents the simulator environment Esim and Mr depicts the real world Ereal. We formally define Sim-to-Real gap as G(\u03c0):\n$G(\u03c0) := \u03c8\u03c2(\u03c0\u03ad) \u2013 \u03c8r(\u03c0\u03ad)|\u03c0\u03ad ~ Ms,$ (2)\nwhere y is any evaluation metric to quantify the performance of a policy, which should be calibrated and applied identically in the simulator s and the real-world y, environment.\nSuch performance gap G(\u03c0) is directly a result of policy interactions but is introduced in the policy learning process, and we can analyze the causes from the elements of Ms, which mainly encapsulates: (Ss, As, Ts, Rs). Since the discount factor y is an ideal abstraction of future impact, which is inherently non-perfect, we will not discuss this factor in this survey paper.\nMost of the decision-making is based on the accurate perception of the real world [182], if the observation exists mismatches between the environment for training (sim) and for applying (real), Sim-to-Real gap arises. The observation gap arises for two reasons: 1. The observed information's completeness - \u2206perception, most of the time the osim is too perfect and ideal that osim = ssim, while in oreal exists missing information that o\u021beal \u2260 seal - partial observation problems (POMDP), so for rigorousness, we will use ot in this paper. 2. The mismatch of feature representation - As, caused by perception resolutions, sensor noises, etc., and then it leads to the difficulty of performing as expected in Esim.\nThen, the action-taking also leads to Sim-to-Real gaps in two ways: 1. Action granularity - AA. The actions make real effects, such as grabbing and moving objects for robotics, but a\u015fim \u2208 As are mostly oversimplified or discretized for simulator construction Ms, and ideally executed in Esim by a\u015fim ~ \u03c0\u2081(\u00b7|o\u0219im), osim \u2286 St. However, the control movement in real action space areal \u2208 A, is essentially continuous and flexible, leading to meticulous control options in the real world. 2. System state gaps - Asystem. During action execution, system latency is inevitable. Most simulators assume actions trigger instantly, but real-world mechanical components introduce delays, further aggravating the Sim-to-Real gap.\nBeside the observation and action making, the transition gap from environments is also a severe cause. We analyze this issue by discussing the intuitive 'next-state divergence'. It explains a scenario that, given the same state st and action at, for the next step state st+1 in given two environments Esim and Ereal are different, which is a result of the transition probability differences between Ps and Pr that: Ps (st+1|st, at) \u2260 Pr(st+1|st, at), and the inherent cause is the system dynamics gap - Asystem as shown in the Figure 2.\nThe dynamic systems difference causes the challenge for policy learning and especially the deployment in Ereal.\nThe reward function r\u015fim := r(o\u015fim, a\u015fim) is an another crucial aspect that an RL algorithms performs Sim-to-Real gaps. There are two main reasons why an RL policy may perform unexpectedly due to the reward function: 1. Reward function design in Asystem. The reward function design is based on the understanding of behavior causes and expectations, but researchers tend to leverage accessible simulators to design rather than quantify the real world, then such system gaps would lead to in-comprehensive design of Rs, such as uncovered real-world cases or unexpected actions, and further leads to performance impairment in Ereal, even causes safety concerns. 2. Cascade result of action delay or granularity difference AA also leads to undesired performance impairment since the reward is a direct consequence of action.\nThe above analysis conceptually covers the comprehensive causes of Sim-to-Real problems in RL. In the following section, this paper will discuss the impact the Sim-to-Real issue, and then focus on a categorized technical solution study based on the four aspects: Observation, Action, Transition and Reward. Since the Large Foundation Models are revolutionizing the major research areas, we also spend a section to introduce how each aspect has benefited or can potentially benefit from the the the foundation models."}, {"title": "2.2 Impact", "content": "The Sim-to-Real problem significantly impacts the usability of RL in the real world [63]. This not only leads to monetary costs but also safety concerns, attracting significant attention across multiple research domains [64, 114]. In robotics, simulator-learned walking agents often fail in practical deployment [189], in autonomous-driving cars, fatal crashes happened in real-world executions [193], and in traffic signal control, the simulator-learned traffic light policy can hardly handle the realistic traffic dynamics, leading to unideal solutions [46]. These examples resulted in wasted training costs, time and limited real-world applicability. Thus, in this paper, we explore the causes of Sim-to-Real deployment failure and categorize current solutions based on the components of the MDP, helping researchers easily identify issues and refer to relevant solutions."}, {"title": "3 Techniques", "content": "In this section, we formally introduce the solutions to Sim-to-Real in main conceptions of MDP.\n3.1 Observation\nBridging the sim-to-real (sim2real) gap in RL necessitates addressing discrepancies in observational data, particularly those arising from variations in sensor modalities such as cameras and tactile sensors. Various strategies have been developed to mitigate these differences as shown in Figre. 3 [155]:\nDomain Randomization. Observation-based domain randomization focuses on the state element S of the Markov Decision Process (MDP) to mitigate the sim-to-real gap, distinguishing it from approaches like transitional domain randomization, which target transition dynamics as discussed in Section 3.3. Observational domain randomization introduces variability into the visual parameters of the simulation-such as textures, lighting, and object positions-or into the sensors used for observations. By exposing models to a diverse range of simulated scenarios, this technique enhances robustness to the unpredictable nature of real-world environments [206].\nCommon methods include randomizing textures, lighting conditions, object positions, object and background colors, as well as camera-related parameters such as position, depth, and field of view [62, 120, 163, 179, 207, 244]. In practice, these features are often used in tandem to create diverse training scenarios that encompass both subtle changes in appearance (e.g., lighting or colors) and more significant variations in spatial configuration or perspective (e.g., camera adjustments or object placements). For example, [120] demonstrates the effectiveness of domain randomization by training a robotic system to perform tasks such as pushing, moving, and picking up objects, even in the face of substantial changes in lighting, texture, and object position. By exposing the agent to a wide variety of simulated conditions, the system becomes more robust to real-world visual challenges, resulting in a more adaptable policy for use in deployment. A natural conclusion to draw would be to heavily randomize as many features as possible to produce as robust of an RL policy as possible, but this can destabilize RL policy training and lead to divergence [244]. Both [244] and [163] address this by utilizing a curriculum-based domain randomization approach. [163] introduces Automatic Domain Randomization (ADR), which both eliminates the need for manual fine-tuning of randomization ranges for domain randomization and uses a curriculum-based approach, gradually increasing environment difficulty as the policy improves. They utilized ADR to train both the policy and vision model, incorporating Gaussian noise with randomized parameters into observations during policy training, while randomizing visual features such as lighting conditions and camera perspectives to improve vision training. More recently, [244] introduced a generalizable framework for vision-based reinforcement learning which includes a curriculum-based approach to domain randomization for stabilizing RL training and improving sim-to-real transfer. Utilizing this curriculum-based approach, they achieved state-of-the-art performance for vision-based RL.\nDomain Adaptation. This category focuses on aligning the observation feature distributions between simulated and real data. The observation can be any sensible features that are helpful for decision-making, such as images [94, 239], sensors [28], or LiDARS, etc. Techniques such as adversarial training [23, 90, 104, 148, 180], and embedding alignment [167] are employed to minimize the discrepancy between the two domains, enabling the model to perform consistently across both environments [93].\nGoing beyond the simple feature embeddings, [102] proposed a self-supervised framework to optimize latent state representation through sequence-based objectives, which demonstrates superior performance in visual robotic manipulation tasks. [78] extended the concept of domain adaptation to real-synthetic depth data by coupling realistic degradation and enhancement techniques, it models realistic noise patterns in synthetic depth maps and enhances real-world depth data using a color-guided sub-network, achieving generalization to diverse real-world scenarios without requiring additional fine-tuning. And [174] emphasizes the potential of domain adaptation across novel modalities by tackling the sim-to-real gap in event-based cameras.\nHowever, those approaches require computationally expensive adaptation during training, the other branch of work considers the efficiency [22]. In [211], authors proposed Bi-directional Domain Adaptation to bridge the sim-vs-real gap in both directions: real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap, which proves with dramatic speed-up compared to the traditional method. [248] introduced a complementary real-to-sim adaptation framework, \u201cVR-Goggles\", that shifts the focus from adapting synthetic data to real domains to translating real-world image streams back into synthetic modalities during deployment, it minimizes computational overhead in the training phase while maintaining model performance across diverse real-world scenarios. Recently, [73] proposed an architecture that combines domain adaption and inherent inverse kinematics into one model, which helps reconstruct canonical simulation images from randomized inputs and improves robot grasping accuracy. A different line of work adapts ideas of metacognition from psychology [72] to AI systems [226]. Here, a \u201cmetacognitive model\u201d to identify or reason about failures in a base model. Early work has focused on training an additional model to predict failures [50, 178], while more recent approach known as error detection rules allows for lightweight learned rules [116]. To-date these techniques have been applied to perception problems; integrating them in an RL framework is a promising future avenue to address the sim-to-real gap.\nSensor Fusion. Combining data from multiple sensors can enhance the robustness of RL policies [147]. For example, integrating visual data with depth sensors [21], combining LiDAR and camera inputs [92, 184], or merging auditory and inertial measurements allows the model to compensate for the limitations of individual sensors, leading to improved performance in complex tasks [134]. Specifically, [60] proposed a multi-sensor fusion framework in four-wheel-independently-actuated electric vehicles, combining GPS and inertial measurements to address biases and noise in individual sensors, which highlights the critical role of sensor fusion in improving real-time estimations for sim-to-real applications, particularly in dynamic environments.\nFoundation Models. Recent advancements have explored the integration of large language models (LLMs) [2] and multimodal foundation models [164] to understand physical world [16], and further mitigate observation discrepancies in sim2real scenarios. These models, with their extensive pretrained knowledge and reasoning capabilities, can be leveraged to interpret and align observational data across domains. For example, natural language descriptions have been utilized to create a unifying signal that captures underlying task-relevant semantics [240], which are also known as semantic anchors, remain consistent in Esim and Ereal, aiding in bridging the visual gap between simulation and reality [250]. Vision-Language Models (VLMs), by combining visual and textual data processing, can assist in generating descriptive annotations for sensory inputs, facilitating better understanding and alignment between simulated and real-world observations [48, 158], and even facilitate simulation framework designs [185].\nIn summary, addressing observation discrepancies in sim2real transfer involves a combination of techniques aimed at enhancing model robustness and adaptability. The incorporation of LLMs presents a promising avenue for further reducing the observation gap, thereby improving the efficacy of RL policies in real-world applications.\""}, {"title": "3.2 Action", "content": "Action-taking is a key step to proceeding with any active control policy and results in the environment to make a difference. This section covers three main aspects of action that can mitigate the Sim-to-Real problem, as shown in Figure 4. The methods are categorized into Action Space Scale, Action Delay, and Action Uncertainty.\nAction Space Scale. Actions have the most direct influence on the environment. However, due to the simulator's limitations, they are often discretized or simplified to reduce the design effort of fidelities. The most common scenario is the Discrete (sim) to Continuous (real) gap. To bridge such gaps between the high-level discrete action space learned by the agent and the robot's real-world low-level continuous action space, [10, 115] proposes a subgoal model to identify nearby waypoints in the simulator navigation graph during navigation tasks, which helps the policy under low fidelity to perform well in the real world. And Action Shielding [1, 9] focuses on ensuring that the actions selected by an agent in a simulated environment remain safe [161], feasible, and effective when transferred to the real world. Common methods often leverage a safety layer or filtering mechanism that evaluates an agent's chosen actions against predefined constraints or real-world feasibility metrics [83]. For instance, during the transition from a discretized (sim) to continuous (real) action space, shielding can act as an intermediary, modifying or rejecting unsafe actions to prevent potential damage to the real-world system or environment. This ensures that high-level policies developed in the simulator can operate reliably and safely in dynamic, low-fidelity real-world scenarios.\nAction Delays. Another idealization of action-taking in the simulator is that the action often happens immediately. However, in the real world, it mostly comes with a delay [58, 64, 255]. Multiple domains tackle delayed action problems, such as network management [85, 124, 125, 198], which deals with the impracticality of real-time blocking or scheduling. In the energy domain, [8, 202] manage energy without compromising the timely flow of data. Considering such delay variables in RL methods is an important step before real-world deployment.\nIn a Constant-delayed MDP system, [71] proposes a predictive model inspired by how humans subconsciously anticipate the near future in physical environments to deal with delay-led consequences. In Random-Delay MDP (RDMDP), early work [11] empirically points out that randomized delays in the training process help to learn a more robust policy in the real world. Until [24] formally defined the RDMDP and proposed a Delay-Correcting Actor-Critic (DCAC), which adopts action buffers and leverages delay measurements to correct for delays in the agent's actions. This approach generates actual on-policy sub-trajectories from off-policy samples, successfully improving the policies' performance under real-world action delay scenarios. In contrast, [243] presents the Prediction model with Arbitrary Delay (PAD), a multi-step prediction model that mitigates cumulative error through a single prediction step rather than iterative updates, as in DCAC. PAD employs a gated unit that dynamically adjusts the feature extraction layers for different delays, enabling quick adaptation to random delay scenarios. Similarly, [191] defines the problem as a Control-delay MDP and proposes two temporal difference-based methods, D-SARSA and D-Q, which compensate for action delays without state augmentation [159] by updating Q-values based on effective delayed actions, improving performance under delayed conditions. The paper [55] tackles execution delay in RL by using a Delayed-Q algorithm. Instead of relying on traditional state augmentation, which can exponentially increase complexity, this paper infers future states using a forward model based on the delayed action sequence. The algorithm then updates the Q-values with the inferred future state, allowing the agent to make more accurate decisions that compensate for action delays.\nAction Uncertainties. Action-taking inevitably involves uncertainty. Even a well-learned policy can encounter unseen scenarios, making real-world decision-making challenging. Incorporating uncertainty quantification brings great benefits for a simulator-trained policy to generalize to wider real-world scenarios. Here we cover two aspects of uncertainty-enhanced action taking: Action Advising and Action Robust RL.\nAction Advising [99] is an RL technique where an agent receives guidance from a more experienced entity (e.g., a human or another agent) on which action to take in uncertain situations. Recently, the work [49] proposes RCMP (Requesting Confidence-Moderated Policy advice), which uses epistemic uncertainty to guide action selection. RCMP estimates uncertainty by learning multiple value function estimates and computing their variance, providing a reliable measure of action confidence. This is especially useful for sim2real tasks where accurate decision-making under uncertainty is crucial. [144] introduces a Model Predictive Controller that prioritizes safer actions by evaluating each action's expected collision probability and uncertainty. Actions with lower uncertainty and lower collision probability are chosen, allowing the agent to cautiously avoid dynamic obstacles. This approach enables safer decision-making in safety-critical scenarios by avoiding high-risk actions when uncertainty is detected.\nAnother branch of research treats action-related uncertainty-aware Reinforcement Learning as Action Robust RL. As a sub-domain of robust RL [224], it is different from Action Advising, without relying on external advisors, it focuses on improving the robustness of an agent's actions in uncertain or adversarial environments with unexpected disruptions. In [205], the authors address action uncertainty by introducing two models: Probabilistic Action Robust MDP (PR-MDP) and Noisy Action Robust MDP (NR-MDP). These models help in selecting safer actions under uncertainty by considering adversarially affected outcomes, enabling the RL agent to maintain stable performance even under unexpected disturbances in real world. Following this work, the paper [135] introduces the ARRLC algorithm and handles action uncertainty by simulating the agent's chosen action being replaced by an adversarial action with a probability p. ARRLC uses both optimistic and pessimistic estimates of the Q-function, allowing the agent to balance exploration and adversarial planning effectively.\nFoundation Models. Since foundation models are trained on massive corpus and show strong zero-shot capabilities, they are adopted to solve the generalizability challenges in unseen or rare scenarios' action-takings. Such as [51] combines local policies with VLMs for motion planning, by training the simple local policies, these policies are serving as an action pool, e.g., pick, open, close, etc., and the foundation model will provide a planning strategy using these actions to finish the task. It shows superior performance on Robosuite benchmark [257]. Similarly, [177] proposes SayNav, which grounds LLMs for dynamic planning to effectively navigate and finish tasks in new large-scale environments. Specifically, SayNav integrates an incremental scene graph generation, an LLM-based planner, and a low-level executor, and achieves state-of-the-art performance on the Multi-Object Navigation task.\nThe LLMs are also proved to be able to improve RL's sample efficiency by leveraging their internal knowledge to generate preliminary rule-based controllers for robot tasks, which guide the exploration process and reduce the number of interaction samples required for effective learning [33]."}, {"title": "3.3 Transition", "content": "In Sim-to-Real challenges, discrepancies in transition dynamics between simulated and real-world systems significantly impair policy deployment performance as showcased in exploration [46], in this section, we will introduce four categories of methods that solve the Sim-to-Real by bridging the transition dynamics gaps.\nDomain Randomization. This method introduces variability within the simulator by randomizing physical parameters, enabling the simulated environment to encompass a wide range of potential real-world conditions [37, 52, 207]. By exposing policies to diverse simulated scenarios, domain randomization enhances robustness, facilitating smoother transitions to real-world environments where conditions may not precisely match any single simulated setup. E.g., the work [217] randomized environmental factors such as friction, and motor torque, etc., ensuring that the trained policy is generalized effectively to real-world conditions without additional fine-tuning. And Mehta et al. [150] introduced Active Domain Randomization (ADR), which enhances traditional domain randomization by concentrating training on the most challenging environment variations. ADR actively identifies and prioritizes configurations that lead to significant policy discrepancies, training agents on variations most likely to improve generalization to unseen real-world conditions.\nDomain Adaptation. This strategy focuses on aligning parameter distributions between simulated and real-world domains [35, 69]. This typically involves adversarial training techniques to minimize discrepancies between system features in both domains [141, 169, 216]. By adapting the simulated domain to closely match the target domain's distribution, domain adaptation reduces mismatches in transition dynamics, enabling policies to generalize more effectively to real-world conditions while preserving specific traits essential for successful deployment.\nGrounding Methods. Grounding methods adjust simulator dynamics to align with real-world dynamics through grounded actions. Hanna and Stone [84] proposed Grounded Action Transformation (GAT), which adjusts simulator dynamics to align with real-world dynamics through grounded actions. Building upon this, Desai et al. [57] introduced Stochastic Grounded Action Transformation (SGAT), incorporating stochastic models into the grounding process using a probabilistic approach to model transition dynamics. Unlike GAT's deterministic setup, SGAT better approximates real-world stochastic behavior by learning a distribution over possible next states, enhancing robustness and policy transfer in variable environments. Karnan et al. [107] developed Reinforced Grounded Action Transformation (RGAT), which integrates RL directly into the grounding process. RGAT treats grounding as an RL problem, enabling end-to-end training of the action transformer as a single neural network, reducing error accumulation by learning a unified transformation function that optimally adjusts actions in simulation to match real-world dynamics. Further extending grounding methods, Desai et al. [56] introduced Generative Adversarial Reinforced Action Transformation (GARAT), framing action transformation as an Imitation from Observation (IfO) problem. GARAT employs a generative adversarial approach to minimize distribution mismatches between source (simulator) and target (real-world) dynamics. Through adversarial training, GARAT learns an action transformation policy that mimics the target environment by observing state transitions without explicit action labels, allowing for more accurate simulator adjustments. Additionally, Da et al. [47] introduced uncertainty quantification to the GAT framework, enhancing decision-making reliability during Sim-to-Real policy training.\nLLM-Enhanced Approaches. Based on the grounding methods, a recent study [44] tends to incorporate the LLM inference ability to the forward model's prediction of real-world dynamics, and then, based on better learned forward model, the predicted \u015dt+1 is more reliable, so the inverse model can produce grounded actions in an effective way by taking the ($t+1, st). The empirical results show that such an attempt can efficiently improve the accuracy of real-world next-state prediction, and analysis shows a positive correlation between the accuracy of real-world dynamics depiction and the sim-to-real performance of the learned policy. This work offers a novel approach to incorporating LLMs' knowledge into Sim-to-Real policy training.\nIn summary, addressing transition dynamics discrepancies in Sim-to-Real involves a combination of traditional methods such as domain randomization, domain adaptation, and grounding methods, alongside emerging LLM-enhanced strategies. These approaches collectively enhance the robustness and adaptability of RL policies, facilitating more effective deployment in real-world applications."}, {"title": "3.4 Reward", "content": "In reinforcement learning (RL), the design of reward functions is crucial for effective policy learning, especially when transferring from simulation to real-world environments (Sim-to-Real). To address the challenges associated with reward functions in Sim-to-Real scenarios, two primary categories of techniques have been explored: reward shaping and LLM-based reward design.\nReward Shaping. Reward shaping techniques focus on modifying the reward function to provide more informative and dense feedback, thereby guiding the agent toward desired behaviors more efficiently. These methods are particularly beneficial in Sim-to-Real contexts, where discrepancies between simulated and real environments can impede learning.\nPotential-based reward shaping [13] augments the original reward function with a potential function that reflects prior knowledge about the task. This ensures that the optimal policy remains unchanged while accelerating the learning process by providing intermediate rewards that guide the agent toward the goal [173, 245], automaton-guided reward shaping [197, 219] further refine this approach by leveraging structured representations - automata, to mitigate sparse reward challenges. It dynamically updates reward functions based on the utility of automaton transitions, improving both learning speed and robustness. Another approach involves assistant reward agents, which collaborate with the primary policy agent to generate supplementary reward signals based on future-oriented information. These auxiliary agents enhance sample efficiency and convergence stability by dynamically adapting the reward structure, facilitating effective exploration and exploitation during training [117].\nLLM-Based Reward Design. The advent of large language models (LLMs) has opened new avenues for automating and refining reward function design in RL, particularly for complex tasks requiring nuanced reward structures. These techniques leverage LLMs' generative and reasoning capabilities to address reward design challenges in Sim-to-Real scenarios.\nAutomated reward function generation uses LLMs to create reward function code from natural language task descriptions. Frameworks like CARD iteratively produce and refine reward functions without human intervention, aligning the generated rewards with task objectives through dynamic feedback mechanisms [201]. Evolutionary reward design combines LLMs with evolutionary algorithms to optimize reward functions. LLMs propose diverse candidate reward structures, which are then evaluated and improved through evolutionary search, resulting in more effective and tailored rewards [157]. Finally, text-to-reward frameworks such as Text2Reward [234] automates the creation of dense reward functions from textual task specifications. By translating natural language descriptions into executable reward code, these systems reduce the need for domain-specific expertise while enabling rapid development of reward functions for various tasks [187], and this reveals a potential angle in solving the Sim-to-Real problems, especially in the zero-shot RL direction. Zero-shot or few-shot RL typically involves learning a representation that encapsulates task-relevant features. Based on this representation, policy generation occurs during inference time. At this critical stage, the design and refinement of reward functions play an important role. By dynamically adjusting rewards during the inference process, it is possible to effectively bridge the sim-to-real gap.\nIn summary, tackling the challenges of reward function design in Sim-to-Real scenarios involves both traditional techniques like reward shaping and innovative approaches leveraging LLMs. These methodologies enhance the alignment between simulated training and real-world deployment, improving the robustness and effectiveness of RL policies."}, {"title": "4 Domain-Based Discussions", "content": "Sim-to-Real transfer is a pervasive challenge across reinforcement learning (RL) applications, with each research domain adopting specialized simulators and benchmarks to address its unique real-world complexities. In this section, we categorize the literature into subdomains and provide an overview of the prevalent research resources-including simulation platforms and evaluation benchmarks, as well as the distinct research focuses in each domain.\n4.1 Domain-Specific Research Focus"}, {"title": "4.2 Simulators and Sim-to-Real Benchmarks", "content": "The discussion in Sec. 4.1 reveals the necessity of conducting domain-specific Sim-to-Real design, and in this section, we introduce the 'Simulators' and Sim-to-Real 'benchmarks' from a domain-specific level. Similarly, we divide based on Robotics, Transportation, Recommender Systems, and others in Table. 3. This categorization helps researchers to easily focus on the most relevant simulators, codes, and benchmarks.\nRobotics. There are various simulators developed to model robotic systems with high fidelity, there are also researchers who tend to leverage LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation [96, 108", "112": "is an open-source 3D robotics simulator that integrates with the Robot Operating System (ROS) to provide realistic rendering of environments and physics for testing robot models and algorithms. MuJoCo [208", "40": "serves as an easy-to-use Python module for physics simulation, supporting both robotics and RL research with real-time collision detection and multi-body dynamics. OpenAI Gym [162", "257": "is a simulation framework designed for robot learning, providing a collection of benchmark tasks and environments to test the performance of RL algorithms in manipulation and control tasks"}]}