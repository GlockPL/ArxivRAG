{"title": "SPRIG: Improving Large Language Model Performance by System Prompt Optimization", "authors": ["Lechen Zhang", "Tolga Ergen", "Lajanugen Logeswaran", "Moontae Lee", "David Jurgenst"], "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many scenarios, but their performance depends, in part, on the choice of prompt. Past research has focused on optimizing prompts specific to a task. However, much less attention has been given to optimizing the general instructions included in a prompt, known as a system prompt. To address this gap, we propose SPRIG, an edit-based genetic algorithm that iteratively constructs prompts from prespecified components to maximize the model's performance in general scenarios. We evaluate the performance of system prompts on a collection of 47 different types of tasks to ensure generalizability. Our study finds that a single optimized system prompt performs on par with task prompts optimized for each individual task. Moreover, combining system and task-level optimizations leads to further improvement, which showcases their complementary nature. Experiments also reveal that the optimized system prompts generalize effectively across model families, parameter sizes, and languages. This study provides insights into the role of system-level instructions in maximizing LLM potential.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have proven highly effective at many tasks (Naveed et al., 2023) and prompting has become the primary way for end-users to elicit desired responses (Brown et al., 2020). These prompts contain a variety of instructions such as explanation of the task (Li et al., 2022), personas (Kim et al., 2024), formatting constraints (Wang et al., 2023), and meta-rules like \"think carefully\" (Li et al., 2024). Previous studies have shown that the selection of prompts can have a substantial impact on the quality of the output (Reynolds and McDonell, 2021). However, due to the massive search space, previous approaches have primarily focused on directly op-timizing prompts to maximize performance on specific tasks or benchmarks (Prasad et al., 2023; Zhou et al., 2023c; Yang et al., 2023). While effective, they are typically task-specific, requiring new prompts to be crafted for every new task. Here, we consider an alternative approach focused on optimizing the system prompt, i.e., the set of general instructions that precede any task-specific details (Figure 1), with the goal of identifying task-agnostic generalizable prompting strategies.\nPrior work has shown that meta-instructions can be effective for improving performance (Reynolds and McDonell, 2021). Most notably, evoking Chain of Thought (CoT) reasoning with instructions like \"let's think step by step\" has led to gains for several types of tasks (Wei et al., 2022), though not all tasks benefit (Sprague et al., 2024). Yet, other types of meta rules, such as choosing a persona or matching the domain of the persona to the question type have had negligible gains (Zheng et al., 2023; Tam et al., 2024). A recent survey paper (Schulhoff et al., 2024) suggests that these existing system prompting strategies are isolated and highly sensitive to specific scenario details, with the systematic function and generalization mech-"}, {"title": "2 Related Work", "content": "Prompt selection has been extensively studied and proven to significantly impact model output quality (Reynolds and McDonell, 2021). Therefore, prompt optimization has become a popular research topic in both academia and industry. Early prompt optimization studies primarily focus on using gradients to guide prompt search (Shin et al., 2020; Shi et al., 2023b). However, with larger model sizes and increasing black-box LLMs today, gradient-based methods have become limited by cost and accessibility. Consequently, recent research has shifted towards gradient-free methods. Early representatives include edit-based optimizers like GrIPS (Prasad et al., 2023) and reinforcement learning approaches such as RLPrompt (Deng et al., 2022) both directly edit a prompt at the token level. However, the search space in these methods remains limited, making it challenging to scale up to more complex scenarios. Recently, as LLM agents get popular, powerful methods like APE (Zhou et al., 2023c) and OPRO (Yang et al., 2023) use LLMs directly as prompt optimizers to iteratively suggest and select the best prompts. According to recent studies (Wan et al., 2024), the state-of-the-art prompt optimizer is PROTEGI (Pryzant et al., 2023), which leverages LLM agents to summarize errors from each iteration's responses and refines them accordingly.\nPrevious prompt optimization methods largely focus on optimizing the instructions for specific tasks (which we refer to as Task Prompt) which inherently have limited generalizability. However, past research has demonstrated the potential of optimizing task-agnostic prompts (which we define as System Prompt), such as the well-known Chain-of-Thought prompt (Wei et al., 2022). Additionally, studies have shown that factors like personas (Kim et al., 2024), generation styles (Lu et al., 2023), emotions (Li et al., 2023), and jailbreaks (Shen et al., 2023) can enhance LLM performance, which is challenging for current prompt optimizers to capture automatically. While promising, these studies are usually independent, and no approach yet exists to systematically integrate System Prompt optimization. Therefore, we aim to address this gap by developing a System Prompt optimizer that discovers an effective System Prompt, enabling a single prompt to boost performance across various domains."}, {"title": "3 SPRIG: System Prompt Refinement for Increased Generalization", "content": "To address the large design space of system prompts, we use a genetic algorithm inspired approach, SPRIG, that iteratively adapts the best candidate prompts. Following we describe the algorithm, data, and search heuristics used.\nPrompt Component Corpus Our approach builds on a corpus of possible instructions in system prompts, referred to as components. By starting from a large selection of possible components, we ensure that prompts are coherent while also gaining significant efficiency. We define a component as a minimum prompt unit with complete semantics (typically a sentence, like \u201cLet's think step by step\").\nComponents are drawn from multiple sources. Our prompt component corpus, denoted P, contains 300 prompt components in 9 categories, including 146 good property, 43 role, 22 style, 17 emotion, 13 scenario, 9 jailbreak, 16 behavioral, 18 Chain-of-Thought, and 16 safety components. The majority of prompt components are sourced from the taxonomy and studies mentioned in the recent prompt survey paper (Schulhoff et al., 2024), while a smaller portion are synthesized by GPT-4 or crafted by authors (e.g., some good properties and scenarios). Details and citations of prompts in P are shown in Appendix Table 1.\nSPRIG pipeline We design a genetic pipeline SPRIG for System Prompt optimization. The pipeline applies edit-based, gradient-free beam search to iteratively optimize the prompt. Figure 2 shows the SPRIG workflow. At each iteration, the model begins with Beam Size number of System Prompts from the previous iteration. Then, the model enumerates the following edit operations for each prompt: (1) Add: Add a component from P into the existing prompt. (2) Rephrase: Rephrase a component in the existing prompt using paraphrasing model tuner007/pegasus_paraphrase (Zhang et al., 2020). (3) Swap: Swap the order of two components within the existing prompt. (4) Delete: Delete a component from the existing prompt. Note that no restrictions are imposed on the edits above for a more comprehensive exploration, meaning that identical, semantically irrelevant or even conflicting-components may appear in the same prompt.\nEach step generates tens of thousands of candidate prompts. To filter this search space efficiently, we use the Upper Confidence Bound (UCB) algorithm (Auer et al., 2002) to prune the Add edits. The 60 prompt components with the highest UCB scores are chosen to reduce the chances of selecting poorly performing components again, where $UCB_i = R_i + C \\sqrt{\\frac{\\ln(n)}{N_i}}$ and i refers to the prompt component, $R_i$ is the average score improvement of component i, C is the Exploration-exploitation balancing constant (we choose C = $\\sqrt{2}$ to encourage more exploration), n is the total number of times any component is selected, and $n_i$ is the number of times component i is selected. To measure score improvement, k questions are randomly selected from each task's training set, and all new prompts are evaluated on this sampled subset. The top Beam Size prompts with the highest average scores from this iteration proceed to the next iteration."}, {"title": "4 Experiments: Optimization Benefits", "content": "In this section, we evaluate SPRIG's performance on the test set of in-domain tasks in our benchmark combination. These questions are invisible in the prompt optimization process."}, {"title": "4.1 Experiment Setup", "content": "Tasks To maximize the generalization ability of the optimized System Prompt, we select a broad range of tasks, using a combination of 42 different benchmarks covering 7 categories (reasoning, math, social-understanding, commonsense, faithfulness, knowledge, language-understanding). Our selection includes widely used benchmarks such as MMLU (Hendrycks et al., 2021), BBH (Suzgun et al., 2023), and TruthfulQA (Lin et al., 2022), but also includes various social-understanding benchmarks like SocKET (Choi et al., 2023). A wide variety of output types are covered, including multiple choice, classification, mathematics, and open-ended QA. The full list of benchmarks and categories is shown"}, {"title": "4.2 Results", "content": "Optimizing the System Prompt provides consistent improvement to LLMs on par with task optimization, as seen in Figure 3, when compared with the Blank system and Simple task combination baseline. These improvements were similar across all three models, shown in Appendix Figure 12. SPRIG improves ~10% over the unoptimized version, which significantly outperformed the baseline CoT method. Although its performance still"}, {"title": "5 Experiments: Generalization", "content": "In the next series of experiments, we test how well the system prompts generated by SPRIG generalize to new settings.\nCross-model Generalization The current system-optimized prompts were all generated with respect to a specific LLM. Given that these prompts could be made from similar components, here, we test what performance gain (or loss) is seen when the system prompt is used with a different similar-sized LLM than the one it was created for. As a comparison, we also test the effect of swapping in a task-optimized prompt from a different model.\nOptimized system prompts generalize well across models in terms of performance gain, as shown by the aggregated performance in Figure 8.2"}, {"title": "Language Generalization", "content": "The LLMs used in our experiments are capable of reasoning in different languages and can support input in multiple languages. Although our previous experiments were only in English, the optimizations to the system- and task- parts of the prompt may still provide performance improvements for tasks in other languages. Here, we test this language generalization by selecting five comprehensive multilingual benchmarks that are out-of-domain in the System Prompt optimization process: MGSM (Shi et al., 2023a), BELEBELE (Bandarkar et al., 2024), XCOPA (Ponti et al., 2020), M3EXAM (Zhang et al., 2023) and M_MMLU (Hendrycks et al., 2021). Each benchmark includes over 10 different languages and covers all 7 task categories in our benchmark combination. We directly use the same optimized System Prompt from \u00a7 4.2 (in English). Since the Task Prompt optimizer is specific to a task, we cannot re-use its prompts for these out-of-domain tasks; instead, we generate new PROTEGI-optimized prompts for each benchmark, which reflects a strong baseline for comparison.\nOur optimized system prompt from \u00a7 4.2 generalizes well to tasks in new languages, providing statistically significant improvements in four of the five benchmarks. SPRIG shows a clear advantage over other approaches on XCOPA (Causal Commonsense Reasoning) and the comprehensive"}, {"title": "Model Size Generalization", "content": "All prompts were generated for and tested on mid-sized LLMs. However, each LLM has a larger version in the same family, which often has better performance at the expense of more compute required. Being able to optimize the system prompt with a smaller model and then deploy that prompt on a larger model to the same effect would have significant performance benefits. Therefore, here, we test for generalization when using a prompt from a smaller LLM with a larger version. Specifically, we test with LLAMA3.1-70B-INSTRUCT, MISTRAL-LARGE-INSTRUCT-2407 and QWEN2.5-72B-INSTRUCT. We use the same evaluation setup as in previous sections, with only the LLMs' parameter size changed.\nBoth system- and task-optimized prompts do not provide statistically significant performance gains when created using a smaller model and then applied to a larger, as shown in Figure 10.3 However, a system+task optimized prompt provides a 1.6% improvement, suggesting that this approach can generalize. Therefore, we conclude that existing prompt optimizations can generalize to larger parameter sizes but need to consider both the system"}, {"title": "6 Analysis: Prompt Embedding Space", "content": "Given the performance improvements and generalization seen when using the prompt instructions introduced by SPRIG, it is reasonable to wonder what effect these instructions are having on the neural activations such that the LLM is likely to decode the correct answer. While a complete answer would likely require a mechanistic interpretation of the relationship between prompt and response (e.g., Bhargava et al., 2023), here, we attempt to gain some intuition on the instructions' effects by visualizing the embedding space during different optimization strategies and comparing the changes relative to the Simple baseline instructions.\nHere, we randomly sample 420 questions (10 per task) and probe the intermediate LLM hidden states under different experiment settings. We flatten the hidden states and perform Principal Component Analysis (PCA), and select the first two principal components for visualization.\nFigure 11 shows the PCA results for LLAMA3.1-8B-INSTRUCT. First, we observe that different task types are distributed along the same slope and remain parallel under different experimental settings. Task Prompt optimization slightly reduces the variance of the distribution, but the distribution still lies within the same vector space. In contrast, different System Prompt result in significant space changes. The basic CoT causes a slight overall shift, while SPRIG substantially moves the distribution to a new area. The other two LLMs' PCA are shown in Appendix Figure 18 and 19, and show similar trends. Thus, we propose the hypothesis that System Prompt optimization searches for appropriate re-gions in the global space, while Task Prompt optimization performs fine-tuning within a local space. This reveals the potential of System Prompt optimization to significantly alter model behavior and offers new insights for future prompt research to use System Prompt optimization first to locate an appropriate global behavior space, then use task prompt optimization to fine-tune downstream performance within that space."}, {"title": "7 Conclusion", "content": "This study introduced a novel genetic edit-based optimization framework, SPRIG, to improve LLM performance with the systematic construction of general-purpose system prompts. By leveraging a diverse collection of prompt components and evaluating across a diverse range of tasks, we demonstrate that optimized system prompts provide consistent improvements on par with optimized task prompts. Moreover, combining system and task prompt optimizations offers complementary benefits, leading to further improvements in model performance across varied domains. Further, we find that these performance benefits for an optimized prompt generalize across (i) model families, (ii) model sizes, and (iii) different languages. Our findings highlight the potential of system prompt optimization to complement and enhance LLM performance for new languages and models."}, {"title": "8 Limitations", "content": "Despite the promising results of SPRIG, several limitations remain in our study. First, the computational cost of optimizing system prompts is higher compared to task-specific optimization methods even though certain pruning was applied, which could limit its scalability to real-world applications. Therefore, exploring more effective pruning or exploring algorithms would be essential in future work. Second, data contamination has become a significant issue in LLM benchmarking today (Magar and Schwartz, 2022), especially for benchmarks that include tasks from several years ago. While we have aimed to select only benchmarks where there was still room for improvement (e.g., BigBench-Hard) or benchmarks that were released very recently after these LLMs were released, our research did not deeply explore how to mitigate this impact in system prompt evaluation. Future work could further investigate the effectiveness of prompt optimization on more recent benchmarks that are less affected by data contamination. Finally, our approach relies heavily on human-collected prompt components, which may introduce biases or constraints. While we have surveyed a broad collection of papers to cover a diverse design space of possible system-level instructions, future work could explore adaptive mechanisms for automatically expanding the component corpus to further improve the flexibility and performance of the optimization process."}, {"title": "9 Ethical Considerations", "content": "While this research has made efforts to minimize potential ethical issues, several ethical implications may still be present. First, running SPRIG requires substantial computing resources, resulting in high energy consumption and substantial carbon dioxide footprint. Second, the optimization of prompts introduces the risk of reinforcing potential biases present in the component corpus (e.g., any systematic downstream behavioral changes from prompting an LLM to be a \"professor\u201d), which may propagate unintended stereotypes or discriminatory behavior in model outputs. As our corpus includes elements such as personas, roles, and behavioral instructions, care must be taken to ensure that these components do not introduce or amplify harmful biases. Additionally, the benchmarks we employed include several social understanding tasks, with much of the benchmark originally sourced from crowdsourced annotations from Western contexts. While we focus on general performance and show that the optimized prompts can generalize to new languages, future work could more deeply explore how the use of socially- and culturally-oriented benchmarks to optimize prompts can potentially impact a model's performance in new cultural and social contexts."}, {"title": "A Appendix", "content": "A.1 Model Running Details\nWe run all our experiments on 4 NVIDIA-L40S-48GB GPUs. All LLM inferences are powered by VLLM 0.5.4 (Kwon et al., 2023), Hugging Face Transformers 4.43.3 (Wolf et al., 2020) and Py-Torch 2.4.0 (Paszke et al., 2019) on a CUDA 12.4 environment. Temperatures are set to 0.0 to minimize the effect of randomness.\nSPRIG spends around 60 hours to run a full optimization on one LLM with 4 GPUs, while PROTEGI takes around 10 minutes to optimize one task prompt on one LLM with 4 GPUs. Since our experiments only involved around 50 fixed tasks, the efficiency of SPRIG is still much lower than that of PROTEGI. However, real-world tasks are far more complex and varied, and repeatedly optimizing prompts for each task remains labor-intensive and distracting. Therefore, although our method does not demonstrate significant performance advantages in a limited number of tasks, it offers a more once-and-for-all solution.\nA.2 Prompt Component Corpus Details\nWe list the counts and representative in each prompt component category in Table 1.\nA.3 Benchmark Details\nWe list all benchmarks, categories, metrics and descriptions in Table 2. For each benchmark, the train/dev/test split is 40%:20%:40%. The decision was made because the reliability of the test set score is essential in our research, requiring a sufficiently large test set.\nA.4 Best System Prompts\nWe list the best system prompts from SPRIG for each LLM in our study in Table 3."}]}