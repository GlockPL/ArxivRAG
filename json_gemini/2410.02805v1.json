{"title": "Trust-informed Decision-Making Through\nAn Uncertainty-Aware Stacked Neural Networks Framework\nCase Study in COVID-19 Classification", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Fang Chen", "Amir H. Gandomi"], "abstract": "This study presents an uncertainty-aware stacked\nneural networks model for the reliable classification of COVID-19\nfrom radiological images. The model addresses the critical gap in\nuncertainty-aware modeling by focusing on accurately identifying\nconfidently correct predictions while alerting users to confidently\nincorrect and uncertain predictions, which can promote trust\nin automated systems. The architecture integrates uncertainty\nquantification methods, including Monte Carlo dropout and\nensemble techniques, to enhance predictive reliability by assessing\nthe certainty of diagnostic predictions. Within a two-tier model\nframework, the tier one model generates initial predictions and\nassociated uncertainties, which the second tier model uses to\nproduce a trust indicator alongside the diagnostic outcome. This\ndual-output model not only predicts COVID-19 cases but also\nprovides a trust flag, indicating the reliability of each diagnosis\nand aiming to minimize the need for retesting and expert verifica-\ntion. The effectiveness of this approach is demonstrated through\nextensive experiments on the COVIDx CXR-4 dataset, showing a\nnovel approach in identifying and handling confidently incorrect\ncases and uncertain cases, thus enhancing the trustworthiness of\nautomated diagnostics in clinical settings.", "sections": [{"title": "I. INTRODUCTION", "content": "THE COVID-19 pandemic affected numerous countries,\nleading to millions of cases and fatalities worldwide.\nDetecting and diagnosing COVID-19 at an early stage proved\nto be crucial in controlling its spread [1]. Rapid diagnosis\nfacilitated timely medical intervention and recovery. Alongside\npolymerase chain reaction (PCR) tests, chest radiography (X-\nrays) and computed tomography (CT) scans were employed\nas key diagnostic tools. Due to the limited availability of\nPCR tests in many regions, medical imaging emerged as a\nprimary method for diagnosing COVID-19 [1]. As a standard\nprocedure, these images required manual analysis by clinical\nexperts. However, the ongoing shortage of healthcare profes-\nsionals, especially in developing nations and smaller hospitals,\nmade quick diagnosis challenging and exacerbated the work-\nload on existing experts. At this moment, the advantages of\nAI became more obvious in healthcare.\nHowever, it appears that no AI-based healthcare tools\nhave been officially incorporated into clinical guidelines as\na standard part of medical practice [2]. A critical factor that\nhinders AI acceptance and adoption among both healthcare\nprofessionals and patients is the lack of trust in the decisions\nmade by AI systems [3].\nNumerous studies have investigated the underlying factors\ncontributing to the persistent challenge of establishing trust in\nAI within the healthcare domain, and several key factors have\nbeen identified [3]. One major factor contributing to the lack\nof trust in Al systems is their inconsistent performance across\ndifferent clinical settings [4]. This inconsistent performance\ncan be interpreted as uncertainty in machine learning (ML)\nand AI predictions.\nUncertainty mainly originates from two sources [5]: (I) data\nuncertainty, which is due to elements such as noise, com-\nplexity, and limited knowledge about environmental conditions\n(aleatoric uncertainty), and (II) parametric uncertainty, which\noccurs when the model is inadequate because of imprecise\nunderstanding of its components (epistemic uncertainty). The\npresence of various sources of uncertainty requires a model\nto explicitly assert the certainty of its outputs to establish its\nreliability and trustworthiness. For this purpose, uncertainty\nquantification techniques have been developed to assess and\ncommunicate the confidence levels associated with the model's\npredictions. The quantified uncertainty serves as a metric,\nwhere values above a specific threshold are flagged as un-\ncertain, while those below the threshold are deemed certain.\nIdeally, in an optimally calibrated machine learning model,\nthe quantified uncertainty for all correct predictions would\nfall below this threshold (indicating certainty or confidence),\nand for all incorrect predictions, it would exceed the threshold\n(indicating uncertainty). However, incorrect predictions could\nalso fall below this threshold, leading to confidently incorrect\npredictions. This conventional approach, where confidently\nincorrect predictions are not adequately addressed, exacerbates\nthe erosion of trust in ML and AI, particularly in high-stakes\nfields such as medical diagnosis.\nTherefore, this study proposes an expressive uncertainty-\naware model with a dual output: (1) the predicted outcome\nand (2) a binary flag indicating whether the outcome should be\ntrusted. This second flag is specifically designed to mark only\nconfidently correct predictions as trustworthy, thereby helping\nto avoid confidently incorrect predictions. This second output\nenables the model to flag predictions (especially in medical\ndiagnosis within healthcare) with a \"do not trust me\" alert,\nprompting human experts (such as physicians) to re-evaluate\nthem. Thus, the model not only enhances user experience\nbut also fosters greater adoption by encouraging cautious and"}, {"title": "II. BACKGROUND", "content": "ML including neural networks (NNs) have reached a peak\nin accuracy. Individuals may misinterpret high accuracy as\nindicating high confidence, based on their intuitive under-\nstanding of probabilities. In classification problems, a frequent\nmisunderstanding of 'confidence' arises when the probability\nassigned to a predicted class is mistaken as a true measure\nof the model's certainty. This is misleading because these\nprobability scores, especially in NNs, are typically calculated\nusing the Softmax function, which normalizes outputs to\nrepresent class probabilities rather than confidence levels [6].\nTo evaluate confidence or its inverse, uncertainty, it is cru-\ncial to recognize that most conventional ML models, including\nNNs, typically generate deterministic predictions by producing\na single output, or point estimate, for a given input [7]. This\napproach is not able to capture the variability (or in other\nwords uncertainty) in the model's predictions.\nTo illustrate this, consider a neural network $f$ parameterized\nby $w$, which maps an input $x \\in X$ to an output $y \\in V$.\nThe target output is obtained by optimizing the parameters\nof the network, such that $y = f_w(x)$. Rather than relying\non point estimation for $w$, assigning a probability distribution\nover the model parameters enables the derivation of a proba-\nbility distribution for predictions, allowing for the quantifying\nuncertainty regarding the model's knowledge (referred to as\nepistemic uncertainty). Bayesian inference methods - including\nMarkov chain Monte Carlo (MCMC) [8], Variational inference\n(VI) [9], Monte Carlo dropout (MCD) [6], Variational Au-\ntoencoders (VAE) [10], Bayes By Backprop (BBB) [11] - are\ncommonly employed to estimate the posterior distribution of\nthe model parameters to achieve this. In addition to Bayesian\ntechniques, ensemble learning is another method frequently\nused to quantify uncertainty. In a typical ensemble, each model\nindependently predicts the output for a given input. When\nthese models are diverse-built with different architectures,\nparameters, or trained on various data subsets-they produce\nprobabilistic predictions instead of single-point estimates. For\nfurther details on uncertainty quantification techniques, inter-\nerested readers can refer to [5].\nBy leveraging the aforementioned uncertainty quantification\n(UQ) methods, there has been a growing interest in the\ndevelopment of uncertainty-aware ML models, particularly\nNNs. These research efforts can be categorized into two main\ncategories:\nThe first category of studies emphasizes quantifying un-\ncertainty to enhance decision-making by communicating pre-\ndiction uncertainties. Typically, these studies generate pre-\ndictions accompanied by uncertainty estimates using one of\nthe aforementioned UQ methods. Predictions with the highest\nuncertainty are flagged as potentially inaccurate, enabling\nmore informed and cautious decision-making. Among these\nstudies, MCD is widely employed in healthcare to quantify\nuncertainty and improve decision-making. For instance, in\ncardiac arrhythmia detection, gated recurrent neural networks\n(GRUs) with MCD provide well-calibrated uncertainty esti-\nmates, crucial for clinical confidence [7]. Similarly, deep learn-\ning models with MCD are utilized for stroke outcome predic-\ntion, helping to identify high-risk predictions that necessitate\nfurther human evaluation [12]. Another application involves\nthe multi-level context and uncertainty aware (MCUa) model\nfor breast histology. The model uses context-aware networks\nto learn spatial dependencies among image patches and applies\nMCD to measure confidence levels based on the standard\ndeviation of multiple predictions. Lower standard deviations\nare interpreted as higher confidence, and predictions with\nlow uncertainty are selected as confident. [13]. Furthermore,\nMCD enhances colorectal polyp classification by utilizing\npredictive variance and entropy for uncertainty measurement,\nalong with temperature scaling for confidence calibration [14].\nApplication of MCD is not limited to health care and in the\ndomain of credit card fraud detection, MCD combined with\nensemble methods quantifies prediction uncertainty, thereby\nenhancing the clarity of the system's reliability in the detection\nprocess [15].\nWhile MCD is a popular choice in healthcare, Bayesian\ndeep learning techniques have found application across a wider\nrange of domains, providing robust uncertainty quantification.\nIn engineering applications, Bayesian neural networks (BNNs)\nand stochastic variational Gaussian processes (SVGPs) are\nused for building energy modeling, providing predictions\nwith confidence intervals that optimize resource allocation\nand enhance model robustness [16]. In nuclear power plants,\nBayesian models estimate predictive uncertainty to enhance\ndecision-making and risk management in health monitoring\nsystems [17]. Another study leverages Bayesian networks\nin agribusiness risk assessment to quantify uncertainty and\nimprove out-of-domain calibration, aiding financial decision-\nmaking [18]. Furthermore, in transformer diagnostics, Gaus-\nsian Bayesian networks (GBNs) are combined with black-"}, {"title": "III. METHODOLOGY", "content": "Stacked generalization, commonly known as stacking, is\na robust ML technique that involves two distinct levels of\nmodels: the base models (level-0) and the meta-model (level-\n1). In the first level, various algorithms, which can be either\nheterogeneous (different types of models) or homogeneous\n(same type of models) [32], are trained on the original training\ndataset. These base models generate predictions which are then\nused as input features for the meta-model. The meta-model's\npurpose is to learn the optimal way to combine these base\nmodels' predictions to achieve the best possible performance\n[33]. Additionally, the meta-model can also incorporate the\noriginal input features from the training data alongside the\nbase model outputs to enhance its learning process.\nThis study draws upon the concept of traditional stacking\nmodels to propose a new architecture that, while resembling\nstacking, serves a different purpose: trust-informed prediction.\nIn the proposed method, while maintaining a similar two-\nlevel architecture (level-0 for the base model and level-1\nfor the meta-model), the meta-model's objective is different.\nHere, the meta-model aims to learn the relationship between\nthe base-model predictions and their associated uncertainties.\nConsequently, the output of the meta-model is a flag that\nindicates whether the model's prediction is trustworthy or\nnot. To align with this new objective, the architecture of\nthe proposed method is described in detail in the following\nsections."}, {"title": "A. Uncertainty-Aware Stacked Neural Networks", "content": "Figure 2 illustrates the overall flow of the uncertainty-\naware stacked neural networks (U-SNN) framework, which\nis architected with two integrated layers: the Level 0 base\nmodel as the initial predictor and the Level 1 meta-model as\nthe trust evaluator. This study applies the U-SNN framework\nto the classification task, beginning with a dataset $D =\n{(X_1,Y_1), (X_2,Y_2), ..., (X_N, Y_N)}$, where $x_i$ and $y_i$ represent\nthe $i^{th}$ observation vector and its corresponding label in a $d$-\ndimensional feature space. The initial step involves splitting\nthe dataset into training and testing subsets, denoted as $D_{train}$\nand $D_{test}$, respectively. In this study, $D_{train}$ is exclusively used\nfor training both the base model and the meta-model, while\n$D_{test}$ is reserved solely for evaluating the performance of the\nproposed method. This approach ensures that the base and\nmeta-models do not have access to or influence from the\ntest dataset, thereby providing an unbiased assessment of the\nproposed method.\nThe base model classifier generates class predictions, with\nthe associated uncertainty estimations used to construct a\nnew training set for the meta-model. For clarity, this second\ntraining set is referred to as the meta-train set. The uncertainty\nestimations of the base model in this study are calculated\nusing three distinct UQ techniques: (1) MCD, (2) Ensemble,\nand (3) Ensemble Monte Carlo dropout (EMCD). Detailed\ndescriptions of each technique are provided in the subsequent\nsection III-B.\nFollowing the training of the base model, the meta-train\nset is constructed by combining the original input features $X_i$"}, {"title": "B. Uncertainty Quantification Methods", "content": "1) Monte-Carlo Dropout (MCD): Building on the concepts\nof UQ outlined in the section II, MCD is implemented to\napproximate Bayesian inference in deep neural networks.\nDropout originally designed as a regularization technique\nto prevent overfitting by randomly deactivating a subset of\nneurons during training."}, {"title": "V", "content": "This process can be adapted for uncertainty estimation by\napplying dropout during the inference stage. By performing\nmultiple forward passes through the network with dropout\nenabled during inference, each pass results in slightly different\npredictions due to the random dropout of neurons, thereby\nproducing a distribution of outputs for a given input. Each\nneuron in the network effectively samples from a Bernoulli\ndistribution, and the collection of predictions across multiple\nstochastic forward passes serves as a Monte Carlo approxima-\ntion of the posterior distribution.\nHere, PE as an uncertainty evaluation metric is calculated\nas follows 2:\n$PE(x) = -\\sum_{c=1}^{C} \\mu_{pred} (x, c) log[\\mu_{pred} (x, c)]$                                     (2)\nEq. 2 represents the prediction entropy PE(x), calculated\nover C classes, where $\\mu_{pred} (x, c)$ denotes the mean predicted\nprobability of class c for the input x calculated as Eq. 3:\n$\\mu_{pred}(x, c) = \\frac{1}{M} \\sum_{m=1}^{M} p(y=c | x, w_m)$                             (3)\nwhere $p(y=c | x,w_m)$ denotes the probability that the\ninput x is assigned to class c, as determined by the softmax\nfunction, using the set of parameters $w_m$ from the $m^{th}$ iteration\nof the model, and M signifies the count of such iterations.\n2) Ensemble: In an ensemble of neural networks, each\nmodel independently computes a prediction for the same input.\nThe diversity among these models, which can arise from\ndifferences in architecture and parameters leads to a range\nof predictions. The variation of these predictions highlights\nepistemic uncertainty, representing the model's uncertainty\ndue to incomplete knowledge. A greater variation in the\npredictions signals more significant uncertainty, indicating that\nthe models do not agree, while a lesser variation suggests a\nhigher confidence in the predicted outcome. PE in ensemble\nsetting is calculated as 4:\n$PE(x) = -\\sum_{c=1}^{C} \\mu_{pred} (x, c) log [\\mu_{pred}(x, c)]$                                     (4)\nEq. 4 describes the prediction entropy PE(x) for ensem-\nble methods, where $\\mu_{pred} (x, c)$ indicates the mean predicted\nprobability of class c for the input x, derived as shown in Eq.\n5:\n$\\mu_{pred} (x, c) = \\frac{1}{N} \\sum_{n=1}^{N} p(y=c | x, \\theta_n)$                               (5)\nIn Eq. 5, $p(y=c | x, \\theta_n)$ is the probability that the input\nx belongs to class c, computed by the softmax function using\nthe parameters $\\theta_n$ of the $n^{th}$ model in the ensemble, and N is\nthe total number of models in the ensemble."}, {"title": "3) Ensemble Monte-Carlo Dropout (EMCD)", "content": "The integra-\ntion of ensemble methods with MCD (EMCD) offers a dual\napproach to uncertainty quantification. In this setting, each\nneural network within the ensemble applies dropout during the\ninference phase, not just during training. This process results\nin a varied set of predictions for each model, collectively\ncontributing to a broader distribution of predictions. PE for this\napproach is calculated to measure the dispersion of predictions\nacross the ensemble and across multiple stochastic passes for\neach model. The formula for PE is given by:\n$PE(x) = -\\sum_{c=1}^{C} \\mu_{pred} (x, c) log[\\mu_{pred}(x, c)]$                                     (6)\nHere, $\\mu_{pred} (x, c)$ represents the mean predicted probability\nof class c for the input x, computed across all models in the\nensemble and all dropout iterations:\n$\\mu_{pred}(x, c) = \\frac{1}{NM} \\sum_{n=1}^{N} \\sum_{m=1}^{M} p(y=c | x, W_{nm})$                             (7)\nIn this expression, $p(y=c | x, W_{nm})$ is the probability that\nthe input x is classified as class c, using the set of parameters\n$W_{nm}$ during the $m^{th}$ Monte Carlo iteration of the $n^{th}$ model\nin the ensemble. N is the number of models in the ensemble,\nand M represents the number of Monte Carlo iterations per\nmodel."}, {"title": "C. Evaluation Metrics", "content": "Considering the proposed two-level architecture, traditional\nerror-based performance metrics (such as the F1 score) for\neach model individually assess their training effectiveness\nbut fail to provide a comprehensive evaluation of the entire\nframework's performance. To gain a holistic understanding,\nit is essential to integrate the outputs of both the base and\nmeta-models.\nThus, new quantitative performance metrics have been\nproposed to evaluate trust-informed predictions (based on\nuncertainty estimations) by combining the outputs of both\nmodels, in a manner analogous to the traditional confusion\nmatrix. In the context of binary classification, the base model's\npredictions are compared with the ground truth labels, result-\ning in four categories: true positive (TP), false positive (FP),\ntrue negative (TN), and false negative (FN). Concurrently, the\nmeta-model's predicted confidence labels are compared with\nthe ground truth trustworthy labels, yielding four additional\ncategories: true trustworthy (TT), false trustworthy (FT), true\nuntrustworthy (TU), and false untrustworthy (FU). By integrat-\ning these correctness and confidence classifications, 16 distinct\noutcomes are generated, as illustrated in Table I. For ease of\nreference, the proposed confusion matrix is called the trust-\ninformed confusion matrix.\nIn the proposed trust-informed confusion matrix, four com-\nbinations-FNTT, FPTT, FNFU, and FPFU-never occur.\nThis exclusion is rooted in the definition of the meta-train\nset target variable as described in Eq. 1. As per this definition,\nincorrect predictions are never classified as trustworthy; hence,\nfalse positives and false negatives cannot be designated as\ntrue trustworthy (TT). Furthermore, since incorrect predictions\ncannot qualify as true trustworthy, they are also precluded from\nbeing classified as false untrustworthy (FU).\nConsidering the proposed confusion matrix, optimal perfor-\nmance is achieved when the model confidently makes correct\npredictions and is uncertain when an incorrect decision is\nlikely. This means the model should never confidently make an\nincorrect prediction, and conversely, should never be uncertain\nwhen making a correct prediction. The idea is that when the\nmodel flags a prediction as uncertain, it should be reviewed by\nan expert for further examination. For the model to be efficient,\nonly a small percentage of predictions\u2014those where the model\nis unlikely to be accurate (i.e., where the model recognizes\nits own incorrectness)\u2014should be flagged for further review.\nThis approach minimizes the workload for expert review while\nmaintaining high reliability.\nBuilding upon the trust-informed confusion matrix, several\nquantitative criteria have been established as follows. These\ncriteria are designed to evaluate how closely the model adheres\nto the optimal performance principles described above. These\ncriteria are:\n*   Confident Accuracy Rate (CAR): CAR is a metric that\n    quantifies the proportion of instances where the model is\n    accurate and confident, indicating its trustworthiness. It is\n    defined as Eq. 8. The rationale behind naming this ratio\n    Confident Accuracy lies in the definition of the meta-train\n    set target variable as \"trustworthy,\u201d as described in Eq. 1.\n    In this context, \"trustworthy\" signifies predictions that are\n    both correct and made with high confidence. Generally,\n    a higher CAR is desirable.\n    $CAR = \\frac{TPTT + TNTT}{Total Outcomes}$                               (8)\n*   Confident Precision Rate (CPR): CPR is a metric that\n    measures the proportion of accurate predictions fagged as\n    trustworthy, formulated by Eq. 9. Similarly, the rationale\n    behind naming this ratio Confident Precision lies in the\n    fact that this metric captures the percentage of correct\n    predictions made with high confidence (which, according\n    to the Eq. 1, are flagged as trustworthy) among all correct\n    predictions. A CPR close to 1 is desirable, indicating\n    that the model's confident predictions are almost always\n    correct.\n    $CPR = \\frac{TPTT + TNTT}{TAP}$                               (9)\n    where TAP stands as total accurate predictions defined\n    by Eq. 10:\n    $TAP = TPTT +TPFU + TPFT + TPTU +TNTT+TNFU +TNFT+TNTU$                               (10)\n*   Trust Precision Rate (TPR): TPR is a metric that measures\n    the proportion of predictions flagged as trustworthy that\n    are actually trustworthy, formulated by Eq. 11. A TPR\n    close to 1 is desirable. Naturally, 1-TPR represents the\n    percentage of records that the model erroneously flags as\n    trustworthy.\n    $TPR= \\frac{TPTT + TNTT}{TTP}$                               (11)\n    where TTP stands as total trustworthy predictions defined\n    by Eq. 12:\n    $TTP = TPTT + FNTT + FPTT + TNTT+\nTPFT + FNFT + FPFT+TNFT$                               (12)\n*   False Trust Rate (FTR): FTR measures the proportion\n    of incorrect predictions that the model has erroneously\n    classified as trustworthy. This measure is crucial in\n    applications where false trust in incorrect predictions\n    could lead to significant consequences. A lower FTR\n    is desirable, as it indicates fewer instances where the\n    model's incorrect predictions are mistakenly trusted. The\n    FTR is defined as Eq. 13:\n    $FTR = \\frac{FPFT + FNFT}{Total Outcomes}$                               (13)\n*   Review Alert Ratio (RAR): RAR is a metric that mea-\n    sures the proportion of predictions flagged as untrust-\n    worthy, requiring expert review, represented by Eq. 14.\n    This metric encompasses all untrustworthy predictions,\n    irrespective of whether they genuinely necessitate review\n    or are incorrectly flagged as untrustworthy. A high RAR\n    signifies inefficiency, indicating that a larger proportion of\n    instances are being flagged for review, thereby increasing\n    the review burden.\n    $RAR = \\frac{TUP}{Total Outcomes}$                               (14)\n    where TUP is defined as total untrustworthy predictions\n    as Eq. 15:\n    $TUP = TPFU + TPTU + FNFU + FNTU+\nFPFU + FPTU +TNFU+TNTU$                               (15)\n*   Miscalibration Review Ratio (MRR): MRR quantifies the\n    proportion of predictions flagged for review due to the\n    base model's miscalibration. Specifically, it measures the\n    rate at which the base model, despite being correct, has\n    low confidence (indicating a miscalibration in the base\n    model's confidence assessment) and thus flags predic-\n    tions as untrustworthy. A lower MRR indicates better\n    calibration of the base model's confidence with its actual\n    predictive accuracy. The MRR is defined as Eq. 16:\n    $MRR = \\frac{TPTU +TNTU}{TUP}$                               (16)\n*   True Review Ratio (TRR): TRR is a metric that quantifies\n    the proportion of instances where the model correctly\n    identifies its incorrect predictions and flags them as\n    untrustworthy formulated by Eq. 17. This metric indicates"}, {"title": "IV", "content": "the model's ability to recognize and appropriately flag its\nown errors. A TRR close to 1 is desirable, indicating\nthat the model effectively flags only incorrect instances\nas untrustworthy.\n$TRR= \\frac{FNTU + FPTU}{TUP}$                               (17)\n*   False Review Ratio (FRR): FRR quantifies the proportion\n    of correct predictions that are incorrectly flagged as un-\n    trustworthy due to a meta-model error, thereby requiring\n    unnecessary review, as formulated in Eq. 18. An FRR\n    close to 0 is desirable.\n    $FRR = \\frac{TPFU + TNFU}{TUP}$                               (18)"}, {"title": "IV. DATA SETS", "content": "The dataset utilized in the study is COVIDx CXR-4 [34], an\nexpanded multi-institutional open-source benchmark dataset\nspecifically designed for chest X-ray image-based computer-\naided COVID-19 diagnostics. This dataset significantly ex-\npands upon its predecessors, the COVIDx CXR datasets, by\nincreasing the total patient cohort size to 84,818 images from\n45,342 patients across multiple institutions. The age distribu-\ntion of the patients ranges widely, though there is a notable\nbias, with over half of the patients being between 18 and 59\nyears old. Additionally, the dataset maintains a nearly equal\ngender distribution and varied imaging views. The COVIDx\nCXR-4 dataset includes two main classes: positive COVID-19\ncases and negative cases. Among the 84,818 images, 65,681\nare positive COVID-19 cases, while 19,137 are negative cases,\nreflecting a significant class imbalance. This dataset is publicly\navailable [35]."}, {"title": "V. EXPERIMENTAL SETUP", "content": "A. Transfer Learning as Image Embedding Generator\nThe dataset utilized in this study, COVIDx CXR-4, lacks\nsufficient data to train a deep neural network effectively using\nCNNs from scratch. Therefore, this study adopted the transfer\nlearning (TL) approach to prepare the dataset for the proposed\nmethod. TL is a robust technique within machine learning\nthat alleviates the computational burden and extensive data\nrequirements associated with training CNNs from the ground\nup."}, {"title": "B. Base model configuration", "content": "Here, the primary goal of TL was to transfer knowledge by\nusing pre-trained models as an embedding function (or, more\nsimply, feature extractors). These models, initially trained and\noptimized on extensive datasets such as ImageNet, had their\nfinal fully-connected layers removed, and the remaining net-\nwork layers were frozen. This repurposed the networks as fixed\nfeature extractors for the COVIDx CXR-4 dataset, effectively\nharnessing their pre-established computational intelligence for\nnew data applications.\nIn this study, three different pre-trained models were uti-\nlized: EfficientNetB0 [36], BigTransfer [37], and Vision Trans-\nformer [38]. These models required an input image size of\n224x224 pixels. Moreover, the output embedding (feature)\nvectors were standardized to 256 dimensions per image across\nall models. Utilizing an assortment of pre-trained models,\nincluding both CNNs and transformers, the study sought to\nminimize the impact of any individual model's initial training\non the overall outcomes.\nThe embedding vectors (feature representations) extracted\nfrom the pre-trained models served as inputs for the base\nmodel. In this study, the base model is built using neural\nnetworks to perform binary classification. Considering the\nthree different UQ techniques employed (MCD, Ensemble, and\nEMCD), the following configurations for the base model were\ndevised and implemented:\n1) A Neural Network with MCD: Here, a fully connected\nlayers with an output layer equipped with a softmax function\nis employed. To determine the optimal architecture, the dataset\nwas first split into training and testing sets using a 70/30\nratio. The training set was used to train the models, while the\ntest set was reserved for evaluating their performance. Vari-\nous architectures were explored by employing Keras Tuner's\nHyperband algorithm. Keras Tuner, a library built on top of\nTensorFlow and Keras, automates the hyperparameter tuning\nprocess. Hyperband, an advanced tuning algorithm, dynam-\nically allocates resources to train multiple models, stopping\nthose that underperform early. The search space included\nvariations in the number of hidden layers, ranging from one\nto four, and the number of neurons per layer, ranging from\n16 to 512. Given the imbalanced nature of the dataset, class\nweights were computed to ensure balanced learning across\nclasses. These weights were incorporated during the training\nprocess to prevent bias towards the majority class. The best-\nperforming architecture was selected as the optimal base\nmodel for evaluating the proposed method. Additionally, MCD\nwas employed during the prediction phase to estimate the\nmodel's uncertainty, which was crucial for the subsequent\nstages of the study. In this study, the number of iterations\nfor MCD was set to 100.\nIn this study, three different pre-trained models were used,\nresulting in three sets of embedding vectors. For each set, a\nbase model with its optimal architecture was determined."}, {"title": "2) An Ensemble of Neural Networks", "content": "In this setting, the\nbase model configuration leverages an ensemble approach.\nThus, instead of relying on a single neural network, an\nensemble of 30 distinct neural networks was employed. For\neach model in the ensemble, a neural network was built with\na random number of hidden layers (ranging from one to four)\nand neurons per layer (ranging from 16 to 512) employing\na softmax in the output layer. Similarly, class weights were\nutilized to address data imbalance for each neural network in\nthe ensemble."}, {"title": "3) An Ensemble of Neural Networks with MCD", "content": "This set-\nting integrates MCD with an ensemble of neural networks. The\npurpose of this configuration is to observe how the introduction\nof dropout during inference affects uncertainty estimation and\nmodel robustness. This approach ensures a direct comparison\nwith the previous ensemble setup by using the same ensemble\nmodel configuration as the prior setting, with the addition of\nMCD during the inference phase."}, {"title": "C. Meta-model configuration", "content": "The meta-model is designed as a neural network, and its\noptimal architecture is determined using the Keras Tuner's\nHyperband algorithm. This method, similar to that employed\nfor the single neural network with the MCD base model,\nefficiently explores the hyperparameter space to identify the\nmost effective architecture. The search space for the meta-\nmodel includes variations in the number of hidden layers,\nranging from one to four, and the number of neurons per layer,\nranging from 16 to 512, allowing the model to adapt to the\ncomplexities of the data. Class weights are again utilized to\naddress data imbalance, maintaining fairness in the learning\nprocess.\nFor the meta-model dataset generation, the confidence\nthreshold is a crucial parameter that impacts the labeling of\nthe data. In this study, five different thresholds were used:\n0.05, 0.1, 0.2, 0.3, and 0.4. These thresholds determine the\ncut-off points for labeling predictions as confident or not,\nwhich in turn affects the training and performance of the\nmeta-model. By experimenting with these different thresholds,\nthe study aims to identify the optimal level at which the\nmodel can reliably distinguish between confident and uncertain\npredictions."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "This section delves into the empirical outcomes derived\nfrom the application of the proposed method:\n*   Initially, the analysis commences with a focus on a\n    selected confidence threshold of 0.1, evaluating the per-\n    formance across three distinct UQ settings: MCD, En-\n    semble, and EMCD. This section discusses their respec-\n    tive contributions to enhancing trust-informed decision-\n    making. Additionally, this examination is broadened to\n    include a discussion about the effects of various pre-\n    trained models on the models' performance, providing a\n    deeper understanding of how foundational architectures\n    influence model reliability.\n*   Subsequently, the discourse broadens to include a com-\n    parative analysis across a spectrum of confidence thresh-\n    olds, ranging from 0.05 to 0.4. This section aims to delin-\n    eate how varying confidence levels influence the model's\n    accuracy and the efficacy of trust-informed decision-\n    making, providing more insights into the model's robust-\n    ness.\n*   The final section investigates the integration of PE along-\n    side image embeddings as inputs to the meta-models.\n    This exploration is aimed at discerning the impact of\n    such integrations on the overall efficacy and accuracy of\n    the meta-model, thereby shedding light on the potential\n    enhancements introduced by combining these inputs."}, {"title": "A. Results and discussion at Confidence Threshold 0.1:", "content": "The performance evaluation process begins with partitioning\nthe dataset into training and testing sets through stratified sam-\npling based on the target variable. This ensures that both sets\nmaintain a similar distribution of classes. While evaluating a\nmodel on a single test set might provide an initial performance\nsnapshot, it does not guarantee reproducible results across\ndifferent data splits. This is because"}]}