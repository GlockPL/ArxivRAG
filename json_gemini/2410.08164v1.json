{"title": "AGENT S: AN OPEN AGENTIC FRAMEWORK THAT USES COMPUTERS LIKE A HUMAN", "authors": ["Saaket Agashe", "Jiuzhou Han", "Shuyu Gan", "Jiachen Yang", "Ang Li", "Xin Eric Wang"], "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.", "sections": [{"title": "1 INTRODUCTION", "content": "Since its invention, the mouse has been controlled by humans for interacting with computers. But does it really have to be? Autonomous Graphical User Interface (GUI) agents offer the promise of*\n solving very specific and highly varied user queries\u2014such as data entry, scheduling, and document creation for individual users, and streamlining operations in commercial settings\u2014in the most gen- eral way: through direct UI interaction using the mouse and keyboard. Moreover, by eliminating the need for constant manual interaction, these agents not only boost efficiency but also improve accessibility, empowering individuals with disabilities to interact with technology in new, transformative ways. Recent advancements in Multimodal Large Language Models (MLLMs), such as GPT-40 (OpenAI, 2023) and Claude (Anthropic, 2024), have laid the foundation for the develop- ment of GUI agents for human-centred interactive systems like desktop OS (Xie et al., 2024; Bonatti et al., 2024).\nHowever, automating computer tasks presents significant challenges. First, the vast range of constantly-evolving applications and websites requires the agent to possess specialized and up-to- date domain knowledge and the ability to learn from open-world experience. Second, complex desktop tasks often involve long-horizon, multi-step planning with interdependent actions that must be executed in a specific sequence. The agent must, therefore, create a clear plan with interme- diate subgoals and track task progress. Third, GUI agents must navigate dynamic, non-uniform interfaces, processing large volumes of visual and textual information while operating within a vast action space. This involves distinguishing between relevant and irrelevant elements, accurately in- terpreting graphical cues, and responding to visual feedback during task execution.\nIn this paper, we present Agent S, a new agentic framework that tackles these challenges towards the goal of using computers like a human. First, to enhance the GUI agent's capabilities in solving diverse, long-horizon desktop tasks with specific domain knowledge, we propose an Experience- Augmented Hierarchical Planning method. This approach leverages Online Web Knowledge and past experiences stored in Narrative Memory to decompose the complex, long-horizon task into a structured plan of manageable subtasks (see Figure 1). Online Web Knowledge provides up-to-date external knowledge about specific applications, allowing the agent to adapt to frequently chang- ing software and websites. Narrative Memory contains high-level, abstractive task experiences from past interactions, equipping the agent with contextual understanding for effective task planning. The agent monitors task completion progress, and during each subtask execution, it retrieves detailed, step-by-step subtask experience from Episodic Memory to dynamically refine its actions and contin- uously improve its planning ability. Successful subtasks and the full task experience are evaluated, summarized, and stored in episodic and narrative memory to enable continual improvement.\nFurthermore, we introduce a specific language- centric Agent-Computer Interface (ACI) (Lieberman & Selker, 2003) as an abstraction layer to improve grounding, safety, and ef- ficiency for MLLM-based GUI agents. The ACI defines an interaction paradigm by (1) a dual-input strategy using visual input for understanding environmental changes together with an image-augmented accessibility tree for precise element grounding; (2) a bounded action space of language-based primitives (e.g., click (element_id)) that are conducive to MLLM common-sense reasoning and generate environment transitions at the right temporal resolution for the agent to observe immediate and task-relevant environment feedback.\nOur approach shows a remarkable improvement in the overall performance of Agent S on the OSWorld benchmark (OpenAI, 2023) (from 11.21% to 20.58%, with a relative improvement of 83.6%), establishing the new state-of-the-art results. The detailed comparison is shown in Fig- ure 2, which demonstrates consistent improvements by Agent S across five broad computer task categories over the OSWorld agent. We also evaluate our Agent S on a concurrent work- WindowsAgentArena (Bonatti et al., 2024), where we observe a performance improvement from 13.3% to 18.2% on an equivalent setup without any explicit adaptation. The improvement demonstrates the broad generalizability of Agent S to different operating systems. We detail the component-wise improvements introduced by the proposed strategies through ablation studies and"}, {"title": "2 RELATED WORK", "content": "MLLM Agents. The advent of Multimodal Large Language Models (MLLMs) has led to a host of works that utilize them as a reasoning backbone in Agentic Systems (Sumers et al., 2024). These Agents augment LLMs with Memory, Structured Planning (Wang et al., 2023; Shinn et al., 2023; Weng et al., 2023), Tool Use (Schick et al., 2023; Shen et al., 2023; Patil et al., 2023) and the ability to Act in external environments Park et al. (2023). These agents have shown promise in domains ranging from embodied simulators (Liang et al., 2023; Song et al., 2023) to video games (Wu et al., 2023; Wang et al., 2024) and scientific research (Bran et al., 2023). For Software Engineering (Hong et al., 2024; Qian et al., 2024) in particular, Yang et al. (2024) proposed an Agent-Computer Interface (Lieberman & Selker, 2003) for MLLM agents to understand and act more efficiently and reliably. Our work extends and integrates these individual modules into a new MLLM agent framework for computer control.\nGUI Agents. MLLM agents have been applied to execute natural language instructions in both web and OS environments. Early research concentrated on web navigation tasks, utilizing MLLMs to in- teract with web interfaces (Gur et al., 2024; He et al., 2024; Kim et al., 2023; Shaw et al., 2023; Putta et al., 2024). Recently, the focus has shifted to OS-level environments, leading to the development of benchmarks and frameworks such as OSWorld Xie et al. (2024) and WindowsAgentArena Bonatti et al. (2024) for desktop control, and DiGIRL (Bai et al., 2024) and AndroidWorld (Rawles et al., 2024) for mobile environments. These OS-level tasks offer broader control capabilities beyond the limitations of single-browser contexts in web navigation. Methodologically, earlier GUI agents employed behavioral cloning with reinforcement learning (Humphreys et al., 2022), in-context tra- jectory examples (Zheng et al., 2024b), state-dependent offline experience (Fu et al., 2024b), and reusable skill generation (Wang et al., 2024). Contemporaneous work on GUI agents for video games and OS (Wu et al., 2024; Song et al., 2024; Tan et al., 2024) propose varying instances of cognitive architectures (Sumers et al., 2024). Our work contributes unique modules such as experience-augmented hierarchical planning and ACI for GUI control, integrated with a novel con- tinual memory update framework.\nRetrieval-Augmented Generation (RAG) for AI Agents. RAG (Fan et al., 2024) improves the reliability of MLLM inference by augmenting the input with reliable and up-to-date external knowl- edge. Similarly, MLLM agents benefit from retrieving task exemplars (Kim et al., 2024), state-aware guidelines (Fu et al., 2024a), and past experiences (Kagaya et al., 2024). Our use of experience for augmentation differs in three ways: 1) our hierarchical planning leverages both full task experience and subtask experience; 2) the full task experience is summarized into an abstractive textual reward for subtask planning; 3) the subtask experience is assessed and annotated by a self-evaluator before being stored in memory."}, {"title": "3 AGENT S", "content": "Agent S, illustrated in Figure 3, is a novel framework that integrates three main strategies in a closed loop to tackle complex GUI-based operating system control tasks: experience-augmented"}, {"title": "3.1 EXPERIENCE-AUGMENTED HIERARCHICAL PLANNING", "content": ""}, {"title": "3.1.1 MANAGER: FUSING EXTERNAL KNOWLEDGE AND INTERNAL EXPERIENCE FOR PLANNING", "content": "The Manager G is the primary plan generator module in our system. It receives a task \\(T_u\\) from the user and the initial environment observation \\(0_0\\) (Annotated Accessibility Tree + Screenshot) from the ACI as input. The manager formulates an observation-aware query Q based on the user instruction and its observation in a \"How to do X\" format. This query is used for two types of retrieval. First, the query is used for Online Web Search through Perplexica Search Engine\u00b9 to get external knowledge. Then the same query is used to retrieve a similar task experience summary from the Manager's own Narrative Memory \\(M_n\\). The retrieval is based on the similarity of the query embedding.\nThe Narrative Memory includes summaries of both successful and failed trajectories with specific actions removed as abstractive full task experience \\(E_n\\). The success/failure is evaluated by the Self-Evaluator S module (described in Subsection 3.1.3) without any human feedback or ground truth information. This two-step retrieval provides the Manager with both the general and specific"}, {"title": "3.1.2 WORKER: LEARNING FROM SUBTASK EXPERIENCE AND TRAJECTORY REFLECTION", "content": "The subtasks \\((s_0..s_n)\\) generated by the Manager G are executed sequentially by Worker modules \\((W_0..w_n)\\). Each Worker can take multiple time steps within one episode to complete a subtask \\(S_i\\). Firstly, the combination of the User Task \\(T_u\\), the subtask \\(s_i\\) and the contextual information \\(C_s\\), are used as a query to retrieve similar subtask experience \\(E_{s_i}\\) from the Worker's Episodic Mem- ory. The Episodic Memory is indexed by the concatenation of the task query, the subtask, and the contextual information \\((Q, S_i, C_{s_i})\\), based on the similarity of the embedding. As opposed to Narrative Memory, Episodic Memory includes a complete plan with specific grounding actions and only summaries from the subtask trajectories designated as DONE or successful by a Worker. Ad- ditionally, a Trajectory Reflector submodule \\(TR_i\\) is associated with each worker. This submodule observes the entire episode as the worker is executing the subtask and provides reflective advice to the agent-helping it think of alternative strategies and avoid repetitive actions."}, {"title": "3.1.3 SELF-EVALUATOR: SUMMARIZING EXPERIENCES AS TEXTUAL REWARDS", "content": "The Self-Evaluator S is responsible for generating experience summaries as textual rewards r for the Manager and Worker modules. In the case of the successful end of an episode signaled by the Worker with a DONE signal, the evaluator observes the complete episode and generates learning in the form of a summarization of the strategy used by the worker to complete that subtask. This strategy is fed back into the Worker's episodic memory \\(M_e\\). In the case of the end of the complete user-provided task, indicated either by the successful completion of all subtasks or by the maximum number of steps limit, the evaluator generates a learning signal in the form of the summary of the entire task completion process. This summary is fed back and saved in the narrative memory \\(M_n\\) of the Manager. This process of Observations, Hierarchical Action Generation, and Rewards in the form of textual summaries to update the internal memories of the Manager and Worker mirrors a classic Hierarchical Reinforcement Learning process - but uses Retrieval as a learning strategy."}, {"title": "3.2 \u039c\u0395\u039cORY CONSTRUCTION AND UPDATE", "content": "Initial Memory Construction via Self-supervised Exploration. To bootstrap Narrative \\(M_n\\) and Episodic Memories \\(M_e\\), Agent S conducts self-supervised exploration on a set of synthetically gen- erated tasks (see Figure 4). We utilize two methods to create two types of random exploration tasks: environment-independent tasks and environment-aware tasks. For environment-independent tasks, we leverage a task generator to generate the top 50 most common tasks from the various"}, {"title": "3.3 AGENT-COMPUTER INTERFACE", "content": "Current desktop environments are designed to accommodate two distinct user types: (1) human users, who can perceive and respond to subtle visual changes in real-time, and (2) software pro- grams, which execute predefined tasks through scripts and Application Programming Interfaces (APIs). However, these interfaces are inadequate for MLLM agents tasked with GUI control and ma- nipulation at the fundamental keyboard-mouse level. These agents operate on a different paradigm: they respond in slow, discrete time intervals, lack an internal coordinate system, and cannot effi- ciently process fine-grained feedback after each minor mouse movement or keyboard input. Draw- ing inspiration from the ACI developed for Software Engineering agents (Yang et al., 2024), we propose the creation of a novel ACI to bridge the gap between the unique operational constraints of MLLM agents and the requirements of open-ended GUI-control tasks.\nPerception and Grounding. Current MLLMs can effectively reason about certain elements and features in an image, but they cannot directly ground and pinpoint specific elements in images as they lack an internal coordinate system. In GUI manipulation, agents need to constantly interact with fine UI elements, and previous works have shown that grounding is a significant bottleneck in these agents (Xie et al., 2024; Zheng et al., 2024a). Desktop environments, however, provide an easily parseable Accessibility Tree with coordinate information about almost every element in the UI. Thus, our ACI design incorporates a dual-input strategy with different purposes for each input."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Benchmarks. We evaluate Agent S on OSWorld (Xie et al., 2024), a benchmark for testing the multimodal agents' capability of executing a wide range of computer tasks in a real computer envi- ronment. This executable environment allows free-form keyboard and mouse control of real com- puter applications, including OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP), and Workflow (tasks involving multiple apps). In addition, we also evaluate the generalization of Agent S on WindowsAgentArena (Bonatti et al., 2024), a contemporaneous benchmark in the Windows operating system.\nSettings & Baselines. Since the OSWorld benchmark contains 369 tasks on Ubuntu, for the backbone model of Agent S, we leverage GPT-40 and Claude-3-Sonnet, respectively. For Win- dowsAgentArena, we test all 154 tasks on GPT-40. We use the PaddleOCR2 toolkit as our OCR tool in augmenting accessibility trees for grounding. The embedding model for the retrieval we use is text-embedding-3-small. Agent S takes the accessibility tree and screenshot as inputs, so we also use the reported results in OSWorld (Xie et al., 2024) and WindowsAgentArena (Bonatti et al., 2024) with same input setting as baselines. The OSWorld baseline takes the coordinates-based accessibil- ity tree and screenshots as input for spatial grounding to generate the action with coordinates at each step. The WindowsAgentArena baseline NAVI (Bonatti et al., 2024) utilizes an accessibility tree, OCR, and Proprietary models to process the screenshot and create Set-of-Marks as input. Its action space includes a constrained set of primitives but allows multiple actions to be chained together."}, {"title": "4.2 MAIN RESULTS", "content": "OSWorld. Table 1 shows the performance comparison between Agent S and the baseline models, evaluated across the whole OSWorld test set. For the GPT-40 model, Agent S achieves an overall success rate of 20.58%, nearly doubling the performance of the best corresponding baseline (GPT- 4o with 11.21%). Agent S consistently outperforms the baselines in the \u201cDaily\" and \"Professional\" tasks, where it reaches 27.06% and 36.73% success rates, respectively, compared to the best base-"}, {"title": "4.3 ABLATION STUDY", "content": "To demonstrate the effectiveness of individual modules of Agent S, we stratified sampled a subset of 65 instances, testsub\u00b3 from the full test set for the ablation study. Considering the inference cost, we utilized GPT-40 as the LLM backbone for all ablation studies for both the baseline and Agent S.\nLearning from experience enhances the domain knowledge of GUI agents. The Experiential learning process of Agent S involves searching web knowledge, retrieving full task experience from narrative memory and retrieving subtask experience from episodic memory. To assess the impact of different components, we systematically remove each component and observe performance changes across different task categories. The results are shown in Table 2. Learning from universal experi- ence available as web knowledge allows Agent S to make informed plans across a wide range of tasks and has the most significant impact. The learning from Narrative and Episodic memories synergies effectively with web retrieval, and the results detail how their ablation affects the agent's ability to handle complex tasks, underscoring the value of experiential learning. These results demonstrate that each component plays a critical role in enhancing the agent's domain knowledge. Removing all three components (w/o All) degrades the performance significantly, revealing the importance of learning from experience in the design.\nACI elicits better reasoning abilities of LLMs and supports better agentic learning. Figure 6 presents the results of the ablation study on the ACI module. Comparing the baseline with Agent"}, {"title": "4.4 ERROR ANALYSIS", "content": "We performed a thorough error analysis on the tasks that Agent S failed within testsub of the OS- World. There are three types of errors that we observed: (1) Planning Error: A planning error occurs when the agent generates unsuitable plans for a task, including inaccuracies in the plan, misleading subtask information, or misalignment of subtask sequence with task requirements. (2) Grounding Error: A grounding error arises when the agent fails to accurately interact with target elements despite their visibility and the application of correct reasoning. This includes incorrect element se- lection or inaccurate coordinate selection due to the inherent limitations of our action space (e.g., selecting the center instead of a more precise part of the element). (3) Execution Error: An execu- tion error emerges when the agent makes incorrect decisions or fails to adjust its behavior during"}, {"title": "4.5 GENERALIZATION TO DIFFERENT OPERATING SYSTEMS", "content": "We test the Agent S framework with no modification on WindowsAgentArena (Bonatti et al., 2024), a Windows OS benchmark released contemporaneously with our work. We compare Agent S with the similar configuration with GPT-4o as the MLLM backbone, Accessibility Tree + Image as the input, and parsing with OCR. As shown in Table 4, Agent S outperforms the Navi agent without any adaptation to the new Windows environment."}, {"title": "5 CONCLUSION", "content": "In this work, we present Agent S-A novel framework for developing fully Autonomous Graphical User Interface (GUI) agents that can perform a wide range of user queries by directly controlling the keyboard and mouse. Through the Agent S framework, we show the benefits of Learning from Expe- rience for Task-oriented GUI agents. We also discuss the concept of an Agent Computer Interface for the GUI domain, arguing in favour of an abstraction layer that allows MLLM agents to perceive and reason at a language level with rich and continuous feedback. By leveraging Experience-Augmented Hierarchical Planning, Online Web Knowledge, and an Agent-Computer Interface (ACI), Agent S demonstrates SOTA performance on the OSWorld benchmark and generalizability across different operating systems. We demonstrate the potential of MLLM agents to learn from external sources and through direct interaction with the environment, without any human or environmental feedback in the GUI agents domain, thus opening a discourse on zero-shot, agentic methods for GUI agents.\nFuture Work. A key metric that has been unaddressed in existing work on MLLM agents for computer control, including ours, is the number of agent steps and wall clock time required for task completion. While our work focuses on achieving significant improvement in task performance, future work can consider a shortest-path navigation formulation of GUI control and evaluate the Pareto-optimality of various agents on the dimensions of time and accuracy. In our work, we use the state-of-the-art GPT-40 and Claude-3.5-sonnet models. However, future work can extend the ideas of experiential learning and Agent Computer Interface for smaller, open-source MLLMs which could be fine-tuned to bridge the gap."}, {"title": "A AGENT-COMPUTER INTERFACE", "content": ""}, {"title": "A.1 CONSTRAINED ACTION SPACE", "content": "To facilitate the agent's accurate and effective task execution, we define a constrained action space, which simplifies the action selection process, making it easier for the agent to ground its decisions in a well-structured set of operations. As summarized in Table 5, each action type has certain parameters and detailed in description."}, {"title": "A.2 ABLATIONS ON AGENT COMPUTER INTERFACE", "content": "The incorporation of Retrieval-as-Learning method enhances the performance of both the Baseline and Agent S models, with a notably greater impact observed for Agent S, as shown in Table 6."}, {"title": "A.3 ABLATIONS ON LEARNING", "content": "The results presented in Table 7 demonstrate the critical role played by both the Continual Learning component and the Self-Evaluator in enhancing the performance of Agent S."}, {"title": "C EXPERIENCE-AUGMENTED HIERARCHICAL PLANNING", "content": ""}, {"title": "Observation-Aware Query", "content": "The Manager formulates a query Q based on the user task \\(T_u\\) and initial observation \\(0_0\\):\n\n\n\nNarrative Memory \u2013 Storing Full Task Experiences", "latex": ["Q = LLM(T_u, 0_0)"]}, {"title": "C.1 MANAGER: FUSING EXTERNAL KNOWLEDGE AND INTERNAL EXPERIENCE FOR PLANNING", "content": ""}, {"title": "External Knowledge Retrieval", "content": "The query Q is used to retrieve external knowledge \\(K_{ext}\\) using the Perplexica search engine:", "latex": ["K_{ext} = Retrieve(Web, Q)"]}, {"title": "Fusion of Internal Experience and External Knowledge", "content": "The internal narrative memory expe- rience \\(M_n(Q)\\) and external knowledge \\(K_{ext}\\) are combined using the Experience Context Fusion module:", "latex": ["K_{fused} = MLLM(M_n(Q), K_{ext})"]}, {"title": "Subtask Planning", "content": "The fused knowledge \\(K_{fused}\\) is used by the Manager to generate a queue of subtasks \\((s_0, s_1, ..., s_n)\\) and associated contexts \\((C_{s0}, C_{s1}, ..., C_{sn})\\):", "latex": ["{(S0, Cso), (S1, Cs1),..., (Sn, Csm)} = MLLM(Kfused)"]}, {"title": "C.2 WORKER: LEARNING FROM SUBTASK EXPERIENCE AND TRAJECTORY REFLECTION", "content": ""}, {"title": "Subtask Execution", "content": "Each Worker wi retrieves subtask experience si by querying the episodic memory \\(M_e\\):", "latex": ["E_{s_i} = Retrieve(Me, (Tu, Si, Cs_i))"]}, {"title": "Trajectory Reflection", "content": "The Worker reflects on the entire episode using a Trajectory Reflector TR\u2081:", "latex": ["Reflection = TR\u00bf (trajectory)"]}, {"title": "Action Generation", "content": "Using the retrieved subtask experience Es, the Worker generates a structured response for a grounded action aj:\n\n\n\nSubtask Completion", "latex": ["aj = MLLM(Es\u2081, observation, Reflection)"]}, {"title": "C.3 SELF-EVALUATOR: GENERATING SUMMARIZED EXPERIENCES AS TEXTUAL REWARDS", "content": ""}, {"title": "Episodic Experience Update", "content": "If a Worker completes a subtask, the Self-Evaluator S generates an Episodic ExperienceEe, as a summary of the strategy used:", "latex": ["Rs\u2081 = S(Episode)"]}, {"title": "Narrative Experience Update", "content": "When the entire task is completed by the Manager G, the Self- Evaluator generates a task completion reward r, which is saved into the narrative memory, indexed by the observation-aware query formulated by the Manager:", "latex": ["Enu = S(G(Tu))", "Mn Save(Mn, Q, Enu)"]}, {"title": "D SUPPLEMENTARY EXAMPLES FOR QUALITATIVE ANALYSIS", "content": "Here we present additional examples of successful and failed tasks as supplements to the qualitative analysis in \u00a74.2. Furthermore, we provide a more detailed error analysis to complement \u00a74.4."}, {"title": "D.1 SUCCESS EXAMPLES", "content": "In this section, we present successful task examples from a variety of domains."}, {"title": "D.2 DETAILED ERROR ANALYSIS AND FAILURE EXAMPLES", "content": "In this section, we analyze the sources of execution errors as defined in \u00a74.4, followed by presenting several examples of failed tasks, each with a detailed error analysis provided for the respective case. Empirically, Grounding and planning errors often directly lead to execution errors (e.g., failing to interact with the correct target element can result in repetitive actions, and incorrect planning"}]}