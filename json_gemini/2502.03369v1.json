{"title": "Learning from Active Human Involvement through Proxy Value Propagation", "authors": ["Zhenghao Peng", "Wenjie Mo", "Chenda Duan", "Quanyi Li", "Bolei Zhou"], "abstract": "Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called Proxy Value Propagation for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state-action pairs in the human demonstration are labeled with high values, while those agents' actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents' exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human-in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) has been successfully applied in many domains, ranging from board game Go [46], strategy game StarCraft II [42], autonomous driving [18], and even nuclear fusion [7]. Existing RL methods assume the manually designed reward functions can fully express human intents and preferences. However, the resulting agents might exhibit biased, misguided, or undesired behaviors due to faulty reward functions [23, 40, 20]. Moreover, the poor sample efficiency as well as the safety concern due to the trial-and-error exploration prevent the real-world deployment of RL.\nHuman-in-the-loop methods are promising to achieve alignment, learning efficiency, and safety. Human-in-the-loop policy learning relies on human subjects to oversee the learning process of the autonomous agents, thus it can better align the learned behaviors with the preferences of humans compared with handcrafted reward functions. Different forms of human involvement in human-in-the-loop policy learning have been studied over the years. Human subjects can advise actions upon the requests of the robots [28] or provide preference-based feedback to assess the relative value of the collected trajectories [53, 5, 38, 52, 36, 11, 34]. These methods learn from passive human involvement, where the human subjects do not provide real-time feedback and intervention during data collection. For safety-critical tasks such as autonomous driving, safety is undoubtedly the first priority in human preference and the passive involvement methods yield unbounded risks in such settings. An increasing body of works focuses on active human involvement, where human subjects actively intervene and provide demonstrations during the execution time [17, 47, 29, 26]. With online correction and demonstration from human subjects, AI alignment and training-time safety of the system can be substantially enhanced."}, {"title": "Related Work", "content": "AI alignment is one of the major issues in learning trustworthy intelligent agents for real-world applications. It is difficult to represent various human preferences into a scalar reward function in existing Reinforcement Learning (RL) methods [40, 6]. Meanwhile, the manually designed reward function, which might be misaligned with human preferences, often leads to undesired behaviors [23, 20]. As a promising complement to RL, Human-in-the-loop Learning (HL) can overcome costly reward engineering and convey human intents to the learning process directly through human involvement. Compared to imitation learning (IL) [14, 9], where the agent learns directly from high-quality human demonstration, HL methods benefit from interactive human involvement and feedback during the training, mitigating the possible distributional shift that usually happens when learning from offline data [39].\nPreference-based RL. A large body of work focuses on learning human preference via ranking pair of trajectories generated by the learning agent [5, 11, 38, 52, 41, 36, 22, 51]. InstructGPT [34] aligns language models by first supervised learning in demonstration and then finetuning by the reward learned from human preference feedback. Preference learning can be applied to the tasks that human can not conduct, such as moving a six-legged Ant robot by assigning exact torque at each joint [5]. For those tasks that human can demonstrate, these methods do not fully utilize real-time feedback from human subjects during agent-environment interaction.\nHL with Passive Human Involvement. Different from preference-based RL, human subjects can provide direct feedback to the learning agent during training through passive human involvement. Some works learn policy from human-provided evaluative feedback, a Boolean flagging correct or wrong actions [19, 3, 32]. This is similar to the intervention in our framework. However, in [32], humans provide high-level instructions, e.g. pointing to the left/right, while in PVP humans provide intervention and low-level demonstrations. The other line of work allows the neural policy to operate the robot and the human subjects can provide demonstration upon the requests from the learning agents [28, 30, 16]. The expert policy will intervene when uncertainty is huge, where the agent uncertainty is estimated by the variance of actions [30]. These methods reduce the cost of human resources but have potential risks to human subjects since they do not fully control the system. For example, when human subjects use these algorithms to train autopilot AI, they are exposed to significant risks if they are in a self-driving cars due to unpredictable agent behaviors.\nLearning from Active Human Involvement. For safety-critical tasks such as autonomous driving, the safety of both the controlled vehicles and the human subjects is the top priority. There are many works that allow human subjects to proactively involve the agent-environment interactions based on their own judgment to ensure safety, which we call active human involvement. Human subjects"}, {"title": "Problem Formulation", "content": "Policy learning aims at finding a policy to solve the sequential decision-making problem, which is usually modeled by a Markov decision process (MDP). MDP is defined by the tuple $\\mathcal{M} = (S, A, P, r, \\gamma, d_0)$ consisting of a state space $S$, an action space $A$, a state transition function $P : S \\times A \\rightarrow S$, a reward function $r : S \\times A \\rightarrow [R_{min}, R_{max}]$, a discount factor $\\gamma \\in (0, 1)$, and an initial state distribution $d_0 : S \\rightarrow [0, 1]$. The goal of conventional reinforcement learning is to learn a novice policy $\\pi_\\eta(a|s) : S \\times A \\rightarrow [0, 1]$ that can maximize the expected cumulative return: $\\pi_\\eta = \\arg \\max_{\\pi_\\eta} \\mathbb{E}_{\\tau\\sim P^\\pi} [\\sum_{t=0}^T \\gamma^t r(s_t, a_t)]$, wherein $\\tau = (s_0, a_0, ..., s_T, a_T)$ is the trajectory sampled from trajectory distribution $P^\\pi$ induced by $\\pi_\\eta, d_0$ and $P$. Here $\\pi_\\eta$ defines a stochastic policy, while deterministic policy can be denoted as $\\mu_n(s) : S \\rightarrow A$ and its action distribution is a Dirac delta distribution $\\pi_\\eta(a|s) = \\delta(a - \\mu_n(s))$.\nThe reward function imposes an assumption that the reward can fully reflect the intentions of the users and incentivize desired behaviors. However, this assumption may not always hold and the learned agent may obtain biased behaviors or figure out the loophole to finish the task [23, 40]. Revisiting the primal goal when developing learning systems, we find the reward is not a necessity since what we really want to achieve is the realization of human preference in the learned behaviors and, as suggested by [40], the ultimate source of information about human preferences are human behaviors.\nImitation Learning (IL) methods directly learn $\\pi_\\eta$ from human behaviors. Assuming a human expert has a human policy $\\pi_\\eta^h(a^h|s) : S \\times A \\rightarrow [0, 1]$, which outputs human action $a^h \\in A$. Note that human action shares the same action space as novice action. IL learns from the trajectories generated by human policy $\\tau^h \\sim P^h$ and optimizes the novice policy to close the gap between $\\tau_\\eta \\sim P^{\\pi_\\eta}$ and $\\tau^h$. Instead of generating an offline dataset and training novice policy against it [14, 9], we can incorporate a human subject into the loop of training for providing online data. This can mitigate the distributional shift since the data generated with human-in-the-loop has closer state distribution to that of the novice policy [39]. This can be modeled by introducing an intervention policy $I(\\cdot|s, a_n)$ to describe human subjects' intervention behaviors. In earlier methods such as DAgger [39], the intervention policy is a Bernoulli distribution and the control authority switches back and forth between the novice and the expert. It is unrealistic to invite a real human subject to be involved in such training. Later studies allow the human subjects to intervene and take full control [49, 43, 26, 54], which we call such setting as learning from active human involvement. During training, a human subject accompanies the novice policy and can intervene with the agent by taking over the control to demonstrate desired behaviors. The intervention policy can be considered as a deterministic policy denoted by $I(s, a_n) : S \\times A \\rightarrow {0, 1}$ where $a_n \\sim \\pi_\\eta(\\cdot|s)$ is agent's action. With notations above, the behavior policy $\\pi_b$ that generates actions during training is:\n$\\pi_b(a|s) = (1 - I(s, \\mu_n(s)))\\delta(a - \\mu_n(s)) + I(s, \\mu_n(s))\\pi_\\eta^h(a|s)$.\nWith such a model of active human involvement, we can now formulate our objectives.\nTask-specified metrics. Our primal goal is to find novice agents whose behaviors are well-aligned with human preferences. In this work, we inform the human subjects of the primal goal of the tasks, e.g. navigating to the destination in driving tasks. They are also aware of how task-specified metrics, such as success rate and route completion provided by the test environments, are computed."}, {"title": "Method", "content": "We propose the Proxy Value Propagation (PVP) method which can transform a value-based RL method into an efficient reward-free human-in-the-loop policy optimization method that learns from active human involvement. PVP is compatible with various task settings, such as continuous and discrete action spaces, as well as various human control devices. In this section, we first summarize the basic workflow of value-based RL before introducing the motivation and the design of PVP. We then describe the implementation details.\nValue-based RL: The proposed human-in-the-loop method results from the minimum modification of existing reinforcement learning methods. Thus, we briefly introduce the background of related methods. Value-based RL optimizes the value function and policy iteratively. On the value function side, we denote the state-action value and state value of policy $\\pi$ as $Q(s, a) = \\mathbb{E} [\\sum_{t=0}^T \\gamma^t r(s_t, a_t)]$ and $V(s) = \\mathbb{E}_{a\\sim\\pi(\\cdot|s)}Q(s, a)$, respectively. A neural network is commonly used to estimate the value function with Bellman backup: $Q(s, a) \\leftarrow r(s, a) + \\gamma \\max_{a'} Q(s', a')$, where $s'$ is the next state. To learn the value network $Q_\\theta$ parameterized by $\\theta$, stochastic gradient descent on the temporal difference (TD) loss is conducted $\\mathcal{J}_{TD}(\\theta) = \\mathbb{E}_{(s,a,s')} |Q_\\theta(s, a) - (r(s, a) + \\gamma \\max_{a'} Q_{\\hat{\\theta}}(s', a'))|^2$, where $Q_{\\hat{\\theta}}$ can be a delay-updated target network. In this work, we adopt the TD learning in the reward-free setting. Remove the reward in the TD loss, the TD loss becomes:\n$\\mathcal{J}_{TD}(\\theta) = \\mathbb{E}_{(s,a,s')} |Q_\\theta(s, a) - \\gamma \\max_{a'} Q_{\\hat{\\theta}}(s', a')|^2$.\nOn the policy side, based on the learned value function, the deterministic policy $\\mu_\\eta$ parameterized by $\\phi$ can be learned by maximizing the Q values: $\\mathcal{J}(\\phi) = \\mathbb{E}_s Q(s, \\mu_\\eta(s; \\phi))$. The optimal policy is expected to maximize Q values:\n$\\mu_\\eta(s) = \\arg \\max_a Q(s, a)$.", "subsections": [{"title": "Proxy Value Propagation", "content": "We illustrate the active human involvement of PVP in Fig. 1. During training, the human subject supervises the agent-environment interactions (Fig. 1 A). Those exploratory transitions by the agent are stored in the Novice Buffer $B_n = \\{(s, a_n, s')\\}$. At any time, the human subject can intervene the free exploration of the agent by pressing a button in the control device (Fig. 1 B). While pressing the button, the human takes over the control and provides a demonstration of how to behave. During human involvement, both human and novice actions will be recorded into the Human Buffer $B_h = \\{(s, a_n, a^h, s')\\}$. Concurrently with the human-agent shared control, our method keeps updating the novice policy by the novel Proxy Value Propagation mechanism (Fig. 1 C), which will be discussed later.\nIn the shared human-agent control, human intervention serves as a distinct indicator of suboptimal agent performance, which could result from the agent executing perilous actions or exhibiting ineffective behaviors. Thus, the optimal policy learned by the agent should (1) strive to approximate the behaviors demonstrated by the human subjects and (2) avoid performing actions that are intervened by humans.\nThe key insight of this work is that we can manipulate the Q values to induce desired behaviors, given that value-based RL has the nature to seek value-maximizing policy as Eq. 3. As shown in Fig. 1 C, for emulating human behavior and minimizing intervention, we sample data $(s, a_n, a^h)$ from the human buffer and label the Q value of the human action $a^h$ with +1 and the novice action $a_n$ with -1. This is achieved by fitting the Q network directly with PV loss:\n$\\mathcal{J}_{PV}(\\theta) = \\mathbb{E}_{(s,a_n,a^h)} [|Q_\\theta(s, a^h) - 1|^2 + |Q_\\theta(s, a_n) + 1|^2]I(s, a_n)$.\nThe transitions in the novice buffer are not intervened by the human subject, meaning they are aligned with human preferences. Meanwhile, those transitions also contain information of the forward dynamics [24, 55]. To exploit the information contained in these transitions, instead of discarding these data as in [17], we propagate the proxy values to these states via TD learning in Eq. 2 and use those transitions together with those human-involved transitions for the policy learning. The final value loss is evaluated as follows:\n$\\mathcal{J}(\\theta) = \\mathcal{J}_{PV}(\\theta) + \\mathcal{J}_{TD}(\\theta) = \\mathbb{E}_{(s,a_n,a^h)\\sim B_h} [|Q_\\theta(s, a^h) - 1|^2 + |Q_\\theta(s, a_n) + 1|^2]I(s, a_n) + \\mathbb{E}_{(s,a,s')\\sim B_h \\cup B_n} |Q_\\theta(s, a) - \\gamma \\max_{a'} Q_{\\hat{\\theta}}(s', a')|^2$.\nThen we follow the policy update process outlined in the base RL methods."}, {"title": "Analysis", "content": "Connection to CQL. The proposed PVP method can be interpreted as adopting the Conservative Q-Learning (CQL) [21] objective for reward-free and online learning settings. It augments the CQL objective with an extra L2 regularization term imposed on the Q-values for human-involved transitions. In our online learning setting, Eq. 5 can be reformulated as:\n$\\mathcal{J}(\\theta) = \\mathbb{E}_{B_h, I(s, a_n) = 1}[Q(s, a^h) + Q(s, a_n) + 2 + 2(Q_\\theta(s, a^h) - Q_\\theta(s, a_n))] + TD loss$.\nCQL was originally proposed to mitigate the problem of overestimated Q-values in offline RL settings. These overestimations often lead to suboptimal policies due to the optimistic selection of actions with misleadingly high values. In our work, we deal with human actions and novice actions sampled from two different distributions, where overestimation might also occur. However, unlike CQL, PVP does not have access to a reward function, meaning the Q-values are not grounded in an estimation of true values. The additional L2 regularizer therefore serves to impose constraints on the Q-values, helping to prevent unbounded growth and potential overfitting. In Sec. 5.4, we compare the learned proxy Q-values under both CQL and PVP objectives. Our results indicate that human and agent actions are more distinguishable when learned through PVP."}]}, {"title": "Conclusion", "content": "Learning through active human involvement is a promising approach enabling safe and efficient policy learning. In this work, we propose Proxy Value Propagation (PVP) that can effectively learn from the intervention and the corrective feedback from active human involvement. PVP can be seamlessly integrated into existing value-based RL methods and achieves highly efficient reward-free policy learning, without offline pretraining and reward engineering. Human-in-the-loop experiments show the proposed method achieves superior performance and better user experience across diverse environments with different action spaces and human control devices, showing that the learning from active human involvement is a efficient policy learning method aligning human preference.\nLimitations. (1) We only apply our method to two value-based RL methods. Advanced techniques such as exploration encouraging [33] and prioritized replay buffer [44] can be added to further improve the result. (2) Our method is not applicable to tasks where humans can not provide demonstrations. (3) We assume that human always demonstrates desired actions. We will show in Appendix D that suboptimal human behaviors will damage learning. In this case, we can define a sparse cost function in the training environment and utilize constrained optimization [2] to penalize bad demonstrations. (4) We assume that human subjects are available and attentive throughout the entire training. While our method is proven to be effective even under heavy traffic environments, we plan to further enhance its sample efficiency. We will achieve this goal by conducting offline RL training and policy evaluation in the background or passively involving human subjects whenever the model is uncertain about the environment."}, {"title": "Ethics Statement", "content": "Human subjects get paid to participate in the experiments. They can pause or stop the experiment if any discomfort happens. No human subjects are injured because all tasks we test are in virtual simulation. Each experiment will not last longer than one hour and subjects will rest at least three hours after one experiment. During training and data processing, no personal information is revealed in the collected dataset or the trained agents. We have obtained IRB approval to conduct this project."}, {"title": "Human Subject Research Protocol", "content": "Recruiting and Requirement. For our study, we recruit 5 human subjects. All of them are college students and have the age from 20 to 30 years. Furthermore, every participant are required to have a valid driver's license and have experience in playing video game. Participation in our study is entirely voluntary. We ensure transparency by informing all subjects about the nature of the experiments and how their demonstrations would be used. Every subject provide written consent, confirming they are fully aware and in agreement. Additionally, the study is conducted with the IRB approval.\nOnboarding Period. Participants are required to undergo a practice session, during which they drive under complete control to get a sense of the control devices (wheel, gamepad and keyboard), the environment interface, the dynamics of each environment and how an episode will fail or success. Each subject get familiar with all the control devices and all the environments, which is indicating by performing at least 10 successful episodes, before they participate in the main experiments.\nMain Experiment. During the initial stages of the formal experiment, subjects are advised to retain full control of the agent for the first one or two episodes. Subsequently, they may begin to let the agents taking control and intervene as necessary. The objective in all driving experiments is twofold: firstly, to safely navigate the vehicle to its designated destination, and secondly, to ensure the vehicle's operation aligns with traffic regulations and human preferences.\nSubjects are encouraged to perform intervention whenever they perceive the vehicle might be in a dangerous situation, in violation of traffic rules, or in whatever scenario the human subjects feel they wouldn't behave in the way the novice policies do.\nTo ensure data integrity and counter potential proficiency biases, the order of experiments with different control devices, tasks and training algorithms is randomized for each subject. By doing this we mitigate the bias that a subject might become more familiar with the task when experimenting different algorithms.\nUser Study Questionnaire. We design a user study questionnaire to assess the experience of human subjects. The questionnaire is provided in Appendix B. Three aspects are considered:\n\u2022 Compliance measures whether the behaviors of the agent satisfy human intents. For example, a highly compliant agent behaves like human such that the human subjects feel like they are completing objectives by themselves.\n\u2022 Performance is the subjective evaluation from human subjects on whether the agent can solve the primal task, e.g. driving to the destination in navigation tasks. This score should be low if the agent cannot learn a particular behavior or forget it even though human subjects have taught the agent multiple times.\n\u2022 Stress gauges the cognitive cost human subjects pay to train the agent. A typical source of stress is the annoying oscillation and jitter the agent demonstrates. Unexpected behavior that requires human's instant reaction also creates stress. A lower score means more stress.\nThe same questions are repeated for each algorithm the human subject experimenting on."}, {"title": "Demo Video", "content": "Please find our demo video in the supplementary material. This video shows the footage of human experiments and the comparisons between agents learned by the baselines and the proposed method. The video contains three sections:\n1. The first section shows how we learn the driving policy in CARLA task within 20 minutes. We also compare the behavior of agents learned from PVP and TD3 baseline.\n2. In the second section, we show the footage of MetaDrive human experiment where the human subject uses a gamepad as the control device. We present the behavior comparison between PVP and TD3 baseline.\n3. In the third section, we show the applicability of our method to other tasks. PVP performs well in GTA V and can drive smoothly on the highway. In the discrete control tasks, the behavior comparison between PVP and DQN baseline in MiniGrid Empty Room and Four Room are provided."}, {"title": "Preference Alignment", "content": "Here we provide a conceptual framework to describe the compliance of human intention. First, we introduce a ground-truth indicator $C : S \\times A \\rightarrow {0, 1}$ of the intention violation, denoting whether the action is undesired. C is not revealed to the learning algorithm.\n$C(s, a) = \\begin{cases} 1, & \\text{if a violates human intention} \\\\ 0, & \\text{otherwise.} \\end{cases}$\nWe will derive the upper bound of the discounted occurrence of intent violation, a measure of training time human intent compliance:\n$\\Sigma_{\\pi_b} = \\mathbb{E}_{s_0 \\sim P^{\\pi_b}} [\\Sigma_{\\pi_b}(s_0) = \\Sigma_{t=0}^{\\infty} \\gamma^t C(s_t, a_t)]$,\nwhere $P^{\\pi_b}$ denotes the probability distribution of trajectories deduced by the behavior policy $\\pi_b$."}, {"title": "Environment Details", "content": null}, {"title": "Impact of Control Devices", "content": null}]}