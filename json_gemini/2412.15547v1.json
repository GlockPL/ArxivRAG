{"title": "NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning", "authors": ["Zheyuan Zhang", "Yiyang Li", "Nhi Ha Lan Le", "Zehong Wang", "Tianyi Ma", "Vincent Galassi", "Keerthiram Murugesan", "Nuno Moniz", "Werner Geyer", "Nitesh V Chawla", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Diet plays a critical role in human health, yet tailoring dietary reasoning to individual health conditions remains a major challenge. Nutrition Question Answering (QA) has emerged as a popular method for addressing this problem. However, current research faces two critical limitations. On the one hand, the absence of datasets involving user-specific medical information severely limits personalization. This challenge is further compounded by the wide variability in individual health needs. On the other hand, while large language models (LLMs), a popular solution for this task, demonstrate strong reasoning abilities, they struggle with the domain-specific complexities of personalized healthy dietary reasoning, and existing benchmarks fail to capture these challenges. To address these gaps, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first graph question answering dataset designed for personalized nutritional health reasoning. NGQA leverages data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is healthy for a specific user, supported by explanations of the key contributing nutrients. The benchmark incorporates three question complexity settings and evaluates reasoning across three downstream tasks. Extensive experiments with LLM backbones and baseline models demonstrate that the NGQA benchmark effectively challenges existing models. In sum, NGQA addresses a critical real-world problem while advancing GraphQA research with a novel domain-specific benchmark. Our codebase and dataset are available here.", "sections": [{"title": "1 Introduction", "content": "Diet is a cornerstone of human health, playing a pivotal role in both maintaining well-being and preventing disease. Despite the well-documented benefits of balanced nutrition, unhealthy eating habits remain alarmingly prevalent in modern society (WHO, 2021). In the United States alone, approximately 42.4% of adults are classified as obese (CDC, 2020a), and in 2017, poor dietary habits contributed to over 11 million deaths and a substantial number of disability-adjusted life-years (DALYs), often linked to factors such as excessive sodium intake (Afshin et al., 2019; WHO, 2023). These statistics underscore an urgent need to promote healthier eating habits on a societal scale. However, nutritional health requires complex domain knowledge, and there is no one-size-fits-all solution for healthy diets, as the nutritional needs of individuals can vary widely based on their health conditions. For example, a diet suitable for someone with a high body mass index (BMI) may differ drastically from that of an individual with a low BMI. Likewise, while individuals recovering from opioid misuse may benefit from a high-protein diet, such dietary choices can be harmful to those managing chronic kidney disease (Mahboub et al., 2021)."}, {"title": "Why this benchmark matters:", "content": "Numerous efforts have sought to address the challenges in personalized nutritional health, with Nutrition Question Answering (QA) emerging as a popular task (Min et al., 2022; Bondevik et al., 2024). Recent advancements in large language models (LLMs) have demonstrated significant potential in this domain, offering sophisticated reasoning capabilities to analyze and interpret nutritional information (Mavromatis and Karypis, 2024). However, these efforts remain constrained by two major limitations. First, to the best of our knowledge, no existing benchmark truly personalizes answers based on users' specific health conditions, primarily due to the inaccessibility of individual medical data (B\u00f6lz et al., 2023). This lack of user-specific datasets has severely hindered the development of effective solutions. Second, while LLMs exhibit impressive reasoning capabilities in general domains, the medical and nutritional intricacies of this task impose severe limitations on their effectiveness (Mialon et al., 2023). Current benchmarks fail to capture the domain-specific complexities of personalized health-aware dietary reasoning, making it difficult to evaluate, let alone improve, these models in meaningful ways.\nTo address these critical gaps and advance the understanding of healthy diet personalization, we propose the Nutritional Graph Question Answering (NGQA) benchmark. This is the first benchmark in the personalized nutritional health domain to evaluate whether a specific food is healthy for a user, supported by detailed reasoning of the key contributing nutrients. By recognizing the intricate interplay between a user's medical conditions, dietary behaviors, and the nutrition of foods, we frame this task as a knowledge graph question answering problem. Specifically, using data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS), we construct the NGQA benchmark and categorize questions into three complexity settings: sparse, standard, and complex. Each question type is further evaluated through three downstream tasks, binary classification (-B), multi-label classification (-ML), and text generation (-TG), to explore distinct reasoning aspects (Figure-1 (a)). We conduct extensive experiments using various LLM backbones and baseline models to ensure the benchmark is both appropriately challenging and meaningful for advancing the field. Our contributions can be summarized as follows:"}, {"title": "\u2022 Novel Benchmark for Personalized Nutrition.", "content": "We present NGQA, the first benchmark to incorporate users' medical information in a nutritional question answering task, addressing a significant research gap in the domain of personalized healthy diet research."}, {"title": "\u2022 Advancing the GraphQA Ecosystem.", "content": "NGQA introduces a domain-specific benchmark and extends GraphQA benchmarks beyond datasets like WebQSP and ExplaGraphs in the general domain. This addition broadens the scope of GraphQA research, enabling a more comprehensive evaluation of GraphQA models' capabilities beyond general reasoning tasks."}, {"title": "\u2022 Comprehensive Resource and Evaluation.", "content": "Through extensive experiments, NGQA provides a challenging benchmark, a complete codebase supporting the full pipeline from data preprocessing to model evaluation, and an extensibility for integrating new models. This comprehensive resource helps advance research in both personalized nutritional health and the broader GraphQA field."}, {"title": "2 Related Work", "content": "Question Answering in Nutritional Health Domain. Question answering has become an essential tool in the nutritional and health domain, offering a flexible framework for applications such as food recommendation (Min et al., 2022; Bondevik et al., 2024). Knowledge graphs (KGs) have been widely used to model relationships between foods, ingredients, and health, supporting tasks like ingredient substitution and adaptive dietary recommendations (Haussmann et al., 2019; Chen et al., 2021; Fatemi et al., 2023a; Xu et al., 2024). Recent approaches incorporate health metrics into QA systems, focusing on recipe recommendations and nutritional ontologies (Li et al., 2023; Seneviratne et al., 2021). However, existing methods lack true personalization, as highlighted by (B\u00f6lz et al., 2023), due to the absence of user-specific medical data. Our work fills this gap by introducing the first GraphQA benchmark for personalized nutritional health, enabling models to provide tailored nutritional reasoning and explanations."}, {"title": "3 NGQA Benchmark", "content": "Data Source. Using data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS), we construct the first GraphQA benchmark designed to address personalized healthy nutrition intake questions. This benchmark integrates detailed user health profiles, dietary behaviors, and comprehensive food nutritional information, enabling a fine-grained analysis of how individual health conditions interact with food nutrition. By representing these relationships through graph structures, the benchmark supports answering complex nutritional questions while capturing the intricate interplay between users' medical conditions and dietary choices. The following sections provide a detailed discussion of these datasets and their integration into our benchmark."}, {"title": "3.1 Data Collection", "content": "User Data Collection. The NHANES dataset forms the foundation of our work for collecting user data. We extract medical information, dietary habits, and food intake records to construct the graph. Specifically, NHANES provides laboratory reports detailing body metrics like Body Mass Index (BMI) and blood pressure, along with biochemical markers such as blood urea nitrogen. It also includes questionnaire responses on prescription drug usage, adherence to special diets, and overall health status. Additionally, NHANES records users' food intake history and dietary behaviors, such as the frequency of adding salt at the table. Our study incorporates 54 distinct dietary habits, with detailed data processing methods provided in Appendix-B. This comprehensive dataset serves as the backbone of our graph, capturing user health conditions and dietary patterns with granular detail.\nFood Data Collection. Nutritional information for food items is sourced from FNDDS. FNDDS connects NHANES food codes to detailed nutritional data cataloged in the What We Eat in America (WWEIA) database. Using FNDDS, we associate each food item in NHANES with its full nutritional composition. Additionally, FNDDS links food items to ingredient information and classifies them into broader food categories. For example, a food item like \"apple\" is linked to its nutrient values"}, {"title": "Graph Retrieval Augmented Generation.", "content": "Knowledge Graph Question Answering (KGQA) has progressed from early semantic parsing and retrieval-based methods to advanced techniques leveraging large language models (LLMs) and graph neural networks (GNNs) for reasoning and retrieval (Jiang et al., 2023; Kim et al., 2023; Gao et al., 2024). Building on this progress, Graph-Retrieval Augmented Generation (Graph-RAG) has emerged as a widely studied method, offering more precise, context- and structure-aware reasoning compared to traditional text-based RAG methods (Lewis et al., 2020; Lazaridou et al., 2022; Guo et al., 2024; Wen et al., 2023). Despite the development of various LLM-powered models, benchmarks for the Graph-RAG task remain scarce and lack standardization. Early benchmarks focus primarily on general graph tasks such as shortest paths and node degree (Fatemi et al., 2023b; Wang et al., 2024a), while (He et al., 2024) introduces a GraphQA benchmark for complex reasoning using general-purpose datasets. Building on their framework, we develop the first domain-specific benchmark in the nutritional health domain, bridging the gap between general GraphQA research and personalized health-aware reasoning. More detailed literature is available in Appendix-A."}, {"title": "3.2 Data Annotation", "content": "Real-world data is inherently messy and incomplete, and the datasets we use are no exception. Spanning from 2003 to 2020, NHANES provides data for approximately 100,000 users and over 2 million food records. While this dataset offers an invaluable resource for studying nutrition and health, it includes inconsistencies, ambiguities, and irrelevant entries. To establish a scientifically robust and meaningful benchmark, precise data annotation is essential. This involves not only cleaning and filtering the data but also carefully defining and validating annotations to accurately capture real-world relationships between health conditions, dietary behaviors, and food options. Our annotation process refines both user and food datasets to ensure relevance, accuracy, and applicability to real-life scenarios.\nUser Filtering. Annotating user data requires careful consideration of the complex interactions between nutrition and health. For instance, elevated blood urea nitrogen (BUN) levels may indicate kidney dysfunction, warranting a low-protein diet, but could also result from insufficient water intake. To maintain scientific rigor and practical relevance, we focus on annotating four prevalent health statuses-obesity, hypertension, opioid misuse, and diabetes-that are directly influenced by dietary interventions. Additionally, we annotate nine special diets reported by users, reflecting health-related dietary practices. Further details on the definitions and implications of these health statuses and diets are provided in the Appendix-B. To ensure consistency and relevance, we exclude users under 18, focusing solely on adult dietary patterns.\nFood Filtering. For food annotation, we identify practical entries in the FNDDS database that align with real-world dietary reasoning. While FNDDS supports comprehensive nutritional analysis, it includes many entries unsuitable for practical use, such as raw ingredients or standalone additives. To address this, we restrict our focus to the \"mixed dishes\" category, as it represents combined recipes closest to real-life diets. Additionally, we include other relevant categories, such as bakery products and desserts (definitions of FNDDS categories are available in the Appendix-I). Finally, we apply a keyword-based deduplication method to remove highly similar entries.\nMulti-step Annotation. Using the previously defined standards and tagging schemes, our annotation process systematically establishes \"match\" or \"contradict\" relationships between user health conditions and food nutritional profiles. For example, the tag \"high_calorie\" contradicts the condition \"obesity\", while \"low_sodium\" matches with \"hypertension\". To ensure accuracy and reliability, we"}, {"title": "3.3 Tagging Scheme.", "content": "To evaluate whether a food is specifically healthy for a user based on their personal health conditions, we propose a tagging scheme that assigns nutrition-related tags to both users and foods. This systematic framework aligns food nutritional properties with user health needs, enabling robust assessments of food suitability.\nFor food tagging, we build upon established guidelines and introduce newly applied standards. Prior works have utilized recommendations from the World Health Organization (WHO) and the Food Standards Agency (FSA) (Wang et al., 2021), while we extend this by incorporating the more detailed EU Nutrition & Health Claims Regulation (Commission, 2006) and the Codex Alimentarius Commission (CAC) (Alimentarius, 1985, 1997). These standards define precise thresholds for nutrient claims. For instance, the EU regulation permits labeling a food as \"low sodium\" only if it contains no more than 0.12 g of sodium per 100 g (Commission, 2006). Foods meeting such criteria are tagged with corresponding labels like \"low_sodium\" or \"high_protein\", reflecting their nutritional properties.\nOn the user side, health tags are derived from the NHANES dataset, which includes laboratory results and self-reported health information. For example, users with high blood pressure, as defined by American Heart Association (AHA) thresholds or similar guidelines, are tagged with \"hypertension,\" indicating that a low-sodium diet would be beneficial (Grillo et al., 2019; Smyth et al., 2014).\nBy linking health and food tags, our scheme effectively represents personalized dietary needs and captures the interplay between medical conditions and nutritional requirements. The detailed standards and additional tags for other nutrients and health conditions are described in Appendix-B. By integrating this methodology into our graph-based benchmark, we provide a framework for advancing personalized dietary reasoning and evaluating models in this domain."}, {"title": "4 Task Definition and Evaluation", "content": "With the annotated data in place, we designed three distinct types of questions, i.e., sparse, standard, and complex, to capture varying levels of difficulty and emulate real-world scenarios in personalized nutrition reasoning. This stratification ensures that our benchmark accommodates a wide range of research and application needs, spanning from controlled, idealized setups to challenging, real-life cases, as illustrated in Figure-3 (a).\nSparse questions address scenarios with minimal available information. In this setting, each food has only one nutrition tag linked to a single user health condition. This setup reflects real-world cases where labels are scarce or data is incomplete, challenging models to reason effectively with limited information. Although sparse questions may appear simple to human observers, the unique link between the user and the food significantly increases the difficulty of subgraph retrieval, making models vulnerable to interference from irrelevant nodes.\nStandard questions represent the balanced and idealized scenarios in our benchmark. In this category, foods are linked to multiple nutrition tags, which either match or contradict several user health conditions. This configuration reflects controlled cases where the relationship between dietary choices and health outcomes is clear-cut, enabling a focused evaluation of model performance. Standard questions serve as a foundation for benchmarking in structured and well-defined environments.\nComplex questions are designed to replicate the intricacies of real-life nutritional decision-making. Foods in this category may simultaneously have tags that both match with and contradict a user's health conditions. For instance, a food may be low in sodium (beneficial for hypertension) but also high in sugar (problematic for diabetes). These scenarios require models to navigate conflicting information, prioritize user health needs, and perform nuanced trade-off reasoning. This category closely mirrors the ambiguous and multifaceted challenges of real-world dietary decisions.\nThe benchmark's statistical breakdown is presented in Table-1. To further evaluate the complexity and informativeness of the questions, we introduce the Signal-to-Noise Ratio (SNR). SNR measures the ratio of nodes or tags relevant to the"}, {"title": "4.1 Question Setting", "content": "answer (signal) against the total nodes or tags in the graph (noise). As shown in Table-2, sparse questions exhibit the lowest SNR, reflecting the limited resources available for these tasks. Conversely, complex questions, despite containing conflicting information, achieve the highest SNR, underscoring the rich contextual information necessary for accurate reasoning. More statistics of the benchmark are available in Appendix-E."}, {"title": "4.2 Task Setting", "content": "To enhance the generality and versatility of our benchmark, we design three distinct downstream task types, each centered on the same domain question but requiring different forms of output, as illustrated in Figure-3 (b). This diversity ensures the benchmark accommodates a wide range of methodologies and research focuses while fostering innovation in addressing personalized nutrition challenges. The tasks are defined as follows:\nBinary Classification (-B): This task requires a simple \"yes\" or \"no\" response, indicating whether a specific food is suitable for a user based on their health profile. It emphasizes straightforward decision-making, reflecting applications like automated diet advisories or recommendation systems.\nMulti-Label Classification (-ML): In this task, models must retrieve the nutritional tags associated with a food and determine which match with or contradict the user's health conditions. By demanding richer output, this task evaluates the model's ability to leverage graph information and identify nuanced relationships.\nText Generation (-TG): The output is a natural language explanation of why a food is healthy or unhealthy for a user. This task assesses a model's capability for interpretable and user-friendly reasoning, which is crucial for real-world applications such as personalized dietary assistant chatbots."}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate model performance, we adopt task-specific metrics tailored to each type. For classification tasks, we use standard metrics like accuracy, recall, precision, and F1 score for comprehensive performance assessment. Multi-label classification tasks extend these metrics to their weighted versions, accounting for the distribution of multiple labels across samples. Text generation tasks are evaluated with widely used metrics such as ROUGE, BLEU, and BERT scores, which collectively assess relevance and semantic similarity to reference texts. The definition of ground truths is available in Appendix-B. This multifaceted design supports diverse model architectures and evaluation strategies, providing a robust foundation for advancing personalized nutrition research. By bridging the gap between controlled research environments and the complexities of real-world applications, our benchmark fosters innovation and opens new avenues for addressing healthy dietary reasoning."}, {"title": "5 Experiments", "content": "In this section, we conduct extensive experiments to evaluate existing Graph-RAG models' reasoning capability on the proposed benchmark. For baseline models, we select the five most classical baselines: KAPING (Baek et al., 2023), CoT-Zero (Kojima et al., 2022), CoT-BAG (Wang et al., 2024a), ToG (Sun et al., 2024), and a naive plain Graph-RAG pipeline (implementation details in Appendix-C). For the main experiments, we choose GPT-40-mini as the LLM backbone, we also conduct additional experiments on a series of other classic LLM backbones in Appendix-D. Note that we didn't select the most advanced LLM backbones or the most sophisticated fine-tuned baselines because we argue our contributions focus primarily on the proposed benchmark with the novel tasks for this specific domain, and the experiment results along with the hallucination analyses have demonstrated our tasks are properly designed where the classic baselines can be adequately challenged while maintaining efficiency. In the following sections, we go through the experiment results for each task."}, {"title": "5.1 Experiment Settings", "content": "Table-3 (a) presents the performance of baseline models on the binary classification task, which evaluates the models' ability to provide a decisive \"yes\""}, {"title": "5.2 Binary Classification Task", "content": "or \"no\" response based on summarized reasoning. The results reveal a notable conservatism in model behavior, as evidenced by the low recall scores. This likely stems from the sensitive nature of medical questions, where LLMs try to avoid offering simple \"yes\" answers without explanations unless their confidence is exceptionally high. Despite this challenge, the experiments yield two important insights into how external domain knowledge can support LLMs in this scenario. First, increasing the number of links in the graph (e.g., from Sparse to Standard questions) consistently improves recall across all baselines. This indicates that richer external knowledge provides LLMs with greater context and reassurance, enabling them to produce more confident positive answers. Second, ToG significantly outperforms other baselines, showing performance gains unique to this task. We attribute this improvement to ToG's effective pruning mechanism, which removes irrelevant nodes and increases the SNR. By reducing noise and focusing on relevant information, ToG enhances LLMs' ability to make confident and accurate decisions."}, {"title": "5.3 Multi-label and Text Generation Task", "content": "Table-3 (b) and (c) present the performance of baseline models on the multi-label classification (ML) and text generation (TG) tasks. The ML task evaluates models' ability to retrieve nutrition tags associated with foods and user health conditions, while the TG task tests their capacity to generate natural language explanations, offering a more comprehensive and realistic evaluation. The results reveal similar patterns across tasks: while baselines are competent at identifying nutrition tags from the graph, the primary challenge lies in correctly identifying the relevant tags based on user health conditions, as indicated by the overall high recall scores in the ML task.\nBoth tasks are most challenging on sparse question sets due to their low-resource nature. Conversely, models achieve the best performance on complex question sets, which may appear counterintuitive. However, as shown in Table-2, complex questions have a higher Signal-to-Noise Ratio (SNR), providing models with a clearer signal that offsets their logical complexity. Additionally, the ToG model performs similarly on the standard and complex question sets due to its pruning process, which increases SNR by removing irrelevant nodes. While effective, this process can also discard valuable information, leading to lower performance on complex questions. This trade-off contrasts with ToG's success in binary classification task and high-"}, {"title": "5.4 Efficiency and Retrieval Quality", "content": "Beyond model performance, efficiency is a critical consideration in Graph-RAG systems. To evaluate this, we conduct an efficiency analysis of baseline models on our benchmark, as shown in Figure-4. As can be seen, the binary classification task exhibits the fastest runtime, as it requires the shortest output. In contrast, the multi-label classification and text generation tasks involve longer outputs, leading to slower performance. Due to ToG's reliance on multiple LLM calls during the retrieval process, its runtime is significantly slower compared to other methods. Additionally, the quality of subgraph retrieval plays a crucial role in downstream reasoning. To assess this, we perform a retrieval quality analysis using ToG as a case study, comparing it against a plain Graph-RAG pipeline, as illustrated in Figure-5. As shown, the retrieval scores of ToG align with its performance in the main experiments, confirming our assumption that fluctuations in ToG's performance are rooted in its pruning process during the subgraph retrieval phase."}, {"title": "5.5 Error Analysis", "content": "In this section, we analyze the types of hallucinations observed in our experiments using a specific example and demonstrate the importance of external domain knowledge in mitigating these errors. Traditional LLM-enhanced methods are well-known for their susceptibility to hallucination errors, particularly in domain-specific tasks like nutritional health (Mialon et al., 2023). Figure-6 illustrates an example where we evaluate whether the food \"Taco, corn tortilla, beef, cheese\" is a healthy option for a user who is obese and recovering from opioid misuse. Our analysis identifies two main types of hallucinations. The first is Factual Hallucination, where the model produces incorrect or irrelevant information, often due to reliance on general knowledge not explicitly included in the graph. These errors are common when LLMs perform direct inference without external knowledge and occasionally occur when retrieved graphs contain noise. For example, the model incorrectly deemed the taco unsuitable, overlooking the fact that corn tortillas are relatively low in carbohydrates.\nThe second type is Contextual Hallucination, where the model fails to prioritize tags that directly relate to the user's health profile, focusing instead on less relevant attributes. This issue is less pronounced in ToG due to its ability to retrieve compact, focused subgraphs, unlike simpler methods like KAPING and CoT-Zero, which lack effective pruning. In this case, the taco's high sodium and cholesterol overshadowed its alignment with the user's specific health needs for a low-carb, high-protein diet, leading to a less optimal assessment.\nIn summary, these hallucinations highlight the importance of our domain-specific benchmark in establishing a rigorous framework to evaluate and improve LLMs, advancing both the nutritional health domain and Graph-RAG research while fostering the development of more robust and generalizable models (More examples in Appendix-H)."}, {"title": "6 Conclusion", "content": "In this work, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first dataset designed to address the critical challenges of personalized nutritional health reasoning. By leveraging user-specific medical data and framing the problem as a knowledge graph question answering task, NGQA bridges the gap between general-purpose benchmarks and domain-specific applications. Our benchmark not only advances the scope of GraphQA research by incorporating complex, real-world nutritional scenarios but also provides a comprehensive resource for evaluating and improving models in this domain. We believe NGQA lays the foundation for future research in personalized diet and health-aware reasoning, fostering innovation in both nutritional health and GraphQA."}, {"title": "7 Limitation", "content": "In this section, we discuss the limitations of this work and outline directions for future research. First, the benchmark includes a limited number of health conditions, though more are available. For example, osteoporosis suggests a high-calcium diet, a renal diet indicates low protein intake, and high low-density lipoprotein (LDL) levels may call for a low-cholesterol diet. As noted in the paper, we prioritized conditions most prevalent in the United States and most relevant to dietary interventions, but expanding to include additional conditions could enhance coverage and utility. Second, while we focus on the interplay between dietary behaviors and medical conditions, other factors, such as food insecurity, remain unexplored. NHANES offers extensive socioeconomic data, presenting opportunities to extend the benchmark to account for broader determinants of dietary decision-making. Third, for simplicity, complex questions are reduced to binary classification by counting \"match\" and \"contradict\" tags. However, real-life dietary decisions require nuanced trade-offs and reasoning that go beyond this approach. More sophisticated evaluation methods could better reflect practical scenarios. Lastly, the benchmark could benefit from additional tasks. For example, the existing graphs support questions like, \"What alternative foods could meet a user's dietary preferences and medical needs?\" Incorporating such tasks would broaden the benchmark's scope and encourage further innovation. Despite these limitations, this work establishes a robust baseline as a pioneering effort in personalized nutrition reasoning. We defer these challenges to future work, envisioning the benchmark as a foundation for ongoing advancements in this critical domain."}, {"title": "Ethics and Privacy Statement", "content": "Safeguarding privacy and adhering to ethical principles are paramount when working with sensitive health-related data. The National Health and Nutrition Examination Survey (NHANES) serves as a benchmark in this regard, strictly complying with confidentiality protocols mandated by public legislation. These robust privacy measures enable us to achieve our research goals while remaining fully aligned with the survey's established guidelines. Notably, the NHANES dataset is anonymized, with personally identifiable information (PII)-such as social security numbers and physical addresses\u2014removed. Despite the absence of PII, the dataset retains its utility for detailed analyses, allowing us to investigate the relationship between users' medical data and health-aware food recommendations as presented in this study. Additionally, in practical applications, the generated recommendations and interpretations are treated as personal medical records, ensuring sustained privacy protection. By adhering to these principles, our research maintains the highest levels of ethical responsibility and data privacy."}, {"title": "A Additional Related Work", "content": "Prior Works in Nutrition Personalization. With growing awareness of the importance of dietary health, various studies have sought to incorporate health metrics into applications such as food recommendation systems. These approaches can be grouped into three primary categories. First, some research emphasizes single indicators like calorie or fat content, as highlighted in works by Ge et al. (Ge et al., 2015) and Shirai et al. (Shirai et al., 2021; Li et al., 2024), though such metrics often fail to represent the multifaceted nature of a balanced diet. Second, simulated health data has been utilized, as demonstrated by Wang et al. (Wang et al., 2021), but these methods often diverge from real-world data distributions. Finally, recent studies have applied global health guidelines to develop composite health scores, such as those by Bolz et al. (B\u00f6lz et al., 2023) and Zhang et al. (Zhang et al., 2024a). However, foods deemed healthy by general standards can still negatively affect certain individuals (Yue et al., 2021), highlighting the absence of a universal solution. The primary challenge remains the scarcity of accurate user health data, a gap our benchmark uniquely addresses."}, {"title": "A.1", "content": "Knowledge Graph Question Answering. Knowledge Graph Question Answering (KGQA) has undergone significant advancements, evolving from early approaches such as semantic parsing and retrieval-based methods. Initial models translated natural language queries into structured formats like SPARQL for execution on knowledge graphs (Sun et al., 2019; Zhang et al., 2022). Many of these methods employed pre-trained models like BERT for query encoding and used frameworks such as GNNs or LSTMs for retrieving entities and subgraphs (Yasunaga et al., 2021; Taunk et al., 2023).\nMore recent progress integrates large language models (LLMs) to improve both retrieval efficiency and reasoning ability (Sanchez and Zhang, 2022; Liu et al., 2024a; Tan et al., 2024). Approaches like Jiang et al. (Jiang et al., 2023) and Wang et al. (Wang et al., 2023) utilize LLMs to transform queries into formats such as SQL or SPARQL, enhancing retrieval accuracy. Others, such as Kim et al. (Kim et al., 2023) and Gao et al. (Gao et al., 2024), focus on reasoning over retrieved subgraphs or triples, tackling multi-hop reasoning tasks in KGQA. However, most benchmarks in this field are designed for general-purpose datasets and fail to address domain-specific complexities, such as the challenges unique to nutritional health reasoning."}, {"title": "A.2", "content": "Graph-Retrieval Augmented Generation. Graph neural networks exhibit powerful potentials in dealing with complicated structural data (Wang et al., 2024d; Liu et al., 2023; Wang et al., 2024c) and it can facilitate LLM to better understand real world tasks (Wang et al., 2024b; Huang et al., 2024; Liu et al., 2024b). Graph-Retrieval Augmented Generation (Graph-RAG) extends the Retrieval-Augmented Generation (RAG) framework (Lewis et al., 2020) by enriching large language models with structured knowledge retrieval. While traditional RAG retrieves unstructured text, Graph-RAG leverages GNNs to retrieve structured subgraphs encoded as triples, improving reasoning precision and minimizing redundancy (Guo et al., 2024; Wen et al., 2023; Lazaridou et al., 2022).\nExisting Graph-RAG benchmarks primarily evaluate basic graph reasoning tasks, such as shortest paths, node degree, and edge existence (Fatemi et al., 2023b; Wang et al., 2024a). Although these benchmarks provide insights into foundational reasoning, they lack domain specificity. Recent work by He et al. (He et al., 2024) introduced benchmarks targeting advanced reasoning in general graph contexts, but domain-specific benchmarks for applications such as nutrition remain underdeveloped. By adapting the principles of Graph-RAG, our work introduces the first benchmark designed to tackle personalized health-aware reasoning, addressing this critical gap in the literature."}, {"title": "A.3", "content": "National Health and Nutrition Examination Survey (NHANES) is a publicly available dataset collected by the U.S. Centers for Disease Control and Prevention (CDC) to assess the health and nutritional status of the U.S. population through interviews, physical examinations, and laboratory tests. Data is released every two years and encompasses five main categories: Demographics, Dietary Data, Examination Data, Laboratory Data, and Questionnaire Data. These comprehensive datasets provide a wealth of information on health indicators, dietary behaviors, and medical conditions."}, {"title": "B.1", "content": "The Food and Nutrient Database for Dietary Studies (FNDDS) is a comprehensive resource developed by the U.S. Department of Agriculture (USDA) to facilitate dietary intake analysis by providing detailed nutritional information for foods and beverages consumed in the United States. It serves as the backbone for analyzing dietary recall data collected through the What We Eat in America (WWEIA) program, which is a component of NHANES. WWEIA captures dietary intake data through 24-hour dietary recall interviews, linking reported food and beverage items to their corresponding nutrient profiles in FNDDS. Together, FNDDS and WWEIA enable researchers to study dietary patterns, nutrient intake, and their relationship to health outcomes, making them critical tools for advancing nutrition research and public health policy."}, {"title": "B.2 Data Source Description", "content": "Dietary habit data was sourced from various NHANES tables, including the Diet Behavior and Consumer Behavior datasets, which capture user-reported behaviors and preferences related to food choices, preparation methods, and consumption patterns. Traditional processing approaches proved insufficient for the complexity and diversity of these features. To address this, a thorough manual review was conducted by a team of four researchers. Key features indicative of dietary habits, such as awareness of healthy eating practices or frequency of consuming processed or frozen foods, were identified and categorized. Users were then grouped into high and low habit categories based on their responses, with the top 10% and bottom 10% assigned corresponding habit tags. For instance, users reporting the highest milk consumption were tagged with \"drink lots of milk,\" while those with minimal consumption were labeled as \"drink little or no milk.\" This process generated 54 distinct dietary habit tags, which were incorporated as nodes in the graph. These habit nodes provide critical insights into user behaviors, enabling a nuanced understanding of the relationship between dietary patterns and health outcomes."}, {"title": "B.3 Dietary Habit Processing Details", "content": "In this section, we discuss the overall mapping relationship between health indicators and nutrition. In total, we involve nutrition tags for 16 different nutrients focusing on various health as-"}, {"title": "B.4 Full Mappings of Nutrition Tags", "content": "pects, including 7 for macro-nutrients (calories, carbohydrates, protein, saturated fat, cholesterol, sugar, and dietary fiber) and 9 for micro-nutrients (sodium, potassium, phosphorus, iron, calcium, folic acid, and vitamin C, D, and B12) following the tagging scheme introduced in (Zhang et al., 2024c). A detailed table"}]}