{"title": "GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models", "authors": ["Jonathan Drechsel", "Steffen Herbold"], "abstract": "Al systems frequently exhibit and amplify social biases, including gender bias, leading to harmful consequences in critical areas. This study introduces a novel encoder-decoder approach that leverages model gradients to learn a single monosemantic feature neuron encoding gender information. We show that our method can be used to debias transformer-based language models, while maintaining other capabilities. We demonstrate the effectiveness of our approach across multiple encoder-only based models and highlight its potential for broader applications.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) is often seen as a neutral tool without personal preferences or biases (Jones-Jang & Park, 2022; Jiang, 2024), but it can still exhibit and even amplify bias, including gender bias (Nadeem et al., 2020), with harmful impacts in crucial areas such as law enforcement, healthcare, and hiring (Buolamwini & Gebru, 2018; Ferrara, 2023). For instance, Amazon's AI-powered hiring tool, trained on resumes from a male-dominated tech industry, was found to favor male candidates, penalizing resumes referencing women's colleges (Dastin, 2022). This underscores a crucial problem: AI models, though seemingly neutral, can inherit and amplify real-world biases.\nRecent research has explored how gender bias appears in transformer-based language models (Nemani et al., 2024). Proposed solutions include specialized training (Zmigrod et al., 2019; Webster et al., 2021), pruning biased neurons (Joniak & Aizawa, 2022), post-processing steps that adjust model outputs without modifying internal parameters (Ravfogel et al., 2020; Liang et al., 2020; Schick et al., 2021), and methods to measure the gender bias (May et al., 2019; Nangia et al., 2020; Nadeem et al., 2021).\nIn this paper, we propose a novel approach to address gender bias in language models by leveraging gradients from gender-related inputs. We hypothesize that these gradients contain valuable information for identifying and modifying gender-specific features. Our method aims to learn a feature neuron that encodes gender information from the input, i.e., model gradients. Unlike existing approaches for extracting monosemantic features (e.g., Bricken et al. (2023)), our approach enables the learning of a feature neuron with a desired, interpretable meaning, such as gender. The feature neuron is modeled as bottleneck in a simple encoder-decoder architecture for model gradients. The decoder essentially learns what parts of the model needs to be updated to change a feature, e.g., to change gender bias.\nThis paper investigates two hypotheses: (H1) It is possible to learn targeted a single feature neuron from the model's gradients with a desired interpretation, such as gender. (H2) The feature neuron for gender can be used to change the model's gender bias without negatively effecting other capabilities. By exploring these hypotheses, we demonstrate the potential of targeted feature learning and achieve state-of-the-art results for gender debiasing when using GRADIEND together with INLP (Ravfogel et al., 2020). Although this study focuses on gender bias, the proposed encoder-decoder approach is generic and should also be able to learn other features.\nFor clarity, in this study, gender is treated as binary (while acknowledging and respecting non-binary gender identities). The term pronouns is used to refer specifically to third-person singular gendered pronouns (i.e., \"he\" and \"she\u201d), and name refers exclusively to first names."}, {"title": "2. Related Work", "content": "This section reviews interpretable feature learning, explores existing methods for gender-debiasing transformer models, and examines techniques for measuring gender bias."}, {"title": "2.1. Interpretable Feature Learning", "content": "Interpretable feature learning aims to identify and understand the internal representations of neural networks, focusing on how individual neurons or groups of neurons relate to specific concepts. Early methods focused on visualizing learned features through saliency maps (Simonyan et al., 2014) and activation maximization (Erhan et al., 2009), which highlighted the impact of input features on model predictions. Recent advancements focus on separating networks into semantically meaningful components like individual neurons or circuits (Olah et al., 2020). Research into monosemantic neurons, which correspond to a single natural feature, provides clearer and more interpretable insights compared to polysemantic neurons (Jermyn et al., 2022). Bricken et al. (2023) proposed to learn unsupervised a sparse autoencoder that extracts interpretable features in a high dimensional feature space, which are analyzed for semantical meaning based on their behavior. Follow-up studies (Templeton et al., 2024) improved scalability and identified specific features such as a gender-bias awareness feature in Claude 3 Sonnet (Anthropic, 2024). However, this approach requires learning numerous potential features and testing for meaningful interpretations, leaving it uncertain whether a desired feature will actually arise."}, {"title": "2.2. Transformer Gender-Debiasing Techniques", "content": "Various techniques have been proposed to mitigate gender bias in transformer language models, either by creating debiased models or through post-processing adjustments.\nCounterfactual Data Augmentation (CDA) (Zmigrod et al., 2019; Lu et al., 2020) is a straightforward method which swaps gender-related words consistently within a training corpus (e.g., replacing he/she), enabling further training on a balanced dataset. Webster et al. (2021) found experimentally that increasing DROPOUT during pre-training effectively reduces gender bias. Joniak & Aizawa (2022) applied Movement Pruning (Sanh et al., 2020) to reduce gender bias in language models by pruning weights that minimally impact neutral tasks, effectively removing gender bias associations while maintaining model performance.\nThe Iterative Nullspace Projection (INLP) (Ravfogel et al., 2020) is a post-processing method for gender-debiasing by iteratively training a linear classifier to detect gender based on model embeddings and subtracting the classifier's nullspace from the embeddings to remove gender-related information. Similarly, SENTDEBIAS (Liang et al., 2020) estimates a linear subspace associated with gender bias by using CDA to generate sentence pairs with swapped gendered terms. Sentence embeddings are then debiased by subtracting their projection onto this subspace. SELFDEBIAS (Schick et al., 2021) addresses bias in generated text by running inference with and without a bias-encouraging prefix, then downweighting tokens favored in the biased version. However, this task-specific approach is unsuitable for downstream tasks like GLUE (Wang et al., 2018)."}, {"title": "2.3. Techniques for Measuring Gender Bias", "content": "Sentence Encoder Association Test (SEAT) (May et al., 2019) extends Word Embedding Association Test (WEAT) (Caliskan et al., 2017) by using sentence templates to evaluate social biases in encoder models. It compares association strengths between embeddings of predefined attribute (e.g., gender-specific names) and target sets (e.g., stereotypical professions) using cosine similarity. Bias is expressed as an effect size, where larger values indicate stronger bias.\nStereoSet (Nadeem et al., 2021) is a benchmark dataset with context-rich sentences for intrasentence and intersentence tasks. This study focuses on the intrasentence task, where a sentence (e.g., Girls tend to be more [MASK] than boys) requires the model to predict the masked word from three options: stereotypical (e.g., soft), anti-sterotypical (e.g., determined), and meaningless (e.g., fish). Two metrics are considered: 1) Language Modeling Score (LMS), which measures the proportion of meaningful (stereotypical or anti-stereotypical) options chosen over meaningless ones, reflecting the model's language understanding. 2) Stereotype Score (SS), which quantifies bias as the proportion of stereotypical options selected over anti-sterotypical ones. A balanced model achieves 50%.\nCrowdsourced Stereotype Pairs (CrowS) (Nangia et al., 2020) is a crowdsourced dataset consisting of pairs of sentences: one expressing a stereotype (e.g., Woman don't know how to drive), and the other its anti-stereotypical counterpart (e.g., Man know how to drive). A bias score is computed considering the model's preference for one sentence over the other, similar to SS.\nLi et al. (2021) analyze the attention associations between gendered pronouns (e.g., she) and occupations (e.g., nurse) in transformer models,, using gender-swapped sentences (e.g., replace he by she). The attention scores between the gender-swapped pronouns and the occupation are then compared to identify gender bias on attention head level. However, the approach does not compute a model-specific, aggregated bias score usable for comparison."}, {"title": "3. Methodology", "content": "In this section, we introduce a novel approach for targeted feature learning and changing a model's gender bias. Our method utilizes a straightforward encoder-decoder architecture that leverages gradient information to encode a gender-related scalar value. This scalar is then decoded into gradient updates, which we believe that they are capable of changing the model's gender bias towards the encoded gender value. An overview of our approach is shown in Figure 1."}, {"title": "3.1. Motivation", "content": "Gradient-based explanation methods, such as Grad-CAM (Selvaraju et al., 2017) and Integrated Gradients (Sundararajan et al., 2017), have proven effective in providing insights into a model's internal workings (Chen et al., 2020; Selvaraju et al., 2020; Lundstrom et al., 2022), highlighting which parts of the model were crucial to a specific prediction. During the training of neural networks, the optimizer inherently determines which neurons require updates, specifically those that contributed incorrectly to the model's output.\nWe exploit this mechanism by adopting a Masked Language Modeling (MLM) task, a standard pre-training task for transformer language models (Devlin et al., 2018). Our task involves sentences containing a name, with the masked token corresponding to the gendered pronoun (he or she).\nAlice explained the vision as best [MASK] could.\nGradients are computed for both pronouns as targets:\nFactual: Alice explained the vision as best she could .\nCounterfactual: Alice explained the vision as best he could.\nThis factual-counterfactual evaluation strategy, inspired by CrowS (Nangia et al., 2020), uses gradient differences to isolate gender-related updates by eliminating non-gender-related changes common to both cases. This difference yields two inverse directions: strengthening or mitigating gender bias, depending on the gradient order. In the mitigating direction, the factual gender-related updates are eliminated, effectively removing the established factual associations, while the counterfactual updates are emphasized to facilitate the learning of new, counterfactual associations."}, {"title": "3.2. GRADIEND", "content": "In general, we aim to learn how to adjust model parameters to achieve a desired factual or counterfactual state. We hypothesize that the gradients contain the necessary information for this purpose and that the feature changing behavior can be controlled via a learned neuron.\nLet $W_m \\in \\mathbb{R}^n$ denote then model parameters for which the feature is learned. We consider three types of gradients: (1) gradients from the factual masking task $\\nabla_+ W_m$, (2) gradients from the counterfactual masking task $\\nabla_- W_m$, and (3) the difference between these two gradients $\\nabla_+ W_m := \\nabla_+W_m - \\nabla_-W_m$. Here, $\\nabla_{\\cdot} W_m$ represents a vector in $\\mathbb{R}^n$, where each component corresponds to the gradient for the parameter at this position. We frame the problem as a gradient learning task to predict the gradient difference $\\nabla_+ W_m$ from the factual gradients $\\nabla_+W_m$:\nLearn $f$ s.t. $f(\\nabla_+W_m) \\approx \\nabla_+W_m$.\nFor this study, we propose a simple encoder-decoder structure $f = dec \\circ enc$, where:\n$enc(\\nabla_+W_m) = tanh(W_e^T \\cdot \\nabla_+W_m + b_e) =: h\\in \\mathbb{R}$,\n$dec(h) = h W_d + b_d \\approx \\nabla_+W_m$.\nHere, $W_e, W_d, b_d \\in \\mathbb{R}^n$ and $b_e \\in \\mathbb{R}$ are learnable parameters, resulting in a total of 3n+1 parameters. We refer to this approach as GRADIent ENcoder Decoder (GRADIEND)."}, {"title": "3.3. GRADIEND for Gender Debiasing", "content": "While GRADIEND is defined to work with binary features in general, we restrict the following proof of concept to gender. In this setup, (H1) suggests that using the proposed factual and counterfactual masking tasks guides the encoder to encode a gender-related scalar feature $h$, indicating that the factual gradients contain gender-specific information. (H2) asserts that $dec(h)$ can be used to adjust the model's gender bias, e.g., by choosing a specific gender factor $h$ and learning rate $\\alpha$ to update the model parameters as follows:\n$W_m := W_m + \\alpha \\cdot dec(h)$.\nExperiments show that gender-related inputs are mostly mapped to values close to \u20131 and +1, corresponding to male and female gender or vice versa. WLOG, we assume positive values represent the female and negative values the male gender, and we modify the model accordingly to standardize the results for simplified definitions and consistent visualizations."}, {"title": "4. Data", "content": "The training and evaluation of GRADIEND relies on specialized data summarized in Table 1. The datasets include two name collections (NAMEXACT and NAMEXTEND), a dataset linking names to gender through masked pronouns (GENTER), a dataset of exclusively gender-neutral text (GENEUTRAL), and a dataset associating masked names with gender stereotypes (GENTYPES). We split the datasets used for training (GENTER and NAMEXACT) in train/val/test sets, and denote a specific split of a dataset by an index, e.g., NAMEXACTtrain. Detailed descriptions of dataset generation can be found in Appendix A."}, {"title": "5. Metrics", "content": "In this section, we introduce the metrics utilized for evaluating GRADIEND in learning a gender feature neuron and applying it for debiasing using this neuron."}, {"title": "5.1. Encoder as Gender Classifier", "content": "Following (H1), we evaluate whether the GRADIEND encoder encodes gender-related information by treating it as gender classifier. GENTER is used as labeled data, with +1 for female gradients and -1 for male ones. To measure how well the encoded values distinguish between genders, we compute the Pearson correlation coefficient (Cohen et al., 2009), denoted as $Cor_{GENTER}$, between the labels and the encoded values. Additionally, we compute the balanced accuracy, denoted as $ACC_{GENTER}$, where positive encoded values are classified as female and as male otherwise.\nAn encoded value of 0 can be seen as a neutral class,, representing a natural midpoint between the male and female extremes. For datasets with labels \u00b11 for gender and 0 for gender-neutral texts, we calculate again both, correlation and balanced accuracy. In this case, values \u2265 0.5 are classified female, < -0.5 as male, and intermediate values as neutral. Specifically, these three-label-metrics are computed across a combination of three datasets:\n\u2022 GENTER: Labeled as \u00b11 as before for $Cor_{GENTER}$.\n\u2022 GENTER0: A variation of GENTER where instead of the pronoun, a random gender-neutral token is chosen for masking and target calculation, considering a sentence of GENTER with a valid name/pronoun pair. This evaluates gender classification in a similar setting to training but with non-gender-related masking.\n\u2022 GENEUTRAL: A dataset where all tokens are gender-neutral by design, with a single token chosen for masking and target calculation.\nGENTER provides labels \u00b11, while the others contribute only labels 0. Across all three datasets, we compute the correlation, $Cor_{Enc}$, and the balanced accuracy, $Acc_{Enc}$. We also calculate the mean absolute encoded value $|h|$ for each dataset, denoted as $|h|_{Genter}, |h|_{Genter^0},$ and $|h|_{GENeutral}$."}, {"title": "5.2. Gender Bias Changed Models", "content": "We introduce metrics to quantify gender bias in a base model. Specifically, we aim to measure how debiased, female-biased or male-biased a model is. To achieve this, we define several intermediate metrics. These are informally introduced here and formally defined in Appendix B.\nLet $F$ represent the female gender, $M$ represent the male gender, and $G \\in \\{F, M\\}$ denote a gender. Let $t$ be a text of GENTYPES, i.e., a stereotyped sentence whose masked token intuitively matches well with names. The considered metrics are as follows: (1) $ACC_{Dec}$ is an accuracy score for an MLM task over GENEUTRAL, i.e., data independent of gender, used to evaluate the model's general language modeling capabilities. (2) $P_t(G)$ denotes the probability of $G$ for task $t$, defined as the sum of all probabilities belonging to single token names of that gender considering the MLM task $t$. (3) $P(G)$ is the average probability of $G$, i.e., $P_t(G)$ averaged over all $t$ in GENTYPES. (4) The Balanced Prediction Index (BPI) quantifies a model's gender bias by combining high $Acc_{Dec}$ to ensure good language modeling capabilities, balanced gender probabilities $(|P_t(F) \u2013 P_t(M)|$ is low) to ensure a low gender bias, and high overall reasonable predictions ($P_t(M) + P_t(F)$ is high). (5) The Female Prediction Index (FPI) measures bias towards the female gender by favoring models with high $Acc_{Dec}$, low $P_t(M)$, and high $P_t(F)$. (6) Similarly, the Male Prediction Index (MPI) measures bias towards the male gender.\nThe purpose of the last three metrics is to select the most debiased, female-biased, and male-biased models from a set of candidates, as discussed in Section 6.3."}, {"title": "6. Evaluation", "content": "In this section, we evaluate four GRADIENDS: two based on BERT (Devlin et al., 2018) (BERTbase and BERTlarge), one large RoBERTa (Liu et al., 2019), and one small DistilBERT (Sanh et al., 2019) model. These models represent different variants of encoder-only architectures, including the lightweight DistilBERT. For simplicity, all metrics are reported for the test split (or the entire dataset if not split), unless stated otherwise. For example, $Cor_{GENTER}$ is $Cor_{GENTERtest}$ and $Cor_{GENTERval}$ refers to the $Cor_{GENTER}$ metric for GENTERval."}, {"title": "6.1. Training", "content": "For each base model, we trained three GRADIENDS with different random seeds, selecting the best model based on $Cor_{GENTERval}$ at the end of the training process. Importantly, the prediction layers were excluded from GRADIEND parameters to ensure debiasing targets the language model itself, not just the classification head. In each training step, a batch is processed with a uniformly randomly selected gender, sampling only names of that gender from NAMEXACTtrain and pairing them with sentences from GENTERtrain. In total, we perform 23,653 training steps, corresponding to the number of template sentences in GENTERtrain. Further details about the training setup are provided in Appendix C."}, {"title": "6.2. Encoder as Gender Classifier", "content": "We analyze the scalar encoded value of the GRADIEND models in Figure 2 and Table 2 to evaluate (H1). We use three different datasets (see Section 5.1): 1) For GENTERtest, we perform gendered masking by pairing each template with one male and one female name from NAMEXACTtest, resulting in 5,406 samples. 2) In GENTER0, we use the same data as in 1), but with non-gender-specific tokens masked. 3) For GENEUTRAL, we perform masking on 10,000 samples. As shown in Figure 2, all models encode primarily values of -1 and +1 for gender-specific masking. For non-gender-specific masking, encoded values tend to center around a neutral value of 0.0, though the classification is less precise compared to gender-related masking. Importantly, the non-gender-specific masks were not seen during training."}, {"title": "6.3. Decoder as Bias-Changer", "content": "In this section, we investigate how the learned representation of the decoder can change the model's bias. The adjustment is controlled by two key parameters: the scalar input to the decoder network $h$ (called gender factor) and the learning rate $\\alpha$, which determines the scaling of the decoder output before adding it to the model weights. To assess the impact of these parameters, we evaluate the GRADIEND models across a grid of 15 gender factors and 16 learning rates, modifying the model weights as $W_m := W_m + \\alpha \\cdot dec(h)$. The results for BERTbase are shown in Figure 3, with additional plots for the other base models provided in Appendix D.1.\nInterestingly, all plots exhibit a nearly point-symmetric behavior. Due to the simplicity of the GRADIEND decoder architecture, the only difference between cells with inverted gender factor and learning rate is twice the bias term $b_e$, which becomes less influential as the gender factor grows.\nSpecifically, for $P(F)$ (Figure 3(a)) and $P(M)$ (Figure 3(b)) show an inverse pattern. The normalization of the encoder and the definition of $\\nabla_+ W_m$ (Section 3.2) result in a consistent pattern across all models: when the signs of $h$ and $\\alpha$ are equal, the model biases toward male, whereas different signs lead to a bias toward female. $Acc_{Dec}$ (Figure 3(c)) reveals a broad region of high probability, provided the learning rate remains moderate.\nFigures 3(d) to 3(f) illustrate the optimal models for BPI, FPI, and MPI, respectively. Colored arrows in the subplot captions illustrate how each sub-metric influences the aggregated metrics. Visually, these figures capture the trade-offs inherent in this debiasing approach, similar to those observed in other studies (Joniak & Aizawa, 2022). These aggregated metrics demonstrate that while stronger bias modification degrades language modeling performance, a safe region exists with moderate gender factors and learning rates. Considering the BPI plot (Figure 3(d)) and gender factor $h = 0.0$, the GRADIEND decoder's bias vector be effectively learned an appropriate debiasing direction.\nBased on BPI, FPI, and MPI, we identify models that are well-suited for defining debiased, female, and male representations, respectively. Table 3 lists the parameters and metrics for the selected models across all base models.\nSurprisingly, for the RoBERTa base model, $P(F) > P(M)$ holds true, unlike all other base models. This indicates a female bias in the given task, contradicting our expectation that language models typically exhibit male bias (although this bias direction is not captured by SS, SEAT, and CrowS).\nInterestingly, the difference in BPI between GRADIENDBPI and its base model is relatively small, whereas the corresponding differences for FPI and MPI are much larger, respectively. This observation suggests that biasing a model (in either direction) is easier than debiasing it. Notably, FPI approaches nearly zero for GRADIENDMPI models, and MPI similarly is close to zero for GRADIENDFPI models."}, {"title": "6.4. Comparison to Other Debiasing Techniques", "content": "In this section, we compare the three selected models for each base model presented in Table 3. Additionally, we report results for five different debiasing approaches introduced in Section 2.2: CDA (Zmigrod et al., 2019), DROPOUT (Webster et al., 2021), INLP (Ravfogel et al., 2020), SELFDEBIAS (Schick et al., 2021) and SENTDEBIAS (Liang et al., 2020). We also compare joint approaches combining GRADIENDBPI With INLP and SENTDEBIAS, as both methods only add an additional post-processing step. We employ established gender-bias metrics introduced in Section 2.2: SEAT, Stereotype Score (SS) (Nadeem et al., 2021), and CrowS (Nangia et al., 2020). As shown in Figure 3 and suggested by previous research (Joniak & Aizawa, 2022), debiasing may negatively impact language modeling capabilities. Hence, we also report the Language Modeling Score (LMS) from StereoSet (Nadeem et al., 2021) and GLUE (Wang et al., 2018), a standard NLP benchmark, to measure language modeling. Unlike previous studies, we report all metric scores with a 95% confidence interval, computed via bootstrapping (Davison & Hinkley, 1997), providing a more robust comparison of model performances.\nTable 4 summarizes the main results, with additional sub-results and implementation details in Appendix D.2. Statistically significant improvements (i.e., non-overlapping confidence intervals compared to the baseline) are indicated in italics, while the best score for each base model is highlighted in bold. The comparison of debiasing approaches is challenging due to the high uncertainty and variance across different gender-bias metrics. With confidence intervals as context, the effectiveness of existing debiasing methods appears less clear than suggested by prior research (Meade et al., 2022). Notably, no model significantly improves SEAT or CrowS. For CrowS, baseline values of at most 60% and a target value of 50% make statistical significance unattainable due to large confidence intervals (~6%), caused by the small number of samples (N = 1,508).\nDespite these considerations, GRADIEND shows promising debiasing results while generally maintaining language modeling capabilities. GRADIENDBPI shows a debiasing effect with a significant improvement in SS for BERTlarge. Although GRADIENDFPI is designed to create female-biased model, it achieves debiasing results comparable to GRADIENDBPI, with a significant improvement in SS for RoBERTa. This is intuitively reasonable, as the base models are mostly male-biased, and the debiasing transformation, while effective, is not strong enough to shift the model entirely toward a female bias. GRADIENDMPI also occasionally shows debiasing effects, although we expected that these models become more biased. We confirmed that GRADIENDBPI, GRADIENDFPI, and GRADIENDMPI align with their intended debiasing or bias-strengthening behaviors in some examples (see Appendix F).\nNotably, GRADIENDBPI combined with INLP is the only reported debiasing technique to achieve a significant improvement in SS across all base models, highlighting it robustness. The combination of GRADIENDBPI with SENTDEBIAS also demonstrates a stable debiasing effect, though it is less effective overall.\nReporting confidence intervals highlights that the effectiveness of established debiasing techniques is less clear than previously suggested. GRADIEND combined with INLP achieves stable state-of-the-art debiasing across various base models (H2)."}, {"title": "7. Limitations and Open Questions", "content": "We have demonstrated the effectiveness of GRADIEND as a proof of concept for gender by learning a monosemantic feature and altering the model bias. However, our study has focused only on the simple application of a single, binary feature. Several open questions remain, including the generalization to different targets, such as: (1) different types of biases, (2) continuous monosemantic features (e.g., sentiment analysis), and (3) related binary features (e.g., one feature neuron for each German article der, die, and das). Although tested on encoder-only transformers, it is not limited to this architecture and could be applied to any weight-based and gradient-learned model with a suitable learning task. We are especially interested in its application to state-of-the-art generative transformer models. This should include GPT-like (Radford et al., 2019) transformer models using the Causal Language Modeling (CLM) objective."}, {"title": "8. Conclusion", "content": "We present a novel approach that achieves two key objectives: (1) learning a monosemantic feature for the desired interpretation (gender) based on model gradients, and (2) implementing a debiasing technique to reduce gender bias in transformer language models. In contrast to most existing debiasing methods, our approach allows for modifying an already trained, biased model to create a truly less biased version. This approach is built on a simple encoder-decoder architecture, GRADIEND, featuring a single hidden neuron. The model learns to act as a gender feature in an unsupervised manner, using gradients from a specific MLM-based training task. We successfully applied this method to four encoder-only models and suggest that future work should explore its applicability to larger and generative models."}, {"title": "Impact Statement", "content": "Learning an explainable feature in a manner that allows its modification is an important step towards not only understanding and manipulating models by experts, e.g., for modularization and unlearning, but can also lead to controls for non-expert users, e.g., simple sliders to configure aspects of a model to align them with their expectations."}, {"title": "A. Data", "content": "We publish all of our introduced datasets on Hugging Face, see Table 5. Details regarding the data generation can be found in the subsequent sections."}, {"title": "A.1. \u039d\u0391\u039c\u0415\u0425\u0410\u0421\u0422", "content": "Several datasets were constructed with the help of an existing name dataset (UCI, 2020), which contains 133,910 names with associated genders, counts, and probabilities derived from government data in the US, UK, Canada, and Australia. From this dataset, we derive two subsets based on name ambiguity: NAMEXACT and NAMEXTEND.\nWe refer to NAMEXACT as a collection of names that are exclusively associated with a single gender and that have no ambiguous meanings, therefore being exact with respect to both gender and meaning. First, we filter all names of the raw dataset to retain only names with a count of at least 20,000, resulting in a selection of the most common 1,697 names. Next, we remove names with ambiguous gender, such as Skyler, Sidney, and Billie, which were identified by having counts for both genders in the filtered dataset, removing 67 additional names.\nTo further refine our selection of the remaining 1,630 names, we manually checked each remaining name for ambiguous meanings. For instance, names like Christian (believer in Christianity), Drew (the simple past of the verb to draw), Florence (an Italian city), April (month), Henry (the SI unit of inductance), and Mercedes (a car brand). This exclusion process was performed without considering casing to ensure applicability to non-cased models. The filtering resulted in the exclusion of 232 names, leaving us with a total of 1,398 names in NAMEXACT.\nWe split the data into training (85%), validation (5%), and test (10%) subsets, ensuring that the latter two splits are balanced with respect to gender."}, {"title": "A.2. \u039d\u0391\u039cEXTEND", "content": "We define NAMEXTEND as a dataset that extends beyond the constraints of NAMEXACT by including words that can be used as names, but are not exclusively names in every context.\nTo limit the number of names while ensuring sufficient representations, we set a minimum count threshold of 100 for the raw name dataset. This threshold reduces the total number of names by 72%, from 133,910 to 37,425, helping to save computationally time. This dataset includes names with multiple meanings and gender associations, as the threshold is the only filtering criterion applied.\nTherefore, names that can be used for both genders are listed twice in this dataset, once for each gender. By considering the counts of how often a name is associated with a particular gender, we can define the probability that a name is used for a specific gender. For a given name N and gender F (female) or M (male), we denote this probability as P(F|N) and P(M|N). For example, for the name N = Skyler, the dataset contains the probabilities P(F|Skyler) = 37.3% and P(M|Skyler) = 62.7%."}, {"title": "A.3. GENTER", "content": "For the training of GRADIEND, we introduce a new dataset called GEnder Name TEmplates with pronouns (GENTER), which consists of template sentences capable of encoding factual and counterfactual gender information, as illustrated in the motivating example in Section 3.1. Each entry in the dataset includes two template keys: a name [NAME] and a pronoun [PRONOUN]. For instance, the earlier discussed example sentences can be instantiated from the following template:\n[NAME] explained the vision as best [PRONOUN] could.\nUsing the popular BookCorpus (Zhu et al., 2015) dataset, we generated such template sentences that meet the following criteria:\n\u2022 Each sentence contains at least 50 characters.\n\u2022 Exactly one name from NAMEXACT is contained, ensuring a correct name match.\n\u2022 No other names from NAMEXTEND are included, ensuring that only a single name appears in the sentence.\n\u2022 The correct name's gender-specific third-person pronoun (he or she) is included at least once.\n\u2022 All occurrences of the pronoun appear after the name in the sentence.\n\u2022 The counterfactual pronoun does not appear in the sentence."}, {"title": "A.4. GENEUTRAL", "content": "To evaluate our models on data that is entirely gender-unrelated, we created a separate dataset called GENEUTRAL, again sourced from BookCorpus (Zhu et al., 2015).\nSimilar to GENTER, we removed any sentence containing gender-specific words, as identified for GENTER, as well as sentences with fewer than 50 characters. Additionally, we excluded sentences that contained any name from NAMEXTEND. This filtering process resulted in a total of 7.9M gender-neutral sentences, with all duplicate sentences removed."}, {"title": "A.5. GENTYPES", "content": "Given that the training of GRADIEND specifically involves the relationship between names and their associated genders, we aim to evaluate our models against the gender associations of names with gender-stereotypical contexts."}]}