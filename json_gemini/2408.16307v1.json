{"title": "Safe Bayesian Optimization for High-Dimensional Control Systems via Additive Gaussian Processes", "authors": ["Hongxuan Wang", "Xiaocong Li", "Adrish Bhaumik", "Prahlad Vadakkepat"], "abstract": "Controller tuning and optimization have been among the most fundamental prob-lems in robotics and mechatronic systems. The traditional methodology is usuallymodel-based, but its performance heavily relies on an accurate mathematical modelof the system. In control applications with complex dynamics, obtaining a precisemodel is often challenging, leading us towards a data-driven approach. Whileoptimizing a single controller has been explored by various researchers, it remainsa challenge to obtain the optimal controller parameters safely and efficiently whenmultiple controllers are involved. In this paper, we propose a high-dimensionalsafe Bayesian optimization method based on additive Gaussian processes to op-timize multiple controllers simultaneously and safely. Additive Gaussian kernelsreplace the traditional squared-exponential kernels or Mat\u00e9rn kernels, enhancingthe efficiency with which Gaussian processes update information on unknown func-tions. Experimental results on a permanent magnet synchronous motor (PMSM)demonstrate that compared to existing safe Bayesian optimization algorithms, ourmethod can obtain optimal parameters more efficiently while ensuring safety.", "sections": [{"title": "Introduction", "content": "Optimizing the controller parameters of complex systems involving multiple controllers is a chal-lenging task. This includes the cascade feedback control architecture typically adopted in motor control, as well as advanced controllers involving feedforward, disturbance observer (DOB) (Jung and Oh, 2022), and active disturbance rejection control (ADRC) (Cao et al., 2024), among others. For instance, in the case of permanent magnet synchronous motor (PMSM) control, field-oriented control (FOC) is commonly employed (Gabriel et al., 1980; Lara et al., 2016; Wang et al., 2016). The closed-loop configuration of FOC incorporates three independent proportional-integral (PI) controllers, each with two separate control gains. These six gains require simultaneous adjustment to obtain the optimal parameter combination that enhances control performance. Each adjustment of the parameter combination requires an evaluation process lasting several minutes and also demands extensive experience from a control engineer. Therefore, an efficient and automatic optimization approach using machine learning is needed.\nTraditional automatic tuning and optimization methods rely on simplified reduced-order models with assumptions such as linearity. These assumptions, along with modeling errors, often lead to suboptimal performance of controllers in real-world systems (Berkenkamp et al., 2016). Mean-while, motion data from real-world systems operating under suboptimal conditions often contain"}, {"title": "Problem statement", "content": "The safe optimization problem for complex cascade systems is considered. Cascade systems have multiple controllers, and the output of the outer loop controller serves as the input of the inner loop controller. Consider the discrete-time proportional-integral (PI) control law:\n$u_k = k_p \\cdot (y_k - r_k) + k_i \\sum_{t=0}^{k}(y_k - r_k)$,\nwhere $u_k$ is the control action in time step k, $y_k$ is the plant output, $r_k$ is the reference signal, and $(k_p, k_i)$ are the control gains. In a 2-layer cascade system (Figure 1), the control laws for both layers will be:\n$u_{in}^k = k_p^{in} \\cdot (y_{in}^k \u2013 u_{out}^k) + k_i^{in} \\cdot \\sum_{t=0}^{k}(y_{in}^k - u_{out}^k)$,\n$u_{out}^k = k_p^{out} \\cdot (y_{out}^k - r_k) + k_i^{out} \\cdot \\sum_{t=0}^{k}(y_{out}^k \u2013 r_k)$.\nIn a general form, denote the outermost layer as layer 0, and the nth inner layer as layer n, then the control action $u_k$ in layer n is a function of the plant output $y_k$ in all layers from layer 0 to layer n, the reference signal $r_k$, and the controller parameters $a$:\n$u_k = g((y_k, y_1, ..., y_n), r_k, a)$,\nwhere $a \\in A$, and A is the domain for possible controller parameters. The controller\u2019s performance measure depends on how well it accomplishes its objective. Instead of modeling complex systems, performance measurement is modeled as a function of controller parameters, $J(a) : A \\rightarrow R$, and all constraints are modeled as functions of controller parameters, $G(a) : A \\rightarrow R$. Both $J(a)$ and $G(a)$ are evaluated on the systems, using cost functions such as Integral Square Error (ISE), Integral Absolute Error (IAE), or Integral Time-weighted Absolute Error (ITAE).\nWe are to solve a sequential decision problem that finds a maximizing J(a) while making all G(a) satisfy the constraints. Safety considerations are included in G(a). With the assumption that an initial safe controller and its performance, $(a_0, J(a_0))$, is available, a sequence of parameters $a_1, a_2,..., a_n \\in A$ are selected, and the noisy performance measurement $J(a_n) + d_n$ is obtained after each selection. During the evaluation, $G(a_n) \\geq 0$ must hold with high probability for all G(a_n), where 0 is chosen without loss of generality. In control applications, it is usually desired to find the"}, {"title": "Additive Gaussian processes-based safe Bayesian optimization", "content": "Bayesian optimization uses the Gaussian processes to approximate unknown objective functions. By defining an appropriate covariance function $k(a_i, a_j)$, the Gaussian processes can combine past observations to predict the mean and variance of the value of the objective function at unobserved points:\n$\\mu_n(a) = k_n(a)(K_n + I_n \\sigma_\\text{now})^{-1} \\hat{I}_n, \\sigma_n(a) = k(a, a) \u2013 k_n(a) (K_n + I_n \\sigma_\\text{ow})^{-1} k(a)$,\nwhere $\\hat{I}_n = [J(a_1), ..., \\hat{J}(a_n)]^T$ is the vector of noisy performance measurements, the matrix $K_n$ has entries $[K_n](i,j) = k(a_i, a_j)$, and the vector $k_n(a) = [k(a, a_1), ..., k(a, a_n)]$. $k(a_i, a_j)$ is also called the kernel of the Gaussian processes.\nThrough the mean and variance of the value of the unknown function at each point, the upper and lower bounds of the confidence interval can be calculated:\n$U_n(a) = \\mu_{n-1}(a) + \\beta_n \\sigma_n(a), l_n(a) = \\mu_{n-1}(a) \u2013 \\beta_n \\sigma_n(a)$,\nwhere $\\beta_n$ is a variable defining the confidence interval. Previous safe Bayesian optimization algo-rithms, such as SafeOpt (Sui et al., 2015; Berkenkamp et al., 2016), use the upper and lower bounds of the confidence interval to define safe sets $S_n$, which contain all the parameters a that have high probabilities of getting the values of safety functions $g_i$ above the safe thresholds $h_i$; and sets of potential maximizers $M_n$, which contain a that could obtain the optimum of the performance function j; and sets of potential expanders $E_n$, which contain a that could be recognized as safe after a new iteration. We relax the Lipschitz constants $L$ in the expressions of $S_n$, $M_n$, and $E_n$ for ease of implementation, and show them in Algorithm 1. By limiting the points selected for evaluation in each iteration to $S_n$, previous safe Bayesian optimization algorithms ensure that the iteration process has a high probability of not violating safety constraints. The selection at each iteration follows different acquisition functions, such as the GP-UCB method (Srinivas et al., 2010), or the modified UCB method (hereinafter referred to as \"UCB-LCB\") proposed by Berkenkamp et al. (2016):\n$a_n = arg max_{a \\in E_n \\cup M_n} w_n(a), w_n(a) = U_n(a) \u2013 l_n(a)$.\nDespite various improvements for high-dimensional problems, such as the SwarmSafeOpt algorithm used in Berkenkamp et al. (2016) and the LINEBO algorithm (Kirschner et al., 2019), the squared-exponential (Gaussian) kernels used in these work have limited information acquisition ability in the parameter space. Therefore, we built upon the idea from additive Gaussian processes (Duvenaud et al., 2011), implementing high-dimensional additive structures to the original Gaussian kernels, to obtain a higher information acquisition efficiency."}, {"title": "Theoretical results", "content": "The convergence of previous safe BO algorithms (Sui et al., 2015, 2018) are guaranteed based on two assumptions: by choosing some common Gaussian kernels, (1) the performance function f and safety functions gi have bounded norms in their Reproducing Kernel Hilbert Spaces (RKHS) associated with the GPs, and (2) the safety functions are Lipschitz-continuous. We will prove that the additive Gaussian kernel composed of the one-dimensional Gaussian kernels that satisfy the two assumptions can also make the objective function satisfy the two assumptions. Therefore, the convergence of the proposed method will naturally conform to previous safe BO algorithms."}, {"title": "Experiments", "content": "The efficacy of the proposed algorithm (hereafter referred to as \"our method\") is validated on a PMSM. The architecture of the FOC scheme is depicted in Figure 2, which comprises a cascade control loop. The external controller is a speed controller responsible for regulating the motor\u2019s rotational speed. The internal controllers consist of two current controllers that manage the current output from the inverter. These three controllers are interdependent, so the simultaneous adjustment of the six parameters across all controllers is essential to obtain the optimal parameter combination."}, {"title": "Simulations in Simulink", "content": "In this section, the simulation employs FOC for a PMSM, modeled in Simulink using Simscape Elec-trical components2. The objective is to determine the controller parameters that optimize the speed tracking performance of the PMSM, aiming to maximize transient response speed while minimizing overshoot and steady-state error. This objective is crucial for various industrial applications, including precise robot joint control, industrial automation system control, and electric vehicle control, among others. The transient response of the system is evaluated using the 5% settling time, defined as the duration required for the response curve to reach and remain within 5% of the steady-state value. The performance function is then designed as:\n$J(t_s, O_s, e_{ss}) = w_s \\cdot (t_o - t_s) - w_o \\cdot O_s - w_e e_{ss}$,"}, {"title": "Experiments with Speedgoat real-time machine", "content": "In this section, real-time experiments are conducted using the Speedgoat machine, shown in Figure 5. The configuration includes a controller with integrated speed and current loops, an inverter, and a PMSM. The control algorithm within the controller is adjustable via MATLAB. The transient response of the system is assessed using 5% settling time, and the performance function and two safety functions are designed as described in section 4.1.\nWe use the default param-eters in Speedgoat to build the initial controller, and evaluations confirm that both safety functions meet the minimum thresholds.\nThe initial speed tracking result is represented by the green curve in Figure 6a. The proposed Algorithm 1 is employed to optimize the controller parameters.\nThe configuration of the six base kernels is consis-tent with those detailed in section 4.1, and the explo-ration phase is designed to last for 15 iterations. After the 35th iteration, our method obtains the optimal controller parameter combination, as depicted by the red curve in Figure 6a. Figure 6b illustrates the changes in the performance function, which stabilizes around the final values post-35 iterations. Figures 6c and 6d display the changes in the safety function values. Observations indicate that the values of the safety functions for the parameter combinations assessed by our method are almost all above the minimum threshold, suggesting that the optimization process adheres to safety constraints.\nIt is also pertinent to note that the result from the 40th iteration (illustrated by the purple curve in Figure 6a) demonstrates a higher performance value than that of the 35th iteration, featuring a faster transient response with negligible overshoot and steady-state error. This is an interesting and non-intuitive result, as control engineers typically tune the system performance to resemble the red curve without the vibration in the purple curve. In certain applications, such as industrial high-throughput semiconductor packaging systems, slight vibrations are acceptable as long as the"}, {"title": "Conclusions", "content": "In this study, we propose to replace traditional Gaussian kernels or Mat\u00e9rn kernels with high-dimensional additive Gaussian kernels, enabling the application of safe Bayesian optimization to high-dimensional complex control systems. The additive Gaussian kernels are more efficient in exploring high-dimensional space information, accomplishing the exploration of the safe set in fewer iterations. We verified the effectiveness of the proposed method for PMSM control in both simulation and real-time experiments. The results indicate that our proposed method surpasses existing safe Bayesian optimization algorithms in high-dimensional control system optimization and can be seamlessly integrated into real-world industrial control applications. Although tested only for PMSM control, the proposed algorithm is potentially applicable to other types of control architecture, as well as to other robotic and mechatronic systems, since it is designed for a general dynamical system.\nHowever, the limitation of the work is that the calculations of high-dimensional additive Gaussian kernels become more complex when the dimensions of the control problems get higher. The calculation of Eq. 10 usually takes a long time and even causes the program to crash when the dimension is higher than 10. A possible solution is to use kernel selection methods (Cristianini et al., 2001; Kandola et al., 2002; Ding et al., 2020) to obtain one or more additive Gaussian kernels with the highest efficiency in exploring the parameter space, and use the selected kernels for subsequent optimization."}, {"title": "Detailed proofs of theoretical results", "content": "A Gaussian kernel is defined as:\n$K(x,y) = exp(-\\frac{||x - y||^2}{2\\sigma^2})$\nFor the Lipschitz continuity of the Gaussian kernel, if we consider any two points x and y in the input space X, we need to prove that there exists a constant L such that:\n$|K(x, z) \u2013 K(y, z)| \\leq L||x - y||$\nfor all z \u2208 X.\nGiven that each one-dimensional Gaussian kernel $K_i(x_i, y_i) = exp(-\\frac{(x_i-y_i)^2}{2\\sigma^2})$ is $L_i$-Lipschitz-continuous, then there exists a constant $L_i$ such that:\n$|K_i(x_i, z_i) - K_i(y_i, z_i)| \\leq L_i |x_i - y_i|$\nfor all $x_i, y_i, z_i \\in X_i$.\nd\nTo prove that the additive Gaussian kernel $K(x, y) = \\sum_{i=1} K_i(x_i, y_i)$ satisfies Lipschitz continuity,\nwe need to show that there exists a constant L such that:\n$|K(x, z) \u2013 K(y, z)| \\leq L||x - y||$\nfor all x, y, z \u2208 X.\nConsider the difference of additive Gaussian kernels:\n|$K(x, z) - K(y, z)| = |\\sum_{i=1}^{d} K_i(x_i, z_i) - \\sum_{i=1}^{d} K_i(y_i, z_i)|$\nAccording to the triangle inequality,\n|$K(x, z) - K(y, z)| = |\\sum_{i=1}^{d} K_i(x_i, z_i) - \\sum_{i=1}^{d} K_i(y_i, z_i)| = \\sum_{i=1}^{d}|K_i(x_i, z_i) \u2013 K_i(y_i, z_i)|$.\nSince each $K_i$ is $L_i$-Lipschitz-continuous,\n$|K_i(x_i, z_i) \u2013 K_i(y_i, z_i)| \\leq L_i |x_i - y_i|,$"}]}