{"title": "Safe Bayesian Optimization for High-Dimensional Control Systems via Additive Gaussian Processes", "authors": ["Hongxuan Wang", "Xiaocong Li", "Adrish Bhaumik", "Prahlad Vadakkepat"], "abstract": "Controller tuning and optimization have been among the most fundamental problems in robotics and mechatronic systems. The traditional methodology is usually model-based, but its performance heavily relies on an accurate mathematical model of the system. In control applications with complex dynamics, obtaining a precise model is often challenging, leading us towards a data-driven approach. While optimizing a single controller has been explored by various researchers, it remains a challenge to obtain the optimal controller parameters safely and efficiently when multiple controllers are involved. In this paper, we propose a high-dimensional safe Bayesian optimization method based on additive Gaussian processes to optimize multiple controllers simultaneously and safely. Additive Gaussian kernels replace the traditional squared-exponential kernels or Mat\u00e9rn kernels, enhancing the efficiency with which Gaussian processes update information on unknown functions. Experimental results on a permanent magnet synchronous motor (PMSM) demonstrate that compared to existing safe Bayesian optimization algorithms, our method can obtain optimal parameters more efficiently while ensuring safety.", "sections": [{"title": "Introduction", "content": "Optimizing the controller parameters of complex systems involving multiple controllers is a challenging task. This includes the cascade feedback control architecture typically adopted in motor control, as well as advanced controllers involving feedforward, disturbance observer (DOB) (Jung and Oh, 2022), and active disturbance rejection control (ADRC) (Cao et al., 2024), among others.\nFor instance, in the case of permanent magnet synchronous motor (PMSM) control, field-oriented control (FOC) is commonly employed (Gabriel et al., 1980; Lara et al., 2016; Wang et al., 2016). The closed-loop configuration of FOC incorporates three independent proportional-integral (PI) controllers, each with two separate control gains. These six gains require simultaneous adjustment to obtain the optimal parameter combination that enhances control performance. Each adjustment of the parameter combination requires an evaluation process lasting several minutes and also demands extensive experience from a control engineer. Therefore, an efficient and automatic optimization approach using machine learning is needed.\nTraditional automatic tuning and optimization methods rely on simplified reduced-order models with assumptions such as linearity. These assumptions, along with modeling errors, often lead to suboptimal performance of controllers in real-world systems (Berkenkamp et al., 2016). Meanwhile, motion data from real-world systems operating under suboptimal conditions often contain valuable information that traditional model-based methods fail to fully exploit. Data-driven control optimization addresses this limitation by directly leveraging the information in the motion data to optimize controller parameters. It typically models the system's performance as a function of controller parameters and then explores the optimal parameter iteratively. In this line of research, various algorithms have been designed, with gradient-based algorithms being among the most popular approaches; however, they require accurate gradient estimations (Li et al., 2024), which can be challenging to obtain with noisy experimental measurements and often lead to convergence at local optima. Additionally, genetic algorithms typically involve extensive testing, making them impractical for real-world applications (Davidor, Jan. 1991).\nBayesian optimization (BO) (Mockus, 2012) was introduced to address these limitations by modeling the system's performance function using a Gaussian process (GP) (Rasmussen and Williams, 2006). In this framework, each controller parameter combination is associated with a performance value represented by a Gaussian distribution, which includes noise measurements. Srinivas et al. (2010) demonstrated that BO methods can converge to the global optimum of unknown performance functions in fewer steps compared to genetic algorithms. However, the BO procedure iteratively tests parameters with the highest uncertainty, often evaluating potentially unsafe controller parameters, which may lead to system instability. Therefore, controller optimization requires the use of a safety-aware BO algorithm, and some representative related work is introduced as follows."}, {"title": "Related work", "content": "The SafeOpt (Sui et al., 2015) and StageOpt (Sui et al., 2018) algorithms first address the safety concerns of the BO method. They introduce the safe set to avoid evaluating controller parameters whose safety function values fall below a safety threshold, thereby ensuring safety. Berkenkamp et al. (2016) applied SafeOpt to quadrotor controller tuning, validating SafeOpt's practical effectiveness. However, SafeOpt uses Gaussian kernels or Mat\u00e9rn kernels as the covariance function of the Gaussian processes, which is effective only for low-dimensional problems. Thus, experiments in Berkenkamp et al. (2016) optimize the x, y, and z-axis PI controllers of the quadrotor separately, and each controller has two parameters. Likewise, Fiducioso et al. (2019) added contextual constraints to SafeOpt and only automated the tuning of two parameters for a room temperature controller in a simulator. Additionally, SafeOpt uses the maximum uncertainty sampling acquisition function to balance exploration and exploitation, which causes the evaluated objective function values to fluctuate and not converge. In real control problems, since the optimal solution is unknown, the exact regret cannot be calculated, making it hard to confirm that SafeOpt has obtained the optimal value of the objective function. Although the stage-wise algorithm (Sui et al., 2018) ensures the convergence of the optimization stage, it still does not improve the efficiency in high dimensions.\nDjolonga et al. (2013) assumed that high-dimensional problems could be decomposed into several lower-dimensional subspace optimization problems. Following this, Kirschner et al. (2019) proposed the LINEBO algorithm, claimed as the first and currently the only safe BO algorithm applied to high-dimensional problems. LINEBO decomposes the high-dimensional space into multiple one-dimensional subspaces for safe BO in each subspace, which often requires hundreds or even more than a thousand iterations to find the optimal solution. It is feasible for general optimization problems where performance evaluation can be easily computed in simulation but less feasible for optimizations that involve real-world experiments, such as our control problems.\nTwo main differences exist between control optimization and the general optimization problems addressed in LINEBO (Kirschner et al., 2019), making it less effective for high-dimensional control optimization problems. First, the number of parameters in control optimization is commonly between 6 and 10. For example, the electric motor FOC control system is a cascade loop with three PI controllers and six parameters (Gabriel et al., 1980; Lara et al., 2016; Wang et al., 2016); the quadrotor system has three axes with a total of six control parameters, and sometimes twelve parameters if angle control is considered (Berkenkamp et al., 2016; Yuan et al., 2022); the gantry system used in industrial automation is a cascade system consisting of three axes, each with an outer loop P controller and an inner loop PI controller, so there are a total of six or nine parameters (Rothfuss et al., 2023; Wang et al., 2022, 2023). The problems studied in Kirschner et al. (2019) have 10 to 100 parameters, so the problem scale is different. Second, after each iteration, the controller parameters are applied to the real system to obtain performance and safety evaluations, which usually takes a certain amount of time (ranging from several minutes to tens of minutes). Additionally, the wear on the real system accompanies each evaluation. Therefore, too many iterations are not acceptable in our problem. In contrast, the optimization problems studied in Kirschner et al. (2019) generally do"}, {"title": "", "content": "not involve actual experiments, allowing for hundreds or even thousands of iterations. Hence, a safe optimization algorithm with higher efficiency in high-dimensional control problems is needed.\nAccording to Bengio et al. (2005), the locality of Gaussian kernels prevents GP models from capturing non-local structures. Then Duvenaud et al. (2011) introduced additive Gaussian processes, creating a high-dimensional additive structure for Gaussian kernels, significantly improving the Gaussian process's capability to model high-dimensional unknown functions. Rolland et al. (2018); Kandasamy et al. (2015); Mutny and Krause (2018) demonstrate that additive Gaussian processes have higher efficiency in high-dimensional Bayesian optimization. However, experimental validation involving hardware is limited, and its combination with safety constraints has not been theoretically proved and experimentally validated."}, {"title": "Our contributions", "content": "Given the traits of multi-parameter complex control systems, our main contributions in this work are threefold: 1) We employ high-dimensional additive structures to Gaussian kernels and utilize a stagewise iteration strategy to develop a novel safe Bayesian optimization method specifically designed for high-dimensional control optimization. The convergence of the proposed method is ensured by theoretical analysis. 2) Comprehensive simulation experiments are conducted using FOC with six control gains, demonstrating that the proposed method surpasses traditional frequency response-based methods and conventional safe Bayesian optimization algorithms in terms of control performance and efficiency. 3) Real-time experiments for optimizing PMSM controller parameters are executed using the Speedgoat real-time machine, thereby validating the practical applicability of the proposed method."}, {"title": "Problem statement", "content": "The safe optimization problem for complex cascade systems is considered. Cascade systems have multiple controllers, and the output of the outer loop controller serves as the input of the inner loop controller. Consider the discrete-time proportional-integral (PI) control law:\n$u_k = k_p \\cdot (y_k - r_k) + k_i \\sum_{t=0}^{k}(y_k - r_k)$,\nwhere $u_k$ is the control action in time step k, $y_k$ is the plant output, $r_k$ is the reference signal, and $(k_p, k_i)$ are the control gains. In a 2-layer cascade system (Figure 1), the control laws for both layers will be:\n$u_{in}^k = k_{pin} \\cdot (y_{in}^k \u2013 u_{out}^k) + k_{iin} \\cdot \\sum_{t=0}^{k}(y_{in}^k - u_{out}^k)$,\n$u_{out}^k = k_{pout} \\cdot (y_{out}^k - r_k) + k_{iout} \\cdot \\sum_{t=0}^{k}(y_{out}^k \u2013 r_k)$.\nIn a general form, denote the outermost layer as layer 0, and the nth inner layer as layer n, then the control action $u_k$ in layer n is a function of the plant output $y_k$ in all layers from layer 0 to layer n, the reference signal $r_k$, and the controller parameters a:\n$u_k = g((y_k, y_{k-1}, ..., y_k^n), r_k, a)$,\nwhere $a \\in A$, and A is the domain for possible controller parameters. The controller's performance measure depends on how well it accomplishes its objective. Instead of modeling complex systems, performance measurement is modeled as a function of controller parameters, J(a) : A \u2192 R, and all constraints are modeled as functions of controller parameters, G(a) : A \u2192 R. Both J(a) and G(a) are evaluated on the systems, using cost functions such as Integral Square Error (ISE), Integral Absolute Error (IAE), or Integral Time-weighted Absolute Error (ITAE).\nWe are to solve a sequential decision problem that finds a maximizing J(a) while making all G(a) satisfy the constraints. Safety considerations are included in G(a). With the assumption that an initial safe controller and its performance, (ao, J(ao)), is available, a sequence of parameters a1, a2,..., an \u2208 A are selected, and the noisy performance measurement J(an) + dn is obtained after each selection. During the evaluation, G(an) \u2265 0 must hold with high probability for all G(an), where 0 is chosen without loss of generality. In control applications, it is usually desired to find the"}, {"title": "Additive Gaussian processes-based safe Bayesian optimization", "content": "Bayesian optimization uses the Gaussian processes to approximate unknown objective functions. By defining an appropriate covariance function k(a\u017c, aj), the Gaussian processes can combine past observations to predict the mean and variance of the value of the objective function at unobserved points:\n$\\mu_n (a) = k_n (a)(K_n + I_n \\sigma^2)^{-1}\\hat{J}_n, \\sigma_n(a) = k(a, a) \u2013 k_n (a) (K_n + I_n \\sigma^2)^{-1}k(a)$,\nwhere $\\hat{J}_n = [J(a_1), ..., \\hat{J}(a_n)]^T$ is the vector of noisy performance measurements, the matrix Km has entries $[K_n](i,j) = k(a_i, a_j)$, and the vector $k_n(a) = [k(a, a_1), ..., k(a, a_n)]$. $k(a_i, a_j)$ is also called the kernel of the Gaussian processes.\nThrough the mean and variance of the value of the unknown function at each point, the upper and lower bounds of the confidence interval can be calculated:\n$U_n (a) = \\mu_{n-1}(a) + \\beta_n \\sigma_{n-1}(a), l_n(a) = \\mu_{n-1}(a) \u2013 \\beta_n \\sigma_{n-1}(a)$,\nwhere $\u03b2_n$ is a variable defining the confidence interval. Previous safe Bayesian optimization algorithms, such as SafeOpt (Sui et al., 2015; Berkenkamp et al., 2016), use the upper and lower bounds of the confidence interval to define safe sets $S_n$, which contain all the parameters a that have high probabilities of getting the values of safety functions gi above the safe thresholds hi; and sets of potential maximizers $M_n$, which contain a that could obtain the optimum of the performance function j; and sets of potential expanders $E_n$, which contain a that could be recognized as safe after a new iteration. We relax the Lipschitz constants L in the expressions of $S_n$, $M_n$, and $E_n$ for ease of implementation, and show them in Algorithm 1. By limiting the points selected for evaluation in each iteration to $S_n$, previous safe Bayesian optimization algorithms ensure that the iteration process has a high probability of not violating safety constraints. The selection at each iteration follows different acquisition functions, such as the GP-UCB method (Srinivas et al., 2010), or the modified UCB method (hereinafter referred to as \"UCB-LCB\") proposed by Berkenkamp et al. (2016):\n$a_n = \\underset{a=\\arg\\max_{a\\in E_n\\cup M_nW_n(a)}, w_n(a) = U_n (a) \u2013 l_n (a)$."}, {"title": "Additive Gaussian processes", "content": "Despite various improvements for high-dimensional problems, such as the SwarmSafeOpt algorithm used in Berkenkamp et al. (2016) and the LINEBO algorithm (Kirschner et al., 2019), the squared-exponential (Gaussian) kernels used in these work have limited information acquisition ability in the parameter space. Therefore, we built upon the idea from additive Gaussian processes (Duvenaud et al., 2011), implementing high-dimensional additive structures to the original Gaussian kernels, to obtain a higher information acquisition efficiency."}, {"title": "Theoretical results", "content": "The convergence of previous safe BO algorithms (Sui et al., 2015, 2018) are guaranteed based on two assumptions: by choosing some common Gaussian kernels, (1) the performance function f and safety functions gi have bounded norms in their Reproducing Kernel Hilbert Spaces (RKHS) associated with the GPs, and (2) the safety functions are Lipschitz-continuous. We will prove that the additive Gaussian kernel composed of the one-dimensional Gaussian kernels that satisfy the two assumptions can also make the objective function satisfy the two assumptions. Therefore, the convergence of the proposed method will naturally conform to previous safe BO algorithms."}, {"title": "Experiments", "content": "The efficacy of the proposed algorithm (hereafter referred to as \"our method\") is validated on a PMSM. The architecture of the FOC scheme is depicted in Figure 2, which comprises a cascade control loop. The external controller is a speed controller responsible for regulating the motor's rotational speed. The internal controllers consist of two current controllers that manage the current output from the inverter. These three controllers are interdependent, so the simultaneous adjustment of the six parameters across all controllers is essential to obtain the optimal parameter combination."}, {"title": "Simulations in Simulink", "content": "In this section, the simulation employs FOC for a PMSM, modeled in Simulink using Simscape Electrical components2. The objective is to determine the controller parameters that optimize the speed tracking performance of the PMSM, aiming to maximize transient response speed while minimizing overshoot and steady-state error. This objective is crucial for various industrial applications, including precise robot joint control, industrial automation system control, and electric vehicle control, among others. The transient response of the system is evaluated using the 5% settling time, defined as the duration required for the response curve to reach and remain within 5% of the steady-state value. The performance function is then designed as:\n$J(t_s, O_s, e_{ss}) = w_s \\cdot (t_o-t_s) - w_o \\cdot O_s - w_e e_{ss}$,"}, {"title": "Experiments with Speedgoat real-time machine", "content": "In this section, real-time experiments are conducted using the Speedgoat machine, shown in Figure 5. The configuration includes a controller with integrated speed and current loops, an inverter, and a PMSM. The control algorithm within the controller is adjustable via MATLAB. The transient response of the system is assessed using 5% settling time, and the performance function and two safety functions are designed as described in section 4.1.\nWe use the default parameters in Speedgoat to build the initial controller, and evaluations confirm that both safety functions meet the minimum thresholds. The initial speed tracking result is represented by the green curve in Figure 6a. The proposed Algorithm 1 is employed to optimize the controller parameters. The configuration of the six base kernels is consistent with those detailed in section 4.1, and the exploration phase is designed to last for 15 iterations. After the 35th iteration, our method obtains the optimal controller parameter combination, as depicted by the red curve in Figure 6a. Figure 6b illustrates the changes in the performance function, which stabilizes around the final values post-35 iterations. Figures 6c and 6d display the changes in the safety function values. Observations indicate that the values of the safety functions for the parameter combinations assessed by our method are almost all above the minimum threshold, suggesting that the optimization process adheres to safety constraints.\nIt is also pertinent to note that the result from the 40th iteration (illustrated by the purple curve in Figure 6a) demonstrates a higher performance value than that of the 35th iteration, featuring a faster transient response with negligible overshoot and steady-state error. This is an interesting and non-intuitive result, as control engineers typically tune the system performance to resemble the red curve without the vibration in the purple curve. In certain applications, such as industrial high-throughput semiconductor packaging systems, slight vibrations are acceptable as long as the"}, {"title": "Conclusions", "content": "In this study, we propose to replace traditional Gaussian kernels or Mat\u00e9rn kernels with high-dimensional additive Gaussian kernels, enabling the application of safe Bayesian optimization to high-dimensional complex control systems. The additive Gaussian kernels are more efficient in exploring high-dimensional space information, accomplishing the exploration of the safe set in fewer iterations. We verified the effectiveness of the proposed method for PMSM control in both simulation and real-time experiments. The results indicate that our proposed method surpasses existing safe Bayesian optimization algorithms in high-dimensional control system optimization and can be seamlessly integrated into real-world industrial control applications. Although tested only for PMSM control, the proposed algorithm is potentially applicable to other types of control architecture, as well as to other robotic and mechatronic systems, since it is designed for a general dynamical system.\nHowever, the limitation of the work is that the calculations of high-dimensional additive Gaussian kernels become more complex when the dimensions of the control problems get higher. The calculation of Eq. 10 usually takes a long time and even causes the program to crash when the dimension is higher than 10. A possible solution is to use kernel selection methods (Cristianini et al., 2001; Kandola et al., 2002; Ding et al., 2020) to obtain one or more additive Gaussian kernels with the highest efficiency in exploring the parameter space, and use the selected kernels for subsequent optimization."}, {"title": "Proof of Lemma 1", "content": "Proof. For one-dimensional inputs xi and yi, the Gaussian kernel is defined as:\n$K_i(x_i, y_i) = \\exp \\bigg(-\\frac{||x_i - y_i||^2}{2\\sigma^2}\\bigg)$.\nEach one-dimensional Gaussian kernel Ki has a corresponding RKHS, denoted by Hi, which satisfies the reproducing property.\nSuppose there are d one-dimensional Gaussian kernels, then the additive Gaussian kernel is constructed as:\n$K(x, y) = \\sum_{i=1}^{d}K_i (x_i, y_i)$,\nwhere x = (x1,x2,...,xd) and y = (y1, y2, ..., yd).\nPositive definiteness of the additive Gaussian kernel: We first prove that K(x, y) is a positive definite kernel. For any sample points {x1, x2,..., xn} and corresponding non-zero weight vector \u03b1 = (\u03b11, \u03b12,..., \u03b1n), \u03b1 \u2208 Rn, there is:\n$\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\alpha_j\\alpha_kK(x_j, x_k) = \\sum_{j=1}^{n}\\sum_{k=1}^{n}\\alpha_j\\alpha_k \\sum_{i=1}^{d} K_i ((x_j)_i, (x_k)_i)$.\nSince each Ki is positive definite,\n$\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\alpha_j\\alpha_k K_i ((x_j)_i, (x_k)_i) \\geq 0$,\nthus:\n$\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\alpha_j\\alpha_kK(x_j, x_k) = \\sum_{i=1}^{d}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\alpha_j\\alpha_k K_i ((x_j)_i, (x_k)_i) \\geq 0$,\nwhich shows that K (x, y) is a positive definite kernel.\nConstruction of the corresponding RKHS: Now we prove that the RKHS corresponding to the additive Gaussian kernel can be constructed from the RKHSs of the individual one-dimensional Gaussian kernels.\nAssume Hi is the RKHS corresponding to the kernel K\u2081. For any fi \u2208 Hi, there exists a function Ki(\u00b7, xi) that satisfies the reproducing property:\n$f_i(x_i) = \\langle f_i, K_i(\\cdot, x_i)\\rangle_{H_i}$.\nWe construct the new function space H as the direct sum of these Hi:\n$H = \\bigoplus_{i=1}^{d}H_i$,\nand in this new space, any function f \u2208 H can be represented as:\n$f(x) = \\sum_{i=1}^{d} f_i(x_i)$,\nwhere fi \u2208 Hi."}, {"title": "Proof of Theorem 1", "content": "Proof. Assume there are d one-dimensional Gaussian kernels Ki, each corresponding to an RKHS H\u2081. The additive Gaussian kernel K is defined as:\n$K(x,y) = \\sum_{i=1}^{d} K_i(x_i, y_i)$,\nwhere x = (x1, x2,...,xd) and y = (y1, y2, ..., yd).\nIf a function f has bounded norms in each of the RKHSs corresponding to the one-dimensional Gaussian kernels, there are:\n$|| f_i || \\leq B_i$,\nwhere $|| f_i ||$ denotes the norm of f in the RKHS Hi corresponding to each one-dimensional Gaussian kernel.\nThe RKHS H of the additive Gaussian kernel K is the direct sum of the RKHSS H\u2081:\n$H = \\bigoplus_{i=1}^{d}H_i$"}, {"title": "Proof of Theorem 2", "content": "Proof. A Gaussian kernel is defined as:\n$K(x,y) = \\exp\\bigg(-\\frac{||x-y||^2}{2\\sigma^2}\\bigg)$\nFor the Lipschitz continuity of the Gaussian kernel, if we consider any two points x and y in the input space X, we need to prove that there exists a constant L such that:\n$|K(x, z) \u2013 K(y, z)| \\leq L||x - y||$\nfor all z \u2208 X.\nGiven that each one-dimensional Gaussian kernel $K_i(x_i, y_i) = \\exp \\big(\\frac{(x_i-y_i)^2}{2\\sigma^2}\\big)$ is Li-Lipschitz-continuous, then there exists a constant Li such that:\n$|K_i(x_i, z_i) - K_i(y_i, z_i)| \\leq L_i |x_i - y_i|$\nfor all xi, yi, zi \u2208 Xi.\nd\nTo prove that the additive Gaussian kernel $K(x, y) = \\sum_{i=1}^{d} K_i(x_i, y_i)$ satisfies Lipschitz continuity, we need to show that there exists a constant L such that:\n$|K(x, z) \u2013 K(y, z)| \\leq L||x - y||$\nfor all x, y, z \u2208 X.\nConsider the difference of additive Gaussian kernels:\n$|K(x, z) - K(y, z)| = | \\sum_{i=1}^{d}K_i(x_i, z_i) - \\sum_{i=1}^{d}K_i(y_i, z_i)|$\nAccording to the triangle inequality,\n$|\\sum_{i=1}^{d} K_i(x_i, z_i) - \\sum_{i=1}^{d} K_i(y_i, z_i)| = \\sum_{i=1}^{d} |K_i(x_i, z_i) \u2013 K_i(y_i, z_i)|$.\nSince each K\u2081 is Li-Lipschitz-continuous,\n$|K_i(x_i, z_i) - K_i(y_i, z_i)| \\leq L_i |x_i - y_i|$,\nthus:\n$|K(x, z) \u2013 K(y, z)| \\leq \\sum_{i=1}^{d} L_i |x_i - y_i|$.\nLet $||x - y||_1 = \\sum_{i=1}^{d}|x_i - y_i |$ represents the l\u2081 norm of the vector, there is:\n$\\sum_{i=1}^{d} L_i |x_i - y_i| = (\\sum_{i=1}^{d} L_i) ||x - y||_1$.\nNote that there is the following relationship between the l\u2081 norm and the l2 norm:\n$||x - y||_1 \\leq \\sqrt{d} ||x - y||$,\nthus:\n$\\sum_{i=1}^{d} L_i |x_i - y_i| \\leq (\\sum_{i=1}^{d} L_i) \\sqrt{d} ||x - y||$.\nLet $L = (\\sum_{i=1}^{d} L_i) \\sqrt{d}$, then:\n$|K(x, z) \u2013 K(y, z)| \\leq L||x - y||$,\nwhich shows that the additive Gaussian kernel K satisfies L-Lipschitz continuity."}]}