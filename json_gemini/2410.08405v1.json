{"title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning", "authors": ["Muhammad Awais", "Ali Husain Salem Abdulla Alharthi", "Amandeep Kumar", "Hisham Cholakkal", "Rao Muhammad Anwer"], "abstract": "Significant progress has been made in advancing large multimodal conversational models (LMMs), capitalizing on vast repositories of image-text data available online. Despite this progress, these models often encounter substantial domain gaps, hindering their ability to engage in complex conversations across new domains. Recent efforts have aimed to mitigate this issue, albeit relying on domain-specific image-text data to curate instruction-tuning data. However, many domains, such as agriculture, lack such vision-language data. In this work, we propose an approach to construct instruction-tuning data that harnesses vision-only data for the agriculture domain. We utilize diverse agricultural datasets spanning multiple domains, curate class-specific information, and employ large language models (LLMs) to construct an expert-tuning set, resulting in a 70k expert-tuning dataset called AgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient LMM that can hold complex agriculture-related conversations and provide useful insights. We also develop AgroEvals for evaluation and compare AgroGPT's performance with large open and closed-source models. AgroGPT excels at identifying fine-grained agricultural concepts, can act as an agriculture expert, and provides helpful information for multimodal agriculture questions. The code, datasets, and models are available at https://github.com/awaisrauf/agroGPT.", "sections": [{"title": "1. Introduction", "content": "The widespread availability of joint image-text data across the internet has catalyzed significant advancements in large multimodal models (LMMs), such as CLIP [34], GPT-4 [2], Bard [29], and LLaVA [22]. These models, when instruction-tuned, have demonstrated remarkable zero-shot performance and multi-turn conversational capabilities [31]. However, due to the inherent domain gap, their efficacy often falters in domains requiring specialized expertise, such as bio-medicine, climate science, and geo-sensing, resulting in incorrect responses and hallucinations. To address this limitation, several fields have successfully leveraged existing domain-specific vision-language datasets to create domain-specific instruction-tuning data, enhancing model performance in specialized areas [18,27,41]. For instance, BioMedLLaVA utilized large vision-language data PMC-15M [47] to curate biomedical instruction-tuning data resulting in LLaVA-Med [20].\nDespite these advancements, many domains, including agriculture, possess vision-only datasets but lack high-quality joint image-text data suitable for creating domain-specific instruction-tuning data. To address this challenge, we introduce AgroGPT, the first approach to extending multimodal instruction tuning to the agricultural domain without requiring joint image-text data. Our method consists of three main components: a pipeline to generate expert-tuning data, an efficient multimodal model trained on AgroInstruct to bridge the domain gap, and an evaluation framework for domain-specific evaluation of the expert model. First, we propose a pipeline for synthesizing expert-level instruction-tuning data from vision-only datasets, effectively bridging the multimodal data gap in agriculture. This resulted in AgroInstruct, a 70k expert-tuning dataset for agriculture. Second, AgroGPT, we present an efficient, open-source expert agricultural conversational model, trained on the AgroInstruct to bridge the domain gap. Our proposed model is capable of holding complex conversations about the input agricultural images and can provide useful insights. Finally, we develop AgroEvals, a simple visual question-answering framework to evaluate model performance in agricultural contexts.\nOur main contribution is AgroInstruct, a pipeline for curating expert-level instruction-tuning data solely from vision-only datasets specifically designed to enhance LMMs in the agricultural domain. To this end, we aggregate six diverse image-only datasets spanning four agricultural domains including plant diseases, weeds, farm insects, and fruits. Next, to transform the vision-only data into rich expert-tuning examples, we synthesize context-grounded image descriptions using state-of-the-art general-purpose LMMs, extract relevant image attributes, and collect class-specific background knowledge from agricultural sources. Finally, we develop domain-specific instruction-following examples and system prompts tailored to agricultural contexts. These elements are fed into language-only LLMs to generate diverse and contextually relevant conversations. Further, we implement a rule-based system leveraging dataset attributes to generate concise question-answer pairs, enhancing the variety and specificity of the instruction-tuning data. Through this pipeline, we bootstrapped AgroInstruct, an extensive and diverse multimodal instruction-tuning dataset comprising 70k high-quality conversations."}, {"title": "2. Related Work", "content": "General Large Language Models (LLMs) and Large Multimodal Models (LMMs). Large Language Models (LLMs) possess remarkable capabilities to understand and generate text that resembles human language. This enables them to tackle various tasks effectively, with GPT-4 [32], Llama-2 [43], and Gemini [39]. Recently, these LLMs are extended to multimodal inputs [4] by aligning visual information in the LLM embedding space for which a pre-trained visual backbone [10] is used to process visual data, a large language model [8] for understanding user instructions and generating responses, and a cross-modal connection MLP [21] to integrate visual information with language models. The impressive improvements in tasks related to language instruction following and visual reasoning in nat-"}, {"title": "3. Agricultural Expert-tuning", "content": "In the following sections, we first explain our efficient pipeline for curating expert tuning data to instill agricultural knowledge in Large Multimodal Models (LMMs). We utilize classification-only image datasets and class-specific information to synthesize the data. Next, we describe the architecture of our LMMs, followed by the training process using the expert tuning dataset. This tuning is performed after the vision-language alignment and general visual instruction tuning stages [22]."}, {"title": "3.1. Curation of AgroInstruct for Expert Tuning", "content": "The agricultural domain has several vision-only datasets (as depicted in Figure 2), but it lacks multimodal data, presenting a challenge for developing instruction-tuning datasets. To address this issue, we propose a solution that leverages vision-only datasets from four different domains (diseases, weeds, insects, fruits) as shown in Tab. 1 to synthesize expert tuning data called AgroInstruct following visual instruction tuning [22] and LLaVA-med [20].\nAgroInstruct consists of three types of questions: complex multi-turn conversations, simple questions, and image descriptions. First, we generate an image description with generic LMMs by providing it with the context of the dataset and image-specific attributes. Next, to generate multi-turn complex conversations, we utilize an open-source LLM to generate expert-tuning data by providing it with context-conditioned descriptions, class-specific external background information from reputable agricultural resources, and in-context examples of conversations as shown in Figure 2. This is provided as input into a Large Language Model (LLM) alongside appropriate system prompts. Finally, we utilize attributes of image-only datasets to generate simple questions, such as identifying plants and diseases. Our expert-tuning data, AgroInstruct, consists of 70k conversations belonging to three types (10K descriptions, 35 complex questions, and 35k simple questions). An example of these three types of questions is provided in Figure 5.\nIn the following, we elaborate on each step.\nContext-grounded Image Descriptions. First, we generate context-grounded image descriptions by prompting a vision-language model to describe the input based on dataset-specific attributes such as plant name, disease name, and whether it's a plant or a leaf. We construct a system of prompts to instruct models to utilize context and generate specific and field-specific information. We devised 10 questions to describe the image following MedLLaVA [20]. This process resulted in the construction of approximately 35K descriptions spanning four domains. We only utilize 10k descriptions in our AgroInstruct data, while the rest are utilized as context for the generation of complex conversations explained next.\nComplex, Multi-turn, and Open-ended Conversations. We need a dataset containing multi-turn conversations to equip our model with the capability to answer complex, open-ended, and agriculture-related questions about an image. To this end, we craft a diverse set of conversations featuring multi-round conversations using a language-only Large Language Model (LLM) named Mistral [16]. To guide LLM in generating field-specific and to-the-point conversations, we utilize the following four as context for each input image.\n\u2022 Image Attributes. Image-only datasets (Table 1) have attributes for each image, such as plant name, disease name, fruit name, etc. These attributes are added in the context of LLM to facilitate its understanding of the nature of the input."}, {"title": "3.2. Architecture of AgroGPT", "content": "For our baseline model, we utilized two types of models from the LLaVA family, comprising a visual encoder, a projection layer, and an LLM. For our AgroGPT, we employed two different models: a 7B LLaVA model [22] and an efficient 3B Mipha model [50]. These models are chosen to suit different computational requirements and performance considerations for downstream applications. The LLaVA and Mipha utilize pre-trained CLIP ViT-L/14 [34] and SigLIP [46] as visual encoders. In our experiments, we found SigLIP better than the traditional CLIP model, even when a smaller LLM is used. In conjunction with the projection layer, this visual encoder transforms an image into the language embedding space, producing visual tokens. These image tokens are combined with language instruction tokens and fed into the LLM. Subsequently, the LLM generates text conditioned on the language instructions and the image."}, {"title": "3.3. Expert Tuning AgroGPT with AgroInstruct", "content": "Our main goal is to instruction-tune a vision-language chat model that is expert in the agriculture field. To achieve this, we use the first two stages of visual instruction tuning [22] and introduce an expert tuning stage, as shown in Figure 4. Specifically, the first stage consists of visual-language concept alignment on general images, and the second stage consists of instruction tuning on general vision-language data. We introduce a third expert tuning stage to instill agriculture-related expertise in the model. We kept the visual encoder frozen for expert tuning and trained the projection layer and LLM weights on our AgroInstruct data. For training, we converted all three formats of instructions (descriptions, small questions, multi-turn, and complex questions) into the same conversation format. Then, we randomly sampled conversations, added system messages as shown in Appendix Figure 3, and trained LLM on its original auto-regressive objective. In the ablations, we also compare the effect of expert tuning after stage 1 and stage 2. Generally, expert tuning after stage 2 results in better results."}, {"title": "4. Experimental Results", "content": "Model Training. We use Phi-2-2.7B [15] and Vicuna-7B [48] as our LLMs and CLIP visual encoder ViT-L/14 [34] and SigLIP [46] with resolution 336 as our image encoders. We used Mipha [50] and LLaVA1.5 [22] provided pre-trained weights, which have been trained with visual instruction tuning. Unless stated otherwise, we applied our expert tuning after image-text alignment (stage 1) and visual instruction tuning (stage 2). We use AdamW [23] to optimize along with the cosine learning rate scheduler to train our model. Unless otherwise stated, we train our model for 1 epoch with a batch size of 8 or 16, depending on computational resources. For our training, we utilize 4 A6000 GPUs.\nExpert Tuning Data. To generate our expert tuning, we used two different models. First, we use Mistral7B [16] to generate instruction tuning, following the method proposed in the previous section. Second, we use LLaVA-13B to create descriptions used as in-context information for Mistral7B, which is also explained in the previous section."}, {"title": "4.1. Qualitative Results", "content": "To demonstrate the effectiveness of AgroGPT in identifying agricultural issues (diseases, insects, weeds) and guiding users, we hold conversations on several examples. First, we compared AgroGPT's performance with LLaVA-34B and Bard for crop diseases, farm insects, and weeds on held-out images. The results are shown in Tables 3, 4. These conversations highlight AgroGPT's capability to identify fine-grained attributes (disease, insect, and weed names) and answer field-specific questions. For instance, AgroGPT accurately identifies diseases affecting the leaf, weed name, and farm insect in the images, provides suggestions for managing these issues, and explains its effect on the crop. In contrast, both Bard and LLaVA-34B either fail to identify fine-grained concepts or make incorrect identifications.\nTo evaluate the generalizability of AgroGPT, we further collected several images from online agriculture forums where users have asked questions. We compare AgroGPT's capabilities to answer questions with LLaVA-34B, Bard, and ChatGPT40. The conversations are presented in Table 2, and more results are relegated to the Appendix. AgroGPT can identify diseases correctly, assess potential damages, and suggest possible remedies. In contrast, both Bard and ChatGPT4o are unable to answer it, and LLaVA-34B hallucinates. For more details on quantitative evaluations, please refer to the Appendix."}, {"title": "4.2. Quantiative Evaluations", "content": "For quantitative evaluations, we create AgroEvals by utilizing the test and a holdout set of datasets utilized in the generation of AgroInstruct (see Table 1). Based on these datasets, we create six visual question-answering sets consisting of three groups that test the model's capabilities at different levels. Group 1 consists of questions to ask for the presence of a disease or insect in an image. Since these two attributes are general and coarse, a model trained on general instruction tuning (e.g., visual tuning) can easily answer these questions without expert tuning. Group 2 consists of questions to detect general categories such as fruits or plant names. These two attributes are finer-grained than group 1 but present in general instruction-tuning data. However, Group 3 consists of questions that probe the model for fine-grained and field-specific questions, such as the name of the disease or insect. These concepts are less likely to be present in the general instruction-tuning data and hence general models struggles to answer these questions. Further, for our quantitative evaluations, we use two versions of our models: AgroGPT7B, which is based on LLaVA1.5 [22] and AgroGPT3B, which is based on Mipha3B [50].\nMain Results. In Figure 6, we compare our method's ability to add field-specific knowledge with the baseline of visual instruction tuning across six types of questions from three groups. Models fine-tuned with our method perform on par with visual instruction tuning for Group 1 questions, which tests the model's coarse recognition abilities: presence and absence of disease and insects in a given image. However, our method performs significantly better for Group 2, which tests the model's more fine-grained recognition abilities (e.g., name of fruit or plant name.\nHowever, baseline models (fine-tuned with visual instruction tuning) completely fail on fine-grained questions from Group 3, which tests more fine-grained and relevant concepts such as identifying diseases or insects. On the other hand, our proposed expert tuning can instill these fine-grained concepts in the model and significantly improve their performance. For instance, both 3B and 7B baseline Mipha and LLaVA models completely fail to identify insects, while our method significantly improves this performance. Interestingly, AgroGPT3B performs better on fine-grained questions compared with AgroGPT7B. We hypothesize that this is because of the Mipha [50] baseline using a more capable image encoder (SigLIP [46]) compared with LLaVA's CLIP.\nComparison with ChatGPT. We also quantitatively compare AgroGPT with ChatGPT for benchmark from Group 3 of AgroEvals. We chose Group 3 as it represents fine-grained agriculture knowledge (plant disease and farm insect identification). As shown in Table 6, AgroGPT outperforms ChatGPT by significant margins, such as on diseases, our model has a performance of 51.37% compared with ChatGPT's 30.82%. It is also important to note that the efficient baseline model (Mipha3B) shows very poor performance as it lacks expert knowledge about fine-grained agricultural concepts.\nPerformance on Unseen Datasets. To evaluate our model's generalization to unseen agricultural concepts, we select three datasets: the Sugarcane Leaf Disease dataset [37], the Potato Leaf Disease dataset [33], and the Plant Disease Recognition dataset [35]. We then assessed AgroGPT and our baseline using the AgroEvals protocol described previously. As illustrated in Figure 7(b), AgroGPT demonstrates significantly higher performance, indicating its ability to generalize beyond the datasets used in expert tuning. However, it is crucial to note that AgroGPT may not perform as well on concepts that differ considerably, such as livestock, which are not part of AgroInstruct.\nExpert Evaluations. AgroGPT is an expert model that can answer complex agriculture-related questions in detail, making it difficult to assess its performance solely based on quantitative measures. Hence, to further understand AgroGPT's performance, we perform expert evaluations by designing an interface, generating detail-oriented questions from Group 3 (plant diseases and insects that require more fine-grained knowledge of the field), and anonymously presenting answers of baseline and AgroGPT to the expert evaluators. For a thorough description, please refer to the Appendix. Results are shown in Table 7(a), which clearly illustrate that AgroGPT's answers are overwhelmingly pre-"}, {"title": "5. Conclusion", "content": "In this work, we introduce AgroGPT, an efficient large vision-language model tailored for the agricultural domain capable of conducting multi-turn, multimodal, open-ended conversations. To achieve this, we present a pipeline for generating expert-tuning data from vision-only datasets, following a self-instruct approach augmented with class-specific external knowledge. Using this pipeline, we constructed the AgroInstruct dataset and trained the efficient conversational model called AgroGPT. We also developed AgroEvals to evaluate the domain-specific capabilities of our model. AgroGPT excels at identifying fine-grained, domain-specific concepts and engaging in detailed conversations on agriculture-specific questions."}, {"title": "A. Extended Experimental Details", "content": "A.1. Extended Details of AgroInstruct\nFor the construction of AgroInstrut, we utilized class-based background knowledge fed to LLM to generate context-based question-answer pairs. A few of the external resources we used for background knowledge are as follows. For farm insects, we took information from umass.edu, ufl.edu, and usu.edu. For early weed manage-"}]}