{"title": "SERIES-TO-SERIES DIFFUSION BRIDGE MODEL", "authors": ["Hao Yang", "Zhanbo Feng", "Feng Zhou", "Robert C Qiu", "Zenan Ling"], "abstract": "Diffusion models have risen to prominence in time series forecasting, showcasing their robust capability to model complex data distributions. However, their effectiveness in deterministic predictions is often constrained by instability arising from their inherent stochasticity. In this paper, we revisit time series diffusion models and present a comprehensive framework that encompasses most existing diffusion-based methods. Building on this theoretical foundation, we propose a novel diffusion-based time series forecasting model, the Series-to-Series Diffusion Bridge Model (S2DBM), which leverages the Brownian Bridge process to reduce randomness in reverse estimations and improves accuracy by incorporating informative priors and conditions derived from historical time series data. Experimental results demonstrate that S2DBM delivers superior performance in point-to-point forecasting and competes effectively with other diffusion-based models in probabilistic forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Ho et al., 2020; Song et al., 2020) have emerged as powerful tools for time series forecasting, offering the capability to model complex data distributions. Building on their success in other domains, such as computer vision (Saharia et al., 2022; Rombach et al., 2022) and natural language processing (Reid et al., 2022; Ye et al., 2023), researchers have increasingly applied diffusion models to time series prediction. This approach has shown promise in capturing the intricate temporal dependencies and uncertainty in time series data, leading to significant advancements in forecasting accuracy and reliability (Rasul et al., 2021; Tashiro et al., 2021; Alcaraz & Strodthoff, 2022; Li et al., 2024).\nHowever, the inherent stochasticity of diffusion models makes multivariate time series forecasting challenging. Specifically, most of these methods employ a standard forward diffusion process that gradually corrupts future time series data until it converges to a standard normal distribution. Consequently, their predictions originate from pure noise, lacking temporal structure, with historical time series data merely conditioning the reverse diffusion process and offering limited improvement. This approach often results in forecasting instability and the generation of low-fidelity samples (as shown in Figure 1). While diffusion-based methods perform adequately in probabilistic forecasting, their point-to-point prediction accuracy lags behind that of deterministic models, e.g., Autoformer (Wu et al., 2021), PatchTST (Nie et al., 2022), and DLinear (Zeng et al., 2023).\nTo improve the deterministic estimation performance of diffusion models on time series, we first revisit and consolidate existing non-autoregressive diffusion-based time series forecasting models under a unified framework, demonstrating that these models are fundamentally equivalent, differing primarily in their choice of parameters and network architecture. Based on this framework, we propose a novel diffusion-based time series forecasting model, Series-to-Series Diffusion Bridge Model (S2DBM). S2DBM employs the diffusion bridge as its foundational architecture, which proves effective for multivariate time series forecasting. Specifically, S2DBM uses the Brownian Bridge to pin down the diffusion process at both ends, reducing the instability caused by noisy input and enabling the accurate generation of future time step features from historical time series data. By"}, {"title": "2 RELATED WORKS", "content": "Diffusion-based Time Series Forecasting. Recently, a range of diffusion-based methods are proposed for time series forecasting. These methods generally adhere to the framework of the standard diffusion model, with their primary distinctions stemming from variations in the denoising network and conditional mechanisms.\nTimeGrad (Rasul et al., 2021) is the pioneer of these diffusion-based methods, integrating diffusion models with an RNN-based encoder to handle historical time series. However, its reliance on autoregressive decoding can lead to error accumulation and slow inference times. To tackle this problem, CSDI (Tashiro et al., 2021) employs an entire time series as the target for diffusion and combines it with a binary mask (which denotes missing values) as conditional inputs into two transformers. This masking-based conditional mechanism enables CSDI to generate future time series data in a non-autoregressive fashion. SSSD (Alcaraz & Strodthoff, 2022) uses the same conditional mechanism as CSDI, but replaces the transformers in CSDI with a Structured State Space Model (S4) to reduce the computational complexity and is more suited to handling long-term dependencies. TMDM (Li et al., 2024) integrates transformers with a conditional diffusion process to improve probabilistic"}, {"title": "3 METHODOLOGY", "content": "Most diffusion-based methods for time series forecasting are designed around conditional Denoising Diffusion Probabilistic Models (DDPMs). The forward process, defined by a fixed Markov chain, progressively transforms the future time series $y \\in R^{L\\times d}$ into a Gaussian noise vector $y_T$ according to a predetermined variance schedule {$\\beta_t$}$_{t=1}^T$:\n$q (y_t | y_{t-1}) = N (y_t; \\sqrt{1 - \\beta_t}y_{t-1}, \\beta_tI)$,\nwhere $L$ denotes the length of the forecast window, and $d$ represents the number of distinct features.\nWith the notation $\\alpha_s = 1 - \\beta_s$ and $\\bar{\\alpha}_t := \\Pi_{s=1}^t \\alpha_s$, the forward process can be rewritten as:\n$y_t = \\sqrt{\\bar{\\alpha}_t}y_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim N (0, I)$.\nDuring inference, the model reverses the forward process by considering the following distribution:\n$p_\\theta (y_{0:T} | x) = p_\\theta (y_T) \\Pi_{t=1}^T p_\\theta (y_{t-1} | y_t, x)$,\nwhere $y_T$ is initially sampled from a standard normal distribution $N(0, I)$, the subscripts from 0 to T denote the diffusion steps. $x \\in R^{H \\times d}$ is the historical data, $H$ represents the length of the lookback window."}, {"title": "3.2 REVISITING GENERALIZED DIFFUSION MODEL FOR TIME SERIES", "content": "Most existing diffusion-based time series forecasting methods emphasize their probabilistic forecasting capabilities; however, their performance in point-to-point forecasting remains suboptimal. To develop a specialized diffusion-based model tailored for point-to-point time series forecasting, a deeper understanding of existing approaches is crucial. Therefore, we revisit and consolidate current non-autoregressive diffusion-based time series forecasting models into a unified framework, demonstrating their fundamental equivalence. The primary differences among these models lie in their choice of diffusion-related coefficients and the design of network architectures.\nRecognizing components in existing models, diffusion processes can be viewed in a flexible and adaptable manner. As shown in Eq. (1), the diffusion process incorporates historical data and endows the designed models with distinct properties by adjusting the coefficients $\\hat{\\alpha}_t$, $\\beta_t$, $\\gamma_t$, and $\\delta_t$.\nTheorem 1. The non-autoregressive diffusion processes in time series can be formalized as follows:\n$y_t = \\hat{\\alpha}_t y_0 + \\beta_t\\epsilon + \\hat{\\gamma}_t h, \\epsilon \\sim N(0, I)$.\nThe reverse diffusion process corresponding to $\\beta_t \\neq 0$ can be formulated as:\n$p_\\theta(y_{0:T} | x) := p_\\theta(y_T)\\Pi_{t=1}^T p_\\theta(y_{t-1} | y_t, x)$,\n$p_\\theta(y_{t-1} | y_t, x) := N(y_{t-1}; \\mu_\\theta (y_t, h, c, t), I)$,\nwhere $\\alpha_t$, $\\beta_t$, and $\\gamma_t$ are time-dependent scaling factors. The vector $h = F(x)$ acts as the conditional representation incorporating prior knowledge, with $F(\\cdot)$ serving as the prior predictor that maps historical time series into a latent space. The initial distribution is given by $p_\\theta(y_T) = N(\\tau h, \\beta I)$. The conditioning variable $c = E(x)$ guides the reverse process, where $E(\\cdot)$ denotes the conditioning module. The function $\\mu_\\theta$ predicts the mean of $y_{t-1}$ given inputs $y_t$, h, and c, while $\\sigma^2$ represents the reverse variance schedule.\nMost existing diffusion-based time series forecasting models, including CSDI (Tashiro et al., 2021), SSSD (Alcaraz & Strodthoff, 2022), TimeDiff (Shen & Kwok, 2023), and TMDM (Li et al., 2024), can be interpreted within our proposed framework, as summarized in Table 1. The key differences"}, {"title": "3.3 SERIES-TO-SERIES DIFFUSION BRIDGE MODEL", "content": "As shown in Table 1, existing diffusion-based time series forecasting methods have been extensively studied using various diffusion paradigms and conditional approaches in the formulation of Theorem 1 and achieve promising predictive ability. However, most of these methods focus on the uncertainty estimation ability and typically rely on a data-to-noise diffusion process due to current conditioning mechanisms. As a result, they are often constrained by the intrinsic stochastic nature and are limited in capturing the inherent complexity and dynamic nature of real-world time series data, leading to suboptimal performance in point-to-point forecasting. To address this gap, we propose the Series-to-Series Diffusion Bridge Model (S2DBM), which uses the Brownian Bridge to pin down the diffusion process at both ends, reducing the instability caused by noisy input and enabling the accurate generation of future time step features from historical time series. By adjusting the posterior variance in Theorem 1, S2DBM behaves as a deterministic generative model without any Gaussian noise, thereby ensuring stability and precise point-to-point forecasting results.\nAs shown in Figure 2, S2DBM employs the diffusion bridge as the foundational architecture by adjusting the coefficient schedules. The diffusion bridge pins down the diffusion process at both ends, enabling the accurate generation of future time step features from historical time series data through a data-to-data process.\nCorollary 1 (Brownian Bridge between Historical and Predicted Time Series). Let the coefficient $\\hat{\\alpha}_t$, constrained to be non-negative and decrease monotonically over time t, satisfy the boundary conditions $\\hat{\\alpha}_0 = 0$ and $\\hat{\\alpha}_T = 1$. Additionally, define $\\hat{\\hat{\\alpha}}_t = 1 - \\hat{\\alpha}_t$ and $\\beta_t = \\sqrt{2\\hat{\\alpha}_t(1 - \\hat{\\alpha}_t)}$ The forward process defined in Eq. (1) can be rewritten in closed form:\n$q(y_t | y_0, h) = N(y_t; \\hat{\\alpha}_t y_0 + (1 - \\hat{\\alpha}_t)h, 2\\hat{\\alpha}_t (1 - \\hat{\\alpha}_t)I)$.\nThen, the reverse process transition defined in Eq. (3) turns into:\n$p_\\theta(y_{t-1} | y_t, x) = N(y_t; K_t y_t + A_t y_0(y_t, h, c, t) + \\zeta_t h, \\hat{\\sigma}I)$,"}, {"title": "4 EXPERIMENTS", "content": "In this experiment, the time series forecasting benchmark datasets employed encompass several real-world datasets: Weather, Influenza-like Illness (ILI), Exchange-Rate (Lai et al., 2018), and four Electricity Transformer Temperature datasets (Zhou et al., 2022) (ETTh1, ETTh2, ETTm1, ETTm2). These datasets are extensively utilized for testing multivariate time-series forecasting models due to their diverse and representative nature, offering insights into the model's performance across different domains and conditions. Each dataset is normalized using the mean and standard deviation of the training part.\nWe compared our method with several state-of-the-art and representative baseline models. These include Transformer-based methods: Autoformer (Wu et al., 2021), Informer (Zhou et al., 2022), and iTransformer (Liu et al., 2023b); linear models: DLinear, NLinear (Zeng et al., 2023), and RLinear (Li et al., 2023b); as well as diffusion-based time series prediction methods: CSDI (Tashiro et al., 2021), TMDM (Li et al., 2024), and TimeDiff (Shen & Kwok, 2023).\nTo assess point-to-point forecasting performance, we employ mean squared error (MSE) and mean absolute error (MAE) as primary metrics to quantify discrepancies between forecasted and actual time series values. For evaluating the quality of probabilistic forecasts, we use the continuous ranked probability score (CRPS) (Matheson & Winkler, 1976) across individual time series dimensions and CRPSsum for the aggregate of all dimensions.\nWe trained our model using the ADAM optimizer, setting the initial learning rate at 0.0001 and parameters \u03b2\u2081 = 0.9 and B2 = 0.999. We configured the number of time steps for the S2DBM to be T=50 during the training and inference stages. The computational environment comprised a server with an NVIDIA GeForce RTX 3090 24GB GPU."}, {"title": "4.2 MAIN RESULTS", "content": "Table 2 provides a detailed summary of the point-to-point time series forecasting results for Example 1 of our S2DBM model, compared to other models. For"}, {"title": "4.3 ABLATION STUDIES", "content": "To validate each component of our proposed S2DBM model, we performed a comparative analysis of prediction results using five different models on the ETTh1 and ETTm1 datasets. The results are presented in Table 5. The notation CDDPM indicates that it employs the standard diffusion process instead of the Brownian bridge process used in S2DBM. The notation w/ CSDI E refers to an operation that utilizes the conditioning mechanism of CSDI. Similarly, w/ CSDI \u00b5o indicates the adoption of the denoising network architecture from CSDI."}, {"title": "5 CONCLUSION", "content": "In this paper, we revisit non-autoregressive time series diffusion models and present a comprehensive framework that integrates most existing diffusion-based methods. Building on this theoretical framework, we propose the Series-to-Series Diffusion Bridge Model (S2DBM). Our S2DBM utilizes the Brownian Bridge diffusion process to reduce randomness in diffusion estimations, improving forecast accuracy by effectively leveraging historical information through informative priors and conditions. Extensive experimental results demonstrate that S2DBM achieves superior performance in point-to-point forecasting and performs competitively against other diffusion-based models in probabilistic forecasting."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 PROOFS OF THEOREM 1", "content": "The non-autoregressive diffusion processes in time series can be formalized as follows:\n$y_t = \\hat{\\alpha}_t y_0 + \\beta_t\\epsilon_t + \\gamma_t h, \\epsilon_t \\sim N(0, I)$.\nHere, $\\hat{\\alpha}_t$, $\\beta_t$, and $\\gamma_t$ are time-dependent scaling factors, and h = F(x) serves as the conditional representation acting as prior knowledge.\nSimilarly, the previous state yt-1 can be expressed as:\n$y_{t-1} = \\hat{\\alpha}_{t-1}y_0 + \\beta_{t-1}\\epsilon_{t-1} + \\gamma_{t-1} h, \\epsilon_{t-1} \\sim N(0, I)$.\nWe are interested in the posterior distribution q (yt\u22121 | yt, yo, h). According to the properties of Gaussian distributions, this posterior is also Gaussian and can be written as:\nq (yt\u22121 | yt, yo, h) = N (yt\u22121; K_t y_t + A_t y_0 + \\zeta_t h, \\hat{\\sigma}^2 I),\nwhere kt, At, and \u0120t are coefficients to be determined, and \u00f4f is the variance.\nBy substituting Eq. (8) into the expression for yt-1, we obtain:\nyt-1 = K_t y_t + A_t y_0 + \\zeta_t h + \\hat{\\sigma} \\epsilon'\n= k_t(\\hat{\\alpha}_t y_0 + \\beta_t\\epsilon_t + \\gamma_t h) + A_t y_0 + \\zeta_t h + \\hat{\\sigma} \\epsilon'\n= (k_t\\hat{\\alpha}_t + A_t)y_0 + (k_t\\gamma_t + \\zeta_t)h + (k_t\\beta_t\\epsilon_t + \\hat{\\sigma} \\epsilon'),\nwhere $\\epsilon' \\sim N(0, I)$ is independent of $\\epsilon_t$.\nSince the sum of two independent Gaussian noises is another Gaussian noise, we have:\nk_t\\beta_t\\epsilon_t + \\hat{\\sigma} \\epsilon' = \\sqrt{k_t^2\\beta_t^2 + \\hat{\\sigma}^2} \\epsilon_{t-1},\nwhere $\\epsilon_{t-1} \\sim N(0, I)$.\nComparing this with Eq. (9), we can equate the coefficients:\n$\\hat{\\alpha}_{t-1} = k_t\\hat{\\alpha}_t + A_t$, \\gamma_{t-1} = k_t\\gamma_t + \\zeta_t$, $\\beta_{t-1} = \\sqrt{k_t^2\\beta_t^2 + \\hat{\\sigma}^2}$.\nSolving for Kt, At, and St, we get:\n$K_t$\nAt = $$\\hat{\\alpha}_{t-1} - \\hat{\\alpha}_t K_t$$=\n$\\hat{\\alpha}_{t-1} - A_t K_t$\n$\\hat{\\sigma}^2$ =\n$= \\gamma_{t-1} - k_t \\gamma_t$\n$\\zeta_t = \\gamma_{t-1} - k_t \\gamma_t$\nSince h is completely determined by x, the posterior distribution becomes:\nq (yt\u22121 | yt, yo, x) = N (yt\u22121; K_t y_t + A_t y_0 + \\zeta_t h, \\hat{\\sigma}^2 I).\nHowever, this posterior depends on the unknown data distribution q(yo), making it impractical for direct use. Therefore, we introduce a learnable transition probability po(Yt\u22121 | Yt, x) to approximate q (Yt\u22121 | Yt, Yo, x) for all t. The reverse process is defined as:\nP(Yo:T | X) := P(\u0443) [T Po(Yt-1 | Yt, x),\nPo(Yt-1 | Yt, x) := N(yt-1; \u03bc\u03b8 (yt, h, c, t), I)"}, {"title": "A.3.2 IMPLEMENTATION DETAILS", "content": "As mentioned in Section 3.3, the denoising network of S2DBM adopts the same architecture as CSDI Tashiro et al. (2021) but removes modules related to its original conditioning mechanism.Both the conditional encoder network E and the prior predictor F(\u00b7) in S2DBM employ a simple one-layer linear model (Zeng et al., 2023). Table 7 contains the hyperparameters that for S2DBM training and architecture."}]}