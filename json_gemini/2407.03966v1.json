{"title": "Serialized Output Training by Learned Dominance", "authors": ["Ying Shi", "Lantian Li", "Shi Yin", "Dong Wang", "Jiqing Han"], "abstract": "Serialized Output Training (SOT) has showcased state-of-the-art performance in multi-talker speech recognition by sequentially decoding the speech of individual speakers. To address the challenging label-permutation issue, prior methods have relied on either the Permutation Invariant Training (PIT) or the time-based First-In-First-Out (FIFO) rule. This study presents a model-based serialization strategy that incorporates an auxiliary module into the Attention Encoder-Decoder architecture, autonomously identifying the crucial factors to order the output sequence of the speech components in multi-talker speech. Experiments conducted on the LibriSpeech and LibriMix databases reveal that our approach significantly outperforms the PIT and FIFO baselines in both 2-mix and 3-mix scenarios. Further analysis shows that the serialization module identifies dominant speech components in a mixture by factors including loudness and gender, and orders speech components based on the dominance score.", "sections": [{"title": "1. Introduction", "content": "Automatic Speech Recognition (ASR) technology has experienced rapid advancements in recent years, attributed in part to the introduction of end-to-end architectures [1, 2, 3] based on state-of-the-art network structures like Transformers [4, 5] and Conformers [6, 7, 8]. For example, the Conformer model [8] has achieved a Word Error Rate (WER) below 2% on the Librispeech benchmark \u00b9. However, it is important to note that such low WERs are attained in the context of single-talker speech. In scenarios where multiple speakers talk simultaneously, single-talker systems encounter confusion in determining which speaker to focus on. Multi-talker speech recognition sets a more ambitious objective: transcribing the speech of all speakers, even when their signals overlap [9, 10, 11, 12].\nEarly research adopted a multi-head design, where each head produces transcriptions for one speaker within the overlapped speech [13, 14, 15]. A key challenge of this architecture is the uncertainty in label assignment, where it remains unclear which speaker's text labels should be assigned to each head during loss computation. While some studies address this uncertainty by leveraging biased information such as energy [13] or starting time [14], Permutation Invariant Training (PIT) has gained widespread popularity [15, 16, 17, 18].\nOne significant drawback of the aforementioned multi-head architecture is the necessity to pre-define the number of speakers, with larger speaker counts leading to increased computational demands. Serialized Output Training (SOT) [19, 20], built on the Attention Encoder-Decoder (AED) architecture [21], addresses these challenges by sequentially outputting the transcriptions of each speaker. SOT utilizes a unified decoder for transcribing all speakers' speech, eliminating the need for multiple heads. However, determining the sequence of target text labels, known as label serialization, remains crucial, akin to label assignment in the multi-head architecture. Two prevalent serialization strategies are First-In-First-Out (FIFO) [22] and sequential PIT [19, 23]. FIFO concatenates labels of single-talker speech speaker by speaker, ordered according to starting time, while sequential PIT orders the labels of different speakers in a manner that results in minimal loss based on the output of the current model. Neither FIFO nor PIT is flawless. Sequential PIT employs an 'indeterministic' strategy, lacking a guarantee for consistent logic in serialization a prerequisite for effective model learning from serialized labels. In contrast, FIFO is inherently deterministic and theoretically superior to PIT, which has been supported by the findings from Kanda et al. [19]. However, when the starting time bias is ambiguous, uncertainty affects both training and inference, resulting in a significant performance drop, as demonstrated in the experimental section.\nIn this paper, we present a novel serialization strategy for multi-talker ASR with SOT. The fundamental concept involves training an explicit serialization module to dynamically determine the label order, departing from predefined biases or the minimum-loss order associated with PIT. We hope that the model can learn to discover factors by itself and use these factors to order the speech components, akin to human decision-making on which speech component to focus first. Specifically, the serialization module comprises a connectionist temporal classification (CTC) decoder [24] integrated with the encoder of the AED architecture, trained by minimizing the minimum CTC loss between the encoder output and labels of each speech component. This minimum CTC loss enforces the serialization module to identify a 'dominant component' from the mixed speech to recognize, akin to humans unconsciously paying attention to the dominant speech in a cocktail party environment [25, 26, 27]. Once the serialization module is fully trained, the CTC loss assigned to each speech component, which can be regarded as a dominance score, is used to order the speech components in mixed signals. We, therefore, name the new approach dominance-based serialization.\nOur contribution is twofold: (1) We present a novel dominance-based serialization strategy for multi-talker ASR with SOT. Experimental results demonstrated that this strategy outperforms the prevalent PIT approach, and beats the FIFO"}, {"title": "2. Methods", "content": "SOT constructs the target labels for multi-talker speech by concatenating each speaker's labels according to specific serialization rules. Suppose a multi-speaker speech signal X involves speech from two different speakers, and these speakers appear sequentially in time, then the FIFO serialization strategy generates the target label sequence as follows:\n\\(Lfifo = [ L1 <sc> L2 <sc><eos>],\\)\nwhere L1, L2 represent the content labels of the speech components of the two speakers, respectively. <sc> represents 'speaker change', and <eos> represents 'end of sentence'. Figure 1(a) illustrates the FIFO strategy. The loss function for the FIFO-SOT with the target label Lfifo is therefore defined as:\n\\(Lossfifo = \\sum_n CE(yn, Lfifo),\\)\nwhere CE represents the cross-entropy loss and Lifo is the nth token in Lfifo, and yn is the nth output of the model.\nFor PIT, the order of {Li} in Eq. (1) can be permuted arbitrarily and the minimum CE loss resultant from these permutations is used as the loss for model training, formulated as:\n\\(Losspit = \\min_{\\Phi \\in \\Phi} \\sum_n CE(yn, L),\\)\nwhere \\( \\Phi \\) represents the set of all possible permutations on {Li}. Figure 1(b) illustrates the computation of the PIT-SOT loss.\nThe proposed dominance-based SOT, abbreviated by DOM-SOT, is illustrated in Figure 1(c). Overall, it is an AED architecture augmented by a serialization module (shown by the gray block); this module determines the order of the speakers when constructing the target label for model training, and the serialization module and the AED model are trained jointly. Note that once the model has been trained, the serialization module is discarded during inference.\nMore specifically, given the multi-talker speech X, the feature sequence h is derived by the AED encoder. h is then forwarded to the serialization module, where the CTC losses are computed between h and the labels of all speakers, i.e., {Li}.\nAccording to these CTC losses, the speakers are arranged in ascending order, denoted by e, which is the output of the serialization module, and the target label Le is constructed according to e. Finally, the entire model is trained by:\n\\(Lossdom = \\alpha * \\min_{i=1,..., N} \\{CTC(h, Li)\\} + (1 \u2212 a) * CE(y, L),\\)\nwhere N is the number of speakers in multi-talker speech. a is a hyper-parameter to tune the behavior of the encoder. Our experiments show that a small a is preferred, probably because it avoids the encoder concentrating on a single speaker and ignoring others."}, {"title": "3. Experiment Settings", "content": "The official Train_960h set from the LibriSpeech database [28] was used to construct the training data. Each training sample could be clean speech, 2-talker speech, or 3-talker speech. These three types of speech samples are equal in quantity in the final training set. When constructing 2-talker or 3-talker samples, the clean utterances were randomly sampled from the Train_960h set and then mixed using random and normalized weights.\nTo train the FIFO-SOT model, a 0.25s 4s offset is imposed when mixing any two utterances. Without such offset, the model is hard to train as the bias on starting time is ambiguous. For the PIT-SOT and DOM-SOT models, the same offset occurs in 40% of the training samples, while the remaining samples are mixed without any offset.\nWe tested the various SOT models on both clean speech and multi-talker speech. The clean test used the test-clean dataset from LibriSpeech. The 2-talker and 3-talker test samples were constructed by mixing the utterances listed in 2mix-test-clean and 3mix-test-clean from LibriMix [29], respectively. These utterances are mixed with an offset of 0/1/2/3 seconds, to test performance in different scenarios.\nAll models in the experiments used 40-dimensional Mel-filter Fbank features as the input and adopted the same AED model structure where the encoder is a Conformer and the decoder is a Transformer. Before the encoder, two CNN layers perform sub-sampling. The kernel sizes and strides of both CNN layers were set to 3 and 2. The first and second CNN layer's input/output channels were 1/256 and 256/256, respectively."}, {"title": "4. Experimental Result", "content": "We compare the three SOT systems constructed with different serialization strategies: FIFO-SOT, PIT-SOT, and DOM-SOT. The test includes three groups: clean test, 2-mix talker test, and 3-mix talker test. In each of the mix-talker tests, 4 test conditions are designed, according to the offset among speakers. Note that the models are the same for all these 3 groups and 4 conditions. We report both speaker-blind WERs and speaker-aware WERs.\nThe comparison between FIFO-SOT and PIT-SOT aligns closely with our expectations. The performance difference between the two models on clean data is insignificant. Moreover, in the 2-mix and 3-mix groups, FIFO-SOT consistently outperforms PIT-SOT when the offset is not Os. This is not surprising as the bias on starting time is clear in these test conditions, and the bias has been explicitly exposed to the model when training the FIFO-SOT model. This also aligns with the results reported in [14, 19]. However, when the offset is Os, the performance of FIFO-SOT drastically declines. It is noteworthy that the true offset of the speakers in the Os-offset condition is not exactly 0 due to the leading silence within each utterance. However, this slight offset is not sufficient to form a clear bias, resulting in poor performance. This is an illuminating result and indicates that if the bias is ambiguous in the test, SOT models relying on that bias may completely fail. This is perhaps a general rule and may apply to any bias.\nIn comparison, PIT-SOT remains relatively stable in all the test conditions: it is slightly worse than FIFO-SOT when the bias on time is clear, but remains a rather good performance when the bias disappears. This underscores its effectiveness in dealing with multi-talker ASR tasks.\nFirst of all, DOM-SOT outperforms PIT-SOT under all testing conditions. This is a highly promising result, as the two models were trained with identical resources and nearly identical structures (the extra parameters from the additional projection layer of the serialization module in DOM-SOT can be largely ignored). This indicates that the performance gain obtained with DOM-SOT is of no cost. We therefore conclude that DOM-SOT, like PIT-SOT, is a general architecture for dealing with multi-talker speech, and it is more powerful than PIT-SOT.\nWhen comparing with FIFO-SOT, we can find that in the Os-offset group, DOM-SOT shows an overwhelming advantage. Even on the conditions with a longer offset for which the bias is very clear and the training and test conditions match perfectly for FIFO-SOT, DOM-SOT can achieve comparable or even better performance compared to FIFO-SOT. We highlight that in these conditions, PIT-SOT never beats FIFO-SOT. This provides strong evidence that DOM-SOT is a more powerful architecture than PIT-SOT in tackling multi-talker speech.\nTable 1 presents both speaker-blind WER and speaker-aware WER. It is evident that the speaker-aware WER is a more conservative metric than the speaker-blind WER, as discussed in Section 3.5. Moreover, the comparison between the two types of WERs reveals extra information about the model behavior. For instance, DOM-SOT outperforms in all the conditions in terms of speaker-blind WER but is slightly worse than FIFO-"}, {"title": "5. Analysis & Discussion", "content": "In this section, we aim to discover how DOM-SOT works and outperforms PIT-SOT and FIFO-SOT. To achieve this, we conducted an analysis of the dominance scores of speech components within mixed speech samples, i.e., the CTC loss computed by the serialization module with the transcript of each speech component. The mixed samples are from the 2-mix group, under the Os-offset condition and the 3s-offset condition.\nFirst of all, we found that nearly all samples adhered to the serialization rule based on dominance scores. More specifically, for the 2-mix data with Os/3s offsets, 99.7% and 98.6% of the test data, respectively, were transcribed by putting the speech component with a lower CTC loss ahead. This observation indicates that the DOM-SOT model has effectively learned to utilize CTC loss as a criterion for organizing its output sequence.\nNext, we attempt to identify commonalities in dominant speech. We investigated five potential factors: Loudness, Gender, Content length, Length being overlapped, and Start time. These factors were selected based on human perception of dominant speech [25, 26, 27]. By comparing dominant and non-dominant speech across these factors for instance, if the dominant speech is louder than the non-dominant speech one can identify potential biases used to determine dominant speech. To ensure the results are meaningful, the test sets were filtered to maintain balance across all factors. We also conducted a similar analysis for FIFO-SOT and PIT-SOT. Since the concept of 'dominant speech' is irrelevant in these methods, we define the first transcribed speech as dominant. This is acceptable as the comparison between the two speech components is what matters.\nFigure 2 displays the results. It is evident that for DOM-SOT, there is a distinct bias on loudness and gender. This suggests that when two speeches are mixed, the encoder tends to prioritize recognizing louder male speech as dominant. In contrast, FIFO-SOT uses starting time as the bias, while PIT-SOT uses gender as the major bias. Such findings indicate that DOM-SOT employs a multi-bias approach, making it more robust than single-bias approaches. This clarifies why it outperforms FIFO-SOT and PIT-SOT. Importantly, the biases in DOM-SOT are identified by the model itself rather than being predefined by human intuition. Note that the biases shown with DOM-SOT and PIT-SOT are stable in both the Os-offset and 3s-offset conditions, indicating that they are truly used by the model rather than from statistical randomness."}, {"title": "6. Conclusion & Future Work", "content": "We propose a model-based serialization strategy for the SOT framework, named DOM-SOT. This strategy augments the standard AED model with a serialization module that computes CTC loss for each speech component and orders the labels according to this loss. Surprisingly, this approach demonstrated a remarkable advantage over the prevalent PIT approach in our multi-talker ASR experiments, and yielded comparable or superior results to methods based on human-defined biases, like FIFO, even in test conditions where the bias is strong. Deep analysis showed that the serialization module has discovered multiple factors to order the speech components, especially loudness, and gender. This multi-factor nature explains its superior performance.\nWe also advocate the use of speaker-aware WER as the metric for measuring multi-talker ASR performance, as it highlights the importance of the speaker-change token. Analysis has shown that the performance of all SOT systems decreases when measured by speaker-aware WER, indicating potential errors in the speaker-change token, in particular with the proposed strategy. Future work will investigate this problem, and conduct extensive experiments on other datasets to confirm the superiority of the dominance-based approach over the PIT approach."}]}