{"title": "TRADES: Generating Realistic Market Simulations with Diffusion Models", "authors": ["Leonardo Berti", "Bardh Prenkaj", "Paola Velardi"], "abstract": "Financial markets are complex systems characterized by high statistical noise, nonlinearity, volatility, and constant evolution. Thus, modeling them is extremely hard. Here, we address the task of generating realistic and responsive Limit Order Book (LOB) market simulations, which are fundamental for calibrating and testing trading strategies, performing market impact experiments, and generating synthetic market data. Previous works lack realism, usefulness, and responsiveness of the generated simulations. To bridge this gap, we propose a novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES). TRADES generates realistic order flows as time series conditioned on the state of the market, leveraging a transformer-based architecture that captures the temporal and spatial characteristics of high-frequency market data. There is a notable absence of quantitative metrics for evaluating generative market simulation models in the literature. To tackle this problem, we adapt the predictive score, a metric measured as an MAE, by training a stock price predictive model on synthetic data and testing it on real data. We compare TRADES with previous works on two stocks, reporting a \u00d73.27 and \u00d73.48 improvement over SOTA according to the predictive score, demonstrating that we generate useful synthetic market data for financial downstream tasks. Furthermore, we assess TRADES's market simulation realism and responsiveness, showing that it effectively learns the conditional data distribution and successfully reacts to an experimental agent, giving sprout to possible calibrations and evaluations of trading strategies and market impact experiments. We developed DeepMarket, the first open-source Python framework for market simulation with deep learning. In our repository, we include a synthetic LOB dataset composed of TRADES's generated simulations. We release the code at https://github.com/LeonardoBerti00/DeepMarket.", "sections": [{"title": "1 Introduction", "content": "A realistic and responsive market simulation\u00b9 has always been a dream in the finance world [31, 35, 40, 42, 51, 52]. Recent years have witnessed a surge in interest towards deep learning-based market simulations [15, 18, 36, 44]. An ideal market simulation should fulfill four key objectives: (1) enable the calibration and evaluation of algorithmic trading strategies including reinforcement learning models [75]; (2) facilitate counterfactual experiments to analyze (2.1) the impact of orders [70], and (2.2) the consequences of changing financial regulations and trading rules, such as price variation limits; (3) analyze market statistical behavior and stylized facts in a controlled environment; (4) generate useful granular market data to facilitate research on finance and foster collaboration.\nFor these objectives, two key elements are paramount: the realism and the responsiveness of the simulation. Realism [43] refers to the similarity between the generated probability distribution and the actual data distribution. Responsiveness captures how the simulated market reacts to an experimental agent's actions. Furthermore, the generated data usefulness is crucial for achieving the last objective (4). Usefulness refers to the degree to which the generated market data can contribute to other related financial tasks, such as predicting the trends of stock prices [50, 57, 67].\nBacktesting [24] and Interactive Agent-Based Simulations (IABS) [9] are two of the most used traditional market simulation methods. Backtesting assesses the effectiveness of trading strategies on historical market data. It is inherently non-responsive since there is no way to measure the market impact of the considered trading strategies, making the analysis partial. Minimizing market impact has been the focus of extensive research efforts over many years [1], resulting in the development of sophisticated algorithms designed to mitigate the price effects of executing large orders, through timing, order, size, and venue selection. IABS, on the other hand, enables the creation of heterogeneous pools of simplified traders with different strategies, aiming to approximate the diversity of the real market. However, obtaining realistic multi-agent simulations is challenging, as individual-level historical data of market agents is private, which impedes the calibration of the trader agents, resulting in an oversimplification of real market dynamics."}, {"title": null, "content": "1To abbreviate, throughout this paper, by \"market simulation\", we refer to a limit order book market simulation for a single stock."}, {"title": null, "content": "Recent advances in generative models, particularly Wasserstein GANs [13-15, 36], have shown promise in generating plausible market orders for simulations. However, GANs are susceptible to mode collapse [64] and training instability [12], leading to a lack of realism and usefulness in the generated data. These limitations hinder their ability to satisfy the market simulation objectives and their real-world applicability.\nTo address these shortcomings, we present our novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB market Simulations (TRADES). TRADES generates realistic high-frequency market data, which are time series, conditioned on past observations. We demonstrate that TRADES surpasses state-of-the-art (SoTA) methods in generating realistic and responsive market simulations. Importantly, due to its ability to handle multivariate time series generation, TRADES is adaptable to other domains requiring conditioned sequence data generation. Furthermore, TRADES readily adapts to an experimental agent introduced into the simulation, facilitating counterfactual market impact experiments. In summary, our contributions are:\n(1) Realistic and responsive market simulation method: We develop a Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES), exploiting a transformer-based neural network architecture. We remark that TRADES is easily adaptable to any multivariate time-series generation domain.\n(2) Plug-and-play framework: We release DeepMarket, the first open-source Python framework for market simulation with deep learning. We also publish TRADES's implementation and checkpoints to promote reproducibility and facilitate comparisons and further research.\n(3) Synthetic LOB dataset: We release a synthetic LOB dataset composed of the TRADES's generated simulations. We show in the results (section 7.1) how the synthetic market data can be useful to train a deep learning model to predict stock prices.\n(4) New \"world\" agent for market simulations: We extend ABIDES [9], an agent-based simulation environment, in our framework by introducing a new world agent class accompanied by a simulation configuration, which, given in input a trained generative model, creates limit order book market simulations. Our experimental framework does not limit the simulation to a single-world agent but enables the introduction of other trading agents, which interact among themselves and with the world agent. This defines a hybrid approach between deep learning-based and agent-based simulations.\n(5) First quantitative metric for market simulations: The literature shows a notable absence of quantitative metrics to evaluate the generated market simulations. Typically, the evaluation relies on plots and figures. We posit that a robust and quantitative assessment is essential for the comparative analysis of various methodologies. To this end, we adapt the predictive score introduced in [73] to objectively and quantitatively evaluate the usefulness of the generated market data."}, {"title": null, "content": "2They refer both to orders and LOB snapshots (see details in Sec. 2.2 and Sec. 7)."}, {"title": null, "content": "(6) Extensive experiments assessing usefulness, realism, and responsiveness: We perform a suite of experiments to demonstrate that TRADES-generated market simulations abide by these three principles. We show how TRADES outperforms SoTA methods [9, 14, 15, 69] according to the adopted predictive score and illustrate how TRADES follows the established stylized facts in financial markets [69]."}, {"title": "2 Background", "content": "Here, we provide background information on multivariate time series generation and limit order book markets. Furthermore, since TRADES is an extension of the Denoising Diffusion Probabilistic Model (DDPM), we summarize it in the Appendix A."}, {"title": "2.1 Multivariate time series generation", "content": "Generating realistic market order streams can be formalized as a multivariate time series generation problem. Let $X = {x_{1:N,1:K}} \\in R^{N \\times K}$, be a multivariate time series, where N is the time dimension (i.e., length) and K is the number of features. The goal of generating multivariate time series is to consider a specific time frame of previous observations, i.e., ${X_{1:N,1:K}}$, and to produce the next sample $X_{N+1}$. This task can easily be formulated as a self-supervised learning problem, where we leverage the past generated samples as the conditioning variable for an autoregressive model. In light of this, we can define the joint probability of the time series as in Eq. (1).\n$q(x) = \\prod_{n=1}^N [q(x_n | X_{1}, ..., x_{n-1})]$                                                                                                                                  (1)\nWe leverage this concept at inference time using a sliding window approach. Hence, for every generation step,\u00b3 we generate a single sample $x_N^\\theta \\in R^K$. In the next step, we append the generated $x_N^\\theta$ to the end of the conditional part and shift the entire time series one step forward (see Section 4 for details with TRADES). Because we aim to generate a multivariate time series starting from observed values, we model the conditioned data distribution $q(x_N|X_{1:N-1})$ with a learned distribution $p_\\theta(x_N|X_{1:N-1})$, to sample from it. Hereafter, we denote the conditional part\u00b9 with $x^\\sigma$, and the upcoming generation part with $x^\\theta$."}, {"title": "2.2 Limit Order Book", "content": "In a Limit Order Book (LOB) market, traders can submit orders to buy or sell a certain quantity of an asset at a particular price. There are three main types of orders in a limit order market. (1) A market order is filled instantly at the best available price. (2) A limit order allows traders to decide the maximum (in the case of a buy) or the minimum (in the case of a sell) price at which they want to complete the transaction. A quantity is always associated with the price for both types of orders. (3) A cancel orders removes an active limit order. The Limit Order Book (LOB) is a data structure that stores and matches the active limit orders according to a set of rules. The LOB is accessible to all the market agents and is updated with each event, such as order insertion, modification, cancellation,"}, {"title": null, "content": "3For every T diffusion steps there is a generation step\n4We remark that the conditioning can be composed of real or generated samples.\n5Sometimes, events referring to these orders are defined as deletion."}, {"title": "3 Related Works", "content": "Diffusion models for time series generation. Diffusion models have been successfully applied to generate images [3, 33], video [41], and text [76]. Recently, they have been exploited also for time series forecasting [37, 53], imputation [5, 63], and generation [38]. To the best of our knowledge, only Lim et al. [38] tackle time series generation using diffusion models. They present TSGM, which relies on an RNN-based encoder-decoder architecture with a conditional score-matching network. Differently, our model is a conditional denoising diffusion probabilistic model which relies on a transformer-based architecture. Other diffusion-based approaches for time series [37, 53, 63] address slightly different problems, such as forecasting and imputation.\nMarket simulation with deep learning. Generating realistic market simulations using deep learning is a new paradigm. Traditional computational statistical approaches [16, 29] and IABS [9, 47] rely on strong assumptions, such as constant order size, failing to produce realistic simulations. These methods are mainly used to study how the interactions of autonomous agents give rise to aggregate statistics and emergent phenomena in a system [34]. Limit order book simulations are increasingly relying on deep learning. Li et al. [36] were the first to leverage a Wasserstein GAN (WGAN) [2] for generating order flows based on historical market data. Similarly, Coletta et al. [14, 15] employ WGANs in their stock market simulations, addressing the issue of responsiveness to experimental agents for the first time. Differently from [14, 15, 36], we condition with both the last orders and LOB snapshots, pushing the generation process toward a more realistic market simulation. Hultin et al. [30] extend [21] and model individual features with separate conditional probabilities using RNNs. Instead of relying on GANs, which are prone to model collapse [72] and instability [12], and RNNs, often hampered by the vanishing gradient phenomenon, we exploit diffusion-based models with an underlying transformer architecture. Nagy et al. [44] rely on simplified state-space models [58] to learn long-range dependencies, tackling the problem via a masked language modeling approach [22]. Shi and Cartlidge [55] introduce NS-LOB, a novel hybrid approach that combines a pre-trained neural Hawkes [54] process with a multi-agent trader simulation. We refer the reader to [32] for a comprehensive review of limit order book simulations."}, {"title": "4 Transformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations", "content": "We introduce TRADES, a Transformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations. Conditional diffusion models are better suited than standard diffusion models in generative sequential tasks because they can incorporate information from past observations that guide the generation process towards more specific and desired outputs. We formalize the reverse process for TRADES and the self-supervised training procedure. In Section 5, we specialize our architecture for market simulations."}, {"title": "4.1 Generation with TRADES", "content": "Here, we focus on an abstract time series generation task with TRADES. The goal of probabilistic generation is to approximate the true conditional data distribution $q(x | x^\\sigma)$ with a model distribution $p_\\theta(x | x^\\sigma)$. During the forward process, we apply noise only to the \"future\" \u2013 i.e., the part of the input we want to generate \u2013 while keeping the observed values unchanged. Therefore, the forward process is defined as in the unconditional case in Eq. (11) (Appendix). For the reverse process, we extend the unconditional one $p_\\theta(x_{0:T})$, defined in Eq. (12) (Appendix), to the conditional case:\n$P_\\theta(x_{0:T}) := p(x_T) \\prod_{t=1}^T p_\\theta (x_{t-1}|x_t,x^\\sigma)$ (2)\n$P_\\theta(x_{t-1}|x_t,x^\\sigma) := N(x_{t-1}; \\mu_\\theta(x_t, x^\\sigma, t), \\Sigma_\\theta(x_t, x^\\sigma, t))$ (3)\nWe define the conditional denoising learnable function as in Eq. (4).\n$\\epsilon_\\theta: (x \\in R^{S \\times K}, x^\\sigma \\in R^{M \\times K}, t \\in R^{K}) \\rightarrow \\epsilon_t \\in R^{S \\times K}$ (4)\nwhere M + S = N. We set M = N - 1 and S = 1 for our experiments. Using S > 1 increases the efficiency, but also the task complexity because, in a single generative step, we generate S samples. We exploit the parametrization proposed in [25] described in Eq. (5) to estimate the mean term.\n$\\mu_\\theta (x_t, x^\\sigma, t) = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, x^\\sigma, t))$ (5)\nwhere $\\tilde{x_t} = \\frac{x_t}{\\sqrt{\\alpha_t}} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\;\\; \\epsilon \\sim N(0, 1)$.\nWe do not rely on a fixed schedule as in [25] regarding the variance term. Inspired by [45], we learn it as in Eq. (6).\n$\\Sigma_\\theta (x_t, x^\\sigma, t) = exp(\\nu log \\beta_t + (1 - \\nu) log \\bar{\\beta}_t)$, (6)\nwhere v is the neural network output, together with $\\beta_\\sigma$. Nichol and Dhariwal [45] found that this choice improves the negative log-likelihood, which we try to minimize. After computing $\\epsilon_t$ and $\\sigma_t$, we denoise $x_{t-1}$ as in Eq (7).\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, x^\\sigma, t)) + \\sigma_t z$ (7)\nwhere z ~ N(0, I). After the T steps, the denoising process is finished, and $x_0$ is reconstructed."}, {"title": "4.2 Self-supervised Training of TRADES", "content": "Given generation target $x^\\theta$ and conditional observations $x^\\sigma$, we sample $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, where $\\epsilon$ ~ N(0, I), and train $\\epsilon_\\theta$ by minimizing Eq. (8).\n$L_\\epsilon(\\theta) := E_{t,x_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta (x_t, x^\\sigma, t)||^2]$. (8)\nInspired by [45], we also learn $\\Sigma_\\theta$ optimizing it according to Eq. (9).\n$L_\\Sigma(\\theta) := E_{q}[ - log p_\\theta(x_0 | x_1,x^\\sigma) + D_{KL}(q(x_2|x_1,x^\\sigma)|| p_\\theta(x_1|x_2,x^\\sigma)) + ... + D_{KL}(q(x_T|x^\\sigma_0) || p_\\theta(x_T|x^\\sigma))]$ (9)\nWe optimize Eq. (9) to reduce the negative log-likelihood, especially during the first diffusion steps where $L_\\Sigma$ is high [45]. The final loss function is a linear combination of the two as in Eq: (10).\n$L = L_\\epsilon + \\lambda L_\\Sigma$. (10)\nWe perform training by relying on a self-supervised approach as follows. Given a time series $x_0 \\in R^{N \\times K}$, we apply noise only to its last element \u2013 i.e., $x^\\theta$ \u2013 through the forward pass. Then, we denoise it via the reverse process and learn $p_\\theta(x_{t-1}|x_t, x^\\sigma)$ which aims to generate a new sample from the last observations. Therefore, during training, the conditioning has only observed values. A sampling time TRADES generates new samples autoregressively, conditioned on its previous outputs, until the simulation ends."}, {"title": null, "content": "Notice from Eq (15) of the original DDPM formulation that $\\tilde{E_\\theta}$ is fixed."}, {"title": "5 TRADES for Market Simulation", "content": "To create realistic and responsive market simulations, we implement TRADES to generate orders conditioned on the market state. TRADES's objective is to learn to model the distribution of orders. Fig. 1 presents an overview of the diffusion process and the architecture. The network that produces $\\epsilon_\\theta$ and $\\Sigma_\\theta$ contains several transformer encoder layers to model the temporal and spatial relationships of the financial time series [57]. Since transformers perform better with large dimension tensors, we project the orders tensor and the LOB snapshots to a higher dimensional space, using two fully connected layers. Hence, TRADES operates on the augmented vector space. After the reverse process, we de-augment $\\epsilon_\\theta$ and $\\Sigma_\\theta$ projecting them back to the input space to reconstruct $x_{t-1}$ and compute the loss.\nConditioning. The conditioned diffusion probabilistic model learns the conditioned probability distribution $p_\\theta(o|s)$, where s is the market state, and o is the newly generated order, represented as (p, q, d, b, d, c), where p is the price, q is the quantity, d is the direction either sell or buy, b is the depth, i.e., the difference between p and the best available price, 8 is the time offset representing the temporal distance from the previously generated order and c is the order type - either market, limit or cancel order. The asymmetries between the buying and selling side of the book indicate shifts in the supply and demand curves caused by exogenous and unobservable factors that influence the price [11]. Therefore, as depicted in Fig. 1, the model's conditioning extends beyond the last N-1 orders. To effectively capture market supply and demand dynamics encoded within the LOB, we incorporate the last N LOB snapshots of the first L LOB levels as input, where each level has a bid price, bid size, ask price, and ask size. We set N = 256 as in [15], and L = 10. We argue that this choice of L is a reasonable trade-off"}, {"title": "6 DeepMarket framework with synthetic dataset for deep learning market simulations", "content": "We present DeepMarket, an open-source Python framework developed for LOB market simulation with deep learning. DeepMarket offers the following features: (1) pre-processing for high-frequency market data; (2) a training environment implemented with PyTorch Lightning; (3) hyperparameter search facilitated with WANDB [4]; (4) TRADES and CGAN implementations and checkpoints to directly generate a market simulation without training; (5) a comprehensive qualitative (via the plots in this paper) and quantitative (via the predictive score) evaluation. To perform the simulation with our world agent and historical data, we extend ABIDES [9], an open-source agent-based interactive Python tool."}, {"title": "6.1 TRADES-LOB: a new synthetic LOB dataset", "content": "In LOB research one major problem is the unavailability of a large LOB dataset. In fact, if you want to access a large LOB dataset you need to pay large fees to some data provider. The only two freely available LOB datasets are [27] and [46] which have a lot of limitations. The first one is composed of only Chinese stocks, which have totally different rules and therefore resulting behaviors with respect to NASDAQ or LSE stocks. The high cost and low availability of IOB data restrict the application and development of deep learning algorithms in the LOB research community. In order to foster collaboration and help the research community we release a synthetic LOB dataset: TRADES-LOB. TRADES-LOB comprises simulated TRADES market data for Tesla and Intel, for two days. Specifically, the dataset is structured into four CSV files, each containing 50 columns. The initial six columns delineate the order features, followed by 40 columns that represent a snapshot of the LOB across the top 10 levels. The concluding four columns provide key financial metrics: mid-price, spread, order volume imbalance, and Volume-Weighted Average Price (VWAP), which can be useful for downstream financial tasks, such as stock price prediction. In total the dataset is composed of 265,986 rows and 13,299,300 cells, which is similar in size to the benchmark FI-2010 dataset [46]. The dataset will be released with the code in the GitHub repository. We show in the results (section 7.1) how the synthetic market data can be useful to train a deep learning model to predict stock prices."}, {"title": "7 Experiments", "content": "Dataset and reproducibility. In almost all SoTA papers in this subfield, the authors use one, two, or three stocks [15, 30, 36, 44, 54, 55], most of which are tech. Following this practice, we create a LOB dataset from two NASDAQ stocks - i.e., Tesla and Intel from January 2nd to the 30th of 2015. We argue that stylized"}, {"title": null, "content": "7The data we used are downloadable from https://lobsterdata.com/ tradesquotesandprices upon buying the book indicated on the website, which contains the password to access the data pool."}, {"title": null, "content": "facts and market microstructure behaviors, which are the main learning objective of TRADES, are independent of single-stock behaviors (see [7, 8, 20, 23]8), so the particular stock characteristics, such as volatility, market cap, and p/e ratio, are not fundamental. Each stock has 20 order books and 20 message files, one for each trading day per stock, totaling ~24 million samples. The message files contain a comprehensive log of events from which we select market orders, limit orders, and cancel orders.9. Each row of the order book file is a tuple (pask (t), Vask (t), pbid (t), Vbid (t)) where pask (t) and pbid (t) \u2208 RL are the prices of levels 1 through L, and Vask (t) and Vbid (t) \u2208 RL are the corresponding volumes. We use the first 17 days for training, the 18th day for validation, and the last 2 for market simulations. We are aware of the widely used FI-2010 benchmark LOB dataset [46] for stock price prediction. However, the absence of message files in this dataset hinders simulating the market since the orders cannot be reconstructed. In Appendix B, we provide an overview of FI-2010 and its limitations.\nExperimental setting. After training the model for 70, 000 steps until convergence, we freeze the layers and start the market simulation. A simulation is composed of (1) the electronic market exchange that handles incoming orders and transactions; (2) the TRADES-based \"world\" agent, which generates new orders conditioned on the market state; and (3) one or more optional experimental agents, that follow a user-customizable trading strategy, enabling counterfactual and market impact experiments. So, the experimental framework is a hybrid approach between a deep learning model and an interactive agent-based simulation.\nWe conduct the simulations with the first 15 minutes of real orders to compare the generated ones with the market replay.10 Afterward, the diffusion model takes full control and generates new orders autoregressively, conditioned on its previous outputs, until the simulation ends. After the world agent generates a new order, there is a post-processing phase in which the output is transformed into a valid order. We begin the simulation at 10:00 and terminate it at 12:00. This choice ensures that the generated orders are sufficient for a thorough evaluation while maintaining manageable processing times. On average, 50,000 orders are produced during this two-hour time frame. The output CSV file of the simulation contains the full list of orders and LOB snapshots of the simulation. All experiments are performed with an RTX 3090 and a portion of an A100. In Appendix C, we detail the data pre- and post-processing and model hyperparameter choice.\nBaselines. We compare TRADES with the Market Replay \u2013 i.e., ground truth (market replay) a IABS configuration, and the Wasserstein GAN \u2013 i.e., CGAN - under the setting of [14], similar to the same of those proposed in [13, 15]. We implemented CGAN from scratch given that none of the implementations in [13-15] are available. We report details in the Appendix C. Regarding IABS configuration, we used the Reference Market Simulation Configuration, introduced in [9], which is widely used as comparison [14, 15, 69]. The configuration includes 5000 noise, 100 value,"}, {"title": null, "content": "8These finance seminal papers discuss universal statistical properties of LOBs across different stocks and markets.\n9In LOBSTER, events referring to these orders are defined as deletion.\n10 Market replay denotes the simulation performed with the real orders of that trading day."}, {"title": "7.1 Results", "content": "Here, we evaluate the usefulness, realism, and the responsiveness of the generated market simulations for Tesla and Intel. We train two TRADES Models, one for each stock. After training, we freeze the models and use them to generate orders during the simulation phase. A major disadvantage of market simulation is the misalignment in the literature for a single evaluation protocol. All the other works [14, 30, 44, 71] analyze performances with plots, hindering an objective comparison between the models. To fill this gap we adapt the predictive score [73] for market simulation. Predictive score is measured as an MAE, by training a stock mid-price predictive model on synthetic data and testing it on real data, that evaluates the usefulness of the generated simulations. Appendix D details the computation of the predictive score."}, {"title": null, "content": "Usefulness: TRADES outperforms the second-best by a factor of \u00d73.27 and \u00d73.48 on both stocks.12 We report both stocks' average predictive scores in Table 1. We report the predictive score on the market replay as ground truth (market replay). Notice that the market replay scores represent the desired MAE of each model"}, {"title": null, "content": "11 the full specifics are in the GitHub page of ABIDES in the tutorial section.\n12The values are computed dividing the second best value with the TRADES value s, both subtracted by the market replay predictive score."}, {"title": null, "content": "- i.e., the lower the difference in MAE with the market replay, the better. The table reveals that TRADES exhibits performances approaching that of the real market replay, with an absolute difference of 0.29 and 0.158 from market replay, respectively, for Tesla and Intel, suggesting a diminishing gap between synthetic and real-data training efficacy. Interestingly, although IABS cannot capture the complexity of real trader agents, it outperforms CGAN on Tesla, while it remains the worst-performing strategy on Intel. Note that the mid-price for Intel is, on average, 1/20th of the mid-price for Tesla. This discrepancy explains the difference in the scale of the predictive score. In conclusion, we demonstrated how a predictive model trained with TRADES's generated market data can effectively forecast mid-price.\nRealism: TRADES covers the real data distribution and emulates many stylized facts. To evaluate the realism of the generated time series, we compare the real data distributions with those of the generated one. We employ a combination of Principal Component Analysis (PCA) [56], alongside specialized financial methodologies that include the comparison of stylized facts and important features. In Fig. 2, we show PCA plots to illustrate how well the synthetic distribution covers the original data. TRADES (blue points) cover 67.04% of the real data distribution (red points) according to a Convex Hull intersection compared to 52.92% and 57.49% of CGAN and IABS, respectively. Following [69], we evaluate the simulation realism by ensuring that the generated market data mimics stylized facts derived from real market scenarios. Fig. 3 (1) illustrates that TRADES, similarly to the market replay and differently to IABS and CGAN, obey the expected absence of autocorrelation, indicating that the linear autocorrelation of asset returns decays rapidly, becoming statistically insignificant after 15 minutes. Fig. 3 (2) highlights TRADES's resemblance with the positive volume-volatility correlation. TRADES's generated orders show a positive volume-volatility correlation. Interestingly, TRADES captures this phenomenon better than that particular market replay day. Recall that TRADES is trained on 17 days of the market, which shows this characteristic correctly capturing this correlation. We acknowledge that the real market might not always respect all the stylized facts due to their inherent non-deterministic and non-stationary nature. Also, CGAN resembles this phenomenon but, differently from TRADES, with an unrealistic intensity."}, {"title": null, "content": "Responsiveness: TRADES is responsive to external agent. The responsiveness of a LOB market simulation generative model is crucial, especially if the objective of the market simulation is to verify the profitability of trading strategies or perform market impact"}, {"title": "7.2 DDIM sampling", "content": "One of the known limitations of diffusion models is the sampling time. Indeed, the generation of a single sample necessitates hundreds of iterative passes through the neural network. In this work, each model was trained using a diffusion process comprising 100 steps. Recently, Denoising Diffusion Implicit Model (DDIM) sampling method was proposed in [60] to speed up the generative process. Given that each hour of market simulation required six hours of computation on an RTX 3090, accelerating the simulation process was a relevant improvement. Consequently, we conducted simulations employing DDIM sampling (\u03b7 = 0), which is deterministic, utilizing a single step for each order. We use the same trained model. The results, presented in Table 2, demonstrate that the performance degradation is significant but not disastrous despite a remarkable 100-fold increase in computational efficiency."}, {"title": "7.3 Ablation and sensitivity studies", "content": "Table 3 shows two ablations (i.e., LOB conditioning and augmentation) and two sensitivity analyses (i.e., backbone choice and conditioning method) that highlight the effectiveness of TRADES design choice.\nAblation analyses. We verify two of the hypotheses made in the method design: (1) how much the LOB conditioning part is necessary for the task and (2) how augmenting the feature vectors"}, {"title": null, "content": "13TRADES is frozen, and the same model is used for all simulations.\n14we want to be clear that it technically enables evaluating trading strategies, but it does not assure any profitability in a real market scenario."}, {"title": "8 Conclusion", "content": "We proposed the Transformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES) to generate realistic order flows conditioned on the current market state. We evaluated TRADES's realism and responsiveness. We also adapted the predictive score to verify the usefulness of the generated market data by training a prediction model on them and testing it on real ones. This shows that TRADES can cover the real data distribution by 67% on average and outperforms SoTA by \u00d73.27 and \u00d73.48 on Tesla and Intel. Furthermore, our analyses reflect that TRADES correctly"}, {"title": "F Responsiveness Experiment Settings", "content": "A 8-POV strategy is characterized by a percentage level \u03b4\u2208 (0, 1"}]}