{"title": "Enhancing Agricultural Environment Perception via Active Vision and Zero-Shot Learning", "authors": ["Michele Carlo La Greca", "Mirko Usuelli", "Matteo Matteucci"], "abstract": "Agriculture, fundamental for human sustenance, faces unprecedented challenges. The need for efficient, human-cooperative, and sustainable farming methods has never been greater. The core contributions of this work involve leveraging Active Vision (AV) techniques and Zero-Shot Learning (ZSL) to improve the robot's ability to perceive and interact with agricultural environment in the context of fruit harvesting. The AV Pipeline implemented within ROS 2 integrates the Next-Best View (NBV) Planning for 3D environment reconstruction through a dynamic 3D Occupancy Map. Our system allows the robotics arm to dynamically plan and move to the most informative viewpoints and explore the environment, updating the 3D reconstruction using semantic information produced through ZSL models. Simulation and real-world experimental results demonstrate our system's effectiveness in complex visibility conditions, outperforming traditional and static predefined planning methods. ZSL segmentation models employed, such as YOLO World + EfficientViT SAM, exhibit high-speed performance and accurate segmentation, allowing flexibility when dealing with semantic information in unknown agricultural contexts without requiring any fine-tuning process.", "sections": [{"title": "I. INTRODUCTION", "content": "Agriculture is essential to society as it is responsible for the primary source of food that sustains human life. With the demand for food increasing, agriculture must evolve to produce higher yields while maintaining the sustainability of natural resources such as land and water [1]. Robotics, capable of performing repetitive and labor-intensive tasks such as planting, harvesting, and weeding, can boost productivity in agriculture, minimizing waste, maximizing crop yields, improving working conditions, and thus reducing the physical strain on human laborers. To successfully integrate robotics into agriculture and enhance their performance, the initial priority is enabling robots to accurately perceive the unstructured rural environment. The agricultural environment is frequently complex and dynamic, with plants and crops that may be obscured or have intricate, varied structures. This complexity presents substantial challenges for traditional management methods, underscoring the need for advanced perception approaches to improve the deployability of automation.\n\nThe proposed research introduces an approach to overcome the challenges and complexities of fruit perception through Active Vision (AV). Instead of relying on passive observation, AV allows robots to actively perceive, explore, and reconstruct at run-time their surroundings by planning the optimal position of the camera viewpoint using the Next-Best View (NBV) planning [2] which maximizes the information gained regarding plants and crops. This ensures that even hidden or occluded parts of the environment are effectively captured (Fig. 1). \u03a4\u03bf achieve this, semantic information about the plants and crops needs to be exploited for informative guidance. In this respect, this work applies Zero-Shot Learning (ZSL) [3] to provide useful segmentation, enabling the robot to generalize and adapt to various crops or environmental features without requiring specific training data for each scenario. By leveraging both 3D and semantic data, the robot can reconstruct a detailed, semantic, and context-aware map of the environment. This enhanced understanding allows the robot to strategically adjust its movements and positioning, leading to more effective interactions with the environment.\n\nThis research focuses on the following contributions:\n\u2022 Developed a modular architecture in C++ and ROS 2 for Active Vision in agricultural robotics, addressing the challenge of detecting occluded fruits during harvest;\n\u2022 To the best of the author's knowledge, this is the first work to integrate zero-shot learning perception with Active Vision exploration, enabling environment-independent operation in agriculture;\n\u2022 Conducted extensive evaluations both in simulation and real-world scenarios, in contrast to state-of-the-art methods that primarily focus on simulated environment with supervised learning.\n\u2022 Set a benchmark standard for the lack of reproducibility and availability of open source code in the context of Active Vision in agricultural robotics.\n\nThe entire implementation of this work is open source and available on GitHub at https://github.com/AIRLab-POLIMI/ active-vision."}, {"title": "II. RELATED WORKS", "content": "Active Vision enables robots to optimize their visual sensors' positions to gather useful information for various tasks [4]. Traditional approaches often struggle with efficiently identifying relevant objects in complex settings, particularly in agricultural applications like greenhouse environment, where occlusion is a significant challenge [5].\n\nBurusa et al. [5] have recently aimed to improve the efficiency of AV systems by incorporating semantic information into viewpoint planning. They introduced semantics-aware strategies that prioritize task-relevant plant components, such as tomatoes, peduncles, and petioles, during the view planning process. Their approach features a modular pipeline consisting of sensing modules detecting objects of interest (OOIs) using Mask R-CNN [6] specifically fine-tuned on tomatoes in simulation. It also includes 3D scene representation modules with attention mechanisms for tracking these OOIs across multiple viewpoints, and view-planning modules that determine the next-best viewpoint to optimize task performance. Additionally, in their work, clustering is applied for instance refinement at the voxel-map level.\n\nKriegel et al. [7] introduce a system that integrates AV by analyzing scenes with depth images, identifying and segmenting objects which are then stored in a dynamic database. In their work, iterative view planning improves object recognition and modeling by acquiring multiple views to reduce occlusions and enhance accuracy. A probabilistic voxel space is used to represent explored and unexplored regions, aiding in collision-free path planning, selecting minimal occlusion views, and verifying object poses. The system also autonomously models unknown objects, adding them to the database for future recognition.\n\nAV has been significantly advanced through the integration of Reinforcement Learning (RL) techniques [8], where models are trained to map situations to actions, optimizing decisions to maximize a numerical reward. Ammirato et al. [9] have introduced extensive datasets of images and object detection bounding boxes captured from diverse real-world scenes, providing simulations of robot interactions in various environments. These datasets are fundamental for training and evaluating deep neural networks aimed at predicting optimal moves to enhance object recognition accuracy through RL. Moreover, AV has been further refined using Recurrent Neural Networks (RNNs) [10], which excel at modeling temporal dependencies in sequential visual data. Xu et al. [11] demonstrated autonomous object exploration and identification in indoor environment using 3D attention models. Their approach leverages deep recurrent networks to handle temporal sequences and hierarchical classifiers to accurately identify objects from a large 3D shape collection, prioritizing the most informative views and regions.\n\nTraditional methods such as SVMs, decision trees, K-means, and CRFs were widely used for image segmentation tasks before the rise of Deep learning [12]. Modern approaches focus on semantic and instance segmentation [13], with the former assigning category IDs to pixels and the latter identifying individual objects within the same category. Recent advancements, such as Zero-Shot Learning (ZSL) [14], allow models to recognize unseen classes using auxiliary semantic information, such as text, reducing the need for extensive labeled supervised datasets. Notable innovations such as Segment Anything (SAM) [15], Efficient SAM [16], Grounding DINO [17], and YOLO World [18] push the boundaries by generalizing across new image distributions and tasks without requiring task-specific training.\n\nBuilding on the advancements in segmentation, these improvements also influence 3D reconstruction techniques, which rely heavily on accurate object identification. Probabilistic 3D maps, such as OctoMap [19], are fundamental for fast 3D reconstruction in robotics. These maps use octrees to efficiently represent and update environment, optimizing memory and computation. Recent methods such as BON\u03a7\u0391\u0399 [20] further reduce memory usage without predefined boundaries, dynamically adjusting to data density [21]. Regardless of the data structure, semantic information can be associated with each 3D block and used in NBV strategies, as demonstrated in the literature [5], [11]."}, {"title": "III. THE PROPOSED APPROACH", "content": "The proposed approach features a modular architecture organized into three main components: (1) Robot Block, (2) Segmentation Server Block, and (3) Active Vision Pipeline Block, as shown in Fig. 2. Each component integrates various functionalities to obtain a responsive robotics system, enabling AV for advanced environmental perception."}, {"title": "A. Overall Architecture", "content": "The proposed AV system is built as a multi-threaded and distributed architecture, allowing the deployment of its components across multiple machines, including:\n\u2022 The Robot Block manipulates and communicates the robot's current physical state to other nodes. It also integrates motion planning components, which work together to plan and perform robot movements while interfacing with low-level hardware components such as joints, sensors, and actuators. Lastly, it provides the sensor data necessary for environmental perception.\n\u2022 The Segmentation Server Block is responsible for handling all segmentation requests from the client, i.e. the Active Vision Pipeline Block. It operates the ZSL models to perform inference on-demand. Thanks to its server capabilities, the system can communicate and provide results without wasting resources during run-time. Being independent, it is easily deployable on a dedicated GPU and can seamlessly communicate with other machines.\n\u2022 The Active Vision Pipeline Block manages perception by integrating data from segmented images, requested from the Segmentation Server Block, point clouds extracted from the depth channel of the captured image, and the stored semantic 3D occupancy map, which is updated within the block's logic. Based on the 3D semantic reconstruction, the NBV is determined to guide the robot operating the MoveIt2 API [22] for motion planning. This approach minimizes latency and enhances performance, particularly with dense point clouds, thanks to multi-threaded operations performed."}, {"title": "B. Active Vision Pipeline", "content": "The AV pipeline consists of executing a loop as illustrated in Fig. 2, where perception and planning alternate, updating the semantic 3D occupancy map that is used for NBV planning.\n1) Segmentation as a Service: Segmentation is conducted using a client-server paradigm to alleviate bottlenecks in the image stream, allowing the system to focus on discretized viewpoints as the robot moves. Upon receiving sensor data, the client sends a request containing an image, a text prompt to guide the segmentation process, and confidence thresholds. The server processes this request, performs the inference, and returns the segmented results.\n\nTwo ZSL segmentation approaches are integrated as servers in this work:\n\u2022 Lang SAM (LSAM) [23]: it integrates two models for performing ZSL segmentation: Grounding DINO and SAM. Grounding DINO is a model that detects objects in an image based on a text prompt. The bounding box produced is then passed to SAM, which creates the segmented masks and related confidences of the unknown classes.\n\u2022 YOLO World + EfficentViT SAM (YWES) [24]: The same concept applies to YOLO World + EfficientViT SAM, a combination of YOLO World, an open-vocabulary object detection model, and EfficientViT SAM, a new family of accelerated SAM models.\n\nUpon receiving the client request, the server performs inference by feeding the detection bounding boxes generated by the detection model into the segmentation model. This inference produces a mask and confidence score for each detected instance. The results, along with their corresponding semantic classes, are then packaged into a custom message and sent back to the client speeding up the system workflow.\n2) 3D Occupancy Map Creation: After receiving the segmentation response, point clouds are generated from the sensor's RGB image and depth data, which are used to update the occupancy information. Semantic instances are added to the 3D occupancy map by projecting the ZSL masks onto the depth image, including confidence scores and semantic classes.\n3) Planning: The final step of the AV pipeline is selecting the NBV after the perception phase of each iteration. The process begins by sampling candidate viewpoints and evaluating them to find the most informative one. For each candidate, a camera frustum is generated to define the visible space. An adjustable attention region focuses the evaluation on specific semantic parts of the 3D occupancy map. Ray casting determines which voxels in the attention region are visible and occupied. The efficiency of different ray casting methods is assessed, with a focus on semantically important areas. The utility of each viewpoint is then calculated based on the Expected Semantic Information Gain ($G_{sem}(\\xi)$), computed as the sum of the semantic information of all voxels visible from viewpoint $\\xi$, as in Equation 1:\n\n$G_{sem}(\\xi) = \\sum_{x \\in (X_{\\xi} \\cap B)} I_{sem}(x)$ (1)\n\nwhere $X$ represents the set of voxels visible from viewpoint $\\xi$, and $B$ denotes the voxels within the regions of interest. The expected semantic information $I_{sem}(x)$ for a voxel x is defined by its entropy, as shown in Equation 2:\n\n$I_{sem}(x) = -p_{s}(x) \\log_{2}(p_{s}(x))+\n-(1-p_{s}(x)) \\log_{2}(1 \u2013 p_{s}(x))$, (2)\n\nwhere $p_{s}(x)$ is the confidence score for voxel x.\n\nExpected Semantic Information Gain reflects the amount of new knowledge regarding the environment that would be obtained by observing a specific voxel. Once the utility of the current pose is computed, the system checks if this pose represents the best viewpoint identified so far. Finally, the viewpoint with the highest utility is selected for the next pose, moving the robot accordingly."}, {"title": "IV. EVALUATION", "content": "We conducted experiments to evaluate our system in both real-world and simulated environments. ZSL segmentation with LSAM and YWES was tested independently and within the ROS 2 framework. Additionally, AV for 3D reconstruction in agricultural settings was evaluated using predefined zig-zag movements and our proposed NBV planning approach."}, {"title": "A. Experimental Setup", "content": "a) Robot Platform: The work utilizes the Igus ReBeL 6-DoF robotic arm [25], both in real-world and simulation settings. This lightweight, cost-effective arm is designed for collaborative tasks and offers flexibility through open-source control, though it has limitations in reach and precision due to its plastic construction.\n\nb) Real-world Implementation: A hardware interface [22] converts ROS 2 commands into signals for the robotic arm, while a Realsense D435 [26] camera provides environmental perception. Testing is conducted with an espalier apple tree setup, which is efficient for large-scale production due to its space-saving and yield-enhancing benefits in the primary sector.\n\nc) Simulation Setup: The simulation replicates the real-world configuration in Gazebo Ignition, using a hardware interface [27] [28] and virtual sensors that mimic the Realsense D435. It features virtual tomato plants at different vegetation stages to test the approach under controlled conditions similar to the physical setup.\n\nExperiments were conducted on a Dell XPS 13 9305 with 16 GiB of memory, an Intel i7-1165G7 processor, Intel Xe Graphics, and Ubuntu 22.04.4 LTS. The workstation also had ROS 2 Humble and Gazebo Ignition Fortress for simulations."}, {"title": "B. Experiments", "content": "It is important to note that a direct comparison with the state-of-the-art has not been feasible. This limitation arises primarily from the unavailability of open-source code provided by authors in the relevant literature. Additionally, the high complexity and specialized nature of these systems made it impractical to replicate each relevant system within the domain of agricultural robotics. Consequently, the experimental analysis is focused on evaluating the proposed system's components, specifically in terms of ZSL perception and NBV planning through ray-casting utility optimization across sim-to-real scenarios.\n\nThe initial experiments involved ZSL segmentation using LSAM and YWES models conducted outside our framework. These experiments focused on performing inference on a variety of fruit categories, including 'apple`, \u2018green apple', 'tomato`, `mature tomato`, and `berry`. Each model was evaluated using standard classification metrics: accuracy, precision, recall, and F1-score. Based on the results, the YWES model, which achieved the best overall performance, was selected for further deployment. Subsequent experiments integrated ZSL within our system during the 3D occupancy map creation process. This involved AV, where the robot captured multiple sensor images for segmentation across different viewpoints:\n\na) Simulation experiments: We tested the AV system on tomato plants characterized by multiple clusters of tomatoes. Four distinct scenarios were defined: (1) Full Occlusion (Fig. 3a), where vegetation fully obscured the fruits, (2) Multiple Grapes (Fig. 3b), where the tomato plant displayed an even distribution of grape clusters, (3) Single Grape (Fig. 3c), where only a single cluster of tomatoes was present on a spoiled plant, and (4) Unoriented Start (Fig. 3d), in which the robot arm began from a pose misaligned with the plant, thereby stressing the NBV planning algorithm's ability to optimize exploration and discover the grapes.\n\nb) Real-world experiments: We constructed an apple espalier setup in our laboratory with dense vegetation. Three scenarios were tested: (1) Full Occlusion, where apples were minimally visible even to the human eye, (2) Multiple Fruits, with a variety of apples distributed across the espalier wall, and (3) Unoriented Start, to ensure consistency with the simulation outcomes and validate the convergence capability of the NBV planning algorithm.\n\nIn both real-world and simulated environments, the AV system conducted 14 experiments for 3D reconstruction using predefined and NBV planning. Predefined planning involved eight zig-zag poses, while the NBV planning algorithm used a similar number of poses, except for the full occlusion real-world scenario with six poses. The AV pipeline's accuracy was assessed with an F1-score, comparing the ground truth to the reconstructed voxels in the semantic space."}, {"title": "C. Quantitative Results", "content": "Table I presents the performance of ZSL segmentation models in identifying various fruits using CPU. YWES significantly outperforms the others with faster inference times, while LSAM is about 4.5 times slower. The classification metrics demonstrate YWES's robustness in object identification across different forms, leading to its selection for deployment on the proposed system.\n\nTable II breaks down the quantitative results from simulated and real-world experiments. The number of fruits is evaluated through human visual inspection, showing that our system outperforms the handcrafted baseline in all scenarios. The best results are observed in the Full Occlusion and Unoriented Start scenarios. While Full Occlusion was the primary goal of this research, Unoriented Start further validates the need for informative guidance in recovery behaviors for agricultural tasks. Both multiple and single fruit scenarios showed similar scores in F1 Score and fruit counting, with our approach consistently surpassing the baseline. The ZSL and modular pipeline ensure consistent performance across different crops and operating environments.\n\nThe fruit count in some experiments was constrained by the fixed range of the robotic arm, despite using human-based counting for metrics assessment. Future work will focus on deploying the system on an agricultural robot to enhance coverage, control the mobile platform with NBV planning, and address the limitations of the robotic arm's operational space identified in this benchmark. Although all experiments were conducted on the CPU, as denoted by inference times in Table I, GPU acceleration has the potential to significantly improve inference in future research for real-time performance on a mobile platform."}, {"title": "D. Qualitative results", "content": "1) Full Occlusion (Fig. 3a): In the majority of planning steps, the NBV planning approach outperforms the baseline in informative exploration. The semantic 3D occupancy map is notably richer due to the extensive vegetation, as highlighted by the quantitative results.\n2) Multiple Grapes (Fig. 3b): The performance comparison is very close in this ideal scenario. While the baseline achieves similar results, it remains slightly inferior.\n3) Single Grape (Fig. 3c): Our approach shows poorer performance initially due to the grape's location in a dense, unknown area, requiring an initial entropy exploration phase. The baseline may find the cluster by chance. However, our method ultimately achieves a better F1 Score by the end of the planning steps.\n4) Unoriented Start (Fig. 3d): Starting from a recovery position, our method performs nearly twice as well as the baseline. As with the Full Occlusion benchmark, the semantic 3D occupancy map is notably richer, as reflected in the quantitative results.\n\nFig. 4 shows a qualitative comparison in the Unoriented Start scenario for real-world deployment. Starting from the same unoriented initial robot configuration, our approach effectively recovers a meaningful pose, outperforming the predefined approach. Additional qualitative results for real-world deployment are detailed in Table II and are available in the multimedia documentation of the manuscript."}, {"title": "V. CONCLUSION", "content": "We presented a benchmark for Active Vision in agricultural robotics, focusing on NBV planning. Our system was tested in both simulation and real-world scenarios, using ZSL to ensure perception independence. Future work will focus on deploying the system on a mobile agricultural robot and improving 3D reconstruction, potentially using techniques like Gaussian Splatting or NeRF to optimize Structure From Motion initialization and viewpoints selection.\n\nThese advancements can enhance agriculture by improving resource management and productivity, critical for meeting rising food demands. Accurate fruit counting in occluded environments is key for large-scale farming facing labor shortages. This research enhances agricultural environment perception via Active Vision and Zero-Shot Learning, automating repetitive tasks, and promoting efficiency and sustainability through robotics deployment."}]}