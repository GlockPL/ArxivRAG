{"title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "authors": ["Bj\u00f6rn Deiseroth", "Manuel Brack", "Patrick Schramowski", "Kristian Kersting", "Samuel Weinbach"], "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-FREE which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-FREE shows significant improvements in cross-lingual transfer learning.", "sections": [{"title": "From Text Representations For Machine Learning", "content": "Large language models (LLMs) have shown remarkable abilities in processing natural language and various data types. The tokenizer, an essential part of any language-based LLM, splits input text into subwords and converts textual data into integer representation. It is built by populating a fixed-size vocabulary based on statistical frequencies in a reference corpus (Sennrich, 2016; Kudo and Richardson, 2018). With the LLM's trained embedding layers, these integers are converted into floating-point representations (Mikolov et al., 2013b; Press and Wolf, 2017; Vaswani et al., 2017). These components significantly shape the training objectives and influence what an LLM can process, interpret, and generate. Despite advances, the basic principles of tokenization and embeddings have remained largely unchanged in recent years.\nAlthough this approach has served the LLM community well, and influential characters target to tokenize all kinds of data to \"lead a new industrial revolution\"\u00b9, it has significant inherent weaknesses. For one, tokenizers require dedicated training and, as such, additional computational resources. Design choices and errors at this stage can negatively impact the downstream model (Ali et al., 2023). Any tokenizer's vocabulary is heavily optimized for the reference corpus, leading to strong drops in performance for, e.g., underrepresented languages. We also show that the resulting vocabulary of tokenizers is poorly utilized, where up to 34% of tokens are near duplicates with limited additional information. Despite that, the corresponding embeddings are trained independently. These issues have caused a significant expansion in the size of vocabularies and their corresponding embedding layers, with billions of parameters being allocated exclusively for text encoding and decoding.\nTo remedy these issues and challenge the traditional views, we propose a paradigm shift on how we embed and decode text for LLMs. We present tokenizer-free sparse representations for memory-efficient embeddings (T-FREE) as an alternative to tokenizers. We directly embed each word in the input text with sparse activation patterns over hashed character triplets. Consequently, we eliminate the need for subword tokens, thus retaining near-optimal performance across languages. Additionally, T-FREE explicitly models character overlaps between morphologically similar words without the need to learn an embedding for each variant from scratch. We argue that the converged embedding of such similar words should remain close and, thus, can be heavily compressed\u00b2."}, {"title": "Classic Tokenization Principles", "content": "Before we derive T-FREE in detail, let us first establish some basics of how LLMs traditionally encode and decode text. Most LLM operations are performed in floating-point numbers through a series of matrix multiplications and non-linear activation functions. Consequently, we require techniques that map discrete textual inputs into floating-point representations and inversely transform the predictions of the model back to text.\nTraditionally, the first step in this process is to split any textual input into small chunks referred to as tokens. Generally, these tokens can take arbitrary formats, spanning numerous characters, a single or even multiple words, and may also contain special characters. The latter can be particularly useful to encode programming languages. A tokenizer comprises the steps and rules that are necessary to dissect a text into a sequence of tokens. Importantly, the total number of tokens is restricted, and we refer to the set of all unique tokens as the vocabulary. Each token in the vocabulary is assigned an integer token-id, wherefore tokenizers produce a sequence of token-ids for any textual input. Next, a large matrix of dimensions vocab size \u00d7 hidden size, an LLM's embedding layer, maps each token-id to an internal representation as a floating point vector (cf. Fig. 1a). To produce new text, generative models are trained auto-regressively. That is, they iteratively predict the next token, which is appended to the input text. Therefore, the training objective is formulated as a classification problem: a one-label prediction of the next token over the entire vocabulary. Consequently, the last layer of the model-the LM head\u2014is a projection into the size of the vocabulary and thus also of dimension vocab size \u00d7 hidden size. For decoding, we can, for example, always select the token with the highest assigned value, which is called greedy sampling. The output text is produced by looking up the corresponding text snippet of each predicted token-id in the vocabulary.\nGenerally, it is desirable to encode any text in as few tokens as possible to reduce computational cost. At the same time, different semantic concepts should be separated into distinct tokens to ensure good language comprehension. The combination of both objectives is usually best satisfied by encoding each word as one token."}, {"title": "Tokenizer Algorithms", "content": "The vast majority of LLMs utilize a tokenizer built with one of two approaches. Both progressively build up tokenization rules and their vocabulary based on statistics in a reference corpus.\nByte Pair Encoding (BPE). BPE (Sennrich, 2016) starts with a set of all characters as individual tokens. Progressively, the most frequent token pairs occurring together in the training documents are merged. The resulting new token and the merging rule are added, and the training is completed when the desired number of tokens is reached.\nIn order to encode text with the trained tokenizer, BPE splits the input into individual characters and applies the lowest-ranking merge rule until no more are applicable. This exhaustive search can become computationally intensive, especially for long input sequences and large vocabularies.\nUnigram. Unigram (Kudo and Richardson, 2018) operates inversely to BPE. First, it splits the training corpus into a large set of reference words and their respective frequencies. The vocabulary is initially populated with all possible substrings of these words. At each iteration, Unigram computes a loss of the current vocabulary with respect to the training corpus for all possible tokenizations. The least influential tokens are then removed until the desired vocabulary size is reached. For text encoding, the Viterbi algorithm is applied to determine the most preferred segmentation of a given word based on the ranked available tokens.\nThe text decoding in both cases maps directly back into the vocabulary list and the respective sub-words. To ensure that every word can be represented, a \"byte-fallback\" into unicode is often used for characters not present in the vocabulary."}, {"title": "Facing the Flaws", "content": "Common to both methods is a set of distinct flaws.\nLarge Vocabularies F1) Words that do not appear in the vocabulary are split into multiple tokens and, as such, require more compute during model inference and training. To avoid out-of-vocabulary words and to achieve the best downstream representations on a diverse set of languages and tasks, researchers tend to use ever larger vocabularies. Although some models still rely on a 32k vocabulary (Touvron et al., 2023; Jiang et al., 2023), more recent releases go up to 128k (Meta, 2024) or even beyond 250k (Mesnard et al., 2024; Gomez, 2024). Large vocabularies, in turn, require large embedding and head layers. For example, Command-R (Gomez, 2024) with a hidden dimension of 12, 288 and a vocabulary of 256, 000 tokens uses 6.3B parameters only for the embedding and head layer. Naturally, a large number of parameters complicate model training and may require advanced sharding techniques such as \u201cmodel parallelism\". Even the tokenization itself can become (CPU-) computationally intense for large documents and vocabularies. Naturally, embedding matrices of this scale are generally not an option for smaller \u201con-the-edge\" models. Nevertheless, they still occupy a large portion of parameters in smaller models, e.g. 40% for Gemma-2B (Mesnard et al., 2024).\nDuplicate Tokens F2) Furthermore, the allocated vocabulary is expected to be poorly utilized due to the statistically likely occurrence of near-duplicate tokens. Most prominently, a significant portion of tokens appears multiple times, only differing in capitalization or the existence of a leading whitespace (cf. Sec 4.3). For example, to spell all 64 substrings and variations of the word \u201c_words\"\u2075, we require a total of 37 unique tokens (cf. App. Tab. 7). Since the corresponding embeddings of all tokens are independent and randomly initialized, the representation of each duplicate token needs to be learned from scratch without exploiting morphological synergies. Further, large embedding layers are purely utilized since some tokens will rarely occur. The corresponding embedding weights of these tokens are thus seldom active while still requiring compute.\nTraining data overfitting F3) As discussed above, these tokenizers require dedicated training before the actual model training. In addition to the added computational overhead, the data selection and potential mistakes during tokenizer training have significant impact on the subsequent LLM (Ali et al., 2023). For natural language, for example, this paradigm may result in a vocabulary tailored to one language (usually English) and consequently drops in performance for others, especially those not explicitly included. The resulting LLM may still be somewhat adapted to other languages since many similar low-level structures (Mikolov et al., 2013a). However, its overall training and inference performance will not be as efficient as we demonstrate.\nIn contrast, T-FREE addresses all of these disadvantages. It is computationally efficient and performs good tokenization across languages without duplicates. It drastically reduces the parameters required for text encoding, exploiting word spelling similarities. Importantly, none of these improvements sacrifices downstream model performance."}, {"title": "T-FREE", "content": "A key motivation for T-FREE is the intuition that minor differences in spelling, like leading whitespaces or capitalization, do not hold enough entropy to justify entirely independent tokens. T-FREE directly encodes morphological similarities by representing each word as a multi-label encoding of its character triplets. This designed overlap between words allows us to significantly reduce the size of embedding layers.\nWe now derive T-FREE's approach to text encoding and decoding and discuss implications on LLMs in general. We provide a visualization of each step in Fig. 1b and pseudo-code in App. A."}, {"title": "Text Encoding", "content": "Step 1: Word splitting. First, we rigorously split the text by digits and non-alphanumeric characters. The resulting splits, therefore, contain entire words, digits, or special characters. We consider each digit separately, as it is standard in SOTA LLMs (cf. Tab. 1). Specifically, we include the 10 digits 0 to 9, and otherwise, we rely on attention to comprehend larger numbers or mixtures with characters.\nBy definition, we represent each word with a prefixed and suffixed whitespace. In particular, we assume that an entire word is encoded into a single embedding, and analogously, we predict an entire word at once. Consequently, we no longer need to explicitly model whitespace as a character and eliminate near-duplicate tokens. Nonetheless, we add a dedicated \u201cwhitespace\u201d and \u201cnon-whitespace\" token to the tokenizer. These special tokens allow us to model cases where substrings should (not) be concatenated with whitespace, e.g., single digits of larger numbers. To reduce their need, we further add a rule-set that favors (non-)whitespace in front or after certain characters. Generally, we prefer to add no whitespace after a digit embedding and similarly no whitespace before punctuation. A detailed description of the rule set is found in App. B.\nConsidering the example in Fig. 1b, the input text \"Hello_word!\" would be tokenized as ['Hello', 'word','!'].\nStep 2: Encoding. Next, we define a robust hash function that uniformly encodes a token into n descriptors, where n usually equals the word-length\u2076. Specifically, we apply convolutions of size three and byte-wise stride to each word. This operation yields a set of character triplets, which we refer to as \"trigrams\". Consequently, \u201cHello\u201d is decomposed into {_He,Hel,ell,llo,lo_}. Trigrams usually contain enough information about the relationship between letters to reassemble the word from the unordered set.\nSubsequently, we project each trigram descriptor into a sparse hidden representation vector of m \u201cactive entries\" on the embedding layer. Specifically, T-FREE calculates m numerical hashes of each trigram, which can be considered as identifiers. We map these into the LLMs embedding matrix by calculating each hash value modulo v to identify the active indices. The selection of vocab size v is further explained in Step 3.\nOverall, we obtain n\u00b7m total activations for any single word. To further exploit word similarities and bootstrap training, we calculate k \u2208 [0,m) out of these hash calculations with the lowercased trigram. This mapping from trigram to hidden representation is static and can be precomputed\u2077."}, {"title": "Training Objective & Text Decoding", "content": "As T-FREE's representation of a word is now a multitude of activations, we reflect this change in the LM head, as well (cf. Decode sections in Fig. 1, App. Alg. 3,5). In particular, we change the target loss function from classic single-label binary cross-entropy (BCE) to a multi-label (ML) BCE over all $n \\cdot m$ activations of the next word targets:\n$L_{BCE}^{ML}=-\\sum_{j=1}^{n \\cdot m}[y_{j} \\log (\\hat{y}_{j})+(1-y_{j}) \\log (1-\\hat{y}_{j})]$,\nfor \u0177 being the LM's prediction and y the binary target vocab labels with $\\sum_{j=1} Y_{j} = n \\cdot m$.\nAnalogously, for decoding the next token with T-FREE, we first assemble a dictionary of all possible next words and pre-compute their activation patterns. Importantly, only $n \\cdot m$ out of v entries will be non-zero for each word, and since we choose $m << v$, the dictionary matrix can be encoded as a sparse matrix, thus improving performance. We multiply this dictionary matrix with the predicted logits values of the LLM to finally obtain the argmax prediction of the next token."}, {"title": "Bridging the Gaps", "content": "Notably, this paradigm shift to a multi-class vocabulary allows for more semantically robust decoding. With the classical approach, the distinctly noisy learning process can lead to unrelated concepts appearing among the top predictions (cf. \u2018House' and 'Car' in Fig. 1a). This effect can have a significant impact on next token sampling and potentially devastative outcomes for model modifications such as compression (Deiseroth et al., 2024). In contrast, the trigrammification and resulting embedding overlap of similar words with T-FREE inherently favors similar words during decoding (cf. 'ouse' in Fig. 1b). Moreover, activations in the embedding and LM head are more uniformly distributed, leading to better parameter utilization, and more stable model behavior."}, {"title": "Empirical Evaluations", "content": "We continue with an empirical demonstration of the performance of T-FREE, and how it remedies the flaws of standard tokenizers as outlined in Sec. 2.2. To thoroughly analyze the performance differences, we designed three consecutive experiments: First, we perform hyperparameter ablations on a series of 1B parameter models, which achieve competitive scores on standard benchmarks with a reduced vocabulary, which in turn addresses F1. Second, we analyze the duplicates in the tokenizers of recent LLMs with respect to F2. Notably, T-FREE is by design free of duplicates. Lastly, we look at F3 and evaluate the performance of various tokenizers across languages. Further, we trained 3B parameter models on English and continued training on German data to practically investigate language adaptability. T-FREE has better tokenization performance across languages and outperforms classic tokenizers on language transfer."}, {"title": "Experimental Details", "content": "First, let us clarify some details about our experimental setup. We provide more details for each section in the Appendix.\nData and Code. We use the slimpajama dataset (Soboleva et al., 2023) as our English and Occiglot Fineweb v0.5 (Brack et al., 2024) as our German data corpus. Both datasets contain a diverse range of content and have been extensively filtered and deduplicated.\nAs a baseline, we trained BPE and Unigram tokenizers of sizes 32k and 64k on a random 20GB slimpajama sample using Sentencepieces. More details are described in App. \u0421.\nTo ensure fair comparisons, we trained 1B and 3B models from scratch for the baselines and T-FREE using our adjusted code base"}, {"title": "LLM Pre-Training", "content": "All models are transformer, decoder-only architectures similar to Llama-2. We solely change the tokenizer, embedding layer and LM head. Consequently, ablations with smaller sizes of v result in a lower overall parameter count, heavily skewing the comparison in favor of the baseline. For hyper-parameter ablations of T-FREE,"}, {"title": "T-FREE performs at 8k vocab size", "content": "We present the results of our hyperparameter ablation study of T-FREE for 1B models in Fig. 2. All scores are reported as differences to the Unigram 64k baseline and for fixed parameters m = 10 and k = 0. Generally, T-FREE remains highly competitive with the baseline as all versions outperform the Unigram model on some of the benchmarks. Further, we achieve the best results for a vocab size v of 8k at which T-FREE outperforms the baseline on average. In contrast, a vocab size of < 2k seems insufficient with devastating outliers. We performed further ablations on parameters m and k, which are outlined in App. H.\nThese results demonstrate that T-FREE successfully addresses the flaw of large vocabularies and embedding layers (cf. F1 in Sec. 2.2). We are able to achieve competitive performance with only 12.5%\u00b9\u00b2 of the embedding parameters using T-FREE instead of Unigram.\nNote, that we do not adjust any other model parameters when reducing vocab size. As such, the benchmark results compare a Unigram model with 1.07B parameter against a T-FREE model with 0.84B parameters (for v = 8k). Consequently, we demonstrate that an LLM using T-FREE instead of Unigram performs better, despite having over 20% fewer parameters."}, {"title": "T-FREE has by design no duplicates", "content": "Let us now look into (near) duplicate tokens in commonly used tokenizers (cf. F2 in Sec. 2.2). In general, there are three types of overlaps in vocabularies: 1) The same token with and without capitalization, 2) with and without leading whitespace, and 3) dedicated tokens for multiple digits.\nIn Tab. 1, we report the percentage of duplicate tokens for our baseline tokenizers and commonly used models. Overall, between 15% and 35% of the available vocabulary is spent on (near) duplicate information with limited differences in entropy. Generally, tokenizers contain the most duplicates for capitalization, slightly fewer for whitespaces, and only a few duplicate digits. The relative amount of overlap tends to decrease with larger vocabularies, although Gemma marks an inglorious exception. In contrast, T-FREE is inherently designed to be free of duplicates. We can even adjust the parameter k to explicitly model the overlap of words to their lowercase representations. Consequently,"}, {"title": "T-FREE has less fertility across, and is more adaptive to new languages", "content": "Finally, we investigate the versatility of tokenizers beyond their (main) language (cf. F3 in Sec. 2.2). We report the fertility of our baselines and other popular models in English, German, and three dissimilar languages that also contain significant character-level differences in Tab. 1. Common to all tokenizers is a significantly decreasing performance for non-English languages, especially for Russian and Vietnamese. Naturally, larger vocabulary sizes tend to have better multilingual coverage, in particular to language groups close to English, but still suffer from significant performance drops. In comparison, the tokenization of T-FREE, which is mainly based on whitespace splitting, provides comparably good performance across all 5 languages\u00b9\u00b3. The increases in fertility for Russian or Vietnamese remain small and there is no performance difference for German or Arabic. Note that these synergies were explicitly modeled, and no reference corpus is needed to train and bias the fertility of T-FREE. Consequently, T-FREE allows for easier and more efficient model adaptation to low-resource languages.\nWe now explicitly show the devastating consequences of biased tokenizers on the language transfer capabilities of LLMs. As discussed above, we first train 3B models for T-FREE and Unigram on English, and then transition to German. Through more ablations, we fixed the activations to m = 7 and the lowercase trigram overlap to k = 3. Fig. 3 shows the performance average on the English and German versions of the standard benchmarks."}, {"title": "Discussion", "content": "Prior research has demonstrated that the mapping into a sparse hidden representation and the training of a dense aggregation layer as applied in T-FREE, is a universal function approximator (Aved'yan, 1995). These results provide further theoretical motivation for our approach.\nT-FREE allows for significant compression of an LLMs' vocabulary by more than 85% without performance degradation. Notably, the affected embedding and head layers are by far the largest in LLMs in terms of parameter count. They are also the most influential to an LLM, as they dictate the mapping between text and numerical representations. For one, these massive improvements allow for better utilization of billions of parameters in large models. The compression of T-FREE in particular paves the way to building better low-resource models, by reducing model size and training cost and improving adaptability. For example, in our experiments without pipe or model-parallelism, we were able to triple the micro-batch size, yielding faster training iterations.\nFurthermore, we observed more stable loss curves for T-FREE, in particular for higher learning rates. These improvements may be attributed to the explicit modeling of similar words, the removal of duplicates, and the less volatile multi-label training target. Further, the uniform hashing distributes gradients evenly amongst the available vocab size, in contrast to classical approaches. We provide further details in App. D,G.\nThe rules we use for obtaining word representations are universal and well-defined at pre-training time. They do not change over time, particularly neither when adding languages later on. T-FREE also lowers computational costs due to its low fertility and easy-to-process whitespace splitting. Consequently, pre-processing, training and inference of an LLM all require less compute.\nLastly, T-FREE allows to explicitly model and steer the decoding process at inference time, by altering the available dictionary. Consequently, hallucinations will likely be reduced due to fewer \"generic fall-back\u201d word splits. Moreover, one can dynamically add or remove words. It is worth pointing out that T-FREE's compression benefits can also be combined with traditional tokenizers. Instead of the simple whitespace splitting one could keep traditional tokenization and trigramify \"classic tokens\"."}, {"title": "Related Work", "content": "Few alternatives to BPE and Unigram have been found in recent LLMs and research. The naive approach of splitting the input text into bytes or characters maximizes fertility and thus increases computational requirements. Consequently, prior research has proposed methods for merging bytes, e.g., through state-space models (Wang et al., 2024). However, these approaches still result in performance degradation. Finally, linguistically motivated approaches have built tokenizers based on known morphological rules (Jabbar, 2024). However, these methods are usually tailored to specific applications and are usually too costly and error-prone for large, general-purpose models.\nOther works on weight tying, have halved the parameters of embedding and head layers by using the same matrix in both (Press and Wolf, 2017). Currently, LLMs do not apply weight tying, though, due to its negative impact on performance and the available compute."}, {"title": "Conclusion", "content": "In this work we present T-FREE, an alternative to tokenizers with a simple and explicitly modeled robust hash function on words. It removes the need and pitfuls to limit \u201ca models potential\u201d to a \"pre-pre-trained\u201d tokenizer. We, moreover, fundamentally shift the established target of training language models, previously designed as a single-label problem, into a multi-label prediction based on word similarities. Similarities in particular include leading whitespaces and uppercase variations, for which tokenizers add specific tokens that are independently trained from scratch. These contributions allow us to train language models more robust, more adaptable when continuing pre-training with a new language, and with a significantly (to 12.5%) reduced parameter size without a decrease in benchmark scores. Due to the special role of the matrices, the latter in particular allows one to increase micro-batchsize, which further accelerates training time. Finally, the consequent convolution-like encoding achieves SOTA fertility scores across most languages and enables by design synergies to similar language groups. We demonstrated the latter showing that our 3B almost achieved \u201cnative-language\" performance after a small amount of language-transfer training steps, in contrast to the tokenizer baseline."}, {"title": "Limitations", "content": "With T-FREE we propose a fundamentally different approach to text encoding and decoding in LLMs. Due to the intense resources required to train LLMs, we have focused on evaluating models up to 3B parameters. Evaluations on even larger models and training datasets remain a relevant point of investigation for future work. Nonetheless, we observed an easy transfer from 1B to 3B parameters, and we will continue to train and release more advanced models.\nWe expect T-FREE to experience some numerical instabilities for very long words since single-word embeddings are calculated as the sum of their nm activations. However, less than 2% of the entire slimpajama dataset contains words with more than 10 characters (cf. App. I), and we did not encounter any issues with the benchmarks. Consequently, such potential instabilities remain statistically insignificant. Nonetheless, we could adequately tackle long outliers with an additional split rule based on the words length.\nSimilarly, we did not thoroughly study the effect of repetitive trigrams in words. These did also not occur frequently enough to have any measurable effect on our experiments. As of now, we only accumulate a word pattern in a binary fashion, not accounting for trigrams appearing multiple times in a single word. As a fallback, one could again, split words at the position of repetitions.\nAlthough T-FREE's fertility on code is on par with that of LLama2 (cf. App. E), it could be further improved by explicitly modeling code patterns. In this work, we have focused on natural language and leave detailed evaluations of T-FREE in downstream coding tasks for future research. Furthermore, we did not investigate languages entirely relying on Unicode byte-encodings, such as Chinese.\nFinally, we only studied a single constructed hash function for T-FREE. As this work paves the way to model required language features more explicitly, we are looking forward to variations of the proposed T-FREE method."}, {"title": "T-FREE Algorithm", "content": "Alg. 1,2,3,4,5 show the core steps to encode text into embeddings, and decode text from model predictions with T-FREE. Here, regex.split denotes an algorithm that splits text based on a regular expression, hash denotes an arbitrary hash function like md5, % denotes the mathematical modulo operation. In style of python, f'{token}_' denotes text formatting to indicate the string with content of variable token being followed by an underscore, and EL[i] denotes the i-th entry of matrix EL and 'string'[i : i + 3] three consecutive characters in the text string starting from position i, where 's' is at position 0. Finally, v \u2248 8,000 is the chosen vocabulary size, d \u2248 100,000 is the chosen dictionary size, h \u2248 3,072 the LLMs hidden size. Finally, Oh denotes a zero vector of dimension h and 1vxd a matrix with entries 0 or 1. Note that we included some normalization steps in Alg. 5, which we surprisingly found not beneficial for Alg. 3 in our ablations."}, {"title": "Whitespace encoding", "content": "By default our model is trained to predict full words separated by whitespaces. To not be limited to this use-case, we add a special \u201cnon-whitespace\" and \"whitespace\" token. We empirically evaluated each exception occuring in code tokenization. To further reduce its fertility, we favor \"non-whitespace\" before one of the following characters:\n$.,;:#?!= -+*/\\() < > [ ] &@%_~^\nWe further prefer non-whitespace after one of the following characters:\n#$=-+*/'\\\"(<[~^&@%_\\n1234567890\nAs such, the text \"In 2024\" would result in the split \"[In,2,0,2,4]\" without the need of any special annotations, while \u201cIn20 24\" resolves to \u201c[In,<no_ws>,2,0,<ws>,2,4]\u201d.\nFinally, to further improve code fertility, we merge consecutive <ws> and newline tokens up to 3 times, i.e. 8 consecutive whitespaces would result in a single <|8<ws>|> token."}, {"title": "Tokenizer trainings with sentencepiece", "content": "For training of a unigram tokenizer with the current sentencepiece library, a 20GB reference data corpus reaches the limit of our available 1TB Ram compute node. We thus randomly sample 20GB of the slimpajama dataset and run the following statement for training of the actual tokenizer:\nspm_train --input=20GB_sample.txt\\\n--model_prefix=unigram_64k \\\n--vocab_size=64000 \\\n--character_coverage=0.99 \\\n--model_type=unigram \\\n--byte_fallback=true \\\n--split_by_number=true \\\n--split_by_whitespace=true \\\n--train_extremely_large_corpus=true \\\n--split_digits=true \\\n--allow_whitespace_only_pieces=true \\\n--remove_extra_whitespaces=false \\\n--normalization_rule_name=nfkc \\\n--num_threads 64 --eos_id=0 \\\n--bos_id=-1 --unk_id=2 \\\n--pad_id=1 \\\n--eos_piece=\"<lendoftext|>\" \\\n--pad_piece=\"<|padding|>\" \\\n--unk_piece=\"<|unknown|>\""}, {"title": "Training Configurations", "content": "Training Parameters are listed in Tab. 2."}, {"title": "Fertility Analysis", "content": "We subsequently provide further experimental details on the fertility analysis conducted with respect to F3, Sec. 4.4. As a reference dataset, we used the November 23 dump of Wikipedia in the respective languages. We derived reference tokenization using UDPipe (Straka, 2018). A tokenizer's fertility is then calculated by dividing its total token count for a document by the number of tokens produced"}, {"title": "Training stability", "content": "Memory footage comparing classic tokenizers to T-FREE is found in Fig. 5.\nNote that the hashing step of Alg. 2 uniformly distributes gradients amongst the available vocabulary, as discussed in Sec. 5. This is in contrast to classic tokenizers, as they depend on a bijective single-label mapping, and as such each vocabulary entry update is dependent on its the occurance frequency of the corresponding token within the dataset. Moreover, we explicitly let trigram activations overlap with their lowercase version. We assume that these are responsible for the more stable training dynamics as shown in Fig. 4. Moreover, we found that the lowercase overlap bootstraps learning as shown with the downstream benchmark ablations Fig. 7."}, {"title": "Hyperparameter Ablations", "content": "Some 1,500 determined experiments later...\nAlbeit pretty scarse, some more hyper-parameter ablations are found in Fig. 6,7.\nWe will continue to polish and add more..."}, {"title": "Some Statistics", "content": "Trigram combinatorics. As there are more than v possible words, there will naturally be some overlap in the activations between words. However, assuming an embedding dimension of v \u2248 8,000, m \u2248 8 activations per trigram, and a word of length n = 5, there are (in theory) (nm) \u2248 10\u00b9\u2070\u2078 unique activation patterns.\nThis overlap can be interpreted as an interpolation between input states. For entirely independent inputs, this overlap should be kept small as the results cannot benefit from the states of the shared activations. As such, we require a robust hash function on text, i.e. a mapping from text into sparse activation patterns, for which the overlapping of activations is proportional to the similarity of the input words. We model this through trigrams, and as such, letter-similarity."}, {"title": "More Benchmarks", "content": "We used the code of the eleuther eval harness, and evaluated each benchmark in O-shot and 2-shot. All 18 benchmarks are found in Fig. 11 and Fig. 12."}]}