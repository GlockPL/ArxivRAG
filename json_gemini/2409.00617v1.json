{"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "authors": ["Yifan Wei", "Xiaoyan Yu", "Yixuan Weng", "Huanhuan Ma", "Yuanzhe Zhang", "Jun Zhao", "Kang Liu"], "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in intermediate layers. This study investigates the differences between entity and relational knowledge through knowledge editing. Our findings reveal that entity and relational knowledge cannot be directly transferred or mapped to each other. This result is unexpected, as logically, modifying the entity or the relation within the same knowledge triplet should yield equivalent outcomes. To further elucidate the differences between entity and relational knowledge, we employ causal analysis to investigate how relational knowledge is stored in pre-trained models. Contrary to prior research suggesting that knowledge is stored in MLP weights, our experiments demonstrate that relational knowledge is also significantly encoded in attention modules. This insight highlights the multifaceted nature of knowledge storage in language models, underscoring the complexity of manipulating specific types of knowledge within these models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), trained on extensive knowledge corpora such as Wikipedia, encapsulate a vast amount of factual knowledge and demonstrate exceptional performance in various natural language tasks. Consequently, LLMs are often regarded as knowledge bases that underpin knowledge-oriented tasks. However, leveraging the knowledge within these models effectively requires understanding the mechanisms by which LLMs store and manage factual knowledge. This understanding is crucial for tasks such as model editing, which involves modifying the knowledge embedded in the models.\nCurrent studies have focused on studying the knowledge embedded in LLMs. These works have considered knowledge in the form of triplets (s, r, o), which include the head entity (subject, s), tail entity (object, o), and their relation r, as shown in Figure 1. They have examined how language models encapsulate knowledge in their parameters. For instance, Dai et al. employed a knowledge attribution method and identified specific neurons that express factual knowledge, while Meng et al. used causal tracing to find strong causality between subjects and the MLP module. However, these studies primarily investigate the knowledge in LLMs from the entity perspective. The total different observations could be conducted if we address the same knowledge from the relation. Theoretically, a piece of knowledge includes both entities and their relations; without either, it is incomplete. Therefore, entities and relations are supposed to be equivalent in this context, a premise upon which much current work in model editing is based, given the need to modify knowledge in the model parameters.\nNevertheless, current studies have not yet explored whether such equivalence stands. To fill this gap, we investigate the differences between entity and relation in this paper. To explore this potential equivalence, we employ model editing, a technique for updating or correcting new or erroneous knowledge in language models. We aim to determine whether changes yield consistent outcomes by modifying entity or relational knowledge, observing the effects from both perspectives. Ideally, the effects should be identical since the edited knowledge pertains to the same piece. To further elucidate the differences in where relational and entity knowledge is stored, we examine how relational knowledge is stored in auto-regressive transformer models. We employ causal analysis to explore the relationship between relational knowledge and the various modules of LLMs. Our probing leads to two surprising conclusions: (1) factual knowledge is not stored as a single unit; relations and entities are represented separately within the model parameters, as simply illustrate in Figure 1(b); (2) editing from entity and relational perspectives does not yield the same outcomes, which means the previous located knowledge neurons in previous work are questionable.\nThe findings in this work have profound implications for understanding and utilising LLMs in knowledge representation and model editing. This revelation challenges the validity of existing evaluation methods that assess the success of model edits based on this flawed assumption of equivalence. By revealing these discrepancies, our work provides a new foundation for future research and development in LLM-related tasks, such as model editing."}, {"title": "2 Related Work", "content": "As factual information continues to evolve, the knowledge stored in large language models (LLMs) can become outdated or incorrect. Hence, there is an urgent need to facilitate timely updates of inappropriate knowledge in LLMs while preserving other valuable knowledge. Recently, this issue has garnered significant attention from researchers. Certainly, both parameter-efficient fine-tuning and incremental learning techniques provide avenues for modifying LLMs. However, it is essential to note that these approaches may be prone to overfitting and can incur substantial computational costs, especially when applied to LLMs with an extremely large parameter scale. To address these issues, Sinitsin et al. proposes Model Editing, which aims to efficiently and accurately alter the factual knowledge stored within models. Presently, there are three primary types of model editing approaches: 1) Memory-based Method: These techniques utilize additional trainable parameters to store memory or learn the required adjustments (\u0394) for knowledge updating in the LLMs. 2) Locate-Then-Edit Method: These approaches employ causal mediation analysis to locate knowledge neurons in LLMs and subsequently modify these recognized regions. This paper primarily explores this knowledge localization method. 3) In-Context Knowledge Editing Method: These methods are a training-free paradigm where knowledge editing is achieved directly by concatenating demonstrations within the input context. This paper primarily explores the second type, the Locate-Then-Edit method."}, {"title": "3 Background & Methodology", "content": null}, {"title": "3.1 Task Definition", "content": "Assume that knowledge K = {x,y} is stored in language model in the form of triples (s, r, o). The objective of model editing is to modify a base model $f_\u03b8$, parameterized by \u03b8, which maps the text prompt P as input x to gain control over the model's prediction outputs y, expressed as:\n$f_\u03b8(x) = \\underset{\u03b8}{argmax} p_\u03b8 (y | P)$.\nTo modify the prediction results, model editing aims to update the model parameter $\u03b8^*$ with $f(x; \u03b8^*) = y^*$. Editing reliability is needed to change prediction from y to $y^*$."}, {"title": "3.2 Model Editing Methods", "content": "To explore the connection between model parameters and knowledge, we apply model editing techniques to modify the parameters of transformer-based language models. In this section, we describe the model editing methods applied.\nTo modify specific knowledge K in a model, we adjust the model weight parameters W associated with K. The objective is to optimize the hidden states of both the Attention and MLP components. The target weight W is defined as:\n$\\underset{W}{argmin} \\sum_{i=1}^{n} ||W_{ki} - v_i||^2 + \\sum_{i=n+1}^{n+u} ||W_{ki} \u2013 v_i||^2$\nwhere $k_i$ represent the knowledge index vector obtained through the i-th prompt $x_i$ and $v_i$ represent the target knowledge representation. $\\sum_{i=1}^{n} ||W_{ki} - v_i||^2$ indicates the retention of n pieces of"}, {"title": "3.3 Locating Relation Knowledge", "content": "Casual Tracing. To locate the relations r within factual triplets (s, r, o) in model parameters, we analyze and identify the knowledge neurons with the strongest causal effect on these relations. We employ causal tracing for this purpose, following this procedure:\nStep 1 Clean run. A factual prompt x is passed into the model $f_\u03b8$ and collect all hidden activations {$h^{(l)}_i$ | i \u2208 [1, T], l \u2208 [1,L]}, where T is number of input tokens in x and L is number of layers.\nStep 2 Corrupted run. The relation embeddings [$h^{(0)}_1$, $h^{(0)}_2$, ..., $h^{(0)}_r$] are obfuscated by adding a term \u03f5 to each $h^{(0)}_i$, where \u2208 ~ N(0, v) and v is set to three times the empirical standard deviation of the embeddings. This results in a set of corrupted activations {$h^{(l)}_i$ | i \u2208 [1, T], l \u2208 [1, L]}.\nStep 3 Corrupted-with-restoration run. The model $f_\u03b8$ perform computations on the noisy embeddings, as in the corrupted baseline. However, at a specific token \u00ee and layer \u00ce, $f_\u03b8$ is intervened to output the clean state $h^{(l)}_i$. After this point, all subsequent computations proceed without further intervention.\nWe define  P[y], $P^*[y]$, and $P^*_{i, cleah^{(l)}_i}$ [y] is defined as the probability of final prediction y under the clean, corrupted, and corrupted-with-restoration runs, respectively. The indirect effect (IE) of a particular hidden state hi is calculated as:\nIE = P*$_{i, cleanh^{(l)}_i}$ [y] - $P^*[y]$.\nSevered Causal Analysis. To gain a clearer understanding of the impact of MLP and Attention layers, we perform severed causal tracing analysis using a modified causal graph, following [11]. In the corrupted-with-restoration-run, we freeze the MLP and Attention modules to the corrupted run value so that it's unaffected by the inserting of clean state $h^{(l)}_i$. This can viewed as severing the MLP and Attention computations from the original computation graph. The propagation of noise in the model follows:\n$h^{(l)}_i$ = $h^{(l-1)}_i$ + sever($a^{(l)}_i$, $m^{(l)}_i$)\n$a^{(l)}_i$ = attn($h^{(l)}_i$ , $h^{(l-1)}_i$,...,$h^{(l-1)}_i$))\n$m^{(l)}_i$ = $W^{(l)}_{proj}f_\u03b8(g(\u03b8^{(l)}(a^{(l)}_i, h^{(l-1)}_i))$),\nwhere the function sever(.) denotes the server operation, which separates the MLP or Attention computations from the model."}, {"title": "4 Experiments", "content": "To investigate how knowledge is stored within model parameters, we outline the following Research Questions (RQs):\n\u2022 RQ1: Where is relational knowledge stored? Is it stored in the same manner as entity knowledge within MLPs?\n\u2022 RQ2: Are relation and entity knowledge equally significant in knowledge triplets, regardless of their storage location?"}, {"title": "4.1 Experimental Setups", "content": "In the experiments, we use GPT-2 XL (1.5B) and GPT-J (6B) as the base language models. The experiments are conducted with four NVIDIA RTX A6000 GPUs and ten NVIDIA GeForce RTX 3090 GPUs. The evaluation metrics includes Reliability and Generality.\nReliability quantifies the reliability of the editing process, with higher reliability indicating greater success in editing. To measure reliability, we assess the editing accuracy as follows:\n$M_{rel} = E_{(x,y*)~D} [1_{f(x;\u03b8^*(x,y*))=y*}]$,\nGenerality measures the generalization ability of the edited model's predictions across various inputs or contexts.\n$M_{gen} = E_{(x)~N(x)} [1_{f(x;\u03b8^*)=f(x;\u03b8^*)=y*}]$,\nwhere x refers to the rephrased text prompt, N(x) denotes a set of rephrased prompts equivalent to x."}, {"title": "4.2 RQ1: Causal Analysis for Relation", "content": "We conducted causal tracing analysis to determine the location of relational knowledge within model parameters, with the results illustrated in Figure 2. The procedure of causal tracing analysis is outlined in Section 3.3. By varying the mediator across different positions within the prompt and different model components (such as individual states, MLP layers, and attention layers), we calculated the average indirect effect (AIE) across 1207 factual statements. The results show that, consistent with prior findings [11, 12], there is a high AIE score in the last layers of the final token. This indicates that restoring the hidden states of the MLPs in these layers recovers most of the necessary information. Additionally, we observed a high AIE score in the earlier layers for the intentionally corrupted relation tokens, underscoring the importance of these early layers in predicting plausibility.\nSimilarly, we noted a pronounced AIE in the middle attention layers of the last corrupted token. We found that the knowledge storage location identified by the relation r in the knowledge triples is strongly correlated with both MLP layers and attention layers, as shown in Figure 3. This conclusion differs from previous works identifying knowledge storage in lower MLP layers via entity localization. We discover that knowledge expression localized through relations is closely associated with higher MLP layers and mid-to-upper attention layers. When exploring model knowledge expression from an entity perspective to a relation perspective, the causal locations of knowledge expression in the model change significantly. This indicates that the storage location of knowledge in the model parameters is complex and cannot be simply determined by causal tracing from a single perspective, assuming knowledge is isolated in specific model layers. Therefore, we believe that modifying the corresponding model parameters to control the expression of knowledge through such localization is unreasonable."}, {"title": "4.3 RQ2: Probing the Equivalence", "content": "Under the assumption that entity and relation perspectives are logically equivalent in knowledge triplets, as illustrated in Figure 3, entity knowledge and relational knowledge are considered interchangeable. Based on this assumption, we hypothesize that modifying entity knowledge by altering relational knowledge is theoretically possible. To validate this hypothesis, we apply model editing techniques to modify knowledge in language models from relational and entity perspectives and observe whether the effects remain the same.  Contrary to our assumption, we are surprised the evaluation score for entity lags far from that for relation. Editing relation knowledge achieves high metrics for relation, indicating that these editing methods are effective. However, the results for entity knowledge are noticeably lower, suggesting that editing relation does not effectively alter entity knowledge. This is puzzling because entities and relations within the same triplet define a piece of knowledge. Altering any part of the triplet should theoretically alter the entire triplet, implying equivalence.\nThe results show that the evaluation results are relatively stable with rather minimal fluctuation. The reliability of relation knowledge has improved, but there is a significant decrease in the generality metrics. These findings suggest that model editing from an entity perspective can potentially alter the relation information between pieces of knowledge. Howerver, changes are inconsistent.The above findings indicate that editing entity knowledge and relation knowledge are not exactly equivalent."}, {"title": "5 Conclusion", "content": "This paper reveals that relational knowledge in LLMs is encoded not only in MLP layers but also significantly in attention modules. This finding contrasts with previous assumptions that knowledge is primarily stored in MLP weights. Our analysis demonstrates that entity and relational knowledge are stored separately within LLMs, highlighting the complexity of knowledge storage mechanisms. These insights are crucial for improving model interpretability and developing advanced knowledge-based applications. Furthermore, our findings provide a new view for future research and development in LLM-related tasks, such as model editing."}]}