{"title": "A Joint Learning Model with Variational Interaction for Multilingual Program Translation", "authors": ["Yali Du", "Hui Sun", "Ming Li"], "abstract": "Programs implemented in various programming languages form\nthe foundation of software applications. To alleviate the burden\nof program migration and facilitate the development of software\nsystems, automated program translation across languages has gar-\nnered significant attention. Previous approaches primarily focus\non pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel\ndata is difficult to collect for some language pairs, and the distri-\nbution of program semantics across languages can shift, posing\nchallenges for pairwise program translation. In this paper, we argue\nthat jointly learning a unified model to translate code across multi-\nple programming languages is superior to separately learning from\nbilingual parallel data. We propose Variational Interaction for Mul-\ntilingual Program Translation (VIM-PT), a disentanglement-based\ngenerative approach that jointly trains a unified model for mul-\ntilingual program translation across multiple languages. VIM-PT\ndisentangles code into language-shared and language-specific fea-\ntures, using variational inference and interaction information with a\nnovel lower bound, then achieves program translation through con-\nditional generation. VIM-PT demonstrates four advantages: 1) \u0441\u0430\u0440-\ntures language-shared information more accurately from various\nimplementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel\ndata, 3) addresses the distribution shift of program semantics across\nlanguages, 4) and serves as a unified model, reducing deployment\ncomplexity.", "sections": [{"title": "CCS CONCEPTS", "content": "\u2022 Software and its engineering \u2192 Automatic programming."}, {"title": "KEYWORDS", "content": "Program Translation, Multi-lingual Disentanglement, Variational\nInteraction, Regularization"}, {"title": "1 INTRODUCTION", "content": "Programs form the foundation of computer applications. Various\nprogramming languages have been invented to address diverse\nrequirements. Developing a new application from scratch is time-\nand labor-intensive, whereas referring to and combining existing\ncode is more efficient. Unfortunately, when combining programs\nwritten in different programming languages, the developer usu-\nally suffers from the intensive labor of manually translating pro-\ngrams from one programming language to another. For example,\nmany industries spend hundreds of millions of dollars to convert\ncode written in older programming languages (e.g., FORTRAN\nand COBOL) to newer ones (e.g., Java, C++) [35]. To alleviate the\nburden of program migration and facilitate the development of\nsoftware systems, program translation, which aims to automati-\ncally translate the program from one programming language to\nanother, has drawn significant attention in the software mining\ncommunity [2, 8, 16, 21, 35, 44, 45, 48, 50, 51].\nThe booming development of machine learning coupled with the\navailability of an extensive parallel corpus of programs has led to a\nremarkable enhancement in the performance of program transla-\ntion. For example, Nguyen et al. [26, 27] attempted to translate code\nacross languages, leveraging phrase-based statistical machine trans-\nlation and grammatical rules. Recently, due to the potent power of\nrepresentation learning in the deep neural networks, most modern\nprogram translation approaches based on deep neural networks,\nsuch as sequence-to-sequence models, have advanced the state-of-\nthe-art performance to a new level [7, 9, 13].\nNevertheless, most existing approaches primarily focus on pair-\nwise translation, lacking exploration in multilingual program trans-\nlation. Although pairwise approaches can be employed for each\nlanguage pair separately, these approaches cannot leverage knowl-\nedge from languages outside the current pair, posing a challenge\nfor learning across a pair of languages with only a few parallel data.\nAs shown in Fig. 1, previous approaches, including the multilin-\ngual program translation approach by Zhu et al. [51], are trained\nonly with bilingual parallel data. However, these parallel data are\nsparse in practical datasets. For instance, the multilingual dataset\nCOST [51] contains only 12.56% bilingual parallel data among all"}, {"title": "2 RELATED WORKS", "content": "2.1 Program Translation\nTo meet some specific requirements, rule-based translation meth-\nods have been developed [5, 6, 34, 42]. Further, some works ap-\nplied phrase-based statistical machine translation techniques to\nprogram translation [4, 15, 26-31], which leveraged grammatical\nstructures of programming languages for code migration. Recently,\nvarious deep learning techniques were employed to program trans-\nlations [20, 46, 47], which can be distinguished into pairwise pro-\ngram translation methods [7, 9, 13] and multilingual program trans-\nlation methods [51] by using bilingual or multilingual corpus in\nthe supervised learning.\nThe pairwise approaches trained one model for each translation\ndirection independently [13, 23, 40]. However, a problem encoun-\ntered by many existing models is that program translation datasets\nare usually not balanced in size for all the languages. Some lan-\nguages may have much less parallel data than others. Less parallel\ntraining data can significantly affect the translation performance\nof low-resource languages. Therefore, some works leveraged multi-\nlingual training to improve the performance of low-resource lan-\nguages, which is defined as multilingual program translation. How-\never, the existing multilingual program translation model MuST-\nPT [51] is limited to the pairwise training process and does not take\nfull advantage of the multi-parallel data and partially missing data\n(e.g., non-parallel data)."}, {"title": "2.2 Exploring Non-Parallel Data for Translation", "content": "The quality of translation systems highly depends on the quality of\nthe available parallel data. However, for most languages, parallel re-\nsources are rare or nonexistent. Some approaches have investigated\nthe use of non-parallel data to improve existing program trans-\nlation systems [12, 19, 35, 36, 41, 43]. For instance, Lachaux et al.\n[19] proposed DOBF that leverages the structural aspect of coding\nlanguages and pre-trains a model to recover the original version of\nobfuscated source code. Feng et al. [12] proposed CodeBERT, which\nis a Bert-like model pre-trained in an open-source GitHub reposi-\ntory and used in many down-stream tasks, and then much relevant\nresearch emerged [10, 13]. Rozi\u00e8re et al. [35] proposed TransCoder,\nwhich trains a translation model only using the monolingual corpus\nwith back-translation and denoising auto-encoding.\nA common shortcoming of the above methods is the language-\nshared and language-specific representations among different lan-\nguages are not disentangled in the learning. As shown in many\nresearches, the disentanglement is crucial in such a way that ev-\nery factor of variation is captured in the right part of the rep-\nresentation so that the partially missing data can be better uti-\nlized [11, 14, 22, 32, 37-39, 49]. For example, in neural machine\ntranslation, Zheng et al. [49] proposed the mirror-generative ap-\nproach, which is a single unified architecture in which both trans-\nlation models and language models share the same latent semantic\nspace, therefore both translation directions can learn from non-\nparallel data. Yet the method is limited to bilingual translation and\nrequires extra language models with more space overhead [49].\nInspired by the discussion on cross-domain disentanglement [14],\nwe expand the variational disentanglement to the multi-domain,\nand it is the first attempt to explore the variational disentanglement\nin multi-lingual translation among programming languages."}, {"title": "3 METHOD", "content": "The method is discussed in this section, including the generative\nmodel by variational inference, enforcing disentanglement by inter-\naction information, the overall framework of VIM-PT, and training\nthe joint model from both multi-parallel samples and partially miss-\ning samples."}, {"title": "3.1 Generative Model via Variational Inference", "content": "Consider a sample program implemented in N languages, denoted\nas $(x^1, x^2,...,x^N) \\sim P_D(x^1, x^2,...,x^N)$, where $x^i$ represents the\ncode written in the $i$-th programming language. All of these N\ncodes exhibit the same semantics, i.e., perform the same task, while\neach code $x^i$ involves the language-specific grammatical style. From\nthe perspective of disentanglement, each code is generated through\nthe interplay of a language-shared feature, denoted as $z_s \\in \\mathcal{Z}$,\nand a language-specific feature, denoted as $z^i \\in \\mathcal{Z}^i$ for the $i$-th\nprogramming language.\nAs illustrated in Fig. 2, the generative process involves two steps:\n1) The shared feature $z_s$ is generated from the prior distribution\n$p_{\\theta^*}(z_s)$, while the language-specific feature $z^i$ is generated from\n$p_{\\theta^*}(z^i)$. 2) Subsequently, the code $x^i$ is generated from the condi-\ntional distribution $p_{\\theta^*}(x^i|z_s, z^i)$. Following Kingma and Welling\n[18], these prior distributions $p_{\\theta^*}(z_s)$ and $p_{\\theta^*}(z^i)$, as well as the\nconditional distribution $p_{\\theta^*}(x^i|z_s, z^i)$, are drawn from parametric"}, {"title": "3.2 Disentanglement by Interaction Information", "content": "In ideal disentanglement, the decomposed features should satisfy\ntwo key properties: 1) The language-shared feature $z_s \\in \\mathcal{Z}_s$ and the\nlanguage-specific grammatical style feature $z^i \\in \\mathcal{Z}^i$ should avoid\ncapturing redundant information. 2) For any code implemented in\nthe i-th language, $x^i \\in \\mathcal{X}^i$, the common information between $x^i$ and\nothers $(x^1,..., x^{i-1}, x^{i+1},...,x^N)$ should be the language-shared\ninformation, i.e., $z_s \\in \\mathcal{Z}_S$.\nTo achieve the first property, we employ the mutual information\n$I(\\cdot;\\cdot)$ between $Z^s$ and $Z^i$ to quantify redundancy:\n$I(Z^i; Z^s) = -I(X^i; Z^i, Z^s) + I(X^i; Z^i) + I(X^i; Z^s).$   (4)\nThe proof of this equation can be found in Appendix A.1.\nTo achieve the second property, we use interaction information,\na generalization of mutual information among three or more ran-\ndom variables, to quantify the common information among the\ncode implemented in the i-th programming language, the codes\nimplemented in other languages, and the language-shared fea-\ntures. For brevity, we denote ${x^1, \\dots, x^{i-1}, x^{i+1},\\dots,x^N}$ as $\\mathcal{X}^{\\bar{i}}$ and\n${x^1,\\dots,x^{i-1},x^{i+1}, \\dots, x^N }$ as $\\mathcal{X}^{\\bar{i}}$. Formally, the common infor-\nmation can be expressed as:\n$I(X^i; \\mathcal{X}^{\\bar{i}}; Z_s) = I(X^i;Z_s) - I(X^i; Z_s |\\mathcal{X}^{\\bar{i}}),$  (5)\nwhich can be derived from the definition of the interaction infor-\nmation between three random variables.\nThen, we combine Eq. 4 and Eq. 5 to enforce disentanglement:\n$I(X^i;\\mathcal{X}^{\\bar{i}}; Z_s)-I(Z^i; Z^s) = -I(X^i; Z_s |\\mathcal{X}^{\\bar{i}})+I(X^i;Z^i, Z^s)-I(X^i;Z^i).$  (6)\nUnfortunately, directly maximizing Eq. 6 is still intractable. Let\nus analyze it term by term.\n\u2022 The first term involves $q(z_s|x^{\\bar{i}}) = \\int P_D(x^{\\bar{i}}|x^i)q(z_s|x^1,...,x^N)d\\mathcal{x}^{\\bar{i}}$,\nwhere $p_D(x^{\\bar{i}}|x^i)$ is unknown. Therefore, using the variational\ndistribution $r^i (z_s|x^i)$, the first term can be lower bounded as"}, {"title": "3.3 The overall framework of VIM-PT", "content": "The VIM-PT is architecture-free, which can theoretically and prac-\ntically be adapted to arbitrary sequence-to-sequence architecture,\nwhich involves an encoder and a decoder. Based on the experi-\nmental performance, we finally set up a unified encoder for all\nthe languages with weight-sharing and an independent decoder\nfor each programming language. As illustrated in Figure 4, given a\nmulti-parallel sample $(t^1, \\dots, t^N)$ from the dataset $p_D (t^1,\\dots, t^N)$,\nwhere each sample including a group of codes of the same function\nimplemented by the N different programming languages. The flag"}, {"title": "3.4 Learning from multi-parallel samples", "content": "The disentangle module involves three approximate posteriors $q_{\\phi^1}$,\n$r_{\\phi^1}$, and $p_{\\phi^1}$, and a language-shared approximate posterior $q_{\\phi^s}$. Each\napproximate posterior is parameterized by a standard Gaussian\nprior $z \\sim \\mathcal{N}(0, I)$.\nThe flag representations of instances can be disentangled by\nthe corresponding language-specific projectors $(q_{\\phi^1},\\dots,q_{\\phi^N})$ and\nlanguage-shared projectors $q_{\\phi^s}$, respectively. In addition, the shift-\nshared representations are inferred by corresponding shift-shared\nprojectors $(r_{\\phi^1},\\dots, r_{\\phi^N})$. Then the language-specific representa-\ntions and the language-shared representation are inputted into\n$(\\rho_{\\phi^1},\\dots,\\rho_{\\phi^N})$ to reconstruct the representations of the multi-\nlingual instances.\nThe language-specific representations, the language-shared rep-\nresentation, and the shift-shared representations are denoted by"}, {"title": "3.5 Learning from partially missing samples", "content": "For partially missing samples, we design a training strategy to\nexploit the data that is not multi-parallel. The training strategy is\ndescribed in Algorithm 1.\nAt first, given a partially missing sample from the dataset, there\nmay be some missing instances in one sample. To obtain the pseudo\ninstances, we disentangle the language-specific representations and\nthe language-shared representations to generate the draft transla-\ntion of the missing instances. We sample a set of pseudo instances\n$(t^1,\\dots,t^N)$ from the training dataset randomly to fill the missing\ninstances of the corresponding language in the original sample. And\nthe flag representations can be separated as $(x^1,\\dots,x^N)$. Similar\nto the learning of multi-parallel samples, the language-specific rep-\nresentations, and the shift-shared representations are disentangled\nas $(z^1,\\dots,z^N)$, and $(\\mathcal{Z}_1,\\dots,\\mathcal{Z}_s)$. However, as some instances of\nthe sample are missing, the language-shared representation can not\nbe inferred.\nTo overcome this issue, the method generates the draft trans-\nlation of the missing instances with the pseudo instances. Each\nlanguage-specific representation of the pseudo instance and the\nlanguage-shared representation of ground truth are inputted into\n$(\\rho_{\\phi^1},\\dots,\\rho_{\\phi^N})$ to reconstruct the representations of the multi-\nlingual instances. As the language-shared representation of ground\ntruth is not unique, for each language-shared representation of\n($\\mathcal{Z}_1,\\dots,\\mathcal{Z}_s$), a multi-parallel sample $(x^1, \\dots, \\hat{x}^N reconstructed by\nlanguage-shared information and language-specific information\ncan be obtained. Then the learning of multi-parallel samples can be\nperformed. Especially, only the target programs with ground truth\nare reconstructed."}, {"title": "4 EXPERIMENTS", "content": "4.1 Dataset Description\nWe conduct experiments on the CoST dataset [51], which is a large\nand comprehensive dataset to evaluate the performance of program\ntranslation approaches. The dataset consists of both snippet-level\nand program-level parallel data from 7 programming languages (i.e.,\nC, C#, C++, Java, JavaScript, PHP, and Python) and up to 42 program-\nming language pairs, which was collected from the GeeksForGeeks\nwebsite [1]. The platform ensures its contributors stick to a tem-\nplate in terms of the comments used in their programs and the code\ncorresponding to those comments. The dataset provides a good\nnumber of multilingual instances of code that can be effectively\nused for this task. The detailed statistics of the dataset are shown\nin Table 1, while the train, validation, and test sets are split the\nsame as CoST [51]. For each language, there are some instances of\ncorresponding samples that are missing, especially in C and PHP."}, {"title": "4.2 Experimental Settings", "content": "We choose the following state-of-the-art multi-lingual and pairwise\nprogram translation methods as baselines. All the encoder-only pre-\ntrained models are fine-tuned on the task with the same decoder as\nour methods."}, {"title": "4.3 RQ1: What is the Performance of VIM-PT\nComparing With Baseline Approaches?", "content": "In the experiment, we use the BLEU [33] score as the evaluation met-\nric to evaluate the n-gram overlap between the translated code and\nthe ground-truth target code, which is the most widely used metric\nin program translation [13, 19, 25, 51]. A higher BLEU score indi-\ncates better evaluation performance, which varies from 0 to 100 as a\npercentage. Table 3 shows the experimental results. For each pair of\ntranslations, the three best performing approaches are highlighted,\nwhile dark gray marks an approach with the highest retrieval per-\nformance, and gray and light gray correspond to the second- and\nthird-best approaches. Compared with the pairwise approach like\nCodeBERT with a similar backbone, the multilingual approaches\nincluding MuST-PT, VIM-PT(w/o VT), and VIM-PT perform signifi-\ncantly outperformance. We can attribute this result to (1) expansion\nof the representation scope, as the different language corpora face\nvarious application scenarios and have different functional pref-\nerences, (2) better latent space, which can be jointly constructed"}, {"title": "4.4 RQ2: What is the impact of Variational\nInteraction for the VIM-PT?", "content": "To evaluate the effectiveness of the variational interaction, we set a\nblank controller termed M-PT(w/o VI), which ablates variational in-\nteraction on VIM-PT. As shown in Table 3 and Table 4, the inclusion\nof Variational Interaction (VI) in VIM-PT results in an improvement\nin the average BLEU-4 score at both snippet Level and program\nLevel. Across most programming language pairs, VIM-PT achieves\nhigher performance than M-PT(w/o VI), indicating that variational\ninteraction indeed enhances translation quality.\nIn summary, the incorporation of VI effectively enhances the per-\nformance of the translation model, resulting in more accurate and\nfluent translations. The Improvement in Table 4 indicate the average\nimprovement of the variational interaction across all languages and\nlevels, with an average increase from 4.05% to 11.14%, and from\n1.65% to 10.47% at the program level and snippet level, respectively.\nThe variational interaction leads to notable improvements espe-\ncially in low-resource languages like C and PHP, indicating that\nvariational interaction plays a crucial role in enhancing the perfor-\nmance of the low-resource languages in multilingual translation."}, {"title": "4.5 RQ3: Which rich-resource languages benefit\nthe low-resource languages?", "content": "Due to the diversity between different programming languages,\nwe explore the auxiliary benefits of different programming lan-\nguages as rich-resource to the translation between low-resource\nprogramming languages. As shown in Figure 7, with auxiliary of any\nother programming languages, the performance of each direction\nof program translation has obtained a significant improvement. It\nis because when importing the third programming language, more\ndata are utilized in the learning and the language-shared latent\nspace can be constructed better. The base indicates the pair-wised\napproach based on CodeBERT and the others are the translation\nwith the help of the other languages based on the framework of\nVIM-PT. The auxiliary benefits of different programming languages\nmay depend on the following possible factors."}, {"title": "4.6 RQ4: What is the deployment complexity of\nthe joint model and pairwise approaches?", "content": "To compare the deployment complexity between the pairwise ap-\nproach and VIM-PT, we record the space overhead of the translation\nmodel. As shown in Figure 8, the pairwise approach and VIM-PT\nare both constructed with CodeBERT as encoder and the 6-layer\nTransformer as Decoder. With the number of languages in transla-\ntion increasing, the number of parameters of the pairwise approach\ngrows exponentially, and VIM-PT grows linearly.\nIt is because the encoder-decoder models needed by the pairwise\napproach are coupling with the number of translation directions,\nbut the decoders needed by VIM-PT are coupling with the number\nof languages. In the bilingual program translation, the pairwise\napproach should construct two models for the two directions in\ntranslation, and the number of parameters of the pairwise approach\nand VIM-PT is similar. When the multilingual translation gradually\nincreases to 7 languages, the pairwise approach should construct"}, {"title": "5 THREAT ANALYSIS", "content": "Our results are interpreted with two threats to validity in mind.\n\u2022 The internal threat to validity lies in the implementation of com-\npared techniques. To reduce it, we directly reuse the implementa-\ntion of the compared techniques from their reproducible packages\nand the weights of pre-trained models, if they are available and\nexecutable. Otherwise, we reimplement the techniques strictly\nfollowing the papers on existing mature libraries.\n\u2022 The external threat to validity lies in the dataset used in the\nexperiment. To mitigate the external threat, the widely-used\ndataset is used to evaluate the effectiveness of the method, which\nincludes 7 programming languages and up to 42 pairs and two\nlevels. The artifact contains the dataset and the code is publicly\navailable in the supplementary material."}, {"title": "6 CONCLUSION", "content": "In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior\nto separately learning from bilingual parallel data. We propose Vari-\national Interaction for Multilingual Program Translation (VIM-PT),\na disentanglement-based generative approach that jointly trains a\nunified model for multilingual program translation across multiple\nlanguages. VIM-PT disentangles code into language-shared and\nlanguage-specific features using variational inference and interac-\ntion information with a novel lower bound. With the variational\ninteraction, VIM-PT achieves significant improvement in multi-\nlingual program translation, mines and leverages the capability\nof non-parallel data, addresses the distribution shift of program\nsemantics across languages, and serves as a unified model, reduc-\ning deployment complexity. In the future, more effective selection\nstrategies in filling the partially missing samples can be explored\nbeyond the random selection used in our method."}, {"title": "A APPENDICES", "content": "A.1 $I(Z^i; Z^s) = -I(X^i; Z^i, Z^s) + I(X^i; Z^i) + I(X^i; Z^s)$\nWith the interaction information between three random variables:\n$I(X; Y; Z) = I(X; Z) \u2013 I(X; Z|Y) = I(Y; Z) \u2013 I(Y; Z|X),$  (14)\nwe can obtain the mutual information between $Z^i$ and $Z^s$:\n$I(Z^i, Z^s) = I(Z^i;X^i) \u2013 I(Z^i; X^i|Z) + I(Z^i; Z |X).$  (15)\nDue to the structural assumption on q, the q(z^i|xi) = q(zi|xi, zs)\nholds, so the last term in the above function is eliminated:\n$I(Z^i; Z\\X^i) = H(Z^i\\X^i) \u2013 H(Z^i\\X^i, Z^s)$  (16)\n= $H(Z^i\\X^i) \u2013 H(Z^i\\X^i) = 0,$\nwhich yields\n$I(Z^i, Z^s) = I(X^i; Z^i) \u2013 I(X^i; Z^iZ^s)$  (17)\n= $I(X^i; Z^i) + I(X^i; Z^s) \u2013 I(X^i; Z^i, Z^s).$"}]}