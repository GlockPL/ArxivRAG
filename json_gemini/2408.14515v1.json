{"title": "A Joint Learning Model with Variational Interaction for Multilingual Program Translation", "authors": ["Yali Du", "Hui Sun", "Ming Li"], "abstract": "Programs implemented in various programming languages form the foundation of software applications. To alleviate the burden of program migration and facilitate the development of software systems, automated program translation across languages has garnered significant attention. Previous approaches primarily focus on pairwise translation paradigms, learning translation between pairs of languages using bilingual parallel data. However, parallel data is difficult to collect for some language pairs, and the distribution of program semantics across languages can shift, posing challenges for pairwise program translation. In this paper, we argue that jointly learning a unified model to translate code across multiple programming languages is superior to separately learning from bilingual parallel data. We propose Variational Interaction for Multilingual Program Translation (VIM-PT), a disentanglement-based generative approach that jointly trains a unified model for multilingual program translation across multiple languages. VIM-PT disentangles code into language-shared and language-specific features, using variational inference and interaction information with a novel lower bound, then achieves program translation through conditional generation. VIM-PT demonstrates four advantages: 1) captures language-shared information more accurately from various implementations and improves the quality of multilingual program translation, 2) mines and leverages the capability of non-parallel data, 3) addresses the distribution shift of program semantics across languages, 4) and serves as a unified model, reducing deployment complexity.", "sections": [{"title": "1 INTRODUCTION", "content": "Programs form the foundation of computer applications. Various programming languages have been invented to address diverse requirements. Developing a new application from scratch is time- and labor-intensive, whereas referring to and combining existing code is more efficient. Unfortunately, when combining programs written in different programming languages, the developer usually suffers from the intensive labor of manually translating programs from one programming language to another. For example, many industries spend hundreds of millions of dollars to convert code written in older programming languages (e.g., FORTRAN and COBOL) to newer ones (e.g., Java, C++) [35]. To alleviate the burden of program migration and facilitate the development of software systems, program translation, which aims to automatically translate the program from one programming language to another, has drawn significant attention in the software mining community [2, 8, 16, 21, 35, 44, 45, 48, 50, 51].\nThe booming development of machine learning coupled with the availability of an extensive parallel corpus of programs has led to a remarkable enhancement in the performance of program translation. For example, Nguyen et al. [26, 27] attempted to translate code across languages, leveraging phrase-based statistical machine translation and grammatical rules. Recently, due to the potent power of representation learning in the deep neural networks, most modern program translation approaches based on deep neural networks, such as sequence-to-sequence models, have advanced the state-of-the-art performance to a new level [7, 9, 13].\nNevertheless, most existing approaches primarily focus on pairwise translation, lacking exploration in multilingual program translation. Although pairwise approaches can be employed for each language pair separately, these approaches cannot leverage knowledge from languages outside the current pair, posing a challenge for learning across a pair of languages with only a few parallel data. As shown in Fig. 1, previous approaches, including the multilingual program translation approach by Zhu et al. [51], are trained only with bilingual parallel data. However, these parallel data are sparse in practical datasets. For instance, the multilingual dataset COST [51] contains only 12.56% bilingual parallel data among all possible pairs. In addtion, as shown in Fig. 5 and Fig. 6. the distribution of different language pairs is imbalanced.\nIn this paper, we argue that jointly training a unified model to translate code across multiple programming languages is superior to separately training with pairwise data. A program implemented in different programming languages should exhibit the invariant language-shared semantics, i.e., perform the same tasks. Thus, joint training on multiple different implementations of a program can complement and enhance the learning of the language-shared representation. By constructing the unified language-shared latent space, the rich resource languages can benefit the low resource languages in the translation. However, joint learning involves a new challenge: as shown in Fig. 1, practical datasets always is semi-parallel, which contain only a few complete multi-parallel samples that include all languages, while most are partially missing implementations in some programming languages. To tackle the issue of semi-parallel data, utilizing semi-supervised learning techniques is intuitive. Generative semi-supervised learning offers a solution: all data, including partially missing and multi-parallel data, can be generated from a latent distribution. All these data can be used to learn the latent distribution and generative process with an Expectation-Maximization (EM)-based algorithm [17]. Once the latent distribution and generative process are learned, program translation can be achieved with a conditional generative process.\nBuilding upon the above idea, we propose an innovative approach called Variational Interaction for Multilingual Program Translation (VIM-PT), a disentanglement-based generative approach. VIM-PT disentangles each code into language-shared features and language-specific features based on information theory. The language-shared features should be task-specific and language-invariant like the functional semantics of the program, while the language-specific features should be language-specific and task-invariant like the grammar or syntax of the programming language. Therefore, each code can be generated through the interplay of these features. Specifically, we employ variational inference to learn the generative process, where the prior distributions of both features follow a normal Gaussian distribution. To enforce disentanglement, there will be a new variational interaction bound for the objective, compared to the Evidence Lower Bound (ELBO) in traditional variational inference, which includes three additional terms from interaction information. By optimizing this new variational interaction bound, VIM-PT learns to generate code based on disentangled features. This enables program translation from a source language to a target language by using the language-shared feature from the source language and the language-specific feature sampled from the prior distribution of the target language.\nCompared to previous approaches, VIM-PT demonstrates four strengths: 1) More precisely and completely captures the language-shared feature by jointly learning from the various views of multiple implementations and improves the quality of multilingual program translation. 2) Effectively leverages the power of non-parallel and partially missing data using a generative framework. 3) Addresses the distribution shift of semantics across multiple languages. Different languages are often used for different tasks, such as Python for data science and machine learning, and JavaScript for front-end development, resulting in a distribution shift of semantics in collected data. VIM-PT addresses this by using a conditional generative framework to complete missing implementations in some languages. 4) VIM-PT is a unified model across different translation pairs, reducing the total parameters and making deployment more convenient, especially on edge devices. To evaluate the effectiveness of the VIM-PT, an extensive experiment is conducted on the widely used dataset, which includes 7 general programming languages (i.e., C, C#, C++, Java, Javascript, PHP, and Python). It can be observed that VIM-PT performs performance gains widely over the state-of-the-art approaches.\nWe highlight our contributions in three key aspects:\n(1) We argue that jointly training a unified model to translate code across multiple programming languages is superior to training with pairwise data separately.\n(2) We propose VIM-PT, a disentanglement-based generative approach, for joint learning with variational interaction in multilingual program translation.\n(3) We conducted extensive experiments to demonstrate the effectiveness of VIM-PT and the necessity of jointly learning a unified model in multilingual program translation."}, {"title": "2 RELATED WORKS", "content": "To meet some specific requirements, rule-based translation methods have been developed [5, 6, 34, 42]. Further, some works applied phrase-based statistical machine translation techniques to program translation [4, 15, 26-31], which leveraged grammatical structures of programming languages for code migration. Recently, various deep learning techniques were employed to program translations [20, 46, 47], which can be distinguished into pairwise program translation methods [7, 9, 13] and multilingual program translation methods [51] by using bilingual or multilingual corpus in the supervised learning.\nThe pairwise approaches trained one model for each translation direction independently [13, 23, 40]. However, a problem encountered by many existing models is that program translation datasets are usually not balanced in size for all the languages. Some languages may have much less parallel data than others. Less parallel training data can significantly affect the translation performance of low-resource languages. Therefore, some works leveraged multilingual training to improve the performance of low-resource languages, which is defined as multilingual program translation. However, the existing multilingual program translation model MuST-PT [51] is limited to the pairwise training process and does not take full advantage of the multi-parallel data and partially missing data (e.g., non-parallel data)."}, {"title": "2.2 Exploring Non-Parallel Data for Translation", "content": "The quality of translation systems highly depends on the quality of the available parallel data. However, for most languages, parallel resources are rare or nonexistent. Some approaches have investigated the use of non-parallel data to improve existing program translation systems [12, 19, 35, 36, 41, 43]. For instance, Lachaux et al. [19] proposed DOBF that leverages the structural aspect of coding languages and pre-trains a model to recover the original version of obfuscated source code. Feng et al. [12] proposed CodeBERT, which is a Bert-like model pre-trained in an open-source GitHub repository and used in many down-stream tasks, and then much relevant research emerged [10, 13]. Rozi\u00e8re et al. [35] proposed TransCoder, which trains a translation model only using the monolingual corpus with back-translation and denoising auto-encoding.\nA common shortcoming of the above methods is the language-shared and language-specific representations among different languages are not disentangled in the learning. As shown in many researches, the disentanglement is crucial in such a way that every factor of variation is captured in the right part of the representation so that the partially missing data can be better utilized [11, 14, 22, 32, 37-39, 49]. For example, in neural machine translation, Zheng et al. [49] proposed the mirror-generative approach, which is a single unified architecture in which both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data. Yet the method is limited to bilingual translation and requires extra language models with more space overhead [49]. Inspired by the discussion on cross-domain disentanglement [14], we expand the variational disentanglement to the multi-domain, and it is the first attempt to explore the variational disentanglement in multi-lingual translation among programming languages."}, {"title": "3 METHOD", "content": "The method is discussed in this section, including the generative model by variational inference, enforcing disentanglement by interaction information, the overall framework of VIM-PT, and training the joint model from both multi-parallel samples and partially missing samples."}, {"title": "3.1 Generative Model via Variational Inference", "content": "Consider a sample program implemented in N languages, denoted as $(x^1, x^2, ..., x^N) \\sim P_D(x^1, x^2, ..., x^N)$, where $x^i$ represents the code written in the $i$-th programming language. All of these N codes exhibit the same semantics, i.e., perform the same task, while each code $x^i$ involves the language-specific grammatical style. From the perspective of disentanglement, each code is generated through the interplay of a language-shared feature, denoted as $z_s \\in Z$, and a language-specific feature, denoted as $z^i \\in Z^i$ for the $i$-th programming language.\nAs illustrated in Fig. 2, the generative process involves two steps: 1) The shared feature $z_s$ is generated from the prior distribution $p_{\\theta^*}(z)$, while the language-specific feature $z^i$ is generated from $p_{\\theta^*}(z^i)$. 2) Subsequently, the code $x^i$ is generated from the conditional distribution $p_{\\theta^*}(x^i | z^i, z_s)$. Following Kingma and Welling [18], these prior distributions $p_{\\theta^*}(z_s)$ and $p_{\\theta^*}(z^i)$, as well as the conditional distribution $p_{\\theta^*}(x^i | z^i, z_s)$, are drawn from parametric families of distributions $p_{\\theta}(z_s)$, $p_{\\theta}(z^i)$, and $p_{\\theta}(x^i | z^i, z_s)$, respectively. Therefore, as per Kingma and Welling [18], the generative process can be formally expressed through the marginal likelihood of the joint distribution in the multilingual Bayesian network:\n$P_{\\theta}(x^1, x^2, ..., x^N) = \\int P_{\\theta}(z_s)dz \\prod_{i=1}^N P_{\\theta}(z^i)p_{\\theta}(x^i | z^i, z_s)dzi$. (1)\nWhile Eq. 1 is intractable, variational inference based on Variational Autoencoders (VAE) is typically employed [14, 17, 18]. Considering the inference process in variational inference, as shown in Fig. 3, the language-shared feature $z_s$ can be jointly inferred from all codes implemented in various programming languages, while the language-specific grammatical style feature $z^i$ is inferred from the code implemented in the $i$-th programming language.\nThus, the inference process can be formally expressed via the posterior distribution $p_{\\phi}(z^1, z^2, ..., z^N, z_s | x^1, x^2, ..., x^N)$, which can be approximated by:\n$q_{\\phi}(z^1, z^2, ..., z^N, z_s | x^1, x^2, ..., x^N) = q_{\\phi}(z^1 | x^1)q_{\\phi}(z^2 | x^2) ... q_{\\phi}(z^N | x^N)q_{\\phi}(z_s | x^1, x^2, ..., x^N)$. (2)\nBy combining Eq. 1 and Eq. 2, and omitting subscripts $\\theta$ and $\\phi$ for brevity, the log-likelihood of the joint distribution can be lower-bounded using the ELBO as:\n$\\log p(x^1, x^2, ..., x^N) \\geq E_{q(z^1, ..., z^N, z_s | x^1, ..., x^N)} \\log \\frac{p(x^1, ..., x^N, z^1, ..., z^N, z_s)}{q(z^1, ..., z^N, z_s | x^1, ..., x^N)}$\n$ = \\sum_{i=1}^N E_{q(z^i | x^i)q(z_s | x^1, ..., x^N)} [\\log p(x^i | z^i, z_s)] \\newline - \\sum_{i=1}^N D_{KL}[q(z^i | x^i) || p(z^i)] - D_{KL}[q(z_s | x^1, ..., x^N) || p(z_s)]$, (3)"}, {"title": "3.2 Disentanglement by Interaction Information", "content": "In ideal disentanglement, the decomposed features should satisfy two key properties: 1) The language-shared feature $z \\in Z^s$ and the language-specific grammatical style feature $z^i \\in Z^i$ should avoid capturing redundant information. 2) For any code implemented in the $i$-th language, $x^i \\in X^i$, the common information between $x^i$ and others $(x^1, ..., x^{i-1}, x^{i+1}, ..., x^N)$ should be the language-shared information, i.e., $z \\in Z^S$.\nTo achieve the first property, we employ the mutual information $I(;)$ between $Z^s$ and $Z^i$ to quantify redundancy:\n$I(Z^i; Z^s) = -I(X^i; Z^i, Z^s) + I(X^i; Z^i) + I(X^i; Z^s)$. (4)\nThe proof of this equation can be found in Appendix A.1.\nTo achieve the second property, we use interaction information, a generalization of mutual information among three or more random variables, to quantify the common information among the code implemented in the $i$-th programming language, the codes implemented in other languages, and the language-shared features. For brevity, we denote $\\{x^1, ..., x^{i-1}, x^{i+1}, ..., x^N\\}$ as $x_{\\i}$ and $\\{x^1, ..., x^{i-1}, x^{i+1}, ..., x^N\\}$ as $X_{\\i}$. Formally, the common information can be expressed as:\n$I(X^i; X_{\\i}; Z^s) = I(X^i; Z^s) - I(X^i; Z^s | X_{\\i})$, (5)\nwhich can be derived from the definition of the interaction information between three random variables.\nThen, we combine Eq. 4 and Eq. 5 to enforce disentanglement:\n$I(X^i; X_{\\i}; Z^s) - I(Z^i; Z^s) = -I(X^i; Z^s | X_{\\i}) + I(X^i; Z^i, Z^s) - I(X^i; Z^i)$. (6)\nUnfortunately, directly maximizing Eq. 6 is still intractable. Let us analyze it term by term.\n*   The first term involves $q(z_s | x_i) = \\int P_D(x^i | x_i)q(z_s | x^1, ..., x^N)dx_i$, where $p_D(x^i | x_i)$ is unknown. Therefore, using the variational distribution $r^i(z | x^i)$, the first term can be lower bounded as follows:\n$\\begin{aligned} -I(X^i; Z^s | X_{\\i}) &= -E_{p_D(x^1, ..., x^N)q(z_s | x^1, ..., x^N)} \\log \\frac{q(z_s | x^1, ..., x^N)}{q(z_s | x_{\\i})} \\\\ &= -E_{p_D(x^1, ..., x^N)q(z_s | x^1, ..., x^N)} \\log \\frac{q(z_s | x^1, ..., x^N)}{\\int q(z_s | x_{\\i})r^i(z_s | x^i)} \\\\ &= -E_{p_D(x^1, ..., x^N)} \\left[ D_{KL} \\left[ q(z_s | x^1, ..., x^N) || r^i(z_s | x^i) \\right] \\right] + E_{p_D(x^i)} \\left[ D_{KL} \\left[ q(z_s | x_{\\i}) || r^i(z_s | x^i) \\right] \\right] \\\\ &\\geq -E_{p_D(x^1, ..., x^N)} \\left[ D_{KL} \\left[ q(z_s | x^1, ..., x^N) || r^i(z_s | x^i) \\right] \\right] . \\end{aligned}$ (7)\nThus, when maximizing \u2013$I(X^i; Z^s | X_{\\i})$ using Eq. 7, the variational distribution $r^i(z_s | x^i)$ will be learned to fit $q(z_s | x^1, ..., x^N)$.\n*   Meanwhile, the second term $I(X^i; Z^i, Z^s)$ involves $q(x^i | z^i, z_s) = \\frac{q(z^i, z_s | x^i)p_D(x^i)}{p_D(x^i, ..., x^N)} \\int p_D(x^i, ..., x^N)q(z^i, z_s | x^i, ..., x^N)dx_{\\i}$ , where $p_D(x^i, ..., x^N)$ and $p_D(x^i)$ are unknown. However, it can be lower-bounded using the generative distribution $p(x^i | z^i, z_s)$ as follows:\n$\\begin{aligned} I(X^i; Z^i, Z^s) &= E_{q(z^i, z_s | x^i)p_D(x)} \\left[ \\log \\frac{q(x_i | z^i, z_s)}{p(x^i)} \\right] \\\\ &= H(X^i) + E_{q(z^i, z_s | x^i)p_D(x)} [\\log p(x^i | z^i, z_s)] \\\\ &+ E_{q(z^i, z_s)} \\left[ D_{KL} \\left[ q(x^i | z^i, z_s) || p(x^i | z^i, z_s) \\right] \\right] \\\\ &\\geq H(X^i) + E_{q(z^i, z_s | x^i)p_D(x)} [\\log p(x^i | z^i, z_s)] \\\\ &= H(X^i) + E_{p_D(x^i, ..., x^N)q(z^i | x^i)q(z_s | x^i, ..., x^N)} [\\log p(x^i | z^i, z_s)] \\end{aligned}$ (8)\nwhere H() represents the Shannon entropy.\n*   The third term $-I(X^i; Z^i)$ involves $q(z^i) = \\int p_D(x^i)q(z_s | x^i)dx$, where $p(x^i)$ is unknown. We employ the Variational Information Bottleneck (VIB) [3] to lower bound it:\n$\\begin{aligned} -I(X^i; Z^i) &= -E_{p_D(x^i)} \\left[ D_{KL} \\left[ q(z^i | x^i) || q(z^i) \\right] \\right] \\\\ &\\geq -E_{p_D(x^i)} \\left[ D_{KL} \\left[ q(z^i | x^i) || p(z^i) \\right] \\right] \\end{aligned}$ (9)\nHere we use $-E_{p_D(x^i)} \\left[ D_{KL} \\left[ q(z^i | x^i) || p(z^i) \\right] \\right]$ as its lower bound with the generative distribution $p(z^i)$ defined as the standard Gaussian.\nSubsequently, we can derive a lower bound for Eq. 6 by combining the derived equations. Therefore, we can enforce disentanglement across multiple programming languages by maximizing:\n$\\sum_{i=1}^N \\left[ I(X^i; X_{\\i}; Z^s) - I(Z^i; Z^s) \\right]$ $\\newline \\geq \\sum_{i=1}^N E_{p_D(x^1, ..., x^N)} E_{q(z^i | x^i)q(z_s | x^1, ..., x^N)} [\\log p(x^i | z^i, z_s)] \\newline - \\sum_{i=1}^N E_{p_D(x^1, ..., x^N)} \\left[ D_{KL} \\left[ q(z_s | x^1, ..., x^N) || r^i(z_s | x^i) \\right] \\right] \\newline - \\sum_{i=1}^N E_{p_D(x^i)} \\left[ D_{KL} \\left[ q(z^i | x^i) || p(z^i) \\right] \\right] + \\sum_{i=1}^N H(X^i)$. (10)"}, {"title": "3.3 The overall framework of VIM-PT", "content": "The VIM-PT is architecture-free, which can theoretically and practically be adapted to arbitrary sequence-to-sequence architecture, which involves an encoder and a decoder. Based on the experimental performance, we finally set up a unified encoder for all the languages with weight-sharing and an independent decoder for each programming language. As illustrated in Figure 4, given a multi-parallel sample $(t^1, ..., t^N)$ from the dataset $p_D(t^1, ..., t^N)$, where each sample including a group of codes of the same function implemented by the N different programming languages. The flag tokens are initialized by the unused tokens in the tokenizer. To refer to the programming language, we concatenate the flag tokens, and source code as the input sequence as follows:\n$[[CLS], x^i_1, ..., x^i_k, \\texttt{{flag}}_1, ..., \\texttt{{flag}}_n, [SEP]], n \\in [1, N]$, (12)\nwhere $k$ is the length of the flag token, and $c$ is the length of the source code. Then the sequence of the instance is encoded by the weight-sharing encoders to the initial representations. Then the flag representations are inputted into the variational interaction.\nAfter variational interaction, we can reconstruct the flag representations of the other programming languages for each source code of the sample. Then the target flag representations are concatenated with the source code representations and inputted into the target decoder to generate the target code. The regularized training approach learns the model from the objective which consists of the supervised loss function as well as the regularization term described in Eq. 11."}, {"title": "3.4 Learning from multi-parallel samples", "content": "The disentangle module involves three approximate posteriors $q_{\\phi_i}$, $r_{\\phi_i}$, and $p_{\\phi_i}$, and a language-shared approximate posterior $q_{\\phi_s}$. Each approximate posterior is parameterized by a standard Gaussian prior $z \\sim N(0, I)$.\nThe flag representations of instances can be disentangled by the corresponding language-specific projectors $(q_{\\phi_1}, ..., q_{\\phi_N})$ and language-shared projectors $q_{\\phi_s}$, respectively. In addition, the shift-shared representations are inferred by corresponding shift-shared projectors $(r_{\\phi_1}, ..., r_{\\phi_N})$. Then the language-specific representations and the language-shared representation are inputted into $(\\rho_1, ..., \\rho_N)$ to reconstruct the representations of the multilingual instances.\nThe language-specific representations, the language-shared representation, and the shift-shared representations are denoted by $(z^1, ..., z^N)$, $z_s$, and $(z_1, ..., z_N)$, respectively. Instructed by Equation 11, the objective of the training of multi-parallel samples is formalized as:\n$\\begin{aligned} L &= ((\\lambda + 1) \\cdot [-\\sum_{i=1}^N E_{q(z^i | x^i)q(z_s | x^1, ..., x^N)} [\\log p(x^i | z^i, z_s)\\newline &+ \\log p(t^i | x, t)]]) &\\text{Reconstruction Loss}  \\newline &+ (\\lambda + 1) \\cdot \\sum_{i=1}^N D_{KL}[q(z^i | x^i) || p(z^i)] \\newline &+ D_{KL}[q(z^s | x^1, ..., x^N) || p(z_s)] \\newline &+ \\lambda \\cdot \\sum_{i=1}^N D_{KL}[q(z_s | x^1, ..., x^N) || r^i(z_{\\s} | x^i)], \\end{aligned}$ (13)\nThe first term of the Eq. 13 includes two parts, where the reconstruction loss (Mean Square Error) and translation cross-entropy loss are to reconstruct the representations of flag tokens and source code tokens, respectively. The last term is to minimize the KL divergence between the shift-shared and language-shared representations, so the language-shared representation can be replaced approximately by the shift-shared representation when some instances are missing in the partially missing samples."}, {"title": "3.5 Learning from partially missing samples", "content": "For partially missing samples, we design a training strategy to exploit the data that is not multi-parallel. The training strategy is described in Algorithm 1.\nAt first, given a partially missing sample from the dataset, there may be some missing instances in one sample. To obtain the pseudo instances, we disentangle the language-specific representations and the language-shared representations to generate the draft translation of the missing instances. We sample a set of pseudo instances $(\\hat{t}^1, ..., \\hat{t}^N)$ from the training dataset randomly to fill the missing instances of the corresponding language in the original sample. And the flag representations can be separated as $(\\hat{x}^1, ..., \\hat{x}^N)$. Similar to the learning of multi-parallel samples, the language-specific representations, and the shift-shared representations are disentangled as $(\\hat{z}^1, ..., \\hat{z}^N)$, and $(z_1, ..., z_N)$. However, as some instances of the sample are missing, the language-shared representation can not be inferred.\nTo overcome this issue, the method generates the draft translation of the missing instances with the pseudo instances. Each language-specific representation of the pseudo instance and the language-shared representation of ground truth are inputted into $(\\rho_1, ..., \\rho_N)$ to reconstruct the representations of the multilingual instances. As the language-shared representation of ground truth is not unique, for each language-shared representation of $(Z_1, ..., Z_N)$, a multi-parallel sample $(\\hat{x}^1, ..., \\hat{x}^N)$ reconstructed by language-shared information and language-specific information can be obtained. Then the learning of multi-parallel samples can be performed. Especially, only the target programs with ground truth are reconstructed."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments on the CoST dataset [51], which is a large and comprehensive dataset to evaluate the performance of program translation approaches. The dataset consists of both snippet-level and program-level parallel data from 7 programming languages (i.e., C, C#, C++, Java, JavaScript, PHP, and Python) and up to 42 programming language pairs, which was collected from the GeeksForGeeks website [1]. The platform ensures its contributors stick to a template in terms of the comments used in their programs and the code corresponding to those comments. The dataset provides a good number of multilingual instances of code that can be effectively used for this task. The detailed statistics of the dataset are shown in Table 1, while the train, validation, and test sets are split the same as CoST [51]. For each language, there are some instances of corresponding samples that are missing, especially in C and PHP."}, {"title": "4.2 Experimental Settings", "content": "We choose the following state-of-the-art multi-lingual and pairwise program translation methods as baselines. All the encoder-only pre-trained models are fine-tuned on the task with the same decoder as our methods."}, {"title": "4.3 RQ1: What is the Performance of VIM-PT Comparing With Baseline Approaches?", "content": "In the experiment, we use the BLEU [33] score as the evaluation metric to evaluate the n-gram overlap between the translated code and the ground-truth target code, which is the most widely used metric in program translation [13, 19, 25, 51]. A higher BLEU score indicates better evaluation performance, which varies from 0 to 100 as a percentage. Table 3 shows the experimental results. For each pair of translations, the three best performing approaches are highlighted, while dark gray marks an approach with the highest retrieval performance, and gray and light gray correspond to the second- and third-best approaches. Compared with the pairwise approach like CodeBERT with a similar backbone, the multilingual approaches including MuST-PT, VIM-PT(w/o VT), and VIM-PT perform significantly outperformance. We can attribute this result to (1) expansion of the representation scope, as the different language corpora face various application scenarios and have different functional preferences, (2) better latent space, which can be jointly constructed by different languages to represent the language-sharing semantics, and (3) the auxiliary benefit to low-resource language with rich-resource language, which is last but most important factor. In practical scenarios, where different language corpora are unevenly distributed, building a low-resource programming language to latent space mapping can be benefited from learning among other rich-resource programming languages, which is more effective and robust than building a pairwise mapping using only parallel data.\nIt can be observed that VIM-PT performs performance gains widely over the state-of-the-art approaches on most of the translation pairs, especially at the program level. VIM-PT performs better with the improvement of 4.57% and 1.87% over the state-of-the-art approach in multi-lingual program translation [51] at the program level and snippet level, respectively. In particular, VIM-PT performs better than all the pairwise approaches and multi-lingual approaches on all the translations when Python is the source language or target language. Table 4 further calculates the average score of the translation of different approaches from or to any programming languages based on Table 3. As shown in Table 4, VIM-PT has performed the state-of-the-art at both snippet and program level in all configurations of the average score from or to any programming languages. We notice the performance of MusT-PT is lower than M-PT(w/o VI) on PHP and Python, which indicates that using the weight-sharing decoder is not conducive to the modeling of low-resource languages with few samples, which may be overwhelmed by the rich-resource languages in the joint training."}, {"title": "4.4 RQ2: What is the impact of Variational Interaction for the VIM-PT?", "content": "To evaluate the effectiveness of the variational interaction, we set a blank controller termed M-PT(w/o VI), which ablates variational interaction on VIM-PT. As shown in Table 3 and Table 4, the inclusion of Variational Interaction (VI) in VIM-PT results in an improvement in the average BLEU-4 score at both snippet Level and program Level. Across most programming language pairs, VIM-PT achieves higher performance than M-PT(w/o VI), indicating that variational interaction indeed enhances translation quality.\nIn summary, the incorporation of VI effectively enhances the performance of the translation model, resulting in more accurate and fluent translations. The Improvement in Table 4 indicate the average improvement of the variational interaction across all languages and levels, with an average increase from 4.05% to 11.14%, and from 1.65% to 10.47% at the program level and snippet level, respectively. The variational interaction leads to notable improvements especially in low-resource languages like C and PHP, indicating that variational interaction plays a crucial role in enhancing the"}]}