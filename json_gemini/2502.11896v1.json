{"title": "CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning", "authors": ["Yanxiao Zhao", "Yangge Qian", "Jingyang Shan", "Xiaolin Qin"], "abstract": "Reinforcement learning (RL) in continuous action spaces encounters persistent challenges, such as inefficient exploration and convergence to suboptimal solutions. To address these limitations, we propose CAMEL (Continuous Action Masking Enabled by Large Language Models), a novel framework integrating LLM-generated suboptimal policies into the RL training pipeline. CAMEL leverages dynamic action masking and an adaptive epsilon-masking mechanism to guide exploration during early training stages while gradually enabling agents to optimize policies independently. At the core of CAMEL lies the integration of Python-executable suboptimal policies generated by LLMs based on environment descriptions and task objectives. Although simplistic and hard-coded, these policies offer valuable initial guidance for RL agents. To effectively utilize these priors, CAMEL employs masking-aware optimization to dynamically constrain the action space based on LLM outputs. Additionally, epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling agents to transition from constrained exploration to autonomous policy refinement. Experimental validation on Gymnasium MuJoCo environments (Hopper-v4, Walker2d-v4, Ant-v4) demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated policies significantly improve sample efficiency, achieving performance comparable to or surpassing expert masking baselines. For Walker2d-v4, where LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust RL performance without notable degradation, highlighting the framework's adaptability across diverse tasks. While CAM\u0112L shows promise in enhancing sample efficiency and mitigating convergence challenges, these issues remain open for further research. Future work aims to generalize CAMEL to multimodal LLMs for broader observation-action spaces and automate policy evaluation, reducing human intervention and enhancing scalability in RL training pipelines.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as OpenAI o1 and Google Gemini, have demonstrated remarkable capabilities in reasoning and code generation. These advancements have spurred increasing interest in applying LLMs to decision-making tasks. However, decision-making often requires not only reasoning and prior knowledge but also the ability to adapt and learn interactively\u2014a hallmark of Reinforcement Learning (RL). RL achieves this by enabling agents to interact with their environment, observe feedback in the form of rewards, and iteratively refine their policies. This interactive learning mechanism has driven RL's success in various domains, including robotics, strategic gaming, and autonomous control. The integration of RL with LLMs promises to combine their complementary strengths, paving the way for more sample-efficient learning and enhanced decision-making performance.\nWhile significant progress has been made in leveraging LLMs to augment various RL components\u2014serving as reward designers, information processors, or world model simulators (Cao et al., 2024)\u2014the potential of LLMs as expert policies to directly guide RL agents remains largely unexplored. This underutilization arises from two key challenges: first, the suboptimal performance of LLM-based policies without specialized fine-tuning; and second, the inherent vulnerability of RL algorithms to convergence to suboptimal solutions when guided by imprecise or unreliable feedback. Previous works, such as Hausknecht et al. (2020) and Yao et al. (2020), have primarily focused on text-based game environments, where LLMs generate or refine actions. However, these approaches often suffer from domain specificity and limited generalizability to more complex RL scenarios."}, {"title": "2 Preliminaries", "content": "RL involves an agent interacting with an environment modeled as a Markov Decision Process (MDP) (S, A, P, R, \u03b3). The agent observes a state s \u2208 S, takes an action a \u2208 A via a policy \u03c0(\u03b1|s), transitions to s' ~ P(s'|s, a), and receives a reward r = R(s, a). The objective is to learn a policy that maximizes the expected cumulative reward, E[\u2211t=0^\u221e \u03b3^t R(s_t, a_t)]. In this work, prior knowledge is encoded into the policy using \u3160LLM, a Python-executable function generated by an LLM."}, {"title": "3 Approach", "content": "In this section, we present a detailed introduction to the CAMEL framework, with its schematic workflow illustrated in Figure 1 and the pseudocode (Algorithm 1). The CAMEL framework consists of three key components. 1. Utilizing LLMs to Generate Hard-Coded Policies. This component leverages LLMs to generate Python code that encodes prior knowledge, assuming the optimal policy is near TLLM and guiding policy learning. 2. Masking-Aware Continuous Action Masking. In this component, masking information is incorporated into s and input to the actor model, enabling it to learn and adapt to dynamic masking. By constraining the action space based on \u3160LLM, the RL agent explores more efficiently. 3. Epsilon-"}, {"title": "3.1 Utilizing LLMs to Generate Hard-Coded Policies", "content": "Our prompt (see Figure 2) includes detailed information about the state space S and the action space A, clarifying the dimensions of each, the task objectives, and the MuJoCo XML file. These resources are drawn from the Gymnasium documentation and codebase (Towers et al., 2023). The goal is to create a Python policy function that uses hard-coded parameters to map the environment state s to an action ALLM.\nWe adopt the Chain of Thought approach to guide the LLM with step-by-step reasoning instructions for policy design. However, the generated parameters are hard-coded, making the policies non-adaptive to environmental feedback and potentially unstable. To select the optimal policy, we generate multiple candidates, evaluate their performance in a single episode, and have human experts review the rendered videos. Specifically, the episode return alone may not reliably indicate the policy's quality. For instance, in the Hopper-v4 environment, achieving a stable standing position yields an episode return of 1000 but represents a suboptimal strategy as the agent fails to move forward. In contrast, an unstable forward motion might approach the optimal strategy, albeit with a lower episode return. Thus, human experts evaluate video renderings to assess qualitative aspects of behavior, such as forward progression and stability, to identify the best-performing policy. Figure 3 shows an example of a Python policy generated by the LLM, which uses hard-coded proportional-derivative control logic to compute torque actions for basic stability."}, {"title": "3.2 Masking-Aware TD3", "content": "Previous works, such as (Krasowski et al., 2023; Stolz et al., 2024), proposed deterministic action masking in continuous action spaces. These methods redefine the action space by strictly excluding invalid actions, effectively improving the learning efficiency of RL agents. However, they rely on the assumption that the mask is fully deterministic, which limits their applicability to scenarios where prior knowledge only suggests that certain actions are suboptimal but not strictly invalid. This is because such prior information often lacks precise boundaries for defining optimality.\nTo address this limitation, we propose Masking-Aware TD3, which incorporates dynamic and stochastic masking. Our approach introduces a probabilistic e-masking mechanism, allowing the RL agent to learn under both masked and unmasked conditions. Specifically, the actor model takes the state s and dynamically computed action bounds"}, {"title": "3.3 Epsilon Masking", "content": "Epsilon Masking introduces a mechanism to gradually reduce the influence of \u3160LLM over training. Initially, the masking probability et is set to 1, applying strict constraints based on the LLM's output. Over time, et decreases linearly as et = max(1 - t/T f_m , 0.0), where fm is the masking fraction and T is the total duration of training. This phased reduction enables the RL agent to transition from guided exploration to independent policy learning, optimizing its performance without over-reliance on suboptimal guidance."}, {"title": "4 Experiments", "content": "In this section, we designed three groups of experiments. First, we evaluated the performance of CAMEL-TD3 under the guidance of expert policies and random policies. Subsequently, we analyzed the results of experiments with \u03c0LLM, assessing their potential and characteristics in providing effective guidance."}, {"title": "4.1 Setup", "content": "Our implementation is based on the PyTorch TD3 from CleanRL (Huang et al., 2022), with consistent hyperparameters. Key settings include Epsilon Masking(fm = 0.2) and Action Mapping (bias = 0.3). For expert masking experiments, we utilize pre-trained models as expert models from CleanRL's HuggingFace repository: Hopper-v4 (3244.59 \u00b1 8.55), Walker2d-v4 (3964.51 \u00b1 9.70), and Ant-v4 (5240.79 \u00b1 730.24).\nWe use the Google Gemini 2.0 (gemini-exp-1206) for generating policies. All related code, prompts, outputs, and rendered videos are available at https://github.com/sdpkjc/camel-rl."}, {"title": "4.2 Analysis", "content": "Figure 4 shows that applying Masking-Aware (MA) and Epsilon Masking (EM) enables the RL agent to improve returns, even in evaluation environments without expert masking, as epsilon decreases. In contrast, the control group (Expert masking w/o MA EM) relies entirely on expert masking. In random masking experiments, CAMEL achieves final returns close to the baseline despite random guidance, whereas the control group (Random masking w/o MA EM) fails to learn effective strategies. These results highlight CAMEL's ability to utilize expert masking effectively and adapt to random masking conditions."}, {"title": "5 Conclusion, Limitations, and Future Work", "content": "In this work, we introduced the CAMEL framework, which harnesses the capabilities of LLMs to improve RL in continuous action spaces. By using LLM-generated suboptimal policies as initial guidance and dynamically constraining the action space, CAMEL enhances exploration efficiency and mitigates convergence to suboptimal solutions. While the approach shows significant promise in environments like Hopper-v4 and Ant-v4, its applicability is limited to vectorized observation spaces, and the reliance on expert screening for policy evaluation introduces a manual overhead. Furthermore, the effectiveness of the framework depends on the underlying LLM's capability to model complex dynamics, which can be a limiting factor in environments with high dimensionality or intricate task requirements. Future work could aim to generalize CAMEL to diverse observation and action spaces, automate policy selection to reduce human intervention, and integrate more advanced multimodal LLMs to further enhance adaptability and performance across complex RL scenarios."}]}