{"title": "Orchestrating LLMs with Different Personalizations", "authors": ["Jin Peng Zhou", "Katie Z Luo", "Jingwen Gu", "Jason Yuan", "Kilian Q. Weinberger", "Wen Sun"], "abstract": "This paper presents a novel approach to aligning large language models (LLMs) with individual human preferences, sometimes referred to as Reinforcement Learning from Personalized Human Feedback (RLPHF). Given stated preferences along multiple dimensions, such as helpfulness, conciseness, or humor, the goal is to create an LLM without re-training that best adheres to this specification. Starting from specialized expert LLMs, each trained for one such particular preference dimension, we propose a black-box method that merges their outputs on a per-token level. We train a lightweight Preference Control Model (PCM) that dynamically translates the preference description and current context into next-token prediction weights. By combining the expert models' outputs at the token level, our approach dynamically generates text that optimizes the given preference. Empirical tests show that our method matches or surpasses existing preference merging techniques, providing a scalable, efficient alternative to fine-tuning LLMs for individual personalization.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have emerged as powerful tools used for content generation and increasingly as personal assistants; however, they must be closely aligned with human preferences to ensure safety and reliability. Methods like Reinforcement Learning from Human Feedback (RLHF) [1] align models with general human preferences, however, with the widespread adoption of LLMs comes the need to for alignment with respect to individual preferences. For example, a language model used by a child should be easy to understand and contain safeguards, whereas a language model used by an IT professional should generate far more technical details.\nTowards solving this problem, we follow prior work Jang et al. [2] that introduced the concept of breaking down preferences into varying dimensions, i.e. bases of preferences such as harmful, helpful, concise, or funny. Such a breakdown is simple and intuitive, while allowing for a user-friendly framework to define any specific preference as a combination along these criteria. However, model fine-tuning within such a framework is nontrivial due to the curse of dimensionality: The number of possible combinations increases exponentially with the number of dimensions-causing existing methods based on fine-tuning via RLHF to become intractable very quickly.\nTo this end, Jang et al. [2] and others [3, 4] explored Reinforcement Learning from Personalized Human Feedback (RLPHF). Starting with a pre-trained LLM, they create multiple copies and fine-tune each one with respect to a single preference dimension, i.e., one expert model for humor, another for conciseness, etc. During the fine-tuning process the updates are kept low-rank, using LoRA [5]. Given any user specific preference-a combination of dimensions such as helpful and funny-they create a new LLM by directly merging the LoRA weights of the experts corresponding to the target"}, {"title": "2 Related Work", "content": "Alignment of Language Models to Human Preferences. Alignment of language models to human preferences has arguably begun with prompting [9, 10, 11, 12]. However, without any finetuning, these models sometimes produce outputs that are not well-aligned with human values or preferences [13, 14, 15]; recent works study how to improve their alignment with a general human preference with additional fine-tuning. Many current methods follow the Reinforcement Learning from Human Feedback (RLHF) paradigm, popularized by [1] and leveraged across a myriad of tasks [16, 17, 18], to first learn a reward function to model human preference before optimizing the language model on it. Other recent directions include direct policy optimization (DPO) [19] and reward-ranked tuning [20], which bypasses learning a reward from human preference and instead directly optimizes the policy. In general, such works rely on the reinforcement learning framework to optimize over a single, average human preference. In contrast, a recent line of works Jang et al. [2], Wang et al. [3] explores individual or case-based preferences fine-tuning; however, such methods rely on merging or fine-tuning model weights and is applicable only in white-box model settings whereas our approach treats these fine-tuned models as black-box APIs.\nMulti-Objective Reinforcement Learning (MORL). involves optimizing a decision-making process towards composite, often conflicting objectives [21]. Recent works explore such objective tuples as the Helpfulness-Honesty-Harmlessness (HHH) principle [22, 23], Relevance-Correctness-Completeness [24], and Expertise-Informativeness-Style [2]. Wu et al. [24] propose a PPO-based MORL framework where multiple objectives are combined in the reward model, thus achieving superior performance to traditional RLHF models in long-form question answering tasks. Other works devise similar reward-merging techniques for supervised fine-tuning (SFT)-DPO pipelines [4, 3] or train an additional encoder-decoder network to combine multiple outputs from individually-trained"}, {"title": "3 Mixture of Preference Experts", "content": "The overview of our method is shown in Figure 1, with notation specified in dark shaded boxes. Given relevant expert models (center of the figure), each LLM specialized with respect to an individual preference dimension, we want to be able to generate text that is a likely continuation of the context and fits the multi-dimensional preference specified by the user. Our approach, Mixture of Preference Experts (MOPE) assumes individual experts are black-box and frozen with only their next-token probabilities (or logits) exposed. We propose a novel way to merge the outputs from relevant expert"}, {"title": "3.1 MoPE Inference", "content": "The MoPE inference setting assumes that the user provides an instruction x such as \"Give me a 5 day itinerary for Switzerland\" as well as a preference  $\\xi$ (\"I want a helpful, concise, and funny response!\") that consists of n individual preference dimensions  $\\lbrace p_1, ..., p_n \\rbrace$ (3 in Figure 1: helpfulness, conciseness and humour). We are also provided with n black-box LLMs  $\\lbrace M_1, ..., M_n\\rbrace$, each specialized along the corresponding preference dimension  $p_i$ only. In other words  $M_3$ is for example an LLM optimized to be humorous. We refer to these models as relevant expert models (REM). Given a partial response generated so far  $y_{<t}$ (e.g. \"Sure! Here's a Swiss\"), we want to construct a next token probability distribution in order to decode the subsequent token  $y_t$.\nSince we aim to achieve effective multi-objective personalization, we introduce a trainable neural network called the preference control model (PCM), parametrized by  $\\theta$. PCM takes as input the instruction x, the partial response  $y_{<t}$ and the preference vector  $\\xi$ and outputs a weight vector with length n (one weight for each REM) whose entries are non-negative and sum to 1. Assuming we already have a well-trained PCM, the inference of MoPE is very similar to the standard autoregressive decoding mechanism of LLMs. Specifically, we can construct the following next token probability distribution as a weighted sum of all experts:\n$\\pi_{\\theta}(y_t | x, y_{<t}, \\xi) = \\sum_{i=1}^{n} \\alpha_{\\theta}(x, y_{<t}, \\xi)_i \\cdot \\pi_{M_i}(y_t | x, y_{<t})$\nwhere  $\\pi_{M_i}(y_t | x, y_{<t})$ is the next token probability distribution from expert  $M_i$ and  $\\alpha_{\\theta}(x, y_{<t}, \\xi)$ is the output of the PCM. As each individual  $\\pi_{M_i}(.)$ outputs a probability distribution and  $\\pi_{\\theta}$ is a convex combination of them, it itself is also a well-defined distribution over all tokens in the vocabulary. As each  $M_i$ is specialized for one specific preference dimension, the weights in  $\\alpha_{\\theta}(.)$ specify how much importance should be given to each dimension  $p_i$ at time step t. Individual experts  $M_i$ are frozen and can be treated as black-box APIs since only their output probabilities are needed. Various decoding methods such as greedy or temperature sampling can be used to decode the next token  $y_t$ from the distribution  $\\pi_{\\theta}$. As the PCM only outputs a distribution over n dimensions instead of the vocabulary size, it can be a relatively small model that effectively orchestrates the large models  $M_i$."}, {"title": "3.2 MoPE Training", "content": "An overview of the MoPE training procedure is shown in Figure 2. Our framework uses online RL algorithms such as PPO [35] and REBEL (REgression to RElative REward Based RL) [6] to train the PCM  $\\alpha_{\\theta}$. Similar to prior work [2], we assume we have access to a black-box reward model for each individual dimension (e.g., a reward model that can quantify the level of helpfulness). Alternatively, one can also train such reward models from existing pairwise comparison data such as Cui et al. [36]. We define  $y \\sim \\pi_{\\theta}(. | x, \\xi)$ as the method for generating a response y (a sequence of tokens) following MoPE inference procedure. Furthermore,  $\\pi_{\\theta}(y | x, \\xi)$ represents the probability of generating the response y. Note that since our models are autoregressive, we have  $\\pi_{\\theta}(y | x, \\xi) = \\Pi_{\\tau} \\pi_{\\theta}(y_t | x, \\xi, y_{<t})$, i.e., the likelihood of the whole response is the product of the likelihood of each token. Below we give our formulation for the reward modeling for RL training.\nReward modeling using Bradley-Terry. In the multi-objective personalization setting, instead of having a single reward model for the entire preference, we have access to black-box reward models for individual dimensions (e.g., a reward model for conciseness and a different reward model for humorousness). For our purpose, individual reward models can either be off-the-shelf classifiers, APIs or trained from existing human-labeled data. Given an instruction x and  $\\xi = \\lbrace p_1, ..., p_n \\rbrace$, for each response y MoPE generates, we obtain a vector of reward values  $[r_{p_1}(x, y), ..., r_{p_n}(x, y)]$ from the corresponding reward models. Here  $r_{p_i}$ corresponds to the reward value of the preference dimension  $p_i$. This corresponds to the helpfulness, conciseness, and humour rewards on the right-hand side of Figure 2.\nThe reward models from different dimensions are not necessarily calibrated together (e.g., they may have different scales). We address this by introducing a reference response  $y_{ref}$ (that can come from a"}, {"title": "Online RL", "content": "We aim to find the PCM parameters  $\\theta$ in  $\\alpha_{\\theta}$ to maximize the combined reward r(x, y). Many online RL methods are suitable. In our experiments, we use REBEL [6] due to its simplicity and superior performance, though we found that PPO can also work. We briefly explain REBEL below. Recall the policy  $\\pi_{\\theta}(. | x, \\xi)$ induced by the PCM  $\\alpha_{\\theta}$ in Eq. 1. REBEL iteratively updates the PCM parameter  $\\theta$ via solving the following least square regression oracles:\n$\\theta^{t+1} = arg \\min_{\\theta} E_{x, y_1, y_2 \\sim \\pi_{\\theta^t}(. | x, \\xi)}( \\eta ( ln \\frac{\\pi_{\\theta^{t+1}}(y_1 | x, \\xi)}{\\pi_{\\theta^t}(y_1 | x, \\xi)} - ln \\frac{\\pi_{\\theta^{t+1}}(y_2 | x, \\xi)}{\\pi_{\\theta^t}(y_2 | x, \\xi)}) - (r(x, y_1) - r(x, y_2)))^2$\nwhere  $\\eta$ is a parameter that controls the deviation of  $\\pi_{\\theta^{t+1}}$ to  $\\pi_{\\theta^t}$, and  $y_1, y_2 \\sim \\pi_{\\theta^t}(. | x, \\xi)$ denotes two i.i.d samples from  $\\pi_{\\theta^t}(. | x, \\xi)$. Intuitively, the goal of REBEL is to model the reward difference using  $\\eta ln \\frac{\\pi_{\\theta^{t+1}}(y_1 | x, \\xi)}{\\pi_{\\theta^t}(y_2 | x, \\xi)}$, so that  $\\eta ln \\frac{\\pi_{\\theta^{t+1}}(y_1 | x, \\xi)}{\\pi_{\\theta^t}(y_2 | x, \\xi)}$ can estimate the reward r(x, y) accurately up to some constant that is independent of y. The REBEL's least square regression objective shares some similarities with the algorithms Direct Preference Optimization (DPO) [19] and Identity Preference Optimization (IPO) [38], and learns a policy  $\\pi_{\\theta^{t+1}}$ to approximate the ideal Mirror Descent [39] update  $\\pi_{\\theta^t}(y | x) exp(r(x, y) / \\eta)$. Note that during training, we enumerate all possible preferences  $\\xi$, which allows REBEL to train a single policy that can perform output merging under any preference."}, {"title": "4 Experiments", "content": "4.1 Dataset and Evaluation\nWe evaluate how good baselines and MoPE can satisfy multifaceted preferences on open-ended gen-erations. We evaluate on the same subset of 50 instances of Koala dataset [8] from [2]. Following [2], we construct eight distinct preferences from six individual dimensions: elementary, knowledgeable, concise, informative, friendly and unfriendly. The six dimensions can be categorized into three groups of two where they are opposite of each other (A and B). A preference is then formed by drawing one dimension from each group. The preference dimensions and instructions can be found in Table 1."}, {"title": "4.2 Models", "content": "We use Tulu-7B [7], an instruction-tuned language model, as the base model for reward models and expert models. Specifically, we use the reward model training data from [2] to first train a reward model for each of the six preference dimension in Table 1. We note that although we trained reward model ourselves, the reward model can technically be off-the-shelf classifiers or even black-box API models. Then, to obtain the experts that specialize in each preference dimension, we perform RLHF by using PPO to fine-tune a separate base model for each preference. After training, the six experts are always frozen without any update. Note that we selected these design choices to maintain consistency with Jang et al. [2] to permit a fair comparison. For the preference control model, to illustrate that we can control large expert models, we use a much smaller LLaMA based model that has 160M parameters in total. The final linear layer of this model that originally outputs the next token probability distribution is replaced by another randomly initialized linear layer with a size equal to the number of preference dimensions in a preference, i.e. 3 in our setting which is significantly smaller than the size of the vocabulary. We use LoRA [5] for all model training."}, {"title": "4.3 Baselines", "content": "We compare MoPE with several baselines ranging from directly prompting the model to full-blown multi-objective RL training. In Table 2, we categorize methods with two characteristics: 1. whether weight access to the individual experts is needed. 2. whether additional training (of the experts or other models) is required. Below we give more details and explanations for each baseline.\nVanilla Prompting. To highlight the importance of personalization, we simply prompt the base model (the instruction fine-tuned Tulu-7B model) with the instruction without any preference given. This baseline does not have any personalization.\nPreference Prompting. As a step forward from vanilla prompting, we now prompt the base model with the preference along with the instruction. This tests how good the base model is at following both preferences and instructions.\nPersonalized Soup. For a given preference, Personalized Soup [2] creates a new model by uniformly merging the parameters of the experts that belong to the preference. After merging, preference prompting is used to generate the responses. Because of the weight merging, Personalized Soup needs access to weights of individual experts."}, {"title": "Multi-Objective RL", "content": "The multi-objective RL baseline directly trains the Tulu-7B model \u2013 the base model used for each individual experts. Particularly, we use the trained reward models and perform PPO training on all eight preferences with the Tulu-7B model as the policy. The reward calculation is kept the same as MoPE. Because training is performed directly on the large base model (7B Tulu in this case), weight access to the base / preference model is needed."}, {"title": "Mixture of Preference Experts (MoPE)", "content": "Our method, MoPE, trains a small preference control model to control large expert models. Note that MoPE does not need weight access to the experts."}, {"title": "4.4 Main Results", "content": "Table 3: Pairwise win rate comparison between baselines and MoPE, evaluated by GPT4. MOPE outperforms all baselines in average win rate.\nIn Table 3, we summarize the performance of baselines and MoPE. The first thing to note is that the average win rate for vanilla prompting is significantly lower than other methods, indicating providing preferences in addition to instructions is crucial for preference personalization. Because Tulu-7B is an instruction-tuned model, preference prompting is a very competitive baseline and achieves an average win rate of more than 54%. Personalized Soup is slightly better than the prompting baseline, suggesting merging on the expert weights could slightly improve personalization. Multi-Objective RL is the best baseline since it can train the large 7B model directly with RLHF. MoPE outperforms the baselines, achieving an average win rate of 62.00%. This shows that although MoPE does not directly fine-tune the large experts, learning how to control the specialized experts via output merging using a much smaller model is already capable of achieving strong performance."}, {"title": "4.5 Ablations of MoPE", "content": "In Table 5, we ablate MoPE through several axes of configuration. We first study how MOPE performs without any training in the preference control model. In this case, we simply uniformly merge the outputs from the preference experts. We study two spaces to merge the outputs: logit and probability space. As seen in the first two rows from Table 5, merging on the probability space outperforms its counterpart in logit space. This is possibly due to logit space is not normalized and directly adding the logits from different experts could lead to drastic change of distribution. Merging on the probability space outperforms the prompting baselines and weight merging in Table 3, confirming the empirical benefit of merging in the output probability spaces over merging in the parameter spaces. Next, instead of modeling the reward with the Bradley-Terry calculation, directly averaging the reward from individual dimensions achieves much lower performance. This empirically confirms our intuition of using the reward signal from the BT model: these reward signals are always normalized at the same scale and are more interpretable (i.e., probability of winning over a reference response). Finally, we also show that other online RL algorithms, such as PPO, can also be directly applied to MoPE and achieve competitive performance against baselines, indicating the flexibility of our framework in terms of integrating different RL black-box algorithms."}, {"title": "4.6 Qualitative Analysis", "content": "In Table 4, we provide a few examples of response generated by Preference Prompting, Personalized Soup, and MoPE to qualitatively demonstrate the difference between the methods. In the first example, the generations from Preference Prompting and Personalized Soup both overuse repetitive metaphors, which makes the comparisons more confusing and harder to understand, while the generation from MOPE keeps the use of metaphors relatively simple and straightforward, keeping the message clear and easier to understand while also being more concise, while the more casual tone of the conclusion makes it feel more friendly than the other generations. In the second example, the generations from Preference Prompting and Personalized Soup both use slightly more advanced terminology that could be difficult to understand, while the generation from MOPE uses simpler language in a more straight-forward explanation that is both easier to understand and more concise, while its blunt denial gives it a more unfriendly tone than the other two, which give more mixed responses with more neutral tones. In the third example, the generation from MoPE offers a slightly more in-depth explanation and uses more technical language that an expert is likely to be familiar with without significantly sacrificing conciseness, being of similar length to the other two generations."}, {"title": "4.7 Limitations and Discussion", "content": "Compared to prompting and weight merging, output merging requires more compute and memory to generate responses linear to the number of models contributed to the merged composition. In addition, since output merging has not been used widely, a custom efficient implementation is usually required for faster decoding. We have made an efficient implementation for Huggingface library via explicit key value cache and plan to open source it for future research.\nDespite the compute and memory overhead, output merging has two important benefits that motivate our work. First, compared to weight merging and other fine-tuning methods, output merging does not require access to the expert parameters. Since many state-of-the-art language models [40, 41] are closed-sourced, output merging can be potentially applied to merging of such models whereas other approaches are not applicable. Besides, even with the compute and memory overhead, output merging is more friendly to a large scale deployment setting than weight merging. Given a batch of requests with many distinct preferences, because one set of merged weights can only process requests for a specific preference, weight merging either requires sequential serving the requests or spawning resource that can host as (possibly exponentially) many distinct preferences as requested for parallel serving. Output merging, however, can perform parallel serving with just the resource needed to host linear number of individual experts."}, {"title": "5 Conclusion", "content": "In this work, we explore the problem of LLM personalization, specifically under the scenario where we assume black-box expert models with only access to its output probability. Towards this task, this work introduces Mixture of Preference Experts (MoPE), a method that approaches this task by merging outputs from relevant expert models via a learned composition. Our method leverages a smaller, lightweight preference control model to achieve multi-objective personalization, benefitting both deployment, privacy, and practicality. Empirically, MoPE achieves a new state-of-the-art performance result, without the need to access model weights of individual expert models. Future work include exploring merging half-way, including a mixture of model weight merging along with output-level merging. More broadly, we suggest future work to explore other domains with compositionality, beyond simple preference dimensions and instruction following."}]}