{"title": "WorldScribe: Towards Context-Aware Live Visual Descriptions", "authors": ["Ruei-Che Chang", "Yuxuan Liu", "Anhong Guo"], "abstract": "Automated live visual descriptions can aid blind people in understanding their surroundings with autonomy and independence. However, providing descriptions that are rich, contextual, and just-in-time has been a long-standing challenge in accessibility. In this work, we develop WorldScribe, a system that generates automated live real-world visual descriptions that are customizable and adaptive to users' contexts: (i) WorldScribe's descriptions are tailored to users' intents and prioritized based on semantic relevance. (ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively succinct descriptions for dynamic scenes, while presenting longer and detailed ones for stable settings. (iii) WorldScribe is adaptive to sound contexts, e.g., increasing volume in noisy environments, or pausing when conversations start. Powered by a suite of vision, language, and sound recognition models, WorldScribe introduces a description generation pipeline that balances the tradeoffs between their richness and latency to support real-time use. The design of WorldScribe is informed by prior work on providing visual descriptions and a formative study with blind participants. Our user study and subsequent pipeline evaluation show that WorldScribe can provide real-time and fairly accurate visual descriptions to facilitate environment understanding that is adaptive and customized to users' contexts. Finally, we discuss the implications and further steps toward making live visual descriptions more context-aware and humanized.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated live visual descriptions can help blind or visually impaired (BVI) people understand their surroundings autonomously and independently. Imagine Sarah, who is blind, is exploring the zoo with her 2-year-old toddler. As they walk around the African grassland section, live visual descriptions provide rich information about the texture of the terrain animals are resting on and occasionally notify her about the movement of the zebras and rhinos. They join a giraffe feeding tour, and live visual descriptions narrate when a couple of giraffes reach out near her toddler. She feeds the lettuce leaves in her hand to them and snaps a nice photo. Such contextual visual descriptions can supplement their environmental understanding and support a range of ever-changing scenarios.\nHowever, providing rich and contextual descriptions has been a long-standing challenge in accessibility. Researchers have explored ways to provide BVI individuals with visual descriptions across various visual media. For example, traditional AI captioning for digital media (e.g., images, videos) offers basic information but often misses the nuanced details BVI users need in varied contexts [44, 74]. While human-powered [19, 38, 60, 61] or human-AI hybrid techniques [25, 32, 64] deliver more detailed descriptions asynchronously for digital media, they fall short in real-world scenarios that require descriptions to be timely and pertinent to the user's context. As a result, existing solutions in describing the real world have been limited to leveraging remote sighted assistance (RSA) to access BVI users' live camera streams and describe the visuals, such as Chorus: View [48] with crowd workers, BeMyEyes [2] with volunteers, and Aira [1] with trained agents. However, these human services can be extremely costly, not always available, and potentially raise privacy concerns.\nThe advent of vision-language models (VLMs) and large language models (LLMs) makes it possible to provide automated visual descriptions without human assistance. Off-the-shelf tools, such as SeeingAI [11], EnvisionAI [4] or ImageExplorer [49], enable BVI people to upload an image and receive detailed descriptions. However, the asynchronous and one-size-fits-all nature of the produced descriptions makes it difficult to adapt these tools to dynamic real-world scenarios. Providing seamless real-time automated descriptions is further challenging when considering user needs and contexts [44, 74]. For instance, BVI people have individual preferences [74] (e.g., different visual experiences and familiarity with the environment), and the rich visual contexts may influence information priority depending on users' needs (e.g., walking on the street, or visiting a museum). Furthermore, real-world sounds could hinder the perception of spoken descriptions [23]. Therefore, when providing live visual descriptions in the real world, it is crucial to collectively consider user intent, visual and sound contexts (which we will refer to as the user's context throughout the paper).\nInformed by prior works on providing visual descriptions and a formative study with five BVI people, we identified design considerations for a system to provide live visual descriptions in the real world, such as providing descriptions with overview first then adaptive details on the fly, prioritizing descriptions based on semantic relevance, and enabling customizability based on varied user needs. We then develop WorldScribe, a system that generates automated visual descriptions that are adaptive to the users' contexts in real-time, and in the real world. First, WorldScribe is adaptive to the user's intent, e.g., prioritizing the most pertinent descriptions based on semantic relevance, and visual attributes based on user customizations. Second, WorldScribe is adaptive to visual contexts; for instance, it provides consecutively succinct descriptions for dynamic visual scenes while it presents longer and more detailed ones for stable settings. Third, WorldScribe is also adaptive to sound contexts, e.g., increasing description volume in noisy environments, or pausing when conversations start.\nPowered by a suite of vision, language, and sound recognition models, WorldScribe introduces a description generation pipeline with different VLMs that balances the tradeoffs between their richness and latency to support real-time usage (Figure 1). WorldScribe dynamically assigns prompts to VLMs encoded with user customizations on their information needs and in-the-moment visual contexts (e.g., busy or static), and prioritizes descriptions based on the user intent and the proximity of the described content to the user. WorldScribe also keeps the spoken descriptions up-to-date by examining the object compositions and similarity between the VLM-referred and current camera frames and the changes in user orientations.\nOur pipeline evaluation shows that WorldScribe can provide fairly accurate visual descriptions, cover important information, and prioritize descriptions based on users' intent and proximity. Furthermore, our user study with six BVI participants demonstrates that WorldScribe enables effective environment understanding that is adaptive and customizable to users' contexts. However, there is still a gap in making AI-generated descriptions humanized, user-centric, and context-aware, which we discuss and provide implications for future work. WorldScribe represents an important step towards solving this long-standing accessibility challenge, and its technical approach may find applications broadly for enhancing real-world visual assistance to promote real-world and digital media accessibility."}, {"title": "2 RELATED WORK", "content": "Our work builds upon prior work to provide BVI people with descriptions for accessing digital media and the real world, in order to fulfill their diverse needs. We describe our motivation and insights from previous literature below."}, {"title": "2.1 Descriptions for Digital Visual Media", "content": "To understand digital visual media, BVI people typically rely on textual descriptions. World Wide Web Consortium has established Web Content Accessibility Guidelines for creators to add proper captions to images [82] and audio descriptions to videos [62, 80, 81] for BVI people to receive equal information as sighted people. Several platforms allowed BVI people to request descriptions for images and videos that lack accessible visual descriptions from volunteer describers, such as YouDescribe [38] and VizWiz [19]. Despite the availability of these resources, learning those guidelines and providing good descriptions remain difficult [60]; the scarcity of human resources also makes it hard to address the high volume of requests from BVI people [31], who may have different information needs based on their access contexts [73, 74].\nTo address these challenges, semi-automatic AI systems have been developed to streamline the description authoring process, such as generating initial image captions [32] or audio descriptions [64, 72, 85]. Although these systems reduce laborious tasks, they still require human effort to make one-size-fits-all descriptions usable. Recently, VLM-powered systems can generate high-quality audio descriptions comparable to human describers [77] and allow BVI people to query visual details interactively [8, 24, 37, 72]. However, the asynchronous and one-size-fits-all nature of descriptions makes it difficult to adapt these tools to dynamic real-world scenarios. In response, this work aims to provide live contextual descriptions for BVI users by understanding their intent and visual contexts. This is achieved through a description generation pipeline with dynamic prompt assignments based on user contexts, and different VLMs that balance the tradeoffs between their richness and latency to achieve real-time purposes."}, {"title": "2.2 Descriptions for Real-World Accessibility", "content": "Accessing the real world through descriptions enhances BVI individuals' independence in various tasks, such as object identification [17, 36, 58], line following [46], and navigation [3, 9, 16, 30, 42, 43, 45, 47]. Navigation is especially important but challenging in unfamiliar settings, which demands extensive environmental understanding [30, 45], such as recognizing intersections [45, 47], signs [15, 45, 83], and traffic light statuses [26, 75]. While these systems offered critical task-specific guidance (e.g., audio directions), they lacked visual descriptions for ever-changing surroundings. Tools, such as SeeingAI [11], ImageExplorer [49], and Envision AI [8], enabled BVI users to snap a photo and receive comprehensive visual descriptions within seconds, while BeMyAI [8] allowed BVI people to access details through turn-by-turn interactions (Table 1). Yet, their utility falls short in rapidly changing visual scenes that require live and continuous descriptions.\nAn alternative for understanding the dynamic real world is through human assistance, such as RSA, which connects BVI users with sighted agents via video calls to fulfill requests through verbal"}, {"title": "2.3 Fulfilling Diverse Needs of BVI People", "content": "Creating high-quality descriptions that meet BVI people's diverse information needs is challenging for different visual media. Prior research found that current one-size-fits-all approaches to image descriptions are insufficient for providing necessary details for meaningful interaction [44, 57, 67, 68, 73, 74]. Stangl et al. [73, 74] identified that the source of an image and the user's information goal impacted their information wants, and proposed universal terms (e.g., having identity or names for describing people) as minimum viable content, with other terms provided on demand based on users' contexts (e.g., person's height, hair color, etc.). Guidelines also indicated that the inclusion of certain visual details should be context-based, such as having general information for first access or having details (e.g., color, orientations of objects) when gauging one's understanding of certain image content [13, 65].\nThe varied information needs of BVI people were also found when accessing different types of videos [39, 59, 72], such as different preferences on the audio description content (e.g., object details, settings), and output modalities (e.g., audio, tactile). For 360-degree videos, which offer richer visual information and immersion, BVI people also have varied preferences for linguistic aspects (e.g., level of details, describing from first- or second-person view), or audio presentations (e.g., spatial audio) [25, 40]. Thus, the way of presenting visual information and determining its richness is crucial and depends on the user's needs and context.\nThese findings of users' varied preferences for digital media also extended to the real world. Herskovitz et al. [33] identified the needs of BVI individuals, who often customize assistive technologies for daily activities by combining mobile apps for different visual tasks, such as obtaining clock directions from Compass or descriptions from BlindSquare, or filtering visual information (e.g., text or colors). Overall, prior work in both the digital and real worlds has highlighted the importance of customization for adapting to diverse user preferences and contexts. In this work, we explore live visual descriptions that are context-aware, by enabling customization options on the description content (e.g., level of details, visual attributes), and audio presentation (e.g., pausing or increasing volume) to tailor to the diverse needs of BVI people."}, {"title": "3 FORMATIVE STUDY", "content": "We conducted a formative study to identify design requirements for a system providing live visual descriptions in the real world. We conducted semi-structured interviews with 5 BVI participants (Table 2) to gather feedback on their needs through several potential scenarios. We developed scenarios considering several aspects, including user intent, familiarity with environments, visual complexity, and sound contexts. We developed scenarios considering user intent, familiarity with environments, visual complexity, and sound contexts, as these aspects were identified in previous works as influencing information needs [16, 23, 30, 44, 45, 73, 74]. Participants were asked to imagine using a future live description system capable of capturing visuals and sounds in their surroundings and brainstorm their needs and potential solutions. From these discussions, we extracted key insights reflecting participants' needs and strategies, which we used in the design of WorldScribe."}, {"title": "3.1 Design Considerations", "content": "We reported design considerations derived from our participants:\nD1 - Overview first, adaptive details on the fly. Participants emphasized the need for descriptions with proper levels of granularity, depending on their context. They preferred immediate and succinct information when several important events occurred simultaneously (e.g., multiple barriers and directions during navigation), and longer and detailed descriptions when there was no time pressure (e.g., understanding artwork in a museum). When searching for something, they wanted an overview of the space, including landmarks and spatial locations, followed by more details as they approached the target or encountered similar items requiring differentiation. This approach aligns with the \"Overview first, zoom and filter, then details-on-demand.\" by Shneiderman [70]. Therefore, our solution should provide the proper level of information and delve into details when users express interest.\nD2 - Prioritize descriptions based on semantic relevance. Participants mentioned strategies for filtering and prioritizing complex visual information. The most commonly noted strategy was to prioritize descriptions relevant to their goals of context, such as road signs or barriers during navigation, available stores and offerings during meal times, etc. They also emphasized that nearby objects are more important for safety and should be prioritized over distant information. Our system should present information most relevant to the user's goals and proximity to ensure timely and practical use.\nD3 - Enable customizability for varied user needs. Similar to prior work in Section 2.3, we observed varied individual preferences from our participants. They expressed different information needs depending on the context. For example, descriptions should consider their mobility, such as providing information about objects out of their cane reach (e.g., hanging lights, cars with high ground clearance) or focusing on dynamic obstacles but not static ones in their familiar environments. Participants also noted that sound context influences the consumption of descriptions, with some suggesting pausing or increasing the volume in noisy environments. Some preferred manual control over these options, while others pointed out that the automatic approach would benefit urgent or busy scenarios, such as navigation or if the description content is crucial. Based on these findings, WorldScribe should offer customizable options for description content and presentation to meet diverse user needs."}, {"title": "4 WORLDSCRIBE", "content": "WorldScribe is a system that provides live visual descriptions for BVI people to facilitate their environmental understanding. BVI users can specify their intent, general or specific, which will be decomposed by WorldScribe into specific visual attributes and objects of relevance (INTENT SPECIFICATION LAYER). Then, WorldScribe extracts key frames from the camera video stream (KEYFRAME EXTRACTION LAYER), and employs a suite of vision and language models to generate rich descriptions (DESCRIPTION GENERATION LAYER). The descriptions are prioritized based on users' intent, proximity, and timeliness (PRIORITIZATION LAYER), and presented with audio manipulations based on sound context (PRESENTATION LAYER)."}, {"title": "4.1 Scenario Walkthrough of WorldScribe", "content": "Here, we illustrate WorldScribe in an everyday scenario, taking Brook as the main character, a graduate student who is blind.\nBrook just finished his advising meeting, and he wants to find a lab laptop with powerful computational resources to proceed with his project. The lab is filled with large items like TVs, workbenches with electronics, rows of seats with monitors, personal items, cabinets, and garbage bins, with their layout changing daily based on activities. The only cues Brook has from his labmate are that the laptop is silver (Figure 4a) and located around the student seats amid office or personal objects (e.g., monitors, adapters, backpacks).\nArriving at the spacious lab, Brook specifies his goal by talking to WorldScribe (Figure 4b): \"find a silver laptop on the desk, and monitors or other office objects might be around it.\" He then aims his smartphone camera forward with WorldScribe on. Along the way, WorldScribe provides concise and timely descriptions of objects not directly related to his goal, where several fixtures, such as \"TVs\", \"cabinets\", \"workbenchs\", help him orient himself (Figure 4c).\nAs he approaches his seat, surrounded by relevant items like monitors, chargers, cables, and adapters, WorldScribe becomes more verbose (Figure 4e), providing descriptions, such as \"A black monitor is on with several windows opened\", \"A white adapter is next to an open laptop\", and \"A desk with several laptops on it\". These help Brook ascertain that the laptop he seeks is nearby.\nBrook then scans his surroundings slowly with WorldScribe, listening to the detailed descriptions with objects' visual attributes to help distinguish the laptop he is looking for (Figure 4f), such as \"A black laptop with colorful labels on it is opened on the wooden desk\", and finally, \"A silver laptop with an Apple logo and black keyboard is opened on the white desk\".\nIn the lab, people talk, type on keyboards, or use power tools, generating various noises. Brook is accustomed to these sounds and has customized WorldScribe to accommodate different interference (Figure 4d). For instance, when his labmates talk, Brook wants to join the conversation, so WorldScribe immediately pauses descriptions to let him listen and resumes when they stop talking. Also, if a cellphone or clock rings suddenly, WorldScribe automatically increases the description's volume to ensure he can hear it clearly.\nAfter working for a while, Brook takes a break on the building's balcony and uses WorldScribe to explore his surroundings (Figure 5a). The balcony has several plants, benches on the sides, and coffee tables. When Brook aims the camera at the sky (Figure 5b), WorldScribe describes \"The sky is cloudless, and the sunlight is casting a warm glow over the bench\". Brook then turns to the plants (Figure 5c), wondering if they have begun to germinate in this early spring period, and WorldScribe describes: \"Several young plant seedlings in a stage of early growth.\" The beautiful view revitalizes"}, {"title": "4.2 User Interface", "content": "WorldScribe has a mobile user interface that takes the user's camera stream, environmental sounds, and user customizations as inputs for generating descriptions (Figure 6). The interface includes three pages: (i) a main page with a camera streaming view and speech interface, (ii) a customization page for visual information, and (iii) a page for customizing audio presentation. Users can open the camera on the main page and use speech to indicate their intent (Figure 6a), such as \"find my cat, short hair and pale brown.\" To customize visual attributes of their interest, users can also use the speech on the main page, such as \"I am interested in color\" or \"Tell me everything is pale brown\", or manually toggle options on or off (Figure 6b). Similarly, users can verbally change the presentation of descriptions, such as \"Pause when someone talks\" or \"Increase volume during ringtone\", or manually select the options through the picker (Figure 6c)."}, {"title": "4.3 Intent Specification Layer", "content": "In this layer, WorldScribe aims to obtain the user's intent and needs on visual information to enable customizability (D3). Users specify their intent on the mobile interface, and WorldScribe will classify it as general or specific and generate relevant object classes and visual attributes by prompting GPT-4 [5]. If the intent is general or not specified (Figure 7a&b), WorldScribe takes object classes from existing datasets (e.g., COCO [54], Object365 [69]). If the intent is specific (e.g., \"Find a silver laptop on the desk, and other office objects might be around it\"), WorldScribe prompts GPT-4 [5] to generate a list of relevant objects (e.g., \"[laptop, desk, monitor, ...]\") and adjust visual attributes of interest (e.g., color and spatial information) to Verbose. WorldScribe then uses YOLO World [27] and ByteTrack [86] to support open vocabulary object recognition and tracking. Users can further refine the generated object classes and other visual attributes through speech (Figure 6a) or manually on the customization page (Figure 6b)."}, {"title": "4.4 Keyframe Extraction Layer", "content": "In this layer, WorldScribe aims to identify keyframes that indicate salient visual changes or user interests in the visual scene. To achieve this, our approach uses two methods: camera orientation and visual similarity. First, WorldScribe monitors changes in the camera's orientation using the phone's inertial measurement unit (IMU). A keyframe is selected whenever the camera's orientation shifts by at least 30 degrees (one clock unit) from the previous keyframe, indicating a possible turn into a new visual scene.\nSecond, WorldScribe determines a keyframe by analyzing visual changes across frames. To minimize detection errors, such as misassigned object classes or IDs, we assess the consistency of object composition over $n$ consecutive frames. In each $i^{th}$ frame, a detected object is represented as $(ID_i, C_i)$, where $ID_i$ is the object's index, and $C_i$ is the class. Thus, all objects in the $i^{th}$ frame are represented as $O_i = (ID_i, C_i)$. A keyframe is identified if the object composition remains consistent across $n$ frames, denoted as $O_i = O_{i+1} = ... = O_{i+n-1} \\neq \\emptyset$. The $(i+n-1)^{th}$ frame is then taken as the keyframe. Furthermore, to determine if the user is interested in a visual scene and requires details (conforming to D1), we check the $m$ consecutive keyframes with the same composition and use the latest keyframe to prompt VLMs for detailed descriptions.\nIn scenarios where the object compositions across $n$ consecutive frames are empty, denoted by $O_i = O_{i+1} = ... = O_{i+n-1} = \\emptyset$, it suggests that the predefined object classes may not cover the objects in the scene, resulting in false negatives. Therefore, we still take the $(i+n-1)^{th}$ frame as the keyframe. To eliminate genuinely empty scenes (e.g., aiming at a plain white wall), we measure the similarity between the candidate frame and the previous keyframe. We calculate the cosine similarity ($cos\\_sim$) between the image feature vectors of the two frames, extracted from the FC2 layer of the VGG16 [71]. If $cos\\_sim$ is lower than a threshold $thres$, we count the frame as a keyframe. Furthermore, we observed situations where object compositions differ across consecutive $n$ frames, denoted by $O_i \\neq O_{i+1} \\neq ... \\neq O_{i+n-1} \\neq \\emptyset$. These changes often indicate camera drifting or objects moving in and out of view. In such cases, we check every $2n$ frame, selecting the $(i+2n-1)^{th}$ frame as a keyframe if the condition is satisfied. In our implementation, we empirically set $n=5$, $m=3$, and $thres=0.6$."}, {"title": "4.5 Description Generation Layer", "content": "In this layer, WorldScribe aims to generate descriptions with adaptive details to the user's intent and visual contexts. To achieve this, WorldScribe leverages a suite of VLMs that balances the tradeoffs between their richness and latency to support real-time usage (Figure 8).\nTo provide overview first and details on the fly (D1), WorldScribe recognizes objects by YOLO World [27] and structures its results into short phrases, e.g., \"A chair, a laptop, a monitor, ...\", allowing it to provide an overview of objects in the visual scene in real time (Figure 8b). Then, WorldScribe describes objects and their spatial relationship by prompting Moondream [10] (Figure 8c), a compact vision language model, that can achieve a decent performance in terms of latency and accuracy on this information based on our observation. Finally, WorldScribe prompts GPT-4v [7] to generate descriptions of different levels of details based on user contexts (D1). It offers three levels of detail: Verbose, Normal, and Concise, associated with prompts specifying e.g., \"over 15 words\", \"at least 10 words\", and \"less than 5 words\", respectively. WorldScribe dynamically adjusts these length constraints based on visual complexity. For instance, it becomes Verbose if the visual scene is focused on for a long period, indicating user interest, with consecutive keyframes identified (See Section 4.4). It becomes Concise when multiple objects of interest are detected in consecutive keyframes to ensure timely coverage; otherwise, it remains Normal by default. Along with the recognized visual attributes of interest, WorldScribe dynamically creates prompts to suit users' intent (D3):\n\"You are a helpful visual describer, who can see and describe for BVI people. You will not mention this is an image; just describe it, and also don't mention camera blur or motion. Please ensure you provide these adjectives to enrich the descriptions [desired visual attributes (e.g., color, texture, shape, spatial), with examples adjectives], you should describe each object with ONLY ONE sentence at maximum. Don't use 'it' to refer to an object. Most importantly, each sentence should be [sentence length constraint e.g., verbose, normal or concise].\""}, {"title": "4.6 Description Prioritization Layer", "content": "In this layer, WorldScribe aims to select a description based on the match to the user's intent, proximity to the user, and relevance to the current visual context (D2)."}, {"title": "4.6.1 Sorting GPT-4v detailed descriptions by semantic relevance.", "content": "Descriptions from YOLO World [27] and Moondream [10] provide an overview and general information, which should always be prioritized to help the user construct an initial understanding of the new visual scene (D1). In contrast, detailed descriptions from GPT-4v [7] may contain information irrelevant to the user's intent. Therefore, WorldScribe ranks the descriptions generated by GPT-4v [7] based on their relevance to the user's intent and the proximity of the described content to the user; the nearer, the earlier to describe. To achieve this, we first get a set of descriptions from GPT-4v [7] $S = \\{s_i\\}$. For a description $s_i$, we compute the sentence similarity score $SIM(s_i)$ to the user's intent, and a depth score $Depth(s_i)$. To calculate the depth score, we extract the subject and its descriptors from a description (e.g., \"Two cylindrical trash receptacles\" in Figure 9), locate each in the frame using CLIPSeg [56] (Figure 9b), and crop the salient area on the depth map generated by Depth Anything [84]. We then compute the average depth $Depth(s_i)$ of the cropped area for each description $s_i$.\nTo sort the descriptions in S, We divide them into two sets $S_a = \\{s_i | SIM(S_i) >= Threshold\\}$, and $S_l = \\{s_j | SIM(S_j) < Threshold\\}$. We then sort the descriptions in $S_a$ as a sorted sequence $A = (s_0, s_1, ..., s_{n-1})$ based on the similarity score such that $SIM(s_i) >= SIM(s_{i+1})$. We also sort sentence in $S_l$ as $B = (s_n, s_{n+1}, ..., s_{n+m-1})$ based on the depth score such that $Depth(s_j) >= Depth(s_{j+1})$. Finally, we concatenate the two sequences into final one, $A^\\^B = (s_0, ..., s_{n-1}, s_n, ..., s_{n+m-1})$. This approach ensures the initial descriptions are highly relevant to the user's intent, regardless of proximity. The remaining descriptions are sequenced by their proximity, with nearer elements described sooner."}, {"title": "4.6.2 Selecting up-to-date description based on current context.", "content": "In reality, users may move or turn frequently, and the environment can change significantly, leading to frequent visual changes and generating multiple keyframes and VLM requests in a short time. As VLM inference times vary, some descriptions may become outdated if they take too long to process and will not be useful to the user's current visual context. Thus, we need to select descriptions that best represent the current visual scene. We consider four criteria: camera orientation, object compositions, frame similarity, and similarity to previous descriptions. A description is selected if it satisfies any of these criteria. First, we check if the candidate description's referenced object composition matches the current scene. Second, we compare the user's orientation from the description's referenced frame to the current orientation. Third, we compare the frame similarity between the description's referenced frame and the current frame using feature vectors extracted through VGG16 [71]. Descriptions similar to preceding spoken descriptions are skipped, and the description buffer is renewed when a description is omitted. Descriptions generated by GPT-4v [7] are prioritized, followed by those from Moondream [10] and YOLO World [27]."}, {"title": "4.7 Presentation Layer", "content": "In this layer, WorldScribe aims to make descriptions audibly perceivable to the user by considering users' sound context. To achieve this, WorldScribe runs a sound detection module in the background and automatically manipulates the presentation of the descriptions accordingly. Based on our formative study, WorldScribe enables two audio manipulations on descriptions for the user to better perceive the description content in the noisy environment: (i) Pausing and (ii) Increasing volume. Users can find sound events that interest them and customize the corresponding manipulations on descriptions in WorldScribe app (Figure 6c)."}, {"title": "4.8 Implementation Details", "content": "WorldScribe servers included a local server running on a Macbook M1 Max and another remote server with two embedded Nvidia GeForce RTX 4090. WorldScribe mobile app was built on an iPhone 12 Pro and streamed the camera frames to the local server through a Socket connection. YOLO World [27] and ByteTrack [86] were run on the local server with 5 frames per second (FPS) along with other algorithms, while other models in description generation and prioritization pipeline, including Moondream [10], Depth Anything [84] and CLIPSeg [56] were run on the remote server for each keyframe, as well as the pre-trained model \"all-MiniLM-L6-v2\" for sentence similarity from an open-sourced implementation on Huggingface. Overall, based on the data collected in our user study (Section 5), WorldScribe achieved an overall latency of 1.44s, and"}, {"title": "5 USER EVALUATION", "content": "We conducted a user evaluation with six BVI participants, where they used WorldScribe in three different contexts. This study aimed to explore RQ1: How do users perceive WorldScribe descriptions in various contexts? and RQ2: What are the gaps between WorldScribe descriptions and users' expectations? We detail our study method and results below."}, {"title": "5.1 Participants", "content": "We recruited six BVI participants (2 Male and 4 Female) through public recruitment posts on local associations of the blind. Participants aged from 40 to 87 (Avg. 62.3) and described their visual impairment as blind (N=3) or having residual vision (N=3). Some participants had prior experiences using RSA services and used AI-enabled services, such as BeMyEyes [2] or SeeingAI [11] in their daily lives (Table 2)."}, {"title": "5.2 Study Sessions", "content": "We enacted three different scenarios: (i) specific intent, (ii) general intent, and (iii) user-defined intent. In each session, the descriptions were automatically paused if the speech was detected, including the conversation between participants and the experimenter, and the volume was automatically increased if a ringtone occurred.\nScenario with specific intent. The first scenario, similar to the walkthrough scenario (Section 4.1), happened in our lab space, which is furnished with glass walls, wall-mounted TVs, several work benches with electronics and equipment, several rows of seats with monitors and scattered personal items, and a small kitchen area with microwave, fridge, sink and a lot of cabinets and garbage bins at the corners. The user's intent is \"find a silver laptop on the desk, and monitors or other office objects might be around it.\" This scenario was designed to encourage them to think about the descriptions they need for specific purposes and whether WorldScribe supplements or obscures their intent.\nScenario with general intent. This scenario happened on one of our building's floors, which has many common objects on the intricate hallways, such as poster stands, carts for construction, trash cans, desks, and sofas. On the wall or doors, there were several artworks, paintings, posters or emergency plans, and TVs. Random people were also walking in the hallway or meeting at public tables during the study. The intent of the scenario is \"I am exploring a school building. Describe general information on the appliances and the building decorations.\" This scenario was designed to prompt users to think if WorldScribe descriptions support their understanding of the environment.\nUser-defined scenario. After experiencing the previous scenarios, participants were asked to develop their own defined scenarios. They can also customize their desired visual attributes in WorldScribe mobile app based on their needs. We then took participants to the place they wanted to explore near our experiment sites.\nLimitations. Though we tried to create different real-world scenarios, our study was conducted within our local environment and buildings. This setting may not fully capture the diversity and complexity of real-world environments, potentially limiting the generalizability of our findings to other contexts."}, {"title": "5.3 Procedure", "content": "After providing informed consent, participants were introduced to WorldScribe and the functionalities they could customize, and experienced through each session. Participants opted to either hold the camera on their front or wear the lanyard smartphone mount we prepared. To facilitate the study progress and avoid fatigue, we kept each exploration for around ten minutes or until participants paused spontaneously. At the end of each session, we interviewed our participants about their experiences with WorldScribe. The study took about two hours, and participants were compensated $50 for their participation. This study was approved by IRB in our institution."}, {"title": "5.4 Measures and Analysis", "content": "We asked our participants to comment on their perceived accuracy and quality of descriptions, their confidence in WorldScribe descriptions, and several other open-ended questions. We recorded and transcribed the interviews and recorded all interactions with WorldScribe, which was also used for our pipeline evaluation (Section 6). Two researchers coded all qualitative interview feedback received in all sessions for further analysis via affinity diagramming."}, {"title": "5.5 Results", "content": "5.5.1 Perceived accuracy and skepticism towards the descriptions. Participants perceived WorldScribe descriptions as accurate based on the contextual clues they ascertained but remained skeptical due to a few observed erroneous instances"}]}