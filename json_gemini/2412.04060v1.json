{"title": "Expanding Deep Learning-based Sensing Systems with Multi-Source Knowledge Transfer", "authors": ["Gaole Dai", "Huatao Xu", "Rui Tan", "Mo Li"], "abstract": "Expanding the existing sensing systems to provide high-quality deep learning models for more domains, such as new users or environments, is challenged by the limited labeled data and the data and device heterogeneities. While knowledge distillation methods could overcome label scarcity and device heterogeneity, they assume the teachers are fully reliable and overlook the data heterogeneity, which prevents the direct adoption of existing models. To address this problem, this paper proposes an efficient knowledge transfer framework, HaKT, to expand sensing systems. It first selects multiple high-quality models from the system at a low cost and then fuses their knowledge by assigning sample-wise weights to their predictions. Later, the fused knowledge is selectively injected into the customized models for new domains based on the knowledge quality. Extensive experiments on different tasks, modalities, and settings show that HaKT outperforms stat-of-the-art baselines by at most 16.5% accuracy and saves up to 39% communication traffic.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of deep learning with sensing systems has gained significant attention in recent years due to its effectiveness in processing diverse sensory data across a wide range of applications [16, 23, 24, 43]. As deep learning techniques continue to mature and computing resources advance, there is an increasing demand for the large-scale deployment of these models in sensing systems [8]. However, despite this growing need, the challenge of expanding these deep learning-based systems remains largely unaddressed. In this context, expansion refers to the ability to develop effective models that can seamlessly accommodate new users, devices, environments, or datasets.\nWe identify the problem of expansion in sensory applications as particularly challenging. First, labeling sensor data is both time-consuming and costly, resulting in limited labeled data for new targets, which undermines the effectiveness of direct supervised training methods [44]. Second, data collected from various sources, such as different users, devices, or environments, often exhibit varying distributions. In more extreme cases, data from different sources may represent different categories, leading to performance degradation or rendering existing models inapplicable when transferred to new targets [43]. Third, devices across domains may differ in computational power and memory resources, imposing constraints on model architecture and making current models less suitable for new devices [41]. Moreover, developing models for new targets must also account for overall training and communication costs. Given these challenges, this work addresses a critical question: How can we effectively and efficiently expand existing sensing systems?\nFigure 1 illustrates a realistic scenario for the sensing systems expansion. Existing domains, such as different users, devices, datasets, or organizations, maintain heterogeneous models for processing local data, referred to as the source domains. Due to privacy concerns, the data from one source domain cannot be accessed by others. The new targets, i.e., target domains, have limited labeled data but many unlabeled data due to the large labeling overhead. The data within these domains are non-independent and identically distributed (non-IID). The objective is to provide high-quality customized models for the target domains at a minimal cost.\nExisting approaches struggle to work in the practical scenario. Directly training models on target domain data is less effective due to label scarcity [34, 44]. Federated learning frameworks train models using distributed data from multiple clients [4, 25, 47]. However, many of them provide"}, {"title": "2 MOTIVATION STUDY", "content": "2.1 Problem Formulation\nThere are Ns source domains Ds within an sensing system, each possessing heterogeneous models for data analytics, represented by {{\\textit{f}}_i, {\\textit{g}}_i}, i = 1,..., Ns, where \\textit{f}i and \\textit{g}i are the encoder and classifier for the i-th domain, respectively. The objective is to expand the sensing system to Nr target"}, {"title": "2.2 Potential of Knowledge Transfer", "content": "To demonstrate the potential of knowledge transfer for providing high-quality models, we compare two naive approaches: direct training and model transfer. Direct training only train with the limited labeled data (10%) in the target domain, whereas the model transfer applies the source models to the target domain. Both methods are evaluated on the HARBox dataset (see Section 5.1). In Figure 2(a), the performance of the source models before transfer (after transfer) is measured on the source (target) domain test sets.\nFigure 2(a) demonstrates that direct training yields only around 50% accuracy under conditions of label scarcity. When models from the source domains are applied to the target domain data, they suffer varying degrees of performance degradation due to data heterogeneity. However, the model from domain 5 maintains an accuracy of over 60%, which might be due to a more similar data distribution. These results suggest that transferring high-quality models with appropriate selection strategies to new targets could outperform direct training. However, device heterogeneity and the customized requirements of target domains may restrict the direct adoption of these models. Additionally, when source and target domains exhibit substantial differences in data distribution or contain different categories, even the best-performing model may still fall short of the desired accuracy."}, {"title": "2.3 Limitations of Existing Methods", "content": "We further investigate the limitations of existing knowledge transfer methods on the HARBox dataset, including one domain adaptation method (LEAD [28]), one model merging method (MEHLSoup [20]), and one knowledge distillation method (DistillWeighted [3]). See Section 5.1 for detailed descriptions. Additionally, we introduce a variant of DistillWeighted, named Random, where source models are randomly selected during the knowledge transfer process. The target models are configured as TPN-M (see Section 5.1), and we report the average accuracy across ten target domains.\nIn Figure 2(b), both LEAD and MEHLSoup demonstrate suboptimal performance, as they are restricted to leveraging knowledge only from source models that match the target model architecture -a limitation that also applies to other domain adaptation and model merging methods. In contrast, knowledge distillation methods like DistillWeighted are not bound by source model architecture, leading to better performance. Moreover, while domain adaptation typically relies on a single source model, utilizing multiple source models, as demonstrated by DistillWeighted, results in better target models. This improvement arises because the knowledge from multiple teacher models is more comprehensive and can compensate for each other [45]. However, existing multi-source distillation methods cannot effectively handle the knowledge conflicts among different source models, which negatively impact the quality of the transferred knowledge.\nKnowledge conflicts arise when heterogeneous source models produce different predictions for the same input. Figure 2(c) illustrates the per-sample predictions of two models selected by DistillWeighted, which differ significantly. DistillWeighted simply assign fixed weights to each model, which fails to resolve these conflicts. When some selected models are inaccurate on the target due to data heterogeneity, the"}, {"title": "3 FRAMEWORK OVERVIEW", "content": "We further analyze the challenges in expanding sensing systems and introduce the proposed framework, HaKT."}, {"title": "3.1 Challenges in Scaling Sensing Systems", "content": "To efficiently expand sensing systems to new target domains, three key challenges must be addressed:\nFirst, it is difficult to identify high-quality source models within the sensing system. The performances of a model in its source domain does not necessarily reflect its performance after the transfer due to the varying discrepancies between different source and target domain pairs. Estimating performance degradation based on discrepancy measurements, such as Maximum Mean Discrepancy [6, 33], is impractical because these metrics require access to both source and target domain data, which violates data privacy constraints. Additionally, evaluating model performance solely on the limited labeled data in the target domains may not accurately reflect the model's in-the-wild performance, as those data can hardly capture the diversity of the target data [C1].\nSecond, transferring knowledge from multiple source models to customized target models is challenging. Due to data heterogeneity, the knowledge in source models may not be directly applicable to the target domain data. Additionally, knowledge conflicts occur when source models make conflicting predictions for the same input, leading to contradictory update directions and less effective model training. Resolving these knowledge conflicts is particularly difficult because the quality and relevance of the knowledge from each model can vary dynamically across different samples. For example, a model's prediction may be accurate for some subsets of target data but unreliable for others. This context-dependent variability makes it challenging to determine which model's knowledge should be prioritized [C2].\nThird, minimizing system overhead is crucial during expansion. Evaluating source models on the target domains would incur significant communication and execution costs, especially given the large number of source and target domains in sensing systems. However, if all models are not tested on the target domains, some high-quality models might be discarded, leading to suboptimal performance for new targets. Furthermore, leveraging the knowledge from multiple models to train the target model can also introduce large training overhead, which needs to be minimized particularly for resource constraint devices [C3]."}, {"title": "3.2 System Architecture", "content": "To facilitate efficient sensing system expansion, HaKT is proposed to address the limitations of existing methods and overcome the three aforementioned challenges. The overview of HaKT is presented in Figure 3. HaKT first determines a suitable model skeleton either directly provided by the target or from a model library with device profiling. The model library includes open-source models from the Internet and the architectures from the source domains. Then the Efficient Model Selection Protocol is evoked to select high-quality source models at a low cost. Later, the Sample-wise Knowledge Fusion is performed to aggregate the conflicting knowledge. Subsequently, the target model is trained with the Adaptive Knowledge Injection based on a low-cost training scheme. In HaKT, three core technical components are designed:\n(1) Efficient Model Selection Protocol involves a two-stage process to identify high-quality models at a low cost. To avoid full model transmission and execution, a coarse-grained selection first filters out less effective models based on lightweight features [C1, C3]. The remaining model candidates are then transmitted to the targets for a fine-grained selection, which evaluates them using both labeled and unlabeled data [C1].\n(2) Sample-wise Knowledge Fusion is proposed to resolve the knowledge conflicts. An attention-based mixer is trained to assign sample-specific weights to the predictions of the selected models based on the representation proximity extracted by the selected source and target models [C2]. Additionally, the selected models are partially frozen and fine-tuned to enhance the quality of the fused knowledge [C2, C3].\n(3) Adaptive Knowledge Injection facilitates the injection of fused knowledge to the target models. It utilizes a Knowledge Dictionary to selectively store the fused knowledge and an adaptive learner to dynamically adjust the importance of the knowledge based"}, {"title": "4 DESIGN OF HAKT", "content": "4.1 Efficient Model Selection Protocol\nThe process of the Efficient Model Selection Protocol is detailed in Figure 4. In the Feature-based Coarse Selection stage (Steps FbCS.1-3), domains with potentially high-performing models are identified by comparing lightweight features between the source and target domains. Only source domains with high feature similarity transmit their models to the target, reducing communication traffic and model execution costs. During the Centroids-Accuracy Joint Selection stage (Steps CAJS.1-3), the transmitted models extract centroids and estimate their accuracy on the target domain data, which are leveraged to select the final models."}, {"title": "4.1.1 Feature-based Coarse Selection", "content": "Lightweight features l = {l1, l2, ...}, including the mean value, standard deviation, skewness, and kurtosis, are extracted from both the source domains and target domain data. These features provide a coarse-grained description of domain characteristics without relying on any learning models. The target domain transmits its extracted features to the source domains, where each source domain i computes a feature similarity score: Si = 1/J\u2211j=1sim(ljt, lji) where J is the total number of features. The similarity function sim(\u00b7) is set to cosine similarity. The target domain then selects the top \u03b7% of source domains with the highest similarity scores. Only the selected domains that receive a model inquiry from the target domain transmit their models back. This approach significantly reduces communication traffic, as only a small percentage of models are transmitted and high-level features are much smaller in size compared with those models."}, {"title": "4.1.2 Centroids-Accuracy Joint Selection", "content": "After the models are received in the target domain, they are further selected"}, {"title": "4.2 Sample-wise Knowledge Fusion", "content": "To resolve knowledge conflicts, an attention-based mixer is trained to assign sample-wise weights to the selected models based on their relative importance. Simultaneously, a cost-effective adaptation is applied to the source models to enhance their knowledge quality."}, {"title": "4.2.1 Attention-based Mixer", "content": "The attention-based mixer aggregates conflicting predictions from the selected models by leveraging the sample-wise feature adjacency between the source and target models. The feature ht(k) extracted by the target encoder is projected through a linear layer Lquery to obtain the query vector q(k). Similarly, the features hi(k), extracted by the selected source models, are projected through the respective linear layers Lkey to obtain the key vectors keyi(k):\nq(k) = L^{query}(h_t(k)),\nkey_i(k) = L^{key}_i(h_i(k)).\nDifferent from the traditional attention mechanism [38], which uses a single linear layer to compute the keys, the mixer utilizes multiple linear layers L^{key}_i to accommodate the heterogeneous source model architectures. Since the models from different source domains extract features with varying dimensions, the input size of each L^{key}_i must be tailored accordingly. The output size of L^{key}_i is standardized to a common dimension for subsequent computations. The similarities between the query and the keys are then calculated and normalized using SoftMax to obtain the attention score wi(k) for the i-th model on the data sample x(k):\nw_i(k) = SoftMax(q(k) \\cdot key_i(k)), i = 1, 2, ..., N_p\nThe attention score wi(k) measures the feature similarity between the target and the selected models, which is used to aggregate the predictions from the selected classifiers g_i(k):\np^{mix}(k) = \\sum_i w_i(k)g_i(h_i(k)).\nIf the features extracted by the target model and the i-th model are highly similar, a higher weight wi(k) is assigned to the prediction of the i-th model. This is because the i-th classifier is likely to be more accurate on data from a distribution that closely resembles the data it is trained with. However, training the attention-based mixer is challenging, as it depends on input from the target model, which itself requires training. To address this, a low-cost joint training"}, {"title": "4.2.2 Cost-effective Adaptation", "content": "To further improve the accuracy of the fused predictions, a cost-effective adaptation is applied to the selected models, enhancing the prediction accuracy of each. Since adapting all Np selected source models would be computationally expensive, only their classifiers are trained jointly with the mixer, while their encoders remain frozen. This approach reduces the computational burden, as classifiers are typically lightweight [12]. Early-stage experiments indicate that adapting the classifiers alone is sufficient to provide high-quality predictions for the attention-based mixer. Additionally, freezing the encoders accelerates the knowledge aggregation process. By precomputing features for all target domain data using the frozen encoders and storing them in memory, they are ready to be fetched when the mixer requires the prediction results for a sample x(k). This process eliminates the need to repeatedly execute the forward pass of the selected encoders, thereby reducing the overall computation time."}, {"title": "4.3 Adaptive Knowledge Injection", "content": "The fused knowledge is further distilled into the target model:\nL^{ada} = L^{label} + \u03b1 L^{distill}(g_t(f_t(x(k)), KD(k)),\nwhere Llabel represents the cross-entropy loss on the labeled data, and Ldistill denotes the distillation loss based on the pseudo labels. A knowledge dictionary KD and an adaptive learner are further designed to enhance the training efficacy."}, {"title": "4.3.1 Knowledge Dictionary", "content": "Directly learning from the fused prediction pmix can hinder the convergence of the target model. This is because the aggregation results from the mixer change dynamically during its training (e.g., the pseudo-label of a sample may shift from class A to class C), potentially leading to conflicting gradient update directions. Additionally, the quality of the fused predictions may fluctuate from epoch to epoch. To address this issue, a knowledge dictionary is introduced to provide more stable learning objectives. After each model update, the fused predictions from the attention-based mixer are stored in the knowledge dictionary only if the accuracy of the mixer improves. These predictions are stored in a soft-label format rather than as one-hot vectors, allowing the target model to capture the confidence levels of the mixer in its fused predictions. In subsequent epochs, the target model learns from these soft pseudo labels stored in the knowledge dictionary, rather than directly from the potentially unstable predictions generated by the mixer."}, {"title": "4.3.2 Adaptive Learner", "content": "Given the varying quality of the fused predictions in the knowledge dictionary, an adaptive learner is employed to adjust the weight \u03b1 of the distillation loss: \u03b1 = m(Acctrain \u2212 b), where Acctrain represents the accuracy of the attention-based mixer on the training data. The m and b are predetermined hyperparameters. The m controls the scaling factor of the weight \u03b1, while b serves as a threshold to prevent the target model from learning from fused predictions of low quality. The weight \u03b1 increases when the fused prediction accuracy is high, allowing the model to learn more effectively from reliable predictions."}, {"title": "4.3.3 Low-cost Joint Training", "content": "The training of the attention-based mixer, the unfrozen source classifiers, and the target domain model is complex, as they depend on inputs from each other. To enable a cost-effective training process, a joint training scheme is developed as shown in Algorithm 1."}, {"title": "5 EVALUATIONS", "content": "5.1 Experiment Setting\n5.1.1 Datasets. HaKT is evaluated on four datasets that span various modalities, tasks, and scales. Table 1 provides a summary of these datasets, with further details as follows:\nHARBox [25]. This dataset consists of 9-axis Inertial Measurement Unit (IMU) data collected via crowdsourcing from 120 users. It includes data for five activities, such as walking and hopping.\nImageNet-R [13]. This dataset contains over 30k images from 200 classes in 16 different styles. Each style can be considered a small dataset. We filtered out styles with limited data or unclear labels, resulting in 8 styles for experiments.\nNinaPro [27]. This dataset contains the electromyogram (EMG) data collected from 10 subjects. Two commercial EMG sensors, the Myo Armbands, are deployed around the elbows of the subjects for 6-class gesture recognition.\nAlzheimer's Disease (AD) [24]. This dataset consists of Alzheimer's Disease-related activity data collected from 16 home environments using multiple sensing modalities. It includes 11 activity classes, such as writing and sleeping.\nWhile HaKT is evaluated on these three diverse applications, it has the potential to extend to other sensing systems, such as traffic management or smart agriculture [17, 48], which we plan to explore in future work.\n5.1.2 Model Libraries. Table 1 summarizes the model libraries, which include six different models for each dataset."}, {"title": "5.1.3 Baselines", "content": "The five most relevant baselines are selected and slightly adapted for comparison:\nDistillWeighted [3]. DistillWeighted uses existing vision models to build models for new tasks. Based on the PARC metric [2], it assigns fixed weights to combine the predictions of all source models for knowledge distillation. As executing all source models is too expensive, we pre-select Np models using the PARC metric and then apply DistillWeighted.\nDistillNearest [3]. DistillNearest selects a single model from the most similar source domain based on the PARC metric. The target model then learns from the pseudo labels generated by the selected model and the labeled data.\nAccDistill [19]. Source domain models with top-k accuracy are selected and ensembled in [19]. To support model customization, we modify it by transferring the knowledge from the ensembled model to the target models.\nLEAD [28]. LEAD is a domain adaptation method that adapts the source model to builds instance-level decision boundary for target data using decomposed source features.\nMEHLSoup [20]. MEHLSoup merges multiple source domain models with a learned mixing coefficient, which is optimized by a block coordinate gradient descent algorithm on the target domain data.\nOther knowledge distillation, domain adaptation, or model merging methods are not included, as they have already been outperformed by the considered baselines [3, 20, 28]. Since LEAD and MEHLSoup, as well as other adaptation and merging methods, are unable to handle model heterogeneity, they are not directly comparable to HaKT. To make both methods executable, we select source domains with architectures that"}, {"title": "5.1.4 Real-world Testbed", "content": "The system is deployed on a server equipped with a 12th Gen Intel(R) Core i9-12900KF processor and an edge device, Nvidia Jetson Xavier. To simulate different source domains, the source domain data and models are stored in separate folders on the server due to the limited number of available devices. The target domain data is deployed on the edge device. The model training overhead, including time and memory usage, is measured on the edge devices, which are closely correlated with energy consumption, particularly on edge devices. The communication overhead is monitored by tracking the network traffic between the server and the edge device. The real-life deployment on other devices, such as smartphones or personal computers, is further discussed in Section 6."}, {"title": "5.1.5 Implementation Details", "content": "For different datasets, the learning rates of the target model and the mixer are searched among {5e-4, 1e-3, 5e-3, 1e-2}. The training epochs of both are set to 200 and 100. The scaling ratio m and the bias b in the adaptive learner are determined using a grid search within the ranges [1.0, 4.0] and [0, 0.5], with step sizes of 0.5 and 0.1, respectively. The Np is set to three."}, {"title": "5.2 Result Comparisons in One-Time Sensing Systems Expansion", "content": "5.2.1 Data Split and Training Details. We first compare the results of HaKT against the baselines in a one-time expansion setting, where one domain is randomly selected as the target domain, and the remaining domains serve as source domains. The source domain architectures are randomly selected from TPN-(S, M, L), ResNet-(18, 34, 50), ConvNet-(S, M, L), and ADNet-(S, M, L). The source domain models are trained using supervised learning on the labeled data of each domain. For the target domains, 60% of the data is randomly selected as the training set, 20% as the validation set, and the remaining 20% as the test set. Only \u03b3% of the training set data is labeled, while the rest remains unlabeled. For ImageNet-R, each of the eight styles is tested separately. For the other three datasets,"}, {"title": "5.2.2 Vaired Target Architectures", "content": "The results on different target architectures in the model library are presented in Figure 6. Notably, LEAD and MEHLSoup can only leverage source domains that share the same architecture as the target domains, making them inapplicable when the required architecture (e.g., CPC-S for HARBox) is not available among the source models. HaKT consistently achieves comparable or superior accuracy across four datasets for most architectures. Specifically, HaKT surpasses the best baseline by 7.0%, 16.5%, 15.9%, 6.4%, 5.0%, and 4.0% for CPC-(S, M, L) and TPN-(S, M, L), respectively. On ImageNet-R, when the target models are MobileNet-(S, L), HaKT achieves gains of 4.1% and 3.4% over the best baselines, respectively. Although HaKT performs slightly worse than MEHLSoup on NinaPro, it significantly outperforms MEHLSoup on the other datasets, likely due to the lower data heterogeneity in NinaPro, which aligns better with MEHLSoup's approach. Unlike domain adaptation and model merging methods that are limited to source models matching the target architecture, HaKT can effectively utilize knowledge from source models with diverse architectures, leading to improved performance. These results demonstrate HaKT's versatility in delivering high-quality customized models across diverse tasks and modalities."}, {"title": "5.2.3 Varied Portion of Labeled Data", "content": "The portion of labeled data is varied to evaluate the robustness of HaKT. As shown in Figure 7, HaKT outperforms or achieves comparable performance to the baselines in 13 out of 16 cases. Specifically, on the HARBox dataset, when \u03b3 is 0.10, 0.15, and 0.20, HaKT achieves accuracy improvements of 5.04%, 3.77%, and 4.56%, respectively. This is achieved by leveraging both labeled and unlabeled data to select high-quality models and generate accurate pseudo-labels for unlabeled data through sample-wise knowledge fusion. In few cases, the best baseline slightly outperforms HaKT (e.g., when \u03b3 = 0.05 on HARBox). This may be due to the use of a fixed threshold b in the adaptive learners, which may not yield an optimal weight \u03b1 across different values of \u03b3. To address this, we plan to explore dynamic thresholding to further enhance HaKT's robustness. Besides, we observe that the performance of the baselines is generally lower than reported in their original papers. This discrepancy is likely due to the more challenging experiment settings, which involve limited labels and large heterogeneities that HaKT is specifically designed to address.\nDuring the one-time expansion, HaKT reduces the average communication traffic by 39%, 36%, 8.3%, and 37.5% compared with baselines that use all source domain models on the HARBox, ImageNet-R, NinaPro, and AD datasets, respectively. Additionally, the source model execution cost in HaKT is reduced by approximately 40% when \u03b7 = 60%,"}, {"title": "5.3 Result Comparison on Multi-Round Sensing Systems Expansion", "content": "We further present the effectiveness of HaKT in scaling sensing systems in a multi-round expansion setting."}, {"title": "5.3.1 Data Split and Training Details", "content": "The domains in each dataset are randomly divided into five groups, denoted as {Gi, i = 0,\u2026\u2026,4}. Detailed information about the groups is provided in Table 4. In round j, the domains in {Gi, i = 0,..., j-1} serve as the source domains, while the domains in Gj are the target domains, with their model skeleton randomly selected from the model libraries. Once the models in Gj are ready for use, they are incorporated as source domains in the subsequent round j+1, sharing their knowledge with new targets for further expansion. For instance, during round 2 expansion on the HARBox dataset, the source domains include 60 users from G0 and G1, whose knowledge"}, {"title": "5.3.2 Multi-Round System Expansions", "content": "Table 2 presents the performance in different rounds of expansion. The average accuracy achieved by HaKT is 3.9%, 1.6%, 0.2%, and 5.8% higher compared with the best baselines on the four datasets. Due to the space limit, we present the detailed results of the four rounds on the HARBox dataset in Table 3, and similar results have been observed for other datasets. In Table 3, HaKT outperforms the best baselines by 4.2%, 3.2%, 3.1%, and 4.3% in accuracy. In round j, HaKT provides better customized models for group Gj since it selects better source models, fuses knowledge in finer granularity, and injects knowledge dynamically according to their quality. In contrast, DistillWeighted assigns fixed weights to models, which may not fully capture the varying quality of each source on different"}, {"title": "5.4 Ablation Study", "content": "5.4.1 Effectiveness of the Designs. Table 5 shows that the Feature-based Coarse Selection (FbCS) and Centroids-Accuracy Joint Selection can enhance performance by up to 6.7% and 4.4% in accuracy, respectively. The reason is that the statistical features and high-level representations based on labeled and unlabeled data could accurately reflect the domain similarity and the source model effectiveness. Besides, we observe that a larger improvement from the Coarse Selection typically leads to a smaller incremental gain from the Fine-grained Selection. This may occur because the lightweight features of specific modalities are sufficient for selecting high-quality models. However, the combination of both selection methods demonstrates stronger generalizability across different tasks and modalities. The Sample-wise Knowledge Fusion achieves an 11.5% accuracy improvement on HARBox, which is attributed to the sample-wise weights learned by the attention-based mixer could more effectively combine predictions from the source models. Additionally, the"}, {"title": "5.4.2 Alternatives in Model Selection", "content": "Several alternative model selection methods are compared with the selection approach of HaKT in Table 6. The knowledge transfer process in HaKT is applied to all selection methods. Accuracy and the PARC criteria achieve better performance compared with random selection. However, both methods rely on the labeled data, making them less effective when presented with label scarcity. In contrast, the Efficient Model Selection Protocol in HaKT leverages both labeled and unlabeled data for selection and avoids full model transmission, resulting in a 1.1% accuracy improvement while using only 61.3% traffic of the communication expense."}, {"title": "5.4.3 Alternatives in Knowledge Fusion", "content": "The Sample-wise Knowledge Fusion method is compared with other knowledge fusion methods in Table 7. Nearest represents no fusion, where one single model is selected and used. Weighted indicates the use of the fusion method from DistillWeighted. Equal refers to assigning equal weights to all selected models. The accuracy of the pseudo labels generated by HaKT is 11.6% higher than the best alternative method. Consequently, by learning from the higher-quality fused knowledge, the target models achieve a 16.5% improvement in accuracy."}, {"title": "6 DISCUSSIONS", "content": "Applicability to Resource-Constrained Devices. The diversity of device types in various sensing systems presents challenges for customized model training. To minimize system overhead during expansion, HaKT optimizes communication traffic through an efficient model selection protocol and reduces training memory and time with a low-cost joint training scheme, making the expansion process more feasible for edge servers. However, the complete model training process may still exceed the capabilities of battery-powered IoT devices and wearables. In such cases, offloading model training to nearby trusted edge servers or leveraging edge-cloud collaboration can serve as effective solutions [32, 40].\nPrivacy Concerns during System Expansion. Most domain adaptation methods require simultaneous access to both source and target domain data, which limits their applicability in privacy-sensitive scenarios [11, 28]. In contrast, HaKT better preserves data privacy by exchanging only high-level features and models between domains. While sharing"}, {"title": "7 RELATED WORKS", "content": "Transfer Learning. Transfer learning explores methods to apply existing models to new targets, overcoming data or task heterogeneities [26, 36]. To address data heterogeneity, domain adaptation techniques have been widely studied to align the feature distributions between source and target domain [11, 42, 49]. However, most methods require access to both source and target domain data during training, which is not feasible in the scenario considered. Additionally, some multi-source transfer learning approaches focus on selecting source models with better generalizability [1, 18, 37]. For example, [1] proposes constructing empirical predictors for model selection, which estimate the performance models could achieve after transfer. However, these methods often overlook device heterogeneity, which might prevent the direct adoption of source models in target domains.\nKnowledge Distillation. In knowledge distillation, an efficient student model is trained using the knowledge of one or more teacher models, such as their predicted pseudo labels or intermediate features [14, 22, 30, 39]. Specifically, multi-teacher distillation approaches [3, 22, 46] aggregate the knowledge of multiple teachers by assigning weights, aiming to provide the student model with more accurate and comprehensive knowledge. Most knowledge distillation studies focus on a closed-set problem, where teachers with high-quality knowledge are predetermined and available to use [3, 14, 22, 46]. However, in the sensing system expansion problem, the knowledge from source domain models may not be directly applicable to the target domain due to data heterogeneity, leading to suboptimal performance.\nModel Customization. Model customization has been extensively studied to meet specific computational and performance requirements [5, 29, 41]. Directly training customized models on target data is less effective due to label scarcity [24, 43]. To address this, self-supervised learning methods that leverage unlabeled data have been proposed to enhance performance [23, 44]. These methods are not included as baselines for performance comparison as they are orthogonal to HaKT. Some works explore pre-deployment or post-deployment model generation techniques based on the specific requirements and conditions of target environments [5, 21, 41]. While these methods focus on optimal architecture search in terms of latency and accuracy, HaKT emphasizes the knowledge transfer process from the selected source"}, {"title": "8 CONCLUSION", "content": "To efficiently expand sensing systems, a general knowledge transfer framework, HaKT, is designed to address label scarcity and data and device heterogeneities. HaKT employs an Efficient Model Selection Protocol to identify high-quality source domain models at a low cost. The knowledge from the selected models is aggregated using Sample-wise Knowledge Fusion, which assigns different weights to each sample. The fused knowledge is then distilled into customized target models through Adaptive Knowledge Injection. Extensive experiments across various tasks, modalities, and settings demonstrate the effectiveness and efficiency of HaKT compared with state-of-the-art baselines."}]}