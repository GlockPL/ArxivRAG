{"title": "Learning Autonomy: Off-Road Navigation Enhanced by Human Input", "authors": ["Akhil Nagariya", "Dimitar Filev", "Srikanth Saripalli", "Gaurav Pandey"], "abstract": "In the area of autonomous driving, navigating off-road terrains presents a unique set of challenges, from unpredictable surfaces like grass and dirt to unexpected obstacles such as bushes and puddles. In this work, we present a novel learning-based local planner that addresses these challenges by directly capturing human driving nuances from real-world demonstrations using only a monocular camera. The key features of our planner are its ability to navigate in challenging off-road environments with various terrain types and its fast learning capabilities. By utilizing minimal human demonstration data (5-10 mins), it quickly learns to navigate in a wide array of off-road conditions. The local planner significantly reduces the real world data required to learn human driving preferences. This allows the planner to apply learned behaviors to real-world scenarios without the need for manual fine-tuning, demonstrating quick adjustment and adaptability in off-road autonomous driving technology.", "sections": [{"title": "I. INTRODUCTION", "content": "Off-road planning and navigation present unique chal- lenges due to the unpredictable nature of various terrains and their geometric characteristics. Successfully navigating these environments requires leveraging both visual and geometric features effectively. Modeling tire-terrain interactions and vehicle dynamics across diverse off-road conditions is a com- plex task. Even with accurate models, tuning the planning algorithm to navigate safely across different terrains demands extensive time and expertise. In our research, we introduce a demonstration-based local planning algorithm that bypasses the need for directly modeling these intricate dynamic inter- actions. Instead, it learns navigation preferences from human driving data, demonstrating the ability to adapt these learned behaviors from simulations to real vehicles with minimal manual adjustments.\nOur approach uses utility functions to directly extract key features from segmented images and learns human driving behaviour using demonstration data. This approach diverges from traditional methods, which typically require either extensive labeled data for end-to-end learning or precise sensor calibration and global mapping in classical robotics approaches. By focusing on extracting key features directly in the trajectory space, our method simplifies the process, avoiding the complexity of global map generation. This allows our algorithm to effectively learn driving patterns from as little as 5-10 minutes of driving data. Our approach reduces the need for large datasets and detailed calibration, facilitating a more straightforward and efficient learning algorithm for navigating complex environments. The main contributions of this work are outlined below:\n\u2022 Fast learning capabilities: The planner can learn com- plex navigation behaviors from as little as 5-10 minutes of human demonstration data, significantly reducing the data requirements compared to traditional approaches.\n\u2022 Adaptability across diverse terrains: Our algorithm demonstrates the ability to navigate effectively in a wide array of off-road conditions, including mud, rock, water, and non-traversable areas\n\u2022 Human-like decision making: By learning from human demonstrations, our planner exhibits more intuitive and human-like navigation choices when faced with com- plex terrain configurations\n\u2022 Reduced need for manual tuning: Our approach by- passes the need for extensive manual tuning of cost functions or precise modeling of vehicle-terrain inter- actions, making it more accessible for deployment in diverse environments"}, {"title": "II. RELATED WORK", "content": "Classical work in off-road navigation [1]-[5] has focused on creating the costmap of the environment from sensors' data to represent navigation cost associated with the various types of visual and geometric features of the environment. Earlier approaches [1], [3] relied on feature engineering while later approaches [6], [7] relied on deep learning based semantic segmentation to represent the visual and geometric features of the surrounding terrain. These later approaches train the semantic segmentation pipeline from scratch and use hand designed cost functions for the planner. Although, these costmaps provide rich information for the downstream planning tasks, tuning them to capture the complex dynamic interactions while navigating on various terrain types is extremely challenging and requires significant domain ex- pertise.\nRecent advances in Deep learning have inspired re- searchers in robotics community to develop end-to-end learn- ing algorithms [8], [9] that directly learn the mapping from sensor information to control commands thereby bypassing the need for manual costmap creation and tuning. Despite the promises of these end-to-end approaches, they still require a large amount of data and show poor generalisation to different settings (domain adaptation). Moreover, the black box nature of these approaches make them very hard to debug and deploy on real systems.\nTo deal with these challenges, recent research has focused on learning-based algorithms that combine the strengths of classical and more recent end to end approaches. These efforts aim towards an acceptable trade off between domain expertise, explain-ability and training data requirements.\nRecent works like [10]\u2013[13] leverage human driving data to either directly learn the costmap of the environment or learn the traversability of various terrain types. These costmaps are crucial to solve the optimal control problem and generate appropriate vehicle controls. While these methods effectively model the tire-terrain interactions of the vehicle, they often overlook the higher level reasoning required to navigate in challenging off-road environments. Additionally, these methods primarily depend on real-world data to learn costmaps and traversability. In contrast, our research demon- strates the potential of simulation platforms to learn a local planning algorithm."}, {"title": "III. PROBLEM STATEMENT", "content": "Given a predefined, ordered set of waypoints in an off- road environment, our objective is to develop a local planning algorithm that enables a vehicle to follow these waypoints as accurately as possible. In doing so, the algorithm should incorporate human preferences in selecting the type of terrain the vehicle traverses. In the following section we provide some definitions that are necessary to formalize what we mean by human preferences, but before we do that we discuss some of the assumptions made in this work:\n\u2022 Off-road environment: planar geometry, consists of 4 terrain types: Non traversable (trees, big rocks etc), water, rock and mud.\n\u2022 Vehicle type: Differential drive kinematics, accepts lin- ear and angular velocities as commands.\n\u2022 We consider the discrete case in this work with a step size of \u0394t"}, {"title": "A. Definitions", "content": "\u2022 Pose: A pose represents vehicle position and orientation in a local frame and is given by a 3-tuple (x, y, \u03b8), where (x, y) is the position of the vehicle in a 2-D local frame and \u03b8 is the orientation.\n\u2022 Reference path: Reference path (P) is an ordered set of poses that are collected by manually driving the vehicle in off road environment.\n\u2022 Control Commands: The control command for the vehicle is represented by a 2-tuple (v, \u03c9), where v is the linear velocity and \u03c9 is the angular velocity commands.\n\u2022 Trajectory: We define a Trajectory T as a finite set of ordered poses: $T = [{x_t, y_t, \\theta_t}_{t=1}^n]$. In this work, we consider trajectories of fixed length. The poses in the trajectory are assumed to be generated by the vehicle (or its kinematic model) at a fixed time step \u0394t, while executing a constant control command over the entire horizon of n steps. This results in a one-to-one mapping between the control commands and the trajectory at every instant.\n\u2022 Human preference: At any given moment, human preference is defined by the control commands chosen while following a predefined set of waypoints. Given the one-to-one correspondence between control commands and trajectories, human preference can alternatively be described by the trajectory selected at that instant.\n\u2022 Preference set S: The preference set S is a fixed set that represents all the different preferences (Tra- jectories) available for the human operator at any in- stant. To construct S we first discretize the control commands-linear and angular velocities. Let $A = [{v^i, \\omega^i}_{i=1}^m]$, represent this fixed discrete set of linear and angular velocities. Assuming the vehicle starts at [0,0,0], we then apply forward simulation using the differential drive kinematic model to generate m distinct trajectories over a pre defined time horizon n.\n$T^i = [{x_t^i, y_t^i, \\theta_t^i}_{t=1}^n]$\n$x_t^i = x_{t-1}^i + v^i\\Delta t cos(\\theta_{t-1}^i)$\n$y_t^i = x_{t-1}^i + v^i\\Delta t sin(\\theta_{t-1}^i)$\n$\\theta_t^i = \\omega^i\\theta_{t-1}^i\\Delta t$\n${x_1^i, y_1^i, \\theta_1^i} = {0,0,0} \\forall i \\in {1, .., m}$\nThe set $S = {T^i}_{i=1}^m$ is defined as the preference set. We also refer to A as the control set. Both the preference set and the control set are indexed sets where an index is used to identify the control command/trajectory uniquely. In this work we are only interested in learning the lateral control for the vehicle and fix v is to 1m/s, $(\\forall i \\in {1, .., m})$."}, {"title": "IV. METHODOLOGY", "content": "To characterize the various features of the local envi- ronment and associating them to human preferences, we introduce \"utility feature\" $(U(S, P) \\in R^{m \\times 5})$, defined for a preference set (S) and a reference path (P):\n$U(S) = \\begin{bmatrix} u^1(T^1) & u^2(T^1) & ... & u^5(T^1, S, P) \\\\ u^1(T^2) & u^2(T^3) & ... & u^5(T^2, S, P) \\\\ : & : & & : \\\\ u^1(T^m) & u^2(T^m) & ... & u^5(T^m, S, P) \\\\ \\end{bmatrix}$\n$T_i \\in S, \\forall i \\in 1, ..m$\nHere $u^1(T), .., u^4(T)$ are utility functions associated with 4 different terrain features of the environment and only de- pend on a single trajectory. The utility function $u^5(T, S, P)$ represents the distance-based utility, which is dependent on a specific trajectory within the preference set S, the preference set as a whole, and the reference path P. In the next section we discuss these utility functions in detail."}, {"title": "A. Utility Functions", "content": "The utility feature defined consists of several utility functions each representing a specific feature of the environment. In this work, we consider five utility functions, which are categorized into terrain utility functions and dis- tance utility functions. The first four, $u^1(T)$ through $u^4(T)$, are terrain utility functions that correspond to different terrain types in the environment: non-traversable, water, rock, and mud, respectively. The fifth utility function, $u^5(T, S, P)$, is a distance utility function, which measures a trajectory's relative closeness to the reference path P with respect to all the other trajectories in the preference set S. To calculate the utility functions corresponding to the 4 terrain types, we project the trajectory T on to the camera plane and then use the following equation.\n$u^k(T) = u^k(g(T)) = \\frac{1}{n}\\sum_{j=1}^{j=n} 1^k(I(j))$\nHere k \u2208 1, 2, 3, 4 is the identifier index for the four different terrain types (1-non traversable, 2-water, 3-rock, 4-mud). The projection of T on to the camera plane is given by g(T). $I(j) \\in {1,2,3,4}$, is the pixel label(in the segmented image) that intersects with the $j^{th}$ point in the trajectory and $1^k$ is the indicator function:\n$1^k (x) = \\begin{cases} 1 & \\text{if } x = k \\\\ 0 & \\text{otherwise} \\end{cases}$\nThe distance utility function for a trajectory is defined in the context of a preference set and a reference path. We assume that we are given a preference set $S = {T^i}_{i=1}^m$ and for each of these trajectories $T_i \\in S$, the distance between the last point on the trajectory and the closest waypoint in the reference path to this last point is represented by $d(T^i, P)$. Now we can define the distance utility function $(u^5(T_i, S, P))$:\n$u^5 (T^i, S, P) = 1 - \\frac{d(T^i, P) - d_{min}}{d_{max} - d_{min}}$\nwhere:\n$d_{min} = \\min_{T \\in S} d(T^i, P)$\n$d_{max} = \\max_{T \\in S} d(T^i, P)$\nThus the distance utility function represents the relative closeness of a trajectory from the waypoints within a sample of trajectories."}, {"title": "B. Learning human preferences using demonstrations", "content": "To learn human preferences from human demonstrations we propose a supervised learning approach. Specifically, we frame the problem as a classification task where, at each time step, the planner selects a trajectory from a fixed preference set S of trajectories. The labels for this classification task are derived from human demonstrations, where at each time step, the human operator selects a trajectory from the same fixed preference set S while following a reference path. During this demonstration, the human operator follows the reference path as closely as possible, while simultaneously avoiding or preferring different terrain types based on the real-time camera feed displaying the environment ahead of the vehicle. In the following subsections we define various terms required to formalize the supervised learning problem.\n1) Trajectory Labels: During the data collection the hu- man operator chooses a control command from A at ev- ery instant. The index of this control command uniquely identifies the trajectory/control preference and provides us the ground truth label for that instant. The index label are then converted to one-hot-encoded vectors for the supervised learning training. For an instant t, we use $L_t$ to represent the corresponding ground truth one-hot-encoded vector.\n2) Prediction: Given the utility feature $U_t(S, P)$ at each instant t, the classifier outputs the probabilities $F_t$ of all the trajectories in the reference set. We use Eq. 2 to calculate the first 4 terrain utility functions. To calculate the last utility function we first transform all the waypoints to the local vehicle frame and then use Eq. 4. We finally use to calculate the utility feature $U_t(S, P)$.\nWe implement the classifier using a neural network, de- noted as C, as shown in Fig. 1. C maps the utility feature $(U_t(S, p)$ to the vector of predicted probabilities $(F_t)$ of all the trajectories in the preference set (S). The input layer in C compresses the utility features into an m \u00d7 1 feature vector, which is then transposed and passed through a block of three fully connected hidden layers, followed by another transpose and a softmax operation:\n$F_t = C(U_t(S, P))$\n3) Data collection: Given a reference set of waypoints the human operator is asked to drive the vehicle in the off road environment. At every instant t we record the ground truth labels $L_t$ and calculate the prediction vector $F_t$ using . We aggregate this data to construct the training dataset D:\n$D = [{F_t, L_t}_{t=1}^N]$\nwhere N is total points in D.\n4) Learning objective: Given the dataset D we use the cross entropy loss to minimize the error between the predicted probabilities and the ground truth labels:\n$\\min \\sum_{t=1}^N -L_t \\log(F_t)$"}, {"title": "V. TRAINING AND EVALUATION", "content": "A. Simulation Setup\nWe evaluated our algorithm using the AirSim [14] sim- ulation environment. We developed a Warthog unmanned ground vehicle (UGV) model and created an off-road en- vironment with four distinct terrain types: mud, rock, water, and non-traversable. We attach a front facing camera and a GPS sensor on the warthog to get the segmentation images and the vehicle pose respectively. The camera is situated at a height of 1m from the ground and is tilted by 30 degrees downward to get a better view of the terrain in front.\n\u2022 The simulation environment provides semantic segmen- tation images of size (640 \u00d7 480) at 10Hz.\n\u2022 The internal controller of the warthog expects linear and angular velocities as control commands at 30Hz.\n\u2022 A joystick is used to drive the warthog manually in the environment which provides linear velocities in range of (0, 1m/s) and angular velocity in the range of (-1 rad/s to 1 rad/s).\n\u2022 The reference path is collected by manually driving the vehicle around in the environment.\n\u2022 During the training run, we discretized the joystick's angular velocity into 21 bins with a resolution of 0.1 rad/s while maintaining a constant linear velocity of 1m/s. This discretization resulted in 21 distinct trajec- tories within the preference set S\nGiven the inherent subjectivity of human driving behavior, we establish a terrain preference hierarchy to properly eval- uate our algorithm:\nmud > rock > water > non traversable\nThis qualitative ordering implies that a human driver would prefer to traverse muddy terrain over rocky terrain, choose rocky terrain over water, and avoid non-traversable terrain entirely. By defining this preference order, we can effectively assess our algorithm's performance in replicating human-like decision-making across different terrains.\nTo evaluate our algorithm, we structured the training and testing phases to progressively assess its generalization capa- bilities. During training, we provided scenarios to the human driver that required making choices between terrain types that are consecutive in Eq. 8. This means the driver only encountered decisions between adjacent terrain preferences in our established hierarchy. In contrast, during testing, the algorithm was presented with scenarios involving terrains without any such restrictions, allowing for choices between non-consecutive terrain types.\nFurthermore, we altered the shapes of the terrain patches during testing to configurations not present during training. This variation ensures that the algorithm is evaluated on its ability to handle unfamiliar terrain shapes and combinations, thereby testing its robustness and adaptability beyond the trained scenarios"}, {"title": "B. Training", "content": "Fig. 3a presents a top-down view of the training environ- ment in AirSim. The reference path for the human operator, shown in white, corresponds to a trail within a grassy landscape that the operator is instructed to follow. This trail includes four terrain patches\u2014water, rock, mud, and non- traversable areas designed to present different scenarios to the operator. Although the grassy terrain is traversable, in this work we assume that we want to follow the waypoints as closely as possible so the warthog cannot simply take very wide turns to avoid all the terrain types. Note that during the training , we only provide choices between terrain types that are consecutive in Eq.\nFig. 3b shows the actual path navigated by the human operator during training. The scenarios where the operator deviates from the reference path are numbered from 1 to 10. The first four deviations occur in areas containing only a single terrain (no choice), which the operator simply avoids. In contrast, the subsequent six deviations involve scenarios with two or more terrain types, requiring the operator to make choices between them.\nWe collected the training data as described at 10Hz. During the data collection we drive the warthog for approximately 5 mins and collected 2726 samples. This data is then divided into (80,20) split of training examples and validation examples. We then performed 15 epochs of training using the Adam optimizer [15] with a learning rate of 1 \u00d7 10\u22123 and batch size of 256.\nC. Testing\nFig. 4 shows the reference path and scenarios for test- ing. We evaluate our planner on 7 different scenarios that cover a wide variety of terrain configurations in the off- road environment. The first two scenarios have rock and water terrain types with different shape of terrain patches than the ones used in training. The planner trajectory for this scenario is shown as we can see from the trajectory that the warthog is able to avoid terrain features of different sizes. The terrain shapes are also kept different from the ones in training data of all the subsequent scenarios."}, {"title": "VI. CONCLUSIONS", "content": "In this work we presented a local planning algorithm that uses monocular camera to learn human driving preferences in off-road settings. We presented various experiments in simulation to show the effectiveness of the algorithm. The planner shows quick learning and adaptation, requiring only 5-10 minutes of demonstration data to effectively navigate in challenging off-road environments. Unlike current ap- proaches that rely on extensive labeled datasets or precise sensor calibration, our method can generalize to new terrain configurations not seen during training. This ability to adapt makes our approach particularly suitable for the diverse and unpredictable nature of off-road navigation.\nThe success of our local planner in accurately emulating human driving preferences in off-road scenarios, coupled with its quick adaptability, makes it an attractive candidate for autonomous vehicle navigation in challenging terrains. This research paves the way for more intuitive and human- like autonomous driving solutions but also significantly re- duces the barrier to entry for deploying such technologies in diverse off-road environments. Future work will focus on validating the planner's performance in real-world off-road settings to assess its sim-to-real transfer capabilities along with expanding the range of terrain types, incorporating additional sensor modalities, and integrating with higher- level path planning algorithms to enhance its applicability in real-world autonomous systems."}]}