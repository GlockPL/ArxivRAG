{"title": "Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint", "authors": ["Tan D. Tran", "Canh V. Pham", "Dung T. K. Ha", "Phuong N.H. Pham"], "abstract": "This work proposes an efficient parallel algorithm for non-monotone submodular maximization under a knapsack constraint problem over the ground set of size n. Our algorithm improves the best approximation factor of the existing parallel one from 8 + \u20ac to 7 + \u20ac with O(log n) adaptive complexity. The key idea of our approach is to create a new alternate threshold algorithmic framework. This strategy alternately constructs two disjoint candidate solutions within a constant number of sequence rounds. Then, the algorithm boosts solution quality without sacrificing the adaptive complexity. Extensive experimental studies on three applications, Revenue Maximization, Image Summarization, and Maximum Weighted Cut, show that our algorithm not only significantly increases solution quality but also requires comparative adaptivity to state-of-the-art algorithms.", "sections": [{"title": "1 Introduction", "content": "A wide range of instances in artificial intelligence and machine learning have been modeled as a problem of Submodular Maximization under Knapsack constraint (SMK) such as maximum weighted cut [Amanatidis et al., 2020; Han et al., 2021], data summarization [Han et al., 2021; Mirzasoleiman et al., 2016], revenue maximization in social networks [Han et al., 2021; Cui et al., 2023a; Cui et al., 2021], recommendation systems [Amanatidis et al., 2021; Amanatidis et al., 2020]. The attraction of this problem comes from the diversity of submodular utility functions and the generalization of the knapsack constraint. The submodular function has a high ability to gather a vast amount of information from a small subset instead of extracting a whole large set, while the knapsack constraint can represent the budget, the cardinality, or the total time limit for a resource. Hence, people are interested in proposing expensive algorithms for SMK these years [Amanatidis et al., 2021; Han et al., 2021; Cui et al., 2023a; Pham et al., 2023; Amanatidis et al., 2020].\nFormally, a SMK problem can be defined such as given a ground set V of size n, a budget B > 0, and a nonnegative submodular set function (not necessary monotone) $f: 2^V \\rightarrow R^+$. Every element $e \\in V$ has its positive cost c(e). The problem SMK asks to find $S \\subseteq V$ subject to $c(S) = \\sum_{e \\in S} c(e) \\leq B$ that maximizes f(S).\nOne of the main challenges of SMK is addressing big data in which the sizes of applications can grow exponentially. The modern approach is to design approximation algorithms with low query complexity representing the total number of queries to the oracle of f. However, required oracles of f are often expensive and may take a long time to process on the machine within a single thread. Therefore, people think of designing efficient parallel algorithms that can leverage parallel computer architectures to obtain a good solution promptly. This motivates the adaptive complexity or adaptivity [Balkanski and Singer, 2018] to become an important measurement of parallel algorithms. It is defined as the number of sequential rounds needed if the algorithm can execute polynomial independent queries in parallel. Therefore, the lower the adaptive complexity of an algorithm is, the higher its parallelism is.\nIn the era of big data now, several algorithms that achieve near-optimal solutions with low adaptive complexities have been developed recently (See Table 1 for an overview of low adaptive algorithms). As can be seen, although recent studies make an outstanding contribution by significantly reducing the adaptive complexity of a constant factor approximation algorithm from O(log\u00b2 n) to O(log n), there are two drawbacks, including (1) the high query complexities make them become impractical in some instances [Ene and Nguyen, 2020] and (2) there is a huge gap between the high approximation factors of low adaptivity algorithms, e.g. [Amanatidis et al., 2021; Cui et al., 2023a; Cui et al., 2023b], compared to the best one, e.g. [Buchbinder and Feldman, 2019]. This raises to us an interesting question: Is it possible to improve the factor of an approximation algorithm with near-optimal adaptive complexity of O(log n)?"}, {"title": "Our contributions.", "content": "In this work, we address the above question by introducing the AST algorithm for the non-monotone SMK problem. AST has an approximation factor of 7 + \u20ac, within a pair of O(log n) adaptivity, \u014c(nk) query complexity, where \u20ac is a constant input. Therefore, our algorithm improves the best factor of the near-optimal adaptivity algorithm in [Cui et al., 2023a]. We investigate the performance of our algorithm on three benchmark applications: Revenue Maximization, Image Summarization, and Maximum Weighted Cut. The results show that our algorithm not only significantly improves the solution quality but also requires comparative adaptivity to existing practical algorithms.\nNew technical approach. It is noted that one popular approach to designing parallel algorithms with near-optimal adaptivity of O(log n) is based on making multiple guesses of the optimal solution in parallel and adapting a threshold sampling method\u00b9, which selects a batch of elements whose density gains, i.e., the ratio between the marginal gain of an element per its cost, are at least a given threshold within O(log n) adaptivity [Amanatidis et al., 2021; Cui et al., 2023a]. By making the guesses of the optimal along with calling the threshold sampling multiple times in parallel, the existing algorithms could keep the adaptive complexity of O(log n) and obtain some approximation ratios.\nFrom another view, we introduce a novel algorithmic framework named \"alternate threshold\" to improve the approximation factor to 7 + \u20ac but keep the same adaptivity and query complexity with the best one [Cui et al., 2023a]. Firstly, we adapt an existing adaptive algorithm to find a near-optimal solution within O(log n) adaptivity and give a O(1) number of guesses of the optimal solution. Then, the core of our framework consists of a constant number of iterations. It initiates two disjoint candidate sets and then adapts the threshold sampling to upgrade them alternately during iterations: one is updated at odd iterations, and another is updated at even iterations. Thanks to this strategy, we can find the connection between two solutions for supporting each other in evaluating the \"utility loss\" after each iteration. At the end of this stage, we enhance the solution quality by finding the best element to be added to each candidate's subsets (pre-fixes of i elements) without violating the budget constraint.\nIt must be noted that our method differs from the Twin Greedy-based algorithms [Han et al., 2020; Pham et al., 2023; Sun et al., 2022], which update both candidate sets at the same iterations but do not allow the integration of the threshold sampling algorithm for parallelization. Besides, we carefully analyze the role of the highest cost element in the optimal solution to deserve more tightness for the problem."}, {"title": "2 Related Works", "content": "This section focuses on the related works for the non-monotone case of the SMK problem.\nFirstly, regarding the non-adaptive algorithms, the first algorithm for the non-monotone SMK problem was due to [Lee et al., 2010] with the 5 + \u20ac factor and polynomial query complexity. Later, several works concentrated on improving both approximation factor and query complexity [Buchbinder and Feldman, 2019; Gupta et al., 2010; Mirzasoleiman et al., 2016; Li, 2018; Sun et al., 2022; Pham et al., 2023; Han et al., 2021]. In this line of works, algorithm of [Buchbinder and Feldman, 2019] archived the best approximation factor of 2.6 but required a high query complexity; the fastest algorithm was proposed by [Pham et al., 2023] with 4 + \u20ac factor in linear queries. For the non-monotone Submodular Maximization under Cardinality (SMC) problem, which finds the best solution that does not exceed k elements to maximize a submodular objective value, the best factor of 2.6 of the algorithm in [Buchbinder and Feldman, 2019] still held. Besides, a few algorithmic models have been proposed for improving running time [Badanidiyuru and Vondr\u00e1k, 2014; Kuhnle, 2021b; Li et al., 2022; Buchbinder et al., 2015]. Among them, the fastest algorithm belonged to [Buchbinder et al., 2015] that provided a e + \u20ac factor within O(nlog(1/\u20ac)/e\u00b2) queries. However, the above approaches couldn't be parallelized efficiently by the high adaptive complexity of \u03a9(n).\nThe adaptive complexity was first proposed by [Balkanski and Singer, 2018] for the SMC problem. Regarding adaptivity-based algorithms for non-monotone SMK, the first one belonged to [Ene and Nguyen, 2019] with e + \u0454 and O(log n) adaptive complexity. However, due to the high query complexity of accessing, the multi-linear extension of a submodular function and its gradient in their method becomes impractical in real applications [Amanatidis et al., 2021; Fahrbach et al., 2019]. After that, [Amanatidis et al., 2021] devised a (9.465 + \u20ac)-approximation algorithm within O(log n), which was optimal up to a (log log(n)) factor by adopting the lower bound in [Balkanski and Singer, 2018]. It is noted that improving the adaptive complexity of a constant factor algorithm from O(log\u00b2 n) to O(log n) made an"}, {"title": "3 Preliminaries", "content": "Given a ground set $V = \\{e_1, . . ., e_n\\}$ and an utility set function $f : 2^V \\rightarrow R^+$ to measure the quality of a subset $S \\subseteq V$, we use the definition of submodularity based on the diminishing return property: $f : 2^V \\rightarrow R^+$. f is submodular iff for any $A \\subseteq B \\subseteq V$ and $e \\in V \\setminus B$, we have\n$f(e|A) \\geq f(e|B)$.\nEach element $e \\in V$ is assigned a positive cost c(e) > 0. Let $c : 2^V \\rightarrow R^+$ be a cost function. Assume that c is modular, i.e., $c(S) = \\sum_{e \\in S} c(e)$ such that c(S) = 0 iff S = ().\nThe problem SMK asks to find $S \\subseteq V$ subject to $c(S) = \\sum_{e \\in S} c(e) \\leq B$ that maximizes f(S). We denote by a tuple (f, V, B) an instance of SMK. Without loss of generality, f is assumed non-negative, i.e., f(X) \u2265 0 for all $X \\subseteq V$ and normalized, i.e., f(\u00d8) = 0. We also assume there exists an oracle query, which, when queried with the set S returns the value f(S).\nFor convenience, we denote by S \u222a e as $S \\cup \\{e\\}$. Next, we denote by O an optimal solution with the optimal value OPT = f(O) and $r = arg max_{o \\in O} c(o)$. We also define the contribution gain of a set T to a set S as f(T|S) = f(T\u222a S) \u2212 f(S). Also, the contribution gain of an element e to a set $S \\subseteq V$ is defined as $f(e|S) = f(S \\cup \\{e\\}) \u2212 f(S)$ and f({e}) is written as f(e) for any $e \\in V$.\nIn this paper, we design a parallel algorithm based on Adaptive complexity or Adaptivity, which is defined as follows:\nDefinition 1 (Adaptive complexity or Adaptivity [Balkanski and Singer, 2018]). Given a value of oracle of f, the adaptivity or adaptive complexity of an algorithm is the minimum number of rounds needed such that in each round, the algorithm makes O(poly(n)) independent queries to the evaluation oracle.\nIn the following, we recap two sub-problems which our algorithm need to solve: Unconstrained Submodular Maximization and Density Threshold.\nUnconstrained Submodular Maximization (UnSubMax)\nThis problem requires to find a subset $S \\subseteq V$ that maximizes f(S) without any constraint. The problem was shown NP-hard [Feige et al., 2011a].\nTo obtain mentioned approximation factor, our algorithm adapts the low adaptivity algorithm in [Chen et al., 2019] that achieves an approximation factor of (2 + \u20ac) in constant adaptive rounds of $O(log(1/\\epsilon)/\\epsilon)$ and linear queries of $O(n log^3(1/\\epsilon)/\\epsilon^4)$.\nDensity Threshold (DS). The problem receives an instance (f, V, B), a fixed threshold \u03c4 and a parameter \u20ac > 0 as inputs, it asks to find a subset $S \\subseteq V$ satisfies two conditions: (1) f(S) \u2265 c(S) \u00b7 \u03c4; (2) $\\sum_{e \\in V \\setminus S} f(e|S) \\leq \\epsilon \\cdot OPT$.\nTwo algorithms in the literature satisfy the above conditions, including those in [Amanatidis et al., 2021] and [Cui et al., 2023a]. In this work, we adapt the RandBatch algorithm in [Cui et al., 2023a]. RandBatch requires the set I, a submodular function f(\u00b7), and parameters \u20ac, M to control the solution's accuracy and complexities. RandBatch is combined with the aforementioned density thresholds to set up sieves in parallel for SMK. Due to the space limitations, Pseudocode for RandBatch is given in the appendix.\nFor an instance (V, f, B) of SMK, two subsets I, M of V, a fixed threshold \u03b8 and input parameter \u20ac. The performance of RandBatch is provided in the following Lemmas."}, {"title": "4 Proposed Algorithm", "content": "In this section, we introduce AST (Algorithm 1), a (7 + \u20ac)-approximation algorithm in O(log n) adaptivity and O(n\u00b2 log2 n) query complexity.\nAST receives an instance (f, V, B), constant parameters \u03b4, \u03b5, \u03b1 as inputs. It contains two main phases. At the first phase (Lines 1-14), it first divides the ground set V into two subsets: Vo contains elements with small costs, and V\u2081 contains the rest. AST then calls ParSKP1 [Cui et al., 2023a] as a subroutine which returns a (1/8 \u2013 8)-approximation solution within O(log n) adaptive rounds. Based on that, the algorithm can offer O(log(1/\u20ac)/\u20ac) guesses of the optimal solution for the main loop (Lines 4-14). The main loop consists of O(log(1/\u20ac)/\u20ac) iterations; each corresponds to a guess of the optimal. It sequentially constructs two disjoint solutions, X and Y, one at odd iterations and the other at even iterations. The work of the odd and the even is the same. At the odd (or even) ones, it sets the threshold $\u0398_X (\u0398_Y)$ and calls the RandBatch routine with the ground set I and the function f(\u00b7|X) (f(\u00b7|Y)) as inputs to provide the new set Ai (Bi) (Lines 8, 12). It then updates X (Y) and I as the remaining elements (Line 8 or 12).\nThe second phase (Lines 15-24) is to improve the quality of solutions. If c(X\u2081 U Vo) < 6B, this phase first adapts UnSubMax algorithm [Chen et al., 2019] for unconstrained submodular maximization over X1 U Vo to get a candidate solution S\u2081 (Lines 15-16). This step is based on an observation that X\u2081 is important in analyzing the algorithm's performance. It then selects the sets of the first i elements added into X and Y and finds the best elements without violating the total cost constraint (Lines 19-24). Finally, the algorithm returns the best candidate solution (Lines 25-26). The details of AST are depicted in Algorithm 1.\nAt the high level, AST works follow a novel framework that combines an alternate threshold greedy algorithm with the boosting phase. The term \"alternate\" means that candidate solutions are updated alternately with each other in multiple iterations. At each iteration, only one partial solution is updated based on two factors: one guess of the optimal solution and the remaining elements of the ground set that do not belong to the other solution.\nIt should be emphasized that the alternate threshold greedy differs from recent works [Cui et al., 2023a; Amanatidis et al., 2021] where two candidate solutions for each guess are constructed after only one adaptive round. Alternate threshold greedy also differs from the twin greedy method in [Han et al., 2020], which allows updating both disjoint sets in each iteration. For the theoretical analysis, the key to obtaining a tighter approximation factor lines in aspects: (1) the connections between X and Y after each iteration of the first loop and (2) carefully considering the role of r to eliminate terms that worsen the approximation factor.\nWe now analyze the performance guarantees of AST. We consider X and Y after ending the first loop. We first introduce some notations regarding AST as follows.\n\u2022 Xi, Yi is the set of first i elements added into X and Y, respectively."}, {"title": "Therefore", "content": "Therefore\n$\\displaystyle E[f(S)] \\geq \\frac{5}{7} \\frac{(1-\\epsilon)^5}{(1 - \\frac{5 \\epsilon}{7})}OPT > \\frac{1}{7} OPT \\geq (\\frac{1}{7} - \\epsilon) OPT$\nIf c(r) \u2265 (1 \u2013 \u0454)B, we consider two cases:\n- If a) in Lemma 6 happens, then\n$\\displaystyle E[f(S)] \\geq \\frac{5}{7} (1-\\epsilon)^5 OPT > \\frac{1}{7} OPT$\n- If b) in Lemma 6 happens, then\n$\\displaystyle f(O) \\leq  \\frac{\\frac{5}{7}E[f(S)]}{(1-\\epsilon)^3} + (4\\epsilon + \\frac{2}{EM})OPT + (2 + \\epsilon)E[f(S)]$\n$\\displaystyle \\frac{7E[f(S)]}{(1-\\epsilon)^3}  + \\frac{29}{7} \\epsilon OPT$.\nwhere the inequality (22) is due to $\\epsilon M = (\\frac{1}{4\\epsilon} + 1) > \\frac{2}{\\epsilon}$ for $\\epsilon \\in (0, \\frac{4}{7}), \\delta \\in (0, 1)$. It follows that\n$\\displaystyle E[f(S)] \\geq \\frac{7}{7}(1-\\epsilon)^3(1-\\frac{29}{7})OPT > (\\frac{1}{7} - \\epsilon) OPT$\nwhich completes the proof."}, {"title": "5 Experimental Evaluation", "content": "This section evaluates our AST's performance by comparing our algorithm with state-of-the-art algorithms for non-monotone SMK including:\n\u2022 ParSKP1: The parallel algorithm in [Cui et al., 2023a] that runs in O(log n) adaptivity and returns a solution S satisfying $E[f(S)] \u2265 (1/8 \u2013 \\epsilon)OPT$.\n\u2022 ParSKP2: The algorithm in [Cui et al., 2023a] that runs in O(log\u00b2 n) adaptivity and returns a solution of $E[f(S)] \u2265 (1/(5+ 2\\sqrt{2}) \u2013 \\epsilon) OPT$.\n\u2022 ParKnapsack: The parallel algorithm in [Amanatidis et al., 2021] achieves an approximation factor of (9.465 + \u03b5) within O(log n).\n\u2022 SmkRanAcc: The non-adaptive algorithm in [Han et al., 2021] that achieves an approximation factor of 4 + \u20ac in query complexity of O(nlog(k/\u20ac)/\u20ac).\n\u2022 RLA: The non-adaptive algorithm in [Pham et al., 2023] with a factor of 4 + \u20ac in linear query complexity of O(nlog(1/\u20ac)/\u20ac).\nWe experimented with the following three applications:\nRevenue Maximization (RM). Given a network G = (V, E) where V is a set of nodes and E is a set of edges. Each edge (u, v) in E is assigned a positive weight w(u, v) sampled uniformly in [0, 1] and each node is assigned a positive cost c(u) defined as $c(u) = 1 - \\epsilon \\sqrt{\\sum_{(u,v) \\in E} w(u,v)}$ The revenue of any subset S \u2286 V is defined as $f(S) = \\sum_{u \\in V \\setminus S} \\sum_{v \\in S} w_{u,v}$. Given a budget of B, the goal of the problem is to select a set S with the cost at most B to maximize f(S). As in the prior work, [Amanatidis et al., 2021], we can construct the graph G using a YouTube community network dataset [Han et al., 2021] with 39,841 nodes and 224,235 edges."}, {"title": "6 Conclusions", "content": "Motivated by the challenge of the large scale of input data, in this work, we focus on parallel approximation algorithms based on the concept of adaptive complexity. Moreover, the requirement of improving the approximation factor while decreasing the adaptivity down to log(n) motivates us to propose a competitive new algorithm. We have proposed an efficient parallel algorithm AST based on a novel alternate threshold greedy strategy. To our knowledge, our AST algorithm is the first to achieve a constant factor approximation of 7 + \u20ac for the above problem in the aforementioned adaptivity. Our algorithm also expresses the superiority in solution quality and computation complexity compared to state-of-the-art algorithms via some illustrations in the experiment in three real-world applications. In the future, we will address another valuable question: can we reduce the query complexity of parallelized algorithms for the SMK problem?"}, {"title": "Appendix", "content": "In this section, we recap RandBatch [Cui et al., 2023a] (Algorithm 2 of that paper), a frequently used subroutine in the proposed algorithms. For a discussion of the intuition behind RandBatch and rigorous proof of Lemma 1, we refer the reader to [Cui et al., 2023a] and its full version [Cui et al., 2023b].\nA.1 Proof of Lemma 3\nThe proof of Lemma 3 implies from the proof of Lemma 1.We write down the details of the proof for the sake of completeness. We first recap the Lemma 2 in [Cui et al., 2023b] for supporting the proof of Lemma 3.\nLemma 7 (Lemma 2 in [Cui et al., 2023b]). For any $V_{t^*}$ found in Lines 11-12 of RandBatch and any $V_{t^*}$, let $X(u)$ denote the set of elements in $V_{t^*}$ selected before u (note that $V_{t^*}$ is an ordered list according to Line 6 of RandBatch), and X(u) does not include u. Let A and U be the sets returned by RandBatch($\u03b8, I, M, \\epsilon, f (\u00b7), c(\u00b7)$), where the elements in U are {$U_1, U_2, ..., U_s$} (listed according to the order they are added into U). Let $u_j$ be a \u201cdummy element\u201d with zero marginal gain and zero cost for all $s  j  |I|$. Then we have:\n\u2200j\u2208 [|I|] : E[f(uj|{$U_1, ..., U_{j-1}$} \u2229 A \u222a X(uj))|Fj\u22121] \u2265 (1 \u2212 \u03f5)\u00b2\u03c1 \u00b7 E[c(uj)|Fj\u22121]\n(23)\nwhere Fj-1 denotes the filtration capturing all the random choices made until the moment right before uj is selected."}, {"title": "B Missing proofs of Section 4", "content": "We first provide some notations used for the proofs.\n\u2022 X, Y are the sets after the the first loop of AST.\n\u2022 Supposing X and Y ordered: X = {$X_1, X_2, ..., X_{|X|}$}, Y = {$Y_1, Y_2, ..., Y_{|Y|}$}, we conduct: X\u00b2 = {$x_1, x_2, ..., x_i $}, Y = {$y_1, y_2, ..., y_i $}.\n\u2022 t = max{i \u2208 N : c(X\u00b2) + c(r)  B \u2013 c(r)}, u = max{i \u2208 N : c(Y\u00b2) + c(r)  B \u2013 c(r)}.\n\u2022 Xi and Yi are X and Y after iteration i, respectively.\n\u2022 For x \u2208 XUY, we assume that x is added into X or Y at iteration l(x).\n\u2022  is  at iteration i,  is  at iteration i.\n\u2022  and  are  and  at the last iteration when X and Y are considered to update.\n\u2022 For an element e \u2208 XUY, we denote  ,  ,   and  as X, Y,  and  right before e is selected into X or Y, respectively.\n\u2022 O is an optimal solution, O\u2081 is an optimal solution of SMK problem over the instance (V\u2081, f, B).\n\u2022 O' = O\u2081 \\ X1, O\u2033 = O' \\ {r}.\nB.1 Proof of Lemma 5\nProve a), b). In this case we have c(Xt)  B \u2013 c(r)  \u03f5B, we consider following cases:\nCase 1. If Xt = X, denote Y(t) = Y<xt, i.e., the set Y right before the algorithm obtains Xt. For any element e \u2208 O' \\ (X\u222aY(t)), we have c(Xt) + c(e)  B. Therefore we consider O' \\ (X\u222aY(t)) = O'1 \u222a O'2, where O'1 = {e \u2208 O' \\ (X\u222aY(t)) : f(e|Xt)  c(e)}, O'\u00b2 = {e \u2208 O' \\ (X\u222aY(t)) : f(e|Xt) \u2265 c(e)}. By setting \u2206, at any iteration i we have\n(28)\nB.2 Proof of Lemma 6\nWe prove the Lemma by carefully considering the following cases:\nCase 1. If c(r)  (1 \u2013 \u03f5)B, then B \u2013 c(r) > \u03f5B.\nCase 1.1. If c(Xt) and c(Yu) are both greater than B - c(r). Since c(Xt+1) > \u03f5B and c(Yu+1) > \u03f5B, the algorithm must obtain Xt+1 and Yu+1 after i-th iteration, i \u2265 3. We prove this case when the algorithm obtains Xt before Yu. When considering the subset T \u2286 O' in Lemma 4, the roles of X and Y are the same. Thus we can prove similarly for the remaining case. Since the elements in X are added at the odd iterations, any element e \u2208 O' \\ Xt was not added into X at iteration s = l(xt+1) \u2013 2. It is easy to find that X, \u2286 Xt. By applying Lemma 4, we have\nf(O'\u222a Xt) \u2212 f(Xt \u222a {r}) \u2264 \u2211 f(e|Xt)\n(75)\ne\u2208O'r \\ Xt\n= \u2211 f(e|Xt) + \u2211 f(e|Xt)\n(76)\ne\u2208Y<xt \u2229 O'r\ne\u2208O'r \\ (Xt\u222aY  )\n< \u2211 E[f(e|Y<e)] + \u03f5OPT + \u2211 f(e|Xt).\n(77)\n(1 \u2212 \u03f5)3\ne\u2208Y<xt \u2229 O'r\ne\u2208O'r \\ (Xt\u222aY  )\nBesides, applying Lemma 4 again gives:\nf(O'\u222a Yu) \u2212 f(Yu \u222a {r}) = \u2211 f(e|Yu) + \u2211 f(e|Yu)\n(78)\ne\u2208O'r \u2229 X   \ne\u2208O'r \\ (X   \u222aYu)\n< \u2211 E[f(e|X<e)] + OPT + \u2211 f(e|Yu).\n(79)\n(1 \u2212 \u03f5)3\ne\u2208O'r \u2229 X   \ne\u2208O'r \\ (X   \u222aYu)\nBy the submodularity of f and Xt \u2229 Yu = \u00d8, we have\nf(O') \u2264 f(O'\u222a Xt) + f(O' \u222aYu).\n(80)\nFrom now on, it is necessary to bound some terms of the right hand of (77) and (79).\na) Bound of \u2211f(e|Xt) + \u2211E[f(e|X<e)].\ne\u2208O'r \\ (Xt\u222aY  )\nne\u2208O'r \\ (Xt\u222aY  )\n- If xt+1 \u2209 O'r, O'r \\ (Xt\u222aY  ) = O'r \\ (Xt+1\u222aY  ) and O'r \u2229 Xt = O'r \u2229 Xt+1. Any elemente \u2208 O'r \\ (Xt+1\u222aY  )\nwas not added in to X at iteration s = l(xt+1) \u2013 2. By applying Lemma 1, we have\n\u2211f(e|Xt) = \u2211f(e|Xs)\n(81)\ne\u2208O'r \\ (Xt+1\u222aY  )\ne\u2208O'r \\ (Xt+1\u222aY  )\n= \u2211f(e|Xs)\n(82)\ne\u2208O'r \\ (Xt+1\u222aY  )\n\u2264 c(O'r \\ (Xt+1 \u222a Y <xt)). \u03b8(s+1) (s+1)\n(1 \u2212 \u03f5)2\n+ OOPT .\n(83)\nEM\nOn the other hand, since c(Xt+1) > B \u2212 c(r) \u2265 c(O'r) so c(Xt+1 \\ O'r) > c(O'r \\ Xt+1) \u2265 c(O'r \\ (Xt+1 \u222aY <xt)). Combine this with (83), we have\n(84)\nFrom (86) and (83) with a note that X  \u2286 Xt we obtain\nIf xt+1 \u2208 O'r, O'r \u2229 Xt+1 = (O'r \u2229 Xt)\u222a{xt+1} and O'r \\ (Xt\u222aY  ) = O'r \\ (Xt+1\u222aY  )\u222a{xt+1}. Therefore\n\n(93)\n- b) Bound of \u2211 f(e|Yu). Any element e \u2208 O'r \\ Yu was not added into Yu at the iteration l(yu) \u2013 2. By\napplying Lemma 1 with a note that c(O'r \\ (X   \u222aYu))  B \u2212 r \u2264 c(Yu+1), we obtain\n\u2211 f(e|Yu) \u2264 c(O'r \\ (X   \u222aYu)) \u03b8(u+1)\n(99)\ne\u2208O'r \\ (X   \u222aYu)(1 \u2212 \u03f5)2\n\u03b8(u+1)\n\u2264 c(Yu+1)\n+ OOPT\n.\n(100)\n(1 \u2212 \u03f5)2 EM\n\u2264 f(Yu+1)\nOPT\n+ .\n(101)\n(1 \u2212 \u03f5)4 EM"}, {"title": "We define an even integer l as follows", "content": "We define an even integer l as follows\n(61)\nIt follows that\nWe consider two following cases:\nCase 2.1. If Yu+1 \u2286 Y\u2081, we further consider two sub-cases:\n\u2022 If c(Yi)  (1 \u2013 \u03f5)B, by Lemma 1 we have\nE[f(S)] \u2265 E[f(Yi)] \u2265 E[c(Yi)](1 \u2013 \u03f5)\u00b2  \u2265 (1 \u2212 \u03f5)5 \u03b8Y .\n- If c(Y\u2081) < (1 \u2013 \u03f5)B. Since c(O'r) < \u03f5B, c(Y\u2081) + c(e)  B, for all e \u2208 O\u2019r. Combine this with the Lemma 4 and\nLemma 1, we have\nf(Yu \u222a O') \u2212 f(Yu \u222a {r}) \u2264 \u2211 f(e|Yi)\n(62)\ne\u2208O'r \\ Y\n= \u2211 f(e|Yi) + \u2211 f(e|Yi)\n(63)\ne\u2208O'r \u2229 X   \ne\u2208O'r \\ (X   \u222aY)\n< \u2211 E[f(e|X<e)] + \u03f5OPT + c(O'r)\u03b8Yi(Y  ) + OPT .\n(64)\n(1 \u2212 \u03f5)3\ne\u2208O'r \u2229 X   \nE[f(X-1)]\n< \u2211 + \u03f5 OOT + OTOOT\n(65)\n\n\u2022 Case 2.2. If Y\u2081 \u2282 Yu+1, then c(Yi) + c(e)  B for all e \u2208 O'. We thus also have"}]}